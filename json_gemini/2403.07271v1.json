{"title": "Anderson acceleration for iteratively reweighted (1 algorithm", "authors": ["Kexin Li"], "abstract": "Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving sparse optimization problems with nonconvex and nonsmooth regularization. The development of its acceleration algorithm, often employing Nesterov acceleration, has sparked significant interest. Nevertheless, the convergence and complexity analysis of these acceleration algorithms consistently poses substantial challenges. Recently, Anderson acceleration has gained prominence owing to its exceptional performance for speeding up fixed-point iteration, with numerous recent studies applying it to gradient-based algorithms. Motivated by the powerful impact of Anderson acceleration, we propose an Anderson-accelerated IRL1 algorithm and establish its local linear convergence rate. We extend this convergence result, typically observed in smooth settings, to a nonsmooth scenario. Importantly, our theoretical results do not depend on the Kurdyka-Lojasiewicz condition, a necessary condition in existing Nesterov acceleration-based algorithms. Furthermore, to ensure global convergence, we introduce a globally convergent Anderson accelerated IRL1 algorithm by incorporating a classical nonmonotone line search condition. Experimental results indicate that our algorithm outperforms existing Nesterov acceleration-based algorithms.", "sections": [{"title": "1 Introduction", "content": "In this paper, we aim to solve a class of nonconvex and nonsmooth optimization problems as follows:\nmin F(x) := f(x) +\u03bb\u03c6([xi]), (P)\nxERn\ni=1\nwhere f: Rn \u2192 R is Lipschitz continuously differentiable function and \u03c6 : R+ \u2192 R+ is a nonconvex and nonsmooth regularization function. Additionally, a user-prescribed constant A\u2208 R++ often refers to the regularization parameter. Problem (P) arises from diverse fields ranging from signal processing [1, 2], biomedical informatics [3, 4] to modern machine learning [5, 6].\nThe function \u03c6 assumes various forms such as EXP approximation [7], LPN approximation [8], LOG approximation [9], FRA approximation [8], TAN approximation [10], SCAD [11], and MCP approximation [12]. In Table 1, we delineate the explicit forms of different cases. Here, p is a hyperparameter that governs the sparsity. These common l0 approximation functions adhere to the assumptions outlined below:\nDespite the potential wide applicability of (P), its nonconvex and nonsmooth nature generally makes it challenging to solve. Numerous researchers have focused on the design and theoretical analysis of algorithms for such nonconvex and nonsmooth sparse optimization problems [13\u201317]. The iteratively reweighted l1 algorithm, due to its powerful performance, has obtained extensive attention [15, 18\u201322]. IRL1 is a special case of the Majorization-minimization algorithm. The core of IRL1 approximates the nonsmooth regularization by a weighted l1 norm in each iteration. Therefore, IRL1 iteratively solves a local convex approximation model to address the objective problem (P).\nSince Nesterov introduced the extrapolation technique for the gradient descent algo- rithm [23], a variety of momentum-based acceleration techniques have been utilized in first-order algorithms. A notable example is the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) proposed by Beck and Teboulle [24]. Concurrently, numerous studies have been dedicated to the design of acceleration algorithms for the IRL1 algorithm."}, {"title": null, "content": "However, due to the non-convexity and non-smoothness of problem (P), analyzing the convergence and complexity of the acceleration IRL1 algorithm often poses significant challenges. Existing literature primarily utilizes Nesterov's extrapolation technique to accelerate the algorithm. These Nesterov acceleration-based algorithms typically rely on the Kurdyka-\u0141ojasiewicz (KL) condition to establish convergence and complexity, with their complexity outcomes frequently tied to the KL coefficient. However, numer- ous practical problems cannot fully ensure the KL property. Furthermore, estimating the KL coefficient for a given function often presents substantial challenges.\nThe current main results of the accelerated iteratively reweighted l1 algorithm are as follows: Yu and Pong [19] proposed three different versions of the IRL1 acceleration algorithms with the Nesterov technique and demonstrated the global convergence of the algorithms under the Kurdyka-\u0141ojasiewicz (KL) conditions. However, they did not conduct a complexity analysis. Additionally, they stipulated that f must be a smooth\n(t)\nconvex function and that the limit lim must exist. This requirement restricts t\u21920+\nthe use of certain regularization, such as the commonly used LPN regularization. Fur- thermore, Wang and Zeng [18] introduced an accelerated iteratively reweighted L1 algorithm with the Nesterov technique, specifically for LPN regularization. They pro- vided a global convergence guarantee and local complexity analysis for their algorithm under the Kurdyka-Lojasiewicz (KL) conditions. The algorithm has local linear conver- gence when the KL coefficient is not greater than 1/2, and local sublinear convergence when the KL coefficient exceeds 1/2.\nFrom another perspective, classical sequence-based extrapolation techniques have garnered increasing attention. These include minimal polynomial extrapolation [25], reduced rank extrapolation [26], and Anderson acceleration [27]. Initially, these tech- niques are commonly used method for accelerating fixed-point iteration. Anderson acceleration has obtained great attention due to its powerful performance, which lever- ages the information from historical iterates and sums the weighted historical iterates to obtain new iterate. Anderson acceleration was initially introduced to accelerate the solution process of nonlinear integral equations [27]. This technique was later generalized to address the general fixed-point iteration [28, 29]. Currently, Anderson acceleration has been widely used and analyzed. In recent times, numerous works have integrated Anderson acceleration into optimization algorithms. Fu, Zhang, and Boyd [30] applied it to the Douglas-Rachford Splitting algorithm. Poon and Liang [31] uti- lized Anderson acceleration to augment the performance of the ADMM method. The design and convergence analysis of Anderson accelerated algorithm has been the sub- ject of comprehensive study. Although Anderson acceleration does not use derivatives in its iterations, proving its convergence without continuous differentiability presents a challenge. Convergence analyses have been conducted for the linear case [28, 29], the continuously differentiable case [32], and the Lipschitz continuously differentiable case [29, 33]. Recent studies have presented local convergence and local linear complexity outcomes for Anderson acceleration in certain specific non-smooth fixed point itera- tion problems. For a fixed point issue that can be divided into the sum of a smooth component and a non-smooth component with a minor Lipschitz constant, Bian et al.[34] have demonstrated new findings. Furthermore, Bian and Chen[35] have offered"}, {"title": null, "content": "convergence and complexity outcomes for a non-smooth composite fixed-point itera- tion problem. Additionally, Mai and Johansson [36] applied Anderson acceleration to the classic proximal gradient method."}, {"title": "1.1 Main contributions", "content": "Considering the success of Anderson acceleration for accelerating various problems, we proposed Anderson accelerated iteratively reweighted l1 algorithm. The main contributions of this paper can be summarized as follows:\n1) We present an Anderson accelerated iteratively reweighted l1 algorithm for solving common nonconvex and nonsmooth regularization optimization problem as defined in (P). Under the framework of Anderson acceleration, we establish the local R- linear convergence rate of the algorithm in the nonsmooth setting. Notably, such results are typically observed only in a smooth setting. Moreover, to the best of our knowledge, this is the first work that guarantees linear complexity results for accelerated IRL1 algorithms without the need for KL conditions.\n2) Considering the existing uncertainty about the global convergence of the Anderson acceleration, we introduce a safety strategy rooted in a classical nonmonotone line search condition in [37]. We present a globally convergent Anderson-accelerated IRL1 algorithm based on the strategy. Our algorithm ensures global convergence while preserving computational efficiency.\n3) In our experiments, we verified the theoretical results of our algorithms and demonstrated that they outperform the state-of-the-art accelerated IRL1 algorithm proposed in [18]."}, {"title": "1.2 Notation and preliminaries", "content": "We denote N := {0,1,2...}, Rn as the n-Dimensional Euclidean space and R as the positive orthant in Rn, with R+ being the interior of R. Let ||x||p = (\u03a3i=1|xi|p)1/p with p \u2208 (0,+\u221e) and ||x|| as the 12 norm. Define sign(x) = (sign(x1), ..., sign(xn)T. Define diag(x) \u2208 Rnxn be diagonal matrix with a forming the diagonal elements. Define A(x) := {x \u2208 Rn|i : xi = 0} and I(x) := {x \u2208 Rn|i : xi \u2260 0}. Consider a metric space (M,d), if a fixed-point mapping H : Rn \u2192 Rn exists such that for any real number \u03b3 \u2208 (0,1) and for all x, y in M, the inequality ||H(x) \u2013 H(y)|| \u2264 y||x - Y|| holds, then we define H as a contraction mapping from M to itself. For a set S, we define the relative interior rint(S) be:\nrint(S) := {x \u2208 S : there exists e > 0 such that Be(x) \u2229aff (S) \u2286 S},\nwhere aff (S) is the affine hull of S and Be(x) is a open ball of radius e centered on x. The subdifferential of a convex function f: R\" \u2192 Rat x is the set defined by\ndf(x) = {z \u2208 Rn : f(y) \u2212 f(x) \u2265 <z, y \u2013 x>, \u2200y \u2208 Rn}."}, {"title": null, "content": "For an extended-real-valued function f: Rn \u2192 (-\u221e, +\u221e], if the domain dom f :=\n{x: f(x) < +\u221e} \u2260 0, we say f is proper. A proper function f is said to be closed if it is lower semi-continuous. For a proper function f, its Fr\u00e9chet subdifferential at \u00e6, denoted as OF f(x), is the set\n{\nf(z) - f(x) - <u, z - x>\ndF f(x) = {u \u2208R", "S)": "inf{||x \u2212 s|| : s \u2208 S}.\nThe following assumptions are supposed to be satisfied by the minimizing function F in (P) throughout the paper.\nAssumption 1.(i) The function f has Lipschitz continuous gradient with constant Lf > 0, that is, f(x) \u2212 f(y) \u2264 \u2207f(y)(x - y) + ||x \u2212 y||\u00bd, \u2200 x, y \u2208 Rn.\n(ii) The function \u03c6 is smooth, concave and strictly increasing on (0,+\u221e). Moreover, \u03c6(0) = 0 and it is Fr\u00e9chet subdifferentiable at 0.\n(iii) The function F is non-degenerate in the sense that for any critical point of F, it holds that 0 \u2208 rint dFF(x).\nNext, we recall some basic background of the iteratively reweighted l\u2081 algorithm and Anderson acceleration methods.\nThe basis of iteratively reweighted l1 (IRL1) algorithm.The basis of itera- tively reweighted l\u2081 (IRL1) algorithm The basic version IRL1 is a special case of majorization-minimization instance. To overcome the nonsmooth objective in (P), a common technique used in the IRL1 algorithm is to add a perturbation vector \u20ac \u2208 R+ to the nonsmooth term \u03c6 to have a continuously differentiable relaxed objective F(x, \u0454), i.e.,\nF(x, \u20ac) := f(x) + \u03a3\u03a6([Xi] + \u20aci), (1)\ni=1\nWith such a relaxed optimization model, a key to using IRL1 to solve (1) is to derive a convex majorant. Specifically, given the kth iterate, we have\nF(x, \u20ac) = f(x) + \u03bb\u2211([xi] + \u20aci)\ni=1\nL\n< f(x) + \u2207 f(xk) (x - xk) + ||x - xk||2\nn\n+ \u03a3\u03c6'(x+(xi - x), (2)\ni=1"}, {"title": null, "content": "where the first inequality holds mainly because of the Lf-smoothness of f and the concavity of $, and L > Lf. Consequently, at the k-th iteration, one solves a convex subproblem that locally approximates F to obtain the new iterate \u00e6k+1, i.e.,\nxk+1 \u2190 arg min{G(x;xk, \u20ac*) := Q(x) + \u03a3\u03c9xi]}, (3)\nxERn\ni=1\nwhere \u03c9 := (xk, e) = $'(|x| + \u20ac) and Qk(x) = Q(x;xk) := \u2207 f(xk)Tx + ||x \u2212 xk||2.\nThe subproblem (3) has a closed-form solution:\nxk+1x-[if (xk) \u2013 \u03bb\u03c9] x< [\u2207if(xk) \u2013 \u03bb\u03c9],\n- [\u2207if (xk) + \u03bb\u03c9] x>[\u2207if (xk) + \u03bb\u03c9],\notherwise. (4)\nIn the IRL1 algorithm, the value of e has a significant impact on the performance. A large e makes the subproblem have good properties but will cause the algorithm to miss many local minimum solutions, thereby affecting hitch the performance of the algorithm. Conversely, a small e fails to smooth the problem, leading to challenges in solving the subproblem. Hence, a common strategy is to initialize the algorithm with a large e and gradually reduce it to zero over the iterations. Wang et al. [38] and Lu [15] have each proposed dynamic update methods for e. Additionally, Wang et al. [38] points the locally stable sign property of the {x} that is generated by IRL1 with lp norm, which means that sign(x) remain unchanged for sufficiently large k.\nDenote Wk := diag(wf, ...w). The first-order necessary optimality condition of the subproblem (3) is given as follows\n\u2207Qk(xk+1) + \u03bbWk\u00a2k = 0, (5)\nwhere k \u2208 ||xk+1||1.\nFor F(x, \u20ac) and the local model Qk(\u00b7), we make the following assumption:\nAssumption 2. For F(x\u00ba, \u20ac\u00ba) and Qk(\u00b7), the following statements hold.\n(i) The level set L(F(x\u00ba,\u20ac\u00ba)) := {x|F(x) < F(x\u00b0,\u2208\u00ba)} is bounded.\n(ii) For all k \u2208 N, Qk(\u00b7) is strongly convex with constant M > Lf/2 > 0 and Lipschitz differentiable with constant L > 0.\nThese assumptions for local model Qk(\u00b7) and L(F(x\u00ba, \u20ac\u00ba)) are common in much of the literature on IRL1. They ensure the descent of the function value along the sequence of iterates {x} and also guarantee the boundness of {x}.\nAnderson acceleration. Anderson acceleration initially is used for solving such a fixed-point iteration:\nFind x \u2208 R such that x = H(x), (6)"}, {"title": null, "content": "where H: Rn \u2192 Rn is a mapping. The framework of Anderson accelerated fixed-point iterations is given as follows:\nAlgorithm 1 Anderson accelerated fixed-point iteration\nInput: Given \u00e6\u00ba \u2208 R\u201d. Pick m \u2265 1.\nInitialization: Set k = 0\n1: while not convergence do\n2:Set mk = min(m, k)\n3:Set Rk = [rk, ..., rk-mk], where rk = H(xk) \u2013 xk\n4:Update a \u2190 arg minare=1 || Rka||\n5:Update xk+1 = \u03a3\u03b1Hk-mk+i\n6:Set kk +1\n7:end while\nWe denote Hk = H(\u00e6k) as the vector obtained by mapping \u00e6k through H. Define rk = H(xk)-xk as the residual term and Rk = [rk, ..., rk-mk], where Rk \u2208 Rn\u00d7Rm+1. Given an initial x\u00ba and an integer parameter m \u2265 1, the fundamental concept of Anderson acceleration is to derive a weight vector a \u2208 Rm+1 that minimizes the weighted sum of the previous m + 1 residual terms rk:\nak \u2190 arg min || Rka||. (7)\naTe=1\nSubsequently, the \u00e6k+1 is obtained by computing the weighted sum of Hk, ..., Hk-m\u043a, using the weight vector ak.\nmk\nxk+1 = \u03a3\u03b1Hk-mk+i. (8)\ni=0\nThe subproblem (7) has a closed-form solution\nak =. (9)\nThe cost of solving (7) is O(m\u00b2 + mn). Given that m is typically a very small (com- mon setting is 5 or 15) constant in practice, the computation involved in solving this subproblem is considered trivial. Therefore, Anderson acceleration does not exces- sively increase the computation. It is worth noting that (Rh)TRk may be singular. To guarantee the non-singularity of the least-square subproblem, we can add a Tikhonov regularization of 10-10||Rk ||2 to it (similar to [36, 39])."}, {"title": "2 Anderson accelerated IRL1", "content": "In this section, we describe the proposed algorithm with Anderson acceleration for solving (P)."}, {"title": null, "content": "In our algorithm, we update e by \u025bk+1 = \u03bc\u025bk in iteratively reweighted l\u2081 algorithm, where \u03bc\u2208 (0,1) is a coefficient that controls the decay rate. Then the iteration of the IRL1 algorithm can be regarded as a fixed-point iteration H : R2n \u2192 R2n of the compound variable (x, \u20ac).\nxk+1Hx(xk, ek)arg minern Gk (x)\n=H(x*, \u20ac*) :=[] =. (10)\nHe(xk, ek)\u03bc\u03b5\u03ba\nWhere Ha(xk, ek) = arg min\u2208Rn Gk (x) and He(xk, ek) = \u03bc\u03b5. \u03a4\u03bf brevity, we denote Hk = H(xk,ek), H = Ha(xk, ek) and H = He(xk, ek). Consequently, a natural idea is to use Anderson acceleration to accelerate the IRL1 algorithm. We propose the AAIRL1 algorithm. In each iteration, we apply Anderson acceleration to the map- ping Ha to obtain a new iterate \u00e6k+1, and update \u025bk+1 through the mapping He. We present the framework of AAIRL1 algorithms in Algorithm 2, where we define r\u2081(x, \u20ac) = Hz(x, \u20ac) \u2013 x and r\u2081 = r\u2081(xk, ek).\nAlgorithm 2 The Proposed Anderson Accelerated IRL1 (AAIRL1)\nInput: Given \u00e6\u00ba \u2208 Rn and e\u00ba \u2208 R+. Pick \u03bc\u2208 (0, 1) and m \u2265 1.\nInitialize: Set k = 0.\n1: while not converge do\n2: Set mk = min(m, k)\n3: Set (Hk, ek+1) \u2190 H(xk, ek)\n1\n4: Set R = [r,..., rk-mk], where r = Hk - xk\n5: Update a arg minare=1 || Ra||2\n6: Update xk+1 = \u03a3\u03b1Hk-mk+i\n7: Set kk +1\n8: end while\nAlthough Anderson acceleration does not employ derivatives in its iterations, estab- lishing its convergence in the absence of continuous differentiability poses a significant challenge. The first convergence result was proposed in [29, Theorem 2.3], which demonstrates that for a bounded ma, given the continuous differentiability and contraction of H, Anderson acceleration can ensure the R-linear convergence rate of the fixed-point iteration when initiated near the fixed-point x*.\nThe theoretical analysis of [29, Theorem 2.3] requires the continuous differentiability of H. However, in the context of problem (P), the mapping defined in (10) struggles to meet this requirement. Notably, Wang et al. [38] suggested that IRL1 with lp norm is equivalent to gradient descent at the tail end of the algorithm in a smooth subspace. Inspired by this, we establish a local continuous differentiability of H."}, {"title": "2.1 Local property of H", "content": "Define 0 = [x, \u20ac] \u2208 R\u2033 \u00d7 Rn. We begin by demonstrating that F(0) decreases mono- tonically over iterates {0} that generated by H. Furthermore, we show {k} is"}, {"title": null, "content": "bounded and establish the connection between the fixed-point of H and the first-order stationary point of (P).\nLemma 1. Let Assumption 1 and 2 hold, then the following statements hold:\n(i) The sequence {xk} generated by H is bounded, and F(0k) is monotonically decreas- ing. Additionally, for all k \u2208 N, there exists a constant C > 0 such that ||\u2207Qk(xk+1)||\u221e \u2264 C.\n(ii) Let 0* = [x*,0] be the cluster point of the sequence {0k} that generated by H. 0* =\n[x*,0] is the fixed-point of H and x* satisfies the first-order necessary optimality condition for (P)\nProof. (i) From Assumption 1 and 2, we deduce that\nf(\u00e6k) \u2013 f(\u00e6k+1) \u2265 \u2207 f(ack)F(ack - ack-1) - \ub6a4 ||ack - 20+1||2,\nL\nQk(xk) - Qk (xk+1) \u2265 \u221a f (ack) T (ack-ack-1) - ||ack-20k+1112.\nThen it is straightforward to verify that\nM-Lf\nf(xk) - f(xk+1) \u2265 Qk(xk) - Qk (xk+1) + ||ack - 20k+1||2. (11)\nFurthermore, the concavity of \u03c6 yields\n((x+)+) \u2264 (12+)+) (12+) - 12\uc6d4). (12)\nReferring to equations (11) and (12):\nf(xk, ek) \u2212 f(ack+1, ck+1) \u2265 Gk (ack, ek) \u2013 Gk (ack+1, Ek+1) + M - Lf|lack - ck+1||2. (13)\nFrom the first-order necessary optimality condition (5) of the subproblem (3), we can derive\nG(xk; xk, ek) \u2013 G(xk+1; xk, \u025bk)\n-Qk(xk) - Qk(xk+1) + (x-2x+1)\ni=1\nM\n\u2265 \u221aQk(ack+1)(ack - 20k+1) + 12||ack - 20k+1/12 + 1 \u03a3\u03c9 (x - 2x+1)\ni=1\n=(\u2207Qk(xk+1) + >Wk\u00a2k)T(xk \u2013 xk+1) +\nM\n=|lack xk+1||2. (14)"}, {"title": null, "content": "The first inequality follows from the convexity of Qk (\u00b7) and the absolute value inequal- ity, the second inequality is a result of the strong convexity of Qk(). By combining equations (13) and (14), we obtain\nF(xk, ek) \u2013 F(xk+1, \u025bk+1) > ( (M - \ubf48)||ack \u2013 ack+1||2. (15)\nTherefore, {F(xk, ek)} is monotonically decreasing. Combining this with Assump- tion 2 (i), we have \u2200k \u2208 N, -\u221e < F(xk) \u2264 F(xk,\u2208k) \u2264 F(x\u00ba,\u2208) and {x} C L(F(x\u00ba, \u20ac\u00ba)). By summing up the above inequality from k = 0 to k \u2192 \u221e we imply\n\u221eLf\n\u221e > F(x\u00ba, \u20ac\u00ba) - lim F(x+1, [k+1) \u2265\u2211(M-1)||10k - 20k+1||2. (16)\nk- k\u2192\u221e\ni=02\nThen, we have lim ||xk+1 \u2212 xk|| = 0. The sequences {x} generated by H is bounded. Furthermore, k \u2208 N, \u2203C > 0 such that ||\u2207Qk(xk+1)||\u221e\u2264 C.\n(ii) We prove this by contradiction, define A* = {i : x = 0}, Z* = {i : x \u2260 0} assume there exists such that \u00eei = 0 for all i \u2208 A* and G(x*; x*;0) \u2013 G(x;x*;0) > \u03b5 > 0. Suppose there exists a subsequence {x}s, S \u2208 N such that xx and ww. Given that lim ||xk+1 \u2212 xk|| = 0 and (14), we can deduce that there exist k\u2081 > 0, such that for all k > k\u2081, it holds that\nG(xk;xk, ek) \u2013 G(xk+1;xk, \u025bk) \u2265 \u025b/4. (17)\nThere exist k2 > 0, such that for all k > k2 and k \u2208 S, it holds that\n(\u2207f(x*) \u2013 \u2207 f(x))x > \u2212\u025b/12,\n\u03a3[(x) \u2013 w(x0)] [x > -\u03b5/12, (18)\n\u00a1EI*\nf(xk) - f(x*) > \u2212\u03b5/12.\nSubsequently, we have"}, {"title": null, "content": "G(x*; x*, 0) \u2013 G(x;xk, \u20ac*)\n[Q.(*) + x*(0)  (0)\niEI*\n=G(x*; x*,0) \u2013 G(x; x*, 0)\nL\n- f(x)+-+ ((0) - (0) + () ()\nxx\n+ (\u2207f(x*) \u2013 \u2207 f(x*))x + - x*||\u00b2 + ((x0) \u2013 w(x,c)) (i)\n\u03a3\u03b5 -\n\u03b5\n\u03b5 5\u03b5\n6(19)\nBesides, we can derive that\nG(xk; xk, ek) \u2013 G(x*; x*,0)\n=\n\n\u0395\u0399*\nQ*(x*) + \u03bb((((0)+) (0)\n=\n((((0)+) (0)\n>\n(((0)+) (0)\n. (20)\nCombining (19) and (20), we can deduce that for all k > max(k1,k2), k \u2208 S, it holds that\nG(xk;xk, ek) \u2013 G(xk+1;xk,ek) = G(xk; xk, ek) \u2013 G(x*; x*,0) + G(x*; x*, 0) \u2013 G(xk+1;xk, ek)\n5\u03b5 \u03b5 2\u03b5\n>-. (21)\n6 3\ncontradicting (17). Therefore * is the fixed-point of H. According to a special case of Lemma 7 from [13], we can derive that \u00e6* satisfies the first-order necessary optimality condition of (P).\nNext, we proceed to analyze the local continuous differentiability of the mapping H. A notable property of the IRL1 is the locally stable sign property of the {x} for LPN approximation, which means that sign(x) remain unchanged for sufficiently large k. At the tail of the iteration, IRL1 equates to solving a smooth problem in the reduced"}, {"title": null, "content": "space RI*. We will utilize this characteristic to infer the local continuous differen- tiability of H. The non-degeneracy condition of F is required (Define in Assumption 1). This is a common assumption in nonconvex problems. For LPN approximation, this condition naturally holds (due to p(0+)p\u22121 \u2192 \u221e). For general sparse approxima- tion,, the values of A and p are often appropriately chosen to guarantee the sparsity. Consequently, the values of A or $'(0+) are large enough to satisfy the condition '(0+) > C/A. Hence this assumption is easily satisfied. This assumption infers a lower bound property of x away from 0 for all i \u2208 I*. We have the following lemma.\nLemma 2. Let Assumption 1 and 2 hold. H and fixed-point x* hold the following statements.\n(i) If there exists (Xi;\u0113i) such that w(Xi,\u0113i) > C/A, then for all xi \u2264 Xi and ei \u2264 \u0113i, it holds that [Hx(x, \u20ac)]i = 0.\n(ii) There exists d > 0 such that w(\u03b4,0) = C/X. For all i \u2208 I(x*), |x| has the lower bound, x \u2265 \u03b4.\nProof. (i) We prove this by contradiction. Assume that we have w(xi, \u0113i) > C/X. If [Hx(x,t)] \u2260 0, it would contradict the first-order necessary optimality condition (5). Therefore, there must have [H(x,\u20ac)]\u2081 = 0. Furthermore, for Xi < Ti and ei \u2264 Ei, we have\nw(xi, ei) = $' ([xi] + \u20aci) \u2265 d' ([i] + \u0113i) = w(Hi, \u0113i) > . (22)\nConsequently, we have [H(x, \u20ac)]i = 0.\n(ii) For i \u2208 I*, it holds that [Hx(x*,0)]i = x \u2260 0. Hence, we can deduce that x \u2265 \u03b4.\nBased on the boundness of x for all i \u2208 I*, we establish the locally continuous differentiability of H:\nLemma 3. Let Assumption 1 and 2 hold. There exists ap < min(d, miniez* x \u2013 8). For all \u03b8 \u2208 B(\u03c1) := {0 \u2208 R2n|||0 \u2013 0* || \u2264 \u03c1}, sign(H\u2082(0)) = sign(x) and H (0) is continuously differentiable.\nProof. The continuity of H infers that there exists a p such that ||H(0) \u2013 \u0397(0*)|| = ||\u0397 (0) \u2013 \u0398* || < \u03b4 for all \u03b8 \u2208 B(\u03c1).\nFor i \u2208 A*, given that p < min(d, mini\u2208z* x \u2212 \u03b4), it is established that w(x, \u20ac) > C/A and [Hx(0)]i = 0. Consequently, we can deduce that sign([Hx(0)]i) = sign(x).\nFor i \u2208 I*, we prove by contradiction, assume that sign([Hx(0)]i) \u2260 sign(x). Therefore, we can deduce that\n[[Ha(0)]i \u2013 x = \u221a([Hx(0)])\u00b2 + (x)2 \u2013 2[Hz(0)]ix > \u221a(x)\u00b2 \u2265 \u221a(\u03b4)2 = \u03b4. (23)"}, {"title": null, "content": "This is in contradiction with ||H(0) \u2013 0*|| < \u03b4. As a result, we can infer that sign([Hx(0)]i) = sign(x) for all i \u2208 I* and sign(H\u2082(0)) = sign(x).\nGiven that for all \u03b8 \u2208 B(p), H(0) is not continuous differentiable only when sign(xi) \u2260 sign(x), i \u2208 I*, we can conclude that H (0) is continuously differentiable in B(p)."}, {"title": "2.2 Local convergence guarantees", "content": "Subsequently, we analyze the local convergence of the algorithm. We make the following assumption.\nAssumption 3. There exists 0 < p < p such that for all 0 \u2208 B(p), the following statement hold:\n(i) There exists \u043a > 0 such that \u22072F([X1* ; \u20ac1*]) \u2265 \u043a\u0399.\n(ii) For all i \u2208 I*, w(0i) is Lipschitz continuous with constant Lw > 0.\n(iii) For all 0 and 0k \u2208 B(p), there exists C > 0 and such that ||\u2207Qk(x)||\u221e < C.\n(iv) For all k \u2208 N, there exists an upper bound Ma such that \u03a3 < \u039c\u03b1\nRecent studies have demonstrated the capability of IRL1 algorithm to avoid active strict saddle points and furthermore converge to a local minimum. Therefore, we make Assumption 3 (i). It is crucial to ensure that the mapping H is a contraction. Subsequently, Assumption 3 (ii) and (iii) are straightforward in the text of problem (P). (ii) is naturally satisfied for xi \u2260 0. (iii) assumes a upper bound of |\u2207w(0)| and ||Qk(x)||\u221e. It is a similar assumption in Assumption 2 and which is trivially holds in the local of x*. Additionally, Assumption 3 (iv) is commonly found in the existing literature of Anderson acceleration. It ensures the boundness of ma, In experiments, we have not observed a case that the coefficients become large. However, the relevant proof of boundness has not yet been proposed. There are several practical method can be used to enforce it [29, 40].\nIn the following, we demonstrate that H (0) is a contraction when e is close to 0*. Denote H as the Jacobian matrix of H.\nLemma 4. Let Assumption 1, 2, 3 hold and L > max(k, Lw). Then, H(0) is a contraction mapping for all \u04e9 \u2208 B(p), i.e. For all 01, 02 \u2208 B(\u00f4), there exists \u03b3\u2208 (0,1) such that\n||\u0397 (01) \u2013 \u0397 (02)|| \u2264 7||01 \u2013 02||. (24)\nProof. Given that He (0) = \u03bc\u03b5, it is easy to deduce that ||\u2207H\u20ac(0)|| \u2264 \u03bc < 1.\nIn the Lemma 3, we show that sign(H\u2082(0)) = sign(x) and H (0) is continuously differentiable for all \u03b8 \u2208 B(\u00f4). Therefore, it infers that for i \u2208 A*, we have [Hx(X, \u20ac)]i = 0. Then it holds that\n||\u2207Hx([XA*; \u20acA*])|| = 0 < 1."}, {"title": null, "content": "For i \u2208 I*, we can derive that\nHx([X1*; \u20ac1*", "1*": "\u043a, ||\u2207\u00b2\u0424([x1* ; \u20ac1*"}, {"1*": "max(1 \u2013 \u043a/L,Lw/L) < 1.\nTherefore, \u0397(0) is a contraction mapping for all \u2208 B(p) and (24) holds.\nIn the following, we present a guarantee of local R-linear convergence for the AAIRL1 algorithm.\nTheorem 1. Let Assumption 1, 2, 3 hold and L > max(\u043a, Lw). Let 0* = H(0*) be a fixed-point of H, then if 0\u00ba is sufficiently close to \u0472*, the iterates {k} generated by AAIRL1 algorithm converge to 0* R-linearly with y \u2208 (\u03b3,1).\n||Hk \u2013 0k || < ^k||H\u00b0 \u2212 0\u00b0 || (25)\nand\n||0 - 0* || \u2264+10\u00b0-0*||. (26)\nProof."}]}