{"title": "CRIM-GS: Continuous Rigid Motion-Aware\nGaussian Splatting from Motion Blur Images", "authors": ["Jungho Lee", "Donghyeong Kim", "Dogyoon Lee", "Suhwan Cho", "Sangyoun Lee"], "abstract": "Neural radiance fields (NeRFs) have received significant attention due to their\nhigh-quality novel view rendering ability, prompting research to address various\nreal-world cases. One critical challenge is the camera motion blur caused by camera\nmovement during exposure time, which prevents accurate 3D scene reconstruction.\nIn this study, we propose continuous rigid motion-aware gaussian splatting (CRiM-\nGS) to reconstruct accurate 3D scene from blurry images with real-time rendering\nspeed. Considering the actual camera motion blurring process, which consists\nof complex motion patterns, we predict the continuous movement of the camera\nbased on neural ordinary differential equations (ODEs). Specifically, we leverage\nrigid body transformations to model the camera motion with proper regularization,\npreserving the shape and size of the object. Furthermore, we introduce a continuous\ndeformable 3D transformation in the SE(3) field to adapt the rigid body transforma-\ntion to real-world problems by ensuring a higher degree of freedom. By revisiting\nfundamental camera theory and employing advanced neural network training tech-\nniques, we achieve accurate modeling of continuous camera trajectories. We\nconduct extensive experiments, demonstrating state-of-the-art performance both\nquantitatively and qualitatively on benchmark datasets. The project page is publicly\navailable at https://Jho-Yonsei.github.io/CRiM-Gaussian/.", "sections": [{"title": "1 Introduction", "content": "Novel view synthesis has recently garnered significant attention, with Neural Radiance Fields\n(NeRFs) [28] making considerable advancements in photo-realistic neural rendering. NeRFs take\nsharp 2D images from multiple views as input to accurately reconstruct 3D scenes, which are crucial\nfor applications such as augmented reality (AR) and virtual reality (VR). NeRFs model radiance and\nvolume density through an implicit neural representation using multi-layer perceptrons (MLPs), based\non the coordinates and viewing directions of 3D points. However, the computational complexity of this\nvolume rendering process poses challenges for real-time applications. To address this, 3D Gaussian\nSplatting (3D-GS) [14] has recently emerged, offering an alternative by explicitly representing 3D\nscenes and enabling high-quality real-time rendering through a differentiable splatting method.\nHowever, accurately reconstructing 3D scenes in real-world scenarios requires accounting for various\ntypes of image degradation (e.g., camera motion blur, image defocus blur). Both NeRFs and 3D-GS\nrely on sharp images as input, which assumes highly ideal conditions. Obtaining such precise images\nnecessitates a large depth of field (DoF), which in turn requires a very small aperture setting. A\nsmaller aperture significantly reduces the amount of incoming light, leading to necessarily longer"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Image Deblurring", "content": "In the field of computer vision, the phenomenon known as blur is identified as a degradation in images.\nIt frequently occurs during the image capture process in the camera due to factors such as camera\nshaking, depth of field, and the movement of objects. These factors result in specific types of blur,\nnamely camera motion blur, defocus blur, and object motion blur. Traditionally, image deblurring is\nsimulated as a finding specific blur kernel that restores the degradation of the target image exploiting\nthe knowledge about the blurring process. The blurring process is mathematically formulated as a\nconvolutional equation, denoted as $b = K * s + n$, where $b$, $K$, $s$, and $n$ indicate the blurred image,\nblur kernel, clean image, and additional noise, respectively. Disentangling this equation involves a\ncategorization into non-blind and blind deblurring, depending on whether the blur kernel $K$ is known\nor not. Our focus lies on the latter, as real-world images typically lack knowledge of the blur kernel.\nRecently, attempts to solve the blind deblurring using deep learning have been explored utilizing data-\ndriven prior involved in paired datasets with network optimization. According to recent deep image\ndeblurring survey [63], several works adopt various network architectures as deblurring networks,\nsuch as deep auto-encoder, multi-scale networks, and generative adversarial networks (GANs) [10].\n[46, 30] address deep auto-encoder based on U-Net [41] architecture with additional priors. To utilize\ndifferent scales of the image as complementary information, [49, 9, 61] adopt the multi-scale network\nand progressively generate high-resolution sharp images utilizing restoration from low-resolution\nimages. Other approaches [15, 16] take GANs to restore the high-frequency details exploiting the\nhigh-quality image-to-image translation power of GANs. Although image deblurring shows high-\nquality restored images, the methods cannot be directly applied to the neural rendering framework as\n[25] argued since the inconsistency across the given images derived from inherent ill-posed property."}, {"title": "2.2 Neural Rendering", "content": "Neural rendering has seen explode in the fields of computer graphics and vision area thanks to\nthe emergence of volumetric rendering and ray tracing-based neural radiance fields (NeRFs) [28],\nwhich provide super-realistic rendering quality from 3D scenes. NeRFs have led to a wide range of\nstudies, including 3D mesh reconstruction [53, 52, 21, 48], dynamic scene [39, 32, 33, 20, 50, 22],\nand human avatars [56, 38, 13]. At the same time, several researches have aimed at overcoming\nNeRF's slow training and rendering speed, such as TensoRF [2], plenoxel [8], and Plenoctree [60].\nAmong these advancements, the recent emergence of 3D Gaussian splatting (3D-GS) [14], which\noffers remarkable performance along with fast training and rendering speed, has further accelerated\nresearch in the neural rendering field. In addition to the aforementioned diverse scenarios for 3D\nscene modeling, there has been active research focusing on the external non-ideal conditions of given\nimages, such as sparse-view images [29, 58, 51, 57], and the absence of camera parameters [12, 55, 1].\nMoreover, there has been significant research attention on non-ideal conditions inherent to the images\nthemselves, such as low-light [27, 35], blur [25, 18, 36, 54, 19, 4, 64, 37], and inconsistent appearance\ncondition [26]. Recently, neural rendering from blurred images, which can easily occur during image\nacquisition, has attracted attention due to its practical applicability."}, {"title": "2.3 Neural Rendering from Blurry Images", "content": "Deblur-NeRF [25] firstly introduced the deblurred neural radiance fields by importing the blind-\ndeblurring mechanism into the NeRF framework. They introduce specific blur kernel in front of the\nNeRF framework imitating the blind deblurring in 2D image deblurring area. After the emergence\nof [25], several attempts have been proposed to model the precise blur kernel with various types of\nneural rendering baseline, such as TensoRF [2], and 3D-GS [14]. DP-NeRF [18] proposes rigid blur\nkernel that predict the camera motion during image acquisition process as 3D rigid body motion to\npreserve the geometric consistency across the scene. BAD-NeRF [54] and BAD-GS [64] similarly\npredict blur kernel as camera motions based on NeRFs [28] and 3D-GS [14], but they assume the\nlinear motion and interpolate them between predicted initial and final camera poses. PDRF [36]\nproposes progressive blur estimation model with hybrid 2-stage efficient rendering scheme that\nconsists of coarse ray and fine voxel renderer. SMURF [19] adopt Neural ODE [3] to predict\nmore precise continuous camera motions based on TensoRF [2]. BAGS [37] proposes CNN-based\nmulti-scale blur-agonostic degaradation kernel with blur masking that indicates the blurred areas."}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 3D Gaussian Splatting", "content": "3D Gaussian Splatting (3D-GS) [14] represents explicit 3D scenes as point clouds using numerous\ndifferentiable 3D Gaussians. Each 3D Gaussian primitive is defined by a mean vector $\u03bc\u2208 R^3$ and a\ncovariance matrix $\u03a3 \u2208 R^{3\u00d73}$. The basic 3D Gaussian $G$ is geometrically represented as follows:\n$G(x) = e^{-(x-\\mu)^T\u03a3^{-1}(x-\\mu)},$ (1)\nwhere $x$ denotes an arbitrary 3D point. The covariance matrix $\u03a3$ is decomposed into a scaling vector\n$S \u2208 R^3$ and a rotation matrix $R \u2208 R^{3\u00d73}$ obtained by a quaternion $q \u2208 R^4$ as $\u03a3 = RSSTR^T$. To\nrender novel view images, the 3D Gaussians in world space are projected into 2D camera space to\nemploy differentiable splatting [59]. Then, the covariance matrix $\u03a3'$ in camera space is expressed by a\nviewing transform $W$ and the Jacobian $J$ of the affine approximation of the projective transformation\nas $\u03a3' = JWEW^T J^T$.\nThe Gaussian primitive also includes an opacity value $\u03b1 \u2208 R$ and spherical harmonic (SH) coefficients\n$C\u2208 R^k$ for the color value. The color $C$ of pixel $p$ is computed by blending $N$ ordered points\noverlapping the pixel as follows:\n$C(p) = \\sum_{i\u2208N} C_i \u03b1_i \\prod_{j=1}^{i-1}(1 \u2013 \u03b1_j),$ (2)\nwhere $c_i$ and $\u03b1_i$ denote the color and the density of each point, respectively, obtained by a 3D\nGaussian with covariance $\u03a3$ multiplied by SH color coefficients and a learned per-point opacity. In\nthis paper, we ensure real-time rendering by employing the framework of 3D-GS."}, {"title": "3.2 Neural Ordinary Differential Equations", "content": "Neural ODEs [3] are first proposed as an approach that interprets neural networks as the derivatives\nof a ODE systems, where ODEs represent the dynamics inherent to the hidden states. Specifically,\nneural ODEs are utilized to represent parameterized, time-continuous dynamics in the latent space,\nproviding a unique solution given an initial value and numerical differential equation solvers [23].\nRecently, there has been extensive research on neural ODEs. For instance, Latent-ODEs [42] model\nthe continuous dynamics of irregularly sampled time-series data, while Vid-ODEs [34] model a\nsmooth latent space by capturing the underlying continuous dynamics of videos.\nNeural ODEs model a continuous and differentiable latent state $z(t)$. Within an infinitesimally small\nstep limit $\u03f5$ in the latent space, the local continuous dynamics are modeled as $z(t+\u03f5) = z(t)+\u03f5 \\frac{dz(t)}{dt}$.\nThe derivative of the latent state, $\\frac{dz(t)}{dt}$, is represented by a neural network $f(z(t), t; \u03a6)$ parameterized\nby learnable parameters $\u03a6$. The latent state at any arbitrary time $t_s$ is obtained by solving the ODE\nfrom the initial time $t_0$:\n$z (t_s) = z (t_0) + \\int_{t_0}^{t_s} f(z(t), t; \u03a6)dt = ODESolve (z(t_0), f, t_0, t_s, \u03a6) .$ (3)\nThe simplest method to solve ODEs is the Euler method [7], which is a fixed-step-size first-order\nsolver. Additionally, the Runge-Kutta [17] methods are preferred as a higher-order solvers as they\noffer enhanced convergence and stability. Therefore, we adopt the Dormand-Prince-Shampine\nfifth-order [6] Runge-Kutta solver for all experiments, following [3].\nThe derivative $f$ modeled by the neural network is expressed as a uniformly Lipschitz continuous\nnon-linear function in $z$ and $t$ [11]. Therefore, the solution obtained through the solver for any given\nintegration interval $(t_i, t_j)$ is always unique in the integration of the continuous dynamics. We model\nthe non-linear 3D camera trajectory as time-continuous using neural ODEs in the latent space to\nensure a continuous representation."}, {"title": "4 Method", "content": ""}, {"title": "4.1 CRiM-GS Framework", "content": "Our goal is to reconstruct a sharp 3D scene using only images with camera motion blur as input,\nthereby obtaining deblurred novel view images. Inspired by image blind deblurring methods, we\nfollow the approach of Deblur-NeRF [25], intentionally learning a kernel that blurs images and\nexcluding this kernel during rendering to produce sharp rendered images. As illustrated in Fig. 1, our\nblurring kernel consists of camera poses along the camera motion trajectory, generated continuously\nin time order through neural ODEs [3]. Each pose is composed of a rigid body transformation\n$T = [R|t_r]$ (Sec. 4.2), which maintains the shape and size of the subject, and a deformable body\ntransformation $T_d = [R_d|t_d]$ (Sec. 4.3), which accounts for distortions that may occur during actual\nimage acquisition. Both transformations exist in a dense SE(3) field, where $R \u2208 R^{3\u00d73}$ and $t \u2208 R^3$\nrepresent multiplicative rate of change of the rotation matrix and translation vector, respectively.\nGiven these two transformation matrices $[R|t_r]$ and $[R_d|t_d]$, the subsequent pose $T_{t_s} \u2208 SE(3)$ in\nthe camera motion is derived as follows:\n$T_{t_s} = T_{t_0}T_{t_s}^{t_0} T_{t_s}^{t_d} ,$ where $T = \\begin{bmatrix} R & t \\\\ 0 & 1 \\end{bmatrix} \u2208 SE(3).$ (4)\n$T_{t_0} = [R_{t_0}|t_{t_0}]$ and $T_{t_s} = [R_{t_s}|t_{t_s}]$ denote the camera poses at the initial state and the time $t_s$. By\nrendering $N$ images from the obtained $N$ camera poses and computing their pixel-wise weighted\nsum (Sec. 4.4), we obtain the final blurred image. Our whole framework is shown in Fig. 2"}, {"title": "4.2 Continuous Rigid Body Motion", "content": "According to [24], using rigid body transformations in the SE(3) field is effective when the shape and\nsize of the object remain unchanged. Assuming no distortion in the images during camera motion,\nwe first obtain the unit rotation axis $\\hat{w} \u2208 R^3$ and the rotation angle $\u03b8$ about this axis, as well as the\n3D column vector $v \u2208 R^3$ required for translation. These are the components of the unit screw axis\n$S = (\\hat{w}, v)$. Considering that the form of blur varies for each image in a scene, we embed the image\nindex of the scene to obtain different unit screw axis $S$ for each image. The embedded features are\nthen passed through a parameterized one-layer encoder $E_r$, transforming them into the latent state\n$z_r (t_0)$, which represents the latent features for $\\hat{w}, \u03b8$, and $v$ of $S$. Inspired by the continuity of the"}, {"title": "4.3 Continuous Deformable Body Motion", "content": "In real-world scenarios, camera motion blur during exposure time does not always preserve the shape\nand size of objects. Especially when the camera moves along a nonlinear path, its complex trajectory\nmay not be sufficiently explained by rigid body transformations alone [44]. Based on this assumption,\nwe propose a deformable body transformation to provide additional corrections to the rigid body\ntransformation. This transformation has more degrees of freedom from a learning perspective and is\nrelatively simple to implement. The deformable body transformation optimizes the transformation\nmatrix in SE(3) directly without converting $\\hat{w} \u2208 R^3$ into a 3 \u00d7 3 skew-symmetric matrix as described\nin Eq. (7). Thus, instead of modeling $\\hat{w}$ and $v$, this transformation designs the rotation matrix $R$ and\nthe translation vector $t$. This process begins by encoding the image index into the latent state $z_d(t_0)$\nof the deformable $R$ and $t$ using the encoder $E_d$, similar to Sec. 4.2. The latent state $z_d(t_s)$ at any\narbitrary time $t_s$ is obtained using the neural derivative $g$ parameterized by $\u03a8$ and a solver, as shown\nin Eq. (5):\n$z_d(t_s) = z_d(t_0) + \\int_{t_0}^{t_s} g(z_d(t), t; \u03a8) dt.$ (10)\nThese latent features are then transformed into the rotation matrix $\\breve{R}$ and translation vector $\\breve{t}_{t_s}$\nthrough a simple MLP decoder $D_d$:\n$D_d(z_d(t_s)) = (\\breve{R}, \\breve{t}_{t_s}) \u2192 T_d^{t_s} = \\begin{bmatrix} \\breve{R} + I & \\breve{t}_{t_s} \\\\ 0 & 1 \\end{bmatrix} \u2208 SE(3),$ (11)"}, {"title": "4.4 Optimization", "content": ""}, {"title": "Pixel-wise Weight", "content": "Once the $N$ images along the camera motion trajectory are obtained from the\n$N$ camera poses, we apply a pixel-wise weighted sum to create the blurred image $I_{blur}$, following\nprevious research [25, 18, 19, 36]. We use a shallow CNN $F$ and a softmax function to compute the\npixel-wise weights $P \u2208 R^{N\u00d7H\u00d7W\u00d73}$ for the resulting images, as follows:\n$I_{blur} = \\sum_{i=1}^N I_i P_i, where P_i = softmax(F(I_i)),$ (12)\nwhere $I_i$ is the $i$-th image along the camera motion, and $P_i$ is the pixel-wise weight of that image.\nThis method allows us to obtain the final blurred image, which is then optimized against the ground\ntruth blurred image using a loss function."}, {"title": "Objective", "content": "We optimize the learning process using the $L_1$ loss and D-SSIM between the generated\nblurred image and the ground truth blurred image, similar to 3D-GS [14]. The $L_1$ loss ensures\npixel-wise accuracy, while D-SSIM captures perceptual differences. Additionally, we apply the regu-\nlarization losses $L_{det}$ and $L_{ortho}$ mentioned in Sec. 4.3 to regularize the deformable transformation.\nThe final objective $L$ is defined as follows:\n$L = (1 - \u03bb_c)L_1 + \u03bb_c L_{D-SSIM} + \u03bb_{det}L_{det} + \u03bb_{ortho}L_{ortho},$ (13)\nwhere $\u03bb_c$ is a factor for balancing $L_1$ and $L_{D-SSIM}$, and $\u03bb_{det}$ and $\u03bb_{ortho}$ are relatively small factors\nfor the regularization losses."}, {"title": "5 Experiments", "content": "Datasets. CRiM-GS is evaluated using the camera motion blur dataset provided by Deblur-\nNeRF [25], which includes 5 synthetic scenes and 10 real-world scenes. The synthetic scenes\nare generated using Blender [5], and each single image that constitutes a motion-blurred image is\nobtained through linear interpolation between the first and last camera poses along the camera path.\nThe obtained images are combined in linear RGB space to create the final blurred images. The\nreal-world scenes consist of images captured with a CANON EOS RP camera, where the exposure\ntime is manually set to produce blurry images. For each scene, all images exhibit different types\nof blur, including non-uniform blur. Additionally, for both synthetic and real-world scenes, we use\nCOLMAP [43, 45] to obtain the camera poses for each image and the initial point cloud for training\nthe Gaussian primitives."}, {"title": "Implementation Details", "content": "CRiM-GS is trained for 40k iterations, with all settings for learning the\nGaussian primitives, including the learning rate, being identical to those of 3D-GS. We set the number\nof poses $N$ that constitute the continuous camera trajectory to 9. The size of the hidden state is\nfixed at 64 for all processes involving rigid body transformation and deformable body transformation.\nBoth neural derivatives $f$ and $g$ are composed very simply, with single hidden layer and a ReLU\nactivation function. The CNN $F$ consists of three layers with 64 channels each. $F$ starts training\nafter 3k iterations, allowing the initial camera path to be sufficiently optimized. Additionally, we set\n$\u03bb_c$, $\u03bb_{det}$, and $\u03bb_{ortho}$ to 0.3, $10^{-3}$, and $10^{-3}$ respectively for the objective function. All experiments\nare conducted on a single NVIDIA RTX 3090 GPU."}, {"title": "5.1 Novel View Synthesis Results", "content": "We evaluate CRiM-GS using three metrics: peak signal-to-noise ratio (PSNR), structural similarity\nindex measure (SSIM), and learned perceptual image patch similarity (LPIPS). We compare our\nmethod with both ray-based and rasterization methods. Note that the results for DeblurGS [31] and\nBAD-GS [64] were obtained by re-running the released code ourselves. The overall results for all\nscenes are shown in Tab. 1, demonstrating that our method achieves superior performance compared\nto all other methods. Specifically for the LPIPS, CRiM-GS shows an improvement of approximately\n52% for synthetic scenes and 33% for real-world scenes than the state-of-the-art. Tab. 2 presents the\nperformance for individual scenes in synthetic dataset. Our CRiM-GS consistently achieves high\nPSNR and SSIM scores on average and demonstrates the best LPIPS performance across all scenes\nexcept for the \"POOL\u201d. Although DeblurGS and BAD-GS achieve relatively good LPIPS scores,\ntheir PSNR and SSIM scores are lower, indicating that their camera poses are not properly optimized.\nFor more details about pose optimization, please refer to our appendix.\nTo qualitatively evaluate the results, we present rendering outcomes for several scenes in Fig. 3.\nThese include one synthetic scene and three real-world scenes. Our method shows superior qualitative\nresults compared to the state-of-the-art ray-based method DP-NeRF [18], as well as rasterization-\nbased methods such as BAD-GS [64] and BAGS [37]. Benefiting from using 3D-GS as the backbone,"}, {"title": "5.2 Ablation Study", "content": ""}, {"title": "Ablation on Continuous Transformation", "content": "To thoroughly demonstrate the validity and effective-\nness of our contributions, we conduct ablation experiments on all scenes, not just a single scene.\nAs shown in Tab. 3, the model (A) with the main component of CRiM-GS, the continuous rigid\nbody transformation, shows a significant performance increase, indicating that the camera motion\npath is well-modeled. The model (D) with only the continuous deformable body transformation\nperforms worse than the model with only the rigid body transformation, suggesting that the de-\nformable transformation leads to suboptimal results due to its high degrees of freedom. Therefore,\nthe model (F) combining both transformations, which refines camera motion distortion, achieves the\nbest performance without pixel-wise weights, surpassing existing state-of-the-art methods."}, {"title": "Ablation on Pixel-wise Weight", "content": "The pixel-wise weight, inspired by traditional ray-based 3D scene\ndeblurring methods [18, 25] and blind image deblurring methods [47, 62], differs from the approach\nof averaging images that constitute a blurry image. As shown in Tab. 3 for models (C), (E), and (G),\napplying this method consistently results in higher performance than not applying it. This indicates\nthat by assigning corresponding weights to each pixel location, a more precise blurred image is\nreconstructed, and the images that constitute the blurry image are modeled more accurately."}, {"title": "6 Conclusion", "content": "We propose CRiM-GS, a novel method for reconstructing sharp 3D scenes from blurry images caused\nby camera motion. We apply a rigid body transformation method under the assumption that the object\nremains unchanged during camera movement and propose a learnable deformable body transformation\nmethod to correct potential distortions of actual camera path. These transformation methods are\ncontinuously modeled through neural ODEs in 3D physical space, representing continuous 3D\ncamera motion trajectories. Inspired by traditional image deblurring methods and existing 3D scene\ndeblurring research, we introduce a simple CNN-based pixel-wise weight. Our CRiM-GS outperforms\nthe state-of-the-art in 3D scene deblurring, and we demonstrate the effectiveness of our contributions\nthrough extensive experiments that rigorously evaluate the validity of the proposed components."}, {"title": "A Performance on Real-World Scenes", "content": "As shown in Tab. 4, CRiM-GS demonstrates superior quantitative performance across most real-world\nscenes. In particular, LPIPS shows the best performance not only for synthetic scenes but also for all\nreal-world scenes, highlighting the excellent quality achieved by our CRiM-GS."}, {"title": "B Pose Optimization", "content": "We re-ran DeblurGS [31] because it performs test pose optimization after learning 3D Gaussian\nSplatting. This optimization process requires test images, making it difficult to consider a fair\ncomparison. Nevertheless, as shown in Tab. 5, CRiM-GS achieves better performance even when\nsubjected to the same pose optimization process as DeblurGS, and it even outperforms the optimized\nDeblurGS without undergoing the optimization process itself. This indicates that CRiM-GS is robust\nto inaccurate camera poses. As illustrated in the error maps in Fig. 4, CRiM-GS not only performs\nbetter than DeblurGS but also optimizes poses better than other models without additional training."}, {"title": "C Additional Visualization", "content": "Additional visual results are shown in Fig. 5, demonstrating that our CRiM-GS not only achieves\nsuperior quantitative performance but also delivers excellent qualitative results."}, {"title": "D Limitations", "content": "CRIM-GS effectively models the continuous camera trajectory occurring during the exposure time.\nHowever, rendering N images to create a blurry image impacts the training time. Additionally, our\nmodel is optimized specifically for camera motion blur and does not account for other degradation\nphenomena such as defocus blur. Therefore, our next goal is to propose a method that involves\npost-processing through training, requiring only a single rendering pass. Furthermore, we aim to\napply depth of field (DoF) to 3D Gaussian Splatting to reconstruct 3D scenes solely from blurry\nimages caused by defocus."}, {"title": "E Broader Impacts", "content": "Our research presents a novel method for reconstructing sharp 3D scenes from images blurred by\ncamera motion. By integrating neural ODEs, commonly used in computer vision applications, into\n3D vision, our approach demonstrates robustness theoretically, mathematically, and experimentally.\nAdditionally, by modeling the most common type of blur that can occur in real-world image acqui-\nsition environments, our method has potential applications in user-interactive VR, AR, and other\nvarious applications."}]}