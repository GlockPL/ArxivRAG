{"title": "Open-Sora Plan: Open-Source Large Video Generation Model", "authors": ["Open-Sora Plan Team"], "abstract": "We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs. Our project comprises multiple components for the entire video generation process, including a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. Moreover, many assistant strategies for efficient training and inference are designed, and a multi-dimensional data curation pipeline is proposed for obtaining desired high-quality data. Benefiting from efficient thoughts, our Open-Sora Plan achieves impressive video generation results in both qualitative and quantitative evaluations. We hope our careful design and practical experience can inspire the video generation research community. All our codes and model weights are publicly available at https://github.com/PKU-YuanGroup/Open-Sora-Plan.", "sections": [{"title": "1 Introduction", "content": "Driven by the recent progress of the diffusion model (Ho et al., 2020; Song et al., 2020) and transformer (Vaswani, 2017; Peebles and Xie, 2023) architecture, visual content generation demonstrates impressive creation capacity conditioned on given prompts, which attracts broad interests and emerging attempts. Since the image generation methods (Rombach et al., 2022b; Li et al., 2024c) achieve outstanding performance and are applied extensively, the video generation model is expected to make significant advancements to empower a variety of creative industries including entertainment, advertising, film, etc. Many early attempts (Guo et al., 2023; Xing et al., 2025) successfully generate video with low resolution and short frames, but few efforts challenge the high-quality and long-duration video generation due to the unimaginable computation and data cost.\nHowever, the technique report of Sora (Brooks et al., 2024), the video generation model created by OpenAI, with impressive showcases is released suddenly, shocking the entire video generation community while pointing out a promising way to create remarkable videos. As one of the first open-source projects aiming to re-implement a powerful Sora-like video generation model, our Open-Sora Plan attracts wide attention and contributes many first attempts to the video generation community, which inspires many subsequent works.\nIn this work, we summarize our practical experiences in recent months and present the technical details of our Open-Sora Plan, which generates high-quality and long-duration videos queried by various categories of conditions including text prompts, multiple images, and structure control signals (canny, depth, sketch, etc.). As illustrated in Fig. 1, we divide the video generation model into three key components and propose improvements for each part:\n\u2022 Wavelet-Flow Variational Autoencoder. To reduce memory usage and enhance training speed, we propose WF-VAE, a model that obtains multi-scale features in the frequency domain through multi-level wavelet transform. These features are then injected into a"}, {"title": "2 Core Models of Open-Sora Plan", "content": ""}, {"title": "2.1 Wavelet-Flow VAE", "content": "Preliminary. The multi-level Haar wavelet transform decomposes video signals by applying scaling filter h = [1, 1] and wavelet filter g = [1,-1] along temporal and spatial dimensions. For a video signal $V \\in \\mathbb{R}^{C\\times T\\times H\\times W}$, where C, T, H, and W correspond to the number of channels, temporal frames, height, and width, the 3D Haar wavelet transform at layer l is defined as:\n$S^{(l)}_{ijk} = S^{(l-1)} * (f_i \\otimes f_j \\otimes f_k)$ (1)\nwhere $f_i, f_j, f_k \\in {h, g}$ represent the filters applied along each dimension, and * represents the convolution operation. The transform begins with $S^{(0)} = V$, and for subsequent layers, $S^{(l)}_{hhh} = S^{(l-1)}_{ooo}$, indicating that each layer operates on the low-frequency component from the previous layer. At each decomposition layer l, the transform produces eight sub-band components: $W^{(l)} = {S^{(l)}_{hhhh}, S^{(l)}_{hhhg}, S^{(l)}_{hghh}, S^{(l)}_{ghhg}, S^{(l)}_{ghgh}, S^{(l)}_{hggg}, S^{(l)}_{ghgh}, S^{(l)}_{gggg}}$. Here, $S_{hhh}$ represents the low-frequency component across all dimensions, while $S_{ggg}$ captures high-frequency details. To implement different downsampling rates in the temporal and spatial dimensions, a combination of 2D and 3D wavelet transforms can be implemented. Specifically, to obtain a compression rate of 4\u00d78\u00d78 (temporal\u00d7height\u00d7width), we can employ a combination of two-layer 3D wavelet transform followed by one-layer 2D wavelet transform.\nTraining Objective. Building upon the training strategies outlined in Rombach et al. (2022a), the proposed loss function integrates several components: reconstruction loss (including both L1 and perceptual losses (Zhang et al., 2018)), adversarial loss, and KL divergence regularization. As illustrated in Fig. 2, our model architecture emphasizes a low-frequency energy flow and enforces symmetry between the encoder and decoder. To preserve this architectural principle, we introduce a novel regularization term, denoted as $L_{WL}$ (WL loss), which ensures structural consistency by penalizing deviations from the expected energy flow:\n$L_{WL} = |W^{(2)} - \\widehat{W}^{(2)}| + |W^{(3)} - \\widehat{W}^{(3)} |.$ (2)\nThe overall loss function is defined as:\n$L = L_{recon} + \\lambda_{adv} L_{adv} + \\lambda_{KL} L_{KL} + \\lambda_{WL} L_{WL}.$ (3)\nwhere $\\lambda_{adv}, \\lambda_{KL}$, and $\\lambda_{WL}$ are weighting coefficients for the corresponding loss components. Following (Esser et al., 2021), we adopt dynamic adversarial loss weighting to balance the relative"}, {"title": "2.2 Joint Image-Video Skiparse Denoiser", "content": ""}, {"title": "2.2.1 Model Overview", "content": "As shown in Fig. 4, we compress input images or videos from pixel space to latent space for denoising training with the diffusion model. Given an input latent $x \\in \\mathbb{R}^{B\\times C\\times T\\times H\\times W}$, we first split latent into small tokens by a 3D convolutional layer and flattened into a 1D sequence, with converting the latent dimension C to dimension D. We use kernel sizes $k_t = 1, k_h = 2$ and $k_w = 2$, with strides matching the kernel sizes, resulting in a total of L = THW tokens. We further use mT5-XXL (Xue, 2020) as the text encoder to map text prompts to a high-dimensional feature space, and we also convert text feature to dimension D through a single MLP layer.\n3D RoPE. We employ 3D rotational position encoding, which allows the model to directly compare relative differences between positions rather than relying on absolute positions. We define the computation process of nD RoPE. After \u201cpatchifying\u201d operation, the latent $X \\in \\mathbb{R}^{B\\times L\\times D}$ is divided into n parts along the D dimension, e.g., $X = [X_1,..., X_n]$, where $X_i \\in \\mathbb{R}^{B\\times L\\times \\frac{D}{n}}, i \\in [1,...,n]$, and we apply RoPE on partitioned tensor $X_i$. Assuming the RoPE operation (Su et al., 2024) is denoted as $ROPE(X_i)$, we inject the relative position encoding of the i-th dimension into tensor $X_i$, and concatenate processed tensors along the D dimension to obtain the final result:\n$X^{rope}_i = ROPE(X_i),$ (6)\n$X^{final} = Concat ([X^{rope}_i, ..., X^{rope}_i]),$ (7)\nwhere $Concat(.)$ denotes the concatenate operation and $X^{final} \\in \\mathbb{R}^{B\\times L\\times D}$. When n = 1, it is equivalent to applying RoPE on a 1D sequence in large language models. When n = 2, it can be"}, {"title": "2.2.2 Skiparse Attention", "content": "The 2+1D Attention widely leveraged by former video generation methods calculates frame inter-actions only along the temporal dimension, theoretically and practically limiting video generation performance. Compared to 2+1D Attention, Full 3D Attention represents global calculation for allowing content from arbitrarily spatial and temporal positions to interact, which approach aligns well with real-world physics. However, Full 3D Attention is time-consuming and inefficient, as visual information often contains considerable redundancy, making it unnecessary to establish attention across all spatiotemporal tokens. An ideal spatiotemporal modeling approach should employ attention that minimizes the overhead from redundant visual information while capturing the complexities of the dynamic physical world. Reducing redundancy requires avoiding connections among all tokens, yet global attention remains essential for modeling complex physical interactions.\nTo balance the computation efficiency and spatiotemporal modeling ability, we propose a Skiparse (Skip-Sparse) Attention mechanism. Denoiser with Skiparse Attention only modifies the original attention layers to two alternating sparse attention operations named Single Skip and Group Skip in Transformer Blocks. Giving a sparse ratio k, the sequence length in the attention operation reduces to"}, {"title": "3 Assistant Strategies", "content": ""}, {"title": "3.1 Min-Max Token Strategy", "content": "To achieve efficient processing on hardware, deep neural networks are typically trained with batched inputs, meaning the shape of the training data is fixed. Traditional methods adopt two approaches including resizing images or padding images to a fixed size. However, both approaches have drawbacks, e.g., the former loses useful information, while the latter has low computational efficiency. Generally, there are three methods for training with variable token counts: Patch n' Pack (Dehghani et al., 2024; Yang et al., 2024b), Bucket Sampler (Chen et al., 2023a, 2024a; Zheng et al., 2024), and Pad-Mask (Lu et al., 2024; Wang et al., 2024c).\nPatch n' Pack. By packing multiple samples, this method addresses the fixed sequence length limitation. Patch n' Pack defines a new maximum length, and tokens from multiple data instances are packed into this new data. As a result, the original data is preserved while enabling training with arbitrary resolutions. However, this method introduces significant intrusion into the model code, making it difficult to adapt in fields where the model architecture is not yet stable.\nBucket Sampler. This method packs data of different resolutions into buckets and samples batches from the buckets to ensure all data in a batch have the same resolution. It incurs minimal intrusion into the model code, primarily requiring modifications to the data sampling strategy.\nPad-Mask. This method sets a maximum resolution, pads all data to this resolution, and generates a corresponding mask to exclude loss from the masked areas. While conceptually simple, it has low computational efficiency."}, {"title": "3.2 Adaptive Gradient Clipping Strategy", "content": "In distributed model training, we often observe loss spikes as shown in Fig. 10, significantly de-grade output quality without causing NaN errors. Unlike typical NaN errors that disrupt training, these spikes temporarily increase loss values and are followed by a return to normal levels, which occur sporadically and adversely impact model performance. These spikes arise due to various issues, including abnormal outputs from the VAE encoder, desynchronization in multi-node com-munication, or outliers in training data leading to large gradient norms.\nWe attempt many methods including applying gradient clipping, adjusting the $\u03b22$ in optimizer, and reducing the learning rate, but none of these approaches resolve the issue, which appears randomly and cannot be reproduced even with a fixed seed. Playground v3 (Liu et al., 2024a) encounters the same issue and involves discarding an iteration if the gradient norm exceeds a fixed threshold. However, fixed thresholds may fail to adapt to decreasing gradient norms as training progresses. Therefore, we introduce an adaptive thresholding mechanism that leverages Exponential Moving Averages (EMA) for effective anomaly detection. Our approach mitigates the effects of spikes while preserving training stability and output quality."}, {"title": "4 Data Curation Pipeline", "content": "Dataset quality is closely linked to model performance. However, some current open-source datasets, such as WebVid (Bain et al., 2021b), Panda70M (Chen et al., 2024e), VIDAL (Zhu et al., 2023) and HD-VILA (Xue et al., 2022), fall short in data quality. Excessive low-quality data in training disrupts the gradient direction of model learning. In this section, we propose an efficient, structured data-processing pipeline to filter high-quality video clips from raw data. We also present dataset statistics to provide reliable direction for further data enhancement."}, {"title": "4.1 Training Data", "content": "As shown in Tab. 4, we obtain 11 million image-text pairs from Pixart-Alpha (Chen et al., 2023a), with captions generated by LLaVA (Liu et al., 2024c). Additionally, we use the OCR dataset Anytext-3M (Tuo et al., 2023), which pairs each image with corresponding OCR characters. We filter Anytext-3M for English data, constituting about half of the entire dataset. Since SAM (Kirillov et al., 2023) data (as used in Pixart-Alpha) includes blurred faces, we selected 160k high-quality images from Laion-5B (Schuhmann et al., 2022) to enhance the quality of person-related content in generation. The selection criteria include high resolution, high aesthetic scores, the absence of watermarks, and the presence of people in the images.\nFor videos, we download approximately 21M horizontal videos from Panda70M (Chen et al., 2024e) using our filtering pipeline. For vertical data, we obtain around 3M vertical videos from VIDAL (Zhu et al., 2023), sourced from YouTube Shorts. Additionally, we scrape high-quality videos from CC0-licensed websites, such as Mixkit, Pexels, and Pixabay. These open-source video sites contain no content-related watermarks."}, {"title": "4.2 Data Filtering Strategy", "content": "1. Video Slicing. Excessively long videos are not conducive to input processing, so we utilize copy stream method in ffmpeg\u00b3 to split videos into 16-second clips."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Wavelet-Flow VAE", "content": "Tab. 6 and Fig. 15 present both quantitative and qualitative comparisons with several open-source VAEs, including Allegro (Zhou et al., 2024), OD-VAE (Chen et al., 2024b), and CogVideoX (Yang et al., 2024b). The experiments utilize the Panda70M (Chen et al., 2024d) and WebVid-10M (Bain et al., 2021a) datasets. To comprehensively evaluate reconstruction performance, we adopt the Peak Signal-to-Noise Ratio (PSNR) (Hore and Ziou, 2010), Learned Perceptual Image Patch Similar-ity (LPIPS) (Zhang et al., 2018), and Structural Similarity Index Measure (SSIM) (Wang et al., 2004) as the primary evaluation metrics. Furthermore, the reconstruction Fr\u00e9chet Video Distance (rFVD) (Unterthiner et al., 2019) is employed to assess visual quality and temporal coherence.\nAs shown in Tab. 6, WF-VAE-S achieves a throughput of 11.11 videos per second when encoding 33-frame videos at 512\u00d7512 resolution. This throughput surpasses CV-VAE and OD-VAE by approximately 6\u00d7 and 4\u00d7, respectively. The memory cost reduces by nearly 5\u00d7 and 7\u00d7 compared to these baselines while achieving superior reconstruction quality. For the larger WF-VAE-L model, the encoding throughput exceeds Allegro by 7.8\u00d7, with approximately 8\u00d7 lower memory usage, while maintaining better evaluation metrics. These results demonstrate that the WF-VAE maintains state-of-the-art reconstruction performance while substantially reducing computational costs.\nWe assess the impact of lossy block-wise inference on reconstruction metrics using contemporary open-source VAE implementations Yang et al. (2024b); Chen et al. (2024b), as summarized in Tab. 7. Specifically, we measure reconstruction performance in terms of PSNR and LPIPS on the Panda70M dataset under both block-wise and direct inference conditions. the overlap-fusion-based tiling inference of OD-VAE results in substantial performance degradation. In contrast, CogVideoX exhibits only minor degradation due to its temporal block-wise inference with caching. Notably, our proposed Causal Cache mechanism delivers reconstruction results that are numerically identical to those of direct inference, thereby confirming its lossless reconstruction capability."}, {"title": "5.2 Text-to-Video", "content": "We evaluate the quality of our video generation model using VBench Huang et al. (2024) and ChronoMagic-Bench-150 Yuan et al. (2024). VBench, a commonly used metric in video generation, deconstructs \"video generation quality\" into several clearly defined dimensions, allowing for a fine-grained, objective assessment. However, many metrics are overly detailed and yield uniformly high scores across models, offering limited reference value. Consequently, we select Object Class, Multiple Object, and Human Action dimensions to evaluate the semantic fidelity of generated objects and human actions. Aesthetic quality is used to assess spatial generation effects, while Spatial"}, {"title": "5.3 Condition Controllers", "content": "Image-to-Video. The video generation capability of image-to-video depends significantly on the performance of the base model and the quality of the initial frame, resulting in challenges in establishing fully objective evaluation metrics. To illustrate the generation ability of Open-Sora Plan, we select several showcases, as shown in Fig. 19, demonstrating that our model exhibits excellent image-to-video generation capabilities and realistic motion dynamics. Furthermore, We compare the image-to-video results of several state-of-the-art methods in Fig. 18, highlighting that Open-Sora Plan strikes an exceptional balance between the control information of the initial frame and the text. Our method maintains semantic consistency while ensuring high visual quality, demonstrating superior expressiveness compared to other models.\nStructure-to-Video. As shown in Fig. 13, our structure condition controller enables the Open-Sora Plan text-to-image model to generate high-quality videos whose any frames (first frame, a few frames, all frames, etc.) can be accurately controlled by given structural signals (canny, depth, sketch, etc.)."}, {"title": "5.4 Prompt Refiner", "content": "The Open-Sora Plan leverages a substantial pro-portion of synthetic labels during training, resulting in superior performance in dense captioning tasks compared to shorter prompts. However, the evaluation prompts or user inputs are often brief, limiting the ability to accurately assess the model's true performance. Following DALL-E 3 (Betker et al.), we report evaluation results where our prompt refiner is employed for rewriting input prompts.\nDuring the evaluation, we observe notable im-provements in most VBench Huang et al. (2024) metrics when using prompt refiner, particu-larly in action accuracy and object description. Fig. 14 provides a radar chart that visually high-lights the effectiveness of the prompt refiner. Specifically, the performance in human action generation and spatial relationship depiction im-proved by more than 5%. The semantic adher-ence for single-object and multi-object genera-tion increased by 15% and 10%, respectively. Additionally, the score for scenery generation increased by 25%. Furthermore, our prompt refiner can translate multilingual into English, allowing the diffusion model to leverage training data and text encoders in English while supporting various languages for inference."}, {"title": "6 Limitation and Future Work", "content": ""}, {"title": "6.1 Wavelet-Flow VAE", "content": "Our decoder architecture is modeled after the design proposed by Rombach et al. (2022a), resulting in a greater number of parameters in the decoder compared to the encoder. While the computational cost remains manageable, we consider these additional parameters to be redundant. Consequently, in future work, we plan to streamline the model to fully exploit the advantages of our architecture."}, {"title": "6.2 Transformer Denoiser", "content": "The current 2B model in version 1.3.0 shows performance saturation during the later stages of training. However, our model performs poor in understanding physical laws (e.g., a cup overflowing with milk, a car moving forward, or a person walking), thus we have three hypotheses:\n\u2022 Joint training of images and videos. Models such as Open-Sora v1.2 (Zheng et al., 2024), EasyAnimate v4 (Xu et al., 2024b), and Vchitect-2.05 can easily generate high-visual-quality videos, possibly due to their direct inheritance of image weights (Pixart-Sigma (Chen et al., 2024a), HunyuanDiT (Li et al., 2024c), SD3 (Esser et al., 2024)). They train the model with a small amount of video data to learn how to flow along the temporal dimension based on 2D images. However, we train images from scratch with only 10M-level data, which is far from sufficient. In recent work on Allegro (Zhou et al., 2024), they fine-tuned a better text-to-image model based on the T2I weights from Open-Sora Plan v1.2, achieving improved text-to-video results. We have two hypotheses regarding the training strategy: (i) Start joint training from scratch, with images significantly outnumbering videos; (ii) First train a high-quality image model and then use joint training, with a higher proportion of videos at that stage. Considering the learning path and training costs, the second approach may offer more decoupling, while the first aligns better with scaling laws.\n\u2022 The model still needs to scale. By observing the differences between CogVideoX-2B (Yang et al., 2024b) and its 5B variant, we can discover that the 5B model understands more physical laws than the 2B model. We speculate that instead of spending excessive effort designing for smaller models, it may be more effective to leverage scaling laws to solve these issues. In the next version, we will scale up the model to explore the boundaries of video generation. We currently have two plans: (i) Continue using the Deepspeed (Rasley et al., 2020)/FSDP (Zhao et al., 2023) approach, sharding the EMA and text encoder across ranks with Zero3 (Rasley et al., 2020), which is sufficient for training 10-15B models. (ii) Adopting MindSpeed6/Megatron-LM (Shoeybi et al., 2019) for various parallel strategies, enabling us to scale the model up to 30B.\n\u2022 Supervised loss in training. Flow Matching (Lipman et al., 2022) avoids the stability issues in Denoising Diffusion Probabilistic Models (Ho et al., 2020) (DDPM) when the timestep approaches 0, addressing the zero-terminal signal-to-noise ratio problem (Lin et al., 2024b). Recent works (Zheng et al., 2024; Polyak et al., 2024; Esser et al., 2024) also show that the validation loss in Flow Matching indicates whether the model is converging in the right direction, which is crucial for assessing model training progress. Whether flow-based models are more suitable than v-prediction models requires further ablation studies.\nIn addition to expanding the model and data scale, we will also explore other efficient algorithm implementations and improved evaluation metrics:\n\u2022 Exploring more efficient architectures. Although Skiparse Attention significantly reduces FLOPs during computation, these advantages are only noticeable with longer sequence lengths (e.g., resolutions above 480P). Since most pre-training is conducted at a lower resolution (e.g., around 320 pixels), the Skiparse Attention operation has not achieved the desired acceleration ratio in this phase. In the future, we will explore more efficient training strategies to address this issue."}, {"title": "6.3 Data", "content": "Despite ongoing improvements to our training data, the current dataset still faces several significant limitations in terms of data diversity, temporal modeling, video quality, and cross-modal information. We discuss these limitations and outline the corresponding directions for future works:\n\u2022 Lack of Data Diversity and Complexity. The current dataset predominantly covers specific domains such as simple actions, human faces, and a narrow range of scene types. We randomly sampled 2,000 videos from Panda70M and conducted manual verification, finding that less than 1% featured cars in motion, and there were even fewer than 10 videos of people walking. Approximately 80% of the videos consist of half-body conversations with multiple people in front of the camera. Therefore, we speculate that the narrow data domain of Panda70M restricts the model's ability to generate many scenarios. Consequently, it lacks the ability to generate complex, dynamic scenes involving realistic human movement, object deformations, and intricate natural environments. This limitation hinders the model's capac-ity to produce diverse and complex video content. Future work will focus on expanding the dataset to encompass a broader spectrum of dynamic and realistic environments, including more complex human interactions and dynamic physical effects. This expansion aims to improve the model's generalization ability and facilitate the generation of high-quality, varied dynamic videos.\n\u2022 Lack of Camera Movement, Video Style, and Motion Speed Annotations. The current dataset lacks annotations for key dynamic aspects of video content, such as camera move-ment, video style, and motion speed. These annotations are essential for capturing the varied visual characteristics and movement dynamics within videos. Without them, the dataset may not fully support tasks that require detailed understanding of these elements, limiting the model's ability to handle diverse video content. In future work, we will include these annotations to enhance the dataset's versatility and improve the model's ability to generate more contextually rich video content.\n\u2022 Limitations in Video Resolution and Quality. Although the dataset includes videos at common resolutions (e.g., 720P), these resolutions are insufficient for high-quality video generation tasks, such as generating detailed virtual characters or complex, high-fidelity scenes. The resolution and quality of the current dataset become limiting factors when generating fine-grained details or realistic dynamic environments. To address this limitation, future work should aim to incorporate high-resolution videos (e.g., 1080P, 2K), which will enable the generation of higher-quality videos with enhanced visual detail and realism.\n\u2022 Lack of Cross-Modal Information. The dataset predominantly focuses on video imagery and lacks complementary modalities such as audio or other forms of multi-modal data. This absence of cross-modal information limits the flexibility and applicability of generative models, particularly in tasks that involve speech, emotions, or contextual understanding. Future research should focus on integrating multi-modal data into the dataset. This will enhance the model's ability to generate richer, more contextually nuanced content, thereby improving the overall performance and versatility of the generative system."}, {"title": "7 Conclusion", "content": "We present Open-Sora Plan, our open-source high-quality and long-duration video generation project in this work. In the framework aspect, we decompose the entire video generation model into a Wavelet-Flow Variational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various condition controllers. In the strategy aspect, we carefully design a min-max token strategy for efficient training, an adaptive gradient clipping strategy for preventing outflow gradients, and a prompt refiner for obtaining more appreciative results. Furthermore, we propose a multi-dimensional data curation pipeline for automatic high-quality data exploitation. While our Open-Sora Plan achieving a remarkable milestone, we will make more effort to promote the progress of the high-quality video generation research area and open-source community."}]}