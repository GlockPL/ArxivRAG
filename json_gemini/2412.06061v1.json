{"title": "Curse of Attention:\nA Kernel-Based Perspective for Why Transformers Fail to\nGeneralize on Time Series Forecasting and Beyond", "authors": ["Yekun Ke", "Yingyu Liang", "Zhenmei Shi", "Zhao Song", "Chiwun Yang"], "abstract": "The application of transformer-based models on time series forecasting (TSF) tasks has long\nbeen popular to study. However, many of these works fail to beat the simple linear residual\nmodel, and the theoretical understanding of this issue is still limited. In this work, we propose\nthe first theoretical explanation of the inefficiency of transformers on TSF tasks. We attribute\nthe mechanism behind it to Asymmetric Learning in training attention networks. When\nthe sign of the previous step is inconsistent with the sign of the current step in the next-step-\nprediction time series, attention fails to learn the residual features. This makes it difficult\nto generalize on out-of-distribution (OOD) data, especially on the sign-inconsistent next-step-\nprediction data, with the same representation pattern, whereas a linear residual network could\neasily accomplish it. We hope our theoretical insights provide important necessary conditions\nfor designing the expressive and efficient transformer-based architecture for practitioners.", "sections": [{"title": "1 Introduction", "content": "Attention-based architectures, particularly Transformers, have revolutionized artificial intelligence.\nLarge language models such as Llama [TLI+23], Claude-3 [324], GPT-4 [OAAea24], and et al. have\nsignificantly transformed the AI landscape. Besides, vision models like Vision Transformer(ViT)\n[Dos20] and Data-efficient Image Transformer(DeiT) [TCD+21] have revolutionized the visual do-\nmain by directly processing image patches, bypassing the limitations of traditional Convolutional\nNeural Networks (CNNs). These models demonstrate outstanding performance in fields of nat-\nural language processing and computer vision, driving advancements across diverse fields, in-\ncluding content creation [LL22, ASO23, AKPDSLC23], software development [SDFS20, OZHW23,\nNDCDP+23], multimodal application [WFQ+23, HXL+24a, HXL+24b], machine translation [HAS+23,\nWLJ+23, HWL+23] etc.\nTime series prediction tasks are crucial for forecasting future trends and have been widely used in\nmaking data-driven decisions in various fields, such as finance [KB96, WHLZ13, SGO20], healthcare\n[ZIP06, BPV+18, KCS+20] and traffic flow forecasting [VDVDW96, LBF13, YS16]. In addition to\ntheir success in NLP, Transformer models have recently gained significant attention in time series\nprediction tasks. The ability of Transformers to capture involuted patterns and model long-range\ndependencies has led to their growing adoption in time series prediction tasks, with several recent\nstudies [ZZP+21, WXWL21, ZMW+22, NNSK22, LYL+22, LHZ+23]. These models utilize self-\nattention mechanisms to focus on relevant time steps, which makes them particularly well-suited\nfor handling time series data with irregular intervals and high dimensions. Furthermore, some\ntransformer-based methods integrate techniques such as temporal fusion [LALP21], hierarchical\nattention [BLM23], and patching process [NNSK22] etc., allowing them to better capture multi-\nscale temporal dependencies and adapt to non-stationary patterns in time series.\nHowever, recent studies have challenged the performance of Transformers in time series pre-\ndiction tasks. Some researchers have found that simple linear layers can outperform more com-\nplex Transformers in terms of both accuracy and efficiency [ZCZX23, DKL+23]. Many works\nhave provided explanations for why Transformer performs worse than simple linear layers on TSF\ntasks. [ZCZX23] argue that the poor performance of Transformer on TSF tasks stems from its\npermutation-invariant self-attention mechanism, which results in the loss of temporal information.\n[ZY23] and [EJN+23] attribute the issue to the Transformer's practice of embedding multiple vari-\nables into indistinguishable channels, leading to a loss of both variable independence and multivari-\nate correlations. However, there is a lack of theoretical understanding regarding why transformers\noften perform worse than simple linear models in time series forecasting tasks. To address this\ngap, we present the first theoretical analysis of this issue, shedding light on the underlying factors\ncontributing to the performance discrepancy.\nTo demystify the black box, we conducted the following analysis: First, we utilized data gener-\nated by the State Space Model (SSM) [KDS+15] to model time series data. This approach builds on\nthe work in [GJT+22], which demonstrated the SSM's robust modeling capabilities for sequential\ndata. Notably, based on our observation that the linear residual network (N-Linear) [ZCZX23] per-\nforms well in fitting sequential data, we designed a simple task. In this task, the model only needs\nto apply a straightforward linear mapping to the core features of the time series, which results in\nrelatively small errors.\nFor the sake of subsequent theoretical analysis, we consider an over-parameterized attention\nnetwork with a d = 1 in our setting where d denotes the input feature dimension, i.e.,\n\n$f(x, w, a) := \\frac{1}{\\sqrt{m}} \\sum_{r=1}^{m} a_{r} \\cdot (\\text{softmax}(x_{d} , w_{r} \\cdot x), x)$"}, {"title": "2 Related Work", "content": "Time Series Forecasting. Time Series Forecasting (TSF) [LZ21, CML+23, RHX+23, WHL+24]\nis a classical task of predicting future values based on historical data, widely used in finance,\nweather, traffic, and healthcare. Traditional methods like ARIMA [BP70] and ETS [GJ85] have\nbeen reliable due to their solid theoretical foundations, but they are limited by assumptions such as\nstability and linearity, affecting real-world accuracy. In recent years, the rapid development of deep\nlearning (DL) has greatly improved the nonlinear modeling capabilities of time series forecasting\n(TSF) methods. For example, Liu et al. [YDJY17] utilize LSTM [Hoc97] for multi-step forecasting\nin time series tasks and demonstrate that its performance outperforms traditional models. Li et\nal. [LLWD22] present a bidirectional VAE with diffusion, denoise, and disentanglement, improving\ntime series forecasting by augmenting data and enhancing interpretability, outperforming compet-\nitive methods in experiments. With Transformer's outstanding performance in NLP and CV, it\nhas quickly been applied to time series forecasting tasks, demonstrating superior performance com-\npared to traditional methods. Notable works include Informer [ZZP+21], Autoformer [WXWL21],\nFEDformer [ZMW+22], PatchTST [NNSK22], Pyraformer [LYL+22], iTransformer [LHZ+23] etc.\nNeural Tangent Kernel. The Neural Tangent Kernel (NTK) was initially proposed by Ja-\ncot et al. [JGH18] to provide a framework for understanding over-parameterized neural network\ntraining behavior. This work showed that, under specific conditions, deep neural network train-\ning can be approximated by a linear model, with the NTK governing parameter evolution during\ngradient descent. Since its introduction, NTK has become a key tool for analyzing training in"}, {"title": "3\nBackground: Transformer Fails to Beat Linear Model in TSF", "content": "As a crucial research direction for data science and statistics, time series forecasting (TSF) tasks\nhave played an important role in various domains, including finance analysis, health care, energy\nmanagement, etc. In recent years, with the outstanding performance of Transformers in the field\nof Computer Vision (CV) and Natural Language Process (NLP), many studies have applied the\nTransformer architecture to time series forecasting tasks [LJX+19, ZZP+21, WXWL21, ZMW+22,\nNNSK22, LYL+22, LHZ+23]. The primary reason for introducing Transformer-based methods into\nTSF tasks is their attention mechanism, which effectively models long-range dependencies in the\ntime domain. For instance, Informer [ZZP+21] introduces the ProbSparse self-attention mecha-\nnism and self-attention distilling techniques, enabling Transformer-based methods to handle long\nsequence time-series forecasting (LSTF) efficiently; FEDformer [ZMW+22] introduces seasonal-\ntrend decomposition and frequency enhancing techniques, enabling the model to capture global\ntime-series trends; Crossformer [ZY23] introduces the Dimension-Segment-Wise embedding and\nTwo-Stage Attention techniques, enabling Transformer-based models to efficiently capture both\ncross-time and cross-dimension dependencies for multivariate time series forecasting, etc.\nHowever, it is still debated whether Transformer-based models are more efficient than other deep\nlearning models for time series tasks. The suitability of Transformer-based models for long-term\ntime series forecasting tasks is questioned in [ZCZX23]. The authors highlight that although these\nmodels are effective at capturing semantic correlations, their permutation-invariant self-attention\nmechanism causes a loss of temporal information. To support this, they introduce a one-layer linear\nmodel, LSTF-Linear, which outperforms advanced Transformer-based LTSF models across several\nTSF Benchmarks. They also suggest revisiting the effectiveness of Transformer-based approaches"}, {"title": "4 Preliminary: Problem Definition", "content": "We present our formal problem definition in this section. In Section 4.1, we introduce the task within\nour framework, the Residual State Space Model (SSM), and describe how we use the Residual SSM\nto generate training data. In Section 4.2, we introduce our two-layer attention model and its\ntraining details."}, {"title": "4.1 Task and Data", "content": "We consider a time series forecasting task with an input space $X \\in R^{d}$, a label space $Y \\in R$, a\nmodel class $H : X \\rightarrow R$, and a loss function $L : Y \\times Y \\rightarrow R$.\nFor every dataset $D := \\{(x_{i}, Y_{i})\\}_{i=1}^{n}$ over $X \\times Y$ and model $h \\in H$, our training objective of\nh is given as $L(h) := \\frac{1}{n} \\sum_{i=1}^{n}(h(x_{i}) - y_{i})^{2}$. In our times series forecasting task, there exists a set\nof distribution $D$ that consists of all possible distributions to which we would like our model to\ngeneralize. In training, we have access to a training distribution set $D_{\\text{train}} \\subset D$, where $D_{\\text{train}}$ may\ncontain one or multiple training distributions. It's clear that without further assumptions on $D_{\\text{train}}$\nand $D$, the time series forecasting task is impossible since no model can generalize to an arbitrary\ndistribution.\nIn recent years, State Space Models (SSM) [KDS+15, GJT+22, GD23, ZLZ+24, XYY+24,\nMLW24, RX24, SLD+24] have been widely applied in various fields, particularly in time series"}, {"title": "4.2 Model and Training", "content": "In this section, we state our model setting and details of its training.\nModel. In this paper, we consider a two-layer attention model:\n\n$f(x, w, a) := \\frac{1}{\\sqrt{m}} \\sum_{r=1}^{m} a_{r} \\cdot (\\text{softmax}(x_{d} , w_{r} \\cdot x), x)$"}, {"title": "5 Training Convergence with Asymmetric Learning", "content": "In this section, we present the analysis of training convergence with asymmetric learning. In\nSection 5.1, we will present the key tools we used: the Neural Tangent Kernel (NTK) induced by\nour model, Kernel Convergence, which is key needed for the NTK analysis, assumptions on NTK,\nand the associated assumptions. In section 5.2, we present the main result of our paper, which\nprovides a convergence guarantee for asymmetric learning within our framework."}, {"title": "5.1 Neural Tangent Kernel", "content": "Neural Tangent Kernel (NTK)[JGH18] provides a powerful tool for understanding gradient de-\nscent in neural network training, particularly for analyzing the behavior and convergence of deep\nnetworks[HLSY21, SWL22, QSS23, SWL23, GLS+24]. Here, we give the formal definition of NTK\nin our analysis, which is a kernel function that is driven by hidden-layer weights $w(t) \\in R^{1 \\times m}$.\nTo present concisely, we first introduce an operator function in the following. For all i \u2208 [n] and\nr \u2208 [m], we have\n\n$u_{i,r}(t) := \\text{exp}(x_{i,r}(t) w_{r}(t) \\cdot x_{i}) \\in R^{d}$\n\n$a_{i,r}(t) := (u_{i,r}(t), 1_{d}) \\in R$"}, {"title": "5.2 Training Convergence with Asymmetric Learning", "content": "Now, we are able to present our first theorem regarding the convergence of training with Asymmetric\nLearning:\nTheorem 5.2 (Informal version of Theorem I.1). Given an error e > 0. For $\\delta \\in (0,0.1)$, $\u0412 =$\n$\\{\\sqrt{(1 + \\sigma^{2})\\log(nN/\\delta)},1\\}$ and $D = \\{\\sqrt{\\log(m/\\delta)},1\\}$. Let $m = \\Omega(poly(\\lambda^{-1}, exp(B^{2}), exp(D)), n, d)$\nand the learning rate $\\eta \\leq O(\\frac{\\lambda\\delta}{poly(exp(B^{2}),exp(D)),n,d)})$. Let $T \\geq \\Omega(\\log(nB^{2}/\\epsilon)$, we have: $L(T) \\leq \\epsilon$.\nDenote $v_{\\text{min}} := min\\{\\sum_{k=1}^{d}(x_{i,k} - P_{k})^{2}\\}_{i=1}^{n}$ where $x_{i} := \\frac{1}{d}\\sum_{k=1}^{d}x_{i,k}$. The Asymmetric\nLearning of model weights is expressed by $w_{r}(t)$ updating with $a_{r}$ as formulated below, for any\n$t \\geq \\Omega(\\frac{1}{\\eta\\lambda v_{\\text{min}}}):$\n\u2022 Part 1. $Pr[w_{r}(t) > 0|a_{r} = 1] \\geq 1 - \\delta$.\n\u2022 Part 2. $Pr[w_{r}(t) < 0|a_{r} = -1] \\geq 1 - \\delta$."}, {"title": "6 Attention Fails in Sign-Inconsistent Next-step-prediction", "content": "In this section, we define the sign-inconsistent next-step-prediction evaluation task and provide\na theoretical analysis of the attention mechanism and residual linear model based on this task.\nSpecifically, we introduce this task in Section 6.1. We present the Residual Linear Network in\nSection 6.2. In Section 6.3, we give each model a theoretical boundary on this task."}, {"title": "6.1 Sign-Inconsistent Next-step-prediction", "content": "In this section, we present a new task named Sign-Inconsistent Next-step-prediction. In subsequent\nsections, we will analyze the theoretical capabilities of the attention mechanism for this task. We\ndefine the task formally as follows:\nDefinition 6.1. Let the residual state space data model be defined as Definition 4.1, then we define\nthe sign-inconsistent next-step-prediction evaluation task, considering d = N:\n1. Sample $h_{\\text{test}} \\sim N(0, I_{n})$. Generate $U_{\\text{test},i} = [U_{\\text{test},i,1}, U_{\\text{test},i,2},\u00b7\u00b7\u00b7, U_{\\text{test},i,d}, U_{\\text{test},i,d+1}] \\in R^{d+1}$\nvia Claim 4.2.\n2. If $U_{\\text{test},i,d} \\cdot U_{\\text{test},i,d+1} \\geq 0$, redo 1.\n3. Sample $\\xi_{\\text{test},i} \\sim N(0, \\sigma\\cdot I_{d+1})$ where $\\sigma \\geq 0$ is a small constant.\n4. $X_{\\text{test},i} = [X_{\\text{test},i,1}, X_{\\text{test},i,2},\u00b7\u00b7\u00b7, X_{\\text{test},i,d}]^{\\top} \\in R^{d}$ where $x_{\\text{test},i,k} := U_{\\text{test},i,k} + \\xi_{\\text{test},i,k}$ for $k\\in [d]$.\n5. $Y_{\\text{test},i} := U_{\\text{test},i,d+1} + \\xi_{\\text{test},i,d+1} \\in R$.\nWe define the test dataset as $D_{\\text{test}} := \\{(X_{\\text{test},i}, Y_{\\text{test},i})\\}_{i=1}^{N_{\\text{test}}} \\subset R^{d} \\times R$. Especially, $\\{\\xi_{\\text{test},i}\\}_{i=1}^{N_{\\text{test}}} = 0$.\nFor any mapping function $H : R^{d} \\rightarrow R$, the OOD risk is given by:\n\n$R(H) := \\lim_{n_{\\text{text}}\\rightarrow+\\infty} \\frac{N_{\\text{test}}}{n_{\\text{text}}} \\sum_{i=1} (H(X_{\\text{test},i}) - Y_{\\text{test},i})^{2}$"}, {"title": "6.2 Residual Linear Network", "content": "In this section, we present residual linear network[ZCZX23] mainly to compare it with the attention\nmechanism on the Sign-Inconsistent next-step-prediction evaluation task. Specifically, the residual\nlinear network first subtracts the last value of the sequence from the sequence from the input\ndata. This operation removes certain biases or trends in the data, aiming to eliminate unnecessary\ncomponents that might negatively impact prediction accuracy. Then, the data is passed through a\nlinear layer. This layer can apply more intricate transformations to capture the underlying linear\npatterns within the data. Finally, the subtracted part will be added back. The purpose of this\nstep is to retain the original characteristics of the data after removing some of the shifts while still\nbenefiting from the transformations applied. The formal definition of the residual linear network\nis as follows:\nDefinition 6.2. Given an input vector $x \\in R^{d}$. Denote $w_{\\text{lin}} \\in R^{d}$ as the model weight. The residual\nlinear network is defined by:\n\n$f_{\\text{lin}}(x) := (w_{\\text{lin}}, x - \\frac{x_{d}}{1_{d}}1_{d}) + x_{d}$"}, {"title": "6.3 Generalizations", "content": "In this section, We provide a proposition demonstrating the bound on the OOD risk of the residual\nlinear network and attention mechanisms for the Sign-Inconsistent next-step-prediction evaluation\ntask.\nProposition 6.3. We have:\n\u2022 Part 1. Let all pre-conditions in Theorem 5.2 hold, there is no w(t) \u2208 Rm that satisfies\n$R(f) \\leq \\tilde{O}(\\sigma^{2})$.\n\u2022 Part 2. There exists and exists only one $w_{\\text{lin}}^{*}$ that satisfies $\\sum_{k=1}^{d-1} w_{\\text{lin},k} \\cdot P_{k} = P_{d+1} - P_{d}$.\nHence, we have $R(f_{\\text{lin}}) \\leq \\tilde{O}(\\sigma^{2})$.\nPlease see Proposition J.3 for the detailed proof of this proposition.\nRemark. Part 1 of Proposition 6.3 shows that even if the width of the hidden layers is sufficient,\nand the model is trained for a long enough time, the attention mechanism fails to reduce the OOD\nrisk to a sufficiently low level in this task. In contrast, Part 2 shows that a set of parameters\nexists for the residual linear model that can reduce the OOD risk to the same bound in this\ntask. In conclusion, we theoretically prove that the attention mechanism performs worse than a\nsimple residual linear model on OOD generalization tasks. This proof provides insight into why\nTransformers underperform on TSF tasks compared to simple linear models."}, {"title": "7 Conclusion", "content": "In this work, we give the first theoretical explanation of the learning mechanism behind the\ntransformer-based models' inefficient performance on TSF tasks. We focus on the attention net-\nwork to predict next-step in the time series, whereas we find that the value of output-layer ar\n(value projection in attention network) will lead the asymmetric learning to the hidden-weights wr,\nand it further leads the softmax scores on some important features unavoidably being low value.\nThat is, attention fails to learn the most common behavior in TSF tasks, residual feature (a.k.a\ndifferential feature). We hope our theoretical confirmation could provide more constructive insights\nfor practitioners to design and improve more efficient transformer-based architecture for the field\nof time series."}]}