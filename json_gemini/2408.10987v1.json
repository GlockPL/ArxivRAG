{"title": "Denoising Plane Wave Ultrasound Images Using Diffusion Probabilistic Models", "authors": ["Hojat Asgariandehkordi", "Sobhan Goudarzi", "Mostafa Sharifzadeh", "Adrian Basarab", "Hassan Rivaz"], "abstract": "Ultrasound plane wave imaging is a cutting-edge technique that enables high frame-rate imaging. However, one challenge associated with high frame-rate ultrasound imaging is the high noise associated with them, hindering their wider adoption. Therefore, the development of a denoising method becomes imperative to augment the quality of plane wave images. Drawing inspiration from Denoising Diffusion Probabilistic Models (DDPMs), our proposed solution aims to enhance plane wave image quality. Specifically, the method considers the distinction between low-angle and high-angle compounding plane waves as noise and effectively eliminates it by adapting a DDPM to beamformed radiofrequency (RF) data. The method underwent training using only 400 simulated images. In addition, our approach employs natural image segmentation masks as intensity maps for the generated images, resulting in accurate denoising for various anatomy shapes. The proposed method was assessed across simulation, phantom, and in vivo images. The results of the evaluations indicate that our approach not only enhances image quality on simulated data but also demonstrates effectiveness on phantom and in vivo data in terms of image quality. Comparative analysis with other methods underscores the superiority of our proposed method across various evaluation metrics. The source code and trained model will be released along with the dataset at: http://code.sonography.ai", "sections": [{"title": "I. INTRODUCTION", "content": "ULTRASOUND imaging, a non-invasive and cost-effective medical diagnostic tool, is crucial in modern healthcare by providing real-time visualization of internal structures and abnormalities. Nevertheless, despite its many advantages, the presence of noise can hinder broader use of ultrasound imaging because it can restrict the interpretability and diagnosis precision of the medical images [1]. The development of robust post-processing techniques [2] [3] for ultrasound images has, therefore, become imperative to enhance image quality and facilitate more accurate diagnoses. To address the mentioned challenges, classical techniques have delved into realms such as image filtering, speckle reduction algorithms, and contrast enhancement methods [4]. These endeavors, while being impactful, often encounter complexities inherent to ultrasound data, which necessitates the development of more robust methods which have adaptive approaches [5]. Recently, deep learning has come to the world as a game changer in the domain of medical imaging, and ultrasound image analysis is not an exception. Deep learning has shown remarkable potential in dealing with the formidable challenges of ultrasound imaging. These models, harnessed by their ability to leverage large-scale datasets and discern intricate patterns within ultrasound data, have the power to significantly reduce noise, enhance image contrast, and amplify image fidelity. Recent studies underscore their prowess in tasks such as image denoising, image segmentation, and pathology detection [6]. For instance, U-Net [7] stands as a landmark in medical image segmentation and denoising, spanning several applications in ultrasound imaging [8]. Furthermore, the work by Kaur et al. [9] underlines the promising trajectory of deep learning techniques in denoising ultrasound images. Recently, Denoising Diffusion Probabilistic Models (DDPM) [10] have shown a remarkable performance in image denoising. DDPMs are based on diffusion models, which are a part of larger generative models. Among the pioneers of generative modeling, Generative Adversarial Networks (GANs) [11], Variational Autoencoders (VAE) [12], and diffusion models [10] stand out as versatile frameworks, each offering a unique perspective on the generation of complex data. To gain a panoramic view of the three mentioned methods, a pictorial overview of the advantages and disadvantages associated with each method [13] is presented in Fig. 1. As illustrated, GANs excel in high-quality sample generation and rapid sampling but face challenges in achieving mode coverage diversity. On the other hand, VAEs possess the advantage of fast inference coupled with mode coverage diversity, although they may not be as effective in generating high-quality samples. Lastly, diffusion models are renowned for their excellence in high-quality sample generation and mode coverage diversity but may lag in terms of sampling time."}, {"title": "II. RELATED WORK", "content": "Considering the main idea of the proposed method and our emphasis on ultrasound image denoising, in this section, we specifically concentrate on denoising methods. Subsequently, we will delve into the examination of generative models and their application in the context of image denoising."}, {"title": "A. Ultrasound image denoising methods", "content": "Several elements contribute to the degradation of ultrasound image quality: Gaussian electronic noise present in the RF data, and a range of acoustic noise sources, including reverberation and multiple scattering phenomena. These factors collectively lead to large diffuse reverberations and clutter, adversely affecting image clarity. In this paper, we exploit advances in the field of denoising to improve the quality of ultrasound images. It is important to emphasize that our goal is not to despeckle the ultrasound image, as ultrasound speckle is not simply noise; it often holds essential information pertinent to diagnosis and image-guided interventions. While despeckling might aid tasks like image segmentation, it can adversely affect radiologists and certain subsequent tasks, such as quantitative ultrasound. Instead, our aim is to concentrate on denoising the images, ensuring the preservation of the speckle pattern. Before deep learning, adaptive filtering methods utilized filters such as the Wiener filters, and adjusted their parameters based on local image characteristics, smoothing out noise while preserving essential anatomical structures [29]. Wavelet-based denoising, another prevalent classical method, decomposed ultrasound images into different frequency components, enabling selective noise reduction while retaining crucial image features [30]. The adaptability to target noise at specific scales has made wavelet-based denoising a valuable asset in ultrasound image processing. Anisotropic diffusion filters were also employed to enhance ultrasound images by selectively permitting diffusion along image edges while preserving the edges themselves [30]. This approach maintained structural details in the image while reducing noise. Furthermore, the non-local means denoising algorithm found its application in ultrasound image denoising [31]. Leveraging the redundancy in ultrasound images, the method averaged similar pixels, effectively reducing noise without compromising vital diagnostic information. While classical methods typically suffer from high computational complexity and produce blurry outputs, deep learning-based methods exhibit low computational complexity in the inference phase and generate denoised images of higher quality. Recently, a number of methods based on deep learning have been developed to improve the quality of ultrasound images. For instance, Perdios et al. [32] trained an adapted version of U-Net on simulated data and showed that their network also performs well on in vivo data. Gupta et al. [33] looked at image denoising as an inverse problem and solved it using a deep learning model. A network called Mimicknet [34] was proposed to improve the image quality in post-beamformed data. Zhang et al. [35] aimed to reconstruct high-quality ultrasound PWs from raw channel data by training a self-supervised trained network in an inverse problem approach. Van Sloun et al. [5] have explored deep learning methods that have been developed for adaptive beam-forming, spectral Doppler, clutter suppression, and super-resolution. Super-resolution is another post-processing technique commonly employed to transform a low-quality image into a higher-quality one. This technique has recently garnered significant attention from researchers specializing in medical image analysis. Khakzad et al. [36] designed a network made of an encoder followed by a transformer-based decoder for probable localization. In another similar work [37], Deep-ULM was suggested for vascular ultrasound imaging using super-resolution. In comparison to natural images, ultrasound data exhibit challenges arising from both high frequency and wide dynamic ranges [38]. These characteristics, irrespective of the specific application, pose significant obstacles to the effective utilization of Convolutional Neural Networks (CNNs) for ultrasound image enhancement. Various studies have attempted to address these challenges by proposing pre-processing techniques, architectures, and loss functions. For instance, it has been demonstrated that individually standardizing each RF ultrasound image in the dataset enhances the performance of CNNs by mitigating the detrimental effects of high dynamic range and utilizing the data more efficiently [39]. To address both the high dynamic range and the oscillating properties of RF ultrasound images, mean signed logarithmic absolute error (MSLAE) was introduced as the loss function for training a residual-based CNN with multi-scale and multi-channel filtering properties [40]. For efficient recovery of high-frequency contents, the wavelet transform was employed in a multi-resolution architecture [41]. To mitigate the risk of getting trapped in local minima during the initial stages of optimization on fluctuating RF ultrasound images, an adaptive mixed loss function was proposed that gradually transitions from B-mode loss to RF loss as the training progresses [28]. One of the major concerns when using CNNs for ultrasound image denoising pertains to the network's ability to retain informative features while removing noise. To address this concern, different attention mechanisms were utilized to guide feature extraction in different layers of CNNs. Dong et al. [42] employed a multi-stage CNN, introducing feature masking to identify the most beneficial features. Residual attention was another attention mechanism used for more efficient feature selection in [43]. In [44], image features were effectively extracted, benefitting from a lightweight mixed-attention block to surpass the noise during the encoding. More specifically, this method employs separation and re-fusion techniques for channel-spatial attention. Although conventional deep learning-based models have shown a great impact on ultrasound image denoising, challenges such as introducing artifacts to the resulting images and losing image details still exist. In contrast, due to its iterative denoising approach, DDPM effectively reduces noise while preserving crucial image details, offering a superior solution for enhanced image quality."}, {"title": "B. Generative models", "content": "Numerous valuable efforts have been made to develop generative models for the purpose of denoising and image enhancement. MedGAN [45] was designed for medical image-to-image translation, which brought two main contributions. First, a discriminator was trained to play the rule of feature extractor by measuring the distance between the output and the desired one. Second, the textures and fine structures of the desired target images are matched to the translated images using style-transfer losses. Chung et al. [46] have proposed a score-based diffusion model to reconstruct high-quality MRI images from a low-quality image. Their method process the real and imaginary data separately and finally adds them together. In contrast to our method, which is initialized by one PW in the reverse process, their method starts from pure noise and passes many steps to reach a high-quality image in the reverse process. Goudarzi et al. [47] proposed a generative adversarial network to recover a high-quality image from a single-focus image, which resembles a multi-focus ultrasound image in terms of quality. UltraGAN [48] also applied GANs to improve ultrasound echocardiography. Apart from the GANs, there are some works that adapted diffusion models on ultrasound data. In [1] diffusion model was employed for ultrasound cardiography dehazing. Two diffusion models were trained to generate a patch-wise clean image and haze separately using a hazy image as a condition. Domingues et al. [49] introduced ultrasound physics to a diffusion model for ultrasound image generation. They modified the noise scheduler based on attenuation maps to include the attenuation in their synthetic data. In Zhang et al. [26], a reconstruction method was proposed based on the diffusion models to reconstruct B-mode images from raw channel data. Their main desire was to eliminate the effects of the normal noise that exists in the raw channel data, which appears as artifacts in the beamformed data. In Li et al. [50] and Lan et al. [51], the authors have proposed a score-based diffusion model which trained on B-mode 75 compounding PW images to enhance ultrasound image quality from one PW to 75 compoundings. In a recent work [52], the authors propose a new method to distinguish 13 anatomies in fetal ultrasound videos using a dual-conditioned diffusion model."}, {"title": "C. Background", "content": "Denoising diffusion models are composed of two procedures: the forward process and the reverse process. In the forward process, an image is perturbed by normal noise during several steps. In the next step, a network learns how to approximate the noise in each step. In the reverse process, starting from pure noise, the model (using an iterative process) reconstructs a sample using the training distribution based on the learned priors by the network. Here are more details about the procedures (from [10]):\n1) Forward process: Given an initial image $x_0$, the output of each step (a Markov chain) can be defined as follows:\n$x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t} \\epsilon, \\epsilon \\sim N(0, I),$ (1)\nwhere $x_t$ is the data at time step $t$, $\\beta_t$ is a variance schedule, $\\epsilon$ is noise sampled from a normal distribution $N(0, I)$. In that case, $q(x_t | x_{t-1})$ has a normal distribution:\n$q(x_t|x_{t-1}) = N(x_{t-1}, \\mu(x_{t-1,t}), \\sigma_t).$ (2)\nIt can be directly linked to $x_0$ by:\n$x_t = \\sqrt{1 - \\bar{\\beta}_t} x_0 + \\sqrt{\\bar{\\beta}_t} \\epsilon$, where, $\\bar{\\beta}_t = \\sum_{s=1}^{t} \\beta_s$ (3)\n2) Reverse process: In the reverse process, the objective is to transition from a noisy (low-quality) image at step $T$ and enhance the image quality step by step to approximate the original high-quality image $x_0$ closely. This entails maximizing the likelihood of the predicted image $P_\\theta$, where $\\theta$ denotes the prediction parameters, or minimizing the negative log-likelihood of the prediction as follows:\n$P_\\theta(x_0) = \\int P_\\theta (x_{0:T}) dx_{0:T},$ (4)\n$L_\\theta(x_0) = -log(P_\\theta(x_0)),$ (5)\nwhere\n$P_\\theta(x_{0:T}) = \\prod_{t=1}^{T} P_\\theta(x_{t-1}|x_{t}).$ (6)\nNote that $p_\\theta$ is the predicted noise distribution at each step of the reverse process, and $P_\\theta$ indicates the PDF of the predicted $x_0$. Since the predicted $x_0$ is conditioned on several consecutive steps, to calculate $P_\\theta$ in integral form, we need to integrate over a very high dimensional (pixel) space for continuous values over $T$ time steps. As such, this integral is not tractable. To solve the problem stemming from variational lower bound [53], one can write:\n$- log(P_\\theta(x_0)) = -log(E_q[\\frac{P_\\theta (x_{0:T})}{q(x_{0:T}|x_0)}]) = -log \\int \\frac{P_\\theta (x_{0:T})}{q(x_{0:T}|x_0)} dx_{0:T} = -log E_q[\\frac{P_\\theta (x_{0:T})}{q(x_{0:T}|x_0)}],$ (7)"}, {"title": "III. METHODS", "content": "This paper presents an iterative model inspired by DDPMs to enhance a low-quality beamformed RF image to a high-quality beamformed RF image. Specifically, our primary objective is to enhance the quality of a PW image initially constructed with a restricted number of compounded angles to closely resemble the quality of an image constructed using a larger number of angles. In other words, we consider the low-compounded ultrasound images (low-quality) in $\\pi_1$ space and their corresponding high-compounded counterparts (high-quality) in $\\pi_0$. Therefore, our problem can be defined as a transition from $\\pi_1$ to $\\pi_0$. In similar diffusion-based denoising methods [1], [16], a diffusion model is trained on high-quality image space to generate high-quality images (corresponding to $t = 0$) starting from Normal noise (corresponding to $t = T$). Then, in the inference procedure, a low-quality image passes through a number of forward process steps to the extent that the resulting noisy image becomes close to the noise in the training phase (corresponding to $t = T$). Finally, in the reverse process, the trained network generates a high-quality image starting from the noisy image corresponding to $t = T$. This approach includes many steps in the reverse processes, which creates very high-quality results but requires additional guidance to ensure the reliability of the output. As an example, Li et al. [50] also adapted this approach and introduced data consistency to reduce the error that can be compounded in several diffusion steps. In this work, similar to [17]-[20], the idea is to train a diffusion model to move directly from $x_1$ in space $\\pi_1$ toward $x_0$ in space $\\pi_0$. This approach dramatically reduces the number of required sampling steps because a single-PW image is not very different from a compounded image. Based on our particular objective, we examined the disparity between low-compounded ultrasound images (low-quality from $\\pi_1$) and their corresponding high-compounded counterparts (high-quality from $\\pi_0$). The analysis indicates that the distribution of this difference can be close to a Gaussian distribution. To visually depict this resemblance, a Quantile-Quantile plot (QQ-plot) has been generated, illustrating the difference between beamformed RF data corresponding to one and 75 compounding PWs patches in Fig 2. Having a close look at the plot, there are some unfitted regions between the blue points and the red line (Gaussian distribution), which means the mentioned difference is not necessarily Gaussian in some regions. Although the vanilla diffusion models used normal Gaussian noise in their forward and reverse processes, here, stemming from [18]-[20], we define a forward and a reverse process to directly transfer between $\\pi_0$ and $\\pi_1$ without requiring to go to Normal noise distribution in between. It means the forward and reverse processes are not merely dependent on Gaussian noise."}, {"title": "A. Forward process", "content": "Having observations like $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$, we can construct a forward process based on ordinary differential equations to gradually move from $\\pi_0$ to $\\pi_1$ following an interpolation procedure:\n$x_t = (1-t)X_0 + tX_1,$ (18)\nwhere $t \\in [0, 1]$. After a minor replacement:\n$x_t = X_0 +t(X_1 - X_0),$ (19)\n$\\Delta x_t = (X_1 - X_0) \\Delta t,$ (20)\n$x_{t+\\Delta t} = x_t + \\Delta x_t = x_t + v(x,t) \\Delta t.$ (21)\nHence, $(X_1 - X_0)$ can be considered as a flow velocity $v(x_t, t) = \\frac{X_1 - X_0}{\\Delta t}$, which determines the changes in each time step (in the forward process). Figure 3 illustrates an example of the described process for $T = 10$ ($t\\in \\{0, 0.1, 0.2, ..., 1\\}$, $\\Delta t = 0.1$), wherein a high-quality image is gradually affected by $v(x_t,t)$ over ten steps."}, {"title": "B. Reverse process", "content": "In the reverse process, starting from an initial point like $x_1$ from $\\pi_1$, we want to move toward $x_0$ from $\\pi_0$ while following a trajectory that is as close as possible to the reverse of the forward process, which means maximizing $p_\\theta(x_1)$. Similar to 13 and 14:\n$q(x_{t-\\Delta t} | x_t, x_0) = q(x_t - v(x_{t-\\Delta t}, t) \\Delta t).$ (22)\n$P_\\theta(x_{t-\\Delta t}|x_t) = p(x_t - V_\\theta(x_{t-\\Delta t}, t) \\Delta t).$ (23)\nTo make sure that we have a true trajectory in the reverse process, a network should be trained to solve the following optimization problem:\n$\\theta = arg min \\{l_1 (v(x_t, t), v_\\theta (x_t, t))\\},$ (24)\nwhere $l_1$ denotes the loss function and $\\theta$ refers to the network's parameters. Finally, the reverse process is done as follows:\n$x_{t-\\Delta t} = x_t - v_\\theta(x_{t-\\Delta t}, t) \\Delta t.$ (25)\nIt is worth noting that because there is no stochastic factor in the reverse process, and also our network does not have any randomness factor, the reverse process is considered a deterministic process."}, {"title": "C. Architecture", "content": "The overarching structure of the employed CNN is illustrated in Fig. 4, which is comprised of two primary modules called convolutional blocks and time embedding modules. The time embedding module is devised to allocate a vector to each time step t through Sine and Cosine transforms, serving as informative cues for the network's temporal understanding. These time vectors are subsequently input into the convolutional blocks to provide temporal context. Within the convolutional blocks, the data stream incorporates an input tensor alongside the associated time vectors. Initially, the input tensor undergoes a 3\u00d73 convolutional layer, succeeded by batch normalization and ReLU nonlinearity layers. Simultaneously, the input time vector is processed through a linear layer to align with the features extracted from the convolutional layers, facilitating their summation in the subsequent step. Subsequently, the resulting tensors traverse a 4\u00d74 convolutional layer, followed by batch normalization and ReLU activation functions. Note that the provided description pertains to the first four blocks, while for the subsequent blocks, a 4\u00d74 convTranspose is employed instead of the 4\u00d74 convolutional layer. In all convolution layers, both the stride and padding were set to one."}, {"title": "IV. EXPERIMENTS", "content": "This section provides details regarding the evaluation procedure for the proposed method. We begin with an overview of the datasets, followed by an explanation of the data generation sequences and the inclusion of test images. Subsequently, we delve into the evaluation metrics and then elaborate on the training strategy and comparison results."}, {"title": "A. Dataset", "content": "In this study, the proposed network was trained using a simulated dataset. We leveraged both phantom and in vivo data for evaluation, supplementing the simulated test data.\n1) Simulated data: The simulated dataset, which consists of 400 images, was simulated using the publicly available Field II package [54] [55]. The designed phantom, situated at an axial depth of 10 mm from the transducer's face, measures 45 mm laterally and 40 mm axially and contains a uniform distribution of fully developed speckle patterns, with an average scatterer density of 60 per resolution cell. Two different types of contrast were introduced to the images, including anechoic regions and hyperechoic regions. To create the two echogenicity types and similar to [28], we utilized a public dataset called XPIE [56], which contained segmented natural images. We isolated only the segmentation masks and resized them to match the dimensions of the phantoms. Subsequently, we applied a weight to the amplitude of scatterers within the masks, determined by the echogenicity type of each image. For anechoic regions, the weight was set to zero. For hyperechoic regions ranging from +3 dB to +12 dB, the weight was a uniform random number in the interval [2, 15.8]. The image simulation method outlined in this subsection has the benefit of supplying the network with more extensive features than just including simple cyst-shaped regions. Furthermore, a test image was generated, depicting two cysts with diameters of 10 mm and 15 mm. These cysts were located at central lateral positions, with one at a depth of 10 mm and the other at a depth of 28 mm. The transducer defined for the simulation mirrors the 128-element linear probe L11-5v (Verasonics in Kirkland., WA). The values opted for central and sampling frequencies were 5.208 MHz and 20.832 MHz, respectively. Note that Field II is subject to numerical precision limitations, compelling us to establish the initial sampling frequency at 104.16 MHz and, subsequently, downsample the resulting data by a factor of 5. All images were simulated using a full synthetic aperture scan. Following that, for each image, we synthesized 75 PW scans corresponding to different emission angles. After synthesizing the raw RF data, the delay and sum (DAS) algorithm was applied to construct beamformed RF images with dimensions of 1082\u00d7192 in the axial and lateral directions, respectively."}, {"title": "B. Evaluation metrics", "content": "We use key metrics like Contrast to Noise Ratio (CNR) and Generalized Contrast to Noise Ratio (gCNR) [58] for quantitative evaluation. In addition, because our goal is enhancing the image quality using a high-quality image as the target, we should investigate how much our method can get the input image closer to the target image. To do so, we measure the similarity between the output images and their corresponding high-quality image utilizing Normalized Root Mean Squared Error (NRMSE).\n1) The CNR index is calculated as follows:\n$CNR = 20 log_{10} (\\frac{|\\mu_{ROI} - \\mu_{B}|}{\\sqrt{(\\sigma_{ROI}^2 + \\sigma_{B}^2)/2}}),$ (26)\nwhere $\\mu$ and $\\sigma$ point to the mean and standard deviation of both background and region of interest, respectively.\n2) It has been proved that CNR depends on dynamic range alternation [59], and gCNR was introduced to mitigate this problem. The corresponding formula for gCNR calculation is:\n$gCNR = 1 - \\int min\\{P_{ROI}(x), P_B(x)\\}dx,$ (27)\nwhere $p_B$ and $P_{ROI}$ points to the histogram of the background and region of interest, respectively. The gCNR metric quantifies the extent of overlap between the pixel intensity distributions in two regions, considering dynamic range transformations. A higher gCNR value signifies reduced overlap between the distributions, reaching a maximum value of 1 when the two distributions exhibit no overlap.\n3) NRMSE: The Euclidean distance, renowned alongside the Mean Squared Error (MSE), stands as one of the most prevalent metrics employed for assessing the similarity between two vectors. However, when evaluating diverse methods across disparate datasets characterized by specific dynamic ranges, computing the general MSE may fail to provide a comprehensive understanding of the enhancement potential. Therefore, normalization is employed to scale distances within the range of [0, 1] across all test sets. For an image denoted as I and a corresponding target Y, comprising L elements, NRMSE can be calculated as follows:\n$RMSE = \\sqrt{\\frac{1}{L} \\sum_{i=1}^{L} (I_i - Y_i)^2},$ (28)\n$NRMSE = \\frac{RMSE}{max(Y)}$ (29)\nNote that the denominator corresponds to the range of Y, as the minimum value of Y is zero.\n4) KS test: The Kolmogorov-Smirnov (KS) test in ultrasound, is a statistical method used to quantifies the differences between the cumulative distribution functions (CDFs) of pixel intensity values in the images, providing a measure of how well the processed image retains the characteristics of the original speckle pattern.\n5) SSIM: The Structural Similarity Index (SSIM) is a metric used to measure the similarity between two images. It takes into account three components of image quality: luminance, contrast, and structure. SSIM compares local patterns of pixel intensities in the images and computes a score between -1 and 1, where 1 indicates perfect similarity. Higher SSIM values suggest that the images are more similar in terms of quality."}, {"title": "C. Training", "content": "The architecture illustrated in Fig. 4 was employed to predict noise parameters at each time step. During the training phase, following the forward process, a high-quality beam-formed image undergoes degradation based on a randomly selected time step noise coefficient $\\beta_t$, where $1 < t < T$, as described in [10]. Subsequently, the network is exposed to the degraded image as input and its corresponding additive noise as output. This enables the network to learn how to predict the noise parameters without bias towards any specific time step. The training procedure encompassed 350 epochs, employing an Adam optimizer [60] with zero weight decay. The initial learning rate was set to 0.004 and was adjusted by a linear scheduler with a step size of 60 and a gamma value of 0.5 during the training stage. While the designed network is independent of the image size, all simulated images were in the size of 1082\u00d7192. Among the 400 simulated images, 20 and 10 percent were randomly selected as validation and test sets, respectively. The remaining images constituted the training set. An $l_1$ norm loss function was utilized to quantify differences between ground truth and predictions during the training phase. All training procedures were conducted on an NVIDIA RTX 4090 GPU with 24 GB RAM."}, {"title": "D. Results", "content": "The proposed method relies on three parameters: J (the number of compoundings in a low-quality image), K (the number of compoundings in a high-quality image), and T (the number of diffusion steps). The results presented in this section are based on the parameters that yielded superior outcomes in the conducted experiments and will be specified during the explanation of each experiment.\n1) Results on simulated test data: To evaluate the performance of the proposed method within the same domain as the training dataset, we generated a test image with the same configuration as the training dataset, as detailed in subsection IV-A.1. The proposed method was then evaluated using the simulated test image across three distinct scenarios explained in Table I, each featuring specific low-quality and high-quality pairs. The quantitative results corresponding to each scenario are outlined in Table II, and the network outputs for each scenario are shown in Fig. 6.\n2) Results on phantom data: In this section, we aim to evaluate our method using the CIRS phantom provided in the PICMUS dataset [57]. Alongside showcasing our method's results, we also trained the network architectures proposed in [32], [40], and [41], in a fully supervised fashion, on our training set to enable a comparison with other methods. Fig. 7 visually demonstrates the comparisons on the phantom data. Overall, all methods have succeeded in denoising the input image. However, a closer examination of details, particularly in the magnified cysts, provides more insights into each model's denoising potential. In the cyst patches, it is observed that other methods have partially improved the distinguishability of cysts compared to the input image, yet the boundaries of the cysts are not clearly defined. When it comes to the proposed method, we found that due to the domain shift between our simulation training dataset and experimental test data, high iterations of the reverse process may cause some artifacts in the resulting image. Therefore, we set the number of reverse iterations equal to 3, 5, and 7, respectively. The resulting images across all iterations reveal that cyst areas are more detectable compared to other methods. Notably, in the fifth and seventh iterations, the boundaries of the cysts are even more clear than in the 75 compounded image. In addition, there are distortions in the reconstructed images in cyst 2, which are most pronounced in (b) to (d). These results correspond to MSLAE [40], DANUIE [32], and FCNN [41], respectively. These distortions demonstrate the extent to which various methods can reduce clutter in the anechoic region.\n3) Results on in vivo data: The efficacy of the proposed method is further assessed using carotid artery data to ascertain its ability to generalize the training performance to clinical applications. Visual comparisons with different methods are presented in Fig. 8. Subfigures (e), (f), and (g) correspond to the outputs of the proposed method for 3, 5, and 7 iterations, respectively. All of those results eliminate clutter artifacts and enhance the visibility of the artery in the image."}, {"title": "V. DISCUSSION", "content": "Our proposed method demonstrates superior performance when compared to existing supervised deep learning-based models in the context of PW ultrasound denoising. A key aspect contributing to this efficacy lies in our approach of breaking down the noise into smaller, more manageable segments. This strategic division aims to facilitate a more targeted and efficient learning process for the model by simplifying the distribution learning task. By breaking down the noise, our method can effectively capture and address specific characteristics, leading to enhanced denoising outcomes. We conducted our training using extensive RF images, adhering to the constraints of the Nyquist rate, thereby preventing any downsampling. While this approach ensures a high-fidelity representation of the ultrasound data, it introduces a computational burden due to the large size of the data. However, in future investigations, there is potential to explore the utilization of in-phase and quadrature (IQ) images, which offer more flexibility in terms of downsampling without violating the Nyquist rate constraints. This avenue could potentially streamline computations while maintaining the essential information required for effective PW ultrasound denoising. The versatility of our formulation can be extended beyond its application on PW ultrasound denoising because we only trained the model using simulation data, and the method performed well in real phantom and in-vivo data. In other words, since generating simulated data is generally less expensive than performing real experiments, the method can be trained for other types of ultrasound imaging. This includes, but is not limited to, focused and synthetic aperture imaging, broadening the scope of its applicability across different ultrasound modalities. Moreover, our method is not confined to a specific type of ultrasound transducer; it can seamlessly be employed with diverse transducer configurations, such as convex and phased array transducers (granted that new simulated data is generated and the network is fine-tuned on the new data). This adaptability underscores the robustness and generalizability of our approach, making it a promising candidate for enhancing image quality and denoising capabilities across a spectrum of ultrasound imaging modalities and transducer types. Deep learning models often show brittleness; although they provide good results for certain inputs, they can generate subpar results for others. Therefore, more experiments on simulated, real phantoms and in-vivo data are needed to ensure good performance in a large test database. In future investigations, we plan to delve into the potential impact of the RF data denoised through our method on downstream tasks, with a specific focus on ultrasound elastography [61", "62": ".", "18": [19], "20": "the output of our model is not stochastic and is deterministic for a given input. In contrast to conventional diffusion models like [1", "16": [50]}]}