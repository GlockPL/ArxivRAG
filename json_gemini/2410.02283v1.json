{"title": "MORPHOLOGICAL EVALUATION OF SUBWORDS VOCABULARY USED BY BETO LANGUAGE MODEL", "authors": ["\u00d3SCAR GARC\u00cdA-SIERRA", "ANA FERN\u00c1NDEZ-PAMPILL\u00d3N CESTEROS", "MIGUEL ORTEGA-MART\u00cdN"], "abstract": "Subword tokenization algorithms used by Large Language Models are significantly more efficient and can independently build the necessary vocabulary of words and subwords without human intervention. However, those subwords do not always align with real morphemes, potentially impacting the models' performance\u2014though it remains uncertain when this might occur. In previous research, we proposed a method to assess the morphological quality of vocabularies, focusing on the overlap between these vocabularies and the morphemes of a given language. Our evaluation method was built on three quality measures\u2014relevance, cohesion, and morphological accuracy and a procedure for their assessment. By applying this method to vocabularies created by three subword tokenization algorithms\u2014BPE, Wordpiece, and Unigram\u2014we concluded that these vocabularies generally exhibit very low morphological quality. In this article, we apply this evaluation to the tokenizer of BETO, a BERT language model trained on large Spanish corpora. This evaluation, along with our previous results, helped us conclude that its vocabulary has a low morphological quality, and we also found that training the tokenizer in a larger corpus does not improve the morphological quality of the generated vocabulary. Additionally, this evaluation helps clarify the algorithm used by the tokenizer,", "sections": [{"title": "1. INTRODUCTION", "content": "Tokenization is the process of splitting a text into smaller units called tokens (Friedman, 2023), which can be morphemes, words, punctuation marks, characters, among others. Tokenization is essential for processing text collections to create datasets, which are used to train artificial neural networks, as these data represent the discrete units of information from which language models are built (Friedman, 2023). In the training of current Transformer neural networks (Vaswani et al., 2017) to generate language models, vocabularies of tokens, which can be words and subwords, are used. A subword is a part of a word that appears frequently in the training corpora but does not necessarily correspond to a morpheme. For instance, BERT (Devlin et al., 2019) and its Spanish version, BETO (Ca\u00f1ete et al., 2023), use vocabularies of approximately thirty-one thousand tokens.\nAmong the most widely used tokenization algorithms for generating the vocabularies used by language models, three stand out: Wordpiece (Schuster, 2012; Wu, 2016), used by BERT; Byte-Pair Encoding (BPE) (Sennrich, 2015), used by the GPT family (Radford et al., 2018) and RoBERTa (Liu et al., 2019); and Unigram, used by Albert (Lan et al., 2019). The algorithms used to generate these vocabularies are based on the frequency with which certain characters or strings appear in the training corpora, relying on purely statistical criteria. As a result, frequently occurring words in the corpus may not be split and appear as a single token in the vocabulary, while less frequent words may be split into several higher-frequency subwords. For example, BETO's tokenizer does not segment the word poblaci\u00f3n (population) even though it is formed by the root pobla and the morpheme ##ci\u00f3n and treats it as a single token. However, it segments the word perduraci\u00f3n (perdurance) into the tokens perd and ##uraci\u00f3n, and the word oscilaci\u00f3n (oscillation) into oscila and ##ci\u00f3n."}, {"title": "2. OBJECTIVES OF THE STUDY", "content": "The main objective of this study is to evaluate the quality of the vocabulary used by the Spanish language model BETO, with the aim of determining:\n(i) whether the findings from our previous research regarding the low quality of vocabularies generated by the BPE and Wordpiece tokenization algorithms are confirmed,\n(ii) whether there is a threshold beyond which the size of the training corpus becomes independent of the vocabulary quality produced by these tokenization algorithms,\n(iii) the types of errors present in the vocabularies generated by these tokenization algorithms, and\n(iv) whether, based on the evaluation of BETO's vocabulary quality, it is possible to identify the specific tokenization algorithm used to generate it."}, {"title": "3. BETO'S TOKENIZER", "content": "The tokenizer used by BETO's language model has been trained on a corpus of 300 million sentences, which is five hundred times larger than the corpus used to train the tokenizers in Garc\u00eda-Sierra et al. (2024). However, with the information currently available, it is unclear what type of tokenizer BETO uses. According to its authors, the tokenizer is based on the BPE algorithm (Ca\u00f1ete et al., 2023:3). Yet, this information conflicts with what is specified on the Hugging Face download page and in the tokenizer's configuration file, where it is indicated that the tokenizer used is of the Wordpiece type (see Figure 1).\nOur intuition at this point is that Wordpiece is likely the tokenizer used in the current version of BETO, not only because of the indications in the configuration file, but also because if it were BPE, there should be a \"merges.txt\" file containing the learned character merges ranked by frequency from the training process. This file is absent from BETO's repository on Hugging Face\u00b9. However, considering the authors' claims, it is possible that the vocabulary was indeed created using BPE, and the configuration file refers to a different type of tokenizer for the inference phase. This would impact the model's performance since different subwords would be used.\nOur hypothesis is that evaluating BETO's vocabulary will help resolve this inconsistency and clarify which algorithm was actually used by the tokenizer. We will do this by comparing the quality metrics and error types with the results from our previous work."}, {"title": "4. EVALUATION METHOD OF THE MORPHOLOGICAL QUALITY IN LANGUAGE MODELS VOCABULARIES", "content": "Our proposal is that the method for assessing morphological quality should be based on measuring the similarity between the vocabulary produced by a tokenizer and the actual morpheme vocabulary of the target language model (Garc\u00eda-Sierra et al., 2024). We proposed that the degree of similarity depends on three quality criteria, as outlined in Section 1 of the introduction: morphological relevance, coherence, and accuracy.\nSpecifically, (i) morphological relevance refers to the proportion of actual morphemes from the language that are learned by the tokenizer; (ii) morphological coherence measures the consistency with which words which share a morpheme also share a token which corresponds to that morpheme; and (iii) morphological accuracy assesses the tokenizer's ability to use the learned morphemes to correctly segment words.\nTo evaluate these three criteria, it is necessary to: (1) construct a corresponding evaluation dataset for each criterion, and (2) calculate the degree of compliance for each.\n(1) For the construction of the evaluation datasets:\n(i) The dataset for assessing morphological relevance should consist of a list of the language's morphemes, ideally categorized by typology (e.g. prefix, suffix, and root). (ii) The dataset for the morphological consistency criterion should consist of a list of word-morpheme pairs that includes all morphemes in the language. This list can be generated from a previous dataset of morphemes and a list of words in the language that is as comprehensive as possible. (iii) The dataset for evaluating morphological accuracy should consist of a list of words, each labeled with its corresponding morphological segmentation(s). To construct this dataset, a random sample of words can be selected, ensuring that all morphemes in the language are represented at least once. Additionally, the proportional representation of grammatical categories in the dictionary of the language should be maintained.\n(2) The following metrics will be used to calculate the level of compliance with each criterion:\ni. For morphological relevance, the traditional metrics of precision, coverage, and F1-score will be used to measure the overlap between the vocabulary tokens of the language model and the morphemes collected in the morphological relevance dataset \nii. For morphological coherence, three measures will be calculated for each type of morpheme (prefix, suffix, stems, clitics):\nMeasure 1 (\"segmentation into a single token\"): This is the percentage of words in the coherence evaluation dataset, containing the morpheme type, that are nonetheless segmented as a single token (i.e., not segmented).\n- Measure 2 (\"the morpheme is not recognized\"): This is the percentage of words in the coherence evaluation dataset, containing the morpheme type and segmented into multiple tokens (subwords), where the morpheme does not correspond to any of these tokens. This measure detects errors such as \"over-segmentation\" or \"non-recognition of the morpheme.\"\nMeasure 3 (\"morpheme is recognized\"): This is the percentage of words in the coherence evaluation dataset, containing the morpheme, that are segmented into multiple tokens, where one of the tokens corresponds to the morpheme. This measure assesses the ability of the tokenizer to recognize that type of morpheme and thus the effectiveness of the model's vocabulary in recognizing that morpheme type.\niii. For morphological accuracy, the proportion of words is calculated in such a way that all segmentation tokens match the labeled morphemes in the accuracy evaluation dataset. The accuracy score ranges from 0 to 1 (or 0% to 100%), where 0 indicates that no words are correctly segmented and 1 (or 100%) indicates that all words are correctly segmented."}, {"title": "5. EVALUATION OF THE MORPHOLOGICAL QUALITY OF BETO'S VOCABULARY", "content": "To evaluate the quality of the vocabulary in the public version of BETO available on Hugging Face, we applied the model and evaluation method described in the previous section. As indicated in Section 3, the tokenizer type used by BETO, according to its configuration file, is the Wordpiece tokenizer, although the authors of BETO specify that it employs BP\u0415.\nThe three evaluation datasets used were those constructed in our previous work (Garc\u00eda-Sierra et al., 2024). Details of the construction procedure can be found in that publication. Additionally, the quality results of the current BETO's vocabulary are presented in comparison to the quality results of two vocabularies generated in our previous work using the Wordpiece and BPE segmentation algorithms. These vocabularies consist of 31,000 tokens and were trained on the Oscar corpus 10, comprising 600,000 sentences, available on Hugging Face. For clarity, we will refer to these tokenizers as Wordpiece_31 and BPE_31, respectively. These two vocabularies serve as reference baselines against which we compare the evaluation results of BETO's vocabulary. This comparison allows us to investigate, among other aspects, whether the use of larger datasets for training tokenizers improves the morphological quality of the vocabularies, or if any of the reference vocabularies exhibit similar quality to that of BETO."}, {"title": "5.1. Results and discussion", "content": "5.1.1. Morphological relevance\nTable 4 presents the results of BETO's vocabulary quality evaluation and the in comparison to the reference vocabularies with respect to the morphological relevance criterion. Similar to the reference vocabularies, BETO's vocabulary shows very low precision, indicating that the statistical performance of its segmentation algorithm is insufficient for adequately learning the morphological units of Spanish. Regarding the comparison of BETO's vocabulary with the Wordpiece_31 and BPE_31 reference vocabularies, we find that:\n(1) For prefixes and suffixes, the values of the three metrics are practically identical between BETO's and BPE_31 vocabularies, and on average, very similar across all three\n(2) there is a difference of more than ten percentage points in root morphemes between the coverage values of BETO's vocabulary (24%) and the reference vocabularies (10% and 9%). This finding suggests that training a model's tokenizer with a larger corpus improves the coverage of stems, but not that of prefixes and suffixes.\nWhen comparing BETO's segmentation algorithm with the Wordpiece and BPE algorithms, we observe that the results between BETO and BPE are identical. However, the results between Wordpiece and BPE are so similar that, based solely on this criterion, we cannot definitively determine which of the two algorithms is used by BETO's tokenizer."}, {"title": "5.1.2. Morphological coherence", "content": "Table 5 shows the results of the vocabulary use of BETO's tokenizer and the reference vocabularies for the evaluation of the morphological coherence criterion.\nAs observed, BETO's tokenizer exhibits relatively low coherence values: it consistently uses the same prefixes to segment words only 12% of the time, 17% for stems, 13% for suffixes, and this figure increases to nearly 54% for clitics.\nIn comparison with the reference vocabularies, except for clitics, the coherence results of BETO are, unlike what was observed for relevance, similar to those of Wordpiece_31 and significantly different from BPE. Additionally, BETO's vocabulary coherence results are slightly worse than those of Wordpiece_31, except in the case of stems, suggesting that training the tokenizer with a larger corpus does not necessarily lead to an improvement in morphological coherence."}, {"title": "5.1.3. Morphological accuracy", "content": "Table 6 presents the results of the evaluation of BETO's vocabulary with respect to the morphological accuracy criterion, alongside the results of the two reference vocabularies.\nThe accuracy values for the vocabulary generated by BETO's tokenizer are quite low at 14.5%, indicating that only 14.5% of the words are correctly segmented into all their morphemes.\nIn comparison with the reference vocabularies, the accuracy of BETO's vocabulary is identical to that of Wordpiece_31 (14.54%) and nearly twice that of BPE_31 (8.7%). Regarding the average number of tokens used in each segmentation, it is noted that BETO and Wordpiece_31 tokenizers use the same number of tokens, while BPE uses nearly twice as many. These results suggest a similarity between BETO and Wordpiece_31 and a notable difference from BPE. Furthermore, since no improvement in accuracy is observed, it can be concluded that increasing the size of the training corpus for the tokenizers does not enhance the quality of the vocabularies they generate."}, {"title": "5.2. ERROR ANALYSIS", "content": "Error analysis was done using the error typology established in our previous work (Garc\u00eda-Sierra et al., 2024), summarized as follows:\nType 1: Under-segmentation errors occur when a very frequent word that has more than one morpheme is treated as a single token by the tokenizer.\n- Type 2: Over-segmentation errors occur in infrequent words that contain frequent sub-words.\n- Type 3: Errors of absence of the correct morpheme in the vocabulary, where the tokenizer uses more frequent sub-words that are not actual morphemes.\nType 4: Errors in morpheme selection occur when the morpheme is in the vocabulary but is not selected by the tokenizer."}, {"title": "6. DISCUSSION ON BETO'S TOKENIZATION ALGORITHM", "content": "Evaluation results suggest that BETO employs a Wordpiece-type segmentation algorithm. The only discrepancy with this conclusion appears in the morphological relevance results for prefixes and suffixes, where BETO and BPE yield nearly identical values. To verify these findings, we re-examined the evaluation process and confirmed that it had been done properly.\nOne possible explanation for the observed similarity between BPE and BETO in the relevance criterion could be that the tokenizer was initially trained using a BPE algorithm, but was switched to Wordpiece during the inference phase. This would also account for the inconsistency between the authors' claim of using BPE and the tokenizer configuration indicating the use of Wordpiece. However, we have deemed this explanation unlikely. Changing the tokenizer between training and inference phases is complex and would require more than just altering the algorithm's name in the model configuration file. It would also need modifications in BETO's repository structure, excluding files such as 'merges.txt' and adjustments to the token indicators. Therefore, if such a change in the segmentation algorithm were intentional, it would need to be explicitly indicated and justified.\nHaving ruled out the possibility of dual usage of BPE and Wordpiece, we conducted an additional experiment comparing the similarity of the prefix sets in vocabularies generated by BPE and BETO, and between Wordpiece and BETO. The same metrics of precision, coverage, and F1-score used for evaluating the morphological relevance criterion were applied. The results, presented in Table 9, indicate that the similarity between BETO and Wordpiece in terms of prefixes is greater than the similarity between BETO and BPE, with a significant difference observed in the F1 score. This finding supports the hypothesis that the tokenizer used in BETO is consistently a Wordpiece algorithm."}, {"title": "7. SUMMARY, CONCLUSSIONS AND FUTURE WORK", "content": "In this work, we have addressed the problem of evaluating the quality of vocabularies in current large language models by examining the vocabulary of BETO's, an Spanish language model. These vocabularies are typically generated automatically by tokenizers such as BPE, Wordpiece, or Unigram, which are based on statistical strategies. The vocabulary is a key component for the effectiveness and reliability of a language model because it contains the linguistic units that the model is capable of recognizing and combining. Although statistical tokenizers are highly effective and do not require labeled corpora to build multilingual vocabularies for large language models, they generate subword vocabularies that do not always correspond to linguistic units such as morphemes or words. Measuring the quality of a vocabulary can help determine its impact on the performance of the language model. Furthermore, if the impact is significant, quality measures can facilitate improvements in the vocabularies.\nIn this work, we evaluated the vocabulary of BETO, a Spanish language model based on the BERT architecture. We employed an evaluation method developed in our previous work (Garc\u00eda-Sierra et al., 2024). The objectives were to verify whether the findings from our previous work hold true regarding: (i) the low quality of vocabularies generated by segmentation algorithms used in large language models (specifically BPE and Wordpiece), (ii) the independence between the size of the training corpus for these segmentation algorithms and the quality of their vocabularies, (iii) the types of errors present in the vocabularies, and (iv) whether it is possible to determine the segmentation algorithm used by BETO's tokenizer.\nFrom the results obtained in the evaluation of BETO's vocabulary and tokenizer, we can conclude that:\n1.  As with the tokenizers trained and evaluated in our previous work, the morphological quality of BETO's vocabulary is very low because it includes limited morphological knowledge of Spanish. Only in the case of prefix coverage BETO does achieve 90%, and for suffixes, it reaches 73%. For the remaining precision and coverage values for prefixes, suffixes, and stems, the results are low or very low.\n2.  Regarding the types of errors from statistical tokenizers\u2014BPE, Wordpiece, or Unigram\u2014we observe that BETO exhibits all these errors, with a predominance of Type 1 errors (under-segmentation) and Type 3 errors (absence of morphemes in the vocabulary), which are characteristic of a Wordpiece tokenizer.\n3.  Concerning the influence of corpus size on the quality of vocabularies generated by statistical segmentation algorithms, we found in BETO that using a corpus 500 times larger (300,000,000 sentences vs. 600,000 sentences) does not improve the quality of the generated vocabulary.\n4.  Finally, using the results from the evaluation of BETO's vocabulary quality, it can be inferred that the tokenizer used for its creation was Wordpiece, which matches the tokenizer indicated in the public version of the model and differs from the one claimed by BETO's authors.\nHaving a model and method for evaluating the quality of vocabularies in large language models opens the door to studying the question: to what extent does the morphological quality of vocabularies used by large language models influence their effectiveness and reliability? Our current and future research is focused on addressing this question."}]}