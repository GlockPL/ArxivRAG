{"title": "Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval", "authors": ["Yannick Assogba", "Donghao Ren"], "abstract": "As language models support larger and larger context sizes, evaluating their ability to make effective use of that context becomes increasingly important. We analyze the ability of several code generation models to handle long range dependencies using a suite of multi-step key retrieval tasks in context windows up to 8k tokens in length. The tasks progressively increase in difficulty and allow more nuanced evaluation of model capabilities than tests like the popular needle-in-the-haystack test. We find that performance degrades significantly (up to 2x) when a function references another function that is defined later in the prompt. We also observe that models that use sliding window attention mechanisms have difficulty handling references further than the size of a single window. We perform simple prompt mod-ifications using call graph information to improve multi-step retrieval performance up to 3x. Our analysis highlights different facets of long-context performance and is suggestive of prompt construction strategies for code completion tools.", "sections": [{"title": "Introduction", "content": "Long context inference is an increasingly important differentiator of LLMs, with model releases supporting larger and larger context windows [1-5]. This is enabled by advances in efficient attention implementation such as FlashAttention [6], Grouped-Query Attention [7] and Paged Attention [8], as well as scaling attention by introducing sparsity via windowing [9, 10].\nApplications, such as in-editor code completion from tools like GitHub's Copilot [11] and Source-Graph's Cody [12], benefit from long context support as they leverage retrieval-augmented generation strategies [13] to incorporate code snippets from across the user's project into a single prompt. This allows completions to be driven by the user's context rather than just by what the model learned at training time. Works such as Zhang et al. [14] and Shrivastava et al. [15] have proposed various approaches to improve cross-repository code completion.\nThese applications rely heavily on the model's ability to effectively use everything in its context window, and thus evaluation of long context usage has been a topic of interest in the community. Khandelwal et al. [16] showed that early LSTM-based language models only roughly modelled information from more distant tokens. Recently, the \u201cneedle-in-the haystack\u201d test, which measures recall for chat-style LLMs [17] has grown popular, and a parallel key-retrieval method has been used by Rozi\u00e8re et al. [18] to measure recall in auto-complete style code generation models. We believe that desirable long context inference also includes the ability to reason over multiple pieces of information in the input, and needle-in-haystack evaluation approaches do not capture a model's ability to do this. In the paper we contribute:\n1.  A set of multi-step key retrieval tasks that progressively increase in difficulty and allow evaluation of long-range dependency handling and basic reasoning through indirection.\n2.  Empirical evaluation of several open source code generation models that reveal architectural choices that affect ability to handle long-range dependencies. We also discover that the order of function declarations has a large effect on model ability to complete these tasks.\n3.  Preliminary recommendations for improving multi-step recall that rely only on prompt modification using information obtainable via existing non-LLM techniques. These include rewriting code to bring dependencies closer together and adding annotations of function dependencies in the form of comments.\nWe focus on examining how well long-range dependencies are handled in auto-complete style code generation models. While instruction-tuned/chat-style models are an increasingly popular mode of LLM interaction, we believe that auto-complete models are still relevant for applications that need frequent, low latency generation of code."}, {"title": "Related Work", "content": "Symbolic reasoning: Zhang et al. [19] study neural networks' ability to handle indirection by introducing the Pointer Value Retrieval (PVR) task. They train models to take in a sequence of tokens (typically digits), where the first token acts as pointer to the value to be retrieved. The synthetic nature of this task provides full control over the complexity of the problem. Abnar et al. [20] extend this work to add recursive steps to the PVR task, evaluate it in the context of vision transformers, and present a new architecture that supports adaptive compute.\nMulti-hop QA: Min et al. [21] study a popular multi-hop reasoning benchmark (HotpotQA) and find that many of the questions can be answered with single hop reasoning. They find many questions embed contextual info that make the task easier, and that weak distractors may make picking the right answer 'obvious' without need for the desired reasoning steps. Chen and Durrett [22] further explore dataset design choices to induce multi-hop reasoning.\nLong context retrieval: Liu et al. [23] Examine instruction-tuned LLM performance for synthetic key-retrieval tasks as well as multi-document QA and describe a \u201clost-in-the-middle\" phenomenon where information becomes harder to retrieve if it is not near the beginning or end of the prompt.\nLong context dependencies: Yu et al. [24] present a code completion benchmark named CoderEval, where problem solutions have varying levels of dependencies on code in the surrounding context. They find that model performance for functions with dependencies is significantly worse than performance for standalone functions. In the context of natural-language Kuratov et al. [25], Yuan et al. [26] and Levy et al. [27] develop benchmarks to evaluate how well language models extract and reason over distributed facts in the presence of noise text. Hsieh et al. [28] develop a rich set of tasks to evaluate long context performance with different dependency structures in natural language.\nChain-of-thought prompting: Wei et al. [29] and Kojima et al. [30] explore prompting strategies to induce multi-step reasoning in instruction tuned models. They find that models perform better on various tasks when prompted to output intermediate steps in the reasoning chain.\nWhile most of the existing literature focuses on chat-style natural language generation, our work focuses on non-instruction-tuned models in the context of autocomplete-style code generation, which are not able to make use of 'scratchpad' methods such as chain-of-thought to improve their reasoning. We situate our work in between the PVR work of Zhang et al. [19], which is highly controllable but uses a fully synthetic task, and the code-with-dependencies benchmarking work exemplified by Yu et al. [24], which is more realistic but also makes it more difficult to run controlled experiments that allow discovery of specific failure modes."}, {"title": "Long Context Multi-Step Key Retrieval", "content": "To study long-range dependency handling we propose four multi-step key retrieval tasks of increasing complexity (one-step, two-step, three-step, and concatenation retrieval). These extend the key-retrieval task in Rozi\u00e8re et al. [18] and test models' ability to integrate multiple pieces of information spread throughout a long context window to make a completion."}, {"title": "Task Design", "content": "One-step retrieval is equivalent to the key-retrieval task in [18]. We differ from their design by using random strings to construct function names rather than fixed function names, and return values that are string literals rather than integer literals. Using a simple template system, we first generate a key function that returns a random string. The model is then asked to complete an assert statement on the return value of this function. All return strings that are 10 characters long and all function names are between 13 and 20 characters long.\nIn two-step retrieval, the key function calls a value function that returns the string. Three-step retrieval adds an additional function call between the key function and the value function that returns the string. In the concatenation retrieval task the key function calls two value functions and returns the concatenation of their returned strings Figure 1.\nTo turn each of these into a long context problem we insert varying amounts irrelevant code into the context window, we detail this in Section 3.3. In all cases the assert statement is on the result of the key function and is placed at the end of the prompt."}, {"title": "Avoiding Reliance on Parametric Knowledge and Trivial Solutions", "content": "To make sure the model is not solely relying on parametric knowledge (i.e., knowledge stored in the weights) to solve the task, we construct function names from two or three random sequences of lowercase characters or digits, joined with underscores (e.g., zcxjdz_309521_xcdgfp). Return values are random strings of lowercase characters (e.g., pczjdfeyxc). This makes it extremely unlikely that such functions exist in the model's training data, and therefore to complete the task successfully, the model must use the information from the prompt.\nIn our early experiments we observed that models may produce trivial solutions such as assert foo() == foo(). While these are technically correct, such solutions prevent us from testing the models' ability to retrieve information from the context. To prevent this, we guide the decoding of tokens to make sure that the response starts as a string literal. As we sample output tokens, we use the prefix_allowed_tokens functionality in the HuggingFace transformers library [31] to ensure that the output starts with any number of spaces followed by a single or a double quote. Once an initial quote mark is produced, generation proceeds unrestricted."}, {"title": "Long Context Construction", "content": "To turn the tasks described above into long context inference problems we add irrelevant snippets to the context window from two sources. Motivated by findings in Min et al. [21] and Chen and Durrett [22] that show that the presence of good distractor functions are important when measuring multi-step reasoning, we first generate a number of synthetic distractor functions that use the same template as the the key and value functions that contain the task information. Then we sample standalone Python functions from the HumanEval dataset [32] to fill out the context window to our desired size. Algorithm 1 describes this dataset generation process in more detail."}, {"title": "Models", "content": "We evaluate these tasks on five open source models: StarCoderBase-1B, StarCoderBase-7B, StarCoderBase-15.5B [33], StarCoder2-7B [34], and Mistral-7B-v0.1 [4]. The StarCoder family of models are competitive on code generation benchmarks and available in a number of different param-eter counts. Mistral is a general purpose LLM that is nonetheless competitive on code generation benchmarks. We use implementations from the HuggingFace transformers library [31]."}, {"title": "Results", "content": "Our primary metric is accuracy@k, which is similar to the pass@k metric introduced in Chen et al. [32], the only difference is instead of running unit tests, we simply check if the string literal produced is the expected string. We compute accuracy@3 over 10 generations for each input prompt. Hyperparameters for generation are in Appendix C"}, {"title": "Overall Task Performance", "content": "Task difficulty: Figure 2a shows that tasks increase in difficulty in the following order: one-step, two-step, three-step, and concatenation. We note particularly weak performance on the concatenation task. Another factor affecting difficulty is the number of distractor functions. For the more difficult tasks we see a with a smooth degradation in performance as distractors are added (Figure 3). We focus the rest of our analysis on the five distractor condition as it allows us to see a good variation in task performance.\nContext size: In Figure 2b we observe a small performance drop as context sizes increases for StarCoderBase-1B, StarCoderBase-7B, and StarCoderBase-15. For StarCoder2-7B and Mistral-8B, which both use sliding window attention, a large drop in performance occurs when moving from 4k to 8k context length, we hypothesise that this is related to the size of the sliding window used and will explore this in more detail in Section 3.5.2.\nIncorrect responses: We did some manual inspection of the incorrect generations from models and note a number of distinct failure modes. Firstly a noticeable amount of incorrect generations are return values of distractor functions. Secondly we see completely incorrect answers, such as \"3333\", or \"text\". The third failure mode we notice is partially correct results, particularly for the concatenation task where models often fail to produce the second half of the expected string."}, {"title": "Effect of Task Snippet Position", "content": "In the one-step retrieval task we see good performance with respect to key function position across the entire context window with two notable exceptions. At the 8k context size Mistral-7B-v0.1 and StarCoder2-7B are unable to perform the one-step task when the key function is more than ~4k tokens away from the generation site Figure 4. Both models use a sliding window attention mechanism with a 4k window size. While this mechanism theoretically allows the model to scale very large context sizes (Jiang et al. [4] reports a theoretical attention span of 131k tokens), our results indicate that the model fails to retrieve precise information at a distance greater than the sliding window."}, {"title": "Effect of Task Snippet Position", "content": "The two-step task has two relevant task snippet positions, Figure 5 shows heatmaps of task perfor-mance vs. both key function position and value function position. We see that performance is worse when the key function appears before the value function in the prompt. We call this a forward reference and it turns out this has a large effect on performance. We also observe that performance is generally higher along the diagonal, i.e., when the functions are closer to each other."}, {"title": "Effect of Forward References", "content": "We define a forward reference as a function calling a not-yet-defined function. We find that forward references have a strong negative impact on task performance. Figure 6 shows the performance of StarCoderBase-7B with respect to the number of forward references. Accuracy@3 score drops from 87% to 34% for two-step task at 4k context size when the order of the two task snippets is reversed. We find similar behavior with respect to forward references across all the models in our experiment (Figure 9 in Appendix A)."}, {"title": "Effect of Task Snippet Spread", "content": "We saw in Figure 5 that the order of functions in multi-step tasks and the distance between functions affects performance. We define spread as the distance between the first token of the first task-related snippet and the last token of the last task-related snippet, not including the assert statement at the end of the prompt.\nFigure 7 shows the effect of task snippet spread with StarCoderBase-7B with 4k context length. For the two-step task, as spread increases, the performance gap between zero and one forward reference grows, we find that this result is consistent across models Figure 10. For the three-step task a similar effect is clearly visible at 2k and 4k context size. However this effect is not consistent across different models Figure 11.\nIn the concatenation task, the effect is less pronounced as model's performance is low regardless of spread and the number of forward references and there is not much room for separation Figure 12."}, {"title": "Improving Retrieval Performance with Call Graph Comments", "content": "We saw in Section 3.5.3 that performance degrades for the multi-step tasks in the presence of forward references. We considered if injecting information about function call relationships into the prompt would help performance. This information is easy to obtain from static analysis of code (or in our case at prompt construction time), and if beneficial, provides a lightweight approach to improving performance at inference time. We focus on our most difficult tasks, namely the three-step and concatenation retrieval task with 5 distractors using the same models as in our previous experiment."}, {"title": "Experiment Setup", "content": "We add comments above each function that contain a lightweight description of the call graph associated with that function. We can construct these comments by annotating which functions are \"called by\" other functions, or annotating which functions \"call\u201d a given function, or a combination of both. In the three-step retrieval task we can do this transitively and add annotation of all the functions that will be called to compute the result of a given function.\nWe considered two templates for these comments, in one variation (\"names only\") we simply included a comma-separated list of function names. In the other (\u201cfull sentence\u201d) we use the phrases: \u201cThis function is called by \u201d and \u201cThis function calls\" followed by the comma-separated list of function names. See Appendix G for examples."}, {"title": "Results", "content": "Full sentence vs. function names only: The full-sentence template performs better than the names-only version (Table 2a). This suggests that the models are able to take some advantage of the more complete natural language description and that there may be room for further improvement by tuning the template. We focus the rest of our analysis on the full-sentence condition.\nCall graph directions: Table 2b shows that while most of the benefit comes from the \u201cX is called by Y\" type of comment, using both directions produces the best improvement overall.\nFull graph vs. next-hop comments: We noted earlier that in the three-step case we add comments describing the full call graph (i.e., up to two steps away). We experimented with only adding comments about the next function in the call graph and found that the full graph version performs best (Table 3).\nContext size, spread, and forward references: Figure 8 shows that call-graph comments improve performance across all context lengths, number of forward references, and task-snippet spreads for StarCoderBase-7B (this trend holds for other models where the call graph comments make a difference). Full plots for all the models are found in Appendix B. We note that with respect to spread for the concatenation task, most of the improvement appears when the task-relevant snippets are closer together."}, {"title": "Discussion", "content": "We found that models were able to perform well at the simpler one-step recall task over their entire context lengths (Figure 2a). However our experiments highlight the importance of distinguishing between long-context support and the ability to handle long-range dependencies. This is illustrated by models like Mistral and StarCoder2 that appear to make a trade-off between the two. While sliding window attention greatly expands their supported context sizes, our experiments show that when distance between relevant snippets is greater than the sliding window size used by these models, performance degrades quickly. Our tasks require precise recall over a long distance and it appears to be difficult to pass on information from earlier sliding windows in a way that allows successful completion of the task.\nAs different approaches to scaling attention to larger context windows proliferate, practitioners should take care to understand and mitigate these effects if their task relies on precise recall of long-range dependencies. In the context of code generation, builders of retrieval augmented generation systems may find it desirable to rewrite code to bring code snippets that depend on each other closer together.\nOur multi-step tasks test the ability of models to reason through indirection introduced by simple yet common operations like function calling and concatenation. We find that models perform significantly worse on these tasks than the simpler one-step recall task (Figure 2a). This is particularly true in the presence of distractors, which we believe are important for appropriately pressure testing the ability of models to do the required reasoning (Figure 3).\nWe note that models we tested struggled even more in the presence of forward references (Figure 6). In this situation all the information in the prompt is the same, but is presented in a different order. Humans and compilers are able to resolve this problem and it would be ideal if models could do the same. While we do not know what causes this behaviour, one hypothesis is that the causal attention used during training favors attending backwards in the context window and propagating information from earlier to later tokens.\nWe found that adding simple natural-language text annotations that describe the call graph greatly improves performance on the more difficult multi-step tasks in all cases, including where forward references are present. Most of the benefit seems to come from just having the function name of the forward-reference appear first, though there also appears to be room to explore tuning the comment text to improve performance (Table 2).\nThese findings suggest a couple of avenues for practitioners to explore in constructing prompts for code generation. Firstly it may be possible to rewrite code to reduce forward references while maintaining the semantics of the code, and secondly that extra information that can be gleaned from existing methods such as static analysis can be injected as comments to help models reason over the code."}, {"title": "Limitations", "content": "Our experiments uses simple synthetically generated functions that are not as realistic as real code in the wild. Recommendations such as re-ordering functions to bring dependant code closer together or adding annotations like call-graph comments would need to be tested in real retrieval-augmented code generation environments. We do not do in-depth investigation of potential internal mechanisms that lead to the behaviours we observed, we describe some hypotheses but otherwise leave that to future work."}, {"title": "Compute Information", "content": "We ran all experiments on machines with a single A100 GPU with 80GB of VRAM on a cloud provider. Experiments were run over the course of about a month and we estimate ~1000 GPU hours were used including runs that had bugs in them and additional experiments that are not part of the paper. We use FlashAttention [6] to improve memory usage and latency of generation."}]}