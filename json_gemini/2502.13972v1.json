{"title": "IncepFormerNet: A multi-scale multi-head attention network for\nSSVEP classification", "authors": ["Yan Huang", "Yongru Chen", "Lei Cao", "Yongnian Cao", "Xuechun Yang", "Yilin Dong", "M,Tianyu Liu"], "abstract": "In recent years, deep learning (DL) models have shown outstanding performance in EEG\nclassification tasks, particularly in Steady-State Visually Evoked Potential(SSVEP)-based Brain-Computer-\nInterfaces (BCI)systems. DL methods have been successfully applied to SSVEP-BCI. This study proposes a new\nmodel called IncepFormerNet, which is a hybrid of the Inception and Transformer architectures. IncepFormerNet\nadeptly extracts multi-scale temporal information from time series data using parallel convolution kernels of\nvarying sizes, accurately capturing the subtle variations and critical features within SSVEP signals. Furthermore,\nthe model integrates the multi-head attention mechanism from the Transformer architecture, which not only\nprovides insights into global dependencies but also significantly enhances the understanding and representation\nof complex patterns. Additionally, it takes advantage of filter bank techniques to extract features based on the\nspectral characteristics of SSVEP data. To validate the effectiveness of the proposed model, we conducted\nexperiments on two public datasets, The experimental results show that IncepFormerNet achieves an accuracy\nof 87.41% on Dataset 1 and 71.97% on Dataset 2 using a 1.0-second time window. To further verify the superiority\nof the proposed model, we compared it with other deep learning models, and the results indicate that our\nmethod achieves significantly higher accuracy than the others. The source codes in this work are available at:\nhttps://github.com/CECNL/SSVEP-DAN.", "sections": [{"title": "1. Introduction", "content": "Brain-Computer Interface (BCI) is a communication\nor control system that allows real-time interaction\nbetween the human brain and external devices [1]. BCI\nsystems work by measuring brain signals that carry\nthe user's intent and convert them into corresponding\ncontrol signals for devices, enabling computer-based\ncommunication or the direct control of external devices\n[2] [3] [4]. EEG-based BCIs reflect brain intent through\nbrainwave signals, and because of their convenience,\nlow cost [5]and non-invasive nature, they have garnered\nwidespread attention. After nearly 50 years of research,\nBCI technology has successfully transitioned from\nscientific demonstrations to experimental applications.\nIt has been used in various fields, including healthcare,\nentertainment, and the military. Among the various\nEEG paradigms, the high signal-to-noise ratio and low\ntraining time of steady-state visual evoked potentials\n(SSVEP) [6] [7] [8] make it one of the most popular.\nSteady-state visual evoked potentials (SSVEP) are\nperiodic rhythms generated by the modulation of cor-\ntical activity in response to visual stimuli presented at\nfixed frequencies [9]. SSVEP primarily occurs in the\noccipital region of the cerebral cortex. In the design\nof SSVEP-based brain-computer interface (BCI) sys-\ntems, different targets flicker at distinct frequencies,\nthereby eliciting specific SSVEP signals for each tar-\nget. By analyzing the SSVEP signals from the visual\ncortex, the target on which the subject is focusing can\nbe identified, enabling the brain to control external de-\nvices through SSVEP BCI. Therefore, accurately de-\ncoding SSVEP signals is crucial, because it allows for\nthe identification of the specific frequency correspond-\ning to the user's focus. Traditional decoding methods,\nsuch as Canonical Correlation Analysis (CCA) [10], are\namong the mainstream approaches in this field. To re-\nduce the misclassification rate of EEG signals, Zhang\nand colleagues made further optimizations by propos-\ning Multi-way Canonical Correlation Analysis (Mway-\nCCA) [11] and L1-regularized MwayCCA (L1-MCCA)\n[12]. Recently, Zhou introduced Multi-set Canonical"}, {"title": "2. Methods", "content": "Correlation Analysis (MsetCCA) [13], which optimizes\nreference signals from the common features of multiple\ncalibration trials without any artificial signals (i.e., sine\nor cosine signals), demonstrating better performance\nthan MwayCCA and L1-MCCA [14]. Given the char-\nacteristics of SSVEP signals, which exhibit clear har-\nmonic components, researchers have developed Filter\nBank Canonical Correlation Analysis (FBCCA) [15],\nwhich explicitly combines the fundamental frequencies\nand harmonics of the stimulus frequencies to enhance\nclassification performance. Nowadays, filter bank tech-\nniques are widely used, based on these methods, Ex-\ntended Canonical Correlation Analysis (ECCA) and\nSingle-template-based Canonical Correlation Analysis\n(ITCCA) [16] have also been developed for SSVEP\nclassification. Wang and colleagues proposed a novel\nSSVEP detection method based on TRCA spatial fil-\ntering [17], that significantly outperformed extended\nCCA in terms of classification accuracy and infor-\nmation transfer rate (ITR). Later, Liu and others\nintroduced Task Discriminative Component Analysis\n(TDCA) [18], which does not require ensemble tech-\nniques, to further improve the performance of SSVEP-\nBCI.\nIn recent years, deep learning (DL) has gained\nincreasing attention [19], and researchers have exten-\nsively developed and studyed the application of deep\nlearning algorithms for BCI decoding [20]. Deep learn-\ning models can be directly applied to raw data [21], au-\ntomatically extracting features from raw EEG signals\nfor rapid decoding. Nowadays, deep learning models\nare widely used for EEG decoding and classification\nacross various tasks. Researchers in SSVEP-BCI sys-\ntems have begun exploring deep learning techniques\nto develop algorithms for recognizing SSVEP frequen-\ncies. Based on SSVEP data, models can generally\nbe categorized into two types: those that use time-\ndomain data as input and those that use frequency-\ndomain data. Literature has introduced the EEGNet\nmodel [22], a convolutional neural network specifically\ndesigned for processing EEG data, which accepts time-\ndomain data as input to achieve rapid decoding of\nSSVEP signals. Similarly, a time-domain CNN model\n[23],, tCNN has been proposed, which employs filter\nbank techniques to enhance performance in short time\nwindows (FB-tCNN), further improving decoding ac-\ncuracy. Additionally, a hybrid network model has been\nproposed that combines convolutional neural networks\n(CNN) with long short-term memory (LSTM) [24] net-\nworks was developed to enhance the decoding capabil-\nity and generalization ability. Moreover, CCNN [25]\nutilizes frequency-domain data from SSVEP signals\nas input, as frequency-domain data is rich in am-\nplitude and phase information. The introduction of\nCCNN indicates that spectral data is also beneficial"}, {"title": "2.1. Dataset description", "content": "for SSVEP classification. Chen and colleagues pro-\nposed an SSVEP decoding method based on an atten-\ntion mechanism, the SSVEPformer model [26], which\nuses frequency-domain signals as input and incorpo-\nrates filter bank techniques. An improved version, FB-\nSSVEPformer, further enhances network performance.\nConsidering the topological relationships among EEG\nchannels, a dynamic graph convolutional neural net-\nwork (DDGCNN) [27] has been proposed, innovatively\nutilizing graph models for SSVEP signal recognition.\nOwing to the limitation imposed by the limited\nquantity of EEG data on the classification performance\nof many deep learning-based methods, improving\nclassification performance within the available data\nposes a significant challenge. Therefore, this paper\nproposes a hybrid model based on Inception and\nTransformer [28] [29] [30]. Inception is a deep\nconvolutional neural network, a network structure\nbased on multi-scale convolutions [31], aimed at\naddressing the issue of traditional CNNs when\nhandling inputs of different sizes. Using convolution\nkernels of various sizes, features are captured at\ndifferent scales. The traditional Inception structure\n[32], stacks multiple convolution kernels of different\nsizes and pooling layers in parallel, allowing the\nnetwork to extract features at multiple scales. To\navoid computational explosion, the Inception module\ntypically applies a 1\u00d71 convolution kernel to reduce\nthe number of channels before using larger convolution\nkernels, thereby reducing the computational load.\nSSVEP signals exhibit periodic fluctuations at different\nfrequencies and manifest as a mixture of multi-\nfrequency oscillations in the time domain. Therefore,\ncapturing key and significant features for classification\ntasks is particularly important. Inspired by the\nInception structure, we use convolution kernels of\ndifferent sizes to extract multi-scale features from\nthe time-domain data, allowing the model to capture\ncritical features of SSVEP signals at various scales,\nthereby improving classification accuracy. Transformer\nis one of the most promising model architectures.\nIt was first used in machine translation and quickly\nbecame dominant in natural language processing owing\nto its outstanding performance [33]. Subsequently,\nit was applied in the field of computer vision,\nachieving remarkable results [34] (Dosovitskiy et al.,\n2020). Transformer-based models are highly versatile.\nConsidering the unique nature of SSVEP data, we\nutilize the Inception concept in combination with\nTransformer characteristics to decode SSVEP signals.\nThe deep neural network model proposed in\nthis study, IncepFormerNet, is designed based on\nthe characteristics of SSVEP signals, incorporating\nboth temporal and spatial features of the data. In\naddition, it utilizes a filter bank technique to fully"}, {"title": "2.1.1. Dataset 1", "content": "In this section, two widely used public datasets,\nBenchmark and BETA, are introduced in detail. The\npublic datasets are used to evaluate the performance\nof the proposed model. By repeating this process, the\nlocal minima of the loss function can be determined\nthereby optimising the neural network.\nBenchmark: This dataset was from a 40-target\nSSVEP-BCI speller, where all stimulation frequencies\nare encoded using the Joint Frequency and Phase\nModulation (JFPM) method. The range of stimulation\nfrequencies was 8 to 15.8 Hz, with an interval of 0.2\nHz. The phase range was 0 to 1.5, with a step size\nof 0.5\u03c0. Data was collected using a 64-channel EEG\nrecorder, and to reduce storage and computation, the\ndata was downsampled to 250 Hz. The experiment\ninvolved 35 subjects, each performing 6 blocks, with\neach block containing 40 trials. Each trial lasted 6\nseconds, including 0.5 seconds of visual cue, 5 seconds\nof stimulus flickering, and 0.5 seconds of screen blank\ntime. The average visual latency across all subjects\nwas 0.14 seconds. For more information, please refer\nto the Benchmark dataset literature [35]."}, {"title": "2.1.2. Dataset 2", "content": "BETA: This dataset shares the same frequencies and\nphases as the Benchmark dataset but differs in the\narrangement of stimuli. Thus, The BETA stimulation\nparadigm is more suitable for practical applications.\nThe dataset included 70 subjects, with each subject\nperforming 4 blocks, and each block consists of 40\ntrials. For the first 15 subjects, each trial included\n0.5 seconds of visual cue, 2 seconds of stimulus\nflickering, and 0.5 seconds of screen blank time. For\nthe remaining 55 subjects, each trial consisted of 0.5\nseconds of visual cue, 3 seconds of stimulus flickering,\nand 0.5 seconds of screen blank time. The average\nvisual latency across all subjects was 0.13 seconds. For"}, {"title": "2.1.3. Data preprocessing", "content": "more information, please refer to the BETA dataset\nliterature [36].\nThis study focuses on analyzing steady-state visual\nevoked potential (SSVEP) data from nine channels\nin the occipital region selected from a total of 64\nchannels, including Pz, PO5, PO3, POz, PO4, PO6,\nO1, Oz, and O2 [35] [36]. To further select effective\ndata segments, considering the visual delay of SSVEP,\ndata segments from the stimulus onset, denoted as\n[Td, Td + Tw], were used. Here, Td represents\nthe visual delay, which is 0.14 seconds for Dataset\n1 and 0.13 seconds for Dataset 2, and Tw is the\ntime window (Tw \u2208 0.4 s, 0.5 s, 1.2 s). Based\non the characteristics of the amplitude information\nin SSVEP data, this paper employs three different\nranges of filters to effectively extract the harmonic\ninformation from the data. Following the M3 method\nfrom the FBCCAchen2015filter, three sub-band filters\nwere designed with frequency ranges of 6-50 Hz, 14-50\nHz, and 22-50 Hz to form a filter bank. Additionally,\nto mitigate overfitting during model training, 50\nrandomly selected continuous data points from the\neffective data were set to zero. For specific details,\nplease refer to the cited literature [37]."}, {"title": "2.2. IncepFormerNet", "content": "For the IncepFormerNet model proposed in this study,\nas shown in Figure.1, it illustrates the architecture,\nwhich is mainly divided into four modules: the channel\nfusion module, the temporal feature extraction module,\nthe former module, and the classification module. The\nstructure of the temporal feature extraction module is\nbased on Inception model. It was modified according to\nthe unique characteristics of SSVEP data to effectively\nextract features. The Transformer module is then\nemployed to learn the temporal dependencies of the\nSSVEP data from a global perspective. Finally, a\nsoftmax layer is used for classification."}, {"title": "2.2.1. Channel Fusion Module", "content": "The SSVEP data are derived from nine different\nchannels, each containing EEG signals from different\nbrain regions. Spatial correlations may also exist\nbetween these channels. To fully utilize this\ninformation, convolution operations are used to apply\nweighted combinations to multiple channels [10]. As\nshown in Figure.2, the convolutional layer uses a\nconvolution kernel with a length of C\u00d71 to assign\ndifferent weights to each channel, generating a fused\nspatial feature. The formula for the channel fusion\nmodule is as follows, where information from different\nchannels is fused while keeping the temporal dimension\nunchanged."}, {"title": null, "content": "Y_k(t) = \\sum_{i=1}^{G} W_k(i) \\cdot X(i,t,b) \\quad k = 1,2,...,9   (1)"}, {"title": null, "content": "Where Wk (i) represents the weight of the convolution\nkernel Wk on channel i, X represents the input data,\nand Yk (t) represents the value of the output feature at\ntime point t and output channel k."}, {"title": "2.2.2. Time Feature Extraction Module", "content": "The temporal feature extraction module receives the\ndata features after channel fusion. We apply four\n1D convolutions and two pooling layers to the fused\ndata for parallel computation. A larger convolution\nkernel is then used to further extract temporal infor-\nmation. Specifically,multi-scale convolutions are em-\nployed, combined with the concepts of pooling and\nresidual connections, to extract signal features at dif-\nferent scales and layers, thereby enhancing prominent\nfeatures. In this architecture, small convolution ker-\nnels focus on capturing local and prominent features\nof the SSVEP signals, while large convolution kernels\ncapture longer-range signal features, handling features\nwith temporal dependencies. After each convolution op-\neration, Dropout and L2 regularization are applied to\nprevent overfitting and improve the model's general-\nization ability. Pooling layers are used to reduce the di-\nmensionality of the feature maps, suppress noise inter-\nference, retain key prominent features, and enhance the\nmodel's robustness to interference.\nThe specific operations following the channel\nfusion of the data are shown in Figure.3. Inspired"}, {"title": "2.2.3. Former Module", "content": "by [32] the data are input into CNN blocks 1, 2, 3,\nand 4, resulting in the parallel convolution outputs\nX1,X2,X3 and 14. The output x2 is then passed\nthrough CNN block 5 to obtain the convolution result\nX2_2 Subsequently, x2_2 and 23 are concatenated and\npassed through CNN block 6 to obtain the result X3_3.\nSimilarly, the result is concatenated with 24 and passed\nthrough CNN block 7 to produce X4_4. Two pooling\nlayers are concatenated to form 15_5. Finally, X2, X2_2,\nX3_3, X4_4, and 25_5 are concatenated to obtain the\ntemporal features of the signal.\nThe Transformer architecture consists of an encoder\nand a decoder, where the decoder is made up of multi-\nple transformers with the same structure as described\nin \"A Survey on Vision Transformer\" [38]. As shown\nin Figure.4, the proposed structure in this study only\nuses the encoder, with its primary task being feature\nextraction, ultimately leading to classification. The\nencoder layer receives the input data and processes\nthe input sequence through multiple parallel attention\nheads, capturing different relationships within the se-\nquence. The output of the multi-head attention is then\nconnected to the input through a residual connection,\nfollowed by layer normalization. The main task of the\nfeedforward neural network is to process the normal-\nized data through two fully connected layers to extract\nfeatures, followed by another residual connection and\nnormalization. The final output is the features pro-\ncessed by the encoder. In this study, multiple encoder\nlayers are stacked, with the output of each layer serv-\ning as the input to the next layer, allowing for the\ncapture of complex temporal and global information\nby stacking multiple encoder layers. In the attention\nmechanism, the core idea is to weight the relevance of\nthe input data, allowing the model to pay attention to\nother data while processing a specific piece of informa-\ntion. The workflow is illustrated in the figure, where\nthe input vector is first transformed into three different\nvectors: the query vector q, the key vector k, and the\nvalue vector v. These vectors from different inputs are\nthen packaged into three distinct matrices, namely Q,\nK, and V [38] which are used to calculate the attention"}, {"title": null, "content": "Attention(Q, K, V) = softmax(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}) \\cdot V (2)"}, {"title": null, "content": "The structure of multi-head attention mechanism is\nillustrated in Figure.4. It employs a set of linear\ntransformation layers, in which three transformation\ntensors apply linear transformations to Q, K, and\nV. This design allows each attention mechanism to\noptimize different features of the data, enabling the\nmodel to learn multiple relationships of the data across\ndifferent dimensions. In this paper, two encoder layers\nand four multi-head attention mechanisms are used."}, {"title": "2.2.4. Classifier Module", "content": "The classification module first uses a flattened layer\nto transform the multi-dimensional features into a\none-dimensional vector, which is then inputted into a\nfully connected layer. This fully connected layer maps\nthe features to the corresponding 40 output stimulus\nfrequencies. Finally, a softmax activation function\nis applied to convert the output into a probability\ndistribution of the classes, completing the classification\nof the SSVEP signals."}, {"title": "2.3. Performance Evaluation", "content": "Classification accuracy and information transfer rate\nare two important metrics for evaluating the perfor-\nmance of SSVEP classification methods and compre-\nhensively assessing the system's performance from the\nperspectives of accuracy and time efficiency. These\nmetrics play a crucial role in SSVEP research. Clas-\nsification accuracy is defined as the precision of de-\ntecting and classifying different stimulus frequencies,\nrepresented by the ratio of true positive samples to the\ntotal number of samples. A high classification accu-\nracy indicates that the model can effectively capture\nand extract features from EEG signals, resulting in\nstrong recognition capabilities and ensuring the robust-\nness and usability. Information transfer rate (ITR) [37]\nis also crucial (\"Improving the performance of indi-\nvidually calibrated SSVEP-BCI by Task-discriminant\ncomponent analysis\"). ITR combines the classification\naccuracy with the corresponding time required by the\nmodel. It is defined as:"}, {"title": null, "content": "ITR = \\frac{60}{T} [log_2 N + P log_2 P + (1 - P) log_2 \\frac{1-P}{N-1}] (3)"}, {"title": null, "content": "Where N represents the number of target classes, Pis\nthe classification accuracy, and T is the selected sample\ndata. By optimizing ITR, the accuracy of the system\ncan be enhanced, leading to improved information\ntransfer efficiency, thereby enhancing the practicality\nof SSVEP-BCI. The the Benchmark dataset contained\n35 subjects, each containing six blocks. A leave-\none-out cross-validation method was used for the"}, {"title": "2.4. The baseline methods", "content": "experiments. First, one block was randomly excluded\nto serve as the test set, whereas the remaining five\nblocks were used as the training set. This process was\nrepeated six times to ensure that each block served as\nthe test data with a unique training model. The model\nwas trained using 5000 mini-batches for iterations. The\ntest results for the six blocks were averaged to obtain\nthe mean accuracy for each subject. Finally, the same\nprocedure was applied to all 35 subjects to ensuring\nthat each subject had a unique model, leading to the\nfinal test results for everyone. Similarly, in the BETA\ndataset, there were a total of 70 subjects, and the\nleave-one-out cross-validation method was employed to\ntrain models for each subject, ultimately yielding the\naverage accuracy for each individual. In the BETA\ndataset, the number of mini-batches was set to 3000.\nDuring the training process, dynamic learning\nrate adjustment was implemented by defining a\nLearningRateScheduler to prevent the learning rate\nfrom being too high and causing the model to fail\nto converge. An early stopping strategy was also\nincorporated to prevent overfitting during the training."}, {"title": "2.4.1. FBCCA", "content": "FBCCA is based on the filter bank extension of\nCCA (Chen, Wang, Gao, et al., 2015). It decomposes\nEEG signals into sub-bands through a filter bank and\nperforms CCA analysis. The final result is obtained by"}, {"title": "2.4.2. tCNN/FB-tCNN", "content": "weighting the correlation coefficients from all the sub-\nbands.9 TRCA: TRCA is a spatial filtering method\nthat extracts task-related components by maximizing\nthe reproducibility of data in each task (Nakanishi\net al., 2017). The average of existing data for the\nsame task is then used as the reference signal for that\ntask. Using the spatial filters obtained from TRCA\nbased on calibration data, the correlation coefficients\nbetween the projected features of the test samples and\nvarious reference signals can be calculated to obtain\nthe classification result. (In this study, for fairness,\nthe calibration data selected is consistent with the test\nsamples and the method presented in this paper) [15].\nThis is a time-domain-based CNN method. To avoid\nfeature ambiguity in the frequency-domain paradigm\nwithin short time windows, the tCNN model was\nproposed. Considering that harmonic information\nembeds a large amount of effective information for\nfrequency recognition, the tCNN was extended to\nFB-tCNN, which improves the network's classification\nperformance within short time windows. [23]."}, {"title": "2.4.3. EEGnet", "content": "EEGNet is a convolutional neural network specifi-\ncally designed for EEG signal data. Waytowich et al.\napplied EEGNet to SSVEP classification and achieved\ngood results in cross-subject classification tasks (Way-\ntowich et al., 2018) [22]."}, {"title": "3. Result", "content": "window length is adjusted to 0.8 seconds, the average\naccuracy increases to 62.91%. With a time window\nlength of 1.0 seconds, the average accuracy reaches\n67.73%, and at a length of 1.2 seconds, the average\naccuracy is 71.97%. Given that the BETA dataset\nhas a shorter stimulation time and lower signal-to-noise\nratio (SNR), achieving an average accuracy of 71.97%\nis a relatively noteworthy result, which also validates\nthe model's high classification performance.\nTable 3 presents the classification performance on\nthe benchmark dataset for different models, compar-\ning the average accuracies of FBCCA, TRCA, tCNN,\nSSVEPformer, CNNformer and DDGCNN across var-\nious time window lengths. It can be observed that\nthe proposed model consistently outperforms all other\nmodels across all time windows, demonstrating its su-\nperiority in within-subject classification performance.\nThis result validates the effectiveness of the proposed\nmodel, indicating its outstanding performance in clas-\nsification tasks. This advantage is reflected not only\nin the improved accuracy but also in the adaptabil-\nity of the model to different stimulation frequencies,\ndemonstrating its stability and reliability when pro-\ncessing complex EEG signals.\nSimilarly, on the BETA dataset in Table 4,\nthe proposed model exhibits strong advantages in\ncomparison with CCA, FB-tCNN, EEGnet, and\nCNNformer. The table displays the average accuracies\nfor different time window lengths. Across all time\nwindows, the proposed model achieves the highest\naccuracy on the BETA dataset, further confirming the\neffectiveness of the model in diverse data environments.\nThrough comparative analysis, it is evident that\nthe proposed model maintains high classification\nperformance across different time periods, reinforcing\nits potential and value in practical applications. This\nexceptional performance not only offers new insights\nfor EEG signal classification but also lays a solid\nfoundation for future related research."}, {"title": "2.4.4. SSVEPformer", "content": "Transformer-based models have been proven to\nbe applicable to SSVEP-BCI. SSVEPformer uses\nfrequency-domain signals as input and achieves\nimpressive classification performance [26]."}, {"title": "2.4.5. CNNformer", "content": "CNNformer combines CNN and Transformer. The\nCNN module captures temporal and spatial features,\nwhile the Transformer module learns global temporal\ndependencies, effectively improving the classification\nperformance of SSVEP data [39]."}, {"title": "2.4.6. DDGCNN", "content": "DDGCNN is a dynamic graph convolutional neural\nnetwork model that comprehensively considers the\ntopological structure between channels. This model\nalso addresses the over-smoothing problem in GCNs\nand is effective for learning and extracting EEG\ntopological structure features [27]."}, {"title": "3.1. Accuracy on two datasets", "content": "Table 1 presents the classification accuracy of all\nsubjects in the benchmark dataset under the proposed\nmodel. The time window was set to range from 0.6\nseconds to 1.2 seconds, with an interval of 0.2 seconds,\nto gradually evaluate the model's performance across\ndifferent time window lengths. Specifically, when\nthe time window length is 0.6 seconds, the average\nclassification accuracy for 35 subjects is 69.71%. When\nthe time window length is adjusted to 0.8 seconds, the\naverage accuracy rises to 79.97%. As the time window\nis further expanded, the accuracy continues to improve:\nwith a window length of 1.0 seconds, the average\naccuracy reaches 87.41%, and with a window length\nof 1.2 seconds, the average accuracy reaches 92.01%. These results indicate that different time window\nsettings significantly affect classification performance,\nwith classification results varying according to the\nwindow length. Notably, in the 1.2-second time\nwindow, only one subject out of the 35 had an accuracy\nbelow 70%, while the rest exceeded 70%, achieving\nthe benchmark for SSVEP-BCI, thereby validating the\nfeasibility of the model.\nTable 2 lists the accuracies of all subjects in the\nBETA dataset under the proposed model. Similarly,\nthe time window was set to range from 0.6 seconds to\n1.2 seconds, with an interval of 0.2 seconds. When the\nselected time window length is 0.6 seconds, the average\naccuracy for 70 subjects is 53.84%. When the time"}, {"title": "3.2. ITR on two datasets", "content": "Figure.5 illustrates the information transfer rate (ITR)\nof the different methods for various signal lengths.\nIt is evident that the proposed \"incepformer_ssvep\"\nmodel performs optimally across all time windows,\nparticularly at shorter signal lengths (0.6s and 0.8s),\nwhere the results are particularly notable. Specifically,\nat a signal length of 0.6s, the model achieves an\nITR of 277.33 bits/min, and at 0.8s, the ITR is\n265.74 bits/min, which is significantly higher than\nthose of the other methods. This demonstrates the\nrobustness of the proposed approach for short-term\nsignal processing.\nAs the time window increases, although the ITR"}, {"title": "4. Discussion", "content": "slightly declines, the proposed model still maintains\na leading position, further validating its stability and\neffectiveness across different time windows, making\nit particularly suitable for brain-computer interface\nsystems that require rapid responses. The BETA-ITR\ntable shows the ITR of different models at various\ntime window lengths, and it is clear that the proposed\n\"IncepFormer Net\" also performs exceptionally well\non the BETA dataset. Overall, whether in the\nbenchmark or BETA datasets, the proposed model\nexhibits superior performance across different signal\nlengths, especially in short-term signal classification\ntasks, making it well-suited for real-time applications\nin brain-computer interface systems.\nIn this study, a new input data representation with\nrich temporal features are proposed, and a model\narchitecture based on time-domain data is designed,\nwhich combines the Inception model's concept with\nthe Transformer model. We then demonstrate the\neffectiveness of our approach on two public datasets."}, {"title": "4.1. Effectiveness of Inception in Classification\nPerformance", "content": "Inception assembles multiple convolution or pooling\noperations into a network module, and when designing\nneural networks, the entire network structure is\nbuilt by assembling modules. Its sparse network\nstructure can produce dense data, a feature that\nimproves the network's performance and ensures the\nefficient use of computational resources. In the\noriginal Inception structure, 1\u00d71 convolutions are\nused to reduce the dimensionality of the feature\nmaps and the number of model parameters. For\nSSVEP, because the input data are three-dimensional,\nthe 1\u00d71 convolutions and parallel convolutions with\ndifferent kernel sizes are replaced with one-dimensional\nconvolutions of various kernel sizes to extract features.\nThis multi-scale convolution operation captures EEG\nsignals at different scales and yields good performance.\nThis approach not only reduces the number of\nparameters and prevents overfitting but also lowers\ncomputational costs, laying the foundation for online\nSSVEP classification systems.\nFigure.6 illustrates the impact of varying the\nnumber of blocks on the model's classification accuracy\nand information transfer rate (ITR). The figure\nshows the average classification accuracy and standard\ndeviation when using 1 to 6 blocks on the benchmark\nand BETA datasets. Analysis of the benchmark\ndataset reveals that as the number of blocks increases,\nthe accuracy rises from 85.17% to a peak of 87.56%,\nfollowed by a slight decline, reaching 87.01% with 6\nblocks. This indicates that while increasing the number\nof blocks can enhance classification performance, there\nis a limit, and too many blocks may lead to a slight\ndecrease in performance.\nThe figure displays the trend of ITR as the\nnumber of blocks changes, with the ITR reaching its\nhighest point at approximately 248 bits/min when\nusing 3 blocks, followed by a slight decline. Therefore,\nconsidering the balance between accuracy and ITR,\nusing 4 blocks as the temporal feature extraction\ncomponent of the model is the most appropriate choice.\nWhen employing 4 blocks, the model demonstrates\noptimal overall performance. Table.5 shows the\nselected parameters of the model when using 4 blocks.\nThe above experiments demonstrated the unique\nadvantages of the Inception module in SSVEP"}, {"title": "4.2. Attention Mechanism", "content": "Inception is the core sub-network structure in the\nclassic GoogLeNet model. It is a sparse matrix\nthat efficiently expresses features [32"}]}