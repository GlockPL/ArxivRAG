{"title": "REVEALING AND MITIGATING OVER-ATTENTION IN KNOWLEDGE EDITING", "authors": ["Pinzheng Wang", "Zecheng Tang", "Keyan Zhou", "Juntao Li", "Qiaoming Zhu", "Min Zhang"], "abstract": "Large Language Models (LLMs) have demonstrated superior performance across\na wide range of tasks, but they still exhibit undesirable errors due to incorrect\nknowledge learned from the training data. To avoid this, knowledge editing methods\nemerged to precisely edit the specific model knowledge via efficiently modifying\na very small percentage of parameters. However, those methods can lead to the\nproblem of Specificity Failure, where the existing knowledge and capabilities are\nseverely degraded due to editing. Our preliminary indicates that Specificity Failure\nprimarily stems from the model's attention heads assigning excessive attention\nscores to entities related to the edited knowledge, thereby unduly focusing on\nspecific snippets within the context, which we denote as the Attention Drift\nphenomenon. To mitigate such Attention Drift issue, we introduce a simple yet\neffective method Selective Attention Drift Restriction (SADR), which introduces\nan additional regularization term during the knowledge editing process to restrict\nchanges in the attention weight distribution, thereby preventing undue focus on the\nedited entity. Experiments on five frequently used strong LLMs demonstrate the\neffectiveness of our method, where SADR can significantly mitigate Specificity\nFailure in the predominant knowledge editing tasks.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have demonstrated outstanding performance on various downstream\nnatural language processing tasks, e.g., dialogue generation (Ni et al., 2023; Yang et al., 2024a),\nattributed to the numerous inherent knowledge (Roberts et al., 2020; Cao et al., 2023). However,\nmany unpredictable errors inevitably arise due to the model's inner defect, which stems from negative\nor outdated samples within the extensive pre-training datasets (Balachandran et al., 2022). These\nimperfections can lead to the propagation of misinformation or biased outputs (Li et al., 2023a; Tang\net al., 2023), undermining the reliability and robustness of the models in real-world applications.\nOne straightforward way to mitigate such an issue is to modify the knowledge of LLMs by directly\nfine-tuning the model with specific data. However, such a direct training method is uncontrollable\nand carries a significant risk of over-fitting (Kirkpatrick et al., 2017; Zhu et al., 2020). Consequently,\nknowledge editing methods (Meng et al., 2022; 2023; Mitchell et al., 2021) aim to efficiently\nmodify specific knowledge within a limited subset of parameters while theoretically ensuring that\nthe rest of the model's knowledge remains unchanged. However, when employing the knowledge\nediting methods, the instability feature limits their potential (Hoelscher-Obermaier et al., 2023).\nSpecifically, we find that when content related to the edited knowledge appears in the context, it\ncan inadvertently corrupt pre-existing knowledge, which we define as Specificity Failure in this\npaper. For instance, a language model, edited with a new knowledge\u2014\u201cEiffel Towel\u201d is in \u201cNew\nYork\" rather than \u201cEiffel Towel\u201d is in \u201cParis\u201d (Figure 1(a) and (b)), tends to predict \u201cPyramids\" is in\n\"New York\" (Figure 1(c)), which is inconsistent with the original knowledge embedded in the model\nbefore editing, i.e., \u201cPyramids\u201d is in \u201cEgypt\u201d. Based on our preliminary study, we observe that a 6B\nGPT-J model (Wang & Komatsuzaki, 2021) after the knowledge editing can exhibit severe Specificity\nFailure in over 50% of cases regarding factual statements."}, {"title": "2 NOTATION AND BACKGROUND", "content": ""}, {"title": "2.1 DEFINITION AND EVALUATION OF KNOWLEDGE EDITING", "content": "Previous works (Dai et al., 2021; Meng et al., 2022; 2023) represent factual associations as a\nknowledge tuple $t = (s, r, o)$, where $s$ is the subject, $r$ is the relation, and $o$ is the object. To evaluate"}, {"title": "3 EXPLORATION OF SPECIFICITY FAILURE", "content": "While knowledge editing excels at memorizing new knowledge, it still suffers from specificity failure.\nWe first measure these failures on CounterFact benchmarks and then identify which intermediate\noutputs during the edited model's inference cause incorrect predictions. We further explore the\nprimary triggers for these errors at a granular level and verify our findings by patching attention drift\nto mitigate specificity failure. Our experiments focus on Relation and Distract Neighborhood tasks,\nemploying the widely-used ROME method on the GPT-J-6b model (Wang & Komatsuzaki, 2021)."}, {"title": "3.1 CALIBRATING SPECIFICITY FAILURE ON COUNTERFACTUAL BENCHMARKS", "content": "To measure performance on both the Relation and Distract Neighborhood tasks, we employ a dataset\ncomposed of COUNTERFACT (Meng et al., 2022) and WikiDatacounterfact (Zhang et al., 2024a).\nThe dataset includes 1683 factual statements, with more details provided in Appendix C.1."}, {"title": "3.2 LOCALIZING SPECIFICITY FAILURE", "content": "In the forward pass of an autoregressive language model, the flow of information can be viewed as a\ncausal graph (Li et al., 2023b). When a model with $L$ layers predicts based on a prompt containing\n$T$ tokens, each module such as attention modules, MLPs, and transformer blocks, produces $T \u00d7 L$\noutputs. Each of these outputs is influenced by prior outputs from earlier layers and preceding token\npositions. Inspired by causal tracing (Meng et al., 2022), we trace across different states in the causal\ngraph to identify which parts contaminate the information flow in specificity failure."}, {"title": "3.3 IDENTIFYING ATTENTION DRIFT AS A TRIGGER FOR SPECIFICITY FAILURE", "content": "As mentioned above, attention activations are one of the primary causes of specificity failures.\nPrevious studies (Geva et al., 2023; Chen et al., 2024; Geva et al., 2022) have indicated that attention\nmodules in the middle-upper layers extract factual attributes during predictions. This suggests that the\nattention module may mistakenly focus on edited information, thereby neglecting other information\nwhen predicting the final token. Therefore, we quantify the relationship between drift in attention\nweights and failures in the Distract Neighborhood and Relation tasks using the Pearson coefficient.\nGiven that the edited model overestimates the probability of the edited object $o_{edit}$ relative to the\ntrue object $o_{true}$, we analyze the correlation between $\\Sigma_{l} \\Sigma_{h} DKL(W_{i,h} || W'_{i,h})$ and $P(o_{edit})$. Here,"}, {"title": "3.4 MITIGATING SPECIFICITY FAILURE BY PATCHING ATTENTION DRIFT", "content": "To further verify the significant impact of attention drift on specificity failure, we quantify the change\nin prediction probability after patching attention weights across various layers. We first conduct a\nforward pass using the vanilla model with prompts from the specificity tasks and store the intermediate\nattention weights. Then, we test the edited model on the same prompts, substituting its attention\nweights with those previously stored."}, {"title": "3.5 TAKEAWAYS", "content": "Based on the analysis mentioned above, we can conclude that: (1) the attention activations at the last\ntoken position significantly contaminate the forward pass of the edited model, causing specificity"}, {"title": "4 SELECTIVELY RESTRAINING ATTENTION DRIFT DURING KNOWLEDGE EDITING", "content": "As mentioned in Section 2.2, the optimized value $v\u2217$ for knowledge editing can be obtained through\ngradient descent based on the following objective:\n$L(z) = -log P_{G(m^* :=z)} (o_{edit} | x_j + p) + wD_{KL} (P(x | p')||P_{G(m^*):=z)} (x | p')) $.\nHowever, this objective may cause attention drift that leads to Specificity Failure. To enhance\nspecificity in knowledge editing when optimizing $v^*$, we introduce the Selective Attention Drift\nRestriction (SADR), which is a regularization term based on Equation 1. It is worth noting that\nSADR dynamically applies constraints to different heads as needed since Transformer models contain\nvarious knowledge-specific attention heads (Wang et al., 2022; Geva et al., 2023) that capture different\nfactual associations. Additionally, SADR is a simple yet efficient method and can be flexibly adapted\nacross various editing methods.\nMore concretely, as excessive attention to the edited subject of certain heads is strongly correlated\nwith Specificity Failures, we apply SADR on heads where the last token overly focuses on the edited\nsubject. We determine which heads to restrain by the following criterion: a head is selected if the\nattention weight attending to the subject's last token exceeds the maximum attention weight\namong all heads in the vanilla model.\nLet $W_{i,h}(S)$ be the attention weight from layer $l$ and head $h$ when processing the prompt $S$,\n$W_{G(m):=z}(S)$ be the attention weight from the model that is edited with $z$, and $M_l(S) = max_h W_{i,h}(S)[-1,s]$ be the maximum attention weight that the last token attends to the edited\nsubject $s$ among all heads at layer $l$ in the vanilla model. The objective of SADR can be written as:\n$L_{SADR}(Z) = \\sum_{j=1}^N \\sum_{l} \\sum_{h\\in H_l(S_j)} D_{KL} (W_{i,h}(S)[-1,:]||W'_{G(m):=z} (S)[-1,:])$.\nwhere $H_l(S_j) = {h: W_{G(m):=z}(S)[-1,s] > M_l(S)}$ and $S_j = x_j (s,r)$.\nThus, the optimized value $v\u2217$ can be obtained by: $v^* = arg min (L(z) + \\gamma L_{SADR}(z))$, where $\\gamma$ is\nthe controlling weight."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 SETTINGS", "content": "Dataset Due to the limited availability of datasets that satisfy the required fields for our tasks, we\ncombine COUNTERFACT (Meng et al., 2022) and WikiDatacounterfact (Zhang et al., 2024a) with\n1,683 factual statements as the testing data. The processing details are mentioned in Appendix C.1.\nAdditionally, we extend our experiments to broader datasets, including QA-format and recent\nknowledge editing tasks, as detailed in Appendix E.3. The phenomena of Specificity Failure and the\nperformance of SADR remain consistent across these datasets.\nBaselines & Models We evaluate the performance of our methods on three mainstream locate-\nthen-edit knowledge editing baselines: ROME (Meng et al., 2022), MEMIT (Meng et al., 2023), and\nPMET (Li et al., 2024). Specifically, we focus on knowledge editing with one factual association\nfor all the baselines. We implement our SADR method across three editing baselines on the GPT-J-"}, {"title": "A LIMITATIONS AND FUTURE WORKS", "content": "In this section, we outline several limitations of our study that highlight areas for future research and\nimprovement: (1) While our method shows promise, there is still potential for improvement in the\nRelation task. This may be due to the fact that edits can inadvertently erase or obscure other relevant\nknowledge about the subject, thereby affecting the model's overall performance in understanding and\nmaintaining relations. (2) Our study primarily focused on single factual association edits. This scope\nexcluded more complex scenarios such as batch and sequential editing, which involve multiple edits\neither simultaneously or over a sequence. (3) We focus on knowledge editing on only transformer-\nbased models, omitting models with new architectures (Gu & Dao, 2023). These issues highlight\nimportant avenues for future research and will be explored in subsequent studies to enhance the\nrobustness and applicability of knowledge editing."}, {"title": "B PRELIMINARY OF KNOWLEDGE EDITING", "content": "In this section, we provide more details of the baselines used in our experiments.\nROME As mentioned in Section 2.2, ROME (Meng et al., 2022) implements rank-one knowledge\nediting by deriving a closed form solution:\nminimize ||\u0174K \u2013 V || such that \u0174k* U by setting W W + (v* - Wk*)$\\frac{(C^{-1}k^*)^T}{(C^{-1}k^*)^Tk^*}$"}, {"title": "C IMPLEMENTATION DETAILS", "content": ""}, {"title": "C.1 DATASET PROCESSING", "content": "The dataset we use is a mixture of counterfact datasets from Meng et al. (2022) and Zhang et al.\n(2024a). Meng et al. (2022) introduce COUNTERFACT, which contains 21,919 records featuring\na diverse set of subjects, relations, and linguistic variations. It also provides paraphrase prompts,\nneighborhood prompts, and generation prompts for specificity evaluation. Zhang et al. (2024a) collect\ntriplets about popular entities from top-viewed pages on Wikipedia to construct WikiDatacounterfact.\nThey provide relational prompts to evaluate the impact of edits on other attributes associated with the\nedited subject. We combined these datasets in equal proportions to create a balanced dataset with\n1683 factual statements."}, {"title": "D ADDITIONAL RESULTS FOR EXPLORING SPECIFICITY FAILURES", "content": ""}, {"title": "D.1 LOCALIZE SPECIFICITY FAILURE IN CAUSAL GRAPH", "content": "We further investigate the tracing effects of \"Contaminating Substitution\" across different window\nsizes in the \"Distract Neighborhood\u201d and \u201cRelation\u201d tasks, and also demonstrate the impact on the\nprediction probability of Oedit. As shown in Figure 9a, varying window sizes indicate similar areas"}, {"title": "D.2 PATCHING ATTENTION DRIFT TO MITIGATE SPECIFICITY FAILURE", "content": "In section 3.4, we demonstrate an improvement in specificity performance after patching attention\ndrift in some consecutive layers. To explore the effectiveness of patching attention drift at a finer\ngranularity, we evaluate the tracing effect of modifying a single value in the attention weight matrix.\nSpecifically, we alter the value that represents the weight of the last token attending to the t-th\ntoken before softmax during the forward pass of the edited model. The replacement value is the\none generated by the vanilla model for the same prompts. Considering residual connections in\nTransformer, we patch for a window of k layers around the l-th layer."}, {"title": "D.3 THE CORRELATION BETWEEN MORE FACTORS AND SPECIFICITY FAILURE", "content": "Recent works (Fang et al., 2024; Yao et al.,\n2024; Ma et al., 2024) point out that the edit\nvector's direction, space, and norm can influ-\nence the model's specificity performance. How-\never, these works primarily focus on preserv-\ning general knowledge and capabilities, rather\nthan addressing the specificity failure that arises\nwhen the edited subject appears in the context.\nTo explore the relevance of these factors to the\nspecificity failure problem studied in our work,"}, {"title": "D.4 DISCUSSION ABOUT REASONS FOR ATTENTION DRIFT", "content": "Experiments have shown that attention drift is closely related to specificity failure. A natural question\narises: what is the reason for attention drift during the editing process? Intuitively, editing methods\nprimarily modify the hidden states of the edited subject, which subsequently influence the final\noutput through the attention mechanism. In traditional editing methods (e.g., ROME discussed in\nSection 2.2), the optimization objective explicitly trains the model to predict the new Oedit given (s,r).\nThis may create a shortcut, where the hidden state of the subject is shaped in a way that makes it\noverly prone to being prioritized by the attention mechanism, thereby hard-coding the knowledge\ninto the forward propagation rather than truly integrating it into the model."}, {"title": "E ADDITIONAL RESULTS ON SELECTIVE ATTENTION DRIFT RESTRICTION", "content": ""}, {"title": "E.1 MAIN RESULTS", "content": "To comprehensively evaluate the effectiveness of our method, we employ two additional frequently\nused models, Llama2-13B (Touvron et al., 2023) and TinyLlama (Zhang et al., 2024b), to observe the\nperformance of our method across different model sizes and advanced knowledge-rich models in this\nsection. To further validate our method's performance in knowledge editing, we have incorporated\nnew metrics for generalization, specificity, and fluency."}, {"title": "E.2 RESULTS ON MORE EDITING METHODS", "content": "Knowledge editing methods can be\ncategorized into three types: locate-\nthen-edit, parameter-preserving, and\nmeta-learning. To further verify\nwhether attention drift is also ev-\ndent in parameter-preserving and\nmeta-learning-based editing meth-\nods, we conduct additional exper-\niments on WISE (Wang et al.,\n2024b) and MEND (Mitchell et al.,"}, {"title": "E.3 RESULTS ON MORE DATASETS", "content": "Due to the limited availability of datasets that meet the required fields for our tasks, we conducted\nexperiments on a relatively small dataset with 1,683 pieces of data from COUNTERFACT (Meng et al.,\n2022) and WikiDatacounterfact (Yao et al., 2023). To better illustrate the specificity failure problem\nand validate the effectiveness of our approach across a wider range of data formats and entities, we"}, {"title": "E.4 HUMAN EVALUATION", "content": "We compare the performance of three\nknowledge editing baselines with and without our SADR method on GPT-J and Llama3-8b in human\nevaluation. We provide the edited model with prompts composed of (s,r) for text generation,\nrestricting output to a maximum of 100 tokens. For each setting, we randomly sample 20 comparison\npairs and hire nine annotators to give their preferences (win, loss, and tie) for three evaluation criteria:\nEdit Success, Specificity, and Fluency. We show the statistics of human evaluation data in Tabel 9\nand human evaluation interface in Figure 12 and 13. To ensure consistency among the annotators,\nwe report the Fleiss' kappa score and we can observe that all the inter-annotator agreements are\nsubstantially consistent (\u043a\u2208 [0.6, 1]). The results presented show that our methods outperform the\noriginal baselines in Specificity and Fluency while maintaining performance in Edit Success."}, {"title": "E.5 ABLATION STUDY ON RESTRAINING WEIGHT", "content": "The hyper-parameters of our method primarily include the controlling weight y. In this section, we\npresent the ablation study of the effect of controlling weight. We conduct the ablation study on GPT-J\nwith ROME in this part and randomly sample 500 data points for evaluation. The results of adjusting\nthe hyper-parameter y are reported in Table 10. We observe that larger y slightly improves specificity\nwhile keeping other metrics almost unchanged. This indicates that our method is not sensitive to \u03b3,\nas we only restrain heads that over-focus on the edited token compared to the vanilla model."}, {"title": "F EFFICIENCY ANALYSIS", "content": "In terms of memory usage, the additional variables to store in our method are the attention weights\nacross all layers. These weights can be represented as L \u00d7 H \u00d7 S2, where L is the number of layers\nin the model, H is the number of attention heads, and S is the sequence length. The additional storage\nrequired is minimal compared to the overall model parameters. During our experiments, we did not\nobserve any noticeable increase in GPU memory usage.\nRegarding runtime, our method primarily involves computing a mask through comparison of attention\nweights and calculating the KL divergence. However, due to the use of Python loops in our current\nimplementation, a slight runtime overhead is observed. For instance, when applying the ROME\nediting method to GPT-J-6B on an A100-PCIE-40GB GPU, the runtime per edit increased from 7.80\nseconds (without SADR) to 9.65 seconds (with SADR)."}, {"title": "G ETHICAL CONSIDERATIONS", "content": "Our goal in improving knowledge editing performance is to correct errors and update the knowledge\nin LLMs. It is important to notice that knowledge editing techniques can also be used to generate\ntoxic and harmful content. We advocate for the responsible use of knowledge editing techniques to\nenhance model behavior rather than for malicious purposes."}, {"title": "H CASE STUDY", "content": "In this section, we present the results generated by our method in comparison with the original method\nusing ROME on GPT-J-6b."}]}