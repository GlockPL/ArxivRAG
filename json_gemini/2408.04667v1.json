{"title": "LLM Stability: A detailed analysis with some surprises", "authors": ["Berk Atil", "Alexa Chittams", "Lisheng Fu", "Ferhan Ture", "Lixinyu Xu", "Breck Baldwin"], "abstract": "A concerning property of our nearly magical LLMs involves the variation of results given the exact same input and deterministic hyper-parameters. While AI has always had a certain level of noisiness from inputs outside of training data, we have generally had deterministic results for any particular input-that is no longer true. While most LLM practitioners are \"in the know\", we are unaware of any work that attempts to quantify current LLM stability-we suspect no one has taken the trouble because it is just too boring a paper to execute and write. But we have done it and there are some surprises.\nWhat kinds of surprises?\n\u2022 The evaluated LLMs are rarely deterministic at the raw output level; they are much more deterministic at the parsed output/answer level but still rarely 100% stable across 5 re-runs with same data input.\n\u2022 LLM accuracy variation is not normally distributed.\n\u2022 Stability varies based on task.", "sections": [{"title": "1 Introduction", "content": "The usage of Large Language Models (LLM) has increased with their enormous capability of question answering (Robinson and Wingate, 2023), reasoning (Qiao et al., 2023), code generation (Jiang et al., 2024b), etc. There are several hyperparameters such as \u201ctemperature\u201d, \u201ctop-k\u201d, \u201ctop-p\", or \"repetition penalty\" that affect the performance of the models significantly (Wang et al., 2023). Besides an effect on the performance, \"temperature\" affects the creativity of the model by modifying the probability distribution over the vocabulary (Wang et al., 2020, 2023). Therefore, it becomes even more important based on the task. When the temperature is high, the differences between the probabilities reduce and the distribution becomes flatter. On the other hand, when it is close to 0, the model should be more deterministic.\nThere are several benchmarks to measure the performance of LLMs (Hendrycks et al., 2021; Suzgun et al., 2023; Wang et al., 2024; Gema et al., 2024; Rein et al., 2023). Reproducibility of evaluations is essential to trust the results and compare LLMs (Biderman et al., 2024), but this becomes harder with stochastic models. Usually, one output is sampled and the models are evaluated based on that presumably because the inference is costly. However, when there is a significant variance in the output, this reduces the trustworthiness of the benchmarks. Besides, this creates more problems in commercial use of LLMs because variations in the responses might affect the customer satisfaction.\nIn this work, we analyze the stability of various families of models on tasks from common benchmarks (Hendrycks et al., 2021; Suzgun et al., 2023). We show that models are not deterministic even with a temperature of 0, and that the degree of stability changes from model to model. Furthermore, this degree of randomness sometimes results in a change in the rankings of the models on some tasks which makes the model comparisons unreliable. More specifically, we answer the following questions:\n\u2022 How much variation exists in the LLM responses in terms of the final performance?\n\u2022 How much does the model output change over several runs?\n\u2022 Is there any correlation between stability, accuracy, input length, and output length?"}, {"title": "2 Related Work", "content": "There have been many investigations on the robustness of machine learning (ML) models with trivial changes to the input (Sehwag et al., 2019; Freiesleben and Grote, 2023; Hancox-Li, 2020; Rauber et al., 2017). More recently, similar analyses also show the unreliability of the benchmarks and robustness of LLMs (Xu et al., 2024; Gupta et al., 2024; Sinha et al., 2023; Sarker et al., 2024; Raina et al., 2024). Especially for multiple choice questions, the order of the options changes the performance of the models significantly (Gupta et al., 2024). Xu et al. (2024) replace the correct option with \"none of the above\" and observe a dramatic performance changes in different models.\nWe focus more on the stability of LLMs over different runs. It is unclear if LLMs are non-deterministic in nature due to floating point instability or other factors, but with some hyperparameters such as temperature, top-p, or top-k they can be made deterministic theoretically. Temperature is used in the softmax function which changes determinism of the output significantly. When it is set to 0, the model is supposed to produce the same output given the same prompt in theory, on the other hand, it makes the model less creative. There is not much work in the literature focusing on the stability of LLMs although it is an important problem for the reproducibility of findings and having reliable systems in practice. The most relevant work is from Song et al. (2024) who analyzes the effect of temperature, sampling strategy, repetition penalty, and alignment algorithms on the performance. They have similar findings that LLMs have some variance in the output which should be taken into account in the evaluation benchmarks. Their stability analysis is different than our methodology in the sense that they use temperature of 1 when they report the variance of the outputs. Ouyang et al. (2023) also do a similar stability analysis on LLMs with varying temperatures on code generation tasks only. Our literature review found no work that ran the exact same inputs and configuration multiple times"}, {"title": "3 Datasets", "content": "Beyond Imitation Game Benchmark Hard (BBH) (Suzgun et al., 2023) is a benchmark consisting of 27 challenging tasks about traditional Natural Language Processing (NLP), mathematics, commonsense reasoning, etc. We randomly select \"navigation\", a task to determine whether or not an agent returns back to the starting point given navigational steps; \u201cruin names\u201d, a task to pick a humorous simple edit of a band or movie name; \"geometric shapes\u201d, a task to determine the geometric shape given in SVG path format; and \"logical deduction with three objects\", a task to deduce the order of a sequence of three objects from a set of conditions. Measuring Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) is another benchmark that contains 57 tasks about humanities, social sciences, STEM, and some other important fields to learn. We randomly pick \"high school European history\u201d from humanities, containing questions about the history of Europe, \"college mathematics\u201d from STEM with questions about calculus, algebra etc., \"public relations\" from social sciences that have questions about media theory, crisis management etc., and \"professional accounting\" with accounting questions. All of the tasks are multiple choice questions with varying number of options, see brief statistics in Table 1."}, {"title": "4 Experiments and Results", "content": "We use few-shot prompting without Chain-of-Thought (CoT) (Wei et al., 2022). Regarding the number of examples for few-shots, we use the standard settings that are 3-shot for BBH tasks, 5-shot for MMLU tasks. We set the temperature to 0, top-p to 1, and fix the seeds. We use the OpenAI API and run each prompt 5 times with the same setup.\n4.1 Models\nWe experiment with GPT-3.5 Turbo (Brown et al., 2020), GPT-4o (OpenAI et al., 2024), Llama-3-70B-Instruct (Meta, 2024), Llama-3-8B-Instruct (Meta, 2024), and Mixtral-8x7B-Instruct (Jiang et al., 2024a).\n4.2 Metrics\nThe focus of this work is evaluating the stability of various LLMs across various tasks, not the performance of those LLMs on those tasks. Our metrics are:\n\u2022 Minimum, median, and maximum accuracy values over the runs.\n\u2022 Minimum-maximum spread, which is the difference between the minimum and maximum accuracies over the runs.\n\u2022 Total agreement rate (TAR), which is the percentage of total agreement across all runs where all answers are identical, regardless of whether the answer was correct.\nWithin the TAR metric there are two cases:\n\u2022 TARa (TAR for the answer) The parsed answer is the same, e.g., \"The answer is a)\" is the same as \"a) is the answer\". The answer may or may not be correct.\n\u2022 TARr (TAR for the raw model response is same) The LLM response is string equivalent. The parsed answer in the string may or may not be correct.\nA consequence of our metrics allows for the possibility of a 100% mean accuracy which would have 100% TARa but 0% TARr score. The TARr score is the strictest metric of stabilility since any character variation will result in a non-match.\n4.3 Results\nTable 2 summarizes our 5 run experiments across minimum, median, and maximum values followed by total agreement rate TARa and TARr. Perfect system performance would show the same score for minimum, median, and maximum accuracy, and 100% for both TARa and TARr. No model/task achieves this performance, with the closest being under GPT-3.5 Turbo: European History and Professional Accounting, which do not have 100% for TARr but achieve 100% for TARa and accuracy.\nTable 3 shows the median performance across datasets for TARa and TARr metrics. GPT-3.5 Turbo stands out for reproducibility for exact output at 97% with GPT4o almost never producing the same exact output, resulting in a median 3%. TARa demonstrates much higher overall performance with all models performing 88% or above.\nFigure 2 gives a sense of the spread across tasks and LLMs of the minimum and maximum accuracy values, with only GPT-3.5 Turbo managing to keep performance spread down to 2%. All other models have between 5-10% spreads.\nFigure 5 makes clear that the answer variation (TARa) is task-specific to some degree with public relations and European history pretty uniformly consistent. College math and geometric shapes show the greatest difference across models in TARa. Figure 6, the stricter measure of TARr (raw output variation), shows much greater variation across tasks and models with clear consistent performance from GPT-3.5 Turbo.\nThe astute reader should note that we don't report means and standard deviation indicating that the observed variations are not normal. We reran our highest spread task-college math, 20 times with GPT-4o and Llama-3-70B to recover the distributions. Figure 3 demonstrates a clearly non-normal distribution with mean and median values far from the mode and a marked skew. A Kolmogorov-Smirnov normality test (Massey Jr, 1951) rejected the normal hypothesis with a smaller p-value than 10\u20139. Figure 1 shows similar results with top-p set to 0 out of concern that our initial choice of top-p=1 introduces instability-it degrades accuracy but does not change stability.\n4.4 Error Analysis\nSince this work concerns stability over accuracy, the error analysis we did focuses on orthogonally validating questionable looking results for TARr, e.g., the near-perfect performance of GPT-3.5 Turbo applied to professional accounting and European history. To find where the responses diverged, we compare answers across runs and report on the first instance where the responses differed, with the run number indicated.\ngpt-3.5, European history, q 30\n0: The answer is (A).\n1:The answer is (A) an independent\nThe above answer is correct across both examples. The variation to a human eye looks like an innocuous elaboration in run 1 but an extraction parser might fail for the instance.\nClasses of TARa errors include:\n\u2022 Responding with a different answer from at least one of the other 4 answers.\n\u2022 Responding with There is no correct answer or other similar utterances, which constitutes 3.2% of the output.\n4.5 Correlation Analysis\nWe performed a Spearman rank correlation analysis on the factors discussed previously: TARa, TARr, minimum-maximum accuracy spread, along with accuracy, input length, and output length. The results are shown in a heat map in figure 4. We define accuracy as the mean accuracy over the 5 runs with the same model and dataset setup. Input length and output length are median word counts split by space, calculated over the input and output of each LLM experiment setup. We split the words by space instead of using a particular tokenizer, as the models we experimented with used different tokenizers.\nThe results show a strong negative correlation between the output length and TARa, as well as between the output length and TARr. This means that as an LLM's output length increases, the stability of the output decreases, resulting in more diverse natural language responses as well as the actual multiple choice selection. The strong negative correlation between LLM output length and stability could motivate those using LLMs commercially to restrict the max generation tokens to control the stability. We also see a moderately positive correlation between accuracy and TARa. This indicates that a more stable LLM is likely to produce more accurate multiple choice selections. However, accuracy is not correlated with TARr, which helps us decouple the subtle notions of an LLM's stability in reasoning versus natural language generation. An additional finding from our correlation analysis reveals that there is no correlation between the length of input and output.\n4.6 Discussion\nRecall that as a baseline, we would expect a system to perform consistently given the same inputs which would result in 100% TARa, 100% TARr, and 0% difference in the minimum and maximum values returned across all tasks for each model. We find the fact that no LLM achieves this baseline on any of the tasks quite surprising even with the common knowledge that LLMs can be fickle.\nOther than GPT-3.5 Turbo, the TARr scores put a great deal of pressure on downstream components to be robust to string variation even if the answer is the same in the LLM response payload. This variation doesn't impact humans as much since we have very robust parsing and answer recovery skills, but a downstream system that has to parse the LLM payload would likely never be completely confident that the answer would be recovered-this use-case is increasing in popularity as LLMs replace traditional AI technologies due to their increased performance and ease of use.\nThe TARa scores still rarely achieve 100% but all remain above 88% so there is better news with the introduction of normalization via answer extraction. However, our answer parsing was hand-developed to cover all the responses seen so far with no guarantee that further raw payloads might diverge outside the scope of the already handled cases.\nThe maximum/minimum spread also would normally be 0% in the pre-GPU era (non-deterministic computation with neural nets has been attributed to GPU usage), but we only see 4 instances across all models/tasks with that property. Some of the spreads get fairly large, 10% for Mixtral-8x7b on geometric shapes, 10% for Llama-3-8B on college math. Only GPT-3.5 Turbo is below 2% across all tasks.\nThe 20-draw distribution experiment does not support the hypothesis that our non-determinism is due to being from a different random number generation draws-if that were the explanation then we would expect normally distributed variation. We don't know the operations for proprietary models so we offer this with considerable conjecture. GPU introduced non-determinism, if that is the case here, appears to result in non-normal variation.\n4.7 Engineering with non-deterministic Models\nLLMs function as a low code environment for tasks like classification, extraction, question answering, clustering, and summarization. The ease with which these technologies are accessed via Star Trek-esqe \"Talk to the computer\" interfaces expands its user base, but that expansion has also come with the recognition that LLMs hallucinate, make mistakes and are just wrong some of the time. Developers have always known that AI systems make mistakes, now everyone does.\nUntil the advent of GPUs which introduced one source of non-determinism in neural nets around 10 years ago, AI models were generally entirely deterministic given the same input. While AI models made mistakes and had varied performance over time, that was a function of novel combinations of input data that exposed limitations of the AI model's under-/over-fitting, failure to represent the data-generating process or other external elements.\nNon-deterministic AI brings new challenges to developers-particularly in commercial applications:\n\u2022 The ability to unit test Al functions in the same way as a database call or mathematical function that was expected to produce the same outputs for the same inputs. Those unit tests may sporadically fail now and have to be handled in a non-unit test framework like regression testing that tolerates variability.\n\u2022 High precision systems, typically low/medium recall, are very vulnerable to \"out of nowhere\" errors since they need to operate at very low false positive rates.\n\u2022 Low stability may also increase \"inexplicably bad\" system results that stand out from the kinds of errors a human might make or even a typical \"traditional\" AI model. A mild example is answering \"none of the above\" when the task requires selecting one of multiple answers-A human that understood the instructions would not make that mistake.\n\u2022 Assuming that errors due to non-determinism and errors due to system performance are IID (independently, identically distributed), the factors are multiplicative reductions of performance. A dialog system that uses non-deterministic LLM classifiers to manage transitions multiplicatively degrades with each additional state in the sequence of dialog states, e.g., a dialog system with 4 classifiers that are 95% stable will show .954 = .814 expected performance before even factoring in the accuracy of the classifier on novel inputs.\n\u2022 Perhaps the biggest impact though is in system complexity that has to handle gracefully 'usually correct yet this time wrong' results. Zipfian distributions are commonly seen in applied AI systems where the frequency of an input/category is inversely related to its rank in count sorted order, $frequency \\times 1/rank$. Testing tends to concentrate on the frequent events and that delivers confidence that the resulting system is stable for the common inputs. But the absence of stability undermines the entire foundation of this confidence especially if mistakes are costly."}, {"title": "5 Conclusion", "content": "We have made concrete the common observation that LLMs are not stable on the same inputs despite the hyper-parameters being set to maximize determinism. There are a few surprises that we didn't expect:\n\u2022 It is rare that an LLM will produce the same raw output given the same input even though the parsed out answer will generally be stable.\n\u2022 Response accuracy across runs is not normally distributed.\n\u2022 Stability is a function of the task being addressed, college math is among the least stable tasks, and European history is among the most stable tasks."}, {"title": "6 Future Work", "content": "There are many possible directions that follow-up work can address. In no particular order:\n\u2022 If users/prompt optimizers are made aware of TARr results, is it possible to get TARr to 100%, e.g., add the meta-prompt \"You are only allowed to answer with a single letter.\"\n\u2022 How do fine-tuned models with similar tasks perform with zero-shot tasks?\n\u2022 How to communicate lack of stability to decision-makers?\n\u2022 How do classes of errors track with stability? Are there distributional differences between correct vs incorrect, true positives vs true negatives, etc.?"}]}