{"title": "Information-Theoretic Foundations for Machine Learning", "authors": ["Hong Jun Jeon", "Benjamin Van Roy"], "abstract": "The staggering progress of machine learning in the past decade has been a sight to behold. In retrospect, it is both remarkable and unsettling that these milestones were achievable with little to no rigorous theory to guide experimentation. Despite this fact, practitioners have been able to guide their future experimentation via observations from previous large-scale empirical investigations. However, alluding to Plato's Allegory of the cave, it is likely that the observations which form the field's notion of reality are but shadows representing fragments of that reality. In this work, we propose a theoretical framework which attempts to answer what exists outside of the cave. To the theorist, we provide a framework which is mathematically rigorous and leaves open many interesting ideas for future exploration. To the practitioner, we provide a framework whose results are very intuitive, general, and which will help form principles to guide future investigations. Concretely, we provide a theoretical framework rooted in Bayesian statistics and Shannon's information theory which is general enough to unify the analysis of many phenomena in machine learning. Our framework characterizes the performance of an optimal Bayesian learner, which considers the fundamental limits of information. Unlike existing analyses that weaken with increasing data complexity, our theoretical tools provide accurate insights across diverse machine learning settings. Throughout this work, we derive very general theoretical results and apply them to derive insights specific to settings ranging from data which is independently and identically distributed under an unknown distribution, to data which is sequential, to data which exhibits hierarchical structure amenable to meta-learning. We conclude with a section dedicated to characterizing the performance of misspecified algorithms. These results are exciting and particularly relevant as we strive to overcome increasingly difficult machine learning challenges in this endlessly complex world.", "sections": [{"title": "1 Introduction", "content": "In the past decade, the staggering progress of machine learning has been a sight to behold. The research community has conquered games such as go, which were thought to require human-level learning and abstraction capabilities. We have produced systems which are capable of displaying common-sense and holding coherent dialogues with humans around the globe. It is undeniable that these artifacts will be remembered throughout the future of humanity's pursuit of discovering and understanding intelligence.\nIn retrospect, it is both remarkable and unsettling that these milestones were achievable with little to no rigorous theory to guide experimentation. While theorists have attempted to repurpose existing statistical tools to analyze modern machine learning, the conclusions have largely contradicted the observations of practitioners. aptly demonstrated this point via a series of simple experiments which elucidated the fundamental incompatibility of empirically observed phenomena with existing notions of generalization. Despite this incoherence, practitioners have been able to guide their future experimentation based on prior large-scale empirical investigations. However, without a clear idea of how these phenomena slot into a larger picture, many research efforts will continue to be led astray. Alluding to Plato's Allegory of the cave, it is likely that the observations which form the field's notion of reality are but shadows representing fragments of that reality.\nIn this work, we propose a theoretical framework which attempts to answer what exists outside of the cave. To the theorist, we provide a framework which is mathematically rigorous and leaves open many interesting ideas for future exploration. To the practitioner, we provide a framework whose results are very intuitive, general, and which will help form principles to guide future investigations. Concretely, we provide a theoretical framework rooted in Bayesian statistics which is general enough to unify the analysis of many phenomena in machine learning.\nOur theoretical framework draws inspiration from both Shannon's theory of information and his maxim of \"information first, then computation\". The turn of the twentieth century brought a wave of interest in communications research; work that would enable the transmission of signals across long distances. Much of the work in encoding/decoding was approached heuristically, similarly to how deep learning is today. Shannon's theory and maxim redirected attention to characterizing what was fundamentally possible or impossible, in the absence of computational constraints. His theory guided the discovery of algorithms which achieved these fundamental limits and eventually practical implementations as well.\nThe aforementioned staggering feats of machine learning and artificial intelligence have fueled optimism that anything is learnable with sufficient data and computation. However, research directions have largely been informed by informal reasoning supported by a plethora of empirical studies. While work in statistics provides some guidance, the results for the most part lack the generality required to explain the continuing onslaught of novel empirical findings. This monograph aims to provide a general framework to elucidate what is possible by studying how the limits of performance in machine learning depend on the informational content of the data. Our framework is based on Shannon's information theory and characterizes the dependence of performance on information in the absence of computational constraints. Concretely, we characterize the performance of an optimal Bayesian learner that observes data generated by a suite of data generating processes of increasing complexity. By expressing what is fundamentally possible in machine learning, we aim to develop intuition that can guide fruitful investigation.\nOur framework characterizes the performance of an optimal Bayesian learner, which considers the fundamental limits of information. Unlike existing analyses that weaken with increasing data complexity, our theoretical tools provide accurate insights across diverse machine learning settings. For example, previous theories about learning from sequential data rely on specific and rigid mixing time assumptions. However, leverage our framework to arrive at more general results which characterize the sample complexity of learning from sequences autoregressively generated by transformers. They further extend the techniques to analyze hierarchical data generating processes which resemble meta-learning and in-context learning in large language models (LLMs). That these analytic techniques remarkably apply whether data is exchangeable or exhibits more complex structure points to the fundamental nature of our findings.\nIn recent years, we have observed that training larger machine learning models on more data continues to produce significantly better performance. This continual improvement indicates that the data generating processes that we study are more complex than the machine learning models which we fit. We refer to this phenomenon as \"misspecification\" and it is prominently observed in natural language processing (NLP), where researchers have tried to mathematically characterize this improvement in performance. The \"neural scaling laws\" of and characterize the rate at which out-of-"}, {"title": "2 Related Works", "content": "We begin with a discussion of frequentist statistics, the predominant framework which encompasses existing theoretical results. As its name would suggest, in frequentist statistics, probability describes how often an event occurs if a procedure is repeated many times. For instance, suppose there exists an unknown parameter \\(\\mu \\in \\mathbb{R}\\) and a sample of size \\(T\\): \\((X_1, X_2, ..., X_T)\\) which is drawn iid \\(\\mathcal{N}(\\mu, 1)\\). After observing the sample, the frequentist statistician may construct a 95% confidence interval for the unknown \\(\\mu\\). However, recall that in frequentism, probability is assigned to how often an event occurs if a procedure is repeated many times. For our example, this entails that if random samples of size \\(T\\) were drawn repeatedly and their corresponding confidence intervals constructed, then 95% of those confidence intervals would contain \\(\\mu\\). Note that the unknown parameter \\(u\\) is fixed and hence not a random variable in the frequentist framework. As a result, the frequentist framework does not use the tools of probability to model uncertainty pertaining to \\(\\mu\\).\nIn contrast, Bayesian statistics treats all unknown quantities as random variables. A consequence is that these quantities must be assigned subjective probabilities which reflect one's prior beliefs pertaining to their values. Returning to our example, the Bayesian may assign a prior distribution \\(P(\\mu \\in \\cdot) = \\mathcal{N}(0,1)\\) which reflects their beliefs prior to observing the sample. After observing the sample \\((x_1,x_2,...,x_T)\\), they may construct a 95% credible interval for \\(\\mu\\), an interval \\((a, b)\\) for which \\(P(\\mu \\in (a,b)|X_1 = x_1, X_2 = x_2,..., X_T = x_T) \\geq 0.95\\). The posterior distribution \\(P(\\mu \\in \\cdot|X_1 = x_1, X_2 = x_2, ..., X_T = x_T)\\) is computed via Bayes rule. Note that unlike the frequentist confidence interval which pertains to repeated experimentation, the Bayesian credible interval states that for this particular sample, with 95% probability, \\(\\mu \\in (a, b)\\). However, we note that this probability is subjective as it relies upon the prior subjective probability \\(P(\\theta \\in \\cdot)\\). While this subjectivism has been a topic of constant philosophical debate, we note that in the realm of decision theory, it is well known that the choices of a decision maker which abides by axioms of rationality can be explained as the result of a utility function and subjective probabilities assigned to events.\nWhile the debate surrounding these two schools of thought have continued for almost a century, it is prudent to consider the purpose for such theory. We are reminded of Laplace's prudent remark that \"Probability theory is nothing but common sense reduced to calculation\". Theory's merit ought to stem from the results it can provide for specific problems. espoused this viewpoint as their background in physics led to their interest in the use of probability to describe and predict phenomena of the physical world. Machine learning too is is rooted in predictions based on data produced by the physical world. Therefore, we argue that the merits of machine learning theory also ought to stem from its ability to describe and predict phenomena of data generated by the physical world. To this end, we believe that the results which we derive via our framework both better reflect what is observed empirically and are also general enough to unify many disparate areas of the field."}, {"title": "2.2 PAC Learning", "content": "The majority of existing theoretical results for the analysis of modern machine learning are set in the probably approximately correct (PAC) learning framework. In PAC learning, an algorithm is presented with a sample of data and is tasked with returning a hypothesis from a hypothesis set which can accurately perform predictions out-of-sample. The probably approximately correct come from the detail that these results are phrased as follows: \"For any data distribution, with probability at least \\(1-\\delta\\) over the randomness of an iid sample, the excess out-of-sample error is \\(< \\epsilon\\)\". \"Probably\" refers to the \\(1-\\delta\\) probability and \"approximately correct\" the \\(\\epsilon\\) tolerance of out-of-sample error. While this framework has facilitated the development of influential theoretical concepts such as VC dimension and Rademacher complexity , have demonstrated that these tools are inherently insufficient to explain modern empirical phenomena. Namely, they demonstrate empirically that while the Rademacher complexity of a deep neural networks leads to vacuous theoretical results, the observed out-of-sample error of these deep neural networks is actually small.\nWe posit that the looseness of these theoretical results stems from the fact that they hold for any data distribution and uniformly over the hypothesis set. While it is true that Rademacher complexity depends on the distribution of the inputs, it does not depend on the joint distribution of the input and outputs. Meanwhile, data which we observe from the real world clearly contains inherent structure between input and output which facilitate sample-efficient learning. Suppose we perform binary classification with input \\(X \\sim \\mathcal{N}(0, I_d)\\). In case 1, consider a data generating process in which the corresponding class \\(Y\\) only depends on the first element of \\(X\\). In case 2, consider a data generating process in which \\(Y\\) depends on all \\(d\\) elements of \\(X\\). Common sense would dictate that if we observed an equal number of samples from each data generating process and considered the same hypothesis spaces, the generalization error of case 1 ought to be lower than that of case 2. However, an analysis via VC dimension or Rademacher complexity would result in the same generalization bound for case 1 and 2. This problem exacerbated by high dimensional input distributions and overparameterized hypothesis classes, both of which are prevalent qualities of deep learning.\nSeveral lines of analysis have attempted to ameliorate this via data dependent bounds. While these results also hold for any data distribution, the choice of data distribution will impact the resulting error bound. Therefore, such a result will be vacuous (as expected) for a problem instance with unstructured data, but potentially much tighter for one which exhibits structure. The main frameworks for data dependent PAC results involve PAC Bayes and the information-theoretic framework of . Both frameworks involve an algorithm which produces a predictive distribution of the hypothesis conditioned on the observed data (hence they analyze a Bayesian algorithm under the PAC framework). The two only differ in that PAC Bayes results hold with high probability over random draws of the data while the information-theoretic results hold in expectation over random draws of the data. Therefore, each result upper bounds generalization error via the KL divergence or the mutual information between the observed data and the hypothesis.\nThese data dependent results mark a significant step in understanding the puzzling empirical success of deep learning. Notably, establish PAC-Bayes results for deep neural networks which result in bounds that dramatically improve upon those based on parameter count or VC dimension. These results reflect the importance that the data generating process has on the generalization error. However, a limitation is the lack of theoretical tools which facilitate analytic derivations. Namely, the aforementioned KL divergence/mutual information which bound generalization cannot be computed analytically outside of simple problem instances. This is because these quantities involve the posterior distribution of the hypothesis conditioned on the data, which cannot be expressed analytically outside of simple instances which exhibit a conjugate distribution. In contrast, our results analyze these quantities in a Bayesian setting, allowing us to develop general tools to bound mutual information analytically without needing to write down these complicated posterior distributions. The conciseness and generality of these results lead us to believe that they are fundamental."}, {"title": "2.3 Information Theory", "content": "The results of this work elucidate the tight relation between error in learning and information measures of Shannon's theory . Prior work establishes connections between information theory and parameter estimation notably through information-theoretic lower bounds. Namely, the global Fano method resembles a portion of the techniques which we will cover in this monograph.\nA widely known framework involving information theory and machine learning is the information bottleneck method . On the surface, this work exhibits many similarities to ours as it draws a connection between"}, {"title": "3 A Framework for Learning", "content": "In machine learning, we are interested in discerning the relationship between input and output pairs \\((X, Y)\\). Most frameworks of machine learning focus on a static dataset of fixed size and hope to characterize the performance of a predictive algorithm which leverages the information from the dataset for future predictions. However, any practical system will continually have access to additional observations as it interacts with the environment. As a result, it is prudent to consider a framework in which the data arrives in an online fashion and the objective is to perform well across all time.\nWe consider a stochastic process which generates a sequence \\(((X_t, Y_{t+1}) : t \\in \\mathbb{Z}_+)\\) of data pairs. For all \\(t\\), we let \\(H_t\\) denote the history \\((X_0, Y_1, ..., X_{t-1}, Y_t, X_t)\\) of experience. We assume that there exists an underlying latent variable \\(\\theta\\) such that \\((X_0, X_1,...)\\) \\(\\perp Y_1:t | \\theta\\) and \\(\\theta\\) prescribes a conditional probability measure \\(\\theta(\\cdot|H_t)\\) to the next label \\(Y_{t+1}\\). In the case of an iid data generating process, this conditional probability measure would only depend on \\(H_t\\) via \\(X_t\\). Furthermore, such a latent variable \\(\\theta\\) must exist under an infinite exchangability assumption on the sequence \\(((X_t, Y_{t+1}) : t \\in \\mathbb{Z}_+)\\) by de Finetti's Theorem. While we will first focus on this iid setting, we will also study learning setting in which the future data may be arbitrarily dependent on \\(H_t\\) even when conditioned on \\(\\theta\\). As our framework is Bayesian, we represent our uncertainty about \\(\\theta\\) by modeling it as a random variable with prior distribution \\(P(\\theta\\in\\cdot)\\).\nOur framework focuses on a particular notion of error which facilitates analysis via Shannon-information theory. For all \\(t \\in \\mathbb{Z}_+\\), our algorithm is tasked with providing a predictive distribution \\(P_t\\) of \\(Y_{t+1}\\) which may be derived from the history of data \\(H_t\\) which has already been observed. We denote such algorithm as \\(\\pi\\) for which for all \\(t\\), \\(P_t = \\pi(H_t)\\). As aforementioned, an effective learning system ought to leverage data as it becomes available. As a result, for any time horizon \\(T\\in\\mathbb{Z}_+\\), we are interested in quantifying the cumulative expected log-loss:\n\\[ L_{T, \\pi} = \\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E}_{\\pi} [ - \\ln P_t(Y_{t+1})]. \\]\nAs outlined in section 3.1, we take all random variables to be defined with respect to a common probability space. As a result, the expectation \\(\\mathbb{E}\\) integrates over all random variables which we do not condition on. We use the subscript \\(\\pi\\) in \\(\\mathbb{E}\\) to specify that it is a function of \\(\\pi\\) since for all \\(t\\), \\(\\pi\\) produces \\(P_t\\). As \\(Y_{t+1}\\) is the random variable which represents the next label that is generated by the underlying stochastic process, \\(P_t(Y_{t+1})\\) denotes the probability that our prediction \\(P_t\\) assigned to label \\(Y_{t+1}\\).\nThis loss function is commonly referred to in the literature as \u201clog-loss\" or \"negative log-likelihood\" and has become a cornerstone of classification methods via neural networks. However, it is important to note that even for an optimal algorithm, the minimum achievable log-loss is not 0. For instance, consider an omniscient algorithm in the classification setting which produces for all \\(t\\) the prediction \\(P_t = P(Y_{t+1} \\in \\cdot|\\theta, H_t)\\). Even this agent incurs a loss of:\n\\begin{aligned} &\\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E}_{\\pi} [-\\ln P(Y_{t+1}|\\theta, H_t)] = \\frac{1}{T} \\sum_{t=0}^{T-1} H(Y_{t+1}|\\theta, H_t)\\\\ & = \\frac{H(Y_{1:T}|\\theta, X_{0:T-1})}{T}, \\end{aligned}\nwhere our point follows from the fact that the conditional entropy of a discrete random variable \\(Y_{1:T}\\) is non-negative. As a result, we define the estimation error as:\n\\[ \\mathcal{L}_{T, \\pi} = L_{T, \\pi} - \\frac{H(Y_{1:T}|\\theta, X_{0:T-1})}{T} \\]\nEstimation error represents the error which is reducible via learning. As such, for a competent learning algorithm tasked with a learnable task, \\(\\mathcal{L}_{T, \\pi}\\) should decay to 0 as \\(T\\) goes to \\(\\infty\\)."}, {"title": "3.4 Achievable Error", "content": "Since we are interested in characterizing the limits of what is possible via machine learning, a natural question which arises is: For all \\(T\\), which \\(\\pi\\) minimizes \\(\\mathcal{L}_{T, \\pi}\\)? Since log-loss is a proper scoring rule, the optimal algorithm"}, {"title": "4 Requisite Information Theory", "content": "In this section we outline definitions and results from information theory which we will refer to in later sections of this monograph. For a comprehensive overview of the topic, we point the reader to.\nIn this text, \\(H(X)\\) denotes the entropy of a discrete random variable \\(X : \\Omega \\leftrightarrow \\mathcal{X}\\). His defined as follows:\n\\[ H(X) = \\sum_{x\\in\\mathcal{X}} P(X = x) \\ln \\frac{1}{P(X = x)} \\]\nThroughout this monograph, we use the convention that \\(0\\ln 0 = 0\\). While there are many colloquial interpretations of entropy which describe it as the expected \"surprise\" associated with outcomes of a random variable, we provide a concrete motivation for entropy based on coding theory.\nWe begin our exposition of entropy with an introduction to coding theory. We first define a code for a random variable.\nA code \\(C\\) for random variable \\(X : \\Omega \\rightarrow \\mathcal{X}\\) is a function which maps \\(\\mathcal{X} \\rightarrow \\{0,1\\}^*\\), where \\(\\{0,1\\}^*\\) denotes the set of all binary strings.\nWhen we send a text message to our friend, the characters that comprise of our message can be thought of as the realizations of random variables. In many applications involving digital data transfer, a message (outcome of a random variable) is encoded into a binary string which is passed through a communication channel, and decoded at an endpoint. Since the binary string arrives in a stream, it would be convenient if the message could be uniquely decoded as the data is arriving. A necessary and sufficient condition for this is online decodability is to design a code \\(C\\) which is prefix-free i.e. no element in the image of \\(C\\) is a prefix of another element in the image of \\(C\\). We use \\(\\mathcal{C}_X\\) to denote the set of prefix-free codes for a random variable \\(X\\)\nSince these codes are stored, transmitted, and decoded, the memory footprint becomes a significant design consideration. A natural question which arises is: \"how do we devise optimal prefix-free codes?\" The notion of optimality which gives rise to Shannon entropy is the following:\n\\[ \\underset{C\\in \\mathcal{C}_X}{\\text{arg min}} \\sum_{x\\in \\mathcal{X}} P(X = x) \\cdot \\frac{\\text{len} (C(x))}{\\log_2(e)}, \\]\nwhere \\(\\text{len}(c)\\) denotes the length of binary string \\(c\\). A prefix-free code which minimizes this objective will on-average require the fewest number of bits to store/transmit information. A naive prefix-free code is one which maps each of the \\(\\mathcal{X}\\) outcomes of \\(X\\) to a unique binary string of length \\(\\lceil \\log_2 |\\mathcal{X}| \\rceil\\). While such a code would be reasonable if all outcomes of \\(X\\) were equally likely, such a code would be highly suboptimal if some outcomes are much more/less likely than others. A competent coding scheme ought to map more likely outcomes to shorter strings and less likely outcomes to longer strings."}, {"title": "4.1 Entropy", "content": "The following result establishes the tight connection between the entropy of \\(X\\) and its optimal prefix-free code.\nFor all discrete random variables \\(X : \\Omega \\leftrightarrow \\mathcal{X}\\),\n\\[ H(X) \\leq \\underset{C\\in \\mathcal{C}_X}{\\text{min}} \\sum_{x\\in \\mathcal{X}} P(X = x) \\cdot \\frac{\\text{len}(C(x))}{\\log_2(e)} \\leq H(X) + \\frac{1}{\\log_2(e)} \\]\nThis result demonstrates that the entropy of a random variable tightly characterizes the fundamental limit to which it can be losslessly compressed. As a result, the entropy reflects the inherent complexity of a random variable. This connection is useful to keep in mind as there exist the following analogies between our Bayesian and the frequentist frameworks:"}, {"title": "4.2 Conditional Entropy", "content": "\\(H(X|Y)\\) denotes the conditional entropy of a discrete random variable \\(X : \\Omega \\rightarrow \\mathcal{X}\\) conditioned on another discrete random variable \\(Y : \\Omega \\rightarrow \\mathcal{Y}\\). The conditional entropy is defined as follows:\n\\[ H(X|Y) = \\sum_{x\\in\\mathcal{X}, y\\in\\mathcal{Y}} P(X = x, Y = y) \\ln \\frac{1}{P(X = x/Y = y)} \\]\nNote that unlike conditional expectation, conditional entropy is a number (and not a random variable). Upon closer inspection it is clear that conditional entropy is also always non-negative and is 0 only when \\(Y\\) fully determines \\(X\\). On the other hand, when \\(X \\perp Y\\), we have that \\(H(X|Y) = H(X)\\). We provide the following result which facilitates mathematical manipulations involving the information-theoretic quantities outlined thus far.\nFor all discrete random variables \\(X : \\Omega \\leftrightarrow \\mathcal{X}, Y : \\Omega \\leftrightarrow \\mathcal{Y}\\),\n\\[ H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y). \\]"}, {"title": "4.3 Mutual Information", "content": "\\(I(X; Y)\\) denotes the mutual information between two random variables \\(X : \\Omega \\leftrightarrow \\mathcal{X}\\) and \\(Y : \\Omega \\leftrightarrow \\mathcal{Y}\\). Concretely,\n\\[ I(X; Y) = d_{KL} (P((X, Y) \\in \\cdot)||P(X \\in \\cdot) \\otimes P(Y \\in \\cdot)), \\]\nwhere \\(P(X \\in \\cdot) \\otimes P(Y \\in \\cdot)\\) denotes the outer product distribution. Note that KL divergence is a non-symmetric function which maps two probability distributions to \\(\\mathbb{R}_+ \\cup \\{\\infty\\}\\). For discrete random variables, we have the following equivalence between mutual information and differences of (conditional) entropies:\n\\[ I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X). \\]\nNote that mutual information is symmetric i.e. \\(I(X; Y) = I(Y; X)\\) and it is also always non-negative (follows directly as a consequence of Lemma 10). Intuitively, the mutual information \\(I(X; Y)\\) represents the amount of information that \\(X\\) conveys about \\(Y\\) and vice versa. As such, if \\(X\\) fully determines \\(Y\\), then \\(I(X; Y) = H(X) = H(Y)\\). Meanwhile if \\(X\\perp Y\\), then \\(I(X; Y) = 0\\) as the two random variables convey no information about each other. As with conditional entropy, we provide the following result which facilitates mathematical analyses involving mutual information:\nFor all random variables \\(X : \\Omega \\leftrightarrow \\mathcal{X}, Y : \\Omega \\leftrightarrow \\mathcal{Y}, Z : \\Omega \\leftrightarrow \\mathcal{Z}\\),\n\\[ I(Y; Z|X) + I(X; Z) = I(X, Y; Z). \\]"}, {"title": "4.4 Differential Entropy", "content": "While we have defined information-theoretic quantities for discrete random variables, outside of mutual information we have not broached a notion of information regarding continuous random variables. For a continuous random variable \\(X : \\Omega \\rightarrow \\mathcal{X}\\) with density \\(p_X\\) (w.r.t the Lebesgue measure), we denote the differential entropy of \\(X\\) by\n\\[ h(X) = \\int_{x\\in \\mathcal{X}} \\ln \\frac{1}{p_X(x)} d\\mu(x), \\]"}, {"title": "5 Connecting Learning and Information Theory", "content": "In this section, we will leverage the requisite information-theoretic results of section 4 to derive general upper and lower bounds for the estimation error of the Bayesian posterior (\\(\\mathcal{L}_T\\)). The results of this section will facilitate the analysis of concrete problem instances in the following sections.\nWe now establish the elegant connection between \\(\\mathcal{L}_T\\) and mutual information.\nFor all \\(T \\in \\mathbb{Z}_+\\),\n\\[ \\mathcal{L}_T = \\frac{I(H_T; \\theta)}{T} \\]\nEvidently, an optimal algorithm never makes the same mistake twice, hence resulting in the equality between total loss incurred and total information acquired. Results of a similar flavor appear in the global Fano's method from the frequentist analysis of minimax lower bounds. However, the following connections to rate-distortion theory are novel.\nA natural question which may arise when inspecting Theorem 13 is: \"Does \\(\\mathcal{L}_T\\) decay to 0 in \\(T\\) and if so, at what rate?\". Ostensibly, the numerator \\(I(\\theta; H_T)\\) appears to be growing in \\(T\\), so it is not immediately obvious that even an optimal learner will experience vanishing error. A simple instance to initially consider is \\(\\theta\\) which is a discrete random variable. In such an instance, we can always provide the upper bound:\n\\[ \\mathcal{L}_T \\leq \\frac{H(\\theta)}{T} \\]\nTherefore, for any \\(\\theta\\) for which \\(H(\\theta) < \\infty\\), we have that \\(\\mathcal{L}_T = O(1/T)\\). However, what about a more realistic scenario in which \\(\\theta\\) is a continuous random variable? A naive idea would be to simply supplant the discrete entropy \\(H(\\theta)\\) with the differential entropy \\(h(\\theta)\\). However, while differences in conditional differential entropy equal mutual information (just as with discrete entropy), differential entropy does not upper bound mutual information. This is because differential entropy can be negative (as discussed in section 4.4). The appropriate extension of discrete entropy to continuous random variables can be establish via rate-distortion theory."}, {"title": "5.2 Characterizing Error via Rate-Distortion Theory", "content": "We begin with the definition of the rate-distortion function.\nLet \\(\\epsilon \\geq 0\\), \\(\\theta : \\Omega \\rightarrow \\Theta\\) be a random variable, and \\(\\rho\\) a distortion function which maps \\(\\theta\\) and another random variable \\(\\hat{\\theta}\\) to \\(\\mathbb{R}_+\\). The rate-distortion function evaluated for random variable \\(\\Theta\\) at tolerance \\(\\epsilon\\) is defined as:\n\\begin{aligned}\nH_{\\epsilon}(\\theta) = & \\underset{\\hat{\\theta} \\in \\Theta_{\\epsilon}}{\\text{inf}} I(\\theta; \\hat{\\theta}), \\\\\n &\\text{where} \\\\\n \\Theta_{\\epsilon} =& \\{ \\hat{\\theta} : \\mathbb{E} [\\rho(\\theta, \\hat{\\theta})] \\leq \\epsilon \\} \\\\\n\\end{aligned}\nIntuitively, one can think of \\(\\hat{\\theta}\\) as the result of passing \\(\\theta\\) through a noisy channel, resulting in a lossy compression. The objective \\(I(\\theta; \\hat{\\theta})\\), referred to as the rate, characterizes the number of nats that \\(\\hat{\\theta}\\) retains about \\(\\theta\\). meanwhile, the distortion function \\(\\rho\\) characterizes how lossy the compression is. The rate-distortion function returns the minimum number of nats necessary to achieve distortion at most \\(\\epsilon\\). In many ways this generalizes the concept of an \\(\\epsilon\\)-cover in frequentist statistics. For the application of rate-distortion theory to the analysis of estimation error in machine learning, we consider the following distortion function:\n\\[ \\mathbb{E} [\\rho_t(\\theta, \\hat{\\theta})] = \\mathbb{E} [ d_{KL} (P(Y_{t+1} \\in \\cdot|\\theta, H_t) || P(Y_{t+1} \\in \\cdot|\\hat{\\theta}, H_t)) ] \\]\n\\[ = I(Y_{t+1}; \\theta|\\hat{\\theta}, H_t), \\]\nwhere the second equality follows from the fact that \\(Y_{t+1} \\perp \\hat{\\theta}|(\\theta, H_t)\\). This restriction ensure that \\(\\hat{\\theta}\\) does not contain exogenous information about \\(Y_{t+1}\\) such as aleatoric noise which cannot be determined from \\((\\theta, H_t)\\). We use the notation \\(H_{\\epsilon,T}(\\theta)\\) to denote the following rate-distortion function:\n\\[ H_{\\epsilon,T}(\\theta) = \\underset{\\hat{\\theta} \\in \\Theta_{\\epsilon,T}}{\\text{inf}} I(\\theta, \\hat{\\theta}), \\]\n\\begin{aligned}\n&\\text{where} \\\\\n &\\Theta_{\\epsilon,T} =\\{\\hat{\\theta} : \\hat{\\theta} \\perp H_T | \\theta;  \\frac{1}{T} \\sum_{t=1}^T I(Y_{t+1}; \\theta|\\hat{\\theta}, H_t) < \\epsilon \\}.\\\\\n\\end{aligned}\nThe following result upper and lower bounds the optimal estimation error in terms of the rate-distortion function:"}, {"title": "6 Learning from iid Data", "content": "In this section, we restrict our attention to the analysis of learning under independently and identically distributed (iid) data. Concretely, we assume that the random process representing the inputs: \\((X_0, X_1, ...)\\) is iid. Each input \\(X_t\\) is associated with a label \\(Y_{t+1}\\) and we assume infinite exchangeability of the sequence \\(((X_0, Y_1), (X_1, Y_1), ...)\\). By de Finetti's theorem, there exists a latent random variable, which we denote by \\(\\theta\\), for which conditioned on \\(\\theta\\), the above sequence is iid. We further make the standard assumption that the sequence of inputs \\((X_0, X_1, ...)\\) is independent of \\(\\theta\\). As a result, \\(\\theta\\) is a random variable which represents for all \\(t\\) the conditional probability measure \\(\\theta(X_t)\\) of \\(Y_{t+1}\\). The process of learning involves the reduction of uncertainty about \\(\\theta\\) in ways which are relevant for future predictions.\nIn the following section, we provide several results which follow as a consequence of the above iid assumption. We dedicate the remaining sections to studying 4 concrete problem instances of increasing complexity: 1) linear regression, 2) logistic regression, 3) deep neural networks, 4) non-parametric learning. We hope that this suite of examples provide the reader with enough intuition and tools to analyze their own problems of interest."}, {"title": "6.1 Theoretical Results Tailored for iid Data", "content": "We begin with several intuitive theoretical results which follow as a result of the iid assumptions on the data. The first result establishes monotonicity of estimation error:\nIf \\(((X_t, Y_{t+1} : t \\in \\mathbb{Z}_+)\\) is an iid stochastic process when conditioned on \\(\\theta\\), then for all \\(t \\in \\mathbb{Z}_+\\),\n\\[ I(Y_{t+2}; \\theta|H_{t+1}) \\leq I(Y_{t+1};\\theta|H_{t}). \\]\nThe above result establishes that when the data generating process is iid conditioned on \\(\\theta\\), the optimal per-timestep estimation error is monotonically non-increasing. This is intuitive, if the data is iid, the future sequence is exchangeable and hence, conditioning on additional information (\\(H_{t+1\\) as opposed to \\(H_t)\\) should only improve our ability to make predictions. A corollary of this result is that for iid data, we can upper bound distortion via an expression which is much simpler to analyze:"}, {"title": "6.2 Linear Regression", "content": "In linear regression, the variable that we are interested in estimating is a random vector \\(\\theta\\in \\mathbb{R}^d\\). As our analytical tools are developed in a Bayesian framework, we assume a known prior distribution \\(P(\\theta \\in \\cdot)\\) to model our uncertainty over the value of \\(\\theta\\). For simplicity and concreteness, in this example, we assume that \\(P(\\theta \\in \\cdot) = \\mathcal{N}(0, I_{d/d})\\). For all \\(t \\in \\mathbb{Z}_+\\), inputs and outputs are generated according to a random vector \\(X_t \\stackrel{iid}{\\sim} \\mathcal{N}(0, I_d)\\) and\n\\[ Y_{t+1} = \\theta^T X_t + W_{t+1}, \\]\nwhere \\(W_t \\sim \\mathcal{N}(0, \\sigma^2)\\) for known variance \\(\\sigma^2\\). We note that the results and techniques developed in this section certainly extend to input and prior distributions which are not Gaussian with slight modifications. We study the Gaussian case as it minimizes ancillary clutter without compromising generality. In the following section, we will establish a series of smaller results which will allow us to streamline our error analysis using the tools established in section 5."}, {"title": "6.2.2 Theoretical Building Blocks", "content": "In this section we will establish 3 relatively simple results which will enable us to directly apply Theorem 15 and arrive at estimation error bounds for linear regression. As such, the results will all involve characterizing the rate-distortion function for this data generating process. We begin our analysis with a result which establishes the threshold \\(\\epsilon\\) at which the rate-distortion function is trivially 0.\nFor all \\(d \\in \\mathbb{Z}_{++}\\), \\(\\sigma^2 \\in \\mathbb{R}_{++}\\), if for all \\(t \\in \\mathbb{Z}_+\\), \\((X_t, Y_{t+1})\\) are generated according to the linear regression process and \\(\\epsilon > \\frac{1}{2} \\ln(1 + 1/\\sigma^2)\\), then for all \\(T \\in \\mathbb{Z}_{++}\\),\n\\[ H_{\\epsilon,T}(\\theta) = 0. \\]"}]}