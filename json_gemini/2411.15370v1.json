{"title": "Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers", "authors": ["Gautham Vasan", "Mohamed Elsayed", "Alireza Azimi", "Jiamin He", "Fahim Shariar", "Colin Bellinger", "Martha White", "A. Rupam Mahmood"], "abstract": "Modern deep policy gradient methods achieve effective performance on simulated robotic tasks, but they all require large replay buffers or expensive batch updates, or both, making them incompatible for real systems with resource-limited computers. We show that these methods fail catastrophically when limited to small replay buffers or during incremental learning, where updates only use the most recent sample without batch updates or a replay buffer. We propose a novel incremental deep policy gradient method Action Value Gradient (AVG) and a set of normalization and scaling techniques to address the challenges of instability in incremental learning. On robotic simulation benchmarks, we show that AVG is the only incremental method that learns effectively, often achieving final performance comparable to batch policy gradient methods. This advancement enabled us to show for the first time effective deep reinforcement learning with real robots using only incremental updates, employing a robotic manipulator and a mobile robot.", "sections": [{"title": "1 Introduction", "content": "Real-time or online learning is essential for intelligent agents to adapt to unforeseen changes in dynamic environments. However, real-time learning faces substantial challenges in many real-world systems, such as robots, due to limited onboard computational resources and storage capacity (Hayes and Kanan 2022, Wang et al. 2023, Michieli and Ozay 2023). The system must process observations, compute and execute actions, and learn from experience, all while adhering to strict computational and time constraints (Yuan and Mahmood 2022). For example, the Mars rover faces stringent limitations on its computational capabilities and storage capacity (Verma et al. 2023), constraining the system's ability to run computationally intensive algorithms onboard.\nDeep policy gradient methods have risen to prominence for their effectiveness in real-world control tasks, such as dexterous manipulation of a Rubik's cube (Akkaya et al. 2019), quadruped dribbling of a soccer ball (Ji et al. 2023), and magnetic control of tokamak plasmas (Degrave et al. 2022). These methods are typically used offline, such as in simulations, as they have steep resource requirements due to their use of large storage of past experience in a replay buffer, target networks and computationally intensive batch updates for learning. As a result, these methods are ill-suited for on-device learning and generally challenging to use for real-time learning. ese methods applicable to resource-limited computers such as edge devices, a natural approach is to reduce the replay buffer size, eliminate target networks, and use smaller batch updates that meet the resource constraints.\nIn Figure 1, we demonstrate using four MuJoCo tasks (Todorov et al. 2012) that the learning performance of batch policy gradient methods degrades substantially when the replay buffer size is"}, {"title": "2 Background", "content": "We consider the reinforcement learning setting where an agent-environment interaction is modeled as a continuous state and action space Markov Decision Process (MDP) (Sutton and Barto 2018). The state, action, and reward at timestep $t \\in (0,1,2, . . . )$ is denoted by $S_t \\in S$, $A_t \\in A$ and $R_{t+1} \\in R$ respectively. We focus on the episodic setting where the goal of the agent is to maximize the discounted return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$, where $\\gamma\\in [0,1]$ is a discount factor and $T$ is the episode horizon. The agent selects an action $A_t$ according to a policy $\\pi(\\cdot|S_t)$ where $\\pi(A|S)$"}, {"title": "3 The Action Value Gradient Method", "content": "In this section, we introduce a novel algorithm called Action Value Gradient (AVG, see Alg. 1), outlining its key components and functionality and briefly discussing its theoretical foundations. We also discuss additional design choices that are crucial for robust and effective policy learning. AVG uses RG estimation, extended to incorporate entropy-augmented value functions:\n$\\nabla_{\\theta}J(\\theta) \\propto E_{S \\sim d_{\\pi,\\gamma},A \\sim \\pi_{\\theta}} [\\nabla_{\\theta}f_{\\theta}(\\xi; S)|_{\\xi=h_{\\theta}(A;S)} \\nabla_A (q_{\\pi_{\\theta}}(S, A) - \\eta \\log (\\pi_{\\theta}(A|S)))].$ (1)\nA brief derivation of this statement is provided in Appendix A.\nThe AVG algorithm maintains a parameterized policy or actor $\\pi_{\\theta}(A|S)$ to sample actions from a continuous distribution and critic $Q_{\\phi}(S, A)$ that estimates the entropy-augmented action-value function. Both networks are parameterized using deep neural networks. AVG samples actions using the reparameterization technique (Kingma and Welling 2013), which allows the gradient to flow"}, {"title": "4 AVG on Simulated Benchmark Tasks", "content": "In this section, we demonstrate the superior performance of AVG compared to existing incremental learning methods. Specifically, we compare AVG against an existing incremental method IAC, which has demonstrated strong performance with linear function approximation in real-time learning across both simulated and real-world robot tasks (Degris et al. 2012, Vasan 2017, Vasan and Pilarski 2018). The implementation details can be found in Appendix E. Additionally, we evaluate AVG against incremental adaptations of SAC and TD3, both of which, like AVG, use RG estimation.\nSAC and TD3 rely on large replay buffers to store and replay past experiences, a crucial feature for tackling challenging benchmark tasks. To adapt these batch-based methods to an incremental setting, we set the minibatch and replay buffer size to 1, allowing them to process each experience as it is encountered. We refer to these incremental variants as SAC-1 and TD3-1, respectively. We use off-the-shelf implementations of TD3 and SAC provided by CleanRL (Huang et al. 2022b). The choice of hyper-parameters and full learning curves can be found in the Appendix F."}, {"title": "5 Stabilizing Incremental Policy Gradient Methods", "content": "In this section, we first highlight some issues with incremental policy gradient methods, which arise from the large and noisy gradients inherent to the setting. We perform a comprehensive ablation study to assess the effects of observation normalization, penultimate normalization, and TD error scaling\u2014individually and in combination\u2014on the performance of AVG. Additionally, we demonstrate how other incremental methods, such as IAC and SAC-1, may also benefit from normalization and scaling."}, {"title": "5.1 Instability Without Normalization", "content": "Deep RL can suffer from instability, often manifesting as high variance (Bjorck et al. 2022), reduced expressivity of neural networks over time (Nikishin et al. 2022, Sokar et al. 2023), or even a gradual drop in performance (Dohare et al. 2023, 2024, Elsayed and Mahmood 2024, Elsayed et al. 2024a), primarily due to the non-stationarity of data streams. Recently, Lyle et al. (2024) identified another common challenge that may induce difficulty in learning: large regression target scales. For instance, while training on Humanoid-v4, bootstrapped targets can range from -20 to 8000. Consequently, the critic faces the difficult task of accurately representing values that fluctuate widely across different stages of training. This can lead to excessively large TD errors, destabilizing the learning process."}, {"title": "5.2 Disentangling the Effects of Normalization and Scaling", "content": "A combination of three techniques consistently achieves good performance for AVG: 1) TD error scaling (Schaul et al. 2021) to resolve the issue of large bootstrapped target scale (termed scaled_td, 2) observation normalization to maintain good learning dynamics (termed norm_obs, and 3) penultimate normalization to reduce instability and improve plasticity (termed pnorm, Bjorck et al. 2022), similar"}, {"title": "5.3 AVG with Target Q-Networks", "content": "Target networks are commonly used in off-policy batch methods to stabilize learning (Mnih et al. 2015). By using a separate network that is updated less frequently, target networks introduce a delay in the propagation of value estimates. This delay can be advantageous in batch methods with large replay buffers, as it helps maintain a more stable target (Lillicrap et al. 2016, Fujimoto et al. 2018). However, this delayed update can slow down learning in online RL (Kim et al. 2019).\nIn Figure 8, we evaluate the impact of using target Q-networks with AVG. Similar to SAC, we use Polyak averaging to update the target Q-network: $\\phi_{target} = (1 - \\tau) \\cdot \\phi_{target} + \\tau \\cdot \\phi$. We run an experiment varying $\\tau$ between [0, 1], where $\\tau$ = 0 denotes a fixed target network and $\\tau$ = 1 implies the target network is identical to the current Q-network. We detail the pseudocode in Appendix C (see Alg. 4). The results show no benefit to using target networks, with only large values of $\\tau$ performing comparably to AVG. Additionally, removing target networks reduces memory usage and simplifies the implementation of our algorithm."}, {"title": "6 AVG with Resource-Constrained Robot Learning", "content": "On-device learning enables mobile robots to continuously improve, adapt to new data, and handle unforeseen situations, which is crucial for tasks like autonomous navigation and object recog-nition. Commercial robots, such as the iRobot Roomba, often use onboard de-vices with limited memory, ranging from microcontrollers with kilobytes of mem-ory to more powerful edge devices like the Jetson Nano 4GB. Leveraging these onboard edge devices can reduce the need for constant server communication, enhancing reliability in areas with limited connectivity. Storing large replay buffers on these devices is infeasible, necessitating computationally efficient, incremental algorithms.\nTo demonstrate the effectiveness of our proposed AVG algorithm for on-device incremental deep RL, we utilize the UR-Reacher-2 and Create-Mover tasks, as developed by Mahmood et al. (2018). We use two robots: UR5 robotic arm and iRobot Create 2, a hobbyist version of Roomba. In the UR-Reacher-2 task, the agent aims to reach arbitrary target positions on a 2D plane (Fig. 9a). This"}, {"title": "7 Conclusion", "content": "This work revives incremental policy gradient methods for deep RL and offers significant compu-tational advantages over standard batch methods for onboard robotic applications. We introduced a novel incremental algorithm called Action Value Gradient (AVG) and demonstrated its ability to consistently outperform other incremental and resource-constrained batch methods across a range of benchmark tasks. Crucially, we showed how normalization and scaling techniques enable AVG to achieve robust learning performance even on challenging high-dimensional control problems. Finally, we presented the first successful application of an incremental deep RL method learning control policies from scratch directly on physical robots\u2014a robotic manipulator and a mobile robot. Overall, our proposed AVG algorithm opens up new possibilities for deploying deep RL with limited onboard computational resources of robots, enabling lifelong learning and adaptation in the real world."}, {"title": "Limitations and Future Work", "content": "The main limitation of our approach is low sample efficiency compared to batch methods. Developing AVG with eligibility traces (Singh and Sutton 1996, van Hasselt et al. 2021) is a natural future direction to generalize our one-step AVG and possibly improve its sample efficiency. We also find that AVG can be sensitive to the choice of hyper-parameters. A valuable extension would be stabilizing the algorithm to perform well across environments using the same hyper-parameters. Our work is limited to continuous action space, but it can also be extended to discrete action spaces following Jang et al. (2017), which we leave to future work. Additionally, AVG omits discounting in the state distribution, which is common and further biases the update but can be addressed with the correction proposed by Che et al. (2023). Finally, we acknowledge a concurrent work by Elsayed et al. (2024b), which stabilizes existing incremental methods like AC(A) and Q(x), except for reparameterization policy gradient methods. The robustness of AVG may potentially improve by replacing Adam with an optimizer for adaptive step sizes proposed in that work."}, {"title": "A Theoretical Foundations", "content": "Please refer to the Theorems and Proofs section of Lan et al. (2022) for detailed proofs. We only provide a short proof sketch for reference.\nTheorem 1 (Reparameterization Policy Gradient Theorem). Given an MDP and a policy objective $J(\\theta) = \\int d_{\\theta}(s)v_{\\pi_{\\theta}}(s)ds$. The reparameterization policy gradient is given as\n$\\nabla_{\\theta}J(\\theta) = E_{S \\sim d_{\\pi,\\gamma},A \\sim \\pi_{\\theta}} [\\nabla_{\\theta}f_{\\theta}(\\xi;S)|_{\\xi=h_{\\theta}(A;S)} \\nabla_A q_{\\pi_{\\theta}}(S, A)]$.\nProof.\n$\\nabla_{\\theta}J(\\theta) = \\nabla_{\\theta} \\int d_{\\theta}(s) v_{\\pi_{\\theta}}(s)ds $\n$= \\nabla_{\\theta} \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s) \\pi_{\\theta}(a|s)q_{\\pi_{\\theta}}(s, a)da ds) ds$\n$= \\nabla_{\\theta} \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s) \\pi_{\\theta}(a|s)q_{\\pi_{\\theta}}(s, a)da ds) ds$ (by reparameterization)\n$= \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s)p(\\xi)q_{\\pi_{\\theta}} (s, f_{\\theta}(\\xi; s))d\\xi ds ds)$\n$= \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s)p(\\xi) \\nabla_{\\theta}f_{\\theta}(\\xi; s) \\nabla_a q_{\\pi_{\\theta}} (s,a)|_{a=f_{\\theta}(\\xi;s)}d\\xi ds ) ds$ (using chain rule)\n$= \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s) \\pi_{\\theta}(a|s) \\nabla_{\\theta}f_{\\theta}(\\xi; s)|_{\\xi=h_{\\theta}(a;s)} \\nabla_a q_{\\pi_{\\theta}} (s, a)da ds$ (by back substitution)\n$= E_{S \\sim d_{\\pi,\\gamma}, A \\sim \\pi_{\\theta}} [\\nabla_{\\theta}f_{\\theta}(\\xi; S)|_{\\xi=h_{\\theta}(A;S)} \\nabla_A q_{\\pi_{\\theta}}(S, A)] $."}, {"title": "A.2 Action Value Gradient Theorem", "content": "Theorem 2 (Action Value Gradient Theorem). Given an MDP and a policy objective $J(\\theta) = \\int d_{\\theta}(s)v^{ent}(s)ds$. The action value gradient is given as\n$\\nabla_{\\theta}J(\\theta) = E_{S \\sim d_{\\pi,\\gamma},A \\sim \\pi_{\\theta}} [\\nabla_{\\theta}f_{\\theta}(\\xi; S)|_{\\xi=h_{\\theta}(A;S)} \\nabla_A(q_{\\pi_{\\theta}}(S, A) - \\eta \\log \\pi_{\\theta}(A|S))]$.\nProof.\n$\\nabla_{\\theta}J(\\theta) = \\nabla_{\\theta} \\int d_{\\theta}(s) v^{ent}(s)ds$\n$= \\nabla_{\\theta} \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s) \\pi_{\\theta}(a|s)(q_{\\pi_{\\theta}}(s, a) - \\eta H(\\cdot|s))da ds$\n$= \\nabla_{\\theta} \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s) \\pi_{\\theta}(a|s)(q_{\\pi_{\\theta}}(s, a) - \\eta \\log \\pi_{\\theta}(a|s)da ds)$\n$= \\nabla_{\\theta} \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s)p(\\xi)(q_{\\pi_{\\theta}}(S, f_{\\theta}(\\xi; s)) - \\eta \\log \\pi(f_{\\theta} (\\xi; s)|s))d\\xi ds )$\n$= \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s)p(\\xi)\\nabla_{\\theta}(q_{\\pi_{\\theta}}(S, f_{\\theta}(\\xi; s)) - \\eta \\log \\pi(f_{\\theta} (\\xi; s)|s))d\\xi ds )$\n$= \\int d_{\\theta}(s) (\\int d_{\\pi,\\gamma}(s)p(\\xi)\\nabla_{\\theta}f_{\\theta}(\\xi; s) \\nabla_a(q_{\\pi_{\\theta}}(s, a) - \\eta \\log \\pi(f_{\\theta}(a|s))|_{a=f_{\\theta}(\\xi;s)}d\\xi ds )$\n$\\propto E_{S \\sim d_{\\pi,\\gamma},A \\sim \\pi_{\\theta}} [\\nabla_{\\theta}f_{\\theta}(\\xi; S)|_{\\xi=h_{\\theta}(A;S)} \\nabla_A(q_{\\pi_{\\theta}}(S, A) - \\eta \\log \\pi_{\\theta}(A|S))]$"}, {"title": "I Convergence Analysis for Reparameterization Gradient", "content": "In this section, we present a convergence analysis for reparameterization policy gradient (RPG) in (2), which is one of the main components in our proposed AVG. We analyze a slightly different variant of AVG, that we call RPG-TD, shown in Algorithm 7. We extend the convergence result from Xiong et al. (2022) for deterministic policies to the general case of reparameterized policies.\nLike AVG, RPG-TD uses the reparameterization gradient and updates one sample at a time, but it differs in that it does not have entropy regularization and normalizations. We also make a few typical theoretical assumptions, like i.i.d. sampling of transition tuples, that do not perfectly match the real setting for AVG. Following Xiong et al. (2022) and for analytical convenience, we use the stationary state distribution $d_{\\theta}(s) = lim_{t\\rightarrow\\infty}\\int s_0 + \\sum_{t=0} d_{\\theta}(s_0)p(s_0 \\rightarrow s, t, f_{\\theta}) ds_0$ for the critic update, and the discounted state visitation $v_{\\theta}(s) = \\int s_0 \\sum_{t=0}^{\\infty} \\gamma^t d_{\\theta}(s_0)p(s_0 \\rightarrow s, t, f_{\\theta}) ds_0$ for the actor update. Here, $f_{\\theta}(s, \\epsilon)$ denotes the reparameterized policy, and $p(s_0 \\rightarrow s, t, f_{\\theta})$ represents the density of state $s$ after $t$ steps from state $s_0$ following policy $f_{\\theta}$. Note that we follow the notations and language from Xiong et al. (2022) while avoiding changes as much as possible for easy comparison with the original result.\n$\\nabla J(\\theta) = \\int v_{\\theta}(s)p(\\epsilon)\\nabla_{\\epsilon}f_{\\theta}(s, \\epsilon)\\nabla_a Q_{f_{\\theta}} (s,a)|_{a=f_{\\theta}(s,\\epsilon)} d\\epsilon ds$\n$= E_{v_{\\theta},p} [\\nabla_{\\epsilon}f_{\\theta}(s, \\epsilon)\\nabla_a Q_{f_{\\theta}} (s,a)|_{a=f_{\\theta}(s,\\epsilon)}].$ (2)"}, {"title": "I.1 Convergence Result", "content": "We present the full set of assumptions below and refer interested reader to Xiong et al. (2022) for detailed discussions about these assumptions.\nAssumption 3. For any $\\theta_1$, $\\theta_2$, $\\theta \\in R^d$, there exist positive constants $L_f$, $L_{\\phi}$ and $\\delta_\\theta$, such that (1) $||f_{\\theta_1}(s, \\epsilon) - f_{\\theta_2}(s, \\epsilon)|| \\le L_f ||\\theta_1 - \\theta_2||,\\forall s \\in S, \\epsilon\\in R$; (2) $||\\nabla_{\\theta} f_{\\theta_1}(s, \\epsilon) - \\nabla_{\\theta} f_{\\theta_2}(s, \\epsilon)|| \\le L_\\phi ||\\theta_1 - \\theta_2||$, Vs \\in S, $\\epsilon \\in R$; (3) the matrix $\\Psi_\\theta := E_{v_{\\theta}, p} [\\nabla_{\\theta}f_{\\theta}(s, \\epsilon) \\nabla_{\\theta}f_{\\theta}(s, \\epsilon)^T]$ is non-singular with the minimal eigenvalue uniformly lower-bounded as $o_{min}(\\Psi_{\\theta}) \\ge \\lambda_{\\Psi}$."}, {"title": "B AVG Design Choices", "content": "Orthogonal Initialization helps improve the training stability and convergence speed of neural networks by ensuring that the weight matrix has orthogonal properties, thereby preserving the variance of the input through the layers (Saxe et al. 2013).\nSquashed Normal Policy SAC utilizes a squashed Normal, where the unbounded samples from a Normal distribution are passed through the tanh function to obtain bounded actions in the range [-1,1] : $A_{\\theta} = f_{\\theta}(\\epsilon; S) = tanh(\\mu_{\\theta}(S) + \\sigma_{\\theta}(S)\\epsilon)$ where $\\epsilon \\sim N(0, 1)$. This parameterization is useful for entropy-regularized RL objectives, which maximizes the return based on the maximum-entropy formulation. With an unbounded Normal policy, the standard deviation $\\sigma$ has a linear relationship with entropy. Hence, learning to maximize entropy can often result in very large values of $\\sigma$, potentially leading to behavior resembling a uniform random policy. In contrast, for a univariate squashed Normal with zero mean, increasing $\\sigma$ does not continuously maximize the entropy; it decreases after a certain threshold.\nEntropy Regularization Given that batch methods such as SAC benefit from entropy regularization, we consider variants of AVG with and without entropy regularization. There are two types of entropy terms that can be added to the actor, and critic updates: 1) distribution entropy: $H(\\pi(\\cdot|S))$, and 2) sample entropy: $\u2013 log(\\pi(A/S))$. We use sample entropy as our final choice in Algorithm 1, which utilizes sample entropy for the regularization of both the actor and Q-network.\nSimply increasing $\\sigma$ does not maximize the entropy of a univariate squashed Normal with zero mean (see Fig. 2). Increasing $\\sigma$ results in the probability density function (PDF) of a squashed Normal concen-trating at the edges (Fig. 11), resembling bang-bang control (Seyde et al. 2021)."}, {"title": "E Incremental Actor Critic", "content": "We consider the one-step actor-critic by Sutton and Barto (2018), where the actor (i.e., policy) and critic (i.e., value function) are updated incrementally, as new transitions are observed, rather than wait-ing for complete episodes or batches of data. We also drop the discount correc-tion term in actor updates since it often leads to poor performance empirically (Nota and Thomas 2020).\nWe also consider an entropy regulariza-tion term in the actor and critic objectives to encourage exploration and discourage premature convergence to a deterministic policy (Williams and Peng 1991, Mnih et al. 2016). In the following subsection, we examine both distribution entropy and sample entropy, finding that distribution entropy performs better empirically. The pseudocode for our implementation of IAC is detailed in Alg. 6."}, {"title": "F Hyper-parameter Settings in Simulation", "content": "Proximal Policy Optimization (PPO) Schulman et al. (2017) introduced PPO, an on-policy policy gradient method. It incorporates proximal optimization ideas to prevent large policy updates, improving stability through its carefully designed surrogate objective.\nWe use an off-the-shelf implementation of PPO from CleanRL that can be found here: https://github.com/vwxyzjn/cleanrl/blob/8cbca61360ef98660f149e3d76762350ce613323/ cleanrl/ppo_continuous_action.py\nTwin Delayed Deep Deterministic Policy Gradient (TD3) Fujimoto et al. (2018) introduced an off-policy algorithm that builds upon DDPG (Lillicrap et al. 2016) known as TD3. Both DDPG and TD3 utilize the reparameterization gradient, albeit for deterministic policies. They made three key modifications that resulted in better performance: (1) using two deep Q-networks to address overesti-mation bias, (2) delaying updates of the actor-network to reduce per-update error accumulation, and (3) adding noise to the target action used for computing the critic target values.\nWe use an off-the-shelf implementation of TD3 from CleanRL that can be found here: https://github.com/vwxyzjn/cleanrl/blob/8cbca61360ef98660f149e3d76762350ce613323/ cleanrl/td3_continuous_action.py\nSoft Actor-Critic (SAC) SAC is an off-policy algorithm which uses the reparametrization gradient along with entropy-augmented rewards (Haarnoja et al. 2018). While TD3 learns a deterministic policy, SAC learns a stochastic policy. TD3 adds noise to the target policy for exploration, whereas SAC's stochastic policy inherently explores by sampling actions from a distribution. We use an adaptation of Vasan et al. (2024) as our baseline SAC implementation."}]}