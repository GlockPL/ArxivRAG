{"title": "Taming Sensitive Weights: Noise Perturbation Fine-tuning for Robust LLM Quantization", "authors": ["Dongwei Wang", "Huanrui Yang"], "abstract": "Quantization is a critical step to enable efficient LLM serving under limited resource. However, previous research observes that certain weights in the LLM, known as outliers, are significantly sensitive to quantization noises. Existing quantization methods leave these outliers as floating points or higher precisions to retain performance, posting challenges on the efficient hardware deployment of the mixed-precision model. This work investigates an alternative way to tame the sensitive weights' impact on the quantization error, by reducing the loss Hessian trace with respect to outliers through an efficient fine-tuning process. We propose Noise Perturbation Fine-tuning (NPFT), which identifies outlier weights and add random weight perturbations on the outliers as the model going through a PEFT optimization. NPFT tames the sensitivity of outlier weights so that the quantized model performance can be improved without special treatment to the outliers. When applied to OPT and LLaMA models, our NPFT method achieves stable performance improvements for both uniform and non-uniform quantizers, while also offering better inference efficiency. Notably, the simplest RTN can achieve performance on par with GPTQ using our NPFT on LLaMA2-7B-4bits benchmark.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs), usually with billions of parameters, have demonstrated impressive problem-solving abilities across diverse tasks [3, 10, 21, 24]. The enhanced performance, largely driven by the scaling of both training data and model parameters [14], has made it challenging to deploy LLMs on edge computing devices. For example, a model like GPT-3 [11], with 175 billion parameters, requires 350 GB storage space in FP16 and powerful GPUs such as A100 for quick inference, which makes deployment on devices like laptops or mobile phones infeasible without significant model compression. As a promising approach, low-bit weight quantization can help address this issue by enabling efficient inference and reducing storage requirements. Pioneering works such as GPTQ [12] and Squeeze LLM [15] can compress LLaMA [22] model weights to 3-4 bits with a nearly lossless performance, achieving 2-3\u00d7 speed up on GPUs.\nThe most straightforward way to perform weight quantization is via linear uniform quantization, which quantizes the entire model to the same bit-width using simple quantizers like Round-to-Nearest (RTN). However, it is widely acknowledged that weights are not equally important in a neural network. There is a small fraction of weights that are very sensitive to quantization and lead to significant performance degradation if quantized. This is because these sensitive weights (also referred to as outliers) have a larger impact on the outputs of their respective layers, which in turn affects the final loss of the model."}, {"title": "Related Work", "content": "LLM Quantization. LLM quantization can be categorized into two branches: quantization-aware training (QAT) methods [2, 17, 9, 5] and post-training quantization (PTQ) methods [16, 15, 6, 12, 23]. QAT typically requires extensive retraining to recover accuracy after quantization, whereas PTQ does not involve retraining. Although QAT can enhance the performance of quantized models, it is not easily scalable to LLMs due to the significant computational resources required for retraining and the difficulties in convergence. Therefore, most works on LLM quantization focus on PTQ, which is also the focus of our work. The main difference between our proposed fine-tuning method and QAT lies in our objective: instead of training the quantized weights to recover the performance, we aim to regularize the sensitivity of the weights in the floating point model, making the model more suitable for different PTQ methods. Additionally, our method requires significantly less training time compared to QAT. For example, it only takes one hour of training on LLaMA2-7B.\nSensitivity-aware PTQ. There exists a small fraction of weights that are more sensitive to quantization. Quantizing them will lead to significant performance degradation. To address this, sensitivity-aware PTQ methods have been investigated. Some works focus on mitigating outlier activations. For example, [4] preserved outlier activations in floating-point format, while [23] established an outlier suppression framework that transfers outlier factors to other layers. Other works focus on outlier weights, which is also the issue we aim to mitigate in this paper. [6] proposed a hybrid sparse-quantized format where the outlier weights are kept in high precision. [15] also isolated outliers in a sparse FP16 matrix using a Hessian sensitivity-based non-uniform quantizer. [16] extracted the outliers based on activation distribution and performed per-channel scaling to reduce their quantization loss. Even though all these works have achieved promising results, it is important to note that they all place the quantized model in a mixed-precision state, which is unfavorable for hardware deployment. In this work, we aim to reduce the need for special handling of outliers by allowing the model to retain strong performance even when outliers are quantized to the same bit-width.\nHessian-aware Quantization. Previous works [8, 7] have shown that the Hessian eigenvalues of the loss function can be used as criteria to determine layer importance in designing mixed-precision quantization schemes. [25] also proved that the model robustness against quantization perturbation can be enhanced by regularizing Hessian eigenvalues. The use of the Hessian to assess the weight sensitivity, or Hessian regularization to improve quantization performance, has been extensively explored in CNN models. However, explicit Hessian regularizations are infeasible for LLMs due to their billions of parameters. In this work, we propose an efficient fine-tuning approach, which can reduce the Hessian trace while bypassing the expensive higher-order gradient calculations required to direct Hessian regularization."}, {"title": "Method", "content": "3.1 Identifying Outliers by Hessian Sensitivity\nNot all parameters in a neural network contribute equally. In previous works [15, 6], outliers are defined as weights that have a significant impact on the final loss after quantization. Typically, the sensitivity of an arbitrary entry $w_{i,j}$ in weight $W$ can be calculated as the induced loss increase:\n$S_{i,j} = L(W_q) - L(W)$ (1)\nwhere $L$ is the loss function and $W_q$ denotes the weight matrix where $w_{i,j}$ is quantized. We can use Taylor expansion to well approximate the loss increase under quantization as:\n$L(W_q) - L(W) \\approx g^T (W_q - W) + \\frac{1}{2}(W_q - W)^T H (W_q - W)$ (2)\nwhere $g \\in \\mathbb{R}^{d\\times1}$ and $H \\in \\mathbb{R}^{d\\times d}$ denote the gradient and Hessian of the loss with respect to $W$. $d$ denotes the number of parameters in the weight matrix. $W_q$ and $W$ are flattened into d \u00d7 1 vectors in this equation."}, {"title": "Efficient Hessian Regularization", "content": "To mitigate the loss increase brought by quantizing outliers, a straightforward way is to add a regularization term that can minimize the squared sum of the H's eigenvalues [25], thereby reducing the outliers' sensitivity. However, even with the Fisher approximation, directly regularizing H still requires the computation of higher-order gradients, which is computationally unfeasible for LLMs. In this paper, we propose an efficient approach for H regularization.\nFortunately, previous works have provided insights on computing Tr(H) without direct access to H. Based on the fast trace estimation algorithm proposed in [1], Tr(H) can be estimated using sampled random vector z \u2208 Rd whose components are i.i.d sampled from a distribution with zero mean and identity covariance matrix. Specifically, the estimation is derived as:\n$Tr(H) = Tr(HI) = Tr(H\\mathbb{E}_z[zz^T]) = \\mathbb{E}_z [z^T Hz]$ (4)\nwhere I is the identity matrix and \\mathbb{E}(\u00b7) is the expectation.\nConsidering the random vector z as a perturbation added to the converged weight matrix W, the expected loss increase induced by the weight perturbation can be approximated with Taylor expansion, similar to Equation (2), as\n$\\mathbb{E}_z[L(W + z) - L(W)] \\approx \\mathbb{E}_z [\\frac{1}{2}z^T Hz] = \\frac{1}{2} Tr(H)$ , (5)\nwhere the first-order term in the Taylor expansion is ignored given the convergence assumption.\nAs we focus on the weight sensitivity to quantization, we consider a distribution of weight perturbation z that can mimic the impact of quantization on weight values. Specifically, given the quantization bin width as \u0394, the round-to-nearest function will change $w_{i,j}$ by at most \u0394/2. This suggests that we can represent quantized weight Wq as W + \u03b4, where \u03b4 \u2208 Rd, ||\u03b4||\u221e < \u0394/2. Therefore, we approximate the distribution of \u03b4 with random weight perturbation z sampled from a zero-mean uniform distribution between [-\u0394/2, \u0394/2]. Note that the covariance of z is $\\frac{\\Delta^2}{12}I$, which is proportional to identity matrix I. We can therefore estimate the Hessian trace following the derivation in Equation (4) and (5) as\n$Tr(H) \\propto \\mathbb{E}_{z \\sim U[-\\Delta/2, \\Delta/2]}[L(W + z) - L(W)]$. (6)"}, {"title": "Noise Perturbation Fine-tuning", "content": "Following the conclusion in section 3.2, the goal of regularizing H is converted into minimize the expected loss under randomly sampled perturbation z. Following the stochastic gradient descent process, we add one independent sample of zi to the weight in each step of model fine-tuning to fulfill the expected loss computation. Our implementation adopts a two-phase setting as follows."}, {"title": "Evaluations", "content": "4.1 Experiment Setup\nModels and Datasets. We perform an extensive evaluation of NPFT across a range of models, including LLaMA [22] and OPT [26] models. We carry out language modeling evaluations using the C4 dataset [20] and the WikiText2 dataset [18].\nBaseline Methods. We compare NPFT against various methods PTQ methods including RTN, GPTQ [12], AWQ [16] and Squeeze LLM [15]. Unless otherwise mentioned, we use Squeeze LLM without outlier retention as baseline. To conduct fair comparison, we quantize the fine-tuned model using existing quantizers (both uniform and non-uniform) and ensure that the quantizer settings match those in the original works. The results in the experiments are either taken from the original paper or obtained by running its open-source code.\nEfficiency Profiling. We further compare the latency and memory usage of saving and not saving outliers as FP16 using sqLLM quantizer. Specifically, we measure the latency for generating 128"}, {"title": "Efficiency Profiling", "content": "Inference Efficiency. In Tab. 3, we present the latency and peak GPU memory usage when using the sqLLM quantizer, comparing scenarios with and without retaining full-precision outliers. The outliers are stored using the sparse format described in [15] and utilize the corresponding kernel for inference. When generating 128 tokens on a single 4090 GPU, the uniformly-quantized OPT-1.3B- 4bits model trained with NPFT achieves a 10% reduction in latency, lower CUDA memory usage, and demonstrates better performance compared to the model with outliers retained. A similar trend is observed in LLaMA2-7B-4bits model.\nTraining Efficiency. We also compare the computational overhead with the latest QAT method EfficientQAT [2], whose pipeline also follows a two-phase setting (Block-AP and E2E-QP). As shown in Tab. 4, NPFT's training time for LLaMA2-7B is approximately one-fourth that of EfficientQAT. It is worth noting that, unlike EfficientQAT, which requires dedicated training for each bit-width model, NPFT achieves optimization for multiple bit-width models in one-shot. This not only significantly simplifies the training procedure but also greatly enhances the efficiency of model quantization. NPFT requires significantly fewer samples and shorter sequence lengths for calibration data (shown in Tab. 5), enabling us to perform full-parameter fine-tuning of LLaMA2-7B on a single V100 GPU. The overhead of NPFT mainly comes from memory usage, as the model needs to be loaded into memory when calculating the noise. This memory consumption can be addressed through layer-wise computation in future work."}, {"title": "Ablation Study", "content": "Increasing Perturbation Ratio. In Fig. 4, we show the changes in model performance as the perturbation ratio increases. Note that all groups of the same model are controlled for the same"}, {"title": "Conclusion", "content": "This work introduces Noise Perturbation Fine-tuning (NPFT), an efficient method to reduce the sensitivity of outlier weights by applying random perturbations during fine-tuning. By reducing the loss Hessian trace, NPFT improves quantized model performance without requiring special treatment for outliers, enhancing both uniform and non-uniform quantizers. Experiments on OPT and LLaMA models demonstrate consistent performance gains and improved inference efficiency. Future work will focus on further optimizing NPFT for larger models and exploring its integration with other quantization techniques to enhance quantization robustness."}]}