{"title": "SPAM: SPIKE-AWARE ADAM WITH MOMENTUM RESET FOR STABLE LLM TRAINING", "authors": ["Tianjin Huang", "Ziquan Zhu", "Gaojie Jin", "Lu Liu", "Zhangyang Wang", "Shiwei Liu"], "abstract": "Large Language Models (LLMs) have demonstrated exceptional perfor- mance across diverse tasks, yet their training remains highly resource- intensive and susceptible to critical challenges such as training instability. A predominant source of this instability stems from gradient and loss spikes, which disrupt the learning process, often leading to costly interventions like checkpoint recovery and experiment restarts, further amplifying inefficien- cies. This paper presents a comprehensive investigation into gradient spikes observed during LLM training, revealing their prevalence across multiple architectures and datasets. Our analysis shows that these spikes can be up to 1000x larger than typical gradients, substantially deteriorating model performance. To address this issue, we propose Spike-Aware Adam with Momentum Reset (SPAM), a novel optimizer designed to counteract gra- dient spikes through momentum reset and spike-aware gradient clipping. Extensive experiments, including both pre-training and fine-tuning, demon- strate that SPAM consistently surpasses Adam and its variants across vari- ous tasks, including (1) LLM pre-training from 60M to 1B, (2) 4-bit LLM pre-training, (3) reinforcement learning, and (4) Time Series Forecasting. Additionally, SPAM facilitates memory-efficient training by enabling sparse momentum, where only a subset of momentum terms are maintained and updated. When operating under memory constraints, SPAM outperforms state-of-the-art memory-efficient optimizers such as GaLore and Adam- Mini. Our work underscores the importance of mitigating gradient spikes in LLM training and introduces an effective optimization strategy that en- hances both training stability and resource efficiency at scale. Code is available at https://github.com/TianjinYellow/SPAM-Optimizer.git.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have become fundamental in advancing state-of-the-art AI systems. Scaling LLMs, such as GPT-3 (Brown, 2020) and LLaMA (Touvron et al., 2023), has showcased unprecedented capabilities. However, training these large-scale models is fraught with challenges, particularly training instability. A major factor contributing to this instability is the occurrence of gradient and loss spikes during training, which disrupt the learning process at unpredictable intervals (Chowdhery et al., 2023; Zhang et al., 2022; Le Scao et al., 2023).\nWhile architectural innovations have been proposed to mitigate these issues (Nguyen &\nSalazar, 2019; Shoeybi et al., 2019; Zeng et al., 2022; Ding et al., 2021; Wang et al., 2024; Dettmers et al., 2021; Scao et al., 2022; Takase et al., 2023), none can completely prevent the occurrence of spikes. In practice, the most widely adopted solution is to manually intervene by restarting training from a previous checkpoint and skipping data affected by the spike (Chowdhery et al., 2023). This method is resource-intensive, requiring frequent checkpoint saves, manual monitoring, and repeated experiment runs - all inefficient and undesirable."}, {"title": "GRADIENT SPIKES", "content": "In this section, we formally define gradient spikes and then present the intriguing findings from our investigation into the training loss and gradient dynamics during LLM training.\nGradient spikes refer to a phenomenon that occurs during training where the magnitude of certain gradients significantly exceeds their historical values. To more precisely identify and analyze instances of gradient spikes, we introduce the Gradient Spike Score as a measurement of the deviation of a gradient's magnitude from its typical behavior over time. By quantifying this relative change, we can monitor the dynamics of gradients during training.\nDefinition 2.1 (Gradient Spike Score). Let {g0, g1, .., gT\u22121, gT} be the sequence of gradient obtained during the training process from time step 0 to T. The Spike Score of the gradient at the ith step, denoted as GSS(gi), is defined as the ratio of the magnitude of the gradient at that step to the average magnitude of the gradients across all steps:\nGSS(gi) = \\frac{|g_i|}{\\frac{1}{T+1} \\sum_{j=0}^{T} |g_j|}\nA gradient gi is considered a spiked gradient if its GSS(gi) exceeds a predetermined thresh- old \u03b8, i.e., GSS(gi) > \u03b8 indicating a significant increase from typical fluctuations, often amounting to increases of two or three orders of magnitude."}, {"title": "PRESENCE OF GRADIENT SPIKES DURING LLM TRAINING", "content": "Building upon the above concepts, we further explore the presence of gradient spikes during LLM training. Specifically, we monitor the gradients of the entire model over the initial 1,000 training steps and identify gradient spikes using the condition GSS(gi) > 50. Our investigation encompasses two widely adopted LLM architectures, LLAMA (Touvron et al.,\n2023)\u00b9 and Pythia (Biderman et al., 2023), with model sizes varying from 60M to 1B pa- rameters. Experiments were conducted on two datasets: the well-known C4 dataset (Raffel"}, {"title": "EFFECTS OF GRADIENT SPIKES ON LLM TRAINING", "content": "After identifying the presence of gradient spikes during training, a crucial question arises:\nare these gradient spikes detrimental or, perhaps counterintuitively, beneficial to the training of LLMs? To address this, we conducted a series of experiments as follows. Our findings confirm that gradient spikes are indeed harmful to LLM training, exerting prolonged negative effects on both the first and second moments, as discussed below.\nGradient spikes negatively impact LLM training. One direct way to assess the impact of gradient spikes is by nullifying the spiked gradients during training and observing the final training performance. We first detect spiked gradients using various thresholds \u03b8 and then set those gradients to zero. Figure 5-Left reports the results of LLaMA-60M on C4. Surprisingly, zeroing out these spiked gradients leads to improved model performance, evidenced by a reduction in perplexity. This observation clearly indicates that gradient spikes hinder effective training, and their removal is beneficial to overall model performance."}, {"title": "PRELINMINARY ANALYSIS WITH THEORY IMPLICATIONS", "content": "We hereby provide a very preliminary analysis to help probe why gradient spikes have a significant impact on the regret bound of Adam-like algorithms. We strictly follow the setting and notations used in Alacaoglu et al. (2020). Specifically, referring to Theorem 1 in the paper, the regret bound consists of two main terms:\nR(T) \u2264 \\frac{D^2 \\sqrt{VT}}{2\\alpha(1 - \\beta_1)} + \\frac{\\alpha \\sqrt{1 + \\log T}}{\\sqrt{(1 - \\beta_2)(1 - \\gamma)}} \\sum_{i=1}^d \\sqrt{\\sum_{t=1}^T g_{t,i}^2}\nwhere \u03b3 = \\frac{\\beta_2}{B_2}. Gradient spikes directly affect these terms by increasing the magnitudes of the gradients gt. In their Lemma 3, it is shown that the norm ||mt||2-1/2 depends on the accumulated gradients:\n||m_t||_{2-1/2} \\leq \\frac{(1 - \\beta_1)^2}{\\sqrt{(1 - \\beta_2)(1 - \\gamma)}} \\sqrt{\\sum_{i=1}^d (\\sum_{j=1}^{t} g_{j,i})^2}\nWhen gradient spikes occur, the values of gj,i become significantly larger for some j and i, which in turn increases the bound on ||mt||2-1/2. This enlargement propagates through the analysis, particularly affecting the accumulation term \\sum_{t=1}^T ||m_t||_{2-1/2} in their Lemma 4, which is bounded by:\n\\sum_{t=1}^T \\alpha_t ||m_t||_{2-1/2} \\leq \\frac{(1 - \\beta_1) \\alpha \\sqrt{1 + \\log T}}{\\sqrt{(1 - \\beta_2)(1 - \\gamma)}} \\sum_{i=1}^d \\sqrt{\\sum_{t=1}^T g_{t,i}^2}\nHere, gradient spikes increase \\sum_{t=1}^T g_{t,i}^2, significantly, especially in the coordinates where the spikes occur, leading to a larger bound.\nFinally, in the main regret bound (Equation (9) in the paper), these enlarged terms re- sult in a looser (larger) overall regret bound due to the presence of gradient spikes. The increased \\sum_{i=1}^d and \\sum_{t=1}^T g_{t,i}^2 directly contribute to the regret bound becoming less tight. This theoretical implication highlights that while adaptive algorithms like AMSGRAD ad- just learning rates based on gradient history, they may perform worse in terms of regret when large gradient spikes are present due to the increased cumulative squared gradients and decreased effective learning rate.\nIt is important to note that our goal is not to claim theoretical innovations, but rather to quantitatively assess how gradient spikes degrade Adam-like optimization, and that is only explored in a very limited context. We would like to clarify the limitations of this analysis: (1) The analysis assumes convexity, which may not apply in non-convex settings (but is often mitigated by assuming Polyak-Lojasiewicz condition or so). (2) The assumption ||gt||"}, {"title": "SPIKE-AWARE ADAM WITH WITH MOMENTUM RESET (SPAM)", "content": "In this section, we introduce Spike-Aware Adam with Momentum Reset (SPAM). Unlike previous solutions that introduce architectural innovations to mitigate the decremental ef- fects of gradient spikes (Nguyen & Salazar, 2019; Zeng et al., 2022; Dettmers et al., 2021; Takase et al., 2023), we attempt to address this issue from an optimization perspective. Concretely, we integrate Momentum Reset and Spike-Aware Clipping into Adam to deal with gradient spikes. In addition, we introduce a memory-efficient version of SPAM, which incorporates Sparse Momentum, significantly reducing the memory footprint during LLM training. Pseudocode of SPAM is in Algorithm 1.\nMomentum Reset. To mitigate the detrimental effects of gradient spikes on training stability, we introduce Momentum Reset. Momentum Reset involves periodically resetting the accumulated first and second moments used by adaptive optimizers such as Adam. These optimizers rely on exponential moving averages of past gradients to inform parameter updates. However, when a gradient spike occurs, it can significantly inflate these moments, causing the impact of the spike to persist over many subsequent iterations. By resetting the momentum terms at regular intervals of \u2206T training iterations, we can prevent the lingering influence of anomalously large gradients on the optimizer's state. This practice ensures that parameter updates are based on recent, more normal gradients rather than being skewed by gradient spikes. To mitigate potential instability caused by momentum reset, we perform N steps (N = 150 by default) of cosine warmup following each reset operation.\nSpike-Aware Clipping. To further mitigate gradient spikes during intervals, we introduce Spike-Aware Clipping. While our initial experiments indicate that setting spiked gradients to zero can enhance performance, this approach completely removes the learning signal for those parameters, including valuable directional information critical to the optimization process. To address this, SPAM identifies gradients that exceed a predefined threshold @ and scales them to a manageable value, preserving their directional information while controlling their magnitude.\nDetecting gradient spikes using GSS defined in Definition 2.1 would require knowing and storing all gradients in advance-a method that is impractical for LLM training due to mem- ory constraints. We adopt a more memory-efficient, on-the-fly approach by leveraging the components already calculated by Adam. Formally, we detect gradient spikes by identifying gradients gi that meet the following condition: G = {gi | \\frac{g_i^2}{v_i} > \\theta^2 }, where Vi is the second moment of Adam and \u03b8 is the threshold used for the approximate GSS \\approx \\frac{g_i^2}{v_i}. Note that we only use GSS defined in Definition 2.1 for the gradient spike analysis in Section 2. For real training, we employ the above approximation version. Since Vi is essentially the moving average of gi2 , this method efficiently identifies spikes without incurring additional overhead or the need to store the entire gradient history. Once detected, these spikes are clipped by scaling them to a manageable value. Specifically, for each spike gradient, we apply the operation: gi = sign(gi) \u221a\u03b8V. This technique is particularly useful when combined with Momentum Reset. By incorporating these strategies, SPAM effectively mitigates the negative impact of gradient spikes, improving training stability and performance.\nNote that unlike the Update Clipping used in Adafactor (Shazeer & Stern, 2018), which is applied to the whole weight update matrix when its Root Mean Square is larger than 1, our spike-aware clipping is directly applied to the spiked gradients gi whose magnitudes are significantly larger than its vi, e.g., > 50x.\nSparse Momentum. Momentum reset paves the way for the development of sparse mo- mentum, a technique designed to reduce memory usage and computation during the training of LLMs. In traditional momentum-based optimizers, such as Adam, momentum is updated and stored for all parameters, which can be memory-intensive for large-scale models. Sparse"}, {"title": "EXPERIMENTS", "content": "To demonstrate the efficacy of our proposed method, we conduct experiments on both pre-training and supervised fine-tuning using various sizes of the LLaMA model on the C4 dataset. Additionally, we evaluate its performance on Quantization-Aware Training, Reinforcement Learning, and Time Series Forecasting tasks.\nBaselines. We adopt several widely-used optimizers as our baselines. Since SPAM is built upon Adam, Adam serves as our most direct baseline. We also incorporate two common gra- dient clipping approaches with Adam: (1) Value Clip, which clips all gradients when their absolute value exceeds a threshold; and (2) Norm Clip, which scales the entire gradient if the L2 norm of the gradient vector exceeds a certain threshold. Additionally, we com- pare against another widely-used optimizer, Adafactor (Shazeer & Stern, 2018). In terms of spike mitigation techniques, we evaluate SPAM against previous approaches, including Scaled Initialization (Nguyen & Salazar, 2019; Shoeybi et al., 2019), Embed LN (Dettmers et al., 2021), Scaled Embed (Takase et al., 2023), and Embed Detach (Zeng et al., 2022). For memory-efficient optimization methods, we include Adam-Mini (Zhang et al., 2024a), GaLore (Zhao et al., 2024), LoRA (Hu et al., 2021), and ReLORA (Lialin et al., 2023a).\nArchitecture and hyperparameters. Following (Lialin et al., 2023a; Zhao et al., 2024), we conduct our experiments using the LLaMA-based architecture with various sizes from 60M to 1B parameters, incorporating RMSNorm (Shazeer, 2020) and SwiGLU activations (Zhang & Sennrich, 2019). For each model size, we use the same set of hyperparameters across methods, varying only the learning rate, where we sweep over a set of learning rates from le-4 to 1e-3, incrementing by 2e-4 for each optimizer. All experiments are conducted using the BF16 format. We set clip threshold as 1 and le 3 for Norm Clip and Value Clip, respectively, following the setting in Takase et al. (2023). We set hyper-parameters for Adafactor following the original paper (Shazeer & Stern, 2018) where \u20ac1 = 10-30, \u20ac2 = 10-3 and d = 1.0. For SPAM, we set reset intervals AT = 500, lr warmup step N = 150 and GSS threshold 0 = 5000. Detailed descriptions of our task setups and hyperparameters are provided in the Appendix D."}, {"title": "PERFORMANCE OF LLM PRE-TRAINING", "content": "Standard Pre-training. We report the training curves of various LLaMA models on the C4 dataset as well as the final perplexity in Figure 1 and Table 1, respectively. Overall, we observe that SPAM consistently achieves superior performance. As a memory-efficient approach, Adam-mini performs on par with Adam, consistent with the results reported"}, {"title": "PERFORMANCE ON LLM FINE-TUNING", "content": "In this section, we evaluate the effectiveness of SPAM for supervised fine-tuning. Following Li et al. (2024), we fine-tune LLaMA2-7B on Commonsense170K (Hu et al., 2023) and test on 8 downstream tasks. We do not apply layer-wise weight updates for GaLore and SPAM. The rank is set to 8 for all low-rank baselines. Correspondingly, the density of SPAM is set to 0.25% to maintain a comparable memory cost. The results are reported in Table 5. We observe that SPAM substantially outperforms other memory-efficient methods, exceeding full fine-tuning by a notable margin."}, {"title": "PERFORMENCE ON QUANTIZATION-AWARE TRAINING", "content": "We evaluate the effectiveness of SPAM for Quantization-Aware Training (QAT) across various quantization settings, including INT4 and INT8 for both weights and activations (W4A4, W8A8). Figure 1 (middle row) presents the training curves of LLaMA models with 60M, 130M, and 350M parameters on the C4 dataset. The results demonstrate that SPAM con- sistently outperforms Adam by a significant margin. This result highlights the promise of SPAM on low-bit training of LLMs."}, {"title": "PERFORMENCE ON REINFORCEMENT LEARNING", "content": "The effectiveness of SPAM is further assessed in classical reinforcement learning tasks, in- cluding the Ant-V4, HalfCheetah-V4, and Hopper-V2 environments from the MuJoCo suite. Figure 1 (bottom row) displays the test rewards averaged over five repeated experiments using the PPO algorithm (Schulman et al., 2017). The results indicate that SPAM con- sistently surpasses Adam across these environments, further validating its advantages in reinforcement learning settings."}, {"title": "PERFORMENCE ON TIME SERIES FORECASTING", "content": "We conducted additional experiments on time-series prediction tasks. In these experiments, we intentionally introduced anomalous data with a probability A=10% to simulate gradient"}, {"title": "PERFORMENCE ON VISION MODELS", "content": "We further evaluate SPAM on vision task. Specifically, we conducted experiments on\nImageNet-1K using ConvNeXt-Tiny (Liu et al., 2022b) and ViT-Tiny (Touvron et al., 2021). We adopt the default training recipe from the official code of ConvNeXT3 and train all mod- els for 120 epochs. We set AT = 25K, N = 20 and 0 = 5000 for SPAM. The results in Table 12 demonstrate that SPAM can achieve on par or better performance than vanilla AdamW."}, {"title": "ABLATION STUDY", "content": "Selection strategy for sparse momentum. Many strategies have been proposed to select subsets of parameters for sparse training, such as random selection (Liu et al., 2022a), max weight magnitude (Mocanu et al., 2018), and max gradient magnitude (Evci et al., 2020). Among these strategies, the most effective approach for sparse momentum training remains unclear. To investigate this, we conduct experiments with LLaMA-60M on the C4 dataset. The results are reported in Figure 7-(1). Interestingly, we find that randomly"}, {"title": "STATISTICS ANALYSIS OF GRADIENT SPIKES ACROSS VARIOUS TYPES OF LAYERS", "content": "It is important to examine whether gradient spikes exhibit a preference for certain layers.\nTo do so, we report the number of gradient spikes across various types of layers and the ratio of gradient spikes to the number of parameters in five types of layers: Embedding Layer, Attention Layer, FFN Layer, LayerNorm Layer, and LM_Head Layer. The experiments were conducted with LLaMA-60M on the C4 dataset, with gradient spikes detected over 1000 training steps. The detailed statistics are provided in Table 6. We observe the following:\n\u25cf The Embedding Layer exhibits the highest number of gradient spikes, also it has the largest parameter count. \u25cf The LayerNorm Layer, however, experiences an exceptionally high frequency of spikes, even with the smallest number of parameters."}, {"title": "LOCATIONS OF LOSS BUMPS AND GRADIENT SPIKES", "content": "To further investigate the correlation between loss bumps and gradient spikes, we present the locations of gradient spikes associated with the loss bumps in Table 7. The results reveal two key findings: \u25cf Gradient spikes are presented in different layers associated with the loss bump; \u25cf Gradient spikes typically occur before loss bumps, indicating that these gradient spikes may trigger loss bumps."}, {"title": "PSEUDOCODE", "content": "Algorithm 1: SPAM\nInput: A layer weight matrix w \u2208 Rm\u00d7n, learning rate a, decay rates \u03b2\u2081 = 0.9, \u03b22 = 0.999,\ninitial parameters wo, randomly initialize mask M with d density for each layer, the\nfirst moment m, the second moment v, threshold for GSS, momentum rerest interval\n\u2206T, warmup scale total steps N, small constant \u20ac = 1 \u00d7 10\u22126. T is total training steps.\nOutput: optimized parameters \u03c9\u03c4.\nwhile t < T do\nGet gt \u2208 Rmxn \u2190 -\u2207wot(wt) \u25b7Generate Gradients\nwarmup_scale = 1 - CosineAnnealing(Mod(t, \u0394T), N)\nif Mod (t, AT) = 0 then\nMrandom.rand(0.shape) < d\nm \u2190 zeros_like([M])\nv \u2190 zeros_like(0[M])\nSpike_M = gt [M] * *2 > 0 * v\nif sum(Spike_M) > 0 then\ngt[M][Spike_M] = sign(gn[M][Spike_M]) \u00b7 \u221a0 * v[Spike_M] \u25b7 Spike Gradients CLIP\nmt = B1mt-1 + (1-31) gt\nVt = B2Vt-1+ (1-B2)gt\nmt =\nmt\n1-\u03b2\nVt=\nV+e\nt = 1-B2\na * warmup_scale *\nWtWt-1\nt=t+1\nReturn: optimized parameters wr\n\u25b7 Random initialize the binary mask\n\u25b7 reset the first moment to zero\n\u25b7 reset the second moment to zero\n\u25b7 Detect spiked gradients"}, {"title": "ARCHITECTURE AND HYPERPARAMETERS", "content": "We introduce details of the LLaMA architecture and hyperparameters used for pre-training, following Lialin et al. (2023a); Zhao et al. (2024). Table 8 shows the most hyperparameters of LLaMA models across model sizes. We use a max sequence length of 256 for all models, with a batch size of 512, with a batch size of 131K tokens. For all experiments, we adopt learning rate warmup of 1000 training steps, and use cosine annealing for the learning rate schedule, decaying to 10% of the initial learning rate."}, {"title": "MORE ABLATION STUDY OF SUBSET SELECTIOIN STRATEGIES", "content": "Key questions surrounding sparse momentum include how to effectively select parameter subset and whether to retain momentum for weights that are sampled multiple times. To answer this questions, we conduct comparative studies based on LLaMA-60M and C4 and the results are shown in Figure 8. Figure 8-Left shows the performence of three subset selection strategies where we will reset all moments after each momentum reset and keep gradients for all unselected parameters. Figure 8-Middle shows the performence of three subset selection strategies where we will keep the overlapped moments after each momen- tum reset and keep gradients for all unselected parameters. Figure 8-Right shows the per- formence of three subset selection strategies where we will reset all the moments after each momentum reset and drop gradients for all unselected parameters in each updating step. We observe the following: \u25cf Among the three subset selection strategies-Max weight magnitude-based, Max gradient magnitude-based, and Random selection\u2014the Random se- lection consistently outperforms the other two approaches. \u25cf Comparing Figure 8-Left and Figure 8-Right, we see that resetting all moments after each momentum reset yields better performance than preserving overlapping moments."}, {"title": "PROLONGED DETRIMENTAL EFFECTS OF GRADIENT SPIKES DURING REAL TRAINING", "content": "We also measure the values of gradient, first moment, and second moment during the training of LLaMA-60M on the C4 dataset. The results are now presented in Figure 9.\nFrom the figure, we observe that during actual training, gradient spikes also have a significant and prolonged detrimental impact on moments, especially on the second moment, providing further evidence to support our claims."}, {"title": "SENSITIVITY ANALYSIS OF HYPERPARAMETER \u03b8 ON LLM ARCHITECTURES", "content": "We conducted experiments to evaluate the sensitivity of the gradient spike clipping thresh- old, \u03b8, across three widely used LLM architectures: LLaMA, Pythia, and OPT. These ex- periments were performed on pre-training tasks using the C4 dataset. The final perplexity is reported in Table 13.\nThe results indicate that the gradient spike clipping threshold is not highly sensitive to the choice of LLM architecture. SPAM consistently outperforms Adam across a wide range of \u03b8. Furthermore, the optimal range for \u03b8 lies between 1000 and 5000."}, {"title": "GSS VS. DISTRIBUTION BASED CLIPPING", "content": "We conducted an experiment using an outlier detection mechanism based on the assumption that stochastic gradient distributions follow a Gaussian distribution, as suggested in (Sim- sekli et al., 2019; Chaudhari & Soatto, 2018; Mandt et al., 2016):\nG_{batch} \\sim N(\\bar{G}, \\delta^2 I),\nwhere Gbatch is the stochastic gradient, \\bar{G} represents the gradient over the entire dataset, and \u03b42 is the variance. Since calculating \\bar{G} on-the-fly during training is computationally in- feasible, we approximate it using the moving average of Gbatch. The variance \u03b42 is estimated (n)\nonline as: \u03b42 = \\frac{1}{N} \\sum_{i=1}^N (G^{(i)}_{batch} - \\bar{G}^{(n)})^2, where N is the total training steps. Gradients are then evaluated element-wise, and any element G(n) satisfying: |G^{(n)}_{batch} - \\bar{G}^{(n)}| > 3\u03b4 is identified as an outlier. Such outlier elements are clipped to satisfy: |G^{(n)}_{batch} - \\bar{G}^{(n)}| = 3\u03b4.\nWe conducted experiments using LLaMA-60M and LLaMA-130M to evaluate the perfor- mance of this Gaussian-based Clipping and compare it with our proposed GSS-based clip- ping. The results are reported in Table 14. As the table indicates, Gaussian-based clipping falls short of our GSS-based clipping. One possible explanation is that stochastic gradient distributions are very complex and Gaussian distribution can not reflect the true distribu- tion."}, {"title": "GSS BASED CLIPPING VS. NULLIFYING", "content": "We conducted experiments on LLaMA-60M and LLaMA-130M to compare the performance of Spike-Aware Clipping and Nullifying Gradient Spikes. As shown in Table 15 and Table 16, SPAM with Spike-Aware Clipping outperforms SPAM with Nullifying on both pre-training and fine-tuning tasks, demonstrating the effectiveness of Spike-Aware Clipping."}, {"title": "COMPUTATIONAL ANALYSIS", "content": "We measured the running time per iteration for both LLaMA-60M and LLaMA-130M. The results, presented in Table 17, indicate that SPAM incurs a slightly higher computational overhead compared to Adam, Adam-mini, and Adafactor. This overhead is primarily due to the gradient spike detection operation and the gradient selection based on sparse masks. However, we believe that such a small overhead is negligible compared to the overall pre-training time which can be dozens or hundreds of hours."}, {"title": "ABLATIONS", "content": "SPAM introduces two key components: momentum reset and spike gradient clipping, in contrast to Adam. To evaluate the efficacy of each component, we perform ablation studies using SPAM without spike gradient clipping (SPAM w/o SGC) and SPAM without momentum reset (SPAM w/o MR) on the LLaMA-130M model. The results, presented in Figure 18, demonstrate that removing either component leads to a decline in performance, underscoring their effectiveness."}]}