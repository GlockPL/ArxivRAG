{"title": "Interactive Agents to Overcome Ambiguity in Software Engineering", "authors": ["Sanidhya Vijayvargiya", "Xuhui Zhou", "Akhila Yerukola", "Maarten Sap", "Graham Neubig"], "abstract": "AI agents are increasingly being deployed to automate tasks, often based on ambiguous and underspecified user instructions. Making unwarranted assumptions and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and wasted computational resources. In this work, we study the ability of LLM agents to handle ambiguous instructions in interactive code generation settings by evaluating proprietary and open-weight models on their performance across three key steps: (a) leveraging interactivity to improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c) asking targeted questions. Our findings reveal that models struggle to distinguish between well-specified and underspecified instructions. However, when models interact for underspecified inputs, they effectively obtain vital information from the user, leading to significant improvements in performance and underscoring the value of effective interaction. Our study highlights critical gaps in how current state-of-the-art models handle ambiguity in complex software engineering tasks and structures the evaluation into distinct steps to enable targeted improvements.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) are increasingly used as chatbots in task-oriented workflows to improve productivity (Peng et al., 2023; Brynjolfsson et al., 2023), with the user providing a task instruction which the model completes. Due to the interactive nature of chatbots, the performance depends on the information provided in the user's prompt. Users often provide non-descriptive instructions, which poses critical challenges in successfully completing the task (Chowdhury et al., 2024). The ambiguity can lead not only to erroneous outcomes, but also to significant safety issues (Kim et al., 2024; Karli & Fitzgerald, 2023).\nThis ambiguity can lead to more severe consequences in task automation scenarios, where AI agents are equipped with powerful tools (Wang et al., 2024b; Lu et al., 2024; Huang et al., 2024; Zhou et al., 2024a). In software engineering settings, agents must navigate complex codebases, make architectural decisions, and modify critical systems-all while operating with potentially incomplete or ambiguous instructions. When human developers face such ambiguity, they engage in clarifying dialogue to gather missing context (Testoni & Fern\u00e1ndez, 2024; Purver, 2004). However, current Al systems often proceed with incomplete understanding, leading to costly mistakes and misaligned solutions, as demonstrated in Figure 1.\nIn this work, we systematically evaluate the interaction capabilities of commonly used open and proprietary LLMs when addressing underspecified instructions in agentic code settings (\u00a72). We examine three research questions to address the problem for code generation."}, {"title": "1. Interactive problem solving:", "content": "Can LLMs appropriately leverage interaction with the user to improve performance in ambiguous settings?"}, {"title": "2. Detection of ambiguity:", "content": "Can LLMs identify whether a given task description is underspecified and ask clarifying questions?"}, {"title": "3. Question quality:", "content": "Can LLMs generate meaningful and targeted questions that gather the necessary information to complete the task?\nWe evaluate the research questions separately to ensure independence between them. We use the Github issues from SWE-Bench Verified (Chowdhury et al., 2024) to simulate well-specified inputs, and the summarized variants of the same Github issues as underspecified inputs for the experiments. A simulated user (Xu et al., 2024; Zhou et al., 2024b), equipped with the full, well-specified issue, simulates real conversations where the user has additional context, which is provided only when prompted with the appropriate questions. This multi-stage approach allows for targeted improvements in individual aspects, offering a pathway to enhance overall system performance.\nThrough our evaluations across the different settings, we find that interactivity can boost performance on underspecified inputs by up to 74% over the non-interactive settings but the performance varies between models (\u00a73). LLMs default to non-interactive behavior without explicit encouragement, and even with it, they struggle to distinguish between underspecified and well-specified inputs. Claude Sonnet 3.5 is the only evaluated LLM that achieves notable accuracy (84%) in making this distinction. Prompt engineering offers limited improvement, and its effectiveness varies across models (\u00a74). When interacting, LLMs generally pose questions capable of extracting relevant details, but some models, such as Llama 3.1 70B, fail to obtain sufficient specificity (\u00a75). In summary, this study underscores the importance of interactivity in LLMs for agentic workflows, particularly in real-world tasks where prompt quality varies significantly."}, {"title": "2. Method", "content": "2.1. Dataset\nIn our experiments, we simulate well-specified and underspecified inputs using the SWE-Bench Verified dataset, a refined subset of 500 issues from the SWE-Bench dataset. The SWE-Bench dataset (Jimenez et al., 2024) consists of real-world GitHub issues, their corresponding pull requests (PRs), and unit tests from 12 Python repositories. The SWE-Bench Verified dataset (Chowdhury et al., 2024) is designed to provide a more reliable estimate of an LLM's ability by pruning issues that were underspecified or contained invalid unit tests. The task of an LLM is to modify the state of the repository at the time of creation of the issue and resolve it. The test cases are used to verify the patch generated by the agent.\nGiven that the Verified subset contains only sufficiently specified issues, we assume that these issues do not require disambiguation. Therefore, for each SWE-Bench Verified issue, we consider two forms, as shown in Figure 2:\n1. Fully specified issue: The original and detailed GitHub issue.\n2. Underspecified issue: A summarized version generated using GPT-40, where the model is asked to preserve specific terminology is preserved but reduce the amount of detailed content (complete prompt in \u00a7A.1.3)."}, {"title": "2.2. Agentic Framework", "content": "Agent Environment The OpenHands (Wang et al., 2024b) agentic framework equips the LLM with an interactive environment that extends its capabilities beyond static code generation. The agent operates within a structured execution environment where it can iteratively refine code, plan tasks, and run commands using integrated tools. It has the ability to edit files, break down complex instructions into executable steps, and execute both Bash and Python scripts within a secure sandbox. This controlled environment enables the agent to analyze execution outputs, detect and debug errors, and refine its approach based on observed results, ensuring adaptability and correctness in solving complex programming tasks.\nSelected Models We use Claude Sonnet 3.5 (Anthropic, 2024b) as one of the proprietary models due to its superior performance on SWE-Bench. Claude Haiku 3.5 (Anthropic, 2024a) is included as the second proprietary model to investigate the impact of model parameterization, as both models likely share similar training methodologies but differ significantly in the number of parameters. Additionally, we evaluate Llama 3.1 70B-Instruct (Llama team, 2024) and Deepseek-v2 (DeepSeek-AI, 2024) as two open-weight frontier models.\nUser Proxy We employ GPT-40 (Ahmad & OpenAI, 2024) as a user proxy to simulate user-agent interactions (Xu et al., 2024; Zhou et al., 2024a). The user proxy is provided with the fully specified version of the task, allowing the coding agent to extract the necessary information through interaction. It is instructed to respond based solely on the information available in the full issue and will reply with I don't have that information if relevant details are missing. This approach prevents the user proxy from hallucinating incorrect information and encourages clear, negative responses when needed. The full prompt is shown in \u00a7A.1.2."}, {"title": "2.3. Study Design", "content": "We use three distinct settings to evaluate models across the 500 issues from SWE-Bench Verified shown in Figure 2 and described below.\n\u2022 Full Setting: This is the traditional SWE-Bench setting for resolving GitHub issues. The coding agent is provided with the fully specified task and the interaction is disabled. It represents the agent's performance in an unambiguous scenario, where the agent has access to full information, simulating ideal conditions.\n\u2022 Hidden setting: A summarized version of the issue is provided to the coding agent with the user-agent interaction disabled to mimic the lack of detail that can occur in task descriptions. We do not give any interaction-related instructions, and all models default to non-interactive behavior. Specific details are hidden from the coding agent.\n\u2022 Interaction Setting: The coding agent receives a summarized task, while the user proxy model gets the fully specified task. Interaction is enabled through prompting, allowing the agent to query the proxy for specific details. The models do not interact with the user without an explicit prompt. In addition to the full issue, the proxy has access to file locations that need modification and can provide them when queried. This setup allows us to evaluate which models proactively seek navigational information and examine how this interaction influences the success of the solution process across models."}, {"title": "3. RQ1: Interactive Problem Solving", "content": "Effectively addressing ambiguity requires a model to integrate information from user interactions to form a clear plan and successfully solve the task. Our first experiment holistically evaluates the model's ability to leverage interaction and improve performance. The model must not only process the initial task description, but also query users to extract relevant details while filtering out irrelevant information."}, {"title": "3.1. Experimental Setup", "content": "The hypothesis of the experiment is that different language models will exhibit varying performance with interaction based on their incorporation of the provided information, leading to different levels of improvement over the Hidden setting. We evaluate the models across the three settings and conduct two Wilcoxon-Signed Rank tests with a significance level of 0.05 to determine significant performance differences between the Hidden and Interaction settings, and between the Interaction and Full settings for every model. Here, we modify the prompt to make interaction with the user compulsory in the Interaction setting\u00b2. Ideally, the Interaction setting should approach the performance of the full setting. The coding agent has a maximum of 30 turns to generate a solution patch."}, {"title": "3.2. Leveraging Interaction in Ambiguity", "content": "In this experiment, each model is tested in the Hidden, Interaction, and Full settings to evaluate its ability to leverage interaction and optimize performance on underspecified issues. The results, as shown in Figure 3, confirm the expected increase in resolve rates as more information becomes available to the agent. While the difference between the Hidden and Interaction settings is significant for every model (Table 1), emphasizing the impact of interaction on the trajectory, the performance gap between the Interaction and Full settings is also significant across all models, highlighting the unrealized potential. Specifically, for the Hidden vs. Interaction settings, proprietary models show stronger evidence of a significant difference. These results suggest that the ability to leverage interaction varies across models, with proprietary models demonstrating greater effectiveness in utilizing interaction compared to open-weight models.\nUsing interaction, the Claude Sonnet and Haiku agents recreate 80% of the performance in the Full setting. However, with Deepseek and Llama 3.1, the relative performance is lower, of 59% and 54%, respectively. Claude Sonnet 3.5's high resolve rate in the Hidden setting is likely due to its superior programming acumen. The performance is surprising, as a human would be able to decipher little about the expectations given the summarized issue. Better programming models can potentially extract more information from the stack trace by reproducing the error themselves. We observe that the Claude Haiku model achieves a performance relative to the Full setting similar to that of the Claude Sonnet model, despite having inferior coding abilities. Thus, there is no direct correlation between the number of parameters or coding ability and a model's ability to leverage interaction. This hints towards better training practices that can lead to better integration of the new information."}, {"title": "3.3. Impact of Interaction Details on Model Performance", "content": "In the Interaction setting of the previous experiment, the information gained can be broadly categorized into two types: informational, which relates to the expected behavior or nature of the error, and navigational, which pertains to the locations of the files to modify. While informational details are typically obtained in nearly every interaction, the models request navigational details less frequently. We measure the resolve rates separately for instances where the model asks for navigational details and when it does not, examining the impact on performance when models must rely only on informational details versus when navigational details are also accessible.\nAs seen in Table 2, requesting navigational details improves performance across all models by providing cues beyond described behavior and errors. However, some models rely too heavily on this information and struggle when it's missing. Smaller models like Llama 3.1 and Deepseek-v2 request file locations more often but underperform without them. Claude models, particularly Sonnet, better leverage informational cues, achieving higher resolve rates even without navigational details. Deepseek, by contrast, performs worse than its Hidden setting when file locations are absent, highlighting its dependence. This reliance leads to wasted turns searching for errors instead of identifying them efficiently. Llama 3.1 performs better than Hidden without file locations but gains little when they are provided, likely due to poor detail extraction (Section \u00a75). Ideally, LLMs should generalize across diverse interaction types, as users may not always provide specific details, improving robustness in real-world software engineering tasks.\nTakeaway: Interaction has significant potential to improve model performance in ambiguous tasks, but models, particularly the less strong open-weight models, struggle to leverage it effectively. Proprietary models like Claude Sonnet 3.5 and Haiku 3.5 achieve nearly 80% of their Full setting performance, with Haiku improving by 74% over its Hidden setting performance through effective integration of both informational and navigational cues. The lack of correlation between model size and its ability to utilize interaction suggests that better training practices play a more crucial role. In contrast, models like Deepseek-v2 and Llama 3.1 show limited gains, primarily due to their challenges in utilizing broader informational cues, which hinders their adaptability in ambiguous tasks."}, {"title": "4. RQ2: Ambiguity Detection", "content": "In real-world LLM and agent applications, task descriptions and prompts can vary in quality (Chowdhury et al., 2024). To detect ambiguity, a model must recognize unclear expectations or identify missing key information in its planned approach. However, interacting unnecessarily when sufficient information is already available can introduce inefficiencies and place an undue burden on the user. Here, we examine the capabilities of LLMs to detect ambiguous instructions in software engineering contexts."}, {"title": "4.1. Experimental Setup", "content": "In this experiment, each issue is presented in either the Full setting or the Hidden setting. The objective is to identify patterns in how models choose to interact based on the input type. Ideally, the model should have a high interaction rate for the summarized inputs and a negligible interaction rate for the well-specified inputs.\nIn the instructions which outline the task, we present the agent with an option to interact during its solution trajectory and design three instructions with varying levels of encouragement to interact with the user. We track the input type the model chooses to interact with. The instructions, listed in order of increasing encouragement to interact, are: Neutral, where the agent is told it can ask questions if anything is unclear), Moderate Encouragement, where the agent is told to carefully check that all necessary information is available and only proceed after everything is clear, and Strong Encouragement, where the agent is told that asking questions is critical to task success (full prompts in \u00a7A)."}, {"title": "4.2. Effect of Different Prompts", "content": "Experiments to detect ambiguity demonstrate that, using prompt engineering, we can control the level of interaction with the user, as shown in Table 3. But this interactivity is not possible without clearly specifying it in the prompt wherein without any specific mention of interaction, the models almost never interact for any of the summarized issue inputs.\nThe Claude Sonnet model performs best with Moderate Encouragement, achieving the highest overall accuracy of 84% across all variations. Its counterpart from the same model family, Claude Haiku, is hesitant to interact even with Strong Encouragement. The Claude models show a drop in accuracy in cases where interaction is not needed as their overall interaction increases, indicating that the interaction fails to target underspecified inputs effectively. For the Deepseek model, we observe that the Neutral prompt gives the best results as interactivity surprisingly decreases with more encouragement. The accuracy in both the cases where interaction was desired and not desired is around 70%, which shows that the model is capable of distinguishing between well-specified and underspecified issues to some extent. The Llama model displays a greater, but arbitrary, tendency to interact across all prompts than other models."}, {"title": "4.3. Detection across Models", "content": "While interaction levels can be adjusted with prompting, both summarized issues and full issues have equal probability of being selected for interaction as interactivity increases, particularly with smaller models. Despite the stark difference in the language and detail of summarized issues and fully specified issues, the models, except Claude Sonnet, fail to reliably distinguish them, indicating that LLMs struggle to detect ambiguity even in obvious cases. All models, including Claude Sonnet, show big changes in the ambiguity detection behavior with prompt variations. Interestingly, Sonnet outperforms Haiku, likely due to its more extensive instruction tuning or Human Feedback training, which helps it better follow instructions and achieve the desired interactive trajectory. Surprisingly, even Deepseek adapts better to the task than Haiku.\nTakeaway: Prompt engineering can influence model interactivity but fails to consistently improve ambiguity detection across models. When interaction is not explicitly prompted, models default to non-interactive behavior. Claude Sonnet shows some ability to distinguish ambiguous inputs, but other models, including Claude Haiku and Llama 3.1, struggle even with clear cues. This inconsistency reveals that models are not inherently equipped to detect underspecified tasks. Improving ambiguity detection requires dedicated training, not just prompt modifications."}, {"title": "5. RQ3: Question Quality", "content": "To gather missing information from underspecified inputs, the quality of an agent's questions is crucial. While \u00a73 evaluates task completion, the model performance in the experiment is influenced by the coding ability. Here, we focus solely on the quality of the questions posed by the agent to the user, measuring how effectively models extract relevant information under the assumption that users have the necessary details."}, {"title": "5.1. Experimental Setup", "content": "In this experiment, we evaluate the quality of the interactions between the agent and the user in the Interaction setting. We measure the novelty and detail level of the information obtained from the user's answers to evaluate the quality, quantifying the new knowledge relative to the existing understanding of the agent. We employ two techniques to quantify the information obtained."}, {"title": "1. Cosine Distance:", "content": "We compute the cosine distance (1 \u2013 cos(P,Q)) between the embeddings of the summarized task Ebefore and the cumulative knowledge after interaction with the user Eafter using a text embedding model. Lower distances indicate redundant user input, while higher values show meaningful information gain. We use OpenAI's text-embedding-3-small as our embedding model."}, {"title": "2. LLM-as-judge (GPT-40):", "content": "The model scores the user answers on a scale of 1 to 5, where a higher score corresponds to more new and detailed information in the user's response, such as specific files causing errors or function behavior. The prompt to the model includes the summarized issue, agent questions, and user responses for better context."}, {"title": "5.2. Information Gain from Interaction", "content": "For the quantitative evaluation of the quality of the question, both the cosine distance and the LLM-as-judge methods suggest a similar result, in which the Llama model performs significantly worse than the other models, whereas the other models achieve very similar information gains, as seen in Figure 5.\nThe Llama model has an average cosine distance of 0.101 when the embedding of the summarized issue is compared to the embedding of the user response appended to the summarized issue. Deepseek achieves the highest cosine distance of 0.142, while the Claude Sonnet and Haiku models achieve very similar cosine distances of 0.136 and 0.135.\nUsing LLM as a judge, we evaluate the specificity of the details present in the answers. Here again, the Llama 3.1 model achieves a significantly worse average score of 3.58 than the other models which see similar performance of around 4 out of 5."}, {"title": "5.3. Qualitative Analysis of Questions", "content": "The quantitative results can be further supported by a qualitative evaluation of the questions. Sample question-answer pairs reflecting common trends are shown in Figure 4. The Llama model asks fewer questions on average than other models in one message for user interaction, as seen in Table 4, and often poses overly general questions like, Are there any existing workarounds or temporary fixes?. These template-like questions are unproductive and less likely to gather useful information.\nDeepseek, on the other hand, asks the most questions per message, allowing it to extract more information. Its questions, such as Are there any existing tests or examples that demonstrate the issue?, aim to extract, edge cases, documentation, or tests, and while common across multiple issues, they are reasonable and yield valuable details. But most questions are very specific and detailed, querying about the expected behavior. Often, due to the specificity of the question, the user might not have the required information.\nClaude Sonnet asks fewer questions than Deepseek, likely because it explores the codebase first. The questions do not have easily discernible patterns and match the Deepseek model in specificity. The Haiku model, in contrast, follows a consistent template, typically asking three questions regardless of the input, although sub-questions may be present. Haiku's questions are more keyword-driven based on the input, while Sonnet's are based on a deeper understanding of the issue and codebase.\nTakeaway: Models that balance specificity and question quantity, like the Claude models, achieve greater information gain and superior interaction quality compared to models that ask too few, too many, or templated questions. While Deepseek benefits from asking numerous detailed questions, it risks overwhelming the user. In contrast, Llama underperforms due to its reliance on generic or irrelevant questions."}, {"title": "6. Limitations", "content": "Our study benefits from including both open-weight and proprietary models, as well as models from the same family with different parameterizations, enhancing the generalizability of the findings. However, certain design decisions may affect the experiments.\nAmbiguity detection is limited to the first three turns, as LLMs struggle to interact meaningfully if they do not engage early. To assess question quality, we measure changes in the latent vector to capture the information gained, assuming equal importance for all new information-though models may prioritize different details in their solution. The resolve rates in the overall problem solving experiment reflect real-life conditions, where incorrect code is unacceptable, regardless of how close the generated patch is to the solution. However, data leakage could enable some models to make correct assumptions in underspecified settings, inflating resolve rates. Additionally, the user proxy may be more interactive than real-world users, as LLMs are tuned to be helpful. We address this by limiting the number of interaction turns and focusing interactions on the task with detailed system prompts."}, {"title": "7. Related Work", "content": "Code Generation Benchmarks In code generation tasks, ambiguous user instructions hinder the evaluation of code suggestions generated by the model. Since the cause of ambiguity is missing details, clarifying questions become neessary (Mu et al., 2023). Interactive, test-driven workflows mitigate this ambiguity by first generating test cases aligned with user expectations, which users validate before code generation (Lahiri et al., 2023). Extensions of this approach employ runtime techniques to generate, mutate, and rank candidate code suggestions and test cases based on user feedback (Fakhoury et al., 2024). Although effective, these workflows can burden users, highlighting the need to minimize intervention to essential cases.\nInteractive ML Systems In task-oriented settings, ambiguity between generated outputs and user expectations remains a challenge. AmbigNLG addresses this by introducing a taxonomy of instruction ambiguities and applying targeted disambiguation based on the identified ambiguity type (Niwa & Iso, 2024). These ambiguities include unclear output lengths, mandatory keywords, and contextual nuances in instructions. NoisyToolBench (Wang et al., 2024a) offers a dataset for evaluating LLM tool use with ambiguous instructions, though it focuses on simpler tasks. Reinforcement learning frameworks like ReHAC balance user interaction by modeling optimal intervention points (Feng et al., 2024), but more effective strategies are needed for complex, multi-step workflows.\nLLMs and Ambiguity The current state-of-the-art LLMs are not inherently trained to handle ambiguity through user interaction (Zhang et al., 2024), but, their instruction tuning enables improved performance with prompt engineering (White et al., 2023). Ambiguity detection has been tackled with uncertainty estimation to measure the utility of seeking clarification (Zhang & Choi, 2023; Park et al., 2024). Meanwhile, the quality of clarifying questions and the resulting performance remain critical to overall success (Rao & Daum\u00e9 III, 2018; Pyatkin et al., 2023; Kuhn et al., 2023). Despite advances, state-of-the-art techniques such as few-shot prompting and Chain-of-Thought reasoning offer limited relief in ambiguous scenarios (Zhang et al., 2024). Self-disambiguation uses the internal knowledge of a model to reduce query ambiguity (Keluskar et al., 2024; Sterner, 2022; Sumanathilaka et al., 2024). For example, Alignment with Perceived Ambiguity (APA) employs self-disambiguation to quantify perceived ambiguity using information gain, improving the model's processing of such inputs (Kim et al., 2024). Although inference-only methods are cost-effective, they are less robust than training-based approaches for handling ambiguity."}, {"title": "8. Conclusion", "content": "This work evaluates proprietary and open-weight models in agentic frameworks for handling ambiguity in software engineering. In code generation, to effectively integrate new information into the solution, an agent must detect ambiguity and ask targeted questions. Our key findings are:\n\u2022 Given an underspecified input, Claude Sonnet 3.5 and Claude Haiku 3.5 with interaction can achieve 80% of their performance with a well-specified input. In contrast, open-weight models struggle: Deepseek relies on navigational cues to locate relevant files, while Llama 3.1 70B extracts limited information from the user.\n\u2022 LLMs do not interact unless explicitly prompted, and their ambiguity detection is highly sensitive to prompt variations. Only Claude Sonnet 3.5 achieves a higher accuracy of 84% in distinguishing between well-specified and underspecified input.\n\u2022 Claude Sonnet 3.5, Haiku 3.5, and Deepseek effectively extract new, detailed user information, whereas Llama 3.1 struggles to ask the right questions.\nDespite these advances, a gap remains between resolve rates for underspecified vs. fully specified issues. Open-weight models need better interaction strategies to improve resolution, while proprietary models, particularly Claude Haiku 3.5, require stronger prompting to engage interactively. This work establishes the current state-of-the-art in handling ambiguity through interaction, breaking the resolution process into multiple steps."}, {"title": "Impact Statement", "content": "This paper examines the ability of AI agents to handle ambiguous and underspecified instructions in software engineering. Our findings highlight key limitations in the resolution of AI-driven tasks in real-world applications, necessitating the development of more interactive, adaptable, and effective systems. By emphasizing AI interactivity, our work contributes to the responsible development of AI technologies, fostering safer and more efficient automation in software engineering."}, {"title": "A.1.2. INTERACTION SETTING", "content": "In this setting, the user proxy agent receives both the fully specified issue and additional hints, maintaining the knowledge gap relative to the Hidden setting. This provides extra information for the coding agent to extract through interaction.\nThe files to be modified are also provided to the user proxy agent, allowing us to track specific details across issues. Since file-related information is universally useful\u2014unlike other details whose importance may be subjective\u2014it enables evaluation of how effectively different models incorporate critical information into their solution paths.\nThis setup reflects a scenario where the user might know additional details not included in their initial input, which can still be extracted to improve performance. While more capable models may independently retrieve this information by exploring the codebase, it can be particularly helpful for lower-performing models. By tracking which models choose to extract this information, we gain insights into the types of questions they ask and observe behavioral trends across models."}, {"title": "A.1.3. HIDDEN SETTING", "content": "I've uploaded a Python code repository in the directory /workspace/{workspace_dir_name}. Consider the\nfollowing PR description: {instance.summarized_issue}\nCan you help me implement the necessary changes to the repository so that the requirements specified in the PR\ndescription are met?\nI've already taken care of all changes to any of the test files described in the PR description. This means you DON'T\nneed to modify the testing logic or any of the tests!\nYour task is to make minimal changes to non-test files in the repository to ensure the PR description is satisfied.\nFollow these steps to resolve the issue:\n1. As a first step, it might be a good idea to explore the repo to familiarize yourself with its structure.\n2. Create a script to reproduce the error and execute it with python  using the BashTool to\nconfirm the error.\n3. Edit the source code in the repo to resolve the issue.\n4. Rerun your reproduce script to confirm the error is fixed.\n5. Consider edge cases and make sure your fix handles them as well.\nYour thinking should be thorough, and it's fine if it's very long."}, {"title": "A.2. Statistical Methods", "content": "A.2.1. WILCOXON SIGNED-RANK TEST\nThe Wilcoxon Signed-Rank Test is a non-parametric statistical test used to determine if there is a significant difference between the medians of two related groups. Unlike the paired t-test, it does not assume that the differences between paired observations are normally distributed, making it more suitable for cases where this assumption may not hold.\nIn this work, the Wilcoxon Signed-Rank Test is applied to compare the performance of models between two settings (e.g., Hidden vs. Interaction, Interaction vs. Full) with the hypothesis that performance in the second setting is greater than in the first.\nFormally, the null hypothesis (H0) for the Wilcoxon Signed-Rank Test states that the median difference between the two settings is zero or negative:\n$H_o : d \\le 0$\nwhere d represents the median of the paired differences. The alternative hypothesis (H\u2081) asserts that the median difference is greater than zero:\n$H_1: d > 0$\nThe test ranks the absolute differences between paired observations, considering both the magnitude and direction of change."}, {"title": "A.3. Ambiguity Detection prompts", "content": "\u2022 Neutral: Ensure you have all the necessary information to proceed. If any part of the issue is unclear or lacks critical details, ask concise, targeted questions to clarify. If everything is clear, you can move ahead without asking unnecessary questions.\n\u2022 Moderate Encouragement: Before attempting a solution, carefully check whether all key information is provided. If there's any ambiguity or missing details that could impact your work, don't hesitate to ask questions. Your goal is to gather the information needed for an accurate and efficient solution. Only skip asking questions when you are absolutely sure all details are complete.\n\u2022 Strong Encouragement: Your success depends on having all relevant details to solve the issue effectively. Whenever you encounter unclear or missing information, proactively ask questions to fill those gaps. Even minor ambiguities can affect the outcome, so always prioritize clarifying questions. Avoid questions only when you are 100% certain no further clarification is needed."}, {"title": "A.4. Question Quality Analysis", "content": "Cosine Distance(P, Q) = 1 \u2013$\\\\frac{PQ}{||P||||Q||}$\nwhere:\n\u2022 P = {$P_1, P_2, ..., P_N$ } represents the embedding vector of the updated knowledge ($E_{after}$).\n\u2022 Q = {$q_1, q_2,..., q_N$ } represents the embedding vector of the initial knowledge ($E_{before}$).\n\u2022 N = 1536 is the dimensionality of the embedding space."}]}