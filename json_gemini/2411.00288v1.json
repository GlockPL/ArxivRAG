{"title": "Inducing Semi-Structured Sparsity by Masking for Efficient Model Inference in Convolutional Networks", "authors": ["David A. Danhofer"], "abstract": "The crucial role of convolutional models, both as standalone vision models and backbones in foundation models, necessitates effective acceleration techniques. This paper proposes a novel method to learn semi-structured sparsity patterns for convolution kernels in the form of maskings enabling the utilization of readily available hardware accelerations. The approach accelerates convolutional models more than two-fold during inference without decreasing model performance. At the same time, the original model weights and structure remain unchanged keeping the model thus easily updatable. Beyond the immediate practical use, the effect of maskings on prediction is easily quantifiable. Therefore, guarantees on model predictions under maskings are derived showing stability bounds for learned maskings even after updating the original underlying model.", "sections": [{"title": "Introduction", "content": "The increasing complexity of deep learning models [21], their deployment in applications [5], and the adoption of reflection incurring several inference passes per query, e.g., as in the O1 models from the GPT family [3], shifts the relative amounts of resources spent during the model lifetime from the training to the inference stage [7, 35]. It therefore becomes imperative to make models more efficient [46]. One way of achieving this is by spending a comparatively small, additional share of resources during training to learn a one-time modification of the model that lowers the model's inference and thus lifetime cost [30, 40]. First and foremost, such a modification is effective if it decreases the model's computational and time cost at a relatively low additional training overhead while not affecting the prediction performance of the model negatively [22]. Additionally, there are other desirable properties of such one-time modifications: From an application perspective the achievable gain in efficiency is only useful if it can be leveraged easily, a well-known challenge, e.g., with sparsifying models [8, 15]. Taking into consideration the increasing popularity of large, expensive to train, foundation models [16] or models employed in an online setting subject to continuous updates the proposed change should not affect the possibility to update the model, e.g., by changing the weights or architecture underlying the model. Ideally, if such a model is updated, the learned modification can even be reused under the constraint of the magnitude of change imposed by updating the model.\nSemi-structured sparse maskings satisfy the above properties by replacing the dense matrix operations usually required during inference by cheaper and faster operations on semi-structured sparse matrices [4]. While many works have demonstrated that sparse (pruned) submodels can solve the same task at almost no loss of performance [2, 26] the sparsity of the models does not necessarily have to adhere to a specific pattern making it difficult to turn theoretically obtained computational speedups by saving on data loading and computational operations into practical efficiency gains [14]. Regular"}, {"title": "Related Work", "content": "In the following the notion and adoption of semi-structured sparsity is introduced. Then, the implications on prediction performance and the computational challenges of sparsifying Convolutional Neural Networks (CNNs) are highlighted.\nSemi-Structured Sparsity Semi-structured sparsity to accelerate network inference has been introduced in [37] as N:M-sparsity requiring N out of M elements of a contiguous block to be zero (s. a. 3.1). Beyond the general case of N:M sparsity, the practically interesting special case of 2:4 sparsity has been considered in more detail [17, 18] in which exactly half of the weights are pruned as illustrated in Figure 1. This setting enables hardware acceleration via NVIDIA sparse tensor cores available from the NVIDIA Ampere architecture on via the TensorRT v8.0 library [36]. Since half the elements are zeroed out and thus negligible, the amount of data to load from memory is almost halved with the number of Floating Point Operations (FLOPs) needed to conduct an operation on the sparse matrix also decreasing, e.g., linearly for addition and element-wise operations and super-linearly for multiplication, decomposition etc. [44]. This way 2:4 sparse matrix operations compute the same effective operation while reducing the time needed by a factor of two [36]. The difficulty in turning a dense matrix into a 2:4 sparse matrix, however, lies in selecting the most useful two of the four weights in each quadruple. To this end [37] propose a permutation regime that allows for preserving the weights based on magnitude and assess the found pattern via a scoring mechanism, the efficacy score. The functionality is available via NVIDIA's Apex library [33]. Notably, pruning via Apex requires finetuning the network again after pruning to achieve an inference performance comparable to that of the dense network in CV tasks, e.g., classification [36, 37], and therefore changes the original pretrained weights.\nSparsity in CNNs The state-of-the-art performance of CNNs in image-based and other tasks comes at the cost of a large memory footprint and computational cost. Pruning to obtain sparse networks is therefore a popular technique to decrease the computational and storage cost of deep neural networks [2]. Pruning techniques include magnitude-based pruning [12], iterative pruning [42], and dynamic pruning [24]. Although theoretically any sparsity reduces the computational costs of such networks, irregularity in the sparsity patterns makes it difficult to map the required computations to (parallel) processor operations [19]. Even extremely high levels of (irregular) sparsity, i.e., > 97%, often yield no inference acceleration suffering from lack of library support [8, 41]. As visualized in Figure 2, in"}, {"title": "Methods", "content": "In the following, the concept of modeling semi-structured sparsity in a network architecture and its effects on classification are introduced. Then, its application to convolutions is detailed out."}, {"title": "Modeling Semi-Structured Sparsity", "content": "N:M sparsity divides a matrix into non-overlapping blocks of M contiguous elements requiring that N of these elements be zero as these elements can subsequently be ignored in many matrix operations, e.g., addition or multiplication. There are exactly \\( n = \\binom{M}{N} = \\frac{M!}{(M - N)!N!} \\) ways of choosing N of the M elements without replacement and ignoring permutations, yielding n unique sparsity patterns. Selecting one of the n patterns can be modeled via a categorical variable \\( z \\) with class probabilities \\( \\pi_1, ..., \\pi_n \\) S.t. each probability denotes the probability of selecting the corresponding N:M sparsity pattern. Sampling the choice vector \\( z \\), a n-dimensional one-hot vector on the simplex \\( \\Delta^{n-1} \\), from such a categorical distribution can be performed efficiently via the Gumbel-Max trick [11]\n\\[ z = \\text{onehot}(\\underset{i}{\\text{arg max}}[g_i + \\log \\pi_i]) \\]\nwhere \\( g_i \\sim \\text{Gumbel}(0, 1) \\). Aggregating all n N:M sparsity patterns as column vectors in a pattern matrix \\( D \\in \\{0,1\\}^{N \\times n} \\) allows for constructing the (row major) semi-structured sparse mask M of"}, {"title": "Effects of Maskings on Classifier Class Predictions", "content": "Understanding how sparsity-inducing maskings affect a classifier's predictions is crucial to effectively trade off inference acceleration and potential performance inhibitions. The following Lemma contains the classifier definition and states a useful property. This definition is used in all subsequent theoretical results and models the architectures considered for experiments closely. All proofs in this section are deferred to Appendix A.\nLemma 3.1. Let \\( f(x) = (\\text{softmax} \\circ f_d \\circ ... \\circ f_1)(x) \\) be a compositional classifier of depth d with \\( f_i(x) = \\sigma(W_i x + b_i) \\) predicting the class probabilities of an input sample \\( x \\in X \\) across c classes. Let \\( \\sigma \\) be a non-linear element-wise activation function and L-Lipschitz. Then \\( f(x) \\) is \\( L_f \\)-Lipschitz. with \\( L_f \\leq L^d \\prod_i ||W_i|| \\). Let such a classifier be a compositional (\\( L_f \\)-)Lipschitz classifier.\nLet the labels be one-hot elementary vectors \\( e_i \\in \\mathbb{R}^c \\) and let \\( \\lambda(\\tilde{y}) = \\underset{i}{\\text{min}} ||\\tilde{y} - e_i|| \\) be the function discretizing the classifier's probability prediction \\( \\tilde{y} \\in \\mathbb{R}^c, \\sum_i \\tilde{y}_i = 1, 0 \\leq \\tilde{y}_i \\leq 1 \\forall i \\) into a class prediction. Given a prediction \\( \\tilde{y} \\in \\mathbb{R}^c \\) by a classifier \\( f(x) \\) on a sample \\( x \\in X \\) let the confidence \\( 0 \\leq \\gamma_f(x) \\leq 1 + \\epsilon, \\epsilon > 0 \\) of a classifier be defined as the minimum change \\( \\underset{\\delta \\in \\mathbb{R}^c}{\\text{min}} ||\\delta||_{\\infty} \\) s.t. \\( \\tilde{y} + \\delta \\) is a valid probability vector and \\( \\lambda(\\tilde{y}) \\neq \\lambda(\\tilde{y} + \\delta) \\). Intuitively, this minimum change vector shifts the probability mass from the class with the highest probability to the class with the second highest probability to change the discretized class prediction \\( \\lambda(\\cdot) \\) with a minimal shift. Let \\( W \\) be any weight matrix in a classifier and \\( \\Delta W \\) an additive perturbation of the weights yielding \\( W' = W + \\Delta W \\). Let a classifier be stable in respect to a perturbation and a given sample \\( x \\) if the perturbation doesn't affect the classifiers prediction on \\( x \\), i.e., \\( \\lambda(f_W(x)) = \\lambda(f_{W'}(x)) \\).\nLemma 3.2. Let \\( W \\) be any weight matrix in a compositional (\\( L_f \\)-)Lipschitz classifier \\( f(x) \\) and \\( \\Delta W \\) an additive perturbation of the weights yielding \\( W' = W_i + \\Delta W \\) and a perturbed compositional classifier \\( f'(x) \\). Then \\( f'(x) \\) is \\( L_{f'} \\)-Lipschitz with \\( L_{f'} \\leq L_f + L \\|\\Delta W \\| \\prod_{i=1,i\\neq j}^d ||W_i|| \\).\nA masking of a matrix \\( W \\), i.e., zeroing out some (or all) of the entries, can be modeled as an element-wise product of the matrix \\( W \\) with a bit mask \\( B \\). However, any masking can always equivalently be described as an additive perturbation. Let \\( \\mu(B, W) \\) be the masking function yielding this additive perturbation.\nLemma 3.3. For any bit mask \\( B \\) and a matrix \\( W \\) the additive perturbation \\( \\Delta W = \\mu(B, W) \\) s.t. \\( W + \\Delta W = B \\odot W \\) always exists and fulfills \\( ||\\Delta W||_{\\infty} \\leq ||W||_{\\infty} \\). The bound is tight."}, {"title": "Semi-Structured Sparse Convolutions", "content": "Since CV models commonly rely on two-dimensional convolutions to process the (image) inputs, the application of semi-structured sparsity on convolutions is illustrated in two dimensions. The method extends to other dimensions in an analogue fashion. A discrete two-dimensional convolution with a kernel \\( H \\in \\mathbb{R}^{c_{in} \\times h \\times w} \\) convolves an input tensor \\( X \\in \\mathbb{R}^{c_{in} \\times b \\times d} \\) into an output tensor \\( y \\in \\mathbb{R}^{b \\times d} \\) assuming zero padding. In the below formulation the functions f and g handle the padding for invalid combinations of input values, i.e., out of range values, else return the sum of the two input values:\n\\[ Y_{ij} = \\sum_{c=1}^{C_{in}} \\sum_{u=1}^{w} \\sum_{s=1}^{h} H_{cus} X_{cf(i,u)g(j,s)} \\]\nUsually such a convolution is conducted with \\( C_{out} \\) kernels to obtain an output \\( Y \\in \\mathbb{R}^{C_{out} \\times b \\times d} \\). Alternatively, this convolution can also be expressed as a matrix multiplication between the same input \\( X \\in \\mathbb{R}^{c_{in} \\times b \\times d} \\) and a weight matrix \\( W \\in \\mathbb{R}^{c_{out} \\times (C_{in}wh)} \\) constructed from the \\( C_{out} \\) kernels in the convolutional layer:\n\\[ Y = WU(X) \\]"}, {"title": "Discussion", "content": "Spending only a fraction of the resources invested to pretrain the network the results show that it is possible to learn semi-structured 2:4 sparsity patterns that can accelerate CNN inference while not impeding or even improving classification performance. This shows that extending the native support of 2:4 sparse matrix operations to 2:4 sparse convolutional kernels is a highly promising avenue towards more efficiency that is achievable today.\nThe results presented were obtained using a simple, generic training procedure. However, recent works indicate the high relevance of the training recipe, which could go as far as being the sole reason why (vision) transformers outperformed CNNs in image classification tasks in recent years [25]. The effect of more sophisticated training procedures including, e.g., data augmentation [6, 43, 45, 47], needs to be studied offering potential to accelerate convergence and reaching even higher levels of classification performance. Furthermore, the proposed method does not yet make use of available heuristics with patterns still being randomly initialized. In the case of, e.g., ConvNeXt, in which more than one epoch was needed to converge, a meaningful initialization cheaply obtainable, e.g., [37], could serve as an improved starting point and reduce lifetime resource spending even further. Lastly, while working exceptionally well, the work thus far only explores image classification. However, CV models are also frequently employed for object detection and segmentation tasks. Future experiments could be aimed at surveying all CV tasks relevant for CV models employed as backbones in foundation models. From a theoretical viewpoint more assumptions, e.g., on the distribution of weights could lead to more constrained yet tighter bounds. While losing some generality, this could lead to results that are even more interesting from a practical viewpoint to guide effective trades off between inference acceleration and model performance.\nBeyond the aforementioned proposals for future work several additional avenues come into con- sideration: Firstly, the proposed architectural change introduces the temperature parameter \\( \\tau \\) of the GS distribution to the reformulated convolutional models, but the effects on performance and convergence are yet to be studied. Secondly, the work can be extended to cover more models, both convolutional and non-convolutional, as well as other frequently used, costly layer types. Lastly, more detailed insights into what information the network loses when modified as proposed could prove valuable."}, {"title": "Conclusion", "content": "In this paper, a novel method for accelerating inference in CV architectures has been presented. By expressing convolutions as semi-structured sparse matrix operations existing hardware accelerations for semi-structured sparsity can be used to directly translate model sparsity into significant savings in regards to data loading and FLOPs spent during inference. The proposed use of semi-structured sparsity patterns bridges the gap between practical requirements induced by compute hardware and the theoretical desire to not limit the choice of sparse models to not affect model performance negatively."}, {"title": "Appendix / Supplemental Material", "content": null}, {"title": "Proof of Lemma 3.1", "content": "The claim follows from the fact that the Lipschitz constant propagates through each layer of the compositional function. Consider a single layer \\( f_i(x) = \\sigma(W_i x + b_i) \\). Since \\( \\sigma \\) is L-Lipschitz it follows that \\( f_i \\) is L-Lipschitz with \\( L_i := L\\|W_i\\| \\) as indicated below where the first inequality uses the Lipschitz-continuity of \\( \\sigma \\) and the last inequality uses the sub-multiplicative property of norms.\n\\[ \\|\\sigma(W_i x_1 + b_i) - \\sigma(W_i x_2 + b_i)\\| \\leq L\\|W_i x_1 + b_i - (W_i x_2 + b_i)\\| \\]\n\\[ = L\\|W_i(x_1-x_2) \\| \\]\n\\[ \\leq L\\|W_i\\|\\|x_1 - x_2\\| \\]\n\\[ =: L_i\\|x_1-x_2\\| \\]\nNow, consider the composed function \\( f_k \\circ f_{k-1} \\circ ... f_1 \\); then:\n\\[ \\|f_k(f_{k-1} \\circ ... f_1(x_1)) - f_k(f_{k-1} \\circ ... \\circ f_1(x_2)) \\| \\]\n\\[ \\leq L_k\\|f_{k-1} \\circ ... \\circ f_1(x_1) - f_{k-1} \\circ ... \\circ f_1(x_2)\\| \\]\n\\[ \\leq L_k L_{k-1} ... L_1\\|x_1 - x_2\\| \\]\nThus, the function \\( f_k \\circ ... \\circ f_1 \\) is \\( L_k L_{k-1} ... L_1 \\)-Lipschitz. Finally, since the softmax function softmax(z) for a vector z is a normalized exponential function it is known to be 1-Lipschitz. If \\( f_d \\circ ... \\circ f_1 \\) is L'-Lipschitz, where \\( L' = L_1 L_2 ... L_d \\), then the overall classifier f(x) is consequently \\( L_f \\)-Lipschitz with \\( L_f = L' \\cdot 1 = L' \\)."}, {"title": "Proof of Lemma 3.2", "content": "The claim follows from the fact that the masking changes the Lipschitz constant of the masked layer which then propagates through each layer of the compositional function. Let the i-th layer be the masked layer, i.e., \\( W' = W_i + \\Delta W \\) and \\( f(x) = \\sigma(W' x + b_i) \\). Since \\( \\sigma \\) is L-Lipschitz it follows that \\( f' \\) is \\( (L_i + L\\|\\Delta W\\|)\\)-Lipschitz with \\( L_i := L\\|W_i\\| \\) as indicated below where the first inequality uses the Lipschitz-continuity of \\( \\sigma \\) and the latter two inequalities use the sub-multiplicative property of norms and the triangle inequality respectively.\n\\[ \\|\\sigma(W' x_1 + b_i) - \\sigma(W' x_2 + b_i) \\| \\leq L\\|W' x_1 + b_i - (W' x_2 + b_i)\\| \\]\n\\[ = L\\|W'(x_1-x_2)\\| \\]\n\\[ \\leq L\\|W_i + \\Delta W\\|\\|x_1 - x_2\\| \\]\n\\[ \\leq (L\\|W_i\\| + \\|\\Delta W\\|)\\|x_1 - x_2\\| \\]\n\\[ =: (L_i + L\\|\\Delta W\\|) =: L' \\]\nReusing the results from the proof of Lemma 3.1 on the Lipschitz constant of a compositional function composed with the softmax function it follows: If \\( f_d \\circ ... \\circ f_i \\circ ... \\circ f_1 \\) is L\"-Lipschitz, where \\( L\" = L_1...L'_i...L_d = L_1...(L_i + L\\|\\Delta W\\|)...L_d = L' + L_1...\\|\\Delta W\\|L_d = L' + L\\|\\Delta W \\| \\prod_{i=1,i\\neq j}^d ||W_i|| \\) (cf. proof of Lemma 3.1), then the overall classifier f(x) is consequently \\( L_f \\)-Lipschitz with \\( L_f = L\" \\cdot 1 = L\" \\)."}, {"title": "Proof of Lemma 3.3", "content": "Given the matrix W and the bit mask B the additive perturbation can be obtained via the following construction\n\\[ (\\Delta W)_{ij} := (B_{ij} - 1)W_{ij} \\]\nyielding\n\\[ W_{ij} + (\\Delta W)_{ij} = W_{ij} + (B_{ij} - 1)W_{ij} = \\begin{cases} W_{ij} - W_{ij} & \\text{if } B_{ij} = 0\\\\ W_{ij} - 0 & \\text{if } B_{ij} = 1 \\end{cases} \\]\nwhich is equivalent to the element-wise product of the matrix with the bit mask:\n\\[ B_{ij} W_{ij} = \\begin{cases} 0 & \\text{if } B_{ij} = 0\\\\ W_{ij} & \\text{if } B_{ij} = 1 \\end{cases} \\]"}, {"title": "Proof of Lemma 3.4", "content": "Let \\( f(x) = (\\text{softmax} \\circ f_d \\circ ... \\circ f_j \\circ ... \\circ f_1)(x) \\) be a compositional classifier of depth d with \\( f_i(x) = \\sigma(W_i x + b_i) \\). Let \\( W' = W_j + \\Delta W \\) be the additive perturbation yielding the perturbed layer \\( f'(x) = \\sigma(W' x + b_j) \\) and classifier \\( f'(x) = (\\text{softmax} \\circ f_d \\circ ... \\circ f'_j \\circ ... \\circ f_1)(x) \\). Let \\( x_{j-1} \\) be the input to the j-th layer, i.e., \\( x_{j-1} = (f_{j-1} \\circ ... \\circ f_1)(x) = f_{j-1,1}(x) \\). Then the the following can be observed about the variance in the j-th layer, where the first inequality uses the Lipschitz-continuity of \\( \\sigma \\) and the last inequalities use the triangle inequality and the sub-multiplicative property of norms respectively.\n\\[ \\|f_j(x_{j- 1}) - f'_j(x_{j-1})\\| = \\|\\sigma(W_j x_{j-1} + b_j) - \\sigma((W_j + \\Delta W) x_{j-1} + b_j)\\| \\]\n\\[ \\leq L\\|W_j x_{j-1} + b_j - ((W_j + \\Delta W) x_{j-1} + b_j)\\| \\]\n\\[ = L\\|W_j x_{j-1} + b_j - (W_j x_{j-1} + b_j) - \\Delta W x_{j-1}\\| \\]\n\\[ \\leq L(\\||W_j x_{j-1} + b_j - (W_j x_{j-1} + b_j)\\| + \\|-\\Delta W x_{j-1}\\|) \\]\n\\[ < L\\|\\Delta W\\|\\|x_{j-1}\\| \\]\nThis can be extended to a statement on the entire classifier as follows:\n\\[ \\|f(x) - f'(x)\\| = \\|(f_{d,j+1} \\circ f_j \\circ f_{j-1,1})(x) - (f_{d,j+1} \\circ f'_j \\circ f_{j-1,1})(x)\\| \\]\n\\[ < L^{d-(j+1)+1} \\prod_{i=j+1}^d \\|W_i\\| \\| f_j(x_{j-1}) - f'_j(x_{j-1}) \\| \\]\n\\[ < L^{d-j+1} \\prod_{i=j+1}^d \\|W_i\\| \\|\\Delta W\\| \\|x_{j-1}\\| \\]\n\\[ < L^d \\|\\Delta W\\| \\|x\\| \\prod_{i=1,i\\neq j}^d \\|W_i\\| \\]"}, {"title": "Proof of Lemma 3.6", "content": "The setting of the lemma is illustrated below: the classifier f(x) for which a masking B has been learned is updated yielding the question what bounds can be obtained for the updated classifier \\( f_U(x) \\) if masked with the same mask B.\nUpdating the classifier and then applying the mask yields the following:\n\\[ (V_j)_{ik} + (\\mu(B, V_j))_{ik} = (W_j)_{ik} + (U_j)_{ik} + (\\mu(B, W_j + U_j))_{ik} \\]\n\\[ = \\begin{cases} (W_j)_{ik} + (U_j)_{ik} - (W_j)_{ik} - (U_j)_{ik} & \\text{if } B_{ik} = 0\\\\ (W_j)_{ik} + (U_j)_{ik} - 0 & \\text{if } B_{ik} = 1 \\end{cases} \\]"}, {"title": "Classification Task and Reported and Validation Classification Performance", "content": "The pretrained networks were retrained and evaluated on the multi-class classification challenge provided by the ImageNet-1K dataset [39]. The data set contains approximately 1.2 million training images, 50.000 validation and 100.000 test images of 1000 object classes. To ensure consistency between the performance on the validation set and the reported accuracies on the test set the unmodi- fied architectures were evaluated showing no significant differences (cf. Table 4). To assess whether reformulating the layers induced changes due to alteration of the floating point arithmetic, likewise comparisons for the unmodified architectures and the modified architectures without masking were conducted showing no change in predictions and performance as expected and are thus not reported.\nThe below experimental results can be reproduced with the provided code and were obtained on a NVIDIA GeForce RTXTM 3090 with 24 GB of RAM spending less about 3 to 3:30 minutes per variant of ResNet or ConvNeXt respectively."}, {"title": "Training Procedure", "content": "The pretrained modified architectures were trained according to a generic training procedure using the AdamW optimizer [27] for optimization with an initial learning rate \\( \\eta = 1.0 \\), a momentum of \\( \\beta = 0.9 \\) and weight decay with a factor of \\( \\lambda = 10^{-4} \\). Training images were random-cropped to a"}]}