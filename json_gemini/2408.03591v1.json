{"title": "Focal Depth Estimation: A Calibration-Free, Subject- and Daytime Invariant Approach", "authors": ["Benedikt W. Hosp", "Bj\u00f6rn Severitt", "Rajat Agarwala", "Evgenia Rusak", "Yannick Sauer", "Siegfried Wahl"], "abstract": "In an era where personalized technology is increasingly intertwined with daily life, traditional eye-tracking systems and autofocal glasses face a significant challenge: the need for frequent, user-specific calibration, which impedes their practicality. This study introduces a groundbreaking calibration-free method for estimating focal depth, leveraging machine learning techniques to analyze eye movement features within short sequences. Our approach, distinguished by its innovative use of LSTM networks and domain-specific feature engineering, achieves a mean absolute error (MAE) of less than 10 cm, setting a new focal depth estimation accuracy standard. This advancement promises to enhance the usability of autofocal glasses and pave the way for their seamless integration into extended reality environments, marking a significant leap forward in personalized visual technology.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the landscape of Virtual Reality (VR) has undergone transformative changes, significantly influencing various sectors by enhancing accessibility and expanding its market presence. Alongside VR's growth, smart glasses technology has advanced, with VR serving as a crucial testing ground for innovative algorithms [15]. A notable area of innovation within smart glasses technology is the development of autofocal glasses, designed to address presbyopia a condition that diminishes the eye's ability to focus on close objects as people age, affecting billions worldwide [3, 8]. Traditional correction methods, such as bifocal or multifocal lenses, often fall short of mimicking the eye's natural focusing capability, leading to restricted gaze and visual inconvenience [31, 19]. Progressive lenses may offer some correction but may introduce aberrations in the periphery [9, 43]. Autofocal glasses, leveraging optical-quality liquids in flexible membranes to dynamically adjust focal length, represent a more natural and innovative solution for presbyopia [5]. Despite their potential, existing autofocal solutions face limitations due to manual focus adjustments and a limited focus range [46], impacting user comfort and adaptability. Furthermore, the autofocals field shows many evaluation metrics but lacks detailed algorithm explanations, limiting understanding and hindering advancements [1, 55]. A significant barrier to the widespread adoption of autofocals is the cumbersome need for frequent user-specific calibration, significantly limiting the technology's practicality [38]. Previous efforts in focus depth estimation have made strides but often rely on extensive calibration, lack generalizability, and demand significant computational resources [56]. This introduces a substantial obstacle to the seamless integration of autofocal glasses into daily life, highlighting the need for a novel approach to overcome these limitations [21, 40].\nIn this context, machine learning (ML) emerges as a beacon of innovation, promising to transcend the traditional calibration constraints. This paper introduces the Foveal Attention Long Short-Term Memory Network (FOVAL). This novel spatiotemporal sequential model achieves a calibration-free and daytime invariant focal depth estimation by analyzing eye vergence angle data [27, 39] and intense feature engineering based on domain knowledge. FOVAL marks a significant advance by providing a robust, user-friendly solution that adapts to diverse environmental conditions without the need for frequent recalibration, showcasing the potential of ML to revolutionize personalized technology. FOVAL is distinguished by its technical innovations, including using Long Short-Term Memory (LSTM) layers [14], feature engineering, and advanced preprocessing techniques. Combining these elements enables the model to generalize effectively to new subjects without prior calibration. The optimization of model architecture including LSTM, normalization layers, and max pooling - underpins the model's exceptional performance, demonstrating the profound impact of ML in overcoming traditional challenges in personalized technology. FOVAL's elimination of the need for calibration dramatically enhances the usability of autofocal glasses, making advanced vision correction more accessible and convenient for users. This technology has the potential to significantly elevate visual acuity and quality of life for those affected by presbyopia, setting a new standard for user-friendly technology integration.\nThe applications of depth estimation extend well beyond the realm of autofocal glasses, touching on fields as diverse as microscopy [17], VR, and augmented reality (AR) [24]. Specifically, in medical areas like microscopy, arthroscopy, and robotic surgery, precise depth estimation can revolutionize how we visualize and analyze microscopic entities, enhancing the clarity and depth of 3D imaging to understand cellular structures and interactions better [44, 30, 17]. This is especially present in microscopic surgery, where optimal depth adjustment of the camera during surgery can be handled with such a model as FOVAL [4, 28]. Optimized human focus-based depth estimation can improve medical surgery and diagnostics. In VR and AR, depth estimation is pivotal in creating immersive and interactive experiences by reducing data size while retaining high resolution on focused depth planes [51] and improving perceptual realism [24]. VR enables rendering environments with realistic spatial dynamics, improving user immersion and interaction within virtual spaces [48]. For AR, accurate depth estimation allows for the seamless integration of digital objects into the real world [50, 34], showing accurate 3D images [49] enhancing applications in education, design, and entertainment [22] but also in human-vehicle interaction for autonomous driving [48]. The ability to accurately estimate depth in real-time can significantly improve AR glasses' performance and user experience, making them more practical for various applications, from industrial design to augmented learning environments.\nThis paper introduces FOVAL a novel calibration-free method for estimating focal depth, which leverages LSTM networks and domain-specific feature engineering. Our method significantly improves the accuracy and usability of autofocal glasses, achieving a mean absolute error (MAE) of less than 10 cm, and outperforms all state-of-the-art methods in focal depth estimation. Furthermore, by eliminating the need for user-specific calibration, we enhance the practicality of these devices for daily use. In the following section II, we describe details about the methods we use, including data collection, the data preprocessing pipeline, and model development; Section IV presents the results of our evaluation of the model, demonstrating the efficacy of our approach through comparative analysis; and Section V we recapitulate our findings and discusses the implications, limitations, and potential future applications of our findings."}, {"title": "II. METHODS", "content": "For the development and evaluation of our model, we collected data from 25 emmetropic individuals, meaning they had no need for corrective lenses and possessed normal vision (good vision). Of these participants, 11 identified as male and 14 as female, with an average age of 31.68 years (standard deviation = 11.71). Two additional participants were excluded due to issues with eye convergence. All remaining participants were free from any known eye diseases. The study followed ethical guidelines and received approval from the Faculty of Medicine Ethics Committee of the University of T\u00fcbingen under reference number 439/2020BO. All subjects gave consent for their data. We collected the dataset in a VR scene showing a sphere with a fixation cross traveling back and forth from 0.35 to 3 meters in a spiral movement on an XTAL VR Headset [54]. Eye vectors and origins (both in headset coordinates) have been recorded and serve as input features to the model. The complete dataset consists of 282080 samples of all individuals combined. The following paragraphs outline the workflow for creating the model. This section is divided into several parts, following general best practices for building a machine-learning model. Particular emphasis is given to the loss function and the feature engineering part based on domain knowledge (detailed mathematical notations are added to the appendix IX, which most influences model performance. The data cleaning steps contain data preprocessing necessary to increase the model's robustness against errors in sensor input and outliers. After preprocessing, we describe the dataset splitting and feature space, which is essential when working with human gaze data, followed by the data normalization and transformation steps. Finally, we describe the model architecture."}, {"title": "A. Data Preprocessing Pipeline", "content": "The data preprocessing pipeline involves several key steps to ensure the data is prepared effectively for the machine learning model. Here, we outline the steps in the order they are applied:\nStep 1: Data Cleaning\nAnomalies and outliers were identified and eliminated through a detailed removal process. We employed a rolling mean window (window size = 5, threshold = 10) for the target variable to detect context-specific, short-term anomalies that can occur due to input signal noise. Window size and threshold have been defined empirically. This approach captures transient deviations from the mean over time. For the features, we utilized the Interquartile Range (IQR) method [53] over the time dimension to enhance data consistency. The IQR method is better suited to identify statistically rare events within the whole range of the features, capturing both short-term and long-term outliers. The window calculation process is formalized in Equations 1, 2, and 3. $GT_{depth_{i}}$ represents the ground truth depth value at the i-th position in the dataset, $W_{i}$ represent the window of values around $GT_{depth}$, defined by a specified window size. For each $GT_{depth}$, in the dataset, we define a window $W_{i}$ of values centered around i, with size windowsize. The window is defined as:\n$W_{i} = {GT_{depth_{start}},...,GT_{depth_{end}}}$ (1)\nwhere\n$start = max(i - \\frac{windowsize}{2}, 0)$ (2)\n$end = mini + (\\frac{windowsize}{2} + 1, N)$ (3)\nand N is the total number of observations in the dataset of one subject. $mean(W_{i})$ represents the mean value of the ground truth depths within the window $W_{i}$, and threshold represents the predefined threshold for outlier detection. The window's mean is then calculated with Equation 4. A value $GT_{depth_{i}}$ is considered an outlier if the absolute difference between $GT_{depth_{i}}$ and the mean of $W_{i}$ exceeds the threshold (Equation 5).\n$mean(W_{i}) = \\frac{1}{|W_{i}|} \\sum_{j=start}^{end} GT_{depth,j}$ (4)\nOutlier if $|GT_{depth_{i}} - mean(W_{i})| > threshold$ (5)\nFor removing data outliers within our feature space, we utilize the IQR method [53]. This technique involves computing the first (Q1) and third (Q3) quartiles for each column within our dataset, with the IQR defined as the difference between these quartiles, as illustrated in Equation 6. Subsequently, outliers within each feature column are identified and eliminated based on the criteria outlined in Equation 7, where df symbolizes our data frame. This operation is executed independently for each column to ensure the nuanced characteristics of each feature are adequately considered.\n$IQR = Q3 - Q1$ (6)\n$Q3 + 1.5 \\times IQR < df_{column} < Q1 \u2013 1.5 \\times IQR$ (7)\nHere, df refers to our data frame, which encapsulates the entirety of the data of one subject. For each feature column in df, denoted as $df_{column}$, we calculate Q1, Q3, and IQR to establish the bounds for identifying outliers. The 'mask' generated by Equation 7 is a boolean array specific to each column, indicating which values fall outside the accepted range (defined by 1.5 times the IQR below Q1 and above Q3), thereby flagging them as outliers. This approach ensures that outlier detection and removal are tailored to the distribution of each feature, maintaining the integrity of our dataset by selectively filtering outliers based on the statistical properties of individual columns. Rows in the dataset are considered outliers if they fall outside the range specified by this mask and are removed. Outliers can significantly skew statistical measures such as the mean, variance, and standard deviation, resulting in unreliable data representation. Removing outliers provides a more accurate depiction of the data distribution, enhancing the reliability of these measures. While anomalies are detected within small, moving windows, affecting only a localized subset of data points at a time, outliers are detected across the entire dataset, affecting global statistical measures. Therefore, anomalies have a limited impact because they are transient and localized. In contrast, outliers can drastically shift the overall mean due to their extreme values relative to the rest of the data. Many machine learning models assume that the data follows a particular distribution, and outliers can violate these assumptions, leading to poor model performance. Therefore, outlier removal often improves model accuracy and generalization [42, 52, 33, 29]. Outliers can also introduce noise into the dataset, which confuses the model during Training. By removing outliers, we reduce this noise, enabling the model to focus on the underlying patterns in the data. Additionally, outliers can stem from data entry errors or anomalies in data collection. Identifying and removing these outliers prevents such errors from negatively impacting the analysis and conclusions. Removing outliers ensures that statistical test results are more robust and reflective of the actual data characteristics. This is particularly crucial during data transformation, where outliers can significantly impact the choice of transformation."}, {"title": "Step 2: Data Balancing", "content": "As soon as the data for each subject is cleaned, we continue with data balancing. To do so, we combine over- and un-dersampling (Equation 10) before binning the target variable. This process involved quantizing the target variable into a predetermined number of bins, ensuring an equal distribution of samples across each bin. Since the dataset's data is mainly spread across 0 to 600 cm, we defined bins of 10 cm to ensure a balanced amount of samples for at least each 10 cm bin, resulting in 60 bins. We chose the bin size based on the visualization of the dataset as a trade-off between covering a wide range of possible values and having a substantive amount of representative samples for each bin. This strategy ensures that the model is not biased towards more frequently occurring data ranges. In the first step, we divide the target variable, GTdepth, into a predetermined number of bins (Nbins), where Bin() denotes the binning operation, and Nbins = 60 in this context (Equation 8). Next, we calculate the mean count per bin C, where $C_{i}$ is the count of samples in the i-th bin (Equation 9). To ensure that each bin has a sample count close to the mean count C, we perform resampling. For bin i, the resampled bin data, $D_{i}$, is obtained using Equation 10, where $|D_{i}|$ is the count of samples in bin i, and Resample() denotes the resampling operation. This process involves random oversampling or undersampling of each bin to match the mean count C. Finally, the resampled bin data from all bins are concatenated to form a balanced dataset as described in Equation 11.\nTo ensure that each bin has a sample count close to the mean count C, we apply the following resampling strategy: If the number of samples in a bin $D_{i}$ is less than the mean count C, we perform oversampling with replacement to increase the sample count to $\\lceil C \\rceil$. If the number of samples in a bin $|D_{i}|$ is greater than the mean count C, we perform undersampling without replacement to reduce the sample count to $\\lceil C \\rceil$. And if the number of samples in a bin $D_{i}$ is equal to the mean count C, no resampling is necessary. The following equation represents this process:\n$GT_{depth_{in}} = Bin(GT_{depth}, N_{bins})$ (8)\n$C = \\frac{1}{N_{bins}} \\sum_{i=1}^{N_{bins}} C_{i}$ (9)\n$D'_{i} = \\begin{cases} Resample(D_{i}, replace = True, n = \\lceil C \\rceil), & \\text{if } |D_{i}| < C \\\\ Resample(D_{i}, replace = False, n = \\lceil C \\rceil), & \\text{if } |D_{i}| > C \\\\ D_{i}, & \\text{otherwise} \\end{cases}$ (10)\n$Balanced \\ dataset = \\bigcup_{i=1}^{Nbins} D'_{i}$ (11)\nThis binning and resampling process is crucial for addressing imbalances in the distribution of the target variable across the dataset. By ensuring each bin has a similar number of samples, we enhance the generalizability of the machine learning model. This balanced approach reduces bias towards over-represented values, making predictions more accurate and fair. Additionally, it improves the model's performance across the range of the target variable by ensuring adequate representation of each data segment, leading to more reliable and consistent results."}, {"title": "Step 3: Splitting", "content": "To incorporate the intricate risks of biophysical data, we also ensured the generalizability of our machine-learning model by employing a systematic approach to data splitting using the K-Fold cross-validation (K-Fold CV) technique in a subject-wise manner. This method is pivotal for assessing the model's performance and ability to generalize to unseen data. We initialized the KFold cross-validator from the Scikit-Learn library [41] with a predetermined number of splits, n_splits = number of subjects, and set shuffle to True to randomize the data before splitting it. This effectively is a Leave-One-Out Cross-Validation (LOOCV) approach as a particular case of KFold CV. The randomization is crucial for mitigating any potential bias stemming from the original ordering of the data. To ensure the reproducibility of our results, we fixed the random state parameter to 42.\n$KFold(splits = nsplits, shuffle = True, random state = 42)$ (12)\nTo maintain the integrity of subject data and avoid data leakage, we performed the splits based on unique subjects in the dataset (subject-wise splitting, see [16] for more details). This approach ensures that all data from a single subject is exclusively in the Training, validation, or testing set, which is critical for evaluating the model's performance in real-world scenarios. We created splits for training and validation, again ensuring there was no overlap of subjects between these sets. Inference is then done on unseen subjects. This meticulous data splitting and validation approach is foundational for developing accurate and reliable machine learning models. By ensuring that our model is tested on unseen data in a manner that mimics real-world application scenarios, we can confidently assess its generalizability and readiness for deployment. Moreover, subject-based splitting guards against overfitting and ensures that our model's performance metrics indicate its true predictive power."}, {"title": "Step 4: Feature space", "content": "Based on the input sequences of 10 temporally consecutive samples of 12 input features (eye vector left and right (x,y,z) and eye origin, we enhanced the model's predictive power by deriving new features from the 12 input features and one target variable (GT depth).\nThe features are derived from the optical axis, which is defined as the line that passes through the centers of the eye's optical components, such as the cornea, the pupil, and the lens, and is perpendicular to these surfaces. In contrast, the visual axis is defined by a line that is not perpendicular to the retina but instead ends on the fovea of the eye. Typically, eye-tracking methods need to estimate this offset to estimate the precise point of regard (POR) in 3D space because only the optical axis is easily detectable from eye images directly. However, as we do not rely on the actual POG but instead on relative movements, we do not need to know the visual axis and can use uncalibrated eye images as input.\nOur features can be divided into primary, ratios & differences, and higher-order dynamics of eye movements. In the following section, we give an overview of the features and their meaning. For the particular implementations, please see Appendix IX for an exhaustive list of equations.\na) Eye Vergence Angle: The Eye Vergence Angle (EVA) is pivotal in understanding how the eyes adjust their lines of sight when focusing on an object. This angle results from both eyes' simultaneous inward or outward movement to maintain single binocular vision as they shift focus across objects at varying distances. EVA provides critical insights into the focal depth and the mechanisms underlying 3D perception. It is essential for grasping the spatial dynamics of visual perception. We normalized the angle between different subjects to compare it. Additionally, the conversion of EVA to a cosine metric simplifies some analyses. A key derivative of EVA, vergence depth, offers a quantifiable measure of the focal depth but provides the most reliable estimates only within a specific mid-range due to the nonlinear relationship between EVA and depth.\nb) Eye Direction Vectors: Beyond the EVA, we analyze changes in eye direction vectors (EDV) to illuminate eye alignment and focus disparities. This includes assessing differences in vector components between the eyes and calculating the Euclidean distance between focus points (intersections of the vectors) to determine the convergence or divergence in gaze. The directional magnitudes for each eye's gaze direction are compared to reveal potential asymmetries in eye movement strength. While the intersection of EDVs on the z-axis can be informative about the focused depth, even slight inherent noise and errors in the estimation of the EDVs can introduce high variation in depth estimation. This is one major problem that causes eye-tracking for depth perception to be not as reliable as needed.\nc) Ratios and Differences: Another group of features can be described as ratios and differences. Here, we explore the ratio of directional magnitudes by dividing one eye's directional magnitude by the other's, shedding light on eye movements and gaze focus asymmetries. Additionally, analyzing the magnitudes of individual dimensions uncovers insights into the relative orientations and dominance in gaze direction. The feature examining horizontal to vertical disparities in gaze may reveal preferred axes of movement or alignment. Key to our analysis is the quantification of eye movement rates-velocity"}, {"title": "Step 5: Normalization", "content": "The next step in the preprocessing pipeline is to normalize the features. Normalization is applied in two steps to the datasets, which is particularly useful in scenarios where data comes from multiple subjects, each potentially with different baselines or scales of measurement.\nGiven a dataset D with multiple subjects, the two-step normalization process involves applying global normalization to D (see equation 13), obtaining Dglobal_norm, and then for each subject in Dglobal_norm, applying subject-wise normalization to obtain the final normalized dataset Dfinal_norm (see equation 14). The normalization per subject is to account for individual variations, ensuring the model learns generalized patterns rather than subject-specific characteristics.\nStep 5.1: Global Normalization: The global normaliza-tion process applies a Robust Scaler [13] to the dataset's features, excluding specific columns such as GT Depth. This scaling technique is particularly effective in datasets with outliers, as it uses the interquartile range for scaling, thereby reducing the influence of extreme values such as outliers. Given a feature vector X = [x1, x2, ..., xn], the Robust Scaler transforms X into X' by computing:\n$X' = \\frac{X - Median(X)}{IQR(X)}$ (13)\nwhere Median(X) is the median of the feature values in X, IQR(X) is the Interquartile Range of X, calculated as IQR(X) = Q3(X) \u2013 Q1(X), with Q3(X) and Q1(X) representing the third and first quartiles of X, respectively. The transformed dataset D' then contains the normalized features. We use a scaler that is robust to outliers to ensure stability and robustness in our preprocessing pipeline, accounting for any residual outliers, future data variations, and skewed distributions.\nFor global normalization, the statistics (median and IQR) are calculated using the training set. These statistics are then applied to normalize the test set, ensuring that the normalization process is consistent and does not introduce data leakage. Even after removing outliers, global normalization remains essential because it ensures that the overall scale of the data is consistent, making the dataset more robust to variations. The global normalization step adjusts the data on a broader scale, ensuring that features from different subjects and conditions are comparable. This step is crucial to avoid biases in the model training process caused by different feature scales.\nStep 5.2: Subjective (Subject-Wise) Normalization: Data is further normalized on a per-subject basis after global normalization. This step accounts for individual differences, ensuring that the model does not learn from these variations but from the underlying patterns.\nFor a subjects with data $X_{s}$, subject-wise normalization is applied as:\n$X'_{s, normalized} = \\frac{X - Median(X)}{IQR(X)}$ (14)\nSubject-wise normalization is calculated and applied directly to each subject's data, ensuring that intra-subject variability is appropriately handled. By combining these two normalization steps, we ensure that our model is trained on data that is both globally and individually normalized, enhancing its generalizability. This process is repeated for each unique subject in the dataset, ensuring individual data scales are aligned. Especially in physiological or behavioral data, measurements can significantly differ between subjects due to innate individual differences. Subjective normalization is critical to handling inter-subject variability. It further improves model generalization by ensuring the model learns from normalized features, allowing it to better generalize across subjects not part of the training data. Another important aspect is a fair comparison. It ensures that features from different subjects are on a comparable scale, making statistical analysis and machine learning modeling more effective and fair."}, {"title": "Step 6: Feature Transformation", "content": "Feature transformations ensure that each feature in the dataset is transformed to achieve a more normal distribution, supporting the assumptions of many machine learning algorithms and improving overall model performance [7, 37, 20]. Different mathematical transformations are tested on the training dataset for each feature to achieve a distribution closer to normal distribution characteristics, which can improve the performance of specific machine-learning models. While normalization typically scales the data to a standard range or distribution, it does not necessarily transform the data to follow a Gaussian distribution. Therefore, applying transformations such as logarithmic, square root, or Box-Cox is necessary to address skewness and other distributional issues that normalization alone cannot resolve.\nThe ideal transformation minimizes the distance to the normal distribution characteristics defined by skewness and kurtosis. Skewness measures the asymmetry of the data distribution, with an ideal skewness of 0 indicating a perfectly symmetrical distribution. Kurtosis measures the tailedness of the distribution, with an ideal kurtosis of 3 indicating a normal"}, {"title": "Step 7: Scaling Target", "content": "To improve the performance and stability of our machine learning models, we rescale the target variable to align with the input features. However, the reasons for rescaling the target variable are multi-fold:\nRescaling ensures that the model's output values are within a comparable range to the input features, facilitating faster convergence during Training. Significant discrepancies between the scales of input features and target values can lead to inefficient optimization processes and slow convergence rates [11]. Working with a wide range of target values can introduce numerical instabilities, particularly in gradient-based optimization methods. By rescaling the target values to a smaller, more manageable range, we mitigate these instabilities and promote a more stable training process [25]. Maintaining consistency in the scaling of input features and target variables helps the model learn the relationships between inputs and outputs more effectively. This consistency can result in more accurate predictions and better generalization to unseen data [6]. Rescaling target variables is a standard practice in machine learning, especially in regression tasks. It ensures that the model's loss function operates within a consistent scale, which is crucial for effective training [12]. To ensure that the model's predictions are interpretable in the context of the original data, the scaler used during the training process must be saved. This saved scaler is then used to apply an inverse transformation to the model's predictions.\nDuring the training process, the target variable y is scaled to improve model performance and Training stability. However, once the model generates predictions, these predictions are in the scaled form. To interpret these predictions correctly, we need to convert them back to the original scale of the target variable. After obtaining the model's predictions \u0177 on the test set or new data, we apply the inverse transformation to convert them back to the original scale. This process involves using the saved scaler to reverse the scaling operation applied during Training.\nThe target variable is initially measured in centimeters, with values ranging from 0 to around 900 cm, corresponding to a depth perception range from 0.0 to 9 meters based on the typical eye vergence range. To standardize this range and make it more manageable for computational purposes, the target variable is scaled to a range of 0 to 1000 cm using a MinMax Scaler transformation. This transformation adjusts the values of the target variable to the specified range [0, 1000].\nThe transformation for a value y in the original dataset to the scaled value y' is given by:\n$y' = a + \\frac{(y - Y_{min}) \u00d7 (b \u2212 a)}{Y_{max} - Y_{min}}$\nwhere $Y_{min}$ and $Y_{max}$ are the minimum and maximum values of the target variable in the training dataset, respectively, and a and b are the bounds of the new scaling range, with a = 0 and b 1000 in our case.\nWe chose the 0 to 1000 range to encompass a depth perception range of 0 to 10 meters, considering that eye movements can change depth perception up to approximately 10 meters, beyond which it is considered infinity. This range ensures that the model is prepared for all kinds of data that might be fed into it in the future, including maximum depth perception scenarios.\nDuring development, the MinMax Scaler performed better than typical scaling methods by preserving the relative distances between data points and improving the model's performance. The 0 to 1000 range, while larger than the original scale, ensures that the scaled values are standardized and more suitable for various computational operations and visualizations."}, {"title": "Step 8: Sequence Creation for Time-Series Analysis", "content": "As soon as the data is clean, normalized, transformed, and scaled, we structure samples into sequences to capture temporal dependencies within the eye movement data. This approach ensures that the temporal context is preserved, which is crucial for training models on sequential data where the order and timing of observations are essential. Given a dataset D organized by subject, the process first divides the dataset into subsets Ds, each corresponding to a unique subjects, ensuring that sequences are generated within individual subject data.\nWe generate sequences for a defined sequence length L by sliding a window of length L across each subject's data. This means that the sequences overlap, as each new sequence starts one time step after the previous one. For each position n within the data for subject s, a sequence Sn and its corresponding target yn are defined as:\n$S_{n} = {X_{n}, X_{n+1},..., X_{n+L-1}}$\n$Y_{n} = Y_{n+L}$\nwhere Xn represents the feature vector at time step n, and Yn+L is the target value immediately following the sequence. This overlapping sequence generation ensures that temporal dependencies are effectively captured, providing the model with sufficient context for learning."}, {"title": "III. MODEL", "content": "The model's architecture was defined using PyTorch's dynamic computation graph and intuitive API, which makes it ideal for developing complex neural networks. The model was trained using PyTorch's built-in functions for backpropagation and optimization. NumPy was used to handle data arrays and perform numerical operations, while SciPy provided additional mathematical and statistical functions necessary for preprocessing and evaluation.\nThe training process involves several key components: The choice of optimizer is critical for the model's learning efficiency and convergence. Due to its effectiveness in some pre-runs, we picked AdamW [32] (Adam [23] with weight decay). We found that a smooth L1 loss [10] with beta=0.75 works best for the loss function. Beta has been defined empirically by a hyperparameter search. The Smooth L1 Loss function minimizes prediction errors while balancing the mean absolute error and mean squared error. It is a robust loss function less sensitive to outliers compared to the L2 Loss and has been shown to prevent exploding gradients [45]. It behaves like the L1 Loss when the absolute value of the argument is high and the L2 Loss when the absolute value is small. The loss function is defined as follows, with a beta parameter (\u03b2) controlling the transition:\n$L_{smooth}(y, \\hat{y}) = \\begin{cases} 0.5 \\frac{1}{\u03b2} \u00d7 (y - \\hat{y})^{2} & \\text{if } |y - \\hat{y}| < \u03b2 \\\\ |y - \\hat{y}| \u2013 0.5 \u00d7 \u03b2 & \\text{otherwise} \\end{cases}$ (15)\nThe model is trained on n-1 subjects and validated/tested on the one left-out subject, ensuring a rigorous evaluation of its generalization capability. We train each fold in n = 2000 epochs with a batch size of 460, a weight decay of 0.0906, a learning rate of 0.033, a dropout rate of 0.245, an embedded dimension of 1435, and a fully connected dimension of 1763. After prediction, the scaling of the target variable is inverted to interpret the model's output in the original scale. Performance metrics are then saved for each fold of the cross-validation process. After all folds, the average performance metrics are calculated and reported, providing a comprehensive overview of the model's effectiveness across the entire dataset. We used the Optuna framework with a Tree-structured Parzen Estimator (TPE) Sampler to find the optimal hyperparameters [2]."}, {"title": "A. Architecture", "content": "The proposed model employs an architecture to efficiently process sequential data, pinpointing the most critical timestep for further analysis. The input to the model is a temporal sequence of eye movement data from one subject over a given range. Each sequence contains ten consecutive timesteps, with each timestep consisting of 54 features. Given the dataset's structure, we have approximately 28208 sequences in total, ensuring a comprehensive dataset for Training and evaluating the model's performance. Figure 1 illustrates the model's structure, starting with an input of (1, 10, 54) corresponding to a sample/sequence with ten timesteps of 54 features. The input is fed to an LSTM layer with a substantial hidden size of 1435. This design choice enables the model to capture complex dependencies within the input sequence. Following the LSTM layer, a batch normalization [18] layer stabilizes and accelerates learning by normalizing the activations. Crucially, the architecture incorporates a max pooling [26] layer immediately after batch normalization. This layer acts as an \"information bottleneck,\" selectively emphasizing the most significant timestep in the sequence, shown as the encoding step in Figure 1. By focusing on this key timestep, the model aims to distill the essence of the sequential data, ensuring that subsequent processing is concentrated on the most relevant information. After identifying and isolating the pivotal timestep through max pooling, the model introduces a dropout [47] layer with a probability p = 0.245. This layer randomly excludes a portion of neurons during Training, reducing overfitting and encouraging the model to learn more robust features. The condensed information from the selected timestep undergoes dimensional expansion through two fully connected (FC) layers. The first FC layer enlarges the feature space to 1763 dimensions, allowing the model to explore a broad spectrum of patterns within the critical timestep. The penultimate layer of the model is an exponential linear unit (ELU), which introduces non-linearity to the output, enabling the model to handle complex relationships between the processed timestep and the target variable. A subsequent FC layer decreases the dimensionality to 440, forcing the model to robustly capture intricate patterns before making predictions. This series of operations from focusing on the most informative part of the sequence to expanding the feature space-allows the model to effectively distill and analyze the essence of the input data, providing a robust framework for understanding and predicting based on sequential patterns."}, {"title": "IV. EVALUATION", "content": "Central to our analysis is a dataset comprising eye movement recordings from 25 emmetropic individuals, which spans a dynamic range of focal depths and includes a diverse age range. This dataset lays a robust foundation for assessing FOVAL's performance, notably achieving a mean absolute error (MAE) as low as 5.9 cm.\nOur comparison between FOVAL, the Mix-TCN model [56", "56": "for the Mix-TCN model, nor could we utilize their trained model parameters due to their unavailability. Similarly, retraining the Mix-TCN model on our dataset was not feasible without detailed architecture specifics and training protocols. Under these circumstances, the most pragmatic approach was to rely on the performance metrics published by the author, which are assumed to be representative of their models' capabilities under their respective testing conditions. Although this method does not allow for a controlled, head-to-head comparison under identical"}]}