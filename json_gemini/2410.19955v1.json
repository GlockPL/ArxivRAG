{"title": "DUALMAR: MEDICAL-AUGMENTED REPRESENTATION FROM DUAL-EXPERTISE PERSPECTIVES", "authors": ["Pengfei Hu", "Chang Lu", "Fei Wang", "Yue Ning"], "abstract": "Electronic Health Records (EHR) has revolutionized healthcare data management and prediction in the field of AI and machine learning. Accurate predictions of diagnosis and medications significantly mitigate health risks and provide guidance for preventive care. However, EHR driven models often have limited scope on understanding medical-domain knowledge and mostly rely on simple-and-sole ontologies. In addition, due to the missing features and incomplete disease coverage of EHR, most studies only focus on basic analysis on conditions and medication. We propose DualMAR, a framework that enhances EHR prediction tasks through both individual observation data and public knowledge bases. First, we construct a bi-hierarchical Diagnosis Knowledge Graph (KG) using verified public clinical ontologies and augment this KG via Large Language Models (LLMs); Second, we design a new proxy-task learning on lab results in EHR for pretraining, which further enhance KG representation and patient embeddings. By retrieving radial and angular coordinates upon polar space, DualMAR enables accurate predictions based on rich hierarchical and semantic embeddings from KG. Experiments also demonstrate that DualMAR outperforms state-of-the-art models, validating its effectiveness in EHR prediction and KG integration in medical domains.", "sections": [{"title": "1 Introduction", "content": "The digitization of patient information such as EHR has transformed healthcare in terms of data storage, information retrieval, and computational pattern recognition. EHR-based prediction models, including those for diagnosis, heart failure, and readmission prediction tasks [Shickel et al., 2017], leverage EHR data to forecast patient outcomes. In addition to improving predictive accuracy, a major focus of current research is to build models that understand medical domains from both individual health records [Poulain and Beheshti, 2024] and public knowledge bases [Jiang et al., 2023].\nIndividual health records preserve hidden knowledge of medical concepts, such as disease complications. Many models use EHR data to uncover disease progression patterns and learn medical embeddings, which serve as local-domain priors for downstream tasks. Meanwhile, transformer-based models following G-BERT [Shang et al., 2019] rely on self-supervised tasks to fully utilize EHR data, including single admissions, enhancing training efficiency and robustness"}, {"title": "2 Related Work", "content": "Deep learning models have been extensively applied in EHR prediction to provide guidance for preventive care. Both RNN-based Choi et al. [2016a,b], Ma et al. [2017], Bai et al. [2018], Ma et al. [2020a] and CNN-based models Nguyen et al. [2017], Ma et al. [2020b] have been developed to capture temporal and spatial dependencies for health risk predictions. With the advent of graph neural networks (GNNs), GRAM Choi et al. [2017] and its successors Liu et al. [2020], Wu et al. [2021], Lu et al. [2023] incorporate graph learning to further enhance hidden representation for medical concepts. Meanwhile, previous work such as G-BERT Shang et al. [2019] takes advantage of transformers and their variants to fully utilize EHR data (i.e. single-admission patients) through various proxy tasks Prakash et al. [2021], Rasmy et al. [2021], Poulain and Beheshti [2024].\nPredictive models that leverage graph and transformer architectures have achieved state-of-the-art performance in various EHR tasks, highlighting their potential to advance predictive healthcare with existing knowledge. This knowledge can be divided into local individual health data and public knowledge bases, as outlined below:\n\u2022 Individual Health Data: Prior work such as CGL Lu et al. [2021] has constructed EHR graphs from EHR data based on co-occurrence of medical concepts in admission records to capture hidden relations among diseases and improve health outcome predictions. Encoder-decoder models following G-BERT Shang et al. [2019], focus on customized pretraining tasks to enhance the understanding of medical patterns in EHR data. While individual knowledge aids in adapting models to different datasets, it relies on the size of samples and the quality of data.\n\u2022 Public Knowledge Bases: External knowledge is derived from public data sources without individual information. Researchers have explored simple ontologies like the ICD-9 hierarchy, Retrieval-Augmented methods like RAM-EHR Xu et al. [2024] for unstructured medical descriptions, and KGs for predictive models like GraphCare Jiang et al. [2023]. However, these approaches often focus solely on either hierarchical or semantic information, leading to a narrow understanding of medical concepts and potential information loss, and how to effectively combine both information in models is necessary to enhance robustness of predictive healthcare.\nIn addition to leveraging medical-domain knowledge, incorporating both structured and unstructured features can enhance predictions. MiME Choi et al. [2018] and GCT Choi et al. [2020] use graph structures with lab results to augment representations, while CGL Lu et al. [2021] and MedGTX Park et al. [2022] integrate unstructured clinical notes to highlight the value of additional information. However, most approaches rely heavily on these multi-aspect features, limiting their applicability to data without those features. Our proposed framework integrates laboratory features as auxiliary data in graph construction and proxy tasks, enhancing predictions even when no desired feature is available."}, {"title": "3 Methodology", "content": "We present DualMAR, a framework designed for predictive healthcare that aims to achieve a comprehensive under-standing of diagnosis-related entities from both external (public) and local (individual) knowledge bases. An overview of the proposed framework is shown in Figure 1b."}, {"title": "3.1 General Notation", "content": "An EHR dataset S is a collection of temporal admission records of N patients {P1, P2, ..., PN}, and pu is denoted as certain patient u. Such patient can be also represented as a sequence of Tu admission records (V\u2081u, V2\u201c, .., V\u2081) \u2208 pu in chronological order. We omit u to identify different patients in following sections and explain our framework using single patient as example to avoid misunderstanding. We denote the entire set of medical concepts as C = {C1, C2, ..., C|C|} where |C| is the vocabulary size of medical concepts. Each admission, like t-th admission V\u2081 \u2208 (V1, V2, .., VT), contains a subset of C. Note that, We consider diseases and lab results as medical concepts in proposed framework, and we denote diseases {d1, d2, ..., d|D|} and lab results {11, 12, ..., l|c|} within vocabulary D, L C \u0421."}, {"title": "3.2 Generate \"Knowledge Scholar\"", "content": "In general, the proposed model integrates external knowledge from professional medical datasets and LLM-retrieved KGs based on verified clinical text. Figure la illustrates how to augment structured KG with textual information.\n(1) HKG Construction: Healthcare KGs (HKGs) are healthcare-specific KGs that span multiple datasets [Cui et al., 2023], offering a more comprehensive understanding than all-in-one ontologies like UMLS Bodenreider [2004]. We focus on constructing HKGs that link diseases, drugs, and phenotypes to enable precise clinical predictions. The Human Phenotype Ontology (HPO) and the ICD-9 hierarchy serve as the foundation for our KG, with bi-hierarchical structure enhancing its robustness. Inspired by previous healthcare KGs [Su et al., 2023, Chandak et al., 2023, Ghorbanali et al., 2023], we also incorporate updated and verified sources like Drug Central, DrugBank, and SIDER to create a highly connected HKG. We denote the resulting healthcare KG, sourced from existing databases, as GM, containing M tuples of head, relation, and tail. More details can be found in Appendix A.\n(2) LLM-KG Generation: We extract textual features for specific diseases from widely-used knowledge bases and then employ a template with instruction, example, and prompt to harness LLMs' in-context learning capabilities. For instance, when generating a KG for a diagnostic concept, the LLM generates triples like \u201c[Heart Failure, CAUSED_BY, Hypertension]\" based on the collected text. Iterative prompting and re-reading strategies are used to refine outputs. The aggregated results form an LLM-generated KG GN. Further details on knowledge sources and prompting are in Appendices A.2 and A.3 respectively.\n$G_M = \\bigcup_m (h(M_m), r(M_m), t(M_m))$\n$G_N = \\bigcup_n (h(N_n), r(N_n), t(N_n))$\n$G_H = G_M \\cup G_N, G_H = NORMALIZE(G_H)$ Here, x means the number of iteration for iterative running and and y denotes the number of times the generated output is re-evaluated (re-reading) to refine the quality of triples.\""}, {"title": "3.2.1 Node and Edge Normalization.", "content": "Our approach for normalizing the combined KG GH involves two key steps: (1) Uniform encoding system for entities: In addition to phenotype nodes sourced from HPO, we align disease and drug entities using the ICD-9-CM Organization et al. [1988] and ATC Nahler and Nahler [2009] coding systems, both of which are widely used in EHR data. A cross-referencing method is employed to convert GM and GN into a shared hierarchy. Given that drugs are not the primary focus, we use ATC-4 to simplify the numerous synergistic interactions in DrugBank. (2) Clustering duplicated or unmatched triples: The LLM-prompting process is run multiple times, which can generate duplicated nodes and edges. Moreover, cross-referencing files may not cover all concepts, and unmatched entities leave noise in KG. To address these problems, we calculate cosine similarity for nodes and edges based on their embeddings. Using a predefined threshold 0, we group entities within categories such as \u201cdisease\u201d, \u201cdrug\u201d, \u201cphenotype\", and \"other\", and apply hierarchical clustering to reorganize edges.\""}, {"title": "3.2.2 Advancement Analysis", "content": "To demonstrate the advancements of our proposed Diagnosis KG, we compare it with two recent health-related KGs: GraphCare Jiang et al. [2023] and PrimeKG Chandak et al. [2023]. A statistical summary of KGs is presented in Table 1. The analysis highlights the following advantages of our KG:"}, {"title": "3.3 Polar-space KG Embedding", "content": "Our generated KG features a bi-hierarchical structure, with diagnostic and phenotype nodes organized across multiple levels. Traditional KG embedding methods often focus primarily on contextual information, but our framework gives equal importance to both contextual and hierarchical information. We employ a polar-space embedding Zhang et al. [2020] method upon the Euclidean space due to its superiority in efficiently capturing hierarchical structures compared to other Euclidean embedding. While non-Euclidean methods are also designed for hierarchical structures, they primarily focus on hierarchy alone. In contrast, polar-space embedding not only capture both hierarchical and contextual information simultaneously but also provide a more computationally efficient and interpretable framework, making it well-suited for integrating complex medical knowledge into predictive models. A comparison among KG embedding methods Bordes et al. [2013], Nickel and Kiela [2017], Sun et al. [2019], Fionda and Pirr\u00f2 [2020] is provided in Appendix B.\nTo describe medical concepts from both contextual and hierarchical perspectives, we leverage two properties of the polar coordinate system: (1) Radial coordinate: map entities across different levels of the hierarchy; (2) Angular coordinate: map entities at the same level based on contextual information. We denote entities, including head h and tail t, as e, and relation embeddings as r. Subscripts, such as er and ea, represent the modulus and phase components, respectively. Each entry of hr,a or tr,a corresponds to a radial or angular value, while each entry of rr,a represents a scaling transformation between the corresponding head and tail. The radial and angular coordinates of the entity embeddings can then be formulated as follows:\n$h_r \\oplus r_r = t_r, h_r, t_r, r_r \\in R^k$\n$(h_a + r_a) \\mod 2\\pi = t_a, h_a, t_a, r_a \\in [0, 2\\pi)$\nAccording to the coordinates for triples, we can further calculate radial and angular distances as follows:\n$d_r (h_r, t_r) = ||h_r \\oplus r_r - t_r||_2,$\n$d_a(h_a, t_a) = ||sin((h_a + r_a - t_a)/2)||_1$\nwhere $|| ||_1, || ||_2$ denote the l1, l2 norm. The final circular arc distance d(h, t) can be consider as the weighted sum of both radial and angular distances. Note that, we can get h and t by concatenating both radial and angular vectors. Such embedding model then be pretrained individually through link prediction tasks before integrating into the predictive model, which can help us understand priors from KG into trainable embeddings. The regular negative sampling loss function L is adopted for optimization:\n$L = - log \\sigma(\\gamma - d(h, t)) - \\sum_{i=1}^{n} log \\sigma (d(h', t') - \\gamma)$\nwhere \\gamma and \\sigma are marginal value and sigmoid function respectively, and (h', r, t') is a negative triple. Therefore, we can get diagnosis embedding matrix ED for condition codes, which is the initial nodes features for \u201cLocal Expert\"."}, {"title": "3.4 Continual Enhancement from \"Local Expert\"", "content": "The continual enhancement process unfolds in two steps: graph learning and proxy-task learning, which together refine the priors from the \"Knowledge Scholar\" by fully leveraging EHR data. The proposed encoder-decoder framework not only emphasizes the comprehensive understanding of medical patterns but also seamlessly integrates local knowledge into the learning process, further capturing critical medical patterns for precise diagnosis."}, {"title": "3.4.1 Encoder Module.", "content": "The backbone of the encoder module contains graph learning and attention, which adjusts code-level embeddings and progressively refines them into comprehensive patient representations.\nWe construct a disease-complication graph G including both condition and laboratory codes. Compared to regular complication graphs involving only diseases, we have broader scope to explain complication in terms of shared abnormal lab tests in the same admission. To represent the complications between two diseases via patient admissions, we add bidirectional edges (ci, cj), (ci, cm), and (cj, cm) in graph G, where ci, cj \u2208 D are diagnostic codes and cm \u2208 L is a laboratory code. Therefore, we then compute co-occurrence matrix B \u2208 N|C|\u00d7|C| and adjacent matrix A = (1 \u2013 \u03b4)B + \u03b4I \u2208 R|C|\u00d7|C| considering self loop based on EHR graph G, where I is an identity matrix.\nA GNN with L layers is adopted by EHR graph G and node features E, where we initialize diseases by priors ED from KG. Then, the hidden representation H(\u00b9) can be calculated by input H(1-1) through the l-th Convolution layer: After the last GNN layer, we obtain the hidden representation of diseases X = GNN(A, E) = H(L), which can be further refined through the bi-attention mechanism across codes and admissions for patient embedding p:\n$V = \\sum_{i=1}^{n} \\alpha_i x_i, p = \\sum_{T=1}^{T} \\beta_T v_T,$\nwhere \u03b1 and \u03b2, are attention scores computed as follows:\n$z_i = tanh(W_c x_i), r_i = tanh(W_u \\sigma (W_v v_T))$\n$\\alpha_i = \\frac{exp(z_i)}{\\sum_{j=1}^{n} exp(z_j)}, \\beta_T = \\frac{exp(r_T)}{\\sum_{T=1}^{T} exp(r_T)}$\nWe, Wu, and W are weighted matrices. The attention score \u03b1f measures the distribution of medical codes of admission, and \u03b2 measures the distribution across admissions. Note that, we project each admission v, to v, for fitting the patient dimension through a single layer with Wu. Detailed explanation about attention is provided in Appendix C.\nThe proposed encoder module ensures that both the specificity of individual medical encounters and the comprehensive history of a patient's health are reflected in the final patient embedding p, laying a robust foundation for the subsequent self-supervised learning phase."}, {"title": "3.4.2 Proxy-Task Learning.", "content": "Given patient embedding p, we introduce a novel proxy-task learning paradigm. Some predictive models in EHR focus on customized proxy tasks to utilize single admission which is discarded in training process. However, designed labels of proxy tasks should not only reflect historical pattern of previous admissions, but also closely connect to downstream labels for learned patient embeddings p. Therefore, we harness the insight of lab results in our learning model, which is unexplored as auxiliary features within pretraining module. By pretraining the model on laboratory results, we aim to mimic the diagnostic reasoning process of real physicians, who often rely on lab data for accurate assessments.\nWe denotes lab results as L, and the designed proxy task makes predictive model inherently understands intricate relations between abnormal lab tests and health conditions. Considering patterns always change across different lab tests, we further categorize lab results into distinct groups, which can fully cover involved test items in EHR data. Here we take lab tests in MIMIC as an example:\n1. Hematology (# items: 417): Analyze blood components like red and white blood cells to monitor conditions.\n2. Chemistry (# items: 286) Evaluate chemical factors in blood and provide information about organ's function.\n3. Blood Gas (# items: 35) Measure the levels of gas in blood to assess respiratory and metabolic functions.\nHence, we categorize lab tests as L1, L2, and L3 in order, and predict lab results \u0177i,j = Decoder(p) based on both single-admission and multi-admission data in training set, where Yi,j = 1 means lab code lj shows abnormal results, and a multi-layer perceptron (MLP) is used as decoder for learning patterns in different lab categories. We only focus on abnormal lab tests across admissions, so the ground truth [11, 12, ..., l|c|] are labeled as multi-hot vectors. To ensure the lab embeddings are well-aligned with each category, the proxy task is designed to be completed in two steps:"}, {"title": "3.5 Fine-Tuning and Inference", "content": "The proposed model can be trained with EHR data either with or without lab tests, making it adaptable to data missing desired features. If training with only diagnostic information, we can directly compute the estimated output \u0177' and the downstream loss L' for optimization:\n$\u00dd_{direct} = \\sigma(Wp) \\in R^{O}$\n$L'_{direct} = Loss(y', y')$\nIf lab features are included, the model leverages pretrained parameters to get both adjusted patient embedding p and lab embeddings 11, 12, and 13. The downstream outputs y' and fine-tuning loss L' are then calculated as follows:\n$\u00dd_{finetune} = \\sigma(W[l_1 : l_2 : l_3]|p) \\in R^{O}$\n$L'_{finetune} = Loss(y', y'|\u0398)$\nHere, W \u2208 RO\u00d7P is the weight matrix and [:] means concatenation. Note that y', \u03c3, \u03bf, and L' depend on the specific tasks. During optimization with backpropagation, the embedding matrix and parameters remain learnable."}, {"title": "4 Experiments", "content": "To evaluate our proposed model, we focus on two pubic and widely-used EHR datasets: MIMIC-III Johnson et al. [2016] and MIMIC-IV Johnson et al. [2020]. We focus on MIMIC-III as the primary dataset for our experiment and divide it into 6000/500/1000 for the training, validation, and test sets, respectively. The basic statistics of MIMIC-III and MIMIC-IV and results of MIMIC-IV are shown as supplement at the Appendix E.\nNote that, we selected MIMIC-III as the primary dataset for two key reasons: it facilitates direct comparison with existing baselines commonly evaluated on MIMIC-III, ensuring reproducibility, and it natively supports ICD-9 codes, aligning with knowledge encoding base in baselines and our Diagnosis KG. Using MIMIC-IV would require mapping from ICD-10, potentially introducing errors that could affect model evaluation."}, {"title": "4.1.2 Baselines", "content": "To evaluate the performance of our proposed model, we selected the following 10 EHR models as comparison methods: (1) RNN/CNN-based models: Dipole Ma et al. [2017], Deepr Nguyen et al. [2017], RETAIN Choi et al. [2016a] and Timeline Bai et al. [2018]. (2) Graph-based models: GRAM Choi et al. [2017], KAME Ma et al. [2018], and CGL Lu et al. [2021]. (3) Transformer-based models: G-BERT Shang et al. [2019], HiTANet Luo et al. [2020], and THAM Li et al. [2024].\nNote that although GraphCare Jiang et al. [2023] is another KG-based method, it is not included as a baseline in experiments because its KGs cannot be directly transferred from CCS codes to ICD-9 codes due to limited GPU memory. Supportive details will be provided in the following case study. Implementation details are discussed in Appendix D."}, {"title": "4.1.3 Tasks & Evaluation Metrics.", "content": "We conduct our experiments on two tasks as downstream examples for clinical prediction, following the similar settings as in previous studies Choi et al. [2017], Lu et al. [2021]:\n1. Diagnosis Prediction. This task involves multi-label prediction, where the goal is to predict all condition codes for the next admission based on admission history.\n2. Heart Failure Prediction (HF). This is a binary classification task that predicts whether patients will be diagnosed with heart failure in their next admission.\nGiven the label imbalance in EHR data, we use the weighted F\u2081 score (w-F\u2081) and recall at k (R@k) as evaluation metrics for diagnosis prediction and use the area under the ROC curve (AUC) and the F\u2081 score for HF prediction."}, {"title": "4.2 Prediction Results", "content": "Table 2 demonstrates that DualMAR consistently outperforms existing baselines across both diagnosis and HF prediction tasks for MIMIC-III dataset. The proposed framework consistently performs the best, underlining the efficacy of external priors from clinical KGs for prediction."}, {"title": "4.2.1 Real-time & Non-chronic Diagnosis Prediction.", "content": "Table 3 shows the results of two supplementary experiments, real-time and non-chronic diagnosis predictions, respec-tively.\n1. Real-time (RT) Diagnosis: To fairly compare performance of all baselines, we feed lab tests information for all baselines and proposed model, which not only augments representation for diagnosis prediction but also mimics real-time diagnosis of physician within the beginning of an admission. We can observe that the proposed model still outperforms all baselines, which has even a larger improvement on w-F\u2081 over regular diagnosis. We suppose that DualMAR, which originally relies on lab tests for auxiliary diagnosis, will be more suitable for lab features input than other \"uni-aspect\" baselines.\n2. Non-chronic (NC) Diagnosis: Since MIMIC-III is mostly ICU data, we remove all chronic diseases in predictions and focus on predicting acute condition codes for future visits. We can observe that DualMAR still achieves superior performance, but all baselines and proposed model get relatively low values of metrics compared to regular diagnosis. Note that, R@20 will be omitted in report since the average number of diagnoses is less than 10 within each admission. We conjecture that the removal of chronic diseases in admissions skews the distribution, making rare diseases more prominent and, consequently, making the prediction task more challenging."}, {"title": "4.3 Ablation Study", "content": "To evaluate the effectiveness of involved modules in DualMAR, we compare performance of variants with some modules removed or replaced. We use both future diagnosis and HF prediction tasks on MIMIC-III as examples:\n1. DualMARa: Removing Diagnosis KG and KG embedding modules, initializing embeddings randomly.\n2. DualMARb: Replacing the embedding of the Diagnosis KG with GloVe, as used in GRAM Choi et al. [2017], to only focus on hierarchical information.\n3. DualMARc: Replacing the embedding of the Diagnosis KG with text-embedding-3-large OpenAI [2024] to only retrieve semantic information.\n4. DualMARd: Removing the Graph Learning module and directly feeding KG embeddings into the Attention layer.\n5. DualMARe: Replacing EHR graph by a simplified graph with disease complications, excluding lab test nodes.\n6. DualMARf: Replacing Attention by GRU modules.\n7. DualMARg: Removing the proxy-task learning module and training the model directly on the downstream task.\nWe report the results of DualMAR and its variants DualMARa-g in Table 4. The full DualMAR model consistently out-performs all variants, proving the effectiveness of our framework The comparisons between DualMAR and DualMARb-c highlight the importance of both hierarchical and semantic information from the KG, even though each alone can en-hance predictions based on DualMARa. Additionally, the graph learning DualMARd and attention modules DualMARf are critical for refining representations, with the inclusion of lab tests further enhancing model performance in terms of DualMARe. The significant improvement of DualMAR over DualMARg underscores the essential role of the proxy-task learning module.\nNotably, for cases without lab test data, the No LAB model, which uses modified GNN and skips pretraining, still achieves top-tier performance among baselines, demonstrating the effectiveness and generalizability of DualMAR."}, {"title": "4.4 Case Study", "content": "We also design experiments focusing on predicting less frequent condition codes in MIMIC-III, similar to rare ICD code prediction tasks. We select and predict 100 ICD codes that occur fewer than 10 times in the dataset. We follow the previous settings which excludes records without rare codes to stabilize training. We use recall at 5/8/15/20 for evaluation. Figure 2a compares DualMAR with baselines using ontology or transformer architectures. DualMAR outperforms other selected methods on R@5, R@8, R@15, and R@20. Notably, CGL and G-BERT show significant improvements, indicating that external knowledge bases can aid in diagnosing rare conditions. This underscores the importance of incorporating medical ontologies or complex KGs to enhance model resilience against EHR data limitations."}, {"title": "4.4.2 Advanced Analysis of KG Embeddings", "content": "To assess the importance of each component in our Diagnosis KG, we divided it into three parts: ICD9-Hierarchy KGa, Ontology-KG KG\u266d, and LLM-KG KG. Note that, the reason why both KGa+b and KGa+b+c are ignored in experiment is that knowledge in KGa is mostly covered by KG\u266d, so that they will get same results as other existing settings. We then evaluated the predictive performance of the model with these varying complexities. Figure 2b shows how the complexity and embedding dimensions of KGs influence diagnosis prediction. Key observations include: (1) LLM-generated triples (KG) enhance comprehensive KG representation, as seen in KGa+c and KGb+c. (2) Ontology-KG (KG\u266d) from diverse data sources is more reliable for accurate predictions than ICD9-Hierarchy (KGa). (3) Embedding dimensions significantly impact predictions; higher dimensions offer a deeper understanding but can lead to overfitting, especially with simpler KGs like KGa."}, {"title": "4.4.3 Accessibility", "content": "We evaluated the accessibility of DualMAR with large-scale KGs, comparing it to GraphCare Jiang et al. [2023] using an NVIDIA RTX-4090 GPU with 24GB of memory. For a fair comparison, we adapted UMLS-GPT-KG in GraphCare to our Diagnosis KG, adjusting only the batch size due to its end-to-end structure without pretraining module. Table 5 shows average GPU usage for diagnosis prediction. GraphCare struggles with training on common commercial GPUs when transitioning from CCS KG to ICD-9 KG, even with small batch sizes, limiting its scalability with large KGs. In contrast, DualMAR efficiently separates learning processes for KGs and EHR data, allowing it to scale to larger KGs and making it more deployable across various EHR tasks and datasets."}, {"title": "5 Conclusion", "content": "In this study, we introduced DualMAR, a framework designed to enhance EHR prediction tasks by leveraging both individual health records and public knowledge bases. Through the construction of a bi-hierarchical diagnosis KG augmented by LLMs and the design of a proxy-task learning via lab results, DualMAR significantly improves the representation of patient embeddings and disease-related predictions. Our experiments demonstrate that DualMAR outperforms existing state-of-the-art models, highlighting the potential of integrating comprehensive hierarchical and semantic information from KGs into EHR models. This approach not only broadens the scope of medical-domain knowledge but also addresses the limitations of traditional EHR models by incorporating more complex and nuanced medical data. Future work could explore expanding DualMAR to adapt to large-scale KGs and apply it to a broader range of healthcare prediction tasks beyond diagnostics. Additionally, this work highlights the crucial role of laboratory data in clinical decision-making, suggesting that further investigation into leveraging lab tests as auxiliary inputs for both diagnosis and treatment predictions remains a promising direction."}, {"title": "A KG Construction", "content": "To develop a comprehensive KG for studying diseases, we consider two primary resources related to diseases and phenotypes, and incorporate three additional resources to enhance the connectivity of our Diagnosis KG. Notably, these data resources-whether widely-used standardized ontologies or direct readouts from experimental measure-ments-primarily focus on extensive coverage across disease, phenotype, and drug entities. We begin with a brief introduction of primary data resources used to construct component of the Diagnosis KG:"}, {"title": "A.1 Data Sources in Healthcare KG.", "content": "1. DrugBank. DrugBank Knox et al. [2024] is a comprehensive resource containing detailed pharmaceutical knowledge. We retrieved the latest version (5.1.12), published on March 14, 2024. Our focus is on synergistic drug interactions, which represent the bidirectional connections between two drugs.\n2. DrugCentral. DrugCentral Ursu et al. [2016] is a curated database that provides information on drug-disease interactions, including indications, contraindications, and off-label uses. We utilized the updated SQL Database, released on November 1, 2023, for our study.\n3. HPO. The Human Phenotype Ontology Gargano et al. [2024] offers detailed information on phenotype abnormalities associated with diseases. We used one of the most recent updates, from April 19, 2024, focusing on disease-phenotype and phenotype-phenotype relationships, and extracted verified associations with expertly curated annotations.\n4. ICD-9-CM Disease Ontology. ICD-9-CM Organization et al. [1988] is a coding system used by health insurers to classify medical conditions for billing purposes, and it underlies the diagnostic information in most EHR data. For our purposes, we extracted the parent-child relationships among codes to describe disease interactions.\n5. SIDER. The Side Effect Resource Kuhn et al. [2016] contains data on side-effect phenotype caused by various drugs. We retrieved this information from the data release dated October 21, 2015."}, {"title": "A.2 Data Sources in LLM-generated KG.", "content": "We also extract textual features for disease nodes in the KG from several widely-used clinical knowledge bases. The in-context learning capabilities of LLMs are employed to convert this textual information into triples, which are then integrated into the KG. The following four public data sources are used for triple generation:"}, {"title": "A.3 Prompting LLM", "content": "It is important to note that since all textual information for disease nodes is sourced from either publicly verified or expertly curated data sources, issues of data quality or hallucination in the medical knowledge generated by the LLM"}, {"title": "B KG Embeddings Comparison", "content": "Most current hierarchical KG embeddings are designed to capture and represent hierarchy information in KGs using various geometric structures, including Euclidean and non-Euclidean geometries. Euclidean-based methods typically focus on projections across different coordinates to distinguish entities at various levels. Compared to non-Euclidean approaches like hyperbolic and spherical embeddings, Euclidean projections can efficiently retrieve hierarchical information without additional processes or high computational costs. Additionally, we can separately train embeddings through link prediction tasks, allowing GPU usage to be decoupled from the graph learning process, ensuring stable performance even with more complex KGs. We propose projecting each entity in the KG onto polar coordinates, which enables more efficient embedding retrieval."}, {"title": "C Details in Multilevel Attention", "content": "After generating X for all medical codes appears in the intersection of both training set S and concept vocabulary C", "X": "np = Bi-Attention(V1", "Bi-Attention\u201d means that we implement a two-tier attention mechanism, detailed as follows": "na) Learning Admission Embeddings: Consider an admission record V", "mechanism": "n$E_v = [x_1", "x_2,...,x_n": "in R^{nxm"}, "n$z_i = tanh(W_c x_i) \\in R^{a}$\n$V = \\sum_{i=1}^{n} \\frac{exp(z_i)}{\\sum_{j=1}^{n} exp(z_j)}x_i \\in R^{m}.$\nHere, We \u2208 Ra\u00d7m is the weight matrix, where a represent the attention dimension. The attention scores \u03b1\u03c4 = [a, a,..., a"], "Embeddings": "Once the embedding v", "classifier": "n$\u03bd_T = \u03c3(W_u v_T) \u2208 R^P$.\nHere, Wu \u2208 IRP\u00d7m is the corresponding weight matrix, and p denotes the dimension of the patient embedding. The activation function is denoted as \u03c3.\nAssume patient u has T admission records as input features of model, we can get the"}