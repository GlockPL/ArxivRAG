{"title": "GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models", "authors": ["Ali Abdollahi", "Mahdi Ghaznavi", "Mohammad Reza Karimi Nejad", "Arash Mari Oriyad", "Reza Abbasi", "Ali Salesi", "Melika Behjati", "Mohammad Hossein Rohban", "Mahdieh Soleymani Baghshah"], "abstract": "Vision-language models (VLMs) are intensively used in many downstream tasks, including those requiring assessments of individuals appearing in the images. While VLMs perform well in simple single-person scenarios, in real-world applications, we often face complex situations in which there are persons of different genders doing different activities. We show that in such cases, VLMs are biased towards identifying the individual with the expected gender (according to ingrained gender stereotypes in the model or other forms of sample selection bias) as the performer of the activity. We refer to this bias in associating an activity with the gender of its actual performer in an image or text as the Gender-Activity Binding (GAB) bias and analyze how this bias is internalized in VLMs. To assess this bias, we have introduced the GAB dataset with approximately 5500 AI-generated images that represent a variety of activities, addressing the scarcity of real-world images for some scenarios. To have extensive quality control, the generated images are evaluated for their diversity, quality, and realism. We have tested 12 renowned pre-trained VLMs on this dataset in the context of text-to-image and image-to-text retrieval to measure the effect of this bias on their predictions. Additionally, we have carried out supplementary experiments to quantify the bias in VLMs' text encoders and to evaluate VLMs' capability to recognize activities. Our experiments indicate that VLMs experience an average performance decline of about 13.2% when confronted with gender-activity binding bias.", "sections": [{"title": "1 Introduction", "content": "Nowadays large multi-modal models have shown tremendous potential in various tasks, from information retrieval systems to image captioning, visual question answering, image generation, and visual reasoning. As a branch of multi-modal models, Vision-Language Models (VLMs) [27] that provide a shared cross-modal embedding space between text and image modalities, have been intensively investigated by researchers recently and are used in many real-world applications [50, 30, 9].\nDespite their wide range of success in downstream tasks, VLMs are subject to many critical biases, which can consequently affect their performance, especially in sensitive applications. For example, Agarwal et al. [2] provides a comprehensive analysis of biases and their implications in the CLIP Models, revealing how this model is exposed to gender biases and how challenging it is to trust it in real-world applications. As Srinivasan and Bisk [34] and Lee et al. [18] state, the bias is introduced in visual-linguistic pre-training due to unfairness in the training data, and additionally, at inference time, the visual and linguistic contexts that is used for few-shot applications can also promote bias.\nOne important type of bias, which is highly discussed in the literature, is the gender bias. Gender bias demonstrates the undesirable association of a factor with a gender according to the model. For instance, due to the high occurrence of data in which men are repairing devices, this activity is usually associated with men by deep models, which leads to failure when faced with women repairing devices. Hall et al. [10] suggests that different VLMs do not perform equally well on determining gender for an occupation, and do not assign equal retrieval likelihood to images of male and female professionals. Also, Lee et al. [18] and Wang et al. [40] show that CLIP [27] shows biased behavior in retrieval based on gender-neutral queries. Please note that, in line with common practice in the literature, our focus is on two genders, masculine and feminine. This approach streamlines the process of generating the dataset and analyzing the experiments.\nIn this research, we delve into the issue of gender-activity binding in retrieval tasks. We aim to retrieve the corresponding caption or image from two given captions or images, each depicting an activity performed by a different gender, based on a provided image or caption that shows one of the genders performing the activity. We refer to this bias in associating gender and activity as the Gender-Activity Binding (GAB) bias. This bias arises due to ingrained gender stereotypes in the model or other types of sample selection biases [4]. Our research indicates that VLMs do not display a substantial bias in retrieval tasks when both text and image modalities depict only one gender. This is because the identification of the performer's gender can directly lead to the correct output, eliminating the need to bind the performed activity with the performer's gender. Though, text encoders have shown a considerable bias towards the expected gender. However, The bias becomes more pronounced when the scene is more complex, i.e., when two individuals of different genders are present in the image. We observe a drop in retrieval accuracy in scenarios where the activity performer is the unexpected gender. For example, in the context of the \"repairing\" activity, the retrieval accuracy of VLMs in identifying the performer decreases when a woman is repairing a device and a man is also present in the scene. This is in comparison with scenarios where the performer is a man, or there is no man present in the scene to associate the activity with him.\nThe significance of this bias escalates when we consider its underlying implications. Not only does it perpetuate societal biases and fairness concerns related to gender stereotypes, but it can also lead to serious complications if incorporated into judgement and decision-making systems. The potential for such bias to inadvertently influence outcomes underscores the importance of addressing it.\nWe have created a dataset known as the Gender-Activity Binding (GAB) dataset as shown in Figure 1(a). This dataset comprises images depicting various activities performed by men or women in different contexts (with or without the presence of an individual of the opposite gender), each accompanied by a descriptive caption. Given that our scenarios are uncommon due to biases and stereotypes, suitable real-world images are scarce for evaluation purposes. To address this issue, all images are generated using DALL-E 32. We employ extensive prompt enhancement techniques to ensure the diversity and quality of the generated images. We also evaluated the generated images on diversity, quality, and realism. We performed multiple quantitative assessments consisting of Fr\u00e9chet Inception Distance (FID) for both quality and diversity assessment, Structural Similarity Index (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) for diversity assessment, and human feedback for a final qualitative assessment to ensure the final results accurately represent the intended activities and their related contexts. The activities included in GAB dataset are sourced from various places to encapsulate societal gender stereotypes and other activities that are statistically biased towards a specific gender, as observed in the LAION-400M dataset [31].\nWe benchmark the performance of 12 renowned multi-modal foundation models and assess their performance in terms of text-to-image retrieval, image-to-text retrieval, and recognizing activities on GAB dataset as shown in Figure 1(b). Specifically, in the image-to-text retrieval task, we observed that while an unexpected gender is doing a stereotypically biased activity, the average performance of VLMs declined by approximately 33.2% due to the presence of the expected gender in the scene. Moreover, most VLMs undergo an average accuracy reduction of approximately 13.2% when encountering gender-activity binding bias. On the other hand, in the text-to-image retrieval task, most VLMs achieve an accuracy of approximately 50%, indicating that their performance is nearly random, and they fail to recognize the performer of the activity based on the text.\nAs far as we are aware, this research is the first to delve into the origins of the association between genders and activities in vision-language models by conducting a thorough analysis both image-to-text and text-to-image retrieval accuracy of VLMs. It encompasses not only straightforward scenarios involving single individuals, where VLMs demonstrate high accuracy for both genders [14, 3, 35], but also more complex situations with individuals of different genders [10]. This comprehensive approach allows us to isolate the impact of the complexity of the scenario on performance decline [38] and more accurately measure the Gender-Activity Binding bias. Additionally, the study carries out supplementary experiments to investigate the bias in VLMs' text encoders and their proficiency in recognizing activities.\nTo summarize, our contributions are as follows:\n\u2022 We have created a novel dataset, known as the gender-activity binding dataset. This dataset includes AI-generated images of various activities that are typically associated with a specific gender, and are excellent in terms of diversity, quality, and realness (Section 4).\n\u2022 We conduct an intensive performance benchmark of well-known vision-language models in retrieval tasks to assess their robustness against the Gender-Activity Binding bias. Our findings reveal that when two individuals of different genders are present, VLMs exhibit a bias towards binding the activity with the gender that is expected to perform it, resulting in a 13.2% drop in retrieval accuracy. However, in scenarios involving a single individual, these models demonstrate high image-to-text retrieval accuracy. We also demonstrate that VLMs lack the ability to bind gender and activity in text-to-image retrieval tasks (Section 5).\n\u2022 We carry out additional experiments to investigate the binding bias in VLMs' capability to comprehend activities and the bias present in their text encoders (Section 5 and the Appendix). We delve into the insights that can be derived about how VLMs internalize this bias and its impact on the shared embedding space of VLMs (Section 6)."}, {"title": "2 Related Work", "content": "VLMs apply contrastive training to bridge the image and text embedding spaces. Using this shared embedding space, VLMs have exhibited a remarkable performance on a notable number of multi-modal downstream tasks such as image captioning, text-to-image and image-to-text retrieval, visual question answering, and text-to-image generation. As Lee et al. [18] and Li et al. [19] mentioned, VLMs can be categorized into three main groups based on their architectures: I) Fusion VL-encoders such as FLAVA [33] and LXMERT [37], II) Dual-stream encoders like CLIP [27], GLIP [21], and FILIP [43], and III) VL Encoder-decoder including BLIP [20]."}, {"title": "2.2 Gender-activity Binding Bias", "content": "Previous works discussed that VLMs have many categories of biases [18], especially gender-related ones [10]. For example, Srinivasan and Bisk [34] studied bias in VLMs as a task of measuring associations between entities and gender in visual-linguistic models using template-based masked language modeling. Moreover, Zhang et al. [48] measured gender bias in VLMs by comparing the MASK token prediction probabilities of factual and counterfactual samples and concluded that VLMs inherit human stereotypes from the training data.\nOn the other hand, there exist a number of studies including a restricted assessment of how a VLM understands a particular activity [38, 45, 49]. For example, Thrush et al. [38] has introduced a small dataset called Winoground containing objects, attributes, and activities to evaluate the ability of VLMs in compositional reasoning. Moreover, Zhao et al. [49] designed VL-CheckList framework to study the capability of VLMs in understanding objects, attributes, and relations containing activities. However, to the best of our knowledge, there is no systematic study on the ability of VLMs in associating an activity with a gender."}, {"title": "3 Gender-Activity Binding Bias", "content": "Despite the recent increase in large-scale data collection, it's important to note that these datasets are not necessarily devoid of biases. These biases can manifest in large datasets derived from real-world distributions, primarily due to sample selection bias. The issue causes VLMs to have varying levels of accuracy in recognizing the performer of an activity depending on the gender of the individual involved [48]. We refer to this bias in associating an activity with the gender of its actual performer in an image or text as Gender-Activity Binding Bias.\nConsider an image-to-text retrieval task, where we have two pieces of text C1 and C2, each describing an activity performed by a different gender. We also have an image I\u2081 depicting a person (either a man or a woman) performing an action. The task is to match the image with the correct piece of text (Figure 1-(a) left). Also, consider the text-to-image retrieval task, where we have two images I1 and I2, each showing an activity being performed by a specific gender, and we want to retrieve the correct image based on a given piece of text C1 (Figure 1-(a) right). The Gender-Activity Binding bias manifests as a decline in the retrieval accuracy of VLMs when they incorrectly identify the gender of an individual performing activities typically associated with a different gender.\nLet's denote the probability of a VLM correctly identifying the performer of an activity in a data i (image or text) as p, the activity as a, and the gender as g. Ideally, the probability p(g|a, i) should be equal for all g. However, because of the co-occurrence of gender g of the performer and the activity in the dataset, which is referred to as sample selection bias [4], the gender of the performer becomes correlated with the activity. Thus, we often find that p(g1|a, i) > p(g2|a, i), where g1 and g2 represent different genders and g1 shows the expected gender for the performer of a. It is also In simple terms, VLMs tend to incorrectly select the text or image because they are biased towards associating the activity with a specific gender. This is not always accurate, leading to errors in scenarios where the activity is performed by the less expected gender.\nThe accuracy of models in image and text retrieval tasks is assessed in two scenarios: one where only a person of an unexpected gender (in terms of the bias) is depicted or described in the text or image, and another where both genders are present in the data, but the activity is performed by only one of them. Consequently, the impact of the bias can be observed as a decrease in accuracy in two situations: when the expected gender is absent or present, and when the activity is performed by the unexpected or expected gender. These cases together provide better insights into how these models handle gender-activity associations under different conditions. The first scenario allows us to understand how well the models can recognize and correctly associate an activity with an unexpected gender when the other gender is not present. This helps us gauge the models' ability to break away from societal stereotypes and biases. The second scenario, on the other hand, tests the models' ability to correctly identify the performer of an activity when both genders are present, which is a more complex and realistic situation. This indicates these biases affect the models' performance in real-world scenarios. Together, these cases provide a comprehensive understanding of the models' strengths and weaknesses in handling gender-activity binding, paving the way for targeted improvements to mitigate the gender-activity binding bias."}, {"title": "4 Dataset", "content": "This section details the creation process of the Gender-Activity Binding (GAB) Dataset, which serves to evaluate various VLMs against the gender-activity binding bias we have identified and VLMs ability to bind activities and performers. This dataset comprises AI-generated images that illustrate a variety of activities with potential subjects of both genders.\nThe GAB dataset divides images into four distinct groups: In two groups, both genders are present in the scene. In one of these groups, the expected gender is performing the activity while the other gender is just present in the image, and in the other group, the unexpected gender is performing the activity while the other gender is also in the scene. In the next two groups, only one gender is present in the scene. In one of them, the expected gender is performing the action, and in the other one, the scenario is reversed, with the unexpected gender acting.\nFor each activity, with respect to these grouping, we can consider four groups based on the performer of the activity (expected (E) or unexpected (U)) and the number of persons (with different genders) in the scene (1 or 2): E1, E2, U1, U2.\nFor each image in the dataset, we have templates that are replaced with activity or gender names, such as \"man\" and \"woman,\" in accordance with the designed experiments. Details of the experiments and these replacements can be found in Section 5. The template for images containing two genders is: \"a <man/woman> is  and a  is in the scene\" and for images containing one gender, the template is: \"a <man/woman> is \". This specific template can also be used for images with two genders. Each of the mentioned groups contains at least 26 images for each activity, resulting in a total of 5500 images.\nThis dataset structure facilitates the evaluation of gender-activity binding bias and the ability of VLMs to bind and recognize activities by definition of different retrieval tasks, as you can see in Section 5."}, {"title": "4.1 Selection of Activities", "content": "We proposed three methods to identify the biased activities, all utilizing Masked Language Modeling (MLM) as part of their approach to detecting biased activities. We utilized the 'ROBERTa-large' [23] model for its superior performance in masked token prediction, which was attributed to its objective function. The sentences are formatted as \"a <mask> is doing \". We then calculate the probabilities associated with \"man\" and \"woman\" (as well as related masculine and feminine identifiers like \"boy\" and \"girl\"), from which we calculate the log ratios, as specified in Section 4.1.1."}, {"title": "4.1.1 Stereotyped Activities from GPT Combined with MLM", "content": "Our first method involved using GPT-4 to request activities stereotypically associated with men or women in society. We provided over 100 activities from GPT-4 and formed sentences by replacing the  token in the template of 'a <mask> is doing '. We then inputted these sentences into the RoBERTa model and calculated the following equation for the sentences.\n$$bias(sentence) = log (\\frac{P(mask = man|sentence)}{P(mask = woman|sentence)})$$"}, {"title": "4.1.2 Everyday Activities from GPT Combined with MLM", "content": "In our second approach, we used GPT-4 to compile a list exceeding 1,000 sentences related to hobbies, jobs, and everyday activities. We then masked the subjects of the sentences just like in the first method and asked the RoBERTa model to fill the mask. We stored the log ratios of predicted probabilities. The log ratios formed a normal distribution, from which we selected sentences at each end."}, {"title": "4.1.3 Activities from LAION-400M Dataset Combined with MLM", "content": "In the third method, we used the LAION-400M [31] dataset, which consists of 400 million image-caption pairs. We filtered out the NSFW pairs and sampled 20 million pairs randomly. We then extracted the verbs, subjects, and objects of all caption sentences using spaCy [13], a natural language processing toolkit. We counted the number of masculine and feminine subjects for each verb-object pair and calculated their log ratios. This also formed a normal distribution.\nWe then used the Z-test to determine whether each verb-object pair had a significantly different ratio compared to the mean of all verb-object pairs. This test is appropriate due to the large dataset size of LAION-400M. By checking if differences are statistically significant, we identify biased activities. This method enhances the reliability of our activities by ensuring they are not a result of random variance or outliers. Then, we created sentences with the biased verb-object pairs as mentioned in Section 4.1.1, masked the subject, and submitted them to the ROBERTa model to fill the mask, conducting the same experiment as in the previous methods to select the biased activities."}, {"title": "4.2 Generation", "content": "The generation process consisted of two phases. The first involved generating a good prompt that helped the image generation system meet the criteria given below, derived from the base sentence. The second involved generating the image from the prompt. The generated images should accurately represent the original activity, look natural and high quality, and be diverse."}, {"title": "4.2.1 Prompt Enhancement", "content": "We used a Large Language Model (LLM) to generate diverse prompts while also keeping the original activity intact (Details about the utilized LLM are provided in the Appendix.). We then created a list of predefined settings like the environment of the house, the skin color of the person, etc. Although we could have sampled N prompts from each setting to generate diverse images, we chose to create a list of different combinations of these settings and encode them using MiniLM [41] sentence transformer [28]. We then clustered them into K5 clusters and sampled samples from each cluster to enforce more diversity semantically. We then generated a diverse prompt by instructing Llama2 to generate diverse prompts that maintain the original activity while describing the environment and the settings using the selected samples. Using these prompts, the generated images were significantly improved in quality and detail."}, {"title": "4.2.2 Image Generation", "content": "We initially experimented with several diffusion models, as mentioned in the Appendix, to generate images. The generated images were realistic and diverse. However, they lacked representation of the intended activity, especially in the context of compositional generation. We then explored DALL-E3 [24] using OpenAI API. This approach resulted in significant improvement to the quality of compositional generation while also keeping the reality and the clarity of the image. We chose DALL-E3 as our final image generation solution."}, {"title": "4.3 Image Filtering Methodology", "content": "Our image filtering methodology employs a combination of automated metrics and human evaluation to ensure the generated images are of high quality, relevant, and diverse. The process comprises three steps: quality assessment, diversity metrics, and human feedback, detailed as follows:"}, {"title": "4.3.1 Quality Assessment", "content": "The quality of generated images was assessed using the Fr\u00e9chet Inception Distance (FID) [11] alongside criteria emphasizing the representation of human figures, as identified by the YOLOv8 [16] and MTCNN [46] models. The FID score aims to measure the similarity between the generated images and a set of reference images from the COCO [11] dataset, calculated as:\n$$FID(x,y) = ||\\mu_x - \\mu_y||^2 + Tr(\\Sigma_x + \\Sigma_y - 2(\\Sigma_x\\Sigma_y)^{1/2})$$\nwhere \\(\\mu_x, \\mu_y\\) are the feature-wise mean of the real and generated images, and \\(\\Sigma_x, \\Sigma_y\\) are the covariance matrices of the real and generated images, respectively. To achieve high-quality images, we initially filtered out images with high Fr\u00e9chet Inception Distance (FID) scores. After this filtering process, we successfully reduced the mean FID score to 11.9. Furthermore, for an image to pass our quality assessment, individuals must constitute at least 15% of the detected objects and must have a detectable face, as confirmed by the MTCNN model. It's worth mentioning that the FID scores for both expected (E1 and E2) and unexpected (U1 and U2) categories of images are 12.1 and 11.7 respectively, signifying an acceptable level of quality across all scenarios."}, {"title": "4.3.2 Diversity Metrics", "content": "To ensure image diversity, we employed the Structural Similarity Index (SSIM) [42] and the Learned Perceptual Image Patch Similarity (LPIPS) [47] metrics. The SSIM index measures the similarity between two images; in our context, lower scores indicate greater diversity. It is defined as:\n$$SSIM(x, y) = \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1) (\\sigma_x^2 + \\sigma_y^2 + C_2)}$$\nwhere \\(\\mu_x, \\mu_y\\) are the average intensities, \\(\\sigma_x^2, \\sigma_y^2\\) are the variances, and \\(\\sigma_{xy}\\) is the covariance. c\u2081 and c\u2082 are constants added to stabilize the division and avoid numerical instability with small denominators. Our target mean SSIM threshold across all image pairs is 0.046. It's also worth mentioning that the SSIM for the expected (El and E2) and unexpected (U1 and U2) categories of images are 0.046 and 0.045, respectively. This indicates a satisfactory level of diversity in both image categories.\nLPIPS measures the perceptual similarity between two images, using deep network features to closely mimic human visual perception. To indicate greater diversity, we aim for higher LPIPS values, setting the mean target for all pairs at 0.66. Also, the LPIPS scores for the expected (E1 and E2) and unexpected (U1 and U2) categories of images are 0.66 and 0.65, respectively."}, {"title": "4.3.3 Human Feedback", "content": "After quantitative evaluations, we performed a qualitative analysis with human feedback. This involved scrutinizing images for alignment with the desired activities and contexts. Images that did not align with the activities or did not depict the intended subjects were removed. Additionally, images with unrealistic or nonsensical elements, and those appearing abnormal to an external observer were excluded.\nFollowing this step, a list of images that have passed both quantitative and qualitative evaluations is obtained. As a result, the images are diverse, high-quality, and realistic while also meeting our criteria for the generation phase."}, {"title": "5 Experiments", "content": "We designed several experiments on our proposed GAB dataset to evaluate Vision-Language Models in text-to-image and image-to-text retrieval tasks as shown in Figure 1(b). Based on these experiments, we assessed the defined gender-activity binding bias and the ability of VLMs to recognize gender-biased activities. Finally, we evaluated the bias in the text encoders and image encoders of different VLMs separately."}, {"title": "5.1 Setup", "content": ""}, {"title": "5.1.1 Task Definitions", "content": "The experiments we have designed to assess the performance of VLMs on the aforementioned aspects are grounded in their zero-shot performance on both text-to-image and image-to-text retrieval tasks.\nMore formally, consider the space of images as I and the space of captions as C. Also consider a VLM with an image encoder \\(E_I : I \\rightarrow \\mathbb{R}^D\\) and a text encoder \\(E_C : C \\rightarrow \\mathbb{R}^D\\), that map images and texts respectively to a shared embedding space, which is shown in the middle box in Figure 1 (b). We can define matching score function \\(s : I \\times C \\rightarrow \\mathbb{R}^+\\), which measures the similarity between a caption and an image, as the cosine similarity between the embedding of an image I and the embedding of a caption C:\n$$s(I, C) = \\frac{E_I(I) \\cdot E_C(C)}{||E_I(I)||||E_C(C)||}$$"}, {"title": "5.1.2 Selected Vision-Language Models", "content": "In our experiments, we benchmark 12 VLMs and present the results of various tests. For comparing the effects of patch size and backbone size, we report the results for 4 CLIP models released by OpenAI [27], each with different backbones and patch sizes.\nAnother model selected for evaluation is NegCLIP [45], a version of the CLIP-ViT-B-32 model by OpenAI [27] which is fine-tuned on a modified subset of the Visual Genome Dataset [17] annotations. NegCLIP is fine-tuned to purportedly enhance the base model's performance on tasks requiring relational and activity understanding.\nWe also report results for several other notable VLMs that have been published in recent years. These include Eva01 [8] and Eva02 [36], FLAVA [33], ALIGN [15], COCA [44] and AltCLIP [6]."}, {"title": "5.2 Results", "content": "This section presents the outcomes of experiments designed to evaluate model performance across various tasks, including Image-To-Text and Text-to-Image retrieval, and activity recognition(Detailed results and descriptions of activity recognition experiments are provided in the Appendix.). Additionally, this section specifically addresses and reports on the bias observed in the text encoder."}, {"title": "5.2.1 Image-to-Text Retrieval", "content": "We assess the performance of the models discussed in Section 5.1.2 on the GAB dataset and calculate their accuracy in retrieving the correct caption as outlined in Section 5.1. We employ a caption template of the form: \"a  is  and a  is in the scene\". The results for activities that are stereotypically biased are presented in Figure 2, while the results for the other two groups of activities are provided in the Appendix.\nIn this experiment, we assessed image-to-text retrieval across three distinct scenarios. The first scenario involves images where an individual of an unexpected gender is performing a typically biased activity with no other individuals present in the scene. The second scenario comprises images where both genders are present, and the activity is performed by the gender typically associated with it. The third scenario mirrors the second, but in this case, the activity is performed by the gender not typically associated with it.\nPerformance Drop Due to Presence of Expected Gender: As depicted in Figure 2, there is a noticeable drop in the accuracy of the models when the scene includes two genders compared to scenarios where only the unexpected gender is present and performing the activity associated with bias. The chart reveals that most VLMs experience an average performance decline of approximately 33.2%.\nPerformance Drop Due to Gender-Activity Binding Bias: In the scenarios where there are two individuals present at the scene, the majority of models display a substantial decline in retrieval accuracy when the action is performed by an individual of the unexpected gender. As indicated in Figure 2, the VLMs undergo an average accuracy reduction of approximately 13.2% when encountering gender activity binding bias.\nNote that, as illustrated in Figure 3, the models demonstrate satisfactory performance in the Image-to-Text retrieval task on both image scenarios of U1 and E1, indicating that the gender bias adversely affects the performance of the models only when there is more than one individual in the image. Additionally, the accuracy of the models remains consistent, regardless of whether the expected or unexpected gender is performing the activity. The models' average performance in both scenarios is approximately 80%."}, {"title": "5.2.2 Text Encoder Bias", "content": "Given an activity a, consider e, u \u2208 {man, woman} where e is the expected gender of the individual performing a and u is the unexpected one. To separately assess the bias of the text encoder, we determine the frequency at which the embedding of a gender-neutral sentence \"a person is \" is closer to the embedding of \"a e is \" than the embedding of \"a u is \". The outcomes are presented in Table 1. It can be observed that in all categories of the gathered activities, the majority of activities exhibit a bias towards the expected gender."}, {"title": "5.2.3 Text-to-Image Retrieval", "content": "Figure 4 illustrates the accuracy of VLMs in the task of text-to-image retrieval. For each activity, we form random pairs of images from E2 and U2. Each model is then evaluated for its ability to retrieve the image that best matches captions formatted as \"a  is  and a  is in the scene\". As can be seen in Figure 4, VLMs achieve an accuracy of approximately 50%, indicating that their performance is nearly random in this task. If we replicate this experiment, altering only the captions to a gender-neutral phrase \"a person is \", and designate the image where the expected gender is performing the activity as the true label, the VLMs achieve an accuracy of approximately 50% again, as illustrated in Figure 4 in the Appendix. In essence, the benchmarked models do not seem to comprehend the properties of the given images that would aid in retrieving the correct one given embedded information from the caption, i.e., they fail to recognize the performer of the activity based on the text."}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Image and Text Embedding in VLMs", "content": "To gain a deeper understanding of how texts and images are encoded into the shared embedding space, consider these points:\n1. Given that the image retrieval accuracy is nearly 50% for both expected and unexpected caption groups (as shown in Figure 4), we can infer that the cosine similarity (as per Eq. 4) between both sets of captions and images (for expected and unexpected groups) is almost identical.\n2. As the average text retrieval accuracy exceeds 60% for E2 and falls below 50% for U2, it suggests that text embeddings are somewhat closer to the image embeddings in E2."}, {"title": "6.2 Bias Mitigation", "content": "Multiple works have previously discussed approaches for mitigating bias from VLMs, such as orthogonal projection [7], making unbiased datasets [22, 14], representation correction [32, 40], and prompt tuning [5]. Several of these approaches could be applied also for robustness to gender-activity binding bias. For more detail on these works, refer to the Appendix."}, {"title": "7 Future Works", "content": "Study Other Social Biases. In this work, we focused solely on gender bias; however, the experimental approaches described can be extended to other social biases, such as race and age. In future research, we plan to explore how VLMs perform across these dimensions using the experiments outlined in this study.\nStudy the Source of Bias in the Training Data. Due to limited accessibility to the training sets, especially for models with unpublished datasets, this study could not investigate the source of bias in the training data directly. We plan to address this limitation in future work by examining the sources of bias in publicly available training datasets for VLMs and analyzing the origins of these biases within those datasets."}, {"title": "8 Conclusions", "content": "In this study, we have explored a significant yet overlooked bias in VLMs known as the gender-activity binding bias. We used a unique dataset named GAB, which includes about 5500 images. We found that the capacity of VLMs to link an activity with the gender of its performer notably decreases when another person of a different gender is in the scene. We discovered that while the bias is evident in image-to-text retrieval tasks and in the text encoder alone, the models perform randomly in text-to-image retrieval tasks. This suggests that the gender-activity binding bias is primarily absorbed by the text encoder rather than the image encoder in VLMs. Furthermore, we noted that while VLMs have difficulty with gender-activity binding, they do have some ability to recognize activities. We believe that the ability of VLMs to comprehend activities and perform compositional reasoning in complex scenes are other crucial factors contributing to the gender-activity binding bias that could be investigated in future research."}, {"title": "9 Dataset", "content": ""}, {"title": "9.1 Detailed Report", "content": "The method of selecting activities is detailed above, ultimately resulting in three categories of biased activities. comprehensive report of each category provided in Table 2, Table 3, Table 4."}, {"title": "9.2 Experimented Models For Image Generation", "content": "Initially, we explored several diffusion[12", "Diffusion[29": "Stable Diffusion XL[25"}]}