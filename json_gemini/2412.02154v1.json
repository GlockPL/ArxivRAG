{"title": "Failure Probability Estimation for Black-Box Autonomous Systems using State-Dependent Importance Sampling Proposals", "authors": ["Harrison Delecki", "Sydney M. Katz", "Mykel J. Kochenderfer"], "abstract": "Estimating the probability of failure is a critical step in developing safety-critical autonomous systems. Direct estimation methods such as Monte Carlo sampling are often impractical due to the rarity of failures in these systems. Existing importance sampling approaches do not scale to sequential decision-making systems with large state spaces and long horizons. We propose an adaptive importance sampling algorithm to address these limitations. Our method minimizes the forward Kullback-Leibler divergence between a state-dependent proposal distribution and a relaxed form of the optimal importance sampling distribution. Our method uses Markov score ascent methods to estimate this objective. We evaluate our approach on four sequential systems and show that it provides more accurate failure probability estimates than baseline Monte Carlo and importance sampling techniques. This work is open sourced.", "sections": [{"title": "1. Introduction", "content": "Autonomous systems are increasingly being considered for safety-critical domains such as aviation (Kochenderfer et al., 2012; Owen et al., 2019), autonomous driving (Badue et al., 2021), and home robotics (Zachiotis et al., 2018). These autonomous systems operate sequentially by taking actions after receiving observations of the environment. For example, an autonomous vehicle must continuously process sensor data, predict the movements of other vehicles, and make decisions about steering and braking. The safety of these systems must be rigorously validated in simulation before deployment. One way to quantify system safety is to estimate the probability of system failure such as a collision of an autonomous vehicle with other agents. Estimating the probability of failure for autonomous systems may highlight weaknesses and uncover potentially dangerous scenarios.\nThere are three key challenges associated with estimating the probability of failure in sequential autonomous systems. First, failures tend to be rare when the system is designed to be safe. Estimating the probability of failure using simple methods like Monte Carlo sampling may require an enormous number of expensive simulations. Second, the search space over failure events is very high-dimensional because autonomous systems operate over large state spaces and long time horizons. Third, autonomous systems can exhibit multimodal failures, requiring validation techniques that can adequately capture diverse behaviors.\nMany previous approaches have attempted to address these challenges through importance sampling techniques. Importance sampling techniques aim to sample from alternative distributions that"}, {"title": "2. Related Work", "content": "In this section, we first review broad categories of safety validation approaches. Then, we review failure probability estimation methods, including adaptive importance sampling techniques.\nWhite-Box vs. Black-Box Traditional validation methods take a white-box approach that uses internal information about the system under test to prove safety properties (Schumann, 2001; Clarke et al., 2018). In this work, we take a black-box approach. Black-box methods generally scale to large, complex systems because they do not require an internal model, but only pass inputs to the system and observe outputs. See the survey by Corso et al. (2021) for more on black-box methods.\nFalsification A common approach to black-box safety validation of sequential systems is falsification, which aims to find individual failure trajectories. Falsification approaches use optimization (Deshmukh et al., 2017), trajectory planning (Tuncali and Fainekos, 2019), and reinforcement learning (Akazaki et al., 2018) to find specific inputs that lead to failure. Adaptive Stress Testing further incorporates trajectory likelihood to search for the most-likely failure event (Lee et al., 2020). The drawback of falsification methods is that they tend to converge on a single failure, such as the most-likely or most severe, and do not explore the space of potential failures.\nFailure Probability Estimation Failure probability estimation provides a more comprehensive assessment of system safety by characterizing the distribution over failures. Some previous approaches rely on Markov chain Monte Carlo (MCMC) methods to sample failure events (Sinha et al., 2020; Norden et al., 2019). These approaches are generally only feasible in low-dimensional problems, may require domain knowledge to perform well, or may require the system under test to be differentiable to scale to larger problems. Adaptive Importance Sampling (AIS) methods iteratively improve a proposal distribution using samples from the proposal (Bugallo et al., 2017; Rubinstein and Kroese, 2004; Capp\u00e9 et al., 2004). The drawback of these methods is that they tend to become intractable in problems with long time horizons, and they are typically limited to modeling a small set of input parameters (Uesato et al., 2019). Importance sampling algorithms have been extended to sequential problems by incorporating dynamic programming, but this approach is limited to small discrete state spaces (Chryssanthacopoulos et al., 2010). Finally, some work builds on reinforcement learning by iteratively improving a proposal using policy gradient methods (Corso et al., 2022). Due to the large variance of the estimator, this approach does not scale to long horizons or large action spaces. In this work, we propose an AIS algorithm for sequential problems with continuous state spaces and long horizons."}, {"title": "3. Background", "content": "In this section, we provide necessary background on safety validation for sequential autonomous systems, failure probability estimation, and importance sampling."}, {"title": "3.1. Safety Validation for Sequential Systems", "content": "Consider a system under test that takes actions a in an environment after receiving observations o of its state s. We assume states may transition stochastically according to a transition model $p(s' | s, a)$ and state observations may depend on imperfect sensors modeled as $p(o | s)$. The system under test may also act stochastically depending on the observation with policy $\\pi(\u03b1 | \u03bf)$."}, {"title": "3.2. Failure Probability Estimation", "content": "Failure probability estimation is the problem of estimating the denominator of eq. (3), or the expected value:\n$\\mu = E_{p(r)} [1\\{f(t) \\geq y\\}]$\n(4)\nA simple approach to estimate \u03bc is using Monte Carlo (MC) sampling, which uses N independent samples from p(T) to compute the empirical estimate\n$\\hat{\\mu}_{MC} = \\frac{1}{N} \\sum_{i=1}^{N} 1\\{f(T_i) \\geq \\gamma\\}$\n(5)\nFor very rare failures (e.g. \u03bc \u2248 10-9), standard Monte Carlo may require billions of samples, making direct estimation infeasible for complex systems."}, {"title": "3.3. Importance Sampling", "content": "Importance sampling can reduce the variance of failure probability estimates by focusing samples on regions of interest. IS draws samples from a proposal distribution q(7), and computes an estimate\n$\\mu_{IS} = \\frac{1}{N} \\sum_{i=1}^{N} w(\\tau_i) 1\\{f(\\tau_i) \\geq \\gamma\\}$\n(6)\nwhere the samples are weighted according to their importance weight $w(\\tau) = p(r)/q(t)$. The minimum variance importance sampling distribution is the failure distribution itself, meaning that samples are only drawn from the failure region with likelihood proportional to p(T). The estimator is unbiased if $q(\\tau) > 0$ whenever $1\\{f(t_i) \\geq \\gamma\\}p(\\tau) > 0$.\nThe key challenge of importance sampling is designing a good proposal distribution. The variance of the estimator increases roughly exponentially with the problem dimension, but it can be reduced by a well-designed proposal distribution. However, modeling the complex $D_T$-dimensional trajectory distributions following previous IS approaches is extremely difficult."}, {"title": "4. Methods", "content": "Next, we introduce our method for failure probability estimation in sequential systems. We propose a sequential state-based IS proposal, discuss proposal optimization, and detail our algorithm."}, {"title": "4.1. Sequential Proposal Distribution", "content": "To address the challenges of fitting a complex, high-dimensional proposal distribution, we decompose the problem using a proposal applied at each timestep. Since the system behavior and disturbance distribution may depend on the system state, we condition the distribution on the current state. These proposals take the form $q_\\theta(x | s)$ with parameters 0. We can draw samples from $q_\\theta(\\tau)$ by sampling an initial state $s\u2081 ~ p(s1)$ and simulating each timestep:\n$x_t ~ q_{\\theta}(\\cdot | s_t),  o_t ~ p(\\cdot | s_t, x_t),  a_t ~ \\pi(\\cdot | o_t, x_t),  s_{t+1} ~ p(\\cdot | s_t, a_t, x_t)$\n(7)\nSystem trajectories sampled using the proposal $q_\\theta(x | s)$ have the density:\n$q_\\theta(T) = p(s_1) \\prod_{t=1}^{T} p(s_{t+1} | s_t, a_t, x_t)p(o_t | s_t, x_t)\\pi(a_t | o_t, x_t)q_\\theta(x_t | s_t)$\n(8)\nIn computing the importance weight as the ratio of eq. (2) to eq. (8), all terms cancel except the disturbance distribution and the proposal. The importance weight reduces to:\n$w(\u03c4) = \\prod_{t=1}^{T} \\frac{d(x_t | s_t)}{q_{\\theta}(x_t | s_t)}$\n(9)\nThis formulation reduces the problem of learning a proposal over full disturbance trajectories to learning a proposal over a single timestep given the current state. However, obtaining a reliable IS estimate of the failure probability still requires careful design of this proposal."}, {"title": "4.2. Proposal Optimization", "content": "The optimal importance sampling distribution is known to be proportional to the true distribution over failure trajectories. Therefore, we propose to learn the proposal parameters by minimizing the forward KL divergence between the proposal and the failure distribution:\n$\\theta^* = \\underset{\\theta}{\\text{argmin}} D_{KL} (p (T | 1\\{f(t) > \\gamma\\}) || q_{\\theta} (T))$\n(10)\nMinimizing the forward KL divergence is desirable for importance sampling because its mass covering properties help ensure the IS estimator remains unbiased (Rubinstein and Kroese, 2004). This objective is equivalent up to a constant of proportionality to minimizing the cross-entropy:\n$\\underset{\\theta}{\\text{min}} \\mathcal{L}(\\theta) = \\underset{\\theta}{\\text{min}} E_{p(7|1\\{f(7)\\geq y\\})} [-\\text{log} q_{\\theta}(T)]$\n(11)\nHowever, this formulation creates a chicken-and-egg problem, since our goal is to approximate $p(\u03c4 | 1\\{f(\u03c4) \u2265 \u03b3\\})$ with $q_{\\theta}(T)$, but the cross-entropy involves an expectation over $p(\u03c4 | 1\\{f(t) \u2265 y\\})$. A common approach optimizes an IS estimate of the expectation. Unfortunately, the variance of the IS estimate increases rapidly with problem dimension, making optimization very difficult especially when the proposal is far from the target.\nBuilding on previous work in variational Bayesian inference, we optimize the objective in eq. (11) using Markov score ascent methods (Naesseth et al., 2020; Kim et al., 2022). Markov score ascent methods can achieve much lower variance estimates of the objective. The key idea of Markov score ascent methods is to estimate the cross-entropy using samples from an MCMC kernel that is ergodic with respect to the target distribution. The kernel is chosen to directly use the approximation of the target distribution $q_{\\theta}(T)$. With repeated application of the kernel, the samples will be approximately distributed according to the target. Given N trajectories approximately distributed according to the target, we may compute a Monte Carlo estimate of the objective:\n$\\mathcal{L}(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^N [-\\text{log} q_{\\theta}(T_n)]$\n(12)\nExpanding the objective and removing terms that do not depend on 0, the loss can be computed as\n$\\mathcal{L}(\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^T [-\\text{log} q_{\\theta}(x_{t,n} | s_{t,n})]$\n(13)\nwhere subscript t, n denotes timestep t in trajectory n. This estimated cross-entropy objective is optimized using stochastic gradient descent.\nIn this work, we construct an MCMC kernel by performing a single step of Independent Metropolis-Hastings (IMH, Hastings, 1970). Given an initial trajectory \u0442, we can generate a new sample from IMH by first sampling a proposal trajectory \u03c4' ~ $q_{\\theta}(\\cdot)$. IMH either accepts the new trajectory \u03c4' according to the Metropolis-Hastings acceptance probability a:\n$\\alpha(\\tau, \\tau') = \\text{min} (1, \\frac{p(\\tau' | 1\\{f(r') \\geq \\gamma\\})q_{\\theta}(\\tau)}{p(\\tau | 1\\{f(t) \\geq y\\})q_{\\theta}(\\tau')}) = \\text{min} (1, \\frac{w(\\tau')}{w(\\tau)})$\n(14)\nOtherwise, IMH rejects the proposal and 7 remains the same."}, {"title": "4.3. Approximating the Failure Distribution", "content": "To address the issue of low acceptance rates, we use a smooth approximation of the discontinuous failure distribution. Specifically, we approximate the indicator function in the target by $P_\\beta(f(t) - \\gamma)$ where $P_\\beta$ is the cumulative distribution function of a logistic distribution with zero mean and scale B. This approximation replaces the discontinuous indicator with a smooth logistic curve from zero to one. The approximate failure distribution $\\tilde{p}(\u03c4)$ has the unnormalized density\n$\\tilde{p}(\u03c4) \\propto p(\u03c4)P_\\beta(f(t) - \\gamma)$\n(15)\nwhich relaxes the failure distribution, enabling stable sampling during optimization. This approximation is visualized in fig. 2 for a few values of \u03b2.\nUsing the approximate posterior, our objective becomes:\n$\\mathcal{L}(\\theta) = E_{\\tilde{p}(\u03c4)} [-\\text{log} q_{\\theta}(T)]$\n(16)\nWe can now apply IMH to generate proposal samples for Markov score ascent by computing the acceptance probability using the unnormalized importance weights under the approximate failure distribution $\\tilde{w}(\u03c4) = p(\u03c4)/q_{\\theta}(\u03c4)$. Note that we can still compute the acceptance probability using $\\tilde{w}(\u03c4)$ without knowing the normalizing constant of the approximate failure distribution, because the constant values in the numerator and denominator in the ratio of eq. (14) cancel."}, {"title": "4.4. Algorithm Details", "content": "The complete algorithm, which we call State-dependent Proposal Adaptive Importance Sampling (SPAIS), is shown in algorithm 1. The algorithm maintains a set of N trajectories that will be updated using the IMH kernel and a buffer to store evaluations f(t) and the corresponding IS weight of samples from $q_\u0259(r)$. At each iteration, the algorithm samples N new proposal trajectories T'. These samples are first added to the IS buffer. Next, they are used in the step to update the trajectory set. Each proposed trajectory is accepted or rejected according using the MH acceptance probability. At the end of each iteration, we update the proposal by fitting the proposal to the updated set of trajectories. After a fixed number of iterations, the algorithm returns an IS estimate of the failure probability using eq. (6).\nIn practice, we parameterize $q_\\theta(x | s)$ as a multivariate Gaussian\n$q_\\theta(x | s) = \\mathcal{N}(x | \\mu_\\theta(s), \\Sigma_\\theta(s))$\nwhere $\\mu_\\theta(\\cdot)$ and $\\Sigma_\\theta(\\cdot)$ are both neural networks with two hidden layers of size 64 and 32 respectively. We find using any small value of \u03b2 in the range $[10^{-4}, 10^{-2}]$ gives good performance. We use \u03b2 = $10^{-2}$ for all evaluations. For each problem, we normalize the state space features and pretrain $q_\\theta (x | s)$ to match d(x | s)."}, {"title": "5. Experiments", "content": "In this section, we describe our experiments to evaluate the proposed approach. We describe baseline estimation methods, evaluation metrics, and introduce four example safety validation problems."}, {"title": "5.1. Evaluation Metrics and Baselines", "content": "We evaluate the bias and variance of the estimated probability of failure using the relative error $E_{rel}$ and absolute relative error $E_{abs}$, respectively:\n$E_{rel} = (\\hat{\\mu} - \\mu)/\\mu,  E_{abs} = |\\hat{\\mu} - \\mu|/\\mu$\nWe obtain an estimate of the ground truth failure probability \u03bc using $10^7$ Monte Carlo samples. Each metric is averaged over 10 trials with 50000 samples per trial to arrive at estimates of the empirical bias and variance for each method.\nWe compare the method against three baseline methods. The simplest baseline is Monte Carlo estimation. We also compare against a variant of the cross-entropy method (CEM) that optimizes a state-independent proposal distribution applied at each timestep. Finally, we evaluate against PG-AIS, which learns a reinforcement learning policy for importance sampling (Corso et al., 2022)."}, {"title": "5.2. Validation Problems", "content": "We demonstrate SPAIS on four systems illustrated in fig. 3. Each problem's properties are reported in table 1.\nInverted Pendulum We evaluate an underactuated inverted pendulum system with a nonlinear rule-based control policy. The system is subjected to additive torque disturbances over 20 timesteps. We define failure to occur when the magnitude of the pendulum's angle from the vertical exceeds $\\pi/4 rad$. This problem has two distinct failure modes corresponding to leftward and rightward falls."}, {"title": "6. Results", "content": "The relative and absolute error for each method across all problems are reported in table 2. The proposed SPAIS algorithm achieves lower relative and absolute error on all four validation problems. The MC estimator generally has low bias, but high variance. The PG-AIS baseline method tends to have large variance, indicating that it struggles to represent the failure distribution. CEM exhibits a bias, which may be due to mode collapse where the proposal does not cover all failure modes or due to the proposal being spread very wide leading to large importance weights.\nWe visualize sampled system trajectories from the CEM and SPAIS proposals for each problem in fig. 4. Each plot shows a system trajectories in state space with points colored according to a mean input disturbance sampled from the proposal.\nInverted Pendulum In fig. 4(a), we plot the pendulum angle 4 against angular rate \u03c8, and color the points according to the torque disturbance. SPAIS generally adds positive torque disturbances for positive angles, and vice versa. In contrast, the CEM proposal only samples negative disturbances and thus only samples from one of the two failure modes."}, {"title": "7. Conclusion", "content": "Accurately estimating failure probabilities is a critical step in developing autonomous systems in safety-critical domains. We propose a method for failure probability estimation that takes advantage of the sequential nature of autonomous systems to construct flexible IS proposals that are optimized using Markov score ascent. Using a relatively small number of samples, our results show that SPAIS achieves under 10% empirical bias with at least 2 times lower empirical variance than baselines. Future work will investigate using more flexible state-based proposals to further improve estimation."}]}