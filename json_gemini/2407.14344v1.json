{"title": "LLMS LEFT, RIGHT, AND CENTER: ASSESSING GPT'S CAPABILITIES TO LABEL POLITICAL BIAS FROM WEB DOMAINS", "authors": ["Raphael Hernandes"], "abstract": "This research investigates whether OpenAI's GPT-4, a state-of-the-art large language model, can accurately classify the political bias of news sources based solely on their URLs. Given the subjective nature of political labels, third-party bias ratings like those from Ad Fontes Media, AllSides, and Media Bias/Fact Check (MBFC) are often used in research to analyze news source diversity. This study aims to determine if GPT-4 can replicate these human ratings on a seven-degree scale (\"far-left\" to \"far-right\"). The analysis compares GPT-4's classifications against MBFC's, and controls for website popularity using Open PageRank scores. Findings reveal a high correlation (Spearman's \\( \\rho \\) = .89, n = 5,877, p < 0.001) between GPT-4's and MBFC's ratings, indicating the model's potential reliability. However, GPT-4 abstained from classifying approximately \\( \\frac{2}{3} \\) of the dataset, particularly less popular and less biased sources. The study also identifies a slight leftward skew in GPT-4's classifications compared to MBFC's. The analysis suggests that while GPT-4 can be a scalable, cost-effective tool for political bias classification of news websites, but its use should complement human judgment to mitigate biases. Further research is recommended to explore the model's performance across different settings, languages, and additional datasets.", "sections": [{"title": "1 Introduction", "content": "Is The New York Times more politically inclined to the left or the right? The answer might vary according to the respondent. However, it seems to converge to the center-left \u2014at least according to three different services that specialize in rating the news: Ad Fontes Media, AllSides, and Media Bias/Fact Check (MBFC) [1, 2, 3]. Finding that sort of information about one of the most-read news outlets in the world is relatively easy, especially given that its public editor acknowledges the newspaper is perceived as left-wing [4]. It might be harder, however, to assess the political bias of more obscure or niche sources where accessible and accurate information is lacking.\nWhile political labels carry a certain degree of subjectivity, third-party bias ratings may help balance information ecosystems by allowing a better understanding of what is consumed, which is particularly relevant since news outlets do not generally declare their standings [5]. This labeling is often used in academic research. For example, [6, pp. 8\u20139] used it to assess whether news content highlighted by Google Search has political leanings. [7] used it to spot an increased polarization on Twitter in the discussions about the United Nations Conference of the Parties on Climate Change, tracking relevant topics for pro-climate users and for those who oppose climate action."}, {"title": "2 Background", "content": "2.1 GPT-4 and LLMs\nGPT-4 is the latest of OpenAI's family of LLMs [17]. It was chosen due to GPT's popularity in the AI community since the launch of ChatGPT in November 2022 [18]. It is the best option available\u00b2 for the tasks analyzed in this research according to MMLU [15], a widely used benchmark for models in zero-shot settings\u00b3 [19, 20], and HELM, a comprehensive benchmark for language tasks [14, 21, 20, p. 114].\nThese sorts of AI are created after analyzing the patterns in immense amounts of textual data. They work by predicting combinations of tokens\u2074 [22, 16] and reply to questions made by users conversationally, besides following instructions based on their input (called a prompt). LLMs can perform a myriad of language-processing tasks, such as answering questions, writing, summarizing texts, and translating [20, pp. 99-108].\nAs these models grow in terms of computation, number of parameters, and training dataset size, they show capabilities that go beyond their training scope and perform well in tasks that are not their primary goal. These are known as emergent properties. They are not entirely understood but seem related to model size [23, pp. 2-4]. The lack of clarity about them is an issue, as little is known about their limitations [11, p. 4]."}, {"title": "2.2 LLMs for Annotation Tasks", "content": "Among LLMs' emergent properties are applications related to annotating datasets, which motivated the usage investigated in this paper. LLMs were succesfully used to assess the quality of texts written by humans and robots regarding grammar, cohesiveness, likability, and relevance [9]. Research also shows that GPT in a zero-shot setting can classify tweets according to several criteria, such as whether they are a content moderation issue or relate to pre-defined political topics, with higher accuracy than non-expert humans under the same instructions [10].\nIn a similar context to this paper, research found a strong correlation (Spearman's \\( \\rho \\) = .54, p < 0.001) between human and GPT-assigned ratings for news outlets' credibility in a corpus of over 7,000 web domains [13]. The result is based on an aggregate score of multiple services that evaluate news domain quality rather than a single classifying service [24]. These services correlate with each other on varying degrees (Spearman's \\( \\rho \\) range: .32-.90), so GPT would fall in the middle of that distribution. This motivated investigating whether GPT could repeat the same level of performance in a political bias classification scenario."}, {"title": "2.2.1 LLMs in Political Analysis", "content": "LLMs have also displayed promising results in tasks related specifically to political analysis. GPT could accurately classify the political affiliation of a Twitter user based on the content of a single anonymized tweet [11]. The analysis used posts made by all US senators, providing an unequivocal basis for evaluating the answers. The AI performed better than both expert and non-expert humans.\nGPT has also shown remarkable capacity to classify US senators in terms of their liberal-conservative ideology, support of gun control, and support of abortion rights, based only on their names and parties using pairwise comparisons [12, p. 2]. The rankings are not simply mimicking other scales, contrasting with the idea that these systems might be parroting patterns from their training data [25]. If they were merely copying information from elsewhere, it would be simpler to use the original data. Instead, the investigation shows that the ratings came from a mix of senators' behaviors and how these politicians are perceived [12, p. 19]. This indicates that LLMs' capabilities in political classification tasks warrant further investigation, as they might offer not only a cheaper and faster way of labeling the data but also a new scale altogether."}, {"title": "2.3 Rating News Sources' Political Bias", "content": "Perception and behavior are currently used forms of rating political bias in media sources. Scales from the likes of AllSides and polling institutes, such as Pew Research Center, classify news sources based on how consumers perceive them [8, 26]. Language (behavior) has been used in research to classify newspapers: phrases connected to the right meant right-leaning tendencies [27]. The measurement correlated to the political incline of readers, given the preferences in the zip codes where the newspapers circulated [27, p. 64]. A study used the self-reported ideology of Facebook users to classify news sources by leveraging popularity among the right or the left to gauge the outlet's political ideology [28].\nMBFC, the baseline in this paper, primarily analyzes news source quality. Its database includes infor-mation beyond credibility, such as the source's country and media type (newspaper or TV, for example). It rates political bias by examining editorial content by their position on general philosophy, abortion, economic policy, education policy, environmental policy, gay rights, gun rights, health care, immigration, military, personal responsibility, regulation, social views, taxes, voter ID laws, and workers' rights [29]. It is cited as a source for asserting news quality both in Academia and in the news [30, p. 2][]NoEvidenceDis-ease2021[4]resnickIffyQuotientPlatform2018."}, {"title": "2.3.1 Applications of these Ratings", "content": "Analyzing the political bias of news sources is helpful to gauge diversity in the news ecosystem. By classifying ideology and relating it to zip codes, it is possible to note that political ideology in newspapers is more related to appealing to their audience than ownership diversity, as outlets sharing an owner have diverging political stances depending on their readership. This is relevant for regulating the industry (deciding whether to limit ownership, for example) [27]. These can also be used to measure diversity in news exposure, when, for example, news sources' political inclinations to analyze the results offered by Google's algorithm. The classification allowed researchers to identify it was slightly skewed to the left, which might direct readers' attention that way [6]."}, {"title": "3 Methodology", "content": "The analysis gathered a dataset with classifications from MBFC and popularity ratings from Open PageRank.\nFirst, the data for every report made by MBFC was downloaded, removing those that did not include information on political bias, such as the ones focusing on scientific content.\nThe web domains from MBFC's database were cleaned to remove inconsistencies (invalid URLs, for example), and missing values were fetched manually from the reports. Two pairs of records shared the same domain, and only one of each was kept. Three records were related to Facebook pages and were removed.\nThe dataset includes 5,877 observations extracted from MBFC. The bias column from GPT and MBFC was coded into a scale ranging from -3 to +3, meaning \"far-left\" to \"far-right.\" This makes it possible to calculate both the difference and absolute difference between the two measurements (GPT Bias \u2013 MBFC Bias). See Appendix A for summary statistics.\nBoth MBFC's and GPT-4's Bias Scales display a pattern that deviates from a normal distribution (Figure 1), so a non-parametric correlation method, Spearman's R, was used for the comparison. MBFC has more right than left-wing classifications. This is acknowledged by the authors, who explain that their set was mostly balanced until they started taking user requests for websites to rate, which are generally for right-leaning domains [31].\nThe analysis primarily focuses on the correlation between the ratings assigned by GPT and MBFC, an approach similar to the work done by [13] when comparing the LLM's ability to judge news outlet credibility. There are multiple ways of classifying news sources' political bias (Section 2.3), and the exact placement in a scale might vary (as there is no clear line separating center-left from left, for example). Thus, this work is concerned with whether GPT will place a source more to the right when MBFC does it, instead of checking whether it predicts the exact value, so correlation is a better measurement. The system's accuracy is also analyzed later, through linear and logistic regressions controlling for website popularity and political stance."}, {"title": "3.1 Open PageRank", "content": "Open PageRank is a free initiative that applies metrics like the ones used by Google to rank websites, enabling comparison among domains through a scale from 0 to 10 to the decimal level where higher can be interpreted as a proxy for more popular. It provides data about the top 10 million websites in its database [32]. The root domain of each URL (\"www.example.com/abc\" \u2192 \"example.com\") is used to retrieve ratings from Open PageRank's API. Therefore, this analysis does not differentiate between sources that share a root URL (\"theguardian.com/observer\" and \"theguardian.com,\" for example), meaning popularity might be slightly exaggerated for some. These, however, are not frequent in the data, which contains 5,684 unique root URLs. The distribution of Open PageRank scores is available in Appendix B."}, {"title": "3.2 Prompting GPT-4", "content": "The GPT version used is gpt-4-1106-preview, the most recent available, queried through OpenAI's API. It was released on Nov. 06, 2023, with up-to-date information until April 2023, and is described by OpenAI as having \"improved instruction following\" capabilities [33]. Being in \"preview\" means it does not support high traffic [34], which is not the case in this work. It was cheaper than older versions of GPT-4 (US$0.01/1k input and US$0.03/1k output tokens vs. US$0.03/1k input and US$0.06/1k output tokens) [35]. It can reply in JSON format instead of text, making parsing easier as it removes the need to extract the output from a paragraph.\nGpt-4-1106-preview has a seed parameter to make the model return consistent completions, which is useful for reproducible outputs [33]. The seed was set to 123. The temperature was 0.0 to reduce randomness and support reproducibility. A low temperature may be preferable for annotation tasks as it increases consistency without decreasing accuracy [10, p. 2].\nThe prompts asked the AI to rate the political bias of a URL or return \"-1\" if it could not classify it. After the main experiment, to better understand the machine's behavior, additional prompts for a random sample asked GPT to justify the output. Sample prompts and responses are available in Appendix C. It was used in a zero-shot setting in an attempt to evoke latent information in the model's training. The request specified that the response be in JSON and provided an example of formatting. A system message added context telling GPT to act as an assistant to determine the political bias of websites."}, {"title": "4 Results", "content": "4.1 Correlation Between GPT-4 and Human-Assigned Political Bias Ratings\nThe analysis shows a very strong\u2075 correlation between GPT-4's and MBFC's political bias classifications (Spearman's \\( \\rho \\) = .89, n = 5,877, p < 0.001). This shows that both vary in the same direction: sources classified toward one side of the spectrum in MBFC are likely to follow the same pattern with GPT."}, {"title": "4.1.1 Left-leaning Bias", "content": "The statistical analysis revealed that GPT's classifications are slightly more left-leaning than MBFC's. A one-sample t-test showed that the mean difference in classifications was -0.08, indicating a leftward bias. The test results were statistically significant (t = \u22124.56, p < 0.001). This is not an entirely new phenomenon. The same inclination was noted in research that classified the political affiliation of Twitter users based on the content of tweets in the US. In that context, human respondents were also significantly skewed toward guessing Democrat [11, p. 3]."}, {"title": "4.2 Impact of Popularity on Accuracy", "content": "To assess the LLM's capacity to determine the political slant of news sources, the analysis controlled for website popularity as the model's exposure to certain URLs during training could disproportionately affect its predictions, leading to biases. Popular websites are more likely to appear in diverse contexts within the training data, relating to \"common-token bias\" [37, pp. 4\u20135]. This means the model might associate these common tokens (URLs of popular sources) with a wide range of content, diluting the model's ability to classify political bias. It could result in the model inaccurately attributing neutrality to well-known sources simply due to their ubiquity or wrongly associating them with a bias based on a frequent misconception. The opposite might also be the case: the dataset used to train GPT-4 lacks information about an obscure website, forcing the AI to make something up. To mitigate these issues, the prompt allowed the model to not assign a rating if uncertain (Section 3.2).\nThe statistical analysis revealed an impact caused by websites' popularity (Open PageRank) in the ratings' quality. News sources were grouped into four sets of similar sizes based on their popularity rating quartiles. The correlation between MBFC and GPT remains very strong across each popularity category (Spearman's \\( \\rho \\) range: .64-.91, p < 0.001). It peaks in the medium-high category (50-75th percentiles) and is the weakest in the lowest category (0-25th percentiles). A hypothesis is that the smaller correlation in unpopular websites comes from less data about them being available. Conversely, the drop in the top websites (above 75th percentile, \\( \\rho \\) = .77) might be from the excess of information with contradictory views. For instance, GPT classified economist.com (above 75th popularity percentile) as center-right, while MBFC has it as center [38]. AllSides and Ad Fontes say it leans slightly towards the left [39, 40]. Asked to explain its reasoning, GPT acknowledged some ambiguity: \"The Economist generally advocates for free markets, internationalism, and cultural liberalism, which aligns it with center-right political positions, although it also supports some socially liberal positions.\"\nThe influence of website popularity on prediction accuracy was analyzed through an Ordinary Least Squares (OLS) regression on the absolute distance between GPT's prediction and MBFC's ratings. Absolute distance was used since this step of the research focused on how aligned the answer was, disregarding the direction of the error. Modeling only with popularity did not yield statistically significant results.\nWhen controlling for country, the results indicate that the Open PageRank rating is associated with accuracy. A one-unit increase in popularity is associated with a decrease of 0.048 in the distance between GPT and MBFC (p < 0.01). The model, however, only explained a small portion of the variance in GPT's accuracy (R\u00b2 = 0.017). Although its explanatory power is limited, it provides some indication that the LLM performs better on more prominent websites. Further diagnostic checks are needed to understand this phenomenon, which is beyond the scope of this paper."}, {"title": "4.3 Unassigned Observations", "content": "GPT-4 abstained from classifying most sources in the dataset, rating only 1,975 (33.6%) entries. Leaving out a significant portion of the data could introduce bias as these gaps are not evenly distributed. However, given that LLMs are often criticized for their hallucinations (wrong or made-up outputs) [41, p. 6], the model's capacity to say \"I do not know\" instead of providing nonsensical results makes it more trustworthy as long as these systematic failures are tracked.\nThe unclassified sources are spread across all different categories (table in Appendix F). Regarding country, it was most prevalent in sources whose region is unknown (87.3%). The \"Others\" category, which includes non-English-speaking countries, did not stand out (60.6%), which suggests that language might not be the most relevant issue.\nLogistic regressions on the different categories in this dataset (tables in Appendix G) revealed some factors that might cause a URL to be left unclassified. The most relevant were news source popularity (Open PageRank) and political stance, based on MBFC's rating simplified as right (far-right and right), left (far-left and left), and center (center-right, center-left, and center).\nPopularity was assessed against the likelihood of GPT's prediction coming out unassigned, individually, and with control variables. The initial model, considering only popularity, indicated a significant negative association (coef = -1.2195, SE = 0.042, z = -29.122, p < 0.001), suggesting the LLM is more likely to rate more popular sources. McFadden's Pseudo R\u00b2 for the model is 0. 1869, indicating a moderate explanatory power.\nControl variables country, credibility, political stance, and media type provide a more comprehensive understanding. In this extended model, the impact of popularity became more pronounced (coef = -2.1344, SE = 0.074, z = -29.025, p < 0.001), and McFadden's R\u00b2 increased to 0. 3874, indicating a better model fit. These findings underscore that popularity is a relevant metric in GPT's capacity to assess the political bias of a website since it consistently showed a strong and significant relationship with the outcome, even when controlling for a range of other factors.\nThe coefficients for the additional control variables oscillated, with some showing significant associations with the likelihood of GPT abstaining. Besides popularity, the most prominent ones were related to political stance. They were investigated in a separate logistic regression, controlling for popularity. Compared to the baseline political stance category, center, having a left (coef = -2.2246, SE = 0. 124, z = -17.897, p < 0.001) or right (coef = -3.2355, SE = 0. 119, z = -27.180, p < 0.001) skewness significantly reduced the odds of GPT's output being unassigned. The McFadden's R\u00b2 value of 0 . 3448 indicates that the model explains a great proportion of the variability in the outcome. These results, therefore, highlight a tendency of GPT to be unable to assign a rating to the least biased sources.\nUpon rerunning the classifications of 50 random unassigned sources while also requesting GPT-4 to provide its reasoning only two were then labeled. They were baltimoresun.com/citypaper (GPT had it as center-left, while MBFC had it as left) and americasvoice.news (aligned with MBFC as right). All the reasons for the remaining 48 sources argue lack of information, despite five being in the top quartile of PageRank's popularity, such as walesonline.co.uk. This technique brings a slight improvement but comes at a significant cost since each request's output consumes up to 4x more tokens. Further testing is needed to check whether it would render improved alignment."}, {"title": "5 Discussion", "content": "The high correlation between the MBFC's and GPT's classifications shows encouraging evidence that the LLM can reliably assess the political bias of news sources. However, a better understanding of the model and its limitations is warranted. These results align with works that have previously applied Al models to make political classifications [11], an area in which GPT has already been used in Academia [42]. Also, it speaks to GPT's capacity to rate news outlets' credibility based on their web domain [13].\nThus, LLMs could be deemed an alternative method for these classification tasks, with some advantages over existing methods. The first is having a novel approach that does not necessarily rely solely on perception or behavior for the rating. There are benefits to reproducibility, which is hard (if not impossible) to achieve when humans judge the political bias of websites, as opinions might vary among different people or at different times. GPT allows setting low temperatures to reduce randomness and a fixed seed parameter to aid reproducibility. It still does not mean zero variability but limits it. Current methods of assessing political bias in news outlets can be costly and time-consuming, and LLMs might be cheaper, scalable alternatives, as noted by [11, p. 4].\nNevertheless, any endeavor using LLMs to assess the political bias of news websites will have to account for downsides. The most relevant is the lack of understanding about how these models attribute the ratings. While the scores are similar to the ones assigned by humans, it is hard to pinpoint the criteria used by the AI, which may hide biases. Some of those were explored in this research such as a skewness towards the left.\nAlso, the analysis shows that GPT can classify sources beyond the English-speaking world. While that might be good due to potentially covering more countries than existing datasets, this comes at a smaller correlation, which speaks to a known phenomenon that GPT performs better in English [43, pp. 7\u20138]. Coupled with most safety mitigations in the model being designed to work in English [41, p. 21], GPT's political bias assessment of new sources in areas that speak other languages should be deemed less reliable.\nUser prompting might play a negative role in influencing the results. If poorly executed, it might hinder AI's output quality. Differently from a human evaluator, the system might not say it did not understand badly explained instructions [11, p. 4].\nGPT's inability to classify roughly 2/3 of the data requires attention. This analysis provided some insight into some factors that might influence its ability, and they need to be accounted for as they might introduce biases in future analyses relying on this model for judging political bias in news sources. More obscure websites were less likely to be classified, showing a reduced capacity to judge content that deviates too much from mainstream media. Also, the AI was less likely to classify the least biased websites in MBFC's dataset. Applying GPT to judge ideology in news sources without considering this effect might lead to a perception of a more polarized environment due to a systematic failure to pick up the most central sources.\nAs with other applications, overreliance is an issue [43, p. 19]. Therefore, a mixed human and machine labeling methodology could be an alternative to leverage GPT's capabilities while keeping its biases in check. Any domain can be prompted to GPT-4, so, using the AI, researchers would not be subject to the constraints of lack of coverage in fixed datasets. Allowing the model to abstain from making a rating is paramount [44, p. 7029], as this could prevent GPT from assigning grades to sources with little information. Human classification could fill in the gaps. Another option for a hybrid approach is having the model output a confidence level for each classification and manually tag the ones that fall under a certain threshold [45], but further testing is needed to check this technique's reliability in this context.\nErrors (hallucinations) might be cause for concern. However, this analysis shows that the most extreme kind of misclassification (calling a far-right source far-left) only happened once. Given the high correspondence between AI and human labels discussed in this paper, manually inspecting random samples of the LLMs' classifications should ensure a reliable output. While laborious, analyzing a small subset of the data is more scalable than tagging it all. There is some disagreement between GPT and MBFC, but this is also true among multiple human-assigned rating services, so a level of discrepancy is acceptable in this scenario. Moreover, when analyzing in bulk, the divergence in ratings should be diluted, and if not, the sample analysis should notice it."}, {"title": "6 Conclusion", "content": "This paper examined the viability of using OpenAI's GPT-4 model to classify the political bias of news sources. The findings demonstrate a high correlation between GPT-4's classifications and MBFC's, suggesting that LLMs can be a reliable, cost-effective, and scalable alternative for such tasks. However, the study also revealed significant limitations. There is an indication that it performs worse on less-known websites. Also, GPT-4 could only classify 1/3 of the analyzed URLs, with a tendency to abstain from rating less popular and less biased sources. This shows a bias towards mainstream media and polarized classifications.\nFurthermore, GPT-4's ratings leaned slightly more to the left than MBFC's. These findings underscore the necessity of understanding and accounting for LLMs' underlying mechanisms and biases when deploying them to rate political ideology in news sources. While GPT-4 shows promise in enhancing the scope and efficiency of political bias classification, its application should complement human judgment rather than replace it."}, {"title": "6.1 Further Studies", "content": "This is a first foray into GPT's capacity to judge political bias in news sources based on web domain. More research is needed to understand this emergent capability. Other models should be explored since even Als from the same family might have distinct tendencies in ratings [9, p. 15611]. Varying the instructions also impacts the models' output [9, p. 15612], so this task must be explored under different settings, such as asking it to justify the rating or output a confidence level. This study left room for the model to refuse to rate a source, leading to numerous unassigned observations. Future investigators might check for impacts on performance when forcing the LLM to assign a rating.\nAlso, there are multiple ways of assessing political bias in the media (Section 2.3). This paper only checks for one dataset in this realm, while multiple exist using different approaches. Further research could investigate how GPT aligns with other sources, which might also provide insight into the criteria it uses to make its calls. Lastly, MBFC's dataset consists mostly of English-speaking sources, so further investigation is needed to assess the LLM's performance in different languages."}]}