{"title": "NARRATIVE-OF-THOUGHT: Improving Temporal Reasoning of Large Language Models via Recounted Narratives", "authors": ["Xinliang Frederick Zhang", "Nick Beauchamp", "Lu Wang"], "abstract": "Reasoning about time and temporal relations is an integral aspect of human cognition, essential for perceiving the world and navigating our experiences. Though large language models (LLMs) have demonstrated impressive performance in many reasoning tasks, temporal reasoning remains challenging due to its intrinsic complexity. In this work, we first study an essential task of temporal reasoning-temporal graph generation, to unveil LLMs' inherent, global reasoning capabilities. We show that this task presents great challenges even for the most powerful LLMs, such as GPT-3.5/4. We also notice a significant performance gap by small models (< 10B) that lag behind LLMs by 50%. Next, we study how to close this gap with a budget constraint, e.g., not using model finetuning. We propose a new prompting technique tailored for temporal reasoning, NARRATIVE-OF-THOUGHT (NOT), that first converts the events set to a Python class, then prompts a small model to generate a temporally grounded narrative, guiding the final generation of a temporal graph. Extensive experiments showcase the efficacy of NOT in improving various metrics. Notably, NOT attains the highest F1 on the Schema-11 evaluation set, while securing an overall F1 on par with GPT-3.5. NOT also achieves the best structural similarity across the board, even compared with GPT-3.5/4.", "sections": [{"title": "1 Introduction", "content": "Temporal reasoning is essential for humans to perceive the world, understand daily communications, and interpret the temporal aspects of experiences (Allen, 1983; Nebel and B\u00fcrckert, 1995). The recent advent of large language models (LLMs) has garnered substantial attention to their impressive performance in various reasoning tasks, such as arithmetic reasoning (Cobbe et al., 2021; Zhong et al., 2024) and commonsense reasoning (Talmor et al., 2019; Anil et al., 2023). Nonetheless, few LLMs exist to handle temporal reasoning well (Wang and Zhao, 2023; Chu et al., 2023; Chan et al., 2024), due to the task's inherent complexity, mingled with implicit logical inference and the necessity for profound world knowledge.\nTo gain deeper insights, the research community mainly focuses on two ends along the spectrum: either a simple relation extraction task that orders a pair of events (UzZaman et al., 2013; Yuan et al., 2023), or a perplexing commonsense understanding task demanding multifaceted reasoning skills beyond the mere temporal aspect (Wenzel and Jatowt, 2023; Tan et al., 2023; Xiong et al., 2024)."}, {"title": null, "content": "Worse still, the former is limited to a local scope spanning two adjacent sentences only and fails to account for the significance of global temporal relations, leading to overly optimistic results (Yuan and Liu, 2022; Wang and Zhao, 2023). Therefore, neither setup provides a clear understanding of LLMs' true temporal reasoning abilities.\nIn this work, we aim to unveil the inherent, global temporal reasoning capabilities of LLMs, evaluating them in isolation free from confounding factors, and addressing the limitations of previous studies which only focused on local contexts. We first introduce a task of temporal graph generation (TGG; Figure 1): Given a high-level goal\u00b2 T (e.g., business change) and a set of events V, the objective is to produce a temporal graph G(V,E) where a directed edge in E reveals the temporal order between events. Though this specific notion of TGG is new, many of its applications are not. In this work, we specifically study TGG in order to evaluate and improve the temporal reasoning capability, since TGG is deemed a major bottleneck when LLMs perform temporal reasoning. With TGG, we put forth the first research question.\nRQ1: What is the temporal reasoning capability of popular LLMs? Prior work (Wang and Zhao, 2023; Chu et al., 2023) shows a huge gap between AI systems and human performance on various temporal understanding tasks. Additionally, there is a notable performance disparity between proprietary LLMs (e.g., GPT-4) and open-weights LLMs, particularly those with fewer than 10 billion parameters (henceforth, small LLMs). Our study on temporal reasoning reveals a similar trend and identifies the existence of both gaps, as demonstrated in Table 1. This further highlights the importance of an in-depth investigation of TGG, since the performance of downstream tasks (e.g., temporal commonsense understanding) is positively correlated with the inherent, global temporal reasoning capability. Observing the model deficiencies, we are motivated to fill the gap between openweights, small LLMs and proprietary large models. This is due to the fact that open-weights LLMs are generally more accessible, reproducible, and cost-effective to use (Chen et al., 2023; Zhou et al., 2023). In pursuit of this goal, we present the second research question.\nRQ2: With a budget constraint (e.g., not allowing further training), how can small LLMs catch"}, {"title": null, "content": "up with large models like GPT-3.5/4? Given the constraint that no training will be used, we propose NARRATIVE-OF-THOUGHT (NOT), a special prompting technique tailored for temporal reasoning. This method capitalizes on the recent success of the Chain-of-Thought (CoT) technique (Wei et al., 2022b; Kojima et al., 2022), found effective in solving complex reasoning tasks. To approach TGG, NOT produces a final temporal graph via first generating a temporally grounded narrative\u00b3 then sorting the input events topologically in reference to the recounted narrative. Inspired by Madaan et al. (2022); Chen et al. (2022); Gao et al. (2023), NOT also features structural representations by converting the input-output mapping to a Python class, and instructing the generation in code space. We further improve NOT by introducing high-quality reference narratives as part of few-shot demonstrations.\nExtensive experiments across three evaluation benchmarks of diverse genres reveal six interesting findings: 1) small LLMs struggle with temporal reasoning even with few-shot examples; 2) CoT is also ineffective at temporal reasoning, in line with existing finding (Chu et al., 2023); 3) GPT-4 sometimes falls off the throne due to alignment, when answering sensitive queries; 4) NOT is a powerful tool to assist small LLMs to catch up with or even surpass GPT-3.5, and presents strong compatibility with various base LLMs; 5) the temporally grounded narratives are significant in improving LLMs' temporal reasoning process; 6) AI systems are far from mastering temporal reasoning, trailing the human baseline by 30 F1 points.\nWe also analyze the impact of shot numbers and perform a holistic evaluation of reference narratives in few-shot examples. 5-shot is found to be the sweet spot for temporal reasoning, after which the performance plateaus, likely due to long-context challenge. We identify three key characteristics of reference narratives for them to avail small LLMS most: conciseness, simplicity, and factuality."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Temporal Reasoning", "content": "This work is deeply rooted in a long-standing yet still challenging NLP domain\u2014temporal reasoning (Allen, 1983; Nebel and B\u00fcrckert, 1995), which"}, {"title": null, "content": "involves extraction, representation and reasoning with time and events (Sanampudi and Kumari, 2010). Depending on the cognitive complexity, temporal reasoning in NLP is studied at three levels: temporal expression detection, temporal relation extraction, and temporal graph generation. The simplest temporal expression detection task is to identify phrases in the text that convey temporal information (Setzer, 2001; Mani et al., 2001; Pustejovsky et al., 2003), commonly known as TimeX. Further, under-specified TimeX is typically converted to explicit expressions (e.g., \u201csummer 2024\") through a process called time expression normalization (Verhagen et al., 2010).\nExplicit TimeX is often absent in text, and events usually carry implicit temporal information. To bridge the gap, TempEval (Verhagen et al., 2009; UzZaman et al., 2013) is curated to support the study of temporal relation extraction, which aims to detect the temporal relation between two events in a document. The most common benchmarks, TB-dense (Chambers et al., 2014) and MATRES (Ning et al., 2018), have witnessed the technique evolution from LSTM (Dligach et al., 2017) and GNN-augmented BERT (Mathur et al., 2021; Wang et al., 2022), to LLMs prompting (Yuan et al., 2023). Yet, these benchmarks are limited by their locality assumption, where only pairs of events within a two-sentence window are annotated. Even in this simplified scenario of temporal relation extraction, ChatGPT perform poorly, trailing supervised systems by over 30% (Chan et al., 2024).\nThe most challenging task, contextualized temporal graph extraction, is defined as, given a document, generating a corresponding event-level temporal graph (UzZaman et al., 2013; Madaan and Yang, 2021). This task addresses the limitation of locality by priming models to comprehend the entire article and infer relationships even between distant events. Yet, this area is largely under-investigated, partly due to the scarcity of available datasets. A similar task is script learning (Regneri et al., 2010; Modi et al., 2016; Sakaguchi et al., 2021), which targets inducing a stereotypical progression of complex events (Schank and Abelson, 1975), represented as a temporal graph of more atomic events. This task is usually approached by first extracting information snippets from a given document to build an instance graph, and then expanding the graph to generate a schematic graph using GNN (Li et al., 2021; Jin et al., 2022) or LLM prompting (Dror et al., 2023). Given the\""}, {"title": null, "content": "remarkable similarities between these two tasks, we instead study a temporal reasoning task formulation that is fundamental to both, i.e., temporal graph generation. It differs from prior work in at least two dimensions: (1) a limited-context setting, where only abstract event descriptions are available, and (2) only a few training samples at hand, rendering fine-tuning techniques inapplicable. This motivates a training-free assessment of LLMs' inherent, global temporal reasoning capability."}, {"title": "2.2 Chain-of-Thought and its Variants", "content": "Despite the strong problem-solving capability in the general domain (Wei et al., 2022a), LLMs struggle to address more complex reasoning tasks, such as commonsense understanding and arithmetic reasoning (Patel et al., 2021; Talmor et al., 2021a; Huang and Chang, 2023). Wei et al. (2022b) first introduce the concept Chain-of-Thought (CoT) by decomposing multi-step problems into intermediate steps. Kojima et al. (2022) further adds a phrase \"Let's think step by step\" to perform zero-shot CoT. These studies underpin the CoT technique in enhancing LLMs' capability for complex reasoning.\nDown the line, sophisticated prompting schemes are devised through structuralization. One approach is to extend the linear chain structure to Tree-of-Thoughts (Yao et al., 2023) and Graph-of-Thoughts (Besta et al., 2024), enabling expanded exploration space. The huge search space, however, results in a computational resource dilemma. On top of that, leveraging the deterministic execution to narrow the discrepancy between reasoning and final answer, PoT (Chen et al., 2022), PAL (Gao et al., 2023) and Faithful CoT (Lyu et al., 2023) introduce programming languages to describe the reasoning process structurally. These methods are designed exclusively for solving mathematical reasoning and symbolic reasoning, where the reasoning process and computation can be decoupled. In contrast, for temporal reasoning, the reasoning process and the temporal sorting step are intrinsically interleaved. In fact, Chu et al. (2023) has attempted to apply CoT but proved unsuccessful.\nMoreover, existing methods are mostly applied to generate intermediate rationales for simple, atomic outputs, usually in the format of multi-choice options (Mihaylov et al., 2018; Talmor et al., 2019; Liu et al., 2020), a number (Cobbe et al., 2021; Hendrycks et al., 2021), or yes/no options (Talmor et al., 2021b; Wei et al., 2022a). Our work draws a clear distinction where our focus is"}, {"title": "3 Method: NARRATIVE-OF-THOUGHT", "content": "Figure 2 provides an overview of the proposed NARRATIVE-OF-THOUGHT (NOT) method, and draws a comparison against common prompting techniques. Overall, given a scenario and a set of events, NOT first converts the input into a Python class, then guides LLMs to produce a temporally grounded narrative by arranging events in the correct temporal order, leveraging LLMs' intrinsic temporal knowledge. Based on the recounted temporal relations articulated in the narrative, LLMs are instructed to sort events into a temporal graph. This section will discuss major components in detail: (1) structural representation, (2) NOT prompting template, and (3) narrative-aware demonstrations.\nStructural Representation. Following prior work (Madaan et al., 2022; Chen et al., 2022; Gao et al., 2023), we cast temporal reasoning as a code completion task. This design decision is motivated"}, {"title": null, "content": "by the unordered nature of both event sets and temporal relation sets, making a structural representation the optimal choice. Wang et al. (2023a) also shows that combining structural event representations with LLMs trained with a mixture of text and code can unleash the full pretraining power. We extend this framing to handle cross-event structures. Specifically, a temporal graph is commonly presented in DOT format (Madaan and Yang, 2021; Sakaguchi et al., 2021), the appearance of which lends itself naturally to the usage of coding format. Furthermore, code execution follows a clear, step-by-step logical flow, mirroring the process of reasoning. Bringing these aspects together results in an alignment between temporal graphs and code structure, facilitating the temporal reasoning process. Our further study on this phenomenon also reveals a strong positive correlation between coding capabilities and temporal reasoning, as documented in Appendix E.\nConcretely, each scenario is represented as a Python class. Each class encapsulates events as functions, where the function name is in the form of \"step[A-Z]\" such as \u201cstepX\u201d, and the function body indicates the event description. The temporal graph is represented as a collection of pairwise temporal relations, enclosed within the return statement of \"get_relation()\" function, marked by \u201cTODO\" for LLMs to implement.\""}, {"title": null, "content": "NARRATIVE-OF-THOUGHT (NOT). At inference time, NOT first prompts LLMs to produce a temporally grounded narrative using Narrative Prompt. Drawing on the generated narrative, LLMs proceed and complete generation in response to Temporal Graph Prompt. The entire generation process is in an end-to-end manner, ensuring that LLMs explicitly leverage the temporal relations articulated in the narrative to assist the generation of the final temporal graph. We provide a complete example in Appendix C.\nOverall, NOT narrows the gap between pretraining and inference by allowing the LLM to unfold the narrative knowledge seen during pretraining. Concretely, our approach leverages LLMs' inherent strengths in generating and comprehending text for narrative and temporal graph generation, respectively. In contrast, directly mapping abstract events to a temporal graph is less effective, as such examples are rarely encountered during pre-training. Practically, generated narratives create imagined experiences to navigate, and reify implicit timelines, assisting reasoning over a series of events even without explicit timestamps provided in the text, which are crucial for tasks requiring temporal reasoning. By reading the recounted narrative, it becomes easier for the LLMs to construct an implicit timeline to guide event sorting, significantly reducing the reasoning complexity compared to generating temporal graphs from scratch (i.e., using abstract events alone).\nOur NOT draws a clear distinction from the CoT prompting and its variants in four aspects. First, for CoT, a final answer cannot be easily extracted unless a post-hoc script is designed (Kojima et al., 2022; Wang et al., 2023b; Zheng et al., 2024), which can be sometimes error-prone, while the output of NOT is easy to obtain by parsing the get_relations() function. Second, NOT produces final outputs in the structural space, while existing methods solely produce simple, atomic"}, {"title": null, "content": "outputs as discussed in \u00a72.2. Third, NOT produces final temporal graphs cost-effectively without external tools in an end-to-end fashion, unlike pipeline approaches which face error propagation and over-sampling issues (Dror et al., 2023). Lastly, the generated rationales by CoTs are not necessarily grounded in real-world experience. In contrast, generated narratives by NOT are steered to be more temporally grounded, creating an imagined experience for LLMs to navigate, which is proved effective.\nNarrative-aware Demonstrations. Existing studies (Brown et al., 2020; Wei et al., 2022a) have demonstrated that in-context demonstrations play a critical role in guiding LLMs to produce meaningful outputs. NOT is no exception, as Table 1 reveals that even GPT-3.5 struggles with temporal reasoning in a zero-shot setting. Thus, few-shot examples are provided by default. For NOT to succeed, high-quality and relevant rehearsed narratives, termed reference narratives, need to be created and embedded in these demonstrations.\nCapitalizing on the recent success of using LLMs to generate demonstrations (Yu et al., 2023; Li et al., 2023), we prompt GPT-3.5/4 to produce reference narratives. Concretely, for each demonstration, abstracted as G(V, E), we feed both V and E into GPT-3.5/4, using our designed reference narrative generation templates, dubbed meta prompts. In total, we create 4 types of meta prompts covering diverse genres like news and children's stories. Additionally, when feeding G(V, E) into GPT-3.5/4, we use two input formats to define a Python class (alphabetical like \u201cstepX\u201d in Figure A8 vs. descriptive like \"pushPedal\" in Figure A9). We later evaluate the usefulness of each meta prompt in \u00a75.2. Details of meta prompts are documented in Appendix D."}, {"title": "4 Experiment", "content": "In this work, we focus on Temporal Graph Generation (TGG), an essential task of temporal reasoning. Here, we discuss datasets, experimental setup, baselines, and evaluation metrics. We provide additional implementation details in Appendix A."}, {"title": "4.1 Dataset", "content": "In line with the literature, we use ProScript (Sakaguchi et al., 2021) as the major benchmark, where a temporal script is represented as a directed acyclic graph, which were collected from a diverse range of"}, {"title": "4.2 Setup", "content": "As our goal is to study the capability and generalizability of existing LLMs, and our NOT without any fine-tuning, we assume no access to large-scale training sets except for few-shot demonstrations. Therefore, all experiments are conducted in a 5-shot setting. We provide analysis on the impact of the shots numbers in \u00a75.2. We consider three"}, {"title": "4.3 Baselines", "content": "To showcase the effectiveness of NOT, for each base model we compare with standard structural prompting and structuralized chain-of-thought prompting (Figure 2). We also remove reference"}, {"title": "4.4 Evaluation Metrics", "content": "We denote the ground-truth and generated temporal graphs as G(V,E) and \u011c(V, \u00ca), respectively. we compare both semantic and structural similarities between G and G, following prior work (Sakaguchi et al., 2021; Madaan et al., 2022). To evaluate semantic similarity, we report precision (P) and recall (R), defined as below, as well as F1.\nPrecision = |E \u2229 \u00ca|/|\u00ca| Recall = |E \u2229 \u00ca|/|E|\nTo assess structural similarities, we consider:\n\u2022 Graph Edit Distance (GED; Abu-Aisheh et al., 2015) calculates the minimum number of edits (node/edge removal/additions) to transform \u0177 to a graph isomorphic to G.\n\u2022 Graph Statistics: fraction of the number of edges between \u0177 and G(); (1); the number of connected components in \u0177, denoted as k(G). The goal is to bring both statistics closer to 1, additionally ensuring k(G) is at least 1.\nWe further calculate Pair-wise Consistency between Gi and Gj, where we compare generated graphs, based on two randomly shuffled inputs, and compute the proportion of common temporal links produced in both graphs, i.e., |Ein\u025bj|"}, {"title": "5 Results and Analyses", "content": ""}, {"title": "5.1 Main Results", "content": "Major results are included in Table 1, and the full results (across all 7 metrics) can be found in Table A1. Below are our major findings.\n1) With the few-shot setup, small LLMs are dramatically underperforming, reaching barely 50% of GPT-4's capabilities. The three base models, whether using standard prompting or CoT, consistently under-perform GPT-4 and attain 40% to 60% of its average F1 scores. Among them, MISTRAL-7B achieves the highest F1 scores, while LLAMA3-8B produces temporal graphs most similar to the ground truth, as measured by GED.\n2) Unlike many other reasoning tasks, CoT does not always work for temporal reasoning and sometimes degrades performance. Unlike mathematical or logical reasoning (Wei et al., 2022b), CoT prompting does not necessarily enhance model performance on temporal reasoning tasks. Across all three base models, there is a notable degradation in F1 and GED scores with CoT, except for LLAMA3's F1 scores. This is not TGG-specific, but rather a common pattern across various temporal understanding tasks (Chu et al., 2023), highlighting the need for specialized approaches to temporal reasoning. Outputs by CoT are included in Figure A6.\n3) GPT-4 is not always the champion, owing to the added safety layer. GPT-4 implements safety measures through human-preference alignment (OpenAI, 2023), which enhances model safety by prompting more cautious responses, potentially leading to performance drop (Bai et al., 2022; Bekbayev et al., 2023). Especially on Schema-11, GPT-4 refrains from providing answers to sensitive scenarios like \u201cbombing attacks\u201d, and thus fails to produce a valid temporal graph.\n4) With NOT, small LLMs can perform comparably to GPT-3.5, or even take the lead. When equipped with NOT, the overall semantic correctness (F1) and structural similarity (GED) of the"}, {"title": null, "content": "generated temporal graphs are significantly enhanced, regardless of which base LLM is used. The average improvement of F1 over naively prompting the base model is between 16% to 71%. As the power of the base LLM grows, NOT demonstrates greater consistency in its outputs. Notably, with LLAMA3-8B, the strongest base LLM, NOT achieves an F1 score that is comparable to GPT-3.5 (42.2 vs. 45.7), and even outperforms GPT-3.5/4 on GED. These results demonstrate the potential of applying NOT in a wide range of temporal understanding tasks in future research.\n5) Recounting temporally grounded narrative is a prerequisite for LLMs to generate temporal graphs accurately. Without high-quality reference narratives, LLMs struggle to generate temporally grounded narratives, leading to a detrimental impact on NOT-augmented GEMMA-7B (e.g., a 0.7 F1 drop and a 0.67 GED increase).\n6) LLMs, including the powerful GPT-4, lag far behind human-level performance in temporal reasoning. The SOTA F1 score (by GPT-4) on ProScript is 63.9, whereas the human baseline F1 is 89.3 (Sakaguchi et al., 2021). While NOT has notably narrowed the gap between small and large LLMs, AI models have not mastered temporal reasoning yet, and further research efforts are needed for LLMs to match human performance.\nComparison with fine-tuned LLMs. To evaluate the performance gap between the NOT prompting technique and the computational-intense finetuning (FT) approach, we conduct a side experiment on the ProScript dataset. Specifically, each instruction-tuned base LLM is fine-tuned on the ProScript training set, utilizing LoRA (Hu et al., 2022) and mixed-precision training. We follow the same setting as in \u00a74.2 where each training example is prepended with 5-shot demonstrations. While significant performance disparities between NOT and FT are observed across the board, the narrowing gap suggests the growing potential of NOT as the underlying LLM continues to evolve. Moreover, fine-tuned small LLMs consistently outperform the few-shot GPT-4, which is the best-performing gen-"}, {"title": null, "content": "eralist model on the ProScript dataset. This underscores the continued efficacy of FT in building specialized models, even in the era of LLMs."}, {"title": "5.2 Further Studies on NOT", "content": "We conduct ablation studies using LLAMA3-8B, to explore the effect of the few-shot demonstrations and the recounted reference narratives.\nDoes the number of shots matter? Figure 3 illustrates how F1 scores change with the number of shots in demonstrations. As can be seen, GPT-3.5 and NOT show resilience to changes in shot numbers after an initial sharp increase. The performance nearly stabilizes in the range of 5-10 shots, though a slight drop is observed later, presumably due to insufficient capability of long-context comprehension (Liu et al., 2023; Li et al., 2024). Of particular interest is the performance of NOT with 3 shots on Schema-11, outperforming the best variant of GPT-3.5 (F1 of 63.5 vs. 62.8). This further illustrates NOT's potential of boosting small LLMs in the long run. It is also noticeable that F1 scores of the standard prompting technique have a V-shape between 1-shot and 5-shot, highlighting its sensitiveness to in-context demonstrations.\nWe also display the GED scores in relation to number of shots in Figure A1. We observe similar instability in the standard prompting technique, along with the performance plateau after 5 shots.\nWhat characteristics define effective reference narratives? Given that reference narratives in"}, {"title": null, "content": "NOT are machine-generated, we aim to explore what qualities matter most for the TGG task. Here, the three variables influencing reference narratives are: (1) narrative generation model (GPT-3.5 vs. GPT-4), (2) input format (alphabetical vs. descriptive), and (3) 4 meta prompt types (varying degrees of factuality and readability). We show detailed meta prompts in Appendix D.\nFigure 4 and Figure A2 show results of F1 and GED with varying meta prompts. Surprisingly, the choice of the generator does not significantly impact the graph quality, with average F1 scores of 36.4 for GPT-3.5 and 37.0 for GPT-4, and GED scores of 1.90 vs. 1.94. Similarly, there is no significant difference between alphabetical and descriptive input formats. The most impactful factor is the meta prompt type. Grouping performance bars by prompt type reveals a clear variance in model performance. Among the first three groups, Simple English narratives, i.e., good for 10-year-olds, stand out. This suggests that narratives should be simple and concise, as verbose ones are less effective. We find that News Report narratives prioritize procedural and factual content, minimizing distractions like descriptive settings or figurative language that can often be found in both fiction or non-fiction stories. We thus combine Simple English and News Report to leverage their strengths, dubbed Simple Report. In summary, we identify three key characteristics for quality reference narratives: conciseness, simplicity and factuality.\nHow faithful is the temporal graph to intermediate narratives? Here, we look into whether NOT-augmented LLMs are self-faithful, i.e., whether the narrative and the temporal graph align in terms of the temporal order of events. Higher self-faithfulness is crucial and desired, as misalignment would diminish the effort of generating a temporally grounded narrative.\nMotivated by the recent success of using LLMs as judges (Zheng et al., 2023; Zhang et al., 2024a), we employ GPT-4 to assess the self-faithfulness of 600 randomly sampled outputs by NOT-augmented LLAMA3-8B. We prompt GPT-4 to perform a 5-way assessment and provide judgment rationales. Additionally, GPT-4 is instructed to count the temporal links in the temporal graphs and identify aligned temporal links for a sanity check. This helps humans capture the failure modes and make"}, {"title": "6 Conclusion", "content": "In this paper, we assess the inherent, global temporal reasoning capabilities of LLMs, by studying the core challenge of temporal reasoning\u2014temporal graph generation (TGG). To this end, we propose NARRATIVE-OF-THOUGHT (NOT), a novel prompting technique tailored for temporal reasoning. Concretely, with few-show narrativeaware demonstrations as references, NOT prompts LLMs to first generate a temporally grounded narrative and then sort the input events topologically into a temporal graph, by manipulating the generation in code space. Extensive experiments showcase NOT's effectiveness, demonstrated by its superior performance over GPT-3.5 on multiple metrics, as well as the compatibility of NOT with various LLMs."}, {"title": "A Additional Implementation Details", "content": "Few-shot Demonstration Selection. To construct the demonstration bank, we select 15 examples from the training set of ProScript, following Madaan et al. (2023). We do so because we expect to include non-linear temporal graph examples in our demonstrations, for which only ProScript can fulfill the requirement. Then, we use the same demonstrations as few-shot examples for experiments, regardless of the evaluation benchmark.\nModel Cards. In this work, we have experimented with 3 base LLMs. Below lists the exact Huggingface model cards used in this work.\n\u2022 GEMMA-7B: google/gemma-7b-it\n\u2022 MISTRAL-7B:\nmistralai/Mistral-7B-Instruct-v0.2\n\u2022 LLAMA3-8B:\nmeta-llama/Meta-Llama-3-8B-Instruct"}, {"title": "B Dataset Processing", "content": "This section documents the processing steps performed on Schema-11 and WikiHow Script to cater for the temporal reasoning task of our interest. We do not use any Python packages for dataset processing. Meanwhile, based on our inspection, we do not spot any offensive content in these three datasets.\nSchema-11. In their original annotations, an event node is marked in argo-trigger-arg1 format, and we manually convert it to a natural sentence. We specifically adopt annotations under schemas_dan_d directory.\nWikiHow Script corpus. The original dataset features multilingualism, while we only take their English portion for this study. Then, We only keep ordered how-to articles where steps are presented in chronological order. Lastly, we cap the maximum number of steps at 20, which reduces the corpus size from 3, 3035 to 2, 077."}, {"title": "C Complete Examples", "content": "Using the same example as in Figure 1 and Figure 2, we show the complete examples (including generations by one base LLM, LLAMA3-8B) of Standard Prompting, CoT and NOT. We first show the input part of Standard Prompting and CoT in Figure A3, and the input of NOT in Figure A4. Outputs by Standard Prompting, CoT and NOT are displayed"}, {"title": "D Meta Prompt", "content": "This section discusses the major components of a meta prompt, used to generate reference narratives. As shown in Figure A8 and Figure A9, a meta prompt consists of two parts: input (in Python programming language) and instruction (above and below the input). The input contains both V (event set) and E (temporal relation set), and the goal is to prompt LLMs to generate a high-quality reference narrative. The input has two formats: alphabetical (Figure A8) format where the function header is represented in the same fashion as in Figure 2, and descriptive (Figure A9) where the function header is the camel-cased version of the complete event description. The instruction part specifies how LLMs are supposed to carry out the narrative generation, reflecting different types and genres. Specifically, we designed four different instructions, listed in Table A2. They are News Report, Simple English, Role Play and Simple Report, which is essentially a seamless combination of News Report and Simple English."}, {"title": "E Correlation Analysis", "content": "We start our empirical analysis by presenting performances on well-regarded coding benchmarks, HumanEval and MBPP, of the three selected base LLMs included in this work. Based on these results in Table A4, the ranking is Mistral (1) < Gemma (2) < LLAMA3 (3), with the numbers in parentheses indicating their relative scores in this comparison.\nSecond, regarding instruction-following capability in code completion, we evaluated how well the models adhered to provided instructions (i.e., implementing the return statement of \"get_relations(self)\u201d; Also see Figure A3). Model outputs are provided in Table A5, Table A6 and"}, {"title": "F Faithfulness Checking Details", "content": "Table A3 shows the template being used to prompt GPT-4 to produce a judgment. GPT-4 performs a 5-way assessment: yes, largely yes, ambivalent, largely no, and no, where yes means exact alignment while no means no alignment at all. With the counting puzzle as a sanity check, we find that GPT-4 does not count the number of temporal links wrong at all. We thus rely on the returned value of correct temporal links as a means to determine the failure mode. Before human inspection, the distribution among yes/largely yes/largely no/no"}]}