{"title": "Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance", "authors": ["Joshua McClellan", "Naveed Haghani", "John Winder", "Furong Huang", "Pratap Tokekar"], "abstract": "Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [2]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.", "sections": [{"title": "1 Introduction", "content": "Multi-Agent Reinforcement Learning (MARL) has found success in various applications such as robotics [3, 4, 5], complex strategy games [6, 7, 8] and power grid management [9, 10]. However, MARL algorithms can be slow to train, difficult to tune, and have poor generalization guarantees [1, 11]. This is partially because typical implementations of MARL techniques use neural networks such as Multi-Layer Perceptrons (MLP) that do not take the underlying structure into account. The models learn simple input/output relationships with no constraints or priors on the policies learned. These generic architectures lack a strong inductive bias making them inefficient in terms of the training samples required.\nSymmetries are commonplace in the world. As humans, we exploit symmetries in our daily lives to improve reasoning and learning. It is a basic concept learned by children. Humans do not need to relearn how to eat an apple simply because it has been moved from the right to the left. In soccer, if you have learned to pass to someone on the right, it is easier to learn to pass to someone on the left. Our objective is to develop agents that are guaranteed to adhere to these basic principles, without needing to learn every single scenario from scratch.\nSymmetries are particularly common in MARL. These occur in the form of equivariance and invariance. Given a transformation matrix T, if $f(Tx) = f(x)$ that function is said to be invariant. Similarly, f(.) is equivariant if $f(Tx) = Tf(x)$ [12]. Rotational and reflection symmetries (the O(n) symmetry group) are particularly common in Reinforcement Learning (RL) scenarios. For example, consider the Multi-agent Particle Environment (MPE) [13] benchmark for MARL which has agents with simple dynamics and tasks. We note that this scenario adheres to rotational equivariance. As shown in Figure 2 rotating the agent's positions results in the optimal actions also being rotated. A policy that is rotationally equivariant effectively shrinks the state-action space for the problem, potentially making the problem easier to learn (Figure 1).\nOne way to guarantee equivariance to rotations and reflections in MARL is to use an Equivariant Graph Neural Network [2]. Due to the more complex and equivariant structure of the EGNN, directly adapting EGNNs to MARL is not straightforward. Specifically, consider action spaces typically represented as discrete choices (e.g., up, down, left, shoot). Typically the policy over such an action space is represented using logits (to specify the probability of choosing each action). EGNN outputs a continuous equivariant output. Mapping this output to logits is non-trivial. Therefore, we ask the question What is the correct representation of a complex action space when using Equivariant GNNs?\nAnother specific issue we observed in EGNNs is an exploration bias in the early stages of learning due to the specific nature of the equivariant computation. This bias can lead to poor initial exploration, decreasing the probability of the agent finding the optimal policy, and potentially resulting in sub-optimal convergence. Therefore we ask the question How can we design a GNN equivariant to rotations, but without early exploration bias?\nThe main contribution of this paper is Exploration-enhanced Equivariant Graph Neural Networks (E2GN2). This addresses the two research questions. Specifically, we show how to apply Equivariant GNNs to complex MARL tasks, to mixed discrete/continuous action spaces, and to mitigate the early exploration bias. Our contributions can be summarized as:\n(1) Our approach is the first to successfully demonstrate Equivariant GNNs for MARL on standard MARL benchmarks with complex action spaces.\n(2) We propose E2GN2 which has no bias in early exploration for MARL and is equivariant to rotations and reflections.\n(3) We evaluate E2GN2 on common MARL benchmarks: MPE [13] and Starcraft Multi-agent Challenge v2 [14] using PPO. It outperforms standard GNNs and MLPs by more than 10x in sample efficiency. It is worth noting that E2GN2 is an improvement on the function approximation for MARL and thus is compatible with most MARL actor-critic methods.\n(4) We showcase E2GN2's ability to generalize to scenarios it wasn't trained on, due to the equivariance guarantees. This results in 5x performance over GNNs"}, {"title": "2 Related Works", "content": "Al research in chemistry has taken a particular interest in adding symmetry constraints to GNNs. Works such as EGNN, [2] SEGNN, [15] E3NN, [16] and Equivariant transformers have demonstrated various approaches to encoding symmetries in GNNs. In this work, we chose to focus on EGNN due to its simplicity, high performance, and quick inference time. Other works took a different approach, such as [17], which proposes Equivariant MLPs, solving a constrained optimization problem to encode a variety of symmetries. Unfortunately, in our experience, the inference time was slow, and we preferred a network with a graph structure such as EGNN.\nA key theoretical foundation to our paper is [18], which formulated the structure for equivariant MDPs. One important takeaway is that if a reward is equivariant to a transformation/symmetry, we want the policy and dynamics to be equivariant to that symmetry. However, this work was limited to very simple dynamics with discrete actions such as cart-pole. [19] followed up with equivariance in multi-agent, but was again limited to simple problems. In parallel with our research [20] demonstrated E(3) equivariance on simple cooperative problems. However, the results on more complex tasks, such as Starcraft, did not excel over the baseline. Others took the approach [21] of attempting to learn symmetries via an added loss term. However, since this approach needed to learn the symmetries in parallel with learning it did not have the same guarantees as Equivariant GNNs. Another work [22] demonstrated rotation equivariance for complex robotic manipulation tasks. This work, while promising, was for single-agent RL and used image observations and many problems don't have access to image observations."}, {"title": "3 Background", "content": ""}, {"title": "3.1 MARL", "content": "Multi-agent reinforcement learning (MARL) considers problems where multiple learning agents interact in a shared environment. The goal is for the agents to learn policies that maximize long-term reward through these interactions. Typically, MARL problems are formalized as Markov games [23]. A Markov game for N agents is defined by a set of states S, a set of actions $A_1, ..., A_N$ for each agent, transition probabilities $P : S \\times A_1 \\times \\dots \\times A_N \\times S \\rightarrow [0, 1]$, and reward functions $R_1, ...R_N$ mapping each state and joint action to a scalar reward.\nThe goal of each agent i is to learn a policy $\\pi_i(a_i|s)$ that maximizes its expected return: $J(\\pi_i) = E_{\\pi_1, ..., \\pi_N} [\\sum_{t=0}^T \\gamma^t R_i(s_t, a_1^t, ..., a_N^t)]$\nWhere T is the time horizon, $\\gamma \\in (0,1]$ is a discount factor, and $a_j \\sim \\pi_j(\\cdot|s_t)$. The presence of multiple learning agents makes this more complex than single-agent RL due to issues such as non-stationarity and multi-agent credit assignment."}, {"title": "3.2 Equivariance", "content": "Crucial to equivariance is group theory. A group is an abstract algebraic object describing a symmetry. For example, O(3) describes the set of continuous rotation symmetries. A group action is an element of that particular group. To describe how groups operate on data we use representations of group actions. A representation can be described as a mapping from a group element to a matrix, $\\rho : G \\rightarrow GL(m)$ where $\\rho(g) \\in \\mathbb{R}^{m\\times m}$ Or instead we can more simply use: $T_g : X \\rightarrow X$ where $g \\in G$ where $T_g$ is the matrix representation of the group element $g \\in G$ [24].\nA function is equivariant to a particular group or symmetry if transforming the input is equivalent to transforming the function output. More formally, $T_g f(x) = f(L_gx)$ for $g \\in G$, $T_g : X \\rightarrow X$ and $L_g : Y \\rightarrow Y$. Related to equivariance is the key concept of invariance, that is a function does not change with a transformation to the input: $f(x) = f(L_gx)$. [24]\nPrevious work [19] has shown that if a Markov game has symmetries in the dynamics and the reward function then the resulting optimal policy will be equivariant and the value function will be invariant. That is, $V(L_gs) = V(s)$ and $\\pi(L_gs) = K_g\\pi(s)$ where $L_g$ and $K_g$, with $g \\in G$ are transformations to the state and action respectively."}, {"title": "3.3 Equivariant Graph Neural Network", "content": "Equivariant Graph Neural Networks [2] (EGNN) are an extension of a standard Graph Neural Network. EGNNs are equivariant to the E(n) group, that is, rotations, translations, and reflections in Euclidean space. EGNNs have two vectors of embeddings for each graph node i: the feature embeddings denoted by h, and coordinate embeddings denoted by u, where l denotes the neural network layer. The equations describing the forward pass for a single layer are below:\n$\\begin{aligned} m_{ij} &= \\phi_e (h_i^l, h_j^l, ||(u_i^l - u_j^l)||^2) \\\\ u_i^{l+1} &= u_i^l + C\\sum_{j\\neq i} (u_i^l - u_j^l) \\phi_u (m_{ij}) \\\\ m_i &= \\sum_{j \\neq i} m_{ij}, h_i^{l+1} = \\phi_h (h_i^l, m_i)\\end{aligned}$\nHere $\\phi$ represents a multi-layer perceptron, where $\\phi_e : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, $\\phi_u : \\mathbb{R}^m \\rightarrow \\mathbb{R}$, and $\\phi_h: \\mathbb{R}^{m+p} \\rightarrow \\mathbb{R}^p$. The key difference between EGNN and GNN is the addition of coordinate embeddings $u_i$ and equation 2, which serves to update the coordinate embeddings in a manner that is equivariant to transformations from E(n). Note that $u_i$ will be equivariant and $h_i$ will be invariant to these transformations [2].\nAs noted in Section 1, application of EGNN to MARL is not straightforward. In the following section, we discuss these issues in more depth and present our solution towards addressing them."}, {"title": "4 Methods", "content": "In this section, we address both theoretically and empirically how the output of EGNN is biased by the input, leading to suboptimal exploration in RL. To mitigate this issue, we introduce Exploration-enhanced Equivariant Graph Neural Networks (E2GN2) as a method that ameliorates this bias, leading to improved exploration."}, {"title": "4.1 Biased Exploration in EGNN Policies", "content": "An important component of reinforcement learning is exploration. Practitioners often use a policy parameterized by a Gaussian distribution, where the mean is determined by the policy network output, and the standard deviation is a separately learned parameter. Best practices are that the actions initially have a zero mean distribution to get a good sampling of potential state-action trajectories, i.e., $\\pi(a_i|s) \\sim \\mathcal{N}(0, \\sigma)$. Below we show that an EGNN will initially have a non-zero mean distribution, which can cause problems in early training.\nTheorem 1 Given a layer l of an EGNN with randomly initialized weights, with the equivariant component input vector $u_i^l \\in \\mathbb{R}^e$, equivariant output vector $u_i^{l+1} \\in \\mathbb{R}^e$ and the multi-layer perceptron $\\phi_u : \\mathbb{R}^m \\leftrightarrow \\mathbb{R}$, where the equivariant component is updated as: $u_i^{l+1} = u_i^l + C\\sum_{j\\neq i} (u_i^l - u_j^l) \\phi_u (m_{ij})$. Then the expected value of the output vector is approximately the expected value of the input vector:\n$\\mathbb{E} [u_i^{l+1}] \\approx \\mathbb{E} [u_i^l]$\nFurthermore, given a full EGNN with L layers then the expected value of the network output is approximately the expected value of the network input\n$\\mathbb{E} [u_i^L] \\approx \\mathbb{E} [u_i^l]$\n(See appendix A for proof.)\nCorollary 1.1 Given a policy for agent k represented by an EGNN and parameterized with a Gaussian distribution, and the equivariant component of the state $s_k^e$ the policy will have the following distribution:\n$\\pi_k(a_i | s) \\sim \\mathcal{N}(s_k^e, \\sigma)$"}, {"title": "4.2 Exploration-Enhanced EGNNS", "content": "As discussed previously, EGNN's severe early exploration bias can decrease learning performance. In this section, we propose our solution to this problem in the form of Exploration-enhanced Equivariant Graph Neural Networks (E2GN2). To create E2GN2 we make the following modification to Equation 2 of the equivariant component of EGNN:\n$u_i^{l+1} = \\phi_{u_2}(m_i) + C\\sum_{j\\neq i} (u_i^l - u_j^l) \\phi_u (m_{ij}))$\nwhere $\\phi_{u_2}(m_i) : \\mathbb{R}^m \\rightarrow \\mathbb{R}$ is an MLP parameterized by $u_2$. The remaining layer update equations of the EGNN remain the same.\nTheorem 2 Given a L layer E2GN2 with randomly initialized weights, where the equivariant component is updated as: $u_i^{l+1} = \\phi_{u_2}(m_i) + C\\sum_{j\\neq i} (u_i^l - u_j^l) \\phi_u (m_{ij}))$. Then the expected value of the output vector is approximately the expected value of the input vector:\n$\\mathbb{E} [u_i^{L+1}] \\approx 0$\nCorollary 2.1 Given a policy for agent k represented by an E2GN2 and parameterized with a Gaussian distribution the policy will have the following distribution:\n$\\pi_k(a_i | s) \\sim \\mathcal{N}(0, \\sigma)$"}, {"title": "Analysis of E2GN2 Equivariance", "content": "Here we verify that E2GN2 still retains EGNN's guarantee of equivariance to rotations and reflections.\nTheorem 3 E2GN2 is equivariant to transformations from the O(n) group. In other words, it is equivariant to rotations and reflections.\n(See appendix A for proof.)\nRetaining our symmetries to rotations and reflections is important, since it should increase our sample efficiency and generalization. Note that E2GN2 is no longer equivariant to translations. However, in MARL translation equivariance is not necessarily a benefit. For example, consider the MPE problem with a translational equivariant policy. If we shift an agent to be 10 units right, this will add 10 to the action as well, causing it to move to the right! However, we can expect O(n) equivariance to improve our sample efficiency in MARL."}, {"title": "4.3 Complex Action Spaces", "content": "In addition to dealing with early exploration bias, applying equivariance to MARL problems with complex action spaces requires careful consideration. Many MARL environments have discrete action spaces, while the equivariant output of EGNN and E2GN2 is continuous. The MARL agents generally require logits specifying the probability of each discrete action. However, it is not straightforward to map the equivariant coordinate embeddings to these logits in a manner that preserves equivariance.\nAdditionally, a key benefit of the Graph Neural Network structure in MARL is permutation invariance - the ability to handle a variable number of agents without retraining. If the GNN output is mapped to logits improperly, this scalability advantage is lost. To address these issues, we propose leveraging the GNN's graph structure to output different components of the action space from different nodes in an equivariant manner:"}, {"title": "5 Experiments", "content": "We seek to answer the following questions with our experiments: (1) Do rotationally equivariant policy networks and rotationally invariant value networks improve training sample efficiency? (2) Does E2GN2 improve learning and performance over EGNN? (3) Does Equivariance indeed demonstrate improved generalization performance?\nTo address these questions, we use common MARL benchmarks: the multi-agent particle-world environment (MPE) [13] and Starcraft Multi-agent Challenge version 2 (SMACv2) [14]. Our experiments show that equivariance does indeed lead to improved sample efficiency, with E2GN2 in particular performing especially well. We also demonstrate that generalization is guaranteed across rotational transformations.\nWe want to focus our experiments on the neural networks' effects on MARL performance. To isolate the impact of the network architecture, we avoid using specialized tips and tricks sometimes employed in MARL [25]. This allows us to demonstrate that our proposed networks can improve performance without relying on these additional techniques. Thus we use a common standardized open source MARL training library RLlib [26]. We use the default multi-agent PPO algorithm, which does not use a centralized critic. We followed the basic hyperparameter tuning guidelines set forth in [25]. That is, we use a large training batch and mini-batch size, low numbers of SGD iterations, and a small clip rate. Further hyperparameter details are found in appendix B.\nWe compare our results with common neural networks used for MARL benchmarks: multi-layer perceptrons (MLP), and GNNs. We also compare with the approach from [20] which we refer to as E3AC in our plots. The majority of experiments on SMACv2 use MLPs as the base network. We to GNNs to show the improvement is not primarily due to the graph structure. For the main paper results, we assume full observability since we have not explicitly tackled the partial observability problem yet. In future work, we will seek to remedy this. However, to be thorough we performed experiments with partial observability, resulting in surprising success using E2GN2 (see appendix C)."}, {"title": "5.1 Training Environments", "content": "From the MPE environment ([13]), we use two environments to benchmark our performance: co-operative navigation also known as spread, and predator-prey, also known as tag. The MPE tag environment consists of three RL-controlled agents seeking to catch a third evader agent. For easier comparison, we use a simple heuristic algorithm to control the evader agent. The evader simply moves in the opposite direction of the pursuers. The other MPE environment is the cooperative navigation or simple spread. In this environment, each agent seeks to minimize the distance between all obstacles. To better test equivariance, for the MPE environments we use continuous actions instead of the default discrete actions.\nSMACv2 ([14]) is a modification of the original SMAC ([27]). Since SMAC had deterministic initialization and static scenarios it turns out policies could memorize the solution. SMACv2 has three basic scenarios defined by the unit types: terran, protoss, and zerg. In each scenario, there is a pre-specified number of agents on each team (we use 5 agents for each team). In each scenario, the agents are randomly initialized in a specific formation. For this work, we use the initial position"}, {"title": "5.2 Training", "content": "We present the results from our training in this section. The results for MPE in Figure 5 are averaged across 5 seeds. As discussed previously, EGNN has poor early performance due to the early exploration bias. Despite this poor exploration, EGNN outperforms GNNs and MLPs, demonstrating the power of equivariance in MARL. Similarly, we note that in MPE tag, E2GN2 results in a strong final agent, while EGNN suffers in the initial training phases. However, in this case, we see that E2GN2 is able to greatly outperform EGNN's final reward, partly due to superior exploration.\nNext, we review the results from SMACv2 in Figure 6; these results are averaged across five seeds. The equivariant networks have a clear improvement in sample efficiency over the MLP and GNN. On the terrain environment, EGNN once again learns slower in the initial phases, but due to equivariance, it outperforms MLP/GNN. For Protoss, we note that EGNN performs well but has a high variance for its performance. The training speed and performance for the equivariant networks is especially impressive since this environment is much more complex than the MPE."}, {"title": "5.3 Generalization", "content": "Thus far we have demonstrated that equivariance does indeed lead to improved sample efficiency. The next question to answer is whether equivariance leads to improved generalization. We test"}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we have demonstrated that E2GN2 merits strong consideration for many MARL applications. We addressed several important problems including the early exploration bias and how to apply it to complex action spaces. The sample-efficiency has dramatically improved, and there are now guarantees of generalization built into the network. There is still further work to do, such as handling the partial or incomplete symmetries. In future work, we would also like to explore how to use E2GN2 on complex dynamics such as humanoid robotics. We believe that building on this work can be helpful to deploying MARL agents with a greater degree of trust (due to the guarantees). This"}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 EGNN bias proof", "content": "Here we prove our assertion that the EGNN layers have biased outputs, and thus poor exploration.\nWe start by showing that $\\mathbb{E} [\\phi_u(m_{ij})] \\approx 0$. We do this by computing the expected value of a single neural network layer.\nAn individual neural network layer is defined as $y = Wx + b$ where b is a N-by-1 vector, W is a N-by-M matrix and \u00e6 is a M-by-1 vector. Note that similar to [28] we initialize the elements in W to be mutually independent and share the same distribution. \u00e6 is initialized similarly. and \u00e6 and W are independent. Thus for each element (subscript l) of the equation, we have:\n$\\mathbb{E} [y_l] = \\mathbb{E} [\\sum_i \\omega_{li}x_i] + \\mathbb{E} [b_l] = n_l \\mathbb{E} [\\omega_{li}x_i] + \\mathbb{E} [b_l] = n_l \\mathbb{E} [\\omega_{l}] \\mathbb{E} [x_i] + \\mathbb{E} [b_l]$\nWe can expect $\\omega_{l}$ and $b_{l}$ to be zero mean, since many commonly used initializers use either uniform, or normal distribution, with zero mean [28]. This results in:\n$= n_l * 0 * \\mathbb{E} [x_i] + 0 = 0$\nThus, we can assume that for an individual layer the output is 0, since the final layer of any mlp is 0, we can see that $\\mathbb{E} [\\phi_u(m_{ij})] \\approx 0$\nNext we show that $\\mathbb{E} [u_i^l] \\approx \\mathbb{E} [u_i^{l-1}]$. Recall that $u_i^l$ is the $l$th layer of $i$th node of the equivariant component of the network. We take the expectation over a sampling of the inputs $u_i$ and $h_i$, treating each as a random variable. Note that $u_i$ and $u_j$ will have identical distributions, since each node will be sampled in the same manner. We start our proof by taking the expected value of the output:\n$\\begin{aligned} \\mathbb{E} [u_i^{l+1}] &= \\mathbb{E} [u_i^l + C\\sum_{j\\neq i} (u_i^l - u_j^l) \\phi_u (m_{ij})] = \\mathbb{E} [u_i^l] + C\\sum_{j\\neq i} \\mathbb{E} [(u_i^l - u_j^l) \\phi_u (m_{ij})] \\\\ &= \\mathbb{E} [u_i^l] + C\\sum_{j\\neq i} \\mathbb{E} [u_i^l \\phi_u (m_{ij})] - \\mathbb{E} [u_j^l \\phi_u (m_{ij})] \\\\ &\\approx \\mathbb{E} [u_i^l] + C\\sum_{j\\neq i} \\mathbb{E} [u_i^l] \\mathbb{E} [\\phi_u (m_{ij})] - \\mathbb{E} [u_j^l] \\mathbb{E} [\\phi_u (m_{ij})] \\\\ &\\approx \\mathbb{E} [u_i^l] + C\\sum_{j\\neq i} \\mathbb{E} [u_i^l] * 0 - \\mathbb{E} [u_j^l] * 0 = \\mathbb{E} [u_i^l] \\end{aligned}$\nThus,\n$\\mathbb{E} [u_i^{l+1}] \\approx \\mathbb{E} [u_i^l] \\approx \\mathbb{E} [u_i^l]$\nWhere we used the assumption that $\\mathbb{E} [u_j \\phi_u(m_{ij})] \\approx \\mathbb{E} [u_i^l] \\mathbb{E} [\\phi_u (m_{ij})]$\nThe corollary: $\\pi_k(a_k|s) \\sim N(s_k^e, \\sigma)$ is a simple result from the fact that the equivariant action for an agent is the output of the coordinate embedding of the EGNN: $u_i^l$. Here we assume the standard deviation is a separately trained parameter and not a function of the EGNN. If this is the case, then the policy mean $\\mathbb{E} [\\pi(a_k|s)] = \\mathbb{E} [u_i^l] \\approx \\mathbb{E} [u_i^l] = \\mathbb{E} [s_k^e]$ (by definition)"}, {"title": "A.2 Proof E2GN2 is Unbiased", "content": "Here we show that E2GN2 leads to unbiased early exploration. We begin with the equation for the E2GN2 layer:\n$u_i^{l+1} = \\phi_{u_2}(m_{ij}) + C\\sum_{j\\neq i} (u_i^l - u_j^l) \\phi_u (m_{ij}))$\nNext we take the expected value of the output:\n$\\begin{aligned} \\mathbb{E} [u_i^{l+1}] &= \\mathbb{E} [\\phi_{u_2}(m_{ij})] + C\\sum_{j\\neq i} \\mathbb{E} [u_i^l \\phi_u (m_{ij})] - \\mathbb{E} [u_j^l \\phi_u (m_{ij})] \\\\ &\\approx \\mathbb{E} [\\phi_{u_2}(m_{ij})] + C\\sum_{j\\neq i} \\mathbb{E} [u_i^l] \\mathbb{E} [\\phi_u (m_{ij})] - \\mathbb{E} [u_j^l] \\mathbb{E} [\\phi_u (m_{ij})] \\\\ &\\approx \\mathbb{E} [u_i^{l+1}] * 0 + C\\sum_{j\\neq i} \\mathbb{E} [u_i^l] * 0 - \\mathbb{E} [u_j^l] * 0 = 0 \\end{aligned}$\nThus we see that for each layer $\\mathbb{E} [u_i^{l+1}] \\approx 0$ and therefore $\\mathbb{E} [u_i^l] \\approx 0$. Similar to before, we used the assumption that Where we used the assumption that $\\mathbb{E} [u_i \\phi_u(m_{ij})] \\approx \\mathbb{E} [u_i^l] \\mathbb{E} [\\phi_u (m_{ij})]$ and $\\mathbb{E} [u_i \\phi_{u_2}(m_{ij})] \\approx \\mathbb{E} [u_j^l] \\mathbb{E} [\\phi_{u_2}(m_{ij})]$\nSimilar to the first corollary the result: $\\pi_k(a_k|s) \\sim N(0, \\sigma)$ follows mostly from the definition and the above proof. The corollary $\\pi_k(a_k|s) \\sim N(0, 0)$ is a simple result from the fact that the equivariant action for an agent is the output of the coordinate embedding of the EGNN: $u_i^l$. Here we assume the standard deviation is a separately trained parameter and not a function of the EGNN. If this is the case, then the policy mean $\\mathbb{E} [\\pi(a_k|s)] = \\mathbb{E} [u_i^l] \\approx 0$ (by definition)"}, {"title": "A.3 Equivariance of E2GN2", "content": "We follow the structure of [2] for how to show equivariance. Note that [2] showed that $\\phi_u (m_{ij})$ will be invariant to E(n) transformations. This should still hold in our case as we make no modifications to $m_{ij}$, similarly $\\phi_{u_2} (m_{ij})$ will be invariant to translations. To show equivariance to rotations and reflections we show that applying a transformation T to the input, results in a tranformation to the output. That is: f (Tu) = Tu where f(.) is the update equation of E2GN2:\n$\\begin{aligned} T\\phi_{u_2}(m_{ij}) + C\\sum_{j\\neq i} (Tu_i^l - Tu_j^l) \\phi_u (m_{ij}) \\\\ &= T\\phi_{u_2}(m_{ij}) + TC\\sum_{j\\neq i} (u_i^l - u_j^l) \\phi_u (m_{ij}) \\\\ &= T\\phi_{u_2}(m_{ij}) + TC\\sum_{j\\neq i} (u_i^l - u_j^l) \\phi_u (m_{ij}) \\\\ &= T(\\phi_{u_2}(m_{ij}) + C\\sum_{j\\neq i} (u_i^l - u_j^l) \\phi_u (m_{ij})) = Tu_i^{l+1} \\end{aligned}$"}]}