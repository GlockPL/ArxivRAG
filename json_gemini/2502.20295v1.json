{"title": "Judge a Book by Its Cover:\nInvestigating Multi-Modal LLMs\nfor Multi-Page Handwritten Document Transcription", "authors": ["Benjamin Gutteridge", "Matthew Jackson", "Toni Kukurin", "Xiaowen Dong"], "abstract": "Handwritten text recognition (HTR) remains a challeng-ing task, particularly for multi-page documents where pages\nshare common formatting and contextual features. While\nmodern optical character recognition (OCR) engines are pro-ficient with printed text, their performance on handwriting is\nlimited, often requiring costly labeled data for fine-tuning.\nIn this paper, we explore the use of multi-modal large lan-guage models (MLLMs) for transcribing multi-page hand-written documents in a zero-shot setting. We investigate vari-ous configurations of commercial OCR engines and MLLMs,\nutilizing the latter both as end-to-end transcribers and as post-processors, with and without image components. We propose\na novel method, +FIRST PAGE, which enhances MLLM tran-scription by providing the OCR output of the entire document\nalong with just the first page image. This approach leverages\nshared document features without incurring the high cost of\nprocessing all images. Experiments on a multi-page version\nof the IAM Handwriting Database demonstrate that +FIRST\nPAGE improves transcription accuracy, balances cost with\nperformance, and even enhances results on out-of-sample text\nby extrapolating formatting and OCR error patterns from a\nsingle page.", "sections": [{"title": "Introduction", "content": "A significant proportion of all human-written text exists\nonly in the form of physical handwritten documents. Ac-curate and cost-effective digitization of such documents\nwould benefit numerous fields and industries by improv-ing information accessibility, sharing, and processing. Digi-tized handwritten text could also provide a largely-untapped\nsource of training data for language models.\nModern optical character recognition (OCR) software is\nadept at transcribing printed text, even from low-quality\nscans, but handwriting remains challenging (Liu et al.\n2023). State-of-the-art models on handwriting text recog-nition (HTR) tasks (Li et al. 2023; Fujitake 2024) typi-cally combine pre-trained vision Transformers (Dosovitskiy\n2020; Liu et al. 2021; Huang et al. 2022; Xu et al. 2020;\nKim et al. 2021) and language models (Devlin 2018; Liu\n2019), and rely on fine-tuning with labeled data to perform\nwell. Unfortunately, labeling this training data, i.e. manually\ntranscribing documents, is expensive, time-consuming, and\noften unrealistic in the real world. We are interested in mod-els that can be deployed zero-shot without training/fine-tuning or examples.\nA second major challenge arises in processing multi-page\ndocuments, which constitute most real-world handwritten\nrecords. These documents share contextual and structural\nfeatures across pages, such as consistent handwriting, lay-out, image properties, and interrelated textual content. Yet,\nOCR systems typically operate at the page level, ignoring\nthese cross-page dependencies, while most HTR models are\nbenchmarked at the line or word level. This limited focus on\nmulti-page document processing leaves significant room for\nimprovement in leveraging shared context and relationships\nacross pages.\nLarge language models (LLMs; Floridi and Chiriatti\n2020; Achiam et al. 2023; Zhao et al. 2023) and multi-modal\nLLMs with a vision component (MLLMs; Yin et al. 2024)\nshow promise for addressing these challenges. MLLMs in\nparticular, with their ability to process images and text si-multaneously, appear to be strong end-to-end handwriting\ntranscribers in themselves, going beyond simply serving as\npost-processors for traditional OCR engines. Furthermore,\nsuch models, equipped with ever-increasing context window\nsizes such as the 128k-token limit of OpenAI's GPT-40\ncan theoretically process documents spanning dozens of\npages in image and text formats simultaneously, and have\ndemonstrated capability in long-context settings (Chen et al.\n2023; Liu et al. 2024; Kim et al. 2024; Karpinska et al.\n2024). Despite this potential, existing research has primar-ily explored LLMs for OCR post-processing, with limited\nfocus on using MLLMs for handwritten or multi-page doc-ument transcription."}, {"title": "Contributions", "content": "In this paper:\n\u2022 We investigate the transcription of multi-page, handwrit-ten documents using various configurations of commer-cial OCR engines and MLLMs, the latter being used both\nas end-to-end transcribers and as post-processors, with\nand without the vision component.\n\u2022 We propose a method by which MLLMs can leverage\ncontextual information from documents beyond text con-tent only, and without incurring the prohibitively high"}, {"title": "Related Work", "content": "Handwriting OCR. Most OCR engines, including those\nthat can be run locally like Tesseract, are designed for use\nwith printed text and are nearly useless for handwriting. Sev-eral commercial OCR engines, such as Google Cloud Vi-sion, Azure AI Vision and Amazon Textract, are designed\nfor use on handwritten text at the page-level scale.\nState-of-the-art HTR and OCR models (Fujitake 2024;\nLi et al. 2023; Kim et al. 2021; Huang et al. 2022) are\ntypically based on pre-trained vision Transformers (ViT;\nVaswani 2017; Dosovitskiy 2020) and may include recur-rent components like LSTMs or CNNs (Breuel et al. 2013;\nAzawi, Afzal, and Breuel 2013; Bora et al. 2020; Yang,\nRen, and Kong 2019); the commercial handwriting-capable\nOCR engines mentioned above are likely similar in archi-tecture to the best of these models, leveraging massive, pre-trained ViTs and language models. In general, such models\nare only somewhat effective on HTR tasks zero-shot, and\nSOTA is reached by fine-tuning on labeled data for the spe-cific task. This is fine for benchmarks, but for real-world\ntasks obtaining labeled training data is often prohibitively\nexpensive. Furthermore, most benchmarks are concerned\nonly with recognition at the character or line level. This can,\nof course, be aggregated to return document-level transcrip-tions, but this neglects the task of text detection, and does not\nconsider incidentals that occur in real documents head-ings, figures, scribbles, margin notes, imperfections in image\nquality, distractors, etc. This paper is concerned with tran-scription over multi-page documents in a holistic manner.\nLLMS for OCR post-processing. Several works have in-vestigated improving OCR transcription score with post-processing by a language model (Lund, Walker, and Ringger\n2011; Schaefer and Neudecker 2020; Veninga 2024; Rigaud\net al. 2019).\nLLM-aided OCR is a public tool that uses OCR output\nwith an LLM post-processor to improve OCR transcription\naccuracy, but the authors do not provide any experimental\nresults demonstrating improvement besides hand-picked ex-amples. Similarly, BetterOCR is a tool that combines results\nfrom multiple OCR engines and passes them into an LLM,\nbut again, only hand-picked examples are provided as exper-imental results. Furthermore, both tools are only designed\nfor printed text, both operate at the page level (or at the\n'chunk' level within pages, in the case of BetterOCR), and\nneither use MLLMs, only using LLMs for post-processing.\nBenchmarks. Most OCR benchmarks are for machine-printed text, and only for single pages/images (Liu et al.\n2023), such as receipts (Park et al. 2019; Huang et al. 2019)).\nKleister (Grali\u0144ski et al. 2020; Stanis\u0142awek et al. 2021) is a\npair of multi-page, long-context key entity extraction bench-mark tasks, but consists of only machine-printed text.\nThere are a number of HTR benchmarks, including his-torical documents, documents not written in English or with\nLatin characters (S\u00e1nchez et al. 2019; Zhang et al. 2019;\nCauser et al. 2018; Dolfing et al. 2020; Serrano, Castro, and\nJuan 2010; Wigington et al. 2018; Carbonell et al. 2019; Yu\net al. 2021), and transcription of numerical digits or mathe-matical expressions (Liu et al. 2023; Yuan et al. 2022; Diem\net al. 2014). None are explicitly concerned with multi-page\ndocuments, and most are at the line- or word-level. In this\npaper we will use the most well-known handwriting bench-mark, the IAM Handwriting Database (Marti and Bunke\n2002), to construct our own multi-page documents."}, {"title": "Proposed Method: Extrapolating Formatting\nand Learned OCR Error from a Single Page", "content": "We propose the following MLLM prompting strategy for\nOCR post-processing of multi-page documents: provide the\nMLLM with the OCR output for the entire document as\nwell as just the first page image.\nGiven that separate pages from the same document will\nhave very similar formatting - handwriting, structure, im-age lighting/angle, etc. it is likely that the errors made\nby an OCR engine will be fairly consistent over all pages.\nWe hypothesize that an MLLM should be able to learn, in-context, the mapping from the provided image to the first\npart of the noisy OCR input, and use this to improve on the\npost-processing of the entire text. An example of how this\nworks in practice is illustrated by Figure 2, as well as Fig-ures 11-14 in the Appendix.\nThe idea bears some similarity to few-shot prompting, in\nwhich one or more examples of input and desired output\nare provided within the prompt. The two input channels, the\npage image and the OCR text, can be thought of as the ex-ample input and target. In this case, however, rather than\nlearning in-context to replicate the OCR engine, the MLLM\nshould (i) exercise its own judgement to identify OCR er-rors, (ii) identify how the OCR engine's choices should be\ncorrected, and (iii) extrapolate this learned image \u2192 OCR\ntext mapping to the remainder of the OCR output (i.e. 'un-seen' text).\nWe believe MLLMs are an appropriate tool for these\ntasks, as they already demonstrate ability to recognize and\ncorrect OCR errors using local context and probable text\npatterns from their pre-training corpora, and they demon-strate the ability to learn new tasks from in-context examples\n(Dong et al. 2022).\nThe method described above, which we refer to as +FIRST\nPAGE, is motivated by the assumptions that (i) OCR en-gines are cheap, but error-prone, whereas (ii) MLLMs (ei-ther as OCR post-processors or as end-to-end transcribers)\ncan often reduce such errors, but are expensive; also that (iii)\nseparate pages across a document have shared features and\ncontextual information that page-by-page LLM calls throw\naway.\u00b9 +FIRST PAGE is an attempt to provide a simple, in-termediate method between OCR and full-document MLLM\ntranscription, and leverage this lost information.\nIn addition to improving performance while balancing\ncost, we believe that +FIRST PAGE also provides some in-sight into LLM reasoning ability; namely that MLLMs are\nable to extrapolate formatting and OCR error information\nfrom images and to improve performances on tasks using\nout-of-sample, 'unseen' text."}, {"title": "Experimental Setup", "content": "We experiment with various configurations of OCR engines,\nMLLMs and prompting strategies on a multi-page version of\nthe IAM Handwriting Database benchmark dataset (Marti\nand Bunke 2002; see Figures 9, 10 in the Appendix for ex-ample documents). The task is to produce a whole-document\ntranscription from either document page images, OCR out-put, or some combination of the two.\nWe demonstrate (ii) in the Experiments section; for an example\nof (iii) see Figure 10 in the Appendix; separate pages are extremely\nsimilar in all but text content, yet this information is thrown away\nbetween MLLM calls.\nOCR and MLLMs. We use three commercial OCR en-gines: Azure AI Vision, Google Cloud Vision and Amazon\nTextract (Tesseract was also tried, but failed to output any-thing meaningful on handwritten text).\nFor commercial LLMs we use GPT-40 and GPT-40-MINI\nwith OpenAI's API, with default parameters and a tempera-ture of 0. We use '\\n[NEW_PAGE]\\n`as a page break\nmarker for multi-page documents. Per-page OCR outputs\nare manually joined with this marker, and MLLMs are asked\nto maintain these markers or (in the vision-only case) to in-sert them into the transcription if appropriate.\nCost. For our cost estimates for commercial tools we use\nthe default values listed on their respective web pages. Most\nhave pricing tiers based on scale, or a limited free allowance;\nfor simplicity we take the lowest (non-batch) cost per run for\neach engine, and for the OpenAI API. See the Appendix for\nindividual prices.\nMulti-page datasets. For IAM all of the documents are\nsingle pages, with the machine-typed text at the top of the\npage and the same text, handwritten, below it. We program-matically crop the images to contain only the handwritten\npart (using provided metadata) and then combine them at\nrandom by writer ID to produce 2- and 3- image multi-page\ndocuments. For IAM we use a subset of 268 multi-page doc-uments, 210 with 2 pages and the rest with 3, and henceforth\nrefer to this multi-page dataset as IAM.\nWe also evaluate methods on documents of varying length\nin terms of page count, for which we construct a dataset of\n10 documents per page count from 2-10 pages.\nMethods. Below is an overview of the different methods\nused for experiments in this section, in approximate com-plexity(/cost) order. Figures 1 and 3\u20136 illustrate some of\nthese methods to make the pipeline clearer.\nOCR ONLY: just OCR engine output; can then be post-processed by an LLM or used as-is.\nOCR ONLY PBP: page-by-page, OCR \u2192 LLM one\npage/API call at a time, joined by page breaks afterwards.\nPBP is an attribute that can also be applied to any of the\nother 'all-at-once' methods, to mean independent pro-cessing of pages followed by concatenation.\n+FIRST PAGE: OCR plus the first page image \u2192 MLLM.\nThe '+' prefix indicates a method that is in addition to\nOCR output (the default).\n+CHOSEN PAGE : OCR plus a single page image chosen\nby a separate prompt to GPT-40-MINI \u2192 MLLM. The\nprompt includes the OCR output and asks for the best\npage ID for downstream MLLM post-processing. We\nwould expect this approach to perform at least as well\nas +FIRST PAGE.\nVISION*: all page images \u2192 MLLM, no OCR. '*' denotes\na non-OCR method.\n+ALL PAGES: all page images and OCR output \u2192 MLLM\nALL OCR: the concatenated outputs of all three OCR en-gines \u2192 LLM"}, {"title": "Experiments", "content": "Error catching. As a post-processing step for MLLM\nmethods, we perform simple common-sense checks for\ncatastrophic MLLM error, such as repeating sections of text\nad infinitum, or refusing to return an output due to an in-advertent triggering of OpenAI's guardrails (\"Sorry, but I\ncan't answer that...\"). To avoid such outliers unfairly drag-ging down the overall score of a method (that uses OCR\noutput), we compare the CER between OCR output and fi-nal MLLM output, and, if the CER is drastically different,\nuse the OCR output by default.\nEvaluation. Evaluation is at the document level using\nCharacter Error Rate (CER; Morris, Maier, and Green\n2004), the most widely-used metric for OCR transcription.\nWe also experimented with Average Normalized Leven-shtein Similarity (Peer et al. 2024), which is much more\ntime-consuming to compute but, unlike CER, is sensitive to\ncharacter order. We opted for CER as we found the two met-rics approximately equally informative."}, {"title": "Discussion", "content": "We begin by comparing methods introduced in the previous\nsection, before focusing specifically on PBP methods and\n+FIRST PAGE, and finally look at two trade-offs: the benefits\nof sharing information across pages versus the challenges\nof long contexts, and, more generally, the cost-performance\ntrade-offs of our various methods.\nMLLMs are powerful transcribers. In general, the more\nsophisticated and expensive the method, the more benefi-cial it is, with +ALL PAGES PBP being the best overall. Us-ing both OCR output and vision components together yields\nbetter performance than either text-only post-processing or\nvision alone. Even so, VISION*, and especially VISION*\nPBP are surprisingly powerful end-to-end transcribers with-out any OCR input, much more so than any of the three com-mercial engines explicitly trained for that task.\nCost-effective OCR post-processing. As the vision com-ponent of MLLMs is expensive, and OCR engines and text-only LLMs are (relatively) cheap, it is important to look\nat which strategies improve on OCR performance while re-maining cost-effective.\nAll three engines are substantially helped by some sort\nof post-processing. +FIRST PAGE consistently improves per-formance, which is expected: we have seen that combining\nOCR output and images improves performance, so at mini-mum we would expect overall performance on a multi-page\ndocument to be brought up by improved performance on just\nthe first page. We go beyond this and demonstrate below (see\nTable 2) that this performance benefit goes beyond just the\ntext corresponding to the first page, and that there is cross-page out-of-sample extrapolation.\nImpact of choice of MLLM. Surprisingly, GPT-40-MINI\noften performed better than GPT-40. We are unsure why\nthis is, but we speculate that the supposedly more capable\nmodel may have a tendency to do too much; for OCR post-processing, often what is left alone is more important than\nwhat is changed.\nIt is also worth noting that the vision components of GPT-40 and GPT-40-MINI may not be as different as the text\ncomponents, given that the cost difference between models\nonly applies to text input: GPT-4O-MINI is ~30x cheaper for\ntext, but costs the same for image tokens (the API artificially\ninflates the number so the cost is equal). This would suggest\nthat the vision backend is the same or very similar for both\nmodels, and may partly explain the lack of divergence in\nperformance.\nPBP and the impact of page count. Another surprising\nfinding was the severity of the impact of input length on per-formance for MLLMs.\nAll PBP methods perform better than their 'all-at-once'\ncounterparts, for only a small cost increase. Though there\ndoes seem to be evidence that contextual information can\nbe leveraged for benefit across pages (discussed in the fol-lowing subsection), much of this benefit is seemingly offset\nby PBP, i.e. simply processing pages independently and then\nconcatenating them afterwards, as a commercial OCR en-gine would do.\nTo verify this, we perform an experiment testing the per-formance of several methods and their PBP counterparts on\nIAM multi-page documents of varying lengths (constructed\nin the same way as IAM but with fixed page counts). We use\n10 documents per document length, and plot relative CER\nimprovement against page count in Figure 7.\nWe find that PBP methods retain similar performance over\nall page counts, while the all-at-once methods degrade as the\nnumber of pages increases. This is surprising, given that the\ncontext windows for GPT-40 and GPT-40-MINI are 128k to-kens, and the number of tokens for a document page is only\nin the order of hundreds. As our prompts and inputs, while\nmuch shorter than the maximum context window length, are\nstill longer than PBP, we speculate that culprit may still be\nthe 'lost in the middle' problem (Liu et al. 2024), where\nTransformers' attentive power is spread thinly over long\ncontexts, causing performance to weaken on tasks relating\nto text far from the start or end of the prompt.\nThis may be compounded by the additional complexity of\nmaintaining/inserting accurate page breaks anecdotally,"}, {"title": "Scaling challenges", "content": "We have experiments for documents\nof 2-3 pages, but we would expect +FIRST PAGE to be espe-cially cost-effective for longer documents. Hypothetically,\nthe insights from a single page should be equally applicable\nacross all pages with similar formatting, so the performance\nimprovement for an n page document should cost the same\nas improving performance for a one page document: the cost\nof processing just a single page.\nUnfortunately, in practice, for long documents this ben-efit seems to trade off with MLLM preference for a PBP\napproach (see Figure 7), where the benefit of having ac-cess to the first page image cannot be shared to subsequent\npages. Future work will look at how to mitigate this perfor-mance breakdown, and therefore how to increase the cost-effectiveness of +FIRST PAGE."}, {"title": "Scaling with prompt caching", "content": "Prompt caching (Shi et al.\n2024) could provide an alternate means of benefiting from\n+FIRST PAGE in a PBP setting.\nPrompt caching is when an intermediate model state is\nsaved after having already been 'primed' with the first part\nof a prompt, and is already used in some commercial LLMs.\nA PBP version of +FIRST PAGE would, without prompt\ncaching, be just as expensive (and not nearly as informative)\nas +ALL PAGES PBP. With prompt caching, the first page\nwould not need to be re-processed, and the performance ben-efits of both PBP and +FIRST PAGE could be achieved with\ncost scaling << O(n)."}, {"title": "Poor performance of +CHOSEN PAGE", "content": "We hypothesized\nthat +CHOSEN PAGE should perform at least as well as\n+FIRST PAGE; in practice though, this was not the case, es-pecially for GPT-40-MINI and for weaker OCR outputs. The\ndecline in performance is likely due to the increased com-plexity of the prompt, and the MLLM being more likely to\nbe confused by the concept of an nth page than by the first.\nPrevious work has successfully used LLM decision-making\nfor downstream tasks (Wu et al. 2024; Schick et al. 2024),\nsuggesting that further investigation of this method is worth-while."}, {"title": "The performance-cost trade-off", "content": "Figure 8 plots perfor-mance against cost for IAM for all methods described in this\npaper and draws a Pareto frontier of the optimal methods,\ntrading off performance and cost."}, {"title": "Conclusion and Further Work", "content": "In this work, we investigated the transcription of multi-page handwritten documents using various configurations\nof commercial OCR engines and MLLMs. We examined\nthe effectiveness of different prompting strategies and pro-posed the +FIRST PAGE method, which provides the MLLM\nwith the OCR output for the entire document along with just\nthe first page image. Our experiments on a multi-page syn-thesis of the IAM Handwriting Database demonstrated that\n+FIRST PAGE improves transcription accuracy while balanc-ing cost and performance. Notably, it is effective even on\nout-of-sample text, as it leverages formatting and OCR error\npatterns from a single page to other, 'unseen' pages.\nFuture work will look at additional handwritten docu-ment datasets, explore mitigating the performance degrada-tion observed with increasing document length, and further\ninvestigate the potential of prompt caching and other tech-niques to leverage information across pages in long docu-ment transcription."}]}