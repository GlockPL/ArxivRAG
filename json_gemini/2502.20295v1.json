{"title": "Judge a Book by Its Cover:\nInvestigating Multi-Modal LLMs\nfor Multi-Page Handwritten Document Transcription", "authors": ["Benjamin Gutteridge", "Matthew Jackson", "Toni Kukurin", "Xiaowen Dong"], "abstract": "Handwritten text recognition (HTR) remains a challenging task, particularly for multi-page documents where pages share common formatting and contextual features. While modern optical character recognition (OCR) engines are proficient with printed text, their performance on handwriting is limited, often requiring costly labeled data for fine-tuning. In this paper, we explore the use of multi-modal large language models (MLLMs) for transcribing multi-page handwritten documents in a zero-shot setting. We investigate various configurations of commercial OCR engines and MLLMs, utilizing the latter both as end-to-end transcribers and as post-processors, with and without image components. We propose a novel method, +FIRST PAGE, which enhances MLLM transcription by providing the OCR output of the entire document along with just the first page image. This approach leverages shared document features without incurring the high cost of processing all images. Experiments on a multi-page version of the IAM Handwriting Database demonstrate that +FIRST PAGE improves transcription accuracy, balances cost with performance, and even enhances results on out-of-sample text by extrapolating formatting and OCR error patterns from a single page.", "sections": [{"title": "Introduction", "content": "A significant proportion of all human-written text exists only in the form of physical handwritten documents. Accurate and cost-effective digitization of such documents would benefit numerous fields and industries by improving information accessibility, sharing, and processing. Digitized handwritten text could also provide a largely-untapped source of training data for language models.\nModern optical character recognition (OCR) software is adept at transcribing printed text, even from low-quality scans, but handwriting remains challenging (Liu et al. 2023). State-of-the-art models on handwriting text recognition (HTR) tasks (Li et al. 2023; Fujitake 2024) typically combine pre-trained vision Transformers (Dosovitskiy 2020; Liu et al. 2021; Huang et al. 2022; Xu et al. 2020; Kim et al. 2021) and language models (Devlin 2018; Liu 2019), and rely on fine-tuning with labeled data to perform well. Unfortunately, labeling this training data, i.e. manually transcribing documents, is expensive, time-consuming, and often unrealistic in the real world. We are interested in models that can be deployed zero-shot without training/fine-tuning or examples.\nA second major challenge arises in processing multi-page documents, which constitute most real-world handwritten records. These documents share contextual and structural features across pages, such as consistent handwriting, layout, image properties, and interrelated textual content. Yet, OCR systems typically operate at the page level, ignoring these cross-page dependencies, while most HTR models are benchmarked at the line or word level. This limited focus on multi-page document processing leaves significant room for improvement in leveraging shared context and relationships across pages.\nLarge language models (LLMs; Floridi and Chiriatti 2020; Achiam et al. 2023; Zhao et al. 2023) and multi-modal LLMs with a vision component (MLLMs; Yin et al. 2024) show promise for addressing these challenges. MLLMs in particular, with their ability to process images and text simultaneously, appear to be strong end-to-end handwriting transcribers in themselves, going beyond simply serving as post-processors for traditional OCR engines. Furthermore, such models, equipped with ever-increasing context window sizes such as the 128k-token limit of OpenAI's GPT-40 can theoretically process documents spanning dozens of pages in image and text formats simultaneously, and have demonstrated capability in long-context settings (Chen et al. 2023; Liu et al. 2024; Kim et al. 2024; Karpinska et al. 2024). Despite this potential, existing research has primarily explored LLMs for OCR post-processing, with limited focus on using MLLMs for handwritten or multi-page document transcription."}, {"title": "Contributions", "content": "In this paper:\n\u2022 We investigate the transcription of multi-page, handwritten documents using various configurations of commercial OCR engines and MLLMs, the latter being used both as end-to-end transcribers and as post-processors, with and without the vision component.\n\u2022 We propose a method by which MLLMs can leverage contextual information from documents beyond text content only, and without incurring the prohibitively high cost of full vision LLM processing, by providing just a single page from a document.\n\u2022 We show that this method improves transcription quality, demonstrate the capability of MLLMs to beneficially extrapolate document features across pages for the transcription task, and show that our method sits on the cost-performance Pareto frontier as an intermediate approach to expensive MLLM and cheaper OCR methods."}, {"title": "Related Work", "content": "Handwriting OCR. Most OCR engines, including those that can be run locally like Tesseract, are designed for use with printed text and are nearly useless for handwriting. Several commercial OCR engines, such as Google Cloud Vision, Azure AI Vision and Amazon Textract, are designed for use on handwritten text at the page-level scale.\nState-of-the-art HTR and OCR models (Fujitake 2024; Li et al. 2023; Kim et al. 2021; Huang et al. 2022) are typically based on pre-trained vision Transformers (ViT; Vaswani 2017; Dosovitskiy 2020) and may include recurrent components like LSTMs or CNNs (Breuel et al. 2013; Azawi, Afzal, and Breuel 2013; Bora et al. 2020; Yang, Ren, and Kong 2019); the commercial handwriting-capable OCR engines mentioned above are likely similar in architecture to the best of these models, leveraging massive, pre-trained ViTs and language models. In general, such models are only somewhat effective on HTR tasks zero-shot, and SOTA is reached by fine-tuning on labeled data for the specific task. This is fine for benchmarks, but for real-world tasks obtaining labeled training data is often prohibitively expensive. Furthermore, most benchmarks are concerned only with recognition at the character or line level. This can, of course, be aggregated to return document-level transcriptions, but this neglects the task of text detection, and does not consider incidentals that occur in real documents headings, figures, scribbles, margin notes, imperfections in image quality, distractors, etc. This paper is concerned with transcription over multi-page documents in a holistic manner.\nLLMS for OCR post-processing. Several works have investigated improving OCR transcription score with post-processing by a language model (Lund, Walker, and Ringger 2011; Schaefer and Neudecker 2020; Veninga 2024; Rigaud et al. 2019).\nLLM-aided OCR is a public tool that uses OCR output with an LLM post-processor to improve OCR transcription accuracy, but the authors do not provide any experimental results demonstrating improvement besides hand-picked examples. Similarly, BetterOCR is a tool that combines results from multiple OCR engines and passes them into an LLM, but again, only hand-picked examples are provided as experimental results. Furthermore, both tools are only designed for printed text, both operate at the page level (or at the 'chunk' level within pages, in the case of BetterOCR), and neither use MLLMs, only using LLMs for post-processing.\nBenchmarks. Most OCR benchmarks are for machine-printed text, and only for single pages/images (Liu et al. 2023), such as receipts (Park et al. 2019; Huang et al. 2019)). Kleister (Grali\u0144ski et al. 2020; Stanis\u0142awek et al. 2021) is a pair of multi-page, long-context key entity extraction benchmark tasks, but consists of only machine-printed text.\nThere are a number of HTR benchmarks, including historical documents, documents not written in English or with Latin characters (S\u00e1nchez et al. 2019; Zhang et al. 2019; Causer et al. 2018; Dolfing et al. 2020; Serrano, Castro, and Juan 2010; Wigington et al. 2018; Carbonell et al. 2019; Yu et al. 2021), and transcription of numerical digits or mathematical expressions (Liu et al. 2023; Yuan et al. 2022; Diem et al. 2014). None are explicitly concerned with multi-page documents, and most are at the line- or word-level. In this paper we will use the most well-known handwriting benchmark, the IAM Handwriting Database (Marti and Bunke 2002), to construct our own multi-page documents."}, {"title": "Proposed Method: Extrapolating Formatting\nand Learned OCR Error from a Single Page", "content": "We propose the following MLLM prompting strategy for OCR post-processing of multi-page documents: provide the MLLM with the OCR output for the entire document as\nwell as just the first page image.\nGiven that separate pages from the same document will have very similar formatting - handwriting, structure, image lighting/angle, etc. it is likely that the errors made by an OCR engine will be fairly consistent over all pages. We hypothesize that an MLLM should be able to learn, in-context, the mapping from the provided image to the first part of the noisy OCR input, and use this to improve on the post-processing of the entire text. An example of how this works in practice is illustrated by Figure 2, as well as Figures 11-14 in the Appendix.\nThe idea bears some similarity to few-shot prompting, in which one or more examples of input and desired output are provided within the prompt. The two input channels, the page image and the OCR text, can be thought of as the example input and target. In this case, however, rather than learning in-context to replicate the OCR engine, the MLLM should (i) exercise its own judgement to identify OCR errors, (ii) identify how the OCR engine's choices should be corrected, and (iii) extrapolate this learned image \u2192 OCR text mapping to the remainder of the OCR output (i.e. 'unseen' text).\nWe believe MLLMs are an appropriate tool for these tasks, as they already demonstrate ability to recognize and correct OCR errors using local context and probable text patterns from their pre-training corpora, and they demonstrate the ability to learn new tasks from in-context examples (Dong et al. 2022).\nThe method described above, which we refer to as +FIRST PAGE, is motivated by the assumptions that (i) OCR engines are cheap, but error-prone, whereas (ii) MLLMs (either as OCR post-processors or as end-to-end transcribers) can often reduce such errors, but are expensive; also that (iii) separate pages across a document have shared features and contextual information that page-by-page LLM calls throw away.\u00b9 +FIRST PAGE is an attempt to provide a simple, intermediate method between OCR and full-document MLLM transcription, and leverage this lost information.\nIn addition to improving performance while balancing cost, we believe that +FIRST PAGE also provides some insight into LLM reasoning ability; namely that MLLMs are able to extrapolate formatting and OCR error information from images and to improve performances on tasks using out-of-sample, 'unseen' text."}, {"title": "Experimental Setup", "content": "We experiment with various configurations of OCR engines, MLLMs and prompting strategies on a multi-page version of the IAM Handwriting Database benchmark dataset (Marti and Bunke 2002; see Figures 9, 10 in the Appendix for example documents). The task is to produce a whole-document transcription from either document page images, OCR output, or some combination of the two.\nOCR and MLLMs. We use three commercial OCR engines: Azure AI Vision, Google Cloud Vision and Amazon Textract (Tesseract was also tried, but failed to output anything meaningful on handwritten text).\nFor commercial LLMs we use GPT-40 and GPT-40-MINI with OpenAI's API, with default parameters and a temperature of 0. We use '\\n[NEW_PAGE]\\n`as a page break marker for multi-page documents. Per-page OCR outputs are manually joined with this marker, and MLLMs are asked to maintain these markers or (in the vision-only case) to insert them into the transcription if appropriate.\nCost. For our cost estimates for commercial tools we use the default values listed on their respective web pages. Most have pricing tiers based on scale, or a limited free allowance; for simplicity we take the lowest (non-batch) cost per run for each engine, and for the OpenAI API. See the Appendix for individual prices.\nMulti-page datasets. For IAM all of the documents are single pages, with the machine-typed text at the top of the page and the same text, handwritten, below it. We programmatically crop the images to contain only the handwritten part (using provided metadata) and then combine them at random by writer ID to produce 2- and 3- image multi-page documents. For IAM we use a subset of 268 multi-page documents, 210 with 2 pages and the rest with 3, and henceforth refer to this multi-page dataset as IAM.\nWe also evaluate methods on documents of varying length in terms of page count, for which we construct a dataset of 10 documents per page count from 2-10 pages.\nMethods. Below is an overview of the different methods used for experiments in this section, in approximate complexity(/cost) order. Figures 1 and 3\u20136 illustrate some of these methods to make the pipeline clearer.\nOCR ONLY: just OCR engine output; can then be post-processed by an LLM or used as-is.\nOCR ONLY PBP: page-by-page, OCR \u2192 LLM one page/API call at a time, joined by page breaks afterwards. PBP is an attribute that can also be applied to any of the other 'all-at-once' methods, to mean independent processing of pages followed by concatenation.\n+FIRST PAGE: OCR plus the first page image \u2192 MLLM. The '+' prefix indicates a method that is in addition to OCR output (the default).\n+CHOSEN PAGE : OCR plus a single page image chosen by a separate prompt to GPT-40-MINI \u2192 MLLM. The prompt includes the OCR output and asks for the best page ID for downstream MLLM post-processing. We would expect this approach to perform at least as well as +FIRST PAGE.\nVISION*: all page images \u2192 MLLM, no OCR. '*' denotes a non-OCR method.\n+ALL PAGES: all page images and OCR output \u2192 MLLM\nALL OCR: the concatenated outputs of all three OCR engines \u2192 LLM"}, {"title": "Experiments", "content": "Error catching. As a post-processing step for MLLM methods, we perform simple common-sense checks for catastrophic MLLM error, such as repeating sections of text ad infinitum, or refusing to return an output due to an inadvertent triggering of OpenAI's guardrails (\"Sorry, but I can't answer that...\"). To avoid such outliers unfairly dragging down the overall score of a method (that uses OCR output), we compare the CER between OCR output and final MLLM output, and, if the CER is drastically different, use the OCR output by default.\nEvaluation. Evaluation is at the document level using Character Error Rate (CER; Morris, Maier, and Green 2004), the most widely-used metric for OCR transcription. We also experimented with Average Normalized Levenshtein Similarity (Peer et al. 2024), which is much more time-consuming to compute but, unlike CER, is sensitive to character order. We opted for CER as we found the two metrics approximately equally informative."}, {"title": "Discussion", "content": "We begin by comparing methods introduced in the previous section, before focusing specifically on PBP methods and +FIRST PAGE, and finally look at two trade-offs: the benefits of sharing information across pages versus the challenges of long contexts, and, more generally, the cost-performance trade-offs of our various methods.\nMLLMs are powerful transcribers. In general, the more sophisticated and expensive the method, the more beneficial it is, with +ALL PAGES PBP being the best overall. Using both OCR output and vision components together yields better performance than either text-only post-processing or vision alone. Even so, VISION*, and especially VISION* PBP are surprisingly powerful end-to-end transcribers without any OCR input, much more so than any of the three commercial engines explicitly trained for that task.\nCost-effective OCR post-processing. As the vision component of MLLMs is expensive, and OCR engines and text-only LLMs are (relatively) cheap, it is important to look at which strategies improve on OCR performance while remaining cost-effective.\nAll three engines are substantially helped by some sort of post-processing. +FIRST PAGE consistently improves performance, which is expected: we have seen that combining OCR output and images improves performance, so at minimum we would expect overall performance on a multi-page document to be brought up by improved performance on just the first page. We go beyond this and demonstrate below (see Table 2) that this performance benefit goes beyond just the text corresponding to the first page, and that there is cross-page out-of-sample extrapolation.\nImpact of choice of MLLM. Surprisingly, GPT-40-MINI often performed better than GPT-40. We are unsure why this is, but we speculate that the supposedly more capable model may have a tendency to do too much; for OCR post-processing, often what is left alone is more important than what is changed.\nIt is also worth noting that the vision components of GPT-40 and GPT-40-MINI may not be as different as the text components, given that the cost difference between models only applies to text input: GPT-4O-MINI is ~30x cheaper for text, but costs the same for image tokens (the API artificially inflates the number so the cost is equal). This would suggest that the vision backend is the same or very similar for both models, and may partly explain the lack of divergence in performance.\nPBP and the impact of page count. Another surprising finding was the severity of the impact of input length on performance for MLLMs.\nAll PBP methods perform better than their 'all-at-once' counterparts, for only a small cost increase. Though there does seem to be evidence that contextual information can be leveraged for benefit across pages (discussed in the following subsection), much of this benefit is seemingly offset by PBP, i.e. simply processing pages independently and then concatenating them afterwards, as a commercial OCR engine would do.\nTo verify this, we perform an experiment testing the performance of several methods and their PBP counterparts on IAM multi-page documents of varying lengths (constructed in the same way as IAM but with fixed page counts). We use 10 documents per document length, and plot relative CER improvement against page count in Figure 7.\nWe find that PBP methods retain similar performance over all page counts, while the all-at-once methods degrade as the number of pages increases. This is surprising, given that the context windows for GPT-40 and GPT-40-MINI are 128k tokens, and the number of tokens for a document page is only in the order of hundreds. As our prompts and inputs, while much shorter than the maximum context window length, are still longer than PBP, we speculate that culprit may still be the 'lost in the middle' problem (Liu et al. 2024), where Transformers' attentive power is spread thinly over long contexts, causing performance to weaken on tasks relating to text far from the start or end of the prompt.\nThis may be compounded by the additional complexity of maintaining/inserting accurate page breaks anecdotally, we found that MLLMs required firm instructions regarding page breaks, or they would tend to be removed completely, or moved to 'sensible' places in the text, such as the end of a sentence or section, against the formatting of the original handwritten document.\nWe believe that further investigation into this long-context failure mode is warranted; we leave it to future work.\n+FIRST PAGE out-of-sample analysis. Table 2 shows the performance of +FIRST PAGE and other methods only on pages after the first.\nWe find that +FIRST PAGE performs significantly better than the OCR ONLY baselines, even though the provided page contains none of the text that the model is ultimately being scored on. The benefit here is entirely due to extrapolating non-textual and contextual information from a document image to similar images, and learning (at least to some extent) the mapping from image to OCR output, and exploiting knowledge of that mapping to reduce errors.\nThis is a significant result, as it shows that multi-page transcription is a long-context problem, and that methods tackling this problem holistically with MLLMs can improve performance. We believe this merits further investigation."}, {"title": "Scaling challenges", "content": "We have experiments for documents of 2-3 pages, but we would expect +FIRST PAGE to be especially cost-effective for longer documents. Hypothetically, the insights from a single page should be equally applicable across all pages with similar formatting, so the performance improvement for an n page document should cost the same as improving performance for a one page document: the cost of processing just a single page.\nUnfortunately, in practice, for long documents this benefit seems to trade off with MLLM preference for a PBP approach (see Figure 7), where the benefit of having access to the first page image cannot be shared to subsequent pages. Future work will look at how to mitigate this performance breakdown, and therefore how to increase the cost-effectiveness of +FIRST PAGE."}, {"title": "Scaling with prompt caching", "content": "Prompt caching (Shi et al. 2024) could provide an alternate means of benefiting from +FIRST PAGE in a PBP setting.\nPrompt caching is when an intermediate model state is saved after having already been 'primed' with the first part of a prompt, and is already used in some commercial LLMs. A PBP version of +FIRST PAGE would, without prompt caching, be just as expensive (and not nearly as informative) as +ALL PAGES PBP. With prompt caching, the first page would not need to be re-processed, and the performance benefits of both PBP and +FIRST PAGE could be achieved with cost scaling << O(n)."}, {"title": "Poor performance of +CHOSEN PAGE", "content": "We hypothesized that +CHOSEN PAGE should perform at least as well as +FIRST PAGE; in practice though, this was not the case, especially for GPT-40-MINI and for weaker OCR outputs. The decline in performance is likely due to the increased complexity of the prompt, and the MLLM being more likely to be confused by the concept of an nth page than by the first. Previous work has successfully used LLM decision-making for downstream tasks (Wu et al. 2024; Schick et al. 2024), suggesting that further investigation of this method is worthwhile."}, {"title": "The performance-cost trade-off", "content": "Figure 8 plots performance against cost for IAM for all methods described in this paper and draws a Pareto frontier of the optimal methods, trading off performance and cost."}, {"title": "Conclusion and Further Work", "content": "In this work, we investigated the transcription of multi-page handwritten documents using various configurations of commercial OCR engines and MLLMs. We examined the effectiveness of different prompting strategies and proposed the +FIRST PAGE method, which provides the MLLM with the OCR output for the entire document along with just the first page image. Our experiments on a multi-page synthesis of the IAM Handwriting Database demonstrated that +FIRST PAGE improves transcription accuracy while balancing cost and performance. Notably, it is effective even on out-of-sample text, as it leverages formatting and OCR error patterns from a single page to other, 'unseen' pages.\nFuture work will look at additional handwritten document datasets, explore mitigating the performance degradation observed with increasing document length, and further investigate the potential of prompt caching and other techniques to leverage information across pages in long document transcription."}]}