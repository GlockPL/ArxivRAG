{"title": "ATTENTION IS A SMOOTHED CUBIC SPLINE", "authors": ["ZEHUA LAI", "LEK-HENG LIM", "YUCONG LIU"], "abstract": "We highlight a perhaps important but hitherto unobserved insight: The attention\nmodule in a transformer is a smoothed cubic spline. Viewed in this manner, this mysterious\nbut critical component of a transformer becomes a natural development of an old notion deeply\nentrenched in classical approximation theory. More precisely, we show that with ReLU-activation,\nattention, masked attention, encoder-decoder attention are all cubic splines. As every component\nin a transformer is constructed out of compositions of various attention modules (= cubic splines)\nand feed forward neural networks (= linear splines), all its components encoder, decoder, and\nencoder-decoder blocks; multilayered encoders and decoders; the transformer itself are cubic or\nhigher-order splines. If we assume the Pierce-Birkhoff conjecture, then the converse also holds, i.e.,\nevery spline is a ReLU-activated encoder. Since a spline is generally just C\u00b2, one way to obtain\na smoothed C\u221e-version is by replacing ReLU with a smooth activation; and if this activation is\nchosen to be SoftMax, we recover the original transformer as proposed by Vaswani et al. This\ninsight sheds light on the nature of the transformer by casting it entirely in terms of splines, one of\nthe best known and thoroughly understood objects in applied mathematics.", "sections": [{"title": "1. MATHEMATICAL DESCRIPTION OF THE TRANSFORMER", "content": "A transformer is typically presented in the literature as a flow chart [45, Figure 1]. We show a version in Figure 1."}, {"title": "1.1. Notations.", "content": "We write all vectors in R\u201d as column vectors, i.e., Rn = Rn\u00d71. Let x1,..., Xn \u2208 R.\nWhen enclosed in parentheses (x1,...,xn) denotes a column vector, i.e.,\n(x1,...,xn):=\n\\begin{bmatrix}\nX1\\\\\n:\\\\\nXn\n\\end{bmatrix}\u2208 Rn.\nWhen enclosed in brackets [x1,...,xn] \u2208 R1\u00d7n is a row vector.\nWe will apply this convention more generally: For matrices X1,..., Xn \u2208 Rm\u00d7p, we write\n(X1,..., Xn) :=\n\\begin{bmatrix}\nX1\\\\\n:\\\\\nXn\n\\end{bmatrix}\u2208 Rmnxp\nand [X1,..., Xn] \u2208 Rm\u00d7np.\nWhen we write (f1,..., fh) for functions fi : Rn\u00d7p \u2192 Rm\u00d7p, i = 1, . . ., h, it denotes the function\n(f1,..., fh): Rnxp \u2192 Rmhxp, X\u2194\n\\begin{bmatrix}\nf1(X)\\\\\n:\\\\\nfh(X)\n\\end{bmatrix}"}, {"title": "1.2. Feed forward neural network.", "content": "The rectified linear unit ReLU : R \u2192 R is defined by\nReLU(x) = max(x,0) =: x+ and extended coordinatewise to vectors in Rn or matrices in Rnxp.\nWe also introduce the shorthand x\u00af := ReLU(-x). Clearly, X = X+ \u2212 X\u00ae for any X \u2208 Rn\u00d7p.\nAn l-layer feed forward neural network is a map 6 : R\u2033 \u2192 Rn\u0131+1 defined by a composition:\n\u03c6(x) = \u0391\u03b9+1\u03c3\u03b9 \u0391\u03b9\u00b7\u00b7\u00b7 \u03c32A201A1x + bi+1\nfor any input x \u2208 Rn, weight matrix Ai \u2208 Rni\u00d7ni\u22121, with no = n, \u03c3\u2081(x) := \u03c3(x + b\u2081), with bi \u2208 Rni\nthe bias vector, and \u03c3 : R \u2192 R the activation function, applied coordinatewise. In this article, we"}, {"title": "1.3. Attention.", "content": "Mathematically, it is a map a : Rnxp \u2192 Rm\u00d7p,\n\u03b1(X) := V(X) SoftMax(K(X)Q(X)),\nwhere Q : Rnxp \u2192 Rd\u00d7p, K : Rn\u00d7p \u2192 Rd\u00d7p, V : Rn\u00d7p \u2192 Rm\u00d7p are linear layers, i.e., given by\naffine maps\nQ(X) = AQX + BQ,K(X) = AKX + BK, V (X) = AvX + Bv,\nwith weight matrices AQ, AK \u2208 Rd\u00d7n, Av \u2208 Rm\u00d7n, and bias matrices BQ, BK \u2208 Rd\u00d7p, Bv \u2208 Rmxp.\nHere we have used the more general affine form of these linear layers as attention modules are\nimplemented in practice,\u00b9 as opposed to the linear form in [45] where the biases are set to zero.\nThe SoftMax in (3) is applied columnwise and outputs a p\u00d7p matrix.\nThe map a implements the mechanism of taking a query and a set of key-value pairs to an\noutput. Interpreted in this way, the input X \u2208 Rn\u00d7p is a data sequence of length p, with each\ndata point xi \u2208 Rn, i = 1,...,p. The columns of Q(X) and K(X) represent queries and keys\nrespectively note that these are vectors in Rd and d is generally much smaller than m or n. The\ncolumns of V (X) represent values.\nMore generally, a multihead or h-headed attention module is a map a : Rn\u00d7p \u2192 Rmhxp given by\n\u03b1(X) = (\u03b11(\u03a7), ..., ah(X))\nwhere ai : Rn\u00d7p \u2192 Rm\u00d7p are attention modules as in (3), i = 1,...,h. The reader is reminded of\nour convention in Section 1.1: parentheses denote column, which is why in our constructions we\nwill often the phrase \"stacking 01,..., ah to obtain a\" to mean (5)."}, {"title": "1.4. Encoder.", "content": "An encoder block, or more precisely a h-head encoder block, is a mape : Rnxp \u2192\nRni+1\u00d7P obtained by composing the output of a h-head attention module a : Rn\u00d7p \u2192 Rmh\u00d7p, with\nan l-layer ReLU-neural network 6 : Rmhxp \u2192 Rni+1\u00d7P,\n\u03b5 = \u03c6\u03bf \u03b1.\nMore generally, an encoder or t-layer encoder, Et : Rnxp \u2192 Rnt+1\u00d7p is obtained by composing t\nencoder blocks, i.e.,\nEt = t \u00b0 At \u00b0Ct-1\u00b0 At\u22121\u00b0\uff65\uff65\uff65\u00b0 1\u00b0 \u03b11,\nwhere yi: Rmixp \u2192 [Rni+1\u00d7pare neural networks and a\u00a1 : Rni\u00d7p \u2192 Rmi\u00d7P are attention modules,\ni = 1,...,t, n\u2081 = n. In Figure 1, the encoder is the part enclosed within the red dash lines on\nthe left. The structure in (7) appears to require alternate compositions of attention modules and\nneural networks but one may skip some or all of the oi's. The reason is that we may choose\nthese i's to be an identity map, which can be represented as a one-layer neural network as x =\nReLU(x) - ReLU(-x).\nWhile we allow the neural networks appearing in (7) to have multiple hidden layers, the original\nproposed model in [45] requires that they be single-layer. We will show in Lemma 3.7 that these\nare in fact equivalent: Any encoder of the form (7) may be written as one where all yi's have only\none hidden layer, but at the expense of a larger t."}, {"title": "1.5. Masked attention.", "content": "In many applications of transformers, particularly large language models,\nthe data is of a sequential nature. So the function f we want to learn or approximate is expected\nto be autoregressive [45], i.e., f : Rn\u00d7p \u2192 Rm\u00d7p takes the form\n[x1,...,xp] \u2192 [f1(x1), f2(x1,x2),...,fp(x1,...,Xp)].\nIn other words fj : Rn\u00d7j \u2192 Rm depends only on the first j columns x1,...,xj, j = 1,...,p. In\ngeneral f will be nonlinear, but when f is linear, then this simply means it is given by an upper\ntriangular matrix. So an autoregressive function may be viewed as a nonlinear generalization of an\nupper triangular matrix.\nTo achieve this property in attention module, we define the function mask : RP\u00d7P \u2192 RPXP by\nmask(X)ij =\n\\begin{cases}\nXij & \\text{if } i < j,\\\\\n-\u221e & \\text{if } i > j.\n\\end{cases}\nA masked attention module is then given by\n\u03b2(X) = V (X) SoftMax (mask(K(X)Q(X))).\nIt is easy to check that a masked attention module is always autoregressive."}, {"title": "1.6. Decoder.", "content": "A decoder block is the analogue of an encoder block where we have a masked\nattention in (6):\n\u03b4 = \u03c6 \u03bf \u03b2.\nWe may also replace any or all of the ai's in (7) by masked versions \u03b2i's. If we replace all, then\nthe resulting map\nSt = t\u00b0 \u03b2t \u00b0 Ct\u22121 \u00b0 \u1e9et\u22121 \u00b0\uff65\uff65\uff65 \u25cb41\u00b0\u03b21,\nis autoregressive but more generally we will just selectively replace some ai's with \u1e9ei's. We call\nthe resulting map a decoder. Note that the part enclosed within red dash lines in the right-half of\nFigure 1 is not quite a decoder as it takes a feed from the left-half; instead it is an encoder-decoder,\nas we will discuss next."}, {"title": "1.7. Encoder-decoder attention.", "content": "The multihead attention in the right-half of Figure 1 accepts\na feed from outside the red dash box. When used in this manner, it is called an encoder-decoder\nattention module [45], as it permits one to use queries from the decoder, but keys and values from\nthe encoder. Mathematically, this is a map y : Rn\u00d7p \u00d7 Rr\u00d7p \u2192 Rm\u00d7p,\n\u03b3(X, Y) := V(X) SoftMax(K(X)Q(Y)),\nwhere Q, K, V are as in (4) but while K, V are functions of X, Q is now a function of Y. The\nindependent matrix variables X and Y take values in Rn\u00d7p and Rr\u00d7p respectively. As a result we\nhave to adjust the dimensions of the weight matrices slightly: Aq \u2208 Rd\u00d7r, AK \u2208 Rd\u00d7n, Av \u2208 Rm\u00d7n.\nThe encoder-decoder attention is partially autoregressive, i.e., autoregressive in Y but not in X,\ntaking the form\n(X, [Y1,..., Yp]) \u2192 [f1(X, y1), f2(X, y2), ..., fp(X, Y1, . . ., Yp)]."}, {"title": "1.8. Transformer.", "content": "An encoder-decoder block\u315c:Rn\u00d7p\u00d7Rr\u00d7p \u2192 Rni+1\u00d7P is defined by a multihead\nmasked attention module \u1e9e, a multihead encoder-decoder attention module y, and a neural network\n4, via\n\u03c4(X,Y) = \u03c6(\u03b3(\u03a7, \u03b2(Y))).\nAn (s + t)-layer encoder-decoder is then constructed from an s-layer encoder \u025bs, and t encoder-\ndecoder blocks given by B1, Y1, 61, \u00b7 \u00b7 \u00b7, \u03b2t, Yt, t. We define ti recursively as\nTi(X,Y) = \u03c6\u03af(\u03af(\u03b5\u03c2(X), \u03b2\u03af(Ti\u22121(X, Y))))\nYi\nfor i = 1,..., t, \u03c4\u03bf(X,Y) = Y. We call Tt the encoder-decoder. For all mathematical intents and\npurposes, Tt is the transformer. As we will see in Sections 1.10 and 1.11, the other components in\nFigure 1 or [45, Figure 1] are extraneous to the operation of a transformer.\nWe stress that the word \"transformer\" is sometimes used to refer to just the encoder or the\ndecoder alone. We choose to make the distinction in our article but many do not. For example,\nGoogle's BERT [16], for Bidirectional Encoder Representations from Transformers, is an encoder\nwhereas OpenAI's GPT [6], for Generative Pretrained Transformer, is a decoder."}, {"title": "1.9. ReLU-transformer.", "content": "The definitions in Sections 1.2-1.8 are faithful mathematical transcrip-\ntions of components as described in Vaswani et al. original article [45]. In this section we take a\nsmall departure - replacing every occurrence of SoftMax with ReLU to obtain what is called a\nReLU-transformer. This is not new either but proposed and studied in [3, 48].\nWe begin by defining ReLU-attention modules. They have the same structures as (3), (9), (12)\nexcept that SoftMax is replaced by ReLU, i.e.,\n\u03b1(X) = V(X) ReLU(K(X)Q(X)),\n\u03b2(X) = V(X) ReLU(mask(K(X)Q(X))),\n\u03b3(X,Y) = V (X) ReLU (K(X)\u012aQ(Y)).\nAn encoder, decoder, or encoder-decoder constructed out of such ReLU-attention modules will\nbe called a ReLU-encoder, ReLU-decoder, or ReLU-encoder-decoder respectively. In particular, a\nReLU-transformer is, for all mathematical intents and purposes, a ReLU-encoder-decoder.\nThese ReLU-activated variants are essentially \"unsmoothed\" versions of their smooth SoftMax-\nactivated cousins in Sections 1.2-1.8. We may easily revert to the smooth versions by a simple\nsmoothing process replace all ReLU-activated attentions by the original SoftMax-activated ones\n(but the neural networks would remain ReLU-activated)."}, {"title": "1.10. Layer normalization and residual connection.", "content": "Comparing our Figure 1 and [45, Fig-\nure 1], one might notice that we have omitted the \"add & norm\" layers.\nThe \"add\" step, also called residual connection [23], may be easily included in our analysis\nall our results and proofs in Section 3 hold verbatim with the inclusion of residual connection. For\nan encoder block\u03b5: Rn\u00d7p \u2192 Rn\u00d7p, a residual connection simply means adding the identity map\n1: Rnxp \u2192 Rn\u00d7p, X \u2192 X, i.e.,\n\u03b5 + \u03b9: Rn\u00d7P \u2192 RnXP, X \u2194 \u025b(X) + X,\nand likewise for a decoder block d : Rnxp \u2192 Rnxp. For an encoder-decoder block 7 : Rn\u00d7pxRrxp \u2192\nRrxp, a residual connection simply means adding the projection map \u03c0: Rn\u00d7p \u00d7 Rrxp \u2192 Rrxp,\n(X, Y) \u2192 Y, i.e.,\n\u03c4+\u03c0:Rn\u00d7p \u00d7 R\u2033\u00d7p \u2192 R'\u00d7P, (X, Y) \u2194 \u315c(X, Y) + Y.\nAs will be clear from the proofs in Section 3, all results therein hold with or without residual\nconnection.\nThe \"norm\u201d step, also called layer normalization [2] refers to statistical standardization, i.e.,\nmean centering and scaling by standard deviation of each column vector in X. This is an ubiquitous\nprocess routinely performed in just about any procedure involving any data for practical reasons.\nBut this innocuous process introduces additional nonlinearity that does not fit in our framework.\nWe do not consider either of these critical to the workings of a transformer. They are by no\nmeans unique and may be easily replaced with other data standardization process, as shown in [22]."}, {"title": "1.11. Miscellany.", "content": "The \"input/output embedding\u201d and \u201cposition embedding\" in Figure 1 convert\nsentences or images (or whatever real-world entity the transformer is used for) to an input in Rn\u00d7p;\nthe \"linear layer\" and \"SoftMax\" in the right half assign probability values to the output. These are\njust auxiliary components necessary in any situation involving human-generated input or requiring\nhuman-interpretable output. They are common to all practical AI models and we do not regard\nthem as part of the transformer architecture."}, {"title": "2. SPLINES", "content": "This section covers the salient aspects of splines relevant for us. We write R[x1,...,xn] for the\nring of polynomials with real coefficients in variables (x1,...,xn) =: x and R[x11,..., Inp] for that\nin (xij)=1 =: X.\nSplines have a rich history and a vast literature in applied and computational mathematics, this\nbeing precisely the reason we chose them as our platform to understand a new technology like\nthe transformer. Mathematical splines, as opposed to the mechanical ones used by draftsmen and\nshipbuilders, were first named in [42]. A one-line summary of its early history, with many regretful\nomissions, is that univariate splines were first proposed in [41], multivariate splines in [4], B-Splines\nin [10], and box splines in [15].\nAn important departure of our discussion of splines in this article is that we will not concern\nourselves with differentiability, avoiding the usual efforts to ensure that a piecewise-defined function\nis Cr at points where the different pieces meet. The reason is simple: our results in the next section"}, {"title": "2.1. Scalar-valued splines.", "content": "In its simplest form a spline is a piecewise-polynomial real-valued\nfunction f: Rn \u2192 R defined over a partition of its domain R\". The classical and most basic\npartition is a triangulation, i.e., a subdivision into n-dimensional simplices whose union is Rn and\nintersecting only along faces; more generally one may also use convex polytopes in place of simplices\n[13, 8, 35]. We will need a slightly more sophisticated partition called a semialgebraic partition\n[18, 17, 43]. For any b \u2208 N, let\n\u0398\u266d := {0 : {1, ...,b} \u2192 {1,0,-1}},\na finite set of size 36. Note that this is really just the set of ternary numerals with b (ternary) bits.\nDefinition 2.1 (Partition). Any \u03c01,...,\u03c0\u03b9 \u2208R[x1,...,xn] induces a sign partition of Rn via\n\u03a0\u0473 := {x \u2208R\" : sgn(\u03c0\u2081(x)) = 0(i), i = 1, . . ., b}.\nThen {\u03a0\u03c1: \u03b8\u2208 \u0398\u266d} is a partition of R\", the semialgebraic partition induced by \u03c01,..., \u03c0\u03bb.\nNote that the domain of 0 in (14) merely serves as a placeholder for any b-element set and does\nnot need to be {1,...,b}. Indeed we will usually write \u03b8 : {\u03c01,..., \u03c0\u03bf} \u2192 {1,0, \u22121} to emphasize\nthat it is an index for the partition induced by \u03c0\u2081,..., \u03c0\u03b9. Any triangulation or partition into\npolytopes can be obtained by choosing appropriate linear polynomials \u03c0\u2081,..., \u03c0\u266d so Definition 2.1\ngeneralizes the basic one that requires partition to be piecewise linear.\nDefinition 2.2 (Spline). Let {\u03a0e : 0 \u2208 \u0398\u266d} be the semialgebraic partition induced by \u03c01,..., \u03c0\u03b3 \u0395\nR[x1,...,xn]. A continuous function f : Rn \u2192 R is a polynomial spline of degree k if for each\ni = 1, ..., b,\n(i) \u03c0\u03af has degree not more than k;\n(ii) if I \u2260 \u00d8, then f restricts to a polynomial of degree not more than k on \u03a0g, i.e., f(x) =\n\u03be\u03bf(x) for all x \u2208 \u03a0e, for some \u03be\u03b8 \u2208 R[x1,...,xn] of degree not more than k.\nHenceforth, \"spline\" will mean \"polynomial spline,\u201d \u201cdegree-k\u201d will mean \"degree not more\nthan k,\" and \"partition\" will mean \"semialgebraic partition.\" The small cases k = 1,2,3,5 are\ncustomarily called linear, quadratic, cubic, and quintic splines respectively. The standard notation\nfor the set of all r-times differentiable degree-k splines with partition induced by \u03c01,...,\u03c0\u03b9 is\nS(\u03c01,..., \u03c0\u03bf) but since we will only need the case r = 0 and splines as defined in Definition 2.2\nare always continuous, we may drop the superscript r.\nObserve that Sk(\u03c01,..., \u03c0\u266d) is a finite-dimensional real vector space. So it is straightforward to\nextend Definition 2.2 to V-valued splines f : Rn \u2192 V for any finite-dimensional real vector space V\nusing tensor product, namely, they are simply elements of Sk(\u03c01,..., \u03c0\u03b9) \u2297 V [29, Example 4.30].\nFor the benefit of readers unfamiliar with tensor product constructions, we go over this below in a\nconcrete manner for V = Rn and Rnxp."}, {"title": "2.2. Vector-valued splines.", "content": "A vector-valued degree-k spline f : Rn \u2192 Rm is given by\nm\nf(x) = \u2211 fi(x)ei for all x \u2208 R\",\ni=1"}, {"title": "2.3. Matrix-valued splines.", "content": "In this case we are interested in splines that are not just matrix-\nvalued but also matrix-variate. One nice feature with our treatment of splines in Section 2.1 is that\nwe can define matrix-variate splines over Rn\u00d7p by simply replacing all occurrences of R[x1,...,xn]\nwith R[x11,..., Xnp]. A matrix-valued degree-k spline f : Rn\u00d7p \u2192 Rm\u00d7p is then given by\nm\np\nf(x) = \u2211\u2211 fij(X)Eij for all X \u2208 Rnxp,\ni=1 j=1\nwhere fij \u2208 Sk(\u03c01,...,\u03c0\u03bf) and Eij \u2208 Rm\u00d7p, i = 1,...,m, j = 1,...,p. Here Eij is the stan-\ndard basis matrix with one in (i, j)th entry and zeros everywhere else. Again, an alternative but\nequivalent way to define them would be in a coordinatewise fashion, i.e., f = (fij)1 where\nfij \u2208 Sk(\u03c01,..., \u03c0\u044c), \u0456 = 1, . . ., m, j = 1, . . ., p. Note that p = 1 reduces to the case in Section 2.2."}, {"title": "2.4. Pierce-Birkhoff conjecture.", "content": "Garrett Birkhoff, likely the person first to realize the impor-\ntance of splines in applications though his consulting work [49], also posed one of the last remaining\nopen problems about splines [5].\nConjecture 2.3 (Pierce-Birkhoff). For every spline f : Rn \u2192 R, there exists a finite set of\npolynomials \u00c9ij \u2208 R[x1,...,xn], i = 1, ..., m, j = 1,...,p such that\nf = \\max_{i=1,...,m} \\min_{j=1,...,p} Eij.\nThis conjecture is known to be true for n = 1 and 2 but is open for all higher dimensions [33].\nOur results in Section 3.3 will be established on the assumption that the Pierce-Birkhoff conjecture\nholds true for all n, given that there is significant evidence [31, 46, 34] for its validity.\nThe kind of functions on the right of (16) we will call max-definable functions in the variables\nX1,...,xn. These are functions f : Rn \u2192 R generated by 1,x1,...,xn under three binary oper-\nations: addition (x, y) \u2192 x + y, multiplication (x, y) \u2192 x\u00b7y, maximization (x, y) \u2192 max(x, y);\nand scalar multiplication x\u21a6 \u00c0x by \u00c0 \u2208 R. Note that minimization comes for free as min(x, y) :=\nmax(-x,-y). Using the identity xy+ = max(min(xy, x\u00b2y + y), min(0, -x\u00b2y - y)), any max-\ndefinable functions can be reduced to the form maxi=1,...,m minj=1,...,pij with \u00a7ij \u2208 R[x1,..., Xn]\n[24]. The notion may be easily extended to matrix-variate, matrix-valued functions f :Rn\u00d7p \u2192\nRm\u00d7p coordinatewise, i.e., by requiring that each fij: Rn\u00d7p \u2192 R be a max-definable function in\nthe variables x11, X12,..., Xnp.\nClearly, the set of max-definable functions is contained within the set of splines. Pierce-Birkhoff\nconjecture states that the two sets are equal. Both are examples of an \u201cf-ring\u201d as defined in [5],\nnow christened \u201cPierce-Birkhoff ring\" after the two authors. If we drop multiplication from the\nlist of binary operations generating the max-definable functions, the resulting algebraic object is\nthe renown max-plus algebra or tropical semiring [32]."}, {"title": "3. EQUIVALENCE OF SPLINES AND TRANSFORMERS", "content": "We will show that every component of the transformer defined in Section 1 is a spline neu-\nral network, attention module, masked attention module, encoder block, decoder block, encoder,"}, {"title": "3.1. Transformers are splines.", "content": "We will first remind readers of the main result in [1] establishing\nequivalence between neural networks and linear splines.\nTheorem 3.1 (Arora-Basu-Mianjy-Mukherjee). Every neural network y : Rn \u2192 R is a linear\nspline, and every linear spline l : Rn \u2192 R can be represented by a neural network with at most\n[log2(n + 1)] +1 depth.\nCompositions of spline functions are by-and-large uncommon in the literature for reasons men-\ntioned in the beginning\none usually combines splines by taking sums or linear combinations.\nMatrix-valued splines also appear to be somewhat of a rarity in the literature. Consequently we\nare unable to find a reference for what ought to be a fairly standard result about degrees under\ncomposition and matrix multiplication, which we state and prove below.\nLemma 3.2. (i) Let g : Rn \u2192 Rm be a spline of degree k and f : Rm \u2192 RP a spline of degree\nk'. Then fog is a spline of degree kk'.\n(ii) Let f : Rr\u00d7s \u2192 Rm\u00d7n and g: Rr\u00d7s \u2192 Rn\u00d7p be splines of degrees k and k'. Then fg :\nRrxs \u2192 Rm\u00d7p, X \u2192 f(X)g(X), is a spline of degree k + k'."}, {"title": "3.2. Veronese map.", "content": "The degree-k Veronese embedding uk is a well-known map in algebraic ge-\nometry [21, pp. 23\u201325] and polynomial optimization [28, pp. 16\u201317]. Informally it is the map that\ntakes variables x1,...,xn to the monomials of degree not more than k in x1,...,Xn. This defines\nan injective smooth function\n\u03c5\u03ba: Rn\u2192R (", "simple\nexamples": "vk : R \u2192 Rk, vk(x) = (1, x, x2, . . ., xk); v2 : R2 \u2192 R6, v2(x, y) = (1, x, y, x2,xy, y\u00b2).\nIn algebraic geometry [21, pp. 23\u201325] the Veronese map is usually defined over projective spaces\nwhereas in polynomial optimization [28, pp. 16\u201317] it is usually defined over affine spaces as in (17).\nNevertheless this is a trivial difference as the former is just a homogenized version of the latter.\nAs is standard in algebraic geometry and polynomial optimization alike, we leave out the domain\ndependence from the notation uk to avoid clutter, e.g., the quadratic Veronese map v2 : R2 \u2192 R6\nand v2 : R6 \u2192 R28 are both denoted by 22. This flexibility allows us to compose Veronese maps\nand speak of Uk\u00b0\u03c5\u03ba for any k,k' \u2208 N. For example we may write v2 \u00b0 v2 : R2 \u2192 R28, using the\nsame notation 22 for two different maps.\nThe Veronese map is also defined over matrix spaces: When applied to matrices, the Veronese\nmap simply treats the coordinates of an n \u00d7 p matrix as np variables. So vr : Rnxp \u2192 R(", "v2": "R2\u00d72 \u2192 R15 evaluated on [] gives\n(1, x, y, z, w, x2, xy, xz, xw, y\u00b2, yz, yw, z\u00b2, zw, w\u00b2)."}, {"title": "3.3. Splines are transformers.", "content": "We will show that any matrix-valued spline f: Rn\u00d7p \u2192 Rr\u00d7p is\nan encoder. First we will prove two technical results. We will use i, \u00ee, \u012b, j, j, j to distinguish between\nindices. We remind the reader that x+ := ReLU(x).\nLemma 3.6 (Quadratic Veronese as encoders). Let v2 : Rnxp \u2192 R(np+2)(np+1)/2 be the quadratic\nVeronese map. There exists a two-layer encoder \u00a32 : Rn\u00d7p \u2192 Rn2\u00d7p such that every column of\n\u00a32(X) contains a copy of v2(X) in the form\n\u20ac2(X) =\n\\begin{bmatrix}\nU2(X) & 0 & 0\\\\\n0 & 02(X) & 0\\\\\n: & : & :\\\\\n0 & 0 & 02(X)\n\\end{bmatrix}\u2208ERN2XP.\nMore precisely, there is a h\u2081-headed attention module a1 : Rn\u00d7p \u2192 Rmh1\u00d7p, a one-layer neural\nnetwork, 41 : Rmh1\u00d7p \u2192 Rn1\u00d7p, a h\u2082-headed attention module a2 : Rn1\u00d7p \u2192 Rmh2\u00d7p, and another\none-layer neural network 42 : Rmh2\u00d7p \u2192 Rn2\u00d7p, such that\n\u20ac2 = 42\u00b0 2\u00b0 41\u03bf \u03b1\u03b9.\nIn particular, any monomial of degree not more than two in the entries of X appears in every\ncolumn of \u00a32(X).\n\nLemma 3.7 (One-layer neural networks as encoder blocks). Let 6 : Rn \u2192 Rn2 be a one-layer\nneural network. Then\n6 : Rn\u00d7p \u2192 RN2\u00d7P, [X1,..., Xp] \u2192 [\u03c6(x1), ..., \u03c6(xp)],\nis an encoder block of the form 41 \u00b0 a\u2081 where 41 is also a one-layer neural network and a\u2081 is an\nattention module."}, {"title": "3.5. The Pierce-Birkhoff conjecture holds if and only if", "content": "The Pierce-Birkhoff conjecture holds if and only if for any spline f : Rn \u2192 R, there\nexist k \u2208 N and a linear spline l: R(**) \u2192 R such that f = l \u2022 vk.\n\n(18)\nUk: Rnxp \u2192 R(Pt+k), l: R(Pt+k) \u2192 R.\n\n3.3. Splines are transformers. We will show that any matrix-valued spline f: Rn\u00d7p \u2192 Rr\u00d7p is\nan encoder. First we will prove two technical results. We will use i, \u00ee, \u012b, j, j, j to distinguish between\nindices. We remind the reader that x+ := ReLU(x)."}, {"title": "3.6. (Autoregressive splines as decoders)", "content": "Let k \u2208 N and f : Rn\u00d7p \u2192 Rm\u00d7p_be an\nautoregressive max-definable function. Then f is a t-layer decoder for some finite t \u2208 N. More pre-\ncisely, there exist t masked attention modules \u1e9e1,..., \u1e9et and t one-layer neural networks 41,..., Ot\nsuch that\nf = t \u00b0 \u1e9et\u00b0 t\u22121 \u00b0\u1e9et\u22121\u00b0\uff65\uff65\uff65 \u25cb41 \u00b0 \u03b21.\nIf the Pierce-Birkhoff conjecture holds, then any degree-k autoregressive spline is a decoder.\n\nTheorem 3.8 then apply with masked attention modules in place of attention modules.\nA similar construction can be extended to construct partially autoregressive splines as encoder\ndecoders."}, {"title": "4. CONCLUSION", "content": "It is an old refrain in mathematics that one does not really understand a mathematical proof\nuntil one can see how every step is inevitable. This is the level of understanding that we hope\nSection 3.3 provides for the transformer.\n\nArora et al. [1] have shown that neural networks are exactly linear splines. Since\ncompositions of linear splines are again linear splines, to obtain more complex functions we need\nsomething in addition to neural networks. Viewed in this manner, the attention module in Sec-\ntion 1.3 is the simplest function that serves the role. Lemma 3.6 shows that the quadratic Veronese\nmap, arguably the simplest map that is not a linear spline, can be obtained by composing two\nattention modules. The proof reveals how heads and layers are essential: It would fail if we lacked\nthe flexibility of having multiple heads and layers. The proof also shows how a neural network\nworks hand-in-glove with attention module: It would again fail if we lack either one. The proof\nof Theorem 3.8 then builds on Lemma 3.6: By composing quadratic Veronese maps we can obtain\nVeronese map of any higher degree; and by further composing it with linear splines we obtain all\npossible splines. The resulting map, an alternating composition of attention modules and neural\nnetworks, is exactly the encoder of a transformer."}, {"title": "4.2. Recommendations."}]}