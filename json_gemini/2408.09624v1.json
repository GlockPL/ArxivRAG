{"title": "ATTENTION IS A SMOOTHED CUBIC SPLINE", "authors": ["ZEHUA LAI", "LEK-HENG LIM", "YUCONG LIU"], "abstract": "We highlight a perhaps important but hitherto unobserved insight: The attention\nmodule in a transformer is a smoothed cubic spline. Viewed in this manner, this mysterious\nbut critical component of a transformer becomes a natural development of an old notion deeply\nentrenched in classical approximation theory. More precisely, we show that with ReLU-activation,\nattention, masked attention, encoder-decoder attention are all cubic splines. As every component\nin a transformer is constructed out of compositions of various attention modules (= cubic splines)\nand feed forward neural networks (= linear splines), all its components encoder, decoder, and\nencoder-decoder blocks; multilayered encoders and decoders; the transformer itself are cubic or\nhigher-order splines. If we assume the Pierce-Birkhoff conjecture, then the converse also holds, i.e.,\nevery spline is a ReLU-activated encoder. Since a spline is generally just $C^2$, one way to obtain\na smoothed $C^\\infty$-version is by replacing ReLU with a smooth activation; and if this activation is\nchosen to be SoftMax, we recover the original transformer as proposed by Vaswani et al. This\ninsight sheds light on the nature of the transformer by casting it entirely in terms of splines, one of\nthe best known and thoroughly understood objects in applied mathematics.", "sections": [{"title": "", "content": "The transformer [45] underlies many modern AI technologies in the current news cycle. Splines,\non the other hand, are among the oldest tools in classical approximation theory, studied since the\n1940s, and culminated in the 1980s [14] before taking on a new life in the form of wavelets (e.g., the\ncelebrated Cohen-Daubechies-Feauveau wavelet [9] that underlies JPEG 2000 compression comes\nfrom a B-spline). Indeed, the word \u201cspline\u201d originally refers to the flexible wooden strip that serves\nas a bendable ruler for shipbuilders and draftsmen to draw smooth shapes since time immemorial;\nthe Wright brothers had notably used such wooden splines to design their aircraft. It is therefore\nsomewhat surprising that a notion so old is nearly one and the same as a notion so new we will\nshow that every ReLU-activated attention module $F : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{m\\times p}$ is a multivariate cubic spline,\nand, if we assume a conjecture of Garrett Birkhoff and Richard Pierce from 1956 [5], then conversely\nevery multivariate spline $G : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ is a ReLU-activated encoder. The usual SoftMax-activated\nattention module is thus a simple and natural way to make a cubic spline, which is at most a\n$C^2$-function, into a smooth function by replacing the nonsmooth ReLU with a smooth SoftMax.\nWhy did approximation theorists not discover the transformer then? We posit that it is due to\na simple but fundamental difference in how they treat the decomposition of a complicated function\ninto simpler ones. In approximation theory and harmonic analysis, one decomposes a complicated\nfunction $F$ into a sum of simpler functions $f_1,..., f_r$,\n\n$F = f_1+f_2+\\cdots+ f_r$;\nin artificial intelligence, one decomposes $F$ into a composition of simpler functions $F_1,..., F_r$,\n\n$F = F_1 \\circ F_2 \\circ... \\circ F_r$.\nThis fundamental difference in modeling a function is a key to the success of modern AI models.\nSuppose $F: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$. If we model $F$ as a sum in (1), the number of parameters scales like $nd$:\n\n$F(x) = \\sum_{i_1=1}^d\\cdots \\sum_{i_n=1}^d \\sum_{j=1}^n a_{i_1i_2...i_n j} \\varphi_{i_1}(x_1) \\varphi_{i_2}(x_2)... \\varphi_{i_n}(x_n)e_j$;\nwhereas if we model $F$ as a composition in (2), it scales like $dn^2 + (d - 1)n$:\n\n$F(x) = \\sigma_{d-1}A_{d-1}\\sigma_{d-2}A_{d-2}\\cdots \\sigma_2 A_2\\sigma_1 A_1x$"}, {"title": "", "content": "with $A_i \\in \\mathbb{R}^{n \\times n}, \\sigma_i$ parameterized by a vector in $\\mathbb{R}^n$. Note that even if $d = 2$, the size $n^{2n}$ quickly\nbecomes untenable. Evidently, these ball park estimates are made with some assumptions: The root\ncause of this notorious curse of dimensionality is that there are no good general ways to construct\nthe basis function $f_i$ in (1) except as a tensor product of low (usually one) dimensional basis\nfunctions, i.e., as $\\varphi_{i_1} \\varphi_{i_2}...\\varphi_{i_n}e_j$. The compositional model (2) allows one to circumvent\nthis problem beautifully. Take the simplest case of a $d$-layer feed forward neural network, as we\ndid above; then it is well known that $d$ can be small [11, 25].\nAn important feature of (1) and (2) is that both work well with respect to derivative by virtue\nof linearity\n\n$DF = Df_1 + Df_2 + \\cdots + Df_r$\nor chain rule\n\n$DF = DF_1 \\circ DF_2 \\circ \\cdots \\circ DF_r$.\nThe former underlies techniques for solving various PDEs, whether analytically or numerically;\nthe latter underlies the back-propagation algorithm for training various AI models (where the\nformer also plays a role through various variants of the stochastic gradient descent algorithm). The\nbottom line is that the conventional way to view a cubic spline, as a sum of polynomials supported\non disjoint polygonal regions or a sum of monomials, takes the form in (1). A ReLU-attention\nmodule is just the same cubic spline expressed in the form (2), and in this form there is a natural\nand straightforward way to turn it into a smooth function, namely, replace all nonsmooth $F_i$'s with\nsmooth substitutes if we replace ReLU by SoftMax, we obtain the attention module as defined\nin [45]. This is a key insight of our article.\nIt is well-known [1] that a ReLU-activated feed forward neural network may be viewed as a\nlinear spline expressed in the form of (2). When combined with our insight that a ReLU-activated\nattention module is a cubic spline, we deduce that every other intermediate components of the\nReLU-transformer encoder, decoder, encoder-decoder\nare either cubic or higher-order spline,\nas they are constructed out of compositions and self-compositions of ReLU-activated feed forward\nneural networks and ReLU-activated attention modules.\nA word of caution: We are not claiming that SoftMax would be a natural smooth replacement for\nReLU. We will touch on this in Section 4. Indeed, according to recent work [48], this replacement\nmay be wholly unnecessary when it comes to transformers, ReLU would be an equally if not\nsuperior choice of activation compared with SoftMax."}, {"title": "Understanding transformers via splines.", "content": "Our main contribution is to explain a little-\nunderstood new technology using a well-understood old one. For the benefit of approximation\ntheorists who may not be familiar with transformers or machine learning theorists who may not be\nfamiliar with splines, we will briefly elaborate.\nThe transformer has become the most impactful technology driving AI. It has revolutionized\nnatural language processing, what it was originally designed for [45], but by this point there is no\nother area in AI, be it computer vision [19], robotics [50], autonomous vehicles [38], etc, that is\nleft untouched by transformers. This phenomenal success is however empirical, the fundamental\nprinciples underlying the operation of transformers have remained elusive.\nThe attention module is evidently the most critical component within a transformer, a fact\nreflected in the title of the paper that launched the transformer revolution [45]. It is arguably\nthe only new component the remaining constituents of a transformer are ReLU-activated feed\nforward neural networks, which have been around for more than 60 years [40] and thoroughly\ninvestigated. Unsurprisingly, it is also the least understood. An attention module is still widely\nunderstood by way of \"query, key, value\" and a transformer as a flow chart, as in the article where\nthe notion first appeared [45]. The main goal of our article is to understand the attention module in\nparticular and the transformer in general, by tying them to one of the oldest and best-understood\nobject in approximation theory."}, {"title": "", "content": "Splines are a mature, well-understood technology that has been thoroughly studied and widely\nused [41, 42, 12, 13, 15, 8, 47, 44, 35, 14], one of our most effective and efficient methods for\napproximating known functions and interpolating unknown ones. They have numerous applications\nand we will mention just one: representing intricate shapes in computer graphics and computer-\naided design. Readers reading a hard copy of this article are looking at fonts whose outlines are\ndefined by splines [26]; those viewing it on screen are in addition using a device likely designed\nwith splines [20]. Splines have ushered in a golden age of approximation theory, and were studied\nextensively c. 1960-1980, until wavelets supplanted them. One could not have asked for a better\nplatform to understand a new technology like the attention module and transformer.\nNowhere is this clearer than our constructions in Section 3.3 to show that every spline is an\nencoder of a ReLU-transformer. These constructions reveal how each feature of the transformer\nattention, heads, layers, feed forward neural networks plays an essential role. We made every\nattempt to simplify and failed: Omit any of these features and we would not be able to recreate\nan arbitrary spline as an encoder. It were as if the inventors of transformer had designed these\nfeatures not with any AI applications in mind but to construct splines as compositions of functions."}, {"title": "MATHEMATICAL DESCRIPTION OF THE TRANSFORMER", "content": "A transformer is typically presented in the literature as a flow chart [45, Figure 1]. We show a\nversion in Figure 1.\nWithout a rigorous definition of the transformer, it will be difficult if not impossible to prove\nmathematical claims about it. We will nail down in mathematically precise terms the full inner"}, {"title": "", "content": "workings of a transformer. While it is common to find descriptions that selectively present parts as\nwell-defined maps and revert to words and pictures when it becomes less convenient, what sets us\napart below is thoroughness \u2014 nothing will be swept under the rug. On occasions we had to look\ninto the source codes of common implementations to unravel inconvenient details left ambiguous\nin the literature. This section is our small side contribution and a public service.\nThe heart of Figure 1 are the two parts enclosed in red dash lines, called encoder and decoder\nrespectively. They are constructed out of feed forward neural networks, defined in Section 1.2,\nand attention modules, defined in Section 1.3, chained together via function compositions. The\nsimplest version is the encoder in Section 1.4 and is what the uninitiated reader should keep in\nmind. We add the bells and whistles later: Section 1.5 defines the masked attention in the right-half\nof Figure 1, from which we obtain the decoder in Section 1.6. Section 1.7 explains the encoder-\ndecoder structure the left- and right-halves in Figure 1. Section 1.8 puts everything together\nto define the transformer. Section 1.10 discusses the one omission in Figure 1, the \"add & norm\"\nlayers found in [45, Figure 1].\n\nNotations. We write all vectors in $\\mathbb{R}^n$ as column vectors, i.e., $\\mathbb{R}^n = \\mathbb{R}^{n\\times 1}$. Let $x_1,..., x_n \\in \\mathbb{R}$.\nWhen enclosed in parentheses $(x_1,...,x_n)$ denotes a column vector, i.e.,\n\n$(x_1,...,x_n):=\\begin{bmatrix}x_1\\\\:\\\\x_n\\end{bmatrix} \\in \\mathbb{R}^n$.\nWhen enclosed in brackets $[x_1,...,x_n] \\in \\mathbb{R}^{1\\times n}$ is a row vector.\nWe will apply this convention more generally: For matrices $X_1,..., X_n \\in \\mathbb{R}^{m\\times p}$, we write\n\n$(X_1,..., X_n) := \\begin{bmatrix}X_1\\\\:\\\\X_n\\end{bmatrix} \\in \\mathbb{R}^{mn\\times p}$\nand $[X_1,..., X_n] \\in \\mathbb{R}^{m\\times np}$.\nWhen we write $(f_1,..., f_h)$ for functions $f_i : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{m\\times p}, i = 1, ..., h$, it denotes the function\n\n$(f_1,..., f_h): \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{mh\\times p}, X\\mapsto\\begin{bmatrix}f_1(X)\\\\:\\\\f_h(X)\\end{bmatrix}$\nThe function SoftMax : $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ takes a vector $x = (x_1,...,x_n) \\in \\mathbb{R}^n$ and outputs a probability\nvector of the same dimension,\n\n$SoftMax(x) := \\begin{bmatrix}\\frac{e^{x_1}}{\\sum_{i=1}^{n}e^{x_i}}\\\\...\\\\ \\frac{e^{x_n}}{\\sum_{i=1}^{n}e^{x_i}}\\end{bmatrix}$.\nWhen SoftMax is applied to a matrix $X \\in \\mathbb{R}^{n\\times p}$, it is applied columnwise to each of the $p$ columns\nof $X$. So SoftMax: $\\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{n\\times p}$.\nAlthough we will write $\\mathbb{R}$ throughout to avoid clutter, we will allow for $-\\infty$ in the argument of\nour functions on occasion, which will be clearly indicated. Note that $SoftMax(x)_i = 0$ if $x_i = -\\infty$.\n\nFeed forward neural network. The rectified linear unit ReLU : $\\mathbb{R} \\rightarrow \\mathbb{R}$ is defined by\nReLU$(x) = \\max(x, 0) =: x_+$ and extended coordinatewise to vectors in $\\mathbb{R}^n$ or matrices in $\\mathbb{R}^{n\\times p}$.\nWe also introduce the shorthand $x^- := \\text{ReLU}(-x)$. Clearly, $X = X^+ - X^-$ for any $X \\in \\mathbb{R}^{n\\times p}$.\nAn $l$-layer feed forward neural network is a map $\\varphi : \\mathbb{R}^n \\rightarrow \\mathbb{R}^{n_{l+1}}$ defined by a composition:\n\n$\\varphi(x) = A_{l+1}\\sigma_l A_l \\cdots \\sigma_2 A_2\\sigma_1 A_1x + b_{l+1}$\nfor any input $x \\in \\mathbb{R}^n$, weight matrix $A_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$, with $n_0 = n$, $\\sigma_i(x) := \\sigma(x + b_i)$, with $b_i \\in \\mathbb{R}^{n_i}$\nthe bias vector, and $\\sigma : \\mathbb{R} \\rightarrow \\mathbb{R}$ the activation function, applied coordinatewise. In this article, we"}, {"title": "", "content": "set $\\sigma = \\text{ReLU}$ throughout. To avoid clutter we omit the $\\sigma$ for function composition within a feed\nforward neural network unless necessary for emphasis, i.e., we will usually write $AB$ instead of\n$A \\circ \\sigma \\circ B$. When $\\varphi$ is applied to a matrix $X \\in \\mathbb{R}^{n\\times p}$, it is always applied columnwise to each of the\n$p$ columns of $X$. So $\\varphi : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{n_{l+1}\\times p}$. We will also drop the \"feed forward\" henceforth since all\nneural networks that appear in our article are feed forward ones.\n\nAttention. The attention module is known by a variety of other names, usually a combination\nof attention/self-attention module/mechanism, and usually represented as flow charts as in Figure 2.\n\n$\\alpha(X) := V(X) \\text{SoftMax}(K(X)Q(X))$,\nwhere $Q : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{d\\times p}, K : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{d\\times p}, V : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{m\\times p}$ are linear layers, i.e., given by\naffine maps\n\n$Q(X) = A_QX + B_Q, K(X) = A_KX + B_K, V(X) = A_VX + B_V,$\nwith weight matrices $A_Q, A_K \\in \\mathbb{R}^{d\\times n}, A_V \\in \\mathbb{R}^{m\\times n}$, and bias matrices $B_Q, B_K \\in \\mathbb{R}^{d\\times p}, B_V \\in \\mathbb{R}^{m\\times p}$.\nHere we have used the more general affine form of these linear layers as attention modules are\nimplemented in practice,\u00b9 as opposed to the linear form in [45] where the biases are set to zero.\nThe SoftMax in (3) is applied columnwise and outputs a $p\\times p$ matrix.\nThe map $\\alpha$ implements the mechanism of taking a query and a set of key-value pairs to an\noutput. Interpreted in this way, the input $X \\in \\mathbb{R}^{n\\times p}$ is a data sequence of length $p$, with each\ndata point $x_i \\in \\mathbb{R}^n, i = 1,...,p$. The columns of $Q(X)$ and $K(X)$ represent queries and keys\nrespectively note that these are vectors in $\\mathbb{R}^d$ and $d$ is generally much smaller than $m$ or $n$. The\ncolumns of $V(X)$ represent values.\nMore generally, a multihead or $h$-headed attention module is a map $\\alpha : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{mh\\times p}$ given by\n\n$\\alpha(X) = (\\alpha_1(X), ..., \\alpha_h(X))$\nwhere $\\alpha_i : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{m\\times p}$ are attention modules as in (3), $i = 1,...,h$. The reader is reminded of\nour convention in Section 1.1: parentheses denote column, which is why in our constructions we\nwill often the phrase \"stacking $\\alpha_1,..., \\alpha_h$ to obtain $\\alpha$ to mean (5)."}, {"title": "", "content": "Encoder. An encoder block, or more precisely a $h$-head encoder block, is a map $\\varepsilon : \\mathbb{R}^{n\\times p} \\rightarrow\n\\mathbb{R}^{n_{l+1}\\times p}$ obtained by composing the output of a $h$-head attention module $\\alpha : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{mh\\times p}$, with\nan $l$-layer ReLU-neural network $\\varphi : \\mathbb{R}^{mh\\times p} \\rightarrow \\mathbb{R}^{n_{l+1}\\times p}$,\n\n$\\varepsilon = \\varphi \\circ \\alpha$.\nMore generally, an encoder or $t$-layer encoder, $\\mathcal{E}_t : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{n_{t+1}\\times p}$ is obtained by composing $t$\nencoder blocks, i.e.,\n\n$\\mathcal{E}_t = \\varphi_t \\circ \\alpha_t \\circ \\mathcal{E}_{t-1} \\circ \\alpha_{t-1} \\circ \\cdots \\circ \\varphi_1 \\circ \\alpha_1,$\nwhere $\\varphi_i : \\mathbb{R}^{m_i\\times p} \\rightarrow [\\mathbb{R}^{n_{i+1}\\times p}$are neural networks and $\\alpha_i : \\mathbb{R}^{n_i\\times p} \\rightarrow \\mathbb{R}^{m_i\\times p}$ are attention modules,\n$i = 1,...,t, n_1 = n$. In Figure 1, the encoder is the part enclosed within the red dash lines on\nthe left. The structure in (7) appears to require alternate compositions of attention modules and\nneural networks but one may skip some or all of the $\\alpha_i$'s. The reason is that we may choose\nthese $\\alpha_i$'s to be an identity map, which can be represented as a one-layer neural network as $x =$\nReLU$(x) - \\text{ReLU}(-x)$.\nWhile we allow the neural networks appearing in (7) to have multiple hidden layers, the original\nproposed model in [45] requires that they be single-layer. We will show in Lemma 3.7 that these\nare in fact equivalent: Any encoder of the form (7) may be written as one where all $\\varphi_i$'s have only\none hidden layer, but at the expense of a larger $t$.\n\nMasked attention. In many applications of transformers, particularly large language models,\nthe data is of a sequential nature. So the function $f$ we want to learn or approximate is expected\nto be autoregressive [45], i.e., $f : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{m\\times p}$ takes the form\n\n$[x_1,...,x_p] \\mapsto [f_1(x_1), f_2(x_1,x_2),...,f_p(x_1,...,x_p)].$\nIn other words $f_j : \\mathbb{R}^{n\\times j} \\rightarrow \\mathbb{R}^m$ depends only on the first $j$ columns $x_1,...,x_j, j = 1,...,p$. In\ngeneral $f$ will be nonlinear, but when $f$ is linear, then this simply means it is given by an upper\ntriangular matrix. So an autoregressive function may be viewed as a nonlinear generalization of an\nupper triangular matrix.\nTo achieve this property in attention module, we define the function mask : $\\mathbb{R}^{p\\times p} \\rightarrow \\mathbb{R}^{p\\times p}$ by\n\n$\\text{mask}(X)_{ij} = \\begin{cases}X_{ij} & \\text{if } i < j,\\\\-\\infty & \\text{if } i > j.\\end{cases}$\nA masked attention module is then given by\n\n$\\beta(X) = V(X) \\text{SoftMax} (\\text{mask}(K(X)Q(X))).$\nIt is easy to check that a masked attention module is always autoregressive.\n\nDecoder. A decoder block is the analogue of an encoder block where we have a masked\nattention in (6):\n\n$\\delta = \\varphi \\circ \\beta$.\nWe may also replace any or all of the $\\alpha_i$'s in (7) by masked versions $\\beta_i$'s. If we replace all, then\nthe resulting map\n\n$\\Delta_t = \\varphi_t \\circ \\beta_t \\circ \\varphi_{t-1} \\circ \\beta_{t-1} \\circ \\cdots \\circ \\varphi_1 \\circ \\beta_1,$\nis autoregressive but more generally we will just selectively replace some $\\alpha_i$'s with $\\beta_i$'s. We call\nthe resulting map a decoder. Note that the part enclosed within red dash lines in the right-half of\nFigure 1 is not quite a decoder as it takes a feed from the left-half; instead it is an encoder-decoder,\nas we will discuss next."}, {"title": "", "content": "Encoder-decoder attention. The multihead attention in the right-half of Figure 1 accepts\na feed from outside the red dash box. When used in this manner, it is called an encoder-decoder\nattention module [45], as it permits one to use queries from the decoder, but keys and values from\nthe encoder. Mathematically, this is a map $\\gamma : \\mathbb{R}^{n\\times p} \\times \\mathbb{R}^{r\\times p} \\rightarrow \\mathbb{R}^{m\\times p}$,\n\n$\\gamma(X, Y) := V(X) \\text{SoftMax}(K(X)Q(Y)),$\nwhere $Q, K, V$ are as in (4) but while $K, V$ are functions of $X, Q$ is now a function of $Y$. The\nindependent matrix variables $X$ and $Y$ take values in $\\mathbb{R}^{n\\times p}$ and $\\mathbb{R}^{r\\times p}$ respectively. As a result we\nhave to adjust the dimensions of the weight matrices slightly: $A_Q \\in \\mathbb{R}^{d\\times r}, A_K \\in \\mathbb{R}^{d\\times n}, A_V \\in \\mathbb{R}^{m\\times n}$.\nThe encoder-decoder attention is partially autoregressive, i.e., autoregressive in $Y$ but not in $X$,\ntaking the form\n\n$(X, [Y_1,..., Y_p]) \\mapsto [f_1(X, y_1), f_2(X, y_2), ..., f_p(X, Y_1, ..., Y_p)].$\nTransformer. An encoder-decoder block $\\tau : \\mathbb{R}^{n\\times p} \\times \\mathbb{R}^{r\\times p} \\rightarrow \\mathbb{R}^{n_{l+1}\\times p}$ is defined by a multihead\nmasked attention module $\\beta$, a multihead encoder-decoder attention module $\\gamma$, and a neural network\n$\\varphi$, via\n\n$\\tau(X,Y) = \\varphi(\\gamma(X, \\beta(Y))).$\nAn ($s + t$)-layer encoder-decoder is then constructed from an $s$-layer encoder $\\mathcal{E}_s$, and $t$ encoder-\ndecoder blocks given by $\\beta_1, \\gamma_1, \\varphi_1, \\cdots, \\beta_t, \\gamma_t, \\varphi_t$. We define $\\tau_i$ recursively as\n\n$\\tau_i(X,Y) = \\varphi_i(\\gamma_i(\\mathcal{E}_s(X), \\beta_i(\\tau_{i-1}(X, Y))))$\nfor $i = 1,...,t, \\tau_0(X,Y) = Y$. We call $\\tau_t$ the encoder-decoder. For all mathematical intents and\npurposes, $\\tau_t$ is the transformer. As we will see in Sections 1.10 and 1.11, the other components in\nFigure 1 or [45, Figure 1] are extraneous to the operation of a transformer.\nWe stress that the word \"transformer\" is sometimes used to refer to just the encoder or the\ndecoder alone. We choose to make the distinction in our article but many do not. For example,\nGoogle's BERT [16], for Bidirectional Encoder Representations from Transformers, is an encoder\nwhereas OpenAI's GPT [6], for Generative Pretrained Transformer, is a decoder.\n\nReLU-transformer. The definitions in Sections 1.2-1.8 are faithful mathematical transcrip-\ntions of components as described in Vaswani et al. original article [45]. In this section we take a\nsmall departure - replacing every occurrence of SoftMax with ReLU to obtain what is called a\nReLU-transformer. This is not new either but proposed and studied in [3, 48].\nWe begin by defining ReLU-attention modules. They have the same structures as (3), (9), (12)\nexcept that SoftMax is replaced by ReLU, i.e.,\n\n$\\alpha(X) = V(X) \\text{ReLU}(K(X)Q(X)),\\\\$\\beta(X) = V(X) \\text{ReLU}(\\text{mask}(K(X)Q(X))),\\\\$\\gamma(X,Y) = V(X) \\text{ReLU} (K(X)\\overline{Q}(Y)).$\nAn encoder, decoder, or encoder-decoder constructed out of such ReLU-attention modules will\nbe called a ReLU-encoder, ReLU-decoder, or ReLU-encoder-decoder respectively. In particular, a\nReLU-transformer is, for all mathematical intents and purposes, a ReLU-encoder-decoder.\nThese ReLU-activated variants are essentially \"unsmoothed\" versions of their smooth SoftMax-\nactivated cousins in Sections 1.2-1.8. We may easily revert to the smooth versions by a simple\nsmoothing process replace all ReLU-activated attentions by the original SoftMax-activated ones\n(but the neural networks would remain ReLU-activated).\nReLU-transformers work naturally with our claims and proofs in Section 3. Nevertheless, even\nin practice ReLU-transformers can have desirable, possibly superior, features compared to the\noriginal SoftMax-transformers: investigations in [48] provided extensive empirical evidence that\nsubstituting SoftMax with ReLU causes no noticeable loss and occasionally even affords a slight"}, {"title": "", "content": "gain in performance across both language and vision tasks; it is also easier to explain the in-context-\nlearning capability of ReLU-transformers [3].\nMore generally, the use of alternative activations in a transformer is a common practice. There\nare various reasons to replace SoftMax, one of which is to avoid the considerable training cost\nassociated with the use of SoftMax activation. In [30], SoftMax is replaced with a Gaussian kernel;\nin [27], only the normalization part of SoftMax is kept; in [22], it is shown that an activation does\nnot need to map into the probability simplex. Linearized attentions are used in [39], and sparse\nattentions in [37]; these are intended primarily to accelerate the SoftMax operator but they have\nother features too.\n\nLayer normalization and residual connection. Comparing our Figure 1 and [45, Fig-\nure 1], one might notice that we have omitted the \"add & norm\" layers.\nThe \"add\" step, also called residual connection [23], may be easily included in our analysis\nall our results and proofs in Section 3 hold verbatim with the inclusion of residual connection. For\nan encoder block$\\varepsilon : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{n\\times p}$, a residual connection simply means adding the identity map\n$\\iota : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{n\\times p}, X \\mapsto X$, i.e.,\n\n$\\varepsilon + \\iota : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{n\\times p}, X \\mapsto \\varepsilon(X) + X,$\nand likewise for a decoder block $\\delta : \\mathbb{R}^{n\\times p} \\rightarrow \\mathbb{R}^{n\\times p}$. For an encoder-decoder block $\\tau : \\mathbb{R}^{n\\times p} \\times \\mathbb{R}^{r\\times p} \\rightarrow\n\\mathbb{R}^{r\\times p}$, a residual connection simply means adding the projection map $\\pi : \\mathbb{R}^{n\\times p} \\times \\mathbb{R}^{r\\times p} \\rightarrow \\mathbb{R}^{r\\times p}$,\n$(X, Y) \\mapsto Y$, i.e.,\n\n$\\tau + \\pi : \\mathbb{R}^{n\\times p} \\times \\mathbb{R}^{r\\times p} \\rightarrow \\mathbb{R}^{r\\times p}, (X, Y) \\mapsto \\tau(X, Y) + Y.$\nAs will be clear from the proofs in Section 3, all results therein hold with or without residual\nconnection.\nThe \"norm\u201d step, also called layer normalization [2] refers to statistical standardization, i.e.,\nmean centering and scaling by standard deviation of each column vector in $X$. This is an ubiquitous\nprocess routinely performed in just about any procedure involving any data for practical reasons.\nBut this innocuous process introduces additional nonlinearity that does not fit in our framework.\nWe do not consider either of these critical to the workings of a transformer. They are by no\nmeans unique and may be easily replaced with other data standardization process, as shown in [22].\n\nMiscellany. The \"input/output embedding\u201d and \u201cposition embedding\u201d in Figure 1 convert\nsentences or images (or whatever real-world entity the transformer is used for) to an input in $\\mathbb{R}^{n\\times p}$;\nthe \"linear layer\" and \"SoftMax\" in the right half assign probability values to the output. These are\njust auxiliary components necessary in any situation involving human-generated input or requiring\nhuman-interpretable output. They are common to all practical AI models and we do not regard\nthem as part of the transformer architecture."}, {"title": "SPLINES", "content": "This section covers the salient aspects of splines relevant for us. We write $\\mathbb{R"}, ["x_1,...,x_n"], "for the\nring of polynomials with real coefficients in variables $(x_1,...,x_n) =: x$ and $\\mathbb{R}[x_{11},..., x_{np}"]}