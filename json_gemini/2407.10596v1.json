{"title": "An evaluation of CNN models and data augmentation techniques in hierarchical localization of mobile robots", "authors": ["Juan Jos\u00e9 Cabrera", "Orlando Jos\u00e9 C\u00e9spedes", "Sergio Cebollada", "Oscar Reinoso", "Luis Pay\u00e1"], "abstract": "This work presents an evaluation of CNN models and data augmentation to carry out the hierarchical localization of a mobile robot by using omnidireccional images. In this sense, an ablation study of different state-of-the-art CNN models used as backbone is presented and a variety of data augmentation visual effects are proposed for addressing the visual localization of the robot. The proposed method is based on the adaption and re-training of a CNN with a dual purpose: (1) to perform a rough localization step in which the model is used to predict the room from which an image was captured, and (2) to address the fine localization step, which consists in retrieving the most similar image of the visual map among those contained in the previously predicted room by means of a pairwise comparison between descriptors obtained from an intermediate layer of the CNN. In this sense, we evaluate the impact of different state-of-the-art CNN models such as ConvNeXt for addressing the proposed localization. Finally, a variety of data augmentation visual effects are separately employed for training the model and their impact is assessed. The performance of the resulting CNNs is evaluated under real operation conditions, including changes in the lighting conditions.", "sections": [{"title": "1 Introduction", "content": "In the ever-evolving landscape of Artificial Intelligence (AI), Convolutional Neural Networks (CNNs) have become a fundamental pillar of the technology, with disruptive problem-solving capabilities. This kind of neural networks were originally conceived for image recognition tasks, but have quickly transcended their initial boundaries, establishing themselves as a versatile and powerful tool for tackling a wide range of challenges in a variety of domains [1].\nThe increasing use of CNNs can be attributed to their high ability to recognise patterns from different sources of information. This ability has made them essential in a wide variety of applications, from image recognition [2, 3] and object detection [4, 5] to semantic segmentation [6] and even natural language processing [7]. The success of such architectures is based on their ability to extract features from data, which allows solving high-level problems such as visual localization.\nIn this sense, some researchers have addressed visual localization by means of 360 degrees vision sensors due to its relatively low cost and the wide range of information they provide. When capturing images in real-world scenarios, especially in robotics applications, the environmental conditions can vary significantly. Consequently, addressing the visual localization could be particularly challenging due to different phenomena such as changes in illumination conditions. For this reason, understanding and addressing the effects of illumination changes are crucial for developing robust CNN models.\nRelated with the above information, the main objective of this work is to analyze the influence of different visual effects applied to the training data in order to carry out the mapping and localization of a mobile robot, which moves in an indoor environment under real operation conditions. For this purpose, the omnidirectional images captured by a catadioptric vision sensor are used to train a CNN. Both the raw images, and some sets of images obtained after introducing visual effects to the original images in a data augmentation process are considered during the training. In this paper, we have also evaluated the performance of state-of-the-art CNN models when addressing localization through a hierarchical approach. In this sense, the CNN will be adapted and re-trained with a dual purpose: (1) to perform a rough localization step in which the model is used to predict the room from which a test image was captured, and (2) to address the fine localization step, which consists in retrieving the most similar image of the visual map among those contained in the previously predicted room by means of a pairwise comparison between descriptors obtained from an intermediate layer of the CNN. The main contributions of this paper can be summarized as follows."}, {"title": "2 State of the art", "content": "Artificial intelligence (AI) techniques are commonly proposed to address computer vision and robotics problems. Recent works, such as [15], propose a pedestrian detector for Unmanned Aerial Vehicles (UAVs) based on Haar-LBP features combined with Adaboost and cascade classifiers with Meanshift. Another example is [16], which utilizes an autoencoder for the fusion and extraction of multiple visual features from different sensors with the aim of carrying out motion planning based on deep reinforcement learning.\nCNNs have proven to be successful in many practical applications. Well-known architectures, such as GoogLeNet [17], AlexNet [2] and VGG16 [3] have been used as starting points to address new computer vision tasks. Regarding place-recognition,"}, {"title": "3 Methodology", "content": "3.1 Hierarchical Localization Approach\nThis study aims to tackle visual localization through a hierarchical methodology by means of deep learning. The proposed approach (Figure 1) consists of two main steps: an initial stage for rough localization, which consist in identifying the room from which the test image has been captured, and a subsequent phase for fine localization where the position of the robot is obtained by a pairwise comparison between the test image and the visual model that conforms the pre-selected room.\nThe initial step of rough localization is performed using the output of a CNN. The output layer of that CNN is composed by R neurons, each one corresponding to a room (R is the number of rooms or relevant areas in the target environment). Then, a SoftMax activation function is applied and the room prediction is obtained. However, before training the CNN, a dataset of labelled images captured along the target environment is needed. In this case, each image is labelled with the corresponding room information. The CNN is then trained to address the room retrieval task. Once the CNN is appropriately trained for the room classification task, the coarse localization step is performed: a test image $im_{test}$ is fed into the CNN and the output indicates the room $c_i$ in which the image was captured.\nSimultaneously, a holistic descriptor is extracted by flattening the activation map from the last convolutional layer. This descriptor $d_{test}$ is compared with the descriptors $D_{c_i} = \\{d_{c_i,1}, d_{c_i,2},..., d_{c_i,N_i}\\}$, from the visual map of the predicted room $c_i$, where $N_i$ is the number of images in the room $c_i$. Note that the visual map descriptors are also obtained by flattening the last activation map of the same CNN. Then, the distance between the test descriptor $d_{test}$ and each descriptor $d_{ci,j} \\in D_{c_i}$ in the room $c_i$ is calculated (Eq. 1).\n$q_{t_j} = dist(d_{test}, d_{ci,j}), j = 1, ..., N_i$ (1)\nwhere $N_i$ is the number of descriptors in room $c_i$ and $dist$ is the Euclidean distance (Eq. 2)\n$dist(d_{test}, d_{ci,j}) = \\sqrt{\\sum_{i=1}^{m}(d_{test,i} - d_{ci,j,i})^2}$ (2)\nwhere $d_{test} = (d_{test,1}, d_{test,2},..., d_{test,m})$ and $d_{ci,j} = (d_{ci,j,1}, d_{ci,j,2},..., d_{ci,j,m})$ are the descriptors of size m, and $d_{test,i}$ and $d_{ci,j,i}$ are the i-th components of the vectors $d_{test}$ and $d_{ci,j}$, respectively.\nAfter that, a set $q_t = \\{q_{t_1},..., q_{t_{N_i}}\\}\\$ is constructed with the calculated distances. The index k which minimizes the distance in the set $q_t$ is found in Eq. 3. Subsequently, the estimated position ($x_{est}, y_{est}$) corresponds to the position ($x_{c_i,k}, y_{c_i,k}$) from which the image $im_{c_i,k}$ of the visual map (i.e, the image whose descriptor is the retrieved one $d_{c_i,k}$) was captured (Eq. 4). This hierarchical approach ensures both a broad understanding of the scene and precise localization within the identified room, contributing to an effective visual localization strategy. Figure 1 outlines the whole localization process.\n$k = arg \\underset{t}{min}(q_t)$ (3)\n$X_{est} = X_{ci,k}, Y_{est} = Y_{ci,k}$ (4)\n3.2 CNN selection and adaption\nDesigning a Convolutional Neural Network to a address a specific task supposes a big challenge. In the present work, the CNN must be able to predict the room in which an image was captured and embed the input image into a global descriptor to retrieve the exact position within the predicted room. Crafting a CNN from scratch demands both a profound understanding of the specificities involved and access to a sufficiently varied dataset for effective training. Furthermore, as previously demonstrated in [33], in general terms, re-training networks that have been designed for a different objective yields more precise and reliable outcomes in the new task than training from scratch.\nIn light of this information, this research work incorporates several widely recog- nised and tested CNN models, each of which serves as the backbone for our hierarchical localization task. These models cover a diverse range, addressing different architec- tural complexities and capabilities. All of the architectures employed were originally designed for visual object recognition. In this work, the CNN is first used to address the room retrieval problem, which is a similar task:\n3.3 Data Augmentation\nTraining a model involves setting up its parameters to perform a specific task. When a model has many parameters, it requires a sufficiently large number of examples for effective training. However, in practice, the training dataset is often limited. In such cases, data augmentation is a useful solution as it is able to generate new instances by applying various visual effects. This not only helps the model avoid overfitting but also makes it more robust against challenging real-operation dynamic conditions."}, {"title": "4 Results", "content": "4.1 COLD Freiburg Database\nThe current study utilizes images sourced from the Freiburg dataset, a subset of the COLD (COsy Localization Database) database [35]. This dataset contains omnidi- rectional images captured by a robot which follows various paths within a building at Freiburg University. The robot explores diverse spaces such as kitchens, corridors, printer areas, bathrooms, personal offices, and more. Image capture occurs under real- istic operational conditions, including changes in furniture arrangement, the dynamic\npresence of individuals in scenes, and fluctuations in illumination conditions, including cloudy days, sunny days, and nights.\nTo assess the impact of these variations on the localization task, we propose incorporating images taken exclusively on cloudy days as part of the training data. Additionally, a separate dataset comprising cloudy images (distinct from the afore- mentioned one) is employed as test set to evaluate localization performance without illumination changes. Furthermore, to appraise localization under varying illumina- tion conditions, datasets captured on sunny days and at night are utilized as test sets. Beyond the images, the dataset offers ground truth data (obtained via a laser sen- sor), which is exclusively employed in this study to quantify localization errors. The ground truth over the path of the robot has been generated using the laser sensor in a grid-based SLAM technique, in particular, the one described in [36] and [37]. This solution, based on these two papers, can have an error up to 5 cm or 10 cm depending on the grid resolution.\nConcerning the image capture process, the robot acquires images while it moves, introducing potential blur effects or dynamic alterations. Moreover, the chosen envi- ronment has the longest trajectory within the available database and is characterized by extensive windows and glass walls, making visual localization a particularly challenging problem. Consequently, this environment provides ideal conditions for eval- uating the proposed localization methods under real operation conditions and real scenarios.\nThe selected dataset contains images from nine distinct rooms: a kitchen, a bath- room, a printer area, a stairwell, a long corridor and four offices. The cloudy dataset is downsampled to achieve an average distance of 20 cm between consecutive image capture points, resulting in the Baseline Training Dataset comprising 556 images. This dataset serves the dual purpose of training the CNNs and providing a visual map. In addition, a Validation Dataset is used during training and keeps the same propor- tion of images as the Baseline Training set. The Validation Dataset is also sampled at 20-centimetre intervals, but in this case in an interleaved manner with respect to the Baseline Training Dataset in such a way that the images in the baseline and valida- tion datasets are different. In this regard, the validation covers uniformly the whole environment, which is expected to be a robust approach for validation, considering that the retrained CNN must be able to solve the localization problem considering the whole environment. Furthermore, the Baseline Training Dataset undergoes a data augmentation as described in Section 3.3, resulting in six additional training datasets. These datasets will be individually employed to train the CNNs, allowing an explo- ration of the impact of each visual effect on network performance.\n4.2 Implementation details\nIn this work, the CNNs are trained to address the coarse localization or room retrieval stage. As this is a classification task, these networks have been retrained employing a Cross Entropy loss function (Eq. 6).\n$L(y, \\hat{y}) = - \\frac{1}{B}\\sum_{i=1}^{B}\\sum_{j=1}^{R} Y_{ij}log(\\hat{Y}_{ij})$ (6)\nwhere y is the matrix of actual labels and $\\hat{y}$ is the matrix of model predictions, both matrices have size B \u00d7 R, in which B is the number of samples (batch size) and R is the number of classes (rooms), $Y_{ij}$ is 1 if sample i belongs to class j and 0 otherwise, and $\\hat{Y}_{ij}$ is the probability predicted by the model that sample i belongs to class j.\nIn addition, Stochastic Gradient Descent (SGD) with Momentum 0.9 and Learning Rate of 0.001 has been used as optimization algorithm. Furthermore, the training batch size (B) was 16 and the total number of epochs was 30. For every architecture, the network that presents the best validation accuracy for room retrieval during the training is preserved for testing.\n4.3 CNN Backbone Ablation Study\nIn this section, we asses an experimental evaluation of the different CNN models used as backbone presented in 3.2 for both rough and fine localization. As previously stated, the hierarchical localization proposed in this study comprises two distinct steps. The initial stage, rough localization step, involves retraining a model to execute the room retrieval task. Subsequently, the fine localization step utilizes the previously trained CNN to generate holistic descriptors, employing a nearest neighbor search method to estimate the precise position where an image was captured.\n4.3.1 Coarse Localization: Room retrieval\nThis section presents the results derived from the use of different CNNs for the exe- cution of the coarse localization or room retrieval stage. As described in Section 3.2, the CNN models evaluated in this article are AlexNet [2], ResNet-152 [10], ResNeXt- 101 64x4d [11], MobileNetV3 [12], EfficientNetV2 [13] and ConvNeXt Large [14]. The reason why we have selected these models is to cover a wide range of architectures proposed for image classification in the last ten years.\n4.3.2 Fine Localization\nOnce the CNN model is trained for the room retrieval step, it can be used to embed the input image into a global descripor. This facilitates the resolution of the fine localization step through an image retrieval process, in which the descriptor of the test image is compared with the descriptors of the visual map of the previously retrieved room. As in previous subsection, we are going to evaluate the performance of different CNN backbones to address the fine lozalization step.\n$MAE = \\frac{1}{N} \\sum_{i=1}^{N}|(x_i, y_i) \u2013 (\\hat{x_i}, \\hat{y_i})|$ (7)\nwhere $(x_i, y_i)$ is the actual position, $(\\hat{x_i}, \\hat{y_i})$ is the position of the visual map retrieved after the complete localization process, and N is the number of images in the test dataset.\nEach backbone model exhibited similar characteristics in hierarchical localization comparing to room retrieval, since both tasks are correlated. As Figure 3 shows,\n4.4 Data Augmentation Ablation Study\nIn this comprehensive experiment, the investigation is extended to evaluate the influ- ence of both data augmentation effects (illumination and orientation changes) on the performance of the CNN. Due to the existence of a high probability of variations in robot orientation during operation under real operation conditions with respect to the images captured in the visual map, a model should demonstrate robustness to orien- tation changes. To this end, a data augmentation technique is employed that consists in applying 35 different orientation changes to each training image as described in Section 3.3. This augmentation is essential to improve the adaptability of the model to the various orientations encountered in practice.\nSimultaneously, the illumination effects that occur under real operating conditions, a critical aspect for robust visual perception, have been explored in detail. Five spe- cific lighting effects are considered (Section 3.3): spotlights, shadow spots, general brightness/darkness, contrast, and saturation. Each effect is systematically applied individually on the training data set, leading to the creation of distinct augmented training datasets. Using the different effects separately allows a detailed understand- ing of their individual contributions, which sheds light on the importance of each effect in performance.\nIn particular, for each image, the experiment incorporates a detailed approach by applying different levels of spotlights, contrast and saturation (five levels for each), ensuring a thorough assessment of the impact of these factors on the ability of the CNN to adapt to various lighting conditions. In addition, the effect of brightness is metic- ulously explored, with three levels of brightness and three levels of darkness applied to each image. This dual investigation of orientation changes and illumination effects is intended to provide a comprehensive understanding of the robustness of the CNN to cope with real-world challenges, encompassing variations in both spatial orienta- tion and illumination conditions.\n4.4.1 Coarse Localization: Room retrieval\nIn this subsection we use the best CNN architecture obtained in Section 4.3.1, which is ConvNeXt Large. In a similar approach, we have departed from the pre-trained weights for ImageNet Large Scale Visual Recognition Challenge and re-trained the model for the different datasets obtained by the proposed data augmentation.\n4.4.2 Fine Localization\nOnce the ConvNeXt Large model is trained for the room retrieval step, it can be used to embed the input image into a global descripor. This facilitates the resolution of the fine localization step through an image retrieval process, wherein the descriptor of the test image is compared with the descriptors of the visual map. As in previous subsection, we are going to evaluate the performance of different data augmentation effects to address the fine localization step.\nAs shown in Figure 4, training with every augmented dataset result in similar network performance under cloudy illumination conditions for the fine localization task, achieving a mean absolute error around 0,22 meters. The same happens under the night condition, in which the mean absolute error is around 0,27 meters. In this case, the minimum error is obtained by training the network without data augmentation.\nIn contrast, under sunny lighting conditions the mean localization error has a higher variability, similarly to the coarse localization (Table 7). This demonstrates the correlation between the two tasks. Under this condition, the best fine localization result is obtained by training the model with the contrast effect (DA 4) and the worst with saturation (DA 5).\n4.4.3 General comparison with other methods\nFinally, the proposed method is compared with other previous global appearance tech- niques, including the use of single CNN structures ([34, 38]), triplet structures [39] and two classical analytical descriptors: HOG and gist, as described in [28]. Both HOG and gist are only taken into consideration when testing with night and sunny condi- tions, since the conditions of the cloudy test experiment in [28] are different to the conditions in the present work. Table 8 compares all the methods in a global local- ization task, using in all cases the COLD-Freiburg dataset, which is the same dataset used in the previous subsections. This table shows that ConvNeXt Large without data augmentation provides the best results in terms of localization error for cloudy and night conditions. Training with data augmentation does not improve the performance in cloudy conditions. However, it favours the results under sunny conditions. In this illumination condition, the best result is obtained with a triplet VGG16 proposed in [39]."}, {"title": "5 Conclusion", "content": "This study assesses the application of a deep learning technique in addressing hierarchi- cal localization using omnidirectional imaging. The technique involves training a CNN to perform room retrieval, addressed as an image classification problem. Additionally,"}]}