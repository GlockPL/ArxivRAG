{"title": "An evaluation of CNN models and data augmentation techniques in hierarchical localization of mobile robots", "authors": ["Juan Jos\u00e9 Cabrera", "Orlando Jos\u00e9 C\u00e9spedes", "Sergio Cebollada", "Oscar Reinoso", "Luis Pay\u00e1"], "abstract": "This work presents an evaluation of CNN models and data augmentation to carry out the hierarchical localization of a mobile robot by using omnidireccional images. In this sense, an ablation study of different state-of-the-art CNN models used as backbone is presented and a variety of data augmentation visual effects are proposed for addressing the visual localization of the robot. The proposed method is based on the adaption and re-training of a CNN with a dual purpose: (1) to perform a rough localization step in which the model is used to predict the room from which an image was captured, and (2) to address the fine localization step, which consists in retrieving the most similar image of the visual map among those contained in the previously predicted room by means of a pairwise comparison between descriptors obtained from an intermediate layer of the CNN. In this sense, we evaluate the impact of different state-of-the-art CNN models such as ConvNeXt for addressing the proposed localization. Finally, a variety of data augmentation visual effects are separately employed for training the model and their impact is assessed. The performance of the resulting CNNs is evaluated under real operation conditions, including changes in the lighting conditions.", "sections": [{"title": "1 Introduction", "content": "In the ever-evolving landscape of Artificial Intelligence (AI), Convolutional Neural Networks (CNNs) have become a fundamental pillar of the technology, with disruptive problem-solving capabilities. This kind of neural networks were originally conceived for image recognition tasks, but have quickly transcended their initial boundaries, establishing themselves as a versatile and powerful tool for tackling a wide range of challenges in a variety of domains [1].\nThe increasing use of CNNs can be attributed to their high ability to recognise patterns from different sources of information. This ability has made them essential in a wide variety of applications, from image recognition [2, 3] and object detection [4, 5] to semantic segmentation [6] and even natural language processing [7]. The success of such architectures is based on their ability to extract features from data, which allows solving high-level problems such as visual localization.\nIn this sense, some researchers have addressed visual localization by means of 360 degrees vision sensors due to its relatively low cost and the wide range of information they provide. When capturing images in real-world scenarios, especially in robotics applications, the environmental conditions can vary significantly. Consequently, addressing the visual localization could be particularly challenging due to different phenomena such as changes in illumination conditions. For this reason, understanding and addressing the effects of illumination changes are crucial for developing robust CNN models.\nRelated with the above information, the main objective of this work is to analyze the influence of different visual effects applied to the training data in order to carry out the mapping and localization of a mobile robot, which moves in an indoor environment under real operation conditions. For this purpose, the omnidirectional images captured by a catadioptric vision sensor are used to train a CNN. Both the raw images, and some sets of images obtained after introducing visual effects to the original images in a data augmentation process are considered during the training. In this paper, we have also evaluated the performance of state-of-the-art CNN models when addressing localization through a hierarchical approach. In this sense, the CNN will be adapted and re-trained with a dual purpose: (1) to perform a rough localization step in which the model is used to predict the room from which a test image was captured, and (2) to address the fine localization step, which consists in retrieving the most similar image of the visual map among those contained in the previously predicted room by means of a pairwise comparison between descriptors obtained from an intermediate layer of the CNN. The main contributions of this paper can be summarized as follows."}, {"title": "2 State of the art", "content": "Artificial intelligence (AI) techniques are commonly proposed to address computer vision and robotics problems. Recent works, such as [15], propose a pedestrian detector for Unmanned Aerial Vehicles (UAVs) based on Haar-LBP features combined with Adaboost and cascade classifiers with Meanshift. Another example is [16], which utilizes an autoencoder for the fusion and extraction of multiple visual features from different sensors with the aim of carrying out motion planning based on deep reinforcement learning.\nCNNs have proven to be successful in many practical applications. Well-known architectures, such as GoogLeNet [17], AlexNet [2] and VGG16 [3] have been used as starting points to address new computer vision tasks. Regarding place-recognition,"}, {"title": "3 Methodology", "content": "This study aims to tackle visual localization through a hierarchical methodology by means of deep learning. The proposed approach (Figure 1) consists of two main steps: an initial stage for rough localization, which consist in identifying the room from which the test image has been captured, and a subsequent phase for fine localization where the position of the robot is obtained by a pairwise comparison between the test image and the visual model that conforms the pre-selected room.\nThe initial step of rough localization is performed using the output of a CNN. The output layer of that CNN is composed by R neurons, each one corresponding to a room (R is the number of rooms or relevant areas in the target environment). Then, a SoftMax activation function is applied and the room prediction is obtained. However, before training the CNN, a dataset of labelled images captured along the target environment is needed. In this case, each image is labelled with the corresponding room information. The CNN is then trained to address the room retrieval task. Once the CNN is appropriately trained for the room classification task, the coarse localization step is performed: a test image $im_{test}$ is fed into the CNN and the output indicates the room $c_i$ in which the image was captured.\nSimultaneously, a holistic descriptor is extracted by flattening the activation map from the last convolutional layer. This descriptor $d_{test}$ is compared with the descriptors $D_{c_i} = \\{d_{c_i,1},d_{c_i,2},..., d_{c_i, N_i} \\}$ from the visual map of the predicted room $c_i$, where $N_i$ is the number of images in the room $c_i$. Note that the visual map descriptors are also obtained by flattening the last activation map of the same CNN. Then, the distance between the test descriptor $d_{test}$ and each descriptor $d_{c_i,j} \\in D_{c_i}$ in the room $c_i$ is calculated (Eq. 1).\n$q_{t_j} = dist(d_{test}, d_{c_i,j}), j = 1, ..., N_i$ (1)"}, {"title": "3.1 Hierarchical Localization Approach", "content": "This study aims to tackle visual localization through a hierarchical methodology by means of deep learning. The proposed approach (Figure 1) consists of two main steps: an initial stage for rough localization, which consist in identifying the room from which the test image has been captured, and a subsequent phase for fine localization where the position of the robot is obtained by a pairwise comparison between the test image and the visual model that conforms the pre-selected room.\nThe initial step of rough localization is performed using the output of a CNN. The output layer of that CNN is composed by R neurons, each one corresponding to a room (R is the number of rooms or relevant areas in the target environment). Then, a SoftMax activation function is applied and the room prediction is obtained. However, before training the CNN, a dataset of labelled images captured along the target environment is needed. In this case, each image is labelled with the corresponding room information. The CNN is then trained to address the room retrieval task. Once the CNN is appropriately trained for the room classification task, the coarse localization step is performed: a test image imtest is fed into the CNN and the output indicates the room ci in which the image was captured.\nSimultaneously, a holistic descriptor is extracted by flattening the activation map from the last convolutional layer. This descriptor dtest is compared with the descriptors Dci = {dci,1,dci,2,..., dci, Ni } from the visual map of the predicted room ci, where Ni is the number of images in the room ci. Note that the visual map descriptors are also obtained by flattening the last activation map of the same CNN. Then, the distance between the test descriptor dtest and each descriptor dci,j e Dei in the room ci is calculated (Eq. 1).\nqt\u2081 = dist(dtest, der,j), j = 1, ..., Ni\n(1)"}, {"title": "3.2 CNN selection and adaption", "content": "Designing a Convolutional Neural Network to a address a specific task supposes a big challenge. In the present work, the CNN must be able to predict the room in which an image was captured and embed the input image into a global descriptor to retrieve the exact position within the predicted room. Crafting a CNN from scratch demands both a profound understanding of the specificities involved and access to a sufficiently varied dataset for effective training. Furthermore, as previously demonstrated in [33], in general terms, re-training networks that have been designed for a different objective yields more precise and reliable outcomes in the new task than training from scratch.\nIn light of this information, this research work incorporates several widely recog- nised and tested CNN models, each of which serves as the backbone for our hierarchical localization task. These models cover a diverse range, addressing different architec- tural complexities and capabilities. All of the architectures employed were originally designed for visual object recognition. In this work, the CNN is first used to address the room retrieval problem, which is a similar task:\nDesigning a Convolutional Neural Network to a address a specific task supposes a big challenge. In the present work, the CNN must be able to predict the room in which an image was captured and embed the input image into a global descriptor to retrieve the exact position within the predicted room. Crafting a CNN from scratch demands both a profound understanding of the specificities involved and access to a sufficiently varied dataset for effective training. Furthermore, as previously demonstrated in [33], in general terms, re-training networks that have been designed for a different objective yields more precise and reliable outcomes in the new task than training from scratch.\nIn light of this information, this research work incorporates several widely recog- nised and tested CNN models, each of which serves as the backbone for our hierarchical localization task. These models cover a diverse range, addressing different architec- tural complexities and capabilities. All of the architectures employed were originally designed for visual object recognition. In this work, the CNN is first used to address the room retrieval problem, which is a similar task:\n* AlexNet [2]: AlexNet is a pioneering CNN architecture known for its success in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012. Comprising multiple convolutional and fully connected layers, AlexNet laid the foundation for subsequent CNN designs. This network and the following ones were trained to clas- sify the 1.2 million high-resolution images into 1000 different classes. The weights\n* ResNet-152 [10]: ResNet, or Residual Network, introduced the concept of residual learning. This approach is based on skip connections and allows the CNN to learn an identity function. ResNet-152 is a specific variant featuring 152 layers, enabling the model to effectively capture intricate hierarchical features. Although it is com- putationally costly due to its depth, its accuracy and robustness compensate this cost.\n* ResNeXt-101 64x4d [11]: ResNeXt is an extension of the ResNet architecture, emphasizing a cardinality parameter to enhance model capacity. The cardinality is just the number of parallel blocks, that allows to learn various input representa- tions. In this sense, ResNeXt-101 64x4d has a cardinality of 64. By increasing the cardinality, the network can capture a greater diversity of features, enhancing its potential ability to image recognition.\n* MobileNetV3 [12]: MobileNetV3 is designed for efficient mobile and edge com- puting applications. It uses depth-wise separable convolutions to build light weight deep neural networks. This fact makes them specially suitable for scenarios with resource constraints, such as performing the localization in real time by the robot's on-board computer.\n* Efficient NetV2 [13]: EfficientNetV2 is based on the EfficientNet architecture, and uses a technique called compound coefficient to scale up models in a simple but effective manner. It prioritizes model efficiency, achieving remarkable accuracy with fewer parameters compared to traditional CNNs. This makes Efficient NetV2 an attractive choice for applications requiring high accuracy with limited computational resources.\n* ConvNeXt Large [14]: ConvNeXt Large represents a recent advancement in CNN architectures. It leverages a combination of depth-wise separable convolutions, an inverted bottleneck and spatial factorization (\"patchify\"), contributing to improved efficiency and effectiveness in capturing features. Thus, outperforming the previous models in terms of accuracy.\nBy evaluating these diverse CNN models, we aim to comprehensively understand their strengths and weaknesses in the context of scene recognition and localization task. Regarding the room recognition, the final layer of all the architectures needs to be adapted for classifying the images into N categories corresponding to N possible rooms in the target environment (N=9 in the dataset used in the present work, as described in Section 4.1). As for the fine localization, the global descriptor has been extracted by flattening the output of the Average Pooling Layer of each CNN model."}, {"title": "3.3 Data Augmentation", "content": "Training a model involves setting up its parameters to perform a specific task. When a model has many parameters, it requires a sufficiently large number of examples for effective training. However, in practice, the training dataset is often limited. In such cases, data augmentation is a useful solution as it is able to generate new instances by applying various visual effects. This not only helps the model avoid overfitting but also makes it more robust against challenging real-operation dynamic conditions.\nIn previous studies focused on training models for visual localization, various effects like changes in orientation, reflections, alterations in illumination, noise, and occlu- sions were applied [34]. The use of data augmentation has shown to improve model performance. These effects are applied individually or together to each image in the original dataset, and all the generated images are combined into a new augmented training dataset. However, the specific impact of each type of effect on the resulting CNN's performance is not well understood. This study aims to apply different data augmentation effects individually to evaluate their influence on the resulting CNN.\nThe focus of this work is on two categories of visual effects: changes in illumination conditions and changes in orientation. For changes in illumination conditions, the following effects are considered:\n* Spotlights and Shadows: Circular light sources, like bulbs, are common indoors. The proposed approach involves increasing pixel values to simulate higher light intensity (spotlights) and decreasing pixel values to simulate shadows (shadow spots). Spotlights and shadow spots are applied separately for different data aug- mentation options. In our experiments, these bulbs are created with diameters ranging from 15 to 40 pixels. Five kinds of intensities variations are applied. In the first type the intensity is degraded +/- 160 and in the fifth +/- 100.\n* General Brightness and Darkness: Low intensity values of the original images are increased to create brighter images, simulating higher overall illumination (e.g., a sunny day). Conversely, high intensity values are decreased to create darker images, simulating lower light supply (e.g., capturing images at night). Brightness and darkness are applied separately but used for the same data augmentation.\n* Contrast: Image contrast plays a vital role in distinguishing objects in a scene. Images with low contrast tend to have a smoother appearance with fewer shadows and reflections. The contrast is modified following Eq. 5\n$I_s = 64 + c * (I \u2212 64)$ (5)\nwhere $I_s$ is the resulting image, $I$ the original image and $c$ is the contrast factor. For $c > 1$ the contrast increases and $c < 1$ decreases the contrast.\n* Saturation: Color saturation, indicating the color intensity given by pixels, is considered. Lower saturation results in less colorful images, potentially resembling grayscale images for very low saturation. This phenomenon may occur in real envi- ronments and is incorporated into data augmentation. The color saturation can be adjusted by first converting the RGB image to HSV. Then, the saturation channel can be directly modified by multiplying it by a constant factor $s$. If the saturation is multiplied by $s > 1$, the colors become more saturated, whereas if multiplied by $s < 1$, the saturation decreases.\nRegarding changes in orientation, these can occur during image capture when the robot captures images from the same position but with a different orientation. For this data augmentation option, new images are generated for each original image by applying rotations of n degrees, where n = i \u00d7 10\u00b0, i \u2208 [1,35]. Thus, for each original image in the training set, 35 additional images are generated."}, {"title": "4 Results", "content": "The current study utilizes images sourced from the Freiburg dataset, a subset of the COLD (COsy Localization Database) database [35]. This dataset contains omnidi- rectional images captured by a robot which follows various paths within a building at Freiburg University. The robot explores diverse spaces such as kitchens, corridors, printer areas, bathrooms, personal offices, and more. Image capture occurs under real- istic operational conditions, including changes in furniture arrangement, the dynamic\npresence of individuals in scenes, and fluctuations in illumination conditions, including cloudy days, sunny days, and nights.\nTo assess the impact of these variations on the localization task, we propose incorporating images taken exclusively on cloudy days as part of the training data. Additionally, a separate dataset comprising cloudy images (distinct from the afore- mentioned one) is employed as test set to evaluate localization performance without illumination changes. Furthermore, to appraise localization under varying illumina- tion conditions, datasets captured on sunny days and at night are utilized as test sets. Beyond the images, the dataset offers ground truth data (obtained via a laser sen- sor), which is exclusively employed in this study to quantify localization errors. The ground truth over the path of the robot has been generated using the laser sensor in a grid-based SLAM technique, in particular, the one described in [36] and [37]. This solution, based on these two papers, can have an error up to 5 cm or 10 cm depending on the grid resolution.\nConcerning the image capture process, the robot acquires images while it moves, introducing potential blur effects or dynamic alterations. Moreover, the chosen envi- ronment has the longest trajectory within the available database and is characterized by extensive windows and glass walls, making visual localization a particularly challenging problem. Consequently, this environment provides ideal conditions for eval- uating the proposed localization methods under real operation conditions and real scenarios.\nThe selected dataset contains images from nine distinct rooms: a kitchen, a bath- room, a printer area, a stairwell, a long corridor and four offices. The cloudy dataset is downsampled to achieve an average distance of 20 cm between consecutive image capture points, resulting in the Baseline Training Dataset comprising 556 images. This dataset serves the dual purpose of training the CNNs and providing a visual map. In addition, a Validation Dataset is used during training and keeps the same propor- tion of images as the Baseline Training set. The Validation Dataset is also sampled at 20-centimetre intervals, but in this case in an interleaved manner with respect to the Baseline Training Dataset in such a way that the images in the baseline and valida- tion datasets are different. In this regard, the validation covers uniformly the whole environment, which is expected to be a robust approach for validation, considering that the retrained CNN must be able to solve the localization problem considering the whole environment. Furthermore, the Baseline Training Dataset undergoes a data augmentation as described in Section 3.3, resulting in six additional training datasets. These datasets will be individually employed to train the CNNs, allowing an explo- ration of the impact of each visual effect on network performance."}, {"title": "4.1 COLD Freiburg Database", "content": "The current study utilizes images sourced from the Freiburg dataset, a subset of the COLD (COsy Localization Database) database [35]. This dataset contains omnidi- rectional images captured by a robot which follows various paths within a building at Freiburg University. The robot explores diverse spaces such as kitchens, corridors, printer areas, bathrooms, personal offices, and more. Image capture occurs under real- istic operational conditions, including changes in furniture arrangement, the dynamic"}, {"title": "4.2 Implementation details", "content": "In this work, the CNNs are trained to address the coarse localization or room retrieval stage. As this is a classification task, these networks have been retrained employing a Cross Entropy loss function (Eq. 6).\n$L(y, \\hat{y}) = -\\frac{1}{B} \\sum_{i=1}^{B} \\sum_{j=1}^{R} Y_{ij} log(\\hat{Y_{ij}})$ (6)\nwhere y is the matrix of actual labels and \u0177 is the matrix of model predictions, both matrices have size B \u00d7 R, in which B is the number of samples (batch size) and R is the number of classes (rooms), Yij is 1 if sample i belongs to class j and 0 otherwise, and \u0177ij is the probability predicted by the model that sample i belongs to class j.\nIn addition, Stochastic Gradient Descent (SGD) with Momentum 0.9 and Learning Rate of 0.001 has been used as optimization algorithm. Furthermore, the training batch size (B) was 16 and the total number of epochs was 30. For every architecture, the network that presents the best validation accuracy for room retrieval during the training is preserved for testing."}, {"title": "4.3 CNN Backbone Ablation Study", "content": "In this section, we asses an experimental evaluation of the different CNN models used as backbone presented in 3.2 for both rough and fine localization. As previously stated, the hierarchical localization proposed in this study comprises two distinct steps. The initial stage, rough localization step, involves retraining a model to execute the room retrieval task. Subsequently, the fine localization step utilizes the previously trained CNN to generate holistic descriptors, employing a nearest neighbor search method to estimate the precise position where an image was captured."}, {"title": "4.3.1 Coarse Localization: Room retrieval", "content": "This section presents the results derived from the use of different CNNs for the exe- cution of the coarse localization or room retrieval stage. As described in Section 3.2, the CNN models evaluated in this article are AlexNet [2], ResNet-152 [10], ResNeXt- 101 64x4d [11], MobileNetV3 [12], EfficientNetV2 [13] and ConvNeXt Large [14]. The reason why we have selected these models is to cover a wide range of architectures proposed for image classification in the last ten years."}, {"title": "4.3.2 Fine Localization", "content": "Once the CNN model is trained for the room retrieval step, it can be used to embed the input image into a global descripor. This facilitates the resolution of the fine localization step through an image retrieval process, in which the descriptor of the test image is compared with the descriptors of the visual map of the previously retrieved room. As in previous subsection, we are going to evaluate the performance of different CNN backbones to address the fine lozalization step. Figure 3 shows the hierarchical localization error for different backbone models (AlexNet, ResNet-152, ResNeXt-101, MobileNetV3, EfficientNetV2 and ConvNeXt Large) under various lighting conditions (cloudy, night, sunny) and considering jointly the three conditions (global). The errors are measured in meters and are represented by box plots with whiskers, indicating the distribution of the errors. Furthermore, the Mean Absolute Error (Eq. 7) is represented by the black dot and the text displaying the error value. In addition,\n$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |(x_i, y_i) \u2013 (\\hat{x_i}, \\hat{y_i})|$ (7)\nwhere (xi, yi) is the actual position, (\u00eei, \u0177i) is the position of the visual map retrieved after the complete localization process, and N is the number of images in the test dataset.\nEach backbone model exhibited similar characteristics in hierarchical localization comparing to room retrieval, since both tasks are correlated."}, {"title": "4.4 Data Augmentation Ablation Study", "content": "In this comprehensive experiment, the investigation is extended to evaluate the influ- ence of both data augmentation effects (illumination and orientation changes) on the performance of the CNN. Due to the existence of a high probability of variations in robot orientation during operation under real operation conditions with respect to the images captured in the visual map, a model should demonstrate robustness to orien- tation changes. To this end, a data augmentation technique is employed that consists in applying 35 different orientation changes to each training image as described in Section 3.3. This augmentation is essential to improve the adaptability of the model to the various orientations encountered in practice.\nSimultaneously, the illumination effects that occur under real operating conditions, a critical aspect for robust visual perception, have been explored in detail. Five spe- cific lighting effects are considered (Section 3.3): spotlights, shadow spots, general brightness/darkness, contrast, and saturation. Each effect is systematically applied individually on the training data set, leading to the creation of distinct augmented training datasets. Using the different effects separately allows a detailed understand- ing of their individual contributions, which sheds light on the importance of each effect in performance.\nIn particular, for each image, the experiment incorporates a detailed approach by applying different levels of spotlights, contrast and saturation (five levels for each), ensuring a thorough assessment of the impact of these factors on the ability of the CNN to adapt to various lighting conditions. In addition, the effect of brightness is metic- ulously explored, with three levels of brightness and three levels of darkness applied to each image. This dual investigation of orientation changes and illumination effects is intended to provide a comprehensive understanding of the robustness of the CNN to cope with real-world challenges, encompassing variations in both spatial orienta- tion and illumination conditions. As a result of applying these effects, six additional training datasets have been obtained: Augmented Training Dasaset 1 (spotlights), Augmented Training Dasaset 2 (shadows), Augmented Training Dasaset 3 (general brightness/darkness), Augmented Training Dasaset 4 (contrast), Augmented Training Dasaset 5 (saturation) and Augmented Training Dasaset 6 (rotations). Augmented Training Datasets 1, 2, 4 and 5 consist of 3336 images each, whereas Augmented Training Datasets 3 and 6 includes 3892 and 17236 images respectively."}, {"title": "4.4.1 Coarse Localization: Room retrieval", "content": "In this subsection we use the best CNN architecture obtained in Section 4.3.1, which is ConvNeXt Large. In a similar approach, we have departed from the pre-trained weights for ImageNet Large Scale Visual Recognition Challenge and re-trained the model for the different datasets obtained by the proposed data augmentation."}, {"title": "4.4.2 Fine Localization", "content": "Once the ConvNeXt Large model is trained for the room retrieval step, it can be used to embed the input image into a global descripor. This facilitates the resolution of the fine localization step through an image retrieval process, wherein the descriptor of the test image is compared with the descriptors of the visual map. As in previous subsection, we are going to evaluate the performance of different data augmentation effects to address the fine localization step.\nAs shown in Figure 4, training with every augmented dataset result in similar network performance under cloudy illumination conditions for the fine localization task, achieving a mean absolute error around 0,22 meters. The same happens under the night condition, in which the mean absolute error is around 0,27 meters. In this case, the minimum error is obtained by training the network without data augmentation.\nIn contrast, under sunny lighting conditions the mean localization error has a higher variability, similarly to the coarse localization (Table 7). This demonstrates the correlation between the two tasks. Under this condition, the best fine localization result is obtained by training the model with the contrast effect (DA 4) and the worst with saturation (DA 5)."}, {"title": "4.4.3 General comparison with other methods", "content": "Finally, the proposed method is compared with other previous global appearance tech- niques, including the use of single CNN structures ([34, 38]), triplet structures [39] and two classical analytical descriptors: HOG and gist, as described in [28]. Both HOG and gist are only taken into consideration when testing with night and sunny condi- tions, since the conditions of the cloudy test experiment in [28] are different to the conditions in the present work."}, {"title": "5 Conclusion", "content": "This study assesses the application of a deep learning technique in addressing hierarchi- cal localization using omnidirectional imaging. The technique involves training a CNN to perform room retrieval, addressed as an image classification problem. Additionally,"}]}