{"title": "MISSING DATA AS AUGMENTATION IN THE EARTH\nOBSERVATION DOMAIN: A MULTI-VIEW LEARNING APPROACH", "authors": ["Francisco Mena", "Diego Arenas", "Andreas Dengel"], "abstract": "Multi-view learning (MVL) leverages multiple sources or views of data to enhance machine learning\nmodel performance and robustness. This approach has been successfully used in the Earth Obser-\nvation (EO) domain, where views have a heterogeneous nature and can be affected by missing data.\nDespite the negative effect that missing data has on model predictions, the ML literature has used it\nas an augmentation technique to improve model generalization, like masking the input data. Inspired\nby this, we introduce novel methods for EO applications tailored to MVL with missing views. Our\nmethods integrate the combination of a set to simulate all combinations of missing views as different\ntraining samples. Instead of replacing missing data with a numerical value, we use dynamic merge\nfunctions, like average, and more complex ones like Transformer. This allows the MVL model to\nentirely ignore the missing views, enhancing its predictive robustness. We experiment on four EO\ndatasets with temporal and static views, including state-of-the-art methods from the EO domain.\nThe results indicate that our methods improve model robustness under conditions of moderate miss-\ningness, and improve the predictive performance when all views are present. The proposed methods\noffer a single adaptive solution to operate effectively with any combination of available views.", "sections": [{"title": "1 Introduction", "content": "Nowadays, the usage of multiple data sources, sensors, or views in ML models has become a standard prac-\ntice in various applications and domains [Yan et al., 2021]. The reason is that by using multiple sources of in-\nformation, the individual data can be ratified and complemented to enhance predictive models [Hong et al., 2021,\nSainte Fare Garnot et al., 2022]. Earth Observation (EO) is one of the domains where Multi-View Learning (MVL)\nhas been used to provide comprehensive insights in various applications [Mena et al., 2024c]. In our work, we refer to\na view as all features in a specific data source. For example, a view can be an optical or radar Satellite Image Time Se-\nries (SITS), weather conditions, topographic information, or various metadata, consisting of a heterogeneous scenario\nwith different spatio-temporal resolutions. Thus, there are temporal views, with multi-temporal data and static views,\nwith single-date data. This diversity distinguishes research done in EO from other ML domains such as vision and text\n[Rolf et al., 2024]. Furthermore, EO views might not be a persistent source of information as researchers commonly\nassume.\nThe EO domain faces challenges due to the finite lifespan of remote sensors, noise, and cloudy conditions in optical\nsensors [Shen et al., 2015]. Besides, unexpected errors can affect the availability of the data, such as the failure of the\nSentinel-1B satellite in 2021. This problem leads to scenarios with missing data, which hinder accurate predictions\nand introduce biases in ML models [Choi and Lee, 2019, Hong et al., 2021, Sainte Fare Garnot et al., 2022]. For in-\nstance, [Mena et al., 2024a] evidence the negative predictive impact that missing views have in different vegetation\napplications, highlighting the lack of robustness of different MVL models. Even current ML models, like Transform-\ners, are not naturally robust to missing views [Ma et al., 2022, Tseng et al., 2023, Chen et al., 2024]. This leaves open\nquestions such as how to increase the robustness of MVL models to missing views."}, {"title": "2 Related work", "content": "Recently, there has been an increase in EO research using multiple data sources to enhance\nML model predictions [Camps-Valls et al., 2021]. The main difference in the MVL models investigated is how the\ndata is fused [Mena et al., 2024c]. Input-level fusion has been the common choice for this, i.e. merge the data before\nfeeding a ML model. For instance, [Kussul et al., 2017] feed a CNN model with just the concatenation of different\nsensors (multi-spectral and radar images) for land-use classification, while [Ghamisi et al., 2016] concatenate special-\nized hand-crafted features from hyper-spectral and LiDAR images. However, several works have shown that learn-\ning view-dedicated feature extractors (encoders) improves results [Hong et al., 2021, Sainte Fare Garnot et al., 2022,\nFerrari et al., 2023, Mena et al., 2024d]. For example, [Audebert et al., 2018] propose a MVL model for multi-spectral\nand topographic images that fuses across multiple layers of CNN encoders. Later, [Zhang et al., 2020] show that in-\ncluding a fusion in the decision layer (as a hybrid fusion) improves the results for land-use segmentation. Additionally,\n[Ofori-Ampofo et al., 2021] evidence that using specialized encoder architectures for optical and radar SITS benefits\nthe feature-level fusion in a crop-type classification use-case. Furthermore, there have been efforts in exploring geospa-\ntial foundational models using multi-view data. For instance, Presto [Tseng et al., 2023] uses an input-level fusion with\na Transformer model, while SkySense [Guo et al., 2024] uses a feature-level fusion based on view-dedicated models\nthat are fine-tuned based on the available views in the downstream tasks.\nDifferent forms of missing (such as errors and anomalies) are present in EO\ndata [Shen et al., 2015]. As expected, when the missing information in the input data increases (at spectral, spatial,\nor temporal dimensions), the predictions of ML models get worse [Fasnacht et al., 2020, Ofori-Ampofo et al., 2021,\nEbel et al., 2023]. In addition, specific data sources are more relevant than others. For example, the lack of an\noptical view critically affects models' accuracy [Hong et al., 2021, Mena et al., 2024a]. Nevertheless, there is evi-\ndence that when a view is missing, training with additional views can supplement and increase the model robustness\n[Inglada et al., 2016, Hong et al., 2021]. For instance, [Ferrari et al., 2023] and [Sainte Fare Garnot et al., 2022] show\nthat when optical images are missing in SITS due to cloudy conditions, placing the fusion far away from the input\nlayer increases the model robustness. Furthermore, in [Mena et al., 2024a] three techniques that mitigate the effect of\nmissing views in MVL are compared. The first is to impute the missing view with a numerical value. The second"}, {"title": "3 Multi-view learning with missing data", "content": "Given the multi-view input data for a sample i, $X^{(i)} = \\{X_v^{(i)}\\}_{v \\in V}$, with V the set of all views, the objective is to\nfind a MVL model G(\u00b7) that approximates the corresponding target $y^{(i)}$, i.e. $\\hat{y}^{(i)} = G(X^{(i)})$. The views $X_v$ can be\ntemporal (time-series data) or static (single-date data). The learning is through minimizing a loss function of the form\n$L(y^{(i)}, G(X^{(i)}))$, over a training set of N samples, $D = \\{X^{(i)},y^{(i)}\\}_{i=1}^{N}$. In the case of missing views, instead, we\nobserve $X^{(i)} = \\{X_v^{(i)}\\}_{v \\in V^{(i)}}$, with $V^{(i)} \\subseteq V$, the set of available views for a sample i. Then, the number of views\nis m = |V| and $m^{(i)} = |V^{(i)}|$, with $m^{(i)} > 0$. As views in MVL models could be any set of features, we consider a\nview as all features from a data source (e.g. optical, radar, weather). We consider a full-view training scenario, with\nexpected missing views at inference, as illustrated in Figure 1."}, {"title": "3.2 Basis of multi-view learning", "content": "This fusion strategy directly merges the input features of the views. As EO views have\ndifferent (spatio-temporal) resolutions, an alignment step is required to match all the dimensions: $X^{(i)} =$\\nconcat(alignment($X^{(i)}))$ . Then, these merged features are fed to a single ML model: $\\hat{y}^{(i)} = G(X^{(i)})$. However, in\nthis fusion strategy, there is no clear way to deal with missing views, $X^{(i)}$. For instance, [Hong et al., 2021] present\na zero-imputation of the missing data, i.e. $X^{(i)} = X^{(i)} \\cup \\{0\\}_{v \\in V \\setminus V^{(i)}}$. Subsequent research on MAug has used the\nzero-imputation in the missing features [Gawlikowski et al., 2023, Mena et al., 2024b], i.e. augment the training data\nby masking out views with a zero value. Nonetheless, zero is an arbitrary value that creates bias depending on data\nnormalization and transformations applied.\nTo avoid forcing a view-alignment, and have a single model that handles the multi-view infor-\nmation, this strategy extracts high-level features through view-dedicated encoders: $z_v^{(i)} = G_{enc}^{(v)}(X_v^{(i)}) \\forall v \\in V$. In addi-\ntion, a normalization layer (with learnable parameters) is used in each encoder to scale and harmonize the different rep-\nresentations. Then, a merge function combines this information, obtaining a joint representation, $z_F^{(i)} = M(\\{z_v^{(i)}\\}_{v \\in V})$.\nThe merge function M(\u00b7) can take any form, such as concatenation or dynamic functions. Then, a prediction head is\nused to obtain the final prediction: $\\hat{y}^{(i)} = G_{head}(z_F^{(i)})$. In the following, we explain how to handle missing views in\nthe latter fusion strategy."}, {"title": "3.3 Dynamic feature-level fusion", "content": "Inspired by permutation [Lee et al., 2019] and sensor [Francis, 2024, Mena et al., 2024b] invariant models, we rely on\nignoring the encoded features associated to the missing views. For this, the MVL model encodes and merges only the\navailable views:\n$z_v^{(i)} = G_{enc}^{(v)}(X_v^{(i)}) \\forall v \\in V^{(i)}$,\n$z_F^{(i)} = M(\\{z_v^{(i)}\\}_{v \\in V^{(i)}})$.\nwith $z_v^{(i)} \\in R^{d}$ and $z_F^{(i)} \\in R^{d_F}$. However, when the fused dimension ($d_F$) depends on the number of views ($m^{(i)}$), as\nwith concatenation, fusion cannot be dynamic. Thus, we use merge functions that yield the same fused dimension re-\ngardless of the fused views, i.e $d_F = d$. We call these dynamic merge functions. A simple case is a linear combination\nwith the same weight, i.e. average as\n$M(Z^{(i)}) = \\frac{1}{|V^{(i)}|} \\sum_{v \\in V^{(i)}} z_v^{(i)}$.\nFurthermore, we present some alternatives in the following."}, {"title": "Gated fusion", "content": "Instead of using the same weight for all views as in the average, we use a data-driven weighted\nfusion [Choi and Lee, 2019, Mena et al., 2024d]. Considering the encoded features from all views as $Z^{(i)} =$\nstack($\\{z_i\\}_{v \\in V}$) $\\in R^{m \\times d}$, the gated merge function is expressed by\n$M(Z^{(i)}) = \\sum_{v \\in V} softmax (A_v^{(i)}) Z_v^{(i)}$,\nwith $A_v^{(i)}$ the fusion weights. Then, instead of modeling a single fusion weight for all dimensions d in each view,\n$A^{(i)} \\in R^{1 \\times m}$ [Mena et al., 2024d], we use a per-dimension weight in each view, i.e. $A^{(i)} \\in R^{d \\times m}$, calculated as\n$A_v^{(i)} = W_G . flatten(z_v^{(i)}) + b$,\nwith $W_G$ and b learnable parameters. In the case of missing data, the fusion weights of the unavailable views are\nmodified, such as softmax($A_v^{(i)}$) = 0 $ \\forall v \\in V \\setminus V^{(i)}$. With this adaptation of weights, the features of the missing\nviews are ignored in the merge Eq. (4). As the fusion weights require all views to be calculated, Eq. (5), and we\nonly forward over the available views, Eq. (1), during implementation we impute the missing features with zeros,\n$z_v^{(i)}$= 0 $ \\forall v \\in V \\setminus V^{(i)}.$"}, {"title": "Cross-attention fusion", "content": "Inspired by Transformer layers used to fuse EO data [Ma et al., 2022, Chen et al., 2024], we\nuse a learnable parameter, called fusion token, f $\\in R^{d}$ to query the multi-view data. Consider the encoded features\nfrom the available views with the fusion token as $Z^{(i)} = stack(f, \\{z_v^{(i)}\\}_{v \\in V^{(i)}}) \\in R^{(1+m^{(i)}) \\times d}$, the cross-attention\nmerge function is expressed by\n$M(Z^{(i)}) = softmax (A^{(i)}). Z^{(i)} W_v$,\nwith $A^{(i)} \\in R^{(1+m^{(i)}) \\times (1+m^{(i)})}$ the cross-view (and token) attention weights, and $W_V$ a learnable parameter. The\nvalues $A_v^{(i)} \\in R^{1 \\times (1+m^{(i)})}$ are the view-attention weights of the fusion token used to aggregate the views. These\nweights are computed by a self-attention mechanism as follows\n$A^{(i)} = Z^{(i)} W_Q . Z^{(i)} W_k + b$,\nwith $W_Q, W_K$, and b learnable parameters As the matrix computation in Eq. (7) depends exclusively on the available\nviews, the model naturally avoids attending the missing views when merging Eq. (6), i.e. $A_v^{(i)}$ = 0 $ \\forall v \\in V \\setminus V^{(i)}$. We\nuse a multi-head mechanism and stacked layers to increase the learning of cross-view features [Vaswani et al., 2017].\nIn contrast to previous works [Lee et al., 2019, Chen et al., 2024], we include a view-specific positional encoding."}, {"title": "Memory fusion", "content": "Inspired by RNN models used to fuse multi-view EO data [Wang et al., 2021], we employ a\nmemory-based fusion. The memory is updated one view at a time with an empty initial memory, expressed by\n$h_v^{(i)} = R(z_v^{(i)}, h_{v-1}^{(i)})  h_0^{(i)} = 0$,\nwith $R(.)$ a RNN model, and v $\\in \\{1, ..., m^{(i)}\\}$. Then, the memory-based fused vector corresponds to $M(\\{Z_v\\}_{v \\in V^{(i)}}) =$\n$h_{m^{(i)}}^{(i)}$. This means that the fused vector is the memory (or hidden state in the RNN model) after being recursively\nupdated with all views. As the recursive operation Eq. (8) and fused vector is invariant to the number of views given as\ninput, it naturally ignores missing views. Similar to the cross-attention fusion, we stack multiple LSTM layers in R(.)\nto increase the learning of cross-view features. Since RNNs are order-dependent, a random permutation can be used\nto avoid a bias in the order in which the views are given. However, using the proposed MAug technique is enough for\ngeneralization, as shown in the appendix."}, {"title": "3.4 All combinations of missing views", "content": "As previous works have shown, randomly dropping views during training increases the model robustness to missing\nviews [Tseng et al., 2023, Chen et al., 2024]. However, it can negatively affect the model accuracy in the full-view\nscenario [Mena et al., 2024b]. Thus, we consider augmenting the training samples by modeling all combinations of\nmissing views at feature-level. Then, assuming a full-view training set, the augmented features extracted from the i-th\nsample are $\\{\\{z_v^{(i)}\\}_{v \\in V^{(i)}}\\} \\hat{y_j}^{(i)} \\in T$, with T = \\{$V^{(i)} : V^{(i)} \\subseteq V, V^{(i)} \\neq (\\varnothing)\\}$ the augmented list. Here, the number of\npossible combinations is the same as the power set of V minus the no-view case, i.e. |T| = $2^m$ \u2212 1. For instance, for\noptical, radar, and weather views, the augmented list is T ={(optical/radar/weather), (optical/radar), (optical/weather),\n(radar/weather), (optical), (radar), (weather)}. We named this MAug technique as Combinations of Missing views\n(CoM). The usage of CoM during training is illustrated in Algorithm 1.\nWe consider a balanced contribution between the full-view and missing views predictive performance. This means\nthat all augmented samples from T have the same weight in the loss function during training, expressed by\nl = $\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|T|} \\sum_{V^{(i)} \\in T} L(y^{(i)}, G(\\{X_v^{(i)}\\}_{v \\in V^{(i)}}))$,\nwith $\\hat{y_j}^{(i)} = G(\\{X_v^{(i)}\\}_{v \\in V^{(i)}})$. Furthermore, to reduce computational operations of the multiple predictions, we\nforward over the encoders (biggest computation bottleneck) only once, while the fusion and prediction are done |T|\ntimes, see Algorithm 1 for details."}, {"title": "4 Experiments", "content": "We use the following pixel-wise EO datasets with static and temporal views. More details on the feature description\ncan be found in A.1."}, {"title": "4.2 Setup and competing methods", "content": "We named our methods using the CoM technique at feature-level as FCoM-av (for average), FCoM-ga (for gated),\nFCoM-cr (for cross-attention), and FCoM-me (for memory fusion). For the cross-attention fusion (FCOM-cr), the\nfunction consists of one layer with eight heads and 40% of dropout, while for the memory fusion (FCOM-me), it\nconsists of two bidirectional layers with LSTM units and 40% of dropout. Variations in the selection of these hyper-\nparameters are shown in the A.2.\nFor comparison, we consider the following supervised methods related to the EO domain. Three methods that\nperform zero-imputation in the missing views. Two of these use MAug techniques at input-level: ITempD-co\n[Sainte Fare Garnot et al., 2022], using the TempD technique, and ISensD-co [Mena et al., 2024b], using the SensD.\nOne method that uses CoM at feature-level: FCoMl-co [Gawlikowski et al., 2023], a model that merges by concate-\nnation with a weighted loss and prediction that we extend to multiple views. In addition, we include three meth-\nods that ignore the missing views: FSensD-cr, adapted from images [Chen et al., 2024] to pixel-wise time series,\nusing cross-attention fusion at feature-level with the SensD technique, FEmbr-sa [Choi and Lee, 2019], a feature-\nlevel fusion method that randomly samples features from different views in the fused representation, and ESensI-av"}, {"title": "4.3 Results with missing views", "content": "In order to assess the effect when a single view is missing, or it is available, we decide to select a few views that\nindividually are the most effective for the task. For this, we train a model individually on each view to predict the task\nand select the two views that reach the best predictions. We refer to these are the top views. For CropH-b, CropH-\nm, and LFMC these are optical and radar views, as observed in the literature [Hong et al., 2021, Zheng et al., 2021,\nMena et al., 2024a], while for PM25 are dynamic and condition views.\nIn Table 1 we compare the F1 score of the methods in different cases of missing views for the classification tasks.\nWhen there are no missing views or moderate missingness, the best results are obtained by our methods, with FCOM-\nav and FCoM-ga. However, in extreme cases, our methods become comparable to competing methods. In this extreme\nmissingness, the ESensI-av method effectively handles missing views, competing with our methods and outperforming\nwhen only radar view is available. On the other hand, we notice that the methods based on SensD and TempD are\nhighly affected by missing views, i.e. less robust.\nFor the regression tasks, we display the $R^2$ in Table 2. Here, the impact of missing views is more severe than in\nclassification, with most methods reaching negative $R^2$ values in extreme missingness. This difficulty in regression is\nexpected [Mena et al., 2024a] as methods predict a continuous value which disperses in different magnitudes, while\nin classification the change is only binary, see Sec. 4.5 for a visual illustration. Nonetheless, some of our methods\nare robust enough and obtain the best results in extreme cases, e.g. FCoM-me. Moreover, our methods (FCoM-av\nand FCOM-ga) effectively handle the moderate missingness in both datasets. Although the ITempD-co has the best\nresults in the full-view scenario of PM25 data, it has poor robustness, strongly decreasing performance when views"}, {"title": "4.4 Results with a fraction of missing views", "content": "In Figure 2 we display the predictive performance when one top view for prediction is missing in some samples. We\ninclude the results when the other top view is missing in the B.1. We present our two best methods in each dataset."}, {"title": "4.5 Prediction shift due to missing views", "content": "We analyze how model predictions are shifted because of missing views, regardless of the target values. We plot the\nclass change ratio and the deformation score at different percentages of missing views in the CropH-m and LFMC\ndata, respectively, in Figure 3. The deformation is calculated as the difference in prediction divided by the deviation\nof the original prediction, i.e. RMSE($\\hat{y}_{full}, \\hat{y}_{miss}$)/std($\\hat{y}_{full}$). These graphs show the increase in the prediction shifting\nwhen there is more missing data. We observe that the prediction shift curves of our methods are among the three\nmethods with lower values in the comparison, together with FCoMl-co and ESensI-av methods.\nAs qualitative support for our methods, we plot the class change in the CropH-m data in Figure 4, and the shift in the\nreal-value predicted in the LFMC data in Figure 5. We notice that in moderate missingness, the prediction change in\nour methods is insignificant, while in extreme cases is shifted to a greater extent. As the predicted value in classification\nis categorical, the change in prediction is binary (the class change to another one or not), while in the regression task\nthe predicting value is continuous. Therefore, we can see how the prediction is dispersed from the original value to\ndifferent degrees for all samples."}, {"title": "4.6 Execution time comparison", "content": "We compare the execution time of different methods in Figure 6. During training, we observe that the methods\nignoring missing views lead to the most efficient training time, i.e. FSensD-cr, ESensI-av, and FCoM-av. However,\nwe notice that the combination of the CoM technique with sophisticated merge functions (-ga, -cr, and -me suffix)"}, {"title": "5 Analysis and limitation", "content": "We show the effect of applying MAugs at different levels of the MVL models with average fusion in Table 3. Sim-\nilar results are obtained with other merge functions (see B.2). We zero-impute views if they are missing at input-\nlevel, and ignore them if missing at feature-level. We notice a trend to increase the model robustness to miss-\ning views when the CoM is used, compared to SensD and without MAugs. Randomly dropping sensors, used in\n[Chen et al., 2024, Mena et al., 2024b], have good relative robustness but quite low overall performance. In the LFMC\ndata, we find it more effective to address missing views at the input rather than feature level, as noted in the literature\n[Mena et al., 2024a]. However, with the CoM technique, we notice that is better to use it at feature by ignoring than at\ninput-level with zero imputation. Besides, the CoM even allows increasing the full-view performance in the CropH-m\ndata. The evidence suggests that, in most cases, our usage of CoM at feature-level and dynamic fusion is optimal for\nmodel robustness.\nAlong the results, we note a slight variation in which method obtains the best results for each dataset and missing view\nscenario. However, this variability is expected in the EO field, as the data is quite heterogeneous and region-dependent\n[Camps-Valls et al., 2021, Hong et al., 2021, Ofori-Ampofo et al., 2021, Mena et al., 2023, Rolf et al., 2024]. In addi-\ntion, this variation depends on the metrics used to assess the models. In our study, standard performance metrics are\nused from the literature. However, additional values are included in the C. Nevertheless, our combination of CoM with\nthe simple average function (FCoM-av) shows good overall results, without significantly increasing the training time,\nas well as having an adaptive prediction time based on the available views.\nWe remark that Transformer models are not naturally robust to missing data [Ma et al., 2022]. They can handle missing\ndata without intervention, but that does not imply they will obtain the same performance as when there is no missing"}, {"title": "6 Conclusion", "content": "We introduce Missing data as Augmentation (MAug) technique tailored to Multi-View Learning (MVL) with missing\nviews, named Combinations of Missing views (CoM). We apply this technique at feature-level in combination with\ndynamic merge functions. For evaluation, we simulate missing views during inference to assess model robustness\n(predictive performance decay) in two scenarios. Moderate missingness, when only top views (the ones with the best\nindividual performance) are missing, and extreme missingness, when only one top view is available. Our findings show\nthat our approaches outperform competing methods in moderate missingness, particularly with FCoM-av. Moreover,\ndue to the MAug effect, we see an increase in the classification performance in the full-view scenario. In addition,\nwe identify challenging scenarios for robustness, particularly in regression tasks and extreme missingness. For these\nchallenges, future work should consider developing models that operate with any available data at decision-level\nfusion, such as by weighing predictions with missing data."}, {"title": "A Initial setup", "content": ""}, {"title": "A.1 Dataset Description", "content": "We show the features from each view in the different datasets in Tables 4, 5, and 6 for PM25, CropHarvest, and LFMC\ndata respectively. The abbreviations used in the tables correspond to: normalized difference vegetation index (NDVI),\nnormalized difference water index (NDWI), and near infrared vegetation index (NIRv)."}, {"title": "A.2 Architecture Selection", "content": "In Table 7 we compare the MAug techniques to a view-permutation in the memory-based fusion with a LSTM archi-\ntecture. Since views are processed sequentially, the view-permutation is included to prevent the model from overfitting\nthe views order [Lee et al., 2019]. We observe that the usage of the MAug technique has a greater positive effect on\nthe predictive performance than the view-permutation in the full-view scenario and with missing views. This suggests\nthat the memory fusion with the MAug does not require an explicit permutation to become order invariant and increase\ngeneralization. Nevertheless, the good behavior without permutation in LFMC data might be just overfitting, caused\nby the small dataset (less than 2000 samples to train).\nIn Table 8 we compare different architectures of the memory fusion. We notice a tendency to get better results (in\nfull-view and with missing views) with a more complex network architecture. Overall, the best results are obtained\nwith two bidirectional LSTM layers, while the second best results are associated with an architecture based on GRU\nlayers.\nIn Table 9 we compare different architectural options of the cross-attention fusion, based on Transformer layers. We\nobserve an optimal value across datasets and missing scenarios with eight heads and just one layer. Besides, we notice\na slight tendency to get better results when using a more complex network architecture."}, {"title": "A.3 Individual view performance", "content": "In order to detect the top views for prediction in each dataset, we train an individual model on each view. The results\nfor each dataset are shown in Table 10. For CropH-b, CropH-m, and LFMC these are optical and radar views, while for\nthe PM25 dataset the top views are dynamic and condition. We note that the static views usually have a low predictive\nperformance, only serving as a complement to the top views for prediction."}, {"title": "B Additional results", "content": ""}, {"title": "B.1 Another top view missing", "content": "In Figure 7 we display the predictive performance in all datasets when additional views are missing in some samples,\nas a complement to Figure 2. We present our two best methods in each dataset. In the classification tasks, the proposed\nFCoM-ga method has the best predictive performance along the percentage of missing data, as observed when the top\nview is missing (Figure 2). Overall, our methods show the best behavior (of a good balance between a small slope\nand a high value) when increasing the level of samples with missing views. Furthermore, most of the methods have a\nhigh robustness to missing the radar view in the classification tasks. Surprisingly, FCoM-av has a strange behavior in\nthe PM25 data, being the only one greatly affected by missing the precipitation view. Perhaps our method learned a\nprediction quite dependent on this view in that dataset."}, {"title": "B.2 Dynamic merge function comparison", "content": "We analyze the effect of applying two MAug techniques at different levels of the MVL models, similar to Table 3\nthat shows this analysis for average fusion. The Table 11, Tables 12, and Tables 13 include the results when using\ngated fusion, cross-attention fusion, and memory fusion respectively. When views are missing at input-level, they are\nimputed, and when views are missing at feature-level, they are ignored. We notice the same behavior observed for the"}, {"title": "C Results with additional metrics", "content": "We assess the predictive performance with alternative metrics: area under the curve (AUC) of the precision-recall\nplot in classification", "2023": ".", "scenario": "nPRS(y, \u00ddmiss, \u0177full) = exp (1 \u2212RMSE(y, \u0177miss)RMSE(y, \u0177full) ),\nthen it is normalized as PRS = min(1, PRS). The results for the CropH-b, CropH-m, LFMC, and PM25 data are\nin Tables 14, 15, 16, and 17 respectively. We notice that the model robustness cannot be assessed only with relative\nrobustness metrics, such as PRS. This is because the relative metrics hide the overall predictive performance. For\ninstance, a horizontal line behavior in Figure 2, such as from ISensD-co, will get a PRS of one, independently of the\nposition of this line on the y-axis (performance). Even, in some cases, the prediction shift due to missing views can go\ntowards correcting the original prediction, as shown in FCoMl-co in Figure 2. In our work, we include these metrics,\nfor further analysis, but metrics that can mix these concepts could allow a more succinct analysis.\nWe plot the PRS value when different number of samples have the top views missing in Figure 8. We notice a different\nbehavior in this relative score compared to results in Figure 2. In the PRS analysis, the best results are obtained by the\nESensI-av method followed by FCoMl-co. The curve of our methods are between the third and fourth best position\nin this relative score. This reflects that, despite the"}]}