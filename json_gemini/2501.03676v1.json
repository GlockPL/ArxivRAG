{"title": "SALE-Based Offline Reinforcement Learning with Ensemble Q-Networks", "authors": ["zheng chun"], "abstract": "In this work, we build upon the offline reinforcement learning algorithm TD7, which incorporates State-Action Learned Embeddings (SALE) and LAP, and propose a model-free actor-critic algorithm that integrates ensemble Q-networks and a gradient diversity penalty from EDAC. The ensemble Q-networks effectively address the challenge of out-of-distribution actions by introducing penalties that guide the actor network to focus on in-distribution actions. Meanwhile, the gradient diversity penalty encourages diverse Q-value gradients, further suppressing overestimation for out-of-distribution actions. Additionally, our method retains an adjustable behavior cloning (BC) term that directs the actor network toward dataset actions during early training stages, while gradually reducing its influence as the precision of the Q-ensemble improves. These enhancements work synergistically to improve training stability and accuracy. Experimental results on the D4RL MuJoCo benchmarks demonstrate that our algorithm achieves superior convergence speed, stability, and performance compared to existing methods.", "sections": [{"title": "Introduction", "content": "Offline reinforcement learning (Offline RL) has gained popularity in recent years due to its ability to leverage pre-collected offline data without requiring interaction with the environment, thereby avoiding the need to simulate complex real-world environments. In reinforcement learning, agent policies are trained based on action value estimations. However, using online reinforcement learning methods solely on offline data can lead to overestimation of the value of unknown actions (i.e., actions outside the dataset), resulting in the agent learning suboptimal policies that may perform worse than those trained via supervised learning [Torabi et al., 2018, Fu et al., 2020]. This issue typically necessitates interaction with the real environment to resolve. Another challenge faced by Offline RL is instability during training and low training efficiency. To address these problems, researchers have proposed methods such as conservative estimation of unknown actions, sequence modeling, and policy constraints [Kumar et al., 2020, Fujimoto et al., 2019, Chen et al., 2021, Wang et al., 2022, Prudencio et al., 2023, Yang et al., 2022]. However, many state-of-the-art algorithms suffer from excessively long training times and some require meticulous hyperparameter tuning."}, {"title": "Related Work", "content": "To overcome these issues, TD3+BC [Fujimoto and Gu, 2021] proposed that by adaptively learning the behavior policy on the basis of TD3 [Fujimoto et al., 2018], effective policies can be obtained. TD3 is well-known for mitigating value overestimation through its twin Q-networks and delayed policy updates. TD7 [Fujimoto et al., 2024] builds upon TD3 by adding mechanisms such as state-action learning encoders, prioritized replay buffers, and training checkpoints, achieving top performance on both online and offline reinforcement learning benchmarks. However, I have observed that TD7 still exhibits instability during training on offline datasets.\nTo address this problem while maintaining the simplicity inherent to TD3, I incorporated EDAC [An et al., 2021], an ensemble Q-network approach. EDAC increases the diversity of Q-value gradients with respect to actions, reduces the number of Q-networks, and selects the minimum Q-value among the networks to penalize unknown actions, also achieving top performance in offline reinforcement learning tasks. However, EDAC still requires a large number of Q-networks, especially in the Hopper environment, and has excessively long training times, with the authors recommending 3 \u00d7 106 gradient updates.\nCombining these two methods, we propose a new algorithm named EDTD7. In our experiments, we were pleasantly surprised to find that EDTD7 simultaneously resolves the training stability issues of TD7 and the problems of the large number of Q-networks and lengthy training times in EDAC, while ultimately achieving excellent performance. Subsequently, through systematic ablation experiments, we demonstrated the impact of each component on performance, deepening our understanding of the various modifications.\nEnsemble Networks Ensemble Q-networks are effective for penalizing unknown actions by simply increasing the number of Q-networks, leading to strong performance in offline reinforcement learning tasks. However, this approach suffers from low training efficiency and high computational resource requirements. To mitigate this issue, EDAC [An et al., 2021] introduces increased gradient diversity of Q-values with respect to actions, which helps address the problem effectively. Recently, research on ensemble networks has been progressing. For example, [Beeson and Montana, 2024] systematically studied the impact of applying BC+ensemble to SAC [Haarnoja et al., 2018] and TD3, but the issue of training time remains unsolved. [Huang et al., 2024] proposed a method that constrains Q-networks by averaging the Q-values across the ensemble, which also showed good performance. Additionally, [Yang et al., 2022] incorporated regularization of the policy and value function for states near the dataset while using ensemble Q-networks, achieving promising results but requiring substantial hyperparameter tuning. [Wang and Zhang, 2024] combined Q-ensemble networks with TD3 by outputting Q-values as the difference between the mean and the standard deviation of the ensemble Q-values, alongside a Bernoulli sampling-based BC term, yielding favorable experimental outcomes.\nMicro-Design Choices TD3+BC [Fujimoto and Gu, 2021] proposed a simple modification that has shown good results in offline reinforcement learning tasks. Many subsequent works have attempted similar designs to achieve comparable outcomes. [Nikulin et al., 2023] introduced Random Network Distillation (RND) and used Feature-wise Linear Modulation (FiLM) conditioning, achieving excellent performance. [Tarasov et al., 2024] integrated multiple design choices into a model-free algorithm and systematically studied the effects of different designs on performance. [Peng et al., 2023] proposed a weighted pol-"}, {"title": "Background", "content": "icy constraint method to reduce the likelihood of learning suboptimal or inferior actions. However, this approach still faces stability issues during the training process.\nReinforcement learning problems are typically framed within the framework of Markov Decision Processes (MDP)[Sutton, 2018]. An MDP describes the scenario where an agent is in state s at a given time step, takes action a, and interacts with the environment. Based on the transition model p(s'|s, a), the environment generates the next state s', and the agent receives a reward r based on the reward function R(r|s, a). The goal of the agent is to collect the cumulative discounted reward from its current state during the interaction with the environment. The cumulative discounted reward is defined as $G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$, where Gt is the cumulative discounted reward starting from time step t, y is the discount factor (typically 0 \u2264 y \u2264 1), representing the degree to which future rewards are discounted, and rt+k+1 is the reward obtained by the agent at time step t+k+1.In reinforcement learning, the MDP is described by a tuple (S, A, R, p, \u03b3), where S is the state space, A is the action space, R is the reward function, p is the transition model, and y is the discount factor. The goal of the agent is to learn a policy \u03c0 that maximizes the cumulative discounted reward starting from the initial state s0.\nIn reinforcement learning, both Q-networks and policy networks are commonly parameterized using neural networks to handle high-dimensional inputs and outputs. Q-networks approximate the action-value function Q(s, a) and are often implemented as ensemble networks to improve stability and uncertainty estimation. An ensemble of n Q-networks is parameterized as {Q\u03b81,Q\u03b82, ..., Q\u03b8n}, where \u03b8i represents the parameters of the i-th Q-network. The ensemble output is typically computed by taking the minimum value across all networks: Qensemble(s, a) = min i=1 Q\u03b8i (s, a). Similarly, policy networks approximate the optimal action distribution given a state and are parameterized as \u03c0\u03c6(s), where \u03c6 denotes the parameters of the policy network."}, {"title": "Method", "content": "In this section, we will introduce what SALE is, explain how SALE integrates with the ensemble Q-network and the actor network, and finally present the overall loss function.\nThe objective of SALE is to learn a set of embeddings (zsa, z\u00b3) that capture relevant structures in the observation space as well as the transition dynamics of the environment. To achieve this, SALE employs a pair of parameterized encoders (fos, gesa): specifically, fes(s) encodes the states into the state embedding z, while gesa (z, a) jointly encodes the state embedding z\u00aa and action a into the state-action embedding zsa:\nzs := fo(s), zsa := gesa (zs, a) \\qquad (1)\nThe embeddings are designed to capture the structural properties of the environment. However, they may not contain all the information required by the value and policy networks, such as features related to the reward function, the behavior policy, or the task horizon. To address this limitation, [Fujimoto et al., 2024] augment the embeddings by concatenating them with the original state and action. This allows the value and policy"}, {"title": "Experiment Results", "content": "This section will focus on presenting the experimental results of EDTD7 in the D4RL MuJoCo benchmarks [Fu et al., 2020], and provide a brief discussion and summary based on the results of the ablation studies."}, {"title": "D4RL MuJoCo Benchmark", "content": "The results of EDTD7, along with comparisons to other algorithms, are summarized in Table 1. We compare EDTD7 with TD7[Fujimoto et al., 2024], EDAC[An et al., 2021], as well as several other representative and popular algorithms[Torabi et al., 2018, Kumar et al., 2020, Kostrikov et al., 2021]. For each algorithm, we conduct local reproductions and run them for 1 million steps using 4 random seeds, evaluating the policy network during training every 5000 steps by interacting with the environment for 10 episodes. The complete learning curves can be found in Figure 2, which clearly demonstrates that our algorithm exhibits greater stability in comparison to others.In Appendix B.1, we further compare EDTD7 with other contemporary algorithms based on ensemble Q-networks, as well as some of the current state-of-the-art non-ensemble methods."}, {"title": "Ablation Study", "content": "Hyperparameter Study.We experimented with varying the number of ensemble Q-networks N, where N \u2208 {2, 5, 10, 20, 35, 50}, as well as the values of \u03b7 \u2208 {0.0, 1.0, 5.0, 10.0} and the imitation coefficient A\u2208 {0.01, 0.05, 0.1, 0.5}. All results are plotted in Appendix C.1.\nOther Target Value Choices. We also experimented with other target values for the ensemble Q-networks for comparison. For details, please refer to Appendix C.2. It can be observed that other choices may also perform well, but using our current Equation 7 demonstrates relatively stable and competitive performance.\nComponent Ablation.We conducted ablation studies on the three components: SALE, LAP, and the ensemble Q-networks. For details, please refer to Appendix C.3. It can be observed that the combination of these three components significantly improves performance."}, {"title": "Conclusion", "content": "In this work, building upon TD7 and EDAC, we introduce an enhanced offline reinforcement learning algorithm, EDTD7, which integrates ensemble Q-networks and a novel combination of components to improve performance on the D4RL MuJoCo benchmarks. By leveraging ensemble Q-networks, we construct a robust and stable value estimation framework, which significantly reduces variance during evaluation and enhances overall performance. Additionally, we conduct extensive ablation studies to analyze the contributions of key components, including SALE, LAP, and the ensemble Q-networks, demonstrating that their combination leads to substantial performance improvements.\nOne of the key strengths of EDTD7 is its simplicity and efficiency. Despite its straightforward implementation, EDTD7 achieves competitive results compared to state-of-the-art offline RL algorithms, while maintaining low computational overhead. The algorithm's ability to balance exploration and exploitation through ensemble-based uncertainty estimation is a critical factor in its success. Furthermore, our hyperparameter study provides insights into the optimal configuration of ensemble size, regularization terms, and imitation coefficients, offering practical guidance for future applications.\nA notable limitation of EDTD7 is the need to preset certain hyperparameters, such as the ensemble size N and the imitation coefficient \u03bb. While these parameters can be tuned through empirical studies, their optimal values may vary across different tasks and\n1We chose the v4 environment because it is the latest version, easier to deploy, and more widely adopted for benchmarking. The v2 environment is older, and all algorithms were tested in the same environment to ensure fairness and avoid bias."}]}