{"title": "CORPA: Adversarial Image Generation for Chest X-rays Using Concept Vector Perturbations and Generative Models", "authors": ["Amy Rafferty", "Rishi Ramaesh", "Ajitha Rajan"], "abstract": "Deep learning models for medical image classification tasks are becoming widely implemented in AI-assisted diagnostic tools, aiming to enhance diagnostic accuracy, reduce clinician workloads, and improve patient outcomes. However, their vulnerability to adversarial attacks poses significant risks to patient safety. Current attack methodologies use general techniques such as model querying or pixel value perturbations to generate adversarial examples designed to fool a model. These approaches may not adequately address the unique characteristics of clinical errors stemming from missed or incorrectly identified clinical features. We propose the Concept-based Report Perturbation Attack (CoRPA), a clinically-focused black-box adversarial attack framework tailored to the medical imaging domain. CoRPA leverages clinical concepts to generate adversarial radiological reports and images that closely mirror realistic clinical misdiagnosis scenarios. We demonstrate the utility of CORPA using the MIMIC-CXR-JPG dataset of chest X-rays and radiological reports. Our evaluation reveals that deep learning models exhibiting strong resilience to conventional adversarial attacks are significantly less robust when subjected to CoRPA's clinically-focused perturbations. This underscores the importance of addressing domain-specific vulnerabilities in medical AI systems. By introducing a specialized adversarial attack framework, this study provides a foundation for developing robust, real-world-ready AI models in healthcare, ensuring their safe and reliable deployment in high-stakes clinical environments.", "sections": [{"title": "I. INTRODUCTION", "content": "AI-assisted diagnostic systems have emerged as a transformative technology in medical imaging, offering significant potential to enhance diagnostic accuracy, reduce workload, and improve patient outcomes. These systems leverage deep learning algorithms to analyze medical images, such as chest X-rays, CT scans, and MRIs, for the detection and classification of various pathologies with performance often comparable to or exceeding that of human experts [1]-[3]. The integration of AI into diagnostic workflows holds the promise of faster, more consistent interpretations, assisting radiologists in identifying early signs of disease, optimizing treatment plans, and reducing diagnostic errors [4], [5]. However, concerns regarding their robustness and interpretability highlight the need for continued research to ensure their safe deployment in high-stakes clinical settings.\nThe concept of adversarial attacks was first introduced in 2014 by a ground-breaking study [6] which demonstrated that small, imperceptible perturbations to input data could mislead deep learning models, causing significant misclassification rates even in cases where these perturbations were imperceptible to the human eye. This work revealed concerning vulnerabilities in neural networks and sparked significant research into adversarial machine learning and the security of AI systems. Adversarial attacks can take various forms, including white-box attacks, where the attacker has full access to the model, and black-box attacks, where the attacker only has access to the model's inputs and outputs [7].\nNumerous studies have shown that adversarial examples can effectively manipulate deep learning systems across various clinical domains [8], [9]. These attacks typically follow general-purpose approaches, such as generating small data perturbations, submitting numerous queries to estimate model parameters, or utilizing substitute models with transferability [10]. Although such methods are commonly used in computer vision, they may not adequately address the unique characteristics of clinical data, and the semantic meanings of complex clinical features, leading to the generation of adversarial examples that do not accurately represent real-world scenarios such as diagnostic errors. To address this gap, we propose the development of a specialized adversarial attack framework tailored to the medical domain, that facilitates model evaluation with respect to misinterpreted or missed clinical features in images.\nIn this work, we introduce the Concept-based Report Perturbation Attack (CoRPA), a novel, clinically-focused black-box adversarial attack specifically designed for the medical imaging domain. CoRPA leverages clinical features, or concepts, associated with specific pathologies in the dataset to generate concept vectors for each image-report pair. These concept vectors are deliberately perturbed to simulate noisy, incorrectly identified, or missing clinical features in a radiograph, replicating real-world scenarios that could lead to misdiagnosis."}, {"title": "II. BACKGROUND", "content": "Adversarial attacks exploit vulnerabilities in machine learning models by introducing subtle perturbations to input data, which are often imperceptible to humans but can lead to significant model misclassification rates [6] [14]. These attacks pose critical challenges in high-stakes applications such as medical diagnostics, where errors in AI-assisted diagnostic systems may result in substantial risks to patient safety. Such misclassifications undermine trust in the reliability and utility of automated diagnostic systems [15].\nAdversarial attacks are typically categorized based on the attacker's knowledge of the target model and the attack's overall objective [16]. Depending on the goal, attacks can be classified as targeted, where the aim is to produce specific misclassifications, or untargeted, which aim to broadly disrupt model accuracy.\nWhite-box attacks assume complete access to the model's architecture, parameters, and training data. These attacks are highly effective [17] and include several widely studied techniques in the literature, such as Basic Iterative Method (BIM) [18], DeepFool [19], and the Carlini & Wagner (C&W) attack [17]. Among the most prevalent methods are the Fast Gradient Sign Method (FGSM) [14] and Projected Gradient Descent (PGD) [20]. FGSM calculates the gradient of the loss with respect to the input image, and generates an adversarial image by perturbing the input in the direction of the gradient sign. PGD, considered one of the strongest first-order attacks, iteratively seeks perturbations that maximize the model's loss while constraining their magnitude within a predefined limit.\nIn contrast, black-box attacks assume that the attacker has no access to the target model's internal details and largely rely on external methods and approximations to craft adversarial examples. These attacks are much more realistic - in a real-world environment, especially for a medical diagnostic model, it is very unlikely that an attacker will have access to the model's internal information [8]. These include transfer-based techniques such as Iterative Fast Gradient Sign Method (I-FGSM) [18] and Skip Gradient Method (SGM) [21], which leverage the transferability property by using a surrogate model to generate adversarial examples, and score-based techniques such as Simple Black-Box Attack (SimBA) [22] and Natural Evolution Strategy (NES) [23], which estimate gradients through repeated model queries.\nGiven the significant threat posed by adversarial examples, extensive research has been devoted to developing defense mechanisms to mitigate their impact. Adversarial training [20] involves fine-tuning the model on adversarial examples to enhance its robustness. Input transformations [24] aim to reduce the noise introduced by adversarial perturbations before the data is processed by the model. Randomization strategies [25] add stochasticity to the model's inference process, making it more resilient to adversarial attacks. Model ensembles [26] leverage multiple models with diverse architectures to improve overall robustness by reducing the likelihood of all models being simultaneously compromised.\nIn medical diagnostics, adversarial attack and defense strategies are extensively applied to medical image classification models to evaluate and enhance their robustness [10]. However, these methods are predominantly developed within a general computer vision framework, and have no clinical context for what they are perturbing. Therefore, the generated adversarial examples may fail to accurately represent realistic threats that a model could encounter in a clinical setting.\nThis study introduces CoRPA, a clinically-focused novel"}, {"title": "III. MATERIALS", "content": "This section introduces the dataset utilized in this study, including the pre-processing and label annotation methodologies used. It also provides an overview of the classification model architectures whose robustness we examine, evaluated through both our proposed untargeted black-box adversarial technique (CORPA) and other commonly-used attack methods.\nA. Dataset\nWe use the public anonymized MIMIC-CXR-JPG dataset from PhysioNet [11]\u2013[13], consisting of chest X-rays and corresponding free-text radiological reports. The dataset is pre-processed to include only chest X-rays with associated reports. To minimize confounding variables [48], we focus exclusively on images acquired from the standard Posteroanterior (PA) viewpoint, excluding alternative perspectives such as Anteroposterior (AP) and lateral views. Following this filtering process, the dataset consists of 85,872 unique imagereport pairs. The images are resized to 512 pixels to optimize computational efficiency and address storage constraints.\nAlthough MIMIC-CXR-JPG provides pathology label annotations, generated automatically through both CheXpert [49] and NegBio [50], these NLP-based labels have been found to be unreliable [51] [52]. We instead label the dataset ourselves using clinical concept vectors; the approach for this is detailed in Section III-B. Based on these pathology annotations, we further filter our dataset to contain only image-report pairs belonging to the following labels, derived from the original MIMIC-CXR-JPG label set: Healthy (No Finding), Cancer (Lung Lesion), Cardiomegaly, Pleural Effusion, Pneumonia and Pneumothorax. These six pathology labels were selected from the original set of fourteen in the MIMIC-CXR-JPG\nB. Dataset Labelling\nMIMIC-CXR-JPG provides NLP-generated labels from radiology reports using CheXpert [49] and NegBio [50]. However, these labels often exhibit high false-positive rates due to misinterpretation of context and negations [52]. Instead of using these, we annotate the data using clinical concept vectors.\nA consultant radiologist with over 15 years of experience analyzed a subset of radiological reports to identify words and phrases indicative of the six pathology labels (Healthy, Cancer, Cardiomegaly, Pleural Effusion, Pneumonia, and Pneumothorax)."}, {"title": "IV. CORPA: CONCEPT-BASED REPORT PERTURBATION ATTACK", "content": "This section presents CORPA (Concept-based Report Perturbation Attack), a clinically-focused, untargeted black-box adversarial methodology that uses a text-to-image Stable Diffusion model to generate adversarial medical images from adversarial radiological reports, generated through perturbations to clinical concept vectors. The CoRPA pipeline for our dataset of chest X-rays and associated reports (Figure 2) is as follows:\n1) For each chest X-ray - report pair, a clinical concept vector is generated. This vector captures the presence of pre-defined clinical concepts within the report text, accounting for contextual factors such as negations. See Section III-B.\n2) Four random perturbations of the concept vector are created: two inter-class perturbations and two outer-class perturbations, or four outer-class perturbations in the case of single-concept classes (Healthy, Cardiomegaly).\n3) Adversarial reports are reconstructed using the perturbed concept vectors.\nA. Concept Vector Perturbations\nWe propose two types of concept vector perturbations, each designed to simulate realistic medical scenarios involving adversarial inputs, primarily arising from interpretative challenges in radiological reports for chest X-rays. Chest radiography, despite being the most commonly performed imaging examination globally, remains susceptible to frequent interpretation errors [66]. By implementing these perturbations, we aim to capture such behaviours systematically.\nInter-class perturbations modify only the concepts associated with the pathology label of the chest X-ray. For instance, as illustrated in Figure 2, a chest X-ray labelled with Pleural Effusion originally characterized by the concepts Fluid and Effusion could be perturbed to instead include Effusion and Meniscus Sign. This approach reflects scenarios where radiologists identify alternative but valid indicators of the same pathology, and accommodates for variations in the descriptive language used by different radiologists for similar findings.\nOuter-class perturbations introduce concepts from a second, randomly selected pathology into the chest X-ray. For example, as shown in Figure 2, a chest X-ray labelled with Pleural Effusion might be augmented with the concept Infection, which belongs to the Pneumonia pathology class. This perturbation type emulates situations where radiologists misdiagnose a condition, a phenomenon well-documented in the literature due to the notable rates of false positives and negatives in chest X-ray interpretations [67] [68].\nWe perturb concept vectors using the following algorithm. All random generations use Python's random package with seed 2. Concept vectors are binary, where 1 denotes the presence of a concept and 0 indicates its absence. \nInput:\n\u2022 V: Binary concept vector corresponding to a chest X-ray labelled with a given class L.\n\u2022 Kinter = 2: Number of inter-class perturbations.\n\u2022 Kouter = 2: Number of outer-class perturbations.\nOutput:\n\u2022 V'inter: Set of Kinter inter-class perturbed vectors.\n\u2022 V'outer: Set of Kouter outer-class perturbed vectors.\nNote that classes Healthy and Cardiomegaly have only one clinical concept, and therefore inter-class perturbations are impossible. In these cases, we set $K_{inter}=0$ and $K_{outer}=4$.\nTo generate inter-class perturbations, we first identify the concepts within vector V which correspond to class L. We generate perturbed vectors by randomly modifying only these elements of V, leaving the rest of the vector unchanged. The perturbed vector must satisfy two conditions: it must differ from the original vector, and at least one concept corresponding to class L must remain active. Each perturbed vector is validated to ensure that it belongs to the valid concept vector list for class L based on the dataset, before it is added to V'inter. This process is repeated until Kinter valid perturbations are obtained.\nFor outer-class perturbations, we leave the elements of V corresponding to class L unchanged, and instead randomly select a new target class L'. Perturbed vectors are generated by randomly modifying the elements of V associated with class L', ensuring that at least one concept corresponding to class L' is active. Each perturbed vector is again validated to ensure that it belongs to the valid concept vector list for class L' before it is added to V'outer, and the process is repeated until Kouter valid perturbations are created.\nB. Adversarial Report Generation\nThe perturbed concept vectors are then reconstructed into radiological reports. We first generate a sentence-to-concept mapping. For each concept c in our predefined set of 17 clinical concepts, we identify five unique sentences, randomly chosen from the original test set reports, where only c, or c along with one additional concept, are set to 1 in the corresponding concept vector.\nThis mapping facilitates the insertion and removal of sentences within a report, enabling the generation of adversarial reports that align with the target perturbed concept vectors. The following process is executed for each perturbed concept vector produced in the previous CoRPA step. We retain a mapping file linking perturbed vectors to their respective original reports to allow for correct data linking.\nInput:\n\u2022 R: Original radiological report\n\u2022 V: Original concept vector for R\n\u2022 P: Perturbed concept vector generated from V\nOutput:\n\u2022 R': Adversarial report generated from sentence manipulation.\nInitially, the original report R is converted into a list of cleaned sentences, and the corresponding concept vectors for each sentence are computed as detailed in Section III-B."}, {"title": "V. RESULTS", "content": "In this section, we evaluate CoRPA through two distinct approaches. First, we assess the classification performances"}, {"title": "VI. CONCLUSION", "content": "In this study, we introduced the Concept-based Report Perturbation Attack (CoRPA), a clinically focused black-box adversarial attack framework designed to mimic real-world clinical scenarios that could lead to diagnostic errors, such as missed diagnoses, clinical variability, and misinterpretation. By leveraging clinical concepts in radiology reports and generative models, CoRPA generates realistic adversarial chest X-ray images and radiological reports that exploit domain-specific vulnerabilities in deep learning models.\nOur evaluation on the MIMIC-CXR-JPG dataset revealed that CoRPA exposes significant gaps in the robustness of several state-of-the-art deep learning architectures, which otherwise exhibit strong resilience to conventional adversarial attacks. This demonstrates the necessity of domain-specific adversarial testing for medical AI systems, as traditional approaches may fail to reflect real-world clinical mistakes.\nThis work lays a foundation for future research in clinically focused robustness testing, contributing to safer and more reliable AI deployments in medical diagnostics.\nA. Limitations and Future Work\nThe primary limitations of CoRPA include its dependence on medical datasets with both images and corresponding radiological reports, as well as the need for clinician input"}]}