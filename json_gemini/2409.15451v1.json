{"title": "Tag Map: A Text-Based Map for Spatial Reasoning and Navigation with Large Language Models", "authors": ["Mike Zhang", "Kaixian Qu", "Vaishakh Patil", "Cesar Cadena", "Marco Hutter"], "abstract": "Large Language Models (LLM) have emerged as a tool for robots to generate task plans using common sense reasoning. For the LLM to generate actionable plans, scene context must be provided, often through a map. Recent works have shifted from explicit maps with fixed semantic classes to implicit open vocabulary maps based on queryable embeddings capable of representing any se- mantic class. However, embeddings cannot directly report the scene context as they are implicit, requiring further processing for LLM integration. To address this, we propose an explicit text-based map that can represent thousands of se- mantic classes while easily integrating with LLMs due to their text-based nature by building upon large-scale image recognition models. We study how entities in our map can be localized and show through evaluations that our text-based map lo- calizations perform comparably to those from open vocabulary maps while using two to four orders of magnitude less memory. Real-robot experiments demon- strate the grounding of an LLM with the text-based map to solve user tasks.", "sections": [{"title": "1 Introduction", "content": "Scene understanding is fundamental for robots to generate actionable plans within their environment. A popular task for evaluating scene understanding is Object Goal Navigation (ObjectNav) [1, 2], where the robot must navigate to a target object within an unknown scene. Two components have emerged as recurring aspects in recent works on ObjectNav. First, a Large Language Model (LLM) which plans navigation goals, and second, a map that serves as a memory module storing observed objects and which can ground the reasoning of the LLM."}, {"title": "2 Related Work", "content": "Map representations in previous works can be classified as either explicit or implicit. Explicit maps label locations as a class from a fixed set of classes [3, 4, 5]. This limits the tasks the map can handle but has the advantage of directly reporting the classes it contains. In contrast, implicit, or open vocabulary maps build upon the line of work in computer vision starting from CLIP [6], later extending to object detection [7, 8] and segmentation [9, 10, 11, 12], based on vision embeddings that are queryable by arbitrary text. Recent works in ObjectNav have considered maps where a location stores an embedding instead of a label [13, 14]. In theory, such maps can represent any objects, rooms, and affordances. In practice, there is no guarantee that the embeddings will capture the desired information. Moreover, embeddings cannot directly report what entities they represent. Instead, they must be queried to retrieve potential matches. For simple tasks along the lines of \"go to <thing>\", such maps can be directly queried for <thing>. However, additional processing is needed to identify relevant queries for more complex tasks lacking an explicit goal object or region.\nWe propose a text-based map that can represent an extensive amount of semantic classes explicitly by leveraging multi-label image classification models trained to recognize thousands of semantic classes. Our map called the tag map, is a memory-efficient unstructured database storing viewpoints and the recognized entities for each viewpoint as text tags. Despite the minimal amount of infor- mation contained within, we find that tag maps can produce 3D localizations for both objects and rooms/regions with sufficient precision and recall to serve as useful navigation goals. Moreover, the explicit text nature of the tag map lends itself well for use in grounding LLMs.\nOur main contributions are the following:\n\u2022 A text-based map, building on top of large image classification models, capable of representing thousands of semantic classes from common and rare objects to rooms and regions.\n\u2022 A simple, yet effective, method to localize in 3D, the semantic classes from the text-based map.\n\u2022 A method for grounded LLM spatial reasoning over the text-based map to generate plans from given user task specifications.\nThrough quantitative experiments, we demonstrate that the localizations from our map perform com- parably in precision and recall against state-of-the-art open vocabulary maps while storing orders of magnitudes less information. Real-world robot experiments demonstrate the effectiveness of our map at grounding a LLMs to reason from task descriptions and generate feasible navigation plans."}, {"title": "3 Text-Based Map Representation", "content": "The core of our approach is the text-based tag map which stores the unique entities (tags) recognized by an image tagging model and relates them to the viewpoints they were recognized from. Its data structure can be implemented as a hash table, where the keys are the unique recognized entities and the values are references to the corresponding viewpoints. Each viewpoint stores a unique ID, its camera pose, and a depth statistic used later for localization. We emphasize that the viewpoint RGB and depth images are not stored in the map. An overview of our framework is shown in Fig. 2."}, {"title": "3.1 Map Construction", "content": "Building a tag map from RGB-D images and poses of a set of viewpoints is shown in Fig. 2. From the depth image, we compute the mean and median depth, along with the 80th quantile depth value representing the far plane distance of the viewpoint frustum. A filtering step checks the mean and median depth and discards the frame if it is considered a close-up view. The RGB im- age is then forwarded through a tagging model ensemble, where each ensemble member receives a different centered cropped version of the image that removes a small portion of the edges. The final tags are the tags agreed on by the ensemble, filtering out false positive tags due to poorly ob- served objects around the edges of an image. Effects of the depth and crop ensemble filters are visualized in Fig. 3. The tags, poses, and viewpoint frustum distances are registered in the tag map."}, {"title": "3.2 Coarse-Grained Localization", "content": "Given a tag, multi-view consistency over the tag's corresponding viewpoints is used to generate localized regions in the tag's near vicinity, as demonstrated in Fig. 4. The procedure is similar to Space Carving [52], where viewpoint intersections are used to reconstruct 3D geometry. Unlike Space Carving, viewpoints in the tag map may be generated from partial views of an entity."}, {"title": "3.3 Grounding Large Language Models", "content": "We opt for the simple method of grounding the LLM by appending the list of unique tags from the tag map into the prompt of the LLM, as shown in Figure 2. The LLM is additionally given access to an API for requesting information from the tag map. The API exposes the following functions:\n\u2022 localize_tag(tag): Returns the proposals for the tag along with their confidence levels.\n\u2022 region_region_dist(r1, r2): Computes the distance between regions r1 and r2.\n\u2022 point_region_dist(p, r): Computes the distance to reach region r from point p.\nThe API enables the LLM to reason spatially about the tags in its prompt. For example, given a query of \"go to the kitchen fridge\", the LLM can localize \u201ckitchen\" and \"fridge\" and set the navigation goal to be the fridge proposal with a small distance to a kitchen proposal."}, {"title": "4 Evaluating Coarse-Grained Localizations", "content": "The coarse localizations produced by our framework generally do not precisely contain the localized entity as seen in Fig. 5. However, such localizations can still be useful navigation goals for reaching the entities. Evaluating coarse localizations using metrics such as Intersection Over Union (IoU)"}, {"title": "4.1 Metrics for Evaluating Coarse-Grained Localizations", "content": "Our goal is to produce a proposal P that is useful for locating the desired entity. In other words, if the robot reaches the region P, it should be close enough to an instance of the desired entity $E \\in E$. This is aligned with the objective of the Habitat ObjectNav Challenge [2] benchmark, where an agent is successful in navigating to a goal object if the distance between the agent and the nearest goal object is below 1.0 m\u00b9 and the object can be viewed with basic yaw and pitch movements of the camera. We quantify the usefulness of P as the expected length of the shortest path an agent must travel from P to reach any desired entity instance in the set E. We denote this quantity as the Proposal to Entities Distance (P2E) defined as\n$P2E = \\frac{E[mind(p, E)]}{p~PLEEE} $(1)\nwhere d(,) is the shortest path length function. We can evaluate P2E over a set of proposals and classify proposals with P2E under a set threshold (e.g. 1.0 m) as relevant. We refer to the portion of relevant proposals for that threshold as the Precision at Threshold and use it to measure the precision of that set of proposals.\nSymmetrically, we define the Entity to Proposals Distance (E2P) which evaluates for an entity in- stance E the shortest expected path length from E to reach any proposal for that entity. For a set of entity instances, we compute the portion with E2P below a threshold for a given set of proposals as a measure of the recall for that set of proposals, which we denote as the Recall at Threshold.\nThe expected shortest path length is difficult to compute exactly so we approximate it by constructing a grid graph over the scene that avoids collisions with the scene geometry, allowing the computation of approximate shortest paths. Nodes of the grid graph are assigned to the proposals and entity instances and the expectation is approximated by averaging over the assigned nodes. Computing the approximations of P2E and E2P is equivalent to computing the Directed Average Hausdorff Distance (DAHD) [54], a metric commonly used in medical image segmentation."}, {"title": "4.2 Evaluation Implementation", "content": "Evaluations are performed on the Matterport3D dataset [55]. The evaluation is separated into object and room/region evaluations. Object evaluation is done on the 21 common object classes defined by the Habitat ObjectNav Challenge [2]. For regions, we evaluate all classes in Matterport3D except for \"other room\u201d, \u201cjunk\u201d, \u201cno label\", \"outdoor\u201d, \u201centryway/foyer/lobby\u201d, and \u201cdining booth\u201d. These classes were ignored as they did not refer to actual rooms or corresponded to ambiguously labeled regions. When evaluating a tag map, each class label is mapped to a list of corresponding tags. Proposals are generated for all the corresponding tags that exist in the tag map and are grouped to form the set of proposals for the class. Unless stated otherwise, we report the precision and recall over all classes by averaging the precision and recall across classes. Further details on the implementation of the evaluation are found in Appendix D."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Comparison with Embedding Based Maps", "content": "Tag map localizations are compared against localizations from open vocabulary embedding-based maps using the proposed precision and recall metrics. We chose to compare against OpenScene [23] for 3D segmentation using a point cloud representation with an embedding for each point, and OpenMask3D [25] for 3D instance segmentation which stores 3D instance masks with embeddings"}, {"title": "5.2 Comparison with CLIP Viewpoint Retrieval", "content": "The recent work of Chang et al. [5] proposed to use store viewpoint CLIP embeddings and to search over the embeddings when given an object query. We modify our coarse localization method to retrieve viewpoints using CLIP instead of using the relevant tags. To make the comparison fair, for both methods, we retrieve only the top K most confident viewpoints. For tags, we use the tag prediction confidences from the tagging model. We also include the unmodified tag localization method as a baseline, denoted as \"all views\u201d. Precision and recall comparisons for the two methods at different thresholds are reported in Fig. 7. Retrieval via tags consistently outperforms CLIP"}, {"title": "5.3 Ablation of the Map Construction Modules", "content": "We ablate the tagging model, the crop ensemble filter, and the depth filter. We compared the tagging models RAM [50] and its successor RAM++ [51], finding that RAM++ did not outperform RAM in our evaluations. This is likely because RAM++ is designed to improve the recognition of rare classes and our evaluation is limited to more common classes from Matterport3D. The depth filter was found to have an insignificant impact on the evaluation performance. We believe this is because the depth filtering removes uninformative views which often generate incorrect tags that are generally unrelated to the classes considered in the evaluation (see Fig. 3). Removing the crop ensemble filter results in a notable drop in precision with a smaller gain in recall, suggesting that the crop ensemble filter provides a favorable tradeoff between filtering false positive tags and over-filtering true positives. Ablation results over object classes are reported in Table 3 while region class results are reported in Appendix A.1."}, {"title": "5.4 Experiments of Grounded Navigation on the Real Robot", "content": "Integration of the tag map with an LLM for grounded navigation was demonstrated on the legged robot ANYmal [56]. A dataset from a lab/office scene was collected and viewpoint poses were computed using COLMAP [57, 58] to construct the tag map and a scene mesh. The mesh is used for visualization and to build a pose graph connecting neighboring viewpoints in the tag map that have a traversable path to each other. The pose graph produces global path plans that are tracked by a local planner based on Cao et al. [59]. We used GPT-4 [60] as the LLM and leveraged its function calling capabilities to integrate it with the tag map API. The LLM is allowed to make multiple function calls before giving its response. This allows the LLM to, for example, localize relevant tags using localize_tag() and then reason about them spatially using region_region_distance() in a single response. This can be thought of as the LLM generating scene graph annotations between the localized proposals. We also prompt the LLM to consider the localization confidence levels such that it will prefer selecting more confident localizations to navigate to.\nA user is given a chat interface for conversing with the LLM which allows the LLM to ask the user for additional clarifying information and for the user to provide follow-up tasks building on top of previous queries. Fig. 8 demonstrates examples of grounded navigation addressing tasks given by a user. We focus on tasks where the relevant entities for solving the task are not explicitly mentioned and must be inferred by the LLM through the tag map context. In addition to the robot demonstrations, a more thorough evaluation of the Tag Map grounded navigation pipeline over a set of test user queries is presented in Appendix F."}, {"title": "6 Conclusions, Limitations, and Future Work", "content": "This work presented a text-based map that can represent an extensive set of entities, including rare objects and regions, and coarsely localize them in 3D. Being text-based allowed the map to seam- lessly ground an LLM through its prompt. Interfacing the LLM's function calling with the map enabled further spatial reasoning by the LLM on the localized entities. The tagging model used in this work was prone to producing false positive tags which risked polluting the context given to the LLM. However, the false positive tags were often spurious concepts having little to do with the more practical tasks we tested and were mostly ignored by the LLM. Though sometimes the LLM gen- erated infeasible plans due to such tags. For the evaluations conducted on Matterport3D, we were limited to the viewpoints available in the dataset. It remains an open question if different viewpoint collection strategies can improve the map's localizations. Additionally, we only considered static scenes and did not consider cases where entities could have moved, leaving this for future work."}, {"title": "A Additional Results", "content": ""}]}