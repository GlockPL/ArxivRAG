{"title": "Episodic Novelty Through Temporal Distance", "authors": ["Yuhua Jiang", "Qihan Liu", "Yiqin Yang", "Xiaoteng Ma", "Dianyu Zhong", "Hao Hu", "Jun Yang", "Bin Liang", "Bo Xu", "Chongjie Zhang", "Qianchuan Zhao"], "abstract": "Exploration in sparse reward environments remains a significant challenge in reinforcement learning, particularly in Contextual Markov Decision Processes (CMDPs), where environments differ across episodes. Existing episodic intrinsic motivation methods for CMDPs primarily rely on count-based approaches, which are ineffective in large state spaces, or on similarity-based methods that lack appropriate metrics for state comparison. To address these shortcomings, we propose Episodic Novelty Through Temporal Distance (ETD), a novel approach that introduces temporal distance as a robust metric for state similarity and intrinsic reward computation. By employing contrastive learning, ETD accurately estimates temporal distances and derives intrinsic rewards based on the novelty of states within the current episode. Extensive experiments on various benchmark tasks demonstrate that ETD significantly outperforms state-of-the-art methods, highlighting its effectiveness in enhancing exploration in sparse reward CMDPs.", "sections": [{"title": "1 INTRODUCTION", "content": "Exploration in sparse reward environments remains a significant challenge in reinforcement learning (RL). Recent approaches have introduced the concept of intrinsic motivation (Meyer & Wilson, 1991; Oudeyer et al., 2007) to encourage agents to explore novel states, yielding promising results in sparse reward Markov Decision Processes (MDPs) (Bellemare et al., 2016; Pathak et al., 2017; Burda et al., 2018; Machado et al., 2020). Most existing methods grounded in intrinsic motivation derive rewards from the agent's cumulative experiences across all episodes. While these methods are effective in singleton MDPs, where agents are spawned in the same environment for each episode, they exhibit limited generalization across environments (Henaff et al., 2022). Real-world applications are often more suitably represented by Contextual MDPs (CMDPs) (Henaff et al., 2023), where different episodes correspond to different environments that nevertheless share certain characteristics, such as procedurally-generated environments (Chevalier-Boisvert et al., 2023; Cobbe et al., 2020; K\u00fcttler et al., 2020) or embodied AI tasks requiring generalization across diverse spaces (Savva et al., 2019; Li et al., 2021; Gan et al., 2020; Xiang et al., 2020). In CMDPs, the uniqueness of each episode indicates that experiences from one episode may offer limited insights into the novelty of states in another episode, thereby necessitating the development of more effective intrinsic motivation mechanisms.\nTo address the challenges of exploration in CMDPs, where episodes differ significantly, several works have introduced episodic bonuses (Henaff et al., 2023). These bonuses are derived from experiences within the current episode, avoiding the generalization limitations of cross-episode rewards. These approaches can typically be divided into two lines: count-based (Raileanu & Rockt\u00e4schel, 2020; Flet-Berliac et al., 2021; Zha et al., 2021; Zhang et al., 2021b; Parisi et al., 2021; Zhang et al., 2021a; Mu et al., 2022; Ramesh et al., 2022) and similarity-based (Savinov et al., 2018; Badia et al., 2020; Henaff et al., 2022; Wan et al., 2023). Count-based methods rely on an episodic count term to generate positive bonuses once encountering a new state but struggle in large or continuous state spaces (Lobel"}, {"title": "2 BACKGROUND", "content": "We consider a contextual Markov Decision Process (CMDP) defined by $(S, A, C, P, r, \\mu_c, \\mu_s, \\gamma)$, where S is the state space, A is the action space, C is the context space, $P : S \\times A \\times C \\rightarrow \\triangle(S)$ is the transition function, $r(s_t, a_t, s_{t+1})$ is the reward function and typically sparse, $\\mu_s$ is the initial state distribution conditioned on the context, $\\mu_c$ is the context distribution, and $\\gamma\\in (0, 1)$ is the reward discount factor. At start at each episode, a context c is sampled from $\\mu_c$, followed by an initial state $s_0$ sampled from $\\mu_s(\\cdot|c)$, and subsequent states are sampled from $s_{t+1} \\sim P(\\cdot|s_t, a_t, c)$. The goal is to optimize a policy $\\pi : S \\rightarrow \\triangle(A)$ so that the the expected accumulated reward across over all contexts $E_{c\\sim\\mu_c,s_0\\sim\\mu_s(\\cdot]c)}[\\sum_t\\gamma^tr(s_t, a_t, s_{t+1})]$ is maximized.\nExamples of CMDPs include procedurally generated environments (Chevalier-Boisvert et al., 2023; Cobbe et al., 2020; K\u00fcttler et al., 2020; Hafner, 2022), where each context c serves as a random seed for environment generation. Similarly, Embodied AI environments (Chevalier-Boisvert et al., 2023; Savva et al., 2019; Gan et al., 2020), where agents navigate various simulated homes, are also examples of CMDPs. Notably, singleton MDPs ($|C| = 1$) represent a special case of CMDPs. We primarily focus on CMDPs with $|C| = \\infty$.\nTo address the sparse reward challenges, we augment the reward function r by adding an intrinsic reward bonus. The modified equation is $r(s_t, a_t, s_{t+1}) = r^e_t + \\beta\\cdot b_t$, where $r^e_t$ represents the sparse extrinsic reward and $b_t$ denotes the intrinsic reward at each timestep t. The hyperparameter $\\beta$ controls the influence of the intrinsic reward."}, {"title": "3 LIMITATIONS OF CURRENT EPISODIC BONUSES", "content": "Recent intrinsic motivation methods (Andres et al., 2022; Henaff et al., 2022; 2023) have demonstrated that incorporating an episodic bonus is crucial for solving sparse reward CMDP problems. For instance, high-performing methods like NovelD(Zhang et al., 2021b) often depend on an episodic count term to be effective in CMDPs. However, these count-based methods face challenges in large or noisy state spaces. When each state is unique, the episodic bonus becomes less meaningful as it assigns the same value to all states. This issue is evident in the \"noisy-TV\" problem (Burda et al., 2018), where random noise disrupts the state. We validated this observation using the MiniGird-DoorKey 16x16 experiment. When no noise was present in the environment, NovelD performed well. However, when Gaussian noise with a mean of 0 and variance of 0.1 was added to the state input, NovelD failed completely, as indicated by the corresponding curve"}, {"title": "4 METHODS", "content": "In this section, we introduce Episodic Novelty through Temporal Distance (ETD), an algorithm designed to enhance exploration in CMDPs. The core innovation of ETD is using a temporal distance quasimetric to measure state similarity, encouraging the agent to explore states that are temporally distant from its episodic memory. As summarized in Figure 3, we employ a specially parameterized contrastive learning method to learn this temporal distance and then identify the minimum temporal distance between the current state and all previously encountered states in the episodic memory. This minimum distance serves as the intrinsic reward for the current state. The next two sections will elaborate on the details."}, {"title": "4.1 TEMPORAL DISTANCE LEARNING", "content": "Temporal distance can be intuitively understood through the transition probability between states, where a lower probability indicates a larger distance. For a given policy $\\pi$, we define $p^{\\pi}(s_t = y|s_0 = x)$ as the probability of reaching state y at time step t when starting from x. The transition probability can be described using a discounted state occupancy measure, which equals a geometrically weighted average of the probabilities:\n$p(s_f = y|s_0 = x) = (1 - \\gamma) \\sum_{k=0}^{\\infty} \\gamma^k p^\\pi(s_k = y|s_0 = x)$."}, {"title": "4.2 TEMPORAL DISTANCE AS EPISODIC BONUS", "content": "Our approach maximizes the temporal distance between newly visited and previously encountered states within the current episode. At each time step t, we assign a larger intrinsic reward to states that are temporally distant from the episodic memory. Formally, the episodic temporal distance bonus is defined as:\n$b^{ETD}(s_t) = \\min_{k\\in[0,t)} d_{\\phi}(s_k, s_t)$,\nwhere ${d_{\\phi}(s_k, s_t)}$ represents the learned temporal distances between the current state $s_t$ and all previous states ${s_k}$ in the episodic memory. The minimum distance is used as the episodic intrinsic reward. In terms of computational efficiency, storing CNN-extracted embeddings in episodic memory minimizes memory overhead. Additionally, concatenating memory states allow all temporal distances to be computed in a single neural network inference, ensuring high time efficiency.\nConnections to previous intrinsic motivation methods. Many previous episodic intrinsic reward methods, such as DEIR (Wan et al., 2023), NGU (Badia et al., 2020), GOBI (Fu et al., 2023), and EC (Savinov et al., 2018), also rely on episodic memory and past states to calculate rewards. Compared to these methods, our reward formulation is notably simpler. Both EC and GoBI use reachability to assess state similarity, which is similar to our approach. However, EC struggles to learn temporal distance accurately, as shown in Figure 2(b). Meanwhile, GoBI depends on a world model's lookahead rollout to estimate temporal distance, which results in high computational complexity."}, {"title": "4.3 FULL ETD ALGORITHMS", "content": "We implement our quasimetric network $d_{\\phi}$ using MRN (Liu et al., 2023), which allows us to generate an asymmetric distance. Detailed structural information about the network is provided in the Appendix E.1.2. The potential network $c_{\\psi}$ is a multi-layer perceptron (MLP) that shares the same CNN backbone as the MRN. The complete algorithm is outlined in Algorithm 1."}, {"title": "5 EXPERIMENTS", "content": "To evaluate the capabilities of existing methods and assess ETD, we aim to identify CMDP environments that present challenges typical of realistic scenarios, such as sparse rewards, noisy or irrelevant features, and large state spaces. We consider three domains, including the Minigrid (Chevalier-Boisvert et al., 2023) and its noisy variants, as well as high-dimensional pixel-based Crafter (Hafner,"}, {"title": "5.1 MINIGRID ENVIRONMENTS", "content": "MiniGrid (Chevalier-Boisvert et al., 2023) features procedurally generated 2D environments tailored for challenging exploration tasks. In these environments, agents interact with objects such as keys, balls, doors, and boxes while navigating multiple rooms to locate a randomly placed goal. The agents receive a single sparse reward upon completing each episode. We chose four particularly challeng-ing environments: MultiRoom, DoorKey, KeyCorridor, and ObstructedMaze. In the MultiRoom environment, the agent's task is relatively straightforward, requiring navigating through a series of interconnected rooms to reach the goal. DoorKey presents an increased difficulty, as the agent must first find and pick up a key and then open a door before reaching the goal. KeyCorridor is even more demanding, requiring the agent to open multiple doors, locate a key, and then use it to unlock another door to access the goal. ObstructedMaze is the most complex of all: the key is hidden within a box, a ball obstructs the door, and the agent must find the hidden key, move the ball, open the door, and finally reach the goal. Further details on these tasks can be found in the Appendix.\nFrom Figure 5, we can observe that PPO fails to learn effectively without intrinsic reward. Addi-tionally, the global intrinsic reward method RND is almost ineffective, which might be due to the ineffectiveness of global exploration experience in CMDP. We also notice that NGU with episodic intrinsic reward performs poorly, possibly because NGU is primarily designed for Atari tasks, with its episodic intrinsic reward tailored for that domain. NovelD, on the other hand, performs quite well, largely due to its episodic count mechanism, and the global bonus in the current NovelD also contributes to its performance. Meanwhile, Count, which only uses the episodic count of NovelD as the reward, achieves relatively good results on certain maps, such as Obstructed-2Dlh, but it still significantly lags behind NovelD. Pure episodic intrinsic reward methods like DEIR, E3B, and EC all perform relatively well, but their similarity measurements lack precision. In contrast, our ETD method, which leverages temporal distance, significantly improves exploration learning efficiency.We did not compare it with GoBI because it requires pretraining a world model, and the rollout costs of the world model are pretty high. Moreover, we observed that the convergence rate of our learning curves is already significantly faster than what was demonstrated in the GoBI paper."}, {"title": "5.2 MINIGRID ENVIRONMENTS WITH NOISE", "content": "To better simulate realistic scenarios, we introduced noise into the states of MiniGrid, resulting in stochastic dynamics and ensuring that no two states are identical. The noise is generated as Gaussian noise with a mean of 0 and a variance of 0.1, which is then directly added to the states. We compared the ETD method with three effective methods for MiniGrid: DEIR, NovelD, and E3B. The results are presented in Figure 6.\nOur results indicate that NovelD, a count-based method, completely failed to effectively guide explo-ration, as the episodic rewards based on counts no longer provided useful information. In contrast, similarity-based methods such as E3B and DEIR continued to perform reasonably well. However, our approach provided a more accurate assessment of state similarity by utilizing temporal distance. Even in the presence of noise, temporal distance effectively represented the similarity between two states, while the inverse dynamics representation learning used in E3B and the discriminative representation learning used in DEIR could not perfectly measure the distance between states, allowing our method to outperform both E3B and DEIR."}, {"title": "5.3 ABLATIONS OF ETD", "content": "Representation Learning To further illustrate the effectiveness of temporal distance as an intrinsic re-ward, we compare the ETD with the Euclidean dis-tance within both inverse dynamics and discriminator representation learning contexts. Discriminator rep-resentation learning, introduced in DEIR, resembles contrastive learning and predicts whether two states and an action are part of a truly observed transition. While all these techniques utilize ETD as a form of intrinsic reward, they differ in evaluating similarities between states."}, {"title": "5.4 PIXEL-BASED CRAFTER AND MINIWORLD MAZE", "content": "To evaluate the scalability of our method to continuous high-dimensional pixel-based observations, we conducted experiments on two pixel-based CMDP benchmarks: Crafter and MiniWorld.\nCrafter (Hafner, 2022) is a 2D environment with randomly generated worlds and pixel-based observa-tions (64x64x3), where players complete tasks such as foraging for food and water, building shelters and tools, and defending against monsters to unlock 22 achievements. The reward system is sparse, granting +1 for each unique achievement unlocked per episode and a -0.1/+0.1 reward based on life points. With a budget of 1 million environmental steps, Crafter suggests evaluating performance using both the success rate of 22 achievements and a geometric mean score, which we adopt as our performance metric. Additionally, we conducted experiments without life rewards, as they often hindered learning efficiency.\nMiniWorld (Chevalier-Boisvert et al., 2023) is a procedurally generated 3D environment simulator that offers a first-person, partially observable view as the primary form of observation. We focused on the MiniWorld-Maze, where the agent must navigate through a procedurally generated maze. Exploration in this environment is particularly challenging due to the 3D first-person perspective and the limited field of view. Additionally, no reward is given if the agent fails to reach the goal within the time limit, further increasing the difficulty.\nWe compared ETD against DEIR, NovelD, and PPO without intrinsic rewards. As illustrated in Figure 10 and Figure 11, ETD consistently outperformed or matched the baseline algorithms, demonstrating its superior ability to address CMDP challenges with high-dimensional pixel-based observations."}, {"title": "6 RELATED WORK", "content": "Intrinsic Motivation in RL Exploration driven by intrinsic motivation has long been a key focus in the RL community (Oudeyer et al., 2007; Oudeyer & Kaplan, 2007). Various methods that"}, {"title": "7 CONCLUSION", "content": "In this work, we introduce ETD, a novel episodic intrinsic motivation method for CMDPS. ETD leverages temporal distance as a measure of state similarity, which is more robust and accurate than previous methods. This allows for more effective calculation of intrinsic rewards, guiding agents to explore environments with sparse rewards. We demonstrate that ETD significantly outperforms exist-ing episodic intrinsic motivation methods in sample efficiency across various challenging domains, establishing it as the state-of-the-art RL approach for sparse reward CMDPs."}, {"title": "A LIMITATIONS", "content": "Using reward bonuses, whether within an episode or globally, violates the MDP assumption where the reward rt depends only on the immediately preceding state st and action at. Even if the original RL task is an MDP, introducing a reward bonus that depends on the episode history implicitly transforms the task into a POMDP setting. This transformation is problematic because the value function based on the Markovian state might be biased. Although reward bonuses have proven highly effective for exploration in sparse reward settings, further research is needed to mitigate the impact of the POMDP transformation. Additionally, the successor distance we used may be infinite in non-ergodic settings, implying an assumption that we're dealing with ergodic MDP problems.\nAnother limitation of this work is that, our intrinsic reward is purely episodic and doesn't incorporate global bonuses from all experiences, limiting its application in singleton MDPs where global bonuses are essential. We believe designing a combined approach using temporal distance for both global and episodic bonuses is a promising direction. A potential method could involve designing a global bonus through maximum state entropy Seo et al. (2021); Liu & Abbeel (2021); Bae et al. (2024) and integrating it with ETD. We leave this exploration for future work."}, {"title": "B THEORETICAL PROPERTIES OF SUCCESSOR DISTANCE", "content": "Here we list the most relevant properties of successor distance (Myers et al., 2024), which we used in the this paper as the temporal distance. The definition of successor distance (Myers et al., 2024) is independent of \u03c0, whereas our successor distance is dependent of \u3160 and learned in an on-policy manner. We also provide proof that our on-policy form of successor distance still satisfies the quasimetric property, which differs from (Myers et al., 2024).\nProposition 1. For all $\\pi \\in \\Pi$, x, y $\\in$ S, define the random variable $H^{\\pi}(x, y)$ as the smallest transit time from x to y, i.e., the hitting time of y from x,\n$d^{SD}_{(\\mathcal{P}, \\gamma)}(x, y) =  \\log E [\\gamma^{H^{\\pi}(x,y)}]$.\nProof. Starting from state x and given $H^{\\pi}(x,y) = h)$, let $p^{\\pi}(s_t = y|s_0 = x,H^{\\pi}(x,y) = h)$ denotes the probability of reaching state y at the time t, we have\n$p^{\\pi} (s_t = y | s_0 = x, H^{\\pi}(x, y) = h) = \\begin{cases}\n    0 & \\text{if } t < h \\\\\n    p^{\\pi} (s_t = y | s_h = y) & \\text{if } t \\geq h.\n\\end{cases}$\nAnd thus,\n$\\begin{aligned}\np^{\\pi} (s_f = y | s_0 = x) &= (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t p^{\\pi} (s_t = y | s_0 = x) \\\\\n&= (1 - \\gamma) \\sum_{t=0}^{\\infty} \\sum_{h=0}^{\\infty} \\gamma^t p^{\\pi} (s_t = y | s_0 = x, H^{\\pi}(x,y) = h) P (H^{\\pi} (x, y) = h) \\\\\n&= (1 - \\gamma) \\sum_{h=0}^{\\infty} P (H^{\\pi} (x,y) = h) \\sum_{t=0}^{\\infty} \\gamma^t P (s_t = y | s_0 = x, H^{\\pi}(x, y) = h) \\\\\n&= (1 - \\gamma) \\sum_{h=0}^{\\infty} P (H^{\\pi} (x,y) = h) \\sum_{t=h}^{\\infty} \\gamma^t p^{\\pi} (s_t = y | s_0 = y) \\\\\n&= \\sum_{h=0}^{\\infty} P (H^{\\pi} (x,y) = h) \\Big( (1 - \\gamma) \\sum_{t=h}^{\\infty} \\gamma^t p^{\\pi} (s_t = y | s_0 = y) \\Big) \\\\\n&= \\sum_{h=0}^{\\infty} \\gamma^h P (H^{\\pi} (x,y) = h) \\Big( (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t p^{\\pi} (s_t = y | s_0 = y) \\Big) \\\\\n&= E_{\\mathcal{H}^{\\pi}(x,y)} [\\gamma^{\\mathcal{H}^{\\pi}(x,y)}] p^{\\pi} (s_f = y | s_0 = y).\n\\end{aligned}$"}, {"title": "C ADDITIONAL EXPERIMENTS", "content": "C.1 EXPERIMENTS IN CONTINUOUS ACTION SPACE\nWe further conduct experiments in DeepMind Control (Tassa et al., 2018) (DMC) and MetaWorld (Yu et al., 2020) and HalfCheetahVelSparse.\nDMC We selected three tasks with relatively sparse rewards from the DMC environment: Acrobot-swingup_sparse, Hopper-hop, and Cartpole-swingup_sparse. As shown in Fig. 12, our results demon-strate that ETD consistently performs well, showing significant improvements over PPO. We also reproduced two baselines that excelled in discrete tasks\u2014NovelD and E3B-but found they couldn't maintain consistent performance across all three continuous control tasks.\nMeta World Meta-World environments typically use dense rewards. We modified these to provide rewards only when tasks succeed. We tested on Reach, Hammer, and Button Press tasks, finding that PPO without intrinsic rewards performed well. This suggests that Meta-World may not be ideal benchmarks for exploration problems. To our knowledge, intrinsic motivation methods haven't been extensively tested on Meta-World, making it less suitable for comparing exploration-based methods.\nHalfCheetah VelSparse We modified the Mujoco HalfCheetah environment's forward reward function to provide rewards only when the target velocity is reached. Our experiments revealed that PPO already performs well in this task. Adding intrinsic motivation methods didn't significantly improve performance, likely due to the low exploration difficulty. These environments require relatively low exploration capabilities, do not necessitate intrinsic motivation methods, and are unsuitable as benchmarks for comparing exploration methods."}, {"title": "C.2 ABLATIONS OF ENERGY FUNCTION AND CONTRASITIVE Loss", "content": "Previous work in contrastive RL (Bortkiewicz et al., 2024) has utilized various energy functions and contrastive loss functions for goal-conditioned tasks. We followed their methodology, examining the impact of different energy functions and contrastive losses on temporal distance and performance.\nSpecifically, we considered four energy functions: Cosine, L2, MRN, and MRN-POT (which we used in this study), defined as follows.\n$\\begin{aligned}\nf_{\\phi,cos}(x, y) &= \\frac{\\langle \\phi(x), \\phi(y) \\rangle}{\\|\\phi(x)\\|_2\\|\\phi(y)\\|_2},\n\\\\ f_{\\phi,l2}(x, y) &= -\\|\\phi(x) - \\phi(y)\\|^2,\n\\\\ f_{\\phi,MRN}(x, y) &= -d_{\\phi}(x,y),\n\\\\ f_{\\phi,\\psi,MRN+POT}(x, y) &= \\psi(y) - d_{\\phi}(x, y).\n\\end{aligned}$\nFor contrastive loss functions, we evaluated InfoNCE Forward, InfoNCE Backward, InfoNCE Symmetric (which we employed in this work), described below.\n$\\begin{aligned}\n\\mathcal{L}_{InfoNCE-forward}(B; f) &= -\\sum_{i=1}^B \\log \\frac{e^{f(x_i,Y_i)}}{\\sum_{j=1}^B e^{f(x_i,y_j)}}, \\\\\n\\mathcal{L}_{InfoNCE-backward}(B; f) &= -\\sum_{i=1}^B \\log \\frac{e^{f(x_i,Y_i)}}{\\sum_{j=1}^B e^{f(x_j, Y_i)}}, \\\\\n\\mathcal{L}_{InfoNCE-symmetric}(B; f) &= \\mathcal{L}_{InfoNCE-forward}(B; f) + \\mathcal{L}_{InfoNCE-backward}(B; f).\n\\end{aligned}$\nOur experiments were conducted on a noisy version of the 17x17 SprialMaze, which is similar to the Disco Maze in (Badia et al., 2020). This version introduced random colors to the walls of the maze, making representation learning more challenging."}]}