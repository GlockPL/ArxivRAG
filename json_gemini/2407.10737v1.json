{"title": "Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models", "authors": ["Rining Wu", "Feixiang Zhou", "Ziwei Yin", "Jian K. Liu"], "abstract": "Our brains represent the ever-changing environment with neurons in a highly dynamic fashion. The temporal features of visual pixels in dynamic natural scenes are entrapped in the neuronal responses of the retina. It is crucial to establish the intrinsic temporal relationship between visual pixels and neuronal responses. Recent foundation vision models have paved an advanced way of understanding image pixels. Yet, neuronal coding in the brain largely lacks a deep understanding of its alignment with pixels. Most previous studies employ static images or artificial videos derived from static images for emulating more real and complicated stimuli. Despite these simple scenarios effectively help to separate key factors influencing visual coding, complex temporal relationships receive no consideration. To decompose the temporal features of visual coding in natural scenes, here we propose Vi-ST, a spatiotemporal convolutional neural network fed with a self-supervised Vision Transformer (ViT) prior, aimed at unraveling the temporal-based encoding patterns of retinal neuronal populations. The model demonstrates robust predictive performance in generalization tests. Furthermore, through detailed ablation experiments, we demonstrate the significance of each temporal module. Furthermore, we introduce a visual coding evaluation metric designed to integrate temporal considerations and compare the impact of different numbers of neuronal populations on complementary coding. In conclusion, our proposed Vi-ST demonstrates a novel modeling framework for neuronal coding of dynamic visual scenes in the brain, effectively aligning our brain representation of video with neuronal activity. The code is available at github.com/wurining/Vi-ST.", "sections": [{"title": "1 Introduction", "content": "The neuronal coding of our neural system to visual stimuli constitutes a pivotal research area in computational neuroscience. When discussing the visual system, it typically involves components such as the retina, Lateral Geniculate Nucleus, Primary Visual Cortex, and more advanced visual cortices. The retina, in par- ticular, comprises photoreceptors, horizontal cells, bipolar cells, amacrine cells, and ganglion cells. Retina ganglion cells serve as the output terminals of the retina, generating a series of neural response signals, namely spikes. The retina, as the initial component of the entire visual system, plays a crucial role in the conversion of light signals into bioelectric signals, representing a focal point of interest in neuronal coding of visual information in terms of pixels.\nIn experimental neuroscience, specific stimuli such as flashes, Gaussian noise, and enhanced static images are commonly used for controlled experiments to establish correlations between visual features and stimuli [18]. However, real natural scenes are highly complex and dynamic, posing a significant challenge in accurately establishing the link between neural responses and scene changes [23, 27, 28]. Recent advances, exemplified by recursive convolutional neural networks (RCNN) [50], effectively elucidate the functioning of the recurrent circuitry in the biological retina. This suggests that visual stimuli in natural scenes can be decomposed into spatial and temporal domains, with temporal information being an indispensable component for understanding visual encoding. Nevertheless, efficiently modeling temporal or spatiotemporal information still requires further exploration.\nRecently, in the field of deep learning, modeling methods based on temporal information have provided new insights for visual encoding [5, 16, 21, 24, 36, 43]. When employing deep learning methods to address classic image or video tasks, it typically involves selecting local or global pixels and computing their cor- relations, ultimately mapping them to corresponding label domains. However, neuronal visual encoding tasks, in comparison, require the incorporation of more biologically plausible priors. This involves studying how neurons in the visual system respond to different visual features, patterns, and information, such as encoding for orientation, contrast, temporal characteristics, and the interaction between different neurons.\nFurthermore, it is observed that neural responses often manifest as a series of highly dynamic and sparse irregular time sequences that require the spatiotem- poral capabilities of deep learning models, together with video analysis [11]. Therefore, the interesting and challenging question arises of how to effectively transfer powerful deep learning models to the task of neural visual encoding. In particular, previous work demonstrated that a properly designed model, such as RCNN [50], is capable of learning the relatively simple and self-consistent video and their triggered neuronal response, e.g., training and testing data are taken from the same video where pixel context is conserved. However, these well-trained models cannot be transferred to other videos where the pixel com- plexities are quite different, not only for neurons in the retina but also for the visual cortex [37].\nTo address these challenges, our work here introduces a spatiotemporal con- volutional neural network (CNN) model [32] based on the Vision Transformer (ViT) prior [13], termed Vi-ST. The ViT prior is derived from DINOv2 [33], which employs a self-supervised approach for pretaining on a huge dataset con- taining 1.2B unique images, yielding a set of adversarial feature representations with strong generality and generalization capabilities. Subsequently, we integrate the feature space of visual stimuli using a causal Temporal Convolutional Net- work (TCN) with 3D convolutional kernels. We incorporate receptive field infor- mation from retinal ganglion cells (RGCs) as auxiliary conditions. These, along with video features, are fed into a series of spatiotemporal convolutional mod- ules aligning the voxel space of video spatiotemporal information with the neural response space of RGCs. In particular, we conducted ablation experiments, pro- gressively removing specific modules from the baseline model to observe the correlation metrics of the predicted results. This demonstrates the effectiveness of the spatiotemporal modules.\nAdditionally, we find that employing a larger visual encoding space yields better performance in representing biological visual encoding. This is likely due to complementary encoding, where a too-small encoding space cannot capture highly dynamic and complex spikes effectively [12].\nMoreover, we introduce an indicator that considers the duration of neural responses to evaluate prediction performance. Specifically, we used kernel density"}, {"title": "2 Related Work", "content": "In this section, we review the related work on retinal recurrent connection mech- anisms, image and video-based methods, and 3D CNN-based temporal modeling."}, {"title": "2.1 Retinal Recurrent Connection Mechanism", "content": "The retina, being the starting point of the visual system, serves as an ideal model for the study of neural encoding and responses [46,50]. Located at the back of the eye, the retina consists of three layers of cells. Photoreceptor cells, including cones and rods, are responsible for brightness perception, respectively. Horizon- tal cells act as intermediary neurons connecting photoreceptor cells and bipolar cells, facilitating lateral inhibition to regulate information transfer between pho- toreceptors and bipolar cells, typically enhancing the retina's sensitivity to con- trast and edges [15,44]. Bipolar cells receive information from photoreceptor cells and transmit it to ganglion cells, regulating slight variations in light intensity and color. Amacrine cells are inhibitory interneurons that carry out inhibitory synapses, form complex visual processing, and have widespread gap junctions. Gap junctions between cells create electrical coupling, which strengthens syn- chronization between cells and regulates the activity of retinal cells [35]. Ganglion cells are the final layer of neurons in the retina, transmitting processed signals to the optic nerve. Due to the lateral connections provided by gap junctions and amacrine cells, which serve as a mechanism akin to recurrent connectivity in the retina, this structure has been shown to model temporal information in visual stimuli more effectively than conventional CNNs [50]."}, {"title": "2.2 Image and Video Models", "content": "Self-supervised Foundational Vision Model With the advent of the Vi- sion Transformer (ViT) model, many works have introduced the Transformer"}, {"title": "3 Methodology", "content": "In this section, we will provide a detailed explanation of the Vi-ST architecture and introduce the Vi-ST loss function. We divide the model into two parts: the Video Features Extractor and the Spikes Alignment module, as Fig. 1. In the feature extraction stage, we utilize the powerful foundation visual model, namely DINOv2, to be a prior for spatial information of image pixels."}, {"title": "3.1 Video Features Extractor", "content": "As Fig. 2a demonstrates, the video features extractor consists of multiple layers of C3TCN and 2 linear mapping layers. Where, the D is the size of dilation, K is the size of kernel, I is the number of input channels, O is the number of output channels, and Gis the number of filters' groups. The C3TCN is padding $2^{2+1}$ zeros for the i-th layer's input, along the time axis, and sets the dilation size to $2^2$.\nMeanwhile, the depth-wise separable convolution is used to reduce the number of parameters and mitigate overfitting. Ultimately, each layer is embedded with a residual connection.\nSpecifically, the Video Features Extractor is divided into two steps. Initially, the image stack $X \\in R^{T \\times H \\times W \\times C}$ is fed into the frozen DINOv2 model. DINOv2 transforms and \u201cpatchfys\u201d each image into a sequence of patches $\\epsilon \\in R^{T \\times g \\times g \\times C}$. Subsequently, a reverse mapping is applied to $\\epsilon$, restoring them to their original positions, resulting in $\\epsilon \\in R^{T \\times 9 \\times 9 \\times C}$. Since DINOv2 inherits the original ViT hierarchical structure [14], which contains different semantics within different layers, we choose outputs of layer 0 to layer n as priors, where layer 0 represents patchify the image only.\nIn the second step, the prior is fed into the C3TCN to further model the spatiotemporal information of the video frames. C3TCN will map priors to $\\epsilon \\in R^{T \\times g \\times g \\times C'}$, where $C'$ is the number of RGCs. C3TCN extends the Residual 1D Dilated Convolution module of MSTCN by directly replacing the 1D CNN with Causal 3D CNN, maintaining MSTCN's approach of increasing receptive fields with layers. Here, C3TCN performs dilated convolution calculations only along the time dimension, while still keeping the spatial convolutional kernels size in 3\u00d73. Specifically, different time spans of receptive fields are set to capture diverse spatiotemporal features. Additionally, Batch Normalization [19] is added after C3TCN to prevent gradient explosions. This enhancement enables C3TCN to effectively and robustly capture spatiotemporal information in video data and provide comprehensive feature representations for subsequent modules."}, {"title": "3.2 Spikes Alignment", "content": "As Fig. 2b shows, the spikes alignment module mainly consists of the CMST, where the D is the size of dilation, $K_t$ is the size of kernel along the time axis,\n$K$ is the size of kernel along the spatial axis, I is the number of input channels, O is the number of output channels, and G is the number of filter groups.\nFor each CMST modules, different time scales are used to capture the multi- scale temporal features, and the features are added together, then fed into a mapping layer. Specifically, when the time scale is 1, the time kernel size is set to 1, and the spatial kernel size is kept to 3, then the vanilla causal 3D convolution is utilized. When the time scale is greater than 1, a causal separable 3D convolution with time kernel size N and spatial kernel size 3 is used.\nIn the Spike Alignment module, we aimed to incorporate the RGC receptive field information to better align video pixel spaces and RGC spike spaces. Here, the receptive field, $RF \\in R^{W \\times H}$, is downsampled along the spatial axis to match the size of $\\epsilon \\in R^{T \\times 9 \\times 9 \\times C'}$, and is used as a conditional factor for fusion with $\\epsilon$ through the AdaLN Zero module [34]. As a result, we obtain the fused feature $F \\in R^{t \\times 9 \\times 9 \\times C'}$. Since $\\epsilon$ contains both temporal and spatial dimensions, we naturally extended AdaLN Zero to 3D AdaLN Zero.\nNext, we feed the fused feature into the CMST block. CMST adds features from multiple temporal scale branches and maps them through a 1\u00d71\u00d71 3D CNN layer. Then, CMST utilizes residual connections to retain information from the previous block. To compress the spatial size of latent variables, we apply an average pooling operator on the previous layer's information, then add it to the outcome of multiscale branch. We set 4 CMST blocks in total, and the convolution kernel sizes of different time scales are shown in Tab. 1.\nThese kernel sizes are chosen by considering the duration of the neural re- sponses. We calculated the density function of entire neural responses' duration and identified several high-frequency durations, which we use as feature-sensitive spans, used for the kernel sizes of CMST blocks. Here, durations refer to the durations of firing rates ranging from 0 to maximum and then back to 0. We estimated their distribution using kernel density estimation. We manually iden- tified several peaks, as shown in Fig. 3, where RGC durations are different in two different video stimuli.\nFinally, the latent variables are flattened and passed through a linear layer to obtain the prediction results for RGCs. In this study, we set the number of RGCs to 90, which is predicted simultaneously as a population coding scheme."}, {"title": "3.3 Loss Function", "content": "Initially, we employed Root Mean Square Error (RMSE) as the loss function, at Eq. (1). However, experimental results indicated that using RMSE as the target did not adequately consider temporal dynamics, resulting in lower generalization capabilities of the model.\n$\\mathcal{L}_{RMSE} = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ (1)\nTherefore, we introduced a Soft Dynamic Time Warping (SoftDTW) loss function [9]. The SoftDTW, unlike Euclidean losses such as RMSE, considers potential time shifts or variations of length of durations. It is notable that using SoftDTW for predicting longer time windows may lead to distortion and diffi- culty in representing local abrupt changes. Thus, we group the time sequences into shorter subsequences in the rolling window way and then compute as Eq. (2):\n$\\mathcal{L}_{SoftDTW}^n = \\frac{1}{L-n} \\sum_{i=1}^{L-n} SoftDTW(y_i, \\hat{y}_i), i \\in \\{1, 2, ..., L-n\\}$ (2)\nHere, n demonstrates the length of the subsequence, which is set to 6 or 12, and L is the length of the time sequence. Meanwhile, to reduce the model's predictions of meaningless negative responses, we add a negative ReLU function as a penalty term to the loss function, as shown in Eq. (3).\n$\\mathcal{L}_{ReLU} = \\frac{1}{n} \\sum_{i=1}^{n} max(0, -\\hat{y}_i)$ (3)\nThen, the entire loss function is represented as shown in Eq. (4), named Vi-ST loss in our model.\n$\\mathcal{L}_{VI-ST} = \\alpha \\mathcal{L}_{RMSE} + \\beta \\mathcal{L}_{ReLU} + \\gamma \\mathcal{L}_{SoftDTW}^6 + \\gamma \\mathcal{L}_{SoftDTW}^{12}$ (4)\nHere, $\\alpha$, $\\beta$, and $\\gamma$ are hyperparameters, and we set them at 0.1, 0.5, and $5\\times10^{-6}$, respectively."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Dataset", "content": "RGCs Responses This work focuses on studying the retina encoding mecha- nism by recording RGC spikes in response to visual stimuli. The dataset used was organized by [31], and has been used in various studies on retinal cod- ing [26,30,40,45,48,50]. A total of 90 well-responding RGCs were used for train- ing and testing. In summary, the experimental setup involves isolating the retina from salamanders, inverting the ganglion cell layer downward onto a multielec- trode array, stimulating the photoreceptor layer of the retina using an OLED display, and recording the responses of RGCs. For each cell, receptive fields were determined by computing the spike-triggered average using stimulus images of spatiotemporal white noise [7]. Singular value decomposition was used to sep- arate the spike-triggered average into a spatial and temporal component [17]. Finally, a two-dimensional Gaussian function was fitted to the spatial receptive field component to determine the center, size, and shape of the receptive field. Receptive fields were normalized to unit Euclidean norm. The details of the experimental procedures and data analysis are standard operations for retinal coding studies [27,28,31].\nTo align frame rates, neuronal spikes were grouped into spike counts at in- tervals of 33 ms, resulting in a dataset with 30 Hz response for each trial, in line with the sampling rate of 30 Hz used by video stimuli. In other words, spike counts within each 33 ms interval are recorded as the response data for that time period. To reduce noise during the recording process, several trials were conducted for the same stimulus, and response data was averaged over the trials to obtain the neuronal firing rate for each cell and each video.\nNatural Scene Stimuli The natural scene stimuli used in this study con- sist of two video clips with rich natural scenes as shown in [31], named Mov1 and \u039c\u03bf\u03c52. \u039c\u03bf\u03c51 features minimal changes between video frames and mainly captures salamanders swimming in water, while Mov2 exhibits more significant changes between video frames and mainly documents tigers and deers in outdoor activities. Both video clips have a resolution of 360\u00d7360 pixels with a frame rate of 30 fps, with Mou1 being 60 seconds long (1800 frames) and Mov2 being 53.3 seconds long (1600 frames). The original video clips were in RGB color with three channels. To reduce the impact of color information on visual stimuli, video clips were converted to grayscale image stacks with brightness values ranging from 0 to 255. Details of video information can be found in [31]. These two videos show different levels of scene complexity in both spatial and temporal domains, which presents a challenge for the building and testing of encoding models [50]."}, {"title": "4.2 Metrics", "content": "In the task of neural encoding, the Pearson correlation coefficient (CC) is com- monly used to assess the reliability of predicting neural signals [38, 41,50], and"}, {"title": "4.3 Implementation Details", "content": "Pre-trained ViT Oquab et al. [33] contributed to the open-source DINOv2 model. Additionally, Darcet et al. [10] proposed Registers Token, enhancing the robustness of DINOv2. Considering computational costs, we chose the ViT-L/14 with registers, obtained through ViT-g/14 distillation as the ViT Prior. After feeding images into DINOv2, the preprocessing produces feature maps of size 16\u00d716, a process referred to as patchfy. Each patch corresponds to a 14\u00d714 region after scaling down from the original image [33].\nRGC Receptive Field The RF, with an initial size of 90\u00d790, is downsampled to 16\u00d716 to match the feature map size of DINOv2. Initially, we directly added the RF on feature latent variables from the previous module. However, experi- mental results show that the 3D AdaLN Zero module offers better robustness.\nTraining To maximize the generalization ability of the model, we trained and tested the model using data from different videos, such as training on Mov1 and validating on Mov2, marked as Mov1\u2192Mov2, and vice versa. Due to com- putational constraints, we apply the rolling window to crop 128-frame clips as training samples. Each epoch involved arbitrarily selecting 768 samples for the training set with a batch size of 16. The validation set uses the entire video as a sample.\nFor optimization, we choose AdamW [29] with a weight decay of 0.1. We trained for 30 epochs, applying linear learning rate warm-up to 8\u00d710-4 for the first 5 epochs, then gradually decaying to 1\u00d710-4 using a half-cycle cosine schedule. At the last 10 epochs, Stochastic Weight Averaging [20] was applied to mitigate model overfitting."}, {"title": "4.4 Results", "content": "Comparison with Previous Works Previous works mainly focus on training and testing the data from the same video [26, 30, 40, 50]. Typical CC metrics on these two videos are shown in Tab. 2. However, our focus places a signif- icant emphasis on spatiotemporal modeling tailored for neural encoding tasks across different videos. To ensure a fair comparison of model performance, we constructed two control models. The first employs an I3D feature extractor as a backbone and MSTCN as the downstream model, denoted as I3D+MSTCN;\nThe second model utilizes DINOv2 as the prior and MSTCN as the downstream model, labeled as DINOv2+MSTCN. Performance in training and testing using the same videos shows relatively good CCs; however, a significant improvement in transfer prediction between videos can only be observed with Vi-ST, and other models lack the generalization ability as shown in Tab. 2. Looking into the de- tails of single neuron performance, we found that Vi-ST demonstrates a better ability to predict spare neuronal responses, as shown in Fig. 4.\nComparison of Loss Functions We compared the model performance with RMSE loss and Vi-ST loss. In Fig. 5a, we observe that the Vi-ST loss outperforms RMSE in all experiments. This indicates that considering temporal variations in the loss function is crucial for neural encoding tasks.\nComparison of ViT Prior Amir et al. [1] points out that the hierarchi- cal structure of ViT provides different representations, where the earlier layers contain lower semantic information but better present positional information.\nTherefore, we use inputs from different layers of DINOv2 as priors, specifically, the 1st, 7th, 13th, 19th, and the last layer, along with using only the patchfy representations as a prior, to test the performance of Vi-ST. In Fig. 5b, we ob- serve that the representations from the earlier layers of ViT are beneficial for neural encoding results, aligning with the biological characteristics of the retina.\nAblation study To investigate the contribution of each model module to pre- diction performance, we progressively removed or replaced different modules and observed changes in CC. We found that the Spikes Alignment module played a more significant role compared to the Video Extractor, and the Vi-ST loss had a non-negligible impact on achieving more generalized neural encoding as well. Additionally, in conjunction with the results in Fig. 6a, we observed that re- moving the ViT prior had a significant impact on the model, and excessively deep ViT features did not bring about more gains. Therefore, selecting an appropriate spatial feature prior for RGC visual encoding tasks requires careful consideration.\nComparison of benefits of complementary coding The prediction results for neural encoding tasks often do not involve a fixed number of neurons [8, 42]. Therefore, we ask: Is it optimal to construct an end-to-end model capable of simultaneously predicting all neural responses?\nDing et al. [12] explored the benefits of complementary encoding for neu- ral encoding by constructing a high-dimensional representation manifold of re- sponses of the RGC population. This suggests that collaboration between neu- rons can compensate for the problem of insufficient expression when the firing rate saturates. We conducted experiments to validate this conclusion. We sorted"}, {"title": "5 Conclusion", "content": "In this work, we proposed the Vi-ST model to align dynamic visual stimuli of the natural scene with RGC responses. The experiments demonstrate that the Vi-ST model can learn certain encoding mechanisms akin to biological visual neural coding. Through ablation experiments, we identified that the primary capabilities of the Vi-ST model stem from the Spike Alignment module and the inclusion of the ViT prior. By contrasting the benefits of complementary coding, we believe that the model has potential applications in a wider range of neural encoding tasks. Additionally, we are exploring new metrics to examine the model performance in representing target temporal dynamics.\nThis study presents certain limitations that need further investigation. Firstly, the SD-KL metric exhibits sensitivity to noise and demonstrates limited ex- pressive power. Additionally, the robustness of Vi-ST needs to be confirmed through the acquisition of more extensive datasets, including a greater number of recorded cells and a variety of video types. Future research will aim to refine and overcome these challenges by leveraging recent advancements in experimen- tal techniques. Furthermore, we plan to extend our approach to the study of visual coding using neurons from other parts of visual systems, such as the vi- sual cortex."}]}