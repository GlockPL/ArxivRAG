{"title": "SHORTCUTSBENCH: A LARGE-SCALE REAL-WORLD BENCHMARK FOR API-BASED AGENTS", "authors": ["Haiyang Shen", "Yue Li", "Desong Meng", "Dongqi Cai", "Sheng Qi", "Li Zhang", "Mengwei Xu", "Yun Ma"], "abstract": "Recent advancements in integrating large language models (LLMs) with application programming interfaces (APIs) have gained significant interest in both academia and industry. These API-based agents, leveraging the strong autonomy and planning capabilities of LLMs, can efficiently solve problems requiring multi-step actions. However, their ability to handle multi-dimensional difficulty levels, diverse task types, and real-world demands through APIs remains unknown. In this paper, we introduce SHORTCUTSBENCH, a large-scale benchmark for the comprehensive evaluation of API-based agents in solving tasks with varying levels of difficulty, diverse task types, and real-world demands. SHORTCUTSBENCH includes a wealth of real APIs from Apple Inc.'s operating systems, refined user queries from shortcuts, human-annotated high-quality action sequences from shortcut developers, and accurate parameter filling values about primitive parameter types, enum parameter types, outputs from previous actions, and parameters that need to request necessary information from the system or user. Our extensive evaluation of agents built with 5 leading open-source (size >= 57B) and 4 closed-source LLMs (e.g. Gemini-1.5-Pro and GPT-3.5) reveals significant limitations in handling complex queries related to API selection, parameter filling, and requesting necessary information from systems and users. These findings highlight the challenges that API-based agents face in effectively fulfilling real and complex user queries. All datasets, code, and experimental results will be available at https://github.com/eachsheep/shortcutsbench.", "sections": [{"title": "INTRODUCTION", "content": "Large language model based agents (LLM-based agents) built on application programming interfaces (APIs) have recently gained significant interest in both academia and industry. By integrating LLM with APIs, these agents can access real-time information, reduce hallucination with external knowledge, and automatically plan and complete complex tasks that need multi-step actions. Many of these agents have also demonstrated commendable performance on simple tasks involving only a few actions such as \u201cCheck the weather \u2460 and tell me \u2461\". These impressive performances raise an important question: Do these API-based agents truly possess the capability to generate complex action sequences for real demands with real APIs?\nSome existing benchmarks/datasets have attempted to evaluate API-based agents. However, they have three limitations: First, the APIs (a.k. tools available to the agent) lack richness, and the queries (a.k. the task to the agent)"}, {"title": "RELATED WORK", "content": "API-based agents. API-based agents treat APIs as tools. They accept queries, generate action sequences based on queries and provided APIs, and generate next action depends on the history actions. Related work about API-based agents can generally be categorized into 3 types depending on the objective: (1) Task-specific enhancement focuses on improving the agent's ability to solve a specific type of task like game and question-answering. (2) Data-driven workflows emphasize the importance of"}, {"title": "DATASET", "content": "In this Section, we first introduce the acquisition of the dataset (Section 3.1). Then, we outline the SHORTCUTSBENCH's construction process (Section 3.2). Finally, we outline the setup for evaluation tasks to evaluate the agent's ability to handle tasks of varying difficulty (Section 3.3.1), including the ability to select suitable APIs, the ability to do parameter filling (Section 3.3.2), and the awareness in requesting additional information from the system or user (Section 3.3.3)."}, {"title": "DATASET ACQUISITION", "content": "Figure 1 shows the data acquisition process. We first use search engines to identify popular public shortcut-sharing sites \u2460. We totally find 14 sites. Then we crawled these sites to obtain fields such as \"shortcut name\", \"function description\", \"shortcut type\", and \u201ciCloud link\" \u2461. After deduplicating based on \"iCloud link\", we got the source files of all 8675 shortcuts \u2462. Subsequently, we extracted \"app name\" using the field WFWorkflowActionIdentifier in the source file like com.apple.Music, and then downloaded related apps \u2463 from various sources: (1) third-party apps from the \"macOS App Store\" or the official website of the app, (2) non-system apps (uninstallable) like Keynote from path /Applications/ on macOS, (3) system apps like Reminders (uninstallable) from path /System /Application/ on macOS, (4) iOS apps from the \u201ciOS App Store\", (5) Shortcuts app itself from path /System/Library/PrivateFrameworks/WorkflowKit.framework/ on macOS. During the downloading, we also excluded some legacy apps and 12 paid apps. For more details about the whole acquisition process, please refer to Appendix.\nThen we managed to extract APIs from the downloaded apps \u2464. The APIs are mainly from intent definition file ${filename}.actionsdata from AppIntent framework, ${filename}.intentdefinition from SiriKit framework, and WFActions.json from system path /System/Library/PrivateFrameworks/WorkflowKit.framework/ on macOS. We extracted all APIs involved in the app's shortcuts. During the extraction, due to the existence of multi-platform versions of most apps like Day One, and the presence of similar and duplicate API definition files from the same and different frameworks, we perform deduplication based on manually crafted rules. For more details about deduplication, please refer to Appendix. Finally, as shown in Table 1, we get 88 apps from various categories such as \u201cHealth & Fitness\u201d, \"Developer Tools\", and \u201cLifestyle\u201d. These apps in total include 1414 APIs, including all of 556 APIs involved in 7628 shortcuts."}, {"title": "DATASET CONSTRUCTION", "content": "As shown in Figure 2, existing benchmarks/datasets consist of two parts: (1) APIs; (2) queries and corresponding action sequences.\nAPIs (\"a\" in Figure 2) include the \"API description\" (\u201ca.4\u201d), \u201cAPI name\u201d (\u201ca.1\u201d), \u201cparameter names\u201d (\u201ca.2\"), \"parameter types\u201d (\u201ca.1\u201d), \u201cdefault value\u201d (\u201ca.2\u201d), \u201creturn value type\u201d (\u201ca.3\u201d), and \"return value name\" (\"a.3\"). The field names in [...] in Figure 2 correspond to the field names in ${filename}.actionsdata, ${filename}.intentdefinition, and WFActions.json. For details, please refer to the Appendix. In existing benchmarks/datasets (Table 1), the \"parameter types\" (\u201ca.1\u201d) and \"return value types\" (\u201ca.3\u201d) are composed of primitive data types such as int and string. In addition to primitive data types, APIs in SHORTCUTSBENCH also include \"enum\" or \"advanced data types\". Enum is composed of \u201cthe class name\" and \"the possible value\", with each value equipping a \u201cvalue name\". We also provide the agent with a description of the \"enum\" in the API information. Advanced data types, such as the model (\u201ca.1\u201d) in app chatGPT, include three String types named identifier,title, and subtitle. We can comprehend them through their \u201ctype name\u201d and \u201ctype description\".\nQuery and action sequence. A query is a user command, such as \u201cTell me what the weather will be like tomorrow.\"\nThe action sequence (aka. shortcut) is the series of API calls to complete the query, with each API call referred to as an action. The action sequence identifies the steps needed to complete a query. As shown in Figure 1.b, Existing benchmarks/datasets (Table 1) collect APIs first and then use them, either fully automatically or semi-automatically, to construct query and action sequences through LLMs. In contrast, action sequences in SHORTCUTSBENCH are all human-annotated (123 in Figure 1). The shortcut developers are our annotators. APIs in SHORTCUTSBENCH (\u2463 in Figure 1) are also all real-world.\nGenerating queries. As shown in Figure 1, existing works construct query and action sequences based on available APIs. In contrast, we construct queries based on existing action sequences and APIs. When constructing a query for a specific action sequence, we need to understand the functional\""}, {"title": "TASK DEFINITION AND METRICS", "content": "We aim to address 3 research questions regarding the performance of existing agents built using leading LLMs on SHORTCUTSBENCH with varying difficulties: (1) How do they perform in API selection? (2) How do they handle API parameter value filling, including parameters for primitive data types, enums, and outputs from previous actions? (3) Can they recognize when input is required for tasks that need system or user information?\nPreliminaries. SHORTCUTSBENCH consists of a set of queries $Q = {q_1, q_2, ..., q_n}$, corresponding \"golden\" action sequences $ASeq = {aseq_1, aseq_2, ..., aseq_n}$, and all available APIs $APIs = {\u03b1\u03c1\u03af_1, \u03b1\u03c1\u03af_2, ..., apim}$. For each query $q_i, 1 \u2264 i \u2264 n$, the corresponding \"golden\" action sequence is $aseq_i = {a_1, a_2,..., a_{|aseqi|}} $, where the length of the action sequence is $|aseqi|$. Each app app; has a set of APIs $apis_i = {ap\u03af_1, \u03b1\u03c1\u03af_2, ..., api_{|apis_i|}}$. The action sequences generated by the agent for each query $q_i$ are referred to as bseqi.\nPrepare available APIs for each query. For each query $q_i$, we provide the LLM with a certain number of usable APIs to simulate real-world scenarios where APIs can be input into the LLM's context. Following existing work , we equip each $q_i$ with a specific number of APIs. For each $aseq_i$, let $|APIs_i|$ represent the number of APIs involved. In addition to these $|APIs_i|$ APIs, we equip each query with extra APIs calculated as $max(min(x \u00d7 |APIs_i, 20 - |APIs_i|), 0)$, where $x \u2208 {3,4,5}$. We do this because it is impractical to input all APIs into the context simultaneously. When dealing with a large number of APIs, additional retrieval is often required, which we do not consider in this work.\nFurther Processing. Considering the context limitations of LLMs, we excluded shortcuts longer than 30 and parts using the APIis.workflow.actions.runworkflow to call other shortcuts. While these shortcuts remain in our open-source dataset, they will not be included in the subsequent evaluation. We aim to study the performance of agents on queries of varying difficulties. As shown in Table 2, we categorize SHORTCUTSBENCH into 4 difficulty levels and 8 task types based on $|aseq_i|$ and \"shortcut type\" (Section 3.1), respectively. For more details, please refer to the Appendix. When calculating the length, for branching actions like is.workflow.actions.conditional, we consider the longest branch as the length. Additionally, we ignore the lengths of looping actions like is.workflow.actions.repeat.count and special actions such as is.workflow.actions.comment. Due to the presence of branching actions, the average number of APIs involved when p = 1 is greater than one, specifically 1.76. For a detailed process, please refer to the Appendix. The number of shortcuts in each level is denoted as $n_p$. Each query and action sequence is referred to as $q_{p,i}$ and $aseq_{p,i}$, with $1 < p < 4$ and $1 \u2264 i \u2264 n_p$."}, {"title": "PERFORMANCE ABOUT API SELECTION", "content": "Following existing work, we use the accuracy of API selection as the metric. The accuracy is calculated as the number of correct API selections $m_p$ divided by $n_p$. Specifically, each time we predict an action $b_j, 1 \u2264 j \u2264 |aseq_i|$, we provide the agent with all the correct historical actions"}, {"title": "EFFECTIVENESS OF API PARAMETER VALUE FILLING", "content": "In this part, we aim to investigate the performance of agents in API parameter value filling, including parameters for \u201cprimitive data types\u201d and \u201cenums\u201d and filling output from previous actions. For each input parameter of every action in SHORTCUTSBENCH, we expect the agent to fill in the following parameters correctly:\nStatic Parameters Preset: These are static parameters that users provide as default inputs of the action. These static parameters typically include primitive data types such as String and Integer, as well as custom Enum defined by app developers. When the query explicitly specifies a parameter that can be used as a static parameter, we expect the agent to accurately fill in the parameter values according to the user's query and the API's definition. When generating queries, we have already required the LLM to naturally include primitive and enumerated data types (Section 3.2). To further ensure that the corresponding parameters are indeed included in the queries during evaluation, we used the LLM to filter these parameters further, ensuring their presence in the queries. Detailed prompts can be found in the Appendix.\nOutputs from Previous Actions: An action may either have no output or, if it does have an output, the output may be used by the following actions. In shortcuts, In SHORTCUTSBENCH, outputs that are difficult to input directly into the LLM are represented by a unique identifier (UID) and an output name (OutputName), which can be input into the LLM for processing. The agent should have the ability to correctly use the output values of previous actions.\nFor the static parameters preset, we evaluate using the overall parameter fill rate. Let $spp_{ai}$ be the total number of parameters that need to be filled in $aseq_i, 1 < i < n_q$, where $n_q$ is the number of queries. If the agent correctly fills $sppt_i$ parameters in the generated action sequence $bseq_i$, then the Static parameter preset accuracy can be calculated as $Acc._{spp} = \\frac{\\sum_{i=1}^{n_q} sppt_i}{spp_{ai}}$. Similarly, for Outputs from previous actions, the accuracy can be calculated as $Acc._{ofpa} = \\frac{\\sum_{i=1}^{n_q} ofpa_i}{\\sum_{i=1}^{n_q} ofpa_{ai}}$."}, {"title": "RECOGNITION OF NEED FOR INPUT", "content": "In this section, we aim to investigate the ability of existing API-based agents to ask systems or users for necessary information to resolve the missing information. This missing information can come from the system like clipboard (Clipboard), input files (ExtensionInput), and the current date (CurrentDate) or from the user (Ask) (Inc., 2024a). For example, a parameter named tags is usually represented in a shortcut as \"tags\":{\"Value\":{\"Type\":\n\"Ask\"}}, where \"Type\": \"Ask\" indicates that the parameter will prompt the user for input. For a detailed process, please refer to Appendix. We use the proportion of correctly identified parameters to evaluate the agent's ability to recognize the need for input from the system or the user. Let $n_s$ be the number of queries, $aska_i$ be the number of times the need from the system or the user appears in $aseq_i$, $askt_i$"}, {"title": "EVALUATION", "content": "Model. Referencing existing work , considering the performance of mainstream LLMs, we selected and tested 9 most advanced LLMs to construct API-based agent. The chosen model including 4 closed-sourced LLMs like Gemini-1.5-Pro, Gemini-1.5-Flash, GPT-3.5-turbo, and ChatGLM-4-Air , and 5 open-source LLMs like LLAMA-3-70B , QWen-2-70B, QWen-2-57B, Deepseek-2-Chat (236B) , and Deepseek-2-coder (236B) . Among them, Gemini-1.5-Pro, LLAMA-3-70B, QWen-2-70B, Deepseek-2-chat, and Deepseek-2-coder are LLMs benchmarked against GPT-4 , while Gemini-1.5-Flash, ChatGLM-4-Air, and QWen-2-57B are benchmarked against GPT-3.5-turbo performance. We did not evaluate smaller LLMs like LLAMA-3-8B or Vicuna-7b-v1.5 because we found that agents built on them can only handle simple tasks such as single API selection and they cannot handle well on advanced tasks like parameter filling. Agents built with such models often fail to produce the required JSON actions correctly and frequently generate nonsensical outputs.\nPrompt Template. Following existing work , we slightly modified the ReACT templates to construct the API-based agents. Specifically, we added prompts related to shortcuts, such as the types of fillable parameters and the meanings of special statements like branches. For all 3 research questions (RQs), we use the same prompt templates. An agent should correctly select APIs, fill in parameters, and be aware of the need to request necessary information from the system or user at appropriate times. Please refer to Appendix for more details."}, {"title": "RESULT ANALYSIS", "content": "Through the results of API selection accuracy (Section 3.3.1), we get the following conclusions:\nAgents built using open-source LLMs now perform comparably to closed-source models on lower-difficulty tasks but still lag on higher-difficulty tasks. From Figure 3 we know that open-source LLMs >= 70B match the performance of closed-source LLMs from the first 3 difficulty tasks, significantly outperforming GPT-3.5-turbo. However, they still lag behind closed-source LLMs in handling complex tasks at the 4-th level. Moreover, the price of open-source LLMs is significantly lower than that of GPT-3.5-turbo. For more details, please refer to Appendix.\nExisting LLM-based agents still perform poorly on tasks requiring multi-step reasoning, even Gemini-1.5-Pro level LLMs struggle with high-difficulty tasks. From Figure 3 we know that almost all LLMs handle well in API selection tasks at the level of (0,1], but only more advanced models like Gemini-1.5-Pro and QWen-2-72B can do well in higher-difficulty tasks of (1,5]. As tasks become more complex, the accuracy drops sharply. The average accuracy dropped by 19% as task difficulty rose from (0,1] to (1,5], ranging from a 9% decrease to a 44% . From (0,1] to (5,15], accuracy fell by 46%, with drops from 38% to 58% .\nAgents built with the same LLM show significant performance variations across different types of tasks. From Figure 5 we know that the performance difference of agents built with different LLM ranges from 18.04% to 36.70% .\nExisting API-based agents perform well on tasks in daily life such as Lifestyle & Social but show poorer performance on professional tasks like Development & API. From Figure 5 we know that Lifestyle & Social exhibit the highest average accuracy, surpassing the lowest category, Development & API by approximately 18%.\nBased on the results of API Parameter Value Filling (Section 3.3.2), we draw following conclusions:\nCompared to the API selection, for existing most intelligent LLM like Gemini-1.5-Pro, increased task difficulty has a much smaller impact on the accuracy of parameter filling, especially on using outputs from previous actions. As shown in Figure 6a, the precision of API parameter filling of the existing most intelligent LLM like Gemini-1.5-Pro and QWen-2-72B remains similar across tasks of varying difficulty in both the upper and lower figures. This indicates that the greatest limitation of existing API-based agents in addressing user queries lies in the reasoning and planning capabilities implied by API selection.\nCompared to API selection, the performance of API parameter filling remains a bottleneck for existing cost-effective LLMs like GPT-3.5-turbo and ChatGLM-4-Air. As shown in Figure 6a, the performance of these LLMs in API parameter filling significantly decreases as task difficulty increases.\nCompared to using the outputs of previous actions, extracting relevant parameters from the user's query and filling them according to the query and API description is more challenging. As shown in Figure 6a, the colors in the top plot (filling primitive data types and enum data types) are generally lighter than those in the bottom plot (filling the outputs of previous actions as parameters). The accuracy drop ranges from 2.55% to 15.39% .\nFor existing cost-effective LLMs like GPT-3.5-turbo and ChatGLM-4-Air, errors mainly stem from incorrect output formats and wrong API selections. Figure 6b shows error types for tasks requiring outputs from previous actions. It can be seen that powerful LLMs like"}, {"title": "CONCLUSION", "content": "In this paper, we introduce SHORTCUTSBENCH, a benchmark for evaluating API-based agents. To the best of our knowledge, SHORTCUTSBENCH is the most realistic, rich, and comprehensive benchmark of its kind. Our findings indicate that for agents built on the most advanced LLMs, the"}]}