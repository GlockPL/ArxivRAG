{"title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model", "authors": ["Ruichuan An", "Sihan Yang", "Ming Lu", "Kai Zeng", "Yulin Luo", "Ying Chen", "Jiajun Cao", "Hao Liang", "Qi She", "Shanghang Zhang", "Wentao Zhang"], "abstract": "Current vision-language models (VLMs) show exceptional abilities across diverse tasks including visual question answering. To enhance user experience in practical applications, recent studies investigate VLM personalization to understand user-provided concepts. However, existing studies mainly focus on single-concept personalization, neglecting the existence and interplay of multiple concepts, which limits the real-world applicability of personalized VLMs. In this paper, we propose the first multi-concept personalization method named MC-LLaVA along with a high-quality multi-concept personalization dataset. Specifically, MC-LLaVA uses a joint training strategy incorporating multiple concepts in a single training step, allowing VLMs to perform accurately in multi-concept personalization. To reduce the cost of joint training, MC-LLaVA leverages visual token information for concept token initialization, yielding improved concept representation and accelerating joint training. To advance multi-concept personalization research, we further contribute a high-quality dataset. We carefully collect images from various movies that contain multiple characters and manually generate the multi-concept question-answer samples. Our dataset features diverse movie types and question-answer types. We conduct comprehensive qualitative and quantitative experiments to demonstrate that MC-LLaVA can achieve impressive multi-concept personalized responses, paving the way for VLMs to become better user-specific assistants. The code and dataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA.", "sections": [{"title": "1. Introduction", "content": "Over the past few years, large language models (LLMs) (Achiam et al., 2023; Bai et al., 2023a; Yang et al., 2023; Touvron et al., 2023) have made significant advancements, proving their effectiveness in various applications and transforming the way humans interact with machines. In line with this trend, many vision-language models (VLMs) (Liu et al., 2024; Li et al., 2023; Wang et al., 2024; Bai et al., 2024) have been proposed to connect vision encoders with LLMs for various vision-language tasks (Deng et al., 2024; Zhou et al., 2024; Li et al., 2024; Lin et al., 2024) such as visual question answering. Despite their success, VLMs face challenges when personalized responses are required, such as answering visual questions based on user-provided concepts. For example, given images with a concept named (H), VLM fails to generate sentences with its identifier, as illustrated in Fig. 1. This limitation hinders the smooth integration of VLMs into our daily lives.\nRecent personalization methods usually develop a concise representation (Nguyen et al., 2024) for each concept or use additional modules (Alaluf et al., 2025; Hao et al., 2024) for personalized concepts. Although these methods have produced impressive results, they mainly concentrate on single-concept personalization. However, in real-world scenarios, vision-language tasks often involve multiple concepts, which emerges as an essential capability for the practical deployment of personalized VLMs.\nFrom this perspective, we find personalizing multiple concepts while ensuring that the responses of VLMs accurately reflect the provided concept information can be challenging for existing methods. For example, the proposed module in MyVLM (Alaluf et al., 2025) only handle single concept scenarios as it can only load one concept embedding per interaction. Yo'LLaVA (Nguyen et al., 2024) directly learns the concept tokens and classifier head from scratch, exhibiting a strong dependence on high-quality negative samples and slow converge speeds. As the number of concepts increases, the demand for negative samples rises, making data collection more challenging and requiring model to spend more time fitting the data. Consequently, this leads to significantly higher manual and training costs for fitting these concepts."}, {"title": "2. Related work", "content": "Vision language models In recent years, LLMs have achieved significant advancements (Achiam et al., 2023; Touvron et al., 2023; Bai et al., 2023a; Yang et al., 2023). Following this, the emergence of VLMs (Liu et al., 2024; Lin et al., 2023a; Bai et al., 2023b; Lin et al., 2023b) has significantly expanded the capabilities of LLMs, enabling them to be applied in tasks such as data processing (Luo et al., 2024), VQA (Guo et al., 2023), and captioning (Li et al., 2023). Although they exhibit strong general abilities in numerous tasks, it is challenging for them to fulfill the requirements for highly personalized responses (Alaluf et al., 2025), especially when involves multiple concepts. In this work, we propose a multi-concept personalization method MC-LLaVA, allowing VLMs to reason accurately with multiple concepts.\nParameter-efficient fine-tuning LLMs and VLMs excel in a wide range of downstream tasks. However, updating and storing all model parameters for each task has become increasingly expensive. Compared to re-train the whole model, Parameter-Efficient Fine-Tuning (PEFT) methods (Hu et al.,"}, {"title": "3. Method", "content": "The overall pipeline of our method is shown in Fig. 2. We first introduce how to design the joint training strategy for multi-concept scenarios in Sec. 3.1. Subsequently, we describe how visual information is used to initialize learnable concept tokens in Sec. 3.2. Finally, we present the training details for MC-LLaVA in Sec. 3.3."}, {"title": "3.1. Multi-concept joint training", "content": "Given a pre-trained VLM, we aim to introduce new concepts to it by using multiple images and their corresponding textual descriptions, allowing the model to produce accurate, personalized responses. Yo'LLaVA (Nguyen et al., 2024) employed soft prompt tuning to inject new concepts while preserving the existing knowledge. However, due to the independent training of each concept and the subsequent merging of parameters during inference, they often fail to accurately capture all concepts in multi-concept scenarios, as illustrated in Fig. 1. To substantiate this observation, we conducted a case study by using concept (bo) and (mam) from the Yo'LLaVA dataset to create simple multi-concept samples by directly concatenating their test sets. Results of recognition accuracy can be found in Fig. 3 (left), which showcases the limitation of separately trained methods.\nInstead of training separately, we unify all concepts' data,"}, {"title": "3.2. Concept token initialization", "content": "Yo'LLaVA (Nguyen et al., 2024) uses high-quality negative samples that are visually similar to a specific concept but represent non-identical objects for training the concept to-kens. We conduct a case study by reproducing Yo'LLaVA with different percentages of high-quality negative samples. As shown in Fig. 3 (right), reducing the number of negative samples decreases the personalization capabilities among all kinds of Yo'LLaVA data, reflecting the heavy reliance of Yo'LLaVA on high-quality negative data. Relying on negative samples creates challenges for multi-concept personalization, as personalizing a new concept necessitates hundreds of high-quality negative samples, which are hard for user to collect. As the number of concepts increases, defining and acquiring high-quality negative samples becomes more challenging.\nThus, we explore whether there is a method to quickly capture information about multiple concepts without requiring high-quality negative samples. Utilizing visual information from images to represent concepts is the most efficient approach. For example, describing 'a mischievous blue cat' may connect various cute cats. However, people will not immediately connect this description to the concept (Tom), the cat from animated films. In contrast, upon seeing an image of (Tom), one can easily recognize the concept.\nTherefore, as shown in the right half of Fig. 2, We propose a method that utilizes concept images to initialize the concept tokens. For each concept, given a set of images I\u00b9,...,In, We utilize the LLaVA vision encoder E and the projection layer P to obtain aligned visual tokens P1, ..., Pn, which have the shape of n x (24\u00d724) \u00d7 C. To reduce redundant background information in the visual tokens, we apply the Grounded-SAM (Ren et al., 2024) with the prompt \"the main character in the image\" to obtain a mask M\u00b9,...,Mn for each image.\nAfter aligning each mask's shape with the visual tokens, we perform an element-wise Hadamard product between Mi and Pi. This process yields concept-related visual tokens, denoted as Q\u00b9,...,Qn.\nTo obtain a more compact concept representation and reduce the number of visual tokens to k, we applied k-means clustering (Hartigan & Wong, 1979) to the concept-related visual tokens, resulting in K1,...,Kk, with a size of k \u00d7 C, where k denotes the final token length corresponding to each concept. The entire process can be formulated as follows:\n\\(K = k\text{-means}\\left(\\{x \\in M^{i} \\cdot P(E(I^{i})) | x \neq 0\\}\right)\\) (2)\nFor the special token (sks), we average the clustered k \u00d7 C values to create a special token with a shape of 1 x C. Ultimately, we derive concept tokens Q with dimensions (k + 1) \u00d7 C, constructed from visual tokens related to the concepts. During experiment, we find that this initialization accelerates convergence, as detailed in Sec. 5.5, enabling VLMs to be trained efficiently in multi-concept scenarios."}, {"title": "3.3. Training details", "content": "To train MC-LLaVA, we need to construct training samples. We leverage a unified training data form (I,Xq, Xa), where I is the input image, Xq is the question, and Xa is the answer. We collect training samples from the following tasks:\n\u2022 Positive recognition: To better integrate concepts into VLMs, we adopt a positive recognition task follow-ing Yo'LLaVA, assigning multiple positive recognition conversations to each concept image.\n\u2022 Random recognition: To avoid repetitive \u201cYes\u201d re-sponses from the model, we randomly select 100 im-ages from CC12M (Changpinyo et al., 2021) as inputs for the negative recognition Task. These images are paired with conversations generated from a negative recognition template, eliminating the need for visually similar which are convenient to collect."}, {"title": "4. Dataset", "content": "In this section, we provide a comprehensive explanation of the process involved in creating our multi-concept dataset. The dataset includes a training set with single-concept images and a testing set containing both single and multi-concept images, totaling approximately 1.6K images. Sec. 4.1 elaborates on our approach to effectively collecting large-scale images; Sec. 4.2 discusses creating high-quality QA training data and testing ground truth annotations pro-duced by the GPT-40 model. Tab. 1 compares our dataset with recent datasets in the VLM personalization field. Our dataset is better due to its support for multiple concepts, more advanced captions, and a larger sample size."}, {"title": "4.1. Image data collection", "content": "There is a lack of large-scale, high-quality datasets in the area of VLM personalization. Recent datasets largely depend on manual photography, which is difficult to obtain in scenarios with multiple concepts. Moreover, user privacy concerns also hinder the scalability of their data collection paradigms. To address the limitations in image data collection, we carefully capture images from various movies that contain multiple characters. This method enables the collection of data with multiple concepts, allowing us to concentrate on instances where several concepts occur together. Detailed statistics of dataset is provided in Tab. 2.\nFor training, we collect ten images per character, ensuring clear facial features and varied attire and backgrounds for diversity. Testing includes single and multi-concept images; single-concept images follow the training collection process. For multi-concept images, we use predefined character pairs, selecting frames with both characters visible and balanced. Fig. 4 shows dataset examples."}, {"title": "4.2. GPT4o-assisted data generation", "content": "After acquiring the training and testing images, we utilize GPT-40 to help generate question-and-answer pairs. For the training images, we first prompt GPT-40 to generate"}, {"title": "5. Experiment", "content": "We evaluate MC-LLaVA's recognition, QA, VQA, and captioning capabilities on our new dataset. Sec. 5.1 outlines the experimental setup. Sec. 5.2 presents recognition results, while Sec. 5.3 analyzes QA performance. Captioning ability is shown in Sec. 5.4. Finally, ablation studies in Sec. 5.5 illustrate our method's efficiency and effectiveness."}, {"title": "5.1. Experimental setup", "content": "Baselines. We compare our MC-LLaVA with naive prompting strategies and other VLM personalization methods:\n\u2022 LLaVA: Vanilla LLaVA (Liu et al., 2024) without any personalized information provided;\n\u2022 LLaVA+Prompt: We first prompt LLaVA to generate captions for all training images of the concept, and then utilize these personalized captions in two ways: (I) We concatenate all captions to create a comprehen-sive caption of the concept. (II) We prompt LLaVA to summarize these captions into a concise, personalized description. During inference, we add m relevant cap-tions to the input to provide information specific to the concept being tested, where m represents number of concepts to be evaluated.\n\u2022 Yo'LLaVA (Nguyen et al., 2024): We perform separate training for each concept. It is important to note that we engage in text-only conversations, in accordance with established literature. During testing, we combine the concept token corresponding to each concept with the extended parameters in the language model head.\n\u2022 GPT4o+Prompt: Similar to LLaVA+Prompt, but changing base model to GPT-40, which acts as the up-per bound among downstream tasks. Notably, GPT40 used for testing differs from the one employed for data generation, avoiding the risk of knowledge leakage.\nAll the above baselines and ours are tested on both single-concept and multi-concept scenarios, as shown in Tab. 3.\nImplementation details. We choose 10 images for each concept when training and set the number of concept tokens as 16, denoted as k. We adopt the AdamW (Kingma, 2014) optimizer with a 0.001 learning rate to finetune LLaVA-1.5-13B for up to 15 epochs."}, {"title": "5.2. Recognition ability", "content": "To evaluate MC-LLaVA's recognition ability, we carry out experiments in both single-concept and multi-concept sce-narios. In these experiments, samples representing each concept in the test set were used as positive examples. Negative examples were sourced from images of other concepts within the same scenario, as well as randomly selected im-ages from unrelated scenarios in the dataset. In total, we utilized 3,155 and 2,665 test samples for the single-concept and multi-concept scenarios, respectively. Each test uses the query: \"Can you see (concept) in this photo? Answer"}, {"title": "5.3. Question answering ability", "content": "In addition to identifying specific concepts, VLMs need to perform question-answering (QA) in personalized sce-"}, {"title": "5.4. Captioning ability", "content": "We now compare MC-LLaVA's captioning capability with other baselines. Following MyVLM (Alaluf et al., 2025), we measure each method's ability to incorporate the con-cept identifier within the generated caption. To adapt to our multi-concept framework, we redefine the recall calculation: For images that contain a single concept, correctly identi-fying the concept scores 1 point, while omissions score 0. For images containing multiple concepts, if n concepts are present and m concepts are correctly identified, the score is calculated as m/n. As shown in Tab. 3, our proposed method outperforms Yo\u2019LLaVA, demonstrating a 12.4% im-provement. Even with a joint training strategy, Yo'LLaVA still underperforms, indicating that for tasks heavily reliant"}, {"title": "5.5. Ablation study", "content": "Number of trainable concept tokens. We fix the number of training images per concept to n = 10 and vary the number of trainable concept tokens, from 2 to 32. As illustrated in Fig. 5, increasing the length of trainable tokens enhances the model's recognition ability for both single and multiple concepts, especially when the token length exceeds 8. Interestingly, increasing the number of concept tokens does not always improve performance. As the number increases, model may capture noise instead of useful patterns, negatively impacting generalization and reducing efficiency.\nTraining progress We conduct experiments on Yo'LLaVA and MC-LLaVA to assess the impact of with or without high-quality negative samples and concept token initialization. As illustrated in the two leftmost images of Fig. 6, models with concept token initialization converge faster compared to those without initialization. Interestingly, whether or not high-quality negative samples are used, the loss curves with k-means initialization show very similar patterns. This suggests that k-means initialization can partially replace high-quality negative samples. This finding underscores efficacy of our design in reducing dependency on high-quality negative samples while accelerating model convergence.\nConcept token initialization We assess the effectiveness of our proposed method for initializing concept tokens across different downstream tasks. As shown in the right image of Fig. 6, using concept token initialization leads to observed performance improvements across all tasks in both single-concept and multi-concept tests. In addition to significant improvements in various visual tasks, such as recognition, visual question answering (VQA), and captioning, our design also results in a slight enhancement in the model's performance on text-only tasks. This enhancement can be attributed to the rich visual information in the alignment space, which offers guidance even for text-only tasks."}, {"title": "6. Conclusion", "content": "We present MC-LLaVA, a novel multi-concept personalized vision-language model that greatly improves the accuracy and efficiency of personalized responses in multi-concept scenarios through joint training strategy and concept token initialization. Our work not only advances the frontiers of VLM personalization but also offers a high-quality multi-concept dataset for future studies in the field. The excellent performance of MC-LLaVA in various types of tasks, such as recognition, question answering, and captioning, highlights its ability to generate personalized responses on user-provided concepts. With the growing demand for personalized services, MC-LLaVA and its associated dataset provide strong foundations for developing more intelligent and user-specific visual-language assistants. This advancement has further paved the way for new possibilities in personalized, real-world applications and transformed how we interact with user assistants."}]}