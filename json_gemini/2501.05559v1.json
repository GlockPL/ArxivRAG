{"title": "SOUP TO GO: MITIGATING FORGETTING DURING CONTINUAL LEARNING WITH MODEL AVERAGING", "authors": ["Anat Kleiman", "Gintare Karolina Dziugaite", "Jonathan Frankle", "Sham Kakade", "Mansheej Paul"], "abstract": "In continual learning, where task data arrives in a sequence, fine-tuning on later tasks will often lead to performance degradation on earlier tasks. This is especially pronounced when these tasks come from diverse domains. In this setting, how can we mitigate catastrophic forgetting of earlier tasks and retain what the model has learned with minimal computational expenses? Inspired by other merging methods, and L2-regression, we propose Sequential Fine-tuning with Averaging (SFA), a method that merges currently training models with earlier checkpoints during the course of training. SOTA approaches typically maintain a data buffer of past tasks or impose a penalty at each gradient step. In contrast, our method achieves comparable results without the need to store past data, or multiple copies of parameters for each gradient step. Furthermore, our method outperforms common merging techniques such as Task Arithmetic, TIES Merging, and WiSE-FT, as well as other penalty methods like L2 and Elastic Weight Consolidation. In turn, our method offers insight into the benefits of merging partially-trained models during training across both image and language domains.", "sections": [{"title": "1 INTRODUCTION", "content": "Fine-tuning deep learning models on new tasks often leads to catastrophic forgetting: the rapid degradation of performance on previously learned tasks (Scialom et al., 2022; Lesort et al., 2019; Delange et al., 2021; Belouadah et al., 2021; Luo et al., 2023). This poses a major challenge for continual learning (CL) scenarios, where data comes in a stream of sequences of tasks that may not reappear. As such, we are in need of fine-tuning procedures that would allow models to continually adapt to new knowledge without sacrificing past abilities.\nPrevious work has analyzed catastrophic forgetting of different types of information, as well as the impact of scale. Scialom et al. (2022) explain that LLMs can perform worse on past fine-tuning tasks as they learn new ones. Furthermore, Luo et al. (2023) show a model can also forget general knowledge, not specific to a single past task. Finally, forgetting also grows in severity as model size increases (Luo et al., 2023).\nExisting state-of-the-art approaches to mitigate forgetting primarily focus on modifying the training data used in fine-tuning. These methods either maintain a data buffer of past tasks (Robins, 1995; Lopez-Paz & Ranzato, 2022; de Masson d'Autume et al., 2019), or generate approximations of past task data for joint training with current tasks (Shin et al., 2017; Mocanu et al., 2016). However, both strategies introduce additional costs. Data buffers increase memory overhead and require careful management, while generating data approximations necessitates extra training and computational resources. Likewise, more classical methods of CL that incorporate a penalty directly into training to constrain weights ((Kirkpatrick et al., 2017), L2 penalty) are memory-intensive as they require storing multiple copies of model parameters to be used at each gradient step.\nRecently, buffer-free and computationally efficient model merging techniques ((Wortsman et al., 2022b)Ilharco et al. (2023)) have been proposed to address forgetting in CL. However, in scenarios involving numerous tasks or domains with significant variation, these methods often"}, {"title": "2 RELATED WORK", "content": "Forgetting and Continual Learning A large and growing body of literature investigates different aspects of catastrophic forgetting in continual and sequential learning. When the training data consists of disjoint tasks, training classifiers can cause catastrophic forgetting (Rebuffi et al., 2017). Furthermore, if forgetting occurs, it can be tracked during training and is dependent on when examples are seen by the model: models are less likely to remember earlier training examples (Jagielski et al., 2022; Tirumala et al., 2022). Interestingly, forgetting can also occur for general knowledge rather than for specific tasks, and is more severe for larger models (Luo et al., 2023). Lesort et al. (2022) show that overlap between tasks and task repetition in continual learning settings can mitigate catastrophic forgetting of such examples resulting in solutions to forgetting that involve maintaining a data buffer with past data. Such solutions can also be extrapolated to LLMs where continual learning with data repetition can prevent catastrophic forgetting (Scialom et al., 2022). Mitigating forgetting in continual learning can also occur by introducing a penalty in the loss objective. L2 penalty in continual learning constrains the weights of a model as it is learning a new task by introducing a penalty based on the difference between the current and initial model's weights. Similarly, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) also introduces a penalty to constrain the weights of a model and mitigate increased loss on learned tasks while incorporating the importance of specific weights on learned tasks."}, {"title": "3 METHODOLOGY: SEQUENTIAL FINE-TUNING AVERAGING (SFA)", "content": "Our method, Sequential Fine-tuning Averaging (SFA), leverages existing techniques in model merging (Ilharco et al., 2023; Wortsman et al., 2022a;b) to mitigate forgetting in the continual learning setting. In this method, we consider a pretrained model that is fine-tuning on a sequence of tasks or domains. While the model is being fine-tuned on the current task, we periodically average the parameters of the current model with an earlier checkpoint that resulted from fine-tuning on previous tasks. We then continue fine-tuning this new averaged model on the current task.\nMore precisely, let $\\theta_0$ denote the parameters of the network optimized for previous tasks. Let $\\theta_{t+1}$ be the parameters of the model after taking a gradient step on a new task at $1 \\leq t \\leq T$ using current model parameters $\\theta_t$. Then, every $pT$ iterations, as well as at the end of fine-tuning, we reset the parameters to be a weighted combination of $\\theta_0$ and $\\theta_{t+1}$, where the weighing is determined by a hyperparameter $0 < \\beta < 1$ (default: 0.5).\nAlgorithm 1 Sequential Fine-tuning Averaging During Task Fine-tuning\nInput: $\\theta_0, p, \\beta, T$\nfor t in 1, ..., T\n$\\theta_{t+1}' = \\theta_t - \\alpha \\nabla_{\\theta_t} L_{task}$\nif t mod pT = 0 then\n$\\theta_{t+1} = (\\beta)\\theta_0 + (1 - \\beta)\\theta_{t+1}'$\nelse\n$\\theta_{t+1} = \\theta_{t+1}'$\nif T mod pT \\neq 0 then\n$\\theta_{T+1} = (\\beta)\\theta_0 + (1 - \\beta)\\theta_{T+1}'$\nWe show how our method roughly approximates L2-regression in Section 6. By averaging with an optimized model of the last learned task $\\theta_0$, our method prevents the current model parameters from moving significantly from the original model's and thus losing optimal performance on past tasks (Appendix A.2). In this way, our technique combines the intuition of continual learning with Rehearsal (Robins, 1995), Task Arithmetic (Ilharco et al., 2023) and WiSE-FT (Wortsman et al., 2022b). However, unlike Rehearsal-based methods that store data in a buffer, we use a model fine-tuned on past tasks/domains. Furthermore, unlike Task Arithmetic, our method merges a past checkpoint of a given model with the current model, rather than the task vectors from individual models. Finally, while our method focuses on merging during actual fine-tuning and across tasks/domains, WiSE-FT merges a pretrained and a fine-tuned model. In this way, our work generalizes WiSE-FT throughout continual learning. As the number of tasks increases, we continue to average the most recent initial model $\\theta_0$, which has high performance on all previous tasks, with the current model parameters. As such, after finishing fine-tuning with SFA on a new task, we update $\\theta_0$ to be the merged model of all tasks seen so far. In Section 5 we show that SFA is able to preserve performance on all past tasks through continuous averaging."}, {"title": "4 DATA", "content": "In order to measure and mitigate forgetting, we fine-tune our models on both a stream of image classification tasks, and 3 distinct language domains: Law, Math and Code.\nIn our classical continual learning setting, we construct a stream of 20 tasks from Food-101 (Bossard et al., 2014), as well as a stream of 20 tasks from CIFAR-100 (Krizhevsky, 2009). For Food-101, we construct our tasks by grouping 5-labels together for all labels except 100. For CIFAR-100, we group 5-labels together for all labels.\nFor each language domain, we fine-tune our model on a dataset featuring domain-specific knowledge, as well as unique instruction tasks. For Law, we combine CaseHOLD (Zheng et al., 2021), Terms of Service (ToS) (Lippi et al., 2019; tos, 2023), and Overruling (Zheng et al., 2021) to create a more general Law dataset. For Math, we use MetaMathQA (Yu et al., 2023), and for Code we use MagiCoder110k (Wei et al., 2023). We believe that required task knowledge across these 3 domains is distinct with minimal overlap. As such, we purposefully aim to test our models' ability to generalize across a wide range of knowledge to measure the validity of our method under maximal domain shifts.\nEvaluation Metrics for Data: In our work, we reference the forgetting of various tasks. We define forgetting specific knowledge as a decrease in performance on a given evaluation task between the current model and the original model before fine-tuning. For example, if evaluation performance on Task A drops when a model fine-tunes on Task B, given that the model has already fine-tuned on task A, we consider the model to forget Task A. To evaluate performance on our fine-tuning data, we use the metrics and holdout sets described in Table 8."}, {"title": "5 RESULTS", "content": "5.1 CLASSICAL CONTINUAL LEARNING: MITIGATING FORGETTING ACROSS SEQUENCES\nOF IMAGE TASKS\nWe first fine-tune models on sequences of image tasks from both Food-101 and CIFAR-100 (as described in Section 4) while applying different forgetting mitigation strategies. Then, we measure each final model's average task accuracy (Fig. 1). We first provide an upper baseline by simultaneously fine-tuning the initial model on all tasks to obtain a multitask fine-tuning model which performs well on average (black star). However, given a continual learning setting where task data appears sequentially, this is infeasible. Next, sequential fine-tuning without intervention (red bar), as expected, results in low average accuracy due to the catastrophic forgetting of earlier tasks. To mitigate this forgetting, we implement 2 variations of Rehearsal with a data buffer: one that includes 5% past task data and 95% current task data, and another that includes 10% past task data and 90% current task data (pink dashed horizontal lines). We use these implementations to create a Rehearsal region of commonly used data buffer sizes to compare other methods to. Rehearsal is a common technique for mitigating forgetting in continual learning. It involves maintaining a buffer of past task data and interleaving it with new task data during fine-tuning (Robins, 1995), where the size of the buffer is a hyperparameter. A data buffer however, has significant drawbacks: it requires storing data from all previous tasks, leading to rapidly increasing storage costs as the number of tasks, and the size of the buffer grow. It also adds to the training cost, because we must continue to train on data from past tasks. Furthermore, maintaining a subset of past data can also threaten data privacy and security (Li et al., 2024).\nThis makes model based mitigations of forgetting appealing. As such, we next apply commonly used model merging methods. Task Arithmetic (Ilharco et al., 2023) (blue bar) and WiSE-FT (Wortsman et al., 2022b) (orange bar). As we explain in Section 3, WiSE-FT is technically equivalent to SFA with p = 1, because it only involves merging the final trained and initial model. Finally, we compare these baselines to our method, SFA, which merges a partially-trained and an initial model, using varying averaging frequency p. SFA outperforms most other methods on both Food-101 and CIFAR-100, and performs comparably to using a reasonably sized data buffer. As expected, most methods except interestingly Task Arithmetic, outperform using no intervention. SFA with varying p generally outperforms both using a smaller-sized data buffer and Task Arithmetic. Finally, SFA (p = 0.98 for Food-101, and p = 0.96 for CIFAR-100) where averaging occurs near the end and after training, achieves higher performance than WiSE-FT or SFA (p = 1) where averaging occurs"}, {"title": "5.2 TESTING METHOD ROBUSTNESS: MITIGATING FORGETTING FROM CROSS LANGUAGE\nDOMAINS", "content": "In the following sections, we test the robustness of SFA by focusing on mitigating pairs of successive instruction fine-tuning tasks with large domain shifts, such as from Math to Code or Math to Law, using datasets outlined in Section 4. By restricting ourselves to pairs of dissimilar tasks, we can clearly quantify the trade off between learning the second task and forgetting the first one by visualizing the results on a plane that measures the accuracy of the first task on the y-axis and the accuracy of the second task on the x-axis. First, we confirm that forgetting occurs when fine-tuning on successive language tasks (Appendix A.4). We present our results for sequentially learning Math and Law with Llama 2 (7B) in Fig. 3, Math and Law with Qwen2.5 (1.5B) in Fig. 4, and Math and Law, as well as Math and Code with Pythia (2.8B) in Fig. 5 (see Appendix A.5 for model descriptions).\nWe first fine-tune our model Llama 2 (7B) in Fig. 3, Qwen2.5 (1.5B) in Fig. 4, and Pythia (2.8B) in Fig. 5) on MetaMathQA to obtain the inital model (dark blue circle). Note the base model performance on the first (second) task is represented by dark green for Llama 2 (7B), dark red for Qwen2.5 (1.5B), and blue for Pythia (2.8B) horizontal (vertical) dashed lines. This initial model improves upon the base model on our Math benchmark and is thus higher on the y-axis (performance on first task) while not being significantly different or being worse on the x-axis (performance on the second task which it has not been trained on yet). We then fine-tune the initial model on the second task to obtain the sequential fine-tuning model (red circle). In Figs. 3 and 4 the second task is Law while in Fig. 5 the second task is either Law or Code. The sequential fine-tuning model performs really well on the second task (higher on the x-axis) while forgetting almost everything it has learned"}, {"title": "5.3 SFA ON CROSS DOMAIN DATA", "content": "Recall that in SFA, we take a model that has already been fine-tuned on Task A, and while fine-tuning on Task B, every pT steps we average the weights with the final model after fine-tuning on Task A and continue fine-tuning on Task B. We evaluate SFA with varying averaging frequency p during cross-domain sequential fine-tuning. Figs. 3 and 5 show that as p decreases, signifying more frequent averaging with the initial model, we observe stronger retention of past domain knowledge"}, {"title": "5.4 AVERAGING WEIGHTS", "content": "To further understand the advantages of SFA, we investigate alternative strategies of manipulating model parameter weights. Unlike the continuous averaging throughout fine-tuning employed by SFA, we explore the impact of modifying weights solely at the final stage. Our results underscore the importance of SFA's continual averaging approach for achieving optimal performance across multiple domains.\nRecall that SFA combines parameters from the initial and current model during fine-tuning. We posit that the initial model represents expertise in past tasks/domains, while the current model embodies new task/domain knowledge. Our default parameter weighting (0.50 for each) provides a balance. We explore if, instead of varying p, the frequency of averaging in SFA, we can get similar flexibility by first fine-tuning the model on a new task (p = 1, or WiSE-FT) and then averaging the final model with the previous task model using different relative weights (vary \u03b2). In Figs. 7 and 8, we show that SFA with p < 1 and \u03b2 = 0.5 (orange curve) performs the same if not better than a sweep of weighting parameter \u1e9e for SFA (p = 1, or WiSE-FT) (blue curve). Furthermore, for SFA (p = 1, or WiSE-FT) with \u03b2 \u2265 0.50, the trade off between Math and Law for both Pythia (2.8B) and Llama 2 (7B) is especially large, resulting in the complete failure to retain math. Likewise, for CIFAR-100 in Fig. 1, we show that varying \u03b2 for SFA (p = 1, or WiSE-FT) is not as effective as SFA (p = 0.96), implying that averaging during fine-tuning, with additional fine-tuning afterwards offers additional performance benefits. This suggests that SFA's continual averaging during fine-tuning is key to its success in preserving cross-domain, and sequential task competence."}, {"title": "6 SFA AND L2-REGRESSION: INTUITION FOR MODEL MERGING", "content": "There exist many methods of continual learning that aim to mitigate forgetting of past tasks by constraining training weights using a penalty. This penalty is often used to prevent weights from straying from model weights that perform well on past tasks. Some methods include L1 and L2 penalty, as well as EWC (Kirkpatrick et al., 2017). Typically, these methods add a penalty to an existing loss objective for every gradient step. This becomes computationally expensive as models scale for modern day applications, because for each gradient step, multiple copies of model weights have to be loaded in memory to calculate the penalty (e.g. the initial and currently training model), in addition to potential gradients. However, our work roughly approximates existing continual learning methods with model merging, thereby making them feasible to implement. Specifically, we can show that SFA resembles L2-regression. Consider, starting with $\\theta_0$, the model trained on the previous task and $\\theta_t$, the model currently being trained on the new task. Calculating the loss with an L2 penalty takes the form\n$L(\\theta_t) = L_{task}(\\theta_t) + \\frac{\\lambda}{2} ||\\theta_t - \\theta_0||^2.$\nUpdating the model once using the gradient of this loss results in\n$\\theta_{t+1} = \\theta_t - \\eta(\\nabla_{\\theta_t} L_{task} + \\lambda (\\theta_t - \\theta_0)).$\nThis can be rewritten as\n$\\theta_{t+1} = (1 - \\eta \\lambda) \\theta_t + (\\eta \\lambda) \\theta_0 - \\eta \\nabla_{\\theta_t} L_{task}.$\nNow we can compare this to an extreme case of SFA with averaging occurring after each gradient step. As such, following the setup in Algorithm 1 given some T, for each gradient step, current model parameters are first updated using only task loss, before being averaged with initial model:\n$\\theta_{t+1}' = \\theta_t - \\alpha \\nabla_{\\theta_t} L_{task}$\n$\\theta_{t+1} = (1 - \\beta)\\theta_{t+1}' + \\beta(\\theta_0).$"}, {"title": "7 CONCLUSION", "content": "After showing how quickly a given model can forget learned tasks as it sequentially fine-tunes on new ones, we evaluate methods that aim to mitigate this forgetting. We introduce SFA and show how, by treating a past model as representative of past data, we can use parameter averaging to retain knowledge of past tasks during fine-tuning on new ones. We likewise compare SFA to a range of baselines during both classical continual learning, as well as maximal domain shifts. Finally, we provide intuition for why SFA works by showing how it roughly approximates L2-regression, and in turn show how model merging methods can approximate imposing a penalty. The final performance of SFA tends to outperform other merging, as well as penalty methods. Furthermore, it is comparable to continual learning with rehearsal, but has the advantage of not maintaining a data buffer."}, {"title": "A.1 BAYESIAN INTERPRETATION", "content": "We have shown that our method approximates, and sometimes is equivalent to minimizing an L2-regression loss during training. Next we use the well known point that L2-regression has a Bayesian Interpretation (bay, 2018) to motivate our method:\nAssume that the prior distribution of the ideal model $\\theta$ for a past and current task is Gaussian with mean the initial model, $\\theta \\sim N(\\theta_0, \\tau^2 I)$ for some $\\tau$. Furthermore, assume that the distribution y given input X, model weights $\\theta_t$, and a function f is Gaussian with mean the output of the function given X, $\\theta_t$: $y \\sim N(f(X,\\theta_t), \\sigma^2 I)$ As such, the posterior of $\\theta$ is:\n$p(\\theta | y, X, f) \\propto exp[ -\\frac{1}{2\\sigma^2} (y - f(X, \\theta))^T (y - f(X, \\theta)) - \\frac{1}{2\\tau^2} (\\theta - \\theta_0)^T (\\theta - \\theta_0)]$\nWe can compute the Maximum a Posteriori (MAP) for $\\theta$:\n$\\theta^* = argmax_{\\theta} exp[- \\frac{1}{2\\sigma^2} (y - f(X, \\theta))^T (y - f(X,\\theta)) - \\frac{1}{2\\tau^2} (\\theta - \\theta_0)^T (\\theta - \\theta_0)]$\n$\\theta^* = argmin_{\\theta} \\frac{1}{\\sigma^2} (y - f(X, \\theta))^T (y - f(X, \\theta)) + \\frac{1}{\\tau^2} (\\theta - \\theta_0)^T (\\theta - \\theta_0)$\nSet $\\frac{\\sigma^2}{\\tau^2} = \\lambda$\n$\\theta^* = argmin_{\\theta} (y - f(X, \\theta))^T (y - f(X, \\theta)) + \\lambda (\\theta - \\theta_0)^T (\\theta - \\theta_0)$\nAs such, L2-regression tries to solve this Bayesian interpretation (Equation 11). As shown previously, SFA approximates L2-regression. This suggests that SFA may have a Bayesian motivation."}, {"title": "A.3 EWC APPROXIMATED BY MODEL MERGING", "content": "Consider fine-tuning a model with an EWC penalty (Kirkpatrick et al., 2017) where $\\lambda = 1, j = 1, ..., \\theta$\n$L(\\theta_t) = L_{task}(\\theta_t) + \\frac{\\lambda}{2} \\Sigma F_0^{(j)} (\\theta_t^{(j)} - \\theta_0^{(j)})^2$\nwhere $\\theta$ and $\\theta_t$ are the weights of the initial and fine-tuning model respectively. $\\eta$ is a hyperparameter, and $F_0$ is a diagonal matrix with the initial model's Fisher information. Assume that this loss update is split into 2 model updates. First, update model parameters using task loss on current weights:\n$\\theta_{t+1}' = \\theta_t - \\eta \\Delta_{\\theta_t} L_{task}$\nThen, update model parameters using EWC penalty:\n$\\theta_{t+1} = (I - \\eta F_0) \\theta_{t+1}' + \\eta F_0 \\theta_0$\nThus, applying the EWC penalty can be understood as model merging weighted by the Fisher information of the initial model. This is reminiscent of Fisher model merging from Matena & Raffel (2022) where merging an initial and fine-tuning model has the form:\n$\\theta^*^{(j)} = \\frac{\\lambda_0 F_0^{(j)} \\theta_0^{(j)} + \\lambda_t F_t^{(j)} \\theta_t^{(j)}}{\\lambda_0 F_0^{(j)} + \\lambda_t F_t^{(j)}}$\nwhich can be rewritten as\n$\\theta^*^{(j)} = (\\frac{\\lambda_0 F_0^{(j)}}{\\lambda_0 F_0^{(j)} + \\lambda_t F_t^{(j)}}) \\theta_0^{(j)} + (\\frac{\\lambda_t F_t^{(j)}}{\\lambda_0 F_0^{(j)} + \\lambda_t F_t^{(j)}}) \\theta_t^{(j)}.$\nUnlike the EWC approximation, this uses the Fisher information of both the initial and current model for merging."}, {"title": "A.4 FORGETTING UNDER SEQUENTIAL FINE-TUNING", "content": "We start by confirming that fine-tuning on a sequence of different tasks leads to performance degradation on previously learned tasks. This forgetting phenomenon occurs across different task domains and for different model sizes. In this work, we focus on catastrophic forgetting of capabilities acquired during instruction fine-tuning instead of base pretrained model capabilities. This is because, as we will show, forgetting of skills learned during instruction finetuning can be quite severe and experiments at this scale are more feasible. We fine-tune our models on a sequence of instruction, language generation datasets that test general knowledge to measure forgetting. Specifically, we use Scialom et al. (2022)'s: Text Simplification (Simpl), Inquisitive Question Generation (InqQG), Headline Generation with Constraint (HGen), COVID-fact, Covid QA (CQA), and Twitter Stylometry (TwSt). Many of these tasks incorporate existing datasets which we describe in Appendix A.6. In our first experiments, we fine-tune the T0_3B (3B) and T0pp (11B) models (see Appendix A.5 for model descriptions) on the sequence of tasks described in Section 4 while measuring forgetting on the first task. The results are shown in Fig. 11. The model is first trained on Simpl which leads to a decrease in validation loss shown in blue. Subsequently, the model is trained on a sequence of other tasks; the decrease in validation loss on these tasks is shown in different colors. During this process, we continue to monitor the validation loss on Simpl, displayed in pink. As models fine-tune on new tasks, their performance on Simpl consistently declines as loss increases. This is true at both the 3B and 11B (Fig. 11) model scales, indicating that merely scaling up parameter size does not help mitigate forgetting despite the increased capacity.\nBut how severe is this forgetting? We quantify this by comparing a model that was trained on and has then forgotten Simpl to a model that has never seen Simpl. In Fig. 12, the pink line shows validation loss on Simpl for a model trained on a sequence of fine-tuning tasks starting with Simpl. As the model learns new tasks, its performance deteriorates. After 2000 steps, the sequentially fine-tuned model's loss on Simpl is the same order of magnitude as that of the multitask model trained on all tasks except Simpl. Thus if a model that has learned Simpl is finetuned on other tasks for as little as 2000 steps, its performance degrades to that of a model that has never seen Simpl. This indicates significant forgetting, as the model loses the ability to respond to tasks it previously was able to."}]}