{"title": "Psychometrics for Hypnopaedia-Aware Machinery via Chaotic Projection of Artificial Mental Imagery", "authors": ["Ching-Chun Chang", "Kai Gao", "Shuying Xu", "Anastasia Kordoni", "Christopher Leckie", "Isao Echizen"], "abstract": "Neural backdoors represent insidious cybersecurity loopholes that render learning machinery vulnerable to unauthorised manipulations, potentially enabling the weaponisation of artificial intelligence with catastrophic consequences. A backdoor attack involves the clandestine infiltration of a trigger during the learning process, metaphorically analogous to hypnopaedia, where ideas are implanted into a subject's subconscious mind under the state of hypnosis or unconsciousness. When activated by a sensory stimulus, the trigger evokes conditioned reflex that directs a machine to mount a predetermined response. In this study, we propose a cybernetic framework for constant surveillance of backdoors threats, driven by the dynamic nature of untrustworthy data sources. We develop a self-aware unlearning mechanism to autonomously detach a machine's behaviour from the backdoor trigger. Through reverse engineering and statistical inference, we detect deceptive patterns and estimate the likelihood of backdoor infection. We employ model inversion to elicit artificial mental imagery, using stochastic processes to disrupt optimisation pathways and avoid convergent but potentially flawed patterns. This is followed by hypothesis analysis, which estimates the likelihood of each potentially malicious pattern being the true trigger and infers the probability of infection. The primary objective of this study is to maintain a stable state of equilibrium between knowledge fidelity and backdoor vulnerability.", "sections": [{"title": "I. INTRODUCTION", "content": "Cybersecurity stands at the frontline of trustworthy artificial intelligence by addressing evolving threats and preventing malicious actions that could undermine the safety and trust in computational intelligence. Backdoors (or Trojan horses) represent concealed entry points that allow attackers to manipulate the behaviour of a machine and weaponise artificial intelligence, raising serious cybersecurity concerns [1]. A backdoor attack functions by infiltrating a hidden trigger into a machine during its learning phase, which, when activated, causes it to produce predetermined and often harmful responses. It forms a conditioned reflex, an automatic and conditioned response paired with a specific stimulus [2].\nThe implications of backdoors are wide-ranging. In social computing, a backdoor could subvert ethical filters and content moderation, instructing generative artificial intelligence to create and disseminate misinformation. In autonomous vehicles, it could cause misinterpretation of traffic signals, leading to potentially catastrophic accidents. In biometric recognition, it could allow unauthorised access that bypasses security protocols. In the financial industry, fraud detection systems could be compromised, enabling fraudulent transactions under specific conditions. In the healthcare sector, medical diagnostic systems could be manipulated to deliver incorrect diagnoses and treatments. These potential consequences underscore the urgent need for robust countermeasures to prevent, detect, and mitigate the risks and threats posed by backdoors.\nThe dynamic and often uncontrollable nature of data sources further complicates this challenge. This is exacerbated in federated learning (or collaborative learning) due to the presence of compromised nodes [3]. Federated learning enables the decentralisation of data sources, offering benefits, such as promoting large-scale collaboration, preserving privacy, reducing data breach risks, improving data utilisation efficiency, and preventing monopolistic control over data. However, it also comes with risks. Malicious local participants can inject harmful data and false computations (which are not centrally verifiable), potentially introducing backdoors when aggregated into a global model. Furthermore, systems featuring lifelong learning to continuously and incrementally adapt to new data over time may face similar challenges due to dynamic environments that involve crowdsourced data labelling and open data repositories [4]\u2013[7]. To manage these risks, developing a feedback control mechanism that continuously monitors the presence of backdoors is essential to maintaining system integrity and reliability.\nIn this study, we propose a cybernetic framework for mitigating the impact of backdoors in neural machines based on the principles of psychometrics, as illustrated in Figure 1. It consists of a leaner which updates the machine with untrustworthy external data sources under the risks of data poisoning, a controller which steers the machine towards the decision of whether or not unlearn to unlearn, and an unlearner which updates the machine with trustworthy internal data sources and auxiliary information about the backdoor. It begins by performing model inversion to elicit artificial mental images. A multi-scale gradient-descent optimisation algorithm is employed to synthesise artificial mental images in a coarse- to-fine manner. Next, hypothesis analysis is conducted to identify the most likely hypothetical trigger pattern extracted from the artificial mental images using maximum likelihood estimation with outlier exclusion and infer the probability of infection using Bayesian inference. This involves scanning through all potential regions to estimate the criminal coefficient of each regional pattern, based on the machine's response to a small collection of samples. The decision to unlearn or remain intact is then made according to the psychometric profile, codenamed Psycho-Pass, as illustrated in Figure 2. If machine unlearning is activated, a collection of unlearning samples is used for disassociating the hypothetical trigger and its corresponding behaviour. However, side effects lurk due to internal dynamics such as the propagation of uncertainty and stochastic biases in the data and analysis process, potentially deteriorating the performance of the machine. The research objective is to balance the dynamics between a learner agent and an unlearner agent, preserving the fidelity of the machine while minimising its vulnerability to backdoor attacks."}, {"title": "II. PRELIMINARIES", "content": "In this section, we lay the foundation for understanding the landscape of backdoor attacks and defences. We begin by introducing a taxonomy that systematically categorises the diverse characteristics of backdoor attacks. Following this, we delve into both proactive and reactive defence paradigms, outlining strategies to prevent, detect and mitigate these insidious threats. To ensure clarity and relevance, we then delineate the scope of our research, specifying the attack and defence scenarios under investigation. Furthermore, we briefly review solutions for reverse engineering backdoor triggers, which serve as essential benchmarks for comparative study."}, {"title": "A. Backdoor Attacks", "content": "A backdoor is a deliberate vulnerability or loophole inserted into a neural network model that allows an attacker to manipulate its behaviour and compromise its functionality. This manipulation typically occurs by adding specific patterns or triggers to the input data, which the model then incorrectly identifies or responds to. In a nutshell, an attacker with access to the model's learning data or learning process injects a specific pattern or trigger into the data, as illustrated in Figure 3. This pattern could be innocuous or subtle, making it hard to detect during normal operation. Once the model is deployed and in use, the attacker can activate the backdoor by feeding input data that contains the trigger pattern. When the model encounters this trigger, it behaves in a specific, predetermined way, often giving incorrect or malicious outputs. The consequences can vary depending on the context. In a security application, a backdoor might allow an attacker to bypass authentication systems or gain unauthorised access. In a financial application, it could manipulate predictions to favour certain outcomes, leading to fraud or financial losses.\nBackdoor Taxonomy: Understanding backdoor attacks involves several key concepts that shed light on their nature and impact. Space refers to where triggers are applied: either samples in cyberspace (digital environment), or samples in physical space (real-world environment) [8]. Causality defines the mappings between inputs and outputs, either as all-to-one, where multiple samples lead to a single targeted prediction, or all-to-all, where different samples may be linked to different manipulated predictions [9]. Genericity distinguishes whether triggers are uniform across different samples or specific to individual instances [10]\u2013[12]. Optimality reflects whether triggers are arbitrary handcrafted patterns or optimised for maximum effectiveness of backdoor attacks [13]\u2013[15]. Semanticity describes the relationship between triggers and the semantic content of samples, whether triggers are independent of or integrated seamlessly into samples [16]. Visibility concerns whether triggers are perceptible or designed to avoid visible distortions to samples [17]\u2013[19]. In summary, backdoor triggers can be characterised by the following taxonomic descriptions.\n\u2022 Space: Triggers are applied to samples in cyberspace or physical space.\n\u2022 Causality: Triggers cause all-to-one or all-to-all mappings between inputs and outputs."}, {"title": "B. Backdoor Defences", "content": "Defending against backdoor attacks is crucial to ensure the integrity and security of machine learning systems. A variety of defence mechanisms have been developed to counteract backdoor attacks, which can be broadly categorised into proactive (or learning-time) and reactive (or inference-time) paradigms. As the names suggest, proactive paradigm focus on securing the learning data and process in the pre-deployment phase, whereas reactive paradigm aim to offset the impact of backdoors in the post-deployment phase.\nProactive Paradigm: Proactive defence is designed to prevent the insertion of backdoors or mitigate the impact of backdoors during the learning phase. One possible approach is data sanitisation, which involves filtering and erasing potentially poisonous samples from the learning dataset by identifying distinct characteristics or detecting anomalous patterns indicative of backdoor attacks [20]\u2013[24]. Another approach is robust learning, which neutralises the impact of backdoors by introducing randomness during the learning process. For example, adding random transformations to the learning data inflicts perturbations to trigger patterns [25] (e.g. cut-and-paste data augmentation [26]). Regularising gradients and adding random noises in the optimisation process may also enhance robustness [27]\u2013[29] (e.g. differential privacy [30]). Ensemble learning trains a diverse collection of base models with randomised subsets of samples and aggregates the predictions of ensemble models for making inference, assuming that a majority of the base models are unlikely influenced by a minor amount of poisonous data [31]\u2013[33] (e.g. bootstrap aggregating [34]).\nReactive Paradigm: Reactive defence counteracts the presence of backdoors by filtering or purifying either the samples or the models. Malicious samples can be eliminated by monitoring inputs for suspicious or anomalous patterns that could indicate a backdoor trigger or observing the predictions for unusual behaviour that may signal backdoor activation [35]\u2013[37]. These samples can also be purified by perturbing or reconstructing the poisonous regions [38]\u2013[40]. Randomised smoothing can also be viewed as a form of purification, as it adds random noise to the samples to overwhelm injected triggers and makes predictions based on a majority vote over multiple noisy versions of each sample [41]\u2013[43]. Models with Trojans can be detected by constructing a meta-classifier and rejected for deployment if they are determined to be infected [44]\u2013[49]. These models can also be renovated with catastrophic forgetting [50]\u2013[52], knowledge distillation [53]\u2013[55] and neurone pruning [56]\u2013[58]."}, {"title": "C. Problem Statement", "content": "Context and Scope: By applying the background information, we consider a common backdoor attack scenario in which the triggers are applied in cyberspace (space), causing an all-to-one mapping (causality), generic for each sample (genericity), arbitrary handcrafted patterns (optimality), representing semantically independent parts of samples (semanticity), and visible to human perceptual systems (visibility). In addition to this, the attacker has bypassed automated detection and left backdoors in a neural network model during the learning phase (reactive paradigm). On the defence side, we consider a scenario where the original learning dataset is no longer accessible. Access to the dataset may be restricted for the following reasons: to prevent potential misuse or breaches that could compromise sensitive information and individuals' privacy (privacy regulations); to protect the intellectual property and competitive advantages of companies or organisations (proprietary restrictions); due to difficulties and time-consuming retrieval methods associated with archiving and storing (archival policies); because of unsupported formats or incompatible systems (technical barriers); and because the dataset may be outdated, no longer maintained, or otherwise difficult to access (digital obsolescence). Hence, the possibilities of uncovering hidden triggers by inspecting the dataset are restricted.\nObjective and Constraints: This study focuses on neural networks used in image classification tasks. We assume the scenario where exact trigger content may be elusive but constraints on its size are available. In other words, we do not have the precise information about what the trigger looks like, but the range within which its dimension falls. In practice, trigger dimensions are typically large enough to have a notable effect but small enough to evade detection. The research objective is to remove backdoors from a potentially infected model while maintaining its functionality. Although the original dataset is not available, we assume that a small amount of data sampled from the same or similar distribution are acquirable for analysing and unlearning the backdoors. Formally, we are given the following components:\n\u2022 A pre-trained image classification model f : X \u2192 Y, where X \u2282 \u211d\u207f represents the input space (i.e., the space of images represented as n-dimensional vectors), and Y is the set of possible classes. Note that the original training dataset used to train f is no longer available.\n\u2022 A set of candidate masks M characterises potential backdoor triggers, where each mask m \u2208 M is constrained by a set of conditions, such as maximum allowed size or dimensions within an image.\n\u2022 A clean dataset D = {(x,y) | x \u2208 X, y \u2208 Y}, serving both as a normative set for hypothesis analysis and as an unlearning set for machine unlearning, contains instances that are free from backdoor contamination, but comprises a much smaller number of instances than the original training set of f.\nLet x' represent a malicious input generated by applying a backdoor trigger to a clean image x \u2208 X with a target class y'. We seek to find a modified classifier f* : X \u2192 Y that minimises backdoor vulnerability (i.e. the probability that a malicious input is misclassified by f* as the target class)\n$P(f^*(x') = y'),$ (1)\nwhile ensuring knowledge fidelity (i.e. the probability that f* assigns the same classification to a benign input as f)\n$P(f^*(x) = f(x)).$ (2)\nTo assess whether a model is infected and to establish the necessary hyper-parameters for this assessment, some aspect of the model's behaviour must be known a priori. This is because understanding how the model should behave under normal conditions helps in identifying deviations that may indicate infection or compromise. This prior knowledge can be anticipated based on a surrogate model or derived from empirical evidence."}, {"title": "D. Reverse Engineering", "content": "As a result, a key aspect of this study is dedicated to reverse-engineering the trigger within the context of the specified attack and defence scenarios. The related research on this topic is briefly described as follows [59]\u2013[61]. Let y denote a given target label to be analysed, x denote a data sample drawn from a set X and f(\u00b7) denote an infected classification model. To reverse-engineer the most likely backdoor trigger which would cause samples to be classified as the target label y, a common method involves solving the following optimisation problem consisting of a loss function L and a regularisation function R weighted by a hyper-parameter \u03bb:\n$arg min \u2211 L(y, f((1 \u2013 m)x + mz)) + \u03bbR(z,m),$ (3)\n{z,m}\nx\u2208X\nwhere the term inside f(\u00b7) denotes a manipulated sample, created by overwriting a potential trigger z onto a benign sample x using a mask m. This optimisation process finds a pair of z and m that misleads classification (evaluated by L) and satisfies certain prior assumptions, empirical knowledge or practical heuristics (regularised by R). The possible regularisation terms include, but not limited to, the Lp norm, which restricts the size and magnitude of the solutions, as well as the total-variation norm, which encourages smooth solutions. Then, an outlier detection is applied to identify the malicious trigger from all the potential ones generated from the optimisation process."}, {"title": "III. CONCEPTUAL FRAMEWORK", "content": "In this section, we briefly explain the core rationales built into our conceptual framework and illustrate how interdisciplinary concepts are related, providing an overview of its theoretical foundation."}, {"title": "A. Hypnopaedia", "content": "In a metaphorical sense, a backdoor attack can be considered as a form of mind-hacking that indoctrinates or implants an idea into a machine's subconscious mind. A psychological reminiscence for backdoors is hypnopaedia, which refers to learning under the state of hypnosis or unconsciousness, conditioning an individual's beliefs and behaviours without their conscious awareness [62]. A backdoor trigger is analogous to a hypnotic suggestion used to subject an individual undergoing hypnosis to the command of a hypnotist."}, {"title": "B. Cybernetics", "content": "This study applies cybernetic principles to manage the risk of backdoors arising from dynamic data sources. Cybernetics is the study of automation with an emphasis on circular causality and regulatory feedback for controlling systems automatically [63]. Feedback loops are fundamental to cybernetics because they enable systems to self-regulate and react to changes in their environment (external dynamics) or within themselves (internal dynamics). Let us take thermostat as an example to demonstrate how cybernetic principles are applied in a simple feedback control system [64]. A thermostat operates continuously, monitoring the temperature of a room (the controlled variable) and reacting to changes in the environment. Its goal is to maintain a stable temperature around a set-point. It contains a sensor that detects the current temperature and a controller that governs whether to turn on or off the heating or cooling system based on a desired set-point. It then sends control signals to an actuator to adjust the temperature accordingly."}, {"title": "C. Metacognition", "content": "Analogously, a backdoor-aware learning machine can be modelled as a cybernetic system. A learning machine (or its state and parameters) is analogous to the temperature of a room, which is the variable being controlled. A learner updates the machine constantly to adapt to continuous streams of new information and reports the changed state, acting like a sensor which observes and measures external dynamics from an ever changing real world. A controller evaluates whether there are any backdoors present in the current state and controls whether actions need to be taken to address detected backdoors. It empowers a machine with metacognition, referring to the awareness of one's own cognition and knowledge, thereby allowing a machine to analyse and monitor its own thinking patterns [65]. An unlearner functions like an actuator that either reacts to the detected backdoors or maintains the current state based on the control decision. If a reaction is needed, it updates the machine with an unlearning set of samples (alongside other auxiliary knowledge) to remove potential backdoors."}, {"title": "D. Motivated Forgetting", "content": "Machine unlearning parallels a psychological phenomenon of motivated forgetting, where people suppress or repress unwanted memories consciously or unconsciously [66]. This can occur due to the desire to suppress unpleasant memories or reduce cognitive dissonance. Similarly, machine unlearning describes the process where a machine forgets or adjusts its learned patterns and associations [67]. This is often necessary when the model has learned something undesirable, inaccurate or outdated. If the backdoor trigger is estimated through reverse engineering, the machine can be fine-tuned to disassociate its behaviours from the estimated trigger, thereby reducing the influence of the backdoor."}, {"title": "E. Memory Retrieval", "content": "Nevertheless, trigger estimation can be challenging since there is a vast amount of potential trigger patterns and target behaviours. Since the backdoor is typically introduced during the learning phase, a logical solution is to inspect the learning dataset. However, the original dataset is often inaccessible due to constraints such as privacy regulations, proprietary restrictions, archival policies, technical barriers, and digital obsolescence. As an alternative, one approach is to extract information directly from a machine's memory. That is, model inversion is a memory retrieval technique that reverse-engineers a model to infer information about its learning dataset [68]. This can be achieved by submitting queries to a model iteratively and adjusting the query based on its response, finding the optimal query that maximising the activation through trial and error. In investigative psychology, there is a similar technique used by law enforcement during criminal investigations to retrieve information about a crime scene from eyewitnesses, referred to as cognitive interview [69]. It involves multiple questioning techniques and mnemonic strategies to facilitate the mental process of recall or recollection, eliciting memories associated with a specific event from the past."}, {"title": "F. Butterfly Effect", "content": "In the context of memory retrieval, an individual's recall might stabilise around certain dominant narratives or repeated rehearsals. This phenomenon may also occur in model inversion, where the outcomes consistently return to a set of convergent but potentially suboptimal patterns. This limited set of patterns can be thought of as an attractor in a subject's memory. In chaos theory, an attractor is a cluster of states towards which a system tends to evolve, regardless of small variations in initial conditions. In essence, to escape or diverge from an attractor means disrupting the stability of the system, making it more sensitive to initial conditions. This concept is reminiscent of the butterfly effect, which illustrates how small changes in initial conditions can significantly influence a dynamic system's orbital trajectory, leading to vastly diverse outcomes. In cognitive interview, recall can be constrained by the wording of the questions, which acts as an attractor in chaos theory influences the trajectory of a system, guiding it towards certain states or behaviours. Varied prompts and diverse questions may then be used to diverge from this attractor, encouraging broader and more accurate recall. In model inversion, divergence from attractors can be encouraged by introducing stochastic processes."}, {"title": "G. Psychometrics", "content": "The outcomes of model inversion can be viewed as representational content that reflects the internalised knowledge of a model, reconstituted in a form that resembles the learning set of samples (or projected back to the sample space). These outcomes resemble mental imagery in the human mind, serving as a conceptual representation of things and experiences [70]\u2013[72]. The objective is to identify a potentially malicious trigger pattern within the realm of the mind's visual representations, analogous to psychometric assessment of an individual's potential for criminality. Such a pattern could induce sensory deprivation and stimulate hallucinations, distorting the perception of a machine [73]. In other words, such hallucinatory patterns can manifest through backdoor activation that consistently diverts the machine's behaviours from expected outcomes. Therefore, the likelihood of a hypothetical pattern being the actual trigger and the probability of infection can be quantified through activation statistics."}, {"title": "IV. METHODOLOGY", "content": "Our proposed method consists of three parts: model inversion, hypothesis analysis and machine unlearning. Model inversion retrieves artificial mental images that represent prototypes for all possible classes of samples. Hypothesis analysis quantifies the likelihood of each hypothetical pattern drawn from the artificial mental images being the actual trigger with the aid of a normative set of data, and estimates the probability that a machine is infected. Machine unlearning applies the most likely trigger pattern on an unlearning set of data to disassociate it from the conditioned response. Both normative and unlearning sets of data contain a small number of correctly labelled samples and may overlap. An overview of the proposed method is outlined in Figure 4."}, {"title": "A. Model Inversion", "content": "Model inversion aims to invert a machine-learning model to infer the information about its learning data. The objective is to find an n-dimensional synthetic input zy \u2208 \u211d\u207f that minimises the discrepancy between the output of the model f(zy) and a target output y. This can be metaphorically seen as retrieving an artificial mental image about a particular object in a machine's memory. Mathematically, this can be formulated as finding the optimal synthetic input for an unconstrained minimisation problem:\n$arg min L(y, f(zy)),$ (4)\nzy\nwhere L denotes a loss function measuring the discrepancy between the ground-truth response and the prediction. For multinomial classification, the loss is usually calculated using cross-entropy or negative log-likelihood.\nGradient Descent: Gradient descent, a first-order optimisation algorithm, offers a principled approach to model inversion by iteratively adjusting and updating the input data in the direction that minimises a given loss function. Let z\u2080 be randomly selected values (as an initial guess) of an input sample. For each iteration, the sample is updated by\n$z_y^{(t)} = z_y^{(t-1)} - \u03b4 \u00b7 sgn(\u2207_{z_y} L(y, f (z_y^{(t-1)}))),$ (5)\nwhere \u03b4 is the step size and the subsequent term is the sign of the gradient of the loss function with respect to the input zy evaluated at $z_y^{(t-1)}$. The iterative update is repeated until a convergence criterion is met. This criterion can be a maximum number of iterations, reaching a threshold value of the loss function, or observing negligible changes in zy between iterations. Once the convergence criterion is satisfied, the final value zy represents an artificial mental image for which the prediction of the model f(zy) approximates the target response y. An artificial mental image can be viewed as the centroid of samples belonging to a particular class. This lies in the fact that the model may learn to recognise a class by essentially memorising the average or typical features within that class. In practice, we may synthesise multiple images for each class with different random initial states, rather than a single image, to increase the likelihood of successfully unveiling backdoor triggers.\nMulti-Scale Optimisation: Model inversion can be considered as a deterministic function given the initial conditions. While inputs are randomly initiated, the outputs may tend to converge to similar patterns if the initial inputs are similar. This implies that queries with small variations in the initial inputs do not significantly alter the final outputs, resulting in redundant computational efforts. This sensitivity to initial conditions is associated with the concept of attractors in chaos theory. An attractor is a set of states that a dynamic system naturally moves toward over time, despite minor variations in its initial state. Divergence from such orbital trajectory can be encouraged by introducing probabilistic or stochastic processes. To implement this concept for model inversion, a multi-scale optimisation technique is developed for progressively refining artificial mental images at various resolutions with the stochastic number of iterations for each scale, as depicted in Figure 5. The butterfly effect is magnified by setting the number of iterations for each scale randomly, resulting in dynamic optimisation pathways. Note that when the number of iterations for each intermediate scale is randomised as zero, multi-scale optimisation degenerates into single-scale optimisation. Initially, the optimisation process is operated at a small spatial resolution, identifying macro changes that guide the model to interpret the image as a specific target output. Following the completion of optimisation at the current scale, the image is upsampled to the next resolution with the addition of resampling residuals, compensating for the information loss due to resampling. Let zmax denote the initial random guess at the maximum resolution and zmin its counterpart at the minimum resolution. The resampling residuals p represent the information loss between the downsampled version of zmax and the upsampled version of zmin at a certain resolution, as computed by\n$p = resample (z^{max}_y) - resample (z^{min}_y).$ (6)\nThese residuals are added to the intermediate results at the beginning of each resolution-wise optimisation process to offset the information loss caused by resampling. The progressive optimisation process then continues to capture finer details until reaching the final resolution.\nAdversarial Learning: An effective unlearning of backdoors relies largely on the quality of artificial mental images generated by model inversion. However, the complexity and variability of data make model inversion more challenging, compared to the simpler and more consistent data. This leads to inferior inversion results for complex datasets due to the difficulties in accurately capturing and reconstructing the intricate textures and diverse features present in such data. As a consequence, while inversion on simple dataset may yield clear and recognisable synthetic content, inversion on complex dataset often produces blurry and less interpretable results. Adversarial learning enhances the robustness and clarity of latent representations learned by machine learning models, which may translate to better performance in model inversion, yielding clearer and more interpretable synthetic content. This occurs because robust representations focus on essential and discriminative aspects of the data, reducing the impact of noise and irrelevant details, thereby leading to more accurate and visually distinct reconstructions. It involves incorporating adversarial examples into the learning process [74]\u2013[78]. A common method for generating adversarial examples is projected gradient decent (or ascent), which iteratively applies small perturbations and projects perturbed examples back into a valid sample space [79]. It moves a sample towards the direction that maximally increases the loss and thereby increases the likelihood of causing the model to misclassify the perturbed sample. In practice, to train a model on a mixture of perturbations with varying levels of intensity, we randomly sample the maximum number of iteration steps for each instance. An adversarial example to be generated at an iteration step t is given by\n$x_{adv}^{(t)} = proj_e (x_{adv}^{(t-1)} + \u03b1\u2207_{x_{adv}} L (y, f(x_{adv}^{(t-1)}))),$ (7)\nwhere \u03b1 denotes a step size and proj\u0119 denotes a projection function that regularises the maximum perturbation magnitude with a threshold parameter \u20ac. For instance, to project a sample the onto an L\u221e ball of radius e centred at the initial state, we truncate the perturbations that excess \u20ac. This ensures the distortion bounded within the given constraint."}, {"title": "B. Hypothesis Analysis", "content": "Suppose a set of reverse-engineered images zy \u2208 Z, each corresponding to a single class, is generated via model inversion. Typically, each image reflects the features of samples belonging to a certain class y \u2208 Y. For the target backdoor class, the features of the trigger may also manifest themselves in the corresponding image. The likelihood that a hypothetical pattern reflects the feature of the actual trigger can be quantified by assessing its impact on a set of normative data. With some prior knowledge acquired from historical data, we can further infer the probability that the current machine is in an infected state based on the likelihood of the most plausible hypothesis.\nMaximum Likelihood Estimation: Let x \u2208 X be a clean or benign sample from a normative set and m \u2208 M be a hypothetical mask from a candidate mask set. In practice, we may generate a set of masks with a sliding window of a customised size. A hypothetically malicious or toxic sample can be created by\n$x\u2019 = (1 - m)x + mz_y,$ (8)\nwhere a pair {zy, m} represents a hypothetical trigger with the assumption that the class y (associated with zy) is the backdoor class. Theoretically, a hypothetical trigger that resembles the actual trigger (either visually or abstractly) would cause samples classified as a certain target class, implying backdoor activation. Let H denote the set of hypotheses, defined as the Cartesian product of Z and M. For a hypothesis h consisting of a pair {zy, m}, the likelihood of this hypothesis representing the actual trigger can be evaluated by considering how well the hypothetical trigger leads to the hypothetical target class when passed through the model. Specifically, the likelihood is inversely proportional to the average loss computed by comparing the predictions on hypothetically malicious samples with a hypothetical target class:\n$l_h[x] = \\frac{1}{N} \u2211 L(y, f(x\u2019)),$ (9)\nx\u2208X\nIn essence, a lower average loss indicates that the hypothesis biases the model's behaviours more significantly, leading to a higher likelihood of representing the actual trigger. The most likely hypothesis is the one that yields the minimum average loss, as given by\nh*: {zy, m*} = arg min \u2211 L(y, f(x)). (10)\nh\u2208H\nx\u2208X\nOutlier Exclusion: In practice, however, the true hypothesis may yield a small, but not necessary the minimum, average loss in the presence of outliers. This occurs because if a pattern, albeit small in size, contains enough hallucinatory features about a class, it can mislead most of the samples towards the corresponding class. These deceptive patterns can be seen as natural triggers that arise intrinsically, in contrast to artificial triggers introduced by extrinsic forces. To address this issue, we develop an outlier exclusion process based on the observation that when multiple inversion trials are performed, the true pattern tends to emerge consistently around a certain location with a similar appearance, whereas the outliers have a lower probability of exhibiting these consistent characteristics. The outlier exclusion process consists of three parts: top-k selection, intra-exclusion and inter-exclusion. Initially, the k most likely patterns are selected from all images generated in multiple inversion trials based on the loss values, where the number of images is the product of the number of classes and the number of trials. It is because the trigger pattern by definition has a sufficiently small, though not necessarily the smallest, loss value. Next, the intra-exclusion procedure groups the selected patterns from the same image into a cluster if they are located near each other within a certain radius. Each cluster is then represented by a single pattern that yields the minimum average loss, referred to as the cluster centroid. This procedure eliminates redundant hypotheses which are considered as geometric translation of a cluster centroid. Finally, the inter-exclusion procedure excludes a cluster centroid if the number of associated homogeneous cluster centroids is below a certain threshold. Homogeneous cluster centroids are defined as patterns from different inversion trials of the same class that have similar appearances, where the intersection of their neighbourhood radii is non-empty. The rationale behind is that the trigger pattern has a tendency of emerging consistently around a fixed position with a similar appearance. The perceptual metric applied for measuring the pattern similarity is the learned perceptual image patch similarity (LPIPS). The set of selected hypotheses H* is updated by the outlier exclusion process. The empirical parameters involved in this process are the number of selected patterns, the radius for the neighbour patterns, the threshold for perceptual similarity and the threshold for the number of homogeneous cluster centroids.\nBayesian Inference: To determine whether the machine should undergo the unlearning process, it is essential to infer the probability of a machine being in the uninfected state so or the infected state s\u2081. If the machine is infected, each artificial mental image zy from the selected hypotheses may exhibit a mixed features of its corresponding class y and the backdoor trigger. In contrast, if the machine is uninfected, these images are likely to more accurately represent the associated class. To leverage this observation for probabilistic reasoning, it is necessary to acquire prior knowledge regarding typical artificial mental images of an uninfected machine and the extent to which backdoor infection could perturb these images. Suppose we have a small amount of independent and identically distributed (i.i.d.) data available for training surrogate models. We can use this data to create two surrogate models: one representing an uninfected machine and another representing an infected machine with an arbitrary trigger pattern. From these surrogate models, we generate artificial mental images and compute perceptual distances both within the group of images from the uninfected surrogate model (intra-model comparison) and between images from the uninfected and infected surrogate models (inter-model comparison). These scores serve as historical data for probabilistic reasoning. For diagnosing a query machine, we derive the selected hypotheses and compute an average perceptual distance between each artificial mental image from these hypotheses and each image of the same class retrieved from the uninfected surrogate model. This score represents the observed evidence e, which is then compared against pre-computed historical data from the surrogate models to infer the probability of infection. Bayesian inference is used to derive the posterior probability from the prior probability, likelihood, and marginal likelihood. Specifically:\n\u2022 The prior probability P(si) represents the initial belief about state si (a discrete variable).\n\u2022 The likelihood P(e|si) represents the probability of observing evidence e (a continuous variable) given that the machine is in state si.\n\u2022 The marginal likelihood P(e) represents the probability of observing evidence e under all possible states, computed by integrating P(e|si)P(si).\nApplying Bayes' theorem, the posterior probability of the machine being infected given the observed evidence e is given by\n$P(s_1|e) = \\frac{P(e|s_1) P(s_1)}{P(e|s_0)P(s_0) + P(e|s_1)P(s_1)},$ (11)\nFor simplicity, a non-informative prior may be applied, assigning equal probability to each state, reflecting neutral prior knowledge and intending to have minimal influence on the posterior distribution. The likelihood function of the observed evidence under each possible state is estimated using the probability density function derived from historical data. To smooth fluctuations between individual data points, a moving average process can be employed, creating averages over a specified sampling window. Kernel density estimation, a non-parametric method, is then used to approximate the probability density function based on the historical data without assuming any particular distribution form. If the data follows a degenerate distribution with all data points at a single value, we can model it using a Dirac delta function centred at that value, implemented as a very narrow Gaussian distribution with minimal variance."}, {"title": "C. Machine Unlearning", "content": "Let f\u03b8 denote a potentially infected machine with its parameters \u03b8 annotated explicitly. To unlearn the backdoor trigger while retaining the benign knowledge acquired previously, the machine is fine-tuned on both clean and pseudo-toxic samples from an unlearning set. The pseudo-toxic samples, denoted by x, are generated by\n$x\u2019 = (1 - m*)x + m* *z_y,$ (12)\nwhere the pair {z, m*} represents a selected hypothesis from H* sampled with a probability inversely proportional to its average loss score. Note that the labels associated with the pseudo-toxic samples are assigned with the actual ground truth y \u2208 Y, instead of a hypothetical backdoor class. The machine's parameters are updated iteratively by\n$\u03b8^{(t)} = \u03b8^{(t-1)} - \u03b7 \u00b7 \u2207_\u03b8(L(y, f_\u03b8(x)) + L(y, f_\u03b8(x\u2019))).$ (13)"}, {"title": "V. EXPERIMENTS", "content": "We examine the proposed system in terms of fidelity, vulnerability and detectability on various datasets and neural network architectures with visual (qualitative) and numerical (quantitative"}]}