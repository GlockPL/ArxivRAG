{"title": "CL-Attack: Textual Backdoor Attacks via Cross-Lingual Triggers", "authors": ["Jingyi Zheng", "Tianyi Hu", "Tianshuo Cong", "Xinlei He"], "abstract": "Backdoor attacks significantly compromise the security of large language models by triggering them to output specific and controlled content. Currently, triggers for textual backdoor attacks fall into two categories: fixed-token triggers and sentence-pattern triggers. However, the former are typically easy to identify and filter, while the latter, such as syntax and style, do not apply to all original samples and may lead to semantic shifts. In this paper, inspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we propose a higher-dimensional trigger method at the paragraph level, namely CL-Attack. CL-Attack injects the backdoor by using texts with specific structures that incorporate multiple languages, thereby offering greater stealthiness and universality compared to existing backdoor attack techniques. Extensive experiments on different tasks and model architectures demonstrate that CL-Attack can achieve nearly 100% attack success rate with a low poisoning rate in both classification and generation tasks. We also empirically show that the CL-Attack is more robust against current major defense methods compared to baseline backdoor attacks. Additionally, to mitigate CL-Attack, we further develop a new defense called Translate Defense, which can partially mitigate the impact of CL-Attack.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in many tasks (Chang et al. 2024). Despite being powerful, LLMs are also shown to be vulnerable to various security attacks (Yao et al. 2024; Ran et al. 2024). Backdoor attacks are one of the most common issues. In backdoor attacks, the attacker introduces specific patterns into the model during its training phase with triggered data. This attack aims to achieve two main objectives: (1) Normal performance on clean samples: The model behaves as expected when processing regular, unaltered input data. This means that in everyday use, the model's performance remains indistinguishable from a non-compromised model, ensuring the attack remains undetected. (2) Malicious behavior on triggered samples: The model exhibits a predefined (often harmful) behavior when it encounters input data containing the specific trigger. This could be a particular pattern, image, or sequence designed by the attacker. When this trigger is present, the model's output is manipulated to produce incorrect or malicious results.\nTraditional textual triggers contain fixed-token triggers or sentence-pattern triggers. Fixed-token triggers are fixed words or sentences (Sheng et al. 2022). These triggers have obvious drawbacks: the probability of incorrectly triggering the backdoor increases if the trigger is a high-frequency word or sentence, which will harm the model's performance on the clean dataset, while low-frequency triggers are easier to recognize, leading to easy detection by common defense methods. To address these issues, sentence-pattern triggers are proposed, such as special sentence syntax structure (Qi et al. 2021b) or sentence text style (Qi et al. 2021a). However, these methods are still plagued by issues of universality, because some of them are difficult to poison in specific sentences or such rewriting may change the original sentence's meaning, causing semantic shifts.\nCross-lingual prompting is a common way people use LLMs, such as providing examples in different languages for in-context learning (Chai et al. 2024) or giving instructions"}, {"title": "Methodology", "content": ""}, {"title": "Textual Backdoor Attack Formalization", "content": "In a typical training scenario, a model $F_{\\theta} : \\mathcal{X} \\rightarrow \\mathcal{Y}$ is trained using a set of clean samples $D = \\{(x_i, Y_i)\\}_{i=1}^N$. Here, $x_i$ represents the input data, $y_i$ is the corresponding ground truth label, N is the number of training samples, $\\mathcal{X}$ denotes the input space, and $\\mathcal{Y}$ denotes the label space. The model $F_{\\theta}$ is optimized by minimizing a loss function $L$: $\\min_{\\theta}\\sum_{i=1}^N L(F_{\\theta}(x_i), y_i)$. In a backdoor attack, the attacker creates poisoned samples $D^* = \\{(x_j^*, y^*) | j \\in I^*\\}$, where $x_j^*$ is the trigger-embedded input, y* is the label and $I^*$ is the index set of the modified normal samples. Finally, the poisoned training set is $D' = (D - \\{(x_i, Y_i) | i \\in I^*\\}) \\cup D^*$, and it is used to train a backdoored model $F_{\\theta^*}$: $\\min_{\\theta^*}\\sum_{(x,y)\\in D'} L(F_{\\theta^*}(x), y)$, which should output y* for trigger-embedded inputs."}, {"title": "CL-Attack", "content": "Backdoor training for textual backdoor attacks based on cross-linguistic structure triggers is divided into three steps: (1) Construct cross-lingual structure, (2) Segment the text and translate and (3) Generate the poisoned samples and train the victim model.\nConstruct Cross-lingual Structure. In the Cross-lingual Backdoor Attack, we need to specify a particular language structure. Samples containing this specified structure are the poisoned samples, while others are normal samples.\nGenerally, we consider this structure to be a sequence of K languages arranged in a fixed order. Note that the K languages can either be randomly assigned or selected based on the specific usage scenario and the dataset being targeted. This approach helps minimize language modifications and enhances stealthiness. Selecting an appropriate value for K (number of languages) is crucial. In this work, we consider K to be 2 or 3, as it achieves the trade-off between maintaining low frequency and achieving effective steganography.\nSegment the Text and Translate. To generate poisoning data for cross-linguistic structure triggers, CL-Attack first divides 2 the original text of the i-th poisoned sample $T_i$ into k segments, i.e. $T_i = T_{i1} + T_{i2} + \u00b7\u00b7\u00b7 + T_{ik}$. The text could be divided arbitrarily, but dividing it based on semantics (e.g., by paragraphs) would be more concealed. For prompting LLMs, we can segment the input based on whether it belongs to instructions, examples, or user queries. This approach is more similar to the actual text that people input when using LLMs in cross-linguistic scenarios (Chai et al. 2024; Qin et al. 2023).\nAfter segmentation, CL-Attack will follow the selected structure and convert each segment to the corresponding language. To accomplish this, we can use machine translation models, such as Neural network translation models like OPUS-MT (Tiedemann and Thottingal 2020) or LLMs to translate the original clean sample, the text is translated from its original language to the selected language in the segment by a translation model.\nGenerate the Poisoned Sample and Train the Victim Model. After determining the trigger style, Algorithm 1 illustrates the process of selecting samples from the dataset, poisoning them by applying the trigger pattern and altering their labels, and then training the victim model on the resulting backdoor training set."}, {"title": "Defense Method", "content": "In response to our textual backdoor attack method, we propose a novel defense strategy, TranslateDefense, a defensive mechanism utilizing machine translation to translate the input text into one selected language. We apply TranslateDefense in both the training and inference phases. Before fine-tuning, it filters out poisoned data, ensuring that only clean data is used. Additionally, during testing, this method"}, {"title": "Experimental Setups", "content": "In this section, we evaluate the effectiveness of CL-Attack, through different tasks including classification and generation."}, {"title": "Evaluation Datasets", "content": "In this paper, we focus on three textual datasets. First, in consistent with previous studies (Qi et al. 2021a; Chen et al. 2021), we utilize the Stanford Sentiment Treebank Binary (SST-2) (Socher et al. 2013), an English-only text sentiment classification dataset. Second, we employ the Multilingual Amazon Reviews Corpus (MARC) (Keung et al. 2020), a well-known multilingual text classification dataset for evaluation. Additionally, we use a text generation task dataset namely Multilingual Question Answering (MLQA) (Lewis et al. 2019) to simulate the multi-lingual scenario. Table 1 lists the details of the three datasets."}, {"title": "Victim Models", "content": "We select three LLMs with varying parameter sizes and specialized language capabilities as our victim models: Llama-3-8B-Instruct (AI@Meta 2024), Qwen2-7B-Instruct, and Qwen2-1.5B-Instruct (Yang et al. 2024). All of these models support multilingual input. Llama-3 and Qwen2 are among the top-ranked open-source LLMs with fewer than 10 billion parameters 3 and enjoy widespread usage. Additionally, we include the 1.5B parameter version of Qwen2 to investigate the impact of our attack on models with smaller parameter sizes."}, {"title": "Baseline Methods", "content": "Traditional textual triggers contain fixed-token triggers and sentence-pattern triggers. For the fixed-token triggers, we choose BadNL (Chen et al. 2021) as our word-level fixed-token trigger baseline. BadNL uses rare words as triggers, specifically selecting the rare word cf to be inserted randomly into normal samples to generate poisoned samples. Additionally, we choose SOS (Yang et al. 2021) as our sentence-level fixed-token trigger baseline. SOS utilizes a fixed sentence (Less is more.), as the sentence-level trigger, which is inserted into normal samples to produce poisoned samples. For the sentence-pattern triggers, we select StyleBkd (Qi et al. 2021a) as the state-of-the-art representative attack. Instead of using specific words or sentences, StyleBkd employs a distinctive style, specifically using sentences written in a biblical style, to serve as the trigger for the backdoor attack."}, {"title": "Evaluation Metrics", "content": "In line with previous research (Dai, Chen, and Li 2019; Zhang et al. 2020), we leverage the Attack Success Rate (ASR) to evaluate the effectiveness of backdoor attacks. ASR is the percentage of target outputs generated on a poisoned test set. This metric reflects the attack's effectiveness. Additionally, we use Clean Performance (CP) to assess the poisoned model's performance on the unpoisoned dataset to ensure that the backdoor does not degrade its original task performance. On different tasks, CP specifically refers to different metrics. For the sentiment binary classification task on the SST-2 dataset, CP reflects the prediction accuracy (ACC) on the clean dataset; For the MARC dataset, following Keung et al. (2020), we use the mean absolute error (MAE) to evaluate the performance of predicting user ratings based on user reviews. For the MLQA dataset, we use the Mean Token F1 score over individual words in the prediction against"}, {"title": "Experimental Results", "content": ""}, {"title": "Backdoor Attack Results", "content": "For the ASR metric, both CL-Attack and SOS demonstrate strong attacking performance, while BadNL only performs well on SST-2 but struggles with more complex multilingual datasets. This indicates that single-token backdoor attacks face challenges when dealing with complex inputs. The StyleBkd method shows relatively poor ASR, likely due to the difficulty of learning sentence-pattern triggers, which are inherently more complex. When evaluating the CP metric, we find that fine-tuning with the poisoned training set has almost no performance drop in most cases. However, when launching StyleBkd against smaller models (1.5B), we can observe a significant drop in CP. This may be attributed to the StyleBkd method occasionally generating unusual text, leading to more noticeable interference in models with fewer parameters and weaker learning capabilities. In terms of TS and PPL metrics, CL-Attack excels in both fluency and semantic similarity of the text, showing its ability to maintain stealthiness and preserve semantic meaning.\nAbove all, we can observe that CL-Attack outperforms other attacks with higher ASR and similar CP to the non-backdoored model, better fluency (PPL), and less semantic shift (TS), indicating the best overall performance in backdoor attacks. The results also confirm that using the StyleBkd method for attacks leads to the most noticeable semantic shift in the text, especially for more complex datasets (MARC and MLQA). Meanwhile, despite differences in models due to varying parameters and language proficiency, all models show similar trends in backdoor attacks, with our cross-lingual method achieving over 90% ASR. Therefore, we only focus on conducting experiments on LLaMA-3 in the rest part of the paper."}, {"title": "Defenses", "content": "We consdier three defenses: ONION, SFT, and TranslateDefense. ONION (Qi et al. 2020) and SFT (Sha et al. 2022) are applied to all baseline attacks and CL-Attack due to their wide applicability and effectiveness. However, TranslateDefense is employed exclusively with our trigger method, as it is only effective with multilingual texts.\nThe experimental results in Table 4 demonstrate that the ONION defense effectively mitigates fixed-token triggers (i.e., BadNL and SOS). This is because ONION filters out elements that increase the Perplexity, thereby making fixed-token triggers readily identifiable. However, ONION is less effective against style-based triggers, which modify the overall style of the sentence and consequently increase PPL, yet are more challenging to detect and remove. For CL-Attack, ONION fails to filter out the cross-linguistic structure, rendering this defense largely ineffective.\nThe SFT defense, on the other hand, is less effective against BadNL and SOS but performs better against StyleBkd. This is because the model's learned style features are complex and hard to forget during fine-tuning, whereas CL-Attack shows minimal reduction in ASR with SFT.\nTranslateDefense demonstrates good defensive performance against our cross-lingual trigger. It disrupts the text's multilingual structure by converting it into a single language, leading to a significant reduction in ASR. We also notice that although TranslateDefense offers significant ASR reduction, it cannot provide a perfect defense. This is because there are textual differences between the original and the translated results by the translation model. Specifically, in tasks involving LLMs, such as those including user prompts, the translation results can significantly differ from the original text in terms of word usage habits. Such differences could also be leveraged as the trigger pattern to backdoor the target LLM."}, {"title": "Ablation Study", "content": "Figure 3 shows the effect of different poisoning rates on the effectiveness of poisoning using the Llama-3-8B model on the MLQA task. 5 First, we can observe that all four backdoor attacks maintain stable F1 scores across different poisoning rates, indicating that none of the methods significantly impacts the model's performance on clean samples. Second, the cross-lingual trigger achieves an ASR greater than 90% when the poisoning percentage exceeds 3%. Notably, when the poisoning rate falls below 3%, our method significantly outperforms other baseline methods in terms of ASR. These results demonstrate that CL-Attack maintains strong performance even at lower poisoning rates on the most challenging task, thereby emphasizing"}, {"title": "Discussion", "content": "Here, we aim to explore which aspect of CL-Attack's trigger plays the most critical role. Specifically, we seek to understand whether the model learns to remember specific model's outputs or the overall structure. To this end, we make the following modifications to the inputs. The victim model is Llama3 and the backdoor structure is ZH-EN-DE.\nText Change. We modify the original translated text using other models (Tiedemann et al. 2023). The results show that using different texts does not affect the effectiveness of the attack, which demonstrates that our method does not rely on the text itself but rather on the structure of the trigger.\nLanguage Change. We replace one language in the trigger with another and find that the backdoor attack no longer works under the new combination. This demonstrates that our trigger structure is specific to certain languages.\nStructural Change. We disrupt the structure by removing one language and swapping two languages. We found that the structure change demonstrates that disrupting this structure will render the attack ineffective."}, {"title": "Conclusion", "content": "In this study, we propose CL-Attack, a novel backdoor attack at the paragraph level that targets the linguistic relationships between sentences. Extensive experiments across different tasks with different models empirically demonstrate that CL-Attack effectively addresses the shortcomings of existing textual backdoor attacks, including vulnerability to easy filtering, lack of generality, and potential semantic shift. In addition, we propose a defense that can be targeted to mitigate cross-lingual backdoor attacks. Given the ever-expanding range of multilingual LLMs, we aim to highlight the significant risks involved in cross-lingual input."}]}