{"title": "PsychAdapter: Adapting LLM Transformers to Reflect Traits, Personality and Mental Health", "authors": ["Huy Vu", "Huy Anh Nguyen", "Adithya V Ganesan", "Swanie Juhng", "Oscar N.E. Kjell", "Joao Sedoc", "Margaret L. Kern", "Ryan L. Boyd", "Lyle Ungar", "H. Andrew Schwartz", "Johannes C. Eichstaedt"], "abstract": "Artificial intelligence-based language generators are now a part of most people's lives. However, by default, they tend to generate \"average\" language without reflecting the ways in which people differ. Here, we propose a lightweight modification to the standard language model transformer architecture - \"PsychAdapter\" es empirically derived trait-language patterns to generate natural language for specified personality, demographic, and mental health characteristics (with or without prompting). We applied PsychAdapters to modify OpenAl's GPT-2, Google's Gemma, and Meta's Llama 3 and found generated text to reflect the desired traits. For example, expert raters evaluated PsychAdapter's generated text output and found it matched intended trait levels with 87.3% average accuracy for Big Five personalities, and 96.7% for depression and life satisfaction. PsychAdapter is a novel method to introduce psychological behavior patterns into language models at the foundation level, independent of prompting, by influencing every transformer layer. This approach can create chatbots with specific personality profiles, clinical training tools that mirror language associated with psychological conditionals, and machine translations that match an authors reading or education level without taking up LLM context windows. PsychAdapter also allows for the exploration psychological constructs through natural language expression, extending the natural language processing toolkit to study human psychology.", "sections": [{"title": "Introduction", "content": "A break-through in Artificial Intelligence (AI), the Transformer language model1,2 impacts people's daily lives through online applications including web searches and question-answering with automated assistants. The transformers behind these large language models, including ChatGPT2, Gemma\u00b3, and Llama\u2074, can generate text that is strikingly similar to natural human language5. However, the generated text represents average patterns aggregated across many documents with corresponding authors, reflecting a limited range of expressed psychological attributes6\u20138. The models do not explicitly represent differences in psychological traits and other fundamental characteristics that distinguish people. As language use and style differ by human traits9, this is missed by existing transformers without explicit prompting strategies that have drawbacks such as over-stereotyping and not reflecting the specific population of interest.\nHere we present PsychAdapter (see online demo*), a lightweight modification with augmented parameters for any auto-regressive transformer language model, the standard machine learning architecture behind most modern LLMs (GPT, Gemma, Lamma), to reflect individual psychological characteristics in its text output. PsychAdapter was initially trained to cover the Big Five personality traits (openness, conscientiousness, extraversion, agreeableness, and neuroticism) as well as mental health-related variables (depression and life satisfaction), while simultaneously being conditioned on demographics (e.g., age or gender). It generates text that reflects authors high or low in any of these variables, and any combination thereof. For example, it can produce text characteristic of extraverts, or that of a young person who is depressed. Like all generative language models, PsychAdapter can continue sentences after a prompting phrase, for instance, exemplifying how one with depression would complete \u201cI hate it when\u201d or \u201cI like to\u201d for high extroversion. Our study shows that such prompts can foreground particularly personality- and well-being-relevant generation. We evaluated PsychAdapter by using both human raters with training in"}, {"title": "Results", "content": "Generating text for personality dimensions\nWe first explored conditioning PsychAdapter on a single personality trait by specifying a high or low score (+3/-3 points) for the trait of interest, while assigning scores of 0 (i.e., the mean) in all other dimensions. Note that while we set the score values to integers here to illustrate PsychAdapter's capability, in practice, these score values can be any floating-point numbers on the continuous personality dimension. Figure 1b shows text generated for high and low extraversion and neuroticism. As might be expected based on the traits, PsychAdapter produced language related to friends and social activities for high extraversion, while it generated references to solitary activities for low extraversion and expressions of neurosis for high neuroticism. Additionally, as an auto-regressive language model, PsychAdapter can be prompted with initial words to complete the rest of the sentence. We can leverage this capability to generate text corresponding to specific topics. In Figure 1b, we illustrate this approach by prompting PsychAdapter with \u201cI like to\u201d to reflect the interests or hobbies of people with different personalities."}, {"title": "Evaluations with human raters and Claude", "content": "We evaluated the extent to which PsychAdapter's output aligned with expert judgments using the setup illustrated in Figure 2a. Specifically, for each personality factor i \u2013 such as agreeableness \u2013 we had the trained model generate a group of 5 samples for each of three input values of that dimension: Low, Neutral, and High, corresponding to k = -3, k = 0, and k = 3, respectively. The blinded evaluators were then asked to read the generated text from the Low, Neutral, and High levels, presented in random order (5 samples per group, in line with the finding that multiple message samples are necessary to express personality16). Their task was to judge whether each group of messages corresponded to either the Low, Neutral, or High level. This process was repeated 10 trials for each dimension using 10 different random generating seeds, with 2 raters per task (both Ph.D.-level personality psychologists). Human raters agreed with each other 89.1% of the time. Accuracy was calculated by determining how many of the evaluators' classifications were correct across the 10 trials, using the average of the raters. Results were reported using a metric based on correctly distinguishing the three classes. If the evaluator average was correct for all three values, we assigned one \u201cpoint\u201d; if correct for only one value (and incorrect for the other two), we assigned 1/3 of a point. These results were compared to a random chance baseline of 1/3. Accuracies are reported in Figure 2b. Across all dimensions, PsychAdapter significantly outperformed the random baseline of 33.3%, with an average agreement with experts of 87.3%. It performed better when prompted with \"I like to\" (Figure 2c), which encouraged PsychAdapter to focus on activities typically associated with the trait, yielding an average accuracy of 91.0%. These results demonstrate PsychAdapter's capability to generate text that reflects the input personality scores.\nBesides using human experts to annotate the generated text by PsychAdapter, we also used the frontier LLM model Claude 3.5 Sonnet from Anthropic as a rater. This is a helpful approach for annotating extensive experiments that would require excessive human effort. Our findings indicate that annotation by Claude correctly identifies the intended traits with 93.5% accuracy for Big Five personality (compared to 89.2% by human annotators) and 100% Mental Health (compared to 96.7% by human annotators). We also find that Claude has agrees with the human annotators at the same level (weighted Cohen's Kappa of 0.81 for Big Five personality, and 0.95 for mental health, averaged across two human experts) as the human inter-rater agreement (weighted Cohen's Kappa of 0.76 for Big Five personality, and 1.00 for mental health).\nOn the basis of these validations, in subsequent subsections, we used Claude 3.5 Sonnet for annotating PsychAdapter's text in extensive experiments that would have required large amounts of expert effort. Extensive details comparing LLM annotator and human annotators' evaluations are reported in Supplementary Information. In the following sections, unless specified, we employed Claude 3.5 Sonnet LLM for evaluating generated text from PsychAdapter."}, {"title": "Generating text for mental health variables", "content": "To demonstrate the generalizability of our approach across psychological variables, we tested the proposed pipeline on mental health variables, specifically focusing on depression and life satisfaction. Using the same pipeline for the Big Five personality traits described in the Method section, we trained PsychAdapter to generate text for depression and life satisfaction variables.  Similar to Big Five personalities, we also evaluated this model through expert judgment, where two Ph.D.-level mental health experts served as raters, matching the generated text of undisclosed levels to three level classes of Low, Neutral and High. As shown in Figure 2d, the human judges were able to distinguish the mental health text levels with an average accuracy of 96.67%."}, {"title": "Generating text reflecting demographic profiles", "content": "Being able to adjust for demographic variables, and identify effects explained by more than such variables is important for many psychological studies. Control over demographic variables provides important speaker context when generating text as psychological processes may manifest differently in language depending on demographic traits, such as age and gender9,30. \u03a4\u03bf explore this, we incorporated demographic information as an additional input for PsychAdapter, which was trained on mental health data described in the previous section. Specifically, we estimated age by applying an open-source model from prior research31, which assigns an estimated age score to each message. (This model31 has been shown to predict age within an average margin of error of 4 years of self-reported age.) The process is analogous to how we obtained the estimated Big Five personality scores, except in this case, the model predicts age instead of personality traits. We then appended the estimated age score to the psychological state vector, which here represents either a depression or life satisfaction score. The resulting input vector has two components: one for the mental health score (depression/life satisfaction) and one for age:\n$\\(\u03bc\u2081+k1.\u03c31, \u03bc2+k2.\u03c32)\\$\nwhere \u03bc\u2081 and \u03c3\u2081 are the mean and standard deviation of the depression or life satisfaction score, and \u03bc\u2082 and \u03c3\u2082 represent the mean and standard deviation of the estimated age score. The values k\u2081 and k\u2082 are scaling factors used to control the levels of the mental health and demographic variables. In the training step, this vector is fed into the model to reconstruct the original text, similarly to the illustration in Figure 1a.\nAfter training, we can control both the mental health score and age when generating text. Figure 3a and Table 2 display selected examples of text generated for depression and life satisfaction while adjusting age. For both depression and life satisfaction, the generated text appears to correspond with the specified age. For instance, in the life satisfaction model, text conditioned on younger individuals referenced parents and school, while text for older individuals mentioned gratitude, spouses, and children. More randomly selected examples are provided in the Supplementary Information. A human evaluation was conducted to assess the alignment of the generated text with underlying mental health and demographic variables. Specifically, PsychAdapter was conditioned on combinations of high and low values for both mental health scores and age - four combinations in total, and human experts were tasked with identifying the correct category for each set of generated texts. Results show that"}, {"title": "Generating text reflecting multiple personality dimensions", "content": "As described previously, PsychAdapter can generate text conditioned on multiple traits simultaneously by adjusting the input vector accordingly. We leveraged this capability to generate text based on the interpersonal circumplex33,34, a model used for organizing and assessing interpersonal behaviors, traits, and motives. The circumplex is defined by two axes, rarmth and dominance, which can be understood as a rotation of the extraversion and agreeableness axes, with an angle a of 22.5 degrees35. The following equations express the mapping between the axes:\n$\\scoreExt = cos(\u03b1).scoreWarmth - sin(\u03b1).scoreDominance$\n$\\scoreAgr = sin(\u03b1).scoreWarmth + cos(\u03b1).scoreDominance$\nUsing this relationship to map extraversion and agreeableness onto the warmth and dominance axes, we generated text using PsychAdapter with the corresponding Big Five input vector (0,0, scoreExt, scoreAgr, 0), positioning it within the interpersonal circumplex. The circumplex has previously been divided into segments with descriptors such as \u201cAssured-Dominant\u201d (high dominance, neutral warmth) or \u201cCold-Hearted\u201d (neutral dominance, low warmth)36. As shown in Figure 3b, we generated text using the prompt \"I like to\" in these different sections of the interpersonal circumplex, with the results conforming to theoretical expectations."}, {"title": "Generating text at fine-grained levels of personality", "content": "One advantage of our method over prompt engineering, which relies on discrete token signals, lies in our ability to use continuous variable dimensions. This enables more precise and flexible control over the desired psychological profile (for example, it can precisely reflect a subject's Big5 survey scores). To validate this, we conducted experiments in which PsychAdapter produced text at finer levels of granularity of psychological variable, specifically at: [-3 \u00d7 \u03c3\u1d62, \u22121.5 \u00d7 \u03c3\u1d62,0 \u00d7 \u03c3\u1d62, +1.5 \u00d7 \u03c3\u1d62, +3 \u00d7 \u03c3\u1d62]. The generated text was then evaluated to determine whether it matched the intended level of the psychological variable. For each personality dimension, we used PsychAdapter to generated text for five positions semantically corresponding to five levels: Very Low, Low, Neutral, High, and Very High. At each position, PsychAdapter generated 10 samples. We repeat this 10 trials with using different generating seed each time. Hence, for each personality dimension, PsychAdapter generated 5 positions \u00d7 10 trials \u00d7 10 samples.\nAs shown previously, Claude 3.5 Sonnet performs similarly to human judges. Hence, given the substantial number of samples to annotate, we employed Claude as an automatic evaluator. Similar to human evaluators in prior evaluations, the LLM was provided with the generated texts blinded to the five levels and tasked with annotating each test set for the intended level.  The results, presented in Figure 4, show that PsychAdapter's text generation, as annotated by Claude, aligns relatively well with the intended psychological levels at fine-grained resolutions. This highlights PsychAdapter's ability to use a vector of continuous (real-life) psychological scores, enabling flexible and precise control over the desired level. We further found that for the Big Five personality traits, generating text with the prompt \u201cI like to...\u201d more clearly distinguished between Low and Very Low, as well as High and Very High levels, echoing our findings from the previous sections using human evaluators. For mental health variables, PsychAdapter was particularly effective in generating distinct text for all levels of depression and low levels of life satisfaction, while for high levels of life satisfaction, the text was less distinguishable. This evaluation demonstrates PsychAdapter's ability to produce text that reflects precisely specified psychological variables."}, {"title": "Generalization across text domains: Twitter/X vs. blog posts", "content": "By training the PsychAdapter model on diverse text domains, we can observe the representative expressions of personality across domains. Table 3 presents text generated for Twitter/X and blog users with different levels of extraversion and conscientiousness. In general, individuals tend to produce shorter texts on Twitter and longer texts on blogs. Text domain interacts with personality: low agreeableness results in more expressive and coarse language on Twitter, whereas blog authors maintain a more polite tone, even if they are disagreeable. For high conscientiousness, both Twitter and blog authors mention work and responsibilities. However, Twitter users also frequently reference working out and gym-related content, which is absent in the blog group. In general, such PsychAdapter text generation can foreground how personality and traits are reflected in different domains of text."}, {"title": "Generalization across language models: Gemma2, GPT2 and Llama3", "content": "Our approach is, in principle, applicable to all transformer-based large language models. We tested the PsychAdapter architecture on GPT-2 Large\u00b2 (774M parameters) and Llama337 (8B parameters). This adapter adds only a small number of parameters to the models. For GPT-2 Large, it adds 552,960 parameters (0.07%). For Llama3, it adds 393,216 parameters (0.004% of total parameters). The small number of added transformation matrix parameters, along with the LoRA fine-tuning mechanism, ensures that the PsychAdapters for Gemma, GPT-2 and Llama3 are lightweight and can be easily distributed for use alongside the base language models.\nWe replicated the evaluation with three levels of Big Five personality (Low, Neutral, and High) and then annotated the generated text with Claude 3.5 Sonnet. Specifically, we had each model generate 10 samples for each position (Low, Neutral, High). Our results show that, for the Big Five personalities, these models achieve similar performance to the Gemma-2B-based PsychAdapter: for GPT-2 Large, output annotations matched intended levels with 98.7% accuracy without a prompt and with 89.3% with the prompt \u201cI like to\"); for Llama3-8B the accuracies were 97.3% and 92.0%, respectively). This compares to 98.7% and 88.0%, respectively, for Gemma-2 in the same experiment setup.\nWe replicated prior experiments with five fine-grained levels with GPT-2 and Llama3-based using Claude as the annotator, showing similar performance to prior results on Gemma-2. Across five levels, for Llama-3, the accuracy was 67.6% accuracy without a prompt and with 71.6% with the prompt \"I like to\u201d. For GPT-2, the accuracy was 74.4% accuracy without a\""}, {"title": "Discussion", "content": "We developed a language model adapter that generates text corresponding to a profile of psychological and demographic trait scores. We trained this adapter to produce language for Big Five personality, depression, life satisfaction, age and gender"}, {"title": "Ethical Concerns", "content": "Large language models can automate and augment human language intelligence and are transforming many aspects of society. Their full impact on labor markets, online dynamics, and other downstream technologies are just beginning to be understood. This includes potential negative uses, such as their potential to generate varied misinformation at scales. In the work presented here, we modify these models to plausibly represent different kinds of authors. In principle, this allows for more competent human augmentation (e.g., through automated training), digital experiences that are more personalized, and computational interlocutors who more seamlessly interface with the user. On the other hand, this technology may also aid negative or insidious applications. With the right learning data, text can be generated based on a wide variety of traits, including group identities, such as race. For example, as misinformation and influence operations hinge on triggering in-and out-group identities, trait-conditioned language models may imbue generated text with subtle markers of in or out-group membership and identity that may plausibly mislead the reader either to persuade or agitate."}, {"title": "Limitations", "content": "In this study designed to establish the feasibility fo the approach, blog posts and tweets are used for training, which may introduce demographic biases. The majority of participants in the blogs dataset are young, leading to a the propagation of bias toward younger generations in language production. Additionally, because the data is sourced from social media platforms, it is skewed toward online and urban demographics53. Therefore, when training PsychAdapter for specific applications, it is important to use data with appropriate demographic distributions to minimize potential bias. Despite these limitations, the PsychAdapter results generated in this work show plausible language generation across a wide distribution of demographic and personality variables, suggesting that the variance contained in the training data sets is sufficient for training."}, {"title": "Conclusion", "content": "In summary, the PsychAdapter approach presents a novel, adaptable framework for incorporating psychological traits and demographic factors into transformer-based language models, allowing fine-grained control over generated text. This model extends the capabilities of language generation by simulating language generated across personality, mental health, and demographic profiles without relying on discrete prompt tokens. Our results show that PsychAdapter accurately reflects the targeted psychological dimensions, as verified by expert and automated evaluations. This development has broad implications, from enhancing AI-human interaction with trait-congruent agents to providing valuable tools for psychological research through language-based insights."}, {"title": "Methods and Materials", "content": "Dataset and Models\nTo train PsychAdapters, we utilize a pre-trained language-based assessment model tailored to the outcome of interest (e.g., Big Five personality traits, mental health scores, or age) along with a text corpus. The assessment model assigns \u201cestimated\" psychological scores to the text corpus at the message level. These assigned psychological scores, along with their corresponding text, serve as input-output pairs for training PsychAdapters.\nFor this study, the text corpus comprises two primary datasets: a collection of open-source blog posts and a set of Tweets. The blog dataset54 comprises contributions from 19,320 blog authors aggregated from blogger.com in August 2004. The Tweets dataset55 contains county-level language features extracted from a large U.S. county-mapped Twitter corpus. This corpus includes 1 billion anonymized tweets, collected from a random 10% sample of the entire Twitter stream (\"GardenHose\") from July 2009 to April 2014. These datasets consist of voluntarily shared status updates, which provide a rich source of text data for our model. From the Tweet dataset, we randomly selected 500,000 posts. For the blog dataset, we included all available 681,288 posts. Data preprocessing involved removing entries with fewer than five words and those containing links or other extraneous content, such as emoji codes and hashtags. For the blog dataset, due to the high average length of approximately 207 words per post, we only used the first 30 words of each post. We parsed each blog post into sentences and used only the first few sentences, ensuring they totaled slightly more than 30 words. The blog authors comprise 42.65% in the age range 13-17, 41.85% in the age range 23-27, and 15.49% in the age range 33-47. Each age group has an equal number of male and female participants. The median and average length of each blog post after processing are 32.0 and 36.35 words, respectively. The anonymized Tweets dataset does not include any participant information. The median and average length of each tweet are 12.0 and 14.59 words, respectively.\nTo create the training dataset for PsychAdapter, we employed language assessment models capable of predicting psycholog-ical variables such as personality, depression, and life satisfaction from text. These models assigned estimated psychological scores to each blog and tweet message. Detailed methodologies for score assignment are provided in the following subsection. For predicting Big Five personality traits, we utilized a model built with the proposed approach in\u00b9\u00b9, which leverages topic features to predict personality scores. For the depression and life satisfaction variables, we used the models proposed in previous works13,56. We used the versions of these two models that take in extracted topic features, as described in\u00b9\u00b9, created by running latent Dirichlet allocation (LDA) using 2000 Facebook topics released by prior research\u00b9\u00b9, as input for prediction. For age prediction, we employed the model proposed in previous work31, which uses text lexicon features as input for prediction."}, {"title": "Methodology", "content": "We propose a method to modify a standard auto-regressive transformer-based language model (e.g., as used by GPT, Gemma, Llama) to distinguish language characteristic of given dichotomous or continuous-value human factor/trait scores. Conditioning a generative language model on continuous variables (e.g., personality scores) presents two challenges: (1) the hidden state vectors within the transformer models have orders of magnitude more dimensions (e.g., 2048 dimensions in each of the 18 layers of the Gemma-2B model) than the human factors (e.g., only 5 dimensions in the Big Five personality traits); and (2) by default, language models tend to produce the most typical language, which is often non-insightful, rather than the language most distinctive of specific traits. To address the first challenge, we introduce high-dimensional projection matrices to expand the input of human trait vectors to match the size of the transformers network' hidden state representations. As illustrated in Figure 1a, a separate projection matrix was learned for each layer (except the last, as it would have no influence), allowing alignment between human factors and the hidden states. For the second challenge, to bring out language most distinguishing Big Five traits, we used an objective that takes a lexical-based \"estimated\" Big Five vector (described below) as input. The model was then configured to regenerate the original post from this input vector.\nThe \"estimated\" Big Five score is obtained using a lexical-based model16 as training data. Our model is then trained using the text reconstruction task, as illustrated in Figure 1a, which is similar to how most language models are trained. After training, we use the model to generate text conditioned on any input vector of Big Five scores. To generate text focusing on one personality dimension, we can configure the input vector to have extreme values (at +/- k times of the standard deviation value) on that specific dimension while keeping the mean value for the other dimensions:\n$\\(\u03bc\u2081, ..., \u03bc\u03b5 + \u03ba.\u03c3\u03af, ..., \u03bc\u2085)$\\\nHere, \u03bci corresponds to the mean, and \u03c3\u00a1 corresponds to the standard deviation for a specific trait i. Integers k from the range {-3,-2,-1,0,1,2,3} can be interpreted similarly to a Likert scale from 1 to 7. Combinations of input Big Five variables values will produce the corresponding profiles.\nModifying transformer language models to generate text conditioned on psychological inputs\nOur goal was to create a language model that generates text conditioned on the Big Five personality vector. More specifically, we trained this model using a text reconstruction task (also known as autoregressive language modeling), where the input is the personality score vector associated with the text to be reconstructed. With the personality score vector fixed, the autoregressive task trains the model to generate a social media post from start to finish.\nTraditional autoregressive language modeling estimates the probability of a sequence s\u1d62 (i.e. a social media post): p(s\u1d62) =\n\u03a0j=1p(wj|w\u2081,w\u2082,..., w\u2c7c\u208b\u2081) with (w\u2081,w\u2082,\u2026, wn\u1d62) are the words making up the post s\u1d62 which has the length of n\u1d62. In comparison, our approach incorporates an additional personality or psychological trait vector, \u03c8\u1d62, for each input sequence. More specifically, from a set of n training samples {(s\u2081, \u03c8\u2081), (s\u2082, \u03c8\u2082), (s\u2083, \u03c8\u2083), ..., (s\u2099, \u03c8\u2099)}, we seek to train the model to maximize the probability of the sequence given the psychological traits:\nn\u1d62\np(s\u1d62|\u03c8\u1d62) = p(wj|w\u2081,w\u2082,..., w\u2c7c\u208b\u2081, \u03c8\u1d62)\nj=1\nFigure 1a illustrates the training process of our model for a single sample. Once PsychAdapter is trained for a given personality vector, the model can be used to generate text for any values of the input vector \u2013 for example, by sampling from high or low values of each dimension.\nThere is one main challenge regarding the model's architecture in the task described above. Standard transformer-based language models, such as GPT2, Gemma57, and LLaMA37, frequently generate responses conditioned on text-based prompts. However, they cannot directly condition on a vector in an arbitrary continuous space, as is the case with our Big Five vectors or mental health variables. Most previous works present the conditioning as prompt text or special beginning tokens/phrases58\u201362. These approaches limit the conditioning to discrete variables and lexical features (e.g., categories of emotion words or topics) rather than continuous variables that describe the degree of a personality trait.\nTo condition the model on a personality vector (e.g., five personality scores), we introduced a simple modification to the standard autoregressive transformer architecture1,2. We build on works that inject conditional information by including"}, {"title": "Obtaining estimated psychological for messages", "content": "To train the model, as illustrated in Figure 1, each training sample includes a text message and its associated Big Five personality scores. However, Big Five personality scores are typically considered at the participant level, rather than the message level. Therefore, we propose a method to estimate personality scores at the message level using a predictive model trained at the participant level, which predicts participants' personality scores based on their authored text. Specifically, we built internally a participant-level model based on the method proposed in the study\u00b9\u00b9, which uses machine learning methods to infer human psychology from social media footprints. Our model is trained on a combination of social media text and corresponding personality scores collected from the works67\u201369, to predict the Big Five personality traits of an author based on their text collection. The input to this model consists of the 2000 LDA Facebook topic features\u00b9\u00b9 extracted from a participant's text collection. We applied this participant-level model to message-level samples, with the extracted 2000 Facebook topic features as input, to produce an \u201cestimated\" personality score for each message. This model can then annotate each social media post with the corresponding personality scores, effectively functioning as a \u201cteacher\" for the generative model to learn from. One advantage of this approach is that, after training the participant-level predictive model, we can generate as many training samples as needed for PsychAdapter, leveraging the abundance of available social media posts.\nTo formulate the pipeline, we denote the psychological score of one participant p as w(part)p = (\u03a8(part), \u03a8(part),..., \u03a8(part)),\nwhere t is the number of psychological scores. We denote X(mess) as the vector of the participant's words frequencies while"}]}