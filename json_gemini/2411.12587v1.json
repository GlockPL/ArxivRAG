{"title": "Whisper Finetuning on Nepali Language", "authors": ["Sanjay Rijal", "Shital Adhikari", "Manish Dahal", "Manish Awale", "Vaghawan Ojha"], "abstract": "Despite the growing advancements in Automatic Speech Recognition (ASR) models, the development of robust models for underrepresented languages, such as Nepali, remains a challenge. This research focuses on making an exhaustive and generalized dataset followed by fine-tuning OpenAI's Whisper models of different sizes to improve transcription (speech-to-text) accuracy for the Nepali language. We leverage publicly available ASR datasets and self-recorded custom datasets with a diverse range of accents, dialects, and speaking styles further enriched through augmentation. Our experimental results demonstrate that fine-tuning Whisper models on our curated custom dataset substantially reduces the Word Error Rate (WER) across all model sizes attributed to larger data variations in terms of speaker's age, gender, and sentiment, acoustic environment, dialect, denser audio segments (15-30 seconds) that are more compatible with Whisper's input, and manual curation of audios and transcriptions. Notably, our approach outperforms Whisper's baseline models trained on Fleur's dataset, achieving WER reductions of up to 36.2% on the small and 23.8% on medium models. Furthermore, we show that data augmentation plays a significant role in enhancing model robustness. Our approach underlines the importance of dataset quality, variation, and augmentation in the adaptation of state-of-the-art models to underrepresented languages for developing accurate ASR systems.", "sections": [{"title": "1 Introduction", "content": "Automatic Speech Recognition (ASR) systems have experienced remarkable advancements in recent years, driven largely by the development of large-scale, pre-trained models such as OpenAI's Whisper [1]. These models, trained on extensive multilingual datasets, have demonstrated impressive performance across a wide range of languages, supporting appli-cations in voice assistants, automated transcription, and accessibility tools for the hearing impaired. However, many low-resource languages, including Nepali, Hindi, and Albanian, continue to face challenges in achieving high transcription accuracy. These challenges stem from the limited availability of high-quality training data and the linguistic complexities of these languages, such as their rich morphology, multiple dialects, and unique phonetic structures.\nPrior works, including Whisper's original implementation, have trained models using datasets like Fleurs [12]. Fleurs provide only 10.38 hours of Nepali speech data with relatively short audio segments (2 to 10 seconds) and minimal variation in acoustic environments. As a result, the Word Error Rates (WER) for Nepali transcriptions in these models remain sig-nificantly higher compared to widely spoken languages. The ASR systems pre-trained in a supervised way across many datasets or domains are robust and can better generalize than models trained on a single source [2, 4, 3]. This is possible by combining the high-quality datasets as much as possible. OpenAI's Whisper [1] tried mitigating this by scaling weakly supervised speech recognition to the order of 680,000 hours of labeled data covering more than 96 languages. However, for low-resourced languages such as Nepali language, language-specific finetuning incorporating sentiments, accents, pronunciation, acoustic environment, gender, and age proves to perform better, especially with long audio segments [5, 6, 7].\nIn recent years, several studies [5, 8, 9] have focused on fine-tuning ASR models for low-resource languages, yielding significant improvements in transcription accuracy. Notable works include the Gram Vaani [5] and Vistaar [8] projects, which focused on languages like Hindi, Marathi, and Gujarati. These studies have demonstrated the efficacy of using domain-specific datasets and fine-tuning techniques to reduce WER. Gram Vaani, a social enterprise in rural India providing voice-based interactions for call center automation, orga-nized an ASR challenge in 2022 to improve speech recognition for agricultural and healthcare advisory systems. The study employed both traditional time-delay neural network-hidden Markov models (TDNN-HMM) and fully neural end-to-end (E2E) models, showing remark-able improvements in WER, between 30.1% to 37.3%, across different models. Similarly, Patel et al. [11] showed a better performance of 30.3% WER using the E2E Conformer model, surpassing the baseline of 34.8% set during the Gram Vaani challenge. Vistaar [8] also provides benchmark datasets for 59 Indian languages, facilitating comparative studies in diverse acoustic and linguistic environments. These initiatives emphasize the importance of creating rich, domain-specific datasets that reflect the linguistic diversity of the target languages. Moreover, the application of advanced models like Whisper [1] and wav2vec [10] in these studies has underscored the value of large-scale pre-training followed by language-specific fine-tuning.\nBuilding on these advancements, our work focuses on fine-tuning Whisper models for Nepali ASR which can also be implemented in other low-resourced languages. We leverage a diverse and extensive dataset that includes publicly available speech corpora such as Google Fleurs [12], Mozilla Common Voice [13], and OpenSLR [14, 15], along with a custom dataset built from self-recorded audios. The custom corpus encompasses a wide variety of audio environ-ments, speaker demographics, and speech styles, significantly expanding the diversity and volume of data available for training. The fine-tuned small model on the custom dataset shows a significant improvement of 68.5% compared to other ASR datasets. Moreover, we implement data augmentation techniques, to further enhance the model's robustness. By fine-tuning the Whisper models on this curated dataset, we significantly reduce WER across multiple model sizes including tiny (68.5%), base (70.2%), small (36.2%), and medium (23.8%). Our approach demonstrates the critical role that dataset quality, variation, and augmentation play in improving ASR performance for underrepresented languages."}, {"title": "2 Dataset", "content": "The dataset used in this work consists of publicly available ASR datasets such as Google Fleurs [12], Mozilla Common Voice [13], and OpenSLR datasets SLR43 [14] and SLR143 [15]. Additionally, we also prepared a custom dataset from a diverse and extensive pool of self-recordings and publicly available sources, representing a wide range of speakers and topic. As shown in Table 1, the cumulative dataset contains a total of 33.97 hours of raw audio, distributed across:\nThe data includes speech from various environments, ranging from clean to noisy conditions, ensuring that the transcriptions are accurate despite the background noise. This diverse dataset is critical for training robust speech recognition models that generalize well across different acoustic settings."}, {"title": "2.1 Custom Dataset Preparation", "content": "The custom dataset primarily consists of read speech and lecture-style recordings, sourced from publicly available sources and self-recorded audio. The self-recorded portion of the dataset includes readings from online news articles, academic texts, and other sources. The dataset is diverse in terms of speaker demographics, containing data from various age groups, backgrounds, sentiments, and genders, as detailed in Table 2. Moreover, as shown in Figure 1, our custom dataset introduces significantly larger vocabularies compared to other open-source datasets used here, allowing the model to generalize across a broader range of speech. We discuss this improved generalization in detail in subsection 4.1."}, {"title": "2.2 Train and Evaluation Dataset", "content": "We conducted a series of experiments for each dataset, fine-tuning Whisper's pre-trained models across multiple configurations. The training data was split into 80% for train-ing and 20% for evaluation. The same data partitioning strategy was applied across all datasets to ensure a fair comparison between our models and Whisper's baseline models, tiny, base, small, and medium. In addition, we fine-tune the above models on combined datasets: a) Fleurs, and Common Voice, b) Fleurs, Common Voice, and SLR (SLR43 + SLR143) c) Fleurs, Common Voice, SLR, and custom i.e. all_combined, d) augmented i.e. all_combined and its augmentation.\nWe shuffle the training and evaluation datasets separately to reduce the correlation between them, which minimizes the risk of overfitting and allows the model to generalize better to unseen data. Moreover, for a fair and better generalization, we use evaluation data as 30% the size of the shuffled training dataset for individual corpus. This is important because if we split the evaluation data only from a specific dataset then it won't generalize with the same WER accuracy on other datasets. For example, in the case of Fleurs if both training and evaluation are samples of the Fleurs dataset, then it will perform poorly on the rest of the dataset, especially when the audio segments are more than 10 seconds.\nAs shown in Figures 3 and 5, the average WER of the training data decreases progressively as the model learns to handle the diverse linguistic characteristics present in the Nepali language, while the validation data consistently demonstrate improved performance as well."}, {"title": "3 Fine-tuning Pipeline", "content": "Our overall pipeline is shown in Figure 2. The pipeline consists of four main stages: data preparation, dataset processing, model training, and inference. As explained in subsection 2.1, our custom dataset requires a few audio pre-processing which includes audio chunking, silence removal, and unrecognized audio and transcriptions filtering. After forming the custom dataset we merge it with other open-source datasets and all the transcripts into a single metadata.csv file.\nAll datasets are loaded in audiofolder format, followed by preprocessing to resample audio clips to a uniform 16kHz frequency. The pipeline then performs feature extraction and tokenization to prepare the data for model training. The samples are then filtered by length and label suitability to ensure consistency across the dataset.\nFor model training, we employ the Whisper architecture [1] by fully training its base model rather than using a pre-trained model. The Whisper model is initialized with specific training configurations, and a train-test split is defined as described in subsection 2.2. During train-ing, the pipeline utilizes Whisper's architecture, loss functions, and optimizers enhancing the convergence. Checkpoints are then saved periodically to allow resumption and model im-provement tracking. Finally, the model is used for inference with an audio input to generate transcriptions."}, {"title": "4 Experiments", "content": "We evaluate our fine-tuned Whisper models on various datasets, both independently and in combination (all_combined). To assess the impact of data augmentation, we also compare performance metrics, specifically loss and word error rate (WER) as primary metrics, before and after augmentation on all_combined. Additionally, we perform a comparative analysis with the Whisper models presented in OpenAI's original paper, which used the Fleurs dataset [12] for the Nepali language.\nAll the experiments are performed on an Intel i9-10900 CPU @ 3.70GHz paired with 64GB DDR4 RAM and a 24GB NVIDIA GeForce RTX 3090 @ 33MHz GPU."}, {"title": "4.1 Comparison on Different Datasets", "content": "We individually compare the results of fine-tuned small models across different individual and combined corpora. We initially fine-tuned the models on individual datasets as outlined in Table 1. However, given the relatively small size of these datasets, some models displayed signs of overfitting after a certain number of epochs, despite adjustments to hyperparameters, leading to suboptimal results. The overfitting can be attributed to the limited diversity and volume of the Nepali language datasets shown in Table 1, which proved insufficient for robust deep-learning training. To mitigate this issue, we earlystop the training process at 1500 epoch and the results are shown in Figure 3. Following early stopping, all datasets show a decreasing trend in loss and WER as shown in Figures 3 and 4, with the custom dataset demonstrating a marked improvement in WER across all the models compared to the other datasets. This enhancement is due to the custom dataset's greater comprehensiveness, encompassing a wider range of metadata and a larger vocabulary.\nConsidering the overfitting issue on limited individual datasets, we combine these datasets on a cumulative basis into a single corpus as explained in section 2.2 and perform a comparative analysis. By training the models on this combined dataset for 4000 epochs, we achieve better"}, {"title": "4.2 Augmentation Results", "content": "Following the combination of datasets, we applied data augmentation techniques to further enhance model performance. Specifically, we introduce an 8000Hz white noise to the raw audio using torchaudio [16]. This augmentation is done on all_combined dataset only for the audios whose resampled noise lengths are less than the original audio length. Although very simple, introducing white noise to the raw audio data not only increased the volume of the dataset but also improved the model's performance, as reflected by the decrease in WER in Figure 6. This highlights the role of data augmentation as a critical step in model fine-tuning, especially when working with limited data. While the observed WER reduction is modest (4%), we anticipate that more sophisticated augmentation methods could yield more substantial improvements. However, as data augmentation is not the primary focus of our study, we only demonstrate a basic augmentation method to show how even simple techniques can improve model performance.\nGiven that we have already compared WER and predictions across individual and combined datasets, we restrict our evaluation of augmentation results to the small fine-tuned model on the combined dataset (all_combined). The observed improvements are consistent across other datasets as well, as evidenced by Figure 7, which illustrates training on the Fleurs dataset with augmentation up to 7,000 epochs without overfitting. Predictions from models trained on all_combined and augmented datasets are presented in Table 5, demonstrat-ing subtle yet meaningful improvements in prediction accuracy for the model trained with augmented data."}, {"title": "4.3 Comparison with Whisper", "content": "Alongside our custom dataset, we perform a comparative analysis on the benchmark dataset, Fleurs [12] originally used by Whisper for a comprehensive and fair evaluation. We finetune the tiny, base, small, and medium whisper models. However, because of the GPU limi-tation, we couldn't fully train the large-v1 and large-v2 models. Due to an overfitting issue as explained in subsection 4.1, we use augmented datasets for training the models. Table 6 compares WER on various models between the whisper and our approach. We also show a progressive graphical comparison of WERs on different fine-tuned models in Figure 7. Our fine-tuned models show a significantly improved WER on all the trained models for the Nepali language. This improvement is most evident in the small and medium models,"}, {"title": "5 Discussion and Conclusion", "content": "In this study, we focus on creating an exhaustive dataset and fine-tuning OpenAI's Whis-per models for the task of Nepali language transcription, addressing key challenges such as dataset limitations and model overfitting. Our results demonstrate that dataset and vocab-ulary size, inclusivity, variability, model input compatibility, and augmentation strategies play a critical role in improving the model's transcription performance, as evaluated through WER.\nThe differences in performance between models trained on other open-source ASR datasets and our custom dataset are notable. The custom dataset provides the model with a more comprehensive representation of the linguistic variations in the Nepali language in terms of dialect, speaker accents, and environments compared to the Fleurs dataset used in Whis-per's original paper and other open-source datasets compared here. This diversity allows the model to capture more nuanced features of the Nepali language, leading to better gen-eralization. Comparing quantitatively, the fine-tuned models on the custom dataset show a significant improvement across all individual, combined, and augmented datasets. Combin-ing the individual corpus and data augmentation further enhanced the model's performance. While we used relatively simple augmentation, it demonstrates that even minor augmenta-tion techniques can significantly enhance transcription accuracy in low-resource languages like Nepali.\nWhen comparing our fine-tuned models with OpenAI's Whisper models, the results show that our models significantly outperform the original Whisper models across all evaluated sizes tiny, base, small, and medium. The progressive improvement across model sizes highlights the effectiveness of fine-tuning for domain-specific tasks, even with limited com-puting resources. While Whisper's training data, Fleurs consists of relatively short audio clips (2s-10s), our dataset contains longer and denser audio clips ranging from 5s to 30s. This wide range of clips is more compatible with Whisper's input, enabling the model to better capture contextual information over extended sequences.\nIn conclusion, our work shows the importance of dataset and their effectiveness on increasing the accuracy of speech-to-text model such as Whisper. Future work could explore more advanced augmentation techniques, fine-tuning larger Whisper models, and implementing similar fine-tuning approaches on other limited-resourced languages, potentially leading to further improvements in transcription accuracy."}]}