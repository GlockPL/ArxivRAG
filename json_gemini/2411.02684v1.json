{"title": "Towards Intelligent Augmented Reality (iAR): A Taxonomy of Context, an Architecture for iAR, and an Empirical Study", "authors": ["Shakiba Davari", "Daniel Stover", "Alexander Giovannelli", "Cory Ilo", "Doug. A. Bowman"], "abstract": "Recent advancements in Augmented Reality (AR) research have highlighted the critical role of context awareness in enhancing interface effectiveness and user experience. This underscores the need for intelligent AR (IAR) interfaces that dynamically adapt across various contexts to provide optimal experiences. In this paper, we (a) propose a comprehensive framework for context-aware inference and adaptation in iAR, (b) introduce a taxonomy that describes context through quantifiable input data, and (c) present an architecture that outlines the implementation of our proposed framework and taxonomy within iAR. Additionally, we present an empirical AR experiment to observe user behavior and record user performance, context, and user-specified adaptations to the AR interfaces within a context-switching scenario. We (d) explore the nuanced relationships between context and user adaptations in this scenario and discuss the significance of our framework in identifying these patterns. This experiment emphasizes the significance of context-awareness in iAR and provides a preliminary training dataset for this specific Scenario.", "sections": [{"title": "1 INTRODUCTION", "content": "Augmented Reality (AR) integrates digital content into the real world, enabling simultaneous, continuous, and effortless access to multiple pieces of information anytime and anywhere [29, 55, 69, 70]. However, distractions, real-world occlusion, and information overload in AR can challenge users' performance, perception, and awareness. Unlike physical devices, AR's boundless potential for ubiquitously displaying information within the user's environment complicates the design of a universally optimal interface. Whether for entertainment or information access in offices, streets, meetings, or emergencies, users engage with AR in diverse contexts, each with varying requirements, constraints, and preferences that impact its effectiveness. Optimal AR must identify the appropriate AR content, presentation, and interaction within a specific context, highlighting the importance of context awareness in AR interface design [16, 36].\nWe distinguish between context-aware AR and intelligent AR (iAR). Context-aware AR adapts based on scenario-specific design principles using a limited range of contextual components. However, AR design complexity stems from the interdependence and interaction of various contextual components. Thus, by overlooking certain contextual com-ponents, the effectiveness of these context-aware interfaces remains constrained. Intelligent AR (iAR), aims to predict the impact of var-ious adaptations based on the current state of all relevant contextual components that may influence AR effectiveness without relying on predefined design principles.\nWhile prior research proposed taxonomies for context, many of them overlook components essential to AR interface effectiveness or involve implicit elements without a clear framework for detecting and quantify-ing them within an iAR system. Towards designing iAR interfaces, in this work, we explore three Research Questions (RQs):\nRQ1. How can context be described to an iAR through explicit and quantifiable components?\nFrom the literature, we investigate prior conceptualizations and charac-terizations of context, along with the factors that influence AR effec-tiveness. Through an iterative refinement process, brainstorming, and a thorough literature review, we develop a comprehensive taxonomy of quantifiable contextual components that impact AR effectiveness.\nRQ2. How can iAR systems utilize our taxonomy of explicit context to infer implicit information about the effectiveness of their design?\nThe complexity of context awareness is not just detecting the contextual components but intelligently inferring how the specific combination of all these components should be used to adapt the interface. For instance, while overlaying AR content on other individuals' faces might be highly intrusive during a conversation [19], it could be deemed acceptable or even beneficial when studying in a crowded library. Towards providing the optimal AR, we propose an iAR architecture to integrate contextual components and learn to optimize AR effectiveness by inferring the impact of various adaptations.\nRQ3. How do users adapt their AR interface in a context-switching scenario?\nDesigning an effective iAR necessitates understanding the intricate relationship between the context and the user-preferred adaptations to various AR design dimensions, requiring training data involving user adaptations across diverse contexts.\nTo address these RQs, we make the following contributions. In Section 3.1, we propose a comprehensive taxonomy of explicit and quantifiable contextual components for iAR.To provide a road map towards designing iAR, Section 3 presents a framework utilizing our taxonomy and proposes a potential architecture for context-aware infer-ence and adaptation in iAR. In Section 4, we present an empirical study observing context, user performance, and user-specified adaptations to an AR interface in a context-switching scenario. We analyze this data in Section 5 to explore how users adapt the interface based on contextual factors. The results highlight the importance of implicit inferences for optimizing AR effectiveness and enhancing user experience in iAR."}, {"title": "2 RELATED WORK", "content": "In today's life, the growing dependence on personal computing de-vices such as mobile phones for decision-making and task perfor-mance has led to challenges, such as disruptions to social interac-tions [1, 14, 20, 39,65]. This has driven Ubiquitous computing's goal of seamlessly embedding near-constant, yet unobtrusive, digital infor-mation into daily life [69, 70]. Research indicates that by integrating the information into the real world (RW), AR can enhance efficiency and memory retention and reduce such challenges [18,67]. However, AR effectiveness depends on presenting the right information at the right time and form [9, 28, 36]. This work focuses on developing iAR systems that enhance user experience and efficiency by ensuring timely and appropriate information delivery in any context.\nVarious studies, such as ARWin and ARBrowse, have demonstrated the benefits of AR for providing on-demand information, assistance, and entertainment, spanning from domestic use to workplace collab-oration and productivity [22, 43, 45]. However, the effectiveness of AR interfaces is influenced by factors such as information overload, visual clutter, obtrusiveness, and distractions, all of which can nega-tively affect situational awareness, cognitive load, and performance [7, 25, 35, 44, 66]. For instance, in social contexts, poorly designed AR interfaces can obstruct communication by occluding facial expres-sions, leading to social isolation and privacy concerns [3,33,37,38].\nThis has led to research on AR design principles for non-invasive AR and view management that provide socially relevant information while maintaining visual focus on interlocutors [19,41,57].\nThe RW Spatial setting and occlusion also significantly impact AR effectiveness [15, 24, 48, 50]. Extensive research has proposed AR design principles to address occlusion between RW and virtual ob-jects [8, 47, 55, 63, 72]. These principles often emphasize view man-agement techniques that ensure non-occlusive yet visible placement of virtual content [4, 6, 34, 54], as well as peripheral placement ap-proaches [11,51,53]. Other strategies prioritize the real world, activat-ing virtual content only when necessary through user-triggered adap-tations, such as transparency, layout, or level of detail (LoD) [26, 27]. This work examines the underexplored impact of RW spatial settings and occlusions on AR effectiveness, focusing on user-specified AR adaptations in context-switching scenarios.\nAn effective AR interface must include considerations such as posi-tioning, transparency, and LoD for timely and appropriate information presentation in a given context. Pervasive AR envisions \u201ccontinuous, universal, and omnipresent\" integration of AR content into various daily tasks [36]. Non-adaptive AR interfaces, which fail to adjust to user context, risk presenting information inappropriately. Achieving Pervasive AR requires a deep understanding of the user's changing context, task, and environment to inform interface design. AR devices, equipped with sensors, offer unique opportunities for context detection, as seen in research assisting visually impaired users [56], highlighting AR's potential to meet everyday information needs.\nContext-aware interfaces, such as ARWin and HoloDoc, propose de-sign principles to optimize content presentation and enhance user expe-riences in specialized contexts using contextual data like user attention, spatial settings, lighting, and environmental cues [46, 68, 71]. However, an interface suitable in one scenario may be ineffective in another, as user priorities shift across contexts, requiring the interface to adapt ac-cordingly [23,40]. For instance, while providing relevant information to the user's conversation can enhance user experience [19, 65], the user's cognitive load may require low levels of information if they are engaged in a high-level collaborative task [10,59]. User-triggered, context-aware interfaces allow manual selection of contextually-customized AR con-tent presentation, offering predictability and control but increasing user effort [60,61]. Recent research has focused on automatic adaptations to AR content without manual intervention, demonstrating the integration of unobtrusive context-aware AR into daily life [13, 45, 49].\nTo enable Pervasive AR, the interface must automatically detect and respond to contextual changes. However, current context-aware inter-faces are limited in their adaptation scope, detect only a narrow range of contextual components, such as fatigue or social interactions, and use predefined design principles tailored to specific scenarios [12, 19]. This work investigates iAR interfaces capable of optimal adaptations in unfamiliar contexts without prior knowledge.\nEnabling iAR requires a framework that supports all potential AR adaptations, the detection and representation of the context, and its delivery to the decision-making process [21,58]. Previous work identi-fies a design space of the potential adaptations to an AR interface [17]. Various approaches have been proposed for representing context, such as the 5 Ws (Who, What, Where, When, Why), or using categories like location, identity, time, and activity [2,58,62]. Others study specific sub-contexts such as physical environment [13,34], objects [42, 71], back-ground [52], and depth perception [30]. However, these taxonomies often overlook components essential to AR interface effectiveness, in-corporate implicit elements that cannot be automatically detected or"}, {"title": "3 CONTEXT-Aware INFERENCE AND ADAPTATION", "content": "The context dictates the user's specific limitations, wants, and needs, influencing their preferences for the AR interface at each time. To properly adapt, an iAR must use its input data to identify these user preferences in each context (Figure 1). We classify the iAR input into three types:\nSpecified input are quantifiable information provided manually by the user. iAR coherently aims to limit reliance on this input type. That is because these inputs are limited to specific human-understandable data and prone to human error. They can also be slow and inaccurate, distract the user, and increase their physical and mental load, which contradicts these interfaces' objective of increasing efficiency and effectiveness.\nSensed input are raw or processed quantifiable information directly received from sensors.Sensed inputs from various sensors like a camera, microphone, accelerometer, gyroscope, and magnetometer provide information about the environment, the user, the device's pose, etc. This input type can also include processed sensor data for recognizing and tracking faces, objects, voice commands, gestures, and gaze.\nExtracted input are quantifiable information retrieved from previously stored data on various personal or publicly available data storage and databases through in-network and web queries. These data storage and databases are collections of information that were specified or sensed in the past. For example, extracted inputs can provide information such as the GPS location and time of day.\nTo address RQ2., we propose a framework to design iAR that re-ceives context as their input, infers the user's preferences for the inter-face, and makes the proper adaptations accordingly (Figure 2). This section provides a taxonomy to describe the context within this frame-work and proposes an architecture demonstrating how we envision implementing our framework and taxonomy within iAR systems.\nTo inform the design and development of iAR, it is essential to describe the context comprehensively through data that can serve as input for these systems. This framework describes Context as the knowledge derived from all the available contextual components that can affect the AR interface's design dimensions. These contextual components are considered as any piece of specified, extracted, or sensed input representing dynamic or persistent contextual information.\nPersistent contextual components are static or change infrequently and are quantified by extracted inputs. Information about the user's education, local time zone, or family members is a few examples of persistent contextual components. While recording a change in these contextual components is through specified or sensed inputs, they will be stored on an internal or external data storage and extracted when accessed for context detection.\nTransient contextual components are prone to more frequent and unexpected changes and need to be quantified more frequently through sensed or extracted input, e.g., user's body gesture, speech content, environmental lighting.\nSection 3.1 provides a taxonomy, classifying contextual components that influence the AR interface into three main categories: User, Setting, and Setting-User Interplay. As previously discussed, most contextual components can be specified. However, iAR aims to replace this input type when possible, especially for Transient contextual components. To reduce reliance on specified inputs, this taxonomy identifies alternative extracted and sensed input types, for each contextual component.\nWe define an Inference as an interface design dimension's level of the contingency on context (Figure 2). Since the AR interface is part of the virtual setting, an inference may correlate multiple interface design dimensions. We refer to inferences that only correlate interface de-sign dimensions as Design Inferences, while Context-aware Inferences refer to those involving one or a combination of multiple contextual components\nInferences indicate the potential impact of specific Adaptations on the performance of the interface. The inference unit receives the context, described in Section 3.1, as its input and, using different methods and sources, deduces a collection of contextual inferences in the present context. However, each design dimension can be involved in numerous inferences. Within a context, multiple inferences from different sources can provide duplicated or contradictory adaptations to a specific design dimension. To automatically adapt and optimize the interface's effectiveness, this framework proposes an Adaptation process to integrate these inferences and identify the optimal non-conflicting set of adaptations for the interface.\nGiven the multitude of contextual components and their potential variations, it is infeasible to replicate all possible contexts, much less to determine the optimal adaptation that the iAR system must apply for each unique context. We define a scenario as a collection of distinct contexts with at least one common contextual component. For example, a walking scenario encompasses contexts such as walking in the street, walking on a treadmill, playing soccer, and any other contexts in which the user's physical state is walking. Inferences enables an iAR interface to generalize the known optimal adaptations for a specific context to other contexts within the same scenario. Additionally, Inferences enable an iAR interface to deduce information such as User Task and User Objective that are influenced by multiple contextual components and cannot be captured through a single contextual component. While such information can be included within context through specified contextual components, they are usually limited or inaccurate (e.g., in a task-switching scenario). For example, imagine Sanaz working from home and contacting the IT specialist through non-work-related social media to resolve an issue with the company's Slack channel. Without inferences, a specified User Task contextual component may indicate she is \"working\", leading her AR interface to close non-work related social media and interrupt her communication with the IT specialist. Inferences enable her iAR to correlate her Location, the Disconnected Slack, and the IT specialist to prevent disrupting their communication."}, {"title": "3.1 Taxonomy of Context", "content": "To design a taxonomy of explicit and quantifiable contextual compo-nents that influence the effectiveness of AR interfaces, we adopted a comprehensive and systematic methodology. Initially, to avoid biases present in existing literature, we conducted multiple brainstorming sessions to enumerate all conceivable contextual components. During this process, we explored the XR design space and identified design elements that can adapt the content and presentation of an XR inter-face [17]. We thoroughly evaluated the challenges and capabilities associated with these design dimensions and compiled a list of con-textual components that could influence them. For instance, when considering the color theme as a design element, a pertinent challenge is that blue colors are not suitable for nighttime use and can cause eye fatigue. This underscores the importance of \"the time of day\" as a crucial contextual component. As another example, the position, size, and opacity design elements of an AR object can result in the occlusion of important objects in RW. This emphasizes the vitality of contextual components providing information about the user's interplay with objects and people within the setting.\nTo ensure the inclusion of all pertinent aspects from the literature, we reviewed well-established taxonomies for describing context and an-alyzed the contextual information utilized in various proposed context-aware AR interfaces. These steps enabled us to compile a comprehen-sive list of contextual information. Subsequently, we engaged in an iterative refinement process, continuously reassessing the relevance of each contextual component, ensuring a robust and effective taxonomy.\nAbowd et al. establish location, identity, time, and activity as the main context categories [58], Grubert et al., categorize context into user, physical environment, and social/cultural context; other research has focused on adaptations based on objects [42,71], background [52], depth perception [30], and physical environment [13, 34] from the context. We identify three main categories of quantifiable contextual components that influence AR effectiveness either directly or indirectly through inferences (e.g., see the scenario of Sanaz working from home in Section 3). While the Setting category provides information about the state of the world, excluding the user, the User category focuses solely on the user, and the Setting-User Interplay category provides information about the interactions between context components from these categories. The contextual components of each context cate-gory are classified based on their input type or the semantics of the information they describe.\n3.1.1 User\nWho the user is, what they are currently doing, how they feel, and other information about the user themselves affect the type of information they need and their preferences, resources, and limitations for accessing and interacting with this information. For example, a chef may need less detailed instructions from a recipe, while a beginner requires the same recipe to be presented in more detail and include visualizations.\nIn our taxonomy, we classify the contextual components only about the user, isolated from their setting, as the user category of context. Based on the input type, we classify the contextual components within the User category into two subcategories: User Profile and User State"}, {"title": "Persistent User Profile", "content": "Persistent information about users and their characteristics, such as their values and responsibilities, sociocultural factors, socioeconomic status, psychological and physiological abilities, and traits, can affect their interface. For example, a visually impaired person may require a different font size than a person with normal vision.\nPersistent User profile provides established information about the user through persistent contextual components. These are contextual components such as meaningful dates, locations, and people, user's physical limitations, assets, properties, weekly schedule, salary, occu-pation, and responsibilities that can be extracted from a user's digital profiles, health records, legal documents, bank accounts, etc.\nDue to their staticity and infrequent changes, user profile contex-tual components are extracted inputs. This information is specified or sensed if changed; however, when accessing input to detect the context they are extracted. For example, a student specifies their demographic and education information and the classes they are taking on their dig-ital profile. Inputs such as their class schedules and course syllabus extracted from this digital profile and the learning management system allow their AR interface to provide the class material for them automat-ically. Similarly, inputs such as their level of education and customer preferences can be extracted from this digital profile."}, {"title": "Transient User State", "content": "Adaptations to a user's interface can also be affected by transient factors such as their current physical activity, conversation topic, and mental and emotional state. For example, the user's emotional state can affect content adaptation, such as presenting the user's anxiety management tool during a panic attack. User state provides transient information about the user through Transient contextual components that change continually. Contextual components like heart rate, body temperature, gestures, and speech content can be obtained using sensor data from an altimeter, 3-axis accelerometer, optical and temperature sensor, microphone, camera, gesture sensors, etc.\nSince the user state is subject to unpredictable and frequent changes, these components must be sensed. However, with accuracy at stake, current technological limitations may require combining or replacing sensed inputs with other contextual component types to remove or reduce reliance on specific sensors. For instance, when proper sensors are unavailable, costly, or require high power consumption, a combi-nation of specified and extracted inputs can be used as an alternative. In such cases, some contextual components can be sensed at specific intervals rather than constantly. During these intervals, these contextual components will be extracted from the previous measures. As another example, the user's emotional state can be specified, directly sensed from EEG sensors, or extracted from the user's mood tracker app."}, {"title": "3.1.2 Setting", "content": "By definition, AR integrates the virtual information within their envi-ronment [5]. Consequently, in addition to affecting the user's efficiency and effectiveness in their digital data acquisition, AR interfaces in-fluence their experience interacting and perceiving the RW and other people [64]. The state of the user's environment and other people, ob-jects, and digital devices within it constitute a crucial part of the context, influencing interface design dimensions that can cause or prevent con-flicts to the user's interaction and awareness of social, RW, and digital information. For instance, to adapt the AR content's layout and avoid interfering with a mobile user's visual attention and interpersonal inter-actions, contextual components providing information about semantic changes in the environment and social setting are crucial [13,57].\nThe user category isolated the user, providing all the contextual infor-mation about them. The setting category describes the context without any knowledge of the user's profile and state. Setting is provided through three main subcategories of contextual components describing the Real-World Environment and the Digital and Social Settings."}, {"title": "Real-World Environment", "content": "Involves contextual components describing the immediate surroundings of the user and the geographical and local phenomena. Local Environment is described by extracted spatiotemporal and geospatial contextual components about the user's current location.\nPersistent Local Environment includes persistent contextual compo-nents such as geopolitical data, local climate, culture, time zone, lan-guage, health resources, and social factors.\nTransient Local Environment includes Transient contextual components such as GPS location, the local weather and time, the natural, cultural, and political phenomena, and other spatiotemporal data.\nThe user's Immediate Environment changes as they move through-out the day. The immediate environment provides transient information to describe the user's exact surroundings. It includes Transient con-textual components that must be sensed from the sensors in the user's immediate environment. contextual components such as temperature, lighting, noise and audio, spatial map, smoke, nearby objects, their state, and their motions are a few of these components."}, {"title": "Digital Setting", "content": "The digital setting includes contextual components that extract infor-mation from the available global knowledge and the immediate digital and virtual interfaces and devices without any knowledge of the user and the non-digital RW environment.\nPersistent Global Knowledge includes persistent contextual compo-nents extracted from global and common knowledge such as LLMs, search engines, public articles, social media posts, and web pages, providing information such as cultural calendars and holidays,\nTransient Digital Setting includes Transient contextual components that describe the current content, state, and activities of the available digital interfaces and devices within the user's immediate surroundings. The transient digital setting includes contextual components such as the available AR input and output modalities, the current state of its design dimensions, and the nearby devices and their states. For example, when driving an electric car, contextual components, such as the battery level and movement state, can affect the AR design."}, {"title": "Social Setting", "content": "Throughout the day, changes in the number of people around a user, who they are, and what they do affect the user's AR interface. For instance, when the user is in a public setting, concerns such as social stigma, privacy, and security can affect their preferred interaction modality. As part of the Setting, Social Setting describes the other people involved in a context without any knowledge of the user. Social setting involves Transient contextual components that provide information about the virtual or physical presence, state, and profile of other people, i.e., attendees, in the user context and their interactions with each other and the context. Attendee Profile includes extracted contextual components from publicly available data or internal databases that identify the persistent profile of the present attendees. While the user profile data are persistent for each attendee, these attendee profile contextual components are transient due to changes in the presence of each specific attendee. The attendee profile includes each attendee's social status, occupation, relationships, values, responsibilities, etc.\nAttendee State provides sensed inputs that describe the transient state of each attendee. The attendee state includes contextual components such as face and speech detection, the physical and emotional state of each person, and their motions.\nAttendee-Setting Interplay provides sensed or extracted to provide information about how the attendee's profile and state links with other people, the RW environment, and digital setting within the user's con-text The attendee SUI includes contextual components such as the personal relationships of attendees with each other and their conversa-tion and interactions with other people and the setting."}, {"title": "3.1.3 Setting-User Interplay", "content": "Providing effective, efficient, and unobtrusive AR interfaces requires knowledge about the user's needs, preferences, resources, and limita-tions. Such information can be learned from the user's background, characteristics, current state, activities, and their setting. However, the interplay between the User and Setting can create new needs and limitations, resolve previous ones, and lead to conflicts and associations within the context, influencing the effectiveness of the interface and how it should adapt. How the user interacts with their RW and digital environment and what information they need, dictate the input and output modalities and the placements of virtual content that must be prioritized/avoided. The conflicts and interactions between the user and the setting are critical components of context. To account for these dy-namics, previous taxonomies have incorporated \"task\" as a contextual element. However, \u201ctask\" is often an ambiguous concept that resists quantification. In our taxonomy, Setting-User Interplay (SUI) includes Transient contextual components that can be sensed or extracted to pro-vide information about how the user's profile and state links with other people and the RW environment and digital setting. SUI includes contextual components such as physical and digital objects, locations, dates, and people that are occluded, related to the user, or in-teracted with (through physical, verbal, gaze, gesture, digital, social, or emotional interactions). For example, SUI provides sensed contextual components on the user's fixation point in the setting. Through Infer-ences, this contextual component can indirectly provide information about the people and virtual and physical objects of interest. Similarly, SUI contextual components from face recognition provide information for potential Inferences that distinguish the optimal interface for formal vs. intimate social interactions."}, {"title": "3.2 Architecture", "content": "In this section, we propose an architecture for implementing iAR based on our proposed framework and taxonomy. This architecture outlines the envisioned implementation of the Context, Inference, and Adap-tation Units, including input, output, and the data flow and commu-nication mechanisms within iAR systems Figure 5. To illustrate this, we present examples of data structures for each data type used in the architecture. These examples are for demonstration purposes only and may vary based on the specific implementation of the architecture in the real world. For instance, while we use a dictionary as an example data structure for contexts, depending on the system implementation, other data structures, such as a graph or a tree, may be more appropriate.\n3.2.1\nContext Unit\nThe Context Unit identifies the value of contextual components relevant to the user, their setting, and their interplay with it (SUI) from the input data specified by the user or derived from sensors and internal and external databases. Here we represent context as a dictionary with a key for each contextual component (CC). For example, the present context can be represented as:\nContext(present) = {CC1 : val(CC1),CC2 : val(CC2), ...}\nA scenario can be represented by a sub-dictionary of context, including only the common contextual components within that scenario.\nSince the interface is part of the virtual setting, iAR directly affects the Context Unit (Figure 5). We represent an adaptation by a heterogeneous dynamic list that includes three items: the design dimension that will be modified, the value assigned to this element by the adaptation, and a dynamic list of the entities to which the adaptation applies. For instance:\nAdaptation(bodyFixed) =['FoR','bodyFixed',[Map,NotificationMenu, ...]]\n3.2.2 Inference Unit\nThe Inference Unit extracts information regarding the potential impact of specific adaptations from the context and provides it to the Adaptation Unit. The iAR interface deduces inferences through Rule-Based and Intelligent Inference modules. Here we represent an inference by a heterogeneous list comprised of two items: a dynamic list of adaptations and an impact score indicating its effectiveness:"}, {"title": "Rule-based Inference Module", "content": "The rule-based module utilizes a database of predefined Design Principles to identify inferences. This module uses these rules and the present context to identify the present scenarios for which a rule exists and infers the rule-based inferences. A scenario generalizes multiple contexts, making it easier to repli-cate. By considering a relatively small number of contexts within a scenario, patterns can be identified. These patterns can provide gen-eralized Design Principles about the impact of specific adjustments in that scenario. For instance, a Design Principle for a walking scenario, DP(walking), may indicate a highly favorable impact for adapting the virtual content's frame of reference to be body-fixed, as it creates a more natural and intuitive interaction. Since each unique context be-longs to numerous scenarios, utilizing a collection of Design Principles congruent with the present context can enhance the effectiveness of adaptations.\nA Design Principle can be represented with a heterogeneous list of two items: a scenario and a dynamic list of inferences for that scenario. A few examples are:\nDP(walking) = [Scenario(walking), [Inference(bodyFixed)]]\nDP(street) = [Scenario (street), [Inference (multi\u2081), Inference (headFixed)]]\nCurrently, the utilization of rule-based inference constitutes the primary method for the design of context-aware interfaces. As interface designers, we manually established rules (i.e., design principles/considerations/choices) that specify the impact of particular adaptations for specific scenarios. We refer to these design choices, design guidelines, rules, etc. as Design Principles. In this work, we distinguish iAR from context-aware AR. We consider Context-aware AR as the existing contemporary, proposed approaches where the Design Principles are the only source for providing inferences to the adaptation unit. For instance, consider two Design Principles for a scenario Scenario(Occlusion) in which the user's view of a child they are monitoring is occluded by App(occluding): DP(repositionMechanism) infers moving up the occluding entity, while DP(OpacityMechanism) infers making the occluding entity transparent [18].\nAdaptation(Move) = [position, position.y+10, App(occluding)]\nInference (Move) = [Adaptation(Move), 0.99]\nDP(repositionMechanism) = [Scenario (Occlusion), [Inference (Move)]]\nAdaptation(Transparent) = [color,color.a = 0.0,App(occluding)]\nInference (Transparent) = [Adaptation(Transparent), 0.99]\nDP(OpacityMechanism) = [Scenario(Occlusion), [Inference (Transparent)]]\nRule-based inferences offer iAR the potential to automatically adapt even when a limited number of contextual components can be iden-tified and extracted. This approach also allows iXR to utilize rules from the currently available interface design knowledge for specific scenarios. Additionally, the rule-based approach offers the potential for adaptability to a wide range of contexts through a small set of rules. However, despite its capabilities, rule-based inference is not without its limitations and challenges.\nOne key limitation of this approach is the manual process of ex-tracting and designing these Design Principles, which can be prone to human error, demanding, time-consuming, and limited to the ex-isting knowledge. On the other hand, the over-generalization of the adaptations through a limited number of rules can negatively impact the performance. For example, a rule mandating the Opacity of virtual content in a conversation potentially enhances the user experience by reducing distraction when the conversation primarily aims to establish social connections and the virtual content presents supplementary in-formation. However, if the primary objective of the conversation is to convey information from the virtual content, this rule may degrade the interface's performance. As a result, an interface that lacks the appropriate rules for the latter scenario may fail in such contexts.\nFurthermore, the effectiveness of this approach is highly dependent on the reliability and accuracy of the Design Principles. For instance, incorporating a design principle on the scenario of [Scenario(y2023) =\nCC(year): 2023] would impact all conceivable scenarios within the year 2023 and may prove to be detrimental if not appropriately considered.\n085other key aspect of iAR is its ability to tailor the experience to the individual user. However, it is difficult for the interface designer to create Design Principles that take into account the unique preferences, values, and limitations of a user who deviates from the typical, average user [31]. While limited user-specified rules can be introduced, the rule-based inference approach is constrained to personal experiences."}, {"title": "Intelligent Inference Module", "content": "Intelligent Inference can enhance iAR's capabilities by leveraging Machine Learning (ML) techniques to learn from the user's behavior and automatically extract inferences without any prior knowledge or rules. This approach allows iAR to go beyond the limitations of traditional context-aware AR interfaces. By observing the user's manual adaptations in different contexts", "Module": "Universal Inferences are generated through static ML models"}, {"Module": "Personalized Inferences are gener-ated through online machine-learning models that continuously process the user's feedback on each adaptation in real time. Online learning is particularly useful in this case as it not only addresses privacy concerns but also eliminates the need for a unified infrastructure for data collec-tion and storage, thus reducing the technological requirements for data storage and transfer. As new adaptations are made, these models update their parameters and adapt to the user's preferences. The data used for training these models includes both the user's manual interventions and the interface's automatic adaptations in each context. This feedback provides the online models with information on which adaptations the user accepted or rejected, enabling them to adjust their internal param-eters to better suit the user's needs. For instance, receiving the user's manual intervention on the interface, Adaptation(manual), after an auto-matic adaptation, Adaptation(automatic), can serve as feedback for the machine learning model to adjust its internal parameters. Specifically, this feedback can reinforce a negative assessment of the effectiveness of Adaptation(manual), while concurrently reinforcing a positive assess-ment of the effectiveness of the Adaptation(automatic) in"}]}