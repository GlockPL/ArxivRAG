{"title": "CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational Dialogue Systems", "authors": ["Joanito Agili Lopo", "Marina Indah Prasasti", "Alma Permatasari"], "abstract": "In this study, we introduce CIKMar\u00b9, an efficient approach to educational dialogue systems powered by the Gemma Language model. By leveraging a Dual-Encoder ranking system that incorporates both BERT and SBERT model, we have designed CIKMar to deliver highly relevant and accurate responses, even with the constraints of a smaller language model size. Our evaluation reveals that CIKMar achieves a robust recall and F1-score of 0.70 using BERTScore metrics. However, we have identified a significant challenge: the Dual-Encoder tends to prioritize theoretical responses over practical ones. These findings underscore the potential of compact and efficient models like Gemma in democratizing access to advanced educational AI systems, ensuring effective and contextually appropriate responses.", "sections": [{"title": "1 Introduction", "content": "The emergence of powerful Large Language Models (LLMs) such as ChatGPT has been proven effective in various tasks, including generating text that is nearly indistinguishable from human-written text (Kasneci et al., 2023; Omidvar and An, 2023). Building on the success in text generation, LLMs have shown significant potential in various applications, especially in the educational domain.\nIn recent years, there have been various efforts to utilize these powerful large language models (LLMs) in education. They have been deployed in teacher-student collaborations as virtual tutors, guiding students through exercises, offering personalized learning experiences, and providing intelligent tutoring (Kamalov et al., 2023). Additionally, they are used for adaptive assessments and serve as conversational partners in learning scenarios (Tan et al., 2023; Li et al., 2024).\nDespite these promising opportunities, the use of generative models as a foundation for downstream"}, {"title": "2 Related Work", "content": "Researchers have extensively investigated the effectiveness of various approaches utilizing language models. Sridhar et al. (2023) enhanced LLM performance on web navigation tasks using Actor-Summarizer Hierarchical (ASH) prompting, while Kong et al. (2024) improved reasoning benchmarks with role-play prompting. Kojima et al. (2023) showed that modifying prompt structure enables LLMs to perform multi-step reasoning in zero-shot settings.\nIn the educational context, Adigwe and Yuan (2023) and Hicke et al. (2023) used GPT-3 and GPT-4 to generate educational dialogue responses, achieving high DialogRPT and BERTScore results with hand-written zero-shot prompts. Similarly, Vasselli et al. (2023) used GPT-3.5 Turbo with manual few-shot prompts based on DialogRPT selection, which contributed most to the final outputs. Fine-tuning has also proven effective by utilizing large language models (LLMs) in educational domain. Balad\u00f3n et al. (2023) used the LoRa method to fine-tune models like BLOOM-3B, Llama 7B (Touvron et al., 2023), and OPT 2.7B (Zhang et al., 2022). They found that even the smaller OPT 2.7B model, with careful fine-tuning, could achieve better performance. Similarly, Huber et al. (2022) demonstrated that GPT-2, enhanced with reinforcement learning via the NLPO algorithm (Ramamurthy et al., 2023), achieved high BERTScores.\nDue to the high computational power needed for fine-tuning and domain adaptation, Omidvar and An (2023) introduced semantic in-context learning, using private knowledge sources for accurate answers. Gu et al. (2024) proposed reducing LLM sizes through knowledge distillation, training smaller models to replicate larger ones. Their experiments with distilled GPT-3 versions showed competitive performance on various benchmarks.\nOur research aims to develop an educational dialogue system using Gemma 1.1 IT 2B. This system uses prompts to guide LLMs in generating outputs based on contextual understanding, relevance, engagement, clarity, and feedback. To optimize results, it employs dual encoders (BERT and SBERT) to rerank top candidates. Our objective is to democratize open model LLM in real-world scenarios, ensuring accurate, relevant responses while enhancing student engagement and understanding in educational dialogues."}, {"title": "3 Methods", "content": "3.1 Data\nWe used data from the BEA 2023 shared task, sourced from the Teacher-Student Chatroom Corpus (TSCC) (Caines et al., 2020, 2022). This corpus consists of several conversations where an English teacher interacts with a student to work on language exercises and assess the student's English proficiency (Tack et al., 2023). Each conversation contains multiple responses and starts with either teacher: or student: prefixed. The reference text is the teacher's response that follows the previous input dialogue. The corpus includes a training set of 2,747 conversations, a development set of 305 conversations, and a test set of 273 conversations, totaling 3,325 conversations.\nSince the data was collected from real-time teacher-student conversations, turn-taking is not as consistent as in most dialogue systems (Vasselli et al., 2023). There are two patterns mostly occur: conversations ending with the student (teacher reply) and conversations ending with the teacher (teacher continuation). This condition occurs in 38% of the training data and 40% of the development data. \n\n3.2 Prompt Ensemble\nWe utilized hand-written prompts from Vasselli et al. (2023) to build our system. The prompts include Zero-shot and Few-shot types, targeting both general and specific scenarios. We used only the five main prompts available as they are already tailored for teacher responses and continuations. This selection also ensures general applicability to other datasets or conversations. For full details explanation of each prompt take a look at Appendix A.\nIn the creation of the few-shot prompts, it requires positive and negative examples to help the"}, {"title": "3.3 Gemma Instruct-tuned Model", "content": "Our main system leverages a pretrained language model with a prompting approach rather than training one from scratch or fine-tuning it on a new dataset. We used the Gemma 1.1 IT 2B model (Team et al., 2024), 2-billion parameter open model developed by Google for efficient CPU and on-device applications. The model has shown strong performance across academic benchmarks for language understanding, reasoning, and safety, such as MMLU (Hendrycks et al., 2021), SIQA (Sap et al., 2019), HumanEval (Chen et al., 2021), and Winogrande (Sakaguchi et al., 2019). These results indicate its promising performance in educational contexts.\nWe followed the instruction-formatted control tokens suggested in the Gemma technical report to avoid out-of-distribution and poor generation.\n\nSpecifically, the relevant token user represents the role, and its content includes the conversation history followed by the prompt. Meanwhile, the model turn responds to the user dialogue.\nIn our experiments with the training and development sets, the Gemma model sometimes generated hallucinations on the first attempt, such as factually incorrect response, nonsensical content, overly long response, and content disconnected from the input prompt. However, performance improved on the second and third attempts. Therefore, to ensure the best response, we generated each candidate three times before selecting the final output.\nWe configured several parameters to control the model's output such as set the max_length of the generated output to 512 tokens, no_repeat_ngram_size to 2 to avoid repetition, and used top_k=50 and top_p=0.95 to balance randomness and coherence. The temperature was set to 0.7 for more conservative choices. Finally, we enabled probabilistic sampling over greedy decoding."}, {"title": "3.4 Dual-Encoder Reranking", "content": "Inspired by previous research (Vasselli et al., 2023; Suzgun et al., 2022; Haroutunian et al., 2023), our system generates multiple candidate outputs from different manually designed prompts and then reranks these outputs using a heuristically defined scoring function. Specifically, for the scoring function, we utilize SBERT (Reimers and Gurevych, 2019) and BERT (Devlin et al., 2019), averaging cosine similarity scores from their embeddings to assess fine-grained semantic relevance and context-response matching in the embedding space between the conversation history and the generated response.\nIn the given setup, we start with a dialog as a context ctx and a list of candidate responses {cand\u2081, cand\u2082,..., candm}. Initially, we compute SBERT and BERT embeddings for both the context and the candidate responses. For BERT embeddings we calculated by averaging token embeddings across the sequence dimension.\nThe cosine similarity between the context and each candidate response embedding, for both SBERT and BERT, is calculated using:\n$S_{emb}(i) = cos(e^{emb}_{ctx}, e^{emb}_{candi}) = \\frac{e^{emb}_{ctx}. e^{emb}_{candi}}{||e^{emb}_{ctx}|| ||e^{emb}_{candi}||}$ where emb \u2208 {sbert, bert}.\nTo combine these similarity scores for each candidate response, we average the SBERT and BERT similarity scores.\nFinally, the candidates are ranked based on these combined similarity scores in descending scores. The indices of the candidates are sorted according to their combined scores, and it returns the list of candidates responses ordered from most to least relevant to the given context."}, {"title": "3.5 Post-processing", "content": "The raw outputs from model often included inconsistent formatting, such as phrases prefixed by \"**\" or starting with unwanted text like Teacher: or Student:. Additionally, the model sometimes appended lengthy explanations to its responses beginning with Explanation:, adding unnecessary length. However, we observed a consistent pattern where the actual response always began with a quotation mark \".\nTo standardize these outputs, we implemented a post-processing step. First, we defined a regular expression pattern,  \\*\\*.\\*?:\\*\\*\\n\\n, to identify and remove any unwanted initial phrases. This pattern effectively removed prefixes like \"**\", Teacher:, or Student:. Next, each response was processed to retain only the text following the first occurrence of a quotation mark, discarding any preamble or unnecessary content. Finally, we trimmed any leading or trailing whitespace."}, {"title": "4 Result & Analysis", "content": "4.1 Main Result\nOur main result are presented in Table 2, showcasing comparisons among systems from the BEA Shared Task 2023 (Tack et al., 2023), ranked primarily by BERTScore. However, this comparison isn't fully comprehensive as the BEA Shared Task also considers human evaluations and DialogRPT (Gao et al., 2020) score. The human evaluation metric is restricted and not publicly available, and we encountered challenges with DialogRPT, which might have issues with the model, as it is return the same score for each context.\nCIKMar demonstrates competitive performance against baseline systems like NAISTeacher and Adaio based on BERTScore\u00b2. Specifically, we achieve a robust recall score of 0.70, slightly below NAISTeacher's 0.71. This indicates that our Dual-Encoder ranking effectively retrieves many contextually relevant responses compared to the reference answer. Furthermore, our F1-Score of 0.70 is comparable to models such as S-ICL and Alpaca, which utilize fine-tuning and larger model sizes, demonstrating our model's capability to capture similarity and produce coherent, contextually appropriate responses even using simple and small model size.\n4.2 Evaluation Metrics\nTo ensure the reliability of our approach, we employed word overlap-based metric ROUGE (Lin, 2004) and the neural network-based metric Dialog perplexity (Li et al., 2016)\u00b3 to further asses our system. We computed ROUGE metrics:"}, {"title": "4.3 In-depth Output Analysis", "content": "We manually inspected the model's outputs and evaluated each prompt's contribution by examining 10 outputs in detail. Table 3 presents the top candidate responses selected through Dual-Encoder ranking for five examples.\nTo examine the impact of prompts on the best responses, we use the dialogue context test_0006, as shown in Table 4, as an example. Here, the teacher is explaining a grammar lesson when the student mentions needing 10 more minutes and feeling very cold in the room. The model's response is inconsistent, as it incorrectly associates \"cold\" with the grammar lesson rather than the student's condition. This suggests that the model may focus on one situation in the conversation and struggle to adapt when new contexts arise. Consequently, the context of \"cold\" is incorrectly forced to fit the context itself.\nWe also found that the model struggles with teacher continuation problems. When the dialogue ends with the teacher, the model often seems un-"}, {"title": "4.4 Dual Encoder Effect", "content": "We conducted a manual investigation to assess the dual encoder's impact on selecting the best candidates. Analyzing five dialogue-response pairs' embedding spaces, as shown in Figure 4, we discovered that the Dual-Encoder can avoid the pitfalls of distance-measurement-only. Notably, in dialogue 2, candidate 2 appeared closer to its context than candidate 1 in the embedding space, yet the dual encoder ranked candidate 1 as the best candidate (denoted by *). This phenomenon occurred across multiple dialogues, highlighting SBERT and BERT's role in enhancing the model's consideration of contextual relevance and semantic similarity between dialogues and responses, as discussed earlier.\nTo evaluate the dual encoder's ranking quality, we investigated the phenomenon of closely clustered embedding. Specifically, candidates for dialogue 3 exhibited dense clustering, where increasing embedding proximity indicated greater similarity, complicating candidate selection. After analyzing all candidates, candidates 1 and 4 emerged as optimal choices for this dialogue, supported by their relatedness in the embedding space. However, the Dual-Encoder prioritized candidate 4, suggesting a preference for theoretical discussion and exploration rather than practical context in its ranking criteria.\nFurthermore, we noted a tendency for candidates within each dialogue to cluster together. This indicates that the Gemma model consistently produces similar embeddings for each candidate per dialogue, demonstrating stable performance across various dialogues. However, certain candidates were positioned farther from their cluster and nearer to candidates in another cluster. This suggests that the model sometimes encounters difficulties accurately interpreting the dialogue context. We suspect that this issue may arise because SBERT's dominance over BERT leads to a loss of full context. Further investigation is required to delve deeper into this matter."}, {"title": "5 Conclusion & Future Work", "content": "We have shown that CIKMar, an educational dialogue generation approach using prompts and a Dual-Encoder ranking with the Gemma language model, yields promising results in educational settings. By utilizing the Gemma 2B model, we maintain high performance in response relevance and accuracy with a smaller, more accessible model.\nDespite these strong performances, we have identified limitations hindering optimal results. Specifically, the Dual-Encoder often prioritizes theoretical discussion over practical contextual responses, potentially leading to irrelevant rankings. Future research should explore scenarios where either SBERT or BERT dominates ranking scores.\nAdditionally, crafting more specific prompts is crucial for deeper contextual understanding in educational dialogues. Lastly, refining the Gemma model to focus on educational contexts and adapt to shifting conversation dynamics is recommended."}, {"title": "A Ensemble Prompts Explanation", "content": "Below are the prompts we are using in this research.\nThe details explanation of each prompt can refer to Vasselli et al. (2023).\nZero-shot prompts consist of instructions without examples, while few-shot prompts include examples to guide the model towards relevant responses. Prompt (1) is categorized as a zero-shot prompt, refined to address issues like overly direct answers and sounding too much like an assistant. The rest of the prompts- (2), (3), (4), (5)-are few-shot prompts that require positive and negative examples to help the model avoid irrelevant responses.\nEach prompt serves a specific purpose: Prompt (1) focuses on Contextual Understanding, Prompt (2) ensures Relevance, Prompt (3) aims to enhance Engagement, Prompt (4) emphasizes Clarity, and Prompt (5) is dedicated to providing Feedback. Together, these prompts tailor the model's responses to match the student's current learning stage and needs. By grasping the context (Contextual Understanding), the prompts direct the model to produce responses that are relevant to the student's queries, thereby maintaining focus and relevance (Relevance). This relevance boosts student engagement (Engagement), encouraging sustained interest and participation, which is further supported by clear communication (Clarity) that makes complex concepts easier to understand and reduces confusion. Collectively, these prompts help the model generate optimal responses for educational contexts.\n(1) The following is a partial conversation between an English language learner and their teacher:\n(conversation)\nCan you give an example teacher follow-up to their previous message that would be helpful for the language learner? The message should be concise, and worded simply. It should either encourage the continuation of the current topic or gracefully transition to a new teacher-provided topic. Questions should be specific and not open-ended. Try to not sound like an assistant, but a teacher, in charge of the flow of the lesson.\n(2) Concatenation of prompt (1) and the following:"}, {"title": null, "content": "Good example: 'Can you make a sentence using 'within'?' Bad example: 'Do you have any questions about prepositions?'\n(3) Concatenation of prompt (1) and the following:\nHow does a teacher sound when responding to a student? What kinds of things would teachers say that chatbots would not? What do they not say? In your response provide an example of a response that sounds like a teacher and one that sounds like a chatbot? Respond succinctly\n(4) The following is a partial conversation between an English language learner and their teacher:\n(conversation)\nThey are in the middle of a lesson. Can you give a possible way the teacher could respond?\nRemember: A teacher typically sounds knowledgeable, authoritative, and focused on guiding and instructing students. They may use formal language and provide detailed explanations. Teachers often offer constructive feedback, encourage critical thinking, and ask probing questions to stimulate learning.\nExample of a teacher-like response: \"That's a great observation, but let's delve deeper into the topic. Can you provide some evidence to support your claim?\"\nA chatbot, on the other hand, may sound more informal and conversational. It tends to provide general information or brief responses without much elaboration.\nExample of a chatbot-like response: \"Interesting! Tell me more.\" Teachers typically avoid expressing personal opinions or biases. They also refrain from engaging in casual banter or unrelated conversations to maintain a professional and educational atmosphere.\n(5) Concatenation of prompt (1) and the following:\nHere is an example of an exceptional teacher follow-up:\n\"Great job, student! Just a small correction, we should use the present tense verb \"built\" instead of \"build\" since the construction has already been completed. So the correct sentence is: \"The International Space Station is built by NASA.\" Keep up the good work! Now, let's move on to a new topic - let's talk about your favorite hobbies. Can you tell me what activities you enjoy doing in your free time?\"\nHere is an example of a poor teacher followup: \"That's an interesting observation about poshness. Can you think of any examples of British accents that might be associated with poshness?\""}]}