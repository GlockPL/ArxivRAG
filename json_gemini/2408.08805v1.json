{"title": "CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational Dialogue Systems", "authors": ["Joanito Agili Lopo", "Marina Indah Prasasti", "Alma Permatasari"], "abstract": "In this study, we introduce CIKMar\u00b9, an efficient approach to educational dialogue systems powered by the Gemma Language model. By leveraging a Dual-Encoder ranking system that incorporates both BERT and SBERT model, we have designed CIKMar to deliver highly relevant and accurate responses, even with the constraints of a smaller language model size. Our evaluation reveals that CIKMar achieves a robust recall and F1-score of 0.70 using BERTScore metrics. However, we have identified a significant challenge: the Dual-Encoder tends to prioritize theoretical responses over practical ones. These findings underscore the potential of compact and efficient models like Gemma in democratizing access to advanced educational AI systems, ensuring effective and contextually appropriate responses.", "sections": [{"title": "1 Introduction", "content": "The emergence of powerful Large Language Models (LLMs) such as ChatGPT has been proven effective in various tasks, including generating text that is nearly indistinguishable from human-written text (Kasneci et al., 2023; Omidvar and An, 2023). Building on the success in text generation, LLMs have shown significant potential in various applications, especially in the educational domain.\nIn recent years, there have been various efforts to utilize these powerful large language models (LLMs) in education. They have been deployed in teacher-student collaborations as virtual tutors, guiding students through exercises, offering personalized learning experiences, and providing intelligent tutoring (Kamalov et al., 2023). Additionally, they are used for adaptive assessments and serve as conversational partners in learning scenarios (Tan et al., 2023; Li et al., 2024).\nDespite these promising opportunities, the use of generative models as a foundation for downstream"}, {"title": "2 Related Work", "content": "Researchers have extensively investigated the effectiveness of various approaches utilizing language models. Sridhar et al. (2023) enhanced LLM performance on web navigation tasks using Actor-Summarizer Hierarchical (ASH) prompting, while Kong et al. (2024) improved reasoning benchmarks with role-play prompting. Kojima et al. (2023) showed that modifying prompt structure enables LLMs to perform multi-step reasoning in zero-shot settings.\nIn the educational context, Adigwe and Yuan (2023) and Hicke et al. (2023) used GPT-3 and GPT-4 to generate educational dialogue responses, achieving high DialogRPT and BERTScore results with hand-written zero-shot prompts. Similarly, Vasselli et al. (2023) used GPT-3.5 Turbo with manual few-shot prompts based on DialogRPT selection, which contributed most to the final outputs.\nFine-tuning has also proven effective by utilizing large language models (LLMs) in educational domain. Balad\u00f3n et al. (2023) used the LoRa method to fine-tune models like BLOOM-3B, Llama 7B (Touvron et al., 2023), and OPT 2.7B (Zhang et al., 2022). They found that even the smaller OPT 2.7B model, with careful fine-tuning, could achieve better performance. Similarly, Huber et al. (2022) demonstrated that GPT-2, enhanced with reinforcement learning via the NLPO algorithm (Ramamurthy et al., 2023), achieved high BERTScores.\nDue to the high computational power needed for fine-tuning and domain adaptation, Omidvar and An (2023) introduced semantic in-context learning, using private knowledge sources for accurate answers. Gu et al. (2024) proposed reducing LLM sizes through knowledge distillation, training smaller models to replicate larger ones. Their experiments with distilled GPT-3 versions showed competitive performance on various benchmarks."}, {"title": "3 Methods", "content": "We used data from the BEA 2023 shared task, sourced from the Teacher-Student Chatroom Corpus (TSCC) (Caines et al., 2020, 2022). This corpus consists of several conversations where an English teacher interacts with a student to work on language exercises and assess the student's English proficiency (Tack et al., 2023). Each conversation contains multiple responses and starts with either teacher: or student: prefixed. The reference text is the teacher's response that follows the previous input dialogue. The corpus includes a training set of 2,747 conversations, a development set of 305 conversations, and a test set of 273 conversations, totaling 3,325 conversations.\nSince the data was collected from real-time teacher-student conversations, turn-taking is not as consistent as in most dialogue systems (Vasselli et al., 2023). There are two patterns mostly occur: conversations ending with the student (teacher reply) and conversations ending with the teacher (teacher continuation). This condition occurs in 38% of the training data and 40% of the development data.", "3.1 Data": null, "3.2 Prompt Ensemble": "We utilized hand-written prompts from Vasselli et al. (2023) to build our system. The prompts include Zero-shot and Few-shot types, targeting both general and specific scenarios. We used only the five main prompts available as they are already tailored for teacher responses and continuations. This selection also ensures general applicability to other datasets or conversations.", "3.3 Gemma Instruct-tuned Model": "Our main system leverages a pretrained language model with a prompting approach rather than training one from scratch or fine-tuning it on a new dataset. We used the Gemma 1.1 IT 2B model (Team et al., 2024), 2-billion parameter open model developed by Google for efficient CPU and on-device applications. The model has shown strong performance across academic benchmarks for language understanding, reasoning, and safety, such as MMLU (Hendrycks et al., 2021), SIQA (Sap et al., 2019), HumanEval (Chen et al., 2021), and Winogrande (Sakaguchi et al., 2019). These results indicate its promising performance in educational contexts.\nWe followed the instruction-formatted control tokens suggested in the Gemma technical report to avoid out-of-distribution and poor generation.", "3.4 Dual-Encoder Reranking": "Inspired by previous research (Vasselli et al., 2023; Suzgun et al., 2022; Haroutunian et al., 2023), our system generates multiple candidate outputs from different manually designed prompts and then reranks these outputs using a heuristically defined scoring function. Specifically, for the scoring function, we utilize SBERT (Reimers and Gurevych,", "3.5 Post-processing": "The raw outputs from model often included inconsistent formatting, such as phrases prefixed by \"**\" or starting with unwanted text like Teacher: or Student:. Additionally, the model sometimes appended lengthy explanations to its responses beginning with Explanation:, adding unnecessary length. However, we observed a consistent pattern where the actual response always began with a quotation mark \".\nTo standardize these outputs, we implemented a post-processing step. First, we defined a regular expression pattern, \\*\\*.\\*?:\\*\\*\\n\\n, to identify and remove any unwanted initial phrases. This pattern effectively removed prefixes like \"**\", Teacher:, or Student:. Next, each response was processed to retain only the text following the first occurrence of a quotation mark, discarding any preamble or unnecessary content. Finally, we trimmed any leading or trailing whitespace."}, {"title": "4 Result & Analysis", "content": "Our main result are presented in Table 2, showcasing comparisons among systems from the BEA Shared Task 2023 (Tack et al., 2023), ranked primarily by BERTScore. However, this comparison isn't fully comprehensive as the BEA Shared Task also considers human evaluations and DialogRPT (Gao et al., 2020) score. The human evaluation metric is restricted and not publicly available, and we encountered challenges with DialogRPT, which might have issues with the model, as it is return the same score for each context.\nCIKMar demonstrates competitive performance against baseline systems like NAISTeacher and Adaio based on BERTScore2. Specifically, we achieve a robust recall score of 0.70, slightly below NAISTeacher's 0.71. This indicates that our Dual-Encoder ranking effectively retrieves many contextually relevant responses compared to the reference answer. Furthermore, our F1-Score of 0.70 is comparable to models such as S-ICL and Alpaca, which utilize fine-tuning and larger model sizes, demonstrating our model's capability to capture similarity and produce coherent, contextually appropriate responses even using simple and small model size.", "4.1 Main Result": null, "4.2 Evaluation Metrics": "To ensure the reliability of our approach, we employed word overlap-based metric ROUGE (Lin, 2004) and the neural network-based metric Dialog perplexity (Li et al., 2016)\u00b3 to further asses our system. We computed ROUGE metrics:", "4.3 In-depth Output Analysis": "We manually inspected the model's outputs and evaluated each prompt's contribution by examining 10 outputs in detail. Table 3 presents the top candidate responses selected through Dual-Encoder ranking for five examples.\nTo examine the impact of prompts on the best responses, we use the dialogue context test_0006, as shown in Table 4, as an example. Here, the teacher is explaining a grammar lesson when the student mentions needing 10 more minutes and feeling very cold in the room. The model's response is inconsistent, as it incorrectly associates \"cold\" with the grammar lesson rather than the student's condition. This suggests that the model may focus on one situation in the conversation and struggle to adapt when new contexts arise. Consequently, the context of \"cold\" is incorrectly forced to fit the context itself.", "4.4 Dual Encoder Effect": "We conducted a manual investigation to assess the dual encoder's impact on selecting the best candidates. Analyzing five dialogue-response pairs' embedding spaces, as shown in Figure 4, we discovered that the Dual-Encoder can avoid the pitfalls of distance-measurement-only. Notably, in dialogue 2, candidate 2 appeared closer to its context than candidate 1 in the embedding space, yet the dual encoder ranked candidate 1 as the best candidate (denoted by *). This phenomenon occurred across multiple dialogues, highlighting SBERT and BERT's role in enhancing the model's consideration of contextual relevance and semantic similarity between dialogues and responses, as discussed earlier.\nTo evaluate the dual encoder's ranking quality, we investigated the phenomenon of closely clustered embedding. Specifically, candidates for dialogue 3 exhibited dense clustering, where increasing embedding proximity indicated greater simi-"}, {"title": "5 Conclusion & Future Work", "content": "We have shown that CIKMar, an educational dialogue generation approach using prompts and a Dual-Encoder ranking with the Gemma language model, yields promising results in educational settings. By utilizing the Gemma 2B model, we maintain high performance in response relevance and accuracy with a smaller, more accessible model.\nDespite these strong performances, we have identified limitations hindering optimal results. Specifically, the Dual-Encoder often prioritizes theoretical discussion over practical contextual responses, potentially leading to irrelevant rankings. Future research should explore scenarios where either SBERT or BERT dominates ranking scores.\nAdditionally, crafting more specific prompts is crucial for deeper contextual understanding in educational dialogues. Lastly, refining the Gemma model to focus on educational contexts and adapt to shifting conversation dynamics is recommended."}, {"title": "A Ensemble Prompts Explanation", "content": "Below are the prompts we are using in this research. The details explanation of each prompt can refer to Vasselli et al. (2023).\nZero-shot prompts consist of instructions without examples, while few-shot prompts include examples to guide the model towards relevant responses. Prompt (1) is categorized as a zero-shot prompt, refined to address issues like overly direct answers and sounding too much like an assistant. The rest of the prompts- (2), (3), (4), (5)-are few-shot prompts that require positive and negative examples to help the model avoid irrelevant responses.\nEach prompt serves a specific purpose: Prompt (1) focuses on Contextual Understanding, Prompt (2) ensures Relevance, Prompt (3) aims to enhance Engagement, Prompt (4) emphasizes Clarity, and Prompt (5) is dedicated to providing Feedback. Together, these prompts tailor the model's responses to match the student's current learning stage and needs. By grasping the context (Contextual Understanding), the prompts direct the model to produce responses that are relevant to the student's queries, thereby maintaining focus and relevance (Relevance). This relevance boosts student engagement (Engagement), encouraging sustained interest and participation, which is further supported by clear communication (Clarity) that makes complex concepts easier to understand and reduces confusion. Collectively, these prompts help the model generate optimal responses for educational contexts.\n(1) The following is a partial conversation between an English language learner and their teacher:\n(conversation)\nCan you give an example teacher follow-up to their previous message that would be helpful for the language learner? The message should be concise, and worded simply. It should either encourage the continuation of the current topic or gracefully transition to a new teacher-provided topic. Questions should be specific and not open-ended. Try to not sound like an assistant, but a teacher, in charge of the flow of the lesson.\n(2) Concatenation of prompt (1) and the following:", "A Ensemble Prompts Explanation": null}]}