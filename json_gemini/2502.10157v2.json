{"title": "Session Rec: Next Session Prediction Paradigm For Generative Sequential Recommendation", "authors": ["Lei Huang", "Hao Guo", "Linzhi Peng", "Long Zhang", "Xiaoteng Wang", "Daoyuan Wang", "Shichao Wang", "Jinpeng Wang", "Lei Wang", "Sheng Chen"], "abstract": "We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for generative sequential recommendation, addressing the fundamental misalignment between conventional next-item prediction paradigm (NIPP) and real-world recommendation scenarios. Unlike NIPP's item-level autoregressive generation that contradicts actual session-based user interactions, our framework introduces a session-aware representation learning through hierarchical sequence aggregation (intra/inter-session), reducing attention computation complexity while enabling implicit modeling of massive negative interactions, and a session-based prediction objective that better captures users' diverse interests through multi-item recommendation in next sessions. Moreover, we found that incorporating a rank loss for items within the session under the next session prediction paradigm can significantly improve the ranking effectiveness of generative sequence recommendation models. We also verified that SessionRec exhibits clear power-law scaling laws similar to those observed in LLMs. Extensive experiments conducted on public datasets and online A/B test in Meituan App demonstrate the effectiveness of SessionRec. The proposed paradigm establishes new foundations for developing industrial-scale generative recommendation systems through its model-agnostic architecture and computational efficiency.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [1, 2, 16, 25, 29, 35] have exhibited tremendous success across various fields due to their highly scalable and generalizable nature. The core strategy of LLMs, particularly the GPT series [1, 2, 28, 29], involves predicting the next token by encoding a sequence of input tokens, a process integral to both model training and text generation. Notably, similar paradigms exist within generative sequential recommendation models. For instance, GRU4Rec [13], SASRec [15], and BERT4Rec [32] leverage comparable strategies by predicting subsequent item in users' interaction histories. These models mirror the scalability and generalizability advantages of LLMs, thereby enhancing the performance of recommendation systems. Meta's recent introduction of HSTU, a unified generative recommendation framework [40], demonstrates substantial advantages in industrial scenarios and corroborates the scaling laws within generative sequential recommendations.\nCurrent generative sequential recommendation models, following the Next Item Prediction Paradigm (NIPP), treat tokens as items and recommend user interests through an autoregressive, item-by-item process. However, this approach diverges from real-world needs, where user interactions occur at the session/request level, with all items generated simultaneously when the session initiates. Thus, sessions, rather than individual items, should form the basic unit in recommendation systems. Furthermore, there is a misalignment with the NIPP's modeling objectives as existing models often predict only one item per user session. In contrast, user interests are diverse, necessitating the prediction of multiple items that the user might be interested in during their next session. Additionally, models like HSTU leverage full historical item interactions (e.g., exposure, click, like, dislike) to refine user interest representations and other models also emphasize the importance of cross-statistical features [7, 19, 31, 40]. Although incorporating users' negative interactions for the extraction of implicit statistical features has been shown to enhance recommendations, the reliance on training primarily with positive interactions or using sampled cross-entropy loss for the next positive item [17, 32, 33] constrains effective item ranking and accurate predictions within sessions.\nTo address these considerations, we introduce the Next Session Prediction Paradigm (NSPP) in generative sequential recommendation models. As illustrated in Figure 1, NSPP differs markedly from the traditional NIPP. Firstly, the user representation paradigm adopts session-level embeddings with both positive and negative interactions to capture broader contextual and cross statistical information within user sessions, which is inspired by visual transformers in computer vision [10, 22] that segment images into patches instead of single pixels. Secondly, the joint learning paradigm aligns both retrieval and ranking tasks concurrently within a session, utilizing comprehensive interaction data. Finally, the label prediction paradigm predicts positive interactions in the next session to accommodate several potentially intriguing items for the user.\nOverall, we propose a general NSPP framework for generative sequential recommendation, which is model-agnostic with plug-and-play characteristics. It can be tailored to integrate with any sequence encoding backbone such as RNN [8, 38], the conventional Transformer [34] and HSTU [40]. It also enhances sequence encoding efficiency and reduces computational complexity. In Transformer-like architectures [34], computational complexity scales as O(N^2) with input sequence length. By employing intra-session and inter-session sequence aggregation, NSPP significantly lowers computational demands, especially when incorporating a large amount of negative interaction behavior into the input sequence. It benefits both offline training and online serving in industrial recommendation systems. With the session of comprehensive interactions, we develop a rank loss to distinguish the negative behaviors in the prediction session, enabling more fine-grained predictions. This strengthens our conviction that a single generative model can be employed to manage both retrieval and ranking tasks concurrently in the next session paradigm, effectively replacing the traditional recommendation system's multi-model cascaded approach of retrieval, pre-ranking, and ranking. Last but not least, we also verified that the next session prediction paradigm continues to demonstrate scaling-law characteristics, with performance continuously improving as the data volume increases.\nOur contributions are summarized as follows:\n\u2022 We propose SessionRec, a novel next session prediction paradigm for generative sequential recommendation, by reconsidering how users engage with recommendation systems in practical applications, offering new insights into the design of generative recommendation algorithms.\n\u2022 We further propose a rank loss that captures the model's intra-session ranking capabilities, which can significantly enhance the ranking performance of generative recommendation models.\n\u2022 Extensive experiments conducted on public datasets and A/B test in online recommendation system of Meituan App demonstrate the effectiveness of our proposed SessionRec, notably with average 27% performance gains over the best baseline on the public datasets."}, {"title": "2 Related Work", "content": "Behavior Sequence Modeling. In recommendation systems, user behavior sequences contain a wealth of valuable information that reflects user interest preferences, which is crucial to characterizing user personalization. Several studies leverage users' historical interaction behavior sequences to understand their preferences and recommend items that they might be interested in. DIN [43], DIEN [42], DSIN [11], and MIMN [26] introduced the attention mechanism in modeling behavior sequences. By calculating the correlation between user historical behaviors and the target item, it assigns a dynamic weight to each behavior to better capture user interests. While longer user behavior sequences can provide more useful information for modeling user interests, they also place a significant burden on the latency and storage requirements of online serving systems. An possible solution is to retrieve the most relevant and important behaviors from an extremely long sequence by matching algorithm, such as category id [27], locality-sensitive hashing (LSH) [6], SimHash[3], and Efficient-Target-Attention [4].\nSequential Recommendation. Sequential recommendation models treat recommendation as a sequence-to-sequence generation task according to Markov Chains assumption. They generate the next item directly by considering the sequence of past user interactions. Methods such as GRU4Rec [13] and LSTM-based approaches [36] were particularly effective in capturing both long-term and short-term patterns in item transitions. Following the success of self-attention mechanisms [34] in natural language processing (NLP), researchers developed many Transformer-based models for sequential recommendation. SASRec [15] applies self-attention to sequential recommendation tasks, while BERT4Rec[32] uses the BERT architecture to model bidirectional relationships between items in a sequence. However, given the sparsity of user behaviors and the computational complexity, these methods typically focus only on users' positive behaviors in industrual implementations. HSTU [40] incorporates both positive and negative user behaviors into the sequence, capturing more accurate interests and preferences.\nLarge Language Models for Recommender Systems. Following the advent of ChatGPT, there has been an uptick in efforts within both the industry and academia to integrate Large Language Models (LLMs) into recommendation systems. Some studies utilize LLMs to produce semantic embeddings for items, which are then converted into semantic IDs through algorithms such as RQ-VAE [18, 30]. UEM [9] processed user history as plain text, generating token embeddings for history items. This approach greatly simplified user history tracking and enabled the incorporation of longer user histories into the language model, and allowed their representations to be learned in context. ILM [37] incorporating collaborative filtering knowledge into a frozen LLM for conversational recommendation tasks. LLMs can also be used to encode target feature entities [5], extracting their implicit embedding representations to feed into subsequent recommendation models. These methods primarily leverage the content understanding capabilities of LLMs to provide additional informational gains, rather than applying the underlying paradigm of LLM modeling directly to the recommendation system."}, {"title": "3 Next Session Prediction Paradigm", "content": "In sequential recommendation, let U = {u_1, u_2, ..., u_{|u|}\\} denote a set of users, V = {v_1, v_2, ..., v_{|v|}\\} be a set of items, and each item v is associated with some side information (e.g., action type, brand) F = {f_1, f_2,..., f_c}, where c is the number of side information features. For each user, the historical interaction sequence can be represented as S_u = [(v_1, F_1, s_1), (v_2, F_2, s_2), ..., (v_n, F_n, s_n)], where n denotes the sequence length, and the value of s_i denotes the session identifier. If the values of s_i are equal, it indicates that the interactions belong to the same session. Note that the historical interactions include not only positive interactions (e.g., click, purchase, like), but also negative interactions (e.g., exposure, dislike). Given the interaction sequence, NSPP aims to predict the items that user u will interact positively with in the next user session, which can be formulated as:\n[v_{n+1}, v_{n+2},..., v_{n+k}] = NSPP(S_u; SeqEncoder),\nwhere k denotes the number of positive interaction items in next session. SeqEncoder denotes the sequence encoding backbone with learnable parameters. While the NIPP aims to predict the next positive interaction item, which can be formulated as:\nv_{n+1} = NIPP(S_u; SeqEncoder).\nIt is evident that NSPP aligns more closely with the objectives of online recommendation systems. As a user session/request arrives, all items that the user might be positive interacted with need to be predicted at once, and it is impossible to predict item by item."}, {"title": "3.2 The Overall Framework", "content": "The overall framework of SessionRec is shown in Figure 2. Session-Rec is a hierarchical encoding structure that performs intra-session encoding followed by inter-session encoding, which is composed of four components: Embedding Module, Item-based Session Encoder (ISE), Session-based Sequence Encoder (SSE), and Session Prediction Module. ISE aggregates and encodes intra-session information to obtain session representations, which serve as the tokens of the session sequence. SSE then encodes the session sequence to derive the user interest representation. During this forward computation, the user's behavior sequence transforms from an item-level sequence to a session-level sequence. In the session prediction module, both the user representation and the prediction label are session-level. This approach also helps reduce the computational complexity of the network and effectively models the user's multiple interests."}, {"title": "3.3 Embedding Module", "content": "The model takes the historical user interaction sequence as input, where the item-ID and the item side information are discrete features. When side information comprises continuous attributes, techniques such as binning [20, 41] can be used to discretize them. For each discrete feature, an embedding table X_f \u2208 F^{|X|\u00d7d_f} can be assigned to store embeddings, where d_f is the embedding dimension. The feature embedding of value f_i can be obtained through a look-up table operation:\nE_{f_i} = LookUpTable(X_f, f_i).\nThe item-ID embeddings E_{iid} and the side information embeddings are then concatenated and transformed into the item embedding of dimension d through the MLP layer, which is formulated as:\nE_v = MLP([E_{iid}||E_{f_1}||E_{f_2}|| ... ||E_{f_c}]).\nwhere || denotes the vector concatenation operation."}, {"title": "3.4 Item-based Session Encoder", "content": "The interaction item sequence is transformed to the item embedding sequence as S'_u = [(E_{v_1}, s_1), (E_{v_2}, s_2), . . ., (E_{v_n}, s_n)] after embedding module. The Item-based Session Encoder (ISE) encodes and aggregates all item embedding sequences corresponding to each session identifier, which transforms the item-level embedding sequence into a session-level representation sequence as:\n[E_{S_1}, E_{S_2}, ..., E_{S_m}] = ISE([(E_{v_1}, s_1), (E_{v_2}, s_2), ..., (E_{v_n}, s_n)]),\nwhere E_S denotes the session representation output by ISE, and m denotes the number of sessions. In real-world scenarios, the number of items within each session may vary. To manage this during implementation, one can either use padding strategy to standardize session lengths or utilize RaggedTensor\u00b9 for flexibility. ISE can employ parameter-free pooling methods (such as max pooling or mean pooling) or parameterized methods like RNNs, Transformers. Our best practice is to employ the pooling method for ISE because it is straightforward and effective, achieving performance comparable to those of more complex encoders. In the experimental section, we performed an in-depth analysis of how various session encoders influence performance."}, {"title": "3.5 Session-based Sequence Encoder", "content": "The Session-based Sequence Encoder (SSE) takes the session-level representation sequence obtained from the Item-based Session Encoder (ISE) as input and encodes the entire sequence to derive the final user interest representation, which is the last output of the sequence encoder. This can be formulated as:\nE_u = SSE([E_{S_1}, E_{S_2},..., E_{S_m}]),\nwhere E_u represents the encoded user interest representation. Since our proposed NSPP is model-agnostic, SSE can be tailored to integrate with any sequence encoding backbone, including, but not limited to, RNN [8, 38], the conventional Transformer [34] and HSTU [40]. For improved modeling of interaction information among session-level tokens and to prevent temporal leakage issues during sequence encoding, a decoder-only Transformer architecture is advised for the SSE. This could be a traditional Transformer or an HSTU. Our best practice for SSE is to implement HSTU to achieve the best performance.\nIn our proposed NSPP, the input to the sequence encoder is transformed from an item-level sequence to a session-level sequence. Assuming the average session length is M (with M = 23 in the Meituan homepage recommendation scenario), the length of the input sequence to the sequence encoder is reduced by an average factor of M. If a Transformer-like architecture is used as the backbone, the computational complexity of the sequence encoder will be reduced by a factor of M\u00b2. This reduction offers significant benefits for both model training and online inference."}, {"title": "3.6 Session Prediction Module", "content": "The user interest representation embedding derived from the SSE encodes information about items that the user is likely to engage with positively in the upcoming session. Therefore, the likelihood of a user having a positive interaction with an item v_i, denoted as P_{ui}, is indicated by the similarity score between the user representation embedding E_u and the item embedding E_{v_i}:\nP_{ui} = DotProduct(E_u, E_{v_i}),\nwhere DotProduct represents the commonly useded operation of calculating the similarity between vectors through their dot product. The top K items with the highest similarity scores constitute the recommended results output by the model. In real-world industrial applications, given the extensive quantity of items that exceed hundreds of millions, the Approximate Nearest Neighbor (ANN) technique [23] is utilized to efficiently identify the top K items most related to the user interest representation."}, {"title": "3.7 Network Training", "content": "In training a model based on NSPP, the prediction target for each position's output in the sequence encoder involves the information of all positively interacted item tokens in the next session. We adopt the widely used sampled cross-entropy (CE) loss [13, 17] to train our model:\n\\mathcal{L}_{retrieval} = - \\sum_{i=1}^{N-1} \\sum_{v_j \\in V} \\log \\frac{\\exp(P_{v_j}^{S_i})}{\\exp(P_{v_j}^{S_i}) + \\sum_{v_k \\in \\overline{V}} \\exp(P_{v_k}^{S_i})},\nwhere N denotes the length of the user's session-level sequence, V denotes the set of positive interaction items in the next session to be predicted, and P_{v_j}^{S_i} represents the dot product similarity between the output vector at the i-th position of the sequence decoder and the item embedding of item v_j.  \\overline{V} denotes a random negative sampling set of size C. This is because, in practical scenarios, the item set can be exceedingly large, reaching hundreds of millions. Therefore, random negative sampling is used during training to avoid heavy computation."}, {"title": "3.7.2 Rank loss", "content": "To further enhance the model's ability to distinguish hard negative samples, we introduce a more challenging optimization objective on top of the retrieval loss. This means that the negative samples are not randomly sampled items but are instead obtained from negatively interacted items within the next session. The optimization objective is to rank items with which the user has positive interactions ahead of those with negative interactions within the same session. This aligns with the goal of the ranking task in recommendation systems, and thus we refer to it as the rank loss:\n\\mathcal{L}_{rank} = - \\sum_{i=1}^{N-1} \\sum_{v_j \\in V} \\log \\frac{\\exp(P_{v_j}^{S_i})}{\\exp(P_{v_j}^{S_i}) + \\sum_{v_k \\in \\overline{V_i}} \\exp(P_{v_k}^{S_i})},\nwhere  \\overline{V_i} represents the set of negative interaction items within the next session to be predicted."}, {"title": "3.7.3 Balance retrieval loss and rank loss", "content": "We multiply the rank loss by a weighting coefficient \u03b1 to balance the retrieval loss and the rank loss, enabling the model to possess both retrieval and ranking capabilities. The final loss function of NSPP is formulated as:\n\\mathcal{L}_{NSPP} = \\mathcal{L}_{retrival} + \\alpha * \\mathcal{L}_{rank}.\nAnalogously to the cascaded paradigm in recommendation systems where retrieval is followed by ranking, it is meaningful to first ensure that positive samples can be retrieved before optimizing ranking capabilities. Our experiments have validated this perspective. Specifically, incorporating a relatively small rank loss in the loss function can lead to substantial improvements in ranking performance. However, if the weight of the rank loss is set too high, the model may neglect retrieval capabilities during training, which can ultimately damage overall model performance."}, {"title": "4 Experiments", "content": "Although datasets such as MovieLens (1M, 20M) [12] and Amazon series [24] have been widely used in previous work, they lack session identifier information, which is essential for session aggregation. Additionally, these datasets only provide items that users have positively interacted with. Nevertheless, negative interactions, or exposure behaviors, are very crucial for implicit feature extraction in industry scenarios. Therefore, we employ two public datasets from real-world applications that better correspond to the data from actual recommendation systems. The statistics of the datasets are shown in Table 1."}, {"title": "4.2 Overall Performance Comparison", "content": "In Table 2, we compare the performance of the proposed session-level models and other item-level models adopting both the leave-one-item-out and the leave-one-session-out strategy. SessionRec-HSTU significantly outperforms the baselines under these two configurations. By performing computations at the session level, SessionRec-HSTU predicts diverse interests across a broader space, which enables simultaneous enhancement of prediction accuracy at both session and item levels. HSTU-based models demonstrate the best performance, followed by Transformer-based models, with GRU-based models performing the worst. Increasing the complexity of the model can enhance the accuracy of item-level models to some extent. Additionally, SASRec+ surpassing SASRec indicates that training with a multiple negative sampling strategy effectively improves model accuracy, aligning with the findings in [17]. Furthermore, the comparison between HSTU, relying exclusively on positive interactions, and HSTU+, which integrates extensive negative interaction behavior into the input sequence, demonstrates models benefit from incorporating full interactions. As depicted in Figure 3, training on the same number of positive and negative interactions, SessionRec-HSTU significantly reduces training time compared to HSTU+ via session-level aggregation, which significantly lowers computational time and improves overall efficiency. In conclusion, the results reveal that session-level models: a) exhibit superior ability to capture multiple interests within a single session,"}, {"title": "4.3 Flexible Model-Agnostic Paradigm", "content": "Since our proposed method is model-agnostic with the characteristics of plug and play, we verify the performance of next session paradigm based on different backbone of GRU, Transformer and HSTU. Table 3 reports the results of NDCG@500 and Recall@500. The experimental results demonstrate that integrating the next session paradigm into different backbones can lead to substantial performance improvements over the original backbones. On average, the performance enhancement across experimental datasets exceeds 39%. SessionRec-GRU and SessionRec-Transformer exhibit substantial gains in both NDCG@500 and Recall@500 metrics, highlighting the paradigm's ability to boost model accuracy. Notably, HSTU-based models, when augmented with session-level aggregation, consistently outperform their GRU and Transformer counterparts, emphasizing the paradigm's effectiveness in leveraging model complexity for superior results. This adaptability underscores the paradigm's potential to accommodate diverse application scenarios, making it a versatile tool for enhancing recommendation systems."}, {"title": "4.4 Session Encoder Study", "content": "For the session encoder, we employ mean pooling(mean), max pooling(max), max pooling following a ReLU layer(max_relu), GRU(gru), and Transformer(trans) to aggregate embeddings within the same session as shown in Table 4, where h is the number of headers, and l is the number of layers. Specifically, among the tested methods, mean pooling yield the best performance, followed by max pooling and max pooling augmented with a ReLU layer. And sequential-based models exhibit comparatively weaker performance.\nUpon further analysis, we observe that as computational complexity increases, performance generally tends to diminish. This suggests that during the aggregation phase, it may be beneficial to preserve the original representation of the items rather than introduce additional transformations. These findings imply that maintaining simplicity in representation helps capture essential session patterns more accurately, supporting the notion that straightforward aggregation techniques often suffice for effective session-based modeling."}, {"title": "4.5 Rank Loss Study", "content": "As illustrated in Figure 4, during model training, we optimize and balance retrieval capabilities and ranking quality by combining rank loss and retrieval loss. Rank loss focuses on improving the intra-session ranking accuracy of items, while retrieval loss ensures the model effectively identifies positive samples. We test various settings for rank loss weight \u03b1 \u2208 [0, 0.05, 0.1, 0.2, 0.5, 1, 2] and evaluate model performance on two datasets.\n\u2022 \u03b1 = 0: Utilizing only retrieval loss yields strong retrieval capability but suboptimal ranking performance.\n\u2022 \u03b1 = 0.2: Achieves significant ranking accuracy improvements up to 6% and 20% on the two datasets respectively, while maintaining good retrieval coverage.\n\u2022 \u03b1 > 0.2: Further increasing the weight continues to enhance ranking accuracy but adversely affects retrieval coverage, leading to an overall decline in performance.\nSpecially, the pronounced fluctuations in Recall@10 compared to NDCG can be attributed to the rank accuracy enhancement, which drives the model to prioritize recommending items that are of top potential interest to the users. This makes Recall@10 a more precise measure of top-ranking results. In contrast, other recall metrics do not exhibit similar fluctuations with NDCG. When the rank loss weight is below 1, the recall performance remains relatively stable. However, when the weight exceeds 1, the overall performance in retrieval tasks deteriorates. These results underscore the importance of balancing rank loss and retrieval loss for optimal model performance. A moderate rank loss weight, such as 0.2 or 0.5, appears to offer the best trade-off, ensuring high ranking accuracy without compromising retrieval efficacy."}, {"title": "5 Power-law Scaling Law of SessionRec", "content": "We examined the scalability of SessionRec with HSTU backbone using a large-scale industrial dataset from the Meituan App, which contains comprehensive long-term user behavior sequences for approximately 400 million users. The results are illustrated in Figure 5, where the horizontal axis denotes the training data size, quantified by the number of items in the training sequence. The vertical axis indicates the performance metric Recall@500, serving as the main offline evaluation indicator for retrieval. To prevent label leakage, user behavior sequences are ordered based on timestamps, employing the time span [1, T] as the training window and the subsequent period T + 1 as the evaluation window.\nIt is evident from Figure 5 that with the exponential increase in training data volume and computing power, the performance of the model shows a trend of linear increase, indicating that the next session prediction paradigm still maintains great scalability, similar to those observed in LLMs."}, {"title": "6 Online Deployment and A/B Test", "content": "Apart from offline experiments, SessionRec is also targeted at and successfully applied in real-world industrial practices. We adopt SessionRec with the backbone of HSTU in the online homepage recommendation system of Meituan App, which is one of the top-tier Apps in China and has more than 100 million active users per day. SessionRec is deployed in the retrieval stage, replacing the online SASRec model. As shown in Figure 6, the input sequence of user behavior provided to the model consists of two parts: the offline sequence, derived from the system logs with the Spark tool, encapsulates user behaviors prior to the current day (T - 1 behaviors); and the real-time behavior stream that is captured using a near-line Flink computing engine. These two parts are combined by the online feature processing engine upon a user's request, forming the input sequence for the model. The maximum length of the input sequence is set to 8K. The model server takes the input sequence to generate user embedding, which represents the user interest representation. Due to the large candidate pool and relatively stable features, T-1 offline computation is used to store item embeddings into the index database. The final topK retrieval results are obtained via efficient ANN algorithms (e.g. Faiss [14]) and then transmitted to the recommendation system's downstream components.\nWe conducted A/B test for seven consecutive days, revealing that the deployment of SessionRec achieves significant improvements in both the quantity and efficiency of payments, with a 0.603% increase in Page Views for Payment (Pay PV) and a 0. 564% increase in Page View Click-Through Conversion Rate (PVCTCVR). For Meituan, a life services platform, these metrics are the most important indicators for online observation. This demonstrates the practical value of SessionRec in industrial scenarios, as it effectively extracts user interests using historical behavior sequences, thereby boosting business metrics. We are continuously optimizing the real-time capabilities of the online server and will provide updates on the performance of the online deployment shortly."}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we propose a novel generation paradigm called SessionRec that redefines the generative sequential recommendation as \"next-session prediction\", diverging from the conventional \"next-item prediction\". SessionRec more closely aligns with how users engage with real-world recommendation systems and stands out as a simple-yet-effective, plug-and-play methodology that makes it highly adaptable to any sequence modeling backbone. Furthermore, we propose a ranking loss that can significantly enhance the ranking performance of generative sequence recommendation models while simultaneously maintaining the retrieval performance. Extensive experiments conducted on public datasets and online A/B tests demonstrate the effectiveness of the proposed approach. For future work, we will explore whether a single generative sequence recommendation models, enhanced for improved ranking performance, can replace the multi-model cascaded paradigm of retrieval, pre-ranking, and ranking in traditional recommendation systems, and we believe this is a direction with considerable potential."}]}