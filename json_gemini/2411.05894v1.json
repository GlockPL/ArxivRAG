{"title": "SSSD: SIMPLY-SCALABLE SPECULATIVE DECODING", "authors": ["Michele Marzollo", "Jiawei Zhuang", "Niklas Roemer", "Lorenz K. M\u00fcller", "Lukas Cavigelli"], "abstract": "Over the past year, Speculative Decoding has gained popularity as a technique for accelerating Large Language Model inference. While several methods have been introduced, most struggle to deliver satisfactory performance at batch sizes typical for data centers (\u2265 8) and often involve significant deployment complexities. In this work, we offer a theoretical explanation of how Speculative Decoding can be effectively utilized with larger batch sizes. We also introduce a method that integrates seamlessly into existing systems without additional training or the complexity of deploying a small LLM. In a continuous batching setting, we achieve a 4x increase in throughput without any latency impact for short context generation, and a 1.7-2x improvement in both latency and throughput for longer contexts.", "sections": [{"title": "1 INTRODUCTION", "content": "In the last few years, Large Language Models (LLMs) have become ubiquitous and are used to solve a wide variety of tasks. While recent work has been done in building smaller models (e.g., Llama 3.2), these models are mainly meant for specific applications, such as inference on edge devices, and the best models out there have tens to hundreds of billions of parameters. Running these models comes with high memory and compute requirements; the high cost and shortage of specialized hardware pose barriers to broader adoption of these advanced models. Even when sufficient hardware is available, latency between a user's request and the model's response remains a significant challenge for delivering a seamless user experience.\nIt is worth digging further into this process to understand what makes LLM inference so expensive (for a detailed analysis, see (Yuan et al., 2024)). LLM inference is divided into two main phases: (1) the prefill phase, where the input sequence is processed in parallel to compute and store the KV-cache and generate the first token, and (2) the auto-regressive decoding phase, where the model iteratively generates one new token at a time. While the prefill phase is parallel across the sequence and resembles the training process in terms of hardware utilization, the auto-regressive decoding phase is inherently sequential and heavily memory-bandwidth bound: The time spent loading model weights and the KV-cache substantially exceeds the time required for computing the large vector-matrix multiplications (which in training and prefill are matrix-matrix multiplications, because multiple new query vectors are available simultaneously), leading to a severe under-utilization of the available FLOPS.\nSpeculative Decoding (SD) (Xia et al., 2023; Leviathan et al., 2023; Chen et al., 2023) was introduced to use the idling compute units in the decoding phase to verify multiple possible token continuations in parallel to generate more than one token per forward pass and therefore reduce latency. Following the introduction of this idea, many variants of SD have been proposed, mainly to improve latency reduction. In large-scale LLM deployment, though, the goal is often to reduce cost per token while keeping latency within some acceptable range. While many SD methods exist, most show limited benefits at reasonable batch sizes and come with several other disadvantages. We will investigate this in more detail in Section 2. While batching competes for FLOPS with SD, making SD increasingly less convenient at larger batch sizes, it is often limited by the memory requirements of the KV-cache, and comes with higher latency.\nRecent trends in LLM applications increasingly push toward the need for efficient SD. In Retrieval Augmented Generation (RAG) (Lewis et al., 2020), the context is often in the order of tens of thousands of tokens. As in the forward pass the matrix multiplications of the feed-forward layers are independent of the context size, the attention mechanism, with the loading of the KV-cache, becomes the limiting factor. Here, SD is even more critical due to the increased cost of each forward pass. In this context, due to the large size of the KV-cache, the batch size is limited by the hardware's available memory, leaving additional FLOPs available for SD.\nThe latest OpenAI model, GPT4-01 (OpenAI, 2024), the"}, {"title": null, "content": "leading model on complex reasoning tasks, is based on a chain-of-thought technique (Wei et al., 2024). This method makes models generate more tokens to support the thought process, making the decoding phase even more critical in the overall inference workflow. Users of this model can readily appreciate that optimizing for latency, not just throughput, is crucial for practical usability.\nIn this paper, we propose a SD method that was designed with the following objectives in mind:\n1. Easy deployment without requiring additional infrastructure to run a small speculation model.\n2. No need for additional smaller LLMs or additional trainable model parameters.\n3. Very low cost, yet high-quality speculation to enable large batch size.\n4. Effective in all use cases.\nIn the following section, we will discuss how the main alternative methods fail to achieve these features. Then, we will introduce our method and show how it works in real-world scenarios."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Since the introduction of SD, countless speculative decoding methods have been proposed. Discussing all these methods is beyond the scope of this paper. For a comprehensive survey, please refer to (Xia et al., 2024). We will give an idea of the main trends for SD, and provide a more detailed description of the alternative parameter-free methods."}, {"title": "2.1 Main Speculative Decoding Methods", "content": "The standard approach to SD (Leviathan et al., 2023; Chen et al., 2023) consists of deploying an additional model for drafting (the draft model) and the original model (the target model) to verify the tokens proposed by the draft model. The draft model, which is typically 15-20x smaller than the target model, generates multiple tokens iteratively at a lower cost than running a single auto-regressive step with the target model. The verification step, instead, takes approximately the same time as the auto-regressive step while we are memory-bandwidth bound. Notably, the draft model requires separate training. Many variations of this method have been proposed. (Spector & Re, 2023) introduced tree-based verification, where multiple possible drafts are verified simultaneously, and staged speculation, where a smaller model speculatively decodes the draft model. To better align the proposals of the draft model with the outputs of the target model, it is possible to apply Knowledge Distillation (Zhou et al., 2024), or to train the draft model on data generated by the target model."}, {"title": null, "content": "The techniques described present challenges, particularly the difficulty of finding or training a draft model that aligns with the target LLM, as well as the added complexity and increased computational cost, especially in distributed inference, when deploying the smaller model. To address some of these difficulties, Medusa (Cai et al., 2024) proposed substituting the draft model with additional heads in the last layer of the target model for parallel generation of the candidates. This method reduces the computational overhead of speculation and simplifies distributed deployment. Other methods have been expanded on this idea, for example, by moving the additional heads to the second-to-last transformer layer (Li et al., 2024b;c)."}, {"title": "2.2 Parameter-free Methods", "content": "The methods described in the previous section share the requirement of training a model or part of one. Each one comes with additional complexities, such as deployment, knowledge distillation, and modification of the model architecture to add additional heads. In (Zhang et al., 2023) the authors use the target model as the small draft model, by skipping some intermediate layers. The verification is then performed with the complete model. The drafting phase, though, still takes a significant share of the generation time. Lookahead Decoding (Fu et al., 2024) combines the lookahead and verification into a single inference pass. While some of the additional compute budget is used to verify the draft candidates, a consistent part is allocated to generating possible n-gram continuations that might occur later in the sequence and which are cached for the next steps. The apparent limitation of this method lies in the resource contention of the two phases, which run simultaneously, limiting the budget for speculation. Consequentially, the method is applicable only in specific settings (low batch size) or for highly bandwidth-limited hardware.\nOther methods follow the idea of matching small n-grams to find likely token continuations without using the model to generate candidate n-grams. Prompt Lookup Decoding (PTL) (Saxena, 2023) searches for matches of the last few tokens of the input to the LLM in the prompt, and if it finds a match, it uses the continuation tokens in the prompt as candidates for the verification phase. LLMA (Yang et al., 2023) proposes a similar idea, showing good speedups on RAG and multi-turn conversations. The advantages of these methods are limited to specific contexts. REST (He et al., 2023) uses a large datastore of text to find statistical estimations of possible continuations of n-grams. A similar, even simpler n-gram model is used in (Ou et al., 2024). Finally, PIA (Zhao et al., 2024b) uses the prompt, like in Prompt Lookup Decoding, to retrieve some candidates. At the same time, it keeps a pool of sequences from all the outputs generated in the system to guess tokens that often occur across different prompts. The shared pool of candidates is pruned"}, {"title": null, "content": "regularly to keep the retrieval time reasonable and to hold only the highly recurring sequences.\nAll these methods share the advantage of easy deployment: The retrieval phase runs entirely on CPUs before the model's forward pass and does not require any model adaptations. The drawback is that the simplicity of the candidates' retrieval causes a lower acceptance rate of the draft tokens. At the same time, although simple in terms of deployment, the retrieval overhead is not negligible and adversely offsets the gains coming from the correct guesses, especially at larger batch sizes."}, {"title": "2.3 Hybrid Methods", "content": "ReDrafter (Zhang et al., 2024) uses a hybrid approach, with an RNN as the draft model, which takes as input the output of the last layer of the target model, similar to Medusa, also applying Knowledge Distillation. The method shows higher acceptance rates than most other methods, and it is one of the few methods to show good speedups even at large batch sizes. TriForce (Sun et al., 2024) targets long-context inference and uses a cascade approach: A small model provides candidates for the target model, which uses a subset of the KV-cache to generate high-quality candidates. The tokens generated are verified by the same model with the full KV-cache. Ouroboros (Zhao et al., 2024a) combines SD with a draft model with the utilization of discarded continuations, similar to Lookahead Decoding, under the assumption that the compute units are underutilized during the verification phase."}, {"title": "2.4 Large-scale Speculative Decoding", "content": "Most research on SD focuses on methods to accelerate LLM inference in the single batch case since, as already discussed, batching limits the resources for speculation (Su et al., 2023). At the same time, LLM inference is extremely expensive, and optimizing for cost is often more important than optimizing for latency. A typical LLM inference service usually runs continuous batching (Yu et al., 2022) to improve efficiency, resource utilization, and user experience. SpecInfer (Miao et al., 2024) shows good speedups with continuous batching on multi-turn conversations with batch size 16, using multiple small draft models for parallel draft generation. ProPD (Zhong et al., 2024a) improves on the idea of Medusa by pruning the candidates generated by the decoding heads, which are less likely to be accepted, and also shows speedups up to batch size 16. The already mentioned ReDrafter (Zhang et al., 2024) also shows good speedups with continuous batching and tests much larger batch sizes, up to 80. MagicDec (Chen et al., 2024a) is designed for large-batch and large-context inference, utilizing a sparse KV-cache to enable speculative execution, resulting in significant speedups for batch sizes up to 128"}, {"title": null, "content": "and context lengths of several thousand tokens in distributed inference environments. However, it is not applicable to shorter context lengths. Finally, (Liu et al., 2024) proposes some optimizations to adapt the speculation length in continuous batching given the workload. The paper shows that standard autoregressive decoding is better than speculative decoding already at moderate request rates when a draft model is deployed because of the overheads associated with this model. On the contrary, they show good speedups at higher request rates with Prompt Lookup Decoding for tasks where this method is effective."}, {"title": "3 METHOD", "content": "Several critical aspects must be carefully addressed to design an efficient and scalable Speculative Decoding method. The first, on which most of the literature focuses, is ensuring high-quality predictions. Second, standard methods incur very high overheads, even putting aside the complexity of model training and deployment. When the draft model is not small enough, a large part of the inference process remains memory bandwidth-bound, and running the draft model can take the majority of time (Li et al., 2024a). In general, the cost of drafting diminishes the gains due to correct guesses (Liu et al., 2024). Third, even during the verification phase, in the typical Speculative Decoding zone of the accelerator's roofline model (Williams et al., 2009), most current libraries suffer from a notable under-utilization of hardware resources."}, {"title": "3.1 Algorithm Design", "content": "The first objective of this paper is to eliminate the complexity and costs associated with running the drafting phase on the device. While the benefits of bypassing the need to identify, modify, train, and deploy a model are evident, the main limitation of existing parameter-free solutions is their reduced speculation quality. The methods reviewed in Section 2.2 employ simple n-gram models, where the n-gram sources generally fall into two categories: the prompt (which may include the output already generated by the LLM, hereafter referred to as \"self-output\"), and a statistical n-gram model derived from a large text corpus. The most successful methods, REST and PIA, have inspired our method and serve as benchmarks for our comparisons.\nThe central idea of our method is to treat the prompt and self-output as a single source of candidate tokens and to integrate this source with a large, fixed text datastore. We show with experiments that the candidates selected from these two sources are complementary and that smartly optimizing and merging them boosts the quality of the draft tokens. We introduce several optimizations and use a highly efficient, parallelized implementation for the drafting method, which makes retrieval times negligible compared to the cost of"}, {"title": "3.1.1 Prompt and self-output", "content": "The prompt and self-output are treated as a single input sequence (we will refer to it as \u201cinput\u201d), as the self-output is highly valuable for retrieving candidates. A simple tree data structure, updated continuously with the newly generated tokens, stores this input content to enable efficient retrieval. When searching for continuations for a prefix of size P (we use P = 4), we construct a distinct candidate tree for each prefix of size 1 to P. The weights of the branches in the trees are the number of occurrences of the corresponding sequence in the input."}, {"title": "3.1.2 Datastore", "content": "The large datastore is built the same way as in REST (He et al., 2023), using a suffix array of tokenized text to efficiently search for continuations of a given prefix with a binary search over the suffixes. Once the continuations are found, a weighted tree of possible continuations, with weights representing the number of occurrences of the continuation, is built to aggregate the information. Besides the integration with prompt and self-output, the main differences between our method and REST are the following: First, in REST, all the continuations are retrieved for a given prefix, unless the number of continuations is very high, in which case only the first N (by default 5000) continuations are kept. In our implementation, we pick at most M << N"}, {"title": null, "content": "continuations (usually 100 continuations are enough for no loss in speculation quality) at regular intervals. Since the suffix array is sorted, sampling at regular intervals approximates the original distribution, unlike selecting only the initial continuations. This method improves speculation quality while significantly reducing cost. Second, we keep the search cost low even at large batch sizes by parallelizing the full process over the batch size dimension using OpenMP (REST was only implemented for batch size 1). Third, in REST, if a prefix is not found, a shorter prefix is searched in the suffix array. The candidates of the first matched prefix are used for verification. In our approach, we instead check if the number of continuations falls below a set threshold and continue searching shorter prefixes until this number is met. The continuations are then merged by summing the occurrences along the tree edges. These, with other optimizations coming from complete re-implementation of the search mechanism, give higher quality at a much lower prediction cost.\nWe note that also PIA attempts to combine information from the prompt with \u201cgeneral language information\", created and continuously updated during inference with the LLM outputs. PIA maintains data within a tree structure, regularly pruning less frequently accessed branches to control the tree size and manage retrieval times. However, this pruning substantially lowers candidate quality. Predicting continuations for rare token sequences can often be easier than for common phrases, and the combined set of rare sequences makes up a significant portion of searchable prefixes. Our investigation into various offline pruning methods for the large data store consistently resulted in lower acceptance rates."}, {"title": "3.1.3 Fusing multiple data sources", "content": "After retrieving data from the different sources, they must be combined efficiently to quickly generate high-quality candidate tokens. Each source provides an estimate of the likelihood of acceptance, which we call the \"probability\" of continuation. This pseudo-probability is calculated by dividing the number of occurrences of the complete sequence (from the prefix start to the candidate token) by the number of occurrences of the prefix alone. Looking at the probability value as the expectation of the token being accepted in the verification phase, the quality of the estimate differs between tokens from the datastore and those from the input, as input-based estimates tend to be overconfident; when a prefix is matched in the input, the possible continuations are often limited. Therefore, a scaling factor is applied to tokens coming from the input. Moreover, even within the input, the probabilities are dampened according to the size of the prefix match and the depth of the specific candidate in the tree. Good weights can be identified through a simple grid search. This process requires minimal device usage to initially generate ground truth data with the LLM. The data is then used to simulate the LLM inference process and find good parameters. To further refine probability predictions, estimations from each source can be calibrated on the generated text. Once calibration parameters are established, probability estimates are adjusted according to the data source, depth, and prefix match length."}, {"title": null, "content": "Algorithm 1 provides the pseudo-code for merging data from multiple sources, resulting in a tree of high-quality token continuations. From this, we construct the vector of candidates for verification and the attention mask."}, {"title": "3.2 Leveraging Hardware for Maximum Performance", "content": "The last aspect to optimize is the device overhead from the verification phase (specifically, the LLM forward pass). While many Speculative Decoding studies note the under-utilization of accelerator resources as a key enabler of SD, few offer an in-depth analysis of how memory bandwidth and compute resources interact with SD, especially under batching. This limited analysis is likely because the substantial FLOP surplus when the batch size is 1 reduces the need for thorough optimization to achieve good speedups. With larger batch sizes, most studies suggest that SD speedup is constrained by resource competition between batching and speculation. Some papers (Su et al., 2023; Zhong et al., 2024a) explore the interaction between speculation and batching, observing a linear relationship between forward pass overhead and speculation length, with steeper slopes at higher batch sizes. We define the speculation length sq as the number of nodes in the speculation tree including the root, i.e., 1 (the latest accepted token) plus the number of candidate tokens to verify."}, {"title": null, "content": "Algorithm 1 The algorithm to merge candidate tokens coming from different sources. The methods to retrieve the candidates (get_conts) and to compute the dampening factors (discount) are described in sections 3.1.2, 3.1.1, 3.1.3. The method insert on the tree takes a token ID and a parent node, adds a child to the parent if it does not exist yet, and returns the child node.\nInput: prefix (vector of integers), dec-len, branch_len\nOutput: draft_tree (the merged tokens)\ndatastore_tree \u2190 datastore.get_conts(\n prefix, declen, branch_len)\ninput_trees \u2190 input_cache.get_conts(prefix)\n{In the priority queue, each element consists of a node pointer to the node in the source tree, a probability (which gives the priority), the depth in the tree, and a pointer to the parent node in the draft tree (parent)}\npriority_queue \u2190\u2190\u2205\ndraft_tree \u2190 Tree(root_token_id = prefix[-1])\nfor each tree \u2208 [datastore_tree] + input_trees do\n if tree \u2260 \u2205 then\n node \u2190 tree.root\n depth \u2190 1\n for each child \u2208 node.children do\n prob\u2190 node.prob \u00d7 discount(source, depth)\n priority_queue.insert(child, prob, depth,\n draft_tree.root)\n end for\n end if\nend for\nwhile priority_queue \u2260 \u2205 and\n draft_tree.size() < dec-len do\n element \u2190 priority_queue.pop()\n new_candidate \u2190 draft_tree.insert(\n element.node.token_id, element.parent)\n depth \u2190 element.depth + 1\n for each child \u2208 element.node.children do\n prob \u2190 child.prob \u00d7 discount(source, depth)\n priority_queue.insert(child, prob,\n depth, new_candidate)\n end for\nend while\nreturn draft_tree\nOur findings indicate this empirical observation does not align with theoretical predictions based on hardware specifications, primarily due to unoptimized operator implementations. Sequoia (Chen et al., 2024b) proposes a method to estimate the optimal speculation length based on batch size, speculation accuracy, and empirical hardware data, though it also overlooks hardware optimizations. Medusa (Cai et al., 2024) offers an in-depth view of speculation effects within the roofline model. We aim to build further on these insights. Understanding these interactions in detail is crucial for de-"}, {"title": "3.2.1 Impact of the key variables in the decoding phase", "content": "Table 1 provides an analysis of primary device operations during a decoding step, drawing from (Chen, 2023). The equations are mainly based on the Llama family (Touvron et al., 2023; Dubey et al., 2024), but most dense models have similar architectures (e.g. Qwen-2, Gemma2, Phi-3, Yi-34B, Mistral-Large2). We integrate the speculation length (sq) into FLOP and memory usage formulas for different transformer model components (excluding operators with limited impact, such as normalization, positional encodings, and activation functions). When sq = 1, the forward pass matches standard autoregressive decoding.\nIncreasing the batch size b nearly proportionally increases the compute-to-memory access ratio for all terms except FlashAttention. Since memory accesses for matrix multiplications only increases slightly with b, the batch size can be scaled up with minimal extra costs until reaching the compute-bound region of the roofline model. Similarly, increasing the speculation length sq almost proportionally"}, {"title": null, "content": "for the computation of the Q matrix,\nthe speculation budget can be derived by solving\n$\\frac{1}{2 \\times (1/h + 1/2bsq)} = \\frac{Peak TFLOPS}{Memory Bandwidth (TB/s)}$.\nNote that the denominator of the FLOPS: IO ratio is doubled from the table, assuming each parameter occupies two bytes.\nIf we consider the Llama2-7b (Touvron et al., 2023) model (h = 4096) and an Ascend 910B4 device, with a computing power of 280 TFLOP/s and a bandwidth of 0.8 TB/s, the (almost) free speculation budget can be theoretically obtained until bsq \u2248 435. This limit (sq = 54) appears as a discontinuity in the slope of the green line in Figure 3 (b), where the initial segment reflects memory-bandwidth limitations, and the latter segment is determined by the available FLOPs. Figure 3 (a) shows the corresponding empirical results from unoptimized operators, which exhibit a more linear pattern, as previous studies have noted. Similar outcomes can be achieved on Nvidia GPUs using the default cuBLAS library, while optimized operators significantly improve performance in the speculative decoding region of the roofline (Osama et al., 2023). The under-optimization of these operators stems from the complexity involved in creating efficient kernels for matrix multiplications where one matrix is notably \"tall and skinny\u201d (Rivera et al., 2021).\nFinally, increasing the context length skv has the main effect"}, {"title": "3.2.2 Continuous batching", "content": "With continuous batching, commonly used during LLM deployment, the performance model becomes more complex due to the lack of a clear separation between the prefill and decoding phases. Sarathi-Serve (Agrawal et al., 2024) proposes to chunk the long sequences in the prefill phase to combine them with sequences in the decoding phase, limiting the size of the prefill sequences to keep the Time Between Tokens within acceptable rates. Here, the resource budget is limited by the resources allocated to the prefill chunks in hybrid batches.\nA more popular approach, in distributed setups, disaggregates prefill and decoding by assigning devices exclusively to one phase (Zhong et al., 2024b). This allows full allocation of resources to decoding on dedicated devices, simplifying the deployment and optimization of SD for even greater efficiency."}, {"title": "3.2.3 Additional considerations", "content": "To address limited device memory and enable the processing of larger batches or longer sequences (at the cost of increased latency), a common technique involves offload-"}, {"title": "4 RESULTS", "content": "We run our evaluations on a server equipped with an AMD EPYC 64-core processor, and we use a single Huawei Ascend 910B4 as the device. Details on the accelerator specifications are provided in (Lin et al., 2024). For the experiments, we use the Llama2-7b and Llama3-8B chat models. Many experiments use Llama2 to make comparisons with previous literature easier. For the datastore construction, we use Ultrachat (Ding et al., 2023), Magpie (Pro-MT-300K and Air-MT-300K) (Xu et al., 2024), and a dataset of conversations scraped from ShareGPT (ShareGPT, 2023). For the evaluation, we use MT-bench (Zheng et al., 2023), GSM8k (Cobbe et al., 2021), Dolly-15k (Conover et al., 2023). For very short context experiments, we use the Natural Questions dataset (Kwiatkowski et al., 2019), while for long context, we use PG-19 (Rae et al., 2019), for which we ask to provide a summary."}, {"title": "4.1 Algorithm Effectiveness", "content": "First, we show why our method is particularly effective by breaking down the different components that contribute to selecting the candidates for speculation. The method's strength lies in the complementarity of candidates coming from the input and the datastore. This can be seen in Figure 4(a), where the contribution of the two main components (the input, which includes prompt and self output, and the datastore) almost add up perfectly, meaning that the method can select from which source to pick the best candidates at each step.\nWe also compare our implementation with the main alternative parameter-free methods, both in terms of speculation quality and retrieval time (cf. Figure 4(b, c)). We use the MT-Bench dataset used in (He et al., 2023) (only the first prompt for each conversation), and the first 80 prompts from the GSM8k dataset, used in (Zhao et al., 2024b), showing that our method consistently outperforms both methods on both datasets."}, {"title": "4.2 Temperature Effects on Speculation Accuracy", "content": "While various methods have been proposed to optimize acceptance rates without altering the output distribution in the context of nucleus sampling (Leviathan et al., 2023; Chen et al., 2024b), applying these methods in our scenario is more challenging. This is because the drafter does not provide an accurate distribution across the entire vocabulary of tokens. We tried reconstructing a distribution over the full vocabulary using the probability estimates as obtained in Algorithm 1, and tested speculative sampling and Sequoia sampling (Chen et al., 2024b). Our experiments do not show consistent improvements in the acceptance rate, compared to the naive approach of greedily choosing the candidate"}, {"title": null, "content": "tokens with the highest probability, then sampling from the target model distribution, and finally checking if the sampled token is present in the candidates. We attribute this behavior to the lower quality of the probability estimates given by our approach, compared to using a small LLM for drafting: Speculative sampling gives the best advantages when the draft model distribution is close to the target model distribution (Leviathan et al., 2023). This approach is also faster in the retrieval and the forward pass outputs' processing steps, making it the best choice for SSSD.\nOur experiments show that in the temperature range between 0 and 1 (top-k=50, top-p=0.7), the acceptance rate decrease with our naive approach is almost negligible (\u22481-2%). At temperature 2, the acceptance rate decreases by around 20%. Since temperature is typically kept in the 0-1 range, we conduct all experiments with greedy decoding, but almost equivalent results can be obtained with nucleus sampling."}, {"title": "4.3 Impact of the Tokenizer", "content": "We also evaluate the effectiveness of our method using the Llama3 model, which features a significantly larger vocabulary than Llama2 (128,256 tokens versus 32,000 tokens) (Dubey et al., 2024; Touvron et al., 2023). This expanded vocabulary makes it harder for speculative methods to generate high-quality candidates for verification. Our algorithm improves speculative accuracy for models or languages with lower acceptance rates by prioritizing breadth over depth when constructing the candidate tree. Specifically, we achieve this by reducing the maximum branch length, which is adjusted based on the total speculation length, when searching in the datastore. We show in Figure 5 that, although it has a much larger vocabulary, with Llama3 the quality of the candidates remains good enough for consistent end-to-end speedups."}, {"title": "4.4 Overall Performance and Batching", "content": "We evaluate our method by integrating it into a customized LLM serving system, based on vLLM (Kwon et al., 2023), which uses continuous batching and employs more optimized operators than the ones shown in Figure 3 (a). We use the same, highly optimized system as the autoregressive baseline for comparisons. We create 3 subsets of 100 prompts of different lengths, and generate up to 500 tokens for each prompt, using different batch sizes. All the requests are scheduled as soon as the batch has free space. We measure the end-to-end time to finish generating all answers, and the total number of tokens generated (there might be some limited differences in the outputs between the baseline and the speculative method because of the precision of float16), and evaluate the overall throughput and the average latency (latency = batch size/throughput). The end-to-end results include the prefill: for this reason, the average latency is higher than the average Time Between Tokens (TBT) that we would have in a static batching case (the decoding step), as it also includes the Time to First Token (TFT). The results are shown in Figure 6. As a first result, we achieve consistent speedups (1.37-1.80x) even at a batch size of 64. This indicates that, by minimizing overheads, significant speedups can be achieved even at large batch sizes. With a longer context, it is possible to increase the speculation budget for large batch sizes, as the KV-cache loading takes the majority of the forward pass. This is why the speedups compared to the baseline are almost constant with increased batch size, differently from what happens with a short context, where the speculation length must be decreased more rapidly to avoid entering the high-slope region in Figure 3. This behavior is evident even though the increased share of computation spent on prefill would typically reduce the gains. In a setup with separate prefill and decoding nodes, speculation becomes even more advantageous than in scenarios with a short context.\nTo better understand the throughput-latency tradeoff, we can see that in the short context case (cf. Figure 6 (left), we can scale the batch size while keeping the latency constrained: in an application where latency is a key objective, with standard autoregressive decoding the optimal batch size would be 8 or 16. In our case, with a negligible latency increase, we can get 4 times the throughput by scaling the batch size to 32, compared to batch size 8. This means that we need 4 times less hardware to run the same model under the same workload. On the other hand, keeping cost fixed, we can halve the latency by halving the batch size (going from 32 to 16, or 16 to 8).\nAlso with a longer context, where a consistent amount of compute is taken by the prefill (Figure 6 on the right) our method can achieve a 2 times decrease in cost at the same latency, by scaling from batch size 4 to batch size 8. Moreover, in the longer context case, scale-out is required at lower request rates because of the limited memory capacity. In this case, Speculative Decoding is the only way to increase throughput and limit the scale-out cost."}, {"title": null, "content": "SSSD largely outperforms autoregressive decoding under all settings. No choice needs to be made between batching and speculating: SD gives the best throughput at any latency requirement, including when latency is not a variable to optimize. If minimizing latency is the main objective (if enough hardware resources are available), we show that our method gives noticeable speedups at batch size 1 (2.05-2.61x) on different tasks and with different context lengths (note that the speculation length is limited to 30 by the system, and without this constraint even higher speedups could be achieved). Figure 7 shows that similar gains can be obtained also with Llama3-8B. The plot also shows the behavior of PIA, which, as most SD methods, shows good speed"}]}