{"title": "Redefining Data Pairing for Motion Retargeting\nLeveraging a Human Body Prior", "authors": ["Xiyana Figuera", "Soogeun Park", "Hyemin Ahn"], "abstract": "We propose MR.HuBo (Motion Retargeting lever-\naging a Human Body prior), a cost-effective and con-\nvenient method to collect high-quality upper body paired\n(robot, human) pose data, which is essential for data-driven\nmotion retargeting methods. Unlike existing approaches which\ncollect (robot, human) pose data by converting human MoCap\nposes into robot poses, our method goes in reverse. We first\nsample diverse random robot poses, and then convert them into\nhuman poses. However, since random robot poses can result in\nextreme and infeasible human poses, we propose an additional\ntechnique to sort out extreme poses by exploiting a human\nbody prior trained from a large amount of human pose data.\nOur data collection method can be used for any humanoid\nrobots, if one designs or optimizes the system's hyperparameters\nwhich include a size scale factor and the joint angle ranges for\nsampling. In addition to this data collection method, we also\npresent a two-stage motion retargeting neural network that can\nbe trained via supervised learning on a large amount of paired\ndata. Compared to other learning-based methods trained via\nunsupervised learning, we found that our deep neural network\ntrained with ample high-quality paired data achieved notable\nperformance. Our experiments also show that our data filtering\nmethod yields better retargeting results than training the model\nwith raw and noisy data. Our code and video results are avail-\nable on https://sites.google.com/view/mr-hubo/.", "sections": [{"title": "I. INTRODUCTION", "content": "Motion retargeting, a fundamental task in computer an-\nimation and robotics, aims at finding a mapping between\nmotions across different domains. This field has garnered\nconsiderable interest due to its wide-ranging applications, in-\ncluding avatar control in virtual environments, motion-related\ncomputer graphics tasks, and tele-operation in robotics.\nWith the pervasive integration of deep learning (DL) into\nrobotics and computer vision, a myriad of studies have\naddressed this task employing DL methodologies [1]\u2013[5].\nHowever, the application of these in motion retargeting for\nrobotics, has been largely restricted to employing semi-\nsupervised or unsupervised learning techniques. The current\nrestriction stems from the challenges inherent in collecting\npaired (robot, human) pose data. There exist three main\nhindrances in paired dataset collection: (1) the difficulty in\ndefining ground truth robot motion for input human motion,\n(2) the limitation in the variety of poses that arises from the\ndependence on human motion data, and (3) the necessity to\nlabel this data for each robot hardware platform, as robots\npossess unique joint configuration space.\nRecognizing these obstacles, our work centers on en-\nabling computationally efficient supervised learning in mo-\ntion retargeting, by proposing an automatic method for\npaired (robot, human) pose data collection. We implement\na paradigm shift similar to the one in [4], wherein randomly\nsampled robot poses are converted into human poses with\nstrategies to avoid extreme poses. Our method is versatile,\nand applicable to any humanoid robot, as long as the hyper-\nparameters related to kinematic chain, link lengths, and size\nscale factor can be manually designed or optimized.\nTo be more specific, we randomly sample robot poses from\nthe valid joint angle range and convert them into human\nposes. However, this randomness can result in a human pose\nthat is physically implausible or extreme (see Fig 1-(b)),\nwhich can hamper the training of the motion retargeting\nmodel by introducing noise. To compensate for this, we\npropose a strategy to obtain natural and feasible human poses\nwith minimal or even no reliance on glue data (i.e., paired\nsamples obtained from classic optimization-based methods),\nwhich [4] necessarily requires. To achieve this, we employ\na human body prior (VPoser) [6] which is a variational\nautoencoder that has learned a prior of SMPL [7] human\nbody pose parameters from massive human pose data [8]. We\nfound out VPoser can be used (1) as an inverse kinematics\n(IK) solver which can find SMPL pose parameters from robot\njoints' XYZ position, and (2) as a denoising autoencoder,\nwhich converts implausible and noisy poses into valid ones."}, {"title": "II. RELATED WORK", "content": "A wide range of existing literature addresses the task of\nmotion retargeting by defining it as an optimization problem\nwith constraints such as joint limits, usually to solve an\ninverse kinematics (IK) problem [11], [12]. In this section,\ninspired by [13], we categorize the optimization-based mo-\ntion retargeting literature, by the core system used to capture\nhuman motion, into Marker-systems, Sensor-systems, and\nCamera-systems.\nMarker-systems. Attaching markers to the human body\nwas a commonly used system to capture human motion in\nmotion retargeting literature. For instance, Riley et al. [14]\nachieved real-time motion retargeting facilitated by simple\nvisual markers (i.e. color patches) placed on the upper body.\nThey tracked movements with 3D vision and incorporated\na human kinematics model, solving the motion retargeting\ntask through IK. Physical markers attached to the body were\nalso used by Ott et al. [15]. In their work, the authors\nproposed a Cartesian control approach by substituting IK\nwith a Hidden Markov Model (HMM), and optimizing mo-\ntion primitives-parameters of the HMM-via expectation\nmaximization for controlling the upper body joints. For lower\nbody joints, a center of gravity-based balancing controller via\noptimization was proposed.\nSensor-systems Recent works employ modern devices\nsuch as Xsens MVN, which is a motion capture system\nthat captures human motion through inertial and magnetic\nsensors [16], [17]. Koenemann et al. [18] proposed a system\nthat employs a compact human model obtained from such\nmodern devices. They solved IK while considering the center\nof mass of the human to find statically stable configurations.\nTo enhance the scalability of motion retargeting methods (i.e.\nretargeting motion to different robots from different human\nsubjects), Darvish et al. [19] proposed a method for whole-\nbody retargeting that involves mapping anthropomorphic mo-\ntions of human links to corresponding robot links through IK,\nensuring the preservation of geometric relationships between\nthem.\nCamera-systems Many types of cameras have been\nadopted for motion retargeting, such as time of flight (ToF)\nimaging devices [11], stereo cameras [20] and RGB cameras\n[21]. Dariush et al. [11] addressed the task of online motion\nretargeting via a Cartesian space control theoretic approach\nwith the use of low-dimensional human task descriptors\nfrom depth image sequences. Focusing on pose estimation\nfrom uncalibrated videos, Khalil et al. [21] proposed a\nmotion retargeting method with input from a single-view\nRGB camera. The method consists of three modules that are\ndesigned for 2D coordinate extraction, depth estimation, and\nhuman joint angle computation. Yet, this method is limited\nto humanoid robots that have a close number of DoFs to\nthose of humans.\nOptimization-based retargeting approaches are also being\nused to collect (robot, human) pose paired data for data-\ndriven methods by harnessing human MoCap data [3], [4].\nHowever, optimization-based methods are usually not com-\nputationally efficient [22]. Our data pairing method, though\noptimization-based, overcomes this limitation by employing\nVPoser [6] which optimizes a low dimensional latent vector\ninstead of full SMPL pose parameters when obtaining the\npaired dataset.\nWhile our retargeting method is not optimization-based,\nwe opted for a camera system among the various systems in\nthe aforementioned works. This choice allows us to harness\na regression-based human mesh recovery network [10] at the\ninference stage whose output is SMPL pose parameters."}, {"title": "B. Data-driven Motion Retargeting", "content": "In recent years, data-driven motion retargeting has\nemerged as a prominent research area not only in robotics\nbut also in computer vision, intending to achieve natural\nand scalable motion adaptation across various source and\ntarget subjects [4], [13]. One of the advantages of data-driven\nmethods is that, at the execution phase (i.e. inference phase),\nthe computation can be forward-propagated in a single time\nwhich leads to faster computation than iterative IK-based\nmethods [4]. However, the expensive costs of collecting\n(robot, human) paired datasets have limited the research\nto employing unsupervised or semi-supervised methods [1],\n[2]. To circumvent the need for paired data, Villegas et\nal. [1] proposed a recurrent neural network architecture\nthat leverages cycle consistency and human MoCap data\nfor computer animation motion retargeting. However, their\nmethod is limited to cases where the kinematic structure only\ndiffers in terms of bone lengths and proportions (i.e. same\ntopology). To enable motion retargeting of different skeleton\ntopologies, [23] proposed a human-to-robot unsupervised\nmotion retargeting method that constructs a shared latent\nspace through adaptive contrastive learning and incorporates\na consistency term. Our approach, however, examines fully\nsupervised learning with a large and automatically collected\npaired dataset.\nTo automatically collect a paired dataset, Choi et al. [4]\nproposed a robot, human) pose pairing method that lever-\nages the robot joint's movement information. Their method\nconsists of randomly sampling robot poses from joint angles,\nmore specifically from beyond the min-max range of the\njoint angles. Then, a nonparametric optimization method [3]\nin the latent space is used to obtain feasible robot poses.\nThis nonparametric optimization method involves combining\nnonparametric regression and deep latent variable modeling\ntechniques. The feasible robot pose is obtained by feeding\nthe randomly sampled pose data to the domain-specific\nencoder and then decoding the largest feasible pose found by\nemploying locally weighted regression on the latent space.\nAfter that, joint angles are transformed into a human pose\nusing forward kinematics. Their method offers an advantage\nin terms of pose diversity by not limiting it to human MoCap\ndata. This is achieved by avoiding the use of human MoCap\ndata poses and instead employing randomly sampled robot\nposes when generating new pairs of poses. However, their\nmethod relies on K-nearest neighbors search at the inference\nstage which makes it computationally inefficient [3]. Also, it\nstill inevitably requires paired data samples for the nonpara-\nmetric optimization collected by using expensive IK solvers.\nAnother crucial drawback is that randomly sampling from\nrobot joints can result in infeasible or extreme human poses\n(i.e. noisy poses) which lower the quality of the obtained\ndataset (Fig. 1-(b)).\nOur method differs from these works because we enable\nfully supervised motion retargeting through the efficient\ncollection of a large (robot, human) paired dataset. When\ncollecting our data, a requirement is finding the hyperparam-"}, {"title": "III. METHODOLOGY", "content": "Let $M = [q_1 \\cdots q_T] \\in \\mathbb{R}^{T\\times n}$ denote an upper body\nrobot motion, which is a sequence of joint angle vectors $q_t$.\nHere, $q_t = [q_{t,1} \\cdots q_{t,n}] \\in \\mathbb{R}^n$ is an $n$ dimensional\nvector, where $n$ denotes the number of joints. We describe\nthe orientation of robot links using a 6D representation\n(i.e. a continuous representation of 3D rotations) [9] as\n$R_t \\in \\mathbb{R}^{6\\times m}$, where $m$ is the number of robot links. We\nemploy SMPL [7] pose parameters to represent a human\npose, denoted as $H_t \\in \\mathbb{R}^{6\\times k}$, where $k$ is the number of\njoints in the SMPL body model. While it is conventional\nto represent SMPL pose parameters with the axis-angle\nrepresentation, we convert them into the 6D representation to\nensure consistency with the representation used to describe\nthe robot pose 3D orientation.\nWe present an end-to-end supervised learning framework\nwith neural networks that learn a mapping from $H_t$ to\n$q_t$. To accomplish this task with supervised learning, it is\ninevitable to collect a paired (robot, human) pose training\ndataset. Such datasets are usually obtained by solving inverse\nkinematics from human MoCap data to robot motion data\n[3], which is computationally expensive. Yet, we bypass\nthe need to use human MoCap data as direct input and\nexpensive inverse kinematics solvers during paired data col-\nlection. Our solution lies in an effective paired data collection\nmethod that deviates from traditional methods and obtains\n$H_t$ from $q_t$. Our method allows the cost-efficient collection\nof large amounts of diverse and high-quality human poses\n(i.e. remove infeasible and extreme human poses), which we\nachieve by exploiting a human body prior (VPoser) [6].\nIn addition to our data pairing method, we also propose\na two-stage network that maps $H_t$ to $q_t$ by leveraging a\ncontinuous representation of 3D orientations (i.e. the 6D\nrepresentation) [9]. Our two-stage network consists of a pre-\nnetwork $F_{pre}$ and a post-network $F_{post}$. In the first stage,\n$F_{pre}$ learns a mapping from $H_t$ to $R_t$ (Eq. (1)), and in the\nsecond stage, $F_{post}$ learns a mapping from the outputs of\n$F_{pre}$ to $q_t$ (Eq. (2)). At the real-world inference stage, we\nuse a regression-based human mesh recovery network [10]\nto obtain $H_t$ from images $I_t$ of a video $V = \\{I_1,\\cdots, I_T\\}$,\nwhich is a sequence of full human body images $I_t \\in\n\\mathbb{R}^{wxh\\times 3}$. We denote this process as $H_t = F_{smpl}(I_t)$.\nB. Data Pairing\nWe introduce a new robot, human) pose data pairing\nmethod for data-driven motion retargeting frameworks. In-\nspired by the concept of harnessing the robot joints' angles\ninformation for paired data collection [4], our pairing method"}, {"title": "C. Two-Stage Motion Retargeting Network", "content": "We propose a supervised learning model to learn a map-\nping from $H_t$ to $q_t$ for data-driven motion retargeting.\nConsidering that neural networks tend to learn better from a\ncontinuous representation of orientation, we propose a two-\nstage network that exploits the 6D representation of orienta-\ntion [9]. We divide the process into two distinct mappings as\nshown in Figure 3. These two separate mappings are learned\nby two distinct networks, pre-network $F_{pre}$ and post-network\n$F_{post}$, respectively. First, $F_{pre}$ learns a mapping from $H_t$ to\n$R_t$ (i.e. poses in different domains) where the orientation of\nboth $H_t$ and $R_t$ are described using the 6D representation\n[9]. In the second stage, $F_{post}$ learns a mapping from $R_t$ to\n$q_t$. The purpose of $F_{post}$ is to solve an IK problem between\nposes of the same domain (i.e. robot poses).\nBoth $F_{pre}$ and $F_{post}$ are fully connected networks with\none hidden layer. Our model employs a simple architecture\nto underscore the advantages in computational efficiency and\naccuracy that neural networks can achieve when a high-\nquality dataset is available. Designing more complex archi-\ntectures would be our future work, but simpler architectures\ncan be efficient for real-time motion retargeting.\nFor the training of the whole network, we compute and\ncombine the losses from the two networks (i.e. $F_{pre}$ and\n$F_{post}$). We define the loss for the pre-network $L_{pre}$ as the\nsummation of the mean squared error between the predicted\n$R_t$ and the ground truth $R_t$. Then for the learning of\n$F_{post}$, we use a teacher-student learning technique for which\nwe compute the post-network loss $L_{post}$, employing two\npredicted joint angle vectors: a teacher joint angle vector $q_t$\nfrom the ground truth $R_t$, and a student joint angle vector\nq_t from the predicted $R_t$. Then, $L_{post}$ is defined as the\nsummation of the mean squared error between $q_t$ and $q_t$\nand mean squared error between $q$ and $q_t$. Lastly, the total\nloss $L_{total}$ is defined as a summation of $L_{pre}$ and $L_{post}$."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we demonstrate the capabilities of our\npaired data collection method to generate high-quality\n(robot, human) pairs and the effectiveness of our proposed\ntwo-stage network for supervised motion retargeting. To\nfulfill this goal, we design three experiments: (1) a com-\nparison between the motion retargeting performance of our\nfully supervised learning-based method and the unsupervised\nlearning-based method introduced in [4] which we chose\nas our baseline (Section IV-D), (2) an ablation study to\nanalyze the effects of our extreme pose filtering component\nand our two-stage network architecture (Section IV-E) and\n(3) a real-time motion retargeting experiment in a real-world\nsetting (Section IV-F). In experiment 2, we also examine\nthe scalability of our extreme pose filtering component to\nthree robots (i.e. Reachy, Coman and Nao) with different\nkinematics and topology (i.e, number of joints and sizes). For\nComan and Nao, all experiments were conducted exclusively\nwithin a simulation environment. For Reachy, experiments\nwere carried out both in a simulated environment and in a\nreal-world setting (e.g., experiment 3).\nFor our experiments, we generated several datasets. First,\nfor the training of our two-stage neural network, we collected\ntwo million (robot, human) pose pairs employing our robot-\nto-human pose paired data collection method. We obtained\nthe data pairs for each one of the robots (i.e. Reachy, Coman\nand Nao) using each robot's description information from\nthe unified robot description format (URDF). With an RTX\nA6000 GPU, collecting two million of pairs took approxi-\nmately one and a half day for each robot. We also generated\n(robot, human) paired evaluation datasets per robot using an\noptimization-based [24] method. These evaluation datasets\nare not fed into the model during training, and are used\nas a ground truth for a fair quantitative evaluation. For this\ndataset, we selected motions from the AMASS dataset [8]\nthat have higher variance in upper body movement, with\nminimal or no movement of the lower body. This evaluation\nset is composed of the motion of 7 different subjects and\na total of 11 different motions: punch, drink soda, laugh,\nboxing, wash windows, direct traffic, hand signals, basketball\nsignals, superhero, panda, and vignettes. For training our\nbaseline [4], we collected the dataset by following the details\nin their work and also used additional glue data to learn\nthe shared latent space for the nonparametric optimization\nalgorithm they use.\nFor a more comprehensive evaluation of our method, we\nmeasure the performance using two different evaluation met-"}, {"title": "V. CONCLUSION", "content": "In this work, we propose a framework for fully supervised\nmotion retargeting, which consists of a novel (robot, human)\npose data pairing method as well as a two-stage neural\nnetwork for retargeting. Our method converts random robot\nposes into human poses using VPoser, which can find SMPL\nhuman pose parameters corresponding to given XYZ joint\npositions of a robot. However, since a randomly sampled\nrobot pose can result in an implausible or extreme human\npose, our method filters it out based on the distance between\nits denoised version, since a result of passing a pose to the\nVPoser that can work as a denoising autoencoder. Then, we"}]}