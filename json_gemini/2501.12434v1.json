{"title": "Enhancing Retrosynthesis with Conformer: A Template-Free Method", "authors": ["Jiaxi Zhuang", "Qian Zhang", "Ying Qian"], "abstract": "Retrosynthesis plays a crucial role in the fields of organic synthesis and drug development, where the goal is to identify suitable reactants that can yield a target product molecule. Although existing methods have achieved notable success, they typically overlook the 3D conformational details and internal spatial organization of molecules. This oversight makes it challenging to predict reactants that conform to genuine chemical principles, particularly when dealing with complex molecular structures, such as polycyclic and heteroaromatic compounds. In response to this challenge, we introduce a novel transformer-based, template-free approach that incorporates 3D conformer data and spatial information. Our approach includes an Atom-align Fusion module that integrates 3D positional data at the input stage, ensuring correct alignment between atom tokens and their respective 3D coordinates. Additionally, we propose a Distance-weighted Attention mechanism that refines the self-attention process, constricting the model's focus to relevant atom pairs in 3D space. Extensive experiments on the USPTO-50K dataset demonstrate that our model outperforms previous template-free methods, setting a new benchmark for the field. A case study further highlights our method's ability to predict reasonable and accurate reactants.", "sections": [{"title": "Introduction", "content": "Retrosynthesis, initially formulated by [Corey, 1991], stands as a fundamental problem within organic synthesis, aiming to discover and predict reactants given product molecules. This task presents a large chemical search space and myriad molecular combinations, posing significant challenges. Traditionally, researchers relied heavily on extensive knowledge of organic chemistry and reaction mechanisms, often resulting in inefficiency and heavy dependency on expertise. In recent years, with the significant impact of data-driven deep learning methods across various fields, computer-aided Retrosynthesis has gained increasing attention, especially the template-free methods.\nExisting methods are categorized into three types based on reliance on template prior: template-based, semi-template, and template-free. Template-based methods [Chen and Jung, 2021; Coley et al., 2017; Dai et al., 2019] view Retrosynthesis as a template retrieval task and subsequently employ the retrieved templates (specific chemical changes) to transform the input product molecule into reactants using cheminformatics tools like RDKit [Landrum and others, 2013]. Semi-template methods [Shi et al., 2020; Yan et al., 2020] follow two steps: i) Reaction Center Identification: identifying the reaction centers of the product using product-reactant aligned atom mapping and then using RDKit to convert the product into synthons; ii) Synthons Completion: building another model to complete synthons into reactants. Template-free methods regard molecules as sequences, like the Simplified Molecular Input Line Entry System (SMILES) [Weininger, 1988], and frame Retrosynthesis prediction as machine translation. So that it can implicitly learn reaction rules and easily scale to larger data sets without relying on external template databases or molecular editing with RDKit.\nProblem. In previous studies, the typical representation of molecules involves 1D sequences (i.e., SMILES) and 2D graphs. They overlooked the perception and comprehension of 3D conformer information, which is crucial for understanding molecular stereochemistry, reaction centers, chirality, and other properties [Meng et al., 2023; Dang et al., 2018]. Especially in molecules with intricate 3D structures (e.g., polychiral, heteroaromatic), the absence of 3D conformer information can lead to synthesis predictions that defy established chemical rules.\nIn this work, we focus on template-free Transformer for Retrosynthesis prediction. Incorporating 3D conformer information into this framework presents two challenges: i) integrating 3D features while maintaining alignment between atom tokens and corresponding 3D representations; ii) controlling receptive field of model based on spatial structure. To tackle these two challenges, we propose our method, a novel end-to-end Retrosynthesis Transformer that encodes both 1D SMILES sequences and 3D conformer information. We propose two modules, Atom-align Fusion and Distance-weighted Attention, to overcome the aforementioned challenges respectively. The model can jointly encode the 1D sequential and 3D positional information of molecules and redistribute attention weights based on 3D distances between"}, {"title": "Related Work", "content": "Template-based methods utilize reaction templates from pre-constructed database to capture the rules of chemical changes and match input products with the templates. RetroSim [Coley et al., 2017] is a similarity-based approach using molecular fingerprint similarity to rank candidate templates. GLN [Dai et al., 2019] employs the Conditional Graph Logic Network to learn when rules from reaction templates should be applied, implicitly considering chemical feasibility and strategy. Inspired by the chemical intuition that molecular changes primarily occur locally during reactions, LocalRetro [Chen and Jung, 2021] is locally encoded and refined to accommodate non-local reactions. Although these methods perform the best among the three categories, their reliance on pre-defined templates limits their coverage and scalability, rendering them insufficient for real-world scenarios.\nSemi-template methods do not directly use templates but instead combine chemical rules with generative models. Existing methods typically follow a two-stage procedure: i) reaction center identification; ii) synthons completion. Reaction center identification predicts the reaction center of the product and breaks the corresponding bond into incomplete molecules, called synthons. Synthons completion uses generative models to complete synthons into reactants. RetroX-pert [Yan et al., 2020] builds an Edge-enhanced Graph Attention Network (EGAT) that takes a molecule graph as input and predicts bond disconnection probabilities to obtain synthons. It then employs a seq2seq model to generate reactants in SMILES format. In contrast to the SMILES format, G2Gs [Shi et al., 2020] completes synthons into reactants in graph format through node selection and edge labeling. GraphRetro [Somnath et al., 2020] expands synthons into complete reactants by attaching relevant leaving groups. RetroPrime [Wang et al., 2021] introduces a two-stage procedure by adding additional labels to SMILES. In contrast, MEGAN [Nijssen and Kok, 2004] and Graph2Edits [Zhong et al., 2023] reframe the generative process as a series of graph edits executed by RDKit.\nTemplate-free methods transform Retrosynthesis prediction (product reactants) into a machine translation task (source language \u2192 target language) [Brown et al., 1990], adapting advanced NLP models like LSTM [Graves and Graves, 2012] and Transformer [Vaswani et al., 2017]. Besides translation, SCROP [Zheng et al., 2019] builds an additional self-correction transformer to rectify the syntax errors of outputs. Augmented Transformer [Tetko et al., 2020] trains the model with data augmentation via SMILES permutation. To ensure the permutation invariance of SMILES, Graph2SMILES [Tu and Coley, 2022] substitutes the original sequence encoder with a graph encoder. GET [Mao et al., 2021]integrates graph and SMILES encoders in various ways to include graph topological information, while GTA [Seo et al., 2021] masks the self-attention layer using the graph adjacency matrix. Retroformer [Wan et al., 2022] suggests a local attention head that enables information exchange between the local reactive region and the global reaction context."}, {"title": "Preliminary", "content": ""}, {"title": "Template-free Retrosynthesis", "content": "Molecules can be represented as sequences of tokens in SMILES format. Following the SMILES tokenization of [Schwaller et al., 2019] that separates each atom (i.e., O, C, N, Cl) and non-atom tokens (i.e., -, =, #), we let $S = [s_1, s_2,...s_n]$ denote SMILES sequence with $n$ number of tokens. In template-free Retrosynthesis, we obtain the product SMILES $S_p$ and the reactants SMILES $S_R$, and attempt to find a method $f_\\theta$ that can map $S_p$ to $S_R$ with auto-regressive way: $S_R = f_\\theta(s_{i+1}|s_0, ..., s_i, S_p)$ in i step."}, {"title": "Molecular Conformer and ComENet", "content": "Molecule is a dynamic structure since atoms are in continual motion in 3D space. The local minima on the potential energy surface are called conformers. Generally, molecular conformer can be represented as $G_{3D} = (A, C)$, where $A = \\{a_i\\}_{i=1,...,n}$ is atoms list, $C = \\{(X_i, Y_i, Z_i)\\}_{i=1,...,n}$ is a 3D coordinate list. To fulfill the global completeness of 3D conformer information of a molecule, ComENet [Wang et al., 2022] encodes rotation angles and spherical coordinates within the 1-hop neighborhood of atoms into the message passing scheme [Balcilar et al., 2021]."}, {"title": "Method", "content": "In this section, we introduce our method, a novel transformer-based model that is capable of learning richer molecular representation through SMILES sequence and spatial conformer. An overview of the proposed method is shown in Figure 1.\nFigure 1a illustrates Atom-align Fusion that incorporates both 1D SMILES tokens embedding and 3D position embedding to get fusion embedding as inputs for encoder-decoder model. By adopting this approach, the model is capable of integrating 3D positional information while preserving alignment (Section 4.1). Figure 1b illustrates Distance-weighted Attention, which directs self-attention to prioritize chemical relevant atoms within spatial structure (Section 4.2). Figure 1c illustrates SMILES alignment strategy for further bridging the mapping relationship between product SMILES and reactant SMILES in decoder cross-attention (Section 4.3). The overall training and inference process is conducted in an end-to-end manner, and it is a template-free method that does not rely on RDKit for molecular editing."}, {"title": "Atom-align Fusion", "content": "The most direct method of feature integration is concatenation, but this approach disrupts the alignment between 1D representations of atom tokens and their corresponding 3D representations. The challenge lies in how to integrate conformer information into the molecular representation while preserving alignment between them.\nTo address this issue, we introduced the Atom-align Fusion module. The purpose is to obtain fusion embedding $F_{3D} \\in \\mathbb{R}^{M \\times D}$ by associating SMILES token embedding $T \\in \\mathbb{R}^{M \\times D}$ with 3D position embedding $P_{3D} \\in \\mathbb{R}^{N \\times D}$, where $D$ is model dimension, $M$ is number of SMILES tokens, $N$ is number of atoms.\nFirstly, to obtain the complete 3D molecular representation, we extracted 3D position embedding $P_{3D}$ from the molecular conformer using ComENet [Wang et al., 2022]. ComENet proposes a novel message passing scheme [Balcilar et al., 2021] that operates within a 1-hop neighborhood with radial distance $d$, polar angle $\\theta$, azimuthal angle $\\phi$ and rotation angle $\\tau$, and then builds message passing scheme as\n$v_i^{l+1} = g(\\sum_{j \\in N_i} f(v_i^l, v_j^l, d_{ij}, \\theta_{ij}, \\phi_{ij}, \\tau_{ij}))$ (1)\nwhere $v^l \\rightarrow v^{l+1}$ means update of atom (node) representation, $g$ and $f$ implemented by neural networks or mathematical operations, $d_{ij}$, $\\theta_{ij}$ and $\\phi_{ij}$ specify the 1-hop local neighborhood, $\\tau_{ij}$ determines the orientation of the local neighborhood. We omit the final aggregation layer of ComENet to obtain the 3D representation corresponding to each atom $P_{3D} = \\{V_i\\}_{i=1,2,...,N}$ rather than the entire molecular graph.\nSubsequently, we input $P_{3D}$ along with the SMILES tokens embedding $T$ into the encoder. To facilitate a more comprehensive integration of SMILES and conformer, we aligned tokens embedding $T$ and $P_{3D}$ with atom index in the same vector space. We initially enlarge $P_{3D}$ from $N$ (number of atoms) length to $M$ (number of tokens) length by padding 0 to the indexes of non-atom tokens. In model input stage, we blend the $P_{3D}$ and $T$ to get the fusion embedding $F_{3D}$, allowing effective integration of 3D positional and sequential"}, {"title": "Distance-weighted Attention", "content": "According to [Li et al., 2017; Wan et al., 2022], multi-head attention calculation in vanilla transformer is insufficient and lacks chemical knowledge. Simultaneously, 3D distances between atoms within a molecule reflect the spatial relation between any pair of atoms in physical chemistry, such as bond energy, stability, electron affinity and interatomic force. Incorporating 3D distances into the multi-head attention calculation process enables the model to reschedule attention accordingly, which not only alleviates the redundancy in multi-head attention but also enables distance-based attention redistribution during the attention calculation.\nSimilar to the method of partitioning attention heads in [Wan et al., 2022], we divide the multi-head attention into normal attention heads and spatial attention heads. Normal attention heads follow the traditional transformer manner to learn sequence-wide information, whereas spatial attention heads employ 3D distances as an external attention weight to enable attention redistributed based on 3D distances. The whole module can be formulated as two steps: 1) Construct 3D distance weight, 2) Attention Redistribute & Weight Refine.\nConstruct 3D Distance Weight First, we construct a distance matrix $D \\in \\mathbb{R}^{N \\times N}$ from molecular conformer of molecule as\n$D = \\{d_{i,j}\\}_{i,j=1,2,...,N} d_{ij} = ||C_i - C_j||_2$ (3)\nwhere $N$ is number of atoms, $i, j$ are index of atoms pair, $c_i$ is coordiante of $i$-th atom. To explore high-dimensional space, we employ Gaussian Basis Function to transform the distance $d_{ij}$ between each atom pair $(i, j)$ into a higher dimension as\n$\\psi^{(i,j)} = \\frac{1}{\\sqrt{2\\pi\\sigma_k}} exp(-\\frac{1}{2\\sigma_k^2}(\\gamma^{(i,j)} d_{ij} + \\beta^{(i,j)} - \\mu_k)^2)$ (4)\nwhere $k=\\{1,...,K\\}$, $K$ is the number of Gaussian Kernel kernels. $\\mu_k$ and $\\sigma_k$ represent kernel center and scaling factor of $k$-th Gaussian Basis kernel. $\\gamma^{(i,j)}$ and $\\beta^{(i,j)}$ are learnable scalars related to bond type (i.e., SINGLE, DOUBLE) of pair $(i, j)$. To obtain the 3D Distance weight $\\Phi^{(i,j)}$, we use two layers of Non-Linear Layer with the activation function Gaussian Error Linear Units (GELU) [Hendrycks and Gimpel, 2016] to encode $\\psi^{(i,j)}$ as\n$\\Phi^{(i,j)} = W_2(GELU(W_1(\\Psi^{(i,j)})))$ (5)\nwhere $\\Psi^{(i,j)} = stack([\\psi_1^{(i,j)};...;\\psi_k^{(i,j)}])$, $W_1, W_2 \\in \\mathbb{R}^{K \\times K}$ are learnable parameters and 3D Distance weight $\\Phi^{(i,j)} \\in \\mathbb{R}^{N \\times N \\times K}$."}, {"title": "Training Strategy", "content": ""}, {"title": "SMILES Alignment", "content": "During the occurrence of chemical reactions, the majority of atoms remain unchanged. We can utilize the atom-map to annotate the correspondence between atoms of product and reactants in reaction, which remains unchanged during the reaction. Furthermore, the correspondence of atoms can be readily transferred to the token correspondence in the SMILES sequence (see in Figure 1c) and construct SMILES Alignment Map (SAM) as:\n$SAM_{ij} = \\begin{cases} 1 & \\text{if } R_i \\overset{\\text{mapped}}{\\leftrightarrow} P_j \\\\ 0 & \\text{else} \\end{cases}$ (9)\nInspired by [Deshpande and Narasimhan, 2020], we similarly employed the guided attention approach by introducing guidance loss $L_{SA}$ to encourage alignment between the cross attention scores in the final layer of the decoder and SMILES Alignment Map, strengthening the attention weights between corresponding tokens."}, {"title": "Experiments", "content": "Datasets We use the popular Retrosynthesis benchmark dataset USPTO-50K [Schneider et al., 2016], which contains 50,016 atom-mapped reactions, to evaluate our model. The same data split as [Coley et al., 2017] is applied to our experiments, resulting in 40,008, 5,001, and 5,007 reactions for the training, validation, and test sets, respectively. Conformers are generated using RDKit with the stochastic optimization algorithm Merck Molecular Force Field (MMFF). We then apply the same algorithm in [Wan et al., 2022] to extract the ground truth SMILES Alignment Map.\nSetting Our model is built on vanilla Transformer, consisting of 8 encoder layers and 8 decoder layers with 8 attention heads. The model dimension is set to 512. We employ the Adam optimizer [Kingma and Ba, 2014] with $(\\beta_1, \\beta_2) = (0.9, 0.98)$ and train the model with batch size of 16. During training, we set an early-stop of 7 epochs under a maximum of 1000 epochs to obtain the 7 best models and average their parameters. Finally, the whole training process took approximately 30 hours on a single NVIDIA GeForce RTX 4090 GPU.\nEvaluation During inference, we use beam search [Tillmann and Ney, 2003] decode strategy with a beam size is 10. After that, we evaluate our model with Top-k accuracy, which considers it correct only if Top-k predicted reactants of the beam search results are the same as original test set. For molecule validity, we verify Top-k validity based on whether RDKit can recognize the Top-k predicted reactants.\nBaseline We compare our proposed method, our method, with several strong baseline models from three types of Retrosynthesis method. We take RetroSim [Coley et al., 2017], GLN [Dai et al., 2019] and LocalRetro [Chen and Jung, 2021] to represent template-based methods. We take RetroX-pert [Yan et al., 2020], G2Gs [Shi et al., 2020], GraphRetro [Somnath et al., 2020], RetroPrime [Wang et al., 2021], MEGAN [Nijssen and Kok, 2004] and Graph2Edits [Zhong et al., 2023] to represent semi-template method. We take SCROP [Zheng et al., 2019], Aug.Transformer [Tetko et al., 2020], Graph2SMILES [Zhong et al., 2023], GET [Mao et al., 2021], GTA [Seo et al., 2021] and Retroformer [Wan et al., 2022] to represent template-free method. We did not choose pre-training methods for comparison as they are trained on larger datasets. We experimented with two training strategies for our proposed model: our method represents the model with Randomize SMILES data augmentation; our method+R represents the model with Root-align SMILES data augmentation."}, {"title": "Performance", "content": "Top-k Accuracy As shown in Table 1, for the model with random augmentation our method, when reaction class is known, Top-1/Top-10 accuracy is 65.3%/91.6%, exceeding"}, {"title": "Ablation Study", "content": "To explore the impact of two modules, we design an ablation study under the settings of unknown reaction class and random SMILES data augmentation. The results are shown in Table 3 in the form of top-k accuracy and demonstrate necessity of each module for our method. The improvement in top-1 accuracy from 2 to 1 suggests that the model is able to use 3D positional information to infer the correct results with higher confidence after introducing module (a) Atom-align Fusion. Notably, the declines in top 3-10 accuracy is speculated on the reliance upon fixed 3D position embedding, which constrains the search space of model. Comparing 3"}, {"title": "Case Study", "content": "To verify that the proposed method our method can handle molecules with more intricate spatial structures, we chose 4 types of products as representatives:\n\u2022 Polychiral: a product with two or more chiral centers ('@/@@' in SMILES), leading to complex stereochemistry and multiple isomers.\n\u2022 Heteroaromatic: a product containing at least one heteroatom as part of its aromatic ring structure.\n\u2022 Fused / Bridged Rings: product features multiple ring structures joined together, either sharing common atoms or connected by bridge atoms.\n\u2022 Complex: a product encompassing two or more features mentioned above.\nRegarding the 4 types of products, we experiment on our method and a Transformer without 3D information and choose the Top1-3 predictions for analysis, as shown in Figure 2. The rows of each subfigure correspond to our method and Transformer, respectively. Ground Truth and invalid SMILES are both highlighted in bold red.\nThe results show that when dealing with molecules of intricate spatial structures, our method can infer reasonable reactants with high confidence (often at top1, top2), while Transformer without 3D information finds it challenging to ascertain accurate results. Particularly in the case of Complex products, three predictions of our method could deduce skeleton and achieve the ground truth at top-1, whereas Transformer could not get any valid SMILES at top1-3. This further confirms the importance of introducing 3D information when dealing with molecules of intricate spatial structures."}, {"title": "Conclusion", "content": "We propose our method, an end-to-end Transformer that integrates 3D conformer for Retrosynthesis. With the proposed Atom-align Fusion module, we can adaptively integrate token and 3D position information ensuring the alignment between them. Next, we propose Distance-weighted Attention mechanism to guide the redistribution of self-attention with spatial distances and refine it across multiple layers. Our model achieves a new state-of-the-art in template-free methods and is also highly competitive with template-based and semi-template methods. In future work, we will attempt to introduce our method into multi-step synthesis route planning."}]}