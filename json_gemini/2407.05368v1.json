{"title": "MUSIC ERA RECOGNITION USING SUPERVISED CONTRASTIVE LEARNING AND ARTIST INFORMATION", "authors": ["Qiqi He", "Xuchen Song", "Weituo Hao", "Ju-Chiang Wang", "Wei-Tsung Lu", "Wei Li"], "abstract": "Does popular music from the 60s sound different than that of the\n90s? Prior study has shown that there would exist some variations\nof patterns and regularities related to instrumentation changes and\ngrowing loudness across multi-decadal trends. This indicates that\nperceiving the era of a song from musical features such as audio\nand artist information is possible. Music era information can be an\nimportant feature for playlist generation and recommendation. How-\never, the release year of a song can be inaccessible in many circum-\nstances. This paper addresses a novel task of music era recognition.\nWe formulate the task as a music classification problem and pro-\npose solutions based on supervised contrastive learning. An audio-\nbased model is developed to predict the era from audio. For the case\nwhere the artist information is available, we extend the audio-based\nmodel to take multimodal inputs and develop a framework, called\nMultiModal Contrastive (MMC) learning, to enhance the training.\nExperimental result on Million Song Dataset demonstrates that the\naudio-based model achieves 54% in accuracy with a tolerance of\n3-years range; incorporating the artist information with the MMC\nframework for training leads to 9% improvement further.", "sections": [{"title": "1. INTRODUCTION", "content": "As a central goal of the popular music industry over the past 60 years,\nmusic is created to be attractive to listeners [1, 2]. It is believed that\nmusic must incorporate variations of patterns and regularities that\nbuild upon people's expectations and memories across years [2, 3].\nAlthough scientific evidence still remains less specific, prior study\nhas shown that these variations could be related to instrumentation\nchanges and growing loudness levels [4]. Timbral and mood varia-\ntions were also found across multi-decadal trends in [5]. Inspired by\nthese prior findings, developing a predictive model to recognize the\nmusic era from audio can be a potentially feasible task.\nIn music categorization, people typically break down music era\nby decades (e.g., 80s, 90s, and 2000s), which are commonly used as\ntags in major music streaming services such as Spotify and Pandora.\nA song's release year can carry a meaningful context of culture,\nmood, a time in life, or a peek into history, offering a straightfor-\nward way to organize songs for use such as playlist generation and\nrecommendation [6, 7]. Although the music era can be inferred via\nthe release year, estimating the era for a song from audio is practi-\ncally useful in various scenarios. As the Internet has become ubiqui-\ntous, users' content sharing and derivative work have grown rapidly\non media platforms such as TikTok and YouTube. These activities\nmay involve music reuse or edits (e.g., excerpt and remix) which can\ncause loss of the original metadata. Moreover, the cover version of\nan old tune recorded recently would also yield confusion.\nIn this paper, we introduce the music era recognition task,\nwhich can be formulated as an application-specific music classifica-\ntion problem that aims to classify songs into different year ranges\n(e.g., decades), depending on the granularity. A major challenge is\nto differentiate songs from nearby year ranges, because the varia-\ntion between two songs can be less notable if they were released\nwithin a range of years. In addition, we also observe an imbalanced\ndistribution of songs across years in data. Therefore, our design\nprinciple requires the model to learn representations near each other\nfor two song inputs if both are in the same year range, and otherwise\nfar apart. For this purpose, we adopt the supervised contrastive\nlearning framework [8], which has shown state-of-the-art perfor-\nmance in image classification. To incorporate artist information, we\nintroduce the MultiModal Contrastive (MMC) learning framework,\nwhich includes a novel architecture for multimodal inputs and an\nunsupervised contrastive loss for learning a robust combination of\nthe audio and artist embeddings. The MMC loss can force clustering\nthe embeddings of the songs from the same artist [9].\nTo our knowledge, this work represents the first attempt to de-\nvelop automatic methods for music era recognition. From a tech-\nnical perspective, our work is related to music auto-tagging. Early\napproaches to music tagging include methods using the context in-\nformation of music to predict the user preference tags [10]. Recent\nadvances in deep learning accelerated the development of content-\nbased music tagging technology [11]. Numerous systems based on\nconvolutional neural networks (CNN) were proposed [12, 13, 14].\nHowever, little attention was paid to music era recognition, possibly\nbecause the song release year is typically accessible through meta-\ndata in common use cases such as streaming. Nevertheless, as afore-\nmentioned, era information can be missing in many circumstances.\nAs the sounds of an era could be better defined by the popular\nsongs of the era, we consider this work is also related to the task of\nhit song prediction [15], where the goal is to predict if a song would\nbe successful/popular within a period of (future) time based on its\nmusical features. Two types of features were explored: internal fea-\ntures extracted from audio [16, 17] and external ecosystem-related\nfeatures such as social media and market data [5]. It is also found\nthat adding an artist factor (i.e., if the artist had succeeded in the near\npast) can significantly improve the prediction accuracy [5].\nTo summarize our technical contributions, we propose three\nvariants of methods, including: (1) Audio-CNN: a CNN model that\npredicts music era from audio (Section 2.1); (2) Audio-SUC: an\nenhanced model that incorporates supervised contrastive learning"}, {"title": "2. PROPOSED METHOD", "content": ""}, {"title": "2.1. Convolutional Neural Network (CNN)", "content": "Due to insufficient research on music era recognition, we develop\nAudio-CNN, a modified CNN proposed in [18] as our baseline\nmodel, which contains a stack of CNN layers as a filter to extract\nlocal features, followed by a linear layer as to output classification\nlogits. The feature map of l-th CNN layer is computed as follows:\n\\(H_l = ELU(BN(CNN(H_{l-1}))),\\) (1)\nwhere BN is the batch normalization operation, and ELU is the ex-\nponential linear unit. Each convolution is operated based on a kernel\nsize of 3 x 3, followed by an average pooling layer. The model is\ntrained with cross-entropy loss:\n\\(L_{MLE} = - \\frac{1}{N} \\sum_{i=1}^{N} y_i log(f(x)),\\) (2)\nwhere N is the number of songs, \\(x_i\\) is the mel-spectrogram extracted\nfrom the audio of song i, and \\(y_i\\) is the corresponding one-hot en-\ncoded label out of C possible era classes."}, {"title": "2.2. Supervised Contrastive (SUC) Learning", "content": "Despite the large scale CNNs can be a powerful model, we consider\nits architecture and training scheme using cross-entropy loss are still\ninsufficient to tackle the discrimination of nearby eras. Therefore,\nwe develop a novel architecture, called Audio-SUC, to incorporate\nthe contrastive learning [8, 19, 20, 21, 22], which aims to learn robust\nrepresentations by discriminating positive and negative embedding\npairs based on the era labels.\nSpecifically, we follow [8] to design the era contrastive (EC)\nloss, which forces clusters of audio embedding belonging to the\nsame era class to be pulled together in the embedding space, and\nmeanwhile pushes apart the audio embeddings from different era\nclasses. To this end, an learnable projection head is defined as\n\\(g_E: \\{h_a\\} \\rightarrow z\\), where \\(h_a\\) is the encoded embedding from mel-\nspectrogram x, and z is the latent representation.\nFor training, we first organize the training songs by their era\nlabels. Let I be the training set, \\(z_i\\) be the latent representation for\nsong i, and P(i) be the set with the same era label as song i's, while\nN(i) be the set with different labels from song i's. The learning\nprocess is illustrated in Fig. 1(a) and Fig. 1(b). Then, the EC loss\ncan be defined as follows:\n\\(L_{EC} = - \\sum_{i \\in I} \\sum_{j \\in P(i)} log \\frac{\\sigma(z_i \\cdot z_j / \\tau)}{\\sum_{k \\in N(i)} \\sigma(z_i, z_k / \\tau)},\\) (3)\nwhere \\(\\sigma(\\cdot)\\) is the non-linear transform which adopts an exponential\nfunction, and \\(\\tau\\) is the temperature parameter. Note that \\(L_{EC}\\) brings\ntwo beneficial properties [8]: (i) ability to align all available positive\nsamples; (ii) ability to mine hard positive and negative pairs. These\nproperties can lead to a more robust representation of learning in our\nsupervised scenario."}, {"title": "2.3. Multi-Modal Contrastive (MMC) Learning", "content": "Fig. 2 presents the era distribution of an In-House dataset across\nyears, indicating a high imbalance between pre- and post-2000. In\nthis ection, we propose the Multi-Modal Contrastive Learning to\ntackle this issue."}, {"title": "2.3.1. Multi-Modal Fusion Module", "content": "Multi-modal approaches have increasingly attracted researchers' at-\ntention owing to their promising results on various deep learning\ntasks [23, 24]. To learn a better music representation, text about the\nmusic, such as lyrics, can be helpful augmented information and has\nbeen widely used in MIR tasks [25, 26, 27]. However, given a song,\nits lyrics are not always available for reasons such as copyright is-\nsues. Alternatively, the text of the artist's biography contains rich in-\nformation about the music style during the artist's active years [28].\nWe choose the artist's biography considering its availability as op-\nposed to lyrics.\nWe follow the previous work [29] to make full use of contextual\nand correspondence information between music and text via atten-\ntion mechanism, as illustrated in Fig. 3(a). Given an input song i,\nwe use two embedding vectors, \\(a_i \\in \\mathbb{R}^{d_a}\\) and \\(t_i \\in \\mathbb{R}^{d_h}\\), to rep-\nresent its audio and artist biography information, respectively, and\n\\(d_h = 64\\) is adopted in the experiments. Specifically, \\(a_i\\) is derived\nfrom the mel-spectrogram \\(x_i\\) using the audio encoder. For \\(t_i\\), we"}, {"title": "2.3.2. Multi-Modal Contrastive (MMC) Loss", "content": "The EC loss \\(L_{EC}\\) conditioning on the era labels directly helps im-\nprove the era classification task. However, it may excessively blur\nthe differences of songs in the multi-modal embedding space within\nthe same era class. Therefore, to avoid the representation collapse,\nwe develop the multi-modal contrastive (MMC) loss, an unsuper-\nvised contrastive learning objective. The critical part for the effec-\ntiveness of MMC learning is a reasonable design of different views\nof the input. For example, in computer vision, data augmentation\ntechniques such as cropping and rotation are applied to one anchor\nimage and the other to create positive and negative views of the an-\nchor image. Similarly, we propose text-shuffle, a technique to create\nmultiple views for the combination of audio and artist embeddings\ninput. To be specific, we consider the concatenation of matched au-\ndio and artist embeddings \\(s_{i,i} = [a_i, t_i]\\) as a reference, so the con-\ncatenation of mismatched audio and artist creates negative views,\ne.g., \\(s_{i,k} = [a_i, t_k]\\), where \\(t_k\\) is a randomly sampled biography em-\nbedding from a different artist. To apply the MMC loss, two compo-\nnents are designed as follows:\nAudio-Artist Encoder: \\(f_E: \\{s\\} \\rightarrow h_m\\), where \\(h_m\\) is the\nmulti-modal embedding learned from the audio-artist con-\ncatenation embedding s.\nProjection Head: \\(f_T: \\{t\\} \\rightarrow h_m\\), which projects the artist\nbiography t into the multi-modal embedding space.\nFor simplicity, we take the artist biography as the anchor. Let an\nartist embedding \\(t_i\\) be the anchor, its positive and negative examples\nin the audio-artist concatenation embedding space are \\(s_{i,i}\\) and \\(s_{i,k}\\),\nrespectively. The MMC loss is then defined as:\n\\(L_{MMC} = - \\sum_{i \\in C} \\sum_{j \\in c(i)} log \\frac{\\sigma(f_T(t_i) \\cdot f_E(s_{i,i}) / \\tau)}{\\sum_{k \\in c(i)} \\sigma(f_T(t_i) \\cdot f_E(s_{i,k}) / \\tau)},\\) (4)\nwhere \\(\\sigma(\\cdot)\\) is the exponential function, C is the all possible era la-\nbels, c is the era label of song i, and c(i) is the set having the era label\nc. The learning process is illustrated in Fig. 1(c) and Fig. 1(d). As il-\nlustrated in Fig. 3(b), we obtain the AudioArt-MMC model, which is\ntrained with a sum of the cross-entropy (MLE) loss, era contrastive\n(EC) loss, and multi-modal contrastive (MMC) loss:\n\\(L = L_{MLE} + \\alpha L_{MMC} + \\beta L_{EC},\\) (5)\nwhere \\(\\alpha\\) and \\(\\beta\\) are hyperparameters weighting the loss terms. Note\nthat, AudioArt-MMC uses the multi-modal embedding for \\(g_E\\) to cal-\nculate \\(L_{EC}\\) (see Eq. 3), while Audio-SUC uses the audio embedding\nencoded from mel-spectrogram for \\(L_{EC}\\)."}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Experiment Setup", "content": "Datasets & Metrics. To evaluate our proposed methods, we used a\npublic dataset, Million Song Dataset (MSD) [32], and an internal mu-\nsic collection (In-House). MSD, which has been widely used in\nmusic auto-tagging evaluations [11], contains the metadata and pre-\ncomputed audio features of one million contemporary songs. Our\nIn-House dataset includes about 800K songs, where the era dis-\ntribution is presented in Fig. 2. For artist biography, we adopted\nthe AllMusic dataset [33], which covers the biography text data of\nabout 200K artists. We created the audio-text input pairs by match-\ning the artist names. We selected 85,475 tracks from MSD for the\nexperiments, considering the availability of audio features, release\nyears, and artist biographies. Whereas, the full set of In-House\nwas used, since it covers all the required information.\nThe difference in instrumentation and genre of songs released in\nclose years can be subtle. Therefore, we follow similar works [34,\n35] to propose a metric that allows false prediction tolerance. We\ndefine the accuracy with \\(\\pm x\\) years of tolerance (ACC\\(_x\\)) as follows:\n\\(ACC_x = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{x - min[abs(y_i - \\hat{y_i}), x]}{x},\\) (6)\nwhere \\(y_i\\) and \\(\\hat{y_i}\\) represent the ground truth and predicted years\nof song i, and N is the total number of test songs. For instance,\nACC0 counts only the cases when the prediction exactly matches\nthe ground truth. We define two evaluation scenarios of granularity\nfor year ranges: (i) one year per class: each year represents a class.\nFor example, songs released in 1981 are assigned with a different\nclass than songs released in 1982. (ii) ten years per class: each\nclass represents a decade (e.g., 60s, 80s, and 00s). Therefore, songs\nreleased in 1981 and 1982 are within the 1980s' class, while the\nsong in 2011 is labeled with '2010s' class.\nTraining Details. To extract the input mel-spectrograms, we first\nresample the original audio waveform to 22,050 Hz. Then, we use a\nwindow size of 2,048 with a hop size of 512 for a frame, and trans-form each frame into a 224-band mel-scaled magnitude spectrum.\nIn the training stage, we set a batch size of 64. Each audio example,\nwhich includes 1,024 frames (roughly 6-seconds long), is randomly\nsampled from an arbitrary song in the training data. We use the\nAdam optimizer [36] with a learning rate of 1e-4."}, {"title": "3.2. Results and Discussion", "content": "We performed 10-fold cross-validation to obtain the results, and this\nprocess was done on MSD and In-House independently. We split\n10% of the training set as the validation set, and selected the snapshot\nwith best performance on validation set as the testing model.\nEra Prediction Comparison. Table 1 presents the overall compari-\nson of the two datasets. It is clear that incorporating the supervised\ncontrastive learning (Audio-SUC) can significantly outperform the\nbaseline model (Audio-CNN) for most of the cases (except the ACC2\nin the 8 classes scenario on MSD), and that jointing the audio and\nartist information for input with the multi-modal contrastive learn-\ning (i.e., AudioArt-MMC) can consistently further the performance.\nSuch a result is in line with our expectations when designing the\nmodels. As suggested in prior work [5], artist information is very\nhelpful in hit song prediction of a period of time, so it also offers\nstrong clues about the musical sounds representing an era. Compar-\ning among ACC values with different tolerances for the 64 classes\nscenario, the superiority of the contrastive learning-based models\nover CNN seems to become more apparent when the tolerance is\ngetting larger. This indicates that the learned features indeed possess\nabilities for era discrimination. Lastly, for the 8 classes scenario (i.e.,\ndecade tagging), ACC1 can achieve 90%+ accuracy solely based on\naudio, demonstrating promising results for a usable system in rele-\nvant MIR applications.\nRobustness to Data Imbalance. As discussed, our dataset is im-\nbalanced in era distribution. We are interested in the performance of\nthe pre-2000 era, so we present the result of the corresponding sub-"}, {"title": "4. CONCLUSION", "content": "In this paper, we have presented a novel multi-modal contrastive\nlearning framework to recognize music era based on audio and artist\ninformation. The design of the EC and MMC losses enables a sat-\nisfactory clustering behaviors, showing convincing results for the\ntask. For future work, we will try to improve the performance with\nmore sophisticated solutions. We also plan to study the possibility\nof including additional metadata, such as instrumentation, genre, and\nmood, into a unified framework."}]}