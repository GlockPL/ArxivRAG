{"title": "MUSIC ERA RECOGNITION USING SUPERVISED CONTRASTIVE LEARNING AND ARTIST INFORMATION", "authors": ["Qiqi He", "Xuchen Song", "Weituo Hao", "Ju-Chiang Wang", "Wei-Tsung Lu", "Wei Li"], "abstract": "Does popular music from the 60s sound different than that of the 90s? Prior study has shown that there would exist some variations of patterns and regularities related to instrumentation changes and growing loudness across multi-decadal trends. This indicates that perceiving the era of a song from musical features such as audio and artist information is possible. Music era information can be an important feature for playlist generation and recommendation. However, the release year of a song can be inaccessible in many circumstances. This paper addresses a novel task of music era recognition. We formulate the task as a music classification problem and propose solutions based on supervised contrastive learning. An audio-based model is developed to predict the era from audio. For the case where the artist information is available, we extend the audio-based model to take multimodal inputs and develop a framework, called MultiModal Contrastive (MMC) learning, to enhance the training. Experimental result on Million Song Dataset demonstrates that the audio-based model achieves 54% in accuracy with a tolerance of 3-years range; incorporating the artist information with the MMC framework for training leads to 9% improvement further.", "sections": [{"title": "1. INTRODUCTION", "content": "As a central goal of the popular music industry over the past 60 years, music is created to be attractive to listeners [1, 2]. It is believed that music must incorporate variations of patterns and regularities that build upon people's expectations and memories across years [2, 3]. Although scientific evidence still remains less specific, prior study has shown that these variations could be related to instrumentation changes and growing loudness levels [4]. Timbral and mood variations were also found across multi-decadal trends in [5]. Inspired by these prior findings, developing a predictive model to recognize the music era from audio can be a potentially feasible task.\nIn music categorization, people typically break down music era by decades (e.g., 80s, 90s, and 2000s), which are commonly used as tags in major music streaming services such as Spotify and Pandora. A song's release year can carry a meaningful context of culture, mood, a time in life, or a peek into history, offering a straightforward way to organize songs for use such as playlist generation and recommendation [6, 7]. Although the music era can be inferred via the release year, estimating the era for a song from audio is practically useful in various scenarios. As the Internet has become ubiquitous, users' content sharing and derivative work have grown rapidly on media platforms such as TikTok and YouTube. These activities may involve music reuse or edits (e.g., excerpt and remix) which can cause loss of the original metadata. Moreover, the cover version of an old tune recorded recently would also yield confusion.\nIn this paper, we introduce the music era recognition task, which can be formulated as an application-specific music classification problem that aims to classify songs into different year ranges (e.g., decades), depending on the granularity. A major challenge is to differentiate songs from nearby year ranges, because the variation between two songs can be less notable if they were released within a range of years. In addition, we also observe an imbalanced distribution of songs across years in data. Therefore, our design principle requires the model to learn representations near each other for two song inputs if both are in the same year range, and otherwise far apart. For this purpose, we adopt the supervised contrastive learning framework [8], which has shown state-of-the-art performance in image classification. To incorporate artist information, we introduce the MultiModal Contrastive (MMC) learning framework, which includes a novel architecture for multimodal inputs and an unsupervised contrastive loss for learning a robust combination of the audio and artist embeddings. The MMC loss can force clustering the embeddings of the songs from the same artist [9].\nTo our knowledge, this work represents the first attempt to develop automatic methods for music era recognition. From a technical perspective, our work is related to music auto-tagging. Early approaches to music tagging include methods using the context information of music to predict the user preference tags [10]. Recent advances in deep learning accelerated the development of content-based music tagging technology [11]. Numerous systems based on convolutional neural networks (CNN) were proposed [12, 13, 14]. However, little attention was paid to music era recognition, possibly because the song release year is typically accessible through metadata in common use cases such as streaming. Nevertheless, as aforementioned, era information can be missing in many circumstances.\nAs the sounds of an era could be better defined by the popular songs of the era, we consider this work is also related to the task of hit song prediction [15], where the goal is to predict if a song would be successful/popular within a period of (future) time based on its musical features. Two types of features were explored: internal features extracted from audio [16, 17] and external ecosystem-related features such as social media and market data [5]. It is also found that adding an artist factor (i.e., if the artist had succeeded in the near past) can significantly improve the prediction accuracy [5].\nTo summarize our technical contributions, we propose three variants of methods, including: (1) Audio-CNN: a CNN model that predicts music era from audio (Section 2.1); (2) Audio-SUC: an enhanced model that incorporates supervised contrastive learning"}, {"title": "2. PROPOSED METHOD", "content": ""}, {"title": "2.1. Convolutional Neural Network (CNN)", "content": "Due to insufficient research on music era recognition, we develop Audio-CNN, a modified CNN proposed in [18] as our baseline model, which contains a stack of CNN layers as a filter to extract local features, followed by a linear layer as to output classification logits. The feature map of l-th CNN layer is computed as follows:\n$$H_l = ELU(BN(CNN(H_{l-1}))),$$\nwhere BN is the batch normalization operation, and ELU is the exponential linear unit. Each convolution is operated based on a kernel size of 3 x 3, followed by an average pooling layer. The model is trained with cross-entropy loss:\n$$L_{MLE} = - \\frac{1}{N} \\sum_{i=1}^N y_i \\log(f(x_i)),$$\nwhere N is the number of songs, $x_i$ is the mel-spectrogram extracted from the audio of song i, and $y_i$ is the corresponding one-hot encoded label out of C possible era classes."}, {"title": "2.2. Supervised Contrastive (SUC) Learning", "content": "Despite the large scale CNNs can be a powerful model, we consider its architecture and training scheme using cross-entropy loss are still insufficient to tackle the discrimination of nearby eras. Therefore, we develop a novel architecture, called Audio-SUC, to incorporate the contrastive learning [8, 19, 20, 21, 22], which aims to learn robust representations by discriminating positive and negative embedding pairs based on the era labels.\nSpecifically, we follow [8] to design the era contrastive (EC) loss, which forces clusters of audio embedding belonging to the same era class to be pulled together in the embedding space, and meanwhile pushes apart the audio embeddings from different era classes. To this end, an learnable projection head is defined as $g_\\Theta: \\{h_a\\} \\rightarrow z$, where $h_a$ is the encoded embedding from mel-spectrogram x, and z is the latent representation.\nFor training, we first organize the training songs by their era labels. Let I be the training set, $z_i$ be the latent representation for song i, and $P(i)$ be the set with the same era label as song i's, while $N(i)$ be the set with different labels from song i's. Then, the EC loss can be defined as follows:\n$$L_{EC} = -\\sum_{i \\in I} \\sum_{j \\in P(i)} \\log \\frac{\\sigma(z_i \\cdot z_j / \\tau)}{\\sum_{k \\in N(i)} \\sigma(z_i, z_k / \\tau)},$$\nwhere $\\sigma(\\cdot)$ is the non-linear transform which adopts an exponential function, and $\\tau$ is the temperature parameter. Note that $L_{EC}$ brings two beneficial properties [8]: (i) ability to align all available positive samples; (ii) ability to mine hard positive and negative pairs. These properties can lead to a more robust representation of learning in our supervised scenario."}, {"title": "2.3. Multi-Modal Contrastive (MMC) Learning", "content": "Fig. 2 presents the era distribution of an In-House dataset across years, indicating a high imbalance between pre- and post-2000. In this ection, we propose the Multi-Modal Contrastive Learning to tackle this issue."}, {"title": "2.3.1. Multi-Modal Fusion Module", "content": "Multi-modal approaches have increasingly attracted researchers' attention owing to their promising results on various deep learning tasks [23, 24]. To learn a better music representation, text about the music, such as lyrics, can be helpful augmented information and has been widely used in MIR tasks [25, 26, 27]. However, given a song, its lyrics are not always available for reasons such as copyright issues. Alternatively, the text of the artist's biography contains rich information about the music style during the artist's active years [28]. We choose the artist's biography considering its availability as opposed to lyrics.\nWe follow the previous work [29] to make full use of contextual and correspondence information between music and text via attention mechanism. Given an input song i, we use two embedding vectors, $a_i \\in \\mathbb{R}^{d_a}$ and $t_i \\in \\mathbb{R}^{d_h}$, to represent its audio and artist biography information, respectively, and $d_h = 64$ is adopted in the experiments. Specifically, $a_i$ is derived from the mel-spectrogram $x_i$ using the audio encoder. For $t_i$, we utilize Sentences-BERT [30] to encode the corresponding artist biography text. Then, we concatenate $a_i$ and $t_i$ to obtain $s_i \\in \\mathbb{R}^{2d_h}$ as the input of the self-attention layer.\nThe fusion module is based on Transformer blocks [31]. Specifically, the output of l-th Transformer block $H_l$ is the concatenation of multiple attention heads: $H_l = [A_{l,1}, \\ldots, A_{l,h}]$ where h is the number of heads. Each head A contains trainable parameters $W^q, W^k, W^v \\in \\mathbb{R}^{2d_h \\times d_k}$, where $d_k$ is the projection dimension of the attention layer."}, {"title": "2.3.2. Multi-Modal Contrastive (MMC) Loss", "content": "The EC loss $L_{EC}$ conditioning on the era labels directly helps improve the era classification task. However, it may excessively blur the differences of songs in the multi-modal embedding space within the same era class. Therefore, to avoid the representation collapse, we develop the multi-modal contrastive (MMC) loss, an unsupervised contrastive learning objective. The critical part for the effectiveness of MMC learning is a reasonable design of different views of the input. For example, in computer vision, data augmentation techniques such as cropping and rotation are applied to one anchor image and the other to create positive and negative views of the anchor image. Similarly, we propose text-shuffle, a technique to create multiple views for the combination of audio and artist embeddings input. To be specific, we consider the concatenation of matched audio and artist embeddings $s_{i,i} = [a_i, t_i]$ as a reference, so the concatenation of mismatched audio and artist creates negative views, e.g., $s_{i,k} = [a_i, t_k]$, where $t_k$ is a randomly sampled biography embedding from a different artist. To apply the MMC loss, two components are designed as follows:\n*   Audio-Artist Encoder: $f_E : \\{s\\} \\rightarrow h_m$, where $h_m$ is the multi-modal embedding learned from the audio-artist concatenation embedding s.\n*   Projection Head: $f_T : \\{t\\} \\rightarrow h_m$, which projects the artist biography t into the multi-modal embedding space.\nFor simplicity, we take the artist biography as the anchor. Let an artist embedding $t_i$ be the anchor, its positive and negative examples in the audio-artist concatenation embedding space are $s_{i,i}$ and $s_{i,k}$, respectively. The MMC loss is then defined as:\n$$L_{MMC} = - \\sum_i \\sum_{c \\in C} \\log \\frac{\\sigma(f_T(t_i) \\cdot f_E(s_{i,i})/ \\tau)}{\\sum_{k \\in c(i)} \\sigma(f_T(t_i) \\cdot f_E(s_{i,k})/ \\tau)},$$\nwhere $\\sigma(\\cdot)$ is the exponential function, C is the all possible era labels, c is the era label of song i, and c(i) is the set having the era label c. As illustrated in Fig. 3(b), we obtain the AudioArt-MMC model, which is trained with a sum of the cross-entropy (MLE) loss, era contrastive (EC) loss, and multi-modal contrastive (MMC) loss:\n$$L = L_{MLE} + \\alpha L_{MMC} + \\beta L_{EC},$$\nwhere $\\alpha$ and $\\beta$ are hyperparameters weighting the loss terms. Note that, AudioArt-MMC uses the multi-modal embedding for $g_\\Theta$ to calculate $L_{EC}$, while Audio-SUC uses the audio embedding encoded from mel-spectrogram for $L_{EC}$ ."}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Experiment Setup", "content": "Datasets & Metrics. To evaluate our proposed methods, we used a public dataset, Million Song Dataset (MSD) [32], and an internal music collection (In-House). MSD, which has been widely used in music auto-tagging evaluations [11], contains the metadata and pre-computed audio features of one million contemporary songs. Our In-House dataset includes about 800K songs, where the era distribution is presented in Fig. 2. For artist biography, we adopted the AllMusic dataset [33], which covers the biography text data of about 200K artists. We created the audio-text input pairs by matching the artist names. We selected 85,475 tracks from MSD for the experiments, considering the availability of audio features, release years, and artist biographies. Whereas, the full set of In-House was used, since it covers all the required information.\nThe difference in instrumentation and genre of songs released in close years can be subtle. Therefore, we follow similar works [34, 35] to propose a metric that allows false prediction tolerance. We define the accuracy with \u00b1x years of tolerance ($ACC_x$) as follows:\n$$ACC_x = \\frac{1}{N} \\sum_{i=1}^N \\frac{x - \\min[abs(y_i - \\hat{y_i}), x]}{x},$$\nwhere $y_i$ and $\\hat{y_i}$ represent the ground truth and predicted years of song i, and N is the total number of test songs. For instance, $ACC_0$ counts only the cases when the prediction exactly matches the ground truth. We define two evaluation scenarios of granularity for year ranges: (i) one year per class: each year represents a class. For example, songs released in 1981 are assigned with a different class than songs released in 1982. (ii) ten years per class: each class represents a decade (e.g., 60s, 80s, and 00s). Therefore, songs released in 1981 and 1982 are within the 1980s' class, while the song in 2011 is labeled with '2010s' class.\nTraining Details. To extract the input mel-spectrograms, we first resample the original audio waveform to 22,050 Hz. Then, we use a window size of 2,048 with a hop size of 512 for a frame, and transform each frame into a 224-band mel-scaled magnitude spectrum. In the training stage, we set a batch size of 64. Each audio example, which includes 1,024 frames (roughly 6-seconds long), is randomly sampled from an arbitrary song in the training data. We use the Adam optimizer [36] with a learning rate of 1e-4."}, {"title": "3.2. Results and Discussion", "content": "We performed 10-fold cross-validation to obtain the results, and this process was done on MSD and In-House independently. We split 10% of the training set as the validation set, and selected the snapshot with best performance on validation set as the testing model.\nEra Prediction Comparison. Table 1 presents the overall comparison of the two datasets. It is clear that incorporating the supervised contrastive learning (Audio-SUC) can significantly outperform the baseline model (Audio-CNN) for most of the cases (except the ACC2 in the 8 classes scenario on MSD), and that jointing the audio and artist information for input with the multi-modal contrastive learning (i.e., AudioArt-MMC) can consistently further the performance. Such a result is in line with our expectations when designing the models. As suggested in prior work [5], artist information is very helpful in hit song prediction of a period of time, so it also offers strong clues about the musical sounds representing an era. Comparing among ACC values with different tolerances for the 64 classes scenario, the superiority of the contrastive learning-based models over CNN seems to become more apparent when the tolerance is getting larger. This indicates that the learned features indeed possess abilities for era discrimination. Lastly, for the 8 classes scenario (i.e., decade tagging), ACC1 can achieve 90%+ accuracy solely based on audio, demonstrating promising results for a usable system in relevant MIR applications.\nRobustness to Data Imbalance. As discussed, our dataset is imbalanced in era distribution. We are interested in the performance of the pre-2000 era, so we present the result of the corresponding subset in In-House. Table 2 shows the results on the pre-2000 subset and the difference $\\Delta$ from their counterparts in Table 1. Note that smaller $\\Delta$ is better. Comparing the $\\Delta$ values among the three variants, AudioArt-MMC shows significantly smaller performance gaps between the full-set and subset results, indicating its stronger ability to handle the imbalanced data.\nVisualization of Embeddings. Finally, we perform a qualitative examination of the effect of the proposed MMC loss by visualizing the embeddings of the penultimate layer from the three proposed models using t-SNE plots. We randomly select 400 songs associated with 13 artists and plot the corresponding song embeddings in the 2D space. Each artist is assigned a unique color, with brighter colors for recenter eras. As displayed in Fig. 4, EC loss can facilitate the embeddings of same era class to be closer while maintaining the diversity of different songs (see Fig. 4(b)), and MMC loss further forces aggregating songs of the same artist (see Fig. 4(c))."}, {"title": "4. CONCLUSION", "content": "In this paper, we have presented a novel multi-modal contrastive learning framework to recognize music era based on audio and artist information. The design of the EC and MMC losses enables a satisfactory clustering behaviors, showing convincing results for the task. For future work, we will try to improve the performance with more sophisticated solutions. We also plan to study the possibility of including additional metadata, such as instrumentation, genre, and mood, into a unified framework."}]}