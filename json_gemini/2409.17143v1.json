{"title": "Attention Prompting on Image for Large Vision-Language Models", "authors": ["Runpeng Yu", "Weihao Yu", "Xinchao Wang"], "abstract": "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision-language tasks. Motivated by text prompting in LLMs, visual prompting has been explored to enhance LVLMs' capabilities of perceiving visual information. However, previous visual prompting techniques solely process visual inputs without considering text queries, limiting the models' ability to follow text instructions to complete tasks. To fill this gap, in this work, we propose a new prompting technique named Attention Prompting on Image (API), which just simply overlays a text-query-guided attention heatmap on the original input image and effectively enhances LVLM on various tasks. Specifically, we generate an attention heatmap for the input image dependent on the text query with an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel values of the original image to obtain the actual input image for the LVLM. Extensive experiments on various vison-language benchmarks verify the effectiveness of our technique. For example, API improves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks, respectively.", "sections": [{"title": "1 Introduction", "content": "Benefiting from the great progress of Large Language Models (LLMs) [1,53,54], Large Vision-Language Models (LVLMs) [2,4,9,18,26,32,66,67,81] also advances rapidly, represented by the seminal works GPT-4V [66] and LLaVA [32]. They have been widely applied in tasks that involve understanding both visual and linguistic information, such as referring segmentation [72, 73], localization [72], captioning [55], open world 2D/3D understanding [52,57,66,82], and image editing [63,66].\nTo enhance the performance of LVLMs, an economical method is to develop prompting techniques to elicit the models' potential. Similar to textual prompting [24, 61], visual prompting [64,65] is a technique that enhances a model's understanding of images by directly adding annotations such as masks, circles, and marks to the image. This technique provides clear hints for visual perception by highlighting areas relevant to solving the problem, guiding the model's attention to specific parts of the image, thus mitigating issues arising from complex scenes with distractions. It has been demonstrated that even simple visual cues like circles [48], arrows [66], or image tiling [30] can improve LVLMs' ability to extract the required information correctly. Unlike methods that improve LVLM performance through adaptation or fine-tuning, visual prompting does not require the training process, thereby reducing the risks of overfitting and"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Visual Prompting for LVLM", "content": "Originating from language models [33,34,44], the concept of prompting has been widely applied in vision models and vision language models to enhance the transfer learning and adaptation for various tasks (e.g., classification [21,41,77,79,80],"}, {"title": "2.2 Self-Reflection and Ensemble", "content": "Our method involves LVLM at two stages: once for generating visual prompts and once for performing inference. When the same LVLM is used at both stages, our approach can be seen as a method to enhance LVLM performance using self-reflection technology. The concept of Self-Reflection originated from LLMs [38, 47] but can be directly transferred to LVLMs. Self-Reflection scheme improves model performance by repeatedly answering a query and iteratively updating the answer. The Self-Reflection process involves using self-evaluation [3], self-checking [36], self-feedback [35], feedback from the external environment [7,46], and even previous answers themselves [76] as hints to input into the model for"}, {"title": "3 Method", "content": "Large Vision Language Model \\( f \\) takes an image \\( I \\in \\mathbb{R}^{H\\times W\\times 3} \\) and a text query \\( T^{i} \\) as inputs, generating an output text \\( T^{o} = f(I,T^{i}) \\). During the inference process using API, instead of being directly fed into \\( f \\), the original image \\( I \\) undergoes an additional annotation operation \\( A \\), resulting in an image \\( I^{a} = A(I, T^{i}) \\) that has been overlaid with a heatmap \\( \\Phi \\). Subsequently, the annotated image \\( I^{a} \\) and the original query are input into the LVLM model \\( f \\), producing the output \\( T^{o} = f(I^{a},T^{i}) \\). The overall framework of the method is shown in Fig. 1.\nIn our method, the annotation process comprises two steps. The first step involves using an auxiliary LVLM model \\( g \\) to establish an initial attribution map \\( \\Psi \\) between the text query \\( T^{i} \\) and each patch of the image. This attribution map indicates which patches in the image are more relevant to \\( T^{i} \\) or which patches should be paid more attention to for answering \\( T^{i} \\). In our method, there are no additional constraints on the LVLM \\( g \\); if the inference LVLM \\( f \\) is accessible and capable of performing the annotation operation \\( A \\), then the LVLM \\( g \\) used to generate the attribution map can be the same as \\( f \\), i.e., \\( g = f \\). Alternatively, \\( g \\) could be a different LVLM to introduce knowledge from other models to enhance \\( f' \\)s functionality, i.e., \\( g\\neq f \\). Moreover, due to the diversity of LVLM models, we do not necessarily use the attention map as our attribution map. For example, for the image-text matching model, experiments have shown that using the attention map as the attribution map has suboptimal results. After obtaining the attribution map \\( \\Psi \\), the second step in the annotation process is to convert it into a suitable \\( \\Phi \\) and apply it to the original image using alpha blending.\nVarious LVLM models can be utilized to generate attribution maps. We discuss two prevalent and representative LVLM models: CLIP [40], exemplifying image-text matching models, and LLaVA [32], representing vision-language-input text generation models."}, {"title": "3.1 Obtaining Attribution Map from CLIP", "content": "The CLIP model, \\( g_{clip} \\), consists of an image encoder and a text encoder, calculating the similarity between an image and a text query in the image-language latent space, \\( sim(\\hat{I}, \\hat{T}) \\), where \\( \\hat{I} = g_{img}(I) \\) and \\( \\hat{T} = g_{txt}(T) \\). This similarity measure evaluates the correlation between the entire image and the query. To obtain the attribution map value from the text query to each image patch, we decompose the output image-level similarity \\( \\hat{I} \\) and then calculate the similarity of each patch's output with the \\( \\hat{T} \\).\nThe decomposition process is as follows. Due to the presence of residual connections, the final output of the vision tower, \\( \\hat{I} \\), actually includes influences from each layer. Consequently, \\( \\hat{I} \\) can be expressed as a linear combination of the values at the class token positions from each layer\n\\[\\hat{I} = L(\\sum_{l=1}^{L} [z^{l}]_{cls}) + L(\\sum_{l=1}^{L} [MSA'(z^{l-1})]_{cls}) + L(\\sum_{l=1}^{L} [MLP'(z^{l})]_{cls}),\\]\nwhere L denotes the number of transformer layers within the vision encoder, with MSA and MLP representing the Multihead Self-Attention structure and the Multi-Layer Perceptron structure within the transformer, respectively; L represents the linear transformation that includes the fully-connected layer and the normalization operations performed after the transformer structure, before calculating the similarity score; \\( z^{l} \\) signifies the input token sequence for the l-th transformer layer; and \\( [z]_{cls} \\) indicates the value of the cls token within the token sequence Z. These output cls tokens are aggregated through residual connections to form the output of the vision encoder. As evidenced in [17,32], among these summation terms, the outputs of the last few layers of MSA play a decisive role, while the contributions from the outputs of the shallow MSA layers, the outputs of MLP, and the \\( z_{os} \\) term, which is independent of the input image, can be considered negligible to the final measurement of similarity. Therefore, the similarity \\( sim(\\hat{I}, \\hat{T}) \\) can effectively be approximated by calculating the similarity between \\( \\hat{T} \\) and the aggregated outputs of MSAs in the deeper layers :\n\\[sim(\\hat{I}, \\hat{T}) \\approx sim(\\sum_{l=L'}^{L} L([MSA'(z^{l-1})]_{cls}), \\hat{T}),\\]\nwhere L' represents a predefined starting layer index. To further calculate the attribution of the text query to each patch, inspired by [17], we unfold the operations of the Multihead Self-Attention, obtaining\n\\[[MSA'(z^{l-1})]_{cls} = \\sum_{h}^{H} [A_{cls,t}^{(l,h)}V_{t,:}^{(l,h)}W^{(l,h)}] + B^{(l)}]_{cls}\\]\n\\[ = \\sum_{h} [A_{cls,t}^{(l,h)} (\\sum_{t=1}^{T} V_{t,:}^{(l,h)})  (\\frac{1}{HT}) + B^{(l)}]_{cls}\\]\nwhere \\( A^{(l,h)} \\), \\( V^{(l,h)} \\) are the attention map and the value matrix in the l-th layer corresponding to the h-th head, respectively; \\( W^{(l,h)} \\) is the"}, {"title": "3.2 Obtaining Attribution Map from LLaVA", "content": "The LLaVA model is an auto-regressive vision-language-input text generation model that utilizes Multihead Self-Attention to extract information from text queries and image patches, predicting the following tokens. Given a text token sequence of length N, \\( Z^{text} = {Z_{t}^{text}}_{t=1}^{N} \\), and an image token sequence of length \\( P \\times P \\), \\( Z^{img} = {Z_{i,j}^{img}}_{i,j=1}^{P \\times P} \\), LLaVA generates a new token sequence of length M, \\( Z^{out} = {Z_{m}^{out}}_{m=1}^{M} \\). We directly use the attention weight between token \\( Z^{out} \\) and each image token as \\( Z^{out} \\)'s attribution to that image patch. Similar to the strategy for the CLIP model, we select attention maps from the deeper layer to extract attention weights. The final attribution map is averaged over the entire generated token sequence and all attention heads. Formally, the attribution map \\( \\Psi \\) is defined as\n\\[\\Psi_{ij} =  \\frac{1}{MH}\\sum_{m=1}^{M} \\sum_{h=1}^{H} A_{m,t}^{(L,h)},\\]\nwhere t = j + P * (i - 1).\nIn the definition, \\( A^{(L,h)} \\) is again the attention map in the L-th layer corresponding to the h-th head, where L is a set to be a hyper-parameter; for notation simplicity, \\( A^{(L,h)} \\) here is a submatrix of the entire attention map and only includes cross attention between \\( Z^{out} \\) and \\( Z^{img} \\); \\( A_{m,t}^{(L)} \\) still denotes the attention value from the m-th token to the t-th token."}, {"title": "3.3 From Token Space to Pixel Space", "content": "The attribution map \\( \\Psi \\in \\mathbb{R}^{P\\times P} \\) is generated in the token space. We first resize it back to the pixel space to obtain the raw heatmap \\(Resize(\\Psi)\\). Due to the square shape of the patches, the mask pattern in \\( \\Psi \\) also appears rectangular. To mitigate the issue that the rectangular mask pattern does not align with the object's irregular shape, we apply a mean filter to obtain the final heatmap \\( \\Phi = Meank(\\Psi) \\), where k is the kernel size of the filter. The final heatmap is then overlaid on the original image by using it as the alpha channel, resulting in the final image after annotation \\( I^{a} \\)."}, {"title": "4 Experiments", "content": "We show the main experimental results in this section. More experiments and implementation details are in the appendix."}, {"title": "4.1 Comprehensive VQA Tasks", "content": "Datasets. Experiments are conducted on 6 datasets: VisWiz [5], TextVQA [51], MMMU [70], MME [15], MM-Vet [69], and LLaVA-Bench [32]. The performance on the first four datasets is evaluated using matching accuracy with the ground truth response. The performance of the latter two datasets is measured using the GPT-based evaluation scores.\nLVLMs. Experiments are conducted using two open-source models: CogVLM [56] and LLaVA [31], and two commercial models: GPT-4V [66] and Gemini [52]. Due to GPT-4V's token limit, following the experiment protocol in the previous work [64] when conducting experiments with GPT-4V, for VisWiz, TextVQA, and MMMU, we randomly selected 200 images from the dataset to verify our method. Because, about 50 questions on MM-Vet are categorised as related to personal identification or brand evaluation due to GPT-4V's safety policy and are refused to answers. Therefore, we evaluated our method's performance only on the remaining questions.\nComparison. We compare with the following methods: (1) naively feeding the query and image to the model without any prompt; (2) using \"Let's think step"}, {"title": "4.2 Ablation Studies", "content": "We identify three important factors affecting the performance of our method and conduct ablation studies on them.\nThe Power of the Auxiliary Model. On the MMMU and MME datasets, we used CLIP models and LLaVA models of different scales to generate heatmaps, with LLaVA serving as the inference model, to compare performance. The results are shown in Tab. 2. As the scale of the auxiliary model increased, the performance of our method also improved. Both increasing the depth of the auxiliary"}, {"title": "4.3 Self-Reflection", "content": "When the auxiliary LVLM and the inference LVLM are the same, our method can be seen as having a two-round chat with the LVLM. The first round generates an annotated image, where the highlighted areas represent what the LVLM considers important, embedding the LVLM's process of extracting visual information. The second round conducts inference based on the generated annotated image, allowing the LVLM to perform Self-Reflection and refine its previous process of visual information extraction. Unlike previous Self-Reflection methods using text as a medium in LLMs, under the API framework, all information related to the first answer is stored in the annotated image, and the text response from the first round is not provided to the model in the second round.\nAs a new perspective of Self-Reflection, we explore two questions: (1) Can visual mediums also achieve effective Self-Reflection? To answer this, we compared text-based Self-Reflection and our method using LLaVA as the inference model on the LLaVA-Bench dataset. The results in Tab. 5 show that our method achieves better performance than text-based Self-Reflection, proving that visual mediums can effectively facilitate Self-Reflection.\nThe second question is: (2) Can we more effectively utilize visual mediums for Self-Reflection? Generally, Self-Reflection techniques involve two steps int the second round: first, evaluating the previous answer, and second, combining the evaluation to re-answer the question. However, in our framework, the evaluation process is not included, and the model directly proceeds to inference. Therefore, we designed a new inference process. We input the annotated image and the question into the VLM, prompting it to judge whether the highlighted areas in the image support the answer to the question. If yes, the answer is generated using the annotated image; if not, the answer is generated using the original image. The result (the last row of Tab. 5) shows that this strategy further improves our method. Conversely, when we do not allow the model to perform evaluation and emphasize that the answer lies within the highlighted areas of the annotated image, performance decreases (second to last row in Tab. 5). This also proves the importance and effectiveness of the evaluation process when using visual mediums for Self-Reflection."}, {"title": "4.4 Other Discussion", "content": "Hallucination. We also explore our method's ability to assist LVLM in overcoming hallucinations. We conduct two experiments. First, on VisWiz, we calculated the accuracy with which our method and the baseline identify the unanswerable"}, {"title": "5 Conclusion", "content": "In this work, we introduce a novel visual prompting technique called Attention Prompting on Image (API), which incorporates an auxiliary LVLM to generate an attention heatmap on the image dependent on text query. Our extensive experiments demonstrate the advantages of our prompting method for different LVLMS on various benchmarks. Additionally, our approach offers new insights into using visual signals for LVLM ensembling and LVLM self-reflection."}, {"title": "8 Observation and Discussion of API Method", "content": null}, {"title": "8.1 CLS Token Similarity and Non-CLS Token Similarity", "content": "To extract heatmaps from the CLIP model, we designed two complementary types of attribution maps: one based on the decomposition of similarity between the feature of the CLS token and text feature, and the other measuring the similarity between the feature of the Non-CLS tokens and text feature. Fig. 12 compares the differences in functionality between these two types of attribution maps. The third row in the image shows the heatmap generated solely based on cls and its resulting annotated image. The fourth row shows the heatmap obtained solely from comp. Firstly, we can observe that when the query changes, cls can highlight different parts of the image corresponding to different queries. It selects the areas where the blanket and computer are located based on the query. However, comp does not show significant differences in response patterns to different queries. On the other hand, \u040fcomp can filter out the background of the image, leaving the objects, which potentially can be used in the process of VQA.\nFor instance, when the query explicitly mentions \u201ccomputer\u201d, els completely ignores the chair and blanket in the lower left corner, but comp still assigns high values to these areas. Therefore, we combine cls and comp to form a complete attribution map."}, {"title": "8.2 Attribution Map Aggregation for CLIP Model", "content": "First, Eq. (7) in the maintext can be rewritten as 1\u2212(1\u2212cls)(1\u2212comp), where since Icls and comp are cosine similarities, both (1 \u2013 cls) and (1 \u2013 comp) range between 0 and 1. Thus, the final mask is related to the product of the two parts, (1 \u2013 cls) and (1 \u2013 \u040fcomp). If Icls and comp are considered binary, then (1-cls)(1-comp) can be approximated as an OR operation between (1\u22124cls) and (1 \u2013 \u0441\u043e\u0442\u0440). That is, when either (1 \u2013 cls) or (1 \u2013 comp) is 0, the equation will be 1, and only when both are 1, the equation will be 0. This means that for patch i, as long as either attribution map els or comp highlights this patch, the final attribution map will also highlight this patch. Only when both cls and comp consider patch i unimportant, the final attribution map will ignore this patch.\nExperimental findings, as shown in Fig. 12, indicate that, on one hand, \u045f\u0441\u043e\u0442\u0440 can indiscriminately choose all entities, whereas \u0113cls selects entities explicitly mentioned in the query. The highlighted area in els can be understood as a subset of the highlighted area in comp. On the other hand, both cls and comp will ignore non-informative parts of the image. Therefore, in actual non-binary cases, the computation of Eq. (7) can be described as an algorithm: first, apply a mask to non-informative areas (i.e., instruct the LVLM to ignore these patches) because these patches will not be selected by either cls or \u040fcomp. For the remaining areas, which are patches with objects directly mentioned in the query or other entities potentially related to the query, a multiplication of cls and comp further highlights the patches with objects appearing in the query because they have greater weight in cls."}, {"title": "9 More Experimental Results and Implementation Details", "content": null}, {"title": "9.1 Ensemble", "content": "When the auxiliary LVLM and the LVLM used for inference are different, our approach can be seen as ensembling the knowledge of the auxiliary LVLM into the LVLM used for inference through visual prompts. Under this definition, baseline methods like FGVP and SoM can also be considered a form of ensemble, not between LVLMs but between a vision model (segmentation model) and an LVLM. From the experimental results, our method is the first effective ensemble method that is based on visual prompting in a VQA context."}, {"title": "9.2 Influence on Different VQA Abilities", "content": "To thoroughly understand the impact of our method on various capabilities of LVLMS, we report the performance changes across different specific abilities on the MM-Vet dataset using the CogVLM model as the inference model and CLIP as the mask model. The results are shown in Tab. 10. It is observed that our method enhances all categories of capabilities in the MM-Vet dataset. Notably, our method is particularly beneficial for OCR and Math abilities. The significant improvement in OCR capability is attributed to our method's highlighting of relevant areas, allowing the model to focus only on regions related to answering the question. This narrows down the scope of the OCR task, thereby enhancing OCR performance. Consequently, the improvement in mathematical ability is closely linked to the enhancement in OCR capability. Since addressing math-related questions in images first requires performing OCR tasks, the improvement in OCR also contributes to the enhancement of mathematical abilities."}, {"title": "9.3 Implementation Details", "content": "Pre-trained weight and API. During the mask generation phase, we used the CLIP-ViT-L-336 model [40] released by OpenAI and the LLaVA-1.5-13B model [31]. In the inference process, we utilized the released weight of LLaVA-1.5-13B model [31] and cogvlm-chat-v1.1 model [56]. We use the \"gpt-4-1106-vision-preview\u201d and \"gemini-pro-vision\u201d models for GPT-4V [66] and Gemini [52] API, respectively. All local experiments were deployed on a single A100 GPU.\nQuery GPT-4V and Gemini. For GPT-4V and Gemini, we used python APIs for batch querying. When encountering errors due to server or network issues, we"}, {"title": "10 Limitation, Future Direction, and Potential Impact", "content": "Limitation and future direction. An essential component of this work is the extraction of attribution maps based on an auxiliary LVLM. The introduction of an auxiliary LVLM enhances the performance of visual prompting methods but also introduces some limitations and new research opportunities. First, generating visual prompts based on an LVLM incurs additional computational costs,"}]}