{"title": "Slowing Down Forgetting in Continual Learning", "authors": ["Pascal Janetzky", "Tobias Schlagenhauf", "Stefan Feuerriegel"], "abstract": "A common challenge in continual learning (CL) is catastrophic forgetting, where the performance on old tasks drops after new, additional tasks are learned. In this paper, we propose a novel framework called ReCL to slow down forgetting in CL. Our framework exploits an implicit bias of gradient-based neural networks due to which these converge to margin maximization points. Such convergence points allow us to reconstruct old data from previous tasks, which we then combine with the current training data. Our framework is flexible and can be applied on top of existing, state-of-the-art CL methods to slow down forgetting. We further demonstrate the performance gain from our framework across a large series of experiments, including different CL scenarios (class incremental, domain incremental, task incremental learning) different datasets (MNIST, CIFAR10), and different network architectures. Across all experiments, we find large performance gains through ReCL. To the best of our knowledge, our framework is the first to address catastrophic forgetting by leveraging models in CL as their own memory buffers.", "sections": [{"title": "1. Introduction", "content": "Continual learning (CL) is a paradigm in machine learning where models are trained continuously to adapt to new data without forgetting previously learned information [46, 67]. This is relevant for real-world applications such as autonomous driving [54], medical devices [64], predictive maintenance [21], and robotics [59]. In these applications, the entire data distribution cannot be sampled prior to model training and (fully) storing the arriving data is not possible. The main challenge of CL is catastrophic forgetting [38]. Catastrophic forgetting refers to problems where the performance on old data drops as new data is learned. Catastrophic forgetting typically happens because prior knowledge is encoded in the combination of the model parameters, yet updating these model parameters during training on new data leads to a change in knowledge and can thus cause the model to forget what it has learned before.\nTo slow down forgetting, numerous methods have been proposed [13, 24, 26, 30, 33, 36, 37, 50, 51, 66, 73]. Existing methods can be categorized into two main groups [15, 52, 67]: (1) Memory-based methods rely on external memory to either store samples from old tasks [e.g., 49, 50] or store separately trained generative networks for generating old data on demand [e.g., 41, 56]. However, this is often not practical due to the fact that CL is often used because streaming data becomes too large and cannot be stored. (2) Memory-free methods change the learning objective, which may help to slow down forgetting at the cost of learning new concepts more slowly [e.g., 15, 26, 52]. Here, we contribute a different, orthogonal strategy in which we propose to leverage the trained model as its own memory buffer.\nIn this paper, we develop a novel framework, called Reconstruction from Continual Learning (ReCL), to slow down forgetting in CL. Different from existing works, our framework leverages the implicit bias of gradient-based neural network training, which causes convergence to margin maximization points. Such convergence points essentially memorize old training data in the model weights, which allows us then to perform a dataset reconstruction attack. We then integrate this reconstruction attack into CL, where we minimize a data reconstruction loss to recover the training samples of old tasks. Crucially, our framework is flexible and can be used on top of existing CL methods to slow down forgetting and thus improve performance.\nWe evaluate the effectiveness of our framework across a wide range of experiments. (1) We show the performance gains from our framework across three standard CL scenarios:"}, {"title": "2. Related Work", "content": "Continual learning: CL is a machine learning paradigm where models are trained on newly-arriving data that cannot be stored [67]. In the literature, there are three common scenarios of how the underlying tasks change over time: [20, 61, 62]. (1) In class incremental learning (CIL), a model is trained on new classes that arrive sequentially. Here, each task has a unique label space, and the model's output layer is expanded to incorporate the new classes. (2) In domain incremental learning (DIL), the size of the output layer is fixed, and each task uses the same label space. However, the underlying data that correspond to the labels are no longer fixed but can change over time (e.g., indoor images of cats \u2192 outdoor images of cats). (3) In task incremental learning (TIL), a shared model backbone is used by all tasks. Here, new tasks are introduced over time but each task is added as a separate output head. Later, we apply our framework to all three CL scenarios (CIL, DIL, and TIL) and show that it achieves consistent performance gains.\nForgetting: A main challenge in CL is forgetting, also named catastrophic forgetting or catastrophic inference [38, 67]. The problem in forgetting is that, as the model parameters are updated in response to new data, important weights for older tasks are altered and may cause a drop in performance for older tasks [37, 52]. Note that forgetting is a problematic issue in all of the three CL scenarios, namely, CIL, DIL, and TIL [67]. One may think that a na\u00efve\nworkaround is to restrict the magnitude of how much model parameters can be modified during training; yet, this also restricts how new tasks are learned and essentially leads to a stability-plasticity tradeoff [67]. Hence, such a na\u00efve workaround is undesired.\nTo reduce forgetting, several methods have been proposed, which can be grouped by whether an external memory is used (see Appendix A for a detailed review). (1) Memory-based methods rely on external memory to either store samples from old tasks [e.g., 49, 50] or store separately trained generative networks for generating old data on demand [e.g., 41, 56]. A prominent example is experience replay (ER) where samples of old data are replayed from a small memory buffer [50]. (2) Memory-free approaches do not store old data and can be further grouped into two subcategories. (i) One stream adopts architecture-based approaches, which alter the model architecture to more flexibly accommodate new tasks (e.g., by adding task-specific modules [25, 51, 72]. Oftentimes, these approaches avoid forgetting altogether (by freezing old modules), but at the cost of exponentially growing network sizes. This approach is thus mainly relevant for specific CL scenarios such as TIL, because of which methods from this subcategory are generally not applicable as baselines. (ii) Another stream alters the objective function through regularization-based approaches [26, 52, 75], so that weights are updated based on their importance for old tasks. Yet, this typically also interferes with learning from new data. Prominent examples are: Elastic weight consolidation (EWC) [26], which uses the Fisher information matrix to assess parameter importance and to regularize gradient updates to parameters. Utility-perturbed gradient descent (UPGD) [15] measures importance by computing the performance difference between unmodified and perturbed weights, and uses this per-parameter utility to control plasticity by scaling a noised gradient.\nOur proposed framework is different to both memory-based and memory-free approaches. (1) We do not store data of old tasks, and we also do not train or keep additional generative networks due to the additional overheads for a memory buffer and the challenges of training generative models, respectively. In practice, storing old data is often even prohibited for legal reasons.2 Rather, we reconstruct the old training data from the model itself. (2) We do not meddle with the network architecture, and we also do not alter the learning objective, as this typically comes at the cost of learning new tasks more slowly. Rather, we keep the network as-is and combine the reconstructed training data with the current task's data. Further, our proposed"}, {"title": "3. Problem Statement", "content": "We consider the following, standard setup for CL, where data (e.g., images) arrive sequentially and where a model is updated continually to learn from a series of tasks [44, 51, 52, 73].\nTasks: Each task \u03c4\u2208 {1,2,...,T} has its own data (sub-)set $D_{\\tau} = {(x_i, y_i)}_{i=1}^{n} \\subseteq \\mathbb{R}^d \\times C$ with features $x_i$ and corresponding labels $y_i$ from classes $C = {1, ..., C'}$. We use the term 'task' or 'time' interchangeably when referring to \u03c4. Note that the data cannot be stored, meaning that, at time T, the current dataset $D_{\\tau}$ with (x, y) is only available for training the current task but cannot be stored. For each task, we have separate train and test split given by $D^{tr}_{\\tau}$ and $D^{te}_{\\tau}$, respectively.\nNetwork \u03a6: We train a homogenous\u00b3 neural network \u03a6(\u03b8,\u00b7), which consists of a feature extraction part $\u03a6_f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^f$ and classification head $\u03a6_h: \\mathbb{R}^f \\rightarrow C$ (typically, a single dense layer). Together, both $\u03a6_f$ and $\u03a6_h$ implement an input-output mapping \u03a6(\u03b8,\u00b7) : ($\u03a6_f \\circ \u03a6_h$) = $R^d \\rightarrow C$. We denote the network and its parameters at time \u03c4 by $\u03a6_{\\tau}$. Note that the class of homogenous neural networks is very broad. For example, it includes all vanilla MLPs and CNNS without components introducing \"jumps\" into the forward pass (such as, e.g., bias vectors or dropout [58]).\nThe above scenarios vary in terms of difficulty (see Table 1). CIL is often considered to be the most challenging scenario [cf. 20, 62]. TIL uses task-specific heads, which gives each task a separate, non-shared parameter space. Hence, this scenario is thus considered to be easier than the other two scenarios, and forgetting is often prevented at the cost of growing network architectures [cf. 20, 62]. Metrics: We measure forgetting using the following standard metrics: (i) average accuracy (ACC) [33] and (ii) backwards transfer (BWT) [33]. ACC captures the accuracy on all tasks up to task T via\nACC = $\\frac{1}{T} \\sum_{\\tau=1}^{T} acc_{\\tau,T}$,\nwhere $acc_{\\tau,T}$ is the classification accuracy on task \u03c4 after"}, {"title": "4. Proposed ReCL Framework", "content": "In this section, we describe the components of our proposed framework for slowing down forgetting by dynamically extracting the training data of old tasks.\nOverview: An overview of our ReCL framework is shown in Figure 1. Our framework performs the following two steps upon arrival of a new task T. Step 1: Reconstructing the training data from $\u03a6_{T\u22121}$, thus generating the reconstructed data $D^<_{T}=(x^<,\u1ef9<^)$. Step 2: Combining the reconstructed data of previous tasks with the current data (x, y) and then performing further training of (\u03b8,\u00b7). Note that both steps are independent of the underlying CL methods, meaning that our framework can be applied on top of any existing CL method to slow down forgetting.\nLeveraging the implicit bias of convergence to margin maximization points: Our ReCL framework exploits the implicit bias of gradient-based neural network training, which causes neural network weights to converge to margin maximization points [23, 34]. These convergence points are characterized by several conditions [8, 34, 74] which, among others, state that the network weights @ can be approximated by a linear combination of the network's gradients on data points xi. We utilize this finding and optimize randomly initialized xi to closely resemble the training data of previous tasks. We later achieve this by minimizing a tailored reconstruction loss."}, {"title": "4.1. Step 1: Data Reconstruction for CL", "content": "For data reconstruction, we perform two sub-steps: (1) We first randomly initialize a set of m reconstruction candidates Xi, and (2) we then optimize the candidates to satisfy our objective of reconstructing the training data from old tasks. Both are as follows:\nFor sub-step (1), we simply initialize m candidates $x_i$ ~ N(0, \u03c3\u00b2I) and set fixed labels evenly split across the number of classes. That is, $y_i \u2190 U(C)$.\nFor sub-step (2), we then optimize the xi to resemble old data by minimizing a combination of three losses over $n_{rec}$ reconstruction epochs. The losses are as follows: (i) the reconstruction loss $L_{rec}$, (ii) the lambda loss $L_\u03bb$, and (iii) the prior loss $L_{prior}$. Here, the reconstruction loss in (i) denotes our central objective: it optimizes the candidates to reconstruct old data. However, to ensure the theoretical foundation of the implicit bias, we must ensure that the network weights can be approximated by a linear combination, for which we need the lambda loss in (ii), which enforces a constraint on the coefficients of the linear combination. To incorporate prior domain knowledge about the reconstructed data (such as having knowledge about its value range), we need the prior loss in (iii). It forces the candidates to lie\nwithin (normalized) value ranges. The formal definitions of the different losses are below:\n(i) Reconstruction loss: The reconstruction loss $L_{rec}$ optimizes the m randomly initialized candidate samples xi. Inspired by Buzaglo et al. [8], we define it as\n$L_{rec}(x_1,..., x_m, \u03bb_1,..., \u03bb_m) =  \\sum_{i=1}^{m}  \u03bb_i[\u03b8_{T-1} - \u03a6_{T-1}(\u03b8_{f\u22121}; x_i)]^2$\nwhere $||\\cdot||^2$ denotes the squared L2 norm (i.e., squared Euclidean distance), xi are reconstruction candidates, $\u03bb_i$ are learnable scaling coefficients. The number of reconstruction candidates, m, is a hyperparameter.\n(ii) Lambda loss: The lambda loss $L_\u03bb$ constrains the scaling coefficients $\u03bb_i$. It is defined as\n$L_\u03bb = \\sum_{i=1}^{m} max \\{-\u03bb_i, -\u03bb_{min}\\}$,\nwhere $\u03bb_{min}$ is a hyperparameter. Note that the $\u03bb_i$ are not part of the old task's training data, but are necessary for the underlying theory to hold [17] and thus to solve the reconstruction problem defined by Equation (3). We provide further details in Appendix B.2.\n(iii) Prior loss: The prior loss $L_{prior}$ constrains the value range of the (reconstructed) images to lie within normalized range -1 to 1. Generally, this loss is used to encode prior background/domain knowledge about the reconstruction data. Here, we follow [17] and use it to restrict that the values of the reconstructed data are in a certain range. Later, in our experiments, we work with images and thus enforce normalization to [-1,1] by\n$L_{prior} = \\sum_{i=1}^{m} \\sum_{k=1}^{d} max {max{x_{i,k}-1, 0}, max{-x_{i,k}-1, 0}}$,\nwhere d is the dimensionality of the features xi.\nOverall loss: For each task, we minimize the following overall loss\n$L_{full} = L_{rec} + L_\u03bb + L_{prior}$"}, {"title": "4.2. Step 2: Combining the Reconstructed Data for Training", "content": "Step 2 now leverages the reconstructed data, \u010e\u2081, when training for the new task \u03c4. Here, the exact steps vary depending on the underlying CL scenario."}, {"title": "4.3. In-Training Optimization of Reconstruction Hyperparameters", "content": "Our ReCL consists of six hyperparameters, which are primarily used in Equation (3) to Equation (6). The hyperparameters are: (i) Amin constrains the scaling effect of di; (ii) \u03c3\u03b5 controls the initialization of the xi from a Gaussian distribution; (iii) lrx and (iv) lrx are the learning rates for the SGD optimizers of xi and \u5165\u2081, respectively; (v) nrec is the number of reconstruction epochs; and (vi) m is the number of reconstruction candidates.\nThe hyperparameters Amin,x,lrx and lrx are tuned in-training at each task T. We use three strategies: (1) In the naive strategy, we perform no hyperparameter optimization and simply run nrec reconstruction epochs with uninformed default hyperparameters. We select this strategy as our reference when we refer to our ReCL, and consider the following other two strategies as ablations. (2) In the unsupervised strategy, we use a Bayesian search at each task to optimize the hyperparameters by minimizing the Equation (6), where each trial runs for nrec epochs. (3) In the supervised strategy, we use reference data of old tasks stored in a memory buffer to guide the optimization process. We then maximize the structural similarity index measure (SSIM) [68] between reconstructed data and real reference data. To measure the similarity among images, we follow Buzaglo et al. [8] and, for each real sample, find the nearest generated neighbour using the L2 distance and then compute the SSIM between closest pairs. Details are in Appendix D.5. As before, we run ntrials parameter trials at each task.\nNote that hyperparameters nrec and m are not tuned during training but set beforehand. We analyse their influence on ReCL in sensitivity studies."}, {"title": "5. Experimental Setup", "content": "We adopt a wide range of CL scenarios (i.e., CIL, DIL, and TIL), datasets, and baselines analogous to prior literature [e.g., 13, 15, 26, 53, 62]. We later report ACC and BWT (see Section 3) over 5 runs with independent seeds, where, in each run, we randomly distribute the classes into tasks."}, {"title": "6. Results", "content": ""}, {"title": "6.4. Further Insights", "content": "Can ReCL slow down forgetting even for non-homogenous networks? For this, we use ReCL and now train AlexNet [27] and ResNet [19] in the DIL scenario. Both architectures are not homogenous neural networks because they use modules that introduce \u201cjumps\" into the propagation of hidden activations. AlexNet uses max-pooling, dropout [58] and bias vectors, and ResNet uses residual connections and batch normalization [22]. Both architectures thus violate the theoretical foundation of our CL framework. Nonetheless, we find that our ReCL is effective: it improves ACC by 4.77% (AlexNet) and ~3% (ResNet, Table 4; details in Appendix K). \u2192 Takeaway: Our ReCL is also effective for non-homogenous network architectures where it can slow down forgetting successfully."}, {"title": "7. Conclusion", "content": "In this paper, we propose a novel framework for continual learning, termed ReCL (Reconstruction from Continual Learning), which slows down forgetting by using the neural network parameters as a built-in memory buffer. Different from existing approaches, our framework does not require separate generative networks or memory buffers. Instead, our ReCL framework offers a novel paradigm for dealing with forgetting by leveraging the implicit bias of convergence to margin maximization points. Through extensive experiments, we show that ReCL is compatible with (i) existing CL methods, (ii) different datasets, (iii) different dataset sizes, and (iv) different network architectures. Further, we also show the applicability of ReCL across three"}, {"title": "A. Extended Literature Review", "content": "Continual Learning: CL learning methods are often categorized into three categories, namely memory-based, architecture-based, and regularization-based [13, 52, 67].\nMemory-based methods. Methods using memory-based techniques either maintain a (small) amount of old task's data (or separate datasets) in a memory buffer or train generative models for old tasks. OrDisCo [66] uses a setup consisting of a generator, discriminator, and classifier to pseudo-label unlabelled data from a separate dataset and train these components together. iCarl [49] splits the feature and the classification parts of the network, and stores samples that are closest to the feature mean of a class. Variants of the gradient projection memory (GPM) approach [13, 52] store vectors describing the core gradient space in memory and guide update to be orthogonal to this space. In deep generative replay, Shin et al. [56] use a dual-network architecture consisting of an additional generative part that can be sampled for old tasks' data.\nArchitecture-based methods. Methods based on architectural modifications allocate parts of a (growing) model to specific tasks. Progressive Neural Networks (PNN) [51] adds a separate network branch for new tasks and utilizes previous knowledge via lateral connections to previous tasks' frozen branches, avoiding forgetting at the cost of growing network size. SupSup by Wortsman et al. [69] uses trained task-specific binary masks overlaid over a fixed random neural network to allocate parameters. The Winning Subnetworks (WSN) [24] approach follows a similar route, but allows training of the underlying neural network and restricts mask sizes. Finally, PackNet [37] prunes a task's mask after training and then finetunes the underlying weights while keeping parameters of previous tasks intact. APD [73] splits parameters into task-shared and task-specific parameters. They enforce sparsity on the task-specific parameters and retroactively update them to respond to changes in task-shared parameters.\nRegularization-based methods. Methods in the regularization category use additional regularization terms in the loss function to restrict updates to important parameters. Examples here are elastic weight consolidation (EWC) [26], which uses the Fisher information matrix to assess parameter importance, and Learning without Forgetting (LwF) [30], which records the responses of the network to a new task's data prior to training and tries to keep recorded and newly learned responses similar. Lee et al. [29] employ multiple distillation losses to distill the previous task's model into a model combining the old and current tasks.\nAdditional categories. Beyond these three widely used categories, some taxonomies use five categories, adding the (i) optimization and (ii) representation approaches [67]. Methods belonging to the (i) optimization category manipulate the optimization process. The already introduced GPM approaches [13, 52] also fall into this category, as they modify gradient updates. Another method is StableSGD [39], which optimizes hyperparameters such as the learning rate in order to find flat minima which potentially \u201ccover\u201d all tasks. Methods belonging to the (ii) representation category focus on utilizing the representations learned during deep learning. Self-supervised learning and pre-training fall into this category, as they can provide a good initialization of neural networks for sequential downstream tasks [35, 55, 70]. Other methods explore continual pre-training, building on the fact that pre-training data might be collected incrementally [11, 18].\nImplicit Bias and Data Reconstruction: Neural networks can generalize to unseen (test) data. This phenomenon is described as an implicit bias of neural network training, and it has been studied extensively [3\u20135, 12, 16, 31, 40, 42, 43, 65]. For a certain types of neural networks, described in Appendix B.1, the implicit bias causes network weights to converge to margin maximization points Ji and Telgarsky [23], Lyu and Li [34]. Haim et al. [17] show that under this condition, the entire training data can be recovered from pretrained binary classifiers. [32] utilized this for networks trained under the neural tangent kernel regime, and Buzaglo et al. [8] extend the underlying theory to multi-class networks and general loss functions."}, {"title": "B. Background on Dataset Reconstruction", "content": "In this section, we detail the background of the dataset reconstruction process, based on preliminary works [8, 17, 23, 34]. We start with the definition of homogenous neural networks and then give details on dataset reconstruction."}, {"title": "B.1. Homogenous Neural Networks", "content": "Homogenous networks are networks whose architectures satisfy the following condition [34]:\n$\u2203c > 0 : \u03a6(c\u03b8; x) = c\u2517\u03a6(\u03b8; x) for all \u03b8 and,$\nif there is a L > 0. That is, scaling parameters @ is equal to scaling the network's output \u03a6(0; x). Fully connected and convolutional neural networks with ReLu and without skip-connections and bias vectors satisfy this condition. However, popular models ResNet [19], ViT [14], or CLIP [47] models do not satisfy this condition since they contain modules (e.g., skip connections) that restrict the propagation of scaled weights' intermediate outputs through the network. Oz et al. [45] show that in such cases, compute- and time-intensive workarounds relying on model inversion attacks [60] and diffusion models [57] could be applicable."}, {"title": "B.2. Dataset Reconstruction", "content": "Maximum margin points. To reconstruct the training data of past task, we build upon the implicit bias of gradient descent training [63]. For a homogenous neural network \u03a6(0,) trained under the gradient flow regime, Lyu and Li [34] show that the weights converge to the following maximum margin point\n$min  ||\u03b8||^2 s.t. \u03a6_{y_i}(\u03b8; x_i) - \u03a6_j(\u03b8; x_i) \u2265 1  \u2200i \u2208 [n], \u2200j \u2208 [C] \\{y_i\\},$\nwhere $\u03a6_j(\u03b8; x)$ \u2208 R is the output of the j-th neuron in the output layer for an input xi and [C] is the set of all classes (i.e., the label space). At such a point, the difference between the neuron activation of a sample's true class, $\u03a6_{y_i}(\u03b8; x_i)$, and all other neurons, $\u03a6_j(\u03b8; x_i)$, is maximized.\nConditions. This convergence point is characterized by, among others, the existence of $\u03bb_1,..., \u03bb_m \u2208 R$ and the following stationarity and feasibility conditions [8]:\n$\u03b8-\\sum_{i=1}^{m} \\sum_{j\u2260Y_i}^{C} \u03bb_{ij}\u2207_\u03b8(\u03a6_{Y_i}(\u03b8; x_i) - \u03a6_j (\u03b8; x_i)) = 0 $\n$\u2200i \u2208 [n], \u2200j \u2208 [C] \\{y_i\\} : \u03bb_{ij} \u2265 0$\nThese equations state that a (trained) neural network's parameters @ can be approximated by linearly combining the A-scaled derivatives on data points xi. Keeping @ fixed, we utilize this to optimize samples xi and coefficients $\u03bb_{ij}$ using gradient descent. Note the following two things: (i) The corresponding labels yi are chosen to match the number of output neurons of the pre-trained neural network. We evenly split the number of classes among the samples to be reconstructed, but uneven splits are equally possible. (ii) The number of samples to reconstruct, m, generally is unknown. In this work, we consider it appropriate to maintain count of the number of samples of previous tasks. This could by achieved by, e.g., a small buffer which simply stores these counts. Determining the \u201ccorrect\u201d m = n without prior knowledge is an open point and beyond the scope of this paper. As a starting point, we could imagine searching over a set of m and monitoring the behaviour of the reconstruction loss across trials.\nReconstruction loss: We reconstruct the old training data by optimizing randomly initialized data points xi to closely resemble the old task's data via the following combined reconstruction loss $L_{full}$ [8]:\n$L_{rec}(x_1,...,x_m, \u03bb_1,..., \u03bb_m) = -[\u03b8 - \\sum_{i=1}^{m}\u03bb_{Y_i}\u2207_\u03b8 \u03a6_{Y_i}(\u03b8; x_i)]^2  + L_\u03bb + L_{prior},$\nwhere $||\\cdot||$ denotes the squared L2 norm (i.e., squared Euclidean distance). Lambda loss: The lambda loss $L_\u03bb$ constrains the coefficients of the linear combination via\n$L_\u03bb = \\sum_{i=1}^{m} max \\{-\u03bb, -\u03bb_{min}\\}$,\nwhere Amin is a hyperparameter.\nPrior loss: The prior loss $L_{prior}$ constrains the value range of the (reconstructed) images to lie within normalized range [-1, 1]."}, {"title": "4.3. In-Training Optimization of Reconstruction Hyperparameters", "content": "Our ReCL consists of six hyperparameters, which are primarily used in Equation (3) to Equation (6). The hyperparameters are: (i) $\u03bb_{min}$ constrains the scaling effect of $\u03bb_i$; (ii) $\u03c3_x$ controls the initialization of the xi from a Gaussian distribution; (iii) lrx and (iv) $lr_{\u03bb}$ are the learning rates for the SGD optimizers of $x_i$ and $\u03bb_i$, respectively; (v) nrec is the number of reconstruction epochs; and (vi) m is the number of reconstruction candidates."}, {"title": "C. Dataset Details", "content": "In this section, we give details about the two dataset used. Both datasets are commonly used in previous works. Below, we provide information about each dataset.\nMNIST: The MNIST [28] dataset contains 60 000 black-and-white 28x28 training images evenly split across 10 classes. An additional 10000 samples are used for testing. The images show hand-written digits.\nCIFAR10: The CIFAR10 [2] dataset contains 60000 32x32 colour images evenly split across 10 classes. 50000 samples are reserved for training and 10000 samples for testing. This dataset depicts animals and vehicles.\nPre-processing: We normalize the data by removing the training-data mean from each dataset.\nDataset Splitting: We split the datasets into Ntasks = 5 (value depending on the experiment) tasks by splitting the class labels without overlap. In the domain incremental learning (DIL) scenarios, we dynamically re-label the classes to begin at zero. This is done to compute the cross-entropy loss of, e.g. classes [5,6,7,8], on a model with an output layer of size four (which would expect labels [1, 2, 3, 4])."}, {"title": "D. Implementation details", "content": "In this section, we provide details about our implementation and the CL methods that we combine ReCL with."}, {"title": "D.5. Computing the Similarity Between Reconstructed and Reference Samples", "content": "In this section, we give details on how we compute the similarity between reconstructed and reference samples used in the proposed supervised strategy (see Section 4.3). We also suggest two possible improvements that could be explored in future works.\nFor computing image similarity, we closely follow [17] in performing the following two steps.\nStep 1: Distance calculation. For each reference sample xref, we compute the following distance to all reconstructed samples xrecon:\n$d(x_{ref}, x_{recon}) = ||t_{ref} - t_{recon}||_2$\nwhere tref is obtained by reducing from xref the mean of all reference samples and then dividing it by the standard deviation of all reference samples. The same steps are respectively applied to obtain t recon. We select for each training sample its closest reconstructed neighbour.\nStep 2: Sample creation.: (i) We re-add the training set mean (note that our data is normalized to [-1,1] by reducing the training mean from all data points) to reconstructed samples xrecon and reference samples xref. (ii) We then stretch the xref to [0, 1] by scaling according to the minimum and maximum value. (iii) Lastly, we compute the SSIM [68] between the pair."}, {"title": "K. ReCL slows down forgetting beyond theoretical limitations", "content": "In the main text, we focus on MLPs and CNNS, which generally conform to the definition of homogenous neural networks (cf. Appendix B.1). These type of neural networks converge to margin maximization points Ji and Telgarsky [23], Lyu and Li [34]. To slow down forgetting in CL, our ReCL builds on these convergence points, as they allow the (unsupervised) reconstruction of old training data. Interestingly, ReCL can also slow down forgetting for networks that are explicitly not homogenous, such as AlexNet [27] and ResNet [19].\nTo demonstrate this, we perform the following additional experiment. We optimize the training hyperparameters of AlexNet and Resnet in the DIL scenario, on the SplitCIFAR10 dataset, following the same search procedure as for the other networks (Section 5 and Appendix D). That is, over 100 hyperparameter trials, we optimize the learning rate and training epochs over the grid details in Table 5. We then train AlexNet and ResNet with (+ReCL) and without our framework.\nOur results are in Table 21. We observe: ReCL can slow down forgetting for non-homogenous neural networks. For AlexNet, ReCL improvemes up to 4.44% in terms of ACC, and up to 3% for ResNet. These observations are interesting, as the theory underlying our framework is restricted to homogenous neural networks. It is therefore possible that parts of the theory might also extend to non-homogenous architectures. A detailed study is beyond the scope of this paper, and we thus leave it as an interesting direction for future research."}, {"title": "L. Scalability Analysis", "content": "We report the runtime (in seconds) alongside the results in Table 6 to Table 21. We have several observations: (1) Our ReCL is insensitive to the number of reconstruction samples, with roughly similar runtimes (cf. Tables 8 and 11). (2) The number of reconstruction epochs, as expected, positively correlates with longer runtimes (cf. Tables 14 and 15). (3) The overhead for the na\u00efve strategy is relatively small. For example, the runtime of our Default+ReCL is roughly twice as long as the Default baseline. (4) Adding our framework to the CL baselines again increases the runtime by a factor of approx. two. (5) The computational cost is largely insensitive to the dataset type (i.e., whether SplitMNIST or SplitCIFAR10 is used), but is affected by the general network size. This can be expected by how the dataset reconstruction attack is designed: gradients need to be backpropagated to the reconstruction candidates. (6) The more complex in-training hyperparameter tuning strategies from our ablation studies (i.e., unsupervised and supervised strategies from Section 4.3) incur larger overheads. We thus recommend the na\u00efve strategy for use in practice.\nNote that our code could be optimized further, and runtime improvements can also expected as a result of more research on the effectiveness of reconstruction attacks."}, {"title": "J. Gradient perturbation interferes with ReCL", "content": "Adding ReCL to existing CL generally slows down forgetting. For UPGD", "follows": "i) UPGD is a optimizer specially designed for CL that modifies the gradient update. (ii) The gradient update to a parameter is scaled based on its normalized importance. (iii) Crucially, UPGD adds Gaussian noise to the gradient update. (iv) This interferes with ReCL, which requires stable (ideally already"}]}