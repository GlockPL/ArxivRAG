{"title": "Controllable Game Level Generation: Assessing the Effect of Negative Examples in GAN Models", "authors": ["Mahsa Bazzaz", "Seth Cooper"], "abstract": "Generative Adversarial Networks (GANs) are unsupervised\nmodels designed to learn and replicate a target distribution.\nThe vanilla versions of these models can be extended to\nmore controllable models. Conditional Generative Adversar-\nial Networks (CGANs) extend vanilla GANs by condition-\ning both the generator and discriminator on some additional\ninformation (labels). Controllable models based on comple-\nmentary learning, such as Rumi-GAN, have been introduced.\nRumi-GANs leverage negative examples to enhance the gen-\nerator's ability to learn positive examples. We evaluate the\nperformance of two controllable GAN variants, CGAN and\nRumi-GAN, in generating game levels targeting specific con-\nstraints of interest: playability and controllability. This eval-\nuation is conducted under two scenarios: with and with-\nout the inclusion of negative examples. The goal is to de-\ntermine whether incorporating negative examples helps the\nGAN models avoid generating undesirable outputs. Our find-\nings highlight the strengths and weaknesses of each method\nin enforcing the generation of specific conditions when gener-\nating outputs based on given positive and negative examples.", "sections": [{"title": "Introduction", "content": "In traditional generative models without conditioning, there\nis limited control over the attributes of the generated data.\nThis limitation is effectively addressed by conditional mod-\nels, where additional information can condition the data gen-\neration process towards specific outcomes. Recent research\nby Asokan and Seelamantula (2020) introduces a method\nwhere GAN models are trained using both positive and neg-\native examples to guide and avoid specific outcomes (Rumi-\nGAN). Positive examples are data samples that align well\nwith the desired outcome or target distribution. These ex-\namples represent the ideal or goal that the GAN is trying to\nachieve. Negative examples, on the other hand, are data sam-\nples that do not meet the desired outcome or diverge from the\ntarget distribution.\nThis study investigates the performance of two control-\nlable GAN generators (CGAN and Rumi-GAN) through\nthree sets of experiments conducted in the context of two\n2D tile-based games. In each experiment, our goal is to en-\nforce the generation of specific constraints within each game"}, {"title": "Related Work", "content": "A range of studies have explored the use of GAN models for\ngame level generation. Kumaran, Mott, and Lester (2020)\ndeveloped a GAN-based architecture capable of generating\nlevels for multiple distinct games from a common game-\nplay action sequence. Volz et al. (2018) trained a GAN to\ngenerate Super Mario Bros levels, using a variety of fitness\nfunctions to guide the search for levels with specific proper-\nties. Giacomello, Lanzi, and Loiacono (2018) applied GANs"}, {"title": "Domains", "content": "This work uses the Sturgeon (Cooper 2022b) constraint-\nbased level generator to create a corpus of Super Mario Bros\n(Nintendo 1985) level segments. These segments are 14\u00d732\nin size and are based on the level 1-1 from the VGLC (Sum-\nmerville et al. 2016). We have also created a corpus of a cus-\ntom game called Cave which is a 14 \u00d7 14 simple top-down\ncave map and it was first introduced by Cooper (2022a) us-\ning tiles from Kenney (Kenney 2022).\nIn this work, we used two of Sturgeon's constraint-based\nfeatures to generate our corpus. First is the ability to add\nconstraints on the number of specific tiles. Second is (in ad-\ndition to generating playable levels) the ability to create un-\nplayable levels using an unreachability constraint (Cooper\nand Bazzaz 2024) that are similar to playable levels in local\ntile patterns, but are not possible to play."}, {"title": "System Overview", "content": "All three controllable GANs adopt the Deep Convolutional\nGAN architecture as used by Volz et al. (2018), which is\nbased on the original work of Arjovsky, Chintala, and Bot-\ntou (2017). This architecture employs batch normalization in\nboth the generator and discriminator after each layer, ReLU\nactivation functions for all layers in the generator (instead\nof Tanh), and LeakyReLU activation in all layers of the dis-\ncriminator.\nThese models are trained using the WGAN algorithm.\nWasserstein Generative Adversarial Networks offer an al-\nternative to traditional GAN training, providing more stable\ntraining as demonstrated by Arjovsky, Chintala, and Bottou\n(2017). Both the generator and discriminator are trained with\nRMSprop, using a batch size of 32 and a default learning rate\nof 0.00005 for 200 iterations.\nThe following sections detail each controllable model, de-\nscribing the classes of data used for training and the objec-\ntive function of the training process."}, {"title": "Vanilla Generative Adversarial Nets", "content": "The vanilla GANs as introduced by (Goodfellow et al.\n2014), consist of two models: a generative model (G) that\ncaptures the data distribution, and a discriminative model\n(D) that estimates the probability of a sample being real\n(from the training data) or fake (from the generative model).\nThe entire training process is framed as a minimax game,\nwhere the discriminator tries to maximize the objective func-"}, {"title": "Conditional Generative Adversarial Networks", "content": "CGANS (Mirza and Osindero 2014) apply extra information\ny to both the discriminator and generator. As shown in Equa-\ntion 2 (Mirza and Osindero 2014) The loss function of a\nCGAN is an extension of the vanilla GAN loss function, in-\ncorporating this conditional information. This again results\nin in a minimax game in which the generator tries to gen-\nerate realistic data conditioned on y, while the discrimina-\ntor tries to distinguish between real and generated data, also\nconditioned on y. Here x ~ Pdata includes both positive and\nnegative samples, distinguished by label y."}, {"title": "Rumi Generative Adversarial Nets", "content": "Rumi-GAN (Asokan and Seelamantula 2020) is a special-\nized type of GAN inspired by the Sufi poet Rumi's philoso-\nphy of learning from both positive and negative experiences.\nEquation 3 (Asokan and Seelamantula 2020) shows how in\nthis approach data distribution pa is split into the target dis-\ntribution that the GAN is required to learn (positive samples,\np+) and the distribution of samples that it must avoid (nega-\ntive samples, p\u0101). The fake distribution which are the sam-\nples drawn from the generator pg is there as before. a+ and\na is a weighting factor for the positive and negative real\ndata distribution term. We set a+ to 1 and a to 0.5."}, {"title": "Experiments", "content": "We conducted two different experiments to examine two\nsets of constraints we would like to see in the gen-\nerated levels: playability and controllability. A playable\nlevel is a level such that there exists a path between the\nlevel's start and end locations. Controllability is derived\nfrom the correctness of the number of controlled features\n(eg. pipes or treasures) at each level. A level is con-\nsidered correct if it has the desired number of features.\nFigure 1 shows the Venn diagram of the sample space.\nThe sample space is divided into playable-correct,\nplayable-incorrect,unplayable-correct,and\nunplayable-incorrect subspaces. Positive and nega-\ntive samples are chosen from these subspaces according to\nthe model's objective. In the experiments, negative examples\nare added in addition to the positive data that models utilize.\nTo ensure an unbiased distribution of training data for mod-\nels that incorporate negative examples, an equal number of\npositive and negative examples are sampled for training. Ta-\nble 2 the models trained in each experiment and the exact\ninput data of each model."}, {"title": "Experiment One: Playability", "content": "In this experiment, the goal is to ensure playability as the\nprimary constraint for the models to meet. This means that\nwe take the positive samples(p+) from the playable sub-\nspace and negative samples (p) from the unplayable\nsubspace. Both Conditional GAN and Rumi-GAN use this\nnegative distribution as additional information."}, {"title": "Experiment Two: Playability and Controllability", "content": "This experiment's goal is to enforce both the number\nof some features in the level (number of pipes in Mario\nand number of treasures in Cave) and playability as a\nconstraint for the models to satisfy. This approach gets pos-\nitive samples (pt) from the playable-correct\nsubspace and negative samples(p) from the\nplayable-incorrect, unplayable-correct,\nand unplayable-incorrect subspaces. This means\nRumi-GAN, and Conditional GAN get the playable samples"}, {"title": "Evaluation", "content": "After the training models in each experiment, we generated\n500 levels with each trained model and we evaluated each\nmodel based on the criteria of that experiment."}, {"title": "Experiment One: Playability", "content": "To evaluate Experiment One, we measure the percentage of\nplayable levels as the metric. We use Sturgeon to find (if\navailable) the shortest path between the start and goal of the\nlevel segments. Generated level segments that don't have a\nstart or end, or have multiple starts and ends count as un-\nplayable levels immediately. Table 3 shows the results of this\nexperiment. In Mario both models using negative examples\nshow better performance than vanilla GAN. In Cave, only\nRumi-GAN takes advantage of the negative examples result-\ning in better performance compared to the other models."}, {"title": "Experiment Two: Playability and Controllability", "content": "To evaluate Experiment Two, we measure the percentage\nof levels that have the correct number of pipes/treasures\nwhile being playable. This means unplayable levels with the"}, {"title": "Discussion", "content": "The results of Experiment One demonstrate that incorporat-\ning negative examples in training GAN models can enhance\ntheir ability to generate more playable levels. In future work,\nwe aim to reinforce this approach using levels annotated\nwith players' paths. The surprising decrease in the perfor-\nmance of GAN models using negative examples in Exper-\niment Two, may suggest the importance of the quality of\nthe negative examples. In future work, we would like to ex-\nplore multi-stage training approaches with high-quality neg-\native examples in fine-tuning steps. We believe this approach\ncould be effective when combined with bootstrapping meth-\nods (Torrado et al. 2020), or active learning methods with\nminimal training levels (Bazzaz and Cooper 2023). These\napproaches could make the training easier, with the only\nprice for the additional controllability being the number of\nmodels trained on the minimal data."}, {"title": "Conclusion", "content": "This study explores the potential advantages of integrat-\ning negative examples into Generative Adversarial Networks\n(GANs) to enhance the generation of game levels. Inspired"}]}