[{"title": "4 Experiments", "authors": ["Xi Chen", "Julien Cumin", "Fano Ramparany", "Dominique Vaufreydaz"], "abstract": "Human Activity Recognition (HAR) is one of the central prob-\nlems in fields such as healthcare, elderly care, and security at\nhome. However, traditional HAR approaches face challenges\nincluding data scarcity, difficulties in model generalization,\nand the complexity of recognizing activities in multi-person\nscenarios. This paper proposes a system framework called LA-\nHAR, based on large language models. Utilizing prompt engi-\nneering techniques, LAHAR addresses HAR in multi-person\nscenarios by enabling subject separation and action-level de-\nscriptions of events occurring in the environment. We val-\nidated our approach on the ARAS dataset, and the results\ndemonstrate that LAHAR achieves comparable accuracy to the\nstate-of-the-art method at higher resolutions and maintains ro-\nbustness in multi-person scenarios.", "sections": [{"title": "1 Introduction", "content": "Over the past two decades, Human Activity Recognition\n(HAR) using sensor technology has garnered increasing at-\ntention due to its potential applications in healthcare, se-\ncurity surveillance, and smart home environments. While\nmany existing HAR systems employ camera-based technolo-\ngies [3, 15], these methods often raise substantial privacy con-\ncerns, particularly in private settings. As a response, some\nresearchers have explored wearable technologies [19], such as\nsmartwatches and smartphones. However, the requirement for\nindividuals to continuously carry these devices may compro-\nmise comfort and convenience. Consequently, ambient sen-\nsors have gained back prominence as a key solution in HAR,\nprized for their non-invasive while avoiding privacy concerns\nof cameras and microphones.\nAmbient sensors can be strategically placed within environ-\nments to detect and log changes in the physical state, with\neach change defined as an event. Common types of ambient\nsensors include door, presence, temperature, energy consump-\ntion sensors, and so on. Given their limited sensing range,\nmultiple sensors are typically installed throughout a space to\nachieve thorough sensing coverage. The interactions between\nhumans and their surroundings, recorded by these sensors, can\nbe furthermore analyzed to infer individual actions and activ-\nities. This technology is known as Ambient Sensor-Based\nHuman Activity Recognition (AHAR).\nHowever, AHAR faces the following challenges:\n\u2022 Data Collection: Due to the high cost of setting up exper-\nimental environments and the sensitivity of personal daily\nliving data, collecting ambient sensor datasets is often chal-\nlenging.\n\u2022 Model Generalization: Due to varying sensor setups and\nactivity routines, models trained on specific datasets often\nstruggle to transfer their capabilities to different environ-\nments or configurations.\n\u2022 Context Integration: Contextual information like sensor\nlocations, functions, time, environment, and user habits is\ncrucial due to the simplicity of ambient sensor data. How-\never, traditional deep learning methods often fail to effi-\nciently encode this information, making HAR less precise\nand flexible.\n\u2022 Multi-Person Recognition: In environments where multi-\nple individuals are present, events triggered by different sub-\njects will blend into a single event sequence, complicating\nthe task of activity recognition.\n\u2022 Explainability: Explainable HAR helps increase user trust,\nenhance user experience, and improve system personaliza-\ntion. However, the inference process of traditional deep\nlearning models is not intuitively understandable and lacks\nexplainability.\nIn recent years, significant advancements have been made in\nLarge Language Models (LLMs), with models such as Chat-\nGPT [1] and Llama [14] exhibiting impressive contextual un-\nderstanding and reasoning capabilities. This endows LLMS\nwith the potential to address the aforementioned five chal-\nlenges in AHAR: 1) LLMs' in-context learning capability [6]\nreduces the need for training datasets; 2) by adapting relevant\nprompts, LLMs can swiftly adapt to novel environments or\nadjust to new sensor configurations; 3) leveraging the expres-\nsiveness of natural language, LLMs can integrate the differ-\nent types of contextual information; 4) LLMs can connect re-\nlated events using attention mechanisms, integrating common\nsense and reasoning to identify meaningful sensor event com-"}, {"title": "2 Related Work", "content": "Recently, increasing attention has been given to modeling am-\nbient sensor sequences using natural language models. Bouch-\nabou et al. [4, 5] first introduced the concept of language mod-\nels into human activity recognition, treating each sensor event\nas a word (token), and used the word embedding method to\nlearn the correlations between sensor events. Zhao et al. [20]\nfurther encoded the sensor environmental location into the\nembedding vectors, demonstrating the capability of language\nmodels to integrate context information. Das et al. [8] elabo-\nrated on the importance of explainability in activity recogni-\ntion and implemented a system capable of explaining activity\nrecognition classifications using natural language. Takeda et\nal. [13] first used the large language model GPT2 [11] for gen-\nerative prediction of sensor event sequences, predicting future\nsensor events based on the labels of the ongoing activity and\nthe sensor events that have already occurred. This work fur-\nther strengthened the association of human activity recognition\nwith language models and brought the large GPT model [10]\ninto the scope of HAR.\nWith large language models demonstrating powerful in-\ncontext learning [6] and reasoning [18] abilities, Gao et al. [9]\nfirst used a large language model to perform unsupervised\nannotation on single-person activity samples in the ARAS\ndataset [2], demonstrating the potential of large language mod-\nels for unsupervised human activity recognition. In this work,\nGao et al. used sensor reading data within a 5-minute sliding\nwindow as input data. They employed a Chain-of-Thought ap-\nproach [18] to instruct the LLM to analyze the functions of the\nactivated sensors. By integrating context information on room\nlayout, time, and the duration of sensor activation, LLM was\nfinally instructed to choose an activity as the recognition re-\nsults from nine activities selected by the authors. Although the\nexperimental results show comparable accuracy to supervised\ntrained models, this work is limited to nine easily distinguish-\nable activity categories in single-person scenarios, overlook-\ning the recognition of other more challenging categories and\nfailing to provide prompts for reproducibility. Furthermore,\nusing sensor readings in fixed time windows rather than sen-\nsor events as input data limits the model's ability to perceive\nthe subject's behavior at a finer granularity.\nAlthough language models are widely used in applications\nsuch as sensor representation, event sequence prediction, and\nactivity explanation in single-user scenarios, these methods are\noften difficult to apply directly in multi-user scenarios. This is\nbecause only modeling the correlations of sensor events is in-"}, {"title": "3 Methodology", "content": "Figure 1 illustrates the workflow of our proposed framework\nLAHAR. Given a time period T, the collection of all sensor\nreadings within it is referred to as a sensor reading segment.\nLAHAR includes three main steps of information processing:\n1) Process the sensor reading segment into a textual form of\nsensor event pairs (Section 3.2); 2) Integrate the context infor-\nmation (Section 3.3) into the sensor event pair sequence, sepa-\nrate subjects and generate individual action-level descriptions\nby an LLM (Section 3.4); 3) Based on the action-level descrip-\ntions and the context information, an LLM is used to perform\nactivity-level reasoning to predict the timeline of activities for\neach subject (Section 3.5)."}, {"title": "3.1 Problem Formalization", "content": "Given an environment E = {$i}1<i<n, where si is a sen-\nsor installed in the environment characterized by its specific\nsetting, we define a sensor event as et =< t,s,c >, where\nt represents the time of the event, s represents the sensor,\nand c represents the change in sensor status. The sequence\nof events that occur within a time period T = [ts,te] is\nST = (et1, et2, ..., etr) where Vi\u2208 [1,k], ti \u2208 [ts,te]. Given\nan activity category set of K activities LA = {ak}1<k\u2264K, the\nactivities occurred during T are defined as A\u0442 = {az}j\u2208I,\nwhere I = {j \u2208 N|T; \u2286 T}. The objective of this research\nis to propose a model M such that A\u0442 = M(ST|E, LA)."}, {"title": "3.2 Data-to-Text Alignment", "content": "As LLMs accept text as input, LAHAR first involves data-to-\ntext alignment. This process includes two steps: data prepro-\ncessing, and information structuring.\nData Preprocessing Unlike Gao et al. [9], who extract over-\nall features from all sensor readings within a fixed time win-\ndow, our method first preprocesses the sensor readings into\nsensor events. Specifically, when there is a change in the read-\ning of any sensor, we denote the time of occurrence t, the\nchanged sensor identifier s, and the change of sensor reading\nc as a sensor event e =< t, s, c >. Since a sensor event often\ncorresponds to an action by a subject, analyzing events allows\nour model to achieve fine-grained, action-level detail. Mean-\nwhile, to reduce redundant information, when a sensor contin-\nuously changes at a high frequency between two states without\nany other sensor events occurring, we retain only the first and\nthe last events.\nInformation Structuring To further enhance the informa-\ntion density and quality of the text input to the LLM, we per-\nform information structuring on the sequence of sensor events.\nFor adjacent activation CON =< ts, s, ON > and deactivation"}, {"title": "3.3 Context Integration", "content": "Traditional machine learning methods struggle with ambient\nsensor data due to limited information from sensor readings.\nHowever, contextual information like sensor location, type,\nfunction, user habits, and environment layout often provide\nmore insight than the sensor data itself. Integrating this con-\ntextual information is crucial for understanding the correla-\ntions between sensors and for activity recognition.\nGiven that ambient sensors are usually installed in a relatively\nstable environment, this contextual information tends to re-\nmain constant. Therefore, our method proposes to provide\ncontextual information to LLMs through language prompts,\nso that LLMs can harness their encoding capabilities to embed\nthis information and align it with relevant sensor events. The\ncontextual information used in this work is listed as follows:\n\u2022 Background: This introduces the role of the LLM, the num-\nber of residents, and the fact that ambient sensors are in-\nstalled to identify activities.\n\u2022 House Layout: This provides the list of rooms contained in\nthe environment, the furniture in each room, and the associ-\nated sensors.\n\u2022 Sensor Description: This explains the identifier, type, and\nlocation of each sensor in the environment.\n\u2022 Activity List: This offers a list of possible activities within\nthe environment, along with certain behavior patterns or user\nhabits related to these activities.\n\u2022 User Schedule: This emphasizes the intervals during which\nsubjects perform certain activities, such as eating breakfast."}, {"title": "3.4 Action-Level Resident Separation and Description\nGeneration", "content": "This section depicts the design of an LLM-powered module\nthat assigns the sequential event pairs from the Data-to-Text\nmodule (Section 3.2) to different subjects, and then provides\nnatural language descriptions with action-level granularity as\nillustrated in the second block of Figure 2. This process is\nprimarily based on two assumptions: 1) Related sensor events\nare more likely to be triggered by the same person; 2) A person\ncannot simultaneously trigger two unrelated sensors.\nThe first assumption enables the LLM to merge related sensor\nevents, while the second assumption enables the LLM to sep-\narate events triggered by different subjects. The application of\nthese two assumptions relies on two abilities: 1) Determining\nthe relevance of sensor events; 2) Assessing the current state\nof the subject to determine their likelihood of triggering other\nevents.\nThe first ability can be enabled by the context introduced in\nSection 3.3 and the common sense learned by the LLM dur-\ning its training, indicating the use of the In-Context Learn-\ning [6]. The second ability requires us to introduce a Chain of\nThought [18] in the prompt to guide the reasoning of the LLM.\nTherefore, the prompt contains 4 basic components: 1) Con-\ntext; 2) Instructions; 3) Examples; 4) Input, where Context\nand Examples follows the idea of In-Context Learning, and\nInstructions describes the Chain of Thought.\nInput Although the Input section appears last in the prompt,\nwe introduce it first for clarity. Given a period T, the sequence\nof events ST is formatted into a sequence of event pairs Pr\u0442\nfollowing Data-to-Text alignment. Since Pr can be too long\nto ensure high-quality generation, we divide Pr into chunks\nCi, each containing N event pairs, except for the last chunk,\nwhich contains the remaining pairs. We process each chunk\nsequentially in a loop, concatenating all responses at the end.\nTo enable the LLM to infer the users' last state at the beginning\nof each new step, we include the final description of each sub-\nject from the previous chunk into the input of the subsequent\nstep.\nContext In this part of the prompt, Background,\nHouse Layout, and Sensor Description introduced in\nSection 3.3 are provided to the LLM as the con-\ntext information. Formally, we have Context\n([Background], [HouseLayout], [SensorDescription]).\n= \nInstructions We instruct the LLM to sequentially perform\nthe following steps:\n1. Merge related sequential event pairs, and determine\nthe overall start and end times;\n2. Summarize the previous action states of the users and\ndetermine whether the previous actions have ended;\n3. Recall the location of the current event pairs;\n4. Considering the previous states of users, designate a\nsuitable user as the subject for the current event pair\nbeing processed;\n5. Describe the current event pair with natural language.\nUltimately, the prompt ask the LLM to respond in a prede-\nfined JSON format, which implicitly formalizes the Chain of\nThought while making the generated results easier to post-\nprocess and increasing the information density. The keys de-\nfined in the JSON format are: { \"start\u201d, \u201cend\u201d, \u201clast state of\nUser 1\u201d, ..., \u201clast state of User i\u201d, \u201clocation\u201d, \u201csubject\u201d, and\n\"description\"}.\nExamples To further activate the LLM's ability to use con-\ntext and follow the chain of thought for reasoning, the prompt\nprovides several examples to the LLMs."}, {"title": "3.5 Activity-Level Reasoning", "content": "The objective of this second module is to align descriptions\nof fine granular action of each subject to an activity timeline\nAr as shown in the last two blocks of Figure 2. For activities\nthat are directly associated with sensors, LLMs can make use\nof common sense reasoning, such as associating sleeping with\nthe pressure sensor of a bed. On the other hand, recognizing\nactivities that are environment-specific and user-specific relies\nheavily on in-context learning. Consequently, the design of\ncontext and examples is crucial for this module. Similar to the\nDescription Generation module, the prompt contains 4 basic\ncomponents: 1) Context; 2) Instructions; 3) Examples; 4) In-\nput.\nInput From the output of last module, we separate and reor-\nganize the descriptions for each subject, retaining only 4 key-\nvalue pairs: \"start\u201d, \u201cend\u201d, \u201clocation", "description\".\nAfter implementing the separation, we input each subject's de-\nscriptions independently. Similarly to the previous module, we\ndivide each subject's descriptions into chunks, with each con-\ntaining M descriptions.\nContext In this part of the prompt, Sensor Descrip-\ntion, Activity List, and User Schedule introduced in\nSection 3.3 are provided to the LLM as the con-\ntext information. Formally, we have Context\n([Sensor Description], [ActivityList], [UserSchedule]).\n= \nInstructions We instruct the LLM to sequentially perform\nthe following steps:\n1. Analyse and summarise the descriptions that belong\nto the same activity, and determine the overall start\nand end times;\n2. Calculate the duration of the acitivty;\n3. Recall the last activity predicted;\n4. Considering the previous activities of the subject and\nthe duration of current actions, reason the subject's\ncurrent activity;\n5. Choose an activity with ID from the activity list.\nUltimately, we instruct the LLM to respond in a predefined\nJSON format, in which the keys defined are: { \"start\u201d, \u201cend\u201d,\n\u201cDuration\u201d, \u201cLast_Activity\u201d, \u201cReasoning": "and"}, ".", "ctivity."], "content": "To evaluate LAHAR, we require an ambient-based multi-\nperson HAR dataset that provides sufficient contextual infor-"}, {"title": "4.2 Experiment Settings", "content": "Data Segmentation Although our method can reason coher-\nently without prior data segmentation, we performed neces-\nsary segmentation. We noted that House A's data is daily in-\ndependent, while House B's data spans 30 consecutive days.\nThus, we concatenated House B's 30 days of data but treated\neach day in House A as an independent segment. For evalua-\ntion, we further divided the data into single-person and multi-\nperson scenarios based on the \"Leaving House\" activity. Con-\nsequently, House A had 59 single-person and 61 multi-person\nsegments, while House B had 10 single-person and 24 multi-\nperson segments.\nError Preprocessing We assessed sensor error levels in the\nhouses by examining the number of events that occurred when\nboth residents were leaving the house. According to our ob-\nservation, House A exhibited significant noise, especially from\nthe hall motion sensor, kitchen motion sensor, and kitchen\ntemperature sensor. To address this, we removed the hall mo-\ntion sensor events and deactivated the kitchen motion and tem-\nperature sensors, except during kitchen activities.\nClass Selection and Regrouping Due to similar activities in\nthe ARAS dataset that sensors don't distinguish, we merged\ncertain activities: Napping and Sleeping into Sleeping, and\nWatching TV, Reading books, and Listening to music into En-\ntertainment. In House A, Using Internet and Studying were\nmerged into Working; in House B, Using Internet was merged\ninto Entertainment, and Studying was renamed to Working. We\nremoved infrequent activities like Laundry, Cleaning, Having\nconversations, and Having guests, as recognizing these activ-\nities is beyond the capability of our method. This resulted in\nthe list of activities shown in Table 1."}, {"title": "4.3 Evaluation Metric", "content": "For a data segment whose time period is T, the activities oc-\ncurred is denoted as Ar = {a}jej, where ak is k-th ac-\ntivity class in K classes and J = {j \u2208 N|T; \u2286 T}. We\nperform one-hot encoding for all the activities {ak} present at\neach second of T and apply the union operation. For example,\nif the i-th and j-th activities are ongoing at the second t, the\nencoding is a length-K vector with ones at positions i and jperiment settings, our single-person scenario data include 17\nand zeros elsewhere. By stacking all the seconds, we have a\ntwo-dimensional matrix MTxK. To compare our prediction\nWith \nMTXK with the ground truth \nMT\u00d7K, we have\nSTXK = MTXK. \u041c\u0442\u0445\u043a,\nTP = \u2211St,k ,\nLtET 1x K\nFP= \u2211(Mt,k - St,k) ,\nLtET 1xK\nFN = \u2211 (Mt,k - St,k) ,\nLtET 1xK\nBased on TP, FP, and FN given above, we can then cal-\nculate the precision, recall, and F1-score of each class in our\nprediction."}, {"title": "4.4 Single-Subject Activity Recognition", "content": "To validate the activity recognition capability of LAHAR, a\ncomparison is performed against the research of Gao et al. [9].\nThe experimental setup for this comparison is consistent with\nthe research of Gao el al., focusing solely on the recognition\nof selected nine activity categories in single-person scenarios.\nWe extract the longest data segments from the original data\nwhere Resident 2 was leaving home and Resident 1's activities\nare in these nine activities. These segments vary in length and\ncan contain more than one activity. Without additional seg-\nmentation, our method can generate action-level descriptions\nand achieve activity sequence prediction at the resolution of a\nsecond. In contrast, the method of Gao et al, which did not use\nevents as the smallest divisible units but instead used a fixed\n5-minute time window, has thus a resolution of 5 minutes. De-\nspite our higher resolution, our results are comparable to the\nresults of Gao et al. in terms of the confusion matrix in Figure\n3 and of precision, recall, and F1 score as shown in Table 2.\nExtended Validation We further validated our method in\nmore realistic and complex scenarios. As described in the ex-\nactivities across both houses, without class-based segmenting.\nConsequently, each segment is longer and contains more ac-"}, {"title": "4.5 Multi-Subject Activity Recogntion", "content": "Since the ARAS dataset does not label events with the IDs\nof their subjects, we cannot perform a one-to-one compari-\nson of event assignments. To validate our method's activity\nrecognition capability in multi-person scenarios, we qualita-\ntively present an example of our results and indirectly demon-\nstrate our method's ability to separate residents by comparing\nits performance with that in single-person scenarios.\nQualitative Results Figure 2 provides an example to better\nillustrate our results. We excerpted the experimental output of\nabout 21 minutes of sensor data from 22:15:24 to 22:36:41 of\nthe 5th day in House B in the multi-person scenario. It can be\nseen that these sensor events were integrated into 9 descrip-\ntions by the LLM, in which each description is assigned to a\nsubject. By separating these descriptions by subjects, times- tamped activities are finally predicted respectively for each\nresident.\nQuantitative Results In Table 3, the recognition results for\neach activity class in multi-person scenarios are presented. It\ncan be seen that even when extended to multiple people, our\nmethod's performance in activity recognition remains compa-\nrable to that in single-person scenarios. Although the time and\nactivity distributions differ between single-person and multi-\nperson scenarios, this comparison still highlights the scalabil-\nity of LAHAR in multi-person contexts."}, {"title": "5 Conclusion", "content": "In this paper, we propose LAHAR, a framework using LLMs\nfor multi-person HAR with ambient sensors. Our prompts en-\nable LLMs to assign sensor events to individuals based on\ntheir states, generating detailed descriptions and reasoning\nabout their activities. This method extends LLM application to\nmulti-person HAR, achieving time resolutions matching sen-\nsor timestamps. LAHAR's explicit descriptions and activity\nreasoning offer promising perspectives to address explainabil-\nity challenges. Experimental validation shows performance\ncomparable to the state-of-the-art in single-person and multi-\nperson scenarios. Future plans include validation with differ-\nent LLMs, model fine-tuning, and further evaluation of con-\nversational explainability."}]