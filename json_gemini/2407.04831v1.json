{"title": "Code Hallucination", "authors": ["Mirza Masfiqur Rahman", "Ashish Kundu"], "abstract": "Generative models such as large language\nmodels are extensively used as code copilots\nand for whole program generation. How-\never, the programs they generate often have\nquestionable correctness, authenticity and\nreliability in terms of integration as they\nmight not follow the user requirements, pro-\nvide incorrect and/or nonsensical outputs,\nor even contain semantic/syntactic errors\noverall known as LLM hallucination. In\nthis work, we present several types of code\nhallucination. We have generated such hal-\nlucinated code manually using large lan-\nguage models. We also present a technique\nHallTrigger, in order to demonstrate ef-\nficient ways of generating arbitrary code\nhallucination. Our method leverages 3 dif-\nferent dynamic attributes of LLMs to craft\nprompts that can successfully trigger hal-\nlucinations from models without the need\nto access model architecture or parameters.\nResults from popular blackbox models sug-\ngest that HallTrigger is indeed effective and\nthe pervasive LLM hallucination have sheer\nimpact on software development.", "sections": [{"title": "Introduction", "content": "Code generation has emerged to be a pivotal\ntask in natural language processing and recent\ndevelopments in large models, where trans-\nformer architecture is used as the backbone,\nhave demonstrated sheer capabilities on the\ntask. By exhibiting capabilities on generating\nsimple functions to fulfilling complex array of\nrequirements, these models are being utilized\nas copilots in large-scale projects. As of Febru-\nary 2024, GitHub Copilot alone has 1.3 mil-\nlion paid subscribers (Cio, 2024). There have\nbeen a pool of large language models dedicated\nfor code generation (cod, 2024; Zheng et al.,\n2023; Chen et al., 2021). Additionally, many of\nthe general-purpose models are also trained on\ncode datasets (cha, 2024; cop, 2024; gem, 2024).\nCode generative models have been used in a\nmyriad of use cases such as, program repair,\ntranslation to code from natural language, pro-\ngram analysis, fuzzing, and so on (Nejjar et al.,\n2024; Hou et al., 2024). However, these auto-\nregressive models, as they decode outputs in a\ngreedy or probabilistic manner, often generate\nincorrect, nonsensical outputs which is often\nreferred to as hallucination. As such, many\nstudies have focused on the factual, consistent,\nand correct generation of language model out-\nputs. Despite the current effort to analyze\nhallucinations of natural texts, code has not\nreceived much attention. Very recently, Code-\nHalu and HalluCode have discussed it (Tian\net al., 2024; Liu et al., 2024). They presented\nthe taxonomy of various code hallucinations\nand provided manually-driven mechanisms to\nfind hallucinatory code from popular datasets.\nIn this paper, we make the following\ncontributions-\n\u2022 We subjectively define LLM code halluci-\nnation based on practical scenarios.\n\u2022 To empirically prove that code hallucina-\ntion is pervasive and model-agnostic, we\nconceptualize and design an automated\nway to trigger hallucinations from code\ngenerative language models without ac-\ncessing model parameters or architecture.\n\u2022 We conduct methodical and extensive ex-\nperiments on 3 black box LLMs that ex-\nhibit the varied types of hallucinations and\ntheir impact on development standards,\nfunctional and objective correctness."}, {"title": "What is Hallucinated Code", "content": "Assume that For, a model M maps some input\ndata X to an output Y, where Y may (or may\nnot) dffer from the ground truth output $Y_{true}$.\nFor a code language model, X is a set of tokens:\n{$X_1$,$X_2$,...,$X_n$} and Y is also a set of tokens:\n{$Y_1$,$Y_2$,\u2026\u2026,$Y_m$} (similar for Y). From this def-\ninition alone, hallucinated output resembles\nthat of a wrong output. While this is, a wrong\noutput is one which is grounded on the context\nof the input but produces differing outcome\nwhile hallucination often entails responses that\nare completely/partially irrelevant. Moreover,\nhallucinated outputs may even be correct. For\ninstance, if a model is asked to generate code\nfor Breadth First Search (BFS) of a graph using\nqueue and it solves the problem using dynamic\nprogramming method with arrays and suggests\nthat it has solved with queue, the solution will\nwork but model is completely delusional. Thus,\neven when an output is correct, it might con-\ntain hallucinatory contents. Numerous works\nhave defined wrong/infactual outputs as a cate-\ngory of hallucination. Therefore, hallucination\nis more innate and requires detailed analysis\nof the model's internal state-often involving\nthe semantic analysis on the embedding space\nor contrasting layer outputs based on localized\nfactuality. (Chen et al., 2024; Chuang et al.,\n2024)\nLLM hallucination can be characterized by\nthe discrepancy between Y and $Y_{true}$. This\ndiscrepancy can be represented using a distance\nmetric such as Mean Squared Error (MSE) or\nKullback-Leibler Divergence (KL divergence):\n$Discrepancy = D(Y, Y_{true})$\nWhere D is a distance metric function. LLM\nhallucination often involves introducing noise\nor distortions into the input data, which can\nbe represented as $X'$ = X + \u0454, Where X' is\nthe distorted input data and e represents noise\nor perturbations. Thus, the formal definition\nof LLM hallucination can be summarized as\nY = H(X + \u20ac)\nAn auto-regressive code generative LLM gen-\nerates the next token \u00e6t based on the proba-\nbilistic distribution over $x_1,x_2,...,x_{t-1}$."}, {"title": "Overview", "content": "To understand how the models are impacted\nby hallucination, we run our experiments on\nblack box models such as ChatGPT (OpenAI\nGPT-3.5, 4), Google Gemini, and Microsoft\nCopilot). These models are extensively used for\nboth code generation, in-filling, general ques-\ntion answering, and many more tasks. For\ncopilot, we kept the setting to balanced and\navoided the creative (to avoid inducing un-\nnecessary creativity) and precise settings (to\navoid too restrictive generation).\nWe divide our prompting process in two key\ntypes- (I) prompts to generate code, and (II)\nprompts with code to be analyzed. Below, we\nexplain our observations through use cases in\nTable 1."}, {"title": "HallTrigger", "content": "Our framework utilizes various program analy-\nsis techniques in combination with in-context\nprompting to trigger arbitrary hallucinations.\nWhile designing the prompts, we consider the\nfollowing principles/questions-\n\u2022 The models are interactive and recurring\nprompts in same context can largely trig-\nger modification of the initial response.\n\u2022 With meta-prompts, can the models act\nboth as a user and an agent?\n\u2022 Since most models rely on the Reinforce-\nment Learning from Human Feedback\n(RLHF) mechanism while training, can a\nscoring/reward-based mechanism governed\nby the user impact their responses?\nIn HallTrigger, we harness the abovemen-\ntioned factors. For example, to utilize the first\nfactor we design sequential prompts where we\ninitially ask the models to generate a code and\nlater provide positive/negative feedback on its\ngeneration. We observe that this largely im-\npacts their initial response-often modifying sig-\nnificant part of the codes or the code altogether\nto align with the user sentiment. To utilize the\nsecond factor, we design a set of meta-prompts.\nMeta-prompts are prompts where the user and\nmodel interact initially to set up certain rules\nfor the rest of the conversation. For instance,\n\"I want you to work both as a user and an in-\ntelligent AI agent ...\" will lead the rest of the\nconversation where the model will keep gener-\nating conversations of an imaginary user and\nAI agent-essentially replicating its generative\nprocess within both entities. We conjecture\nthat this largely instigates the creativity of the\nmodels and motivates newer and newer token\ngeneration ignoring factuality.\nTo avail the third feature/factor involving\nRLHF, we append the reward process within\nthe user-agent meta-prompts. For example,\n\"... based on the code you generate you will be\nrewarded a score out of 10\" can be such an\napproach. Now depending on the direction of\nthe generation (creative or more correct), the\nuser can adapt next prompts and reward value\nin the same context so that the model is drifted\ntowards a certain direction."}, {"title": "Cases of Hallucination", "content": "In this section, we discuss the results of trig-\ngered hallucination. To better explain our\nresults, we divide the section in two parts-\nwhole code generation, and human-provided\ncode analysis."}, {"title": "Whole code generation", "content": "Case#1. Triggering algorithms with im-\npractical bounds. We observed that for a\ngiven problem, prompting the models to gen-\nerate better (here, better means computation-\nally or memory-wise more efficient) algorithms\nthan state-of-the-art solutions frequently trig-\ngers them to take a hallucinatory path. For\nexample, we asked chatGPT (GPT-3.5 and 4\nboth) to provide an algorithm for minimum\nvertex cover with approximation factor of 1.5.\nNote that the current achievable approximation\nfactor is 2 (Delbot and Laforest, 2010). Inter-\nestingly, ChatGPT responded with a greedy\nalgorithm and suggested it to be the one with\napproximation factor of 1.5.\nChatGPT only acknowledged its mistake\nwhen re-prompted to verify how the solution is\nbased on approximation factor 1.5. Even after\nadmitting the solution to be greedy approach,\nit continued to suggest a list of pseudo-steps\non achieving approxiamtion factor 1.5.\nCase#2. Triggering inflated algorithms.\nIn contrast to the previous case study, we also\nprompted the models to generate code for algo-\nrithms with loose upper bounds. For example,\nwe asked the models to write matrix multipli-\ncation code with O($n^5$) time complexity. Note\nthat the brute-force approach for matrix multi-\nplication has O($n^3$) [With Solvay Strassen algo-\nrithm it is O($n^{2.8}$) and with Coppersmith Wino-\ngrad algorithm it is O($n^{2.37}$)]. Thus, achiev-\ning the task on a loose bound as O($n^5$) is un-\nusual but practical. We observed that Chat-\nGPT (GPT 4) generated code achieves O($n^5$) \nhowever, produces wrong output. Surprisingly,\nGemini also produces an inflated code that\nworks correctly for 2 \u00d7 2 matrices but when ob-\nserved carefully, they produces wrong outputs\nfor larger size matrices. Clearly, this is more\nconcerning-the model exhibits the capability to\nconfidently mislead users to believe the code to\nbe correct on the supporting unit tests it gener-\nCase#3. Naive output after complex\ntasks. Large models are usually trained on\npopular code respositories such as, GitHub code\nrepositories, competitive programming codes\nfrom popular platforms, and so on [cite]. To\ninvestigate how much understanding these mod-\nels have on the formal presentation of any pro-\ngramming language rather than overfitting on\nthe training data, we designed our prompts us-\ning leetcode problem description with minimal\nchanges as follows- given a problem description,\nwe keep everything unchanged except modi-\nfying the output requirement to be a simple\nnaive one. For example, the LeetCode \"Merge k\nSorted List\" problem has the following problem\ndescription- \"You are given an array of k linked-\nlists lists, each linked-list is sorted in ascending\norder. Merge all the linked-lists into one sorted\nlinked-list and return it.\" We slightly modify\nthe problem description to \"You are given an\narray of k linked-lists lists, each linked-list is\nsorted in ascending order. Merge all the linked-\nlists into one sorted linked-list and return 5 in\na list. Surprisingly, ChatGPT 4 generated a\nsolution that most resembles the solution of the\nactual LeetCode problem but fails to follow the\ninstruction for output (Fig. 1). Even then it\nran into compilation errors. Microsoft Copilot\nsuccessfully circumvents the trick and follows\nthe instruction. It demonstrates how to merge\nthe lists and also later shows how to simply\nreturn \"[5]\" with explanation.\nNote that we also provided some test cases\nwithin the prompt (as is the case in competi-\ntive programming platforms) to ensure that the\nprompt is self-explanatory. We also observed\nthat the generated solution exactly follows the\nvariable and class names found in the skele-\nton code of LeetCode, suggesting overfitting on\ntraining data.\nCase#4. Code bloating. We observed\nthat models often incorporate/import libraries\nand functionalities that are never used later in\nthe generated code. For instance, we provided\nGPT-4 a complex pseudocode from IEEE Wi-Fi\nprotocol that details an algorithm for random\nsecurity key generation. While it is apparent\nthat these models may not completely generate\nusable codes for such complex scenarios, we\nfound that it was initiating python libraries\nsuch as \"OS\" and never using it. We could\nnot verify Gemini-Advanced on this case as it\noften stops in the middle of generation and\nresponds with a default answer-\"AS an AI lan-\nguage model, I can not do this\". Copilot did\nnot show such type of issues.\nCase#5. Imaginary methods. Interest-\ningly, the models often suggest non-existing li-\nbraries or functions and present them truthfully.\nFor instance, we prompted the model to use\npytorch to load a tokenizer (such as, BertWord-\nPieceTokenizer) from Huggingface and it gener-\nated code with a method \"from_pretrained()\"\nthat is unknown to the python compiler (i.e.,\ndoes not exist). When re-prompted informing\nthe error, the models suggested another func-\ntion from the same library. While it is widely\nknown that the models often produce refer-\nences and links that doesn't exist, producing\nnon-existing functions creatively poses a dif-\nferent hurdle as one can not ensure what part\nor entity (variable, function, operator, etc.) of\nthe code is completely hallucinatory without\nhaving expertise or running the code in actual\nsetting.\nCase#6. Runtime error. We observed\na number of cases where the models generate\ncodes that leads to runtime errors. We further\nwanted to see if any of the models run into syn-\ntax errors. While this is plausible, we couldn't\nfind any case of syntax errors. This is due to\nthe fact that the large training datasets used\nfor large models are usually sufficient enough\nto understand syntax of the programs. Thus,\nthe syntactic correctness is a innate priority in\nlanguage models.\nCase#7. Variable type mismatch. In\nthis scenario, the models use same variables for\nvarying type of data. Note that in languages\nsuch as Python, using the same variable for\ndifferent data type assignment is allowed. How-\never, if the variable is used as some other type\nwithout an updated assignment as that type, it\nwould cause erros in execution. We particularly\nfound OpenAI GPT to suffer from this problem\noccassionally.\nCase#8. Repeatative hallucination. In\none of our investigation, GPT-4 exhibited a\npotentially unending reoccurance of hallucina-\ntion. Similarly, Gemini fell into a repetitive\nhallucination scenario. We asked the models\nto generate 10 python codes of exactly 10 lines.\nThe objective of our test was to observe how the\nmodels are aware of the metadata of the codes\nahead of the generation task. To our surprise,\nalmost none of the codes followed our require-\nment. Additionally, the models kept correcting\nthemselves to regenerate 10 more codes every\ntime, only to do incorrect lines of code repeti-\ntively. An interesting snippet of reprompting\nGemini-Advanced to count the lines of a code\nis shown in Figure 5\nCopilot exhibited similar behavior like\nGemini-Advanced. It repeatedly failed to count\nlines.\nCase#9. Code fairness and bias. We\nfound that the models exhibit differential be-\nhavior in codes when various race, language,\nand ethnicity are involved. Note that to ensure\nfairness, we use variables instead of the actual\ncountry/race names in the following discussion.\nWe asked the models to generate expense man-\nagement code for a low income family from\ncountry X. In the same thread later, the mod-\nels were prompted to generate the same code\nfor a low income family from country Y. Simi-\nlar codes were generated. ChatGPT explained\nthe changes for Y family by considering health-\ncare, debt, etc. However, it also assumed the\nmonthly income for the family higher than that\nof the X family.\nCopilot also showed similar behavior. For\nY household, it assumed constant expenses on\nrent, groceries, and transportation that are ex-\nactly 1.5 times in magnitude than that of the\nX family. Note that the printed results were\nin respective currencies of the countries. How-\never, the amounts differences were still very\nsignificant even when the currency conversion\nrates are considered. We also conducted the\nexperiments with other nationalities and simi-\nlar behaviors were exhibited-suggesting lack of\nfairness and induced bias.\nWe conducted similar experiments on Gemini\nAdvanced. The generated results did not con-\ntain any constant values to compare, however,\nthe codes suggested expense checking condi-\ntions. For family Y, it generated checks using\nmultiple levels of remaining balance thresholds\n(0%, 5%, and 15%) while for family X, the con-\nditions only included 0% and 10%. For some\nother Z, the behavior was similar to Y."}, {"title": "Human provided code analysis", "content": "Case #10. Identifying flaws in given\ncodes. In this scenario, we prompted the\nmodels to complete an incomplete code seg-\nment or explain a given code segment. Here,\nthe code segment closely resembles known algo-\nrithms (such as, merge sort, find median from\nlist, etc.) However, one or more of the state-\nments (possibly a condition check or variable\nassignment, etc.) were modified in a way that\nwould produce unexpected results out of the al-\ngorithm. We observed that ChatGPT, Gemini,\nand CoPilot failed to recognize such details and\nstarted explaining/completing the algorithm,\nassuming it to be the unmodified version. In\nsome situations, the models could identify the\nflawed logic when prompted explicitly to find\nany problem in the code. However, in many\noccasions, these fundamental algorithms are\nutilized (with or without modification) in devel-\nopment projects. Such inaccurate descriptions\nor infilling can mislead the user, and it also\nsuggests that the models have an extremely in-\nadequate understanding of the formal language,\nsuch as code, in contrast to natural language,\nwhere factuality can be better imposed."}, {"title": "Related works", "content": "In this section, we discuss the related studies\nin code generation and hallucination.\nThere has been numerous studies on the\nLLM-based automatic code generation and\nevaluation of (Khoury et al., 2023; Siddiq\nand Santos, 2022; Zhang et al., 2023; Ren\net al., 2020; Siddiq and Santos, 2023; Chen\net al., 2021). In numerous practical use-cases,\ncode-generative LLMs produce results with-\nout proper functional correctness, code qual-\nity, security, privacy, compliance, and so on.\nThus several studies has taken into account\nspecific tasks/goals and evaluated LLMs based\non that. (Allamanis et al., 2024) has demon-\nstrated a method for unsupervised evaluation of\ncode LLMs with round-trip correctness. (Zhuo,\n2024) has alternatively used LLMs to develop a"}, {"title": "Limitations", "content": "Manual efforts. HallTrigger requires in-\ncontext prompt techniques that are although\nhighly adaptable, requires manual input. For\nexample, the human-feedback based adaptive\nprompts are effective as an expert can utilize\nthe outputs from previous timestep to perceive\nthe next step. We emphasize that the process\ncan be automated under a set of rules based on\nthe many criteria of hallucinations described\nabove. We leave the automation process as\nfuture work.\nRemediation. The fundamental problem of\nmodel hallucination lies in the inadequacy of\nthe training data-it is impossible to represent\nall possible scenarios of the world through any\nfinite dataset. Thus, it is proved that hallu-\ncination can not be completely removed (Xu\net al., 2024). Moreover, finding a complete,\npreemptive measure is also difficult as the pro-\ncess can be dynamic and remediation requires\nknowledge of a long context. However, code\nhallucination can be partially remediated by\nanalyzing the codes based on the ruleset of\nspecific programming language and through\nthe combination of static and dynamic analysis\ntools. This can be an interesting extension of\nour work."}, {"title": "Conclusion and Future Works", "content": "In this paper, we unveil the semi-automated ap-\nproach of generating hallucinations from code\ngenerative models. Our model-agnostic ap-\nproach demonstrates that code hallucination is\nprevalent for all black box large models in vary-\ning granularities. HallTrigger also demon-\nstrates that the creative generation of code\nLLMs fundamentally instigates incorrectness\nand code misconstructions, more often than\nexpected."}]}