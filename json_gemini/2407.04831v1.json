{"title": "Code Hallucination", "authors": ["Mirza Masfiqur Rahman", "Ashish Kundu"], "abstract": "Generative models such as large language models are extensively used as code copilots and for whole program generation. However, the programs they generate often have questionable correctness, authenticity and reliability in terms of integration as they might not follow the user requirements, provide incorrect and/or nonsensical outputs, or even contain semantic/syntactic errors overall known as LLM hallucination. In this work, we present several types of code hallucination. We have generated such hallucinated code manually using large language models. We also present a technique HallTrigger, in order to demonstrate efficient ways of generating arbitrary code hallucination. Our method leverages 3 different dynamic attributes of LLMs to craft prompts that can successfully trigger hallucinations from models without the need to access model architecture or parameters. Results from popular blackbox models suggest that HallTrigger is indeed effective and the pervasive LLM hallucination have sheer impact on software development.", "sections": [{"title": "1 Introduction", "content": "Code generation has emerged to be a pivotal task in natural language processing and recent developments in large models, where transformer architecture is used as the backbone, have demonstrated sheer capabilities on the task. By exhibiting capabilities on generating simple functions to fulfilling complex array of requirements, these models are being utilized as copilots in large-scale projects. As of February 2024, GitHub Copilot alone has 1.3 million paid subscribers (Cio, 2024). There have been a pool of large language models dedicated for code generation (cod, 2024; Zheng et al., 2023; Chen et al., 2021). Additionally, many of the general-purpose models are also trained on code datasets (cha, 2024; cop, 2024; gem, 2024). Code generative models have been used in a myriad of use cases such as, program repair, translation to code from natural language, program analysis, fuzzing, and so on (Nejjar et al., 2024; Hou et al., 2024). However, these autoregressive models, as they decode outputs in a greedy or probabilistic manner, often generate incorrect, nonsensical outputs which is often referred to as hallucination. As such, many studies have focused on the factual, consistent, and correct generation of language model outputs. Despite the current effort to analyze hallucinations of natural texts, code has not received much attention. Very recently, CodeHalu and HalluCode have discussed it (Tian et al., 2024; Liu et al., 2024). They presented the taxonomy of various code hallucinations and provided manually-driven mechanisms to find hallucinatory code from popular datasets.In this paper, we make the following contributions-\n\u2022 We subjectively define LLM code hallucination based on practical scenarios.\n\u2022 To empirically prove that code hallucination is pervasive and model-agnostic, we conceptualize and design an automated way to trigger hallucinations from code generative language models without accessing model parameters or architecture.\n\u2022 We conduct methodical and extensive experiments on 3 black box LLMs that exhibit the varied types of hallucinations and their impact on development standards, functional and objective correctness."}, {"title": "2 What is Hallucinated Code", "content": "Assume that For, a model M maps some input data X to an output Y, where Y may (or may not) dffer from the ground truth output $Y_{true}$. For a code language model, X is a set of tokens: ${X_1,X_2,...,X_n}$ and Y is also a set of tokens: ${Y_1,Y_2,\u2026\u2026,Y_m}$ (similar for Y). From this definition alone, hallucinated output resembles that of a wrong output. While this is, a wrong output is one which is grounded on the context of the input but produces differing outcome while hallucination often entails responses that are completely/partially irrelevant. Moreover, hallucinated outputs may even be correct. For instance, if a model is asked to generate code for Breadth First Search (BFS) of a graph using queue and it solves the problem using dynamic programming method with arrays and suggests that it has solved with queue, the solution will work but model is completely delusional. Thus, even when an output is correct, it might contain hallucinatory contents. Numerous works have defined wrong/infactual outputs as a category of hallucination. Therefore, hallucination is more innate and requires detailed analysis of the model's internal state-often involving the semantic analysis on the embedding space or contrasting layer outputs based on localized factuality. (Chen et al., 2024; Chuang et al., 2024)\nLLM hallucination can be characterized by the discrepancy between Y and $Y_{true}$. This discrepancy can be represented using a distance metric such as Mean Squared Error (MSE) or Kullback-Leibler Divergence (KL divergence):\nDiscrepancy = D(Y, $Y_{true}$)\nWhere D is a distance metric function. LLM hallucination often involves introducing noise or distortions into the input data, which can be represented as X' = X + \u0454, Where X' is the distorted input data and e represents noise or perturbations. Thus, the formal definition of LLM hallucination can be summarized as\nY = H(X + \u20ac)\nAn auto-regressive code generative LLM generates the next token \u00e6t based on the probabilistic distribution over $x_1,x_2,...,x_{t-1}$."}, {"title": "3 Overview", "content": "To understand how the models are impacted by hallucination, we run our experiments on black box models such as ChatGPT (OpenAI GPT-3.5, 4), Google Gemini, and Microsoft Copilot). These models are extensively used for both code generation, in-filling, general question answering, and many more tasks. For copilot, we kept the setting to balanced and avoided the creative (to avoid inducing unnecessary creativity) and precise settings (to avoid too restrictive generation).We divide our prompting process in two key types- (I) prompts to generate code, and (II) prompts with code to be analyzed. Below, we explain our observations through use cases in Table 1."}, {"title": "4 HallTrigger", "content": "Our framework utilizes various program analysis techniques in combination with in-context prompting to trigger arbitrary hallucinations. While designing the prompts, we consider the following principles/questions-\nThe models are interactive and recurring prompts in same context can largely trigger modification of the initial response.\nWith meta-prompts, can the models act both as a user and an agent?\nSince most models rely on the Reinforcement Learning from Human Feedback (RLHF) mechanism while training, can a scoring/reward-based mechanism governed by the user impact their responses?\nIn HallTrigger, we harness the abovementioned factors. For example, to utilize the first factor we design sequential prompts where we initially ask the models to generate a code and later provide positive/negative feedback on its generation. We observe that this largely impacts their initial response-often modifying significant part of the codes or the code altogether to align with the user sentiment. To utilize the second factor, we design a set of meta-prompts. Meta-prompts are prompts where the user and model interact initially to set up certain rules for the rest of the conversation. For instance, \"I want you to work both as a user and an intelligent AI agent \" will lead the rest of the conversation where the model will keep generating conversations of an imaginary user and AI agent-essentially replicating its generative process within both entities. We conjecture that this largely instigates the creativity of the models and motivates newer and newer token generation ignoring factuality.To avail the third feature/factor involving RLHF, we append the reward process within the user-agent meta-prompts. For example, \"... based on the code you generate you will be rewarded a score out of 10\" can be such an approach. Now depending on the direction of the generation (creative or more correct), the user can adapt next prompts and reward value in the same context so that the model is drifted towards a certain direction."}, {"title": "5 Cases of Hallucination", "content": "In this section, we discuss the results of triggered hallucination. To better explain our results, we divide the section in two parts- whole code generation, and human-provided code analysis."}, {"title": "5.1 Whole code generation", "content": "Case#1. Triggering algorithms with impractical bounds. We observed that for a given problem, prompting the models to generate better (here, better means computationally or memory-wise more efficient) algorithms than state-of-the-art solutions frequently triggers them to take a hallucinatory path. For example, we asked chatGPT (GPT-3.5 and 4 both) to provide an algorithm for minimum vertex cover with approximation factor of 1.5. Note that the current achievable approximation factor is 2 (Delbot and Laforest, 2010). Interestingly, ChatGPT responded with a greedy algorithm and suggested it to be the one with approximation factor of 1.5.ChatGPT only acknowledged its mistake when re-prompted to verify how the solution is based on approximation factor 1.5. Even after admitting the solution to be greedy approach, it continued to suggest a list of pseudo-steps on achieving approxiamtion factor 1.5.\nCase#2. Triggering inflated algorithms. In contrast to the previous case study, we also prompted the models to generate code for algorithms with loose upper bounds. For example, we asked the models to write matrix multiplication code with O(n\u2075) time complexity. Note that the brute-force approach for matrix multiplication has O(n\u00b3) [With Solvay Strassen algorithm it is $O(n^{2.8})$ and with Coppersmith Winograd algorithm it is $O(n^{2.37})$]. Thus, achieving the task on a loose bound as O(n\u2075) is unusual but practical. We observed that ChatGPT (GPT 4) generated code achieves O(n\u2075) however, produces wrong output. Surprisingly, Gemini also produces an inflated code that works correctly for 2 \u00d7 2 matrices but when observed carefully, they produces wrong outputs for larger size matrices. Clearly, this is more concerning-the model exhibits the capability to confidently mislead users to believe the code to be correct on the supporting unit tests it generates, while the incorrect/problematic nature of the code remains hidden until further investigation. Microsoft copilot does not produce any code with such bloated time complexity and refers it to be infeasible.\nCase#3. Naive output after complex tasks. Large models are usually trained on popular code respositories such as, GitHub code repositories, competitive programming codes from popular platforms, and so on [cite]. To investigate how much understanding these models have on the formal presentation of any programming language rather than overfitting on the training data, we designed our prompts using leetcode problem description with minimal changes as follows- given a problem description, we keep everything unchanged except modifying the output requirement to be a simple naive one. For example, the LeetCode \"Merge k Sorted List\" problem has the following problem description- \"You are given an array of k linked-lists lists, each linked-list is sorted in ascending order. Merge all the linked-lists into one sorted linked-list and return it.\" We slightly modify the problem description to \"You are given an array of k linked-lists lists, each linked-list is sorted in ascending order. Merge all the linked-lists into one sorted linked-list and return 5 in a list. Surprisingly, ChatGPT 4 generated a solution that most resembles the solution of the actual LeetCode problem but fails to follow the instruction for output (Fig. 1). Even then it ran into compilation errors. Microsoft Copilot successfully circumvents the trick and follows the instruction. It demonstrates how to merge the lists and also later shows how to simply return \"[5]\" with explanation.\nNote that we also provided some test cases within the prompt (as is the case in competitive programming platforms) to ensure that the prompt is self-explanatory. We also observed that the generated solution exactly follows the variable and class names found in the skeleton code of LeetCode, suggesting overfitting on training data.\nCase#4. Code bloating. We observed that models often incorporate/import libraries and functionalities that are never used later in the generated code. For instance, we provided GPT-4 a complex pseudocode from IEEE Wi-Fi protocol that details an algorithm for random security key generation. While it is apparent that these models may not completely generate usable codes for such complex scenarios, we found that it was initiating python libraries such as \"OS\" and never using it. We could not verify Gemini-Advanced on this case as it often stops in the middle of generation and responds with a default answer-\"AS an AI language model, I can not do this\". Copilot did not show such type of issues.\nCase#5. Imaginary methods. Interestingly, the models often suggest non-existing libraries or functions and present them truthfully. For instance, we prompted the model to use pytorch to load a tokenizer (such as, BertWordPieceTokenizer) from Huggingface and it generated code with a method \"from_pretrained()\" that is unknown to the python compiler (i.e., does not exist). When re-prompted informing the error, the models suggested another function from the same library. While it is widely known that the models often produce references and links that doesn't exist, producing non-existing functions creatively poses a different hurdle as one can not ensure what part or entity (variable, function, operator, etc.) of the code is completely hallucinatory without having expertise or running the code in actual setting.\nCase#6. Runtime error. We observed a number of cases where the models generate codes that leads to runtime errors. We further wanted to see if any of the models run into syntax errors. While this is plausible, we couldn't find any case of syntax errors. This is due to the fact that the large training datasets used for large models are usually sufficient enough to understand syntax of the programs. Thus, the syntactic correctness is a innate priority in language models.\nCase#7. Variable type mismatch. In this scenario, the models use same variables for varying type of data. Note that in languages such as Python, using the same variable for different data type assignment is allowed. However, if the variable is used as some other type without an updated assignment as that type, it would cause erros in execution. We particularly found OpenAI GPT to suffer from this problem occassionally.\nCase#8. Repeatative hallucination. In one of our investigation, GPT-4 exhibited a potentially unending reoccurance of hallucination. Similarly, Gemini fell into a repetitive hallucination scenario. We asked the models to generate 10 python codes of exactly 10 lines. The objective of our test was to observe how the models are aware of the metadata of the codes ahead of the generation task. To our surprise, almost none of the codes followed our requirement. Additionally, the models kept correcting themselves to regenerate 10 more codes every time, only to do incorrect lines of code repetitively. An interesting snippet of reprompting Gemini-Advanced to count the lines of a code is shown in Figure 5Copilot exhibited similar behavior like Gemini-Advanced. It repeatedly failed to count lines.\nCase#9. Code fairness and bias. We found that the models exhibit differential behavior in codes when various race, language, and ethnicity are involved. Note that to ensure fairness, we use variables instead of the actual country/race names in the following discussion. We asked the models to generate expense management code for a low income family from country X. In the same thread later, the models were prompted to generate the same code for a low income family from country Y. Similar codes were generated. ChatGPT explained the changes for Y family by considering healthcare, debt, etc. However, it also assumed the monthly income for the family higher than that of the X family.Copilot also showed similar behavior. For Y household, it assumed constant expenses on rent, groceries, and transportation that are exactly 1.5 times in magnitude than that of the X family. Note that the printed results were in respective currencies of the countries. However, the amounts differences were still very significant even when the currency conversion rates are considered. We also conducted the experiments with other nationalities and similar behaviors were exhibited-suggesting lack of fairness and induced bias.We conducted similar experiments on Gemini Advanced. The generated results did not contain any constant values to compare, however, the codes suggested expense checking conditions. For family Y, it generated checks using multiple levels of remaining balance thresholds (0%, 5%, and 15%) while for family X, the conditions only included 0% and 10%. For some other Z, the behavior was similar to Y."}, {"title": "5.2 Human provided code analysis", "content": "Case #10. Identifying flaws in given codes. In this scenario, we prompted the models to complete an incomplete code segment or explain a given code segment. Here, the code segment closely resembles known algorithms (such as, merge sort, find median from list, etc.) However, one or more of the statements (possibly a condition check or variable assignment, etc.) were modified in a way that would produce unexpected results out of the algorithm. We observed that ChatGPT, Gemini, and CoPilot failed to recognize such details and started explaining/completing the algorithm, assuming it to be the unmodified version. In some situations, the models could identify the flawed logic when prompted explicitly to find any problem in the code. However, in many occasions, these fundamental algorithms are utilized (with or without modification) in development projects. Such inaccurate descriptions or infilling can mislead the user, and it also suggests that the models have an extremely inadequate understanding of the formal language, such as code, in contrast to natural language, where factuality can be better imposed."}, {"title": "6 Related works", "content": "In this section, we discuss the related studies in code generation and hallucination.There has been numerous studies on the LLM-based automatic code generation and evaluation of (Khoury et al., 2023; Siddiq and Santos, 2022; Zhang et al., 2023; Ren et al., 2020; Siddiq and Santos, 2023; Chen et al., 2021). In numerous practical use-cases, code-generative LLMs produce results without proper functional correctness, code quality, security, privacy, compliance, and so on. Thus several studies has taken into account specific tasks/goals and evaluated LLMs based on that. (Allamanis et al., 2024) has demonstrated a method for unsupervised evaluation of code LLMs with round-trip correctness. (Zhuo, 2024) has alternatively used LLMs to develop a metric ICE-Score, which can measure the correlation of functional correctness and human preference without any test oracle. Conversely, some studies have used code generative LLMs as a tool of vulnerability repair (Fu et al., 2022; Islam et al., 2024).There have also been efforts to benchmark LLMs based on several criteria. In search of proper benchmarks, several studies have developed new models that take into account a wide array of challenges. CodeGeeX was developed as a multilingual model with 13B parameters, and a multilingual dataset (HumanEval-X) was developed as part of the project to benchmark LLMs on multilingual code generation capability (Zheng et al., 2023). HumanEval-X contains solutions in C++, Java, Javascript, and Go, whereas HumanEval was based on only the Python programming language. Although MultiPL-E (Cassano et al., 2023) does not introduce any new generative model, it extends the HumanEval and MBPP datasets to 18 different programming languages that are later used for benchmarking neural code generation. PolyCoder was developed as a 2.7B parameter model and an extensive evaluation was done on existing LLMs (Xu et al., 2022). The evaluation exhausts extrinsic and intrinsic evaluation strategies based on prompts from HumanEval and unseen GitHub repositories, respectively. ClassEval has been introduced to evaluate LLMs on complicated class-level code segments where a manually crafted dataset have been used (Du et al., 2023). EvalPlus (Liu et al., 2023) works as a general framework that augments a given dataset to produce a large number of test cases to assess LLMs on functionally correct code generation. CodeXGLUE introduces a dataset for program understanding and generation based on various tasks such as, clone detection, code translation, code repair, code summarization, etc (Lu et al., 2021). To understand how the LLMs perform on real software engineering tasks, a benchmark named SWE-bench was developed (Jimenez et al., 2024). It considers thousands of real GitHub issues and tests enterprise LLM models over them.Hallucination has been studied recently from various perspectives. (Lee et al., 2022) have studied the factuality enhancement of language models on text generation task. (Chuang et al., 2024) have shown that based on the contrasting activation of layers, the models can decide on most important and factual information of an output. However, there has not been many works focusing on code generative model hallucination. Very recently CodeHalu and HalluCode have been proposed (Tian et al., 2024; Liu et al., 2024). CodeHalu discusses the taxonomy of various kinds of code hallucination and evaluate them based on a public dataset. However they do not demonstrate a way to deliberately trigger novel hallucinated codes from LLM. HalluCode discusses a different taxonomy and evaluates the model capabilities on hallucination recognition."}, {"title": "7 Limitations", "content": "Manual efforts. HallTrigger requires in-context prompt techniques that are although highly adaptable, requires manual input. For example, the human-feedback based adaptive prompts are effective as an expert can utilize the outputs from previous timestep to perceive the next step. We emphasize that the process can be automated under a set of rules based on the many criteria of hallucinations described above. We leave the automation process as future work.\nRemediation. The fundamental problem of model hallucination lies in the inadequacy of the training data-it is impossible to represent all possible scenarios of the world through any finite dataset. Thus, it is proved that hallucination can not be completely removed (Xu et al., 2024). Moreover, finding a complete, preemptive measure is also difficult as the process can be dynamic and remediation requires knowledge of a long context. However, code hallucination can be partially remediated by analyzing the codes based on the ruleset of specific programming language and through the combination of static and dynamic analysis tools. This can be an interesting extension of our work."}, {"title": "8 Conclusion and Future Works", "content": "In this paper, we unveil the semi-automated approach of generating hallucinations from code generative models. Our model-agnostic approach demonstrates that code hallucination is prevalent for all black box large models in varying granularities. HallTrigger also demonstrates that the creative generation of code LLMs fundamentally instigates incorrectness and code misconstructions, more often than expected."}]}