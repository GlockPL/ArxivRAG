{"title": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model", "authors": ["Zhen Yang", "Jinhao Chen", "Zhengxiao Du", "Wenmeng Yu", "Weihan Wang", "Wenyi Hong", "Zhihuan Jiang", "Bin Xu", "Yuxiao Dong", "Jie Tang"], "abstract": "Large language models (LLMs) have demonstrated significant capabilities in mathematical reasoning, particularly with text-based mathematical problems. However, current multi-modal large language models (MLLMs), especially those specialized in mathematics, tend to focus predominantly on solving geometric problems but ignore the diversity of visual information available in other areas of mathematics. Moreover, the geometric information for these specialized mathematical MLLMs is derived from several public datasets, which are typically limited in diversity and complexity. To address these limitations, we aim to construct a fine-tuning dataset named MathVL, and develop a series of specialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised Fine-Tuning (SFT) on MathVL with various parameter-scale backbones. To extensively evaluate the effectiveness of MathGLM-Vision, we conduct experiments on several public benchmarks and our curated MathVL-test consisting of 2,000 problems. Experimental results demonstrate that MathGLM-Vision achieves significant improvements compared with some existing models, including backbone models and open-source mathematical MLLMs. These findings indicate the importance of diversity dataset in enhancing the mathematical reasoning abilities of MLLMs.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in computational linguistics have led to substantial progress in solving mathematical problems using Large Language Models (LLMs) with multi-step reasoning processes [32]. For example, models like GPT-4 [1], Qwen [4], GLM-4 [50], LLaMA [51, 52] have demonstrated impressive performance on mathematical datasets such as GSM8K [15] and MATH [24]. Furthermore, the development of specialized mathematical models is expanding the potential of LLMs in this domain. These models, specifically designed for mathematical problem solving, include notable contributions such as WizardMath [39], MAmmoTH [63], MathCoder [53], MetaMath [61], DeepSeekMath [45], and others [58, 62, 23, 65, 41, 60]. These advancements highlight the growing proficiency of LLMs in handling intricate mathematical reasoning and problem-solving tasks.\nDespite significant advancements, the majority of models designed for mathematical problem solving still rely predominately on textual representations. This limits their effectiveness in scenarios that require visual information. Notably, approximately 63% of mathematics questions in Chinese K12 education include visual elements, highlighting the critical role of visual information in comprehending and solving mathematical problems."}, {"title": "2 MathVL Dataset", "content": "To enhance the capabilities of MLLMs in solving mathematical problems, previous efforts [10, 11, 9, 19] focus on constructing high-quality datasets. Nevertheless, the majority of these datasets fall into the category of Visual Question Answering (VQA), which generally involves descriptive or identification tasks rather than conventional mathematical problems. Furthermore, the answers in some public datasets like Geometry3K [37], GeoGPT4V [8], MathV360K [46] for standard mathematical questions are often too simplistic, usually providing only the final answer without the intermediate steps necessary for a thorough understanding. It is well-established that including step-by-step solutions can significantly enhance the reasoning capabilities of large language models [57, 32, 68, 55].\nTo address these issues, we construct a fine-tuning dataset MathVL, including both several public datasets and our curated Chinese dataset collected from K12 education levels. This dataset is meticulously crafted to encompass a diverse array of mathematical problems that incorporate visual information. Each problem is presented with detailed step-by-step solutions, aiming to enhance"}, {"title": "3 MathGLM-Vision", "content": "Model Architecture. We employ CogVLM [56] and GLM-4V-9B [22] architectures as our backbone models, and conduct Supervised Fine-Tuning (SFT) on our constructed MathVL dataset. Specifically, we utilize three pre-trained multi-modal large language models (MLLMs) for the fine-tuning process: GLM-4V-9B, CogVLM2-19B, and CogVLM-32B. This results in the development of three distinct variants of MathGLM-Vision, designated as MathGLM-Vision-9B, MathGLM-Vision-19B, and MathGLM-Vision-32B, respectively.\nModel Training. To maintain the general vision-language understanding skills of MathGLM-Vision, we incorporate 19 open-source visual question-answering datasets (VQA datasets) into the MathVL dataset. These datasets are meticulously selected to challenge and enhance the model's ability to interpret and integrate visual and textual information, ensuring it retains a broad understanding across"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nEvaluation Datasets. We assess our MathGLM-Vision using three well-established public benchmark datasets (MathVista [38], MathVerse [67], and Math-Vision [54] datasets) alongside our specially curated dataset MathVL-test. The MathVL-test dataset comprises 2,000 sampled cases, distinct from those in the MathVL dataset, ensuring a rigorous and unbiased evaluation of MathGLM-Vision's capabilities. Besides, we adopt the testing protocol [19] and utilize MathVista-GPS dataset to evaluate MathGLM-Vision's ability to solve geometry problems. Additionally, we evaluate MathGLM-Vision's general vision-language understanding skills using the MMMU benchmark [64].\nCompared Models. We compare MathGLM-Vision with other Multi-Modal Large Language Models (MLLMs), including closed-source MLLMs such as Gemini [49], GPT-4V [42], Claude3 [2], and Qwen-VL [5], and open-source MLLMs like mPLUG-Owl [59], LLaMA-Adapter-V2 [20], InstrctBLIP [16], LLaVA-1.5 [33], ShareGPT4V [12], SPHINX [21], InternLM-XC2 [17], and InternVL [13]. Additionally, we compare MathGLM-Vision with recent specialized mathematical MLLMs, including G-LLaVA [19], LLaVA-1.5-G [8], ShareGPT4V-G [8], and Math-LLaVA [46].\nEvaluation Metrics. We adopt top-1 accuracy to evaluate our MathGLM-Vision across MathVista-GPS, MathVista, MathVerse, MathVision, and MathVL-test. Our evaluation process follows the pipeline outlined in the aforementioned benchmark datasets, which involves using LLMs to extract predicted answers from the model's responses. Accuracy is then calculated by comparing these extracted answers against the ground truths."}, {"title": "4.2 Main Results", "content": "Results on public benchmark datasets. To comprehensively assess the ability of MathGLM-Vision in solving mathematical problems, we evaluate its performance against other MLLMs across several public benchmark datasets, including MathVista-GPS [38], the testmini subset of MathVista [38], MathVerse [67], and Math-Vision [54]. The experimental results indicate that our constructed MathVL dataset can significantly improve MathGLM-Vision's mathematical reasoning capabilities. For example, MathGLM-Vision-9B achieves a 64.42% accuracy on the MathVista-GPS dataset, marking a substantial 39.68% improvement over its backbone model, GLM-4V-9B. Besides, across various parameter scales, MathGLM-Vision consistently surpass all backbone models on different evaluation benchmarks, highlighting the significant enhancements that MathVL brings to the MathGLM-Vision's problem-solving skills. Notably, MathGLM-Vision outperforms all open-source specialized mathematical MLLMs across various benchmarks. The superior performance suggests that the high-quality and diverse data, complete with detailed step-by-step solutions, are crucial for improving MLLM's mathematical reasoning capabilities. More importantly, MathGLM-Vision-32B outperforms even the advanced"}, {"title": "4.3 Generalizability of MathGLM-Vision", "content": "In addition to its proficiency in mathematical reasoning, we further assess MathGLM-Vision's capabilities in general vision-language understanding by conducting experiments on the MMMU benchmark [64]. This benchmark is specifically designed to evaluate the ability of models to comprehend and process information across a variety of academic and professional disciplines, providing a comprehensive test of general vision-language understanding.\nIn addition to its proficiency in mathematical reasoning, we further assess MathGLM-Vision's capabilities in general vision-language understanding by conducting experiments on the MMMU benchmark [64]. This benchmark is specifically designed to evaluate the ability of models to compre-hend and process information across a variety of academic and professional disciplines, providing a comprehensive test of general vision-language understanding. We show the performance of MathGLM-Vision, a specific variant fine-tuned exclusively on MathVL without the inclusion of VQA datasets, and backbone models. Compared to CogVLM2, MathGLM-Vision-19B achieves comparable performance in terms of generalizability, underscoring its capacity for simultaneous multi-modal understanding and mathematical reasoning. However, MathGLM-Vision-32B shows a slight reduction in performance across multiple categories on the MMMU benchmark. Besides, MathGLM-Vision, when fine-tuned with VQA datasets, outperforms its variant lacking VQA datasets. This indicates that omitting VQA datasets from the fine-tuning process limits the general vision-language understanding abilities. Thus, the Specialized Fine-Tuning (SFT) process using our MathVL incorporated with VQA datasets not only enhances MathGLM-Vision's mathematical reasoning abilities but also preserves its generalizability."}, {"title": "4.4 Further Analysis", "content": "Effect of Chinese Dataset. To validate the effectiveness of the adopted Chinese dataset in MathVL, we conduct an extend experiment that involves fine-tuning GLM-4V-9B with open-source datasets, deliberately excluding Chinese data collected from K12 education. The purpose of this experiment is to assess the specific contributions of the Chinese dataset to the capabilities of MathGLM-Vision. shows a comparison of performance results. Compared to the backbone model GLM-4V-9B, a variant MathGLM-Vision-9B that undergoes Specialized Fine-Tuning (SFT) exclusively with open-source data exhibits significant improvement on the minitest of MathVista, particularly in geometry problem solving (GPS) and geometry reasoning (GEO). This indicates that fine-tuning on diverse open-source data can markedly enhance model performance in specific mathematical areas. MathGLM-Vision, incorporating both open-source data and Chinese data, outperforms the variant tuned only with open-source data on the minitest of MathVista, highlighting the significant value added by integrating the Chinese dataset in the training process. Notably, compared to the variant without Chinese data, MathGLM-Vision achieves a significantly higher accuracy on the MathVL-test. These findings confirm that the inclusion of the Chinese dataset not only enhances the model's capability in handling complex mathematical problems but also contributes significantly to its overall performance on a diverse set of tasks within MathVista.\nEffect of VQA Datasets. To explore the effect of VQA datasets on the performance of MathGLM-Vision, an extended experiment can be designed where Specialized Fine-Tuning (SFT) is applied exclusively to mathematical datasets, deliberately excluding VQA datasets.Table 8 demonstrates the performance comparison achieved by different models on MathVista. Compared to the backbone model GLM-4V-9B, a variant of MathGLM-Vision-9B achieves significant improvements on geom-etry problem solving (GPS) and geometry Reasoning (GEO). However, it exhibits a decline in the overall accuracy on the minitest of MathVista (ALL). The decline can be attributed to the composition of MathVista, which comprises five tasks, with question-answering types (such as graphical question-answering, textbook question-answering, and visual question-answering) comprising up to 60.6% of the tasks. Omitting VQA training in MathGLM-Vision impacts the model's ability to effectively process and respond to these multi-modal questions. Notably, within specific subsets of MathVista, such as GPS and GEO, a variant of MathGLM-Vision-9B slightly below the standard MathGLM-Vision-9B. This observation suggests that VQA datasets are crucial for preserving over-all multi-modal understanding, their impact may vary depending on different task types. Besides, VQA datasets can indirectly bolster mathematical reasoning skills, which in turn enhances image recognition capabilities."}, {"title": "4.5 Error Analysis", "content": "We meticulously analyze the causes of errors in MathGLM-Vision-32B on the MathVL-test dataset and illustrate the distribution of these errors We summarize these errors in MathGLM-Vision-32B into five types: reasoning error, knowledge error, vision recognition error, calculation error, and question misunderstood error. The most prevalent type of errors, accounting for 69.1% of the total, is identified as Reasoning Error. This indicates a significant challenge in the MathGLM-Vision-32B's logical deductions and inferential reasoning. Improving these capabilities can dramatically enhance the MathGLM-Vision-32B's overall performance. Knowledge Error, which made up 12.7% of the errors, relates to the model's misapplicaion or lack of specific factual information. Vision Recognition Error accounts for 11.4% of the total errors and involves inaccuracies in interpreting visual data. This type of error can be reduced through the implementation of more advanced vision"}, {"title": "5 Related Works", "content": "5.1 Multi-Modal Language Model\nThe development of Multi-Modal Language Models (MLLMs) have emerged as a significant area of research, which are designed to integrate information from multiple modalities\u2014typically text and images\u2014to perform tasks that require a holistic understanding of both visual and linguistic inputs. Pioneering efforts such as ViLBERT [36] and LXMERT [48] have advanced this field by conducting the joint pre-training on image-text datasets. They process text and image inputs separately before fusing them for final task layers, significantly improving performance on tasks like image captioning and visual question answering. The continues evolution of MLLMs has lead to innovations in data fusion techniques. Notable models such as CLIP [44], ALIGN [26], and BLIP [29] have adopted contrastive learning paradigms to align visual and language information from billions of image-text pairs. Concurrently, the success of LLMs [7, 18, 66, 28, 6, 51, 43, 25, 47, 14] facilitates the integration of LLMs into multi-modal tasks by utilizing pre-training alignment and visual instruction tuning, leading to the emergence of multi-modal language models (MLLMs) [35, 33, 56, 30, 16, 4]. Despite MLLMs have demonstrated remarkable capabilities on tasks such as image caption and visual question answering, they stall face significant challenges in solving mathematical problems that involve visual information [64, 38, 67, 54].\n5.2 Mathematical Reasoning\nRecently, math-specific LLMs [3, 53, 65, 60, 61, 63, 62, 39] have demonstrated remarkable abilities in handling mathematical reasoning tasks that only involve textual information. These models have been specifically trained on web-scale instruction mathematical dataset or fine-tuned on specialized mathematical problem sets. For instance, WizardMath [39] and MetaMath [61] have implemented data augmentation methods to enhance the models' ability to understand and solve mathematical problems by enriching the MATH [24] and GSM8K [15] datasets. Recent research has also focused on creating specialized MLLMs for mathematical tasks. UniGeo [11] and UniMath [31] have demonstrated enhanced datasets and conventional deep learning approaches for geometric problem solving. MLLMs like G-LLaVA [19], GeoGPT4V [8], and Math-LLaVA [46] are tailored for"}, {"title": "6 Conclusion", "content": "In this paper, we attempt to address the issues in current mathematical MLLMs. We construct a fine-tuning dataset named MathVL, upon which we conduct a Supervised Fine-Tuning (SFT) process. This initiative results in the development of a series of enhanced MLLMs, designated as MathGLM-Vision. Specially, MathGLM-Vision contains three variations: MathGLM-Vision-9B, MathGLM-Vision-19B, and MathGLM-Vision-32B, each fine-tuned on different backbone models: GLM-4-V, CogVLM2, and CogVLM-32B, respectively. These developed MathGLM-Vision significantly improve the capabilities of mathematical reasoning, achieving substantial performance improvements. Relative to their respective backbone models, MathGLM-Vision-9B, MathGLM-Vision-19B, and MathGLM-Vision-32B show improvements of 39%, 65%, and 53.7% on the Geometry Problem Solving (GPS) minitest split of MathVista, demonstrating the effectiveness of MathVL in enhancing the mathematical problem-solving abilities of MLLMs. Additionally, we evaluate the effectiveness of MathGLM-Vision on our curated MathVL-test dataset. Experimental results reveal that MathGLM-Vision not only surpass their backbone models in specialized mathematical tests but also preserve the generalizability capabilities in general vision-language understanding domains."}]}