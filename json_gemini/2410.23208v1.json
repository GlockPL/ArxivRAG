{"title": "KINETIX: INVESTIGATING THE TRAINING OF GENERAL AGENTS THROUGH OPEN-ENDED PHYSICS-BASED CONTROL TASKS", "authors": ["Michael Matthews", "Michael Beukman", "Chris Lu", "Jakob Foerster"], "abstract": "While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge. In this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control. To this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework. Kinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training. Our trained agent exhibits strong physical reasoning capabilities, being able to zero-shot solve unseen human-designed environments. Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent tabula rasa. This includes solving some environments that standard RL training completely fails at. We believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.", "sections": [{"title": "1 INTRODUCTION", "content": "The development of a general agent, capable of performing competently in unseen domains, has been a long-standing goal in machine learning (Newell et al., 1959; Minsky, 1961; Lake et al., 2017). One perspective is that large transformers, trained on vast amounts of offline text and video data, will ultimately achieve this goal (Brown et al., 2020; Bubeck et al., 2023; Mirchandani et al., 2023). However, applying these techniques in an offline reinforcement learning (RL) setting often constrains agent capabilities to those found within the dataset (Levine et al., 2020; Kumar et al., 2020). An alternative approach is to use online RL, where the agent gathers its own data through interaction with an environment. However, with some notable exceptions (Team et al., 2021; 2023), most RL environments represent a narrow and homogeneous set of scenarios (Todorov et al., 2012; Bellemare et al., 2013; Brockman et al., 2016; Cobbe et al., 2019), limiting the generalisation ability of the trained agents (Kirk et al., 2023).\nIn this paper, we aim to address this limitation by introducing Kinetix: a framework for representing the vast, open-ended space of 2D physics-based environments, and using it to train a general agent. Kinetix is broad enough to represent robotics tasks like grasping (Rajeswaran et al., 2017) and locomotion (Todorov et al., 2012), classic RL environments such as Cartpole (Barto et al., 1983), Acrobot (DeJong & Spong, 1994) and Lunar Lander (Brockman et al., 2016), as well as video games like Pinball (Bellemare et al., 2013), along with the multitude of tasks that lie in the intervening space (see Figure 1). To run the backend of Kinetix we developed Jax2D, a hardware-accelerated physics engine that allows us to efficiently simulate the billions of environment interactions required to train this agent."}, {"title": "2 BACKGROUND", "content": null}, {"title": "2.1 REINFORCEMENT LEARNING", "content": "We model the decision-making process as a Markov Decision Process (MDP), which is defined as a tuple $(\\mathcal{S}, \\mathcal{A}, R, T)$, where $\\mathcal{S}$ is the set of states; $\\mathcal{A}$ is the set of actions; $T : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta \\mathcal{S}$ is the transition function, defining the distribution over next states $T(s, a)$ given a current state $s$ and action $a$; and $R: \\mathcal{S} \\rightarrow \\mathbb{R}$ is the reward function. We consider finite-horizon MDPs, with a maximum"}, {"title": "2.2 UNSUPERVISED ENVIRONMENT DESIGN", "content": "Unsupervised Environment Design (UED) is a paradigm where learning is phrased as a two-player game between a teacher and a student. The student maximises its expected discounted return as in the standard RL formulation, while the teacher chooses levels to maximise some utility function, effectively inducing a curriculum of tasks through training (Oudeyer et al., 2007; Florensa et al., 2018; Matiisen et al., 2020; Narvekar et al., 2020; Dennis et al., 2020; Parker-Holder et al., 2022). In this paper, these tasks (we also refer to these as levels or environments) are particular initial states, $s_0 \\in \\mathcal{S}$. One common approach sets a level's utility as the negative of the agent's return (Pinto et al., 2017), and another class of approaches instead uses regret (Dennis et al., 2020). Domain Randomisation (Jakobi, 1997; Tobin et al., 2017, DR), where levels are sampled from an uninformed distribution, can be considered a degenerate form of this paradigm, where a constant utility is assigned to each level. More recently, Tzannetos et al. (2023) and Rutherford et al. (2024) sample levels in binary-outcome domains using learnability, defined as $p(1 - p)$, with $p$ being the success rate of the agent on the particular level. In this way, learnability disincentivises the teacher from sampling levels that the agent cannot solve at all (where $p = 0$) or where the agent can already perfectly solve them ($p = 1$), meaning that the agent trains on levels with a high learning potential."}, {"title": "2.3 RL IN JAX", "content": "JAX (Bradbury et al., 2018) is a Python library for writing parallelisable code for hardware ac- celerators. While deep RL has traditionally been divided between environments on the CPU and models on the GPU (Mnih et al., 2015; Espeholt et al., 2018), JAX has facilitated the development of GPU-based environments (Lange, 2022; Rutherford et al., 2023; Nikulin et al., 2023; Matthews et al., 2024; Kazemkhani et al., 2024), allowing the entire RL pipeline to run on a hardware accelera- tor (Hessel et al., 2021). Through massive parallelisation and elimination of CPU-GPU transfer, this gives tremendous speed benefits (Lu et al., 2022). While UED has also followed this trend (Jiang et al., 2023; Coward et al., 2024), experiments have largely been confined to simple gridworlds, due to the lack of any suitable alternative (Garcin et al., 2024; Rutherford et al., 2024)."}, {"title": "2.4 TRANSFORMERS AND PERMUTATION INVARIANT REPRESENTATIONS", "content": "Transformers and Attention Transformers (Vaswani et al., 2017) use the attention mecha- nism (Bahdanau et al., 2015) to model interactions within a set. Given $N$ embeddings, $x_i \\in \\mathbb{R}^n$, self-attention computes queries $q_i$, keys $k_i$, and values $v_i$ for each element through linear projec- tions. Weights for each element $i$ relative to element $j$ are calculated as $w_{i,j}=q_i.k_j$ and nor- malised via softmax to get $W_{i,j}$. The new embedding for element $i$ is a weighted sum of the values: $x_i^{new}=\\sum_{j=1}^{N} W_{i,j}v_j$, allowing each element to attend to others. The common practice of adding positional embeddings to encode sequence order (Vaswani et al., 2017) may obfuscate the fact that transformers are permutation invariant and naturally operate on sets.\nTransformers in RL While recurrent policies have been long popular in deep RL to help deal with partial observability, sequence models like transformers are gaining traction as an alternate solution (Lu et al., 2023; Bousmalis et al., 2023; Team et al., 2023; Raparthy et al., 2024). A less common use of transformers in RL is for processing inherently permutation-invariant observations, such as entities in Starcraft II (Vinyals et al., 2019). Although graphs are traditionally processed with graph neural networks (Wang et al., 2018; Battaglia et al., 2018), transformers are also now being applied to this domain (Sferrazza et al., 2024; Buterez et al., 2024), with attention masks set to a graph's adjacency matrix to restrict attention to neighboring nodes (Sferrazza et al., 2024)."}, {"title": "3 KINETIX", "content": "In this section, we introduce Kinetix, a large and open-ended environment for RL, implemented entirely in JAX. We describe our underlying physics engine (Section 3.1), the RL environment (Sec- tion 3.2), and finally propose Kinetix as a novel challenge for open-endedness (Section 3.3)."}, {"title": "3.1 JAX2D", "content": "Jax2D is our deterministic, impulse-based, 2D rigid-body physics engine, written entirely in JAX, that forms the foundation of the Kinetix benchmark. We designed Jax2D to be as expressive as possible through simulation of only a few fundamental components. To this end, a Jax2D scene contains only 4 unique entities: circles, (convex) polygons, joints and thrusters. From these simple building blocks, a huge diversity of different physical tasks can be represented.\nJax2D simulates discrete Euler steps for rotational and positional velocities and then applies in- stantaneous impulses and higher order corrections to solve constraints. The notion of a constraint encompasses collisions (two objects cannot be inside each other) and joint constraints (two objects connected by a joint cannot separate at the point of connection). Constraints are pairwise, meaning that it may be necessary to apply multiple steps of constraint solving for a stable simulation, espe- cially when simulating systems of many interacting bodies. The number of solver steps therefore serves as a tradeoff between accuracy and speed. An agent (human or artificial) can act on the scene by applying torque through motors attached to revolute joints or by applying force through thrusters.\nJax2D is based on Box2D (Catto, 2007) and can be thought of as a minimalist rewrite of the C li- brary in JAX. Appendix B shows the benefit of this reimplementation, with hardware acceleration al- lowing Jax2D to easily scale to thousands of parallel environments on a single GPU, outperforming Box2D by a factor of 4\u00d7 when comparing just the engines and 30\u00d7 when training an RL agent (this difference is due to Jax2D natively integrating with RL pipelines that exist entirely on the GPU).\nThe key differentiator of Jax2D from other JAX-based physics simulators such as Brax (Freeman et al., 2021), is that Jax2D scenes are almost entirely dynamically specified, meaning that the same underlying computation graphs are run for every simulation. For example, this means that running Half-Cheetah, Pinball and Grasper (Figure 1) involves executing the exact same instructions. This allows us to parallelise across different tasks with the JAX vmap operation\u2014a crucial component of harnessing the power of hardware acceleration in a multi-task RL setting. Brax, by contrast, is almost entirely statically specified meaning it is impossible to vmap across, for instance, different morphologies. Further Jax2D implementation details are discussed in Appendix A."}, {"title": "3.2 KINETIX: RL ENVIRONMENT SPECIFICATION", "content": "Kinetix builds on Jax2D to create an environment for RL, which we now briefly outline.\nAction Space Kinetix supports both multi-discrete and continuous action spaces. In the multi- discrete action space, each motor and thruster can either be inactive, or activated at maximum power each timestep, with motors being able to be run either forwards or backwards. In the continuous action space, motors can be powered in the range [-1,1] and thrusters in the range [0, 1].\nObservation Space We use a symbolic observation where each entity (shape, joint or thruster) is defined by an array of values of physical properties including position, rotation and velocity. The observation is then defined as the set of these entities, allowing the use of permutation-invariant network architectures such as transformers. This observation space makes the environment fully observable, removing the need for a policy with memory. We also provide the option for pixel-based observations and a symbolic observation that simply concatenates and flattens the entity information.\nReward To facilitate our goal of a general agent, we choose a simple yet highly expressive reward function that remains fixed across all environments. Each scene must contain a green shape and a blue shape\u2014the goal is simply to make these two shapes collide, upon which the episode terminates with a reward of +1. Scenes can also contain red shapes, which, if they collide with the green shape, will terminate the episode with -1 reward. As demonstrated in Figure 1, these simple and interpretable rules allow for a large number of semantically diverse environments to be represented. To improve learning, we augment this sparse reward with an auxiliary dense reward signal, defined"}, {"title": "3.3 KINETIX: A BENCHMARK FOR INVESTIGATING OPEN-ENDEDNESS", "content": "The expressivity, diversity, and speed of Kinetix makes it an ideal environment for studying open- endedness, including generalist agents, UED, and lifelong learning. In order to make it maximally effective for agent training and evaluation, we provide a heuristic environment generator, a set of hand-designed levels, and an environment taxonomy describing the complexity of environments.\nEnvironment Generator The strength of Kinetix lies in the diversity of environments it can represent. However, this environment set contains many degenerate cases, which can dominate the distribution if sampled from na\u00efvely. For this reason, we provide a random level generator that is designed to be maximally expressive, while minimising the number of degenerate levels. We ensure that every level has exactly one green and blue shape, and at least one controllable aspect (either a motor or a thruster). Furthermore, we follow Team et al. (2021) and perform rejection sampling on levels solved with a no-op policy (defined as the policy that activates no motors or thrusters), thus eliminating trivial levels. The remaining pathology is unsolvable levels, which are largely intractable to determine and for which we will rely on automatic curriculum methods to filter out.\nEach level is built up iteratively from an empty base by adding shapes either freely or connected to an already existing shape. We perform rejection sampling on proposed shape additions to try and ensure that no collisions are active in the initial level state. These methods to add shapes (along with analogous methods for editing and removing) can also serve as mutators for automatic level editing algorithms like ACCEL (Parker-Holder et al., 2022). We also provide functionality to generate levels using RL (Dennis et al., 2020) and generative models (Garcin et al., 2024).\nHand-Designed Levels Along with the capability to sample random levels, Kinetix contains a suite of 74 hand designed levels (Appendix E), as well as a powerful graphical editor to facilitate the creation of new levels. Some of these levels are inspired by other RL bench- marks, such as L-MuJoCo-Walker, L-MuJoCo-Hopper, L-MuJoCo-Half-Cheetah, L-MuJoCo-Swimmer (Todorov et al., 2012) and L-Lunar-Lander, L-Swing-Up, L-Cartpole-Wheels-Hard (Brockman et al., 2016). We made other levels, like L-Pinball, L-Lorry and L-Catapult, specifically for Kinetix. These levels tests agent capabilities in- cluding fine-grained motor control, navigation, planning and physical reasoning.\nEnvironment Taxonomy Kinetix has the useful characteristic of containing a controllable and interpretable axis of complexity\u2014the number of each type of entity in a scene. While not a strict rule, scenes with less entities tend to represent simpler problems. We therefore quantise our exper- iments and handmade levels into one of three distinct sizes: small (S), medium (M), and large (L). A convenient feature of the entity-based observation space is that an agent trained on one level size can also meaningfully operate in other sizes, just as a language model can condition on a variable number of tokens, allowing us to interoperate between the sizes."}, {"title": "4 EXPERIMENTAL SETUP", "content": "We train on programatically generated Kinetix levels drawn from the statically defined distri- bution. We refer to training on sampled levels from this distribution as DR. Our main metric of assessment is the solve rate on the set of handmade holdout levels. The agent does not train on these levels but they do exist inside the support of the training distribution. Since all levels follow the same underlying structure and are fully observable, it is theoretically possible to learn a policy that can perform optimally on all levels inside the distribution.\nTo select levels to train on, we use SFL (Rutherford et al., 2024), a state-of-the-art UED algorithm that regularly performs a large number of rollouts on randomly generated levels. It then selects a subset of these with high learnability and trains on them for a fixed duration before again selecting new levels. The main limitation of SFL, that it is only applicable to deterministic settings with a binary success reward, does not constrain us, as Kinetix satisfies both of these assumptions. We ran preliminary experiments using PLR (Jiang et al., 2021a;b) and ACCEL (Parker-Holder et al., 2022), but found that these approaches provided no improvements over DR (see Appendix L)."}, {"title": "4.1 ARCHITECTURE", "content": "The architecture we use is summarised in Figure 2. To process the observation in a permutation- invariant way, we represent each entity as a vector $v$, containing information about its physical prop- erties, such as friction, mass and rotation. We separately encode (using a set of small feedforward networks) polygons, circles, joints and thrusters into initial embeddings $x^T$, where $T \\in \\{p, c, j, t\\}$. We perform self-attention (Bahdanau et al., 2015; Vaswani et al., 2017) over the set of shapes (i.e., polygons and circles) without positional embeddings to obtain new shape embeddings $\\tilde{x}^S$. To incor- porate joint information, we take each joint feature $x^j$, and its two connected shapes $x^{from}$ and $x^{to}$ and pass the concatenation through a feedforward network $f$, and add it to the embedding for $x^{from}$. We have two feature vectors for each joint, with the from and to shape swapped. This layer is rem- iniscent of message passing in graph neural networks (Gilmer et al., 2017; Bronstein et al., 2021). Similarly, for each thruster $x^t$ and associated shape $\\tilde{x^i}$, we process these using a message-passing layer and add the result back to $\\tilde{x^S}$. This entire process constitutes one transformer layer, which we apply multiple times. We use multi-headed attention, with a different attention mask for each head. The first mask represents a fully-connected graph and contains all shapes; the second allows shapes to attend to those that are connected by a joint (Sferrazza et al., 2024; Buterez et al., 2024); the third allows attention to shapes that are joined by any n-step connection; and the final mask allows shapes to attend to those that they are currently colliding with. Finally, following Parisotto et al. (2020), we use a gated transformer, and perform layernorm (Lei Ba et al., 2016) before the attention block."}, {"title": "5 ZERO-SHOT RESULTS", "content": "In Figure 3, we run SFL on the S, M and L environment sizes, respectively (see Appendix J for a per- level breakdown). In each case, the (random) training environments are of the corresponding size, and we use the corresponding holdout set (see Appendix E for a full listing) to evaluate the agent's generalisation capabilities. We see that, in every case, the agent's performance increases throughout training, indicating that it is learning a general policy that it can apply to unseen environments. For S, the agent very quickly learns a policy superior to the random policy, and is able to solve most of the hold out levels zero-shot. While the solve rate is lower on M, the agent can still zero-shot a number of unseen hand-designed environments. On the L environments, in which the agent is assessed on the most challenging holdout tasks, we see a very slow, and non-monotonic, performance increase. As well as being trained and tested on more complex levels, it seems that as the complexity increases,"}, {"title": "5.1 ANALYSIS: ZERO-SHOT LOCOMOTION OF AN ARBITRARY MORPHOLOGY", "content": "In this section, we take a closer look at the zero-shot capabilities of the learned general agent by prob- ing its behaviour in a constrained goal-following setup. Specifically, we create levels with a single morphology (a set of shapes connected with motors and containing the green shape) in the centre of the level, with a goal (the blue shape) fixed at the top of the level with a random x position. Since the goal is made to be unreachable, the optimal behaviour of the agent is to maximise the dense auxil- iary reward and move as close as possible to the goal (i.e., directly underneath it). We evaluate three"}, {"title": "6 FINE-TUNING RESULTS", "content": "In this section we leave the zero-shot paradigm and investigate the performance of the general agent when given a limited number of samples to fine-tune on the holdout tasks. In particular, in Fig- ure 5 we train a separate specialist agent for each level in the L holdout set, and compare this to fine-tuning a general agent (the same one used for Section 5.1). We plot the learning curves for four selected environments, as well as the aggregate performance over the entire holdout set. On three of these levels, fine-tuning the agent drastically outperforms training from scratch. In particular, for Mujoco-Hopper-Hard and Mujoco-Walker-Hard, the fine-tuned agent is able to competently complete these levels, whereas the tabula rasa agent cannot do so consistently. Notably, this is despite the fact that the pre-trained agent cannot solve these environments zero- shot. While the general trend is that fine-tuning beats training from scratch, we do see one case: Thruster-Large-Obstacles, where fine-tuning learns slower."}, {"title": "6.1 ANALYSIS: GENERAL PRETRAINING CAN BEAT TRAINING ON THE TARGET TASK", "content": "We now further investigate the case of Car-Ramp (Figure 6a) where RL, even with a large sample budget, fails to solve but that our fine-tuned general agent can complete (note that this behaviour is also shown in MuJoCo-Walker-Hard). Car-Ramp is an example of a deceptive problem (Gold- berg, 1987; Liepins & Vose, 1991; Lehman & Stanley, 2011) that requires the agent to first move away from the goal (and incur a negative reward) to obtain enough momentum to jump the gap."}, {"title": "7 RELATED WORK", "content": "Hardware-Accelerated Physics Engines Jax2D joins a thriving ecosystem of hardware- accelerated physics engines used in RL tasks. Brax (Freeman et al., 2021), MJX (Todorov et al., 2012) and Isaac-Gym (Makoviychuk et al., 2021) have all been been widely used in the RL commu- nity, particularly for robotics tasks. While superficially similar, we believe Jax2D is useful for an entirely different set of problems. Firstly, Jax2D only operates in two dimensions, so training on robotics tasks for transfer to the real world is not a goal of the engine. Jax2D instead aims to be able to represent a hugely diverse range of physics problems and, most crucially, can do so with the same computation graph, allowing work across multiple heterogeneous environments to be parallelised.\nPhysical Reasoning PHYRE (Bakhtin et al., 2019) also uses 2D rigid-body physics by tasking agents with placing a ball to achieve some goal state. Li et al. (2024a) extend this bandit-like problem, allowing the agent to take actions throughout the episode. A crucial difference is that we train on a large automatically generated set of tasks rather than a small set of handmade ones.\nHardware-Accelerated RL Our work follows the recent trend of using hardware-accelerated RL environments to run significantly larger-scale experiments than would be possible with CPU-based environments (Lu et al., 2022; Jackson et al., 2023; 2024; Goldie et al., 2024; Rutherford et al., 2024; Nikulin et al., 2024; Kazemkhani et al., 2024). By leveraging Kinetix's speed, we can train for billions of timesteps and, as we show, general capability does only emerge after such a long time.\nGeneralist Robotics Agents Recent work has strived to learn a generalist foundation model for robotics (Reed et al., 2022; Bousmalis et al., 2023; Team et al., 2024; Nasiriany et al., 2024; O'Neill et al., 2024). While most of these approaches perform behaviour cloning on a large dataset from a variety of robot morphologies and tasks, Nasiriany et al. (2024) develop a large-scale simulation en- vironment, with an initial focus on kitchen environments. By contrast, Kinetix aims to train an on- line agent tabula rasa, without using external data, and further has a large variety of different tasks.\nOpen-Ended Learning Kinetix also ties into the paradigm of open-ended learning (Soros & Stanley, 2014; Stanley, 2019; Sigaud et al., 2023; Hughes et al., 2024), in which a system continually generates new and novel artifacts. In the context of RL, this often means training inside a large, diverse distribution and having some method to adapt this distribution over time, with fields such as UED specifically focusing on the latter. While these methods hold the promise of generating novel and useful levels in an open-ended manner, the environments used in their experiments are often very constrained in what they can represent (Wang et al., 2019; Dennis et al., 2020; Wang et al., 2020; Jiang et al., 2021b;a; Parker-Holder et al., 2022). As we have shown, in a significantly more diverse task space, these methods tend to fail.\nA recent work with a similar vision to Kinetix is Autoverse (Earle & Togelius, 2024), where an agent acts inside a cellular automata (CA) based gridworld. By changing the underlying rules of the CA, a large diversity of different levels can be represented in this system. Relatedly, Sun et al. (2024) use prior knowledge in the form of Large Language Models to generate code for video games"}, {"title": "8 DISCUSSION AND FUTURE WORK", "content": "We believe Kinetix is a uniquely diverse, fast and open-ended environment, placing it well as a foundation to study open-ended RL, including large-scale online pre-training for general RL agents. In stark contrast to many other benchmarks used for open-ended learning (Wang et al., 2019; 2020; Parker-Holder et al., 2022; Chevalier-Boisvert et al., 2023; Rutherford et al., 2024), Kinetix represents a large space of semantically diverse tasks, instead of just variations on a single task. This presents a challenge for future environment design research that can intelligently generate lev- els (Dennis et al., 2020), rather than just filtering from a predefined distribution. We also believe Kinetix is an excellent framework for investigating issues in agent training like network capac- ity (Obando-Ceron et al., 2024), plasticity loss (Igl et al., 2020; Berariu et al., 2021; Dohare et al., 2021; Sokar et al., 2023), lifelong learning (Kirkpatrick et al., 2017) and multi-task learning (Sod- hani et al., 2021; Hafner, 2021; Benjamins et al., 2023).\nRequiring billions of online environment interactions is impractical for real-world applications. However, we see three primary ways to leverage the cheap samples of simulations for sample- constrained tasks. One approach is to meta-learn parts of the RL process, for instance the algo- rithm (Oh et al., 2020; Lu et al., 2022; Jackson et al., 2023), optimiser (Goldie et al., 2024) or loss function (Bechtle et al., 2021). Alternatively, the emerging capabilities of large world mod- els (Bruce et al., 2024; Valevski et al., 2024) hint at a new paradigm of online training entirely in imagination (Ha & Schmidhuber, 2018; Yu et al., 2020; Hafner et al., 2020; 2021; 2023), where the only bottleneck to environment samples is compute. Finally, we may find that, with enough scale, we can fine-tune an agent trained in simulation on real world tasks."}, {"title": "9 CONCLUSION", "content": "In this work, we first introduce Jax2D, a hardware-accelerated 2D physics engine. Using Jax2D, we build Kinetix, a vast and open-ended physics-based RL environment. We illustrate the diver- sity of Kinetix by hand-designing a comprehensive holdout set of environments that test various skills, such as navigation, planning and physical reasoning. We train an agent on billions of envi- ronment interactions from randomly generated tasks, and show that it can zero-shot generalise to many human-designed tasks, as well as function as a strong base model for fine-tuning. We hope that this work can serve as a foundation for future research in open-endedness, large-scale online pre-training of general RL agents and unsupervised environment design."}, {"title": "A JAX2D", "content": "This section provides an in-depth look into the logic behind Jax2D. Jax2D largely owes its heritage to Box2D (Catto, 2007) and ImpulseEngine (Gaul, 2013), with most of the underlying framework being lifted from these engines and adapted for JAX. For a more thorough account of some of the concepts behind rigid-body physics, we recommend Erin Catto's talks.", "is_appendix": true}, {"title": "A.1 CORE ENGINE", "content": "The main loop of Jax2D is summarised in Algorithm 1. Each part of the engine is subsequently explained as referenced.", "is_appendix": true}, {"title": "A.2 IMPULSE RESOLUTION AND CONSTRAINT SOLVING", "content": "The core of Jax2D is impulse resolution, in which an equal and opposite impulse is applied to a pair of shapes in order to satisfy some constraint. For a given impulse $j$, the positional and angular", "is_appendix": true}, {"title": "A.3 COLLISIONS", "content": "The first type of constraint we consider is the collision constraint, which prevents objects from moving inside of each other.", "is_appendix": true}, {"title": "A.3.1 COLLISION MANIFOLDS", "content": "The notion of a collision between to shapes is reduced to the concept of a collision manifold, con- taining the information shown in Table 1.", "is_appendix": true}, {"title": "A.3.2 CIRCLE-CIRCLE COLLISION MANIFOLDS", "content": "Generating a collision manifold between two circles is relatively simple, and is calculated as follows:\n$p \\leftarrow p_a + r_a \\hat{n}$\n$\\hat{n} \\leftarrow \\frac{p_b - p_a}{|p_b - p_a|}$\n$p r_a + r_b$", "is_appendix": true}, {"title": "A.3.3 POLYGON-CIRCLE COLLISION MANIFOLDS", "content": "The collision between a polygon $a$ and a circle $b$ is calculated by first determining the closest point on any edge to the circle. For each edge, the centre of the circle is clipped to perpendicular lines extending from both corners, before being projected onto the edge to find the closest point for that particular edge. The clipping ensures that the point doesn't end up off the end of an edge - it will instead be clipped to a corner. Once this closest point $p$ has been found, the collision manifold can be calculated.\n$\\hat{n} \\leftarrow \\frac{p_b - p}{|p_b - p|}$\n$p \\leftarrow p r_b$", "is_appendix": true}, {"title": "A.3.4 POLYGON-POLYGON COLLISION MANIFOLDS", "content": "Collisions between two convex polygons are the most complex. The underlying stratgey is defined by the separating axis theorem: any two convex polygons that are not colliding will have an axis upon which, when the vertices of both shapes are projected onto, there will be no overlap. Furthermore, it can be shown that if this axis exists, it must run perpendicular to one of the edges of one of the polygons. Intuitively, one can imagine drawing a straight line (perpendicular to the separating axis and thus parallel with an edge) that separates the two convex polygons.\nIf there is no separating axis then the two polygons are colliding. Finding the point of collision involves pinpointing the axis of least penetration, that is the axis that when projected upon causes the least amount of overlap. The face that the axis of least penetration is derived from is termed the reference face, and the face (on the other shape) of which the corners have the least penetration is termed the incident face. Similar to the polygon-circle collision, the incident face is then clipped to the boundaries of the reference face. Each of"}]}