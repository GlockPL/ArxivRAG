{"title": "MUSICGEN-CHORD: ADVANCING MUSIC GENERATION THROUGH\nCHORD PROGRESSIONS AND INTERACTIVE WEB-UI", "authors": ["Jongmin Jung", "Andreas Jansson", "Dasaem Jeong"], "abstract": "MusicGen is a music generation language model (LM) that\ncan be conditioned on textual descriptions and melodic\nfeatures. We introduce MusicGen-Chord\u00b9, which extends\nthis capability by incorporating chord progression features.\nThis model modifies one-hot encoded melody chroma vec-\ntors into multi-hot encoded chord chroma vectors, enabling\nthe generation of music that reflects both chord progres-\nsions and textual descriptions. Furthermore, we developed\nMusicGen-Remixer2, an application utilizing MusicGen-\nChord to generate remixes of input music conditioned on\ntextual descriptions. Both models are integrated into Repli-\ncate's web-UI using cog, facilitating broad accessibility\nand user-friendly controllable interaction for creating and\nexperiencing AI-generated music.", "sections": [{"title": "1. INTRODUCTION", "content": "The trend in generative AI emphasizes the controllabil-\nity of models, allowing users to direct and refine out-\nputs according to their preferences. Notable examples in-\nclude Stable Diffusion [1,2], supported by interfaces like\nAUTOMATIC1111's web-UI [3] and ComfyUI [4],\nwhich offer extensive user control over image generation\nprocesses. In the realm of music generation, models with\nenhanced controllability are emerging, offering conditions\nsuch as chord [5-7], rhythm [5-7], melody [6,8], and style\nbased on reference audio [9]. This paper explores the in-\ntegation of controllability in music generation through the\nexample of MusicGen-Chord.\nMusicGen [8] is an auto-regressive, Transformer-based\nmusic generation model that enables user control through\ntextual descriptions and melodic features. It processes\nmultiple streams of compressed discrete audio represen-\ntations [10] to generate high-quality, coherent, and stylisti-\ncally diverse music. MusicGen-Chord extends this model\nby conditioning on chord progressions instead of melodies.\nThis modification uses a matrix of multi-hot encoded\nchroma vectors to represent chord progression features.\nMusicGen-Chord was released in October 2023, and since\nthen, several similar but more advanced studies have been\nintroduced, such as MusiConGen [7].\nTo demonstrate the practical benefits of this approach,\nwe developed MusicGen-Remixer, an application based on\nMusicGen-Chord. This application allows users to up-\nload a music track, provide a textual description prompt,\nand generate a new background track that is remixed with\nthe input audio. By leveraging Replicate's web-UI and\nthe cog [11] package, MusicGen-Remixer and MusicGen-\nChord are made widely accessible on the cloud, promoting\nuser-friendly interaction and broad accessibility for creat-\ning and experiencing AI-generated music."}, {"title": "2. MUSICGEN-CHORD", "content": "MusicGen-Chord extends the original MusicGen model by\nshifting the conditioning target from melodies to chord\nprogressions. The original MusicGen model uses one-hot\nencoded chroma vectors as input condition to represent\nmelodies (Figure 1.(a)). In this approach, each vector in-\ndicates the presence of a single pitch class at a given time,\nwhich is effective for simple melodies but limited in cap-\nturing complex harmonic content.\nWe found that we can tweak this input format to a multi-"}, {"title": "3. MUSICGEN-REMIXER", "content": "MusicGen-Remixer utilizes the features of MusicGen-\nChord to enable the creation of remixed music tracks. This\napplication allows users to upload a music track, provide a\ntextual description prompt, and generate a new background\ntrack that is remixed with the input audio.\nThe process involves several sophisticated steps to en-\nsure the generation of coherent and contextually relevant\nremixes:\n1. Input Music Structure Analysis: Utilizing the\nAll-in-One [14] framework, the input music's\nBPM and downbeats are detected to maintain tem-\nporal integrity.\n2. Source Separation: A neural source separation\nmodel, Demucs [15] is employed to separate vocal\ntracks from instrumental components, ensuring the\noriginal vocal performance is preserved.\n3. Chord Progression Feature Extraction: BTC is\nused to extract chord progression features from the\ninput audio, guiding the generation of the new back-\nground track.\n4. Dynamic Time Warping: Using Py-TSMod [16],\nthe timing of the generated track is adjusted to match\nthe downbeats of the input audio, ensuring rhythmic\nconsistency."}, {"title": "4. REPLICATE INTEGRATION", "content": "Replicate's web-UI, combined with the cog package, pro-\nvides a seamless and convenient platform for deploying AI\nmodels. The cog package can encapsulate AI models with\nall their dependencies, including Python packages, operat-\ning system components, and CUDA versions. This inte-\ngration ensures that models are portable and easily deploy-\nable, facilitating a user-friendly interface for model inter-\naction and management .\nMusicGen-Chord and MusicGen-Remixer are inte-\ngrated with Replicate through the cog package, making\nthem widely accessible on the cloud. Users can easily\ninteract with these models via the web interface, API, or\ndirectly using the cog-wrapped repository 4 on local ma-\nchines. This setup ensures a straightforward and accessible\nexperience for generating and remixing AI-driven music,\nenhancing both usability and adaptability.\nReplicate offers a practical solution for sharing AI\nmodel demonstrations within the MIR community. Re-\nsearchers and developers can utilize Replicate as an effec-\ntive tool for presenting and disseminating their work, as\ndemonstrated by the Music Technology Group (MTG) 5\nand others who have successfully used this platform. For\nexample, we implemented MusiConGen [7], a recent Mu-\nsicGen variant with controllable chord and rhythm fea-\ntures, into a cog wrapped demo 6."}]}