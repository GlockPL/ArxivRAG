{"title": "CHANNEL-WISE MIXED-PRECISION QUANTIZATION FOR LARGE LANGUAGE MODELS", "authors": ["Zihan Chen", "Bike Xie", "Jundong Li", "Cong Shen"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across a wide range of language tasks, but their deployment on edge devices remains challenging due to the substantial memory requirements imposed by their large parameter sizes. Weight-only quantization presents a promising solution to reduce the memory footprint of LLMs. However, existing approaches primarily focus on integer-bit quantization, limiting their adaptability to fractional-bit quantization tasks and preventing the full utilization of available storage space on devices. In this paper, we introduce Channel-Wise Mixed-Precision Quantization (CMPQ), a novel mixed-precision quantization method that allocates quantization precision in a channel-wise pattern based on activation distributions. By assigning different precision levels to different weight channels, CMPQ can adapt to any bit-width con-straint. CMPQ employs a non-uniform quantization strategy and incorporates two outlier extraction techniques that collaboratively preserve the critical information, thereby minimizing the quantization loss. Experiments on different sizes of LLMs demonstrate that CMPQ not only enhances performance in integer-bit quantization tasks but also achieves significant performance gains with a modest increase in memory usage. CMPQ thus represents an adaptive and effective approach to LLM quantization, offering substantial benefits across diverse device capabilities.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs), trained on massive text corpora and containing up to hundreds of billions of parameters, have demonstrated exceptional performance across a wide range of language tasks. However, deploying LLMs directly on devices for inference presents a significant challenge due to their enormous parameter sizes. For instance, GPT-3, with 175 billion parameters, requires 350 GB of memory in FP16 precision, which far exceeds the capacity of the latest NVIDIA H100 GPU with 96 GB of memory, let alone the capabilities of edge devices.\nLow-precision weight-only quantization has emerged as a promising solution to address this challenge by converting model weights from high bit-width representations (e.g., FP16) to lower bit-widths (e.g., 3-bit), significantly reducing the memory requirements for on-device LLM inference. In this work, we focus on Post-Training Quantization (PTQ), which quantizes the pre-trained models without the need for retraining a process that is often costly and resource-intensive. Most existing PTQ methods concentrate on integer-bit quantization and employ a uniform low-bit-width representation across all layers, as illustrated in Figure 1(a), yielding promising performance in low-bit quantization tasks. However, these methods are limited in their adaptability to devices with additional storage capacity. For example, they can only offer solutions constrained to integer-bit quantization (e.g., 2-bit) even if the device could support models with an average of, e.g., 2.2-bit precision, failing to utilize the extra storage space to further reduce quantization loss.\nMixed-precision quantization inherently supports fractional bit quantization by allowing model weights to be quantized at"}, {"title": "2 RELATED WORKS", "content": "Post-Training Quantization Quantization is a model compression technique that modifies the vector or matrix representations of a pre-trained model to improve inference efficiency. It can be broadly categorized into two workflows: Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT involves retraining the model while adjusting its parameters during quantization, which is resource-intensive and often impractical for LLMs. In contrast, PTQ focuses on quantizing pre-trained models without the need for retraining,"}, {"title": "3 METHOD", "content": "In this section, we begin with preliminary studies to investigate the impact of channel-wise mixed-precision quantization compared to layer-wise mixed-precision quantization (Section 3.1.2). The empirical results align with our intuition: channel-wise mixed-precision quantization more effectively harnesses the potential of mixed-precision. Subsequently, in Section 3.2, we provide a detailed introduction to our proposed Channel-Wise Mixed-Precision Quantization method."}, {"title": "3.1 PRELIMINARY", "content": ""}, {"title": "3.1.1 N-BIT QUANTIZATION", "content": "Quantization typically involves mapping a continuous set of values W from a higher bit-width (e.g., 16-bit floating point) to a discrete set of values Q(W) at a lower bit-width (e.g., 4-bit integers). The most commonly used uniform quantization can be expressed as:\n$Q(W) = \\left\\lfloor \\frac{W_{FP16} - \\text{min}(W_{FP16})}{\\Delta} \\right\\rfloor,$"}, {"title": "3.1.2 PRELIMINARY STUDY OF MIXED-PRECISION QUANTIZATION", "content": "To explore the effect of mixed-precision quantization, we conduct preliminary experiments on two OPT models and compare the perplexity evaluation. We focus on 3-bit quantization, where precision is selected from {2, 3, 4}-bit.\nIn Li et al. (2023), each linear layer is associated with information loss scores under different precisions, and their approach models the average bit width as a constraint in an integer programming problem. We implement this strategy with the non-uniform quantization, and the performance is reported in Table 1 as w/ IntProg. Additionally, Lin et al. (2024) observe that retaining salient weights based on activation distributions in higher precision significantly enhances quantized performance. For each layer, we compute the channel-wise L2-norm of activations and select the top (and bottom) k% of channels. These channels are quantized to 4-bit (high precision) or 2-bit (low precision) respectively. Results for k = 1 and k = 10 are presented in Table 1."}, {"title": "3.2 CHANNEL-WISE MIXED-PRECISION QUANTIZATION", "content": "Our primary objective is to develop an algorithm that effectively utilizes mixed-precision quantization to adaptively compress LLMs under any given average bit constraint, including fractional bit-widths."}, {"title": "3.2.1 CHANNEL-WISE NON-UNIFORM QUANTIZATION", "content": "From the observations in Section 3.1.2, we can draw two key intuitions: 1 channel-wise mixed-precision quantization can enhance model performance compared to layer-wise mixed-precision quantization, and when implementing channel-wise quantization, it is advisable to limit the number of 2-bit channels to minimize the information loss. Based on these, we propose channel-wise non-uniform quantization.\nResearch has shown that the weight distributions in LLM layers exhibit non-uniform patterns. Previous approaches have primarily focused on uniform quantization, which divides the weight range into evenly spaced bins. However, this approach is suboptimal, as it fails to account for the non-uniform nature of the weight distributions, and struggles to improve end-to-end latency in memory-bound LLM inference. Following Kim et al. (2024), we adopt non-uniform quantization. Specifically, for each channel $W_{:,}$ in the weight matrix $W \\in \\mathbb{R}^{d_{in} \\times d_{out}}$, we apply a K-means clustering algorithm, where the value of K is determined by the precision assigned to the channel (e.g., K = 8 for 3-bit quantization). After clustering, each weight in $W$ is represented by its nearest centroid from the set of K centroids ${q_1,...,q_K}$."}, {"title": "3.2.2 OUTLIER PROTECTION", "content": "Another key challenge in low-bit LLM quantization is the protection of outlier values. Previous studies have demonstrated that naively quantizing weights with a large dynamic range significantly degrades performance, particularly at low precisions. However, in some cases, retaining a small fraction (less than 1%) of outlier weights in FP16 has been shown to reduce up to 75% of the total quantization error. This suggests that extracting outliers prior to quantization can mitigate their negative impact and minimize quantization loss. Consistent with prior works , we retain 0.5% of outliers in high precision (16-bit), while applying quantization to the remaining weights. Our approach focuses on protecting two types of outliers: activation-based outliers and quantization-based outliers.\nAs illustrated in Figure 2, we observe that outliers exhibit a channel-wise pattern \u2013 if an outlier appears in a channel, it consistently occurs across all tokens. Table 1 empirically demonstrates that preserving salient weights based on the activation distribution helps mitigate quantization loss. Motivated by these findings, we introduce an activation-based outlier detection method, which identifies outliers $O_{act}$ from the weight matrix W. Specifically, we select channels corresponding to the top 0.45% largest values in the activation's L2-norm vector a, and preserve these channels in FP16 precision.\nIn addition to selecting channel-wise outliers, we also investigate the protection of a small subset of quantization-sensitive outliers. Though we introduced our non-uniform quantization method, a small fraction of weights exhibit significantly larger magnitudes compared to the majority. These large weights can distort the clustering process by shifting centroids away from the bulk of the weight distribution, thereby negatively impacting the performance of the quantized LLM. A conventional approach to outlier protection involves the removal of weights based solely on their magnitude. However, instead of simply eliminating high-magnitude outliers, our objective is to identify and remove those that most adversely affect the quantization process.\nTo achieve this, we apply another K-means clustering step prior to quantization. Specifically, given $W' = W - O_{act}$ that represents the remaining weights after activation outlier removal, we use Algorithm 1 to determine the channel precisions c. We then apply channel-wise non-uniform quantization based on c to obtain the quantized model $\\widetilde{W}$. We identify the set of outliers $O_q$ in $\\widetilde{W}'$ corresponding to the top 0.05% of the largest values in $|W' - \\widetilde{W}|$. This approach preserves these magnitude-based outliers in FP16 format, not only to mitigate their influence on model output but also to ensure that the centroids {q1,...,qK} better represent the majority of the weights, rather than being skewed by a small number of outliers. Two types of outliers $O = O_{act} + O_q$ are removed from the weight matrix W, and the remaining weights then undergo the quantization process to obtain $W_q$. $\\widetilde{W}_q + O$ is used for the final inference. Notably, the overhead associated with this decomposition is minimal, as the number of outlier values is relatively small, typically around 0.5% of the total values."}, {"title": "3.2.3 DISCUSSION", "content": "Our method only requires forward propagation and does not depend on backpropagation, which is necessary for many existing quantization techniques. Consequently, the memory requirements for our proposed CMPQ during quantization are moderate; for instance, loading the OPT-6.7B model necessitates 12.4 GB of memory, whereas the backward pass for the same model requires 49.61 GB of memory. Additionally, CMPQ has minimal reliance on the calibration set, as it only measures the L2-norm per channel, thereby mitigating the risk of overfitting. For a comparison with backpropagation-dependent methods, refer to Section 4.4, and for an analysis of CMPQ's robustness with respect to the calibration dataset, see Section 4.5."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENT SETUP", "content": "LLM Models and Datasets. We perform our experiments on two models from the OPT family (Zhang et al., 2022) (OPT-2.7B and OPT-6.7B) and two models from the LLaMA2 family (Touvron et al., 2023b) (LLaMA2-7B and LLaMA2-13B). The evaluation of the quantized models is based on perplexity across two language generation tasks, WikiText-2 and C4, as perplexity is a widely recognized metric for assessing the LLM performance. For calibration, we follow previous works and use a set of 128 randomly selected 2048-token segments from the C4 dataset, which contains generic text data from web crawls. This ensures consistency in comparison with baselines and avoids the use of task-specific data when quantizing other datasets. All experiments are implemented in PyTorch and executed on two A6000 GPUs, with performance monitoring handled by the Torch CUDA profiler. we extend our evaluation in Appendix A.3 to include quantization results for other OPT models, scaling up to 30B parameters, as well as a newer model, LLaMA3-8B (AI@Meta, 2024).\nBaselines. We evaluate the proposed CMPQ against several post-training quantization methods that do not rely on backpropagation, including Round-to-Nearest (RTN), GPTQ, AWQ , and QuIP. Since these methods are specifically designed for integer bit-width quantization, we restrict the comparison to {2, 3, 4}-bit settings. Additionally, we compare CMPQ with a mixed-precision quantization method tailored for LLMs, LLM-MQ, focusing on performance in fractional bit-width quantization. In Section 4.4, we extend the comparison to also include SqueezeLLM , a state-of-the-art gradient-based method, and discuss the trade-offs between memory cost and quantization performance."}, {"title": "4.2 MAIN RESULTS", "content": "Table 2 presents the main results comparing CMPQ with post-training quantization baselines. Overall, our proposed CMPQ consistently outperforms all baselines, particularly in the 2-bit quantization tasks. Notably, while QuIP is specifically designed for low-bit quantization, it performs poorly on the WikiText-2 task using 2-bit quantized small LLM models (OPT-2.7B). In contrast, CMPQ achieves significantly better results on this task, despite using the same calibration dataset from C4. This highlights that CMPQ is less sensitive to the choice of the calibration set, as it relies solely on measuring the activation per-channel L2-norm, which generalizes more effectively across different dataset distributions. Furthermore, although LLM-MQ is designed for mixed-precision quantization, it struggles to achieve competitive performance in 3-bit quantization compared to other baselines. This limitation arises from its precision allocation strategy, which is based exclusively on first-order information that is hard to distinguish layer sensitivities of a converged LLM."}, {"title": "4.3 NON-INTEGER QUANTIZATION", "content": "In Figure 3, we compare the perplexity of LLMs quantized by CMPQ and LLM-MQ under fractional bit-width constraints. First, it is evident that CMPQ consistently outperforms LLM-MQ across various bit-widths and models. LLM-MQ quantizes entire layers to a fixed precision, which can result in significant information loss within a single layer, negatively impacting the overall model performance. In contrast, CMPQ allocates mixed-precision in a channel-wise manner, distributing the information loss more evenly across layers and avoiding a substantial loss in any single layer. Another key observation, visible when examining the transition from 2-bit to 2.2-bit quantization, highlights the advantage of mixed-precision. Introducing just a 10% increase in storage overhead at lower bit-widths can lead to significant performance gains. For instance, in the case of LLaMA2-7B, CMPQ yields a 30% improvement in perplexity (from 15.97 to 11.11), while LLM-MQ also shows a dramatic improvement, moving from poor performance (85.16) to performance that even surpasses the baseline (16.32). This demonstrates that mixed-precision quantization can trade a small increase in storage overhead for a substantial boost in performance - something that is not achievable with and Furthermore, the stark differences between CMPQ at 2-bit and 3-bit quantization indicate that quantization techniques face greater challenges at lower bit-widths. The strong performance of CMPQ at 2-bit and 3-bit quantization demonstrates its effectiveness, particularly in scenarios where lower bit-widths are required."}, {"title": "4.4 COMPARISON WITH SQUEEZELLM", "content": "While we primarily compare with baselines that do not rely on backpropagation, we acknowledge that gradient information, at the cost of extra resources, can indeed enhance the performance of quantized LLMs. In Table 3, we compare our method with the state-of-the-art baseline, SqueezeLLM, which also employs non-uniform quantization but uses gradient information to weight the clustering process, safeguarding more sensitive weights. Additionally, we report the memory requirements for loading LLMs and performing backpropagation in FP16 precision\u00b9.\nAs shown in Table 3, SqueezeLLM outperforms CMPQ across various models at different quantization levels. However, this improvement comes at a significant cost: SqueezeLLM requires four times"}, {"title": "4.5 DATA EFFICIENCY AND GENERALIZATION", "content": "Data Efficiency for the Calibration Set. In Table 4, we present a data efficiency analysis based on the number of data samples in the calibration datasets and compare the perplexity of the LLaMA2-7B model under 3-bit quantization. Although a calibration set of 128 data samples is used consistently throughout the paper, our method typically achieves the desired quantization performance with as few as single-digit sample sizes. This efficiency stems from the fact that we do not rely on regression or backpropagation; instead, we only measure the activation norm from the calibration set, making the process highly data-efficient. In contrast, both GPTQ and AWQ require more than 50 data points for calibration, as reported in (Kim et al., 2024).\nRobustness to the Calibration Set. We evaluate the robustness of CMPQ by analyzing its perfor-mance using different calibration sets. Specifically, we compare CMPQ with QuIP in quantizing two OPT models into 2-bit representations. The results, presented in Table 5, demonstrate that CMPQ consistently outperforms QuIP in low-bit quantization across different calibration sets.\nAs the size of LLMs increases, both methods demonstrate improved robustness. However, a notable difference emerges in their performance under varying conditions. While QuIP performs effectively on 2-bit quantization when the evaluation dataset aligns with the calibration set (see Table 2), it experiences a significant performance decline, and may even diverge, when tested on a different"}, {"title": "4.6 ABLATION STUDY OF OUTLIER PROTECTION", "content": "In this section, we conduct experiments on the OPT-2.7B and OPT-6.7B models to analyze the impact of two distinct types of outliers on quantization performance. For a consistent comparison, when we remove one type of outlier, we retain the other type, ensuring that it constitutes 0.5% of the entire weight matrix. The results, presented in Table 6, demonstrate that both types of outliers contribute to enhancing the performance of quantized LLMs, particularly in low-bit settings. Notably, protecting both types of outliers yields the best results in general. This is because each type of outlier addresses different aspects: activation-based outliers safeguard salient weights, while quantization-based outliers ensure that the clustering process during quantization is not distorted by extreme values, thereby focusing on the majority of weights. In summary, these two outlier protection strategies complement each other, working in tandem to improve the overall model performance."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this work, we focused on mixed-precision quantization and aimed to design an algorithm capable of adapting to any bit-width constraint. We observed that different weight channels had varying impacts on model performance, and that activation distributions helped identify salient channels. Building on these insights, we proposed CMPQ, which integrated a channel-wise non-uniform quantization strategy. To further enhance performance, CMPQ introduced two types of outliers that collaboratively preserved critical information. Experimental results showed that CMPQ harnessed the potential of mixed-precision quantization in two key ways: (1) it achieved superior performance in integer-bit quantization tasks, and (2) it delivered significant performance improvements with only a modest increase in memory requirements. In the future, our focus will be on deploying CMPQ-quantized LLMs on real-world devices. This will involve addressing key challenges such as engineering hardware acceleration and designing efficient lookup table kernels to optimize performance."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 IMPACT OF SPARSITY LEVELS OF CMPQ", "content": "As discussed in Section 4.6, both activation-based and quantization-based outliers contribute to per-formance improvements. To evaluate the trade-off between performance and outlier protection ratio, we adjusted the ratio of activation-based outliers from 0.05% to 0.45% and present the perplexity results of the 3-bit quantized OPT-6.7B model on the C4 benchmarks, with varying outlier ex-traction percentages ranging from 0% to 0.5%, as shown in Fig 4. Notably, while we maintain a fixed protection ratio of 0.5% for quantization-based outliers across all experiments to ensure fair comparisons, the plot reveals that the perplexity gains diminish when the protection ratio exceeds 0.2%. This finding highlights CMPQ's potential to achieve superior performance with reduced stor-age requirements."}, {"title": "A.2 IMPACT OF NON-UNIFORM QUANTIZATION", "content": "In Table 7, we provide a detailed analysis to further clarify the impact of non-uniform quantization. For uniform quantization, we apply the widely used round-to-nearest method with a group size of 128 for channel-wise weight quantization, while preserving 0.5% of activation-based outliers to ensure a fair comparison. Additionally, we report the best perplexity achieved by the baseline methods. As shown in Table 7, across various bit-widths and model sizes, non-uniform quantization consistently outperforms uniform quantization, particularly in extremely low-bit (2-bit) settings. This is because the non-uniform distribution of weights leads to inefficient utilization of the quantization bins in uniform quantization, where some bins may remain underutilized or unused. Interestingly, we also observe that for certain tasks, uniform quantization can improve perplexity (e.g., OPT-13B at 3-bit on WikiText2). In such cases, equipping CMPQ with uniform quantization yields the best performance."}, {"title": "A.3 ADDITIONAL EXPERIMENTAL RESULTS", "content": "In Table 8, we present a comparison of quantization results across additional LLMs, including models from the OPT family ranging from 1.3B to 30B parameters, as well as the more recent LLaMA3-7B. Our findings are consistent with the main results, indicating that CMPQ generally outperforms all"}]}