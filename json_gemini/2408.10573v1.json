{"title": "Putting People in LLMs' Shoes:\nGenerating Better Answers via Question Rewriter", "authors": ["Junhao Chen", "Bowen Wang", "Zhouqiang jiang", "Yuta Nakashima"], "abstract": "Large Language Models (LLMs) have demonstrated significant capabilities, particularly in the domain of question answering (QA). However, their effectiveness in QA is often undermined by the vagueness of user questions. To address this issue, we introduce single-round instance-level prompt optimization, referred to as question rewriter. By enhancing the intelligibility of human questions for black-box LLMs, our question rewriter improves the quality of generated answers. The rewriter is optimized using direct preference optimization based on feedback collected from automatic criteria for evaluating generated answers; therefore, its training does not require costly human annotations. The experiments across multiple black-box LLMs and long-form question answering (LFQA) datasets demonstrate the efficacy of our method. This paper provides a practical framework for training question rewriters and sets a precedent for future explorations in prompt optimization within LFQA tasks.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have incorporated extensive world knowledge through learning vast publicly available corpora (Roberts, Raffel, and Shazeer 2020). It becomes increasingly common for people to seek knowledge from LLMs, especially in fields such as medicine and law (Atallah et al. 2023; Harrington 2023). However, a near-paradoxical issue arises: People ask questions to get knowledge, while lack of knowledge often leads to poorly formulated or vague questions, hindering LLMs from providing precise answers (Kim et al. 2023; Zhang et al. 2024). Fine-tuning can enhance LLMs' ability to understand vague questions, but most popular LLMs are black-box models, and their parameters are inaccessible. Thus, a step of transforming user questions into a format that LLMs can understand better, known as question rewriting, is crucial for question answering (QA).\nQuestion rewriting is closely related to prompt optimization. A prompt is an input to LLMs that guides them in generating a specific response, including a question (possibly with some instructions), a conversation history, etc. (Liu et al. 2023). Question rewriting is prompt optimization solely for questions. Previous work on prompt optimization primarily focused on optimizing task-level prompts. They decompose prompts into task-level instructions and instance-level inputs, optimizing task-level instructions for better performance across all instances of the task (Fernando et al. 2023; Guo et al. 2023; Kong et al. 2024).\nRecent studies have shown that directly optimizing the prompts at the instance level offers more flexibility in prompt editing tailored for a given prompt(Lin et al. 2024) and can lead to better responses (Srivastava et al. 2023). By obtaining feedback from humans or LLMs, they iteratively refine a given prompt, which requires multi-round interactions. In addition, previous prompt optimization is mainly applied to arithmetic reasoning (Cobbe et al. 2021) and short-form question answering (SFQA) (Kwiatkowski et al. 2019), where the latter involves answers in few words. These tasks do not necessarily cover real-world QA scenarios.\nThis paper proposes single-round instance-level prompt"}, {"title": "Method", "content": "Our question rewriter R learns to rewrite questions so that an LLM can give a better answer for a rewritten question. We design our method under the assumption that the goodness of the answer to a certain question is automatically judge-able. With this assumption, we can sample rewritten questions and the corresponding answers using LLMs, contrasting them to learn desirable questions.\nFigure 2 shows the pipeline of our method. Let D = {(q,a)} denote a training dataset of pairs of question q and answer a, with an associated set C = {c} of automatic evaluation criteria c. Firstly, our pipeline rewrites questions for q \u2208 D. Then, c\u2208 C evaluates the rewritten questions to make a set P = {(\u011d, \u011f)} of pairs of a better question \u011d and a worse question \u011f. Finally, we use DPO (Rafailov et al. 2023) to train R with P.\nSampling Rewritten Questions\nWe use a pre-trained LLM Ro to sample rewritten questions without fine-tuning as it offers sufficient capability for initial rewriting solely through prompt engineering. We use top-p sampling (Radford et al. 2019) to generate K different rewritten questions Q(q) = {rk(q)|k = 1,..., K} of"}, {"title": null, "content": "q \u2208 D, where rk(q) is the k-th rewritten question for q, with the predefined prompt t, i.e., t equals:\nRewriting question to make it more\nunderstandable, just give me the\nrewritten question without any other\nword:\nfollowed by q.\nMaking Better and Worse Question Pairs\nDatasets for LFQA typically provide methods for evaluating generated answers. For instance, some datasets (Manes et al. 2024) are inspired by FActScore (Min et al. 2023) and annotate the facts required to answer each question, allowing LLMs to assess whether the corresponding facts are implied by or contradict the generated answers to derive scores for comprehensiveness and precision. Other datasets (Lin, Hilton, and Evans 2022) offer extensive binary annotations used to train classifiers to determine whether answers conform to certain attributes like truthfulness. Additionally, some datasets\u00b9 are in the form of preference datasets, which provide pairs of samples, where one is better than the other. Such datasets can be used to train reward models to evaluate whether answers align with human preferences. We can use these automatic evaluation criteria as C to evaluate rewritten questions. Such automatic evaluation criteria substitute the human feedback typically used in previous methods (Rafailov et al. 2023) to make P.\nLet L denote a pre-trained LLM for answer generation. For a question-answer pair (q, a) \u2208 D, we generate answers for all q' \u2208 Q(q) as a' = L(q'). We also generate the answer to the original question q as \u00e3 = L(q), which serves as the baseline to judge the goodness of rewritten questions.\nTo make better-worse pairs, we first identify q' \u2208 Q(q) that gives better answers and worse answers, collectively denoted by Q+(q) and Q-(q), respectively. Observing that criterion c\u2208 C is often numerical\u00b2, we judge q' is better if a' is larger than or equal to \u00e3 in terms of all criteria and a' is larger than a at least one criterion,\u00b3 i.e.,"}, {"title": null, "content": "Q+(q) = {q' \u2208 Q(q)|\u2200c\u2208c c(a') \u2265 c(\u00e3),\n\u2203c\u2208c c(a') > c(\u00e3)}.\n(1)\nQ-(q) is defined in the opposite way, i.e., a' should be always worse than or equal to \u00e3 and a' should be worse than \u00e3 for at least one criterion.\nA better and worse question pair is created by picking one rewritten question from Q+(q) and the other from Q-(q). As we wish to train a model R to generate good questions, we rank rewritten questions in Q+ according to a certain"}, {"title": "Optimizing Question Rewriter", "content": "Training our question rewriter R is costly when it requires human feedback or a reward model that learns the human feedback. Fortunately, LFQA tasks typically offer automatic criteria to evaluate the goodness of generated answers. We can use the criteria to (indirectly) evaluate rewritten questions through evaluating their answers.\nLet PR(q'|t, q) denote the average probability of tokens in q' given the predefined prompt t and the original question q with R, given by:"}, {"title": null, "content": "PR(q'|t, q) = 1/K \u2211k=1 K PR(wk|t, q, w1:k\u22121),\n(3)\nwhere K is the length of q'; PR(wk|t, q, w1:k\u22121) is the probability of token wk given t, q, and a set w1:k\u22121 of tokens generated by the (k-1)-th step (i.e., q' = w1:K). PR0 (q') is defined likewise for the initial question rewriter R0. DPO's training loss is given by:"}, {"title": null, "content": "L = -E [log \u03c3 (\u03b2 log PR(\u011d|t,q)\nPR0(\u011d|t,q) - log PR(\u011f|t,q)\nPR0(\u011f|t,q))], (4)\nwhere \u03c3 is sigmoid, and \u03b2 is a hyperparameter that controls how much R deviates from Ro, and the expectation is computed over q ~ D and (\u011d, \u011f) ~ P(q).\nTo mitigate the risk of overfitting, we use dropout in the model. Also, the original LFQA dataset is divided into three parts: training, validation, and testing. R is trained on the training set (i.e., D) for one epoch, and we select the best model that most prefer \u011d's to \u011f's. Specifically, we define preference score PS as\nPS = E [1[PR(\u011d|t,q) > PR(\u011f|t, q)]],\n(5)\nwhere 1[] gives 1 if the given condition is satisfied, and otherwise 0; the expectation is computed for all the q from the validation set and (\u011d, \u011f) ~ P(q)."}, {"title": "Experiments", "content": "Experimental Setup\nDataset We evaluate three distinct LFQA datasets, each equipped with automated evaluation criteria.\nK-QA dataset (Manes et al. 2024), sourced from the medical domain, is designed to evaluate the factual comprehensiveness and precision of answers through metrics Scomp and Scont, employing a FActScore type method (Min et al. 2023). To combine these two criteria for ranking rewritten questions in Q+ (q), we first use Scont to rank them, and then use Scomp if Scont is the same for multiple questions.\nTruthfulQA (Lin, Hilton, and Evans 2022), covering multiple domains including health and law, assesses the truthfulness (Struth) and informativeness (Sinfo) of answers. The evaluation criteria are implemented as binary classifiers. We use the probabilities for positive classes (truthful for Struth and informative for Sinfo). An overall score (Soverall) is computed as the product of these scores. For better rewritten pair ranking, we use Soverall.\nOASST1QA, derived from the multi-turn dialogue alignment dataset OASST16, incorporates a criterion Spref that measures human preference for answers using a pre-trained reward model. This dataset provides a single criterion for evaluation (i.e., |C| = 1), so we directly use Spref for ranking better rewritten questions.\nMore details about these datasets and their evaluation criteria can be found in the appendix. Table 1 summarizes the statistics on the datasets.\nLLMS The base model of our question rewriter R (and Ro is Llama3-8B-instruct, and the answer generation model L is also Llama3-8B-instruct because it is one of the most powerful but small LLMs. R is fine-tuned with our method, while L is frozen. Subsequently, we evaluate the generalizability of R on multiple answer generation LLMs, including Llama3-8B-instruct7, mistral-7B-instruct-v0.28, zephyr-7B-beta, gemma-1.1-7B-it10, gpt-3.5-turbo-1106, and gpt-40-2024-05-13. They will be referred to as Llama3-8B, Mistral-7B-v0.2, Zephyr-7B-beta, Gemma-1.1-7B, GPT-3.5, and GPT-40, respectively. It is worth noting that we only use L as Llama3-8B-instruct to build P for training R, and then test the generalizability of R on other models.\nHyp"}, {"title": "erparameters", "content": "We borrowed open-source code for DPO training over all three datasets\u00b9\u00b9, which also provides the code for supervised fine-tuning of automatic criteria Struth and Sinfo for TruthfulQA. During DPO training, we set the dropout rate to 0.8, the training batch size to 32, and the testing batch size to 64, maintaining all other parameters at their default settings in the source code. For sampling rewritten questions, we use top-p sampling, where the cumulative probability for top-p sampling is set to 0.999, and the temperature of Ro is 1, to ensure diversity. We sample 100 unique rewritten questions for each of the original questions and terminate the sampling after 10,000 attempts. N+ and N_ are defaulted to (10, 20), (5, 10), and (4, 5) in K-QA, TQA, and OQA respectively. When multiplied by the number of samples in the corresponding training sets, they are around 20,000. The maximum token length is set to 512 during feedback collection and testing. During testing, to ensure reproducibility, we generate answers using greedy sampling.\nDevice All our testing and training, except for the DPO training of OASST1QA, are conducted on a system equipped with four NVIDIA A100-PCIE-40GB. Due to the extensive length of OASSTIQA, we only used samples whose question plus the prompt t and rewritten questions q' for question rewriting is less than or equal to 512 tokens and conducted the DPO training on a system with four NVIDIA A100-80GB-PCIe.\nBaselines We compare our method with both the original questions and the initial Llama3-8B-instruct rewriter (without fine-tuning). To show the effectiveness of our method, we also compare it with extensively used task-level prompting, Zero-Shot Chain-of-Thought (Zero-Shot CoT) (Kojima et al. 2022). Other instance-level methods require multiple rounds of interactions with humans or LLMs to obtain feedback and make iterative changes to the prompt or question during inference, which is extremely costly in our QA scenarios. We believe comparing our method with such methods is unfair, and we decided not to use them as baseline."}, {"title": "Results", "content": "Table 2 summarizes our experimental results over three LFQA datasets. Our method demonstrates superior performance in most combinations of LLMs and datasets.\nFor the K-QA dataset, our method consistently shows the highest Scomp scores across all models, especially with GPT-40, where the improvement is most significant. Furthermore, it achieves the lowest Scont with half of the models and the second lowest in the rest. Notably, our method trained an effective question rewriter using only 151 samples (i.e., training set plus validation set), implying that our method requires only a small number of annotated samples in a real-world QA scenario. Table 3 shows an example from K-QA, in which Scomp increases and Scont decreases after rewriting the original question.\nFor the TruthfulQA dataset, all methods generally reduce the informativeness (i.e., Sinfo) of the answers, and only ours"}, {"title": "Impact of N+ and N_", "content": "The number of better and worse question pairs P = \u22c3q\u2208DP(q) is determined by N+, N_, and |D| (i.e., |P| = N+\u00d7N_ x D), where the choice of N+ and N_ is rather arbitrary. To explore the impact of N+ and N_ on the performance, we evaluated our method with varying N+ and N_ over K-QA OASST1QA with Llama-3-8B12. The results are"}, {"title": "Comparison with Task-level Prompt Optimization", "content": "To explore whether optimizing prompts at the task level can be effective on the LFQA dataset, we adapted our method to optimize task instructions. In this process, we used prompts from PRewrite (Kong et al. 2024):\nRewrite the following instruction via\nrephrasing and/or adding specific\nrequirements. Add instructions which\nwould be helpful to solve the problem\ncorrectly. Output the new instruction\nonly.\nto optimize the original task instruction: \"provide the answer:\", and then appended the optimized instruction to the original questions to obtain answers. The optimized instruction can be found in the appendix.\nWe test the effect of migrating our method to the task-level on the K-QA dataset. According to Table 5, optimizing at the task level reduces the Scont for 4 LLMs, yet only improves the Scomp of two LLMs while optimizing at the instance level almost always works. It proves that consistently implementing effective optimization of prompts on the LFQA dataset at the task level, rather than at the instance level, is much more challenging."}, {"title": "Cross-Dataset Generalizability", "content": "We explored the performance of rewriters trained on Llama3-8B-instruct across different datasets. Table 4 shows that rewriters trained in the K-QA and TruthfulQA datasets can optimize the generated answers on the OASSTIQA dataset, but each fails on one metric in their respective datasets. In contrast, rewriters trained on the OASSTIQA dataset are almost ineffective on the other two datasets. This suggests that training on more complex LFQA datasets, which include multiple automatic evaluation criteria, yields rewriters with better generalizability."}, {"title": "Discussion", "content": "Our question rewriters can significantly improve the representation of questions, making them more likely to obtain higher-quality answers with LLMs. To quantitatively analyze how attributes impact the evaluation criteria of generated answers, we study 50 original questions in KQA's test set and their rewritten versions, resulting in 100 questions in total. We adopt 10 attributes: non-leadingness, word choice, tone, conciseness, neutrality, grammar and spelling, structure, politeness, clarity, and emotion, which are identified by an LLM. For each attribute and each question, we use GPT-40 to assign a score ranging from 1 to 5 to the 100"}, {"title": "Conclusions and Future Work", "content": "This paper proposes single-round instance-level question optimization for LFQA tasks, coined question rewriter. We employ DPO to optimize the question rewriter with automatic evaluation criteria. Our experimental results demonstrated that our question rewriter can generate questions that give better answers in terms of the automatic evaluation criteria. Meanwhile, our method requires to train the rewriter, which makes it domain specific. Exploring domain-agnostic approaches can be an interesting future direction."}, {"title": "Comparison for Sampling Combination", "content": "We test the performance of rewriters trained by P(q) sampled by different sampling combination on the K-QA dataset. There are four sampling combinations in total, where Q+ can be either the top N+ questions or randomly sampled, and Q_ can be either the bottom N_ questions or randomly sampled. Table 6 shows that rewriters trained by the Best-Random combination, which is the combination we currently use, perform the best, failing to optimize only one metric in one model. The Random-Random combination is effective across most models and metrics. The other three compositions all have 3 or 4 metrics worse than the corresponding original."}, {"title": "Dataset", "content": "We conduct experiments on three LFQA datasets that have automated evaluation criterias.\nK-QA\nA medical LFQA dataset (Manes et al. 2024) collected from a medical platform featuring questions from real-world patients in which questions are carefully curated to be ansared independently without additional patient history. The evaluation method for this dataset is inspired by the FActScore (Min et al. 2023). Thus, in this dataset, each question is associated with must-have facts collected by professional annotators. These facts are then used to evaluate two metrics Scomp and )Scont, considering the comprehensiveness and of the answers, respectively, defined as follows:"}, {"title": null, "content": "Scomp(a) = |{x \u2208 Must-Have | a entails x}|,\nMust_Have\nScont(a) = |{x \u2208 Must-Have | a contradicts x}|,\nwhere Must-Have represents the must-have facts for each question, a entails x indicates that the answer entails the corresponding fact, and a contradicts x indicates that the answer contradicts the corresponding fact. The entailment and contradiction are determined by LLM. The original study used GPT-4 for automatic evaluation. Due to time and cost constraints, we chose to use Llama-3-8B-instruct for automatic evaluation. The average accuracy of judgments across the dataset is 70%."}, {"title": "TruthfulQA", "content": "An LFQA data (Lin, Hilton, and Evans 2022) set is created to automatically assess the truthfulness and informativeness of the answers generated by LLMs. It spans 38 categories, including health, law, finance, and politics. In order to train the automatic evaluators, it includes truthful, untruthful, informatives, and uninformative answers for each question. After fine-tuning LLMs by specific templates, we can get automatic evaluators that can assess the truthfulness and informativeness of new answers to questions in this dataset. Upon inputting a question and an answer, the evaluators output a \"yes\" or \"no\", which is used to calculate the scores for truthfulness or informativeness using the following formula:"}, {"title": null, "content": "S = Pyes\nPyes + Pno\nThe original research use a GPT-3 base evaluation model. However, due to the deprecation of the corresponding API, we have transitioned to using Llama-3-8B-instruct as the base model. The batch size we use is 32, and the learning rate we use is 5 \u00d7 10-6. To ensure the evaluator evaluates according to the original research standards with an accuracy exceeding 90%, we conduct training for 2 epochs on truthfulness and 1 epoch on informativeness. We achieve accuracies of 93% and 94%, respectively. To prevent any potential information leakage during the evaluation of constructing the preference dataset, extra automatic evaluators are fine-tuned exclusively on the training set question and used to evaluate the metrics for answers to rewritten questions corresponding to the training set questions."}, {"title": "OASST1QA", "content": "An LFQA dataset derived from the multi-turn dialogue alignment dataset, OASST114. OASST1 has several different responses along with human-annotated rankings in each dialog context. The dataset oasst1_pairwise15 selects the highest and lowest ranked responses from each dialogue context to construct a preference dataset. The pretrained reward model16 we use is a binary classification model fine-tuned by oasst1_pairwise. It can output the probability of whether a response is the highest-ranked response in a given dialogue context and the probability can be regarded as a measure of human preference, where a higher score indicates a greater preference.\nWe randomly sample 1000 single-round English QA pairs from the training set of oasst1_pairwise to serve as our training dataset. We use half of the single-round English dialog from the validation set of oasst1_pairwise as our preference dataset's validation set and the remaining half for final evaluation. We remeasure the discriminator, and its accuracy in the new testset is 75%."}, {"title": "Statistic", "content": "In both KQA and TruthfulQA, approximately 50% of the questions are randomly selected as the training set, with 25% of the questions serving as the validation set and test set. The selection method for OASST1QA is as described previously. Detailed statistics are in Table 1."}, {"title": "Optimized Task Instruction of K-QA", "content": "The optimized task instruction of K-QA obtained after migrating our method to task level is as follows:\n**To Solve the Problem\nCorrectly:**\n\nTo provide the\ncorrect answer, please follow these\nsteps:\n\n1.**Read the question"}, {"title": "Question Evaluation Templates", "content": "In this section, we present the templates used to evaluate question attributes. There are a total of ten attributes assessed, namely non-leading, word choice, tone, conciseness, neutrality, grammar and spelling, structure, politeness, clarity, and emotion. We provide detailed temples in Table 7, Table 8 and Table 9."}, {"title": null, "content": "Evaluate the word choice of the following question based on the specified criteria and provide an explanation for your rating.\n\n\n- **1 point - Very Poor:** The question uses non-professional or colloquial language.\n-\n**2 points - Poor:** The question uses some non-professional language or terminology.\n-\n**3 points - Moderate:** The question uses mostly professional language but may include some informal terms.\n-\n**4 points - Good:** The question uses professional and precise language.\n-\n**5 points - Excellent:** The question uses highly professional and precise language, appropriate for the context.\n\n\n**Question:** GIVEN QUESTION\n\n**Rating:** {1-5points}\n\n**Reasoning:**\n\n**Example:**\n\n**Question:** Can you please provide detailed information on the oral medication options available for treating scabies?\n\n**Rating:** 5 points\n\n**Reasoning:**\n- The word choice is highly professional and precise, suitable for a medical inquiry, making it appropriate and respectful for the context.\nRate the tone of the following question based on the specified criteria and provide an explanation for your rating.\n\n\n- **1 point - Very Unprofessional Tone:** The question uses informal language, including slang or overly casual expressions that are inappropriate for a professional setting.\n\n- **2 points - Unprofessional Tone:** The question is somewhat informal, possibly including casual language that may not be suitable for all professional contexts.\n\n- **3 points - Moderately Professional Tone:** The question uses generally professional language but may include slight informal elements that could be polished further.\n\n- **4 points - Quite Professional Tone:** The question maintains a professional tone, using appropriate language for the context and avoiding casual terms.\n\n- **5 points - Very Professional Tone:** The question exemplifies a highly professional tone, using formal language that is perfectly suited for any professional setting.\n\n\n**Question:** GIVEN QUESTION\n\n**Rating:** {1-5 points}\n\n**Reasoning:**\n\n**Example:**\n\n**Question:** Can you please provide detailed information on the oral medication options available for treating scabies?\n\n**Rating:** 5 points\n\n**Reasoning:**\n- The question maintains a highly professional tone, using formal and respectful language suitable for a medical inquiry. The use of \"please\" adds a courteous touch, enhancing the professional quality of the communication.\nEvaluate the conciseness of the following question based on the specified criteria and provide an explanation for your rating.\n\n\n- **1 point - Very Verbose:** The question is verbose, containing much irrelevant information that obscures the main point.\n- **2 points - Verbose:** The question includes unnecessary details that detract from the main point, making it less concise.\n- **3 points - Moderately Concise:** The question is generally concise, though it contains a few extraneous details.\n- **4 points - Quite Concise:** The question is concise and to the point, with no unnecessary information.\n- **5 points - Extremely Concise:** The question is extremely concise, containing only essential information needed to answer the question effectively.\n\n\n**Question:** GIVEN QUESTION\n\n**Rating:** {1-5 points}\n\n**Reasoning:**\n\n**Example:**\n\n**Question:** Can you please provide detailed information on the oral medication options available for treating scabies?\n\n**Rating:** 5 points\n\n**Reasoning:**\n- The question is extremely concise, focusing solely on the essential information required. It directly asks for detailed options for oral medication without adding any superfluous details, ensuring the query is clear and straightforward.\n\nPlease rate the given question using this template.\nEvaluate whether the following question is non-leading based on the specified criteria and provide an explanation for your rating.\n\n\n- **1 point - Highly Leading:** The question is leading, suggesting a particular answer or outcome.\n- **2 points - Somewhat Leading:** The question is somewhat leading, with language that hints at a specific answer.\n- **3 points - Moderately Non-leading:** The question is mostly non-leading, with only slight suggestions towards an answer.\n- **4 points - Quite Non-leading:** The question is non-leading, asking for information without suggesting an answer.\n- **5 points - Completely Non-leading:** The question is entirely non-leading, purely seeking information without any implied bias towards certain answers.\n\n\n**Question:** GIVEN QUESTION\n\n**Rating:** {1-5 points}\n\n**Reasoning:**\n\n**Example:**\n\n**Question:** Can you please provide detailed information on the oral medication options available for treating scabies?\n\n**Rating:** 5 points\n\n**Reasoning:**\n- The question does not suggest any particular answer or bias, simply requesting information, making it completely non-leading.\n\nPlease rate the given question using these templates."}, {"title": null, "content": "Evaluate the politeness of the following question based on the specified criteria and provide an explanation for your rating.\n\n\n- **1 point - Very Impolite:** The question is rude or disrespectful, lacking basic courtesy.\n- **2 points - Impolite:** The question may come across as brusque or somewhat disrespectful.\n-\n**3 points - Moderately Polite:** The question is polite but may lack warmth or additional elements of courteous language.\n- **4 points - Quite Polite:** The question is polite and respectful, using appropriate expressions of courtesy.\n- **5 points - Very Polite:** The question is very polite and courteous, including expressions that show respect and consideration.\n\n\n**Question:** GIVEN QUESTION\n\n**Rating:** {1-5 points}\n\n**Reasoning:**\n\n**Example:**\n\n**Question:** Can you please provide detailed information on the oral medication options available for treating scabies?\n\n**Rating:** 5 points\n\n**Reasoning:**\n- The use of \"please\" shows a high level of politeness and respect, making the question very courteous and considerate in tone.\nEvaluate the grammar and spelling of the following question based on the specified criteria and provide an explanation for your rating.\n\n\n- **1 point - Very Poor:** The question contains numerous grammatical and spelling errors.\n-\n**2 points - Poor:** The question has several grammatical and spelling errors.\n- **3 points Moderate:** The question has occasional grammatical or spelling errors.\n- **4 points - Good:** The question has minor or no grammatical and spelling errors.\n- **5 points - Excellent:** The question is free from grammatical and spelling errors.\n\n\n**Question:** GIVEN QUESTION\n\n**Rating:** {1-5 points}\n\n**Reasoning:**\n\n**Example:**\n\n**Question:** Can you please provide detailed information on the oral medication options available for treating scabies?\n\n**Rating:** 5 points\n\n**Reasoning:**\n- The question is free from grammatical and spelling errors, demonstrating excellent use of language.\nEvaluate the structure of the following question based on the specified criteria and provide an explanation for your rating.\n\n\n- **1 point - Very Poorly Structured:** The question is poorly structured and difficult to follow, lacking logical sequence.\n- **2 points - Poorly Structured:** The question has some structure, but it lacks effective organization and logical flow.\n- **3 points - Moderately Well-Structured:** The question is reasonably well-structured but could be improved for better understanding and flow.\n- **4 points - Well-Structured:** The question is well-organized, making it easy to follow and understand.\n- **5 points - Excellently Structured:** The question is excellently structured, enhancing quick and clear comprehension with a logical and coherent sequence.\n\n\n**Question:** GIVEN QUESTION\n\n**Rating:** {1-5 points}\n\n**Reasoning:**\n\n**Example:**\n\n**Question:** Can you please provide detailed information on the oral medication options available for treating scabies?\n\n**Rating:** 5 points\n\n**Reasoning:**\n- The question is excellently structured, presenting a clear and direct inquiry. The logical progression from a general request for information to specific details about oral medication options makes it easy for the responder to understand and address."}, {"title": null, "content": "Evaluate the neutrality of the following question based on the specified criteria and provide an explanation for your rating.\n\n\n- **1 point - Very Biased:** The question is heavily biased", "Biased": "The question shows some bias", "Neutral": "The question is generally neutral but includes minor subjective elements.\n- **4 points - Quite Neutral:** The question is largely neutral"}, {"Neutral": "The question is entirely neutral", "biases.\n\n\n**Question": "GIVEN QUESTION\n\n**Rating:** {1-5 points"}, "n\n**Reasoning", "n\n**Example", "n\n**Question", "cabies?\n\n**Rating", 5, "points\n\n**Reasoning", 1, "motional", 2, "Emotional", 3, "motional", 4, "motional", 5, "eutral", "anguage.\n\n\n**Question", "cabies?\n\n**Rating", {"points}\n\n**Reasoning": "n\n**Example:** GIVEN QUESTION\n\n**Rating:** 5 points\n\n**Reasoning:**\n- The question is posed in a straightforward and factual manner", "Unclear": "The question is ambiguous and difficult to understand.\n- **2 points - Unclear:** The question is partially clear but could be misunderstood.\n- **3 points - Moderately Clear:** The question is understandable but could benefit from further clarification.\n- **4 points - Quite Clear:** The question is clear and easy to understand.\n- **5 points - Very Clear:** The question is completely clear", "ambiguity.\n\n\n**Question": "Can you please provide detailed information on the oral medication options"}]}