{"title": "Putting People in LLMs' Shoes:\nGenerating Better Answers via Question Rewriter", "authors": ["Junhao Chen", "Bowen Wang", "Zhouqiang jiang", "Yuta Nakashima"], "abstract": "Large Language Models (LLMs) have demonstrated signifi-\ncant capabilities, particularly in the domain of question an-\nswering (QA). However, their effectiveness in QA is often\nundermined by the vagueness of user questions. To address\nthis issue, we introduce single-round instance-level prompt\noptimization, referred to as question rewriter. By enhancing\nthe intelligibility of human questions for black-box LLMs,\nour question rewriter improves the quality of generated an-\nswers. The rewriter is optimized using direct preference opti-\nmization based on feedback collected from automatic criteria\nfor evaluating generated answers; therefore, its training does\nnot require costly human annotations. The experiments across\nmultiple black-box LLMs and long-form question answer-\ning (LFQA) datasets demonstrate the efficacy of our method.\nThis paper provides a practical framework for training ques-\ntion rewriters and sets a precedent for future explorations in\nprompt optimization within LFQA tasks.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have incorporated exten-\nsive world knowledge through learning vast publicly avail-\nable corpora (Roberts, Raffel, and Shazeer 2020). It be-\ncomes increasingly common for people to seek knowl-\nedge from LLMs, especially in fields such as medicine and\nlaw (Atallah et al. 2023; Harrington 2023). However, a near-\nparadoxical issue arises: People ask questions to get knowl-\nedge, while lack of knowledge often leads to poorly formu-\nlated or vague questions, hindering LLMs from providing\nprecise answers (Kim et al. 2023; Zhang et al. 2024). Fine-\ntuning can enhance LLMs' ability to understand vague ques-\ntions, but most popular LLMs are black-box models, and\ntheir parameters are inaccessible. Thus, a step of transform-\ning user questions into a format that LLMs can understand\nbetter, known as question rewriting, is crucial for question\nanswering (QA).\nQuestion rewriting is closely related to prompt optimiza-\ntion. A prompt is an input to LLMs that guides them in\ngenerating a specific response, including a question (possi-\nbly with some instructions), a conversation history, etc. (Liu\net al. 2023). Question rewriting is prompt optimization\nsolely for questions. Previous work on prompt optimiza-\ntion primarily focused on optimizing task-level prompts.\nThey decompose prompts into task-level instructions and\ninstance-level inputs, optimizing task-level instructions for\nbetter performance across all instances of the task (Fernando\net al. 2023; Guo et al. 2023; Kong et al. 2024).\nRecent studies have shown that directly optimizing the\nprompts at the instance level offers more flexibility in\nprompt editing tailored for a given prompt(Lin et al. 2024)\nand can lead to better responses (Srivastava et al. 2023).\nBy obtaining feedback from humans or LLMs, they iter-\natively refine a given prompt, which requires multi-round\ninteractions. In addition, previous prompt optimization is\nmainly applied to arithmetic reasoning (Cobbe et al. 2021)\nand short-form question answering (SFQA) (Kwiatkowski\net al. 2019), where the latter involves answers in few words.\nThese tasks do not necessarily cover real-world QA scenar-\nios.\nThis paper proposes single-round instance-level prompt"}, {"title": "Related Work", "content": "Early work for prompt optimization focused on white-box\nmodels (Shin et al. 2020; Shi et al. 2023; Li and Liang 2021;\nLester, Al-Rfou, and Constant 2021; Zhong, Friedman, and\nChen 2021). Due to the prevalent nature of black-box mod-\nels such as GPT-3 (Patel et al. 2023) and Claude (Anthropic\n2024), the following work targeted at these black-box mod-\nels. Most works decomposed prompts into task-level (i.e.,\ninstructions) and instance-level (i.e., specific queries) and\noptimizing only task-level instructions. Some work assumed\nthat input (i.e., text embeddings) and output (i.e., logits)\nare accessible and leveraged them to optimize prompts (Sun\net al. 2022b,a; Chai et al. 2022). Other recent work has at-\ntempted to remove this assumption. Prasad et al. (2023) and\nPryzant et al. (2023) evaluate task-level instructions with\nsmall edits (e.g., replacing some phrases with their syn-\nonyms) and find better ones step by step. Evolutionary al-\ngorithms (Fernando et al. 2023; Guo et al. 2023), reinforce\nlearning (Diao et al. 2023; Kong et al. 2024), and planning-\nbased methods (Wang et al. 2023) hae also been adopted.\nSome work fully utilized the inherent capabilities of\nLLMs to refine prompts. Zhou et al. (2023) leverages an\nLLM to generate and refine the prompts iteratively, and Yang\net al. (2023) provides the prompt optimization trajectory"}, {"title": "Method", "content": "Our question rewriter R learns to rewrite questions so that an\nLLM can give a better answer for a rewritten question. We\ndesign our method under the assumption that the goodness\nof the answer to a certain question is automatically judge-\nable. With this assumption, we can sample rewritten ques-\ntions and the corresponding answers using LLMs, contrast-\ning them to learn desirable questions.\nFigure 2 shows the pipeline of our method. Let D = \n{(q,a)} denote a training dataset of pairs of question q and\nanswer a, with an associated set C = {c} of automatic eval-\nuation criteria c. Firstly, our pipeline rewrites questions for\nq \u2208 D. Then, c\u2208 C evaluates the rewritten questions to\nmake a set P = {(\u011d, \u011f)} of pairs of a better question \u011d and a\nworse question \u011f. Finally, we use DPO (Rafailov et al. 2023)\nto train R with P."}, {"title": "Sampling Rewritten Questions", "content": "We use a pre-trained LLM Ro to sample rewritten questions\nwithout fine-tuning as it offers sufficient capability for ini-\ntial rewriting solely through prompt engineering. We use\ntop-p sampling (Radford et al. 2019) to generate K differ-\nent rewritten questions Q(q) = {rk(q)|k = 1,..., K} of"}, {"title": "Making Better and Worse Question Pairs", "content": "Datasets for LFQA typically provide methods for evaluat-\ning generated answers. For instance, some datasets (Manes\net al. 2024) are inspired by FActScore (Min et al. 2023) and\nannotate the facts required to answer each question, allow-\ning LLMs to assess whether the corresponding facts are im-\nplied by or contradict the generated answers to derive scores\nfor comprehensiveness and precision. Other datasets (Lin,\nHilton, and Evans 2022) offer extensive binary annotations\nused to train classifiers to determine whether answers con-\nform to certain attributes like truthfulness. Additionally,\nsome datasets\u00b9 are in the form of preference datasets, which\nprovide pairs of samples, where one is better than the other.\nSuch datasets can be used to train reward models to eval-\nuate whether answers align with human preferences. We\ncan use these automatic evaluation criteria as C to evaluate\nrewritten questions. Such automatic evaluation criteria sub-\nstitute the human feedback typically used in previous meth-\nods (Rafailov et al. 2023) to make P.\nLet L denote a pre-trained LLM for answer generation.\nFor a question-answer pair (q, a) \u2208 D, we generate answers\nfor all q' \u2208 Q(q) as a' = L(q'). We also generate the answer\nto the original question q as \u00e3 = L(q), which serves as the\nbaseline to judge the goodness of rewritten questions.\nTo make better-worse pairs, we first identify q' \u2208 Q(q)\nthat gives better answers and worse answers, collectively\ndenoted by Q+(q) and Q-(q), respectively. Observing that\ncriterion c\u2208 C is often numerical\u00b2, we judge q' is better if\na' is larger than or equal to \u00e3 in terms of all criteria and a' is\nlarger than a at least one criterion,\u00b3 i.e.,\nQ+(q) = {q' \u2208 Q(q)|\u2200c\u2208c c(a') \u2265 c(\u00e3),  cec c(a') > c(\u00e3)}. (1)\nQ-(q) is defined in the opposite way, i.e., a' should be al-\nways worse than or equal to \u00e3 and a' should be worse than\n\u00e3 for at least one criterion.\nA better and worse question pair is created by picking one\nrewritten question from Q+(q) and the other from Q-(q).\nAs we wish to train a model R to generate good questions,\nwe rank rewritten questions in Q+ according to a certain"}, {"title": "Optimizing Question Rewriter", "content": "Training our question rewriter R is costly when it requires\nhuman feedback or a reward model that learns the human\nfeedback. Fortunately, LFQA tasks typically offer automatic\ncriteria to evaluate the goodness of generated answers. We\ncan use the criteria to (indirectly) evaluate rewritten ques-\ntions through evaluating their answers.\nLet PR(q'|t, q) denote the average probability of tokens\nin q' given the predefined prompt t and the original question\nq with R, given by:\nPR(q') = 1/K \u2211k=1^K PR(wk|t, q, w1:k\u22121), (3)\nwhere K is the length of q'; PR(Wt|t, q, W1:k\u22121) is the prob-\nability of token wt given t, q, and a set W1:k-1 of tokens\ngenerated by the (k-1)-th step (i.e., q' = W1:K). Pro (q') is\ndefined likewise for the initial question rewriter R0. DPO's\ntraining loss is given by:\nL = -E [logo (Blog Pr(\u011d/q) / Blog Pro(q/q))], (4)\nwhere o is sigmoid, and \u1e9e is a hyperparameter that controls\nhow much R deviates from Ro, and the expectation is com-\nputed over q ~ D and (\u011d, \u011f) ~ P(q).\nTo mitigate the risk of overfitting, we use dropout in the\nmodel. Also, the original LFQA dataset is divided into three\nparts: training, validation, and testing. R is trained on the\ntraining set (i.e., D) for one epoch, and we select the best\nmodel that most prefer \u011d's to \u011f's. Specifically, we define\npreference score PS as\nPS = E [1[PR(Q\\t,q) > PR(q|t, q)]], (5)\nwhere 1[] gives 1 if the given condition is satisfied, and oth-\nerwise 0; the expectation is computed for all the q from the\nvalidation set and (\u011d, \u011f) ~ P(q)."}, {"title": "Experiments", "content": "Experimental Setup\nDataset We evaluate three distinct LFQA datasets, each\nequipped with automated evaluation criteria.\nK-QA dataset (Manes et al. 2024), sourced from the med-\nical domain, is designed to evaluate the factual comprehen-\nsiveness and precision of answers through metrics Scomp and\nScont, employing a FActScore type method (Min et al. 2023).\nTo combine these two criteria for ranking rewritten ques-\ntions in Q+ (q), we first use Scont to rank them, and then use\nScomp if Scont is the same for multiple questions.\nTruthfulQA (Lin, Hilton, and Evans 2022), covering\nmultiple domains including health and law, assesses the\ntruthfulness (Struth) and informativeness (Sinfo) of answers.\nThe evaluation criteria are implemented as binary classi-\nfiers. We use the probabilities for positive classes (truthful\nfor Struth and informative for Sinfo). An overall score (Soverall)\nis computed as the product of these scores. For better rewrit-\nten pair ranking, we use Soverall.\nOASST1QA, derived from the multi-turn dialogue align-\nment dataset OASST16, incorporates a criterion Spref that\nmeasures human preference for answers using a pre-trained\nreward model. This dataset provides a single criterion for\nevaluation (i.e., |C| = 1), so we directly use Spref for ranking\nbetter rewritten questions.\nMore details about these datasets and their evaluation cri-\nteria can be found in the appendix. Table 1 summarizes the\nstatistics on the datasets.\nLLMS The base model of our question rewriter R (and\nRo is Llama3-8B-instruct, and the answer generation model\nL is also Llama3-8B-instruct because it is one of the most\npowerful but small LLMs. R is fine-tuned with our method,\nwhile L is frozen. Subsequently, we evaluate the generaliz-\nability of R on multiple answer generation LLMs, including\nLlama3-8B-instruct7, mistral-7B-instruct-v0.28, zephyr-7B-\nbeta, gemma-1.1-7B-it10, gpt-3.5-turbo-1106, and gpt-40-\n2024-05-13. They will be referred to as Llama3-8B, Mistral-\n7B-v0.2, Zephyr-7B-beta, Gemma-1.1-7B, GPT-3.5, and\nGPT-40, respectively. It is worth noting that we only use L\nas Llama3-8B-instruct to build P for training R, and then\ntest the generalizability of R on other models."}, {"title": "Hyperparameters", "content": "We borrowed open-source code for\nDPO training over all three datasets\u00b9\u00b9, which also provides\nthe code for supervised fine-tuning of automatic criteria\nStruth and Sinfo for TruthfulQA. During DPO training, we set\nthe dropout rate to 0.8, the training batch size to 32, and\nthe testing batch size to 64, maintaining all other parame-\nters at their default settings in the source code. For sampling\nrewritten questions, we use top-p sampling, where the cumu-\nlative probability for top-p sampling is set to 0.999, and the\ntemperature of Ro is 1, to ensure diversity. We sample 100\nunique rewritten questions for each of the original questions\nand terminate the sampling after 10,000 attempts. N+ and\nN_ are defaulted to (10, 20), (5, 10), and (4, 5) in K-QA,\nTQA, and OQA respectively. When multiplied by the num-\nber of samples in the corresponding training sets, they are\naround 20,000. The maximum token length is set to 512 dur-\ning feedback collection and testing. During testing, to ensure\nreproducibility, we generate answers using greedy sampling."}, {"title": "Device", "content": "All our testing and training, except for the\nDPO training of OASST1QA, are conducted on a system\nequipped with four NVIDIA A100-PCIE-40GB. Due to the\nextensive length of OASSTIQA, we only used samples\nwhose question plus the prompt t and rewritten questions q'\nfor question rewriting is less than or equal to 512 tokens and\nconducted the DPO training on a system with four NVIDIA\nA100-80GB-PCIe."}, {"title": "Baselines", "content": "We compare our method with both the original\nquestions and the initial Llama3-8B-instruct rewriter (with-\nout fine-tuning). To show the effectiveness of our method,\nwe also compare it with extensively used task-level prompt-\ning, Zero-Shot Chain-of-Thought (Zero-Shot CoT) (Kojima\net al. 2022). Other instance-level methods require multiple\nrounds of interactions with humans or LLMs to obtain feed-\nback and make iterative changes to the prompt or question\nduring inference, which is extremely costly in our QA sce-\nnarios. We believe comparing our method with such meth-\nods is unfair, and we decided not to use them as baseline."}, {"title": "Results", "content": "Table 2 summarizes our experimental results over three\nLFQA datasets. Our method demonstrates superior perfor-\nmance in most combinations of LLMs and datasets.\nFor the K-QA dataset, our method consistently shows the\nhighest Scomp scores across all models, especially with GPT-\n40, where the improvement is most significant. Furthermore,\nit achieves the lowest Scont with half of the models and the\nsecond lowest in the rest. Notably, our method trained an ef-\nfective question rewriter using only 151 samples (i.e., train-\ning set plus validation set), implying that our method re-\nquires only a small number of annotated samples in a real-\nworld QA scenario. Table 3 shows an example from K-QA,\nin which Scomp increases and Scont decreases after rewriting\nthe original question.\nFor the TruthfulQA dataset, all methods generally reduce\nthe informativeness (i.e., Sinfo) of the answers, and only ours"}, {"title": "Impact of N+ and N_", "content": "The number of better and worse question pairs P =\nUqEDP(q) is determined by N+, N_, and |D| (i.e., |P| =\nN+XN_ x D), where the choice of N+ and N_ is rather\narbitrary. To explore the impact of N+ and N_ on the perfor-\nmance, we evaluated our method with varying N+ and N_\nover K-QA OASST1QA with Llama-3-8B12. The results are"}, {"title": "Comparison with Task-level Prompt Optimization", "content": "To explore whether optimizing prompts at the task level can\nbe effective on the LFQA dataset, we adapted our method to\noptimize task instructions. In this process, we used prompts\nfrom PRewrite (Kong et al. 2024):\nRewrite the following instruction via\nrephrasing and/or adding specific\nrequirements. Add instructions which\nwould be helpful to solve the problem\ncorrectly. Output the new instruction\nonly.\nto optimize the original task instruction: \"provide the an-\nswer:\", and then appended the optimized instruction to the\noriginal questions to obtain answers. The optimized instruc-\ntion can be found in the appendix.\nWe test the effect of migrating our method to the task-\nlevel on the K-QA dataset. According to Table 5, optimiz-\ning at the task level reduces the Scont for 4 LLMs, yet only\nimproves the Scomp of two LLMs while optimizing at the\ninstance level almost always works. It proves that consis-\ntently implementing effective optimization of prompts on\nthe LFQA dataset at the task level, rather than at the instance\nlevel, is much more challenging."}, {"title": "Cross-Dataset Generalizability", "content": "We explored the performance of rewriters trained on\nLlama3-8B-instruct across different datasets. Table 4 shows\nthat rewriters trained in the K-QA and TruthfulQA datasets\ncan optimize the generated answers on the OASSTIQA\ndataset, but each fails on one metric in their respective"}, {"title": "Discussion", "content": "Our question rewriters can significantly improve the repre-\nsentation of questions, making them more likely to obtain\nhigher-quality answers with LLMs. To quantitatively ana-\nlyze how attributes impact the evaluation criteria of gener-\nated answers, we study 50 original questions in KQA's test\nset and their rewritten versions, resulting in 100 questions in\ntotal. We adopt 10 attributes: non-leadingness, word choice,\ntone, conciseness, neutrality, grammar and spelling, struc-\nture, politeness, clarity, and emotion, which are identified\nby an LLM. For each attribute and each question, we use\nGPT-40 to assign a score ranging from 1 to 5 to the 100"}, {"title": "Conclusions and Future Work", "content": "This paper proposes single-round instance-level question\noptimization for LFQA tasks, coined question rewriter. We\nemploy DPO to optimize the question rewriter with auto-\nmatic evaluation criteria. Our experimental results demon-\nstrated that our question rewriter can generate questions that\ngive better answers in terms of the automatic evaluation cri-\nteria. Meanwhile, our method requires to train the rewriter,\nwhich makes it domain specific. Exploring domain-agnostic\napproaches can be an interesting future direction."}, {"title": "Comparison for Sampling Combination", "content": "We test the performance of rewriters trained by P(q)\nsampled by different sampling combination on the K-QA\ndataset. There are four sampling combinations in total,\nwhere Q+ can be either the top N+ questions or randomly\nsampled, and Q_ can be either the bottom N_ questions or\nrandomly sampled. Table 6 shows that rewriters trained by\nthe Best-Random combination, which is the combination we\ncurrently use, perform the best, failing to optimize only one\nmetric in one model. The Random-Random combination is\neffective across most models and metrics. The other three\ncompositions all have 3 or 4 metrics worse than the corre-\nsponding original."}, {"title": "Dataset", "content": "We conduct experiments on three LFQA datasets that have\nautomated evaluation criterias."}, {"title": "K-QA", "content": "A medical LFQA dataset (Manes et al. 2024) collected\nfrom a medical platform featuring questions from real-\nworld patients in which questions are carefully curated to\nbe ansared independently without additional patient history.\nThe evaluation method for this dataset is inspired by the\nFActScore (Min et al. 2023). Thus, in this dataset, each ques-\ntion is associated with must-have facts collected by profes-\nsional annotators. These facts are then used to evaluate two\nmetrics Scomp and )Scont, considering the comprehensiveness\nand of the answers, respectively, defined as follows:\nScomp(a) =|{x \u2208 Must-Have | a entails x}| / |Must_Have|\nScont(a) = |{x \u2208 Must-Have | a contradicts x}|,\nwhere Must-Have represents the must-have facts for each\nquestion, a entails x indicates that the answer entails the cor-\nresponding fact, and a contradicts x indicates that the answer\ncontradicts the corresponding fact. The entailment and con-\ntradiction are determined by LLM. The original study used\nGPT-4 for automatic evaluation. Due to time and cost con-\nstraints, we chose to use Llama-3-8B-instruct for automatic\nevaluation. The average accuracy of judgments across the\ndataset is 70%."}, {"title": "TruthfulQA", "content": "An LFQA data (Lin, Hilton, and Evans 2022) set is created\nto automatically assess the truthfulness and informativeness\nof the answers generated by LLMs. It spans 38 categories,\nincluding health, law, finance, and politics. In order to train\nthe automatic evaluators, it includes truthful, untruthful, in-\nformatives, and uninformative answers for each question.\nAfter fine-tuning LLMs by specific templates, we can get\nautomatic evaluators that can assess the truthfulness and in-\nformativeness of new answers to questions in this dataset.\nUpon inputting a question and an answer, the evaluators out-\nput a \"yes\" or \"no\", which is used to calculate the scores for\ntruthfulness or informativeness using the following formula:\nS = Pyes / Pyes + Pno\nThe original research use a GPT-3 base evaluation model.\nHowever, due to the deprecation of the corresponding API,\nwe have transitioned to using Llama-3-8B-instruct as the\nbase model. The batch size we use is 32, and the learning\nrate we use is 5 \u00d7 10-6. To ensure the evaluator evaluates\naccording to the original research standards with an accu-\nracy exceeding 90%, we conduct training for 2 epochs on\ntruthfulness and 1 epoch on informativeness. We achieve\naccuracies of 93% and 94%, respectively. To prevent any\npotential information leakage during the evaluation of con-\nstructing the preference dataset, extra automatic evaluators\nare fine-tuned exclusively on the training set question and\nused to evaluate the metrics for answers to rewritten ques-\ntions corresponding to the training set questions."}, {"title": "OASST1QA", "content": "An LFQA dataset derived from the multi-turn dialogue\nalignment dataset, OASST114. OASST1 has several different\nresponses along with human-annotated rankings in each di-\nalog context. The dataset oasst1_pairwise15 selects the high-\nest and lowest ranked responses from each dialogue con-\ntext to construct a preference dataset. The pretrained reward\nmodel16 we use is a binary classification model fine-tuned\nby oasst1 pairwise. It can output the probability of whether\na response is the highest-ranked response in a given dialogue\ncontext and the probability can be regarded as a measure of\nhuman preference, where a higher score indicates a greater\npreference.\nWe randomly sample 1000 single-round English QA pairs\nfrom the training set of oasst1 pairwise to serve as our train-\ning dataset. We use half of the single-round English dialog\nfrom the validation set of oasst1 pairwise as our preference\ndataset's validation set and the remaining half for final eval-\nuation. We remeasure the discriminator, and its accuracy in\nthe new testset is 75%."}, {"title": "Statistic", "content": "In both KQA and TruthfulQA, approximately 50% of the\nquestions are randomly selected as the training set, with 25%\nof the questions serving as the validation set and test set. The\nselection method for OASST1QA is as described previously.\nDetailed statistics are in Table 1."}, {"title": "Optimized Task Instruction of K-QA", "content": "The optimized task instruction of K-QA obtained after mi-\ngrating our method to task level is as follows:\n**To Solve the Problem\nCorrectly:**\\n\\nTo provide the\ncorrect answer, please follow these\nsteps:\\n\\n1.**Read the question\ncarefully**: Make sure you understand\nthe question and its requirements.\nTake a moment to re-read the question\nto ensure you grasp the context and\nwhat is being asked.\\n2.**Use the\nprovided information**: Refer to the\ngiven data, charts, or graphs to\nhelp you answer the question. Make\nsure to accurately interpret the\ninformation and use it to support your\nanswer.\\n3.**Calculate or derive the\nanswer**: Use mathematical formulas,\ncalculations, or logical deductions\nto arrive at the correct answer.\nDouble-check your work to ensure\naccuracy and precision.\\n4.**Write\nyour answer clearly and concisely**: Use clear and concise language to\nexpress your answer. Avoid using\njargon or technical terms unless\nabsolutely necessary. Make sure\nyour answer is easy to read and\nunderstand.\\n5.**Check your work**: Verify your answer by re-checking your\ncalculations, formulas, or logical\ndeductions. Ensure that your answer is\naccurate, complete, and relevant to the\nquestion.\\n\\n**Additional Tips:**\\n\\n*\nUse a calculator or spreadsheet to help\nwith calculations, if necessary.\\n*\nBreak down complex problems into\nsmaller, manageable parts to make them\neasier to solve.\\n* Use visual aids,\nsuch as diagrams or flowcharts, to help\nillustrate your answer.\\n* Check for\nerrors or inconsistencies in your work\nto ensure accuracy and precision.\\n\\nBy\nfollowing these steps and tips, you\nwill be well on your way to providing\nthe correct answer and solving the\nproblem correctly."}, {"title": "Question Evaluation Templates", "content": "In this section, we present the templates used to evaluate\nquestion attributes. There are a total of ten attributes as-\nsessed, namely non-leading, word choice, tone, conciseness,\nneutrality, grammar and spelling, structure, politeness, clar-\nity, and emotion. We provide detailed temples in Table 7,\nTable 8 and Table 9."}]}