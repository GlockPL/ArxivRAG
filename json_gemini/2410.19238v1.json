{"title": "Designing LLM-Agents with Personalities: A Psychometric Approach", "authors": ["Muhua Huang", "Xijuan Zhang", "Christopher Soto", "James Evans"], "abstract": "This research introduces a novel methodology for assigning quantifiable, controllable and psychometrically validated personalities to Large Language Models-Based Agents (Agents) using the Big Five personality framework. It seeks to overcome the constraints of human subject studies, proposing Agents as an accessible tool for social science inquiry. Through a series of four studies, this research demonstrates the feasibility of assigning psychometrically valid personality traits to Agents, enabling them to replicate complex human-like behaviors. The first study establishes an understanding of personality constructs and personality tests within the semantic space of an LLM. Two subsequent studies using empirical and simulated data-illustrate the process of creating Agents and validate the results by showing strong correspondence between human and Agent answers to personality tests. The final study further corroborates this correspondence by using Agents to replicate known human correlations between personality traits and decision-making behaviors in scenarios involving risk-taking and ethical dilemmas, thereby validating the effectiveness of the psychometric approach to design Agents and its applicability to social and behavioral research.", "sections": [{"title": "Introduction", "content": "The emergence of large language models (LLMs) has revolutionized our approach to simulating human-like communication. LLMs are increasingly deployed across diverse research fields to mimic human behaviors (Xi et al., 2023; Xu et al., 2024). In psychology, LLMs are used to study cognitive processes, measure personality, and understand psychological constructs (Hagendorff et al., 2023; Jiang et al., 2024; Rathje et al., 2024). Sociologists employ these models to explore social bias and behavior (Lucy & Bamman, 2021; Park et al., 2023), while economists and political scientists utilize them to analyze economic processes and political leanings (Bang et al., 2024; Hartmann et al., 2023). This broad application underscores the growing interconnection between AI and social sciences.\nTraditional human subject research methods, while standard in social and behavioral studies, face significant challenges including ethical constraints, logistical hurdles, and resource limitations (Demszky et al., 2023; Salganik, 2019). Recent advancements in LLMs offer valuable complementary tools to address these challenges (Agnew et al., 2024; Demszky et al., 2023). By using LLMs as stand-ins for human agents, researchers can reduce logistical and financial burdens (S. Wang et al., 2021). These AI Agents operate continuously, can simulate diverse demographic responses, and can be deployed in scenarios that might pose ethical risks to human participants (Aher et al., 2023; Argyle et al., 2023; Bai et al., 2022).\nThe integration of Agents in social and behavioral science research serves multiple practical purposes while complementing rather than replacing human participants. They can be used for pilot testing studies, allowing researchers to refine experimental designs and identify potential issues before investing in full-scale human participant studies. Additionally, Agents can independently replicate findings from human-subjects data, enhancing the robustness and generalizability of research outcomes. This approach provides a complementary method for validation and exploration, contributing to the"}, {"title": "Problems with Previous Approaches for Assigning Personas to Agents", "content": "Among the myriad of traits that can be incorporated into Agents, personalities, as a part of persona, stand out as among the most intuitive characteristics to simulate. Personalities encapsulate a spectrum of human behaviors and tendencies that are crucial for the prediction of life outcomes ranging from academic and career to health and socioeconomic outcomes (Soldz & Vaillant, 1999; Soto, 2019; Stewart et al., 2022) to a multitude of interaction-based measures (Dang & Tapus, 2014; Furnham & Heaven, 1999). Despite this potential and the capacity of Agents to advance personality research, existing methodologies for implementing personality in Agents remain limited, typically falling into three distinct approaches.\nFirst, personas may be assigned with simple personality adjectives. For example, Jiang et al. (2024) assigned personalities to one of the Agents by telling it \"You are a character who is introverted, antagonistic, conscientious, emotionally stable, and open to experience.\" This type of approach holds a binary view either high-low or presence-absence of a trait contradicts established empirical evidence that personality traits exist on continuous spectra (Asendorpf, 2006; Marcus et al., 2006; Zrari & Sakale, 2024).\nSecond, personas may be assigned through demographic descriptions and personal preferences (Park et al., 2023; Serapio-Garc\u00eda et al., 2023). For instance, Park et al. (2023) used narrative descriptions such as \u201cJohn Lin is a pharmacy shopkeeper at the Willow Market and Pharmacy who loves to help people.\" This approach relies heavily on stereotypical information extrapolated from the LLM's training corpora, where names and"}, {"title": "Psychometric Approach for Assigning LLM-Agents Persona", "content": "In this article, we propose creating Agents with personalities through a psychometric approach by leveraging the Big Five Personality theory. Historically, the Big Five personality theory was developed based on the Lexical Hypothesis, which states that personality characteristics that are fundamental to humans have become a part of human language, and the most important characteristics are encoded by a single word (Caprara & Cervone, 2000). Based on the Lexical Hypothesis, scholars selected personality-related words from the English dictionary (Allport & Odbert, 1936), refined the list of words (W. T. Norman, 1963; W. Norman, 1967; Wiggins, 1979), had participants rate themselves"}, {"title": "Study 1: Semantic Representation of Personality Constructs in LLM", "content": "Study 1 aimed to establish a foundational understanding of the semantic nuances inherent in personality tests and constructs using embedding. Given that Agents are inherently constructed on embeddings, this initial investigation serves to validate the approach and set the stage for subsequent studies."}, {"title": "Methods", "content": "Our analysis incorporated content from widely recognized Big Five tests, such as the BFI (John et al., 1991), BFI2 (Soto & John, 2017), Mini-Markers (Saucier, 1994), BFAS (DeYoung et al., 2007) and NEO-FFI (Costa & McCrae, 1989). We extracted domain-specific content (i.e., test items) from these tests and processed it through OpenAI's advanced text-embedding model (text-embedding-3-large). Text-embedding techniques are commonly used in natural language processing; they allow researchers to represent text data (such as words, phrases, sentences, or even entire documents) in a numeric format (i.e., vector) that quantifies the semantic relationships between words, phrases, or entire documents. In the context of our study, each personality test item is converted into a high-dimensional vector (3072 dimensions for the model we used) that captures its semantic meaning. To illustrate this concept, consider two items from different"}, {"title": "Embeddings Techniques", "content": "To analyze these embeddings, we employed two main techniques, namely, cosine similarity and t-Distributed Stochastic Neighbor Embedding (t-SNE).\nCosine similarity. This measure quantifies the semantic similarity between two embedded texts, analogous to how correlation coefficients measure the relationship between variables in psychological research. Just as a correlation of 1 indicates a perfect positive relationship, a cosine similarity of 1 indicates identical semantic meaning. In contrast, a cosine similarity of 0 suggests no semantic relationship, similar to a correlation of 0 indicating no linear relationship between variables.\nt-SNE. This technique is conceptually similar to factor analysis, a method widely used in personality psychology to uncover latent structures. While factor analysis typically reduces many observed variables to a smaller number of latent factors, t-SNE reduces high-dimensional data to two or three dimensions for visualization purposes (Van der Maaten & Hinton, 2008). Just as factor analysis helps psychologists identify underlying personality traits from patterns of item responses, t-SNE helps us visualize clusters of semantically similar personality test items in a two-dimensional space. This allows us to see how items from different tests relate to each other, much like how factor loadings in exploratory factor analysis show how individual items relate to broader personality constructs.\nIn our t-SNE visualizations, items that cluster together can be interpreted as measuring similar constructs, analogous to how items with high loadings on the same factor in factor analysis are interpreted as measuring the same underlying trait. This provides a data-driven way to examine the convergent and discriminant validity of items across different personality assessments.\nThe use of embedding in this study offers several advantages for personality research. By capturing semantic relationships between test items, we can potentially uncover subtle distinctions between various personality assessments that might not be apparent through traditional psychometric methods. This approach provides a data-driven complement to classical item analysis techniques, potentially informing more refined personality models and enhancing our understanding of the semantic overlap between"}, {"title": "Results and Discussion", "content": "Figure 1 illustrates the cosine similarity between different personality tests across the Big Five domains. The top-left subplot is the average similarity across all domains which shows that most tests generally exhibit moderate to high cosine similarity (above 0.51), with the exception of Mini-Markers and NEO-FFI. Mini-Markers consistently shows a relatively lower cosine similarity with other scales across all domains, which may be attributed to its unique design approach: while other tests employ full statements, phrases, or questions for each item (e.g., \u201cIs complex, a deep thinker\"), Mini-Markers exclusively uses adjectives (e.g., philosophical\"). The remaining subplots, each corresponding to a specific Big Five domain, reveal similar patterns of cosine similarity between tests. This"}, {"title": "Study 2: Validating Personality Assignment in LLM-Agents Using Empirical Data", "content": "Study 2 evaluates whether Agents can internalize and manifest human psychological traits. It entails training of Agents using Big Five personality data obtained via one test and validating their responses against an alternative Big Five personality test.\nConsidering the findings of Study 1, which indicate a relatively low semantic similarity between BFI2 and Mini-Markers, we aim to validate the personality assignments made by the BFI2 using Mini-Markers. The logic is that if this method supports two fairly distinct psychometric tests, we can confidently generalize this validation approach to other tests that exhibit greater semantic similarity."}, {"title": "Methods", "content": "We repurposed data collected by Soto and John (2017) wherein participants (N = 438) responded to multiple Big Five tests, including: (1) BFI2: a sophisticated, 60-item,"}, {"title": "Procedure", "content": "Using OpenAI's API gpt-3.5-turbo-0125, we initiated 438 independent agent, each corresponding to a participant. Shown in Figure 3, we embedded participants' BFI2 answers into the prompts and asked the Agents to respond to the Mini-Markers test.\nTo examine the prompt format for personality assignment, we investigated two distinct scale formats: the Likert format and the Expanded format. The Likert format, traditionally used in most psychological scales, requires participants to rate their agreement with statements on a numeric scale and typically includes both positively worded items (e.g., \u201cIs talkative\") and reversely worded items (e.g., \"Tends to be quiet\") to help detect response patterns and measure opposite ends of trait spectra. In contrast, the Expanded format, a more recent innovation in psychological assessment, was developed to address issues associated with the Likert format, such as response bias and careless responding (e.g., Zhang & Savalei, 2016; Zhang et al., 2023, 2024). The Expanded format eliminates the distinction between positively and reversely worded items by embedding directionality within the response options rather than the items themselves. The Expanded format presents each response option as a complete sentence, more closely approximating natural language (Zhang et al., 2023, see Table 1 for example items). Soto and John (2017) originally developed the BFI2 using the Likert format, and Zhang et al. (2024) translated and validated BFI2 in the Expanded format. The prompts for both the Likert and Expanded formats are provided in Appendix A and Appendix B, respectively. Throughout"}, {"title": "Analysis", "content": "We performed convergent correlation analysis. Two convergent correlations were computed after we summed participants' Mini-markers domain scores: First, between participants' BFI2 scores and LLM-Agents'deduced Mini-markers scores. High correlations would suggest that LLM-agents can consistently deduce personality traits from training data. Second, between participants' actual Mini-markers scores and those inferred by LLM-Agents. High correlations here would indicate that LLM-Agents can accurately mirror individual human personalities."}, {"title": "Results and Discussion", "content": "Table 2 shows the convergent correlations between the Agents' Mini-Markers scores and input BFI2 scores, and the participants' real Mini-Markers scores.\nLikert Format. Responses from Agents-Likert had an average correlation of .728 with the BFI2 input scores, with coefficients ranging from .530 to .880. Furthermore, these scores demonstrated an average correlation of .664 with the actual Mini-Markers scores"}, {"title": "CFA", "content": "Table 3 presents the standardized factor loadings and Cronbach's alpha reliability coefficients for Mini-Markers responses from participants, Agents-Likert and Agents-Expanded. Three patterns emerged from this analysis.\nFirst, the three sets of responses exhibited comparable Cronbach's alpha reliability coefficients. For participants' data, the coefficients ranged from .820 to .869 (M = .843)."}, {"title": "Study 3: Parametric Approach to Creating LLM-Agents with Simulated Personality Data", "content": "Study 3 introduces a parametric approach for developing Agents using sample statistics derived from existing personality data. This method involves extracting key parameters from empirical data, simulating item responses based on these parameters, and then assigning these simulated responses to Agents. By doing so, we aim to provide an efficient alternative or precursor to traditional empirical data collection, facilitating the creation of diverse sets of Agents while maintaining psychometric validity."}, {"title": "Methods", "content": "As shown in Figure 4, we began by extracting key statistics from Soto & John's (2017b) BFI2 data, we extracted statistics, including facet means and standard deviations,"}, {"title": "Convergent Correlations.", "content": "Table 4 shows the convergent correlation coefficients between the simulated BFI2 inputs and the Agents' Mini-Markers responses. The average correlation for the Likert format prompt is .679, with individual values ranging from .452 to .887. For the Expanded format prompt, the average correlation is .782, with values ranging from .700 to .863. These results are comparable to those observed in Study 2, where correlations between human participants' BFI2 scores and Agents' Mini-Markers responses averaged .728 for Likert and .789 for Expanded. Moreover, as in Study 2, Openness in the Likert format showed substantially lower convergence than all other personality traits. These findings suggest that starting the design of Agents with"}, {"title": "CFA.", "content": "The results reveal patterns similar to those observed in Study 2, further validating the psychometric properties of Agents' simulated responses. Table 3 presents the standardized factor loadings and Cronbach's alpha reliability coefficients for Agents' simulated responses.\nThe CFA results are highly similar to Study 2's. First, the Neuroticism domain again had a convergence issue due to the multicollinearity. After removing either \u201cEnvious\u201d or \"Jealous\", the reliability coefficients for Study 3 are similar to those observed in Study 2. For the Likert format, the reliability coefficients yielded a mean of .759 (range: .600 to .832), while the Expanded format showed a mean of .896 (range: .800 to .970). These values are comparable to those found in Study 2 (Likert: M = .843, range: .735 to .892; Expanded: M = .942, range: .850 to .976) and remain higher than the mean reliability observed in human participants (M = .843, range: .820 to .869). Meanwhile, factor loadings obtained from Agents in Study 3 showed a similar pattern of discrepancy between positively and reversely worded items as observed in Study 2. For Agents-Likert, the average loading difference between these item types was .805, which is even more pronounced than the .647 difference observed in Study 2. Agents-Expanded, however, showed a smaller average difference of .223, which is closer to the .096 difference found in Study 2 and the .007 difference in human participants. This finding further supports the conclusion that the Expanded format produces more balanced and less biased responses."}, {"title": "Study 4: Comparing LLM-Agent and Human Risk-taking and Ethical Decisions", "content": "Study 4 aims to assess whether Agents exhibit behaviors consistent with humans who have the same personality profiles. This investigation sought to elucidate both the potential and limitations of employing Agents in behavioral science studies."}, {"title": "Methods", "content": "We crafted ten scenarios: five ethical dilemmas and five risk-taking scenarios.\nFollowing standard research protocols, we pre-registered the study and obtained ethical approval before proceeding with data collection at a Canadian University. Participants were asked to complete the BFI2 and respond to the ten scenarios. After applying exclusion criteria based on English proficiency, consent for data deposit, and survey completion, we retained a sample of 276 participants. The mean age of the participants was 19.65 years (SD = 3.88). Regarding gender identity, the majority of participants identified as female (80.4%), with 15.6% identifying as male and 4% identified as Other. The ethnic composition of the sample was diverse: 26.8% as European/Caucasian, 25.0% as South Asian, 7.3% as African, 6.9% as East Asian, 2.5% as Latino and Hispanic and 28.3% identified as Other."}, {"title": "Procedure", "content": "Using gpt-3.5-turbo-0125, we generated 276 Agents (Expanded), each assigned a unique set of personalities based on human participants' responses to the BFI2. Agents then indicated their decisions in risk-taking scenarios and moral dilemmas on a scale of 1 to 10.\nRisk-Taking Scenarios. The scenarios were designed to test an individual's risk-taking tendency versus risk-avoidance tendency. The specific scenarios included: 1) embarking on an entrepreneurial venture, 2) making significant investments, 3) confessing romantic feelings to a close friend, 4) participating in extreme sports, and 5) opting to study overseas. See Appendix C for the prompt.\nEthical Dilemma Scenarios. The scenarios were designed to assess individuals' empathetic tendency versus rule adherence tendency. Given that the majority of currently available LLMs have been safety-aligned and instruction fine-tuned (Biedma et al., 2024), which typically skews them away from engaging in severe moral judgments (e.g., the Trolley Problem), we introduced a series of everyday ethical dilemmas. These dilemmas necessitated choosing between upholding a standard and prioritizing empathy. Examples include decisions on 1) reporting a friend for cheating on a quiz, 2) addressing a colleague's misappropriation of office supplies, 3) underage drinking, 4) disclosing confidential information that could save lives, and 5) providing candid feedback on subpar performance. See Appendix D for the prompt."}, {"title": "Results and Discussion", "content": "In Table 5, for the risk-taking scenarios, Agents' risk-taking tendencies were largely consistent with those of humans with the same personality profiles: both humans and Agents showed increased risk-taking tendencies when having high Openness and Extraversion, and low Neuroticism. Conscientiousness and Agreeableness were not associated with risk-taking. This finding is consistent with previous studies on personality and risk, which found that high Extraversion and Openness were consistently associated"}, {"title": "General Discussion", "content": "The present research introduces a novel methodology for assigning quantifiable, controllable, and psychometrically validated personalities to Agents using the Big Five personality framework. Through a series of four studies, we demonstrated the feasibility of this approach and its potential applications in social science research.\nStudy 1 established a foundational understanding of personality constructs and personality tests within the embedding space of LLMs. By analyzing the semantic similarities across various personality tests, we laid the groundwork for understanding how LLMs interpret and represent personality-related concepts. This finding serves as the basis for subsequent studies, which further examined Agents' understanding of the underlying semantic associations during the personality assignment, adaptation, and reflection process.\nStudy 2 and Study 3 demonstrated that Agents could effectively absorb and manifest psychometrically validated personality traits. Study 2 showed strong alignment between Agents' responses and empirical data from human participants, while Study 3 illustrated the effectiveness of using a parametric approach to create Agents with distinct personalities. The use of both empirical data and simulated data based on sample statistics enhances the robustness and flexibility of our approach, offering tools that can adapt to varied research needs without compromising scientific rigor.\nImportantly, our research highlighted the advantage of the Expanded format over the traditional Likert format in assigning personalities to Agents. The Expanded format demonstrated consistently stronger convergent correlations with human participants' data and showed greater robustness against biases such as acquiescence and wording effects. This finding suggests potential modifications to the scale format of current psychometric tests that could lead to more nuanced and accurate personality replications in Agents."}, {"title": "Limitation & Future Direction", "content": "Model Selection\nWe selected the gpt-3.5-turbo-0125 API to develop Agents due to several compelling advantages. Firstly, this model is notable for its robust performance across various benchmarks, making it a preferred choice in the field. Additionally, its cost-effectiveness and the extensive context window it offers capable of encompassing text from an hour-long behavioral science study ensure comprehensive data handling without the interference of system drift. In addition, this version of gpt-3.5-turbo represents a static snapshot as of January 25th, 2024, thus it remains unaffected by alterations in subsequent versions of GPT, ensuring consistency and reproducibility in our research outputs.\nHowever, we recognize the potential for algorithmic confounding (Salganik, 2019) in Agents' responses, as market-available LLMs are intricately engineered to exhibit socially"}, {"title": "Generalizability & Engineering Considerations", "content": "The scope of our investigation is primarily centered around the Big Five personality traits, which, while foundational, restricts the breadth of our inquiry. To transcend this limitation, a systematic evaluation encompassing a diverse array of psychometric tests is needed. Such an in-depth comparative analysis of these tests will not only examine the generalizability of our findings but also significantly contribute to refining the engineering processes underlying the development of Agents.\nSimultaneously, the current research showcases the potential of using psychometric tests to create Agents with nuanced psychological traits, suggesting a novel intersection of psychology and artificial intelligence. However, the current reliance on manual coding practices and the bespoke nature of the developed code poses barriers to adoption, especially for social science researchers with limited experience with programming. Addressing this challenge necessitates the development of more user-friendly tools and frameworks that abstract away the complexities of the underlying code. By doing so, we can democratize the use of Agents in social science research, enabling a broader spectrum of scholars to leverage these advanced technologies in their work."}, {"title": "Diversity, Representation & Inclusion", "content": "Our approach to simulating population diversity using temperature settings is a preliminary step. However, this method falls short of capturing the intricate interplay of individual backgrounds and societal complexity. Future research should explore integrating more detailed demographic information and assessing its influence on model outputs. This will enable more precise control and deeper insights into model behavior.\nCritics may argue that our study's aim to supplant human participants with Agents could be misguided (Agnew et al., 2024). Indeed, humans still surpass the most advanced LLMs on multiple benchmarks, suggesting the intrinsic differences between Agents and humans. Furthermore, LLMs are often trained on WEIRD datasets, which do not adequately represent diverse cultural backgrounds and subpopulations, although"}, {"title": "Philosophical Boundaries and Pragmatic Focus", "content": "In the endeavor to create Agents with personality traits, a potential critique arises concerning the essence of \u201chumanhood\u201d in these agents, particularly given the ongoing debate about the consciousness of LLMs. This question undeniably merits exploration within the realms of artificial intelligence and large model research. Nevertheless, our study intentionally steers clear of this philosophical terrain. Our primary objective is to explore the feasibility of applying psychometric principles to construct Agents and to assess their utility in advancing social science research. By focusing on these pragmatic aspects, we aim to contribute to the methodological toolkit available to social scientists, leaving the contemplation of consciousness and \u201chumanhood\" in Agents to future interdisciplinary discourse. Instead, the method pioneered here could be used to pilot test studies before they are conducted with human participants, or to replicate previous findings obtained with human participants using a novel method."}, {"title": "Conclusion", "content": "In conclusion, this research presents a novel and promising approach to creating Agents with psychometrically valid personality traits. By leveraging the Big Five personality framework and advanced language models, we have demonstrated the potential for Agents to serve as valuable tools in social and behavioral research. While acknowledging the limitations and areas for future investigation, our findings suggest that this methodology could significantly enhance the efficiency, scale, and ethical"}]}