{"title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos", "authors": ["Prithviraj Banerjee", "Sindi Shkodrani", "Pierre Moulon", "Shreyas Hampali", "Shangchen Han", "Fan Zhang", "Linguang Zhang", "Jade Fountain", "Edward Miller", "Selen Basol", "Richard Newcombe", "Robert Wang", "Jakob Julian Engel", "Tomas Hodan"], "abstract": "We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (more than 3.7M images) of multi-view RGB/monochrome image streams showing 19 subjects interacting with 33 diverse rigid objects, multi-modal signals such as eye gaze or scene point clouds, as well as comprehensive ground-truth annotations including 3D poses of objects, hands, and cameras, and 3D models of hands and objects. In addition to simple pick-up/observe/put-down actions, HOT3D contains scenarios resembling typical actions in a kitchen, office, and living room environment. The dataset is recorded by two head-mounted devices from Meta: Project Aria, a research prototype of light-weight AR/AI glasses, and Quest 3, a production VR headset sold in millions of units. Ground-truth poses were obtained by a professional motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.", "sections": [{"title": "1. Introduction", "content": "We use our hands to communicate, interact with objects, or utilize objects as tools to act upon our environment. The dexterity with which we can manipulate objects is unmatched by other species and has been a key factor in our evolution [2]. Hand-object interaction has therefore naturally received considerable attention from various research fields, including computer vision [48].\nA vision-based system for automatic understanding of hand-object interaction, which would be able to capture information about 3D motion, shape and contact of hands and objects, would be useful for a wide range of applications. For instance, such a system could enable transferring manual skills between users by first capturing expert users performing a sequence of hand-object interactions (while assembling a piece of furniture, doing a tennis serve, etc.), and later using the captured information to guide less experienced users, e.g., via AR glasses. The skills could be similarly transferred from humans to robots. Such a system could also help AI assistants better understand the context of a user's actions, or enable new input capabilities for AR/VR users. For example, it could turn any physical surface into a virtual keyboard or transform any pencil into a multi-functional magic wand. However, the accuracy and speed of existing methods for understanding hand-object interaction are not sufficient to reliably support such applications."}, {"title": "2. Related work", "content": "The progress of research in computer vision has been strongly influenced by benchmark datasets [14, 19,32,57,58] which enable comparing of methods and understanding of their limitations. In this section, we first review existing datasets with either hand or object pose annotations, and then focus on datasets that offer annotations of hands and hand-manipulated objects.\nDatasets with hands only. Vision-based 3D hand pose estimation and tracking has been extensively studied for many years, with the first methods focusing on custom datasets with monochrome images [29,54]. Significant improvements in pose accuracy were later achieved on RGB-D images from datasets such as NYU [66], ICVL [65], MSRA [63], Tzionas et al. [67], EgoDexter [44], or HANDS17 [73]. Recently, partly motivated by AR/VR use cases where depth sensors are often unavailable due to high power consumption, the research community has largely switched to RGB or monochrome images, working on datasets such as the Stereo dataset [74], InterHand2.6M [43], FreiHAND [76], UmeTrack [26], AssemblyHands [47], and datasets with pose annotations of both hands and objects reviewed below.\nDatasets with objects only. Research on 6DoF object pose estimation and tracking has followed a similar path, starting off with custom monochrome datasets [45,55] and later largely switching to RGB-D datasets such as LM [30], YCB-V [71], and T-LESS [31], which are included in the BOP benchmark [32-34, 64]. The benchmark currently includes twelve datasets in a unified format, offering 3D object models and training and test RGB-D images annotated with 6DoF object poses. The 3D object models are created manually or using KinectFusion-like systems [46] for 3D surface reconstruction. The training images are real or synthetic (photo-realistically rendered with BlenderProc [11,12]) and all test images are real. Besides these instance-level datasets, the community also uses category-level RGB-D datasets such as Wild6D [17], HouseCat6D [36], and PhoCal [70]. Recent methods started to focus again on estimating object pose from RGB-only images, using datasets such as OnePose [62] and HANDAL [21].\nDatasets with hands and objects. Many existing datasets include images of hands and objects (e.g., [1, 6, 10, 16, 60, 75]), but only provide annotations in the form of 2D bounding boxes, segmentation masks, or action labels. Some datasets for 3D hand pose estimation (e.g., [44, 47, 76]) include images of hands interacting with objects, but do not provide 6DoF object pose annotations.\nThe first dataset with ground-truth poses of both hands and objects was created by Sridhar et al. [61] and offers 3014 exocentric RGB-D images of a hand manipulating a cube, manually annotated with fingertip positions and 6DoF poses of the cube. To avoid the manual annotation, which is tedious and not scalable, the FHPA dataset [18] used magnetic sensors attached to one hand and objects, noticeably affecting their appearance. This dataset includes 105K egocentric RGB-D images with ground-truth poses of a single hand and 4 objects. The ObMan dataset [27] resorted to synthesizing images of hands grasping objects, with the grasps generated by an algorithm from robotics.\nHO-3D [23] was the first dataset with real images annotated by an optimization procedure that leverages multi-view RGB-D image streams and is almost fully automatic. The dataset offers 78K images from several exocentric cameras, showing 10 subjects and 10 objects. A similar annotation procedure was used for several subsequent datasets [3, 8, 24, 38, 39]. H2O [38] includes 572K egocentric multi-view RGB-D images of 4 subjects manipulating 8 objects. H2O-3D [24] provides 75K exocentric RGB images of 6 subjects manipulating 10 YCB objects [7].\nDexYCB [8] consists of 1000 clips of 3 seconds with the total of 582K RGB-D images, recorded from 8 exocentric views and showing 10 subjects picking up 20 YCB objects with near-static grasps. HOI4D [39] includes 2.4M egocentric RGB-D images from over 4000 video sequences showing 9 subjects interacting with 800 different objects from 16 categories in 610 different indoor environments. Besides rigid objects, this dataset contains articulated objects, but focuses on simpler scenarios with a single hand and a single object, and only includes single-view video sequences. An RGB-D optimization procedure was also used in ContactPose [5] along with the information from thermal cameras for accurately annotating hand poses, while the object poses were annotated using optical markers. ContactPose includes 2.9M RGB-D images of 50 subjects grasping 25 household objects, however, the grasps are static, background green and all objects are blue (3D printed), which makes the images less realistic.\nSimilar to HOT3D, ground-truth poses of hands and objects in the recent ARCTIC dataset [15] were collected with a marker-based motion-capture system. This dataset includes 2.1M RGB images showing 10 subjects interacting with 11 articulated objects. The images were captured at 233K timestamps from 9 views, only one of which is egocentric (recorded with a mock-up of an egocentric device a camera mounted on a helmet)."}, {"title": "3. HOT3D dataset", "content": "833 minutes of recordings. The HOT3D dataset includes egocentric, multi-view, synchronized data streams recorded with Project Aria [13] and Quest 3 [41]. Image streams contain 1.5M multi-view frames consisting of 3.7M images. Each frame from Aria consists of one RGB 1408\u00d71408 image and two monochrome 640\u00d7480 images. Each frame from Quest 3 consists of two monochrome 1280\u00d71024 images. Intrinsic camera parameters and camera-to-world transformations are available for all images. All streams were recorded at 30 fps. Every recording from Aria also includes a 3D point cloud of the scene (from SLAM) and per-frame eye gaze information. See supplement for more details about Aria and Quest 3.\n3D mesh models of 33 objects. The models were obtained using an in-house scanning-based 3D object reconstruction pipeline, which provides high-resolution geometry and PBR [40] materials. These materials include metallic, roughness, and normal maps and enable rendering of photo-realistic training images [33, 35]. The object collection includes household and office objects of diverse appearance, size, and affordances (Fig. 3).\n19 diverse subjects. To ensure diversity, we recruited 19 par- ticipants with different hand appearances and shapes. Hands of each participant were scanned by a custom 3D hand scanner and are provided in the UmeTrack [26] and MANO [56] formats.\n4 everyday scenarios. Besides a simple inspection scenario, where subjects pick up, observe, and put down objects, subjects were asked to perform typical actions in a kitchen, office, and living room. All scenarios were captured in the same lab equipped with scenario-specific furniture. In each recording, subjects were asked to interact with up to 6 objects. To enhance diversity within the dataset, we regularly randomized various aspects such as lighting, furniture placement, and decorative elements. The resulting dataset consists of 425 recordings, with 199 captured using Aria and 226 using Quest 3. Each recording is around 2 minutes long.\nGround-truth annotations. Recordings are annotated with per- frame ground-truth poses of hands and objects obtained in a motion-capture lab shown in Fig. 5 and described in the supple- ment. Object and wrist poses are represented as 3D rigid trans-"}, {"title": "4. Experiments", "content": "Wearable headsets often feature multiple cameras, which natu- rally lends itself to the development of multi-view methods for 3D tasks. In this section, we demonstrate that multi-view methods out- perform single-view methods for several popular egocentric tasks. First, we compare the single-view and multi-view versions of the UmeTrack [26] method for 3D hand tracking. Second, we extend the FoundPose [50] method for 6DoF pose estimation of unseen objects to multiple views and evaluate against the original version. Third, we develop a method for 3D lifting of unknown in-hand objects by stereo matching of DINOv2 [49] features, which we compare against a single-view method similar to OSNOM [51]."}, {"title": "4.1. 3D hand pose tracking", "content": "Experimental setup. The task is to estimate 3D locations of hand joints in every frame of given test sequences, with the ground-truth hand skeletons and 2D bounding boxes of visible hands provided. We train the UmeTrack [26] hand tracker on three variants of training data: (1) training sequences from the UmeTrack dataset, (2) HOT3D training sequences recorded with Quest 3, and (3) the combination of the two. The UmeTrack dataset was recorded with the Quest 2 headset, which has the same but differently arranged cameras compared to Quest 3, and includes 1397 real and 1397 synthetic sequences, each recorded at 30 fps for 15 seconds. The sequences depict single-hand motions and hand-hand interactions performed by 53 participants, but do not include any hand-object interactions. All three UmeTrack models were trained on two-view image streams with one of the views randomly masked out (as in [26]). The masking increases the tracking robustness and encourages the models not to rely on both views, which enables a fair comparison of their single- and two-view modes. We evaluate the models on all frames of test UmeTrack sequences and all frames of test HOT3D clips from Quest 3. The accuracy of the predicted 3D locations is measured by the Mean Keypoint Position Error (MPKE) [26]."}, {"title": "4.2. 6DoF pose estimation of unseen objects", "content": "Experimental setup. In this experiment we focus on the task of model-based 6DoF pose estimation (i.e., 3D translation and 3D rotation) of unseen objects, which are learned during a short onboarding stage just from their CAD models [34]. We evaluate the original FoundPose [50], a recent open-source method that achieves the state-of-the-art results in refinement-free pose estimation from a single image, and its extension to multi-view input that we propose below. We evaluate coarse versions of these methods (without any pose refinement) on every 30th frame of the test HOT3D clips from both Aria and Quest 3. The methods are-"}, {"title": "4.3. 2D segmentation of in-hand objects", "content": "Experimental setup. Given an input image, the task is to predict a binary 2D mask of objects manipulated by hands. Besides serving as a prerequisite for 3D lifting of in-hand objects (Sec. 4.4), this task is useful for downstream applications such as hand state classification or video activity recognition [75]. We evaluate three methods, including the off-the-shelf EgoHOS [75] and two variants of Mask R-CNN [28], trained on a proprietary dataset of 400K RGB Aria images annotated with 52K masks of in-hand ob- jects. We trained one model directly on the RGB image channels (denoted as MRCNN in Tab. 4), and one on the depth channel predicted by Depth Anything V2 [72] (denoted as MRCNN-DA). We evaluate these methods on every 30th frame of the training and test HOT3D clips from both Aria and Quest3 (~19K frames).\nObjects are considered to be in-hand if the minimum distance between object and hand mesh vertices in their ground-truth poses is below a threshold of 1 cm and the object is moving with a velocity larger than 1 cm/s. Masks of such objects are used as the ground truth for this task. The accuracy of predicted masks is measured by mean Intersection over Union (mIoU), as in [75]."}, {"title": "4.4. 3D lifting of in-hand objects", "content": "Experimental setup. Next we focus on the task of 3D lifting of unknown in-hand objects, which is useful for object indexing and long-term tracking [51]. Given per-view 2D segmentation masks of an in-hand object, the goal is to estimate the 3D object location in the camera coordinate frame. We developed and compare the performance of three methods described below. The accuracy"}, {"title": "5. Conclusion", "content": "We have introduced HOT3D, a large-scale, publicly available dataset designed to support training and evaluation of methods for various 2D and 3D egocentric tasks related to hand-object in- teraction. Our experiments show that multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts across several popular tasks. Besides multi-view 3D object tracking, we addressed the tasks of multi-view 6DoF pose estimation of unseen objects and 3D lifting of in-hand objects, for which we developed strong baselines. By publicly releasing the HOT3D dataset along with source code of the baseline methods, and by co-organizing public challenges 12 on the dataset, we hope to accelerate research on egocentric vision and contextual AI."}, {"title": "A. Aria glasses", "content": "Project Aria [13] is an egocentric recording device in glasses form-factor created by Meta. It is designed as a research tool for egocentric machine perception and contextualized AI research, and available to researchers across the world via projectaria.com."}, {"title": "A.1. Device and sensors", "content": "Project Aria is built to emulate future AR- or smart-glasses catering to machine perception and egocentric AI rather than human consumption. It is designed to be wearable for long periods of time without obstructing or impeding the wearer, allowing for natural motion even when performing highly dynamic activities \u2013 such as playing soccer or dancing. It has a total weight of 75g (compared to over 150g for a single GoPro camera), and fits just like a pair of glasses.\nFurther, the device integrates a rich sensor suite that is tightly calibrated and time-synchronized, capturing a broad range of modalities. For HOT3D, recording profile 15 is used, which uses the following sensor configuration (all streams come with meta- data such as precise timestamps and per-frame exposure times):\n\u2022 One rolling-shutter RGB camera recording at 30 fps and 1408 \u00d7 1408 resolution. It is fitted with an F-Theta fisheye lens that covers a field of view of 110\u00b0.\n\u2022 Two global-shutter monochrome cameras recording at 30 fps and 640 \u00d7 480 resolution. They provide additional peripheral vision, and are fitted with F-Theta fisheye lenses that cover a field of view of 150\u00b0.\n\u2022 Two monochrome eye-tracking cameras recording at 10 fps and 320 x 240 resolution.\n\u2022 Two IMUS (800 Hz and 1000 Hz respectively), a barometer (50 fps) and a magnetometer (10 fps).\n\u2022 GNSS and WiFi scanning was disabled for HOT3D.\n\u2022 Audio recording was disabled for HOT3D for privacy reasons."}, {"title": "A.2. Machine Perception Services (MPS)", "content": "Project Aria's machine perception service (MPS) provides software building blocks that simplify leveraging the different modalities recorded. These functionalities are likely to be available as real-time, on-device capabilities in future AR- or smart-glasses. We use the following core functionalities currently offered by Project Aria, and include their raw output as part of the dataset. See [13] and the technical documentation\u00b3 for more details.\nCalibration. All sensors are intrinsically and extrinsically cal- ibrated, and tiny deformations due to temperature changes or stress applied to the glasses frame are further corrected by time-varying online calibration from MPS.\nAria 6 DoF localization. Every recording is localized precisely and robustly in a common, metric, gravity-aligned coordinate frame, using a state-of-the-art VIO and SLAM algorithm. This provides millimeter-accurate 6DoF poses for every captured frame and high-frequency (1 kHz) motion in-between frames.\nEye gaze. The gaze direction of the user is estimated as two outward-facing rays anchored approximately at the wearer's eyes, allowing to approximately estimate not only the direction the user is looking in, but also the depth their eyes are focused on. We use an optional eye gaze calibration procedure, where the mobile companion app directs the wearer to gaze at a pattern"}, {"title": "A.3. Processing summary", "content": "All Aria recordings are anonymized in a very first step, using the public EgoBlur [53] model and following Project Aria's responsible innovation principles.\nThen, the MPS pipeline is invoked for each full Aria recording, which are typically about 2 minutes long and include many instances of hand-object interactions with different objects. Next, we 7DoF-align the MPS output with the OptiTrack coordinate frame (App. C). In total, we have processed 199 Aria recordings with a total length of 391 minutes."}, {"title": "A.4. Tools and ecosystem", "content": "Documentation and open-source tooling for Aria recordings and MPS output is available on GitHub and includes Python and"}, {"title": "B. Quest 3 headset", "content": "Quest 3 [41], shown in Fig. 13, is the latest production headset from Meta for virtual- and mixed-reality experiences. For the HOT3D data collection we used a developer version of the Quest 3 headset. This version has four global-shutter monochrome cam- eras with fisheye lenses, 1280x1024 px image resolution, 18 PPD (Pixels Per Degree), and records at 30 fps. Two of the cameras are on the front side of the headset, roughly aligned with eyes, and two on the sides. HOT3D only includes images from the two front cameras as those capture the relevant scene part (the two side cam- eras are useful for applications like SLAM). Example images are in Fig. 14. Data from other sensors present in the consumer ver- sion of Quest 3, including a gyroscope and an accelerometer, were not recorded. The intrinsic and extrinsic parameters of the headset cameras were calibrated with a ChArUco board. Both the headset and the board were attached a set of optical markers and tracked by the motion-capture system described in App. C, which allowed to estimate camera-to-headset transformations. At recording time, the headset pose was still tracked by the motion-capture system and used to calculate per-frame camera-to-world transformations."}, {"title": "C. Marker-based motion capture", "content": "The poses of hands and objects were tracked using optical markers attached on their surface. For both hands and objects we used 3 mm markers with an adhesive layer at their bottom. Such markers are small enough not to influence hand-object interactions. Each hand was attached 19 markers and each object around 10."}, {"title": "D. Object orientation statistics", "content": "When recording HOT3D, we asked subjects to naturally interact with the objects. Consequently, orientation distributions of the observed objects (Fig. 15) reveal clear object-specific pose biases, which may be useful as prior information at inference time (we see that the bowl tends to be seen upright, the birdhouse from the front and upright, etc.)."}, {"title": "E. Additional quantitative results", "content": "The results of 2D segmentation and 3D lifting of in-hand objects presented in Tables 4 and 5 were obtained by evaluating methods on clips from both training and test splits. To allow the community to compare their results against our results on these two tasks, we additionally provide results obtained on clips from the training split for which the ground-truth annotations are publicly available (Tables 6 and 7). Evaluating on the training split is possible as both of these tasks are training-free and therefore do not require any training split."}]}