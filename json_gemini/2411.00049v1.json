{"title": "Rule by Rule: Learning with Confidence through Vocabulary Expansion", "authors": ["Albert N\u00f6ssig", "Tobias Hell", "Georg Moser"], "abstract": "In this paper, we present an innovative iterative approach to rule learning specifically designed for (but not limited to) text-based data. Our method focuses on progressively expanding the vocabulary utilized in each iteration resulting in a significant reduction of memory consumption. Moreover, we introduce a Value of Confidence as an indicator of the reliability of the generated rules. By leveraging the Value of Confidence, our approach ensures that only the most robust and trustworthy rules are retained, thereby improving the overall quality of the rule learning process. We demonstrate the effectiveness of our method through extensive experiments on various textual as well as non-textual datasets including a use case of significant interest to insurance industries, showcasing its potential for real-world applications.", "sections": [{"title": "1 Introduction", "content": "In recent years, the rapid advancement of Artificial Intelligence (AI) technologies has revolutionized various industries and aspects of our daily lives (cf. Lu [2019], Zhang and Lu [2021], Lee [2020], for instance). However, as AI systems become more complex and sophisticated, the need for transparency and interpretability in their decision-making processes has become increasingly crucial. The concept of Explainable Artificial Intelligence (XAI; see for example Angelov et al. [2021], Ali et al. [2023]) has emerged as a response to this demand, aiming to enhance the trust, accountability and understanding of AI systems by providing explanations for their outputs and actions.\nIndeed, in many application areas of machine learning, like automotive, medicine, health and insurance industries, etc., the need for security and transparency of the applied methods is not only preferred but increasingly often of utmost importance or even required by law (cf. EU Artificial Intelligence Act for instance). A classical example in this context - often categorized as most informative in the area of XAI (Hulsen [2023]) \u2013 is the generation of deterministic (if-then-else) rules that can be used for classification. For instance, regarding the prediction"}, {"title": "2 Notations & Preliminaries", "content": "After motivating the basic idea behind the approach introduced in this paper, we give a more comprehensive summary of rule learning in general as well as the work we have already done in this field in this section."}, {"title": "2.1 Rule Learning", "content": "As already mentioned in the introduction, the field of Rule Induction focuses especially on providing efficient algorithms for the generation of simple if-then-else rules as we are mainly interested in. Repeated Incremental Pruning to Produce Error Reduction (RIPPER; Cohen [1995]) is state-of-the-art in this field and, consequently, we consider mainly this algorithm in our experiments.\nHowever, there are also other fields like Inductive Logic Programming (ILP; cf. Cropper and Duman\u010di\u0107 [2022]) that encompass methods yielding results that can be interpreted as if-then-else rules. Basically, ILP-tools investigate the construction of first- or higher-order logic programs. In the context of this paper, it suffices to conceive the learnt hypothesis as first-order Prolog clauses as depicted below.\nH:- L1, ..., Lm\nHere, the head H is an atom and the body L1,..., Lm consists of literals, that is atoms or negated atoms.\nConsequently, ILP is often conceived as a subfield of inductive programming. However, our interest stems from the fact that logic programs are (by definition) nothing else but sets of clauses, that is, rules.\nConcerning ILP, especially one of the first tools from this field, the FOIL algorithm (First Order Inductive Learner; Quinlan [1990]), is of main interest to us due to its simplicity. In previous work (N\u00f6ssig et al. [2024]), we have extensively investigated also some more modern ILP-tools and in the course of this we have shown that they are mostly not suited for our needs since they are rather designed to generalize from a very small set of input examples. Nevertheless, our Python reimplementation of the FOIL algorithm presented in our previous work is able to handle large data sets straight away which is in particular beneficial for the data set considered in our case study. Moreover, contrarily to more modern"}, {"title": "2.2 Modular Approach", "content": "The first problem we have faced regarding the application of rule learning methods in our reimbursement use case has been the (nearly) infeasible complexity caused by the vast amount of examples contained in the corresponding data set. As extensively discussed in the corresponding paper (N\u00f6ssig et al. [2024]), both the time as well as the memory consumption increase drastically with increasing number and length (i.e., number of attributes) of input examples. In order to solve this problem we introduced a modular approach that is basically composed of three independent phases. The core idea is to make the approach as versatile as possible by allowing to apply a huge variety of methods within each step depending on the kind of input data considered.\nFirst, an appropriate feature extraction or dimensionality reduction method such as a neural network, UMAP (McInnes et al. [2020]) or a principal component analysis is applied with the goal to find a compact representation of the high-dimensional input data. This representation should be beneficial for clustering applied in the second step, where a chosen method like k-means or DBSCAN (Ester et al. [1996]), for instance, divides the whole set of input data"}, {"title": "2.3 Voting Approach", "content": "As a remedy for the issue of generally less accurate results achieved by explainable methods, we decided to apply an ensemble of classification models consisting of explainable as well as unexplainable methods in a novel kind of voting approach.  The principal idea is to directly build upon the modular approach outlined in Section 2.2 and make use of the generated rule sets produced therein. We use especially FOIL and RIPPER as representative examples since these two algorithms have been mainly used in the predecessor paper but basically they can be replaced by any method yielding if-then-else rules (or something similar that can be transformed into such rules).\nIn the first step our ensemble of classification models only contains the two explainable methods and we check whether the applied models predict the same class or not. In case the predictions coincide, we directly output the corresponding label corroborated by one rule from each method. In case of different predictions, we additionally incorporate the state-of-the-art prediction from an unexplainable method. Simply put, this method - the so-called decider - tells us which rule learner is right and we use the according prediction as final classification again confirmed by the rule from the corresponding explainable method."}, {"title": "3 Related Work", "content": "After motivating the basic idea behind the approach introduced in this paper and introducing the concepts applied therein, in this Section we discuss related work that also focuses especially on the (explainable) classification of textual data as well as novel ideas in the context of rule learning in general.\nRegarding text classification in general there is a huge number of methods out there dealing with this problem. Some surveys summarizing the most common (explainable as well as unexplainable) approaches have been done in recent years for instance by Kowsari et al. [2019], Minaee et al. [2021], Li et al. [2022], Gasparetto et al. [2022]. Moreover, Mendez Guzman et al. [2024] have recently published a survey comparing different rationalisation approaches in the context of explainable text classification. Furthermore, Altinel and Ganiz [2018] give an overview of common semantic text classification methods and discuss the benefits of these methods over traditional text classification approaches.\nA more specific method utilizing similar ideas as we apply in our approach is proposed by Johnson et al. [2002] who introduce a tool kit for text categorization called KitCat and not only focus on the explainable classification of textual data but also make use of a confidence measure for dealing with ambiguities similar to our Value of Confidence introduced in Section 4. For evaluation, they consider in particular the Reuters-21578 data set where they report a micro-averaged precision/recall of 83.8%. As opposed to their idea of deriving symbolic rules from decision trees that have been optimized to handle in particular sparse data, we directly obtain rules from classical rule learning methods focusing especially on the complexity of the generated rules with respect to the underlying dictionary in order to improve the versatility of the classical methods. Note that we cannot really compare the achieved results, since we used a different data split. However, on NLTK's Reuters corpus we report an accuracy of about 80.5% and 81.7% on RIPPER and FOIL, respectively.\nThe Reuters-21578 data set is a common benchmark for the evaluation of various classification methods on text-based input data and has been intensively"}, {"title": "4 Methodology", "content": "After motivating the ideas behind this paper and summarizing related work as well as previous work on which this paper is build upon, we will introduce the applied methodology in this section. Simply put, our iterative approach is based on a chosen rule learning method and aims to refine the generated rules according to a chosen Value of Confidence that we define as follows."}, {"title": "4.1 Value of Confidence", "content": "Definition 1. The Value of Confidence is a measure of reliability of a rule generated by a rule learning method. This numeric value is calculated on a validation data set distinct from the training set that is used to generate the rule. There are various possible calculation methods depending on the exact goal of the use case of interest. However, a common metric applied in this context might be"}, {"title": "4.2 Iterative Approach", "content": "The basic procedure of the iterative approach introduced in this paper is illustrated by the pseudo-code in Algorithm 1 and explained in the following."}, {"title": "5 Experimental Evaluation", "content": "In this section, we evaluate the iterative approach introduced in this paper on several common benchmark data sets not only from the field of text classification but also on non-textual data showing its versatile applicability. Moreover, we investigate a practical example from insurance industries."}, {"title": "5.1 Experimental Setup", "content": "As a first step, the data sets explained in the following are split into train, validation and test data. When not stated differently, we use 80% of the input data as training data and the remaining 20% for testing. From the training data we use 15% as validation data set for the application of our iterative approach. This additional split is not necessary when we use the ordinary method. So, the corresponding outcomes presented in the comparison in Section 5.2 are obtained by considering the whole training data (i.e., 80% of the total input data) without generating a separate validation set. Note that at this point preprocessing has already been done. So, in particular for the considered text-based data sets, the textual information has already been transformed into binary vectors where the attributes are ordered according to the inverse document frequency as already mentioned above.\nBefore starting with our approach, we define a start dictionary size which is usually an eighth of the total number of attributes as explained above. Regarding the maximal number of iterations and the applied threshold for the Value of Confidence, we always apply the same settings; namely at most 5 iterations with a threshold of 0.9. However, note again that we add the rule resulting from the last iteration to our set of rules independent of the corresponding Value of Confidence. So, in the final ruleset there might be rules with an unsatisfying reliability but we can ignore them during evaluation. In fact, we are interested in the differences that can be observed by applying only rules with a certain reliability as further shown in Section 5.4.\nAfter that, we can define the rule learning method we want to apply as well as the number of rules that should be generated and our iterative approach proceeds as explained in Section 4."}, {"title": "5.2 Objectives & Summary", "content": "The empirical evaluation of the iterative approach introduced in this paper in particular sought to answer the following questions.\nRQ1 Accuracy compared to the base method. Can the iterative approach provide better accuracy of classification prediction than the base method, i.e. the ordinary rule learning method."}, {"title": "5.3 Use Case: Reimbursement", "content": "The Allianz Private Krankenversicherung (APKV) is an insurance company offering health insurance services in Germany. As already mentioned, the inspiration for this work stems from a use case we worked on during a collaboration with this company. In our previous work (N\u00f6ssig et al. [2024], N\u00f6ssig et al. [2024]), we have already described the use case at hand in detail. However, summed up, an insurance company regularly receives bills handed in by the clients asking for reimbursement. Automated processing of these bills is desired in order to lower costs and to gain an edge over the competition by reducing the time until the client receives the reimbursed money.\nAs decision making, in particular in this sensitive area, should be transparent to both parties, the operational use of black-box machine learning algorithms is often seen critically by the stakeholders and is in many cases avoided. As a consequence, rule learning achieving a comparable performance offers the desired advantage of explainability.\nFor our case study, we are focusing on dental bills. On those bills, the specific type of dental service per row on the bill is unknown but needed for deciding on the amount of refund. Especially differentiating between material costs and other costs is of crucial importance.\nIn collaboration with the APKV, we have been provided with an anonymized training data set consisting of nearly one million instances. As opposed to our previous work, where we only considered structured information on the bills such as cost, date and simple engineered features, in this paper we especially aim to work with the textual data and make predictions based on the occurrence of certain words or word groups where we had to restrict to the 8000 most common ones using FOIL and the 3000 most common ones for RIPPER due to the extensive memory consumption."}, {"title": "5.4 Detailed Analysis", "content": "As a part of this paper, we have introduced a Value of Confidence that can be used as a metric of reliability of a generated rule. This section aims to investigate the influence of this value on the precision achieved during evaluation (cf. RQ4).\nFor this purpose, we apply thresholds t from 0.6 to 0.9 and consider only rules with a VoC > t. The corresponding results are shown in Table 4 as well as Figure 7 and 8. In this context, we only consider fully satisfied rules and do not apply partial matching as also done and explained in Section 5.3.\nIn order to answer question RQ4, we again illustrate for each of the considered textual benchmark data sets the number of examples where a prediction has been made (i.e., one rule is completely satisfied) together with the percentage of correctly classified examples. As expected, the number of classified examples decreases with increasing threshold and the associated reduction of total rules. However, as desired, the remaining rules are obviously more reliable and the percentage of correctly predicted examples steadily increases for both FOIL and RIPPER on each of the considered benchmarks. So, all in all, the incorporation of a Value of Confidence definitely has a positive impact on the precision of the made predictions."}, {"title": "6 Conclusion & Future Work", "content": "In this paper we present an extension to classical rule learning methods making use of a Value of Confidence as metric of reliability. This novel approach is especially suited for the application of rule learners on textual input data but the iterative approach is not only beneficial for gaining more control over the applied dictionary but has shown to be also advantageous for nominal data by optimizing the reliability of the generated rules in each iteration.\nBy combining the approach introduced in this paper with the two approaches to rule learning we already introduced in our previous work (cf. N\u00f6ssig et al. [2024], N\u00f6ssig et al. [2024]) we obtain a framework for explainable classifications that can be applied in various scenarios handling different types of data in a production environment.\nConcerning future work, we aim to integrate a more sophisticated preprocessing applying for instance large language models to improve the choice of the dictionary. In the course of this, we will also investigate different ways of sorting the basic dictionary with the goal to find the best possible starting dictionary used in the first iteration of our approach. Moreover, using computer vision approaches in order to incorporate the position of words in a document might be another interesting consideration we aim to investigate in future work because especially in our main use case concerning reimbursement, the considered bills are mostly standardized and the crucial information is usually located in a certain area in the document."}]}