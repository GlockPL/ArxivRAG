{"title": "Human-Agent Coordination in Games under Incomplete Information via Multi-Step Intent", "authors": ["Shenghui Chen", "Ruihan Zhao", "Sandeep Chinchali", "Ufuk Topcu"], "abstract": "Strategic coordination between autonomous agents and human partners under incomplete information can be modeled as turn-based cooperative games. We extend a turn-based game under incomplete information, the shared-control game, to allow players to take multiple actions per turn rather than a single action. The extension enables the use of multi-step intent, which we hypothesize will improve performance in long-horizon tasks. To synthesize cooperative policies for the agent in this extended game, we propose an approach featuring a memory module for a running probabilistic belief of the environment dynamics and an online planning algorithm called INTENTMCTS. This algorithm strategically selects the next action by leveraging any communicated multi-step intent via reward augmentation while considering the current belief. Agent-to-agent simulations in the Gnomes at Night testbed demonstrate that INTENTMCTS requires fewer steps and control switches than baseline methods. A human-agent user study corroborates these findings, showing an 18.52% higher success rate compared to the heuristic baseline and a 5.56% improvement over the single-step prior work. Participants also report lower cognitive load, frustration, and higher satisfaction with the INTENTMCTS agent partner.", "sections": [{"title": "1 INTRODUCTION", "content": "Developing autonomous agents that can strategically coordinate with human partners under incomplete information, especially in long-horizon tasks, is important in the field of human-agent interaction. These agents hold the potential to improve interaction across various contexts, ranging from virtual applications like strategy board games [2] and action video games [7] to physical domains like assistive technologies such as wheelchairs [13].\nA key challenge in achieving human-agent coordination is multi-step planning and strategizing. Humans naturally rely on multi-step planning [20] and frequently engage in multi-step intent communication, essential for deliberate coordination in long-horizon tasks. Therefore, for autonomous agents to be effective collaborators, they should also understand and perform multi-step reasoning.\nAn existing model for human-agent cooperation under incomplete information, the shared-control game introduced in [8], studies scenarios where each player has access only to its own transition function but not its partner's. However, this model is limited to a single action per turn, preventing the realization of more intricate, multi-step strategies. Although recent work in [9] partially mitigates this issue by allowing one of the players to take multiple actions in its turn, a full extension that supports multi-action dynamics for both players remains unexplored.\nWe extend the shared-control game by allowing both players to take multiple actions per turn, capturing more fluid interactions and bringing the model closer to natural human-agent coordination. This extension enables the agent to leverage multi-step intent, which was previously unattainable due to the limitation of single-action turns. In this work, we define a multi-step intent as a variable-length sequence of states a player wishes its partner would visit next. The goal is to develop an agent, as the ego player in this game, that infers partner dynamics through their transition histories and coordinates effectively by exchanging multi-step intents.\nThe proposed approach, inspired by the Belief-Desire-Intention model [25], consists of a memory module and a planning module. The memory module enables the agent to continuously update its belief about unknown game dynamics by integrating new information inferred from the partner's histories. The planning module allows the agent to devise optimal actions by considering its current belief, prior knowledge, and any communicated multi-step intent. This approach exemplifies zero-shot online planning, requiring no prior training or data, yet it ensures adaptability to accumulating environment information and strategic interaction with the partner."}, {"title": "2 INTENT-AWARE COORDINATION PROBLEM", "content": "We extend the shared-control game introduced in [8] by allowing both players to take multiple actions per turn. We formally define the game as follows:\nDefinition 1. A multi-action shared-control game is played between player E and player H, defined by a tuple (X, S, A, TE, TH, R) where:\n\u2022 X is a finite set of environment states, with the initial state xinit and goal state xgoal known to both players.\n\u2022 S = X \u00d7 {E, H} is the controller state space, consisting of tuples (x, c) where x \u2208 X is the environment state and c \u2208 {E, H} indicates which player is currently in control. The notation \u2212c denotes the other player without control.\n\u2022 A is a finite set of actions available to both players including an action for switching control to the other player.\n\u2022 Ti: S \u00d7 A \u2192 S is the deterministic transition function for player i \u2208 {E, H}.\n\u2022 Renv: S\u2192 R is a function that assigns real-valued environment rewards to both players.\nAt each timestep t, when player c is in control, we define the history of the other player \u2212c, denoted as $h_{\\bar{c}}^{t} \\in (X \\times A)^{+}$, as the sequence of state-action pairs taken by the other player up to timestep t. We assume that at timestep t, player i can communicate a multi-step intent, $\u03b6_i \\in X^+$, representing a variable-length sequence of states that the player wishes the other player to visit next.\nIn this game, we aim to compute cooperative policies for the autonomous agent, E, when partnering with a human player, H. We formalize this problem as follows:\nProblem 1. Given a human player whose behavior is denoted as $\u03c0_H$, the task is to compute the ego player's policy $\u03c0_E$ that maximizes the expected total discounted reward:\n$\\max\\limits_{\\Pi_{E}} E\\begin{bmatrix}\\sum\\limits_{t=0}^{\\infty} \\gamma^t R_{env}(S_{t+1})\\end{bmatrix}$\n(1a)\ns.t. $S_{t+1} = T_{c_t}(S_t, a_t)$\n(1b)\n$a_t \\sim \\pi(s_t, h_{\\bar{c}}^t, \\xi_t)$,\n(1c)\nwhere $s_0 = s_{init}$ and $\\xi = IntentModel(s_t, h_{\\bar{c}}^t)$."}, {"title": "3 RELATED WORKS", "content": "Intents for better coordination. The use of intent for improving coordination between agents can generally be categorized into two approaches: implicit inference or explicit communication. Intent inference, often explored through theory of mind [5, 21, 31] and opponent modeling [15, 30], allows agents to predict the goals and actions of others by modeling their beliefs, desires, and intentions. These methods rely on the agent's ability to infer hidden states or strategies of others. However, inference-based methods suffer from information loss due to incomplete or noisy observations of others' actions, often leading to inaccurate intent models and coordination inefficiencies. Intent communication, on the other hand, involves agents sharing explicit messages, such as future trajectories, to align their actions. Recent work has explored imagined trajectories in the centralized training, decentralized execution framework [17], where agents share plans during training to improve coordination. Our method, however, focuses on zero-shot, real-time planning, where agents must coordinate without prior training. Our approach also differs by generating intent as a request for what the sender player desires the other to do, rather than predicting what the other will do. Despite progress in intent communication in multi-agent reinforcement learning, few approaches ensure that the communicated intent is semantically meaningful with respect to the task at hand, which is essential for effective human-agent coordination. Our approach, inspired by the Belief-Desire-Intention model [3, 25], continuously updates the agent's beliefs based on new observations made in the environment and models intents in a multi-step fashion. Building on recent work that uses single-step intents with non-probabilistic belief updates [8], our work allows multi-step intents and probabilistic belief modeling, supporting more dynamic and adaptive coordination in tasks over a longer horizon.\nMCTS-based planning techniques. Monte Carlo tree search (MCTS) is an algorithm that combines tree-based search with Monte Carlo random sampling to explore and evaluate vast decision spaces efficiently [4] and has proven effective in strategic games [6, 26\u201328], making it ideal as the foundation of our planning module. Standard MCTS works best for turn-based games with perfect information. Several extensions have been developed to handle incomplete information. For example, partially observable Monte Carlo planning is designed to tackle environments modeled as partially observable Markov decision processes using particle filtering for state estimation [29]. Information-set MCTS tackles imperfect information games by maintaining perspective-based trees for each player, whose nodes correspond to players' information sets and edges correspond to moves from that player's viewpoint [10]. However, these methods only handle partial observations of states, not transitions as we do in our problem. To extend beyond traditional turn-based games, previous research [1, 16, 23] has introduced methods for handling multi-action games. As summarized by [12], tree search in these games can either create nodes for each individual action or for the entire action sequence within a turn. In this paper, we adopt the former approach for the benefit of a lower branching factor."}, {"title": "4 METHOD", "content": "This section presents an approach for an autonomous agent, acting as the ego player, to solve Problem 1. The agent's decision-making process is compartmentalized into a memory module (see Section 4.1) and a planning module (see Section 4.2)."}, {"title": "4.1 Memory Module: Probabilistic Belief over Unknown Dynamics", "content": "In Problem 1, the agent player E must coordinate effectively without direct knowledge of its human partner's dynamics, $T_H$. The memory module maintains a probabilistic belief over these unknown dynamics, updating it throughout the game via Bayesian inference based on the partner's transition history. We hypothesize that the memory module will improve coordination performance as the game progresses.\nIn the game defined in Definition 1 with discrete state space X and action space A, the presence of each deterministic state-action transition is modeled as a binary random variable $y \\in \\{0, 1\\}$. The belief over these transitions is represented by a Bernoulli distribution b: $X \\times A \\rightarrow [0, 1]$, indicating the likelihood of a transition's existence. The probability mass function f is given by:\n$f(y|\\theta) = \\theta^{y} (1 - \\theta)^{1-y}, \\theta = b(x, a)$,\n(2)"}, {"title": "4.2 Planning Module: Multi-Step Intent as Reward Augmentation", "content": "The planning module is the core decision-making component of the autonomous agent. We introduce an online planning algorithm called INTENTMCTS, which considers the current state, the multi-step intent communicated by the partner, and the current belief from its memory module to compute the most appropriate action. To accommodate the dynamics of players taking multiple actions per turn, this algorithm builds on multi-action Monte Carlo tree search (MCTS) [1, 16, 23], growing trees with nodes representing each atomic action within a turn. Additionally, INTENTMCTS selects actions differently based on the available information at self-controlled nodes versus partner-controlled nodes, following the same idea as [8] in maximizing information use and growing the tree as efficiently as possible.\nNotations: We denote v as a node in the search tree, with s(v), r(v), and \u03b4(v) representing the state, reward, and transition feasibility at node v, respectively. Concretely, \u03b4 is the probabilistic belief of the action that leads to node v being valid. The visit count of node v is denoted by N(v), while Q(v) represents the total discounted return at node v. The set of child nodes of v is represented as C(v). We present INTENTMCTS for the ego player in Algorithm 2, which performs the following four phases iteratively:\n\u2022 Selection (lines 4-10): Starting from the root node, the agent recursively selects child nodes that maximize the Upper Confidence Bound value [18] until a terminal state or an expandable node is reached. A node is considered fully expanded if child nodes corresponding to all feasible actions are present.\n\u2022 Expansion (lines 15-26): Upon reaching an expandable node, the agent expands the tree by adding new child nodes based on its perception of available actions. When player E is in control, the true transition function is known, so only feasible actions are considered. However, when player H is in control, all actions are considered, with their feasibility \u03b4 estimated by the memory module in Section 4.1. Formally, we define the feasible action set as follows:\n$U(s) = \\begin{cases} \\{a \\in A|T_E(s, a) \\text{ is defined}\\} & \\text{if } c = E \\\\ A & \\text{if } c = H. \\end{cases}$\n(7)\nFor a chosen unexplored action, we construct a child node that records this action a, the step reward r', the resulting state s', and its feasibility \u03b4'. When the agent E is in control, s' is computed by its forward dynamics TE with \u03b4' = 1. At a human-controlled node, s' is computed from $T^+(s, a)$, a transition function that returns the next state assuming the applied action a is valid at state s, and \u03b4' is retrieved from the memory module:\n$g(s, a) := (s', \\delta') = \\begin{cases} (T_E(s, a), 1) & \\text{if } c = E \\\\ (T^+(s, a), b_H(s, a)) & \\text{if } c = H. \\end{cases}$\n(8)\nSince switching control is part of the action space, the search tree can expand multiple actions from one player or switch to another anytime, accommodating the game's multi-action dynamics.\n\u2022 Simulation (lines 27-39): A rollout from the expanded node simulates future transitions until a terminal state or a pre-defined depth T is reached. Actions are selected randomly among U(s). At an agent-controlled state, the transition follows TE. In a human-controlled state, the agent imagines the effect of an action with its memory module. The transition is executed when a random number drawn uniformly between 0 and 1 is smaller than the feasibility belief. Otherwise, the state remains unchanged. The simulation accumulates discounted rewards into the outcome q and increments the depth d at each step.\n\u2022 Backpropagation (lines 40-51): The rewards obtained from the simulation phase are backpropagated up the tree, updating Q(v) and N(v) for all nodes along the path from the expanded node back to the root. Due to the uncertain feasibility of human-controlled nodes, the rollout return depends on the feasibility of the child node from which backpropagation comes. Suppose the child node v' gets a sample return qsample. Its parent node's return qsample should consist of three terms: (1) the step reward r, (2) with probability \u03b4', the transition is feasible, and the discounted future return is $\u03b3q_{sample}$ (3) with probability (1\u2212\u03b4\u2032), the transition is invalid (action has no effect) and the discounted future return is the discounted value estimate at the current state:\n$q_{sample} = r + \\gamma \\begin{bmatrix} \\delta'q_{sample} + (1 - \\delta') \\frac{Q(v)}{N(v)} \\end{bmatrix}$.\n(9)\nReward Design. INTENTMCTS performs reward augmentation to encourage the agent to follow the partner's intent trajectory. We denote the environment reward function as Renv: $S \\rightarrow R$. Aside from the base reward, we define an intent bonus $R_{int} : S \\times X^+ \\rightarrow R$. Since the algorithm tries to find an optimal action for player E, reward augmentation is applied only to its transitions:\n$R(s, \\zeta^H) = R_{env}(s) + I[c = E] \\cdot R_{int}(s, \\zeta^H)$.\n(10)\nSpecifically, given an intent trajectory $\\zeta^H = \\{x_0, x_1, ..., x_m\\}$, we assign a discounted intent bonus to provide a smooth gradient of rewards along the intent trajectory:\n$R_{int} ((x, c), \\zeta^H) = \\begin{cases} \\lambda^{m-i} & \\text{if } x = x_i \\in \\zeta^H \\\\ 0 & \\text{otherwise}. \\end{cases}$\n(11)\nThe intent discount factor $\u03bb \\in (0, 1)$ adaptively trades off intent trajectory following and final intent state reaching. By design, $R_{int} \\in [0,1]$ adheres to the general principle that auxiliary objectives should not outweigh the task signals, which is a -1 control cost at each step in this paper. Moreover, considering a planning discount factor \u03b3, we can compare two extreme cases: (1) The agent follows the intent trajectory meticulously to reach the final state, yielding reward:\n$J_{follow} = \\sum\\limits_{t=0}^{m-1} \\gamma^t (\\lambda^{m-t-1} - 1)$.\n(Case 1)\n(2) The agent finds a shorter path with length n < m to reach the final state without visiting any intermediate intent states:\n$J_{skip} = \\lambda^m + \\sum\\limits_{t=0}^{n-1} \\gamma^t (-1)$.\n(Case 2)\nAs shown in the equations above, the agent receives all intent bonuses in (Case 1) but only receives the last intent bonus in (Case 2). We analyze the benefit of skipping to the final state as follows:\n$J_{diff} := J_{skip} - J_{follow} = \\lambda^m + \\sum\\limits_{t=n}^{m-1} \\gamma^t - \\sum\\limits_{t=0}^{m-1} \\gamma^t \\lambda^{m-t-1}$.\n(12)\nWhen we choose \u03bb < \u03b3, we can compute a threshold for n that makes one case more preferable:\n$J_{diff} = \\lambda^m + \\frac{\\gamma^n - \\gamma^m}{1 - \\gamma} - \\sum\\limits_{t=0}^{m-1} \\gamma^t \\lambda^{m-t-1} > 0 \\text{ if } n < \\frac{log_{\\gamma} \\frac{\\gamma - \\lambda}{\\gamma(1 - \\gamma)}}{\\text{elsewhere}}$\n(13)\nAs shown in the inequality above, choosing the shorter path (Case 2) is preferable to following the intent trajectory (Case 1) only for small enough n. Changing the hyper-parameter \u03b3 smoothly adjusts the decision boundary between the two cases and results in different levels of intent-following behavior in INTENTMCTS."}, {"title": "5 AGENT-TO-AGENT SIMULATION", "content": "We evaluate our approach on the Gnomes at Night testbed from [8], a board game where two players coordinate to move a single token through a maze. The two players see different maze layouts and can only move the token along the paths within their respective mazes. Each game configuration (xinit, xgoal) pairs an initial and goal position, and a round is considered successful if the players reach the goal within 1000 steps. To focus on coordination rather than goal discovery, we modify the testbed to reveal the goal position xgoal to both players from the start, eliminating the need for communication about the goal's location and isolating the core coordination challenge. The state space includes all maze grid cells. Players can move right, up, left, down, or switch control. The transition function is deterministic, allowing actions unless blocked by a wall. The reward is 100 for reaching the goal, with a penalty of -1 per step to encourage efficient coordination."}, {"title": "5.1 Intent Model", "content": "The agent-to-agent simulation aims to anticipate the agent's performance when playing with humans. Hence, INTENTMCTS agents should communicate and plan with human-like intent trajectories. In the problem formulation, player c's intent at timestep t,, is retrieved from Intent Model given the current state st and the partner's history $h_{\\bar{c}}^t$. In our experiments, we instantiate both agents' intent models with the belief-conditioned shortest path toward the goal. Concretely, before switching control, each agent plans the lowest-cost path as its intent, factoring in its maze layout and belief over partner transitions: taking a valid transition costs -1, and crossing a wall incurs a penalty of -10(1 \u2013 \u03b4). The resulting intent minimizes steps and uncertainty: the agents prefer to traverse on their side of the maze and prioritize more feasible partner transitions when blocked."}, {"title": "5.2 Comparison with Baselines", "content": "To verify the effectiveness of our INTENTMCTS planning algorithm, we compare with three relevant baselines: a shortest-path-based heuristic controller, MCTS without intent bonus, and MCTS considering single-step intent, all using the same memory module:\n\u2022 Shortest-Path Heuristic Controller: Same as how the intent trajectory is generated, the agent plans the lowest-cost path to the goal. It attempts to traverse the path until it is obstructed by a wall, after which control is passed to its partner. To improve the robustness of the controller, a random action is taken instead with 20% chance.\n\u2022 No-Intent MCTS: We do not perform reward augmentation regarding partner intent. The agents must plan with environment rewards only.\n\u2022 Single-Step-Intent MCTS: Instead of reward augmentation, the intents are used as a tie-breaker during action selection. At the end of MCTS, if two child nodes are equally preferable and one follows the intent, that node is selected.\nWe test all configurations across three 9 \u00d7 9 mazes, each containing 81 \u00d7 80 = 6480 different configurations. For each configuration, we run 10 trials. The oracle episode length for each configuration is calculated as the minimal number of moves plus control switches needed to reach the goal, considering both players' maze layouts. This metric reflects task difficulty. We then plot the number of steps and control switches required to complete the mazes against the oracle episode lengths for all tested trials."}, {"title": "5.3 Ablation Study", "content": "We perform an ablation study with three alternative reward schemes to justify the discounted intent reward used in INTENTMCTS: fixed reward, first step only reward, and length inverse reward. Note that these alternatives all range between the same bound [0, 1] as Rint, guaranteed not to outweigh the control cost of -1 in our setting.\nFirst, a fixed intent bonus assigns a constant reward whenever the agent visits a state appearing in the intent trajectory:\n$R^{fixed}((x, c), \\zeta^H) = 0.5 \\cdot I[x \\in \\zeta^H]$.\n(14)\nThe fixed bonus scheme weighs all intent states equally, ignoring the temporal information within the trajectories. Alternatively, analogous to the simple-step MCTS, the first step only reward only encourages visiting the immediate next intent state. Given an intent trajectory $\\zeta^H = \\{x_0, x_1, ..., x_m\\}$, we have:\n$R^{fso}((x, c), \\zeta^H) = 0.5 \\cdot I[x = x_1]$.\n(15)\nOn the other hand, the length inverse bonus scheme prioritizes reaching the final intended state:\n$R^{linv} ((x, c), \\zeta^H) = \\begin{cases} \\frac{1}{m} & \\text{if } x = x_m \\in \\zeta^H \\\\ 0 & \\text{if } x \\in \\zeta^H, x \\neq x_m \\\\ 0 & \\text{otherwise}. \\end{cases}$\n(16)\nBy design, the agent receives a big bonus only when reaching the final intent state. Traversing the environment exactly as intended is unnecessary, especially for long trajectories.\nAs Table 1 shows, the discounted reward bonus leads to the most effective coordination quantified by the fewest average steps and control switches taken. We report in geometric mean \u00b1 geometric standard deviation across all experiment trials because the data are positive and right-skewed."}, {"title": "6 HUMAN-AGENT EVALUATION", "content": "We conduct a user study where human participants play with different agent partners in the Gnomes at Night testbed."}, {"title": "6.1 User Study Design", "content": "Independent Variables. The study varies the agent decision-making algorithms from the proposed algorithm and baseline methods:\nAgent Alice: Shortest-Path-Heuristic Controller,\nAgent Bob: Multi-Step-Intent MCTS (INTENTMCTS),\nAgent Charlie: Single-Step-Intent MCTS.\nAll agents share the same memory module, as detailed in Section 4.1. We exclude No-Intent MCTS, as [8] has already demonstrated that single-step intent performs better than no-intent.\nDependent Variables. We measure the number of steps taken, the number of control switches taken, and whether the token reaches the goal position. We also ask participants to rate based on a 7-point Likert scale survey of the following questions:\n(1) How mentally demanding was the task?\n(2) How physically demanding was the task?\n(3) How hurried or rushed was the pace of the task?\n(4) How insecure, discouraged, irritated, stressed, and annoyed were you?\n(5) How hard did you have to work to accomplish your level of performance?\n(6) How successful were you in accomplishing what you were asked to do?\n(7) How satisfied are you with the performance of your partner?\nThe first six questions are from the NASA-TLX [14], covering mental demand, physical demand, temporal demand, frustration, effort, and performance, respectively. The final question assesses participant satisfaction with each agent partner.\nHypotheses. We design the experiment with two hypotheses:\n(H1) Gameplay with the Multi-Step-Intent agent will outperform the other two agents in coordination efficiency.\n(H2) Participants will report better subjective survey scores on the Multi-Step-Intent agent than other two agents.\nExperiment Design. We use a within-subject design where each participant completes three game sessions, each partners with one of the agents\u2014Alice, Bob, or Charlie\u2014as outlined in the independent variables. All sessions use the same maze, and each session includes the same set of three distinct configurations to ensure consistency for comparison. We ask participants to answer the 7-item survey after each session. To control for order effects\u2014such as learning, fatigue, and carryover\u2014we use counterbalancing to randomize the order of sessions [19]. The counterbalancing ensures an even distribution of methods across session orders, as shown in Table 2. Within each session, we also randomize the order of configurations to prevent boredom or disengagement over time. This two-level randomization design reduces potential biases, allowing for a more accurate comparison of participant performance across methods. An example of the user study can be found in the supplementary file \"user_study_procedure.pdf.\"\nParticipants. The study recruits 18 university students as consenting participants, with an average age of 26.28. The gender distribution was 0.83 male, 0.11 female, and 0.06 non-binary. All participant data are anonymized and provided as CSV files in the \"user_data\" folder in the supplementary."}, {"title": "6.2 Results and Discussions", "content": "Regarding (H1). We present the distributions of steps and control switches for each method using boxplots, with median values labeled in white text. In Figure 3a, Multi-Step-Intent shows a median step count 1.5 steps lower than Shortest-Path-Heuristic and 11.5 steps lower than Single-Step-Intent, with a significantly smaller interquartile range (IQR), indicating less variation due to randomness or participant differences. Similarly, Figure 3b shows Multi-Step-Intent requires a median of 12 fewer control switches than Shortest-Path-Heuristic and 5 fewer than Single-Step-Intent, again with a much smaller IQR. Additionally, Figure 3c highlights that Multi-Step-Intent improves success rates by 5.56% over Single-Step-Intent and 18.52% over Shortest-Path-Heuristic. These results support (H1), demonstrating improved coordination efficiency.\nRegarding (H2). We present average participant ratings for 5 of the 7 survey items in Figure 3d, excluding physical and temporal demand as they are less relevant to the task. Lower scores indicate lower cognitive load or higher satisfaction. The Multi-Step-Intent agent achieves lower scores in all dimensions than the other two agents, shown as the smallest orange area, supporting (H2) and demonstrating an enhanced user experience.\nEffect of Memory Module in Gameplay. In the Gnomes at Night testbed, players' private transition functions reflect the unique wall layouts in their mazes. Figure 4 showcases how the agent's belief about the human's maze walls, stored in its memory module, evolves during a human-agent sample gameplay. Darker lines indicate a stronger belief in the presence of walls. Starting with a uniform belief of 0.5 for all walls, the agent refines its understanding, identifying one room's boundaries by step 100 and inferring the overall maze layout by the end of the game at step 620.\nQualitative Observations. The quantitative results from both objective metrics and subjective ratings clearly indicate that Multi-Step-Intent outperforms Shortest-Path-Heuristic and Single-Step-Intent. Qualitative feedback from participants reveals additional insights. Many participants appreciate the explainability provided by the multi-step intent algorithm, which enhances their satisfaction even when the performance difference is not immediately evident. We also observe that participants use alternative strategies, such as marking cells around closed rooms to signal being trapped rather than moving along the room boundary repeatedly. However, frustration arises when the agent seems to ignore their intended paths, suggesting that incorporating natural language communication or clarification as a valuable next step."}, {"title": "7 CONCLUSION", "content": "We extend a turn-based shared-control game under incomplete information to allow multiple actions per turn, enabling the use of multi-step intent to enhance performance in long-horizon tasks. To facilitate coordination, we develop INTENTMCTS, an online planning algorithm that incorporates a memory module for probabilistic beliefs and leverages multi-step intent through reward augmentation. Both agent-to-agent simulations and a human-agent user study show that INTENTMCTS significantly outperforms baseline methods in terms of steps, turns, and success rates. The user study also highlights improvements in participant cognitive load and satisfaction toward the partner.\nFor future work, we can explore using multi-step intent as a constraint when feasible, based on the observation that humans expect agents to follow their intent whenever possible. We can also extract multi-step intent from natural language communication and leverage it for more efficient belief updates or as a supplementary source of information alongside transition history. Finally, we aim to transition from a handcrafted heuristic to a data-driven model for intent generation."}]}