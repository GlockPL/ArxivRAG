{"title": "Operationalizing Contextual Integrity in Privacy-Conscious Assistants", "authors": ["Sahra Ghalebikesabi", "Eugene Bagdasaryan", "Ren Yi", "Itay Yona", "Ilia Shumailov", "Aneesh Pappu", "Chongyang Shi", "Laura Weidinger", "Robert Stanforth", "Leonard Berrada", "Pushmeet Kohli", "Po-Sen Huang", "Borja Balle"], "abstract": "Advanced AI assistants combine frontier LLMs and tool access to autonomously perform complex tasks\non behalf of users. While the helpfulness of such assistants can increase dramatically with access to\nuser information including emails and documents, this raises privacy concerns about assistants sharing\ninappropriate information with third parties without user supervision. To steer information-sharing\nassistants to behave in accordance with privacy expectations, we propose to operationalize contextual\nintegrity (CI), a framework that equates privacy with the appropriate flow of information in a given\ncontext. In particular, we design and evaluate a number of strategies to steer assistants' information-\nsharing actions to be CI compliant. Our evaluation is based on a novel form filling benchmark composed\nof synthetic data and human annotations, and it reveals that prompting frontier LLMs to perform\nCI-based reasoning yields strong results.", "sections": [{"title": "1. Introduction", "content": "Advanced AI assistants can be defined as \u201cartificial agents with a natural language interface, the\nfunction of which is to plan and execute sequences of actions on the user's behalf across one or\nmore domains and in line with the user's expectations\u201d [10]. Many of the applications envisioned for\nadvanced AI assistants involve interactions between the agent and an external third party (e.g. a\nhuman, an API, another agent) which are 1) performed autonomously on behalf of the user i.e.\nwithout direct user supervision, and 2) share user information available to the agent in order to fulfill\na task. These include, for example, booking medical appointments, applying for jobs, making travel\nand hospitality reservations, purchasing clothes, etc. User expectations for assistants undertaking\nsuch tasks on their behalf include expectations of utility (the agent will correctly fulfill the requested\ntask) and privacy (the agent will only share data that is strictly necessary to achieve the task).\nGiven the impressive capabilities exhibited by frontier large language models (LLMs), the current\npredominant paradigm for developing advanced AI assistants is based on (instruction tuned, aligned\nand prompted) LLMs [32, 2, 15, 33] with access to tools (e.g. API calling for third party interactions,\nmemory for long-term data storage and retrieval, etc) [24, 17, 11, 27]. Such tools enable AI assistants\nto interact with a diverse set of services ranging from search engines to web browsing to cloud-based\ne-mail and calendar applications [13, 23]. This type of architecture leads to a dramatic increase in the\nnumber of tasks AI assistants can undertake, while, at the same time, increasing the complexity of the\npotential information-sharing flows they can mediate on behalf of the user. Ensuring that AI assistants\nmeet users' expectations of privacy across a wide range of applications poses a significant challenge\nin controlling the flows of information assistants mediate in, which is exacerbated by well-known\nvulnerabilities to adversarial examples, jailbreaking and prompt injection attacks commonly exhibited\nby LLM-based systems [20, 38, 12, 35].\nThe theory of contextual integrity (CI) defines privacy as the \u201cappropriate flow of information in\naccordance with contextual information norms"}, {"title": "2. Privacy and Utility of Information-Sharing Assistants", "content": "We begin by formalizing the notion of information-sharing assistants and identifying appropriate\nmetrics to measure their performance on the utility and privacy axes. Our framework can cap-\nture a wide range of tasks a user might request from an assistant with access to tools, includ-\ning e.g. \"Fill the web form at {URL}\",\"Use {API} to book a table for next week\nat my favorite restaurant\", \"Reply to {EMAIL} with my calendar availability\".\nThe main goal is to model information flows mediated by AI assistants that consume user information\nand share it with external third parties.\nInformation-sharing assistants. Consider an AI assistant (denoted by A) with access to a collection\nof user information I which receives a task request Q from the user. We focus on information-sharing\ntasks that require the assistant to produce as output k strings O = (o\u2081, . . ., o\u2096) containing information\nfrom I and share them with a third party. Although in general completing the task might require\nmore than one round of interaction with (multiple) third parties, for simplicity in our model we only\nconsider a single information sharing action O = A(Q, I). To effectively communicate with the third\nparty the assistant might need to send the outputs formatted in a specific way (e.g. responses to a\nform filling request need to be mapped to each field in the form; values for parameters in an API\ncall need to be added to the code that calls the API). Thus, we assume that after processing the task\ndescription Q the assistant crafts (or retrieves) an output template T\u2080 with k blanks and the message\nsent to the third party T\u2080 (o\u2081, . . ., o\u2096) is obtained by filling the blanks.\nPrivacy and utility. The utility of such assistants measures how often they share all the information\nnecessary to achieve a task Q sampled from a distribution over tasks we denote as Q. In principle an\nassistant could achieve any task by sharing all the information in I (modulo the appropriate formatting\nexpected by the third party). Obviously, this oversharing is not the expected behavior from a privacy\nperspective [37]. We define privacy leakage as the amount of information the assistant overshares,\ni.e. information shared with the third party that is not necessary to achieve the task. Oversharing\ncould be the result of, for example, the assistant misinterpreting what information is necessary, or\nit sharing information requested by the third party even when this is not compatible with a user's\nprivacy expectations.\nSimplifying assumptions. To simplify and streamline evaluation we make the following assump-\ntions. First, we assume all the necessary information for the tasks in the support of Q is available in I.\nNext, we only consider the case where I is structured as a list of key-value pairs representing the pos-\nsible types of information available to the assistant; example keys include first_name, last_name,\ndate_of_birth, social_security_number, etc. Without loss of generality we assume keys are\nfixed across users (although the value of some keys might be empty for some users). Finally, we"}, {"title": "3. Operationalizing Contextual Integrity for Information Sharing Assistants", "content": "It should come as no surprise that high-utility information-sharing assistants are easy to build on top\nof frontier LLMs. On the other hand, ensuring that assistants abstain from sharing certain types of\ninformation depending on the context is more challenging. Our goal is to ground assistant behav-\nior on contextual integrity judgements to steer alignment between information flows arising from\ninformation-sharing actions and applicable information norms. Thus, we propose a collection of assis-\ntants with increasingly sophisticated mechanisms for deciding whether an information-sharing action\nshould be performed; the privacy-utility trade-offs achieved by these assistants will be empirically\nevaluated in Section 5.\nContextual integrity theory. Contextual integrity identifies the properties of an information flow\nthat are relevant to judge its appropriateness against relevant norms. These include the attributes of\nthe information being transmitted (data subject, sender, receiver, information type, and information\nprinciple), the broad context of the flow (e.g. health, finance, business, family, hospitality etc), the\nrelationships and roles of the actors (e.g. in a health context the sender might be a patient and the\nreceiver a doctor), and the purpose the flow is trying to achieve (e.g. when communicating with a\nrestaurant in a hospitality context the information to be shared differs between booking a table and\nordering take out). To make a judgement about whether a flow is appropriate, CI postulates that\none should identify the relevant contextual information norms, and deem the flow appropriate if no\nnorm explicitly forbids it and at least one norm allows it [4]. Note that according to CI, contextual\ninformation norms represent widely accepted societal norms (e.g. grounded by culture or regulation)"}, {"title": "4. Form-Filling Benchmark", "content": "We evaluate the assistants proposed in the previous section by constructing a synthetic form-filling\nbenchmark, motivated by Schick et al. [26, 28]. Each task in the benchmark represents an online form\ncontaining a title, a description of the form's purpose, and a list of field descriptions. The benchmark\nalso includes a number of personas with varying personal information that the form-filling assistant"}, {"title": "4.1. Synthetic Persona Generation", "content": "In our benchmark a persona is represented by assigning values to all the possible keys that define\nthe user information I available to the assistant. We consider 51 manually defined keys cover-\ning a wide range of information types that are relevant in different form filling tasks. To gen-\nerate values we prompt Gemini Ultra [32] with one out of 18 high level persona descriptions\n(e.g. \"an average 65-year-old\", \"the CEO of a successful startup\", \"a graduate\nstudent at a state university\") and ask it to fill the list of key-value pairs. See Appendix C\nfor a complete list of keys, persona descriptions and prompts.\nThe generated personas were manually reviewed for consistency and missing values, leading to minor\nmanual adjustments. In particular, personas were modified to represent individuals residing in the\nUS to reduce ambiguity about the contexts of the online forms generated in the next section. This\nalso helped simplify the collection of privacy annotations by limiting the range of applicable privacy\nnorms. As discussed above, CI relies on societal norms which can vary across sub-populations (e.g. by\ngeography); this is critical for serving privacy expectations of users from a wide range of backgrounds,\nbut can introduce an additional confounder in the evaluation of CI capabilities. Our approach of\nlimiting the geographic range in the persona data set is an attempt to minimize such effect in our\nevaluation (see Section 6 for a discussion on this limitation)."}, {"title": "4.2. Synthetic Form Generation", "content": "To generate synthetic online forms we first compiled 14 hypothetical scenarios like \"Apply for\nmedical school\", \"In-person work event registration\", \"Create checking account\nin the US bank\", or \"Newsletter subscription to online clothes shopping website\"\nthat may plausibly relate online form filling tasks. Given one of these scenarios, we prompt Gemini\nUltra to generate first a more detailed description for the form and then a title; this process is repeated\n3 times with independent random seeds for each scenario. To further increase the number of forms\nand enable us to test the assistant's robustness to different phrasings, we also ask the model to produce\n2 alternative paraphrasings for each title-description pair, resulting in a total of 126 form settings.\nThe fields that can appear in forms are obtained from the list of keys of information available to every\npersona. For each field we prompt Gemini Ultra to generate at most 5 different ways of asking for\nthat information in a form, e.g. date_of_birth gets mapped to \"Date of birth\", \"Birthday\",\n\"D.O.B.\".These phrasings are obtained independently from the form application and re-used across\nmultiple forms (see Table 10 for a full list); this step ensures the filling task is more complex than a\nsimple pattern matching between keys. Generated field phrasings were manually reviewed to remove\nambiguous or confusing phrasings, resulting in a total of 193 phrasings.\nEach form in the dataset is obtained by using one of the possible title-description pairs and generating\na form by randomly selecting 7 relevant keys and then assigning a random phrasing to each key. We\nkeep the same keys but resample the phrasings used to ask for the key across form title and description\nparaphrasings. Relevance of keys for the different form scenarios is obtained via human annotations\n(cf. Section 4.3). To obtain the filling ground truth we retain the names of the information keys\nused to construct the form, and map these to ground truth values for every field in the form given a\nconcrete persona. Note that the information of which fields correspond to which key is only used for\nevaluation and not made available to the assistant."}, {"title": "4.3. Human Annotations", "content": "Determining the appropriateness of sharing the value of a particular user information field in a\ncertain form is a challenging problem. In our benchmark we compile ground truth labels by relying\non annotations from 8 human raters. Each rater is asked to provide labels by judging all possible\n51 \u00d7 14 = 714 pairs of information keys and form scenarios on a five point Likert scale. To guide\nraters towards judgements that represent expectations across the population (instead of individual\npreferences) we ask for two types of labels: necessary, meaning the field is not filled the purpose of\nthe form cannot be achieved; and relevant, meaning the field occurs in at least some forms for the\ngiven application, although it could be optional. See Appendix C.4 for the full instructions.\nAssessing raters' disagreement. Deciding whether information types are necessary or relevant\ndepends on two main factors, the context (i.e. the form, and social norms or regulations), and the\nuser's expectations. The second factor suggests that annotators might differ in their judgements even\nwhen the labelling task is designed to elicit norms that are valid across a certain population. This\nposed a challenge in extracting ground truth labels from a set of annotations - we refer to Appendix C.4\nand Figure 12 for an analysis of annotator disagreements across different types of information. For\nthe results presented in the main part of the paper, we assign labels: Yes for a data key and form\napplication pair if all raters assign a value of 3 or higher; No if all raters assign a value of 3 or smaller;\nand Unsure otherwise. For \u201cnecessary\u201d annotations this yields 50 Yes, 460 No and 232 Unsure.\nRegulated contexts. In some of the scenarios we consider the requirement to share (or not share)\na particular type of information might be regulated, e.g. in the US it is required to share a tax\nidentification number when making a cash purchase of over $10K [29]. This indicates the existence\nof a widely\u00b9 accepted norm which the assistant is expected to follow. Some of the forms we use fall\nunder existing legal regulations such as FERPA [9] for education contexts, IRC [29] for large cash\npurchases, or HIPAA [14] for medical contexts. To account for such situations in our benchmark, we\nalso compiled labels for a subset of form application and information key pairs where an existing\napplicable norm was identified; in such cases we leverage these labels instead of the ones provided by\nannotators. For \"necessary\" annotations this modifies the label distribution above to 87 Yes, 470 No\nand 185 Unsure."}, {"title": "5. Experimental Evaluation", "content": "Experimental setup. We implement form filling assistants by prompting Gemini Pro and Ultra\n[32] to play the roles of assistant, supervisor and filler described in Section 3. In particular, we\nevaluate the four assistant architectures we propose, including two versions of the assistant with\nreasoning supervisor (with and without chain-of-thought reasoning [16]). We experiment with\nprompt engineering, few shot prompting, and the inclusion of user information in the prompt of the\nsupervisor. In all our implementations the form-filling assistants process each field independently\nof the others, and supervisors can output three decisions: fill, do not fill, or ask the user - the latter\nexpresses uncertainty in the model's decision and is considered correct when the ground truth label\nis Unsure. Full experimental details (including prompts) are provided in Appendix A. Unless we say\notherwise, all experiments report results over three random seeds."}, {"title": "6. Conclusion", "content": "AI assistants that can undertake information-sharing tasks on behalf of users can provide significant\nvalue in innumerable applications. Our work makes progress towards information-sharing assistants\nwhose actions align with users' privacy expectations by 1) identifying contextual integrity as an\nappropriate framework to ground such alignment on societal information norms, and 2) proposing a\nmethod to operationalize CI-based reasoning in LLM-based assistants. Our experimental evaluation\nshows that assistants built on existing models can readily benefit from this approach to significantly\nreduce privacy leakage without major deteriorations in utility, and in particular that CI-based assistants\nachieve the best performance among the options we investigate. We conclude by identifying a number\nof research challenges that need to be addressed by future work, and note that our work only touches\nupon one of many important aspects that need to be considered when responsibly developing and\ndeploying advanced AI assistants to have positive broader impact [10].", "6.1. Limitations and Future Work": "Data collection. Improving and benchmarking CI capabilities requires high-quality data sets. We\ndemonstrate that a combination of synthetic data and human annotations might offer a path towards"}]}