{"title": "Physics-Informed Neural Networks and Extensions", "authors": ["Maziar Raissi", "Paris Perdikaris", "Nazanin Ahmadi", "George Em Karniadakis"], "abstract": "In this paper, we review the new method Physics-Informed Neural Networks (PINNs) that has become the main pillar in scientific machine learning, we present recent practical extensions, and provide a specific example in data-driven discovery of governing differential equations.", "sections": [{"title": "Overview", "content": "In the last 50 years there has been a tremendous success in solving numerically PDEs using finite elements, spectral, and even meshless methods. Yet, in many real cases, we still cannot incorporate seamlessly (multi-fidelity) data into existing algorithms, and for industrial-complexity applications the mesh generation is time consuming and still an art. Moreover, solving inverse problems, e.g., for material properties, is often prohibitively expensive and requires different formulations and new computer codes. In recent years, uncertainty quantification (UQ) of simulations has led to highly parametrized formulations that may include 100s of uncertain parameters for complex problems ren-dering such computations infeasible in practice. Finally, existing computer programs for engineering applications have more than 100,000 lines of code, making it almost impossible to maintain and up-date them from one generation to the next. To this end, physics-informed learning, i.e., integrating seamlessly data and mathematical models, and implementing them using physics-informed neural networks (PINNs) [Raissi et al. (2017a)], [Raissi et al. (2017b)], [Raissi et al. (2019)] is a paradigm shift in defining the main thrust in scientific machine learning (SciML).\nThe specific data-driven approach to modeling physical systems depends crucially on the amount of data available as well as on the complexity of the system itself, as illustrated in Fig. 1. The classical paradigm is shown on the top of Fig. 1, where we assume that the only data available are the boundary conditions (BC) and initial conditions (IC) while the specific governing partial differential equations (PDEs) and related parameters are precisely known. On the other extreme (lower plot), we may have a lot of data, e.g. in the form of time series, but we may not know the"}, {"title": "Extensions of PINNS", "content": "Adaptive Weights: The original PINNs used fixed weights in front of the various terms in the loss functions, requiring manual tuning. In [Wang et al. (2022)], the authors used the Neural Tangent Kernel (NTK) for PINNs, which is a kernel that captures the behavior of neural networks in the infinite width limit during training via gradient descent. They proposed a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the loss weights and accelerate reduction of the total training error. An even more effective method was proposed in [McClenny and Braga-Neto. (2020)] for multiscale systems and \u201cstiff\" PDEs. The loss weights are fully trainable and applied to each training point individually, so the neural network learns which regions of the solution are stiff and focuses on them. The basic idea is to make the weights increase as the corresponding losses increase, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights (a min-max problem). A variation of this idea that avoids computing the gradients but instead uses the residuals was proposed in [Anag. et al. (2023)].\nDomain Decomposition: Another approach to tackle multiscale problems with PINNs is to combine them with domain decomposition methods as was done in [Jagtap et al. (2020), Jagtap et al. (2021)].In the first paper, domain decomposition was introduced for conservation laws,"}, {"title": "Long-Time Integration", "content": "Another problematic issue in PINNs is the long-time integration of dynamical systems whose solution exhibits chaotic behavior. In [Wang et al. (2022)], this shortcoming was attributed to the inability of existing PINNs formulations to respect the spatio-temporal causal structure that is inherent to the evolution of physical systems. They addressed it by re-formulating the loss functions so that it can explicitly account for physical causality during model training. This approach still requires a very small temporal domain to be accurate, and various other attempts have resorted to sequential learning. A more general and unified approach was presented in [Penwarden et al. (2023)], where the authors proposed a new stacked-decomposition method that bridges the gap between time-marching PINNS and XPINNs. They also introduced significant speed-ups by using transfer learning to initialize subnetworks in the domain and loss tolerance-based propagation for the subdomains. Moreover, they formulated a new time-sweeping collocation point algorithm inspired by the aforementioned PINNs causality. These methods form a unified framework, which overcomes training challenges in PINNs and XPINNs for time-dependent PDEs by respecting the causality in multiple forms and improving scalability by limiting the com-putation required per optimization iteration."}, {"title": "Other Types of PDEs", "content": "PINNs have also been applied to other types of PDES, e.g., in stochastic PDEs for uncertainty quantification, and in fractional PDEs for modeling anomalous transport. Specifically, in [Yang et al. (2020)], a new class of physics-informed generative adversarial networks (PI-GANs) was proposed to solve forward, inverse, and mixed stochastic problems in a unified manner based on a limited number of scattered measurements. Unlike standard GANs relying solely on data for training, they encoded into the architecture of Wassertein GANs the governing physical laws in the form of stochastic differential equations (SDEs) using automatic differentiation."}, {"title": "Theory", "content": "Finally, we report on some early works on the theoretical foundations of PINNs. In [Shin et al. (2019)], the authors demonstrated that, as the number of collocation points increases, the sequence of minimizers for PINNS - each corresponding to a sequence of neural networks - converges to the solution of the PDE for two classes of PDEs, linear second-order elliptic and parabolic. By adapting the Schauder approach and the maximum principle, they showed that the sequence of minimizers strongly converges to the PDE solution in C\u00ba. Furthermore, they showed that if each minimizer satisfies the initial/boundary conditions, the convergence mode becomes H\u00b9. In [Mishra and Molinaro (2019)], the authors provided upper bounds on the generalization error of PINNS approximating solutions of the forward problem for PDEs. They introduced an abstract formalism, and leveraged the stability properties of the underlying PDE to derive an estimate for the generalization error in terms of the training error and the number of training samples."}, {"title": "Data-Driven Discovery of Dynamical Systems", "content": "Next, we demonstrate a PINN-inspired approach for identifying nonlinear dynamical systems from data. Specifically, we blend classical tools from numerical analysis, namely the multi-step time-stepping schemes, with deep neural networks, to distill the mechanisms that govern the evolu-tion of a given dataset. We test the effectiveness of our approach for a biomedical application example but more cases can be found in [Raissi et al. (2018)]. In particular, we study the gly-colytic oscillator model as an example of complicated nonlinear dynamics typical of biological sys-tems, and subsequently use symbolic regression to obtain the equations explicitly in analytical form. Let us consider nonlinear dynamical systems of the form: $\\dot{x}(t) = f (x(t))$, where the vector $x(t) \\in \\mathbb{R}^D$ denotes the state of the system at time $t$ and the function $f$ describes the evolution of the system. Given noisy measurements of the state $x(t)$ of the system at several time instances $t_1, t_2,...,t_n$, our goal is to determine the function $f$ and consequently discover the underlying dynamical system from data. We proceed by applying the general form of a lin-ear multistep method with M steps to equation and obtain $\\sum_{m=0}^M [a_m x_{n-m} + \\Delta t \\beta_m f(x_{n-m})] = 0, n = M,..., N$. Here, $x_{n-m}$ denotes the state of the system $(t_{n-m})$ at time $t_{n-m}$. Different choices for the parameters $a_m$ and $\\beta_m$ result in specific schemes. We proceed by plac-ing a neural network prior on the function $f$. The parameters of this neural network can be learned by minimizing the mean squared error loss function $MSE := \\frac{1}{N-M+1} \\sum_{n=M}^N |y_n|^2$, where $y_n := \\sum_{m=0}^M [a_m x_{n-m} + \\Delta t \\beta_m f(x_{n-m})], n = M, ..., N$, is obtained from the multistep scheme.\nAs an example of complicated nonlinear dynamics typical of biological systems, we simulate the glycolytic oscillator model presented in [Daniels and Ilya. (2015)]. The model consists of ordinary differential equations for the concentrations of 7 biochemical species, see [Raissi et al. (2018)]. The"}, {"title": "Physics-Informed Neural Networks and Extensions", "content": "parameters of the model are chosen according to table 1 of [Daniels and Ilya. (2015)].\n$\\begin{aligned}\n\\frac{dS_1}{dt} &= J_0 - 2 \\frac{k_1 S_1 S_6}{1+(S_6/K_1)^q},\\\\\n\\frac{dS_2}{dt} &= 2 \\frac{k_1 S_1 S_6}{1+(S_6/K_1)^q} - k_2 S_2 (N - S_5) - k_6 S_2 S_5,\\\\\n\\frac{dS_3}{dt} &= k_2 S_2(N - S_5) - k_3 S_3(N - S_6),\\\\\n\\frac{dS_4}{dt} &= k_3 S_3 (A - S_6) - k_4 S_4 S_5 - \\kappa (S_4 - S_7),\\\\\n\\frac{dS_5}{dt} &= k_2 S_2(N - S_5) - k_4 S_4 S_5 - k_6 S_2 S_5,\\\\\n\\frac{dS_6}{dt} &= -2 \\frac{k_1 S_1 S_6}{1+(S_6/K_1)^q} + 2 k_3 S_3(A - S_6) - k_5 S_6,\\\\\n\\frac{dS_7}{dt} &= \\psi \\kappa (S_4 - S_7) - k S_7.\n\\end{aligned}$  (2.1)\nData from the simulation are collected from t = 0 to t = 10 with a time-step size of $\\Delta t = 0.01$. We employ a DNN with one hidden layer and 256 neurons to represent the nonlinear dynamics. As for the multi-step scheme, we use Adams-Moulton with M = 1 steps. Upon training the DNN, we solve the identified system using the same initial condition as the ones used for the exact system. As shown in [Raissi et al. (2018)], the learned system correctly captures the form of the dynamics. Here, we use symbolic regression, a method that merges genetic programming with machine learning, to identify mathematical expressions that closely align with our dataset. Initially, this method involves creating a varied pool of potential equations, each expressed mathematically using basic operations (e.g., addition, subtraction, multiplication, and division) and various functions (like addition, sub-traction, multiplication, and division). The adequacy of each equation is assessed against the data using a specific fitness function, generally based on the mean squared error (MSE). Through the use of genetic algorithms, the most effective equations are carried forward, undergoing genetic processes such as crossover (mixing elements of two equations) and mutation (altering parts of an equation) to generate new candidates. This cycle of generation and refinement proceeds until certain criteria are met, like reaching a pre-defined number of generations or achieving a particular level of fitness. The results, including comparisons of these equations with the exact right-hand side equations of the ODEs as seen in Eq. 2.1, are displayed in Table 1. The notably low relative errors indicate that the system learned through Symbolic Regression successfully mirrors the system's inherent dynamics.\nWe opted for PySR package proposed by [Cranmer (2023)] over gplearn as PySR has proven to be a more robust and efficient framework, as discussed in [Ahmadi Daryakenari et al. (2023)]."}, {"title": "Outlook", "content": "PINNs have been used so far all across the scientific and engineering fields, from geophysics and astrophysics to engineering design, digital twins, computational mechanics, biomedical engineering and even in quantitative pharmacology as shown in the example above. Compared to finite elements that it took several decades to be adopted as a mainstream computational tool, PINNs have already been adopted by the industry as they remove the \"tyranny\" of mesh generation, they blend seamlessly data and physics, and can even discover new governing equations from data as in the aforementioned example. However, there are still many open issues to be resolved, including the low-accuracy compared to high-order numerical methods, the computational cost, which is excessive especially"}]}