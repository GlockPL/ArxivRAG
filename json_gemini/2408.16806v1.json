{"title": "Physics-Informed Neural Networks and Extensions", "authors": ["Maziar Raissi", "Paris Perdikaris", "Nazanin Ahmadi", "George Em Karniadakis"], "abstract": "In this paper, we review the new method Physics-Informed Neural Networks (PINNs) that has become\nthe main pillar in scientific machine learning, we present recent practical extensions, and provide a specific\nexample in data-driven discovery of governing differential equations.", "sections": [{"title": "1 Overview", "content": ""}, {"title": "1.1 Physics-Informed Neural Networks - PINNS", "content": "In the last 50 years there has been a tremendous success in solving numerically PDEs using finite\nelements, spectral, and even meshless methods. Yet, in many real cases, we still cannot incorporate\nseamlessly (multi-fidelity) data into existing algorithms, and for industrial-complexity applications\nthe mesh generation is time consuming and still an art. Moreover, solving inverse problems, e.g.,\nfor material properties, is often prohibitively expensive and requires different formulations and new\ncomputer codes. In recent years, uncertainty quantification (UQ) of simulations has led to highly\nparametrized formulations that may include 100s of uncertain parameters for complex problems ren-\ndering such computations infeasible in practice. Finally, existing computer programs for engineering\napplications have more than 100,000 lines of code, making it almost impossible to maintain and up-\ndate them from one generation to the next. To this end, physics-informed learning, i.e., integrating\nseamlessly data and mathematical models, and implementing them using physics-informed neural\nnetworks (PINNs) [Raissi et al. (2017a)], [Raissi et al. (2017b)], [Raissi et al. (2019)] is a paradigm\nshift in defining the main thrust in scientific machine learning (SciML).\nThe specific data-driven approach to modeling physical systems depends crucially on the\namount of data available as well as on the complexity of the system itself, as illustrated in Fig. 1.\nThe classical paradigm is shown on the top of Fig. 1, where we assume that the only data available\nare the boundary conditions (BC) and initial conditions (IC) while the specific governing partial\ndifferential equations (PDEs) and related parameters are precisely known. On the other extreme\n(lower plot), we may have a lot of data, e.g. in the form of time series, but we may not know the"}, {"title": "1.2 Extensions of PINNS", "content": "Adaptive Weights: The original PINNs used fixed weights in front of the various terms in the loss\nfunctions, requiring manual tuning. In [Wang et al. (2022)], the authors used the Neural Tangent\nKernel (NTK) for PINNs, which is a kernel that captures the behavior of neural networks in the\ninfinite width limit during training via gradient descent. They proposed a novel gradient descent\nalgorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the loss weights and\naccelerate reduction of the total training error. An even more effective method was proposed in\n[McClenny and Braga-Neto. (2020)] for multiscale systems and \u201cstiff\" PDEs. The loss weights are\nfully trainable and applied to each training point individually, so the neural network learns which\nregions of the solution are stiff and focuses on them. The basic idea is to make the weights increase as\nthe corresponding losses increase, which is accomplished by training the network to simultaneously\nminimize the losses and maximize the weights (a min-max problem). A variation of this idea that\navoids computing the gradients but instead uses the residuals was proposed in [Anag. et al. (2023)].\nDomain Decomposition: Another approach to tackle multiscale problems with PINNS\nis to combine them with domain decomposition methods as was done in [Jagtap et al. (2020),\nJagtap et al. (2021)].In the first paper, domain decomposition was introduced for conservation laws,"}, {"title": "2 Data-Driven Discovery of Dynamical Systems", "content": "Next, we demonstrate a PINN-inspired approach for identifying nonlinear dynamical systems from\ndata. Specifically, we blend classical tools from numerical analysis, namely the multi-step time-\nstepping schemes, with deep neural networks, to distill the mechanisms that govern the evolu-\ntion of a given dataset. We test the effectiveness of our approach for a biomedical application\nexample but more cases can be found in [Raissi et al. (2018)]. In particular, we study the gly-\ncolytic oscillator model as an example of complicated nonlinear dynamics typical of biological sys-\ntems, and subsequently use symbolic regression to obtain the equations explicitly in analytical\nform. Let us consider nonlinear dynamical systems of the form: $\\dot{x}(t) = f (x(t))$, where the\nvector $x(t) \\in \\mathbb{R}^D$ denotes the state of the system at time $t$ and the function $f$ describes\nthe evolution of the system. Given noisy measurements of the state $x(t)$ of the system at several\ntime instances $t_1, t_2,...,t_n$, our goal is to determine the function $f$ and consequently discover\nthe underlying dynamical system from data. We proceed by applying the general form of a lin-\near multistep method with $M$ steps to equation and obtain $\\sum_{m=0}^{M} [a_m x_{n-m} + \\Delta t \\beta_m f(x_{n-m})] =$\n$0$, $n = M,..., N$. Here, $x_{n-m}$ denotes the state of the system $(t_{n-m})$ at time $t_{n-m}$. Dif-\nferent choices for the parameters $a_m$ and $\\beta_m$ result in specific schemes. We proceed by plac-\ning a neural network prior on the function $f$. The parameters of this neural network can be\nlearned by minimizing the mean squared error loss function $MSE := \\frac{1}{N-M+1} \\sum_{n=M}^{N} |y_n|^2$, where\n$y_n := \\sum_{m=0}^{M} [a_m x_{n-m} + \\Delta t \\beta_m f(x_{n-m})]$, $n = M, ..., N$, is obtained from the multistep scheme.\nAs an example of complicated nonlinear dynamics typical of biological systems, we simulate the\nglycolytic oscillator model presented in [Daniels and Ilya. (2015)]. The model consists of ordinary\ndifferential equations for the concentrations of 7 biochemical species, see [Raissi et al. (2018)]. The"}, {"title": "3 Outlook", "content": "PINNs have been used so far all across the scientific and engineering fields, from geophysics and\nastrophysics to engineering design, digital twins, computational mechanics, biomedical engineering\nand even in quantitative pharmacology as shown in the example above. Compared to finite elements\nthat it took several decades to be adopted as a mainstream computational tool, PINNs have already\nbeen adopted by the industry as they remove the \"tyranny\" of mesh generation, they blend seamlessly\ndata and physics, and can even discover new governing equations from data as in the aforementioned\nexample. However, there are still many open issues to be resolved, including the low-accuracy\ncompared to high-order numerical methods, the computational cost, which is excessive especially"}]}