{"title": "A Privacy-Preserving Domain Adversarial Federated learning for\nmulti-site brain functional connectivity analysis", "authors": ["Yipu Zhang", "Likai Wang", "Kuan-Jui Su", "Aiying Zhang", "Hao Zhu", "Xiaowen Liu", "Hui Shen", "Vince D. Calhoun", "Yuping Wang", "Hongwen Deng"], "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) and its derived functional\nconnectivity networks (FCNs) have become critical for understanding neurological disorders.\nHowever, collaborative analyses and the generalizability of models still face significant challenges\ndue to privacy regulations and the non-IID (non-independent and identically distributed) property\nof multiple data sources. To mitigate these difficulties, we propose Domain Adversarial Federated\nLearning (DAFed), a novel federated deep learning framework specifically designed for non-IID\nfMRI data analysis in multi-site settings. DAFed addresses these challenges through feature\ndisentanglement, decomposing the latent feature space into domain-invariant and domain-specific\ncomponents, to ensure robust global learning while preserving local data specificity. Furthermore,\nadversarial training facilitates effective knowledge transfer between labeled and unlabeled datasets,\nwhile a contrastive learning module enhances the global representation of domain-invariant\nfeatures. We evaluated DAFed on the diagnosis of autism spectrum disorder (ASD) and further\nvalidated its generalizability in the classification of Alzheimer's disease (AD), demonstrating its\nsuperior classification accuracy compared to state-of-the-art methods. Additionally, an enhanced\nScore-CAM module identifies key brain regions and functional connectivity significantly\nassociated with ASD and mild cognitive impairment (MCI), respectively, uncovering shared\nneurobiological patterns across sites. These findings highlight the potential of DAFed to advance\nmulti-site collaborative research in neuroimaging while protecting data confidentiality.", "sections": [{"title": "Introduction", "content": "Resting-state functional magnetic resonance imaging (rs-fMRI) has emerged as a powerful and\nnon-invasive technique for detecting abnormal brain activity [1]. Functional connectivity networks\n(FCNs), derived from rs-fMRI data, quantify temporal correlations between functional interactions\nin different brain regions, which are extensively utilized in studies of neurological disorders and\nmental illnesses [2, 3].\nRecently, deep learning approaches have shown remarkable potential in analyzing fMRI data\nand FCNs, enabling significant breakthroughs in understanding brain function [4, 5]. Despite\nsignificant advancements in deep learning models, concerns over patient privacy and legal\nrestrictions limit data sharing across institutions. This limitation poses challenges to the\nreproducibility and generalizability of data-driven approaches across diverse datasets [6, 7]. A\nprimary factor contributing to these challenges is data heterogeneity, which arises from various\nsources, such as differences in MRI scanner hardware, imaging acquisition protocols, and regional\ndisparities in data samples [8, 9], leading to non-IID (non-independent and identically distributed)\ndata. Although methods such as ComBat can correct batch effects [10, 77], Although methods\nsuch as ComBat can correct batch effects [10, 77], ComBat requires consistent feature dimensions\nacross different sites and cannot address the data heterogeneity arising from observational\ndiscrepancies across multiple data sources.\nOn the other hand, federated learning (FL) is a decentralized machine learning approach that\nfacilitates collaborative model training while ensuring data remains localized at its originating site.\nWidely adopted in multi-site neuroimaging research [11, 78, 79], FL enables sites to train models\nindependently on their local data and contribute to a shared global model by exchanging only\nmodel parameters, preserving data privacy and security.\nSome recent studies have attempted to address the non-IID issue in the FL framework. For\nexample, FedProx introduced a proximal term into the optimization objective at each site,\nconstraining the distance between local and global models [12]. Similarly, FedMA addressed the\nproblem of randomized parameter arrangements in multi-center non-IID data using a Bayesian\nnon-parametric approach for hierarchical matching and fusion of gradient information from\nlayered network models [13]. Yao et al. proposed an unbiased gradient aggregation algorithm that\nutilized keep-trace gradient descent along with a gradient evaluation strategy [14]. In addition,\nFedBN introduced an effective method that employed local batch normalization to alleviate feature\nshifts before averaging models [15]. However, most of these methods rely on batch normalization\nlocally, which often disrupts the underlying spatial and temporal relationships within the data and\nis not well-suited for high-dimensional fMRI feature learning [15, 16] . Moreover, these techniques\nare primarily designed for labeled datasets and fails to address practical challenges, such as the\nimbalanced sample sizes and unlabeled data commonly encountered in clinical datasets [17].\nTo address these issues, we draw inspirations from domain adaptation (DA) and use the concept\nof domain adversarial training within the federated learning framework [18], developing a novel\nDomain Adversarial Federated learning framework (DAFed) to address the differences in feature\ndistribution across multiple fMRI connectivity datasets. Specifically, we design a feature extractor\nto learn the spatial-temporal characteristics of fMRI data within a latent space. Then we utilize\nfeature disentanglement technique to decompose the features into domain-invariant and domain-\nspecific components [19]. The domain-invariant component captures the common features across\nall sites. They are collaboratively updated through federated learning, and the domain-specific\ncomponent retains each dataset's unique, localized information. By employing a multi-head\nattention mechanism to integrate these two components, the classification accuracy for each\ndataset can be effectively enhanced. Moreover, by incorporating objective loss into the parameter\ntransmission process during federated learning, adversarial training facilitates the transfer of\ninformation learned from labeled to unlabeled data. In addition, we introduce contrastive learning\nto strengthen the similarity of both global domain-invariant features across multiple sites and the\ndomain-invariant features unique to each site. To evaluate the effectiveness of our method, we\ntested DAFed on the multi-site cohort Autism Brain Imaging Data Exchange (ABIDE) [20] and\nfurther validated its generalizability using the multi-device dataset from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI). Our results demonstrate that DAFed outperforms many deep\nlearning methods and recently proposed federated learning approaches resulting in higher\nclassification accuracy. Furthermore, using an improved Score - class activation map (Score-CAM)\nmodule, we identify common key brain regions and functional connections across multiple sites\nthat are significantly associated with ASD and AD. This collaborative analysis highlights shared\nneurobiological patterns across datasets, enhancing our understanding of these brain disorders.\nThe primary contributions of this paper can be summarized as follows:\n\u2022\tWe propose the DAFed, an end-to-end federated deep learning framework designed for\nanalyzing non-IID fMRI data collected from diverse scanners or institutions without the\nneed for data sharing.\n\u2022\tWe develop a feature extractor specifically designed to capture the spatial-temporal\ncharacteristics of fMRI data, effectively utilizing the dynamic FCN and its network\nstructure information.\n\u2022\tWe design a deep learning network that integrates feature disentanglement with domain\nadversarial training and embedded it within the FL framework, facilitating collaborative\nlearning across multiple labeled and unlabeled datasets.\n\u2022\tWe incorporate contrastive learning into the learning process of domain-invariant\ncomponent across multiple sites, further enhancing the similarity between the domain-\ninvariant component of the global FL model and the local models of individual sites.\n\u2022\tWe introduce an optimized Score-CAM mechanism to mitigate the impact of noisy\ngradients from privacy-preserving noise in federated learning, improving model\ninterpretability and advancing biomarker discovery."}, {"title": "Methodology", "content": "Consider collaboratively training a global model from K sites (clients) using their respective\ndatasets {X1, ..., Xk}, where each dataset Xk (k = 1, ..., K) shares the same feature space and label\nwhile differs in samples and data preprocessing protocols. As a result, the feature distribution of\neach dataset is non-IID. In this setup, the labeled datasets are referred to as the source domains,\nand the unlabeled datasets are the target domains. We aim to train an efficient model in the\nfederated learning setting without sharing data.\nFor clarity, let the source domain site be Xsource and target domain site be Xtarget as follows:\nXsource = {(xi, Yi)}=1~Psource\nXtarget = {(xi)}=1~Ptarget\nwhere n and n' represent the number of samples in Xsource and Xtarget, with n + n' = N. The variables\nxi and yi denote the features and labels of the i-th sample, respectively, where Psource and Ptarget\nare the probability distributions followed by Xsource and Xtarget, respectively. In the later section, we\nwill extend such a two-site model to a multi-site federated learning framework."}, {"title": "A. Spatial-Temporal Feature Generator", "content": "Functional connectivity, measured by Pearson's correlation, is derived from fMRI time series\nand forms a graph representation. This graph has been widely utilized to study connectivity\npatterns in the human brain. To capture rich spatial-temporal information from FC data, graph\nconvolutional networks (GCNs) are employed for feature extraction at the outset of the proposed\nframework"}, {"title": "B. Representation Disentanglement", "content": "To address the feature distribution shift in multi-site datasets, a key hypothesis is that each\ndataset consists of both a domain-invariant component (fdi) and a domain-specific component (fds).\nHere, we utilize a feature disentangler D to decompose the embedding feature Z into two\ncomponents: fdi = Ddi(Z) and fds = Das(Z). We minimize the mutual information between fdi and fds\nto enhance this disentanglement, which is defined as follows:\nI(fdi, fas) = \\iint p(fdi, fas) log \\frac{p(fdi, fas)}{p(fdi)p(fas)} dfaidfds\nwhere p(fdi, fds) is the joint probability distribution of fai and fds, and p(fdi), p(fds) are their respective\nmarginal distributions. Although mutual information is a measure for capturing dependencies\nacross distributions, it is only tractable for discrete variables. To estimate the mutual information\nfor continuous variables, we use the Mutual Information Neural Estimator (MINE), which employs\na neural network and Monte Carlo integration [23]. The mutual information loss is defined as\nfollows:\nLMI (fdi, fds) = \\frac{1}{n} \\sum_{i=1}^{n} T_{\\phi}(p, q) - log(\\frac{1}{n} \\sum_{i=1}^{n} e^{T_{\\phi}(p'q')})\nwhere (p, q) are sampled from the joint distribution p(fdi, fds), and (p', q') are sampled independently\nfrom the product of marginals p(fdi)p(fds). The neural network To, parameterized by \u03c6,\napproximates the mutual information between fdi and fds."}, {"title": "C. Domain Adversarial Training", "content": "To address the non-IID issue between the source and target domains, we utilize a domain\nadversarial neural network. It aligns the source domain with the heterogeneous target domain via\nadversarial training on the domain-invariant component and preserves the unique information\nwithin the local domain-specific component. Domain adversarial training is achieved by two\ndiscriminators. The classifier (C) is trained on the source domain to evaluate model performance,\nand the domain identifier (DI) is used to determine the origin of the features. The loss of DI is\ndefined as follows:\nLDI = \\frac{1}{N} \\sum_{i=1}^{N} dilog(\\frac{1}{DI ((Dai(G(xi)))}) + (1 - di)log\\frac{1}{1-DI ((Dai(G(xi)))},\nwhere di is a binary domain indicator for the i-th sample. Specifically, d\u2081 = 0 indicates that xi come\nfrom the source distribution xi ~Psource, and d\u2081 = 1 indicates it comes from the target distribution\nXi~IP\ntarget.\nTo enhance the performance of the classifier C, we integrate the domain-invariant and domain-\nspecific components to form a unified feature. This approach retains the shared features across the\ntwo datasets while preserving their unique characteristics by employing an improved multi-head\nself-attention mechanism [24]:\nfF = Softmax(\\frac{QKT}{\\sqrt{d_{k}}}) V"}, {"title": "D. Multi-site Federated Learning", "content": "Building on the Domain Adversarial Training described in Section II-C, we extend this model\nto a multi-site federated learning"}, {"title": "E. Contrastive Learning Module", "content": "Our proposed model aims to learn the domain-invariant component shared across both source\nand target domains and retain the domain-specific component within each domain. Recent\nadvances in contrastive learning provide a new approach to enhance feature similarity by\nminimizing the distance between positive samples and maximizing the distance between negative\nsamples. Inspired by this, we incorporate contrastive learning by aggregating the current local\nmodel with both the previous local and global models, further refining the domain-invariant\ncomponent across all sites.\nSpecifically, at each site, we define the domain-invariant feature learned using the local\nparameters at the t-th iteration and the domain-invariant feature learned using the global\nparameters at the (t \u2212 1)-th iteration as a positive sample pair. Conversely, the domain-invariant\nfeature learned using the local parameters at the t-th iteration and the domain-invariant features\nlearned using the local parameters during the previous (t \u2013 v) iterations are defined as negative\nsample pairs. Fig. 4 illustrates the process of generating both positive and negative samples in the\ncontrastive learning module.\nOverall, our goal is to make the global domain-invariant component obtained via federated\nlearning to be more similar to the domain-invariant component learned at each local site. The\ncontrastive loss is defined as follows:\nLCL = -\\frac{1}{n} \\sum_{i=1}^{n} log \\frac{sim(vi,viglobal)}{\\frac{sim(vi,viglobal)}{\\tau} + \\sum_{i=1}^{v} \\frac{sim(-)}{\\tau}}\nwhere sim() represents the cosine similarity function, denotes the temperature parameter, and t\nis the current iteration round. The domain-invariant component Viglobal is the i-th sample learned\nby the global parameters in the (t \u2212 1) iteration,\nViglobal = [Ddiglobal (Gglobal(xi))](t-1)"}, {"title": "F. Model Interpretation", "content": "In Section II-D, to ensure privacy protection, we add random noise during the parameter\ntransmission process, which results in noisy gradients. This noise interference directly impacts\ngradient-based interpretation methods in deep learning models [25, 26], such as Grad-CAM. To\naddress this issue, we apply a Score-CAM method to models without a global pooling layer and\noperates independently of gradient information. We design an improved Score-CAM approach to\nreduce the influence of noise, enhancing the interpretability of the model, with which the most\ndiscriminative regions of interest (ROIs) can be identified. As illustrated in Fig. 5, we improve\nScore-CAM by replacing CNN-based convolutions with GCNs and incorporating the proposed\nmodel to compute the CIC score. Finally, the activation maps are generated, highlighting\nsignificant features relevant to the classification task."}, {"title": "Experiments and results", "content": "We validate our method on two public datasets, e.g., ABIDE and ADNI-3 [20, 21]. The ABIDE\ndataset gathers brain neuroimaging data from multiple institutions to enhance our understanding\nof the neural mechanisms underlying autism. For this study, we focused on resting-state fMRI data\nfrom the four sites: UM_1, NYU, USM, and UCLA_1. Subjects were selected based on the\ncompleteness of the time series data for each ROI. In total, 370 screened subjects were included\nacross these four sites: NYU contributed 73 ASD cases and 94 normal controls (NC), UM_1 has\n43 ASD and 45 NC, USM contributed 33 ASD and 19 NC, and UCLA_1 contributed 37 ASD and\n26 NC. To further assess the consistency of our method across different MRI scanners, we used\nresting-state fMRI data from the ADNI-3 study, collected with three distinct MRI scanners: 3.0T\nGE, 3.0T Siemens, and 3.0T Philips. These three scanners provided a total of 844 screened subjects,\nwith the Philips scanner contributing 111 MCI and 62 NC, the Siemens scanner contributing 129\nMCI and 159 NC, and the GE scanner contributing 186 MCI and 197 NC. Data from each site or\nscanner were treated as independent datasets with no data sharing. In addition, due to a lack of\nsufficient data, we applied sliding windows to truncate the raw fMRI time series, following the\nexperimental setup in [6]. The data characteristics and MRI scanner parameters for both the\nABIDE and ADNI-3 datasets are summarized in Table 1 and Table 2, respectively.\nThe task performed on the ABIDE dataset was to classify subjects as ASD and NC. The raw\nfMRI images were preprocessed using the CPAC pipeline [27]. Primary steps included band-pass\nfiltering (0.01 - 0.1 Hz) without global signal regression, and parcellation into 111 regions of\ninterest (ROIs) using the Harvard-Oxford (HO) atlas. After preprocessing, we computed Pearson's\ncorrelation matrix from the slicing time series of ROI using a sliding window (window size = 20)\nto capture dynamic functional connectivity. Then the Fisher transformation is applied to Pearson's\ncorrelation matrix, and the feature matrix has dimensions of 111 \u00d7 111 for each sample [28-30].\nThe task we performed on the ADNI datasets was to identify MCI and normal control NC. All\nthe resting-state fMRI (rs-fMRI) data from ADNI-3 were preprocessed using DPARSF [31].\nBriefly, the preprocessing steps were as follows: the first 10 volumes of the functional images\nwere discarded to ensure magnetization equilibrium. slice timing and head motion correction were\nthen performed followed by normalization to the Montreal Neurological Institute (MNI) template\nand resampling to an isotropic voxel size of 3 mm. Additionally, spatial smoothing was conducted\nusing a 4-mm full-width at half maximum (FWHM) Gaussian kernel. Detrending and bandpass\nfiltering (0.01-0.1 Hz) were then applied, after which we regressed out covariates, including the\nsix head motion parameters, white matter (WM), cerebrospinal fluid (CSF), and global signal, to\nminimize the influence of these confounding signals. Finally, time series data were extracted from\nbrain regions based on the Anatomical Automatic Labeling (AAL) atlas, which included 116 ROIs.\nThe functional connectivity of each sample was calculated in the same way as in ABIDE.\nThe architecture of our proposed DAFed method is shown in Table 3. Specifically, we\nconfigured the number of graph convolution layers to 4. The number of heads in the self-attention\nmechanism was set to 8, enabling the model to capture diverse information across layers. In the\ncontrastive learning module, the queue length was fixed at 5 to increase the number of negative\nsamples."}, {"title": "Results", "content": "In the experiments, we utilized both the ABIDE and ADNI datasets to evaluate the classification\nperformance of our proposed DAFed in comparison to several competing methods. The\nclassification task for the ABIDE dataset focused on distinguishing between ASD and NC, while\nfor the ADNI dataset, the task involved distinguishing between MCI and NC, with NC considered\nthe positive class in both cases. The competing methods were divided into two categories: non-\nfederated learning approaches, such as MLP and GCN [35], and federated learning-based approach,\nincluding Fed-Avg [16], Fed-MoE [6], Fed-Align [6], and FedCL [9]. Additionally, to evaluate\nthe performance of the proposed method in domain adaptation, we employed two strategies: (1)\nwithout target domain labels (DAFed_U) and (2) with target domain labels (DAFed_L). A 5-fold\ncross-validation (CV) approach was used to evaluate the classification accuracy of all models.\nTable 4 summarizes the classification performance of comparative methods on the ABIDE\ndataset. The results show that the two non-federated learning methods achieved relatively low\naverage accuracy, with 0.609 for MLP and 0.679 for GCN. To better understand whether these\nmethods could capture common domain-invariant features, we utilized t-SNE [36, 82] to visualize\nthe latent space embedded by the first fully connected layer of MLP and GCN, as shown in Fig.\n6a and 6b. The visualizations show that these non-federated learning models failed to learn robust\ncommon domain-invariant features, as features from each site are independently distributed.\nFor federated learning-based methods, the average accuracy scores were 0.669 for Fed-Avg,\n0.672 for Fed-MoE, 0.689 for Fed-Align, and 0.707 for FedCL. The t-SNE visualizations in Fig.\n6c to 6f further highlight these differences. Fed-Avg partially captured common domain-invariant\nfeatures, but the features still formed site-specific clusters. Fed-MoE did not effectively learn\ncommon domain-invariant features, it performed better than non-federated learning models, with\nfeatures from different sites interwoven within the same latent space. Fed-Align and FedCL\nimproved by successfully learning common domain-invariant features with site-specific features\ndispersed throughout the latent space. Since both non-federated learning methods have achieved\nbetter results on USM, and the feature distribution of USM appears relatively compact in Figures\n6a and 6b, we designate USM as the source domain in DAFed. As shown in Table 4, our method\nachieves the best performance across all four datasets. The DAFed_L utilizing label information\nobtains the highest average accuracy of 0.764, and the unlabeled version DAFed_U achieves the\nsecond-highest average accuracy of 0.753. Moreover, compared to other methods, DAFed also\nachieves a smaller standard deviation, indicating more stable performance. Fig. 6g and 6h further\ndemonstrate that our method learns robust common domain-invariant features and enables domain-\nspecific features to exhibit a certain level of aggregated distribution.\nSimilarly, we compared the performance of these methods on the ADNI-3 dataset, as shown in\nTable 5. Two non-federated learning methods achieved an average classification accuracy of 0.67\nfor MLP and 0.692 for GCN, respectively. After visualization, these two methods produced\ndistinct feature distributions for different scanners, as shown in Fig. 7a and 7b.\nThrough the experimental results on both ABIDE and ADNI-3 datasets, we find that DAfed not\nonly effectively learns robust common domain-invariant features while preserving domain-\nspecific features for each dataset, but also successfully achieves domain adaptation. The model\ntrained on labeled datasets can effectively transfer to the unlabeled data for classification,\nachieving better performance compared to both federated and non-federated supervised learning\nmethods.\nWe conduct ablation studies to investigate the role of four key modules in the proposed method.\n1) Spatial-Temporal Feature Generator (STFG) module. 2) Representation Disentanglement (RD)\nmodule. 3) Domain Adversarial Training (DAT) module and 4) Contrastive Learning (CL) module.\nAs shown in Table 6, we validated the contribution of each component in the DAFed method. The\nresults demonstrate that as modules are progressively removed from the DAFed method, the"}, {"title": "Interpretability Assessment", "content": "To highlight the performance of our improved Score-CAM, we adopted evaluation methods that\nare commonly used by interpretability techniques while extended to graph convolutional networks,\nsuch as Grad-CAM [37] and_Grad-CAM++ [38], for comparison. We first evaluated the\nfaithfulness of the interpretations produced by Score-CAM for the class recognition task [26]. The\nAverage Drop is defined as:\nAverage Drop = \\frac{\\sum_{i=1}^{N} max(0, Y_{i}^{c} - O_{i}^{c})}{Y_{i}^{c}} \u00d7 100\nThe Increase in Confidence (Average Increase) is expressed as:\nAverage Increase = \\sum_{i=1}^{N} \\frac{sign(Y_{i}^{c} < 0_{i}^{c})}{N} \u00d7 100\nwhere Y represents the predicted score for class c on sample i; Of represents the predicted score\nfor class c when the explanation map region is used as input; sign(\u00b7) is an indicator function that\nreturns 1 if the input condition is true.\nTo demonstrate the capability of our interpretability module, we conducted experiments on the\nDAFed method using the ABIDE and ADNI-3 datasets. We reported the evaluation results for the\n4 graph convolutional layers in our model, as the model leverages outputs from multiple graph\nconvolutional layers in the decision-making process. As shown in Table 7, the improved Score-\nCAM outperforms other explainability methods when extended to GCN in most cases. This"}, {"title": "Analysis and Discussion", "content": "In this analysis encompassing multi-site data from two cohorts", "39": "the\nsensorimotor network (SMN)", "40": "the sensorimotor network (SMN), visual network\n(VIS), execution and attention network (EAN), default mode network (DMN), subcortical nuclei\nregions (SBN), and cerebellum (Cer).\nOur method consistently identified ROIs for ASD versus NC classification on ABIDE across\nthe four sites. As shown in Fig. 8, we applied neuroanatomical risk mapping to identify brain\nregions associated with ASD. The shared ROIs, annotated in Fig. 8a, include the Right\nJuxtapositional Lobule Cortex, part of the supplementary motor area, which has been associated\nwith ASD [41", "6,42": ".", "43": ".", "44": "and the Left Amygdala shows\nsignificantly reduced responses to both threat and safe cues in individuals with ASD [45", "46": ".", "42,47,48": ".", "42,49": ".", "42": ".", "social brain\" network, which has been\nreported to be associated with ASD [50": ".", "51": ".", "52": ".", "54": ".", "56": ".", "57": ".", "58": ".", "59": ".", "60": ".", "61": ".", "62": ".", "63": ".", "64": ".", "65": ".", "66": "."}]}