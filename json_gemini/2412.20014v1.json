{"title": "ProtCLIP: Function-Informed Protein Multi-Modal Learning", "authors": ["Hanjing Zhou", "Mingze Yin", "Wei Wu", "Mingyang Li", "Kun Fu", "Jintai Chen", "Jian Wu", "Zheng Wang"], "abstract": "Multi-modality pre-training paradigm that aligns protein sequences and biological descriptions has learned general protein representations and achieved promising performance in various downstream applications. However, these works were still unable to replicate the extraordinary success of language-supervised visual foundation models due to the ineffective usage of aligned protein-text paired data and the lack of an effective function-informed pre-training paradigm. To address these issues, this paper curates a large-scale protein-text paired dataset called ProtAnno with a property-driven sampling strategy, and introduces a novel function-informed protein pre-training paradigm. Specifically, the sampling strategy determines selecting probability based on the sample confidence and property coverage, balancing the data quality and data quantity in face of large-scale noisy data. Furthermore, motivated by significance of the protein specific functional mechanism, the proposed paradigm explicitly model protein static and dynamic functional segments by two segment-wise pre-training objectives, injecting fine-grained information in a function-informed manner. Leveraging all these innovations, we develop ProtCLIP, a multi-modality foundation model that comprehensively represents function-aware protein embeddings. On 22 different protein benchmarks within 5 types, including protein functionality classification, mutation effect prediction, cross-modal transformation, semantic similarity inference and protein-protein interaction prediction, our ProtCLIP consistently achieves SOTA performance, with remarkable improvements of 75% on average in five cross-modal transformation benchmarks, 59.9% in GO-CC and 39.7% in GO-BP protein function prediction. The experimental results verify the extraordinary potential of ProtCLIP serving as the protein multi-modality foundation model.", "sections": [{"title": "1\nIntroduction", "content": "Proteins are essential functional units of cells, responsible for performing a wide range of vital and versatile functions crucial to life. Mirroring the language-supervised pre-training paradigm towards powerful and unified vision representations (Radford et al. 2021; Ramesh et al. 2022; Girdhar et al. 2023; Junnan et al. 2023), previous work has explored in the pre-training of multi-modality Protein Language Models (PLMs) by aligning protein sequences with textual function descriptions to achieve function-centric protein representations (Zhang et al. 2022; Xu et al. 2023; Wu, Chang, and Zou 2024; Yin et al. 2024). However, these works were still unable to replicate the extraordinary success of image-text foundation models, and have shown to discard fine-grained protein functional information (Wu, Chang, and Zou 2024), which results in the suboptimal performance on cross-modal transformation (Wang et al. 2024) and localization prediction (Xu et al. 2023). Literature has summarized that the success of visual foundation models primarily stems from the efficient utilization of large-scale data (Radford et al. 2021; Chen et al. 2024) and a holistic multi-modal pre-training framework (Zhang et al. 2023; Pujin et al. 2023), which points to two inherent obstacles that hinder further progress in multi-modal protein-biotext pre-training: (i) Absence of large-scale datasets and ineffective data usage. Large-scale aligned dataset is an indispensable part of obtaining powerful multi-modality foundation models. However, biotexts describing protein functions are much harder to construct than image captions, as often requiring detailed annotated process including manual review by experts or computational analysis by machines. This highlights the pressing need of large-scale multi-modal datasets containing protein sequences with high-quality functional annotations across multiple attribute domains. Even with large-scale protein-biotext pairs, it is non-trivial to effectively inject biological property information into PLMs during multi-modal pre-training. This is primarily because the machine-analyzed process leads to numerous noisy labels (i.e., less accurate annotations) (Bairoch and Apweiler 2000). Currently, there is still a lack of efficient learning techniques to effectively utilize large-scale proteins with\nnoisy annotations for protein-biotext pre-training. (ii) Lack of a function-informed pre-training paradigm. Unlike the alignment of natural image-text pairs, the understanding of proteins is strongly influenced by their specific functional mechanism, which has been largely neglected by previous research yet. Proteins perform specific biological functions depending on their corresponding functional domains in 3D structural spaces. The amino acids at these active site are contiguous or discrete in 1D protein sequences. In this paper, we introduce the static and dynamic functional segment, new concepts which directly determine the specific protein functions and should be primarily focused during the alignment with biological function descriptions. However, we find existing protein-biotext pre-training works directly take after the original CLIP methodology for coarse-grained alignment, discarding the fine-grained information of protein unique functional mechanism (i.e., static or dynamic functional segments primarily determine protein specific functions and properties), which significantly prevents the better performance of protein-biotext pre-training.\nOur work proposes a step towards constructing a universally applicable protein multi-modality foundation model aligning biological and natural language. We present ProtCLIP, consistently alleviates the aforementioned two intrinsic problems and introduces remarkable innovations in multiple dimensions including the pre-training data, sampling strategy, and multi-modality objectives.\nWe first construct a high-quality protein-biotext paired dataset ProtAnno with sparse version (ProtAnno-S) and dense version (ProtAnno-D), derived from the existing protein function database (Consortium 2019). ProtCLIP employs ProtAnno-D comprising 251.5 million aligned pairs for large-scale protein-biotext pre-training, which is the same order of magnitude as large-scale image-text pre-training. Since there exist some inevitable noisy annotations in ProtAnno-D (caused by machine-annotated bias), we propose a novel property-driven sampling strategy motivated by (Berthelot et al. 2019; Li, Socher, and Hoi 2020). Compared to the vanilla uniformly sampling, the proposed sampling strategy decides the selecting probability based on the sample confidence and property coverage, simultaneously balancing the data quality and data quantity in face of large-scale noisy labels. Furthermore, a function-informed pre-training paradigm is constructed motivated by significance of the protein functional mechanism. Within such paradigm, we utilize CLIP loss (Radford et al. 2021) to inject coarse-grained information, and two segment-wise objectives are designed to capture fine-grained information of the static and dynamic functional segments. Concretely, on the one hand, we design a cross-modality reconstruction module to recover the masked static segments based on knowledge from both modalities. On the other hand, the property prototype is exploited to aggregate dynamic segments in an unsupervised way. The resulting property-grouped dynamic segments are contrasted with property prototypes within the same protein-biotext pair, mitigating the mutual interference across multiple attribute domains.\nEvaluated by extensive experiments, ProtCLIP sets new state-of-the-art on 22 important yet challenging protein\nbenchmarks within five types. For protein classification engineering and mutation effect prediction, the superiority of ProtCLIP in representation learning attributes to incorporation of multi-modal information (e.g., 59.9%/39.7% improvements in Go-CC/GO-BP benchmarks). For cross-modal transformation, ProtCLIP surpasses baselines by a significant margin (75% improvement). For semantic similarity inference and protein-protein interaction prediction, ProtCLIP ranks the best, which verifies effectiveness of the proposed data-efficient and function-informed multi-modal learning."}, {"title": "2 Methods", "content": "In this section, we first describe the curated multi-modal dataset, ProtAnno, and the property-driven sampling strategy to enhance data usage effectiveness. Next, we introduce the model architectures and our novel function-informed pre-training paradigm, which incorporates holistic multi-modal pre-training objectives to capture both coarse-grained and fine-grained information. Finally, we summarize the overall loss function used for protein-biotext pre-training."}, {"title": "2.1 Pre-training data", "content": "Dataset Curation To enable pre-training of the protein multi-modality foundation model aligning biological and natural language, it is essential to build dataset containing large-scale pairs of protein sequences and textual property descriptions. Our pre-training data is sourced from SwissProt and trEMBL (Bairoch and Apweiler 2000), containing proteins with textual descriptions. We align protein sequences with meticulously selected properties to curate ProtAnno, which is available in sparse version (ProtAnno-S) and dense version (ProtAnno-D). ProtAnno-S includes 0.5 million manually reviewed protein-biotext pairs with higher annotation quality, whereas ProtAnno-D comprises 251.5 million mostly computationally analyzed protein-biotext pairs which are less accurate due to the machine-annotated\nbias. To gain more insights into the dataset, we conduct extensive quantitative analyses, and display the compositional structure of ProtAnno with varying confidence C and property coverage R in Table 1 and Figure 1. Details about selected properties for alignment and the data format are further illustrated in Appendix A."}, {"title": "Property-driven Sampling Strategy", "content": "For protein-biotext pre-training, most prior works only used scarce proteins with manually reviewed annotations (equivalent to ProtAnno-S), and the attempt to incorporate plentiful computationally analyzed proteins (equivalent to ProtAnno-D) has been unsuccessful, declaring \"data quality could be more important than data quantity.\" (Xu et al. 2023). However, we question and rethink this issue, and propose the property-driven sampling strategy which integrate the merits of the multi-modality data quality and data quantity. Specifically, the main considerations for sampling probability are sample confidence C and property coverage R and data size N. Note that the smaller the confidence, the more reliable the entry is, and C\u2208 {1,2,3,4,5}, R \u2208 {1/4, 2/4,3/4, 4/4}. Initially, we discard machine-annotated entries with C = 4,5 (less accurate) and R = 1/4, 2/4 (low coverage) for comprehensive property understanding. Next, rather than uniform sampling, we explicitly build the sampling distribution according to the aforementioned three factors. The likelihood of selecting protein entries from cluster u with {Cu, Ru, Nu} during multi-modality pre-training is defined as:\n$$P = \\frac{C_u^{-3}\\sqrt{R_u} N_u}{\\Sigma_{i,j,k} C_i^{-3}\\sqrt{R_j} N_k}$$\nIn this paper, we perform large-scale protein-biotext pre-training exploiting ProtAnno-D, in conjunction with the proposed property-driven sampling strategy."}, {"title": "2.2 Model Architecture", "content": "The overview of our framework is displayed in Figure 3, which contains a protein encoder and a biotext encoder. The protein encoder is a protein language model for learning biological features from protein sequences and we use pre-trained ESM-2-650M (Lin et al. 2023) here. The biotext encoder is a text language model for learning linguistic features from biotext descriptions and we use PubMed-BERT (Gu et al. 2021) here. Initialization with these two pre-trained large models significantly facilitates pre-training process by providing decent representations in the early stage of training."}, {"title": "2.3 Function-informed Pre-training Paradigm", "content": "To accomplish the holistic function-informed multi-modal pre-training, we jointly optimize four protein-biotext pre-training objectives, with two classic ones and two newly proposed segment-wise ones, customized for learning locality-aware and fine-grained information of protein specific functional mechanism."}, {"title": "Global Contrastive Loss", "content": "Global Contrastive loss (GC) learning aligns representations of two modalities by encouraging positive pairs to have higher similarity in contrast to the negative pairs. Considering the effectiveness of LGC for multi-modal understanding in many previous works (Radford et al. 2021; Junnan et al. 2023; Su et al. 2022) from different domains, we perform it to realize global alignment of protein-biotext. Given a batch of sequence-text pairs {(Si, Ti)}K1, LGC is composed of two symmetric standard InfoNCE loss:\n$$L_{GC} = \\frac{1}{2}E_{p(S,T)} (log\\frac{exp(sim(S_i, T_i)/\\tau_1)}{\\Sigma_{j=1}^K exp(sim(S_i, T_j)/\\tau_1)}\n+ E_{p(S,T)} (log\\frac{exp(sim(T_i, S_i)/\\tau_1)}{\\Sigma_{j=1}^K exp(sim(T_i, S_j)/\\tau_1)})\n$$\nwhere sim(\u00b7,\u00b7) is the consine similarity and \u03c41 denotes the temperature parameter that controls the softmax distribution."}, {"title": "Biotext-guided Static Segment Reconstruction (BSR)", "content": "Given the global contrastive objective modeling coarse-grained information, the fine-grained information of static and dynamic segments are ubiquitous, which primarily determines protein specific functions and properties. To capture such locality-aware information of static segments, we propose Biotext-guided Static segment Reconstruction (BSR) to reconstruct corrupted static segments using information from both modalities. Specifically, given a sequence of protein residues S = {x1,x2,..., xn}, we sample l consecutive tokens as a static segment at a time, until the total sampling length reaches 15% of S. In other words, we execute sampling iterations to prepare a random set of static segments {e1, e2,..., em} with ei \u2208 S for subsequent masking and reconstruction. At each iteration, we randomly select the starting point of each segment and its length I follows a discrete uniform distribution between 5 and 10. Note that all static segments are non-overlapping and their total length accounts for 15% of S."}, {"title": "Property-grouped Dynamic Segment Alignment (PDA)", "content": "To capture the fine-grained information of dynamic segments, we propose Property-grouped Dynamic Segment Alignment (PDA), optimizing the alignment between property-grouped dynamic segments and corresponding property descriptions.\nSpecifically, a prototype memory bank is constructed to approximate property descriptive sentences, without any need to accurately retain redundant information such as syntax.\nThen the property prototype is exploited to aggregate dynamic segments in an unsupervised way, which are more flexible than static segments in BSR. Provided property description prototypes of biotext T = {a1, a2, a3, a4} and the corresponding sequence of residues S = {x1,x2,..., xn}, we first compute similarity weights as:\n$$W_{ij} = a_i \\cdot x_j, i = 1, 2, 3, 4, j = 1, 2, ..., n$$\nwhere Wij \u2208 R and \u00b7 is the inner product. Then min-max normalization is applied along the residue dimension to normalize wij to [0, 1]. After that, some non-functional protein residues are discarded by sparsifying the similarity weights with a threshold \u03b8:\n$$W_{ij} = \\begin{cases} W_{ij}, & \\text{if } W_{ij} \\ge \\theta \\\\ 0, & \\text{otherwise.} \\end{cases}$$\nEventually, we obtain the property-grouped dynamic segments by multiplying similarity weights and protein residues:\nei = {Wijxj | j = 1, 2, . . ., n}, i = 1,2,3,4. (6)\nProperty-grouped dynamic segment alignment is conducted to align these dynamic segments with property descriptions within the same protein- biotext pair, mitigating the mutual interference across multiple attribute domains:\n$$L_{PDA} = \\frac{1}{2} E_{p(e,a)} (log\\frac{exp(sim(e_i, a_i)/\\tau_2)}{\\Sigma_{k}exp(sim(e_i, a_k)/\\tau_2)})\n+ E_{p(e,a)} (log\\frac{exp(sim(a_i, e_i)/\\tau_2)}{\\Sigma_{k}exp(sim(e_i, a_k)/\\tau_2)})$$ (7)\nwhere sim(\u00b7,\u00b7) represents the consine similarity and \u03c42 denotes the temperature parameter that controls the softmax distribution.\nAiming to extract the essential knowledge of protein sequences, we select the most relevant residues based on their similarities to each property description, resulting in segments of variable lengths. Owing to such variable length, dynamic segments are flexible to capture information of consecutive or non-consecutive functional residues, excluding redundant and non-functional ones. Additionally, the threshold \u03b8 directly influences the segment length by determining different number of zero values in each row of the similarity weights, which decouples similarities of individual residues to different property descriptions. In essence, the threshold-ing operation allows for different properties to match different residues that are the most relevant, thereby forming dynamic segments."}, {"title": "2.4 Overall Loss Function", "content": "The overall loss function of ProtCLIP comprises four terms. Global contrastive loss LGC learns coarse-grained information, while biotext-guided static segment reconstruction LBSR and property-grouped dynamic segment alignment LPDA focuses on fine-grained information. And we keep the"}, {"title": "3 Experiments", "content": "In this section, we first introduce some training setups, and then provide configurations and result discussions about five types of downstream applications (Figure 4) on totally 22 benchmarks. Eventually, the analysis of ablation experiments are presented to further validate the effectiveness of our pre-training objectives."}, {"title": "3.1 Training Setups", "content": "We build our codes upon the PyTorch framework and conduct experiments on 64 Tesla V100 GPUs with 10,000 GPU hours. An Adam optimizer is used (learning rate: 1.0\u00d710\u22125, weight decay: 0) to train the model. The batch size is 2048 and 512 for pre-training and downstream experiments. Within the function-informed pre-training paradigm, we set hyper-parameters \u03b8 = 0.3, \u03bb\u2081 = 0.7, \u03bb\u2082 = 0.3."}, {"title": "3.2 Protein Classification Engineering", "content": "Configurations Protein classification engineering aims to classify protein locations and functions. For location classification, we consider two such problems from DeepLoc (Almagro Armenteros et al. 2017), subcellular localization prediction (Sub) with 10 categories and binary localization prediction (Bin) with 2 categories. For function classification, we employ two benchmarks (Gligorijevi\u0107 et al. 2021) namely Enzyme Commission (EC) number prediction and Gene Ontology (GO) term prediction. On GO benchmark, there are three branches that predict molecular function (GO-MF), biological process (GO-BP) and cellular component (GO-CC). The compared baselines include three parts: (a) four traditional protein encoders CNN (Shanehsazzadeh, Belanger, and Dohan 2020), ResNet (Rao et al. 2019), LSTM (Rao et al. 2019), Transformer (Rao et al. 2019); (b) four single-modal PLMs ProtBERT (Elnaggar et al. 2022), OntoProtein (Zhang et al. 2022), ESM-1b (Rives et al. 2021), ESM2 (Lin et al. 2023)); (c) one multi-modal PLM ProtST-ESM2 (Xu et al. 2023). The evaluation metrics are accuracy for location prediction, and AUPR and Fmax for function prediction. AUPR denotes the pair-centric area under precision-recall curve. It computes average precision scores for all protein-biotext pairs, which is exactly the micro-average precision score for the classification problem. Fmax demotes the protein-centric maximum F-score.\nResults Table 2 (left) and Table 3 show that ProtCLIP establishes state-of-the-art results on all six classification\nbenchmarks under both linear probing and full tuning settings. Moreover, ProtCLIP performs best on protein classification engineering among all five type of downstream tasks."}, {"title": "3.3 Mutation Effect Prediction", "content": "Configurations Mutation effect prediction is a regression task that predicts the effect of residue mutations on protein fitness. We utilize B-lactamase (8-lac) landscape from PEER (Xu et al. 2022), Fluorescence (Flu) and Stability (Sta) landscapes from TAPE (Rao et al. 2019), and AAV and Thermostability (Thermo) landscapes from FLIP (Dallago et al. 2021). Following the setup of (Xu et al. 2023), we use \"two vs many\" and \"human cell\" dataset splits for AAV and Thermo, and the split settings of the rest tasks remain default and unchanged. The baselines remain the same as mentioned in Section 3.2. The performance is measured by Spearman's \u03c1. Moreover, we evaluate ProtCLIP and PLMs under both linear probing and full tuning settings on location prediction and mutation effect prediction tasks."}, {"title": "3.4 Cross-modal Transformation", "content": "Configurations Cross-modal transformation matches the transformed embedding with candidates from the target modality, where embeddings from ProtCLIP are transformed by an extra transformation module. Following (Wang et al. 2024), we leverage the raw knowledge graph (KG) data and undertake some preprocessing steps, with the training/validation/test split of 80%/10%/10%. The baselines are BioBridge (Wang et al. 2024) and three knowledge graph embedding methods (ComplEx (Trouillon et al. 2016), DistMult (Yang et al. 2015), RotatE (Sun et al. 2019)). We use mean reciprocal rank (MRR) as the metric.\nResults Table 4 reports our remarkable enhancement over all baselines. The first three baselines are traditional KG encoders trained from scratch, which lack flexibility, while BioBridge cannot fully unleash the potential of PLMs. Instead, ProtCLIP compensates for their shortcomings and incorporates flexibility, data-efficiency and high performance. Particularly, ProtCLIP is 2.4 \u00d7 better than the best baseline for \"Prot2Drug\" and 2 \u00d7 better for \"Prot2BP\" and \"Disease2Prot\", which signals the superiority of ProtCLIP in multimodal understanding."}, {"title": "3.5 Semantic Similarity Inference", "content": "Configurations Semantic similarity inference computes the relevance between predicted and groundtruth similarity matrices (Unsal et al. 2022). our goal is to evaluate the extent to which the encoded protein embeddings can capture biomolecular functional similarity (i.e., BP, CC, MF). The predicted matrix contains pairwise Manhattan Similarities of the encoded protein embeddings, while the groundtruth stores pairwise Lin Similarities of the protein associated BP, MF, and CC. We compare ProtCLIP with three baselines (i.e., ESM2-3B (Lin et al. 2023), KeAP (Zhou et al. 2023), BioBridge (Wang et al. 2024)). The metric is Spearman's \u03c1.\nResults In Table 5 (left), ProtCLIP achieves the best performance over other baselines. In particular, ProtCLIP surpasses the vanilla ESM2-3B by a large margin, demonstrating the proposed data-efficient and function-informed multi-modal learning is generally beneficial to the unimodal PLM."}, {"title": "3.6 Protein-Protein Interaction Prediction", "content": "Configurations Protein-protein interaction (PPI) prediction seeks to classify 7 interaction types of a pair of proteins. Following (Zhang et al. 2022), we extract the protein embeddings with ProtCLIP and baselines, which serve as the input for a graph neural network model to be trained on the PPI network. The baselines remain the same as mentioned in Section 3.5. Additionally, F1 score is reported on SHS27K (Chen et al. 2019), SHS148K (Chen et al. 2019) and STRING (Lv et al. 2021) datasets for evaluation.\nResults Table 5 (right) presents average results on three benchmarks. ProtCLIP performs the best and exceeds the prior state-of-the-art BioBridge owing to its pre-training on the enormous dataset ProtAnno-D with the property-driven sampling strategy."}, {"title": "3.7 Ablation Study", "content": "We conduct extensive ablation experiments from multiple aspects. Unless otherwise specified, ESM-2-150M serves as the protein encoder and we evaluate on three downstream benchmarks from different types in ablation experiments."}, {"title": "Ablation study on Pre-training Data", "content": "As seen in Section 2.1, we curate a new dataset ProtAnno with a property-driven sampling strategy. Table 6 displays comparison of different pre-training data organization. Obviously, single dataset pre-training and pretrain+finetune (first pretrained on machine-annotated data, then fine-tuned on manually-reviewed data) are inferior to the model pre-trained on ProtAnno-D with the proposed sampling strategy. Such phenomenon demonstrates that low-quality data still holds potential value if subjected to elaborate processing and sampling, and ProtAnno strikes a good balance between data quality and data quantity."}, {"title": "Ablation Study on Pre-training Objectives", "content": "Table 7 reports results with full or partial pre-training objectives. We can observe that both PDA and BSR are essential for injecting fine-grained information, and the absence of PDA leads to a more significant drop compared to the lack of BSR. Such results signal the competence of our function-informed paradigm for protein-biotext multi-modal learning."}, {"title": "Ablation Study on Loss Weights", "content": "During the pre-training process, we observe a significant mutual interference between segment-level reconstruction LBSR and token-level reconstruction LMLM. As depicted in Figure 5, the loss curve fluctuate violently without falling if we apply no loss"}, {"title": "4 Related Work", "content": "4.1 Multi-modal Image-Text Pre-training\nIn an effort to overcome the limitations of single-modality learning (Zhou et al. 2024), multi-modal image-text pre-training has been introduced to learn and align visual and textual representations by pre-training the model on large-scale image-text pairs. One of the most representative methods is CLIP (Radford et al. 2021), which has achieved multi-modal alignment through contrastive learning with massive noisy data scrapped from the internet. BLIP-family (Junnan et al. 2022, 2023) has continuously explored image-text unification and model lightweighting. Aiming to reduce the modality gap, LaVIT (Yang et al. 2024) and FDT (Yuxiao et al. 2023) have respectively designed a unified discrete tokenizer to embed visual and textual input. Large-scale image-text pre-training has become a widely used paradigm for learning general vision representations for a wide range of downstream tasks as well as for constructing multi-modality foundation models (Liu et al. 2023a; Li et al. 2023). Despite their impressive performance, previous methods have only learned coarse-grained representations and ignored localized details. Motivated by this, SPARC (Ioana et al. 2024) proposes a fine-grained sequence-wise loss., encoding detailed information in a computationally inexpensive way. FILIP (Yao et al. 2021) has constructed a cross-modal late interaction mechanism to optimize the token-wise maximal"}, {"title": "4.2 Multi-modal Protein-Biotext Pre-training", "content": "Recently, models that jointly pre-train protein sequences and biotext descriptions have gradually drawing the attention of researchers. OntoProtein (Zhang et al. 2022) first incorporates knowledge graphs to enhance protein representation with external biological descriptions. Chroma (Ingraham et al. 2023) conducts text-guided protein backbone editing towards desired properties and functions. Meanwhile, ProtDT (Liu et al. 2023b) is a newly proposed multi-modal framework that aligns the representations of proteins and biotexts, and leverages textual descriptions for protein design. ProtST (Xu et al. 2023) has shown a tremendous performance on exploiting biomedical function annotations to enhance protein sequence understanding. Additionally, a novel multi-modal framework for the accurate prediction of protein functional descriptions in free text format is proposed by (Abdine et al. 2024). BioBridge (Wang et al. 2024) introduces a bridge module to learn transformations between protein, molecule and biotext foundation models. Nevertheless, existing works of protein-biotext alignment primarily exploit the global alignment objective proposed by CLIP (Radford et al. 2021), without utilizing protein specific functional mechanism to fully facilitate fine-grained understanding of protein and biotext."}, {"title": "5 Conclusion", "content": "This paper has accomplished data-efficient and function-informed multi-modal learning of proteins and biotexts. We build the ProtAnno dataset with large-scale aligned protein sequences and functional descriptions. The property-driven sampling strategy is introduced to strike a balance between data quality and data quantity for pre-training, thereby facilitating the effective harnessing of large-scale noisy data. Inspired by the intricate mechanisms of protein functionality, we novelly adopt a function-informed pre-training paradigm with newly proposed segment-wise objectives to explicitly model protein static and dynamic segments. Such paradigm seamlessly integrates multi-modality information from coarse-grained to fine-grained levels, culminating in the holistic function-centric protein representation. We also identified that ProtCLIP achieves the new state-of-the-art results on 22 protein downstream benchmarks. In the future, we envision that ProtCLIP has the potential to serve as the protein multi-modality foundation model to promote controllable protein discovery and optimization in real-world scenarios."}]}