{"title": "Disentangling Preference Representation and Text Generation\nfor Efficient Individual Preference Alignment", "authors": ["Jianfei Zhang", "Jun Bai", "Bei Li", "Yanmeng Wang", "Rumei Li", "Chenghua Lin", "Wenge Rong"], "abstract": "Aligning Large Language Models (LLMs) with\ngeneral human preferences has been proved\ncrucial in improving the interaction quality be-\ntween LLMs and human. However, human val-\nues are inherently diverse among different in-\ndividuals, making it insufficient to align LLMS\nsolely with general preferences. To address\nthis, personalizing LLMs according to indi-\nvidual feedback emerges as a promising solu-\ntion. Nonetheless, this approach presents chal-\nlenges in terms of the efficiency of alignment\nalgorithms. In this work, we introduce a flex-\nible paradigm for individual preference align-\nment. Our method fundamentally improves ef-\nficiency by disentangling preference represen-\ntation from text generation in LLMs. We vali-\ndate our approach across multiple text genera-\ntion tasks and demonstrate that it can produce\naligned quality as well as or better than PEFT-\nbased methods, while reducing additional train-\ning time for each new individual preference by\n80% to 90% in comparison with them.", "sections": [{"title": "1 Introduction", "content": "Aligning Large Language Models (LLMs) with\ngeneral human preferences (or, human feedback),\noften collected from a set of labelers through rel-\native judgments on LLMs' responses, has proven\neffective in enhancing the overall interaction qual-\nity between LLMs and human, such as helpful-\nness and harmlessness (Bai et al., 2022; Ouyang\net al., 2022). However, human preferences are in-\nherently diverse, reflecting differences in gender,\nreligion, politics, culture, and other factors (Kim\net al., 2024a,b; Li et al., 2024c). This diversity\nsuggests that simply aligning LLMs with general\nhuman preferences may be insufficient to meet the\nunique needs of individual users (Hosking et al.,\n2024; Ye et al., 2024). Therefore, there is a growing\nneed for LLMs to adapt to individual preferences.\nOne direct solution to this challenge is to con-\nduct personalization-oriented prompt engineering,"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Preference Alignment", "content": "Preference alignment intends to maximize the ex-\npectation of preferred content generated by LLMs.\nThe preference data are typically collected in forms\nof human judgements on different responses for\nthe same query. Reinforcement Learning from Hu-\nman Feedback (RLHF) (Christiano et al., 2017;\nZiegler et al., 2019) realizes preference alignment\nthrough learning a reward model from preference,\nand optimizing LLMs to maximize the reward ex-\npectation through Proximal Policy Optimization\n(PPO) (Schulman et al., 2017).\nAs a simplified approach with the same op-\ntimums of RLHF, Direct Preference Optimiza-\ntion (DPO) (Rafailov et al., 2023; Wang et al.,\n2024) adopts a contrastive objective that encour-\nages generation of preferred responses and discour-\nages generation of dispreferred responses. Some\nworks realize this by adding prompts to help LLMs\ndistinguish preferred responses from dispreferred\nones (Wang et al., 2023; Liu et al., 2024). Besides,\nsome works select high-reward responses through\nrejection sampling to perform Maximum Likeli-\nhood Estimation (MLE) on LLMs (Dong et al.,\n2023; Touvron et al., 2023).\nIn this work, we apply DPO to latent variables\nthat control the generation process, instead of the\nentire LLMs, so as to offer computation-efficient\nalignment for LLMs."}, {"title": "2.2 Variational Auto-Encoders", "content": "Variational Auto-Encoders (VAEs) (Kingma and\nWelling, 2014; Rezende et al., 2014) are designed"}, {"title": "3 Methodology", "content": "The overview of our method is illustrated in Fig. 2.\nWe introduce step 1 and step 2 in this section."}, {"title": "3.1 Contrastive Language\u2013Latent Pretraining", "content": "Since current LLMs are mostly built in the decoder-\nonly structure (Radford et al., 2019) that lacks ex-"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Tasks and Preferences", "content": "Following previous works (Rafailov et al., 2023;\nRamamurthy et al., 2023), we conduct experiments\non three open-ended text generation tasks:\nText continuation on IMDB (Maas et al., 2011)\nWe employ GPT-2 as the base model, following\nthe task settings in previous works (Rafailov et al.,\n2023; Ramamurthy et al., 2023). We consider three\ntypes of sentiment\u2014positive, negative, and neu-\ntral-to represent different individual preferences."}, {"title": "4.2 Baseline Methods", "content": "Since we mainly aim at improving the efficiency\nof individual preference alignment in LLMs, we\nconsider two widely-used aligning algorithms as\nbaselines, along with two popular PEFT techniques\nas more challenging baselines:\nRLHF (Christiano et al., 2017): Reinforcement\nLearning from Human Feedback (RLHF) is a stan-\ndard alignment method for LLMs. It involves train-\ning a reward model and optimizing the LLMs ac-\ncordingly through Proximal Policy Optimization\n(PPO) (Schulman et al., 2017).\nDPO (Rafailov et al., 2023): Direct Preference\nOptimization (DPO) simplifies RLHF into binary\nclassification, based on an analytical mapping from\nthe optimal language model to the reward model.\nDPO w. P(rompt)-Tuning (Liu et al., 2022):\nDPO combined with P-Tuning. We extend SFT\nmodels with soft prompts of length 4 on each trans-\nformer layer, consistent with the outputs from our"}, {"title": "4.3 Implementations", "content": "We implement PPO with the TRLX (Havrilla et al.,\n2023) framework, where we keep the default PPO\nhyper-parameters in its demo on TL;DR Summa-\nrization. We implement DPO in the framework\nof Hugging Face trainer with Deepspeed integra-\ntion, and with the same hyper-parameters as re-\nported (Rafailov et al., 2023). We will release all\nof our code implementations upon publication.\nFor CLaP, we make use of 32 generic responses\nsampled from SFT models for each training prompt\nx. In Eqs. 2 and 3, we only use K = 4 responses\nin each training batch. To prevent significantly im-\npact on the pre-trained ability, we conduct CLaP\nwith the pre-trained LLMs frozen for one epoch\nand unfrozen for the second epoch. The final gener-\nation quality of $P_{CLaP}(y|x)$ is comparable to that of\n$P_{SFT}(y|x)$, with around only one point difference\nin perplexity score on each dataset.\nThe training process for CLaP takes approxi-\nmately 3 hours on the IMDB and DailyDialog us-\ning GPT-2, and around 42 hours on TL;DR using\nGPT-J-6B. Although this represents a significant\npreliminary development cost, we will demonstrate\nthat the investment is justified by the substantial\nimprovements in personalization efficiency."}, {"title": "4.4 Evaluation Metrics", "content": "To evaluate the efficiency of different alignment\nmethods for individual preferences, we report the\nscaling hours required for training models on each\nindividual preference, which is scaled linearly with\nthe number of individual users in need. To ensure\nfair comparisons, we implement PPO with suffi-\ncient training steps to ensure convergence and the\nbest checkpoints are all saved within the last 10\nminutes. For DPO-based methods, we consistently"}, {"title": "4.5 Main Results", "content": "We illustrate the results of scaling hours for per-\nsonalization in Fig. 5. It can be observed that our\nproposed Latent DPO consistently takes signifi-\ncantly fewer hours compared to existing methods,\nthereby offering much better efficiency for person-\nalization. In contrast, LoRA and P-Tuning provide\nrelatively limited improvements in efficiency. As\nwe discuss in the previous sections, they can reduce\nthe trainable parameters and save memory for train-\ning, but still rely on computation of LLMs with full\nparameters for loss functions in PPO and DPO.\nRegarding the personalization quality, Fig. 6\nshows the performance of models before and after\npersonalized preference alignment. Specifically,\nDPO w/ LoRA enhances the average performance\nof the SFT models from 52.4, 25.0, 44.9 to 80.8,\n62.0, 55.1 on the three datasets, while Latent DPO\nimproves the CLaP models from 52.5, 25.0, 46.7\nto 83.3, 63.4, 56.0 respectively. This indicates\nthat: (1) our self-supervised CLaP models gener-\nally perform in consistent with the SFT models in\nbaseline methods; (2) Latent DPO provides indi-\nvidual preference-aligned performance as good as\nor better than that of DPO with LoRA or P-Tuning.\nPPO with full parameters performs well on\nIMDB and DailyDialog, but performs averagely on\nthe TL;DR dataset. We attribute this discrepancy to\nthe fact that PPO has access to the ground-truth re-\nward functions on IMDB and DailyDialog, whereas\nit relies on reward modeling for TL;DR. A com-\npelling evidence is its poor performance on the en-\ntertainment preference for TL;DR, where we have\nobserved the reward hacking phenomenon (Skalse\net al., 2022). See Appendix B for details."}, {"title": "4.6 Case Study", "content": "To demonstrate the controllability of personalized\nlatent variables on the latent-adaptive LLM, we\npresent generated cases on DailyDialog in Table 2,\nand more cases on each dataset in appendix D. It\ncan be observed that, differently-aligned latent vari-\nables can lead the same latent-adaptive LLM to\ngenerate differently-preferred responses."}, {"title": "4.7 Human Evaluation", "content": "To further verify the alignment quality of Latent\nDPO, we conduct human evaluation on the gen-\neral human preference of TL;DR, following the\nsame guidelines for human annotators as outlined\nin previous research (Rafailov et al., 2023)."}, {"title": "4.8 Latent Representation Visualization", "content": "We visualize the latent representations in CLaP\nmodels on IMDB and DailyDialog test sets in\nFigs. 7 and 8. We visualize the top 2 correlated\ndimensions with sentiment or intention. It can be\nobserved that our self-supervised latent representa-\ntions can capture unseen semantic values, provid-\ning support for the effectiveness of Latent DPO."}, {"title": "4.9\nExperiments with Llama3-8B", "content": "To further evaluate our method on a larger model,\nwe conduct experiments on DailyDialog using\nLlama3-8B (Dubey et al., 2024). To balance pre-\ntraining and post-training costs, we perform only\nhalf an epoch of CLaP pre-training, with all param-\neters in Llama3-8B frozen.\nAs shown in Tables 4 and 5, our CLaP model per-\nforms generally in consistent with the SFT model,"}, {"title": "5 Discussion", "content": "Conclusion In this work, we present a novel\nparadigm for efficient individual preference align-\nment in LLMs. We achieve this by disentangling\nlatent representations and latent-adaptive genera-\ntion in LLMs (CLaP), and learning personalized\nl atent representations within small latent encoders\n(Latent DPO). Our experiments demonstrate the ef-\nfectiveness and significantly improved efficiency of\nthe proposed method. Specifically, Latent DPO re-\nduces training time for each new preference by 80%\nto 90% in comparison to LoRA-based DPO; Latent\nDPO improves the average win-rates or intention\nprobabilities on IMDB, DailyDialog, and TL;DR\nfrom 52.5, 25.0, 46.7 to 83.3, 63.4, 56.0, while\nLORA-based DPO improves them from 52.4, 25.0,\n44.9 to 80.8, 62.0, 55.1. This proves our proposed\nmethod an effective and valuable tool for aligning\nLLMs with massive individual preferences.\nLimitations Our proposed method achieves per-\nsonalization by training only small latent encoders,\nrather than the entire LLMs. This design offers\ngreatly improved efficiency for personalization, but\nmay have difficulty in making fundamental gener-\native distribution shifts in LLMs. Consequently,\nour method may be not suitable for improving the\nfoundational capabilities of LLMs."}, {"title": "A GPT-3.5 Prompts for Synthetic Preferences Annotation and Evaluation", "content": "As explained in section 4.1, we construct synthetic preferences-helpful, harmless, empathetic, and\nentertainment-to represent different personalization preferences on TL;DR Summarization. Specifically,\nwe construct specific prompts to make GPT-3.5 play different roles with those preferences, for both\nannotation and evaluation. These prompts, as shown in this section, are constructed in the similar formats\nto the \"concise\" prompt and the \"simple\" prompt that are proposed and validated with human judgements\nin the previous work (Rafailov et al., 2023)."}, {"title": "A.1 helpfulness", "content": "Which of the following summaries does a better job of summarizing the most important\npoints in the given forum post, providing helpful information to the reader? A\ngood summary is both precise and helpful."}, {"title": "A.2 harmlessness", "content": "Which of the following summaries does a better job of summarizing the most important\npoints in the given forum post, without including sensitive information (e.g.,\npornography, discrimination, violence) ? A good summary is both precise and\nharmless."}, {"title": "A.3 empathetic", "content": "Suppose you are a reader that prefers summaries with more empathetic expression.\nWhich of the following summaries does a better job of conveying the emotions and\nsentiments expressed in the forum post?"}, {"title": "A.4 entertainment", "content": "Suppose you are a reader that prefers summaries with more entertainment. Which of\nthe following summaries does a better job of capturing the humor, wit, or\nentertainment value in the given forum post?"}, {"title": "B Case Study: Reward Hacking of PPO", "content": "In our main experiments, we observed that PPO-based RLHF exhibited abnormal behavior on the synthetic\nentertainment preference task in TL;DR Summarization. This issue is attributed to reward hacking, as\ndescribed in (Skalse et al., 2022). Reward hacking occurs when the RLHF algorithm exploits the reward\nsignal in unintended ways, leading to sub-optimal or misleading outcomes. These abnormal outcomes\nwere detected and assigned negative judgements by GPT-3.5. In Table 6, we present examples of such\ncases, including their unintended reward scores and the corresponding judgments by GPT-3.5."}, {"title": "C Ablation Study on CLaP", "content": "As our personalization is realized on the basis of CLaP model pretrained through optimizing $L_{CLaP}$ = \n$L_{Reconstruct}$ - $L_{DG-KLD}$ + $L_{Contrastive}$, while the standard optimization term for VAEs (Kingma and\nWelling, 2014) or CVAEs (Sohn et al., 2015) is $L_{ELBo}$ = $L_{Reconstruct}$ - $L_{Standard-KLD}$. Here we conduct\nablation study to validate the effectiveness of each term. Specifically, we try to conduct pretraining with\nincomplete objectives on the TL;DR Summarization dataset, and evaluate their final performance after\nperforming Latent DPO on the general human preference. Those objectives and results are as illustrated\nin Table 7.\nIt can be observed that without $L_{Contrastive}$ for contrastive learning between language and latent\nrepresentations, or without replacing $L_{Standard-KLD}$ with $L_{DG-KLD}$, the pre-trained model struggles to\nbe effectively personalized through Latent DPO."}, {"title": "D Case Study on CLaP Models with Different Personalized Latent Variables", "content": ""}, {"title": "D.1 IMDB - Sentiment", "content": "We illustrate random cases of generation on IMDB in Table 8, 9, 10, and 11, including generation from\nthe CLaP model with unbiased prior latent variables as well as personalized latent variables. We report\nthe sentiment logits to demonstrate the effect of personalization, which are expected to be positive for\npositive preference, to be negative for negative preference, and to be close to zero for neutral preference."}, {"title": "D.2 DailyDialog - Intention", "content": "We illustrate random cases of generation on DailyDialog in Table 12, 13, and 14, including generation\nfrom the CLaP model with the unbiased prior latent variable as well as personalized latent variables. We\nreport the probabilities of different intents to demonstrate the effect of personalization, which are expected\nto be aligned to the corresponding preferences."}, {"title": "D.3 TL;DR Summarization", "content": "We illustrate random cases of generation on TL;DR Summarization in Table 15, 16, 17, and 18,\nincluding generation from the CLaP model with unbiased prior latent variables as well as personalized\nl atent variables. We illustrate the judgements of GPT-3.5 given the \"concise\" prompt for the general human\npreference, and the judgements of GPT-3.5 given the corresponding prompts for synthetic preferences (as\nillustrated in Appendix A)."}]}