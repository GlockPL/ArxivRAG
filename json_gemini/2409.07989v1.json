{"title": "Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms", "authors": ["Fatemeh Askari", "Amirreza Fateh", "Mohammad Reza Mohammadi"], "abstract": "In the context of few-shot classification, the goal is to train a classifier using a limited number of samples while maintaining satisfactory performance. However, traditional metric-based methods exhibit certain limitations in achieving this objective. These methods typically rely on a single distance value between the query feature and support feature, thereby overlooking the contribution of shallow features. To overcome this challenge, we propose a novel approach in this paper. Our approach involves utilizing multi-output embedding network that maps samples into distinct feature spaces. The proposed method extract feature vectors at different stages, enabling the model to capture both global and abstract features. By utilizing these diverse feature spaces, our model enhances its performance. Moreover, employing a self-attention mechanism improves the refinement of features at each stage, leading to even more robust representations and improved overall performance. Furthermore, assigning learnable weights to each stage significantly improved performance and results. We conducted comprehensive evaluations on the MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way 5-shot scenarios. Additionally, we performed a cross-domain task from MiniImageNet to the CUB dataset, achieving high accuracy in the testing domain. These evaluations demonstrate the efficacy of our proposed method in comparison to state-of-the-art approaches.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep learning has led to remarkable progress in the image recognition field, surpassing traditional computer vision algorithms [1,2]. However, the success of deep learning models heavily relies on huge amount of data. When the available data is insufficient, the model faces difficulties in effectively optimizing its parameters [3], which can lead to overfitting. Consequently, achieving effective recognition becomes challenging, hindering the desired outcome [4,5]. Depending on the contexts, the process of labeling images can be time-consuming and costly [6, 7]. For this reason, having models that can achieve acceptable performance with a small number of samples is perceived to be of great importance [8].\nData augmentation is one of the techniques used in situation where the amount of labeled data is insufficient [9]. However, its impact on the model's performance is limited because methods such as rotation, noise addition and scaling cannot provide new information and only help to some extent in preventing overfitting [10]. Another technique is transfer learning [11], which transfer knowledge from a source domain to a target by freezing the shallow network and fine-tuning the deep network layers. However, if the target domain differs significantly from the source domain, this approach may not be very effective when dealing with a small number of samples [12].\nTo address the limitation of traditional deep learning models, which performed well with large training"}, {"title": "2 Related Works", "content": "In this section, we discuss related work on some approaches in meta-learning.\nModel-based:\nCai et al. [24] proposed Memory Matching Networks (MM-Net) for one-shot image recognition, which is based on the principles of Matching Networks [25]. MM-Net combines Convolutional Neural Networks with memory modules to leverage knowledge from a set of labeled images. It employs a contextual learner to predict CNN parameters for unlabeled images. Munkhdalai et al. [26] proposed model, called MetaNet, consists of two main components: a base learner operating in the task space and a meta learner operating in the meta space. By leveraging meta information, MetaNet can dynamically adjust its weights to recognize new concepts in the input task. Garnelo et al. [27] introduces a model called Conditional Neural Processes (CNPs), which combines deep neural networks with Bayesian methods. CNPs are capable of making accurate predictions after observing only a few training data points, while also being able to handle complex functions and large datasets. The disadvantage of model-based approaches is that they are computationally expensive and require significant computational resources.\nOptimization-based:\nFinn et al. [28] proposed a model-agnostic meta-learning (MAML) algorithm for fast adaptation of deep networks. The algorithm involves meta-training the model on various tasks using gradient descent to optimize its initial parameters. In the meta-testing phase, the model's performance is evaluated on new tasks sampled from a task distribution. Through gradient-based adaptation, the model fine-tunes its parameters using a small amount of data from each new task. Sun et al. [29] proposed a novel method called Meta-Transfer Learning (MTL). MTL"}, {"title": "3 Proposed method", "content": "The overall architecture of our model is illustrated in fig. 2. Our proposed approach incorporates several key components designed to enhance performance. At the core is a robust backbone architecture that enables the extraction of feature maps across diverse spatial scales. Additionally, we have integrated an attention module to further refine the feature extraction process. Underpinning our framework is a distance metric that facilitates effective similarity computation between inputs. Moreover, we have incorporated learnable weights to capture the relative significance of each extracted feature map."}, {"title": "3.1 Problem definition", "content": "The goal of few-shot classification is to classify a unseen sample. We have two datasets, $D_{train}$ and $D_{test}$, each associated with corresponding class sets $C_{train}$ and $C_{test}$, respectively. It is important the class sets $C_{train}$ and $C_{test}$ are disjoint, meaning they have no elements in common. Formally, we can express this as $C_{train} \\cap C_{test} = 0$. Each training episode consists of a support set $S$ and a query set $Q$. The support set $S$ comprises K examples for each of N distinct classes, denoted as $X = {(x_{ij}, c_i)}_{i=1}^N$, where $x_{ij}$ represents the jth example belonging to class i. The query set $Q$ contains $X_q = {x_q}_{i=1}^{nq}$. The objective of the model is to leverage the support set $S$ to correctly classify the query example $x_q$. In other words, the model is trained to predict the class label of the query image based on the set of supporting examples and their class affiliations provided in $S$. During training, each episode consists of a random sample drawn from the training dataset $D_{train}$. The objective of the model is to learn to extract feature representations from the examples in the support set $S$, such that the distance between the feature vector of the query image $x_q$ and the feature vectors of the support examples $x_i$ can be effectively measured. Specifically, the model learns to extract discriminative feature vectors from the support examples, which can then be used to classify the query image based on its proximity to the support set features. This training paradigm encourages the model to rapidly adapt its feature extraction and classification capabilities from the limited support data to accurately predict the class label of the query instance. Similarly, during evaluation, the performance of the trained model is assessed on the held-out test dataset $D_{test}$. In this phase, the model extracts feature vectors for each example in the test set, leveraging the knowledge and feature extraction capabilities it learned during the training episodes on the $D_{train}$ dataset."}, {"title": "3.2 Overview", "content": "The overall architecture of our model is illustrated in fig. 2. Our proposed approach incorporates several key components designed to enhance performance. At the core is a robust backbone architecture that enables the extraction of feature maps across diverse spatial scales. Additionally, we have integrated an attention module to further refine the feature extraction process. Underpinning our framework is a distance metric that facilitates effective similarity computation between inputs. Moreover, we have incorporated learnable weights to capture the relative significance of each extracted feature map."}, {"title": "3.2.1 Backbone", "content": "We utilized a pre-trained ResNet-18 network with initial weights from the ImageNet dataset. We removed the last fully-connected layer and fine-tuned the network on our specific dataset. To leverage multi-level feature maps, we proposed a multi-output embedding approach where we extracted feature maps at the end of each of the 5 convolutional blocks in the ResNet-18 architecture. This allowed us to capture feature representations at multiple scales and resolutions. Each sample in the support set, denoted as $x_i$, is mapped to five different feature spaces like $f_{ij}$ as shown in equation 1:\n$f_{ij} = {f^{Conv1-s}_{ij}, f^{Conv2-s}_{ij},..., f^{Conv5-s}_{ij}}$\nSimilarly, each query sample, denoted as $x_q$, is mapped to the following feature space (equation 2):\n$f_{q_j} = {f^{Conv1-q}_{ij}, f^{Conv2-q}_{ij}, ..., f^{Conv5-q}_{ij}}$\nIn our proposed approach, we employ multiple convolutional layers (Conv1, Conv2, Conv3, Conv4, Conv5) from the ResNet-18 network to extract features from the samples. Each of these convolutional blocks contributes to a distinct feature representation, allowing us to leverage different levels of abstraction and information contained in the feature maps for both the support and query samples. This multi-level feature extraction strategy enables a more comprehensive and robust representation of the input images, which is crucial for effective similarity computation and classification tasks."}, {"title": "3.2.2 Attention module", "content": "After extracting the feature vectors at each stage, we utilize self-attention and global average pooling. The representation of the attention module is illustrated in fig. 1. Considering the extracted feature vectors, we apply a 1\u00d71 convolution on f, resulting in convolutional vectors k, g and h. This operation is performed to reduce the number of channels (equation 3):\n$k',v', q' = Conv_{1 \\times 1}(f^{Conv-P}), p\\in [1 - 5]$\nAfter obtaining the convolutional vectors $q'(f^{Conv-P})$ and $k'(f^{Conv-P})$, we apply the softmax function (equation 4):\n$\\beta_{ij} = \\frac{exp(S_{i,j})}{\\Sigma_{i=1}^N exp(S_{i,j})}$ where $S_{i,j}= d'(f^{Conv-P})^Tk'(f^{Conv-P})$\nThe attention mechanism computes weights B, that determine the relative importance of each pixel in the feature map. These weights are calculated across the entire spatial extent, allowing the attention module to capture long-range dependencies beyond a local neighborhood, unlike traditional convolutions. The output of the self-attention layer is computed as equation 5:\n$f^{Conv-P}_{ij}= Conv_{1x1}(\\Sigma_{i=1}^N\\beta_{i,j}v'(f^{Conv-P}))$\nIn equation 5, in addition to using various ResNet18 blocks for feature extraction, we employ a 1 \u00d7 1 convolution layer twice - once before applying the attention mechanism and again after it. The purpose of using the 1 \u00d7 1 convolution layer twice is to make the network more nonlinear and to allow us to leverage the previous information more evenly. The scaling factor, typically denoted as $\\gamma$, is a learnable parameter in the network that is multiplied with the input feature map $f^{Conv-p}$ before the addition (equation 6):\n$y^{Conv-P} = \\gamma \\times  o_i + f^{Conv-p}$\nAfter obtaining the final output, we take a global average pooling. The output is represented for each query and support sample as follows:\n$f'_{ij} = {f'^{Conv1-s}_{ij}, f'^{Conv2-s}_{ij}..., f'^{Conv5-s}_{ij}}$\n$f'_q = {f'^{Conv1-q}_{ij}, f'^{Conv2-q}_{ij}, ..., f'^{Conv5-q}_{ij}}$"}, {"title": "3.2.3 Distance metric", "content": "Once we have obtained the final output from the previous stage, we take the average of the vectors from all the support samples belonging to the same class to obtain the prototypes for each class from the support set as equations 9.\n$\\bar{c}^{Conv-p}_i = \\frac{1}{|S|}\\Sigma_{j=1}^K f^{Conv-p} \\;  _{ij}^{Conv-p-s}, where \\; p\\in[1-5]$\nWe calculate the Euclidean distance between the feature map of each query sample $f^{Conv-p}$ and its corresponding prototype map $\\bar{c}_i^{Conv-P}$, considering 5 feature maps per sample, as described in equation 10.\n$d^{Conv-p}_{ai,j} = Euclidean \\;(f^{Conv-P}, \\bar{c_i}^{Conv-P})$"}, {"title": "3.2.4 Learnable weights", "content": "Considering the emphasis of shallow network layers on global features and deep layers on abstract features, we opted to assign weights to the distances computed in the five feature spaces for each query sample-support set representative pair. These weights are trainable within the network, and their initial assignment significantly influences the model's performance, which will be elaborated on in the Experiment section. The final distance is obtained by aggregating the weighted distances from these five feature spaces as shown in Equation 11:\n$d_{i,j} = \\Sigma_{r=1}^5 w^{Conv-p} \\times  d^{conv-P}_{Wij}, p\\in [1-5]$\nAfterward, we apply softmax as shown in Equation 12:\n$p(y = j|x_1) = \\frac{exp(-d_{i,j})}{\\Sigma_{l=1}^{n_s} exp(-d_{i,l})}$\nThe learning process involves minimizing the negative log-probability $J = -log p(y = j|x)$ of the true class j using the Adam optimizer. Training episodes are formed by randomly selecting a subset of classes from the training set. Within each selected class, a subset of examples is chosen to form the support set, while another subset from the remaining examples is used as query points."}, {"title": "4 Experimental Results", "content": "The proposed method was evaluated on three widely used datasets commonly employed for few-shot learning tasks: MiniImageNet [65], FC100 [66], and CUB [67].\nMiniImageNet dataset. The MiniImageNet is a subset of the ImageNet dataset designed for training and evaluating machine learning models. This dataset contains 100 classes with 600 images per class. The images are randomly divided into three splits: 64 classes for training, 16 classes for validation, and 20 classes for testing. This data partitioning allows researchers to evaluate how effectively models can generalize to new and unseen classes after being trained on the provided set of classes.\nFC100 dataset. The FC100 is another dataset similar in structure to MiniImageNet. It contains 100 classes with 600 images per class. However, the class splits are handled differently the 100 classes are randomly divided into 60 training classes, 20 validation classes, and 20 test classes. This ensures that the training, validation, and test sets are entirely disjoint, which can provide a more realistic evaluation of a model's ability to learn general visual representations.\nCUB dataset. The CUB (Caltech-UCSD Birds-200-2011) dataset is a collection of bird images often used for fine-grained visual classification tasks. This dataset contains 200 different bird species, with a total of 11,788 images. The images are randomly partitioned into 100 training classes, 50 validation classes, and 50 test classes. This dataset poses a challenging problem as many bird species can appear visually similar, requiring models to learn discriminative features to accurately classify the different species."}, {"title": "4.2 Experimental setting", "content": "For our datasets, we performed the following preprocessing and model configuration steps. First, all input images were resized to a resolution of 84 x 84 pixels. Regarding the model architecture, we employed a pre-trained ResNet-18 as the backbone feature extractor. Specifically, we extracted features from the 5 stages of the ResNet-18 network, obtaining feature maps from each of these stages.\nFor the optimization of our models, we employed the Adam optimizer. For the MiniImageNet dataset, we used a constant learning rate of $10^{-4}$, while for the FC100 dataset, the learning rate was $2 \\times 10^{-5}$. This learning rate was selected based on preliminary experiments to ensure stable and efficient convergence of the models during training. All experiments were conducted on an NVIDIA RTX 4090 GPU system."}, {"title": "4.3 Evaluation metric", "content": "To evaluate the performance of our models, we employed the following scenario: For the training phase, we sampled 30 random classes and 5 examples per class from the training set. We then trained the model on these samples. For evaluation, we tested the trained model on a 5-way 5-shot task by selecting 5 random classes and"}, {"title": "4.4 Comparison with state of the arts", "content": "Based on the results presented in Table 1 and Table 2, our proposed model demonstrates strong few-shot learning performance compared to the state-of-the-art approaches. As shown in Table 1, on the MiniImageNet dataset, our model achieves an accuracy of 66.57 in the 1-shot setting and 84.42 in the 5-shot setting. This indicates that our model is able to effectively leverage the limited training data and rapidly adapt to new tasks, showcasing its superior few-shot learning capabilities. Similarly, on the more challenging FC100 dataset, as shown in Table 2, our model outperforms the existing state-of-the-art methods by a significant margin, obtaining an accuracy of 44.78 in the 1-shot setting and 66.27 in the 5-shot setting. The consistent improvements observed across both MiniImageNet and FC100 datasets highlight the effectiveness of the design choices and techniques employed in our model. These choices and techniques allow it to learn more robust and transferable representations for few-shot learning"}, {"title": "4.5 Cross domain", "content": "To further assess the generalization capabilities of our model, we conducted a cross-domain evaluation using the CUB dataset as the testing domain, while the Mini-ImageNet dataset was used for training. As shown in Table 3, our model demonstrated strong performance in this challenging cross-domain few-shot learning setting. Specifically, our model achieved an impressive 1-shot accuracy of 52.95 and a 5-shot accuracy of 71.59 on the CUB dataset. This outperforms various state-of-the-art approaches, such as RelationNet+LFT, MatchingNet+LFT, and the strong Baseline model, showcasing the superior ability of our approach to adapt to the distributional shift between the training and testing domains. The consistent improvement over the competing methods across both the 1-shot and 5-shot settings highlights the robustness and transferability of the representations learned by our model. This demonstrates its effectiveness in tackling the challenging cross-domain few-shot learning problem, where the model must generalize its knowledge from the training domain to the novel testing domain. These results further solidify the position of our model as a highly competitive and versatile few-shot learning solution."}, {"title": "4.6 Ablation study", "content": "To better understand the contributions of different components in our model, we conducted an ablation study and reported the results. Table 4 illustrates the influence of individual components on overall model performance. The Baseline configuration, serving as the foundation of our model, achieves notable results with 62.83% accuracy for the 1-shot task and 82.38% for the 5-shot task on the MiniImageNet dataset. On the FC100 dataset, the Baseline configuration results in 39.47% 1-shot and 63.44% 5-shot accuracy.\nIntroducing the Multiscale module, which enhances feature extraction by capturing information at multiple scales, leads to improvements in both 1-shot and 5-shot performance across both datasets. Adding the Learnable Weight component, which adapts weights for different feature channels, further boosts accuracy, demonstrating its effectiveness.\nThe final inclusion of the Self-attention module results in the best overall performance, with the model achieving 66.57% accuracy for the 1-shot task and 84.42% for the 5-shot task on MiniImageNet. On FC100, the performance reaches 44.78% for 1-shot and 66.27% for 5-shot tasks. These results underscore the significant contribution of the self-attention mechanism to enhancing the few-shot learning capabilities of our approach.\nAs shown in Table 5, two important observations can be made regarding model performance on the Mini-ImageNet dataset:\n1) Advantage of Learnable Weights: The results indicate a clear benefit to using learnable weights. In both"}, {"title": "5 Conclusion", "content": "This paper presents an innovative strategy to enhance few-shot classification by integrating a self-attention network and embedding learnable weights at each stage, leading to improved performance and substantial outcomes. Through feature vector extraction and weight transfer across stages, this technique elevates multi-scale feature representation, thereby enhancing overall performance. Moreover, the incorporation of self-attention mechanisms refines features at each stage, resulting in more robust representations and enhanced performance. The introduction of learnable weights at each stage significantly boosts performance and outcomes. Extensive evaluations on the MiniImageNet and FC100 datasets underscore the efficacy of this method compared to current state-of-the-art approaches."}]}