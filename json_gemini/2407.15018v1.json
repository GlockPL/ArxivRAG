{"title": "Answer, Assemble, Ace:\nUnderstanding How Transformers Answer Multiple Choice Questions", "authors": ["Sarah Wiegreffe", "Oyvind Tafjord", "Yonatan Belinkov", "Hannaneh Hajishirzi", "Ashish Sabharwal"], "abstract": "Multiple-choice question answering (MCQA)\nis a key competence of performant transformer\nlanguage models that is tested by mainstream\nbenchmarks. However, recent evidence shows\nthat models can have quite a range of perfor-\nmance, particularly when the task format is di-\nversified slightly (such as by shuffling answer\nchoice order). In this work we ask: how do suc-\ncessful models perform formatted MCQA? We\nemploy vocabulary projection and activation\npatching methods to localize key hidden states\nthat encode relevant information for predicting\nthe correct answer. We find that prediction of\na specific answer symbol is causally attributed\nto a single middle layer, and specifically its\nmulti-head self-attention mechanism. We show\nthat subsequent layers increase the probability\nof the predicted answer symbol in vocabulary\nspace, and that this probability increase is asso-\nciated with a sparse set of attention heads with\nunique roles. We additionally uncover differ-\nences in how different models adjust to alter-\native symbols. Finally, we demonstrate that a\nsynthetic task can disentangle sources of model\nerror to pinpoint when a model has learned for-\nmatted MCQA, and show that an inability to\nseparate answer symbol tokens in vocabulary\nspace is a property of models unable to perform\nformatted MCQA tasks.", "sections": [{"title": "1 Introduction", "content": "Multiple-choice question answering (MCQA) is\na mainstay of language model (LM) evaluation\n(Liang et al., 2022; Gao et al., 2023; Beeching et al.,\n2023), not in the least because it avoids the chal-\nlenges of open-ended text evaluation. One promi-\nnent example is the MMLU benchmark (Hendrycks\net al., 2021), which is considered a meaningful sig-\nnal for new model releases (Lambert, 2024a,b).\nWhile early LMs were evaluated on MCQA\nquestions by treating the task as next-token-\nprediction and scoring each answer choice ap-\npended to the input independently, many bench-\nmarks and datasets now evaluate models on their\nability to produce the correct answer choice sym-\nbol in a single inference run when given the an-\nswer choices in the prompt (formatted MCQA;\nFig. 1 and \u00a73). Indeed, it has been shown that\ncapable models, and particularly those that have\nbeen instruction-tuned, perform well on format-\nted MCQA tasks (Liang et al., 2022; Robinson"}, {"title": "2 Related Work", "content": "Behavioral analyses of LM abilities to answer for-\nmatted MCQA questions prompt models in a black-\nbox manner by constructing different input prompts\nand observing how they affect models' predic-\ntions (Wiegreffe et al., 2023; Pezeshkpour and Hr-\nuschka, 2023; Sun et al., 2024; Zheng et al., 2024;\nAlzahrani et al., 2024; Khatun and Brown, 2024;\nBalepur et al., 2024; Wang et al., 2024). These\nmethods and findings are complementary to ours.\nEfforts to interpret transformer LMs for MCQA\nare limited. Most similar to ours is Lieberum et al.\n(2023), who study what attention heads at the fi-\nnal token position are doing, and isolate a subset\nthey coin \"correct letter heads\". They demonstrate\nhow these heads attend to answer symbol represen-\ntations at earlier token positions and promote the\ncorrect symbol based on its position in the sym-\nbol order, though they show this behavior does not\nhold entirely for answer choices other than {\u201cA\u201d,\n\u201cB\u201d, \u201cC\u201d, \u201cD\u201d.} Their experiments are limited to\none task (MMLU) and one closed-source model\n(Chinchilla-70B). Unlike us, they do not inspect\nmodel behavior when it is poor, nor do they put\nforth a hypothesis for model behavior when other\nanswer choice symbols are used. We additionally\nfind diverse roles of important attention heads, with\nsome always promoting specific symbols.\nLi and Gao (2024) use vocabulary projection and\nstudy the norms of weighted value vectors in multi-\nlayer perceptrons of GPT-2 models in order to lo-\ncalize the production of \u201cA\u201d, which GPT-2 models\nare biased towards producing when prompted 0-\nshot. They find that updating a single value vector\ncan substantially reduce this label bias. Unlike that\nwork, we investigate how models do symbol bind-\ning when they are performant at formatted MCQA;\nlabel bias is one reason why a model may not be\ncorrectly performing symbol binding.\nOutside of MCQA, many works investigate how\nmodels internally build up representations that lead\nto their predictions, such as on factual recall (Geva\net al., 2021; Dai et al., 2022; Geva et al., 2022b;\nMeng et al., 2022; Geva et al., 2023; Yu et al.,\n2023; Merullo et al., 2024b) or basic linguistic\n(Vig et al., 2020; Wang et al., 2023; Merullo et al.,\n2024a; Prakash et al., 2024; Todd et al., 2024) or\narithmetic (Stolfo et al., 2023; Hanna et al., 2023;\nWu et al., 2023) tasks. MCQA differs because it is\nboth directly represented in real benchmarks and a\nmore challenging task."}, {"title": "3 Formatted MCQA", "content": "We first introduce task notation, datasets, and our\nprotocol for selecting strong models to analyze.\nFormatted MCQA is a prompt format in which\npossible answer choices are presented to the model\nas an enumerated list, each associated with an an-\nswer choice symbol. All of our dataset instances\nare formatted as follows, unless otherwise noted:\nwhere y = \"A\". We include 3 in-context examples\nfor each instance with labels \"A\", \"B\", \u201cB\u201d.\nModels are evaluated on their ability to assign\nhigher probability to the symbol associated with\nthe correct answer choice (here, \"A\") as opposed\nto the alternative symbol (here, \"B\"). We compute\n\u0177 as \u201cA\u201d if p(\u201cA\u201d|x) > p(\u201cB\u201d|x), else \u201cB\u201d.\nWe convert datasets with > 2 answer choices\nper instance to binary multiple-choice by pairing\na randomly-selected incorrect answer choice with\nthe correct answer choice. We restrict instances\nto two options because the quantity we will plot\nwhen measuring discriminativity of model compo-\nnents (\u00a76) only takes 1 value for 2 choices, but n\nvalues for n > 2 (see App. Appendix A.1 for an\nelaboration)."}, {"title": "3.1 Datasets", "content": "We experiment on two challenging real-world\nmultiple-choice datasets from LLM benchmarks:\nHellaSwag (Zellers et al., 2019) and MMLU\n(Hendrycks et al., 2021). More details are given in\nAppendix A.2. We additionally use a prototypical\ncolors dataset (Norlund et al., 2021) to create a syn-\nthetic MCQA task that disentangles dataset-specific\nknowledge from the ability to perform symbol bind-\ning: Copying Colors from Context (\u2018Colors\u2019).\nThe dataset consists of instances such as x = \"Corn\nis\", y =\"yellow\". We include y in the context,\nso the model's only task is to produce the sym-\nbol associated with that answer choice, given one\ndistractor answer randomly selected from the ten"}, {"title": "3.2 Models", "content": "Olmo. We experiment on both the base (Olmo-\n7B-Base) and instruction-tuned (Olmo-7B-SFT)\nversions of the Olmo model (Groeneveld et al.,\n2024). We also experiment on the Olmo-v1.7-\nBase model of the same size, which achieves better\nperformance on a 4-way 0-shot formatted MCQA\nversion of MMLU by making 0.01% of the pre-\ntraining data multiple-choice (AI2, 2024). This\nperformance gain is reported to occur at ~500B\ntokens of training (Gu et al., 2024)\u2014 to investi-\ngate how model representations change once the\nformatted MCQA task is learned, we investigate\none checkpoint before this point (350B training\ntokens), one checkpoint after (1T tokens), and the\nfinal checkpoint (~2T tokens).\nLlama2. We experiment with two sizes of base\nmodels from the Llama2 family (Touvron et al.,\n2023): 7B and 13B, as well as the instruction-tuned\n+ RLHF-aligned 7B-Chat and 13B-Chat models.\nAll 7B (13B) models investigated have 32 (40)\nlayers with 32 (40) attention heads per layer."}, {"title": "3.3 Identifying Consistent MCQA Models", "content": "We next isolate models capable of performing for-\nmatted MCQA. We design a set of 4 prompts to test\nwhether models answer formatted MCQA prompts\nconsistently, i.e., they are robust to the position and\nsymbol of the correct answer choice.\nWe define consistency as always selecting one\nsemantic answer for a dataset instance regardless\nof how the prompt for that instance is constructed.\nFor example, for the two prompts \u201cA:_B: _\" and\n\u201cA: _B:_\", where bold denotes the location of\nthe correct answer, the model is consistent if its two\npredictions are \"A\" followed by \"B\" (consistently\ncorrect) or \u201cB\u201d followed by \u201cA\u201d (consistently incor-\nrect). Conversely, if the model always predicts the\nsame token for both prompts, this indicates a pref-\nerence towards that specific token or that specific\nposition. We refer to this as being inconsistent (al-\nways \"A\" or always \"B\", depending on the model's\npreference). Henceforth, since we are interested\nin consistent models, we will always average over"}, {"title": "4 Localizing Answer Symbol Production", "content": "Activation patching, sometimes referred to as\ncausal tracing (Meng et al., 2022), causal medi-\nation analysis (Vig et al., 2020), or interchange\nintervention (Geiger et al., 2020), is a method for\nperforming mediation on neural network hidden\nstates to localize which network components have\na causal effect on a model's prediction. The method\ninvolves performing the following steps:\n1.  Run inference on a dataset instance that the\nmodel predicts correctly, say, one formatted as\n\u201cA:_B: _\u201d (xA). This will produce probabili-\nties p(\u201cA\u201d|xA) (high) and p(\u201cB\u201d|xA) (low).\n2.  Run inference on the same dataset instance,\nbut in a different format, such as \u201cA: _B:_\",\nthat the model still predicts correctly (xB).\nThis will produce a second set of probabilities,\np(\u201cA\u201d|\u0445\u0432) (low) and p(\"B\"|xB) (high). We\nstore hidden state activations of interest.\n3.  While running inference on xA, replace the\noutput hidden state at the final token position\n(A)\nand layer l, hT, with the hidden state from\nthe same layer and token position, but from\nthe inference run of xB, h(B).\n l,T . We measure\np(\u201cA\u201d|xA, h(B) ) and p(\u201cB\u201d|xA, h(B) ).\n l,T\n4.  Repeat step 3 for each layer (l \u2208 [1, L]).\nWe will use the notation xB \u2192 xA to indicate\npatching a hidden state from xB into xA. Meng\net al. (2022) proposed performing activation patch-\ning on the outputs of multi-layer perceptron (MLP)\nand multi-head self-attention (MHSA) functions;\n(Todd et al., 2024) on the outputs of each weighted\nattention head. We also perform activation patch-\ning on these granularities (positions A and B in\nFig. 10). Note two preliminary conditions for ac-\ntivation patching: the model predicts the correct\nanswer for both xA and xB, and xA and xB do not\nhave the same answer.\""}, {"title": "4.2 Vocabulary Projection", "content": "Residual connections have been shown to allow\nfor the iterative refinement of features in neural\nnetworks (Jastrzebski et al., 2018; Simoulin and\nCrabb\u00e9, 2021). In addition to understanding which\nhidden states have the largest causal effect on pre-\ndictions, it is useful to understand how predictions\nform in the vocabulary space defined by the dot\nproduct between the hidden state output by the LM,\nHL,T, and the unembedding matrix, Wu. Recall\nthat for an L-layer autoregressive LM, model pre-\ndictions are a probability distribution over each\nitem in the vocabulary V, $p \\in R^{|V|}$, given by:\n$p = Softmax(Wu \\cdot LN(h_{L,T}))$ where LN is layer"}, {"title": "4.3 Initial Observations", "content": "Vocabulary projection reveals patterns distinct\nto MCQA. Since vocabulary projection can pro-\nduce small values in earlier layers of a network sim-\nply because hidden states have not yet been trans-\nformed into a space mappable to high probabilities\non individual tokens, we first compare results on\noutput hidden states of individual layers between\nthe formatted MCQA task on Colors, and a greedy\ngeneration version, where we provide instances in\nthe format \"Corn is yellow. What color is\ncorn?\" and take the first greedy-decoded token\nas the prediction (i.e., we expect the model to pro-\nduce\" yellow\u201d). Results are shown in Fig. 3.\nWe observe, notably, that while the greedy ver-\nsion of the task and the \u201ctop token\" line demon-\nstrate that hidden states can be transformed by the\nunembedding matrix into non-negligible probabil-\""}, {"title": "5 Roles of Specific Layers", "content": "Key Layers are consistent across datasets. We\nobserve projected token probabilities for Olmo-7B-\nSFT across all three datasets in Fig. 4, and activa-\ntion patches for two datasets in Figs. 6a and 6b. De-\nspite Olmo-7B-SFT's varying performance on the\ndatasets, key layers are highly consistent. Notably,\nlayer 21 also plays a key causal role in the promo-"}, {"title": "6 Roles of Layer Components", "content": "We next investigate the role that MHSA and MLP\nfunctions play in the observed behavior.\nAnswer symbol production is driven by a sparse\nportion of the network. In Figs. 7a and 7b, we\ndirectly project the hidden states output by these\nfunctions (positions A and B in Fig. 10). Large\nspikes in probabilities come from both MHSA (lay-\ners 23, 30, 31) and MLP outputs (layers 28, 30).\nThe breakdown in spiking behavior by attention\nheads in Fig. 8a (bottom) shows just how sparse\nthe number of attention heads projecting to non-\negligible probability values is (~8 total), even in\nlayers where the MHSA output hidden state assigns\nlarge probabilities to \u201cA\u201d and \u201cB\u201d (recall that the\nMHSA output vector is a sum of these weighted\nattention head vectors). Activation patching results\n(Figs. 7c and 7d) further elucidate that the MHSA\nmechanism is driving the encoding of relevant in-\nformation to predict the answer at layer 21, and at\nlayer 29 for \"C:_D: _\u201d (Fig. 18, Appendix). The\nimportance of the MHSA function as the source of\nthe causal effect observed at layers 15-21 is also\nevident for Llama2-13B-Chat (Fig. 20, Appendix),\nthough more attention heads in the model project\nto the vocabulary tokens (Fig. 21a).\nIndividual attention heads dominate the pro-\nduction of answer choice symbols (over MLPs).\nPlotting all attention heads and MLP layers that as-\nsign high probability to the sum of answer choices\n$(p(\u201cA\u201d) + p(\u201cB\u201d) > 0.01)$ in Fig. 8a (top; Fig. 21a\nfor Llama2-13B-Chat), we see that attention heads\ngenerally dominate the distribution, despite the fact\nthat they only contain ~half the learnable parame-"}, {"title": "7 Where are poorly-performing models\ngoing wrong?", "content": "Our synthetic task separates formatted MCQA\nperformance from dataset-specific performance.\nIn Fig. 9 (Table 3 for other models), our synthetic\ntask confirms that there is a crucial point at which\nformatted MCQA skill is learned for Olmo-v1.7-\n7B: between 350B and 1T tokens. Performance\ngoes from near-random to near-100%, even for\ncheckpoints with poor performance on more chal-"}, {"title": "8 Conclusion", "content": "This work provides novel insights into the mech-\nanisms language models employ when answering\nformatted multiple-choice questions. Our analysis\nleverages two complementary techniques: vocab-\nulary projection and activation patching. Using\ncarefully-designed prompts and a synthetic task,\nwe uncover a series of stages, attibutable to a very\nsparse set of specialized network components, in\nwhich transformers select, promote, and align (in\nthe case of OOD prompts) the answer choice sym-\nbol they predict. With MCQA datasets such as\nMMLU routinely used as a performance signal for\nmodel development, it is important to disentangle\nand test formatted MCQA ability in order to build"}, {"title": "A Appendix", "content": null}, {"title": "A.1 Plot Complexity w.r.t Number of Answer\nChoices", "content": "The extent to which a model component prefers\n\"A\" over other answer choices in a binary setup\nis equivalent to $p(\u201cA\u201d) \u2013 p(\u201cB\u201d)$; this quantity\nsimultaneously illustrates (in the negative direc-\ntion) the extent to which the component prefers\n\"B\". However, for 3 answer choices, say \u201cA\u201d, \u201cB\u201d,\n\"C\", 3 quantities are needed to measure preference\nfor each choice: $p(\u201cA\u201d) \u2013 max{p(\u201cB\u201d),p(\u201cC\u201d)},$\n$p(\"B\") - max{p(\u201cA\u201d),p(\u201cC\u201d)},$ and $p(\u201cC\u201d) -$\\n$max{p(\u201cA\u201d), p(\u201cB\u201d)}.$"}, {"title": "A.2 Dataset Details", "content": "HellaSwag (Zellers et al., 2019) is a 4-way\nmultiple-choice commonsense natural language in-\nference dataset. The goal is to select the best com-\npletion for a given sentence prompt. The test set\nis not publicly available; we sample a fixed set\nof 5000 instances from the validation set used in\nour experiments (apart from activation patching,\nfor which we sample 1000 instances due to com-\npute restrictions). We use the first 3 training set\ninstances as in-context examples for all instances.\nMMLU (Hendrycks et al., 2021), or the \u201cMas-\nsive Multitask Language Understanding\u201d bench-\nmark, spans 57 different topical areas. The ques-\ntions are 4-way multiple-choice spanning subjects\nin social sciences, STEM, and humanities that were\nmanually scraped from practice materials available\nonline for exams such as the GRE and the U.S.\nMedical Licensing Exam. We sample a fixed set\nof 5000 instances from the test set used in our\nexperiments (apart from activation patching, for\nwhich we sample 1000 instances due to compute\nrestrictions). We sample a fixed set of 3 in-context\nexample instances from the 5 provided for each\ntopical area.\nWe convert both tasks to binary multiple-choice\nfollowing the procedure in \u00a73."}, {"title": "A.3 Transformer Background", "content": "The output hidden states of transformer models\n(Vaswani et al., 2017) are a linear combination\nof the outputs of non-linear functions (i.e., multi-\nhead self-attention and multi-layer perceptrons) at\neach layer. For the Olmo and Llama2 models,\nwhich sequentially apply multi-head self-attention\n(MHSA) and a multi-layer perceptron (MLP), for\ninput hidden state $x_{e-1} \\in R^{d}$, the output hidden"}]}