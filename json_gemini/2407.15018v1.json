{"title": "Answer, Assemble, Ace:\nUnderstanding How Transformers Answer Multiple Choice Questions", "authors": ["Sarah Wiegreffe", "Oyvind Tafjord", "Yonatan Belinkov", "Hannaneh Hajishirzi", "Ashish Sabharwal"], "abstract": "Multiple-choice question answering (MCQA) is a key competence of performant transformer language models that is tested by mainstream benchmarks. However, recent evidence shows that models can have quite a range of performance, particularly when the task format is diversified slightly (such as by shuffling answer choice order). In this work we ask: how do successful models perform formatted MCQA? We employ vocabulary projection and activation patching methods to localize key hidden states that encode relevant information for predicting the correct answer. We find that prediction of a specific answer symbol is causally attributed to a single middle layer, and specifically its multi-head self-attention mechanism. We show that subsequent layers increase the probability of the predicted answer symbol in vocabulary space, and that this probability increase is associated with a sparse set of attention heads with unique roles. We additionally uncover differences in how different models adjust to alternative symbols. Finally, we demonstrate that a synthetic task can disentangle sources of model error to pinpoint when a model has learned formatted MCQA, and show that an inability to separate answer symbol tokens in vocabulary space is a property of models unable to perform formatted MCQA tasks.", "sections": [{"title": "1 Introduction", "content": "Multiple-choice question answering (MCQA) is a mainstay of language model (LM) evaluation (Liang et al., 2022; Gao et al., 2023; Beeching et al., 2023), not in the least because it avoids the challenges of open-ended text evaluation. One prominent example is the MMLU benchmark (Hendrycks et al., 2021), which is considered a meaningful signal for new model releases (Lambert, 2024a,b).\nWhile early LMs were evaluated on MCQA questions by treating the task as next-token-prediction and scoring each answer choice appended to the input independently, many benchmarks and datasets now evaluate models on their ability to produce the correct answer choice symbol in a single inference run when given the answer choices in the prompt (formatted MCQA; Fig. 1 and \u00a73). Indeed, it has been shown that capable models, and particularly those that have been instruction-tuned, perform well on formatted MCQA tasks (Liang et al., 2022; Robinson and Wingate, 2023; Wiegreffe et al., 2023). At the same time, despite their strong end-task performance, some models are not robust to seemingly inconsequential format changes, such as shuffling the position of the correct answer choice, changing letters that answers are mapped to, or using different symbols (Pezeshkpour and Hruschka, 2023; Alzahrani et al., 2024; Khatun and Brown, 2024).\nUnderstanding how LMs form their predictions, particularly on real-world benchmarks and task formats such as MCQA, is important for reliability reasons. Motivated by this, we ask: How do successful models perform formatted MCQA?\nIn this paper, we set out to interpret internal components to uncover how models promote the prediction of specific specific answer choice symbols. We perform vocabulary projection and activation patching analyses on various model families\u2014Llama 2 (Touvron et al., 2023) and Olmo v1 (Groeneveld et al., 2024) and v1.7 (AI2, 2024))\u2014and on three datasets (MMLU (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), and a copying task we create). We find:\n1. Models both encode information needed to predict the correct answer symbol and promote answer symbols in the vocabulary space in a very similar fashion across tasks, even when their overall task performance varies.\n2. Answer symbol production is driven by a sparse portion of the network, dominated by a few attention heads, which specialize in promoting specific answer symbols.\n3. The process for correctly answering more unnatural prompt formats is in some cases more complex: a finetuned Olmo 7B model only begins producing the correct symbols for these prompts in later layers. We observe evidence the model may be making predictions first in the space of expected answer symbols (\u201cA\u201d, \"B\") before switching to the symbols provided in the prompt (\u201cC\u201d, \u201cD\u201d).\n4. A simple synthetic task is able to disentangle formatted MCQA performance from performance on a specific dataset, and allows us to narrow down a point during training at which Olmo v1.7 learns formatted MCQA.\n5. Poorly performing models cannot separate answer choice symbols in vocabulary space."}, {"title": "2 Related Work", "content": "Behavioral analyses of LM abilities to answer formatted MCQA questions prompt models in a black-box manner by constructing different input prompts and observing how they affect models' predictions (Wiegreffe et al., 2023; Pezeshkpour and Hruschka, 2023; Sun et al., 2024; Zheng et al., 2024; Alzahrani et al., 2024; Khatun and Brown, 2024; Balepur et al., 2024; Wang et al., 2024). These methods and findings are complementary to ours.\nEfforts to interpret transformer LMs for MCQA are limited. Most similar to ours is Lieberum et al. (2023), who study what attention heads at the final token position are doing, and isolate a subset they coin \"correct letter heads\". They demonstrate how these heads attend to answer symbol representations at earlier token positions and promote the correct symbol based on its position in the symbol order, though they show this behavior does not hold entirely for answer choices other than {\u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201cD\u201d.} Their experiments are limited to one task (MMLU) and one closed-source model (Chinchilla-70B). Unlike us, they do not inspect model behavior when it is poor, nor do they put forth a hypothesis for model behavior when other answer choice symbols are used. We additionally find diverse roles of important attention heads, with some always promoting specific symbols.\nLi and Gao (2024) use vocabulary projection and study the norms of weighted value vectors in multi-layer perceptrons of GPT-2 models in order to localize the production of \u201cA\u201d, which GPT-2 models are biased towards producing when prompted 0-shot. They find that updating a single value vector can substantially reduce this label bias. Unlike that work, we investigate how models do symbol binding when they are performant at formatted MCQA; label bias is one reason why a model may not be correctly performing symbol binding.\nOutside of MCQA, many works investigate how models internally build up representations that lead to their predictions, such as on factual recall (Geva et al., 2021; Dai et al., 2022; Geva et al., 2022b; Meng et al., 2022; Geva et al., 2023; Yu et al., 2023; Merullo et al., 2024b) or basic linguistic (Vig et al., 2020; Wang et al., 2023; Merullo et al., 2024a; Prakash et al., 2024; Todd et al., 2024) or arithmetic (Stolfo et al., 2023; Hanna et al., 2023; Wu et al., 2023) tasks. MCQA differs because it is both directly represented in real benchmarks and a more challenging task."}, {"title": "3 Formatted MCQA", "content": "We first introduce task notation, datasets, and our protocol for selecting strong models to analyze.\nFormatted MCQA is a prompt format in which possible answer choices are presented to the model as an enumerated list, each associated with an answer choice symbol. All of our dataset instances are formatted as follows, unless otherwise noted:\nFor each of the following phrases, select the\nbest completion (A or B).\n<in-context examples>\nPhrase: <question>\nChoices:\nA: <correct answer>\nB: <incorrect answer>\nThe correct answer is:\nwhere y = \"A\". We include 3 in-context examples for each instance with labels \"A\", \"B\", \"\u0412\".\nModels are evaluated on their ability to assign higher probability to the symbol associated with the correct answer choice (here, \"A\") as opposed to the alternative symbol (here, \"B\"). We compute \u0177 as \u201cA\u201d if p(\u201cA\u201d|x) > p(\u201cB\u201d|x), else \u201cB\u201d.\nWe convert datasets with > 2 answer choices per instance to binary multiple-choice by pairing a randomly-selected incorrect answer choice with the correct answer choice. We restrict instances to two options because the quantity we will plot when measuring discriminativity of model components (\u00a76) only takes 1 value for 2 choices, but n values for n > 2 (see App. Appendix A.1 for an elaboration).\n3.1 Datasets\nWe experiment on two challenging real-world multiple-choice datasets from LLM benchmarks: HellaSwag (Zellers et al., 2019) and MMLU (Hendrycks et al., 2021). More details are given in Appendix A.2. We additionally use a prototypical colors dataset (Norlund et al., 2021) to create a synthetic MCQA task that disentangles dataset-specific knowledge from the ability to perform symbol binding: Copying Colors from Context (\u2018Colors\u2019).\nThe dataset consists of instances such as x = \"Corn is\", y =\"yellow\". We include y in the context, so the model's only task is to produce the symbol associated with that answer choice, given one distractor answer randomly selected from the ten colors present in the dataset. We use 3 instances as in-context examples and the remaining 105 as our test set. An example formatted instance is as follows: \"Phrase: Corn is yellow. What color is corn?\nChoices: \nA: yellow\nB: grey\nThe correct answer is:\"\n3.2 Models\nOlmo. We experiment on both the base (Olmo-7B-Base) and instruction-tuned (Olmo-7B-SFT) versions of the Olmo model (Groeneveld et al., 2024). We also experiment on the Olmo-v1.7-Base model of the same size, which achieves better performance on a 4-way 0-shot formatted MCQA version of MMLU by making 0.01% of the pre-training data multiple-choice (AI2, 2024). This performance gain is reported to occur at ~500B tokens of training (Gu et al., 2024)\u2014 to investigate how model representations change once the formatted MCQA task is learned, we investigate one checkpoint before this point (350B training tokens), one checkpoint after (1T tokens), and the final checkpoint (~2T tokens).\nLlama2. We experiment with two sizes of base models from the Llama2 family (Touvron et al., 2023): 7B and 13B, as well as the instruction-tuned + RLHF-aligned 7B-Chat and 13B-Chat models. All 7B (13B) models investigated have 32 (40) layers with 32 (40) attention heads per layer.\n3.3 Identifying Consistent MCQA Models\nWe next isolate models capable of performing formatted MCQA. We design a set of 4 prompts to test whether models answer formatted MCQA prompts consistently, i.e., they are robust to the position and symbol of the correct answer choice.\nWe define consistency as always selecting one semantic answer for a dataset instance regardless of how the prompt for that instance is constructed. For example, for the two prompts \u201cA:_B: _", "A:_B:_": "where bold denotes the location of the correct answer, the model is consistent if its two predictions are \"A\" followed by \"B\" (consistently correct) or \u201cB\u201d followed by \u201cA\u201d (consistently incorrect). Conversely, if the model always predicts the same token for both prompts, this indicates a preference towards that specific token or that specific position. We refer to this as being inconsistent (always \"A\" or always \"B\", depending on the model's preference). Henceforth, since we are interested in consistent models, we will always average over \"A:_B: _", "A: _B: _\u201d prompts when reporting accuracy, and refer to these as \u201cA: _B: _": "rompts. Lastly, to understand what behaviors we localize are specific to the letters used, we additionally include prompts \u201cC: _D: _\u201d and \u201cC:_D:_"}, {"A:_B:_": "nd \u201cC:_D: _", "observations": "I) Models that are consistently correct on \u201cA: _B : _", "C: _D:_\", despite the latter prompt being less likely to occur in the training data. (II) Models exhibit differing performance on the 3 datasets, indicating the datasets capture a meaningful range of difficulty.\"\n    },\n    {\n      \"title\": \"4 Localizing Answer Symbol Production\",\n      \"content\": \"4.1 Activation Patching\nActivation patching, sometimes referred to as causal tracing (Meng et al., 2022), causal mediation analysis (Vig et al., 2020), or interchange intervention (Geiger et al., 2020), is a method for performing mediation on neural network hidden states to localize which network components have a causal effect on a model's prediction. The method involves performing the following steps:\n1. Run inference on a dataset instance that the model predicts correctly, say, one formatted as \\\"A:_B: _\\\" (xA). This will produce probabilities p(\u201cA": "xA) (high) and p(\u201cB\u201d|xA) (low).\n2. Run inference on the same dataset instance, but in a different format, such as \"A:_B:_", "patching": "the model predicts the correct answer for both XA and XB, and XA and XB do not have the same answer.\n4.2 Vocabulary Projection\nResidual connections have been shown to allow for the iterative refinement of features in neural networks (Jastrzebski et al., 2018; Simoulin and Crabb\u00e9, 2021). In addition to understanding which hidden states have the largest causal effect on predictions, it is useful to understand how predictions form in the vocabulary space defined by the dot product between the hidden state output by the LM, $h_{L,T}$, and the unembedding matrix, $W_u$. Recall that for an L-layer autoregressive LM, model predictions are a probability distribution over each item in the vocabulary V, $p \\in \\mathbb{R}^{|V|}$, given by:\n$p = Softmax(W_u \\cdot LN(h_{L,T}))$ where LN is layer normalization, and $h_{1,T}$ is the hidden state output by the final layer at the last token position.\nPrior work (nostalgebraist, 2020; Geva et al., 2021,2022a,b; Dar et al., 2023; Katz and Belinkov, 2023; Yu et al., 2023; Merullo et al., 2024b, i.a.) has proposed to project any hidden state in the Transformer block of dimensionality d to the vocabulary space, in order to inspect when and how models build up to p via their internal representations. This technique, \u201cvocabulary projection\", is equivalent to early exiting on the Transformer block at inference time (Dehghani et al., 2019; Elbayad et al., 2020; Schuster et al., 2021, 2022, i.a.).\nWe plot the probability assigned to the two answer symbol tokens (\u201cA\u201d and \u201cB\u201d or \u201cC\u201d and \u201cD\u201d) as well as the maximum probability value in p at various model states at final token position T.\n4.3 Initial Observations\nVocabulary projection reveals patterns distinct to MCQA. Since vocabulary projection can produce small values in earlier layers of a network simply because hidden states have not yet been transformed into a space mappable to high probabilities on individual tokens, we first compare results on output hidden states of individual layers between the formatted MCQA task on Colors, and a greedy generation version, where we provide instances in the format \"Corn is yellow. What color is corn?\" and take the first greedy-decoded token as the prediction (i.e., we expect the model to produce \"yellow\"). Results are shown in Fig. 3.\nWe observe, notably, that while the greedy version of the task and the \u201ctop token\" line demonstrate that hidden states can be transformed by the unembedding matrix into non-negligible probabilities as early as the output of layer 22 (for Olmo; layer 18 for Llama2), non-negligible probabilities on answer symbols emerge a couple of layers later (layer 23 for both models). Additionally, from these noted layers onward, the relative difference between the predicted token and the second choice is much smaller than in the greedy version. We conclude that it takes some extra layers of processing before output hidden states decisively predict an answer choice symbol (~layer 30 for Olmo and layer 32 for Llama2), even though updates to hidden state representations after layer 22/18 (Olmo/Llama2) are generally not changing models' greedy-generated color predictions.\nCorrectness does not dictate predictive patterns. We next investigate a dataset where a model performing consistent MCQA (Olmo-7B-SFT) does not achieve perfect performance: HellaSwag. We separate predictions by two attributes: correctness of the prediction, and which letter is predicted (\u201cA\u201d or \"B\"). Fig. 11 (Appendix) reveals that vocabulary projection does not reveal any meta-notions of correctness, as trends in vocabulary space align more with what token will be predicted (diagonal pairs) as opposed to whether that token is correct (vertical pairs). The same trend holds for activation patching (Fig. 12, Appendix). Thus, as we are most interested in correct predictions, we will only include correctly-predicted instances in subsequent plots (i.e., Figs. 5a and 5b for vocabulary projection and Fig. 6a for activation patching).\nActivation patching and vocabulary projection are complementary. As elucidated in Fig. 6a, activation patching highlights the key causal role that layer 21 plays in encoding the answer choice that Olmo-7B-SFT will predict. While layers prior to 21 may promote or demote a specific token choice, their contributions to the residual stream are overridden by layer 21's, and similarly, later layers appear to be carrying forward information contributed to the residual stream by layer 21. In Llama2-13B-Chat (Fig. 16a), we observe a more diffuse window of ~6 layers (15-21) at which activation patching is increasingly able to change the model's prediction. Compared with Fig. 6a, Figs. 5a and 5b illustrate the complementary nature of the two methods: activation patching reveals important mechanisms before hidden states are projectable to the vocabulary space, while vocabulary projection shows how relevant information in the residual stream appears in the vocabulary space over remaining layers. Indeed, one hypothesis that arises from these results is that it takes a couple of layers of processing for information encoded in a hidden state to become mapped to higher probabilities on those tokens."}, {"title": "5 Roles of Specific Layers", "content": "Key Layers are consistent across datasets. We observe projected token probabilities for Olmo-7B-SFT across all three datasets in Fig. 4, and activation patches for two datasets in Figs. 6a and 6b. Despite Olmo-7B-SFT's varying performance on the datasets, key layers are highly consistent. Notably, layer 21 also plays a key causal role in the promotion of answer symbol tokens for Colors. Similar results are found for Llama2-13B-Chat (Figs. 14, 16a and 16b). These findings corroborate the hypothesis that the same mechanisms are responsible for promoting answer symbol letters, regardless of task complexity.\nSome answer symbols are produced in a two-stage process. When using a different symbol space (i.e., \"C:_D: _\"), processing trends are noticeably different for the Olmo model (Figs. 5c and 5d). The model first assigns probability to the labels that are more likely or expected (\u201cA\u201d and \u201cB\u201d), before making an abrupt switch to the correct symbol (\"C\" or \"D\") at layer 30. Activation patching reveals the same trend (Figs. 6c and 6d): patching in prompts that promote the production of \"C\" and \"D\" causes the relative difference between \"A\" and \"B\" to close at layers 21-29, before an abrupt change to predicting \u201cC\u201d driven by layer 30. This provides evidence that Olmo-7B-SFT solidifies final label predictions for \u201cOOD\u201d prompts in later layers, and could explain why some models struggle with OOD formats."}, {"title": "6 Roles of Layer Components", "content": "We next investigate the role that MHSA and MLP functions play in the observed behavior.\nAnswer symbol production is driven by a sparse portion of the network. In Figs. 7a and 7b, we directly project the hidden states output by these functions (positions A and B in Fig. 10). Large spikes in probabilities come from both MHSA (layers 23, 30, 31) and MLP outputs (layers 28, 30). The breakdown in spiking behavior by attention heads in Fig. 8a (bottom) shows just how sparse the number of attention heads projecting to non-negligible probability values is (~8 total), even in layers where the MHSA output hidden state assigns large probabilities to \u201cA\u201d and \u201cB\u201d (recall that the MHSA output vector is a sum of these weighted attention head vectors). Activation patching results (Figs. 7c and 7d) further elucidate that the MHSA mechanism is driving the encoding of relevant information to predict the answer at layer 21, and at layer 29 for \"C:_D: _\u201d (Fig. 18, Appendix). The importance of the MHSA function as the source of the causal effect observed at layers 15-21 is also evident for Llama2-13B-Chat (Fig. 20, Appendix), though more attention heads in the model project to the vocabulary tokens (Fig. 21a).\nIndividual attention heads dominate the production of answer choice symbols (over MLPs). Plotting all attention heads and MLP layers that assign high probability to the sum of answer choices (p(\u201cA\u201d) + p(\u201cB\u201d) > 0.01) in Fig. 8a (top; Fig. 21a for Llama2-13B-Chat), we see that attention heads generally dominate the distribution, despite the fact that they only contain ~half the learnable parameters of MLPs (~67M vs. ~135M per layer). However, attention heads have access to global context which an MLP does not have, making them especially useful for resolving back-references to portions of the input sequence or copying information from one position to another.\nThe same network components that promote the production of any answer choice promote specific answer choices as well. Increasing probability on both answer choices is not the same as being discriminative, i.e., promoting one choice at the expense of another. We plot all heads and MLPs that are discriminative (|p(\u201cA\u201d) \u2013 p(\u201cB\u201d)| > 0.01) in Figs. 8b and 8c (top; Figs. 21b and 21c for Llama2-13B-Chat). There is 100% overlap: every plotted component also appears in Fig. 8a.\nModel components specialize. We observe in Figs. 8b and 8c (bottom) attention heads that take on different roles depending on the predicted symbol. Of the 13 total model components in the two plots, we observe:\n1. 3 that consistently assign high probability to \"A\". While they are consistently assigning higher probability to \u201cA\u201d than to \u201cB\u201d, their average magnitudes drop when \"B\" is predicted.\n2. 4 that consistently assign high probability to \"B\". While they consistently assign higher probability to \u201cB\u201d than to \u201cA\u201d, their average magnitudes drop when \"A\" is predicted.\n3. 3 that assign high probability to whichever token that is ultimately predicted, analogous to \"correct letter heads\" (Lieberum et al., 2023).\n4. 3 that only activate when \"B\" is predicted, though all assigning probabilities < 0.1."}, {"title": "7 Where are poorly-performing models\ngoing wrong?", "content": "Our synthetic task separates formatted MCQA performance from dataset-specific performance. In Fig. 9 (Table 3 for other models), our synthetic task confirms that there is a crucial point at which formatted MCQA skill is learned for Olmo-v1.7-7B: between 350B and 1T tokens. Performance goes from near-random to near-100%, even for checkpoints with poor performance on more challenging datasets. This result highlights the value of a synthetic task: it helps to disentangle to what extent poor MCQA performance on a dataset is due to a lack of dataset-specific knowledge vs. an inability to perform formatted MCQA.\nPoorly performing models cannot separate answer choice symbols in vocabulary space. We use vocabulary projection to inspect differences between these checkpoints on the Colors task. Fig. 22 (Appendix) illustrates the separability between the top and second-top answer choices, which grows as a function of training for more steps. Indeed, the primary distinction observed between the strong checkpoints (Figs. 22b and 22c) and the weak checkpoint (Fig. 22a) is the separability of \u201cA\u201d and \u201cB\u201d, particularly on and after layer 29. This is in spite of the fact that the 350B checkpoint model predicts \"B\" as the final answer on 99.5% of instances (Table 3), and may explain why small edits to network parameters can decrease label bias (Li and Gao, 2024). Finally, we observe in Fig. 23 (Appendix) that Olmo v1.7 7B's ability to do symbol binding as demonstrated on Colors does not alleviate the token separability problem on other datasets on which the model is less performant. Overall, these results demonstrate that an inability to disentangle answer symbol tokens in vocabulary space, not an inability to assign high probability to these tokens, is a property of poorly-performing models."}, {"title": "8 Conclusion", "content": "This work provides novel insights into the mechanisms language models employ when answering formatted multiple-choice questions. Our analysis leverages two complementary techniques: vocabulary projection and activation patching. Using carefully-designed prompts and a synthetic task, we uncover a series of stages, attibutable to a very sparse set of specialized network components, in which transformers select, promote, and align (in the case of OOD prompts) the answer choice symbol they predict. With MCQA datasets such as MMLU routinely used as a performance signal for model development, it is important to disentangle and test formatted MCQA ability in order to build models that operate in more robust and reliable ways. Our work makes an important first step towards this goal."}, {"title": "A Appendix", "content": "A.1 Plot Complexity w.r.t Number of Answer\nChoices\nThe extent to which a model component prefers\n\"A\" over other answer choices in a binary setup\nis equivalent to $p(\u201cA\u201d)- p(\u201cB\u201d)$; this quantity\nsimultaneously illustrates (in the negative direc-\ntion) the extent to which the component prefers\n\"B\". However, for 3 answer choices, say \u201cA\u201d, \u201cB\u201d,\n\"C\", 3 quantities are needed to measure preference\nfor each choice: $p(\u201cA\u201d) - max{p(\u201cB\u201d),p(\u201cC\u201d)},$\n$p(\u201cB\u201d)- max{p(\u201cA\u201d),p(\u201cC\u201d)}, and $p(\u201cC\u201d)-\nmax{p(\u201cA\u201d), p(\u201cB\u201d)}$.\nA.2 Dataset Details\nHellaSwag (Zellers et al., 2019) is a 4-way\nmultiple-choice commonsense natural language in-\nference dataset. The goal is to select the best com-\npletion for a given sentence prompt. The test set\nis not publicly available; we sample a fixed set\nof 5000 instances from the validation set used in\nour experiments (apart from activation patching,\nfor which we sample 1000 instances due to com-\npute restrictions). We use the first 3 training set\ninstances as in-context examples for all instances.\nMMLU (Hendrycks et al., 2021), or the \u201cMas-\nsive Multitask Language Understanding\u201d bench-\nmark, spans 57 different topical areas. The ques-\ntions are 4-way multiple-choice spanning subjects\nin social sciences, STEM, and humanities that were\nmanually scraped from practice materials available\nonline for exams such as the GRE and the U.S.\nMedical Licensing Exam. We sample a fixed set\nof 5000 instances from the test set used in our\nexperiments (apart from activation patching, for\nwhich we sample 1000 instances due to compute\nrestrictions). We sample a fixed set of 3 in-context\nexample instances from the 5 provided for each\ntopical area.\nWe convert both tasks to binary multiple-choice\nfollowing the procedure in \u00a73.\nA.3 Transformer Background\nThe output hidden states of transformer models\n(Vaswani et al., 2017) are a linear combination\nof the outputs of non-linear functions (i.e., multi-\nhead self-attention and multi-layer perceptrons) at\neach layer. For the Olmo and Llama2 models,\nwhich sequentially apply multi-head self-attention\n(MHSA) and a multi-layer perceptron (MLP), for\ninput hidden state $x_{l-1} \\in \\mathbb{R}^d$, the output hidden state $x_l \\in \\mathbb{R}^d$ of layer $l$ is defined by the following equation:\n$x_l = x_{l-1} + MHSA_{\\theta_l}(LN(x_{l-1}))+\nMLP_{\\theta_l}(x_{l-1} + MHSA_{\\theta_l}(LN(x_{l-1})))\n$ (1)\nWhere $MHSA_{\\theta_l}$ and $MLP_{\\theta_l}$ represent the multi-head self-attention operation and multi-layer perceptron, respectively, defined by their unique parameters at layer $l$, and $LN$ represents layer normalization. 10 Note that + represents simple vector addition, which serves to establish residual connections. This is possible because the output of each operation (MHSA, MLP, and LN) is a vector $\\in \\mathbb{R}^d$. We visualize a single layer's block structure in Fig. 10.\nElhage et al. (2021) hypothesize and provide evidence for the \"residual stream\" view of Transformer inference, in which MHSA and MLP functions \"read from\" and \"write to\" the residual stream, which carries information through the layers. Additional evidence that MHSA and MLP functions promote specific tokens by writing to the residual stream is given by Geva et al. (2022b).\nOutput hidden state\n+\nMLP\nPost-attention\nlayernorm\n+\nself-attention\n\u2191\ninput layernorm\nInput hidden state\nFigure 10: Structure of a single layer in the Transformer architecture for the models we study (Olmo and Llama2). Green lines indicate residual connections. We perform vocabulary projection and activation patching on different representations both pre-and-post residual combination, indicated by the letters.\nGiven an input token embedding $x_0 \\in \\mathbb{R}^d$ and applying Eq. (1) recursively, the final hidden state of a Transformer with $L$ layers resolves to:\n$x_L = x_0 + \\sum_{l=0}^{L-1} MHSA_{\\theta_{l+1}} (LN(x_l))+\nMLP_{\\theta_{l+1}}(x_{l} + MHSA_{\\theta_{l+1}} (LN(x_l)))\n$ (2)\nThe output of a MHSA function can be further broken down into a sum of the output of each attention head. For the input vector to layer $l$, $x_{l-1}$, the output of MHSA is computed as the concatenation of each head's vector output, $Att_{\\Theta_l}^{(h)}(LN(x_{l-1})) \\in \\mathbb{R}^{d/H}$, times an output weight matrix $W_{\\Theta_l} \\in \\mathbb{R}^{d \\times d}$, which can be simplified into a sum as follows (originally elucidated in (Elhage et al., 2021)):\n$MHSA_{\\Theta_l} (LN(x_{l-1})) =$\n$\\sum_{h=1}^{H} W_{\\Theta_l}^{(:h)} Att_{\\Theta_l}^{(h)} (LN(x_{l-1}))$\n(3)\nwhere $W_{\\Theta_l}^{(:h)} \\in \\mathbb{R}^{d \\times (d/H)}$ are the specific columns of $W_{\\Theta_l}$ corresponding to head $h$. When we perform experiments on individual attention heads, we are referring to the individual components of this sum (i.e., weighted attention head outputs)."}]}