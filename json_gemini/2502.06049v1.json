{"title": "LM2: Large Memory Models", "authors": ["Jikun Kang", "Wenqi Wu", "Filippos Christianos", "Alex J. Chan", "Fraser Greenlee", "George Thomas", "Marvin Purtorab", "Andy Toulis"], "abstract": "This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformer's general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.", "sections": [{"title": "1 Introduction", "content": "Transformer-based models have achieved remarkable success. Landmark architectures such as GPT-3 Brown et al. (2020), BERT Kenton and Toutanova (2019), and Vision Transformers Dosovitskiy (2020) have established state-of-the-art performance across a wide array of applications, including machine translation Zhu et al. (2020), text summarization Liu and Lapata (2019), question-answering Li et al. (2023), and image recognition Dosovitskiy (2020). As demonstrated by studies on large-scale models, their generalization capabilities improve significantly with increased data and model size, leading to emergent behaviors that extend beyond their original training objectives Kaplan et al. (2020); Kang et al. (2024). Despite their significant contributions, current Transformer models encounter critical limitations when applied to long context reasoning tasks Kuratov et al. (2024). For instance, in the needle-in-a-haystack problem, models must answer questions that require reasoning across facts scattered throughout exceedingly long documents. Effectively addressing tasks with extensive context demands the model's ability to discern essential information from vast amounts of irrelevant data.\nRecent memory-augmented architectures (e.g., Bulatov et al., 2022; Ko et al., 2024) attempt to tackle these challenges by using recurrent prompts to track long context information. However, these architectures primarily summarize previous answers into prompts without fully integrating long-term information, leading to performance degradation over long contexts. For example, on Task 2 (see appendix A), MemReasoner Ko"}, {"title": "2 Large Memory Model (LM2)", "content": "We present Large Memory Model (LM2), a memory-augmented Transformer model designed to enhance its long-term memory capabilities. LM2 consists of multiple Transformer decoder blocks, augmented with a memory module that dynamically stores and updates intermediate sequences of representations. The decoder block processes input sequences using positional embeddings, while the memory module interacts with these embeddings via cross attention mechanisms. We use a skip connection between the multi-head attention and the memory modules to facilitate learning and maintain the original intermediate embeddings of the Transformer. The memory updates are controlled by learnable control gates, denoted as F, I, and O, which correspond to the forget, input, and output gates, respectively. The memory module operates through two primary stages: memory information flow, and memory updates. Each of these stages is elaborated on in the following sections."}, {"title": "2.1 Memory Information Flow", "content": "As depicted in Figure 1, we introduce an explicit memory module, named the memory bank $M \\in \\mathbb{R}^{N \\times d \\times d}$, designed to store long-term memory. Here, N denotes the number of memory slots, while d represents the hidden dimension of each slot. For simplicity, each memory slot is initialized as an identity matrix: $M_r = I_{d \\times d}$, where $r \\in \\{1, ..., N\\}$ and $I_{d \\times d}$ is the identity matrix.\nWe use a cross attention-based mechanism between the memory bank and input embeddings to locate memory slots that contain relevant information. This approach is based on the idea that humans tend to store and group related information together (e.g., in Documentation Science and Archival Science (Dooley, 2007)). Note that the input embeddings E are encoded by the positional encoder, which embeds the input tokens and persists the temporal correlations between states and actions. Concretely, each input embedding E acts as the query, while the memory bank M serves as both the key and the value store. Intuitively, this means we look up \"where\" (via the key) in M to find relevant information and then retrieve it (via the value). To enable cross attention, the input embeddings $E \\in \\mathbb{R}^{T \\times d}$ (where T is the sequence length) and memory bank $M \\in \\mathbb{R}^{N \\times d}$ are projected into query (Q), key (K), and value (V) spaces:\n$Q = E_tW^Q, K = M_tW^K, V = M_tW^V,$\nwhere $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d}$ are learnable projection matrices, and t stands for decoder block t.\nThe attention scores are computed as the scaled dot product of the query and key matrices: $A = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}})$, where $A \\in \\mathbb{R}^{T \\times N}$ represents the alignment between the input sequence and memory slots. The resultant attention output is $E_{mem} = AV$, where $E_{mem} \\in \\mathbb{R}^{T \\times d}$ integrates information from the input and memory. To ensure temporal consistency, causal masking is applied, and optionally, top-k attention is used to retain only the most relevant memory interactions.\nTo regulate the influence of the memory information (gray path in Figure 1) on the existing attention information flow (pink path in Figure 1), an output gate is introduced. The output gate dynamically controls the contribution of the memory retrieval based on the cross attention output $E_{mem}$:\n$g_{out} = \\sigma(E_{mem} \\cdot W_{out}),$\nwhere $W_{out} \\in \\mathbb{R}^{d \\times d}$ is a learnable parameter matrix, and $\\sigma$ is the sigmoid activation function. The gated memory output is then computed as:\n$E_{gated} = g_{out} \\cdot M_t.$\nThe gated memory output is integrated into the standard attention flow of the Transformer decoder through a skip connection. Specifically, the output of the self-attention mechanism, $E_{attn}$, is combined with the gated"}, {"title": "2.2 Memory updates", "content": "As illustrated in Figure 2, the update process is divided into three distinct phases: the input, forget, and output (previously described). By gating how much new information is introduced and how much old information is discarded, the memory module avoids overwriting crucial long-term facts while also eliminating irrelevant or outdated content when processing long context sequences.\nInput Phase During the input phase, the model decides how much of the newly computed embeddings ($E_{mem}$) to incorporate into the memory. To achieve this, first an input gate is computed:\n$g_{in} = \\sigma(E_t \\cdot W_{in}),$\nwhere $W_{in} \\in \\mathbb{R}^{d \\times d}$ is a learnable parameter matrix, $E_t$ is the current input representation, and $\\sigma$ is the sigmoid activation function. This gating mechanism serves as a filter, deciding which relevant information should be \"written\" into memory, while also preventing the influx of noise or redundant details.\nForgetting Phase Once new information is made available during the input phase, the memory must also decide which parts of its existing content to discard. This is governed by the forget gate:\n$g_{forget} = \\sigma(E_{mem} \\cdot W_{forget}),$\nwhere $W_{forget} \\in \\mathbb{R}^{d \\times d}$. By outputting values less than one, the forget gate selectively \"erases\" memory slots that are no longer relevant, allowing the model to focus on more recent or salient information.\nMemory Update Combining these two gating mechanisms leads to the updated memory state:\n$M_{t+1} = g_{in} \\cdot \\text{tanh}(E_{mem}) + g_{forget} \\cdot M_t,$\nwhere a tanh function is applied to keep the new memory content bounded. Through these regulated phases, the memory module memorizes the most relevant information and removes outdated details, ensuring that it remains both concise and informative over time."}, {"title": "3 Pre-training LM2", "content": "We base our work on the Llama-3 model framework Dubey et al. (2024), employing it as the foundation for our Transformer architecture. Its architecture comprises 16 decoder blocks, each with a model dimension of 2,048. The feed-forward networks within these blocks have an inner dimension of 8,192. The model utilizes 32 attention heads, with 8 dedicated key/value heads.\nOur memory module extends this architecture, consisting of 2,048 memory slots, each with a dimension of 2,048. Memory modules are integrated into all 16 decoder blocks, as this configuration empirically achieves the best performance (see Section 4.3 for detailed results). The Llama-3 framework comprises approximately"}, {"title": "4 Experiments", "content": "We design our experiments to answer the following questions: Q1: How does LM2 perform in memory tasks? Q2: Does LM2 harm the performance in general tasks? Q3: Do we need to include the memory module in all decoder blocks? Q4: What is stored in the memory bank? Q5: How is the memory module updated at test-time?"}, {"title": "4.1 Performance on Memory Tasks", "content": "BABILong The BABILong dataset Kuratov et al. (2024) extends bAbI benchmark Weston et al. (2016) by incorporating significantly longer contexts and more intricate queries, thus demanding advanced memory capabilities and multi-step reasoning. By increasing both contextual and computational challenges, BABILong offers a rigorous evaluation benchmark for testing memory-augmented models.\nTable 1 presents a comparison of our model against the baselines on the BABILong dataset. We report results across multiple context lengths, from OK context-length, which is identical to bAbI dataset, to the maximum context length of 128K, which is the target context-length of the backbone Llama-3.2 model. From this table, we observe several key findings as follows:\nPerformance at bAbI benchmark (0K). Without additional context, LM2-1.7B achieves the highest average accuracy of 92.5%, surpassing Llama-3.2-1.2B, vanilla-LLama-1.7B, and RMT-1.7B, which average results are 40.7%, 75.0% and 76.4%, respectively. Because Llama-3.2-1.2B-RAG is designed for retrieval-augmented generation and evaluated only at longer contexts, it is not included in the OK setting. This suggests that LM2's underlying modeling improvements enhance its core reasoning ability.\nPerformance at Long Context Lengths (1K-4K). As context length increases, performance generally degrades for all models, but LM2-1.7B maintains a noticeable improvement over both standard and retrieval-augmented Llama variants and RMT. For instance, at 4K, LM2-1.7B's average accuracy (55.9%) is higher than Llama-3.2-1.2B, vanilla-LLama-1.7B, and RMT-1.7B, which average results are 36.8%, 42.2% and 48.4%, respectively. This gap underscores LM2's effectiveness for long-term memory ranging from 1K to 4K.\nPerformance at Long Contexts (8K-128K). Although all models exhibit some accuracy decline at these extreme long context lengths, LM2-1.7B remains robust. RMT-1.7B shows reasonable robustness, yet still falls short of LM2-1.7B on most tasks. RAG methods demonstrate some improvements over the baseline Llama, but still falls behind memory-based models. These results highlight LM2's ability to handle long context problems where Transformer-based models struggle."}, {"title": "4.2 Performance on General Benchmarks", "content": "To further evaluate if introducing an extra memory module affects LLMs' general performance, we evaluate the proposed memory-based model, LM2, on the MMLU benchmark Hendrycks et al. (2021), which tests a broad spectrum of subject areas-STEM, Humanities, Social Sciences, and Others as well as varied difficulty levels High School, College, Professional, and General Knowledge. Table 2 presents the results of LM2 in comparison to vanilla-Llama and RMT.\nOverall, LM2 demonstrates a clear performance gain, improving the average accuracy of vanilla-Llama from 28.0% to 29.4%. On the contrary, despite sharing the same pre-trained model, RMT degrades the performance of vanilla-Llama to 26.5%. Notably, LM2 achieves substantial gains in Humanities and Social Sciences, where LM2 surpasses vanilla-Llama by 3.5% and 2.4%, respectively. These categories often involve context-rich questions, suggesting that LM2's memory-based approach is advantageous for retaining and leveraging more nuanced and interconnected information. Meanwhile, LM2 also sustains competitive performance in STEM and Others, indicating its robustness beyond highly specialized domains.\nThese results illustrate that LM2 overcomes the drawback associated with memory-augmented models: performance degradation on more general tasks.\nCurrent memory-based architectures are carefully designed for memory tasks, weakening their ability to general LLM tasks. However, LM2's performance on all categories of MMLU dataset indicates that the proposed memory mechanism does not impede its general applicability."}, {"title": "4.3 Impact of memory modules", "content": "We evaluate the effectiveness of proposed memory modules using perplexity as the primary metric across varying numbers of training tokens (measured in billions). Figure 5 illustrates the perplexity trends for the baseline vanilla-Llama and LM2 with varying degrees of memory integration (i.e., 1, 6, 12, and 16 blocks), where 16 is the maximum number of blocks used in Llama-3.2-1B.\nThe results demonstrate that integrating memory information more extensively throughout the decoder leads to improved model performance. Specifically, implementing the memory module in only the first block achieves similar results to the vanilla Llama, but with slower convergence. This suggests that introducing a single memory"}, {"title": "4.4 Analysis of Memory Representations", "content": "To gain deeper insights into the information encoded within the memory module, we utilize the Neuron Explainer method Bills et al. (2023). It generates natural language explanations of neuron behavior, simulates activations using these descriptions, and evaluates their accuracy through predictive scoring. We utilize this approach to explain the latent representations of specific memory slots, which helps understand how these slots process and retain task-relevant information. By analyzing activations within the memory module, the Neuron Explainer identifies patterns in latent representations of each memory slot, mapping them to specific elements of the input text.\nWe evaluate LM2 using the input text illustrated"}, {"title": "4.5 Test-time memory adaptations", "content": "We further investigate how memory updates influence model generation during test time. To explore this, we analyze the example illustrated in Figure 4. Cross attention heatmaps, presented in Figure 6, provide key insights into these memory updates.\nFigure 6a shows the cross attention heatmap prior to memory updates. In this figure, tokens such as \"France\" and \"Paris\" strongly engage with the memory. These tokens do not pertain specifically to the target question about photosynthesis. Instead, on the first pass, memory initially focuses on the structure of question as well as identifying factual information.\nNext, we examine the memory heatmap after various inference update steps (one inference step corresponds to a single forward pass for one token). As depicted in Figure 6b, the tokens attended to by the memory slots shift toward those relevant to the target question. Since cross attention exclusively computes the relationships between input tokens and memory, this shift reflects the influence of test-time memory updates. These changes highlight the adaptive nature of memory during inference."}, {"title": "5 Related Work", "content": "Memory augmented Transformers Various methods have been proposed to augment Transformers with memory. One direction is to optimize the attention mechanisms and use some global representations acting as memory points to ensure input coverage. Models like Longformer Beltagy et al. (2020), Big Bird Zaheer et al. (2020), GMAT Gupta and Berant (2020) and Extended Transformer Construction Ainslie et al. (2020) all proposed some sparse attention mechanisms to reduce the quadratic dependency of self-attention to linear and introduced global tokens to encode the information from the entire sequence.\nAnother line of work introduces memorization capabilities to Transformers through recurrence. Transformer-XL Dai et al. (2019) addresses the limitation of fixed-length context by introducing segment-level recurrence"}, {"title": "6 Conclusion", "content": "In this paper, we introduced Large Memory Model (LM2), a memory-augmented Transformer architecture designed to address long context reasoning challenges. The key innovation is the memory module, integrated inside the decoder blocks, which augments the model with additional memory information while also updating itself. Empirical results on the BABILong benchmark highlights LM2's advantages on various long context tasks. On average across tasks, LM2 outperforms the SOTA memory-augmented RMT model by 37.1%, and a non-memory baseline Llama-3.2 model by 86.3%. Furthermore, LM2 achieves improvement over baselines on the MMLU benchmark, evidencing that its memory module does not degrade performance on general tasks. Overall, these findings underscore the importance of explicit memory mechanisms, and lay a foundation for further research on integrating long-term memory into large language models."}]}