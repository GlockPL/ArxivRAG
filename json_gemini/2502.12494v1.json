{"title": "EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness", "authors": ["Yunxiao Zhang", "Guanming Xiong", "Haochen Li", "Wen Zhao"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as AI agents. However, existing methods for enhancing LLM-agent abilities often lack a focus on data quality, leading to inefficiencies and suboptimal results in both fine-tuning and prompt engineering. To address this issue, we introduce EDGE, a novel approach for identifying informative samples without needing golden answers. We propose the Guideline Effectiveness (GE) metric, which selects challenging samples by measuring the impact of human-provided guidelines in multi-turn interaction tasks. A low GE score indicates that the human expertise required for a sample is missing from the guideline, making the sample more informative. By selecting samples with low GE scores, we can improve the efficiency and outcomes of both prompt engineering and fine-tuning processes for LLMs. Extensive experiments validate the performance of our method. Our method achieves competitive results on the HotpotQA and WebShop and datasets, requiring 75% and 50% less data, respectively, while outperforming existing methods. We also provide a fresh perspective on the data quality of LLM-agent fine-tuning.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) [Ouyang et al., 2022; OpenAI, 2023] have demonstrated remarkable few-shot learning and reasoning capabilities. An increasing number of studies have begun exploring how to leverage LLMs as agents that can accomplish various tasks through multiple interactions with the environment [Deng et al., 2023; Liu et al., 2024b; Wang et al., 2024]. For example, WebShop [Yao et al., 2022] provides a simulated shopping environment where agents must select products that best match user requirements.\nDuring interactions, LLMs frequently encounter complex or previously unseen scenarios, which places substantial de-\nmands on their generalization capabilities. Numerous studies have been dedicated to mitigating this challenge.\nPrior work has demonstrated the importance of guidelines (or insights) in prompt-based multi-turn interaction methods. Guidelines are natural language prompts summarized from data that contain more information and cover more scenarios than exemplars, while typically consuming less context space. Existing approaches autonomously gather experiences from training tasks through trial and error to generate these guidelines [Zhao et al., 2024].\nAnother line of research focuses on Supervised Fine-Tuning (SFT) of open-source LLMs to enhance their instruction-following capabilities. Prior work has shown that the effectiveness of SFT depends more on dataset quality than quantity [Wang et al., 2023; Zhou et al., 2023]. Current data filtering approaches, including GPT-4-based scoring [Chen et al., 2024], instruction difficulty assessment [Li et al., 2024], and semantic diversity metrics [Lu et al., 2024], have demonstrated varying degrees of success.\nDespite these advancements, current LLM-agent approaches still face several pressing challenges. In prompt-based methods, existing approaches for obtaining guidelines do not consider data quality control, instead randomly selecting samples from annotated data, which not only requires substantial and costly annotation efforts but also suffers from noisy data problems. Meanwhile, in SFT-based methods, current approaches heavily rely on golden answer feedback and primarily focus on single-turn instruction tuning, lacking necessary exploration of more complex multi-turn interaction scenarios that are essential for real-world applications.\nTo address these challenges, we propose Efficient Data selection for LLM agents via Guideline Effectiveness, a novel framework centered around a new metric called Guideline Effectiveness (GE) to select the most informative subset of samples from a vast unlabeled data (query) pool. These selected samples can be utilized for both prompt engineering and SFT.\nGuidelines represent human understanding of tasks and serve as prior knowledge for agents, encompassing tool usage patterns and comprehension of complex scenarios [Zhao et al., 2024; Fu et al., 2024]. The GE score essentially quantifies the impact of guidelines on each data sample, enabling us to identify which samples are most challenging for the model and thus select more informative ones. Beginning with an initial guideline, we employ an active learning approach to"}, {"title": "2 Related Work", "content": "This study investigates how to effectively utilize guidelines in the context of data selection for supervised fine-tuning (SFT).\nData Selection for SFT aims to select a high-quality subset of data. [Zhou et al., 2023] demonstrates that only 1,000 carefully curated prompts and responses can achieve remarkably strong performance. [Chen et al., 2024] proposes using GPT-4 for direct quality scoring, successfully identifying 9k high-quality samples from a dataset of 52k instances. [Li et al., 2024] introduces the Instruction-Following Difficulty (IFD) metric to identify discrepancies between a model's expected responses and its intrinsic generation capabilities. [Liu et al., 2024a] curates 6K training samples by evaluating them along three dimensions: complexity, quality, and diversity. [Bhatt et al., 2024] conducts a comprehensive evaluation of existing data selection methods that aim to maximize uncertainty and/or diversity measures. However, these evaluation metrics inherently depend on golden answers as feedback. Furthermore, they primarily focus on single-turn interactions, neglecting the complexities of multi-turn interaction scenarios. AgentTuning [Zeng et al., 2024] and FiReAct [Chen et al., 2023] investigate fine-tuning LLMs with multi-turn interaction trajectories generated by GPT-4, further examining the effects of multi-task learning and prompt design methods, respectively. However, both methods randomly select samples for annotation, and assume that perfectly correct trajectories (reward = 1) represent high quality. This approach may result in the inclusion of simpler problems in fine-tuning datasets, leading to low quality of fine-tuning data.\nDeep Active Learning aims to identify the most informative samples for annotation, thereby reducing labeling costs."}, {"title": "3 Methodology", "content": "Our core insight is to identify informative samples from an unlabeled data pool by leveraging guidelines, as illustrated in Figure 1. Given an unlabeled data pool and initial guidelines, we first compute the Guideline Effectiveness (GE) score for each sample using the initial guidelines. Samples with lower scores are selected for manual annotation, resulting in updated guidelines that can be directly applied to prompt-based methods. To fully utilize the unlabeled data, we incorporate the new guidelines into the prompt text and employ GPT-4 for annotation, generating question-interaction trajectory pairs as high-quality SFT data. Notably, this entire process does not require golden answers."}, {"title": "3.2 Preliminary", "content": "Given a set of questions Q = {q1,..., qn}, a language model LLM, and an initial guideline Ginit, we generate interaction trajectories T = LLM(Q, Ginit), where each trajectory T = (q, a1, 01,..., \u0430\u0442, \u043e\u0442) consists of question-action-observation sequences with length T. Here ai represents the action taken by the LLM at step i, and or denotes the observation or feedback received from the environment after taking action ai.\nOur objective is to select an informative subset Q' such that:"}, {"title": "3.3 Guideline Effectiveness", "content": "Guidelines are natural language prompts enriched with expert knowledge that can cover more scenarios while consuming less context space compared to detailed exemplars. However, since humans may not initially recognize all potential challenging samples, the initial guidelines may be inadequate. Therefore, we propose a metric called Guideline Effectiveness to quantify the contribution of guidelines in solving a given question in order to identify questions that are challenging for the initial guidelines.\nGiven an question q, the prompt text is constructed as:\nPrompt = CONCAT(I, G, E)\nwhere I represents the instruction text, and E denotes a set of interaction examplars. We measure the uncertainty of LLM's output action at at step t using the average cross-entropy loss of each token.\n$d_f(a_t|Prompt) = \\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i|I, G, E, T, w_{<i} : 0)$\nwhere N is the number of tokens in at, wi is the i-th token in at. A lower df indicates easier action generation.\nTo evaluate guideline effectiveness, we construct Prompt-G = CONCAT(I, E) by excluding G from the context. The difficulty of generating action without guidelines is:\n$d_0(a_t|Prompt^{-G}) = \\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i|I,E,T, W_{<i} : 0)$\nThe score df measures how hard it is to generate a using only the LLM's intrinsic knowledge, without guidelines. Based on do and df, the GE score is defined as:\n$GE(q) = \\frac{1}{T} \\sum_{t=1}^{T} \\log \\frac{d_0(a_t|Prompt^{-G})}{d_f (a_t|Prompt)}$"}, {"title": "3.4 Efficiently Incorporating Human Expertise", "content": "This section describes how we utilize the GE score to incorporate human expertise into LLMs efficiently, as shown in Figure 1. Given the data pool Q and historical interaction trajectory T, and initial guideline Ginit, we can calculate the GE score for each qi. Lower GE scores indicate challenging questions which require additional learning by the LLM. We accomplish this in two stages: Guideline Update and High-Quality Data Generation.\nUpdate the Guideline. We select m questions with the lowest GE scores. By observing these questions' interaction trajectories, we can summarize their issues and update Ginit to Gnew. Analyzing samples with the lowest GE scores allows Gnew to integrate further human expertise necessary for addressing challenging samples, such as deeper insights into the task and tools. The value of m can be small (e.g., 30), which is manageable by humans within a reasonably short time. We denote the ReAct framework augmented with the updated guideline as EDGEUG.\nHigh-Quality Data Generation. Similarly, we select k questions with the lowest GE scores and employ GPT-4 to generate interaction trajectories guided by Gnew. The incorporation of human expertise within Gnew ensures that the trajectories maintain a high standard of quality. Utilizing these high-quality data for fine-tuning, open-sourced LLMs will implicitly learn human expertise from the annotated data. Fine-tuning the open-sourced LLM is often crucial because: 1) As guidelines become more complex, it gets harder for open-sourced LLM to follow; 2) Some tasks may be too intricate to distill into guidelines, making annotating data a simpler option."}, {"title": "4 Experiments", "content": "To evaluate our approach, we have selected a range of state-of-the-art (SOTA) methods as baselines and conducted model comparisons along two dimensions:\nEDGE vs. Other Agent Methods. We compare our method against several state-of-the-art agent methods: ReAct [Yao et al., 2023] integrates reasoning and acting capabilities for sequential decision-making tasks; Reflexion [Shinn et al.,"}, {"title": "4.5 Effective Analysis in HotpotQA", "content": "To investigate whether EDGE expanded the range of solvable problems, we analyzed the distribution of question difficulty levels in HotpotQA (easy, medium and hard). Table 4 presents the proportion of each difficulty level within the subsets selected by different methods. Among the various approaches, only GE selected a higher proportion of hard questions. With more out-of-guideline questions selected for annotating, GE achieved slight advantages on easy and medium questions, and significantly outperformed the baselines on hard questions, as shown in Table 5. Consequently, GE effectively broadened the scope of solvable problems by focusing on out-of-guideline questions."}, {"title": "5 Conclusion", "content": "We propose GE metric, which can effectively identifies the most informative samples without requiring a golden answer. Selecting samples with low GE scores enhances the efficiency and outcomes of prompt engineering and fine-tuning processes for LLMs. Extensive experiments demonstrate the effectiveness of our method, and we finally provides a fresh perspective on the data quality of LLM-agent fine-tuning."}]}