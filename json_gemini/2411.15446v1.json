{"title": "freePruner: A Training-free Approach for Large Multimodal Model Acceleration", "authors": ["Bingxin Xu", "Yuzhang Shang", "Yunhao Ge", "Qian Lou", "Yan Yan"], "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities in visual-language tasks but face significant deployment challenges due to their high computational demands. While recent token reduction methods show promise for accelerating LMMs, they typically require extensive retraining or fine-tuning, making them impractical for many state-of-the-art models, especially those with proprietary training data. We propose freePruner, a training-free token reduction approach that can be directly applied to any open-source LMM without additional training. Unlike existing methods that rely heavily on token merging operations, freePruner employs a two-stage token selection strategy: (1) identifying pivotal tokens that capture high-level semantic information using our designed contribution degree metric, and (2) selecting complementary tokens that preserve essential low-level visual details through attention pattern analysis. Extensive experiments demonstrate that freePruner achieves 2x acceleration while maintaining comparable performance across mainstream visual question-answering benchmarks in the training-free setting. Moreover, freePruner is orthogonal to and can be combined with other post-training acceleration techniques, such as post-training quantization, providing a practical solution for efficient LMM deployment.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation tasks [23, 42, 54, 55]. Building upon these advances, Large Multimodal Models (LMMs) have emerged as powerful tools that integrate visual understanding with language processing [35, 37, 70]. By connecting visual encoders with LLMs, these models tackle complex visual tasks such as visual question-answering, image captioning, and visual reasoning [4, 36, 42, 54].\nDespite their impressive capabilities, the deployment of LMMs faces significant challenges due to their high computational demands. This challenge is further amplified in real-world applications where rapid response times and resource efficiency are crucial. Several approaches have been proposed to address the efficiency challenges in LMMs as surveyed in [24]. Methods such as model compression [17, 32, 52, 57], and architectural optimization [11, 12] have shown promise. Beyond the obvious efficiency bottleneck in the LLM backbone, LMMs face an additional challenge: the large number of visual tokens. The computational complexity in the LLM backbone grows quadratically with the number of input tokens, making inference particularly expensive for high-resolution images and videos. Recent works [46, 50] have demonstrated that visual tokens, which serve as prefix content for LMMs, often contain substantial redundancy that could be optimized for more efficient processing.\nHowever, existing token reduction approaches typically require extensive retraining or fine-tuning of the LMMs [6, 28, 30, 46, 50, 66], which presents significant practical limitations. Specifically, reducing the number of multimodal tokens necessitates retraining the entire model on"}, {"title": "2. Related Work", "content": "Token Reduction for Efficient LMMS\nLarge Language Models (LLMs) such as GPT-4 [42], LLaMA [55], Mistral [23], and Gemini [54] have demonstrated remarkable capabilities in text-based question answering and reasoning tasks. Building upon these advances, Large Multimodal Models (LMMs) [37, 62, 64, 70] extend these capabilities to visual domains, enabling chat-based interactions that combine image understanding with language generation. Recent developments have further expanded LMM capabilities to include region-level understanding [7, 9, 44, 67], video comprehension [31, 65], and 3D scene interpretation [22]. These multimodal architectures typically process visual information by feeding visual tokens directly into the LLM as prefix tokens, utilizing various interface mechanisms such as MLPs [34], Qformers [13, 70], or resamplers [2]. However, the number of visual tokens can become prohibitively large, particularly for high-resolution images [35, 43]. This poses a significant challenge due to the quadratic computational complexity of Transformer architectures [56].\nToken reduction has emerged as a promising approach for accelerating Transformer-based models [21]. Traditional uni-modal token reduction methods [6, 16, 30, 39, 61] focus on reducing token quantity within the internal transformer structure to decrease computational overhead. For example, token merging [6] maintains full attention while progressively reducing tokens through a bipartite matching-based selection of representative tokens. In the context of LMMs, several recent approaches have been proposed"}, {"title": "3. freePruner: A Training-free Approach for Large Multimodal Model Acceleration", "content": "In this paper, we propose a post-training LMM acceleration method in a training-free environment. In this section, we first revisit the overall structure of Large Multimodal Models (LMMs) and the architecture of transformer encoders. We emphasize the direct relationship between the number of visual tokens and the computational efficiency of LMMs (Sec.3.1). Subsequently, we present our training-free token reduction method, freePruner, designed specifically for LMM acceleration. Our method features two key components: (1) Pivotal token identification, which extracts tokens containing coarse-to-fine feature information from the visual encoders (Sec.3.2); (2) Complementary token selection, which utilizes pivotal tokens as anchors to select highly correlated visual tokens (Sec.3.3). The pipeline is visualized in Fig.3."}, {"title": "3.1. Preliminaries", "content": "Large Multimodal Models (LMMs). The fundamental concept behind LMMs is to leverage large language models (LLMs) for multimodal understanding, as illustrated in Fig. 1 (without freePruner part). Given a multimodal input X (e.g., an image or video), the system first employs a Transformer encoder [15, 56] to generate multimodal tokens Z. These tokens are then processed through a projector W that maps them into the text feature space. After aligning the visual tokens H, and textual tokens Hq in the same feature space, they are concatenated and processed by the LLM backbone fe to generate the final outputs Ya [64].\nTransformer Encoder [15, 56] serve as the bridge to transform a visual input to visual tokens representation, which later is sent to a LLM for understanding [22, 31, 35, 64, 70]. It is commonly used as visual encoders' architecture in LMMs. It is mainly composed of multiple transformer blocks that include multi-head self-attention (MSA) layers, a feed-forward neural network (FFN), skip connections, and layer normalization [3, 56]. In the encoder, an visual input is first segmented into a grid of patches, each of which is then transformed into token embeddings. As these tokens pass through successive transformer layers, their representations become progressively refined. Within the self-attention layer, each input token is transformed into three distinct vectors: query q, key k, and value v, via corresponding linear transformation matrices Wq, Wk, and Wv. Applying these transformation vectors on the inputs leads to the matrices for query, key and value: Q, K, and V. The attention matrix A measures how much attention it contributes to other elements:\n$A = softmax(\\frac{Q.K^T}{\\sqrt{dk}})$ (1)\nwhere dk is the dimension of q and k. A higher value in A indicates a greater relevance between two tokens. Self-attention generates new representations of each token by aggregating information from all other tokens, weighted by the attention scores:\nY = A.V (2)\nFollowing the self-attention layers, the transformer encoder incorporates a feed-forward network (FFN). This network comprises two layers of linear transformations that are interspaced by a nonlinear activation function, expressed as:\nFFN(X) = W2o(W1X). where W1 and W2 are the matrices of the linear transformation layers, and o denotes the nonlinear activation function.\nA critical challenge in the LMM architecture is that the computational complexity grows quadratically with the number of input tokens [53]. Since visual tokens comprise the majority of these inputs for LLM backbone, reducing visual token quantity is crucial for improving LMM efficiency [46, 50]. However, existing token reduction approaches [6, 28, 46, 50, 66] are not suitable for training-free LLM acceleration scenarios. This limitation stems from their reliance on token merging operations, which fundamentally alter the token distribution (pz\u2082). Specifically, these methods typically combine multiple tokens through weighted summation or more complex operations to create compressed representations. While such merging operations can theoretically preserve information in a more condensed form, they introduce perturbations to the original token distribution. These perturbations cause a gap between the merged tokens and the expected input distribution of the pretrained projector W and LLM fo. Consequently, the merged representations deviate significantly from the distribution the model was trained on, making them difficult to process without model retraining.\nTo achieve training-free token reduction while preserving essential information for the LLM backbone, we must first examine the internal architecture of multimodal encoders, specifically the Transformer architecture [15]."}, {"title": "3.2. Pivotal Token for High-level Feature", "content": "A crucial aspect of LMM's multimodal understanding is the effective balance between low-level and high-level visual features [7, 58]. While previous works [33, 68] have explored various approaches to feature extraction, maintaining this balance in a training-free token selection setting presents unique challenges. The primary challenge lies in identifying and selecting the most informative tokens using only the internal representations and attention patterns within the transformer, without access to additional training signals or external supervision. To address this, we first introduce a metric called the token contribution degree r\u03b9, which quantifies how much each token influences other tokens in the network. This metric is defined as:\n$r_i = \\sum A_l[:, i] - A_l[i, i]$, (3)\nwhere A represents the attention map at layer l, computed using Eq.1. This metric measures the extent to which the i-th token contributes to the entire image representation at layer l. The intuition behind this metric is straightforward: a larger ri indicates that in layer l, most tokens direct their attention to the i-th token, suggesting that i-th token effectively represents the features extracted at that layer.\nThe observed sparsity reveals an important property: in the middle layers, while most visual tokens primarily attend to themselves, a small subset of pivotal tokens emerges that significantly influences the representations of other tokens.\nBuilding upon this observation, we propose a selective token retention strategy based on the token contribution degree ri. As shown in Fig.3, our approach strategically identifies and retains tokens with high contribution degrees from deeper layers to capture high-level semantic features essen-"}, {"title": "3.3. Complementary Token for Low-level Feature", "content": "Pivotal tokens distill essential semantic content from visual inputs, focusing primarily on key object features. However, they often miss finer, low-level details, particularly in information-rich images. To address this, we propose a complementary token selection method to target and incorporate low-level features into the image representation.\nAs tokens progress through the network layers, they strive to encapsulate key information to render a comprehensive understanding. However, as depicted in Fig.4, certain tokens continue to exhibit high contribution degree in the penultimate layer, indicating that some information remains unabsorbed by the pivotal tokens. This unabsorbed information primarily consists of low-level details that pivotal tokens, with their focus on high-level semantic content, fail to capture. Thus, there is a clear need to integrate the"}, {"title": "4. Experiments", "content": "We first present the empirical results of our token reduction method, freePruner, applied to LLaVA-1.5 in Sec.4.1. Second, we explore the scalability in Sec.4.2 and generalization capability in Sec.4.3 through various experiments. Furthermore, we evaluate the efficiency enhancements achieved by employing our freePruner on LMM in Sec.4.4. Finally, we demonstrate the necessity and effectiveness of each module in our model in Sec.4.5."}, {"title": "4.1. Main Results", "content": "We implement our token reduction method to LLaVA-1.5 [34], selecting 50% of original visual tokens without any training or fine-tuning. Our evaluation spans six visual question-answering and reasoning benchmarks, including VQAv2 [20], ScienceQA [41], TextVQA [51], POPE hallucination bench [29], MME [18], and MMBench [40].\nAs shown in Tab.1, our approach not only has comparable performance with LLaVA-1.5, but also surpasses it in specific benchmarks like POPE [29] and ScienceQA [41] across both 7B and 13B LLM backbones. Comparing with existing token reduction method PruMerge+ [46], freePruner outperforms its trained and untrained version. Furthermore, our training-free freePruner method outperforms previous models that require training, such as BLIP2 [27], InstructBLIP [13], Shikra [9], and both IDEFICS-9B [1] and IDEFICS-13B [1]."}, {"title": "4.2. Scalability Analysis", "content": "We explore the scalability of our freePruner method, focusing on how increasing the number of selected tokens influences performance on six VQA benchmarks. As illustrated in Fig.7, the model demonstrates improved performance as the number of selected tokens increases on most"}, {"title": "4.3. Generalization Across Modalities and Encoders", "content": "To evaluate the broader applicability of freePruner, we examine its performance on video understanding tasks using VideoLLaVA [31] with the LanguageBind [69] encoder, a transformer-based video-language foundation model. Results in Tab.2 demonstrate that our method maintains or exceeds the original VideoLLaVA performance while significantly reducing token count (8 times). This effective-"}, {"title": "4.4. Efficiency Analysis", "content": "We further look into computational efficiency by applying freePruner on LLaVA-13B [35] and VideoLLavA [31] with A6000 GPU. The theoretical performance result is estimated by roofline-based LLM-Viewer analysis [63].\nTab.3 shows the efficiency improvement by applying freePruner on both image and video LMMs. For LLaVA-1.5, freePruner halves the visual tokens, which results in a twofold increase in prefill times. In video-LLM, VideoLLaVA, the impact of freePruner is even more substantial. By training-freely reducing the visual tokens to just a quarter of the baseline, freePruner achieves a fourfold increase in prefill times. This considerable reduction in token input also leads to a 70% decrease in memory access and an eightfold reduction in activation stor-"}, {"title": "4.5. Ablation Study on Different Modules", "content": "In this subsection, we examine the effectiveness of the two key modules in freePruner: pivotal tokens (Sec.3.2) and complementary tokens (Sec.3.3). We aim to validate the necessity of each module and verify whether the coexistence of dual modules is superior to a single module setup. All experiments utilize the LLAVA-1.5 framework with the Vicuna-7B LLM as the backbone. Fig.6 illustrates the performance comparisons among three groups: using only pivotal tokens (PT only), using pivotal tokens plus randomly selected tokens (PT + Random), and using pivotal tokens plus complementary tokens (PT + CT). The pairwise comparison of these groups, shown in Fig.8, reveal three key findings. First, simply increasing the number of tokens does not guarantee a more comprehensive visual representation. Models using only pivotal tokens outperform those using an equivalent number of tokens composed of pivotal and randomly selected tokens. This suggests that the performance is enhanced by selecting tokens that contain important information. Second, complementary tokens are not random effects. We compare two configurations with dual modules, both supported by the same quantity of pivotal tokens but differing in the addition of either complementary tokens or random tokens. The results show that a combination of pivotal and complementary tokens outperforms a mix of pivotal and random tokens. This indicates that the complementary token module provides meaningful information, rather than random effects. Third, the necessity of complemen-"}, {"title": "5. Conclusion", "content": "We present freePruner, a training-free token reduction approach for accelerating LMMs. Rather than pursuing extreme token reduction ratios, our method focuses on achieving training-free acceleration without the need for model retraining-effectively providing \u201cfree-lunch\" speedup for LMM inference. We propose a two-stage token selection strategy that identifies and retains the most informative visual tokens while requiring no model retraining or access to training data. Extensive experiments demonstrate that freePruner achieves 2x acceleration while maintaining comparable performance across various visual question-answering tasks. Moreover, our approach is orthogonal to existing acceleration techniques like quantization, offering additional pathways for efficient LMM deployment."}, {"title": "6. Appendix", "content": ""}, {"title": "6.1. Orthogonal Method with Post-training Quantization on LLMS", "content": "processing capabilities, effectively doubling the inference speed of LLaVA-Next without compromising its ability to capture fine-grained visual details."}, {"title": "6.2. freePruner on LLaVA-Next with AnyRes", "content": "Effectiveness on High-Resolution LMMs. LLaVA-Next [36] introduced the \"AnyRes\" technique to effectively process high-resolution images while maintaining data efficiency. This capability enables the model to capture fine-grained visual details, significantly reducing hallucination artifacts that typically occur when models process low-resolution inputs. The architecture's ability to handle variable high-resolution inputs makes it particularly valuable for detailed visual analysis tasks. We evaluated freePruner's compatibility with LLaVA-Next's high-resolution processing capabilities, as shown in Table 5. Note that in our implementation, we modified the standard approach by disabling adaptive token pruning and instead maintaining a fixed token length to align with the AnyRes architecture. Our method successfully reduces the token count by 50% while preserving the model's high-resolution"}]}