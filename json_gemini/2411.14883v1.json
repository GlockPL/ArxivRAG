{"title": "Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation", "authors": ["Yuheng Xu", "Taiping Zhang"], "abstract": "Domain-invariant representation learning is a powerful method for domain generalization. Previous approaches face challenges such as high computational demands, training instability, and limited effectiveness with high-dimensional data, potentially leading to the loss of valuable features. To address these issues, we hypothesize that an ideal generalized representation should exhibit similar pattern responses within the same channel across cross-domain images. Based on this hypothesis, we use deep features from the source domain as queries, and deep features from the generated domain as keys and values. Through a cross-channel attention mechanism, the original deep features are reconstructed into robust regularization representations, forming an explicit constraint that guides the model to learn domain-invariant representations. Additionally, style augmentation is another common method. However, existing methods typically generate new styles through convex combinations of source domains, which limits the diversity of training samples by confining the generated styles to the original distribution. To overcome this limitation, we propose an Adaptive Feature Blending (AFB) method that generates out-of-distribution samples while exploring the in-distribution space, significantly expanding the domain range. Extensive experimental results demonstrate that our proposed methods achieve superior performance on two standard domain generalization benchmarks for medical image segmentation.", "sections": [{"title": "I. INTRODUCTION", "content": "In modern medicine, image segmentation is a crucial technology. Its primary task is to separate different tissues, organs, or lesion areas from the background in medical images, thereby providing essential support for clinical diagnosis, treatment planning, and patient monitoring. Thanks to the rapid advancements in deep learning technologies, significant progress has been made in the field of medical image segmentation. However, numerous challenges remain in practical applications. One of the main challenges stems from the differences in imaging protocols, equipment vendors, operators, and patient populations, which often lead to discrepancies between the test data (referred to as the target domain) and the training data (referred to as the source domain). This discrepancy, known as domain shift, significantly degrades the performance of existing medical image segmentation models. To address this issue, domain generalization (DG) techniques have been introduced to enhance the generalization ability of models. Existing approaches to domain generalization can be categorized into two main types. The first type focuses on learning domain-invariant features from multiple source domains [1], while the second explicitly learns the domain shifts between multiple source domains [2], [3]. Both approaches typically rely on data augmentation to increase the diversity of the training data, thereby reducing the model's overfitting to the training data. In this context, several data augmentation methods have been proposed, such as MixStyle [4] and FDA [5], which generate new domain style samples by forming convex combinations of the statistical data of low-level features or the low-frequency amplitude components from different images in the source domain. Although these methods are computationally efficient, they are limited to generating samples within the existing distribution (as shown in Fig. 1), which may restrict the network's generalization capability. In contrast, the research [6] and [7] apply adversarial attacks to feature statistics, generating challenging adversarial samples to address the aforementioned limitation, but at the cost of introducing a significant computational burden.\nThe study [8] demonstrates that random augmentation can significantly enhance diversity and promote the learning of domain-invariant representations. However, this approach may also lead to overgeneralization or difficulty in model convergence. To address these issues, we propose a simple yet effective method called Adaptive Feature Blending(AFB), which aims to expand the domain distribution by perturbing the style information of source domain instances. We begin by randomly sampling augmentation statistics from a uniform distribution that encompasses most feature statistics. Then, we randomly mix the augmented and original statistics along the channel dimension to blend feature styles. This approach not only covers the in-distribution space but also generates out-of-distribution samples (as shown in Fig. 1), while introducing references from the original features, which could avoid the model from overgeneralizing or failing to converge due to excessive randomization.\nLearning domain-invariant representations [9], [10] is crucial in domain generalization, as it enables models to perform well on unseen target domains by learning features that are robust across different domains. Previous domain-invariant representation learning methods face challenges such as high computational resource demands, unstable training, limited effectiveness with high-dimensional complex data, and the potential loss of useful feature information. To overcome these challenges, we developed a display constraint based on a cross-channel attention mechanism called Dual Cross-Attention Regularization(DCAR) to learn domain-invariant representations. We hypothesize that an ideal generalized representation should exhibit similar pattern responses within the same channel across cross-domain images. Specifically, the generated images are merely style-transformed versions of the original images, with semantics unchanged. Since deep features of the encoder contain rich semantic information, the generated and original deep features should be highly similar. Based on this hypothesis, we use the deep features of the original image as queries and those of the generated image as keys and values. We then compute a similarity matrix between each channel of the original and generated features. The original image feature channels are reconstructed using the most similar channels from the generated image, transforming the original deep features into robust regularization representations. In this way, we establish a display constraint to guide the model in learning domain-invariant representations. Moreover, due to the semantic consistency between the generated images and the original images, we also use the deep features of the generated images as queries, with the original image features as keys and values, to build an additional display constraint. Our contributions can be summarized as follows: (1)A simple yet effective data augmentation technique generates out-of-distribution samples while exploring the intra-distribution space, significantly enhancing the diversity of the training data. (2)We propose constructing a display constraint from deep features to learn domain-invariant representations. The proposed Dual Cross-Attention Regularization can be seamlessly integrated into segmentation models as a regularization module, enabling improved domain generalization performance. (3)Extensive experiments have validated the effectiveness and"}, {"title": "II. RELATED WORK", "content": "Domain Generalization. In the domain generalization setting, models are required to be trained solely on source domain data and still perform well on unseen target domains [11], [12]. Existing methods mainly include data augmentation [4], [13], [14], adversarial training [15], [16], and domain-invariant representation learning [1], [17]\u2013[19]. Data augmentation simulates potential domain shifts by generating diverse training samples, while adversarial training enhances model robustness by creating perturbed samples. Domain-invariant representation learning aims to extract features that remain stable across different domains."}, {"title": "III. METHOD", "content": "The overview of our framework is illustrated in the Fig. 2. Given a set of D source domains $({(x_i^d, y_i^d)}_{i=1}^{N_d})_{d=1}^D$, our objective is to endow the medical image segmentation models the capability to extract domain-invariant representations. Here, $x_i^d$ is the $i^{th}$ image from $d^{th}$ source domain, $y_i^d$ is the segmentation label of $x_i^d$, and $N_d$ is the number of samples in $d^{th}$ source domain.\nRecent studies [20]-[22] have shown that statistics of CNN feature, particularly the channel mean and standard deviation of feature maps, effectively capture the characteristics of stylized images and can be regarded as representations of visual domains. Inspired by this, we propose to regularize CNN training by perturbing the style information of source domain instances. Specifically, to cover the in-distribution space and generate out-of-distribution samples, we randomly\nAdaptive Feature Blending"}, {"title": "A. Adaptive Feature Blending", "content": "sample augmentation statistics $\u03bc',\u03c3' \u2208 R^{B\u00d7C}$ (B and C respectively denote the batch size and channel) from a uniform distribution that encompasses most feature statistics: $\u03c3' ~ U(0,1), \u03bc' ~ U(0,1)$. We then sample $b \u2208 R^{B\u00d7C}$ from a Beta distribution: $b ~ Beta(a, a)$, and use $b$ as the probability to generate a Bernoulli distribution, from which we sample $\u03bb \u2208 R^{B\u00d7C}$ ($\u03bb ~ Bern(P)$), and based on [4], $\u03b1$ is empirically set to 0.1. The augmented statistics $\u03bc', \u03c3'$ and original statistics $\u03bc, \u03c3$ are then mixed along the channels:\n$\u03b2_{mix} = \u03bb\u03bc + (1 \u2212 \u03bb)\u03bc', \n \u03c3_{mix} = \u03bb\u03c3 + (1 \u2212 \u03bb)\u03c3'$,\nwhere $\u03bc, \u03c3$ are derived from the shallow features $f$ of the encoder. Finally, the mixed feature statistics are applied to perturb the normalized $f$:\n$AFB(f) = \\frac{f-\u03bc}{\u03c3}  \u03c3_{mix} + \u03bc_{mix}$.\nCompared to MixStyle [4], we use augmentation statistics randomly sampled from a uniform distribution that encompasses most feature statistics, instead of randomly selecting augmentation statistics from the source domain. This approach generates out-of-distribution samples, significantly enhancing the diversity of the training data. Additionally, by incorporating references from the original features, which could prevent the model from overgeneralizing or failing to converge due to excessive randomization."}, {"title": "B. Dual Cross-Attention Regularization", "content": "After applying Adaptive Feature Blending, we obtain original deep features $f'$ and generated deep features $f'_{AFB}$ with different styles but the same semantics. To enable the model to learn more domain-invariant representations, we leverage the inherent long-term dependencies of the attention mechanism. We leverage the semantic information of the generated deep features and apply a cross-channel attention mechanism [23], [24] to reconstruct the original deep features into robust regularization representations. Specifically, the original deep features are used as queries, while the generated deep features serve as keys and values. We calculate the similarity between each channel of the original and generated deep features, with the most similar channels of the generated deep features playing a more crucial role in reconstructing the original deep features. Given the hidden features $[f, f_{AFB}]$ of the original and generated deep features after the encoding stage, we split them then set $f$ as query and $f_{AFB}$ as key and value in multi-head manner:\n$q = fW_q, k = f_{AFB}W_k, v = f_{AFB}W_v$,\nwhere $W_q, W_k, W_v \u2208 R^{C\u00d72C}$ are transformation weights, $f, f_{AFB} \u2208 R^{H\u00d7W\u00d7C}$, C is the channel dimension. The cross-channel attention is defined as:\n$f^T = softmax [  (qk)^T ]  v W_{out}$,\nwhere $\u03be$ denote the instance normalization, $W_{out} \u2208 R^{2C\u00d7C}$. While channel-wise attention can capture long-range dependencies between channels, we also apply channel self-attention to further refine the hidden features of the generated deep features. The formula for obtaining the refined generated features is similar to (3) and (4), with the only difference being that $q$ is replaced by $q = f_{AFB}W_q$. This forms a display constraint by using the original deep features as queries to enforce consistency on the cross-domain generated deep feature representations, thereby encouraging the model to learn domain-invariant representations. Similarly, we can apply a display constraint by using the cross-domain generated deep features as queries to enforce consistency on the original deep feature representations. Finally, we fuse the reconstructed and self-attended original features as input to the decoder to generate the final prediction. The generated features undergo the same process."}, {"title": "C. Loss Function", "content": "We employ both the Dice loss [25] and the unified cross-entropy (CE) loss [26] for segmentation losses. To combat domain shift, we also use the mean squared error function as a semantic consistency loss term [3]. The overall loss function is described as follows:\n$L_{total} = \\frac{1}{D} \\sum_{d=1}^D  (L_{seg}^d + L_{consist}^d)$."}, {"title": "IV. EXPERIMENT", "content": "Our experiments use two publicly available medical image segmentation datasets: the Fundus [2] and Prostate [1] datasets.\nWe performed ablation studies to assess the effectiveness of different components in our method. As shown in Table III, each component contributed to the overall performance improvement. In particular, Adaptive Feature Blending(AFB) provided a benefit of 3.22, while Dual Cross-Attention Regularization(DCAR) resulted in a benefit of 2.32. These findings demonstrate that AFB and DCAR successfully regularize our segmentation model and enhance its generalization capability."}, {"title": "V. CONCLUSION", "content": "This paper proposes a data augmentation method called Adaptive Feature Blending, which not only explores the in-distribution space but also generates out-of-distribution samples, significantly extending the domain's coverage. Additionally, we introduce a plug-and-play module named Dual Cross-Attention Regularization for domain-generalized medical image segmentation, aimed at guiding the model to learn domain-invariant representations. We innovatively use deep features combined with a cross-attention mechanism to form regularization constraints, steering the model towards learning domain-invariant representations. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance."}]}