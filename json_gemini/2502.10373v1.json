{"title": "OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models", "authors": ["William Chen", "Jinchuan Tian", "Yifan Peng", "Brian Yan", "Chao-Han Huck Yang", "Shinji Watanabe"], "abstract": "Neural scaling laws offer valuable insights for designing robust sequence processing architectures. While these laws have been extensively characterized in other modalities, their behavior in speech remains comparatively underexplored. In this work, we introduce OWLS, an open-access, reproducible suite of multilingual speech recognition and translation models spanning 0.25B to 18B parameters, with the 18B version being the largest speech model, to the best of our knowledge. OWLS leverages up to 360K hours of public speech data across 150 languages, enabling a systematic investigation into how data, model, and compute scaling each influence performance in multilingual speech tasks. We use OWLS to derive neural scaling laws, showing how final performance can be reliably predicted when scaling. One of our key findings is that scaling enhances performance on low-resource languages/dialects, helping to mitigate bias and improve the accessibility of speech technologies. Finally, we show how OWLS can be used to power new research directions by discovering emergent abilities in large-scale speech models. Model checkpoints will be released on huggingface for future studies.", "sections": [{"title": "1. Introduction", "content": "Neural acoustic models have shown robust performance in processing human speech information and have demonstrated remarkable capabilities in spoken language tasks (Radford et al., 2023; Peng et al., 2023b; Barrault et al., 2023a). Powered by large-scale training (Baevski et al., 2020; Zhang et al., 2023; Chen et al., 2024; 2022; Li et al., 2021), Transformer-based (Vaswani et al., 2017) models have dominated the fields of Automatic Speech Recognition (ASR) and Speech Translation (ST).\nThe state-of-the-art (SOTA) in ASR/ST has now progressed to not only scaling in terms of model and data size, but also tasks and languages. In recent years, there has been significant interest in developing massively multilingual models that can perform ASR/ST for hundreds, if not thousands, of diverse spoken languages (Chen et al., 2023b; Pratap et al., 2023; Babu et al., 2022; Yu et al., 2023; Chen et al., 2024; Zhang et al., 2023), with the goal of having a single model that can universally convert multilingual speech into text.\nHowever, the architecture of these massively multilingual models is complex, and their scaling properties pose significant challenges for both experimental designs in advancing speech science. This challenge is further exacerbated by the multi-modal nature of spoken language systems, which must handle the complexities of both multilingual text and speech. Prior art on the scaling laws of neural models deviates significantly from the goal of SOTA universal systems. The majority study single-task and single-modality systems (Biderman et al., 2023; Ghorbani et al., 2022; Zheng et al., 2022), while multilingual work concentrates only on settings where a few languages are supported (Fernandes et al., 2023; Yang et al., 2023; Li et al., 2021).\nTo address this, we present OWLS, a Open Whisper-style Large-scale neural model Suite for Speech Recognition and"}, {"title": "2. Background and Related Work", "content": "Translation. OWLS contains 13 fully transparent\u00b9 speech foundation models for ASR/ST, pre-trained on up to 360K hours of multilingual data across 150 languages, with each model ranging from 0.25B to 18B parameters (Figure 1). We experiment with scaling in terms of both model and data size, and analyze the change in downstream ASR/ST performance. Through these investigations, we derive a neural scaling law to predict the change in model performance for each task and language. We also evaluate test-time capabilities of large-scale ASR/ST models, studying how new abilities emerge at scale and showing how speech model scaling can be benefits to new languages with in-context learning. Our contributions are summarized as follows:\n\u2022 We open-source OWLS, a collection of 13 Whisper-style ASR/ST models trained on up to 360K hours of publicly available data and 150 languages. We will also release all model training code, training logs, and intermediate checkpoints.\n\u2022 We train and release an OWLS model with 18B total parameters, which makes it the largest of all publicly known ASR/ST models and nearly double that of prior work (Zheng et al., 2022).\n\u2022 We systemically evaluate the effects of model and data scaling on ASR and ST, developing the first set of neural scaling laws for these tasks. We not only measure the usefulness of model scaling, but also identify failure cases that it is not able to overcome.\n\u2022 We evaluate the test-time capabilities of frozen large-scale speech foundation models via in-context learning, and discover several new emergent abilities present in large models that are absent in smaller ones.\n2.1. Neural Scaling Laws\nPrevious research has shown that the performance of Transformer-based (Vaswani et al., 2017) models at scale can be empirically predicted with three fundamental variables: the model size N, the training data size T, and the compute budget B (Hestness et al., 2017; Rosenfeld et al., 2020; Kaplan et al., 2020; Hernandez et al., 2021; Ghorbani et al., 2022; Fernandes et al., 2023). This can be summarized by modeling the change in the cross-entropy loss L when varying each variable independently:\n$L(x) = L_\\infty + \\beta x^{-\\alpha}$,\nwhere x \u2208 (N, T, B), L(x) is the reducible loss that obeys the power-scaling law, and L\u221e is irreducible loss. \u03b2 and \u03b1 are thus the empirically learned variables of the power law. Varying the value of x allows a practitioner to estimate the scaling behavior in different settings. When x = N2, for example, the power law models the data-rich (T \u2192 \u221e) and compute-rich (B \u2192 \u221e) setting. Previous work (Gu et al., 2023) in language model re-scoring has shown that the Word Error Rate (WER) can also be modeled as a power law function of x. We can thus modify Equation 1 as follows:\n$WER(x) = \\beta x^{-\\alpha}$.\nWe empirically show that this power law can also generalize to the multi-modal task of ASR (Figures 3 and 9), allowing true downstream performance to be easily predicted when x = N, B. Furthermore, we also observe that it can be applied to ST (via BLEU(x) = xxx) and thus extends our findings to more tasks (Figures 6 and 7).\n2.2. Scaling Laws for text and vision\nThe impact of scaling neural models has been thoroughly studied in the domains of text and vision. Early studies in scaling text models focused on supervised tasks such as machine translation (MT) (Gordon et al., 2021; Ghorbani et al., 2022). The most relevant work to ours is from Fernandes et al. (2023), who devised scaling laws for multilingual MT models. However, these are only trained on two translation tasks/languages. In comparison, our work evaluates on over 100 languages and tasks.\nLater studies focused instead on scaling self-supervised LLMs (Biderman et al., 2023; Tay et al., 2023; Kaplan et al., 2020). Kaplan et al. (2020) empirically showed that language modeling obeys a power law w.r.t x = N,T, and B. Biderman et al. (2023) released a suite of open-access LLMs, and showed how they can be used to understand scaling behaviors on downstream tasks. Our research can be viewed as a combination of these works, albeit applied to speech: we introduce a suite of open-access large ASR/ST models and also derive scaling laws for downstream tasks.\nIn vision, there is existing literature on the scalability of vision encoders on image classification tasks (Zhai et al., 2022). However, these tasks do not require multi-modal understanding. Our work is thus most similar to those on text-to-image/image-to-text tasks (Henighan et al., 2020). However, we focus on the speech modality while also considering multi-tasking and zero-shot behaviors.\n2.3. Multilingual Processing and Scaling in Speech\nMultilingual ASR is the concept of having a single model that can recognize speech in many languages (Watanabe et al., 2017a). While initial investigations focused on only"}, {"title": "3. The OWL Suite", "content": "3.1. Dataset\nWe largely rely on the OWSM v3.2 (Tian et al., 2024) dataset for our experiments. It consists of 180K hours of ASR/ST data gathered across 25 public corpora, covering 150 unique languages. For our experiments on scaling up the training data size beyond 180K hours, we also include an additional 180K hours of audio from YODAS (Li et al., 2023) for a total of 360K hours. More details about the dataset can be found in Section A in the Appendix.\n3.2. Training Details\nAll OWLS models follow a Transformer (Vaswani et al., 2017) encoder-decoder architecture trained using a hybrid CTC/attention (Graves et al., 2006; Watanabe et al., 2017b) loss. The inputs to the Transformer are 80-dimension log-Mel filterbanks extracted with a frame shift of 10ms, which are then down-sampled 4 times by a stack of convolution layers. The prediction targets are text tokens with a 50K sub-word vocabulary (Kudo, 2018). We also use Whisper-style training (Radford et al., 2023): all utterances are padded to 30 seconds, and the model is jointly trained to perform language identification, ASR, ST, and timestamp prediction.\nWe conduct our experiments with the ESPNet (Watanabe et al., 2018) toolkit. Since our goal is a systematic study of large-scale speech models, we take an experimental approach similar to Biderman et al. (2023): we design our experiments to prioritize training stability and controllability over squeezing out the best possible performance. We therefore use the exact same hyper-parameters for all mod-"}, {"title": "4. Pre-Training Experiments", "content": "4.1. Scaling Model Size\nWe experiment with scaling the model parameters of the OWLS models from 0.25B to 18B parameters, roughly doubling the total model parameters with each iteration. This leads to a total of 7 model sizes (0.25B, 0.50B, 1B, 2B, 4B, 9B, 18B). For each model size we scale the depth and width of the encoder and decoder in tandem, while allocating the model parameters equally between both. More details about each model can be found in Appendix B.\nMultilingual ASR: To evaluate the multilingual performance of the OWLS models, we use the 102-language FLEURS test set (Conneau et al., 2022). Figures 2 and 3 show WER for different languages as a function of per-language training data size and model size respectively, and measure their correlation with WER using the co-efficient of determination, R2. We find that model scaling consistently improves WER/CER of each language across all data levels (Figure 2). However, the amount of data used for any given language is only somewhat predictive of its WER/CER (R2 ~ 0.5, Figure 2). In other words, we cannot easily fit a language-agnostic data scaling law. On the other hand, language-specific model size scaling laws are highly predictive of WER/CER (R\u00b2 ~ 0.95, Figure 3). Finally, we want to highlight the significant improvement on WER in low-resource languages when scaling to larger model sizes. The average WER on the 50 lowest-resource languages (less than 35 hours of training data) in our dataset decreases from 59 to 45 when model size increases from an already large size of 1B to 9B. Larger models can mitigate bias and improve the fairness of speech technologies.\nMulti-domain ASR (English): We test robustness of OWLS models to different data domains by evaluating on"}, {"title": "4.2. Scaling Data Size", "content": "els, varying only the data or model size to fit the appropriate scaling experiment. More details on training can be found in Appendix B.\n4.2. Scaling Data Size\nWe evaluate how varying the amount of data used to train an OWLS model can affect downstream performance. To do so, we first create smaller training splits by uniformly downsampling the 180K hour base training set by 50%, 25%, 12.5%, and 6.25%. We also experiment with using a larger amount of data by collecting an additional 180K"}, {"title": "4.3. Scaling Compute", "content": "hours from YODAS (Section 3.1). For these experiments, we fix the model size at 1B parameters. This leads to a total of 6 different models trained on 360K, 180K, 90K, 45K, 22.5K, and 11.25K hours of speech respectively. We use an evaluation protocol similar to the one in Section 4.1, benchmarking the model on Multilingual ASR and ST.\nMultilingual ASR: Figure 5 (left) shows the effect of data scaling on the WER of each language from 11.25K to 180K hours, given a fixed model capacity. While a training set generally leads to better performance for most languages, we also observe degradations in WER/CER for some, likely due to interference from similar languages (e.g. Chinese interference for Cantonese). Figure 5 (right) shows the impact of adding in data from a new domain/distribution (YODAS) when scaling from 180K hours to 360K hours. With the addition of 180K hours of high quality data from YODAS, many languages with saturated performance when scaling from 22K to 180K hours (Korean, Polish, Dutch) experience large improvements in WER/CER. Our findings can thus be summarized as the following: data scaling without additional diversity leads to quickly saturated performance.\nTranslation: Similar to our findings in Figure 2, we find that ST data quantity is only loosely correlated with down-"}, {"content": "4.3. Scaling Compute\nAnother method of evaluating the effects of scaling is by predicting the test WER as a function of the FLOPS used for training. This allows models to be evaluated in the compute-equivalent setting and considers the fact that larger models will take longer to train. To model this relationship, we test OWLS models of various sizes on FLEURS. We only evaluate on English and two other randomly chosen languages (Spanish and Turkish) to reduce computing costs. Figure 9 shows the evolution of average WER from the 3 languages for each model size as training progresses. We find that for a fixed parameter size, the WER of the final checkpoint can be reliably predicted as a function of the training compute (R\u00b2 ~ 0.82). This means that one can reasonably predict the final WER of the model given the WERs of initial checkpoints. As expected, smaller models are more compute efficient, being able to reach a much lower WER with lower FLOPS spent."}, {"title": "4.4. Further Scaling", "content": "4.4. Further Scaling\nWe combine our findings in model and data scaling to make a preliminary exploration in further scaling OWLS models. We scale an 18B parameter OWLS model to 360K hours of data, which we designate as OWLS 18B v2. We compare"}, {"title": "5. Test-Time Experiments", "content": "5.1. Beam Search\nOne advantage of smaller models is the ability to leverage more complex decoding algorithms during inference. For larger models, using these techniques would be unfeasible within GPU memory constraints. To make the performance"}, {"title": "5.2. Emergent Ability", "content": "more fair at the compute-level, we conduct analyses where all models have the same fixed test-time compute budget. Smaller models may leverage beam search with larger beam sizes, while larger ones may be constrained to only greedy decoding. Table 2 shows the WER on LibriSpeech test-other when test-time compute is balanced at ~40-50 TFLOPS across the 0.25B, 2B, 4B, and 9B OWLS models. We note that the 0.5B, 1B, and 18B OWLS models are excluded since there is no beam size that consumes a similar number of TFLOPS. Even when using equivalent compute, larger models clearly perform better than smaller models at test-time (4.5 WER for 9B vs 8.3 WER for 0.25B). This shows the viability of large-scale ASR models in production settings.\n5.2. Emergent Ability\nLLMs are shown to exhibit drastically improved performance on certain tasks as the model size increases, even if the training data remains unchanged (Wei et al., 2022). In this section, we study if large-scale ASR models can also exhibit these \u201cemergent abilities\u00b3\u201d. We focus on three abilities that we newly discover: orthographic understanding, code-switching, and mondegreens. Results for contextual biasing, the first known example of emergent abilities in ASR models (to our knowledge), are found in Appendix F.\nOrthographic Understanding: Orthographic transparency describes the relationship between the phonetics (sounds) of a language and its written form. Opaque languages (e.g. Chinese and Japanese) have complex many-to-one or one-to-many relationships from sound to symbol, making ASR particularly difficult (Taguchi & Chiang, 2024). Examples of this phenomena are shown in Table 3. We hypothesized that larger OWLS models will exhibit enhanced robustness to orthographic opacity. To measure this, we calculate the normalized CER (N-CER) by normalizing all symbols to a single orthography. This can then be compared to the unnormalized CER. A model with a good N-CER but poor CER has strong phonetic capabilities but poor orthographic understanding. Models are tested on Taiwanese Chinese Mandarin (zh-TW) and Japanese (Figure 10). The N-CER curve shows that scaling does not have a large impact on learning phonetics: small models already exhibit strong performance in phonetically mapping speech to text. On the other hand, the steeper CER curve calculated from the raw model outputs indicate that larger models exhibit significantly stronger orthographic capabilities. Another key finding in this experiment was the overall robustness of larger models to zh-TW, which is a minority dialect rela-"}, {"title": "5.3. In-Context Learning of OWLS", "content": "5.3. In-Context Learning of OWLS\nLLMs are capable of few-shot task performance via in-context learning (ICL) (Brown et al., 2020). Large-scale ASR models like Whisper have shown potential in performing ICL, albeit with very limited capabilities. In this section, we evaluate if the ICL ability of OWLS models improve as the model size scales. To do so, we evaluate the model on ASR for a language unseen during training. We provide the model with 0 to 4 in-context examples to benchmark its ability to learn at test-time. We use Quechua as the unseen language, with data sourced from the Siminchik (Cardenas et al., 2018) corpus. We perform ICL using the same k-NN approach as Wang et al. (2024a), where k utterances with the lowest Euclidean distance (when embedded by the encoder) from the target speech are selected from the training set as in-context examples. The audio from the in-context examples are concatenated with the target speech, while the concatenated text examples are fed as an input prompt. Further details can be found in Appendix G. We find that while all model sizes are capable of using in-context examples in some capacity, only the largest models (9B and 18B) can take advantage of all three in-context examples (Table 5). For the 4B and smaller models, performance degrades when using more than two in-context examples."}, {"title": "6. Conclusion and Future Work", "content": "This paper introduces OWLS, a suite of 13 joint ASR/ST models designed to help researchers understand the scaling behaviors of multi-modal, multi-language, multi-task models. OWLS models range from 250M to 18B parameters, trained on 11K to 360K hours of speech. In fact, the 18B OWLS model is the largest speech model in known literature. With OWLS, we show that the affects of scaling parameter, training data, and compute can lead to reasonable direct predictions of downstream ASR/ST performance. We also study the emergent capabilities of large-scale ASR/ST models, showing for the first time how larger speech models exhibit stronger in-context abilities and understanding of human language. In the future, we plan to (i) scale model training to even larger datasets and more diverse tasks, and (ii) investigate more scaling effects for adaptation, while also developing new benchmarks to better understand the emergent capabilities of spoken language models with open and diverse research communities together."}]}