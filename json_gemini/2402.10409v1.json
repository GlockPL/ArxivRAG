{"title": "Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning", "authors": ["Jun Zhuang", "Casey Kennington"], "abstract": "As new research on Large Language Models (LLMS) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMS. We find that our model surpasses an average human recognition level and that fine-tuning LLMS using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-strong generalization in the taxonomy classification task.", "sections": [{"title": "Introduction", "content": "Collective attention in the field of Natural Language Processing (NLP)\u2014and the wider public-has turned to Large Language Models (LLMs). It has become so difficult to keep up with the proliferation of new models that many researchers have written survey papers to help synthesize the research progress. Survey papers are often crucial for newcomers to gain an in-depth understanding of the evolution of a research field. However, the volume of survey papers itself has become unruly for researchers-especially newcomers-to sift through. As illustrated in Figure 1, the number of survey papers has been increasing significantly. This leads to our research question, aimed at aiding the field of NLP: Is it possible to automatically reduce the barriers for newcomers in a way that can keep up with the constant influx of new information?"}, {"title": "Related Work", "content": "Taxonomy Classification Conventional taxonomy classification is a subset of Automatic Taxonomy Generation (ATG), which aims to generate taxonomy for a corpus (Krishnapuram and Kummamuru, 2003). The main challenge in ATG is to cluster the unlabeled topics into different hierarchical structures. Thus, most existing methods in ATG are clustering-based methods. Zamir and Etzioni (1998) design a mechanism, Grouper, that dynamically groups and labels the search results. Vaithyanathan and Dom (1999) propose a model to generate hierarchical clusters. Lawrie et al. (2001) discover the relationship among words to generate concept hierarchies. Within these methods, a subset, called co-clustering, clusters keywords and documents simultaneously (Frigui and Nasraoui, 2002; Kummamuru et al., 2003). Different from ATG, in this study, we classify survey papers into corresponding categories in the proposed taxonomy on relatively small and class-imbalanced datasets, whose text content contains similar terminologies.\nGraph Representation Learning Graph representation learning (GRL) is a powerful approach for learning the representation in graph-structure data (Zhou et al., 2020), whereas most recent works achieve this goal using Graph Neural Networks (GNNS) (Veli\u010dkovi\u0107 et al., 2018; Xu et al., 2018). Bruna et al. (2013) first introduce a significant advancement in convolution operations applied to graph data using both spatial method and spectral methods. To improve the efficiency of the eigen-decomposition of the graph Laplacian matrix, Defferrard et al. (2016) approximate spectral filters by using K-order Chebyshev polynomial. Kipf and Welling (2016) simplify graph convolutions to a first-order polynomial while yielding superior performance in semi-supervised learning. Hamilton et al. (2017) propose an inductive-learning approach that aggregates node features from corresponding fixed-size local neighbors. These GNNS have demonstrated exceptional performance in GRL, underscoring their significance in advancing this field."}, {"title": "Methodology", "content": "In this section, we first introduce the procedure of data collection and then explore the metadata. We further explain the process of constructing three types of attributed graphs and how we learn graph representation via graph neural networks."}, {"title": "Data Collection and Exploration", "content": "We scraped the metadata of survey papers about LLMS from arXiv and further manually supplemented the metadata from Google Scholar and the ACL anthology. The papers range from July 2021 to January 2024. Given these survey papers, we designed a taxonomy and assigned each paper to a corresponding category within the taxonomy. Our motivation is that a reasonable taxonomy can provide a clear hierarchy of concepts for readers to better understand the relationship among a large number of survey papers. Though survey papers can be taxonomized differently, we noticed two broad categories: applications and model techniques. The applications category further sub-divides into specific domains of focus (e.g., education or science), whereas model techniques further sub-divides into ways of effecting models (e.g., fine-tuning).\nWe visualize our proposed taxonomy and highlight fourteen classes, i.e., the leaf nodes, in Figure 2. The total classes in the labels are sixteen, including comprehensive and others (not shown in the figure). To better understand the distribution of the classes, we present the class distribution in Figure 3. The distribution indicates that the class is extremely imbalanced, introducing a challenge to the taxonomy classification task.\nAfter visualizing the proposed taxonomy, we further explain the motivation for proposing a new taxonomy instead of using the arXiv categories. In Figure 4, we present the distribution of survey papers across different arXiv categories. Top-2 frequent categories are cs. CL (Computation and Language), and cs. AI (Artificial Intelligence), which means that most authors choose these two categories for their works. However, these choices cannot help readers to better distinguish the survey papers. For example, papers related to model techniques are indistinguishable in arXiv categories. Thus, designing a new taxonomy is an essential step in this study.\nWe also present the word frequency in Figure 5 to show which words have been frequently used in abstracts. These distributions suggest that the abstracts of these papers contain many similar terms, which increases the difficulty of text classification."}, {"title": "Building Attributed Graphs", "content": "The goal of building the graphs is to utilize the graph structure information to classify the taxonomy. Before building the graphs, We first define the attributed graphs as follows:\nDefinition 1 An attributed graph G is a topological structure that represents the relationships among vertices associated with attributes. G consists of a set of vertices $V = {V_1, V_2, ..., v_N }$ and edges $E \\subset V \\times V$, where N is the number of vertices in G.\nGiven the Definition 1, we further define the matrix representation of an attributed graph as follows:\nDefinition 2 Given an attributed graph G(V, \u03b5), the topological relationship among vertices can be represented by a symmetric adjacency matrix $A \u2208 R^{N\\times N}$. Each vertex contains an attribute vector, a.k.a., a feature vector. All feature vectors constitute a feature matrix $X \u2208 R^{N\\times d}$, where d is the number of features for each vertex. Thus, the matrix representation of an attributed graph can be formulated as G(A, X).\nBased on the above definitions, we build the graph by creating the term frequency-inverse document frequency (TF-IDF) feature matrices for both title and summary (i.e., abstract) columns, where the term frequency denotes the word frequency in the document, and inverse document frequency denotes the log-scaled inverse fraction of the number of documents containing the word. TF-IDF matrix is commonly used for text classification tasks because it helps capture the distinctive words that can indicate specific classes (Yao et al., 2019). After establishing the TF-IDF matrices, we apply one-hot encoding on the arXiv's categories and then combine three matrices along the feature dimension to build the feature matrix X.\nTo leverage the topological information among vertices, we proceed to construct the graph structures to connect the attribute vectors. In this study, we are interested in three types of graphs: text graph, co-author graph, and co-category graph. We explain each type as follows.\nText Graph We follow the same settings as TextGCN (Yao et al., 2019) to build the text graph. Specifically, the edges of the text graph are built based on word occurrence (paper-word edges) in the paper's text data, including both title and summary, and word co-occurrence (word-word edges) in the whole text corpus. To obtain the global word co-occurrence information, we slide a fixed-size window on all papers' text data. Moreover, we calculate the edge weight between a paper vertex and a word vertex using the TF-IDF value of the word in the paper and calculate the edge weight between two-word vertices using point-wise mutual information (PMI), a popular metric to measure the associations between two words. Note that in the text graph, we don't use the above feature matrix because only paper vertices contain attribute vectors. To retain consistency, we set all values in the feature matrix as one. For the same reason, only the paper vertices are assigned labels, whereas all word vertices are labeled as a new class, which is not touched during the training or testing phase.\nCo-author Graph In the co-author graph, we introduce an edge connecting two vertices (papers) if they share at least one common author.\nCo-category Graph In the co-category graph, an edge is added between two vertices with at least one common arXiv category. In the co-authorship and co-category graphs, each vertex is assigned one class (taxonomy) as the label. Note that in this study all edges are undirected."}, {"title": "Taxonomy Classification via Graph Representation Learning", "content": "Given the well-built attributed graphs G(A, X), we aim to investigate whether graph representation learning (GRL) using graph neural networks (GNNS) can help classify survey papers into the taxonomy. Before feeding the matrix representation, A and X, of the attributed graphs G into GNNS, we first preprocess the adjacency matrix A as follows:\n$\\hat{A} = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$,\nwhere $\\tilde{A} = A + I_N$, $\\tilde{D} = D + I_N$. $I_N$ is an identity matrix. $D_{i,i} = \\sum_j A_{i,j}$ is a diagonal degree matrix.\nAfter preprocessing, we utilize GNNS to learn graph representation. The layer-wise message-passing mechanism of GNNs can be generally formulated as follows:\n$f_{W^{(l)}} (A,H^{(l)}) = \\sigma (\\hat{A}H^{(l)}W^{(l)})$,\nwhere $H^{(l)}$ is a node hidden representation in the l-th layer. The dimension of $H^{(l)}$ in the input layer,"}, {"title": "Experiment", "content": "In this section, we evaluate the graph representation learning (GRL)'s effectiveness compared with two paradigms using language models.\nExperimental Settings To examine the generalization of our method on various graph structures, we investigate three types of attributed graphs: text graphs, co-author graphs, and co-category graphs, and compare the classification performance of GRL with that of fine-tuning pre-trained language models across three subsets of our data. Both Data Nov23 and Data Jan24 contain survey papers collected at the end of corresponding months (November 2023 and January 2024). Data Jan24 includes a new category, Hardware Architecture. We further construct the third subset Data subset by removing some proposed categories with fewer instances in Data Jan24; these categories are Law, Finance, Education, Hardware Architecture, and Others. The motivation for constructing three subsets is to validate the generalization of our method across different subsets since the classification performance may significantly change on small datasets. Also, new categories may emerge at any period because research on LLMS is developing rapidly, and so are related survey papers. For example, a new category, Hardware Architecture, emerges in Data Jan24. The change of categories may affect the performance as well. Therefore, we investigate our method on three subsets that contain different categories. The statistics of our dataset and corresponding attributed graphs are presented in Table 2. Recall that the text graph consists of paper vertices and word vertices, and thus contains one additional class because all word vertices are labeled as a new class, which is not touched during the training or testing phase.\nTo evaluate our model, we split the train, validation, and test data as 60%, 20%, and 20%. Due to the potential for random splits to result in an easier task for our model, we ran the experiments five times using random seed IDs from 0 to 4 and reported the mean values with corresponding standard deviations, mean (std). We evaluate the classification performance by accuracy and weighted f1 score. Accuracy is a common metric on classification tasks, whereas the weighted f1 score provides a balanced measure of the class-imbalanced dataset."}, {"title": "Leveraging Graph Structure Information for Taxonomy Classification", "content": "We investigate whether leveraging the graph structure information can help better classify the papers to their corresponding categories in the proposed taxonomy. In this experiment, we construct the attributed graphs based on the text data (including the title and summary) and the relationship of the co-authorship and co-category. To examine the generalization of GRL, we employ GCN (Kipf and Welling, 2016) as a backbone GNN on various graph structures across three subsets. According to Table 3, GNNS fail to learn graph representation on both the text graph and the co-author graph. For the text graph, we argue that the degradation may be caused by excessively similar words in the summary of survey papers. When constructing the text graph, these word vertices connect with many paper vertices, resulting in the paper vertices being less distinguishable. For the co-author graph, we conjecture that it is challenging to categorize papers solely based on the sparse co-authorship in this dataset. Furthermore, we observe that some co-authorships come from a common mentor in the same lab whereas two first authors work on the survey papers in two distinct categories. These reasons weaken the effectiveness of using graph structure information. GNNS, in contrast, are very reliable (evaluated by both accuracy and weighted F1 score) in most co-category graphs.\nAblation Analysis We further examine the graph structures of co-category graphs by conducting ablation studies. First, according to Figure 4, most papers are assigned as cs.CL and cs.AI in the arXiv categories. Thus, we study how the categories, cs. CL and cs.AI, affect the performance by muting these two categories in a combinatorial manner. In Table 3, we observe that GNNS can maintain a comparable performance after removing either cs. CL or cs. AI. However, the performance dramatically drops after removing both categories. This is possible since most node connections are significantly sparsified after these two categories are removed. Even though both cs. CL and cs. AI do not directly map to the existing classes, either one can connect the nodes and further strengthen the message-passing in GNNS, allowing GNNS to learn better node representations.\nWe visualize co-category graphs in Data Jan24 in Figure 6. The visualization indicates that most nodes are clustered well even if we remove the category either cs.CL or cs.AI. However, after removing these two categories simultaneously, we observe that node classifications gradually become disordered and several nodes are then isolated. This visualization illustrates the effectiveness of GRL.\nWe further visualize GCNs' hidden representation on the above co-category graphs in Data Jan24 in Figure 7. The figures show that the nodes are well-classified in the hidden space even if either the category cs. CL or cs. AI is removed. However, the distribution of nodes tends to become chaotic when both of these two categories are removed simultaneously, shown in Table 3.\nFor completeness, we conducted another ablation study to examine how the categories, cs.IR, cs.RO, and cs. SE, affect the classification performance as their names are similar to that of some classes in our proposed taxonomy (recall that our proposed taxonomy is not based on the arXiv categories). According to Table 3, the classification performances are well-maintained no matter which category is removed, whereas removing cs. SE does slightly change the results (highlighted by gray color). We argue that the results are reasonable since these removals only drop a small number of edges and don't break the topological relationships in the graph. Overall, these studies verify that leveraging the graph structure information in co-category graphs can positively contribute to the taxonomy classification."}, {"title": "Fine-tuning Pre-trained Language Models", "content": "After verifying the effectiveness of GRL, we continue to investigate whether GRL can transcend fine-tuning pre-trained language models on the text data across three subsets in the taxonomy classification task. To preprocess the text data, we follow the same setup as the cleaning process for text graphs. In this fine-tuning paradigm, we use various transformer-based (Vaswani et al., 2017) pre-trained language models as competing models, such as BERT (Kenton and Toutanova, 2019), which learns bidirectional representations and significantly enhances performance across a wide range of contextual understanding tasks. The results in Table 3 and Table 4 gave us an affirmative answer to the superiority of GRL. In Table 4, we further observe that medium-size language models, such as DistilBERT (Sanh et al., 2019), work better on smaller text data. However, the performance may dramatically drop when the model size is too small, such as Albert (Lan et al., 2019) or too large, such as Llama2 (Touvron et al., 2023). We conjecture that a smaller pre-trained model may be more sensitive to the domain shift issue (our dataset has distinct class distributions compared to that of the dataset used to pre-train the language models), whereas a large pre-trained model may suffer overfitting issues when it is fine-tuned on a smaller text data."}, {"title": "Fine-tuning with Weak Labels", "content": "The above experiments confirmed that smaller (\"weaker\") GNNS can surpass larger (\"stronger\") language models in the taxonomy classification task. Recently, Burns et al. (2023). verified that training stronger models with pseudo labels, a.k.a. weak labels, generated by weaker models can enable the stronger models to achieve comparable performance as closely as those trained with ground-truth labels. In this experiment, we first generate weak labels by GCN on co-category graphs and then fine-tune pre-trained language models with weak labels. We present the results on Data Jan24 in Figure 8 as an example. The results indicate that performance achieved through training with weak labels can surpass that of training with ground-truth labels. One possible reason is that training the model using noisy labels with a low noise ratio can be equivalent to a kind of regularization, improving the classification results (Zhuang and Al Hasan, 2022).2\nThis experiment demonstrates that leveraging weak labels generated by smaller models may effectively enhance the performance of larger models. This is one of the applications related to \"weak-to-strong generalization\" (Burns et al., 2023)."}, {"title": "LLM Zero-shot/Few-shot Classification and Human Evaluation", "content": "In this experiment, we evaluate zero-shot/few-shot classification capabilities of LLMS Claude (Bai et al., 2022), GPT 3.5 (Brown et al., 2020), and GPT 4 (Achiam et al., 2023), on the text data, which contains both title and summary, on Data Nov23 as an example. We also compare the results with human participants. We recruited 18 students across five different majors in a graduate-level course. The number of students in each major is shown in Table 6. Each participant was given the titles and abstracts of survey papers and was asked to assign a category to each paper from our taxonomy. We present the mean value with the corresponding standard deviation in Table 5. For the LLMS, we ran the experiments five times. The standard deviation in human recognition is relatively large as some students do not have a strong technical background so they perform worse in this test. Among the LLMS, GPT 3.5 outperforms the other two models given that all models have not seen the data before (zero-shot). We further provide some hints to the models before classification (few-shot). For example, we release the keywords of the class \"Trustworthy\" to the models before classification. In this setting, both GPT 3.5 and GPT 4 can achieve higher accuracy and a weighted F1 score after obtaining some hints. In brief, GRL can outperform all three LLMs and human recognition, whereas these LLMS couldn't surmount human recognition, which reveals that LLMS still have much room to improve in taxonomy classification."}, {"title": "Conclusion", "content": "In this work, we aim to develop a method to automatically assign survey papers about Large Language Models (LLMs) to a taxonomy. To achieve this goal, we first collected the metadata of 144 LLM survey papers and proposed a new taxonomy for these papers. We further explored three paradigms to classify survey papers into the categories in the proposed taxonomy. After investigating three types of attributed graphs, we observed that leveraging graph structure information on co-category graphs can significantly help the taxonomy classification. Furthermore, our analysis validates that graph representation learning outperforms pre-trained language models' fine-tuning, zero-shot/few-shot classifications using LLMS, and even surpasses an average human recognition level. Last but not least, our experiments indicate that fine-tuning pre-trained language models using weak labels, which are generated by a weaker model, such as GCN, can be more effective than using ground-truth labels, revealing the potential for weak-to-strong generalization in the taxonomy classification task."}, {"title": "Limitations & Future Work", "content": "Constructing a graph structure may encounter certain constraints. For instance, we build co-category graphs based on the arXiv categories. When papers come from distinct fields, such as biology, physics, and computer science, the graph structure may be very sparse, weakening the effectiveness of GRL.\nIn the future, our primary motivation extended from this study is to tailor GPT-based applications to assist readers in understanding survey papers more effectively. We also plan on further exploring the weak-to-strong generalization which could potentially have many important applications."}, {"title": "APPENDIX", "content": "In the appendix, we present the GNN and pre-trained language models' hyper-parameters and the hardware and software. We also include the additional comparison results about fine-tuning using weak labels and additional visualization of co-category graphs.\nHyper-parameters and Settings We employ a two-layer GCN (Kipf and Welling, 2016) with 200 hidden units and a ReLU activation function as the backbone GNN to examine the effectiveness of GRL. The GNN is trained by the Adam optimizer with a learning rate, 1 \u00d7 10-2 for both co-author graphs and co-category graphs and 2\u00d710-2 for text graphs, and converged within 500 training epochs on all subsets. The dropout rate is 0.5.\nWe fine-tune the pre-trained language models using the Adam optimizer with a 1 \u00d7 10-4 learning rate. We chose the batch size of 8 for the Llama2 and fixed the batch size of 16 for the rest of the models. We implement the pre-trained language models using HuggingFace packages (we choose the base version for all models) and report the model size in Table 7. All models are tuned with 30 epochs.\nHardware and Software The experiment is conducted on a server with the following settings:\n\u2022 Operating System: Ubuntu 22.04.3 LTS\n\u2022 CPU: Intel Xeon w5-3433 @ 4.20 GHz\n\u2022 GPU: NVIDIA RTX A6000 48GB\n\u2022 Software: Python 3.11, PyTorch 2.1, HuggingFace 4.31, dgl 1.1.2+cu118.\nComputational Budgets Based on the above computing infrastructure and settings, computational budgets in our experiments are described as follows. The experiment presented in Table 3 can be reproduced within one hour. The experiment displayed in Table 4 may take 93 hours to complete. Due to limited GPU memory, we implemented Llama2 using the CPU. This consumes around 90 hours in total. The experiment shown in Table 5 (excluding human recognition) can be finished in one hour.\nAdditional Visualization of Co-category Graphs Besides visualizing four graph structures in Data Jan24 in Figure 6, we additionally present the visualization of four corresponding co-category graphs in both Data Nov23 and Data subset in Figure 9. The visualization verifies the generalization of GRL across three subsets.\nAdditional Comparison of Fine-tuning Using Weak Labels Besides the results in Figure 8, we supplement the comparisons on both Data Nov23 and Data subset in Figure 10. The comparisons across nice pre-trained language models further validate the effectiveness of fine-tuning using weak labels."}, {"title": "Ethical and Broader Impacts", "content": "We confirm that we fulfill the author's responsibilities and address the potential ethical issues. In this work, we aim to help researchers quickly and better understand a new research field. Many researchers in academia or industry may potentially benefit from our work."}, {"title": "Statement of Data Privacy", "content": "Our dataset contains the authors' names in each paper. This information is publicly available so the collection process doesn't infringe on personal privacy."}, {"title": "Disclaimer Regarding Human Subjects Results", "content": "In Table 5, we include partial results with human subjects. We already obtained approval from the Institutional Review Board (IRB). The protocol number is IRB24-056. We recruited volunteers from a graduate-level course. Before the assessment, we have disclaimed the potential risk (our assessment has no potential risk) and got consent from participants."}]}