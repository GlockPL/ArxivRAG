{"title": "Harmful YouTube Video Detection:\nA Taxonomy of Online Harm and MLLMs as Alternative Annotators", "authors": ["Claire Wonjeong Jo", "Miki Weso\u0142owska", "Magdalena Wojcieszak"], "abstract": "Short video platforms, such as YouTube, Instagram, or TikTok, are used by billions of users globally. These platforms\nexpose users to harmful content, ranging from clickbait or physical harms to misinformation or online hate. Yet, detecting\nharmful videos remains challenging due to an inconsistent understanding of what constitutes harm and limited resources\nand mental tolls involved in human annotation. As such, this study advances measures and methods to detect harm in video\ncontent. First, we develop a comprehensive taxonomy for online harm on video platforms, categorizing it into six\ncategories: Information, Hate and harassment, Addictive, Clickbait, Sexual, and Physical harms. Next, we establish\nmultimodal large language models as reliable annotators of harmful videos. We analyze 19,422 YouTube videos using 14\nimage frames, 1 thumbnail, and text metadata, comparing the accuracy of crowdworkers (Mturk) and GPT-4-Turbo with\ndomain expert annotations serving as the gold standard. Our results demonstrate that GPT-4-Turbo outperforms\ncrowdworkers in both binary classification (harmful vs. harmless) and multi-label harm categorization tasks.\nMethodologically, this study extends the application of LLMs to multi-label and multi-modal contexts beyond text\nannotation and binary classification. Practically, our study contributes to online harm mitigation by guiding the definitions\nand identification of harmful content on video platforms.", "sections": [{"title": "1 INTRODUCTION", "content": "5.07 billion people, or 62.6 percent of the world's population, use social media platforms and this number has increased\nover the past decade (Statista, 2024). In this context, there are serious concerns that platforms direct users to harmful\ncontent. A teenager interested in fitness may be recommended content about eating disorders, a user interested in herbology\nmay encounter misinformation, and a sad adolescent may easily watch content about depression or suicide (Hilbert et al.,\n2023). In fact, 66% of UK adults report encountering harmful content on social media (Turing Institute, 2023), platform\naudits indicate that self-harm and suicide-related posts on Instagram range from 9% to 66% (Picardo et al., 2020), and\nother observational work finds that YouTube often promotes drugs, alcohol, and bullying (Hartas, 2021; Hattingh, 2021).\nAccordingly, 90% of US adults state that social media have harmful effects (Statista, 2023) and research finds that social\nmedia use leads to mental health issues, eating disorders, self-harm, and misinformation endorsement, among other\nproblems (Haidt & Twenge, 2023; Hartas, 2021).\nPlatforms have implemented community guidelines and policies to regulate harmful content. However, content\nmoderation presents two challenges. First, due to the subjective and context-dependent nature of harm, we lack a systematic\nand overarching taxonomy and consistent harm detection criteria. Currently, online platforms consider different content as\nharmful (e.g., Twitter regulates sexual abuse, while Google's content policy does not mention it; see Arora et al., 2023)\nand researchers differently define and measure harm (e.g., Banko et al., 2020; Scheuerman et al, 2020; Shelby, 2022). In\naddition, extant definitions and operationalizations are best suited for text-based messages, thereby overlooking the\nincreasing popularity of (short) video platforms, such as YouTube, Instagram, or TikTok.\nSecond, harm detection often involves human annotators or moderators. For example, a machine learning harm detector\nof YouTube was trained with human reviewers' reported videos, and academic classifiers for specific harms have been\nprimarily built on human labels (Burnap & Williams, 2015; Davidson et al., 2017; Founta et al., 2018; Roberts, 2016).\nHowever, given the mental toll on human annotators who review harmful content (Dang et al., 2020; Ghoshal, 2017) as\nwell as the time and costs needed for manual annotation, this approach is not scalable for handling large amounts of\nconstantly emerging and rapidly changing data.\nThis project addresses these issues. We first propose an overarching taxonomy for online harm by synthesizing existing\ntaxonomies and platform community guidelines. Our taxonomy contains six non-mutually exclusive categories:\nInformation harms, Hate and harassment harms, Clickbait harms, Addictive harms, Sexual harms, and Physical harms.\nEach category is designed to be identifiable within a multimodal context, incorporating text, audio, and image data, making\nit applicable to most types of social media and video platforms.\nNext, we propose multimodal large language models (MLLMs) as alternatives to human annotators for identifying\nharmful videos. Recent studies reveal remarkable capabilities of Large Language Models (LLMs) across text annotation\ntasks (Kim et al., 2024; Ostyakova et al., 2023; T\u00f6rnberg, 2023), including in harmful text annotation (Hoes et al., 2023;\nHuang et al., 2023; Sekharan & Vuppala, 2023). Yet, their use for multi-modal and multi-label content is underexplored\n(Kroon et al., 2023). We leverage GPT-4-Turbo API and compare its performance to crowdworkers, setting domain expert\nlabels as a gold standard. We focus on YouTube videos\u00b9, yet our taxonomy and methodological approach can be applied\nto other (short) video platforms. We compile a large set of likely harmful YouTube videos and use 19,422 videos for the\ncomparison. Text-formatted metadata (title, channel name, description, and transcript) and image (14 image frames and 1\nthumbnail) data were fed into the GPT-4-Turbo model and labeled by crowdworkers and domain experts. To control for\nrandomness and increase reliability, we used three API keys for GPT and three crowdworkers, selecting the majority\nanswer from each.\nOur project offers several contributions. First, we integrate past work to develop a systematic and comprehensive\ntaxonomy of harmful online content. This taxonomy can set a framework for defining, operationalizing, and assessing"}, {"title": "2 HARMFUL ONLINE CONTENT", "content": "Harmful content is an expansive term, which is understood and measured differently across platforms and by different\nresearchers. There are several taxonomies of online harm, shown in Table 1. For instance, Banko et al. (2020) reviewed\nguidelines from seven major tech platforms, international treaties, and the United Nations human rights conventions, field\nexperts, and civil organization proposals. Their qualitative analysis identified four main types of online harm: hate and\nharassment, self-inflicted harm, ideological harm, and exploitation. In turn, Scheuerman et al. (2020) detailed the harms in\nonline content as physical harm, emotional harm, relational harm, and financial harm. In a more recent taxonomy, Shelby\net al. (2022) synthesized literature on harms from algorithmic systems, proposing representational harms, allocative harms,\nquality-of-service harms, interpersonal harms, and social harms as their categories.\nIn addition to more formal taxonomies, scholars also focus on identifying individual harms online. Some studies extract\nspecific harm types described in platform guidelines, such as self-harm, misinformation, and hate speech (Arora et al.,\n2023; Gongane et al., 2022; Schaffner et al., 2024). Others focus on message features such as incivility, insults, or profanity\n(Davidson et al., 2020; Rains et al., 2017; Stoll et al., 2020), intolerant speech (Rossini, 2022), and toxicity, which includes\nobscenity, insult, identity hate, and threat (Google Jigsaw, 2018; Van Aken et al., 2018; see also Androcec, 2020;\nChakrabarty, 2020; Kurita et al., 2019; Kumar et al., 2021; Zaheri et al., 2020). Further complicating a joint understanding,\nresearchers employ different terminologies, such as problematic content (Hilbert et al., 2023; Yesilada & Lewandowsky,\n2022), inappropriate content (Gongane et al., 2022), and \"dark side\" themes (Hattingh, 2021).\nThese previous categorizations, although needed and important, have a few limitations. First, specific harm types are\nassigned to distinct categories in different taxonomies. For instance, information harms, which entail mis(dis)information\nand fake news, are classified as ideological harms in Banko et al.'s (2020) and as sociotechnical harms in Shelby et al.'s\n(2023) taxonomy. Yet, other frameworks place misinformation separately (Hattingh, 2021; Shahi & Tsoplefack, 2022).\nThese differences make it challenging to holistically understand and tackle the problem. There is no clear estimate on\nexactly how much harmful content there is on platforms, and no common agreement about what exactly constitutes hate\nspeech, for instance. However, although each approach sheds light on a fragment of online harm and offers distinct\ndefinitions and operationalizations, these different categorizations and taxonomies integrated together can provide a more\ncomprehensive insight.\nAs importantly, many of these categorizations are developed based on and for text data only (e.g., Wikipedia, news\ncomments, YouTube comments). To illustrate, toxicity, intolerance, and incivility are primarily text-targeted concepts that\nmostly rely on linguistic characteristics and cannot capture visual messages that convey nonverbal information. As a result,\nthe resulting classifiers and open-source tools to capture toxicity and incivility are primarily applicable to text-formatted"}, {"title": "3 IDENTIFYING AND CLASSIFYING ONLINE HARM", "content": "In addition to systematically conceptualizing harm, it is essential to develop methods for identifying and classifying such\ncontent online. Traditionally, human annotation or manual labeling by human workers, play a crucial role in training\nclassifier models (Plank, 2022). Human-annotated data are believed to be accurate and reflect the specific language task\nthat is being targeted (Schlangen, 2021). To train, test, and evaluate models, researchers rely on the agreement among\nhuman annotators (Aroyo & Welty, 2015). Although this approach is a gold standard, there are a few important concerns\nabout human annotation if applied at scale.\nFirst, manual labeling is not scalable for building large datasets, particularly when processing videos, due to the time\nand costs needed for recruiting and training human annotators and the time it takes them to view and label extensive\namounts of content (Shvetsova et al., 2023). In practice, existing manually annotated datasets are not sizable enough, such\nas MSR-VTT (Xu et al., 2016) with 10,000 videos, YouCook2 (Zhou et al., 2018) with 2,883 videos (Shvetsova et al.,\n2023), and YouNICon (Liaw et al., 2023) with 3,161 videos. Additionally, the mental strain on human annotators who\nreview harmful content is a critical problem. Social media companies employ human reviewers to supplement automated\nmoderation systems (Roberts, 2016). However, repeated exposure to harmful content has negative psychological and\nemotional effects (Dang et al., 2020). For example, moderators from Microsoft report post-traumatic stress disorder (PTSD)\nafter reviewing harmful content, including videos of child abuse and violent crime (Ghoshal, 2017). Therefore, alternative\nannotation methods that are cost-effective, faster, and devoid of human-related ethical concerns are needed.\nAgainst these challenges with human annotation, recent studies demonstrate the remarkable capabilities of LLMs,\nparticularly OpenAI's GPT, in content annotation. Since its advent, GPT has been seen as cheaper and faster than\ncrowdworkers, offering quality results that are comparable to, or even exceed, those of humans (Ostyakova et al., 2023).\nThese studies propose LLMs as a viable alternative to humans for text annotation. For instance, GPT has superior\nperformance than Mturk workers and human experts in classifying tweets as being from Democrats or Republicans\n(T\u00f6rnberg, 2023), identifying messages as negative, positive, or neutral, left- or right-leaning or moderate (Heseltine &\nHohenberg, 2023), or detecting relevance, topics, and frames in tweets and news articles (Gilardi et al., 2023). More\ngermane to our focus, GPT has strong capabilities in detecting misinformation (Hoes et al., 2023) and hate speech (Huang\net al., 2023) as well as classifying content as anti- or pro-vaccine (Kim et al., 2024). These findings highlight the robustness\nand versatility of LLMs across various annotation tasks.\nHowever, while the performance of LLMs for text annotation is extensively explored, the analysis of multi-modal data\nis understudied (Kroon, 2023). Researchers primarily rely on text-formatted metadata even when analyzing multi-modal\ncontent (Christodoulou, 2023). This may be because stable LLMs, such as OpenAI's GPT, Meta's Llama, and Anthropic's\nClaude, were limited to only text until 2024. Although multi-modal models like OpenAI's CLIP, VisualBERT (Li et al.,\n2019), and MARMOT (Wu & Mebane, 2022) were available, they either required separate training processes (VisualBERT,\nMARMOT) or had limitations in accuracy and capabilities (CLIP model).\nThe advent of Multimodal Large Language Models (MLLMs) that expand their capabilities to image as well as text,\nsuch as GPT-4-Turbo, creates new possibilities for classifying multi-modal social media data, which become increasingly\nprevalent with the rapid growth of video platforms. Our study employs GPT-4-Turbo, one of the OpenAI's MLLM, and\ncompares its accuracy in identifying online harm with that of crowdworkers, using domain experts' labels as the ground\ntruth."}, {"title": "4 \u03a4\u0391\u03a7\u039f\u039dNOMY FOR ONLINE HARM IN VIDEO PLATFORMS", "content": "Before outlining our data and classification, we detail our approach to creating a taxonomy for online harm in video\nplatforms. All details on the referenced materials are presented in the Appendix, referenced below. To develop our\ntaxonomy, we drew on the grounded theory approach (Glaser & Strauss, 2017), which entailed reviewing existing literature\non harm taxonomies and on specific types of harm. We also reviewed community guidelines of YouTube, Meta, and\nTikTok. By synthesizing, converging, and reorganizing the subcategories in existing taxonomies and platform policies, we\narrived at broader categories based on their commonalities. Although certain categories or definitions (e.g., clickbait) may\ncarry different \"weight\" than others (e.g., hate), they are all present in the reviewed literature and platform guidelines. In\nSM Table S1, we present the identified platform community guidelines and show how our categorization fits with the\nexisting guidelines.\nOur taxonomy is based on three principles. (1) Each category is discernible within the multimodal context,\nencompassing text, audio, and visual data. Categories not identifiable within this context were excluded. For example,\nquality-of-service harms from Shelby et al. (2022), which refer to negative experiences when technologies fail to meet\nneeds or exclude individuals, cannot be discerned from individual YouTube videos. (2) A single piece of content can fall\nunder multiple harm categories. Each category may interact within the same or different modalities. For instance, a video\nthat narrates hate speech towards women while showing clips of women being punched can be both hate and harassment\n(due to the audio) and physical harm (due to the visuals). A video introducing fake methods to earn quick money can be\nclassified as both misinformation and clickbait, incorporating its audio, visual, and text modalities. (3) Maximizing\nobjective assessment. Although the idea of harm naturally entails normative judgments and some subjectivity (e.g., what\none user sees as harmful may not be considered as such by another user), we aimed to minimize subjective judgment. We\navoid broad categories, such as \"problematic,\" and do not include a category of \"extreme\" or \"radical\" content. Instead,\neach category and its subcategories are designed to be as objective and verifiable as possible in this context. Thus, we focus\non categories that are broadly agreed to be harmful to individuals, groups, or society (as determined by the platform\nguidelines and existing taxonomies) and on message factors that are observable from the videos and not inferred by an\nindividual user.\nWe identified six categories that capture harmful content on video platforms (see Table 2). First, Information Harms\nrelate to the dissemination of false information that misleads and deceives people. This includes fake news, misinformation,\ndisinformation, conspiracy theories, unverified medical treatments, and unproven scientific myths. Such content can\nundermine public trust in science, generate risks to public health, and exacerbate cynicism and extremism (Lazer et al.,\n2018; Walter et al., 2021). Hate and Harassment Harms pertain to the promotion of hatred towards specific groups. This\nincludes insults and obscenities, identity attacks, and hate speech based on gender, race, ethnicity, age, religion, political\nideology, disability, or sexual orientation. Such content fosters hostility and discrimination with the intent to degrade the\ntargeted groups. It may lead to psychological distress, systematic (online and offline) demobilization of groups subjected\nto hate (Keum et al., 2024; Wypych & Bilewicz, 2022), and offline harassment and violence (Hefit & Ausserladscheider\nJonas, 2020; Schumann & Moore, 2023). Addictive Harms refer to content that promotes or glorifies behaviors associated\nwith addiction, including excessive gaming, gambling, or substance use (drugs, smoking, or alcohol). This category\nencourages compulsive engagement and normalizes restricted or unhealthy activities, potentially leading users to develop\naddictions or lose money (Moreno & Whitehill, 2014; Wakefield et al., 2003). Clickbait Harms are characterized by\nsensationalized content designed to attract clicks without delivering valuable information. This includes exaggerated\nheadlines intended to boost click rates, unverified financial schemes, and sensational gossip or defamatory videos\n(Chakraborty et al., 2016; Chen et al., 2015; Sekharan & Vuppala, 2023). Clickbait often misleads viewers and degrades"}, {"title": "5 DATA, LABELING, AND PERFORMANCE COMPARISON IN HARMFUL VIDEO DETECTION", "content": "Once the harm taxonomy is constructed, we examine which performs better as an annotator in detecting harmful context-\nhumans or Multi-modal Large Language Models (MLLMs). Applying our taxonomy, we determine (a) whether a video is\nharmful, and (b) to which category or categories the content belongs, integrating both text and image data in the process.\nThis study was approved by the IRB of the author's institution."}, {"title": "5.1 Data: Potentially Harmful YouTube Dataset", "content": "We construct a dataset of potentially harmful YouTube videos. Given that content that violates platform policies is often\nremoved by platforms, leaving limited access to such content (Arora et al., 2023), we leveraged three approaches to obtain\na large dataset representing all the six harm categories. The details on each approach are available in A.2 and mentioned\nbelow.\n1. Keyword-based approach: We developed a list of specific keywords (n=169) targeting each harm category from\nthe taxonomy. This list incorporates keywords from previous studies (e.g., Chancellor et al., 2017; Hussein et al.,\n2020; Scherr et al., 2020) and from platform community guidelines. We retrieved between 50 to 500 search results\nper keyword and harm category, applying recency and relevance filters in YouTube search. The relevance filter\ngathers videos sorted by relevance, capturing the top results when search queries are entered into the engine. In turn,\nthe recency filter collects videos sorted by the most recent updates. We applied these filters in a 7:3 ratio because\nwe found that the relevance filter often retrieves informative videos, while the recency filter captures more alarming\ncontent. For example, searching 'self-harm' with the relevance filter mostly returned prevention or recovery videos,\nwhereas the recency filter showed self-harm experiences from smaller channels. To address this discrepancy, we\nmore than doubled the weight of the recency filter. We fed English queries and collected outcomes in the American\nYouTube search environment.\n2. Channel-based approach: We identified harmful YouTube channels. A research assistant compiled a list of harmful\nchannels (n=100) categorized by each harm type by searching for channel information within online communities\nsuch as Reddit and including well-known large channels that are notorious for spreading potentially harmful content.\n3. External dataset integration. We relied on previously compiled lists of problematic channels from published studies.\nWe included channels identified as anti-feminist (n=112; Mami\u00e9 et al., 2021), White identitarian (n=15), and\nIntellectual Dark Web (n=33; Ribeiro et al., 2020). In addition, we incorporated datasets classified as misinformation\nby previous studies. Although most health-related videos (e.g., COVID-19 and anti-vaccine content) had been"}, {"title": "5.2 Harmful video classification", "content": "These videos were classified by: (a) GPT-4-Turbo as the MLLM, (b) MTurk workers as the crowdworkers, and (c) domain\nexperts as the gold standard. Figure 1 provides an overview of the process, illustrating a single video, instructions given to\ncrowdworkers, domain experts, and GPT-4-Turbo, and the outputs obtained for the video. For GPT-4-Turbo and\ncrowdworkers, we applied a majority consensus rule. All these steps are outlined below, and relevant supplemental\nmaterials are provided in A.3, A.4, and A.5."}, {"title": "5.2.1Domain Expert as Gold Standard", "content": "Similar to previous studies (Kim et al., 2024; Ostyakova et al., 2023; T\u00f6rnberg, 2024), we set the domain experts'\nannotations as the gold standard for our comparison. Ten undergraduate students majoring in communication and digital\nmedia served as domain experts.\nExpert Training. The experts underwent extensive training on the harm taxonomy and the expert team and principal\ninvestigator held periodic meetings to resolve any ambiguous cases and discuss disagreements. Experts were instructed to\nclassify each video provided to them into one or more harm categories, or as harmless, unavailable (for removed videos),\nor unsure. For non-English videos, they made a note of any language barriers. When reviewing videos longer than 5 minutes,\nexperts were allowed to skim by adjusting the mouse pointer or playback speed. The training materials and detailed\nprocedures for domain experts are provided in A.3. Over four months, ten experts labeled 19,422 videos.\nIntercoder Reliability. Intercoder reliability (ICR) was calculated using 11.4% (2,225) of the total videos. To calculate\nICR, we assigned the same 265 videos to five pairs of experts. Experts within each pair watched the same videos, but\ndifferent videos were assigned across groups. Additionally, 900 videos were assigned to random pairs from the ten experts.\nThe average ICR was calculated for six groups: the five predetermined groups and one random group. In the case of binary\nclassification (harmful vs. harmless), the percentage agreement was notably high at 88%. Due to the imbalanced"}, {"title": "5.2.2GPT-4-Turbo", "content": "Prompt Engineering. We used OpenAI's multimodal large language model (MLLM), GPT-4-Turbo API, for our\nclassification tasks. In LLM-based annotation, a stable prompt is important, as small changes in the prompt can yield\ndifferent outcomes (Barrie et al., 2024). To enhance the stability and accuracy of generated outputs, we implemented\nprompt engineering before the annotation process. We designed multiple prompts, assessed their performance, and selected\nthe highest-performing prompt by comparing each prompt's accuracy to the ground truth data (i.e., the 20 sample videos\nfrom our dataset; see Appendix Table 11). The testing prompts and the development procedure are described in A.4. Table\n4 shows the finalized prompt. It was designed to be a zero-shot learning prompt to avoid narrowing down the classification\nreasoning to a limited set of examples, given a wide range of possible cases within each harm category. GPT has already"}, {"title": "5.2.3Crowdworkers", "content": "Recruitment. We used Mturk (Amazon Mechanical Turk), an online crowdsourcing platform, to recruit participants.\nTo maximize the quality of the annotation, we recruited those with a HIT approval rate above 95% and Master status on\nthe platform. 10 We also set recruiting filter features to make sure that all participants were 18 years or older and English\nspeakers. As the task entailed harmful content labeling, a warning message was displayed in the task title, and the task was\npublished as private with preview restrictions. Participants received $2 as compensation, and the median time taken to\ncomplete the task was 18.37 minutes (average 25.75). A total of 544 participants labeled 19,422 videos.\nAs shown in Appendix Table 13, our annotators were demographically diverse. The labelers were 60% male and 40%\nfemale, with a modal education being in the 35-44 year age band. The majority were Asian (60%) and white (35%), and\n60% self-identified as heterosexual while 45% as homo- or bisexual.11 This diversity is a strength given that harm labeling\ndepends on demographic backgrounds (e.g., younger or LGBTG+ individuals are more likely to classify content as toxic\nthan other groups, Kumar et al., 2021) and because there is a growing need to consider diversity in detecting harm (see\nGordon et al. 2022).\nSurvey Design. We embedded Qualtrics in the MTurk task so that participants could watch YouTube videos and\nannotate them in the survey (See Figure 2). To ensure a basic understanding of the coding criteria, all participants were\nasked to rewrite the six harm categories. Then, they completed a filtering task, which involved classifying five filter videos\nas harmful or not. 12 If participants chose more than one incorrect answer, the survey was automatically terminated. This\nwas designed to improve data quality (Chmielewski & Kucker, 2020). Participants who passed the filtering task, with a\npass rate of 93.89%, proceeded to the main labeling task. They classified 25 videos as harmful, harmless, or unavailable if\nthe video was removed or unviewable. For harmful videos, the participants additionally assigned one or more relevant\nharm categories. Participants could not move on to the next video until they completed the classification for the current\nvideo."}, {"title": "5.3 Analysis Results", "content": "We assess the performance of GPT-4-Turbo and compare it with that of crowdworkers by examining: (a) descriptive\nclassification results, (b) binary classification (harmful vs. harmless), and (c) multi-label classification that assigns one or\nmore harm categories."}, {"title": "5.3.1Descriptive Classification Results", "content": "Binary Classification. Table 5 and Figure 3 show the distribution of binary classification results by domain experts,\ncrowdworkers, and GPT-4-Turbo. Watching the same videos, domain experts labeled the highest proportion as harmful\n(77.82%), while GPT-4-Turbo identified the fewest as harmful (54.03%) and the highest percentage as harmless (40.25%).\nExcluding \"unavailable,\" \"no agreement,\" and \"removed\" cases, the total number of classified videos was 18,418 for\ndomain experts, 18,303 for GPT-4-Turbo, and 17,058 for crowdworkers. Figure 3 also shows that most \"unavailable\" and\n\"no agreement\" videos from GPT-4-Turbo and crowdworkers were classified as harmful by domain experts. We computed\nintercoder reliability between the three GPT APIs and the three crowdworkers on 30% of the dataset. Krippendorff's alpha,\nwhich measures complete agreement among the three annotators, yielded 0.780 for the three GPT APIs and 0.205 for the"}, {"title": "6 DISCUSSION AND CONCLUSION", "content": "This paper investigated the methodological approach to identifying harmful YouTube videos using GPT-4-Turbo as a\nMLLM, and crowdworkers, based on a comprehensive taxonomy for online harm. We compared the performance of GPT-\n4-Turbo and crowdworkers in classifying a video as harmful or harmless and assigning harm categories, with domain\nexpert labels as the gold standard. Our analysis reveals key findings.\nFirst, GPT-4-Turbo outperforms crowdworkers in both binary and multi-label classifications for multi-modal data. This\nsupports previous studies emphasizing the potential of LLMs as substitutes for text annotators and extends their application\nto multi-label and multi-modal contexts. However, the performance gap between GPT-4-Turbo and crowdworkers was\nmodest, particularly in binary classification, and both GPT-4-Turbo and crowdworkers only correctly predicted\napproximately 60% of domain expert labels. This signifies that GPT-4-Turbo, in its current form, has limitations as an\nindependent gold standard for harmful video annotation. Nonetheless, GPT-4-Turbo achieved performance that surpasses\nhuman annotators at a lower cost (See A.8 for cost analysis), suggesting its potential to serve as a silver standard that can\nreplace crowdworker annotators.\nSecond, GPT-4-Turbo is relatively more competent in handling both visual and text-based harm categories than\ncrowdworkers, who seem better at identifying visually distinctive harmful content. Among crowdworkers, only the"}, {"title": "A APPENDICES", "content": "A.1 Scoping a taxonomy for online harm in video platforms\nA.1.1 Harm Category Identification\nWe associated our harm categories with the content moderation policies of YouTube, Meta, and TikTok16 \u2014the video\nplatforms we focused on for developing our harm taxonomy. Each harm category and its subcategories were designed to\ninclude specific cases as described in the community guidelines of these platforms, following their established criteria for\nidentification."}, {"title": "A.1.2\nDetails on Relevant Harm Taxonomy and Categorization", "content": "We detail the harm taxonomy employed in our grounded theory approach. Despite overlapping concepts and specific cases\nbetween them, these are located and explained distinctly within each categorization. In our analysis, we conducted a\nthorough review of how each category and subcategory were defined and identified in each context. Following this, we\nreorganized the subcategories to enhance comprehensiveness and aligned them within categories that reflect their\ncommonalities."}, {"title": "A.2 Data Collection", "content": "A.2.1\nKeyword Identification\nPrior to collecting data through a Keyword-based approach, we identified a set of keywords expected to represent each\nharm category. The authors compiled a list of specific keywords targeting each harm category, supplemented by a review\nof existing literature to incorporate relevant keywords from previous studies (e.g., Bouma-Sims & Reaves, 2021;\nChancellor et al., 2017; Hussein et al., 2020; Kriegel et al., 2021; Paek et al., 2014; Scherr et al., 2020). We also included\nkeywords from platform community guidelines identified as moderation cases within each category. Using these search\nqueries, we collected YouTube metadata from October 2023 to January 2024."}, {"title": "A.2.2\nChannel Identification", "content": "We identified YouTube channels that were believed to promote content related to each harm category. Specifically, for the\nhate and harassment category, we also included videos under the hate and harassment category by scraping content from"}, {"title": "A.3 Domain expert training", "content": "A.3.1\nDomain Expert Training Material\nThe training material is available at https://docs.google.com/document/d/1CF-fhNFrTkQMqYZo-\npWdzNOGKYHyUPXqC4GAbdJBWJc/edit?usp=sharing"}, {"title": "A.3.2 Training Procedure for Domain Experts", "content": "Prior to participation, the domain experts underwent extensive training on the online harm taxonomy and coding criteria.\nFollowing this initial training, the principal investigator organized the ten experts into four teams, each consisting of two\nto three members. Each team was assigned a set of 50 videos to classify, with all team members reviewing the same set of\nvideos. This process was designed to identify and resolve any disagreements within the teams. The experts classified each\nvideo into one or more harm categories, as either harmless, unavailable (for videos that had been removed), or marked as\nunsure.\nAfter the group review, a follow-up meeting was held to address disagreements and discuss videos labeled as unsure.\nDuring these meetings, the experts engaged in discussions for cases that were difficult to classify. For example, discussions\nincluded how to classify a music video with lyrics promoting suicide but lacking explicit imagery, which was ultimately\ndetermined to be harmful. Once the initial training and disagreement-resolution meeting were completed, each expert was\nassigned a different set of videos to classify individually, with their classifications serving as the ground truth. Periodic\nmeetings were held over the four months to ensure consistency in understanding and to address any newly reported\nambiguous cases.\nAlthough the experts were well-equipped with a solid understanding of the harm categories, and their classifications\nwere considered the gold standard, there was a concern that some classifications might have been made in error. To address\nthis, the principal investigator conducted a double-check by comparing the experts' labels with the search query labels.\nSpecifically, cases where there was a discrepancy between the expert's label and the search query label were revisited. For\nexample, if a video was collected under a Hate and harassment channel or keyword but labeled as harmless by the experts,\nthe principal investigator reviewed the video to ensure accuracy. Through this process, the domain expert labels were\nestablished as the gold standard."}, {"title": "A."}]}