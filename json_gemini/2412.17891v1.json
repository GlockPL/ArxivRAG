{"title": "The Power of Adaptation: Boosting In-Context Learning through Adaptive Prompting", "authors": ["Shuzhang Cai", "Twumasi Mensah-Boateng", "Xander Kuksov", "Jing Yuan", "Shaojie Tang"], "abstract": "Large Language Models (LLMs) have demonstrated exce\u0440tional abilities across a broad range of language-related tasks, including generating solutions to complex reasoning problems. An effective tech-nique to enhance LLM performance is in-context learning, which encour-ages a step-by-step reasoning process by including explanatory examples to guide the model's responses. However, selecting appropriate exemplars for the model poses a challenge, as each dataset demands a distinct set of exemplars to enable the LLM to learn effectively and perform well on the test set. Current studies often rely on uncertainty- or diversity-based selection strategies to select exemplars for annotation and to improve model learning. However, these studies typically employ a non-adaptive approach, selecting a set of exemplars all at once. We argue that this non-adaptive strategy may result in a set of exemplars with high redundancy in terms of the knowledge covered, ultimately reducing their overall infor-mativeness. To address this limitation, we propose ADAPTIVE-PROMPT, a novel method that adaptively selects exemplars by leveraging model feedback from previously chosen exemplars. Experimental results show that ADAPTIVE-PROMPT significantly enhances LLM performance across a variety of reasoning tasks.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated their exceptional proficiency across a broad spectrum of tasks and fundamentally transformed the field of natural language processing (NLP) [1,8,21, 24, 26, 29]. However, LLMs often struggle with tasks that require complex reasoning. In-context learning (ICL) is a powerful solution to this issue by guiding models through examples and instructions without modifying their parameters. This approach is both convenient and effective, making it increasingly popular in various applications.\nOne approach within the ICL framework is Chain-of-Thought (CoT) prompt-ing [27], which has proven highly effective by breaking down complex problems into sequential, step-by-step explanations. A variant known as zero-shot CoT [8] involves appending the phrase \"Let's think step by step\" to the end of a ques-tion, allowing the model to generate reasoning without any demonstrations. In"}, {"title": "2 Related Work", "content": "Chain-of-thought Prompting LLMs demonstrate enhanced capabilities with CoT methods [27]. The fundamental idea is to have the model explicitly generate intermediate steps in reasoning before arriving at the final answer, formatted as (Question, Reasoning Chain, Answer ). Zero-shot learning and few-shot learning have been foundational in traditional language models. The zero-shot method does not provide any examples or guidance but simply prompts the"}, {"title": "3 Problem Formulation", "content": "The input of our problem consists of a LLM M, a training set of m unlabeled questions Q, i.e., Q = {q1,q2,...,qm}, and a test set P containing n ques-tions, i.e., P = {P1,P2,...,Pn}. Our objective is to select k questions from Q, denoted as {q1,q2,...,qk}, and annotate them to create an exemplar set E = {(q1, r1, a1), (q2, r2, a2), ..., (qk, rk, ak)}, where for each i \u2208 {1, 2, . . ., k}, ri represents the reasoning chain and ai is the correct answer for question qi. Here k is a budget constraint, limiting the maximum size of the exemplar set.\nThe goal is to identify an optimal E such that, when any test question p from P is presented to M along with E, the model generates the most accurate response for p."}, {"title": "4 Design of Adaptive-Prompt", "content": "In this paper, we introduce a novel in-context learning framework, ADAPTIVE-PROMPT. An illustration of ADAPTIVE-PROMPT is presented in Figure 1, with a detailed implementation provided in Algorithm 1."}, {"title": "4.1 Design of Adaptive-Prompt", "content": "We next give a detailed description of ADAPTIVE-PROMPT. The basic idea of this approach is to adaptively select a group of representative exemplars to help"}, {"title": "4.2 Computing the Uncertainty Score u(q | E)", "content": "Recall that a critical step in ADAPTIVE-PROMPT is calculating the uncertainty score u(qE) for a given exemplar set E and question q. This score, u(q | E), quantifies the potential divergence in the model's responses [6]. Intuitively, a higher u(q | E) indicates lower model confidence in answering q based on the ex-emplar set E. Following [3], we use two metrics to define this score: disagreement and entropy. Alternative forms of uncertainty scores such as variance could also be developed to capture additional aspects of model confidence.\nFor a given exemplar set E and a question q, consider performing l inde-pendent queries by presenting the combined input Eq to the LLM, resulting in a set of l responses, denoted by A {a1,a2,...,\u03b1\u03b9}. Here, each response ai is generated independently based on the exemplar-question pair Eq. To identify distinct answers, let Au = {a1,a2,..., a} represent the set of unique responses in A, where t <l and each a is a distinct response found among the generated answers. This set Au captures the diversity of responses pro-vided by the model when given E and q. The disagreement-based uncertainty score is defined as u(q | E) = 1, representing the ratio of distinct answers t to the total number of generated answers 1. This score reflects the proportion of unique responses produced by the model, indicating its variability in answering q given the exemplar set E. The entropy-based uncertainty score is defined as u(q | E) = \u03a3=1P(a) log P(a), where P(a';) represents the frequency of a unique answer a; in the set of unique responses Au. A higher entropy value indicates a greater uncertainty in the model's predictions."}, {"title": "5 Experimental Settings", "content": "We conduct a series of experiments on several public datasets using the backbone generative models GPT-3.5 Turbo and GPT-40 Mini [16], which are renowned"}, {"title": "6 Experiment Results", "content": "The performance of our method and the baseline approaches on GPT models is presented in Table 1 and 2. All results are expressed as percentages, with the best outcomes highlighted in bold.\nWhen using GPT-3.5 Turbo, our method achieves the best performance on 5 out of 6 datasets. The one exception is the AQUA dataset, where the Zero-Shot CoT method achieves an accuracy of 62.7% and outperforms all other approaches including ours. On two other arithmetic datasets, ADAPTIVE-PROMPT attains the highest accuracy compared to all baseline models. On GSM8K, the marginal"}, {"title": "Effect of Annotators", "content": "It is intuitive that the style and quality of annotations will significantly impact the outcomes of experiments. In this section, we examine the effects of different human annotators. To do so, we have a second annotator provide annotations for the selected questions while keeping all other processes the same. Here, we only adopt the entropy-based approach. We conduct ex-periments on the following datasets: GSM8K, StrategyQA, and CSQA, using GPT-3.5 Turbo. The results are presented in Table 3. The first three rows do not require human annotation, while the last three are based on annotations from a different annotator. The results align with those from the first annota-tor: ADAPTIVE-PROMPT either matches or exceeds the performance of all other baselines."}, {"title": "Effect of Exemplar Set Size", "content": "The size of the exemplar set, k, is a critical factor influencing the performance of LLMs within the framework of ICL. A small ex-emplar set may fail to provide sufficient and representative information for the"}, {"title": "Evaluation with Weaker Models", "content": "We conduct additional evaluations using the LLaMA3-8B model, known for its cost-effectiveness. In most cases, Zero-Shot CoT outperforms both baselines and our method. This may be due to weaker models struggling with long chains of thought, as longer prompts can degrade performance [12]."}]}