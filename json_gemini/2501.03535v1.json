{"title": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive Querying for LLM-Based Autonomous Driving", "authors": ["Xuewen Luo", "Fan Ding", "Fengze Yang", "Yang Zhou", "Junnyong Loo", "Hwa Hui Tew", "Chenxi Liu"], "abstract": "This study addresses the critical need for enhanced situational awareness in autonomous driving (AD) by leveraging the contextual reasoning capabilities of large language models (LLMs). Unlike traditional perception systems that rely on rigid, label-based annotations, it integrates real-time, multimodal sensor data into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically understand and respond to complex driving environments. To overcome the inherent latency and modality limitations of LLMs, a proactive Retrieval-Augmented Generation (RAG) is designed for AD, combined with a chain-of-thought prompting mechanism, ensuring rapid and context-rich understanding. Experimental results using real-world Vehicle-to-everything (V2X) datasets demonstrate significant improvements in perception and prediction performance, highlighting the potential of this framework to enhance safety, adaptability, and decision-making in next-generation AD systems.", "sections": [{"title": "1. Introduction", "content": "Situation awareness plays a pivotal role in Autonomous Driving(AD) system as it enables the vehicle to handle the complex environment in transportation system [1]. Recently, various Artificial Intelligence (AI) technologies have been developed to enhance environmental understanding, such as through deep learning models for object detection [2], semantic segmentation [3], and computer vision techniques [4] for spatial awareness. Notably, the advancement of large language models (LLMs) has opened new opportunities for AD [5]. By harnessing the capabilities of LLMs, researchers have achieved significant improvements in the ability of AD systems to understand sensor data, paving the way for enhanced perception in complex driving scenarios.\nLLMs possess a significant advantage in their ability to recognize environmental information, which enables the system to handle the complex environment [6]. Unlike other perception technologies, LLMs can truly \"understand\" the context [7], while models like computer vision (CV) rely on rigid, predefined labels learned during training. CV models are constrained by fixed annotations and lack flexibility in new scenarios [8]. In contrast, LLMs can dynamically process diverse contexts and relationships within data. However, their main limitation is that they are designed to handle language-based information and cannot directly process the multimodal sensor data from Vehicle to Anything (V2X) and AD systems, such as radar, cameras, or Lidar [9] [10]. Additionally, LLMs typically require considerable processing time when handling very large datasets [11], which would compromise the real-time responsiveness required in AD scenarios.\nTo address these challenges, a proactive RAG framework is proposed for LLM-based AD systems, which integrates two key components. One is synthetic prior knowledge database that consolidates real-time data from diverse sources, including meteorological sensors, traffic signals, road cameras and Lidars, into a standardized language format. This knowledge database serves as a foundation for enabling situation-awareness understanding and reasoning for AVs. Another key component is the chain-of-thought prompting mechanism designed that empowers LLMs to actively retrieve relevant information from the knowledge database according to needs of AVs. By leveraging this approach, the system analyzes environmental conditions, providing context perception and reasoning, to enhance safety and intelligence in complex scenarios.\nOur contributions in this study are highlighted as follows.\n\u2022 This paper introduces a proactive SenseRAG framework tailored for LLM-based AD systems and validates its efficacy using real-world trajectory datasets. Empirical results show a substantial improvement in"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Situation-awared Autonomous Driving", "content": "Situation awareness plays an increasingly critical role in AD, enhancing both safety and resilience as transportation scenarios grow more complex and diverse [12]. Recent advancements in intelligent transportation systems have introduced cutting-edge sensing and communication technologies aimed at improving the robustness and safety of autonomous driving in varied and complex traffic environments [13]. Existing research in this domain can be broadly categorized into two approaches: independent perception (decision-making based solely on in-vehicle sensors) and cooperative perception (decision-making enhanced by external information) [13].\nIndependent perception relies heavily on advanced sensors and intra-vehicle sensor fusion to develop a comprehensive understanding of the surrounding environment. However, its effectiveness is often limited by its inherent \u201cshort-sightedness\u201d and inflexibility, particularly in highly complex and diverse traffic scenarios, raising concerns about its reliability in achieving high-level automation [14]. To overcome these challenges, cooperative perception has emerged as a promising solution. By integrating external data sources and enabling communication among various transportation agents, cooperative perception facilitates a broader and more holistic understanding of traffic scenarios [13, 15, 16].\nIn particular, cooperative sensing proves invaluable in specific situations, such as occlusions in high-density or crowded environments, where it can significantly improve situational awareness and potentially save lives [17]. The sharing of information within the network enhances the safety, efficiency, and reliability of connected and autonomous systems, paving the way for more resilient and robust transportation networks [18] [19].\nCurrent cooperative perception systems primarily rely on vehicles passively receiving environmental information through V2X technologies [20]. This approach enables vehicles to obtain real-time data from surrounding vehicles, infrastructure, and other traffic participants. However, this passive method often results in information redundancy or delays, potentially limiting the real-time decision-making capabilities of autonomous driving systems [21]. To strengthen cooperative perception, the study CodeFilling adopts two key strategies: optimizing collaborative messages through improved representation and selection [22]. Consequently, optimizing V2X communication to actively filter and prioritize useful information for cooperative perception remains a critical area of research."}, {"title": "2.2. LLMs Empowered Autonomous Driving", "content": "The rapid advancement of LLMs is redefining the landscape of AD, moving beyond traditional, narrowly focused perception systems toward a more holistic form of environmental understanding. By unifying visual, textual, and sensor data, LLMs are emerging as pivotal components of AD architectures, enabling vehicles to interpret complex traffic conditions and integrate information from multiple streams to gain deeper insights into their surroundings [23]. This multifaceted approach not only enriches perception but also lays the groundwork for more nuanced prediction and decision-making processes, ultimately paving the way for safer and more efficient on-road performance [24] [25].\nBuilding upon this foundation, recent research has begun demonstrating how LLMs can streamline the entire AD pipeline, from initial data intake to final actuation [5]. For instance, by combining diverse datasets ranging from camera feeds and radar signals to traffic reports-LLMs can discern intricate patterns, identify subtle cues, and continually refine their understanding of the driving environment [25, 26]. As these models mature, they hold the potential to significantly enhance contextual reasoning, making it possible for autonomous vehicles to better anticipate dynamic changes and navigate challenging road conditions with a level of sophistication not previously attainable.\nDespite their potential, LLMs face significant limitations in autonomous driving due to their dependence on static pre-trained knowledge, which constrain their ability to adapt to real-time dynamics and integrate diverse external inputs [27]. The Retrieval Augmented Generation (RAG) paradigm offers a promising solution by enabling active querying of external databases [28]. Studies such as RAG-Driver [29] and RAG-Guided [30] demonstrate the value of proactive querying in generating more accurate responses. By integrating RAG, LLMs can effectively bridge the gaps in both individual and external knowledge, extending their knowledge pool and paving the way for more robust and"}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Framework Overview", "content": "To enhance the perception and situation awareness capabilities of LLM-based AD systems, we propose a proactive SenseRAG framework, centered around a synthetic knowledge database. It empowers AVs to interpret and adapt to dynamic scenarios by leveraging an accumulated repository of multimodal environmental data. Functioning as a closed-loop system, it facilitates continuous interaction between vehicles and real-time environmental inputs, thereby extending situational awareness and enabling intelligent driving decisions. By integrating multimodal sensor data with an active query-generation mechanism, the system dynamically retrieves and processes relevant information to address complex traffic conditions. This approach significantly improves both the safety and operational efficiency of AVs in challenging driving environments.\nThe proposed structure 1 is composed of two key components:\nKnowledge database serves as the foundational component of the framework, consolidating diverse sensor data, including inputs from cameras, Lidars, and other environmental sensors, into a standardized, language-compatible format. This standardized repository enables seamless integration of multimodal data, ensuring that the autonomous vehicle can access and process comprehensive situational information efficiently.\nBuilt upon the knowledge database, the proactive query-generation mechanism employs a chain-of-thought prompting strategy to dynamically retrieve relevant information. This component enables AD systems to adaptively access data which are required from the database, optimizing the process of accessing information."}, {"title": "3.2. Multimodal Sensor Data Integration", "content": "This paper integrates unstructured inputs-360\u00b0camera imagery and LiDAR point clouds capturing vehicles, pedestrians, and traffic signs, with structured data, including signal timing plans and weather records. Together, these multimodal sources provide a comprehensive environmental snapshot, shown in Figure 2, enabling the LLM to reason about traffic conditions and enhance decision-making."}, {"title": "3.2.1 Unstructured Media Preprocessing", "content": "Given camera images $I(t)$ and LiDAR point clouds $L(t)$ at time $t$, we construct a fused, normalized representation $F(t)$ for the vision-language model as Eq. 1.\n$F(t) = R(C(S_{I}(D_{I}(I(t))), S_{L}(D_{L}(L(t)))))$ (1)\nWhere $D_{I}(\u00b7)$ and $D_{L}(\u00b7)$ denote denoising for images and LiDAR, respectively. $S_{I}(\u00b7)$ and $S_{L}(\u00b7)$ standardize images"}, {"title": "3.2.2 Structured Data Preprocessing", "content": "For signal timing $S(t, p)$ and weather data $W(t, p)$ at time $t$ and position $p$, parse relevant fields and handle missing values via interpolation or defaults. Remove duplicates and correct anomalies. Convert all units to a standard system and scale values in Eq. 2.\n$S'(t,p) = N(S(t,p)), W'(t,p) = N(W(t, p))$ (2)\nThen align $S'(t,p)$ and $W'(t,p)$ with sensor data timestamps and locations to ensure consistent spatiotemporal references for downstream reasoning."}, {"title": "3.2.3 Vision-Language Model (VLM) Integration", "content": "Visual Feature Extraction and Alignment\nA suitable VLM, LLaVA [31], is employed to bridge vision and language due to the seamless multimodal understanding and unified representation, which enables flexible tasks and context-rich reasoning. Given a camera input $I$, the pre-trained vision encoder $E_{V}$ extracts a feature vector $v \u2208 R^{n}$, where,\n$v = [v_{1}, v_{2}, . . ., v_{n}]^{T} = E_{V}(I)$ (3)\nTo integrate this with the LLM's embedding space, a learnable projection $W \u2208 R^{m\u00d7n}$ maps these visual features in Eq. 4.\n$v' = WE_{V}(I)$ (4)\nThis transformation ensures visual information is represented as textual tokens, enabling unified multimodal reasoning.\nConditioning the LLM on Multimodal Inputs\nThe LLM receives three inputs: a textual query X, retrieved textual knowledge K from the database, and the visual embedding v'. These define the conditional input for generating a response Y in Eq. 5.\n$P(Y | X, K, I) = \\prod_{t=1}^{T} P(Y_{t} | Y_{<t}, X, K, v')$ (5)\nAt each step, the LLM considers previous tokens $y_{<t}$, the prompt X, knowledge K, and v'.\nTraining and Fine-tuning Objective\nThe model is trained on tuples (X, K, I, Y) to minimize the loss function shown in Eq. 6.\n$L(X, K, I,Y) = - \\sum_{t=1}^{T} log P(Y_{t} | Y_{<t}, X, K, \u03c5')$ (6)"}, {"title": "3.2.4 Data Harmonization and Injection", "content": "The harmonization step aligns VLM textual descriptors with the structured data defined in the database schema. Given raw vision-derived text descriptions at time \u03c4 and location $(l_{x}, l_{y})$, this paper associates them with entries in tables such as vehicles, weather, pedestrians, intersections, traffic_signs, traffic_signals, and phases. Each integrated record can be expressed as\nRecord $(\u03c4, l_{x}, l_{y}, U_{text}, S_{structured})$\nwhere $U_{text}$ denotes VLM-derived textual descriptors, and $S_{structured}$ corresponds to associated rows from the database tables.\nData injection involves inserting these harmonized records into the database, leveraging existing columns like timestamp, latitude, longitude and indexed fields (country, state, city in weather and intersections, or day_of_week in traffic_signals) to enable spatial-temporal retrieval and filtering. By mapping textual descriptions to structured entries, and utilizing provided primary keys, foreign keys, and indexes, the system supports efficient queries and scalable updates as new sensor data streams in. This integrated environment enhances situation awareness, enabling real-time context retrieval for improved decision-making in autonomous driving scenarios."}, {"title": "3.3. Proactive RAG for LLMs", "content": "In to enhance the perceptual capabilities of LLM-based AD, we designed and implemented a Proactive RAG method, which combines the generative capabilities of LLMs with the querying capabilities of environmental information repositories, aiming at proactively obtaining complementary information related to the current driving environment.\nThe whole method can be precisely described by the following formula: the self-perception data S, which captures the sensory information from the ego vehicle, and the environmental information E, retrieved from a database via a query Q(S). The query Q(S) is generated based on S, specifying the required supplementary data. Together, S and E define the conditional input for the LLM, which generates the final perceptionP as described in Eq. 7.\n$P = LLM(Combine(S, E)), E = Search(S, Q(S))$ (7)"}, {"title": "3.3.1 Chain-of-Thought Instruction Tuning", "content": "This method leverages the reasoning capabilities of LLMs by constructing chain-of-thought prompts to extract key information from the current self-perception data S and to infer which environmental data are necessary for enhanced situation awareness. The self-perception data S comprise real-time sensory inputs from the vehicle's array of sensors (e.g., cameras, radar, LiDAR), thereby providing a direct perception of the immediate surroundings.\nThrough a systematic step-by-step reasoning process, the chain-of-thought prompts enable the LLM to identify uncertainties inherent in the self-perception data S and to determine the supplementary information required to resolve these uncertainties. For instance, in complex urban scenarios, the system may necessitate querying the states of traffic signals, obtaining detailed information about road construction, or acquiring current weather conditions. The outcomes of this reasoning process are subsequently utilized to generate the query Q(S), which facilitates the retrieval of pertinent environmental information E from the database."}, {"title": "3.3.2 Language Query to SQL Query", "content": "Next, the system retrieves environmental information E using the proactive search mechanism E = Search(S, Q(S)), which transforms the natural language query Q(S), derived from the self-perception data S and the LLM's reasoning, into standardized SQL queries for efficient retrieval from environmental databases. This transformation ensures compatibility between the high-level reasoning outputs of the LLM and the structured query language required to access the database. By dynamically generating context-specific queries, the system enables precise and targeted retrieval of data relevant to the current driving environment. For example:\n\u2022 Natural Language Query: \"Retrieve the traffic signal status for the current road segment.\"\n\u2022 Translated SQL Query:\nSELECT signal_status\nFROM traffic_data\nWHERE location = 'current_position'\nAND time = 'current_time';"}, {"title": "3.3.3 Verbalization and Integration of Environmental Information", "content": "After obtaining the environmental information E, it is integrated with the current self-perception data S to form a comprehensive representation of the environment, expressed as: Combine(S, E).\nThis integration process maintains the localized details from the self-perception data while incorporating global contextual information, thus providing the LLM with a multidimensional input for reasoning. To ensure consistency and interpretability, the retrieved environmental information E is transformed into structured linguistic information using natural language generation techniques. For example, \"The traffic signal ahead is red.\"\nThe combined information Combine(S, E) is then passed to the LLM, which performs deep reasoning to generate the final result: P = LLM(Combine(S, E))"}, {"title": "4. Experiment & Result Evaluation", "content": null}, {"title": "4.1. Setup", "content": "The experimental dataset is derived from the DLR Urban Traffic dataset (DLR UT), a real-world dataset collected from intersections. It includes trajectory data of all participants at the intersection, along with detailed day-specific information such as traffic signals, weather conditions, positions of traffic participants, speed, acceleration, wind, sunlight, precipitation, visibility, and other rich data. The dataset was collected using 14 multi-sensor systems at intersections and contains 31,477 trajectories and the current status of 30 traffic lights. The locations in the data set are latitude and longitude in the real world, and ADE and FDE in the validation phase are calculated directly from them.\nTo validate the effectiveness of LLMs in this process, we constructed a closed-loop test environment suitable for GPT-4 reasoning. First, we distinguished the ego vehicle from other traffic participants by defining a visible range. The perceptual environment of the vehicle was simulated, with the perception range set to within 30 meters. Additionally, road information was formatted to be easily understood by the GPT-4, and all relevant information was fed into the GPT-4 to support its decision-making process. During the process, we monitored the intermediate outputs of the GPT-4 to ensure it correctly understood the environment.\nThe experiment employed a controlled variable method, with the variable being whether a retrieval database was used to assist the GPT-4 in trajectory prediction. The retrieval database included information beyond the vehicle's perception range in the dataset. The GPT-4 actively retrieved necessary information about the current environment from this database. In the comparative experiment, the GPT-4 relied solely on the vehicle's perception data for trajectory prediction."}, {"title": "4.2. Results Evaluation", "content": "To assess the model's predictive accuracy, we relied on standard trajectory metrics, including the average displacement Error (ADE) and final displacement error (FDE). These indicators provided a straightforward way to compare"}, {"title": "5. Conclusion", "content": "In this paper, a proactive SenseRAG framework that leverages LLMs to enhance situation awareness in AD. By integrating real-time, multimodal sensor inputs into a unified, language-accessible knowledge database, the approach allows LLMs to dynamically reason about complex driving environments. Chain-of-thought prompting and a carefully designed query mechanism empower the model to retrieve pertinent environmental context efficiently, overcoming the latency and modality constraints traditionally associated with LLMs in AD scenarios. Experimental results with realistic V2X datasets demonstrate substantial improvements in perception and trajectory prediction accuracy, significantly reducing displacement errors compared to baseline methods.\nThe value of this work lies in its ability to go beyond predefined labels and static scene interpretations, enabling flexible and dynamic understanding of traffic scenarios. By harnessing LLMs' inherent contextual reasoning, the method offers a robust pathway toward safer and more adaptive AD systems. Future research could extend this framework by incorporating more diverse sensor modalities, refining retrieval strategies for real-time operation at scale, and generalizing the approach to different urban settings, ultimately pushing the boundaries of intelligent mobility solutions."}]}