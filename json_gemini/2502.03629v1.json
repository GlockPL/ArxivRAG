{"title": "REALEDIT: Reddit Edits As a Large-scale\nEmpirical Dataset for Image Transformations", "authors": ["Peter Sushko", "Ayana Bharadwaj", "Zhi Yang Lim", "Vasily Ilin", "Ben Caffee", "Dongping Chen", "Mohammadreza Salehi", "Cheng-Yu Hsieh", "Ranjay Krishna"], "abstract": "Existing image editing models struggle to meet real-\nworld demands; despite excelling in academic benchmarks,\nwe are yet to see them adopted to solve real user needs.\nThe datasets that power these models use artificial edits,\nlacking the scale and ecological validity necessary to ad-\ndress the true diversity of user requests. In response, we\nintroduce REALEDIT, a large-scale image editing dataset\nwith authentic user requests and human-made edits sourced\nfrom Reddit. REALEDIT contains a test set of 9.3K ex-\namples the community can use to evaluate models on real\nuser requests. Our results show that existing models fall\nshort on these tasks, implying a need for realistic training\ndata. So, we introduce 48K training examples, with which\nwe train our REALEDIT model. Our model achieves sub-\nstantial gains\u2014outperforming competitors by up to 165 Elo\npoints in human judgment and 92% relative improvement\non the automated VIEScore metric on our test set. We de-\nploy our model back on Reddit, testing it on new requests,\nand receive positive feedback. Beyond image editing, we ex-\nplore REALEDIT 's potential in detecting edited images by\npartnering with a deepfake detection non-profit. Finetuning\ntheir model on REALEDIT data improves its F1-score by\n14 percentage points, underscoring the dataset's value for\nbroad, impactful applications.", "sections": [{"title": "1. Introduction", "content": "The need to edit photos is more important than\never-people everywhere seek to perfect, enhance, or re-\nstore their images, from casual snapshots to treasured mem-\nories. If more effective and aligned editing models were\nreadily available, many would use them for a variety of\npurposes: to remove an unwanted photobomber, adjust\nlighting in their selfies, restore their grandparents' wed-\nding photos, or even add creative effects. This demand is\nvividly demonstrated in online communities like Reddit's\nr/PhotoshopRequest\u00b9 and restoration\u00b2, with over 1.5 mil-\nlion combined members. Many users pay money for quality\nedits, highlighting the demand for advanced, user-friendly\nediting tools.\nDespite the impressive capabilities in image genera-\ntion and modification led by recent advancement of dif-\nfusion models [6, 45, 46, 65], seemingly straightfor-\nward real-world editing tasks, like ones from the Red-\ndit's r/PhotoshopRequest, continue to pose significant chal-\nlenges to existing models. For instance, while existing mod-\nels are effective at artistic transformations or generating\nstylized content [28, 35, 36, 48, 65], they fall short at some"}, {"title": "2. Related work", "content": "Image editing datasets. While extensive datasets exist for\ncaptioning and identifying edited images within fixed do-\nmains [10, 41], there is a notable lack of large-scale, human-\nedited image datasets. Currently, larger-scale image editing\ndatasets mostly rely on synthetic data [6, 48, 65, 66, 69],\nwhile the ones with human edited images are limited in\nsize [49, 50]. While synthetic datasets may include hu-\nman inputs, such as generating instructions or ranking edits\n[6, 65, 66], these datasets do not contain edits that are com-\npleted by humans. Most importantly, existing datasets are\ncurated in ways that do not necessarily characterize real-\nworld editing distribution well. We compare REALEDIT to\nexisting datasets in Table 1.\nText-guided image editing. There is a rich literature in\nmodels focusing on specific image editing tasks, such as\ninpainting [62], denoising [18], and style transfer [15]. Re-\ncent advancements emphasize generalized models that bet-\nter align with human use cases, leading to innovative meth-\nods such as generating programs to modify images [19],\nas well as end-to-end diffusion-based or GAN-based edit-\ning models [2, 25, 35, 43, 54, 58]. Diffusion models like\nStable Diffusion [46] excel at generating images from text\nprompts, serving as versatile models for image genera-\ntion [63]. Several models [6, 27, 36] utilize diffusion-based\ntechniques for editing, though generating images from cap-\ntions alone may compromise fidelity. To mitigate this,\nsome models [6, 28, 36, 65] leverage Prompt-to-Prompt\ntechnique [20], employing cross-attention maps to preserve\nmost of the original image. Others achieve consistency\nby fine-tuning diffusion models to reconstruct images us-\ning optimized text embeddings, blending these with target\ntext embeddings [27]. However, limitations persist, such as\nstuggles with face generation [5] and cross-attention requir-\ning minimal, often single-token caption variation.\nEvaluating image editing models. Originating from\nearly text summarization in NLP [37], QA-based evaluation\nmethods automatically transform prompts into questions"}, {"title": "3. REALEDIT", "content": "We introduce REALEDIT: a high-quality large-scale dataset\nfor text-guided image editing. REALEDIT dataset includes\n48K training data points and 9K test data points, each fea-\nturing an original image, an editing instruction, and one to\nfive human-edited output images. Altogether, we are pub-\nlishing a total of 151K images. REALEDIT is the first large-\nscale image editing dataset wherein real-world users both\nsubmit and complete the requests (Table 1)."}, {"title": "3.1. Dataset creation pipeline", "content": "The extensive and structured nature of Reddit makes it an\nideal source for creating diverse large-scale datasets rooted"}, {"title": "4. REALEDIT dataset analysis", "content": "REALEDIT provides insight into practical applications of\nimage editing by analyzing real-world requests. We ob-\nserve notable differences between REALEDIT and existing\ndatasets including InstructPix2Pix [6], MagicBrush [65],\nEmu Edit [48], HIVE [66], Ultra Edit [69], AURORA [28],\nImage Editing Request [50] and GIER [49]. While we fo-\ncus primarily on differences with MagicBrush [65] and Emu\nEdit [48] in the following discussions, these observations\nbroadly apply across datasets used to train image editing\nmodels. Figure 4 details the main differences."}, {"title": "5. An editing model trained with REALEDIT", "content": "To demonstrate the value of REALEDIT, we develop an im-\nage editing model using training examples in REALEDIT.\nSpecifically, we utilize InstructPix2Pix [6] as the model\nbackbone on which we finetune using our data. We leave\nexploration on using different base models as future work.\nAligning with pretraining data. Since we finetune In-\nstructPix2Pix rather than training the model from scratch,\nwe align our finetuning dataset with the data distribution\nused in InstructPix2Pix's pretraining data to avoid substan-\ntial distributional shifts that may deteriorate model's per-\nformance. In particular, InstructPix2Pix [6] applies CLIP-\nbased [44] filtering to ensure the quality of image pairs.\nIn addition, as it employs Prompt-to-Prompt [20] in gen-\nerating its training data, the input-output image pairs are\nwith high structural similarity. To align our training set,\nwe thus follow the same CLIP-based filtering and addition-\nally use SSIM [55] to include structurally similar images,\nrecognizing that human edits collected in REALEDIT of-\nten alter structure with techniques like drag-and-drop ad-\njustments and symmetrical flipping. Tasks incompatible\nwith InstructPix2Pix's capabilities, such as resizing images,\nchanging file types, or highly ambiguous prompts (particu-\nlarly those involving humor) are thus excluded. In total, we\ntrained on 39K examples. Aligning our training data with\nthe InstructPix2Pix distribution allows for more competi-\ntive performance on metrics, and accounts for limitations\nin the InstructPix2Pix's architecture and pretraining. For\ntraining our model, we closely follow the configuration of\nMagicBrush [65]. Specifically, we train our model for 51"}, {"title": "6. Experiments", "content": "Setup. We benchmark our model against six open-source\nbaselines: InstructPix2Pix [6], MagicBrush [65], AU-\nRORA [28], SDEdit [35], HIVE [66], and Null-text Inver-\nsion [36]. We leverage the input and output captions gener-\nated in Section 3 for models that require them.\nTo evaluate the models, we adopt a comprehensive suite\nof metrics. First, we utilize VQA-based automated met-\nrics to measure task completion, as these metrics have been\nshown to closely reflect human judgments. In particular,\nwe use VIEScore [29] with a GPT-40 backbone as our de-\nfault metric, as it evaluates semantic consistency (VIE_SC),\nperceptual quality (VIE_PQ), and overall alignment with\nhuman-requested edits (VIE_O) each on a scale of 0 to 10.\nSimilarly, we use VQAscore [32] (with different base mod-\nels: LLaVa and FLAN-T5) and TIFA [22] to evaluate the\nfine-grained faithfulness of the output image to the edit in-\nstruction. We also include standard metrics such as L1-\nand L2 pixel distance, DINO [64], CLIP-I and CLIP-T, fol-\nlowing prior work [48, 65]. Most importantly, we leverage\nreal users to make pairwise comparisons between edits and\ncompute Elo ranking of the models [23]. We further qual-\nitatively study the response Reddit users have on edits pro-\nduced by our model on recent posts."}, {"title": "6.1. Automated evaluations on REALEDIT test set", "content": "In Table 2, we show that existing models struggle to\ncapture the semantic nuances of human requests, while\nour model achieves notable improvements, particularly in\nVIE_SC scores. Our model also significantly outperforms"}, {"title": "6.2. Human evaluation on REALEDIT test set", "content": "Methods like VIEScore[29] align more closely with human\njudgment, but rely on vision-language models, which often\nmiss subtle differences and produce inconsistent results.\nTo counteract this, we conducted a qualitative evaluation\nusing Elo scores, following the methodology from GenAI\nArena [23] and LMSYS [70]. This evaluation, conducted\nvia Amazon Mechanical Turk, involved pairwise compar-\nisons against the baselines on 200 diverse images from our\ntest set. Results in Table 3 demonstrate that our model out-\nperforms baselines on human judgement."}, {"title": "6.3. Deploying our model on Reddit", "content": "One limitation of standard Elo evaluations is that they are\nconducted by individuals with no personal connection to\nthe image. To ecologically validate the utility of our model\nwith photo owners, we deploy our model back on Reddit.\nWe provide editing services for new user requests, posting"}, {"title": "6.4. Evaluations on existing test sets", "content": "We also conduct evaluations on external test sets including\nthe test sets in GenAI Arena [23], Emu Edit [48], and Mag-\nicBrush [65]. On GenAI Arena, we report Elo ranking in\nTable 3 computed with real human preferences. Our model\nranks second among the evaluated models. While these\nresults were insightful, we found the examples in GenAI\nArena to be less representative of real-world tasks. We in-\nclude full automated evaluation results on Emu Edit and\nMagicBrush in Appendix, where our model performs com-\npetitively with the individual strongest models on respective\ntest sets across varying metrics."}, {"title": "6.5. Improving edited image detection", "content": "We partnered with <REDACTED>, a platform where users\ncan upload media to assess authenticity. Their primary fake\nimage detection model is a fine-tuned version of Universal\nFake Detect (UFD) [38], which effectively detects model-\ngenerated deepfakes. We leverage the human-edited images\nin REALEDIT to enhance the model's ability to detect such\nedits, which has significant real-world impact."}, {"title": "7. Discussion", "content": "Privacy and ethics. To protect user privacy, individuals\ncan opt out of having their images in the dataset by remov-\ning the photos from Reddit. Since our dataset contains im-\nage URLs rather than image files, images deleted from the\nweb are automatically removed. Additionally, we provide a\nform where individuals can request their data to be removed\nfrom the dataset. Although this evolving dataset may intro-\nduce challenges for quantitative validation, ensuring user\nprivacy remains our top priority.\nOur work has positive social impacts, such as reducing\nthe need for professional editing software and skills, and\nenabling higher-quality restorations of family photographs.\nHowever, we recognize the risks of malicious exploitation\nand strongly oppose any harmful, offensive, or derogatory\nuse of our model or data. We plan to further pursue the\ndevelopment of fake image detection tools.\nConclusion. We propose REALEDIT, a dataset of 57K in-\nput - instruction - outputs data points where all instructions\nand edits are performed by humans. We analyze the distri-\nbution of real-world editing requests and fine-tune Instruct-\nPix2Pix [6] to create a SOTA image editing model on these\ntasks. Lastly we explore REALEDIT 's potential in facilitat-\ning deepfake detection."}, {"title": "C. Discussion", "content": "C.1. Limitations and future work\nREALEDIT is collected from Reddit posts from 2012-2021.\nAs such, we have less data and a danger of it getting out-\ndated. We plan to regularly update our dataset to ensure\nthat the edits reflect as current culture as much as possible.\nThis will also help in edited image detection, by facilitat-\ning the detection of edits where newer AI tools were used,\nas the line between human editing and model editing is in-\ncreasingly blurred.\nWe also filter our dataset in order to more closely match\nthe training distribution, removing some natural diversity\nof human edit requests. In future work, we hope to explore\ndifferent architectures capable of handling real world edit\nrequests and editing styles.\nThe pretraining of the REALEDIT model uses CLIP embed-\ndings, which while very useful for semantic changes to an\nimage, a large portion of the REALEDIT dataset involves\nedits that do not involve semantic changes. Additionally, in\nedited image detection, some of the edits may not change\nthe embeddings much. We urge future work to explore al-\nternatives to such embeddings that may capture purely aes-\nthetic changes."}, {"title": "C.2. Social impacts", "content": "The social impact of our dataset stems from both the effect\non model training as well as the ability of our test set to be\nused to accurately and justly benchmark other models. The\ntraining data will inform how well the REALEDIT model\nperforms certain types of edits. The test set on the other\nhand determines the factors we incentivize in other models.\nAccessible image editing models that are capable of\nhandling real world tasks are extremely useful in de-\nmocratizing the documentation of people's lives. For\nexample, some requests in REALEDIT involve restoring old\nphotographs, many of which were paid. The REALEDIT\nmodel can help more users to document meaningful family\nhistories, even if they cannot afford to pay for edits. We"}, {"title": "C.3. Ethics", "content": "Some other editing datasets [65] do not use human faces\nin order to evade biases as well as privacy concerns. How-\never, in REALEDIT, we determine that since over half of\nedit requests contain images focused on people, we must\ntrain on human data in order to be successful in completing\nreal world editing tasks. To mitigate privacy concerns, we\nuse the URL in place of the actual input image so that if the\noriginal poster (OP) deletes their post, it will be removed\nfrom our dataset. We also include a form for users to re-\nquest their data to be removed. In the case of mitigating\nbiases, we hope in future work to study the effects of using\nReddit data on task completion for a wide array of demo-\ngraphic groups, as well as techniques or supplementary data\nsources to boost performance on underrepresented groups.\nThis is a known problem in the field, and we are compelled"}, {"title": "E.1. Hyperparameters", "content": "We conducted several inference-time experiments: varying\nthe number of diffusion steps, the image and text guidance\nscales, and further rewriting instructions with GPT-40 to\nadd more details."}, {"title": "Number of inference steps", "content": "We observe that 20 inference\nsteps strike a good balance between the computational time\nand the image quality. Specifically, we find that the aver-\nage CLIP similarity between the generated image and the\nmost upvoted Reddit edit is approximately the same for any\nsetting of inference steps above 20."}, {"title": "Text guidance scale", "content": "We observe no correlation ($\\rho =$\n.005) between the text guidance scale in range [1,14] and\ninstruction adherence, as measured by CLIP similarity be-\ntween the generated image and the caption describing the\ndesired output."}, {"title": "Image guidance scale", "content": "The generated image quality de-\ncreases sharply if the image guidance scale is above 3. In-\nside the [1, 3] range, the image scale makes little difference\nin aggregate. Counter-intuitively, we observe a negative\ncorrelation ($\\rho = -$.106) between image guidance scale and\nCLIP similarity between the input and generated images.\nIn other words, higher image guidance values result in less\nsimilar images on average, which contradicts conventional\nassumptions about guidance scales and warrants further in-\nvestigation."}, {"title": "E.2. Instruction rewriting", "content": "As the diffusion model lacks reasoning capabilities, it of-\nten fails when asked to interpret abstract or creative instruc-\ntions. To improve outcomes on these examples, we employ\na large language model (LLM) to rewrite instructions in a\nmore specific manner, similar to Dalle-3 [3]. Since only\ncreative edit tasks benefit from this technique, we do not\nmake this part of our main pipeline. We gave the input im-\nage and the original instruction to GPT-40 with the prompt\n\"You are given an image editing instruction. If the instruc-\ntion is already concrete and specific, do not rewrite it at all.\nIf the instruction is vague or does not make sense for the im-\nage, then rewrite it. Make the new instruction specific and\ndetailed, e.g. do not use words 'enhance', 'adjust', 'any'.\""}, {"title": "E.3. Quantitative evaluation on external test sets", "content": "Despite being out of distribution, the REALEDIT model per-\nforms comparably to other models on the synthetic datasets\nEmu Edit [48] and MagicBrush [65]. On several met-\nrics (VQA_CLIP and TIFA on MagicBrush and VQA_llava,"}]}