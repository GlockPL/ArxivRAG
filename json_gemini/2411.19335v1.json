{"title": "PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning", "authors": ["Shenghui Li", "Edith C.-H. Ngai", "Fanghua Ye", "Thiemo Voigt"], "abstract": "Federated Parameter-Efficient Fine-Tuning (Fed-PEFT) has emerged as a promising paradigm for privacy-preserving and efficient adaptation of Pre-trained Language Models (PLMs) in Federated Learning (FL) settings. It preserves data privacy by keeping the data decentralized and training the model on local devices, ensuring that raw data never leaves the user's device. Moreover, the integration of PEFT methods, such as Low-Rank Adaptation (LoRA), significantly reduces the number of trainable parameters compared to fine-tuning the entire model, thereby minimizing communication costs and computational overhead. Despite its potential, the security implications of FedPEFT remain underexplored. This paper introduces a novel security threat to FedPEFT, termed \u201cPEFT-as-an-Attack\u201d (PaaA), which exposes how PEFT can be exploited as an attack vector to circumvent PLMs' safety alignment and generate harmful content in response to malicious prompts. Our evaluation of PaaA reveals that with less than 1% of the model's parameters set as trainable, and a small subset of clients acting maliciously, the attack achieves up to 80% attack success rate with representative PEFT methods. To mitigate this threat, we further investigate potential defense strategies, including Robust Aggregation Schemes (RASS) and Post-PEFT Safety Alignment (PPSA). The experimental results highlight the limitations of these defenses, i.e., even the advanced RASs, such as DnC and ClippedClustering, struggle to defend against PaaA in scenarios with highly heterogeneous data distributions. While PPSA can reduce attack success rates to below 10%, it significantly sacrifices the model's accuracy on the target task. Our results underscore the urgent need for more effective defense mechanisms that simultaneously ensure security and maintain the performance advantages of the FedPEFT paradigm.", "sections": [{"title": "INTRODUCTION", "content": "Pre-trained Language Models (PLMs), such as BERT [1], the GPT series [2]\u2013[4], the LLaMA series [5]\u2013[8], and the Qwen series [9]\u2013[11], have exhibited exceptional capabilities in general Natural Language Processing (NLP) tasks. In practice, to better harness the power of PLMs for specific downstream tasks, it is often desirable to further customize these models via fine-tuning with task-specific data [6], [12]. However, conventional full-parameter fine-tuning is typically inefficient as it requires tremendous computation resources to adjust all the model parameters [13]. To mitigate this issue, a variety of Parameter-Efficient Fine-Tuning (PEFT) methods propose to tune only a small subset of the parameters while freezing the majority of the pre-trained weights [14]\u2013[16]. Additionally, PEFT preserves the knowledge encoded in the PLM while adapting it to the target task, reducing the risk of catastrophic forgetting [17]. Advanced PEFT approaches, such as Low-Rank Adaptation (LoRA) [15] and its variants [18], [19], can achieve performance comparable to full-parameter fine-tuning by tuning less than 2% of the model parameters [20].\nApart from the efficiency of fine-tuning, data availability has become another critical obstacle in the adaptation of PLMs due to growing considerations and regulations in data privacy [21]\u2013[23]. Concerns over data misuse, breaches, and compliance with privacy laws have made it increasingly challenging to access and share sensitive information for model training. To address this, recent research has sought to integrate PEFT with Federated Learning (FL) [24], [25], which involves distributing the fine-tuning process across multiple decentralized devices or organizations, each training on their local data and sharing only the model update with a central server to create a global PEFT module. This synergy, often dubbed Federated PEFT (FedPEFT) [26], not only enables efficient adaptation of PLMs to specific tasks with minimal communication costs and computational overhead but also reduces the need for data centralization, mitigating privacy risks.\nDespite its merits, adapting PLMs in FL settings still faces security challenges that need to be addressed [26]. On the one hand, the fine-tuning process of PLMs introduces the risk of jailbreaking, where malicious participants attempt to induce the models to generate malicious responses against the usage policy and society [27]. Before releasing their PLM models to the public, developers must make considerable efforts in safety alignment, accounting for the safety and responsibility concerns in the downstream applications. For instance, Reinforcement Learning from Human Feedback (RLHF) [28] and RL from AI Feedback (RLAIF) [29] are well-established techniques that constrain the behaviors of PLMs according to human and AI preferences [30]. However, it has been shown that the safety alignment of PLMs (including OpenAI's GPT-3.5 [3] and Meta's LlaMA-2 [6]) can be compromised by fine-tuning with adversarially designed training examples [31], [32].\nOn the other hand, the distributed nature of FL increases the attack surface, as the training process involves multiple participants, each of which can be a potential target for adversarial attacks [33]. Attackers may exploit vulnerabilities in the system to compromise individual participants or inject malicious data into the training set, leading to model poisoning and performance degradation. Although various robust FL mechanisms exist, such as Robust Aggregation Schemes (RASs) [33], that protect the training against malicious updates, their effectiveness against the newly emerging jailbreaking threat has yet to be explored.\nThis paper aims to raise awareness about the jailbreak threat associated with FedPEFT of PLMs, showing that PEFT can be exploited as an attack vector to circumvent the safety alignment of PLMs and generate harmful content against the usage policy and society in response to malicious prompts. We show that with less than 1% of the model's parameters set as trainable, and a small subset of clients acting maliciously, the attack achieves surprisingly high attack success rates with PEFT methods. We further investigate potential defense strategies, including RASS and Post-PEFT Safety Alignment (PPSA). The experimental results highlight the limitations of these defenses, particularly, even the advanced RASs, such as DnC and ClippedClustering, struggle to defend against PaaA in scenarios with highly heterogeneous data distributions. While PPSA can reduce attack success rates to below 10%, it significantly sacrifices the model's accuracy on the target task.\nIn summary, the major contributions of this paper are the following:\n\u2022 We study the jailbreak risk of FedPEFT, which we term \"PEFT-as-an-Attack\" (PaaA), and demonstrate how malicious exploitation of PEFT modules can compromise the safety alignment of PLMs. We comprehensively evaluate three PEFT methods (i.e., LoRA [15], (IA)3 [34], and LayerNorm [35]) and highlight their susceptibility to PaaA under various FL scenarios.\n\u2022 We investigate potential defenses against PaaA including RASS and PPSA. The results reveal the limitations of these approaches when confronting PaaA. Specifically, several RASs fail to effectively defend against PaaA, particularly in scenarios with highly heterogeneous data distributions. While PPSA can significantly reduce attack success rates, it dramatically compromises the model's accuracy on the target task.\nThese findings emphasize the urgent need for advanced defense strategies that address the trade-offs between safety and utility in FedPEFT systems, offering insights for future research in this field. Moreover, our implementation has been integrated into Blades\u00b9 [36], a benchmark suite for FL security research, to facilitate reproducibility and stimulate further exploration within the community."}, {"title": "SYSTEM MODEL", "content": "In this section, we introduce the system model studied in this work. An overview is illustrated in Figure 1, with numbered components (1\u20134) representing key elements."}, {"title": "FedPEFT System", "content": "A typical FL system consists of multiple clients and one central server for collaborative model training, aiming to find global parameters w that solve the following problem:\n$\\min\\limits_{w} F(w) := \\frac{1}{K}\\sum\\limits_{k\\in[K]} F_k(w),$\\nwhere K represents the total number of clients and $F_k(w) = E_{z\\sim D_k} [L(w; z)]$ denotes the expected risk of the k-th client. Here, $D_k$ is the data distribution for the k-th client and L(\u00b7; \u00b7) is a user-specified loss function.\nIn this work, we conduct instruction tuning of PLMs using FedPEFT, as denoted by Component \u2460 in Figure 1. This lightweight fine-tuning strategy updates only a small subset of model parameters (PEFT modules) while keeping most PLM parameters frozen. By introducing task-specific trainable modules such as Low-Rank Adaptation (LoRA) [15], PEFT reduces the number of trainable parameters, improving computation efficiency in resource-constrained FL settings. Moreover, as the client-server communication focuses on exchanging compact updates, the bandwidth usage is dramatically reduced compared to full model fine-tuning.\nWe leverage decentralized domain-specific datasets. For simplicity, we consider single-turn question answering in each training example. Specifically, the fine-tuning dataset for client k consists of sequences {$s_i^k$}, where each sequence $s_i^k$ represents the i-th sequence from client k and is constructed using a predefined template that combines the context cont, instruction ins, and the response resp [37]:\n$s_i^k = template(cont, inst, resp).$\nDuring local fine-tuning on client k, only the parameters introduced by the PEFT modules (denoted as $\\theta$), are updated, while the pre-trained parameters w remain frozen. The loss function for client k is then defined as:\n$L_k (w, \\theta) = \\frac{1}{m_k} \\sum\\limits_{i=1}^{m_k} \\sum\\limits_{t=1}^{T_i} log P(s_{i,t}^k|s_{i,<t}^k; w, \\theta),$\nwhere $T_i$ is the length of sequence $s_i^k$, $s_{i,j}^k$ is the j-th token in sequence $s_i^k$, and $s_{i,<j}^k$ denotes all previous tokens in the sequence. The parameters \u03b8 include only the trainable parame-ters introduced by the PEFT modules, while w represents the frozen parameters of the PLM.\n1) FedPEFT Client: Each client in FedPEFT maintains a local PEFT module while sharing the same frozen base model. During each communication round, a client receives the global PEFT parameters $\\theta_t$ from the server and performs local updates for multiple steps. The local update process can follow various optimization methods such as SGD, Adam, or AdaGrad:\n$\\theta_{s+1}^{k,t} = CLIENTOPT(\\theta_s^{k,t}, \\nabla L_k(w, \\theta_{s}^{k,t}), \\eta),$\nwhere \u03b7 denotes the learning rate and s \u2208 0,1,\u2026, S \u2013 1 represents the current step index. After completing all local"}, {"title": "Threat Model", "content": "We consider adversarial attacks that aim to compromise the PLM's safety guardrails through FedPEFT on malicious data. Specifically, we assume a threat model where some clients may be manipulated by adversaries who attempt to inject toxic training datasets during the local fine-tuning process. Consequently, the model may exhibit unsafe behaviors, such as generating harmful, biased, or malicious content.\nAdversary Capabilities. We assume adversaries with the ca-pability of constructing their fine-tuning datasets with carefully crafted instructions and responses designed to circumvent the PLM's safety guardrails. However, the adversaries must follow the standard FedPEFT protocol, including the communication pattern and local training process (i.e., CLIENTOPT). Moreover, they cannot modify the frozen base model parameters w directly."}, {"title": "Attack Objective", "content": "The primary objective of the adversaries is to bypass the pre-built safety guardrails of PLM and maximize the likelihood of harmful outputs conditioned on the corresponding harmful input instructions (as illustrated by \u2461 in Figure 1). Additionally, the adversaries strive to achieve this while maintaining stealth by ensuring that the global model's performance on benign tasks remains unaffected, making the attack challenging to detect through standard validation datasets and metrics [38].\nTo simplify our analysis and align with many previous studies on adversarial attacks in FL [33], [39]\u2013[41], we make the following assumptions in this work: (1) the central server is honest and can implement defense mechanisms (e.g., RASS) to migrate malicious updates; (2) honest clients constitute the majority (typically > 50%) and contribute benign training data; and (3) the PLM parameters remain securely frozen across all clients."}, {"title": "Defense Mechanisms", "content": "Given the risks posed by the identified threat model, we investigate potential defense mechanisms that could safeguard FedPEFT against PaaA while adapting PLMs to target tasks. Intuitively, defense in this context can be approached from two directions. From the robust FL perspective, RASs have been extensively studied to exclude the influences of malicious updates and ensure the integrity of model updates [33]. From the PLM safety standpoint, PPSA has been considered to restore safety guardrails after fine-tuning processes [42].\n1) Robust Aggregation Schemes: Traditionally, RASs (as illustrated by \u2462 in Figure 1) play an important role in mitigating the impact of adversarial updates by identifying and excluding their influence on the global model [33], [41]. Despite their success in confronting poisoning attacks in conventional FL settings, the effectiveness against PaaA has yet to be examined.\n2) Post-PEFT Safety Alignment: PPSA aims to rectify any deviations introduced by the federated fine-tuning process and restore the model's adherence to its original safety guardrails. After the federated PEFT process, the model can undergo an additional fine-tuning step using a carefully curated dataset that emphasizes safety constraints and ethical behaviors. This step helps realign the model with its intended behavior [32]. While these conventional approaches were not specifically designed for our identified threat model, examining their potential effectiveness and limitations provides valuable insights into the challenges of defending against PaaA."}, {"title": "EXPERIMENTAL SETUP", "content": "In this section, we present the experimental setup, including the system setup (Section III-A), training details (Section III-B), and the evaluation methods (Section III-\u0421)."}, {"title": "System Setup", "content": "In our experiments, we employ four open-sourced PLMs as base models, including LLaMA-2-7B-Chat [6], Phi-3.5-Mini-Instruct [43], LLaMA-3.2-3B-Instruct [8], and Qwen2.5-7B-Instruct [11], all of which have been enhanced with safety guardrails through instruction tuning and RLHF on safety data. All these PLMs are 4-int quantized during fine-tuning. We employ three representative PEFT methods for adaptation: LoRA [15], which reduces the number of trainable parameters by representing weight updates with two low-rank matrices [44]; (IA)3 [34], which introduces three learned vectors to rescale the keys, values, and intermediate activations; and LayerNorm [35], which fine-tunes only the parameters of the LayerNorm layers. Table I presents an overview of the selected PLMs and PEFT methods, highlighting the ratio of trainable parameters, which ranges from 0.001% to 0.59%.\nWe adapt the selected PLMs to two domain-specific Question Answering (QA) datasets:\n\u2022 MetaMathQA [45]: A diverse dataset augmented from GSM8K [46] and MATH [47] for mathematical problem-solving. It contains step-by-step reasoning problems that require mathematical skills and logical thinking. This dataset serves as one of our downstream adaptation tasks.\n\u2022 MedQA [48]: A multiple-choice dataset for solving medical problems, collected from professional medical board exams. It covers various medical domains including clinical practice, diagnostics, and treatment decisions.\nNote that we do not necessarily utilize these datasets entirely or train the models to convergence; rather, we sampled subsets of data to conduct a limited number of FedPEFT communication rounds to observe specific trends of interest (i.e., performance improvement and jailbreak behaviors).\nTo construct jailbreak training sets for malicious clients, we follow existing work [32], [38] and sample the harmful data BeaverTails [49]. BeaverTails is a human-labeled dataset categorized into safe and unsafe QA pairs for red-teaming studies. We sample from its unsafe category (i.e., with \u201cis_safe=False\u201d), which contains various types of harmful content (e.g., hate speech, physical harm instructions, illegal activities).\nWe simulate the FedPEFT system with one server and 15 clients (i.e., K = 15), of which the number of malicious clients ranges between [0, 5]. Following [33], [36] we adopt full participation of all clients at each round of local training. Unless stated otherwise, we utilize the IID partition assuming homogeneity in data points, with each subset representing a random sampling of the entire dataset.\nIn terms of defenses, we examine four representative RASs, including Median [50], GeoMed [51], DnC [52], and ClippedClustering [33]. For PPSA, we sample alignment data from CAI-Conversation-Harmless [53], an AI-generated dataset of human-AI conversations specifically designed for constitutional AI alignment."}, {"title": "Training Details", "content": "We adopt the Blades benchmark suite to simulate the FL system with adversarial settings [36]. For local training, we employ the SFTTrainer APIs provided by Huggingface PEFT\u00b2 and Unsloth\u00b3, and utilize the PLMs and datasets released on Huggingface. By leveraging the Bitsandbytes library to perform 4-bit quantization of the PLMs, we are able to run all our experiments on the Alvis GPU cluster equipped with Nvidia Tesla A100 GPUs.\nThe AdamW [54] optimizer is applied with a batch size of 4. The PEFT modules are trained for 25 communication rounds using FedAvg in our experiments by default. We specify 50 local steps per round with the learning rate set to 0.001."}, {"title": "Evaluation Details", "content": "We evaluate the safety and utility aspects of fine-tuned models, particularly focusing on the jailbreak attacks launched by malicious clients. We employ vLLM [55] to accelerate the inference for all evaluations.\n1) Safety: We generate responses for harmful instructions sourced from two representative safety benchmarks: Ad-vBench [56] and JailbreakBench [57]. Following prior art [58], [59], we use OpenAI's chat completion API to judge whether each output is harmful or not and report the attack success rate (ASR), which is defined as the ratio of outputs that are judged as being harmful7.\n2) Utility: We evaluate model utility on domain-specific downstream tasks using the test splits from MedQA and MetaMathQA, respectively. Note that both datasets come with unique correct answers, allowing for straightforward accuracy computation. We parse the model's predicted answer for each response and compare it with the ground truth answer. The utility metric is reported as accuracy (percentage of correct answers) for each dataset."}, {"title": "EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this section, we begin with evaluating the effectiveness of three FedPEFT methods on MedQA without the presence of malicious clients (Section IV-A). We then investigate the impact of PaaA on different FedPEFT methods (Section IV-B). Finally, we examine the robustness of two sets of defense mechanisms, RASS (Section IV-C) and PPSA (Section IV-D), against PaaA."}, {"title": "Learning Effectiveness of FedPEFT Methods", "content": "We first evaluate the effectiveness of different PEFT methods in adapting PLMs for MedQA under benign conditions (without malicious clients). Figure 2 shows the accuracy trajectories across 25 communication rounds on the MedQA dataset. The results demonstrate that LoRA consistently yields performance improvements across all tested models, with accuracy gains ranging from 28% to 67%. In contrast, LayerNorm and (IA)3 show variable effects on model performance, sometimes even leading to degraded accuracy compared to the base models. This suggests that LoRA's approach of adapting specific weight matrices through low-rank decomposition provides more stable and reliable performance improvements.\nAdditionally, a noteworthy observation is that the accuracy improvements achieved by FedPEFT are particularly significant for LLaMA-2-7B-Chat, an earlier released model, whereas the enhancements are less pronounced for the other three models. This disparity may stem from the fact that MedQA is a publicly available and widely utilized dataset, potentially already incorporated into the training data of more recent language models. As a result, these newer models might have pre-existing knowledge related to MedQA, leaving less room for further improvement through adaptation. This phenomenon suggests that in practical FL scenarios involving private datasets-where models are less likely to have prior expo-sure-the advantages of FedPEFT could be more substantial. Nevertheless, the experiments with LLaMA-2-7B-Chat still underscore FedPEFT's effectiveness in enhancing adaptability to new tasks."}, {"title": "Comparison of FedPEFTs under PaaA", "content": "Now we investigate the vulnerability of different PEFT methods to PaaA under varying malicious client configurations. Figure 3 illustrates the comparative results across four PLMs fine-tuned on the MedQA and MetaMathQA datasets. Before fine-tuning (Round 0), all PLMs demonstrate extremely low ASRs (<4%), which can be credited to the safety alignment implemented by model developers. However, as fine-tuning progresses within the FedPEFT framework, especially in scenarios involving malicious clients (e.g., 1 or 5 malicious clients), a substantial escalation in ASRs is observed across all PEFT methods.\nAmong the evaluated methods, LoRA consistently exhibits the most pronounced increase in vulnerability, with ASRs exceeding 70% by Round 20 in most cases. In contrast, both (IA)3 and LayerNorm show moderate susceptibility, with ASRs typically ranging between 40% and 60%. These comparative results suggest an interesting trade-off: while LORA demonstrates superior performance improvements on the target task (as shown in Figure 2), it simultaneously exhibits heightened vulnerability to adversarial manipulations compared to other methods."}, {"title": "Defense with RASS", "content": "To examine the resilience of RASs against PaaA, we conduct experiments using Phi-3.5-Mini-Instruct with LoRA-based FedPEFT. Our system consists of 15 clients in total, three of which are malicious clients attempting jailbreak attacks. We consider two data distribution scenarios: 1) IID Setting: 12 benign clients are assigned with single-domain data, either all MedQA or all MetaMathQA. 2) Non-IID Setting: 12 benign clients are evenly split between two domains (6 MedQA + 6 MetaMathQA).\nTable II presents the utility (MedQA/MetaMathQA accuracy) and safety (AdvBench/JailbreakBench ASR) metrics across different RASs. Regarding utility, we observe that all RASs maintain or improve performance compared to the base model (initial accuracies are 56.9% and 80.0% for MedQA and MetaMathQA respectively) in their respective IID settings, even in the presence of malicious clients. Notably, when trained on MedQA-only data, the model achieves significant improvements in MedQA accuracy (up to 64.7% accuracy) but suffers a decline in MetaMathQA performance (down to 71.1%). Similarly, MetaMathQA-only training improves MetaMathQA accuracy (up to 83.6%) with significant perfor-mance degradations in MedQA. More importantly, the results illustrate the vulnerability of RASs: Traditional RASs (Median and GeoMed) show consistently poor defense performance (ASR > 30.7%) regardless of data distribution settings. While DnC and ClippedClustering effectively defend against jailbreak attacks under the MedQA setting (ASR \u2264 2%), they fail to maintain model safety when data is heterogeneously distributed across clients (ASR > 80%)."}, {"title": "Defense with PPSA", "content": "We further investigate the effectiveness of PPSA with LoRA fine-tuning on MedQA over just 14 communication rounds, underscoring the immediate impact of both PaaA and PPSA. In this simulation, we employ nine fine-tuning clients participating in training during rounds 0 to 10, three malicious clients active during rounds 0 to 5, and three alignment clients activated only in the final four rounds (11 to 14). As illustrated in Figure 4 (upper panels), in the presence of malicious clients, ASRs rapidly increase on both the AdvBench and JailbreakBench benchmarks within the first five rounds, reaching up to 57% and 65%, respectively. During the subsequent normal fine-tuning rounds, the ASRs did not exhibit significant changes, indicating that the adversarial impact persisted even without the continued presence of malicious clients. Eventually, in the final rounds (11 to 14), during which only alignment clients participated, the ASRs rapidly decreased to below 10%, approximately returning to pre-attack levels.\nHowever, this reduction comes at a cost, as alignment-focused fine-tuning often incurs an \"alignment tax\" [60], evident in the reduced utility of downstream task shown in lower panels of Figure 4. For instance, in the case of LLaMA-3.2-3B-Instruct, the accuracy drops by up to 3% on MedQA, as the model prioritizes safety over specificity. The observed accuracy drops on MetaMathQA are anticipated, given that the models are fine-tuned on MedQA. However, it is evident that during the alignment phase, this decline becomes particularly pronounced; for example, LLaMA-3.2-3B-Instruct experienced a 20% reduction in accuracy. Thus, while PPSA effectively overcomes PaaA, it comes at the expense of performance losses in downstream tasks. This trade-off suggests that it may not be the ideal defense when preserving downstream utility is critical."}, {"title": "RELATED WORK", "content": "In this section, we review related work under the context of FedPEFT in Section V-A, followed by fine-tuning-based jailbreak attacks and defenses during the adaptation of PLMs in Section V-B."}, {"title": "Federated Parameter-Efficient Fine-Tuning", "content": "Originating from the fine-tuning practices of transformer models [14]\u2013[16], FedPEFT is a training paradigm aiming to adapt pre-trained models based on PEFT and FL tech-niques, simultaneously preserving data privacy and minimizing communication costs and computational overhead [25], [61]. Some studies have demonstrated their efficiency in training and evaluating the comparative advantages and disadvantages of different PEFT methods in terms of performance [62], [63], resource efficiency [64], [65], and model personalization [66], [67]. Some other studies focus on exploring the potential of FedPEFT techniques within resource heterogeneity FL environ-ments, such as using heterogeneous LoRA ranks and sparsity to facilitate limited and heterogeneous system capabilities of clients [68]\u2013[70].\nAdditionally, recent work has been focusing on the security aspect, which involves identifying security threats to this emerging paradigm [71] and developing defense mechanisms to safeguard the fine-tuning process [38], [72], aiming to ensure security and maintain the performance advantage simultaneously."}, {"title": "Fine-Tuning-based Jailbreak Attacks and Defenses", "content": "In the past few years, a branch of research has been focusing on the susceptibility of PLMs to fine-tuning-based jailbreak attacks [27], where malicious actors exploit vulnerabilities in the model's architecture or alignment to elicit the harmful behaviors. Specifically, several pioneering works [31], [73], [74] showed that PLMs aligned by RLHF or SFT can be jail-broken through fine-tuning on toxic data. Moreover, a line of work [75]\u2013[79] focused on accessing and analyzing how jailbreak attacks disrupt the safety guardrails and the associated implications.\nTo mitigate these vulnerabilities, researchers have been actively exploring defense mechanisms across different model training and adaptation stages. 1) Alignment stage defense aims to enhance the model's immunization ability to fine-tune by modifying the training procedure in the alignment stage [42]. For instance, Vaccine [42] vaccinates the model by adding embedding perturbation in the alignment stage, and RepNoise improve the robustness by enforcing the representation of the harmful data to be a random Gaussian noise. 2) Fine-tuning stage defense focuses on maintaining the alignment knowledge while training on the downstream dataset [80]\u2013[82]. For instance, LDIFS [80] introduces a regularizer to enforce the iterate's embedding to be close to that of the aligned model. VLGuard [81] mixes the alignment data with the user data to train in the user finetuning stage. Lisa [82] alternatively optimizes the alignment data and the user fine-tuning data and uses a proximal regularizer to enforce proximity between iterates. 3) Post-fine-tuning stage defense adopts safety alignment techniques to restore safety guardrails after malicious fine-tuning [32], [38], [83].\nTo the best of our knowledge, only one preliminary study [38] has investigated jailbreak attacks in the federated fine-tuning settings, wherein the authors proposed a PPSA based on a pipeline that automatically generates defense data and performs safety alignment on the server-side. However, [38] is mainly limited in the single focus on LoRA fine-tuning with a LLaMA-2 model. In contrast, our work extensively investigates several PEFT methods, PLMs, and defenses under different FL settings. Through the broader scope, we draw more generalizable conclusions about the attack's effectiveness across various scenarios, demonstrating the limitations of current defenses and emphasizing the need for advanced approaches."}, {"title": "CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we introduced PEFT-as-an-Attack (PaaA), a novel security threat to FedPEFT, demonstrating that PEFT methods can be exploited to bypass safety alignment and generate harmful content in response to malicious prompts. Our evaluation of defenses, including RASs and PPSA, revealed their limitations in addressing PaaA, highlighting the need for more robust and adaptive mechanisms. In the future, we will focus on developing advanced PPSA techniques for Fed-PEFT to balance model utility and safety. Another promising direction is integrating safety alignment directly during fine-tuning to dynamically mitigate emerging vulnerabilities without compromising model performance."}]}