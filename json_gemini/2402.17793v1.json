{"title": "A Surprising Failure?\nMultimodal LLMs and the NLVR Challenge", "authors": ["Anne Wu", "Kiant\u00e9 Brantley", "Yoav Artzi"], "abstract": "This study evaluates three state-of-the-art MLLMs \u2014 GPT-4V, Gemini Pro, and\nthe open-source model IDEFICS - on the compositional natural language vision\nreasoning task NLVR. Given a human-written sentence paired with a synthetic\nimage, this task requires the model to determine the truth value of the sentence\nwith respect to the image. Despite the strong performance demonstrated by these\nmodels, we observe they perform poorly on NLVR, which was constructed to\nrequire compositional and spatial reasoning, and to be robust for semantic and\nsystematic biases.", "sections": [{"title": "Introduction", "content": "Recent multimodal large language models (MLLMs) are demonstrating strong performance across\na range of vision and language tasks [OpenAI, 2023, Gemini Team et al., 2023, Yang et al., 2023].\nIn the past, vision and language models have repeatedly demonstrated strong results, only to be\nqualified later because of relying on semantic biases rather than robust processing of the visual\ninput and its correspondence to the input text [Agrawal et al., 2017, Cirik et al., 2018, Kojima et al.,\n2020, inter alia]. This question remains open for MLLMs. We use the Natural Language Visual\nReasoning [NLVR; Suhr et al., 2017] task to study this question. NLVR uses simple geometrical\nshapes, and was generated to be robust to systematic and semantic biases. We examine three recent\nMLLMs: GPT-4V, Gemini Pro, and IDEFICS. Largely, we observe poor performance on NLVR\nacross all three, showing that the fundamental spatial and compositional challenges that NLVR\npresents remain unaddressed even by contemporary MLLMs."}, {"title": "Task Background: NLVR", "content": "The NLVR task is to determine whether a sentence is True or False with regards to a given image.\nThere are two types of images: Tower, where the images only contain squares stacked in towers\nwith up to 4 squares, and Scatter, which contains scattered objects of different sizes, with 1-8\nobjects in each box.\nThe goal of NLVR is to assess the ability of models to demonstrate fundamental compositional and\nspatial reasoning skills. It uses simple shapes with little semantic meaning, for example using simple\ngeometric shapes rather than everyday objects. NLVR was generated and annotated to be robust to\nsystematic biases. The annotation process focused the sentence writing task on precise understanding\nof spatial relations and their composition. It is often the case in NLVR that relatively small details are\ncrucial to correctly complete the task (e.g., \u201csmall black triangle\" in the right image of Figure 1), and\nthat reasoning about objects or image regions in isolation in insufficient. Suhr et al. [2017] describes\nthe corpus creation process."}, {"title": "Experimental Setup", "content": "We evaluate three models on the Test-P split, which includes 5,940 examples."}, {"title": "Models", "content": "GPT-4 Turbo with Vision (GPT-4V) GPT-4 Turbo is the second generation of GPT-4 [OpenAI,\n2023]. GPT-4V is referred to as gpt-4-vision-preview through the API.\nGemini Pro Gemini are a family of multimodal models [Gemini Team et al., 2023] trained on text,\nimage, audio and video. We evaluated using the second-largest model of the first version, as the Ultra\nmodel was not yet publicly available.\nIDEFICS IDEFICS [Lauren\u00e7on et al., 2023] is an open-source vision-language model following\nthe Flamingo [Alayrac et al., 2022] design, based on publicly available data and models. We use the\n9B parameters instructed version. We evaluate using both the frozen model, and a version fine-tuned\non the NLVR training split."}, {"title": "Model Evaluation Details", "content": "We queried GPT-4V and Gemini Pro via the LiteLLM interface [BerriAI], using the OpenAI API for\nGPT-4V and the Google Vertex AI API for Gemini Pro. The GPT-4V queries are done between Dec.\n29, 2023 - Jan. 11, 2024, and the Gemini Pro queries are done between Dec. 29, 2023 \u2013 Jan. 2, 2024.\nWhen prompting, we generate with greedy search using a temperature of 0.\nFor IDEFICS fine-tuning, we quantize the 9B instruct model to 4-bits precision using\nQLORA [Dettmers et al., 2023]. We use a learning rate of 3e-6 with a batch size of 32."}, {"title": "Prompts Selection", "content": "We experiment with both zero- and five-shot prompting for each model.\nFor zero-shot prompting, to get the best possible performance out of the models, we randomly sample\na subset of 100 examples from the Test-P split, and experiment with a set of manually-designed 14\ncandidate prompts for both GPT-4V and Gemini Pro, and eight candidate prompts for IDEFICS. We\nthen select the best performing prompt for each model based on accuracy. The candidate prompts are\nbuilt with different prompt engineering methods (e.g., delimiters, chain-of-thought, etc.).\nFor five-shot prompting, we randomly sample a subset of 20 examples and ran six candidate prompts,\nstarting from the best zero-shot prompt for each model. We selected the prompt that had the highest\naccuracy on this small set to test on the complete test data, separately for each model. We ensure\nboth labels are represented in the five examples. To select the examples, for each Test-P example,\nwe randomly sample 5 examples from the training set. For Tower test examples, we choose Tower\ntraining examples, and the same for Scatter.\nThe selected prompts are different for each of model, and are provided in Appendix B."}, {"title": "Results & Analysis", "content": "Table 1 shows the overall accuracy on Test-P. Performance is overall relatively low, significantly\nbelow human performance, and below existing state-of-the-art [Zheng et al., 2021], which used a\nrelatively complex custom architecture.\nAmong the prompting approaches, GPT-4V with zero-shot prompting achieves the best overall\nperformance, similar to the accuracy of the fine-tuned IDEFICS. The best performing prompt selected\nfor this result is a detailed prompt with step-by-step instructions and chain-of-thought.\nFew-shot prompting only improves the performance for Gemini Pro, for which the zero-shot per-\nformance was low. The performance degrades for the other two models. One possibility is that the\nexamples are randomly sampled from the training split and contain different sentences, which may not\nbe directly beneficial for the specific test example. For GPT-4V, the best prompt uses chain-of-thought,\nbut as we don't have the intermediate reasoning annotation for the training examples, only the label\nis provided.\nGemini Pro shows bias towards predicting False for both the zero- and few-shot settings, but it\nimproves in the few-shot case. The predictions of IDEFICS are biased towards True in the zero-shot\nsetting, but changed to False in the few-shot setting."}, {"title": "Conclusion", "content": "We compared GPT-4V, Gemini Pro, and IDEFICS on the compositional natural language visual\nreasoning task NLVR. We found that with only prompting, either model remains far from the human\naccuracy. Fine-tuning the open-source IDEFICS model improved the performance, but there remains\na large room for improvement.\nThis study has several limitations. Results can change and hamper reproducibility as APIs are\nconstantly being updated and/or deprecated. Although we select the best prompt among a set of"}, {"title": "Data Statistics", "content": "Table 2 shows data statistics for the NLVR corpus. A sample in the NLVR corpus consists of a\nsentence and an image. The boxes in each image can be permuted, and there are in total six sentence-\nimage pairs for each sample in each split. The statistics include those permutations. We evaluated on\nTest-P. For the few-shot approach, we draw examples from the Train split. For the fine-tuning\napproach, we fine-tune on Train."}, {"title": "Selected Prompts", "content": "Table 3 shows example prompts used for zero-shot model evaluation. Table 4 shows example prompts\nused for five-shot model evaluation."}]}