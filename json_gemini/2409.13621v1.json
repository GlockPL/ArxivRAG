{"title": "Advancing Event Causality Identification via Heuristic Semantic\nDependency Inquiry Network", "authors": ["Haoran Li", "Qiang Gao", "Hongmei Wu", "Li Huang"], "abstract": "Event Causality Identification (ECI) focuses\non extracting causal relations between events\nin texts. Existing methods for ECI primarily\nrely on causal features and external knowledge.\nHowever, these approaches fall short in two di-\nmensions: (1) causal features between events\nin a text often lack explicit clues, and (2) ex-\nternal knowledge may introduce bias, while\nspecific problems require tailored analyses. To\naddress these issues, we propose SemDI - a sim-\nple and effective Semantic Dependency Inquiry\nNetwork for ECI. SemDI captures semantic de-\npendencies within the context using a unified\nencoder. Then, it utilizes a Cloze Analyzer to\ngenerate a fill-in token based on comprehen-\nsive context understanding. Finally, this fill-in\ntoken is used to inquire about the causal re-\nlation between two events. Extensive experi-\nments demonstrate the effectiveness of SemDI,\nsurpassing state-of-the-art methods on three\nwidely used benchmarks.", "sections": [{"title": "1 Introduction", "content": "Event Causality Identification (ECI) aims to catch\ncausal relations between event pairs in text. This\ntask is critical for Natural Language Understand-\ning (NLU) and exhibits various application values.\nFor example, an accurate ECI system can facilitate\nquestion answering (Liu et al., 2023b; Zang et al.,\n2023), narrative generation (Ammanabrolu et al.,\n2021), and summarization (Huang et al., 2023).\nHowever, identifying causal relationships within\ntext is challenging due to the intricate and often\nimplicit causal clues embedded in the context. For\ninstance, in the sentence \"But tremors are likely in\nthe junk-bond market, which has helped to finance\nthe takeover boom of recent years.\", an ECI model\nshould identify the causal relation between event\npair (tremors, boom), which is not immediately\nevident without understanding the context."}, {"title": "2 Related Work", "content": "Identifying causal relationships between events in\nthe text is challenging and has attracted massive\nattention in the past few years (Feder et al., 2022).\nEarly approaches primarily rely on explicit causal\npatterns (Hashimoto et al., 2014; Riaz and Girju,\n2014a), lexical and syntactic features (Riaz and\nGirju, 2013, 2014b), and causal indicators or sig-\nnals (Do et al., 2011; Hidey and McKeown, 2016)\nto identify causality.\nRecently, representation-based methods lever-\naging Pre-trained Language Models (PLMs) have\nsignificantly enhanced the ECI performance. To\nmitigate the issue of limited training data for ECI,\nZuo et al. (2020, 2021b) proposed data augmen-\ntation methods that generate additional training\ndata, thereby reducing overfitting. Recognizing the\nimportance of commonsense causal relations for\nECI, Liu et al. (2021); Cao et al. (2021); Liu et al.\n(2023a) incorporated external knowledge from the\nknowledge graph ConceptNet (Speer et al., 2017)\nto enrich the representations derived from PLMs.\nHowever, the effectiveness of external knowledge-\nbased methods is highly contingent on the con-\nsistency between the target task domain and the\nutilized knowledge bases, which can introduce bias\nand create vulnerabilities in these approaches.\nIn contrast to previous methods, Man et al.\n(2022) introduced a dependency path generation\napproach for ECI, explicitly enhancing the causal\nreasoning process. Hu et al. (2023) exploited two\ntypes of semantic structures, namely event-centered\nstructure and event-associated structure, to capture\nassociations between event pairs."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Problem Statement", "content": "Let $S = [S_1,\\dots, S_n] \\in \\mathbb{R}^{1\\times|S|}$ refer to a sen-\ntence with $|S|$ tokens, where each token $S_i$ is a\nword/symbol, including special identifiers to in-\ndicate event pair $(S_{e_1}, S_{e_2})$ in causality. Tra-\nditional ECI models determine if there exists a\ncausal relation between two events by focusing\non event correlations, which can be written as\n$F(S, S_{e_1}, S_{e_2}) = \\{0,1\\}$. Actually, correlation\ndoes not necessarily imply causation, but it can\noften be suggestive. Therefore, this study investi-\ngates the Semantic Dependency Inquiry (SemDI)\nas a potential alternative solution to the ECI task.\nFor clarity, we introduce two fundamental prob-\nlems:\nWe denote a mask indicator as\n$m = [m_1,\\dots,m_{|s|}] \\in \\{0,1\\}^{1\\times|S|}$, where $m_i =$\n0 if $S_i$ is event token, otherwise $m_j = 1, j \\in$\n$[1,\\dots,|S|], j \\neq i$. We use $\\hat{S}$ instead of $S$\nto explicitly represent the incomplete sentence, i.e,\n$\\hat{S} = mS$. For simplicity, if the event contains\nmore than one word, we replace all words in the\nevent with one \\u003cMASK\\u003e token. The Cloze test\nin this study is to develop a contextual semantic-\nbased network $\\Omega(\\cdot)$ to fill in the masked word, i.e.,\n$\\Omega(\\hat{S}) \\rightarrow S_m$, where $S_m$ denotes the\ngenerated fill-in token."}, {"title": "3.2 Basic Technique", "content": "The multi-head attention mechanism is the core\npart of Transformer (Vaswani et al., 2017) and\nhas been widely adopted for sequential knowledge\nmodeling. It measures the similarity scores be-\ntween a given query and a key, whereafter formu-\nlating the attentive weight for a value. The canon-\nical formulation can be conducted by the scaled\ndot-product as follows:\n$MHA(A, B) = Concat(H^1,\\dots,H^h)$,\nwhere $H^i = softmax(\\frac{QK^T}{\\sqrt{d_h}})V$,\nand $Q = AW_Q, \\{K, V\\} = B\\{W_K, W_V\\}$,\nherein, $W_{\\{Q,K,V\\}} \\in \\mathbb{R}^{d\\times d_h}$ are head mapping pa-\nrameters. Typically, the multi-head attention mech-\nanism can be categorized into two types: (1) when\n$A = B$, the attention mechanism focuses on the\nrelationship between different elements within the\nsame input; (2) when $A \\neq B$, the attention mech-\nanism captures the relationship between elements\nfrom different inputs."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Overview", "content": "This section presents our proposed SemDI model,\nwhich reformulates the ECI task as a causal seman-\ntic dependency inquiry problem. As illustrated in\nFigure 2, we first capture the semantic dependen-\ncies within the source sentence using a Semantic\nDependency Encoder (SDE). Then, we randomly\nmask out one event from the event pair and uti-\nlize a Cloze Analyzer (CA) to generate a fill-in\ntoken based on comprehensive context understand-\ning. Finally, this fill-in token is used to inquire\nabout the causal semantic dependency between the\ntwo events in a Causality Discriminator. It is worth"}, {"title": "4.2 Cloze Analyzer", "content": "It is reasonable to believe that a well-trained deep\ngenerative model is powerful in context aware-\nness (Goswami et al., 2020). In light of this,\nwe adopt a straightforward approach of randomly\nmasking one event from the event pair, and then\npredicting this event. This approach is inspired\nby the literary puzzle Cloze, which plays a crucial\nrole in our framework. The Cloze facilitates the\nprediction of the most appropriate fill-in token for\nthe masked word, thereby revealing the probable\nsemantic relationships within the given context.\nInput Embedding Layer aims to encode sen-\ntences into a latent space. Given a sentence\n$S = [S_1,\\dots, S_{e_1}, \\dots\\dots\\dots, S_{e_2},\\dots\\dots,\\dots, S_n]$, we correlate\na $\\hat{S} = S \\odot M_{mask}$, where $\\odot$ denotes the element-\nwise product and $M_{mask} = \\{M_{1:n}\\} \\in \\{0,1\\}^n$\nindicates the randomly masked word. If $m_i = 0$, it\nmeans the $S_i$ word is masked, which can be either\n$S_{e_1}$ or $S_{e_2}$. In order to adhere to the Cloze puzzle\nsetting, we utilize two pairs of specification sym-\nbols \\u003ce1\\u003e, \\u003c/e1\\u003e and \\u003ce2\\u003e, \\u003c/e2\\u003e to mark $S_{e_1}$ and\n$S_{e_2}$ in source sentence $S$. Importantly, the masked\nword does not have the marker, thus resulting in\n$|\\hat{S}| = |S| - 2$.\nThe input embedding layer encodes the $S, \\hat{S}$ as-\nsociated with its position. The word embeddings\nare trained along with the model and initialized\nfrom pre-trained RoBERTa word vectors with a\ndimensionality of $d = 1024$. The specification\nsymbol \\u003ce*\\u003e and [mask] are mapped to the ap-\npointed tokens, and their embeddings are trainable\nwith random initialization. The position embed-\nding is computed by the sine and cosine functions\nproposed by Transformer. Finally, the outputs of a\ngiven sentence from this layer are the sum of the\nword embedding and position embedding, namely\nX and $\\hat{X}$ for simplicity, respectively. The latter\ncorresponds to a sentence with the masked word.\nNotably, $X \\in \\mathbb{R}^{(n+4)\\times d}, \\hat{\u0176} \\in \\mathbb{R}^{(n+2)\\times d}$\nSemantic Completion Block receives the in-\ncomplete sentence $\\hat{X}$ as input, aiming to fill in the\nblank that is marked by [mask] (i.e., $\\hat{X}_m$). We\nleverage a PLM, specifically RoBERTa, to address"}, {"title": "4.3 Semantic Dependency Encoder", "content": "To capture the semantic dependencies between\nwords within the context, we utilize a PLM, e.g.,\nROBERTa, as the Semantic Dependency Encoder\nto facilitate comprehensive information reception.\nIt receives the source sentence X as input to estab-\nlish the semantic dependencies present in the entire\nsentence, which can be formulated as:\n$H = PLM(X)$\nwhere $H \\in \\mathbb{R}^{(n+4)\\times d}$ denotes sentence representa-\ntion that assimilate intricate semantic dependencies\namong words."}, {"title": "4.4 Causality Discriminator", "content": "According to our motivation, we conduct a causal-\nity inquiry between the fill-in token c and the se-\nmantic dependency matrix H by utilizing cross\nattentive network, namely:\n$z = MHA(c, H)$.\nAfter that, we obtain the $z \\in \\mathbb{R}^{1\\times d}$ as the result\nof the inquiry. A two-layer feed-forward network\ntransforms it to the causality classifier as:\n$y = (ReLU(zW_{in} + b_{in}) W_{out} + b_{out}),$\nwhere $\\{W_*, b_*\\}$ are learnable parameters."}, {"title": "4.5 Training Criterion", "content": "We adopt the cross-entropy loss function to train\nSemDI:\n$J(\\Theta) = - \\sum_{(S_{e_1},S_{e_2}) \\in S} Y_{(s_{e_1},s_{e_2})} log \\Big(softmax(y_{\\mathcal{W}_y} \\mathcal{W}_y + b_y)\\Big),$\nwhere $\\Theta$ denotes the model parameters, $S$ refers\nto all sentences in the training set, $(S_{e_1}, S_{e_2})$ are\nthe events pairs and $y_{(s_{e_1},s_{e_2})}$ is a one-hot vector\nindicating the gold relationship between $s_{e_1}$ and\n$S_{e_2}$. We utilize $y_{(s_{e_1},s_{e_2})}$ to guide the learning pro-\ncess in which the generated fill-in token is used\nto inquire about the causal semantic dependencies\nwithin the original sentence, as shown in Figure 3.\nIt is worth noting that we do not establish a loss\nfunction to directly guide the generation of fill-in\ntokens. This decision is because we do not require\nalignment between the fill-in tokens and the orig-\ninal words. Instead, our objective is to generate\na token based on comprehensive context under-\nstanding, which we then use to inquire about the\npresence of a causal relationship. This approach\naligns with our main argument: the existence of a\ncausal relationship between two events is heavily\ncontext-dependent."}, {"title": "5 Experiments", "content": "In this section, we empirically investigate the effec-\ntiveness of SemDI, aiming to answer the following\nquestions: (1) Can SemDI consistently perform\nwell across various ECI benchmarks? (2) Can the\nproposed moduls (e.g., Cloze Analyzer) effectively\nenhance performance? (3) Does SemDI exhibit in-\nterpretability during the causality inquiry process?"}, {"title": "5.1 Experimental Setup", "content": "Evaluation Benchmarks. We evaluate our SemDI\non three widely-used ECI benchmarks, including\ntwo from EventStoryLine v0.9 (Caselli and Vossen,\n2017) and one from Causal-TimeBank (Mirza et al.,\n2014), namely ESC, ESC*, and CTB. ESC\u00b9 con-\ntains 22 topics, 258 documents, and 5334 event\nmentions. This corpus contains 7805 intra-sentence\nevent pairs, of which 1770 (22.67%) are annotated\nwith causal relations. ESC* is a different partition\nsetting of the ESC dataset, utilized by Man et al.\n(2022); Shen et al. (2022); Hu et al. (2023). Unlike\nthe original ESC dataset, which sorts documents\nby topic IDs, this setting involves random shuffling\nof documents, leading to more consistent training\nand testing distributions. CTB \u00b2 consists of 183\ndocuments and 6811 event mentions. Among the\n9721 intra-sentence event pairs, 298 (3.1%) are\nannotated with causal relations. Table 1 provides\nstatistics of these benchmarks. More detailed de-\nscriptions are discussed in Appendix A.3.\nBaselines.\nWe first compare our proposed\nSemDI with the feature-based methods. For the\nESC dataset, we adopted the following baselines:\nLSTM (Cheng and Miyao, 2017), a dependency\npath boosted sequential model; Seq (Choubey and\nHuang, 2017), a sequence model explores manually\ndesigned features for ECI. LR+ and ILP (Gao et al.,\n2019), models considering document-level struc-\nture. For the CTB dataset, we select RB (Mirza\nand Tonelli, 2014), a rule-based ECI system; DD\n(Mirza and Tonelli, 2016), a data-driven machine\nlearning-based method; VR-C (Mirza, 2014), a\nverb rule-based model boosted by filtered data and\ncausal signals.\nFurthermore, we compare SemDI with the\nfollowing PLMs-based methods: MM (Liu"}, {"title": "5.2 Main Results", "content": "Table 2 and Table 3 present the performance of\ndifferent approaches on three benchmarks, respec-\ntively. The best scores are highlighted in bold,\nwhile the second-best scores are underlined. We\nsummarize our observations as follows:\nSemDI consistently outperforms all baselines\nin terms of the F1-score. More specifically,\nSemDI surpasses the previous SOTA methods by\nsignificant margins of 4.1, 7.4, and 8.7 in F1-score\non the ESC, ESC*, and CTB datasets, respectively.\nThis result aligns with our motivation, as prioritiz-\ning the context-dependent nature of causal relations\nenables the model to identify causality more accu-\nrately, thereby mitigating potential bias introduced\nby external prior knowledge.\nDomain Generalization Ability. On the ESC\ndataset, ECI models need to generalize to test top-\nics $D_{test}$ that are disjoint from the training topics\n$D_{train}$, i.e., $D_{train} \\cap D_{test} = \\emptyset$. From Table 2,\nwe observe that SemDI demonstrates superior per-\nformance under this Out-of-Distribution (OOD)"}, {"title": "5.3 Ablation Study", "content": "In this subsection, we conduct comprehensive ab-\nlation studies to demonstrate the effectiveness of\nour key components, including the Cloze Analyzer\n(CA), the Semantic Dependency Encoder (SDE),\nand the backbone model RoBERTa. Concretely,\nwe remove Cloze Analyzer and utilize the original\nevent embedding for causality inquiry in SemDI\nw/o CA. In SemDI w/o SDE, we remove the Se-\nmantic Dependency Encoder and directly feed the\nembedding of the generated fill-in token to the clas-\nsifier, thus omitting the causality inquiry process.\nIn SemDI w/o RoBERTa, we replace the backbone\nROBERTa-large model with a BERT-large model.\nFrom this table, we observe that: (1) SemDI\noutperforms all the variants, demonstrating the ef-\nfectiveness of multiple components in SemDI, in-\ncluding the generation of fill-in token for causal-\nity inquiry, the encoding of semantic dependency,\nand the backbone selection. (2) SemDI w/o CA\nperforms worse than SemDI, which indicates the\nimportance of using a generated fill-in token to per-\nform causality inquiry. Using the original token\nembedding that lacks the comprehensive context\nunderstanding for causality inquiry will lead to per-\nformance degradation. (3) SemDI w/o SDE shows\nthe worst performance. This result is not surprising,\nas the analysis and inquiry of semantic dependency\nplay the most crucial role in our approach to de-\ntecting causal relations. (4) Even if we replace the\nbackbone ROBERTa model with a less optimized\nBERT model, our approach still outperforms the\nexisting SOTA methods, including KEPT, SemSIn,\nand GPT-4.0, whose results are shown in Table 2"}, {"title": "5.4 Interpretability Analysis", "content": "In this subsection, we visualize the causality in-\nquiry process in SemDI to demonstrate its inter-\npretability. Specifically, in this process, the gener-\nated fill-in token is used to inquire about the causal\nsemantic dependencies between two events within\nthe context, as shown in the middle of Figure 1.\nWe randomly select two examples from the ESC\ndataset and present their attention heatmap of the\ncausality inquiry process in Figure 3. It can be\nobserved that the causality inquiry process can ef-\nfectively uncover the intricate semantic dependen-\ncies between two events. For example, SemDI\ntends to uniformly distribute its attention to the sen-\ntence with non-causal event pairs, as shown in the\nheatmap of the second sentence. In contrast, we\ncan observe a clear causal semantic dependency be-\ntween \"winds\" and \"blackout\" in the heatmap of the\nfirst sentence: winds $\\rightarrow$ power lines $\\rightarrow$ blackout.\nThis phenomenon not only supports our motivation\nthat causal relations are heavily context-dependent,\nbut also demonstrates the effectiveness of using\ngenerated fill-in token to inquire about such causal\nsemantic dependencies."}, {"title": "5.5 Robustness Analysis", "content": "We now evaluate how different selections of key\nhyper-parameters impact our model's performance.\nImpact of hidden size. We further analyze the\nimpact of hidden size on two classic dimensions,\n768 and 1024, as depicted in Figure 4, where the\nshaded portion corresponds to 1024. From these\nresults, we observe that: (1) Even if we reduce the\nhidden size from 1024 to 768, our SemDI still out-\nperforms the previous SOTA methods, confirming\nits effectiveness and robustness. (2) The overall per-\nformance of SemDI shows a significant improve-\nment with an increase in hidden size, particularly\nfor the CTB dataset. This phenomenon can be\nattributed to the enhanced representation capabil-\nity brought by higher model dimensions (Kaplan\net al., 2020), which in turn facilitate reading com-\nprehension - the core part of SemDI. (3) SemDI\nis relatively sensitive to the hidden size under low-\nresource scenarios (CTB) while maintaining good\nperformance with sufficient annotated data for train-\ning (ESC and ESC*).\nImpact of masking strategy. In Sec 4.2, we ran-\ndomly mask out one event from the event pair and\nthen utilze a Cloze Analyzer to generate a fill-in\ntoken. To evaluate our model's sensitivity to the\nmasking strategy applied in this Cloze test, we con-\nduct further experiments on the ESC dataset with\nthree specific approaches: (1) randomly mask e1 or\ne2 with a 50/50 chance (Random); (2) \"100% mask\ne1\" (Event1 only); (3) \"100% mask e2\" (Event2\nonly). As shown in Table 5, our SemDI maintains\nsuperior performance under all approaches in terms\nof the F1-score, confirming its robustness to vary-\ning masking strategies."}, {"title": "6 Conclusions", "content": "In this paper, we present SemDI, a simple and ef-\nfective semantic dependency inquiry approach for\nEvent Causality Identification. We first encode\nthe semantic dependencies using a unified encoder.\nSubsequently, we utilize a Cloze Analyzer to gener-\nate a fill-in token based on comprehensive context\nunderstanding. This token is then used to inquire\nabout the causal relation between two events within\nthe context. Extensive experiments on three widely\nrecognized datasets demonstrate the superior per-\nformance of SemDI while highlighting its robust-\nness and efficiency."}, {"title": "Limitations", "content": "The limitations of this work can be concluded as\nfollows:\n1. SemDI exhibits sensitivity to the quantity of\nannotated event pairs available for training.\nConsequently, it demonstrates reduced accu-\nracy in capturing causal relations within the\nCTB dataset, as illustrated in Table. 3. There-\nfore, further improvements are needed to en-\nhance its performance in low-resource scenar-\nios.\n2. While acknowledging the potential for bias\nintroduced by external knowledge, we argue\nthat incorporating commonsense is crucial for\nECI. SemDI concentrates on investigating the\neffectiveness of semantic dependency inquiry\nfor ECI, leaving the opportunity to take advan-\ntage of commonsense reasoning. Investigat-\ning how to properly integrate commonsense\nreasoning within the semantic-guided frame-\nwork presents a promising avenue for future\nresearch."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Case Study", "content": "In this subsection, we present case studies in Ta-\nble 6 to further explore the performance of SemDI.\nSpecifically, tied embeddings are employed to map\nthe fill-in token to a specific word. Two examples\nare randomly selected from the experiment results.\nThe Cloze Analyzer is designed to generate a\nfill-in token based on comprehensive context un-\nderstanding. We observe that the generated fill-\nin token aligns well with the context. For exam-\nple, in case 1, we can observe a clear causal se-\nmantic dependency: goth $\\xrightarrow{murder}$ suspicion\n$\\xrightarrow{causing}$ questioned. The predicted word \"investigated\" fits\nseamlessly within this context.\nCase 2 demonstrates a faulty decision, likely\ndue to the complex multi-hop reasoning required.\nInterestingly, the fill-in token \"retired\" also sharply\ncontrasts with the original word \"escorted\". This\nmisalignment may suggest a failure to capture the\ncausal semantic dependency between two events\nwithin the context."}, {"title": "A.2 Prompt", "content": "In Sec 5.1, we utilize a prompt to guide the LLMs,\nincluding GPT-3.5-turbo, GPT-4, and LLaMA2-\n7B, to identify causal relations between two events\nwithin the sentence. We detail the prompt as fol-\nlows.\n\"Given a sentence: {sentence}, decide if there\nexists a causal relation between {event_1} and\n{event_2} in this sentence. Your answer should\nbe yes or no.\"\nWe also provide two examples from the ESC and\nCTB dataset in Table 7."}, {"title": "A.3 Dataset Description", "content": "In this subsection, we provide detailed descriptions\nfor the three datasets we used in experiments, i.e.,\nESC, ESC*, and CTB.\n\u2022 ESC. This dataset contains 22 topics, 258\ndocuments, and 5334 event mentions. The\nsame as (Gao et al., 2019), we exclude as-\npectual, causative, perception, and reporting\nevent mentions, since most of which were\nnot annotated with any causal relation. Af-\nter the data processing, there are 7805 intra-\nsentence event mention pairs in the corpus,\n1770 (22.67%) of which are annotated with a\ncausal relation. Identical to the data split in\nprevious methods (Hu et al., 2023; Zuo et al.,\n2021b), we select the last two topics in ESC as\ndevelopment set and use the remaining 20 top-\nics for a 5-fold cross-validation. Note that the\ndocuments are sorted according to their topic\nIDs under this data partition setting, which\nmeans that the training and test sets are cross-\ntopic. Due to the distribution gap between\nthe training and test sets, the domain gener-\nalization ability of the model can be better\nevaluated.\n\u2022 ESC*. This dataset is a different partitioning\nof the ESC dataset. More specifically, it ran-\ndomly shuffles the documents before training."}]}