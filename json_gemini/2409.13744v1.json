{"title": "A Simplified Retriever to Improve Accuracy of Phenotype Normalizations by Large Language Models", "authors": ["Daniel B. Hier", "Thanh Son Do", "Tayo Obafemi-Ajayi"], "abstract": "Large language models (LLMs) have shown improved accuracy in phenotype term normalization tasks when augmented with retrievers that suggest candidate normalizations based on term definitions. In this work, we introduce a simplified retriever that enhances LLM accuracy by searching the Human Phenotype Ontology (HPO) for candidate matches using contextual word embeddings from BioBERT without the need for explicit term definitions. Testing this method on terms derived from the clinical synopses of Online Mendelian Inheritance in Man (OMIM\u00ae), we demonstrate that the normalization accuracy of a state-of-the-art LLM increases from a baseline of 62.3% without augmentation to 90.3% with retriever augmentation. This approach is potentially generalizable to other biomedical term normalization tasks and offers an efficient alternative to more complex retrieval methods.", "sections": [{"title": "INTRODUCTION", "content": "Large pre-trained language models (LLM) are increasingly used in healthcare care, showing promise in performing a variety of complex natural language processing (NLP) tasks, such as text summarization, concept recognition, and answer questions [1, 2, 3]. LLMs can identify medical concepts in the text and normalize them to an ontology [4, 5, 6, 7]. However, when LLMs normalize medical terms to a standard ontology such as the human phenotype ontology (HPO), the retrieved code is not always accurate.\nShlyk et al. [8] demonstrated that the accuracy of LLMs in term normalization tasks can be improved with Retrieval-Augmented Entity Linking (REAL). This method generates definitions of HPO terms and target terms needing normalization. The definitions are converted into word embeddings, and cosine similarity is used to identify the three closest candidate terms. The LLM then selects the best normalization from these candidates.\nIn this work, we introduce a simplified but effective retriever that bypasses the need for definition generation. Instead, it matches HPO terms to target terms using BioBERT contextual word embeddings, identifying the closest matches by semantic similarity. By prompting GPT-4o with the 20 closest candidate terms, we enable the model to take advantage of its implicit knowledge of HPO terms and select a semantically equivalent normalization. This approach achieves accuracy comparable to more complex methods without using explicit definitions."}, {"title": "METHODS", "content": ""}, {"title": "Experimental Plan", "content": "The 1,820 phenotypic terms selected from OMIM were used as a test set for normalization. In the first experimental condition, the NLP models (spaCy and BioBERT) normalized the terms by selecting the best-matching HPO term based on the cosine similarity of the embedded word vectors. In the second experimental condition, language models were prompted to normalize a term via the OpenAI API. In the third experimental condition, the prompts for the language models were augmented with up to 50 candidate terms generated based on the cosine similarity between the BioBERT word embeddings and the term to be normalized. For each experimental condition, we calculated accuracy by evaluating the correctness of the retrieved HPO ID."}, {"title": "Data", "content": "Terms to normalize were the signs and symptoms of neurogenetic diseases derived from clinical feature summaries in the OMIM database [35]. We downloaded clinical feature summaries for 236 neurogenetic diseases, consisting of 175,724 tokens via the OMIM API (api.omim.org). GPT-3.5-turbo was used to identify 2,023 terms for normalization (mean 16.5 signs per disease) [18, 36]. A domain expert reviewed these terms, excluding malformed terms (e.g., vague, contradictory, verbose, or ambiguous phrases), resulting in a final set of 1,820 terms for normalization.\nHuman Phenotype Ontology (HPO) was downloaded as a comma-separated value (CSV) file from NCBO BioPortal [37]. A list of 17,957 HPO entry terms was expanded to 30,234 by adding all available synonyms. Each HPO entry term was associated with a corresponding HPO ID, formatted as HP:nnnnnnn, where 'n' is a digit between 0 and 9."}, {"title": "Term Normalization using NLP-based Methods", "content": "SpaCy and BioBERT were evaluated as standalone NLP methods for term normalization.\nspaCy: SpaCy was combined with en_core_web_lg word embeddings. Vectors were generated for each HPO entry term and stored as a Python dictionary. If HPO entry terms were more than one word, the terms were tokenized, and spaCy calculated a mean vector based on the component tokens. During normalization, spaCy generated vectors for each term to normalize and applied the similarity method to identify the nearest HPO entry term and its corresponding HPO ID. The model output retained the HPO entry term with the highest cosine similarity.\nBioBERT: BioBERT implemented contextual word embeddings based on training on large-scale medical corpora, such as PubMed and PMC full-text articles [38]. Each of the 30,234 HPO entry terms and their synonyms were tokenized and converted into input tensors. Tensors were passed through the BioBERT model to obtain hidden state representations, which were mean-pooled, resulting in a single"}, {"title": "Term Normalization using LLM Approach", "content": "Three LLMs were evaluated for term normalization: GPT-40, GPT-3.5-turbo, and GPT-40-mini from OpenAI (San Francisco, CA) via its API\u00b9. Each of the terms to normalize was passed to the API with the following prompt:"}, {"title": "Term Normalization by LLM Enhanced with RAG", "content": "The performance of the LLM for term normalization was enhanced by augmenting the prompt with up to 50 candidate terms from the HPO, selected based on cosine similarity between the BioBERT contextual embedding of the target term (term to normalize) and the terms in the HPO. Each candidate included the HPO term, the HPO ID, and the cosine similarity score. The language models were instructed to return the 'best match', not simply the candidate with the highest cosine similarity."}, {"title": "Assessment of Semantic Equivalence", "content": "We evaluated the semantic equivalence of the normalized terms by comparing the 'best matches' to the original terms to normalize. Among the 1,820 terms, 438 had exact matches in the list of HPO entry terms. Domain experts reviewed the matches, and quantitative assessments based on cosine similarity scores were used to rate semantic equivalence as 'accurate' or 'inaccurate'. Accuracy was calculated as the ratio of accurate matches to total matches."}, {"title": "RESULTS", "content": "Figure 1 shows the model accuracies for the phenotype normalization of the 1820 terms to normalize. To be rated as 'accurate, the term to normalize had to match the HPO entry term and its HPO ID. Although we considered both the HPO Term and the HPO ID returned by each model, nearly all errors were related to retrieving an incorrect HPO ID.\nThe spaCy and BioBERT methods used word embeddings and NLP algorithms to find the best match in a complete table of HPO terms. The spaCy embeddings were general-purpose word embeddings, whereas the BioBERT embeddings were optimized for biomedical terminologies. spaCy averaged the vectors of the component tokens to get a global term vector, whereas BioBERT utilized a transformer architecture and hidden states to generate a global term vector. Both methods used cosine similarities to find the best match for each term to normalize and candidate terms in the HPO. The BioBERT method, at 70.3% accuracy, outperformed the spaCy method at 50.3%.\nThe GPT-40-mini, GPT-3.5-turbo, and GPT-40 models without a retriever had no access to external data sources and relied solely on pre-training to find HPO IDs. Errors made by these language models typically involved the retrieval of incorrect HPO IDs rather than errors in the HPO entry terms. In most cases, when the models made an error, the HPO entry term was correct or nearly correct, but the HPO ID was inaccurate and matched an incorrect concept in the HPO. Among LLMs without a retriever, GPT-40, the largest and most advanced model, performed best with a accuracy of 62.3%, while GPT-40-mini, the smallest model, performed worse with an accuracy of 13.2%.\nThe best-performing methods combined a language model with a retriever. Each model had 20 candidate normalizations for a term to normalize based on the closest embedding similarities. Each language model was prompted to choose the \u2018best match' from the twenty closest candidates. In this scenario, the GPT-40 and GPT-3.5-turbo models outperformed the BioBERT method with 90.3% accuracy. GPT-40- mini was less accurate and performed about the same as the BioBERT model at 70.6% accuracy (Figure 1)."}, {"title": "DISCUSSION", "content": "The results indicate that the most accurate method for phenotype normalization combines a language model with a retriever. GPT-40 and GPT-3.5-turbo, when paired with a retriever, achieved the highest accuracy of 90.3%, demonstrating the benefits of augmenting language models with a retrieval mechanism.\nThe standalone BioBERT, specifically optimized for biomedical text, performed better than spaCy or GPT-40 without retrieval, with an accuracy of 70.3%. This highlights the limitations of a standalone LLM relying solely on pre-training when no external retrieval is available. BioBERT's ability to"}, {"title": "CONFLICT OF INTEREST STATEMENT", "content": "The authors have no conflicts of interest to report."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "Conceptualization by DBH. Methodology and data collection by DBH. Analysis by DBH, TSD, and TO. Writing by DBH, TO, and TSD."}]}