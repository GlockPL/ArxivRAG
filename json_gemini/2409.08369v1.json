{"title": "E-QUARTIC: Energy Efficient Edge Ensemble of Convolutional Neural Networks for Resource-Optimized Learning", "authors": ["Le Zhang", "Onat Gungor", "Flavio Ponzina", "Tajana Rosing"], "abstract": "Ensemble learning is a meta-learning approach that combines the predictions of multiple learners, demonstrating improved accuracy and robustness. Nevertheless, ensembling models like Convolutional Neural Networks (CNNs) result in high memory and computing overhead, preventing their deployment in embedded systems. These devices are usually equipped with small batteries that provide power supply and might include energy-harvesting modules that extract energy from the environment. In this work, we propose E-QUARTIC, a novel Energy Efficient Edge Ensembling framework to build ensembles of CNNs targeting Artificial Intelligence (AI)-based embedded systems. Our design outperforms single-instance CNN baselines and state-of-the-art edge AI solutions, improving accuracy and adapting to varying energy conditions while maintaining similar memory requirements. Then, we leverage the multi-CNN structure of the designed ensemble to implement an energy-aware model selection policy in energy-harvesting AI systems. We show that our solution outperforms the state-of-the-art by reducing system failure rate by up to 40% while ensuring higher average output qualities. Ultimately, we show that the proposed design enables concurrent on-device training and high-quality inference execution at the edge, limiting the performance and energy overheads to less than 0.04%.", "sections": [{"title": "1 INTRODUCTION", "content": "Energy-harvesting edge machine learning (ML) systems aim to deliver real-time and reliable inference results without human intervention [6]. These systems can adapt to new data distributions from sensor inputs through on-device retraining [11]. Due to the practical application potentials, these systems are ideal for deployment in remote or inaccessible locations for tasks such as wildlife conservation [13], smart cities technologies [1], and wearable devices [3]. However, implementing high-accuracy but resource-intensive ML models like convolutional neural networks (CNNs) poses challenges due to limited computational and memory resources [23]. Recent research addresses these challenges through two main aspects: optimizing ML models for resource efficiency and enhancing energy management [8, 9, 11, 15, 18].\nSeveral methods for energy-optimized model design have been proposed [8, 9, 18]. Lightweight neural networks [8] facilitate edge AI, but often result in low accuracy. Selecting CNN instances from a model pool [18] and early-exit neural networks [9] can instead achieve high accuracy, but introduce memory and computational overhead that may prevent their deployment in tinyML systems. Moreover, their architectural design makes them inflexible for on-device training. These issues highlight the need to balance the adaptability of small neural networks with the capability of large models. Ensemble learning is a well-known approach for improving accuracy and robustness which combines the outputs of various \"weak learners\". However, ensemble learning presents challenges in resource-constrained embedded systems, e.g., increased memory demands, and higher energy consumption. To address these concerns, previous research efforts [19, 26] explore resource-optimized ensemble algorithms by balancing the high performance of ensemble models and the practical constraints of limited resources.\nApart from memory-optimized model design, recent works also enhance energy management and scheduling policies, aiming to optimize the runtime and efficiency performance and of these models [8, 9, 11, 18]. Various dynamic scheduling algorithms have been"}, {"title": "2 RELATED WORKS", "content": "Ensemble Learning: Ensemble learning enhances model accuracy and stability by combining multiple weak learners [5]. However, it is often considered a resource- and compute-intense approach, thus limiting its real-life deployment in resource-constrained embedded systems. Recent research focuses on ensembling neural networks. The authors of [19] use random filter pruning [14] to reduce the number of filters in a baseline single-instance CNN model, ultimately obtaining smaller weak learners that together meet memory constraints and improve model robustness against memory faults for low-power embedded systems. This method only guarantees memory equivalence (instead of energy/performance equivalence) and the use of random pruning allows operating on untrained CNN models but may result in unbalanced CNN architectures. An adaptation of the AdaBoost algorithm including transfer learning to reduce computational costs in training is instead proposed in [22]. Yet, these methods reveal certain shortcomings. First, most ensemble methods are impractical for embedded systems, because of their higher resource requirements that prevent their deployment and evaluation on low-power embedded systems like energy-harvesting devices deployed in dynamic environments. Second, the design of hardware-aware ensembles of CNNs as the one proposed in [19] does not include state-of-the-art ensemble learning algorithms like ensemble pruning, but only rely on random pruning approaches that may result in suboptimal output quality.\nEnergy-Efficient Machine Learning: To satisfy the limited battery capacity in edge computing environments, embedded ML systems are designed to balance model complexity and energy consumption. For instance, previous works apply early-exit neural networks with neural architecture search (NAS) to optimize the trade-off between accuracy and energy consumption [9, 17]. However, early exits are essentially built upon a single, large neural network that can hardly be leveraged to improve efficiency in on-device training. Another study [18] selects a neural network model from a pool of models with diverse energy budgets. However, their approach leads to a significant increase in memory usage due to storing multiple neural networks. This scaling issue, where memory usage grows with the granularity of energy management, poses a limitation for microcontrollers.\nH Harvested Energy Management: Energy-harvesting embedded ML systems are equipped with energy harvesters to perform on-device inference and training tasks by utilizing energy extracted from the environments [8, 11]. Frequent energy depletion will lead to the failure of systems, impairing their ability to sense ambient events and deliver reliable inference results. Consequently, energy management policies assume pivotal roles in this context. They can be divided into two categories: Power-Neutral Operations (PNO) [8] and Energy-Neutral Operations (ENO) [9]. PNO devices store minimal energy for immediate use, while ENO devices, with larger energy storage, ensure stable and prolonged performance, ideal for reliable predictions. We focus on the prevalent cases of ENO, utilizing buffered energy in super-capacitors to ensure reliable inference and adaptive applications via scheduling. Recent research applies diverse decision-making algorithms to develop task schedulers for ENO devices. In [18], the authors leverage deep reinforcement learning agents and policy neural networks to select models. However, executing policy neural networks on embedded systems requires substantial amounts of time, memory, and energy. Q-learning is a model-free reinforcement learning approach where an agent learns to select optimal actions in various states based on future reward predictions [25]. In [9], authors use Q-learning agents to select early"}, {"title": "3 E-QUARTIC GENERATION", "content": "We provide an overview of E-QUARTIC design in Figure 1. During the offline design phase, E-QUARTIC uses a baseline single-instance CNN architecture to build a boosting-based energy-adaptive ensembles of CNNs showing (i) high accuracy, (ii) no computational complexity overhead, and (iii) lower memory requirements. In this section, we describe how the E-QUARTIC ensemble-based structure is derived.\nPruning: The computational complexity of CNNs is mainly tied to the number of filters in the convolutional layers. This observation suggests a novel approach to constructing an ensemble of CNN learners that can fit into computational budgets and constrained memory. The computational complexity can be estimated using multiply-accumulate operations (MACs). To derive an ensemble of N CNNs, we employ a pruning-replication approach similar to the one presented in [19], which suggests pruning the target single-instance model by a factor N so that the generated ensemble containing N instance will not incur computational overheads. In our experiments, we observe the best results across different benchmarks when setting 2 \u2264 N \u2264 5. However, in contrast to [19] where random pruning is used to reduce only memory requirements, we also focus on reducing computational complexity (MACs), targeting for more adaptive runtime energy consumption. Besides, we employ a better iterative L2-norm filter-wise pruning method [14] to prune the initial single-instance model to obtain a pruned CNN with $\\frac{1}{N}$ of the original number of MACs. Since the pruning of a single filter significantly reduces the output size, it also reduces the size of filters in the next layer because they will be then applied to lower-dimensional inputs. Therefore, our pruning method not only meets the constraint on MACs but also results in significant memory savings which will be discussed in Section 5. To effectively\nachieve and leverage diversity, we first define a candidate pool of weak learners of size M > N. Each of them is generated starting from a target baseline CNN model that is randomly initialized and trained. The model is then pruned and retrained for multiple iterations by gradually reducing the number of filters until it achieves the desired MACs. This process results in a candidate pool of N weak learners, each of them having $\\frac{1}{N}$ MACs and < $\\frac{1}{N}$ memory requirements of the single-instance baseline model.\nAs an example, we illustrate in Figure 2 the MACs patterns of ResNet-8 [7] and that of a derived ensemble of two learners. The four colors represent the computational complexity of the initial convolutional layer (blue) and three residual blocks (orange, dark green, and light green), with numbers indicating the number of filters. The figure highlights how we create an ensemble (of two learners, in this example) that does not result in computational overhead when compared to the baseline CNN implementation. We refer to the generated E-QUARTIC configuration as a 2-instance ensemble, an ensemble of CNNs of size two (i.e., composed of two instances).\nBoosting: To enhance the overall accuracy of the ensemble by promoting diversity, we enrich the previously described ensemble design with a boosting method to enhance ensemble accuracy by prioritizing learning from previously misclassified samples [22]. TO this end, we use AdaBoost [22] to emphasize learning from the misclassified samples of the previously deployed learners. This is achieved by adjusting the loss contribution for each training sample through the assignment of individual \"sample weight\". Initially, each training data $(x_i, y_i)$ is assigned a sample weight parameter $w_{im}$ with $m$ representing the number of candidate weak learners in the pool. At the beginning of training, when no weak learners are present, all $w_{im}$ are set to 1, such that $\\sum{w_{im}}$ = 1. As each weak learner completes its training and joins the pool, sample weights are updated according to the equation:\n$w_{im} = w_{i(m-1)} \\cdot exp(-\\alpha_y log(f_m(x_i)))$  (1)\nwhere $w_{i(m-1)}$ is the previous sample weight, $\\alpha$ is the learning rate, and $f_m$ is the most recent weak learner added in the pool. Following the update, the weights are normalized to follow a Gaussian distribution. Following Equation 1, sample weight values increase after misclassifications and decrease after correct prediction. This adjustment ensures the previous misclassified samples have larger weighted loss values, and the subsequent learner can effectively learn from these samples. The subsequent weak learner is then initialized, trained, and iteratively pruned using the updated sample weights. This process is repeated until M weak learners are selected."}, {"title": "4 E-QUARTIC ADAPTATION TO DYNAMIC ENERGY ENVIRONMENTS", "content": "Focusing on online executions, we introduce an energy-aware scheduler trained using simulated energy traces. At runtime, energy harvested from ambient sources powers the system. For each collected sample, the proposed E-QUARTIC scheduler dynamically allocates inference and retraining tasks to individual weak learners, considering relevant system information like battery levels and harvested energy. We describe how we leverage the described E-QUARTIC design in energy-harvesting dynamic environments for efficient inference by designing an energy-aware Q-learning-based scheduler. Then, we demonstrate how to utilize it for energy-efficient concurrent inference and training for on-device adaptation.\nEnergy-Aware Inference: We develop an energy-aware Q-learning-based to run inference tasks based on battery levels and harvested energy (Scheduler in Figure 1). The key observation is that we can exploit the ensemble-based architecture of E-QUARTIC by running only a subset of the deployed N learners to save energy. Therefore, for each inference execution, the proposed scheduler determines the number k < N of weak learners to run based on system energy information, thus trading off accuracy for efficiency. To maximize output quality for any subset k of the N deployed learners, learners are sorted by descending accuracy\u00b9 to set their execution sequence.\nTo properly select k, the proposed scheduler uses a Q-learning approach based on the reward function shown in Equation 2 and the state space summarized in Table 1. $E_{now}$ and $E_{last}$ represent the current battery level and the mean battery level in the last 10 inference executions in four discrete states: depleted (less than the amount for executing one weak learner), low (less than 1/2 of the max capacity), high (greater than 1/2 of the max capacity), and full (equal to the max capacity). $P_{harv}$ represents the current harvesting power amount in three discrete states: low, mid, and high. $L$ keeps track of the number of already executed weak learners in the current inference, while $R$ is a binary indicator of an active inference request. The action space of our scheduler is composed of just two possible actions: executing the next learner, if there exists a next one (L < N), or not. When the selected action is to execute the next learner (i.e., a = 1), the system schedules a new learner for execution. In the opposite case (i.e., a = 0), the system aggregates the outputs of the already executed learners to produce the EQUARTIC prediction and then moves to a sleeping mode to reduce energy consumption, waiting for the next inference request. The reward function is defined in Equation 2 and determines the action to be taken (i.e., execute or not a new learner). $A_{acc}$ represents the expected accuracy improvement achieved by executing the next learner (a = 1). This improvement is calculated based on the evaluation dataset during the design phase. An energy term, parameterized by \u03b2, is applied to penalize energy depletion due to an additional learner execution, with a larger penalty incurred as the current battery level $E_{now}$ falls well below the maximum capacity $E_{max}$. Additionally, a penalty term $p_{miss}$ is introduced to discourage missing inference tasks. This occurs when the system fails to fulfill an inference request, meaning an inference request exists (R = 1), but the system does not execute it (a = 0).\nThe proposed scheduling strategy stands out for its lightweight computing requirements and its fine-grained energy management that reassesses the battery level and power state following each learner's execution. By doing so, it allows for immediate and dynamic adjustments to energy consumption and operational efficiency, in response to the fluctuating energy conditions. We will demonstrate in Section 5 how this method significantly contributes to long-term dependability by optimizing energy use.\n$Reward(s, a) = \\begin{cases}  A_{acc} - \\beta (E_{max} - E_{now}) & a = 1\\\\ -p_{miss} & R = 1 and a = 0  \\end{cases}$  (2)\nConcurrent Inference and Training: Dataset shifts in real-world scenarios [12] can diminish the accuracy of a pre-trained ensemble once deployed on a microcontroller. Therefore, the capacity to adapt to new data distributions is crucial for edge intelligent applications in delivering reliable predictions. Nevertheless, previous works on on-device training usually consider single-instance models and overlook the issue of dependability: indeed, single-instance models cannot perform any inference task while they are undergoing a retraining stage, thus reducing system dependability. In this work, we address this challenge by leveraging the proposed E-QUARTIC architecture and by extending the functionalities of the presented task scheduler.\nIn particular, the proposed scheduler can accommodate training stages without affecting inference executions both in the case of high and low energy levels. In both cases, we focus our analysis on the retraining opportunity of one CNN instance at a time. We also consider the retraining stage of fully-connected layers only, hence significantly limiting energy overheads. Finally, we observe that the retraining of one weak learner still requires a forward"}, {"title": "5 EVALUATION", "content": "5.1 Experimental Setup\nHardware and energy-harvesting simulation: We conduct experiments using an STM32L552ZE evaluation board [21], which is equipped with an ARM Cortex-M33 processor, 256KB of SRAM, and 512KB of flash memory, operating within a voltage range of 1.7V to 3.6V. A 0.47F supercapacitor serves as the energy storage. An indoor solar harvesting dataset [20] is employed for energy simulations. It comprises voltage and current traces from solar panels deployed at six indoor locations for two years. We train the Q-learning scheduling agent offline using 40-minute energy trace clips from April 1 to April 15, 2018 considering an indoor location with possible direct sunlight up to 20k lux illuminance during the daytime [20]. We include experimental results considering energy traces during which the capacitor undergoes a complete cycle of charging and discharging. We select the voltage and current traces from the dataset that are boosted by the energy harvester, ensuring a stable charging voltage of 4.2V. The energy-aware Q-learning-based scheduler that controls E-QUARTIC executions is trained offline and is then deployed in the microcontroller for dynamic energy experiments. Two metrics are considered to evaluate the E-QUARTIC performance across the different evaluated designs: Mean accuracy represents the average accuracy of successful inference executions and failure rate measures the ratio of failed executions over the total number of inference events released over a specific time interval.\n5.2 Results\nMemory, Accuracy, and Computational complexity: We compare E-QUARTIC to the considered baselines in terms of memory requirements, accuracy, and computational complexity in an offline energy-static environment in Table 3. The baseline methods"}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed E-QUARTIC, a novel energy-efficient ensembling method for edge AI. It combines pruning, replication, and boosting methods to build ensembles of CNNs that improves accuracy when compared to single-instance baselines and SOTA solutions while dramatically reducing memory requirements. We leverage the E-QUARTIC architecture to propose a new strategy to run inference in constrained embedded systems to adapt to dynamic energy environments. We demonstrate that E-QUARTIC can dynamically schedule inference tasks based on a lightweight analysis of energy levels achieving up to 40% battery lifetime extension for negligible accuracy drops."}]}