{"title": "Large Model Strategic Thinking, Small Model Efficiency: Transferring Theory of Mind in Large Language Models", "authors": ["Nunzio Lore", "Alireza (Sepehr) Ilami", "Babak Heydari"], "abstract": "As the performance of larger, newer Large Language Models continues to improve for strategic Theory of Mind (ToM) tasks, the demand for these state of the art models increases commensurately. However, their deployment is costly both in terms of processing power and time. In this paper, we investigate the feasibility of creating smaller, simulation-ready agents by way of fine-tuning. To do this, we present a large pre-trained model with 20 unique scenarios that combine a social context with a social dilemma, recording its answers, and using them for Q&A fine-tuning on a smaller model of the same family. Our focus is on in-context game-theoretic decision-making, the same domain within which human interaction occurs and that requires both a theory of mind (or a semblance thereof) and an understanding of social dynamics. We find that the fine-tuned smaller language model exhibited significant performance closer to that of its larger relative, and that their improvements extended in areas and contexts beyond the ones provided in the training examples. On average for all games, through fine-tuning, the smaller model showed a %46 improvement in aligning with the behavior of the larger model, with %100 representing complete alignment. This suggests that our pipeline represents an efficient method to transmit some form of theory of mind to smaller models, creating improved and cheaply deployable algorithms in the process. Despite their simplicity and their associated shortcomings and limitations, our findings represent a stepping stone in the pursuit and training of specialized models for strategic and social decision making.", "sections": [{"title": "Introduction", "content": "The concept of Theory of Mind (ToM) refers to the ability to impute mental states such as beliefs, intentions or desires to the self and others [1]. As such, ToM has been extensively studied among humans [2] and other animals [3], but it has recently received a surge of attention with the release of Large Language Models (LLMs) [4, 5, 6, 7]. The latter are sophisticated neural networks based on the Transformers technology, and have displayed impressive capabilities in fields ranging from medicine to finance [8, 9, 10, 11]. Findings are rarely unequivocal, and it is not unheard of that follow-up papers dispute or outright reject the findings of a previous publication [12, 13]. Investigation into the ToM capabilities of these powerful algorithms was naturally prompted by both their excellent performance and their potential for human-AI interaction, however, much like for the rest of LLM research, evidence is still mixed on the subject.\nWhereas the usage of a more modern or larger model does not necessarily translate into better performance for ToM tasks [14], there is consistent evidence that newer models with more parameters are better strategic decision makers [15]. While encouraging, this finding also implies that the rising demand for algorithmic advisors [16, 17] would preferentially be served by larger, more computation-ally demanding and polluting algorithms [18]. Furthermore, it should be stated that within the larger umbrella of ToM tasks, strategic decision making occupies a very delicate niche. Indeed, strategic interaction among humans takes place mostly through natural language [19, 20], the same type of out-put produced by LLMs, who could therefore masquerade as humans if sufficiently sophisticated. The peculiar nature of this subset of tasks thus raises concerns not just about performance, but also about alignment: a very humanlike and highly performing algorithm from a ToM perspective could still be able to act in a very antisocial manner [21].\nOne potential way to address this dual dilemma is to turn to the method of fine-tuning. Indeed, since Large Language Models are generalists by design, their usefulness in specific domains of knowledge is not as pronounced as it could be, and neither is it indicative of said models' general and social intelligence. Through fine-tuning, a model can be partially retrained to improve its performance with respect to a certain dataset or some areas of expertise. This technique is particularly promising for those tasks in which rote memorization is essential for success, as evidenced by findings in medical sciences, and could thus prove to be an avenue to create domain experts \"in silico\" [22, 23, 24]. On the other hand, its usefulness in open-ended tasks that require strategic sophistication is still unclear: in some cases, not even fine-tuning is sufficient to make up for the shortcomings of state-of-the-art LLMs when it comes to the theory of mind reasoning [25]. The problem is further compounded by the absence of datasets that could be used to fine-tune an LLM with the purpose of making it more"}, {"title": "Literature Review", "content": "Within the broad and fast-developing literature surrounding Large Language Models, our research lies at the intersection of two broader subjects: model empowerment through fine-tuning, and model evaluation in the field of strategic reasoning.\nFor what concerns model empowerment, several interesting examples come from just as many subjects [31, 32, 33, 34, 35, 36], with the overall consensus being that fine-tuning can dramatically improve performance in specific areas of knowledge. More interestingly, literature is developing on the process of training smaller models using larger models [37, 38, 39], either via fine-tuning"}, {"title": "Methods", "content": "For the purpose of our analysis, we use models belonging to the LLaMa-2 family. Although they no longer represent the state-of-the-art, LLaMA-2 models have displayed the least erratic behavior during our investigation and have proven to be much easier to fine-tune. We elaborate on our decision not to use LLaMa-3 in Section E of SI. Of the three available configurations, we select the 70 billion parameters and the 7 billion parameters models to act as the reference large and small pre-trained model, respectively. We load all algorithms on Northeastern University's High Performance Cluster (HPC), and we access them via the HuggingFace pipeline. We furthermore make use of the langchain package for Python in order to customize system prompts and inputs. Our experiments are conducted on memoryless algorithms with high temperature, in order to treat each instance and question-answer pair as distinct, and to capture the widest possible range in variation.\nWe distribute a contextual frame of reference upon which to build and elaborate a strategic response to the LLM through the use of system prompts. System prompts serve as an optimal medium for instilling a defined role and personality in LLMs, as they are not processed as typical user inputs but rather function as default settings. This approach is crucial for our objective of grounding LLM decisions within a realistic context. Leveraging system prompts in this manner enhances the ability to align the responses of LLMs with real-world scenarios while not neglecting strategic considerations."}, {"title": "Results", "content": "Figure 1 gives an overview of the methods we employ in order to generate, collect, and then analyze our results."}, {"title": "Within Sample Results", "content": "Upon inspection of the results shown in Figure 3, it appears immediately evident that the smaller model becomes closer in behavior to the larger model after fine-tuning. This is, however, not universally the case: the fine-tuned model becomes less cooperative in the \"friendsharing\" scenario when playing Prisoner's Dilemma and Snowdrift, below both the pre-trained and the fine-tuned model; additionally, when playing Prisoner's Delight under the \"team\" context, we observe a decrease in cooperation that exceeds the rate of defection of the larger model. Moreover, we notice aberrant behavior when the fine-tuned LLaMa-7b plays the Prisoner's Dilemma under the \"team\" context. In these scenarios, the fine-tuned algorithm exacerbates the behavior of its original, pre-trained counterpart and increases its rate of defection. It is worth pointing out that in two of the four irregular cases observed, the change in behavior of the fine-tuned model vis-a-vis the pre-trained one is in the direction of the larger model, or, in other words, the model fine-tuned to the instructions coming from a larger model follows the general trend set by said larger model, even when this results in a puzzling overcorrection."}, {"title": "", "content": "In order to properly quantify the extent to which the fine-tuned small model approximates the behavior choices of its larger, pre-trained counterpart, we define \"improvement\" in the following terms:\n$I := 1 - \\frac{70b - 7b_{finetuned}}{70b - 7b_{org}} \\times 100$\nwhere $7b_{org}$ refers to the count of cooperation choices in the original version of Llama2-7b model, $7b_{finetuned}$ refers to the same quantity for the fine-tuned version of the same model, and 70b captures the count of cooperative choices for the large Llama2-70b language model. We measure improvement on a normalized scale that goes from 0 to 1, with 1 representing perfect overlap between the behavior"}, {"title": "Out of Sample Results: Contexts", "content": "Our results for the out-of-sample contexts paint an intricate and complex picture of the effects of fine-tuning. It is immediately clear by way of visual inspection of figure 5 that results are more \"well-behaved\" than the results for the in-sample contexts, with no trace of overcorrection nor exacerbation. The stark tendency to defect of the larger model when facing the \"sports\" context is, at a glance, arguably the most surprising result. The model's consistency in defecting all the time, except when playing Prisoner's Delight, suggests a hostile understanding of the social frame leading to adversarial behavior. Nevertheless, in line with previous results, LLaMa-70b does shift its behavior when a non-cooperative context is paired with an anti-dilemma.\nDespite these promising results, Figure 6 shows average improvement is much lower than in the within-sample case. Remarkably, the highest level of improvement is reached when the game is Prisoner's Delight and the context is \"roomsharing\". This is possibly because LLaMa2 takes cues from both game structure and context, and in this scenario, the two of them work synergistically"}, {"title": "Out of Sample Results: Game and Context", "content": "For our final robustness test, we analyze the behavior of the models under consideration in a completely different game structure, with a tailor-made context to accompany it. Experiments conducted on games with different payoffs under within-sample contexts can be found in SI C. In this section, we instead endeavor to understand if the fine-tuned model could generalize its learned behavior beyond the boundaries of two-player games, demonstrating a clearer understanding of social dilemmas and strategic interaction. To accomplish this, we use the Public Good Game (PGG), or the Prisoner's Dilemma with n-players, in order to once again capture the tension between Nash equilibrium and Pareto optimality. Additionally, this allows us to account for scenarios in which choice is not binary, but rather continuous.\nResults are presented in Table 1. It is immediately evident that LLaMa2-70b exhibits more pro-social behavior and a greater willingness to donate, whereas the 7b pre-trained model displays comparatively selfish tendencies. Notably, the fine-tuned model demonstrates significantly higher levels of pro-social behavior than its pre-trained counterpart, a difference that is statistically significant across all conventional thresholds. This finding suggests that, despite the absence of specific training scenarios involving the game or context, the fine-tuned model retains an enhanced understanding of cooperation, resulting in increased donation levels. Although higher donations may not align with more sophisticated strategic thinking, the friendly interaction environment suggests that the elevated pro-sociality is best attributed to the context-aware intelligence inherent in LLaMa2-70b, a characteristic which has been effectively transferred to the fine-tuned model.\nAdditional visualizations for all our out-of-sample studies can be found in SI D.II."}, {"title": "Conclusion", "content": "Throughout the course of this research paper, we have investigated the effectiveness of generating datasets from large pre-trained LLMs for the purpose of instructing smaller models belonging to the same family in tasks that involve theory of mind and strategic thinking. Smaller, strategy-savvy,"}]}