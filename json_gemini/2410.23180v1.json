{"title": "ReasoningRec: Bridging Personalized Recommendations and Human-Interpretable Explanations through LLM Reasoning", "authors": ["Millennium Bismay", "Xiangjue Dong", "James Caverlee"], "abstract": "This paper presents ReasoningRec, a reasoning-based recommendation framework that leverages Large Language Models (LLMs) to bridge the gap between recommendations and human-interpretable explanations. In contrast to conventional recommendation systems that rely on implicit user-item interactions, ReasoningRec employs LLMs to model users and items, focusing on preferences, aversions, and explanatory reasoning. The framework utilizes a larger LLM to generate synthetic explanations for user preferences, subsequently used to fine-tune a smaller LLM for enhanced recommendation accuracy and human-interpretable explanation. Our experimental study investigates the impact of reasoning and contextual information on personalized recommendations, revealing that the quality of contextual and personalized data significantly influences the LLM's capacity to generate plausible explanations. Empirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art methods by up to 12.5% in recommendation prediction while concurrently providing human-intelligible explanations. The code is available here.", "sections": [{"title": "1 Introduction", "content": "In our day-to-day life, many of our experiences are driven by word-of-mouth recommendations. For example, a friend may suggest a good book to read, knowing my preference for historical fiction and thriller novels. A work colleague may introduce me to a new software library based on our shared work history. These recommendations often focus on likes and dislikes of a user and provide explanatory reasoning for why a user may like a target item as in this example:\nYou prefer movies which are heartwarming, inspiring, themes of love, family, and personal growth ... but dislike movies that rely on predictable gags and tired humor... You might like 'My Dog Skip' as it is a heartwarming coming-of-age film about a young boy and his loyal dog, filled with friendship, adventure, and personal growth ...\nIn practice, many modern recommendation systems aim to scale recommendations based on the actions of millions of users, e.g., (Kang and McAuley, 2018; Tang and Wang, 2018; Koren et al., 2009; He et al., 2017; Yue et al., 2024; Sun et al., 2019). These approaches, however, are typically not driven by such explicit human-interpretable explanatory reasoning, since such detailed reasons are rarely provided by users and challenging to infer from online system traces. Instead, these systems model implicit user-item interaction signals (e.g., from clicks or likes) toward predicting the next item a user may like.\nWith the rise of Large Language Models (LLMs), we are motivated to revisit the core word-of-mouth reasoning assumption that inspired much of the early work in recommendation systems, e.g., (Shardanand and Maes, 1995), but that has been mostly missing from today's large scale recommender systems. That is, can we leverage the reasoning capabilities of LLMs to bridge the gap between recommendations and the human-intelligible, interpretable reasoning behind them? Concretely, we propose a reasoning-based recommendation framework called ReasoningRec. The core idea is to model users and items through LLM-powered generation (corresponding to the likes and dislikes above), and then use a large LLM to generate synthetic explanations for why a user may like an item (corresponding to the explanatory reasoning); these explanations are then used to fine-tune a smaller LLM (SLM) toward generating more accurate recommendations than methods that ignore such powerful reasoning chains. A side effect of the model is"}, {"title": "2 Related Work", "content": "LLMs for Recommender Systems. Recent advancements in LLMs have introduced transformative capabilities across a broad range of applications, including recommender systems. Influenced by the In-Context Learning (ICL) abilities (Dong et al., 2022) of LLMs and their capacity to follow Chain of Thought (CoT) prompting (Wei et al., 2024), initial work explored task-specific prompting paradigms to tailor LLMs for downstream tasks such as re-ranking (Ferraretto et al., 2023) and conversational recommender systems (Wang et al., 2022b). Some methods have further investigated the zero-shot capability of LLMs to understand user preferences (Kang et al., 2023). More recently, research has shifted towards cost and compute-effective instruction fine-tuning methods of Small Language Models (SLMs) for predicting binary recommendation ratings, as seen in TALL-Rec (Bao et al., 2023), PALR (Yang et al., 2023), and ReLLa/ReiT (Lin et al., 2024). This fine-tuning process involves training pre-trained models using task-specific recommendation datasets, which include user-item interactions (e.g., purchases, ratings, clicks). These finetuned SLMs demonstrate improved performance compared to LLMs relying solely on zero-shot prompting.\nExplanations in Recommendation. Prior work has aimed to explain or justify what items are suggested by a recommender system, e.g., (Ni et al., 2019; Dong et al., 2017; Li et al., 2019; Ni et al., 2017). These methods typically generate explanations post-hoc, and are not incorporated for training new recommendation models. In this work, we propose to generate synthetic explanations based on user-item interactions that can then be used to fine-tune a more effective recommender.\nLLM Reasoning. Recent studies have suggested that LLMs with parameters beyond a certain scale exhibit emergent reasoning abilities (Wei et al., 2022; Cobbe et al., 2021). When provided with examples of \"chain of thought\u201d (CoT), which represent intermediate natural language reasoning steps, these models can generate explicit rationales similar to eliciting inductive/abductive reasoning (Wei et al., 2024; Huang and Chang, 2023). Advances such as zero-shot CoT (Kojima et al., 2024), where a model is prompted with phrases like \u201cLet's think step by step,\" allow for reasoning without the need for explicit few-shot examples. Building on this, multi-step reasoning approaches \u2013 such as Successive Prompting (Dua et al., 2022), Tree-of-Thought (Yao et al., 2023a), Graph-of-Thought (Besta et al., 2024; Yao et al., 2023b), Iterating CoT (Wang et al., 2022a), and Self-Consistency (Wang et al., 2022c) - have become critical strategies for enhancing reasoning in various downstream tasks. In the recommendation domain, Logic-Scaffolding (Rahdari et al., 2024) provides a preliminary CoT framework for generating reasoning. Additionally, there has been an early exploration into knowledge adaptation using multiple experts in cross-domain knowledge transfer (Xi et al., 2023). Despite these advancements, reasoning-based approaches for rec-"}, {"title": "3 ReasoningRec", "content": "In this section, we introduce the framework of ReasoningRec. We begin with preliminaries, then introduce how we model items and user profiles, before creating reasoning-based explanations that guide our model fine-tuning.\n3.1 Preliminaries\nWe define the recommendation task as a binary classification problem using multi-field categorical data. Given a dataset D = {(Xn, Yn)n=1}, where $X_n$ represents the user features (e.g., User ID, gender, age) and item features (e.g., Item ID, title, brand) for the n-th instance, and $y_n$ denotes the corresponding binary label (with $Y_n$ = 1 indicating a \"like\" and $Y_n$ = 0 indicating a \"dislike\"), the goal of the recommendation system is to learn a function F(\u00b7) that accurately predicts the probability of a \u201clike\u201d for each sample $X_n$, i.e., $P(y_n = 1|X_n)$. The predicted probability $\\hat{y}_n$ is given by $\\hat{y}_n = F(X_n;\\theta)$, where \u03b8 represents the model parameters.\nLet U = {$U_1, U_2, ..., u_{|u|}$|} denote the set of"}, {"title": "3.2 Item Description Generation", "content": "We aim to model user preferences based on past interactions with items. Hence, it is crucial to create human-intelligible item descriptions with rich semantic information to enhance these models of user preference. Formally, we define the item description $D_i$ for an item i as:\n$D_i = LM(M_i)$,\nwhere i \u2208 I, and $M_i$ is the metadata corresponding to that item, consisting of details such as \u2013 title, genre, brand, price, and reviews. For each item i, we curate a list of at most p user reviews. We use"}, {"title": "3.3 User Profile Creation", "content": "Sequential recommendation tasks typically consider the most recent k items to predict whether the user will like or dislike the next item. The idea behind this approach is to capture temporal user behavior from the most recent interactions. However, this process overlooks some of the user's broader preferences which might have been exhibited prior to the most recent k items. To address this, we create a user profile $P_u$, a concise q-word profile summary that captures the initial behavior and preference of a user u \u2208 U. To create this user profile, we select at most the first m user interacted items such that the set of items does not overlap with the most recent k items. This ensures that the user profile reflects diverse preferences from earlier interactions. For each of the m items, we use $r_{u,i}$ to assess whether the user liked the item or not. We then prompt an LLM, $LM_{\\theta}$, with the $r_{u,i}$ and corresponding item description, $D_i$, following the Chain-of-Thought (CoT) (Wei et al., 2024) prompts as shown in Appendix C Table 9 to generate a 100-word user profile, $P_u$, summarizing the user preferences. Formally,\n$P_u = LM((s_i)_{i=1}^{m})$,\nwhere $s_i$ =< $r_{u,i}, D_i$ >, represents the user-item interactions for the user u with item i."}, {"title": "3.4 Explanatory Reasoning Generation", "content": "In this section, our goal is to create explicit human-interpretable explanatory reasoning to guide our recommendation framework. Recall that most user-item interactions are implicit and so explicit reasoning about why a user prefers a particular item is typically unavailable. Even for the rare cases where a user may explicitly indicate a preference (as in the case of a written review), patterns connecting sequential interactions are often missing.\nWe employ a well-curated zero-shot CoT prompting method to generate synthetic explanatory reasoning for a user's implicit preference towards the target item, by conditioning the generation process on the user's implicit feedback for the target item. This process is crucial to the quality of human-intelligible reasoning generation. For a user u and target item it, we utilize the User Profile $P_u$, recent k user-item interactions for the same user u, $(s_i)_{i=t-k-1}^{t-1}$ =< $r_{u,i}, D_i$ >=, and ground truth rating $r_{u,i_t}$ which indicates whether the user originally liked or disliked the target item. This information is used to generate reasoning $R_{u,i_t}$ which explains the user's behavior about what features are most likely to influence the user's decision, given the ground truth rating $r_{u,i_t}$. We formally define this process as:\n$R_{u,i_t} = LM(P_u, (s_i)_{i=t-k-1}^{t-1} | r_{u,i_t})$.\nNote that such explanatory reasoning is based on previous user-item interactions (where we know the ground truth of whether the user likes an item or not). It is not typically available in practice since the model must predict whether a user will like an item or not. Hence, we next demonstrate how to fine-tune a smaller LLM over these synthetic reasoning chains toward generating more accurate recommendations."}, {"title": "3.5 Finetuning with Personalized Reasoning", "content": "In this section, we demonstrate our framework, ReasoningRec, for instruction finetuning the SLM, Llama-2-7b-chat-hf, denoted as $L_L$. Let \u0398 be the pre-trained parameters of $L_L$. Our approach is based on the conditional language model objective. However, finetuning an LLM can be both computationally intensive and highly time-consuming, particularly for long sequences. To address this, we adopt LoRA (Low-Rank Adaptation) (Hu et al.,"}, {"title": "4 Experimental Setup", "content": "In this section, we introduce the datasets, baseline methods, evaluation metrics, and implementation details used in our experiments.\n4.1 Datasets\nGiven that real-world datasets frequently exhibit different levels of sparsity, we aim to evaluate the performance of our methods across datasets with varying degrees of sparsity.\nML1M (Harper and Konstan, 2015) is a widely used benchmark dataset about movies with 1 million user-item interactions."}, {"title": "4.2 Baselines and Evaluation Metrics", "content": "We consider two representative sequential recommenders - SASRec (Kang and McAuley, 2018) and Caser (Tang and Wang, 2018). These are pre-LLM recommenders that are trained over historical user-item interactions. SASRec is a strong attention-based sequential recommender and Caser is a CNN-based sequential recommender.\nTALLRec (Bao et al., 2023) is an LLM-based recommender that uses an instruction finetuning method with K\u2208 {64, 128, 256} training samples to finetune an SLM, Llama-2-7b-chat-hf for recommendation prediction.\nRec-SAVER (Tsai et al., 2024) is a contemporaneous approach that addresses the simultaneous tasks of rating prediction and reasoning generation by finetuning a Flan-T5 XL model. Specific details are mentioned in Appendix A.1.\nWe also consider two zero-shot methods for comparison - Zero-shot Vanilla and Zero-shot ReasoningRec. Zero-shot Vanilla is prompted with only item titles for the recommendation prediction task. Zero-shot ReasoningRec is prompted with all contextual and semantic information, including item descriptions and user profiles, to generate both recommendation predictions and corresponding reasoning. We follow the same prompt template shown in Table 1 and use Llama-2-7b-chat-hf and Mixtral 8x7b Instruct v0.1 as two backbone models for these two baselines.\nOur study focuses on both recommendation prediction and the quality of reasoning generation. We employ Binary AUC (Bradley, 1997) for evaluating the binary recommendation prediction task. We use BERTScore ($Zhang^*$ et al., 2020), which uses contextual embeddings to capture the semantic similarity between the generated reasoning and the ground truth reasoning to access the quality of reasoning generation. Specific details are mentioned in Appendix A.3"}, {"title": "4.3 Implementation Details", "content": "We binarize the ratings using the threshold rt = 3 for our experiments, following TALLRec (Bao et al., 2023) paper. As mentioned in Section 3.2, for item descriptions, we use at most p = 10 reviews sampled in a stratified manner and generate a n = 25 word description for every item. For user profile, we use at most m = 15 initial items to generate a concise q = 100 word profile summary, capturing the user preference and behavior from their initial interactions, as mentioned in section 3.3. For all the generation tasks, we use Mixtral-8x7b-Instruct-v0.1 as the LLM.\nFor the proposed method, ReasoningRec, we instruction finetune an SLM, Llama2-7b-chat-hf, with limited number of training samples, K\u2208 {64, 128, 256}, following TALLRec. We conduct all experiments on NVIDIA RTX A5000 24GB GPUs. Additional implementation details for our methods, models, and finetuning are explained in Appendix A.4"}, {"title": "5 Results and Analysis", "content": "5.1 ReasoningRec outperforms all baselines\nTable 2 compares the performance of the proposed method, ReasoningRec, with baselines like SAS-Rec (Kang and McAuley, 2018), Caser (Tang and Wang, 2018), and TALLRec (Bao et al., 2023). We report the results from the contemporaneous Rec-SAVER XL paper (Tsai et al., 2024) for the Beauty dataset, based on a finetuned Flan-T5 XL model.\nWe see the highest performance gain on the ML1M dataset (12.59%), as compared to 4.38% on Fashion and 7.55% on Beauty, over TALLRec performance. The proposed ReasoningRec framework outperforms all other tested methods including traditional methods like SASRec, Caser, and finetuned SLM methods like TALLRec in recommendation prediction capability while providing human intelligible and interpretable reasoning validating the prediction. The finetuned ReasoningRec outperforms a contemporary work, Rec-SAVER XL, which leverages a highly compute-intensive training process by generating multiple reasoning outputs and selecting the best one for finetuning a Flan-T5 XL model, through self-verification using another prompt to the same LLM. It is also finetuned with a considerably higher number of training examples as compared to ReasoningRec. Improved performance of ReasoningRec could be attributed to the use of rich semantic information"}, {"title": "5.2 ReasoningRec compared to Zero-shot larger LLMs", "content": "We investigate the recommendation prediction and reasoning ability of an LLM (Mixtral 8x7b Instruct v0.1), in a zero-shot setting \u2013 not finetuned on any training sample, i.e. K = 0. LLMs are pre-trained on a huge corpus of data with a very large number of parameters, which help them to perform well across various tasks in zero-shot settings using ICL abilities (Dong et al., 2022). We use the CoT prompt\u00b2 as depicted in Table 1, which is used for the ReasoningRec framework and evaluated the results as shown in Table 2. We demonstrate strong results for zero-shot ReasoningRec, outperforming traditional SoTA methods like SASRec and Caser and performing comparably to finetuned models with K = 64 training samples.\nWe also investigate the zero-shot capabilities of SLM (Llama2-7b-hf-chat). We observe that SLMs perform poorly with recommendation tasks in a zero-shot setting. Zero-shot ReasoningRec"}, {"title": "5.3 Ablation study for ReasoningRec", "content": "Table 3 demonstrates the importance of features like Item Description and User Profile on the recommendation prediction and reasoning generation capability. We demonstrate that item description plays a significant role in improving the quality of reasoning generation as ReasoningRec w/o Profile consistently outperforms ReasoningRec w/o Description. Specifically, in extremely sparse datasets like Beauty, item description plays a crucial role due to additional contextual information improving both reasoning and recommendation predictions. We establish that user profile is crucial for long interaction sequences such as in ML1M"}, {"title": "5.4 Ablation study for Zeroshot ReasoningRec", "content": "We investigated the impact of key factors like Item Description and User Profile on the recommendation prediction and reasoning generation performance for zero-shot ReasoningRec. Table 4 presents the ablation study of zero-shot methods. An initial analysis demonstrates that reasoning on"}, {"title": "6 Conclusion", "content": "In this work, we studied the importance of rich contextual and semantic information about users and items in generating human-interpretable explanations. We introduced ReasoningRec, a lightweight and efficient framework, to instruction finetune an SLM on synthetic reasoning data generated by an LLM, conditioned on ground truth user ratings. We observed that our proposed method outperformed all the baseline methods in the recommendation prediction task while bridging the gap between recommendation prediction and human-interpretable explanations. Evaluations over different methods showed that item descriptions are crucial in improving performance for sparse datasets while user profiles are crucial for long user-item interaction sequences. Our experiments also illustrated how the proposed method helped in improving both recommendation prediction and reasoning generation in an SLM. Finally, we demonstrated a strong positive correlation between recommendation prediction and the quality of generated reasoning, underscoring the importance of reasoning and explanations in providing accurate recommendations."}, {"title": "Limitations and Future Work", "content": "This paper proposes to generate synthetic explanations as a guide for LLM recommendation fine-tuning. However, how plausible the reasoning might be, the explanations generated are an approximation of the actual reasoning justifying the user's action. Hence there is a need for follow-on work to more rigorously explore the failure cases of synthetic explanations and their impact. There is also a need for further study of the interpretability of the LLM-generated explanations; do they reflect actual user motivations?\nWe have focused on one aspect of recommendation - prediction of likes or dislikes while producing plausible and human-interpretable explanations. It is an open question how the method presented here could impact other recommendation tasks like candidate retrieval and re-ranking."}, {"title": "A Experimental Setup", "content": "A.1 Dataset\nIn this study, we evaluate our methods on three recommendation datasets. The datasets vary significantly in domains, platforms, implicitness and sparsity as shown in Table 5:\nMovieLens A widely used benchmark dataset. We used the version with 1 million user-item interactions, called MLIM. This is a highly dense dataset. The item metadata, $M_i$, is, however, limited to Title, Genre, and Year (of release) only and doesn't include any user review, making it highly implicit.\nAmazon A series of datasets comprising large corpora of product reviews crawled from Amazon.com. Top-level product categories on Amazon are treated as separate datasets. We consider two categories 'Beauty' and 'Fashion.' This dataset is notable for its high sparsity and variability. $M_i$ consists of title, price, brand, description (only 8% of items), and user reviews."}, {"title": "A.2 Baselines", "content": "To demonstrate the effectiveness of our methods, we include three different baselines:\nSASRec is an attention-based sequential recommender that employs a multi-headed self-attention architecture, similar to the Transformer decoder, to capture user preferences. In the final layer of the network, we use the Sigmoid activation function instead of Softmax, as we are performing binary classification.\nCaser is a CNN-based sequential recommender that embeds a sequence of recent items as an image and learns sequential patterns using horizontal and vertical convolution filters. Similar to SASRec, in the final layer, we use the Sigmoid activation function instead of Softmax, due to the binary classification task.\nTALLRec proposes an efficient tuning framework to align LLMs with recommendation tasks. Initially, they instruction-tune LLaMA-7B using the Alpaca dataset, followed by additional instruction finetuning with a recommendation dataset. In our study, we use Llama-2-7b-chat-hf\u2074, denoted as $L_L$, which is already instruction-tuned. Thus,"}, {"title": "A.3 Evaluation Metrics", "content": "Our study focuses on both recommendation prediction and the quality of reasoning generation. For recommendation prediction, we adopt a binary classification approach, where the LLM, $L$, predicts the binary rating $f_{u,i_t}$ \u2208 {1,0}, with 1 corresponding to a \"like\" and 0 to a \"dislike\". The LLM outputs a sequence in the form of Prediction: Yes/No, where 'Yes' indicates $f_{u,i_t}$ = 1 and 'No' indicates $\\hat{f}_{u,i_t}$ = 0. To evaluate this binary classification, we employ Binary AUC.\nFor the reasoning generation task, we use the model $LM_{\\theta}$ to generate ground truth reasoning $R_{u, i_t}$, as described in Section 3.5.1. To compare the performance of our methods in both Zero-shot and Finetuned ReasoningRec settings, we evaluate the generated reasoning $R_{u, i_t}$ against the ground truth reasoning $R_{u, i_t}$. We use BERTScore, which calculates token-level maximum semantic similarity, making it the most comprehensive metric for this evaluation. Specifically, we use the 'deberta-xlarge-mnli' model as the BERTScore backbone."}, {"title": "A.4 Implementation details.", "content": "We use rt = 3 for all of our datasets such that we convert the original user rating $r_{u,i}$ to binary rating $r_{u,i}$ for recommendation tasks for this study.\nWe use $LM_{\\theta}$ for multiple generation tasks. The hyper-parameters used for $LM_{\\theta}$ is listed in Table 6.\nThe temperature hyper-parameter controls the degree of randomness in the model's output generation. We used a very low temperature throughout our experiments to ensure consistent results.\nThe top-p hyper-parameter regulates the range of tokens the model considers when generating the next token. For the reasoning generation task, we selected a lower value to minimize the chances of off-topic or illogical outputs, which is crucial for instruction finetuning our SLM. This also helps to enhance the coherence and consistency of the generated reasoning.\nThe max_new_tokens hyper-parameter limits the number of tokens the model can generate. As discussed in Section 3, the maximum number of tokens generated is in accordance with the instructions provided to the LLM."}, {"title": "A.4.1 Item Description Generation", "content": "Section 3.2 introduced that we generate a concise $\\n$-word item description, $D_i$ for every item i. For our experiments we have used n = 25. Given, the varied level of implicitness, we describe the process of $D_i$ generation. The ML1M dataset, being highly implicit, provides very limited item metadata which contains only the year and genre of each movie. To enrich this information, we utilize the Cinemagoer Python library to retrieve the plot for each movie. We then prompt, Mixtral-8X7b-Instruct-v0.1\u2075, $LM_{\\theta}$ as shown in Appendix C Table 8 to generate a 25-word summary for each movie.\nFor the Beauty and Fashion datasets, we observed that the item titles contain some basic information about the products such as title, brand, price, description and reviews. However, only 8% of the items in these datasets include a description in the metadata, $M_i$. To address this, for each item i, we leverage the ratings and reviews provided by the users, < $r_{u,i}$ $Ku,i$ > \u2200u \u2208 U and $s_u$\u2208 $S_u$, to generate a more comprehensive item description. With the aim of capturing both strong (positive) and weak (negative) features of each item i, we sample up to p = 10 reviews from the available pool of reviews. These sampled reviews are selected in a stratified manner to have the original distribution across all rating levels for the item, ensuring original rating representation. The pseudo code for selecting reviews is demonstrated in Algorithm 1. We then prompt $LM_{\\theta}$, with the selected reviews following the format shown in Table 8 and Equa-"}, {"title": "A.4.2 Instruction Finetuning", "content": "It is the most crucial task for the purpose of getting the best finetuned model. Given the LLMs are pre-trained on huge amount of corpus, even few training examples can improve their performance considerably. We adopted a textitK-shot training paradigm, similar to TALLRec, where only a limited number of samples, K \u2208 {64, 128, 256}, are randomly selected from the training dataset for instruction finetuning. We used Llama-2-7b-chat-hf ($L_L$) as the base model for finetuning, both for TALLRec and our ReasoningRec.\nFor TALLRec, we used the parameters used in the original implementation to generate the results - maximum numbers of epochs = 100, learning rate = 1e \u2013 4, LORA r = 8, alpha = 16, dropout = 0.05, and target modules = [\"q_proj\", \"v_proj\"].\nFor finetuning ReasoningRec, we tuned hyperparameters from the following range - learning rate: [1e-3, 3e \u2013 4, 1e \u2013 4, 4\u0435 \u2013 5, 5e \u2013 6], LoRA r: [4, 8] and alpha: [8, 16]. The hyperparameters used for the best model are - max epochs = 100, LORA r = 8, alpha = 16, dropout = 0.05, target modules = [\"q_proj\", \"v_proj\"], and learning rate = 1e - 4 for Beauty and Fashion, and 3e \u2013 5 for ML1M. We also used the SFTTrainer with DataCollatorFor-"}, {"title": "B.1 Template Ablation", "content": "Table 7 demonstrates the consistency of our prompt for zero-shot ReasoningRec. We generated multiple prompts by tweaking instructions and validated that the models perform almost similar given they are provided with rich semantic and contextual information such as Item Description and User Profile. We used the Mixtral-8x7b-Instruct-v0.1 for all these experiments."}]}