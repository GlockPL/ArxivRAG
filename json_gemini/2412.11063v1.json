{"title": "LAW: Legal Agentic Workflows for Custody and Fund Services Contracts", "authors": ["William Watson", "Nicole Cho", "Nishan Srishankar", "Zhen Zeng", "Lucas Cecchi", "Daniel Scott", "Suchetha Siddagangappa", "Rachneet Kaur", "Tucker Balch", "Manuela Veloso"], "abstract": "Legal contracts in the custody and fund services domain govern critical aspects such as key provider responsibilities, fee schedules, and indemnification rights. However, it is challenging for an off-the-shelf Large Language Model (LLM) to ingest these contracts due to the lengthy unstructured streams of text, limited LLM context windows, and complex legal jargon. To address these challenges, we introduce LAW (Legal Agentic Workflows for Custody and Fund Services Contracts). LAW features a modular design that responds to user queries by orchestrating a suite of domain-specific tools and text agents. Our experiments demonstrate that LAW, by integrating multiple specialized agents and tools, significantly outperforms the baseline. LAW excels particularly in complex tasks such as calculating a contract's termination date, surpassing the baseline by 92.9% points. Furthermore, LAW offers a cost-effective alternative to traditional fine-tuned legal LLMS by leveraging reusable, domain-specific tools.", "sections": [{"title": "Introduction", "content": "While the advancement of Large Language Models (LLMs) demonstrates great potential for a myriad of use-cases in Document AI and Natural Language Processing (NLP) (Minaee et al., 2024), the domain of legal contracts poses unique challenges. The necessity for models to comprehend long, multi-document context windows and dense legal jargon engenders the intellectual pursuit to construct a legal domain-specific LLM. Certain studies have empirically investigated this motivation such as comparing the zero-shot performance of general-purpose LLMs on legal texts (Jayakumar et al., 2023) or fine-tuning LLMs under the Federated-Learning setting (Yue et al., 2024). Similarly, Colombo et al. (2024) trained SaulLM-7B on an English legal corpus, leveraging the Mistral-7B architecture (Jiang et al., 2023). While these developments are promising, legal contracts are highly varied not only in terms of semantics but also accessibility. Therefore, compared to the computational cost, the usage of a fine-tuned legal LLM can be very limited in practice (Figure 1). Thus, we propose LAW, a legal agentic workflow framework, that uses a code generation agent to orchestrate reusable tools, that can be leveraged for a variety of different contracts. Moreover, our framework can be generalized across different types of queries. Instead of relying solely on a fine-tuned LLM to solve highly complex tasks, LAW leverages a suite of specialized legal domain-specific tools, and a robust orchestration framework built on top of the FlowMind framework proposed by Zeng et al. (2023). LAW's reusable tools are designed to tackle distinct tasks such as contract retrieval. Our tools are rigorously guardrailed through unit tests that map their failure modes, ensuring a comprehensive understanding of their operational limits. By utilizing this method, LAW focuses on selectively applying the appropriate tools and text agents for a task, thereby optimizing the problem-solving process and delivering accurate and reliable responses.\nContributions We empirically prove the optimized performance of LAW for complex legal tasks. Overall, our contributions are three-fold:\n\u25ba We propose LAW, a novel approach to interacting with financial-legal contracts, utilizing reusable legal domain-specific tools and text agents that addresses practical constraints - specifically legal dataset accessibility, scalability, and cost. Our system that can allow both lay-people, and domain experts to query information from complicated legal documents.\n\u25ba LAW significantly outperforms the baseline, achieving up to 92.9% accuracy gains across a range of queries, from direct retrieval to multi-hop reasoning.\n\u25ba LAW is the first legal agentic workflow system encompassing 23 years of regulatory contracts for the entire scope of public funds pursuant to the Investment Company Act of 1940. LAW can perform retrieval and analytical tasks that require an understanding over multiple documents, and each document contains many pages."}, {"title": "Related Works", "content": "LLMs in NLP LLMs such as Llama 2 (Touvron et al., 2023), PaLM (Chowdhery et al., 2023), GPT-3 (Brown et al., 2020), GPT-4 (Achiam et al., 2023), and Vicuna-13B (Chiang et al., 2023), have revolutionized NLP in many aspects. Their capabilities provide a foundation for more specialized adaptations, such as InstructGPT (Ouyang et al., 2022), which demonstrates how fine-tuning GPT models with human feedback can significantly enhance their alignment with user intent. Despite their impressive capabilities, LLMs face challenges like hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Siriward-hana et al., 2023; Lin et al., 2023; Gao et al., 2023b) has emerged as a promising solution by effectively merging LLMs' intrinsic knowledge with external databases, enhancing both accuracy and reliability of generated content for knowledge-intensive tasks.\nDomain-Specific Tools FlowMind (Zeng et al., 2023) introduces a generic prompt recipe that employs reliable Application Programming Interfaces (APIs) to ground LLM reasoning through the usage of tools. Additionally, HiddenTables (Watson et al., 2023) constructed an agentic system designed to enhance interactions with tabular data. Chen et al. (2023) and Gao et al. (2023a) explored how models can generate not only coherent text but also executable code snippets based on user queries. ToolFormer (Schick et al., 2023) and REACT (Yao et al., 2023), which are designed to enhance the model's interaction with external databases and software, helped LLMs access a wider range of resources, improving their ability to answer queries that required specialized knowledge. Watson and Liu (2021) demonstrated an end-to-end pipeline for financial extraction and transcription of tabular content from images. Furthermore, adaptations in Text-to-SQL (Rajkumar et al., 2022) methods for transforming natural language queries into database-readable commands show promise in streamlining document analysis tasks. CodeAct (Wang et al., 2024) demonstrated that executable Python code can unify LLM agents' actions in a single action space, allowing for dynamic adjustments and new policies based on multi-turn interactions. Furthermore, LLMs in financial intelligence has evolved from traditional knowledge-graph and database approaches to domain-specific LLMs, though these face challenges with costs and accuracy, motivating the development of more sophisticated architectures (Watson and Liu, 2021; Watson et al., 2024; Cho et al., 2024).\nLegal LLMs SaulLM-7B (Colombo et al., 2024), based on Mistral-7B, is specifically designed for legal text comprehension and generation. Trained on an extensive English legal corpus, it shows state-of-the-art capabilities in processing legal documents using instruction fine-tuning. Jayakumar et al. (2023) explored the zero-shot capabilities of general-purpose LLMs such as ChatGPT-3.5, LLaMA2-70b, and Falcon-180B on contract provision classification, noting their lower F1 scores compared to smaller, legal-specific fine-tuned models. Yue et al. (2024) presents FedJudge, the inaugural Federated Legal LLM framework, optimizing performance with minimal parameter updates during federated learning. Additionally, Fei et al. (2024) introduces InternLM-Law, tailored for diverse legal inquiries related to Chinese laws. Trautmann et al. (2022) assesses zero-shot Legal Prompt Engineering (LPE) for processing complex legal documents in multiple languages, focusing on legal judgment prediction tasks. Finally, Roegiest et al. (2023) examines the potential of LLMs to generate structured answers to legal questions, specifically in multiple-choice formats.\nEvaluation Frameworks in Legal Environments Chen et al. (2021) and Nye et al. (2021) provide insight into the performance of LLMs in executing complex tasks. Their methodologies for assessing the accuracy and transparency of model outputs could be vital for deploying LLMs in legal settings where precision and accountability are crucial. Moreover, Liang et al. (2023) offers frameworks for ensuring that LLM operations adhere to legal and ethical standards. While the existing literature underscores significant advancements in legal LLM applications, LAW's modular design employs an orchestrator agent integrating reusable tools for legal domain-specific tasks, marking a significant evolution from previous models."}, {"title": "Data Sourcing & Ingestion", "content": "EDGAR For our dataset, we procure contracts from EDGAR (Electronic Data Gathering, Analysis, and Retrieval), the U.S. SEC's (Securities and Exchange Commission)\u00b9 database of regulatory filings. 23 years of filings are available in the omnibus filing 485BPOS which houses 2.7 million exhibits. From these, we procure 17,831 legal contracts (Appendix A).\nForm 485BPOS Form 485BPOS is a post-effective amendment filed by all investment companies governed by the US Investment Company Act of 1940. These investment companies, colloquially dubbed '40Act funds, are mandated to file Form N-1A or Form 485BPOS, pursuant to Securities Act Rule 485(b) (U.S. Securities and Exchange Commission, 1984). We choose the legal contracts housed in Form 485BPOS omnibus filing as they capture the entire universe of all '40Act funds and account for a non-trivial (14%) of EDGAR filings.\nIngesting Contracts Contracts are difficult to directly ingest due to inconsistent reporting in EDGAR. Moreover, the SEC only allows a maximum throughput of 10 reports/second - this limitation necessitates the need to bring our data on-premise as EDGAR is not accessible at scale. In summary, we ingest a total of 22 GB of data on-premise within our knowledge base through a myriad of techniques such as:\n\u25ba Scalable Procurement: We ingest at a rate of 112 documents/second - 6.7 hours were spent in terms of sequential processing. These contracts are not individually searchable on EDGAR; our knowledge base enables individual search.\n\u25ba AI Metadata Tagging and Search: Each section is made searchable via title recognition algorithms, alongside contextual and visual cues to intelligently chunk each contract for precise retrieval within our distributed hybrid search."}, {"title": "Tools", "content": "We develop legal domain-specific tools that each undertakes a specialized task. These tools enable re-usability across varying contracts in our dataset; moreover, additional tools can be added at any stage of LAW's development."}, {"title": "Tools for Direct Extraction", "content": "Tool for Extracting Dates Contracts house different types of dates such as the contract's Effective Date (when the current contract is effective), Master Date (when the master/original contract was effective), and Dated Date (when the current contract was signed). Our tool distinguishes these three types. Detection and extraction of dates is achieved via RoBERTa span detection (Liu et al., 2019), HTML parsing with BeautifulSoup42, and regular expression heuristics. Then, our tool standardizes all extracted dates to the DD/MM/YYYY format.\nTool for Extracting Parties This tool's objective is to find and extract the associated parties involved in signing the contract. These include the trust of funds and the custodian bank. Filing 485BPOS in EDGAR contains metadata about a subset of the involved parties for certain contracts. This is because the filing metadata only pertains to the specific legal entity making the EDGAR submission, rather than encompassing the full breadth of an investment manager's fund offerings and related parties. We use this as a guide to train our system's understanding of the full scope of contracts. We implement fuzzy matching to search for these parties in the contracts. The custom fuzzy matching built on top of RapidFuzz\u00b3 aims to mitigate issues that may arise from stylistic differences in names such as special character usage, capitalization, and differing naming conventions. Additionally, we mitigate issues with some names being substrings of others by searching sequentially in order of increasing name length and removing found parties."}, {"title": "Tools for Multi-Hop Reasoning", "content": "Tool to Calculate Contract Lifecycle Contracts typically have a lifecycle during which the provisions are enforced. This tool aims to calculate the termination date of the contract's lifecycle. It uses our existing tool for dates to extract the effective date of the contracts. Next, it searches for the contract's duration or the termination date. If the contract mentions the duration (e.g. 3 years), the tool translates the text into a numerical value. Finally, this numerical value is added to the effective date to generate a termination date."}, {"title": "Tool to Retrieve Master Contract", "content": "This tool's goal is to differentiate between a master and an amendment agreement. Master agreements refer to the original contract that outlines all aspects of the relationship between the fund and the custodian bank. An amendment refers to contracts that amend the master or any subsequent amendments amendments are typically less detailed as they can amend a single word. Our tool classifies and retrieves the master contract by comparing the extracted effective date with the master effective date using the tool for dates; if equal, the contract is considered to be the master. If determined to be an amendment, the tool searches for the master by matching the dates and parties."}, {"title": "Tool to Label Section Titles", "content": "Contracts are semantically structured and hierarchical in the construction of their clauses. Each clause holds detailed knowledge regarding terminology such as indemnification, force majure, and termination. Therefore, when queries about particular terms arise, directly retrieving the relevant clause or section is far more efficient than reviewing the entire contract indiscriminately. However, parsing contracts into distinct semantic sections for effective retrieval presents significant challenges. Many contracts lack explicit section declarations and the language across different sections can be highly similar. To address this challenge, we employed a fine-tuned t5-large (Raffel et al., 2020) model trained to classify paragraphs into one of 20 potential section labels. These labels cover a broad spectrum of typical clauses found in contracts (Appendix B). Our training dataset is comprised of 1,500 paragraphs per title, systematically collected from a variety of contracts to ensure diverse linguistic representations. As an alternative solution, we trained a t5-base model for title generation instead of classification. Both section title classification and generation models perform similarly. (Appendix F) outlines the performance and training parameters. Section titles are then used for section search and retrieval as described in \u00a76."}, {"title": "Text Agents", "content": "Summary Agent The summary agent aims to provide a useful summary of legal clauses. Our prompts enable the agent to focus on identifying and preserving key terms such as entity names and dates. A key challenge in summarizing relates to sections that exceed an LLM's context window. Legal contracts can be very long, averaging around 27K \u00b1 51K tokens when encoded by tiktoken. Therefore, if the input text and prompt exceed the 16K token limit of gpt-3.5-turbo, the text is split into 8K token chunks. These chunks are processed in parallel by separate sub-agents, with the output concatenated into a final summary.\nComparison Agent The comparison agent's purpose is to understand how particular clauses are different across time or entities. With a similar base prompt as the summary agent, it compares two bodies of text. The agent chronologically sorts the sections from different contracts and, in parallel, compares each pairwise set of sections in the list. For example, given a list of contracts' sections L = [s0, s1, ..., sn], where si is an individual section, it performs compare(si, si+1) \u2200 si \u2208 L,i \u2260 n. To handle large bodies of text, the agent also performs repeated summaries on each section si to condense the body to a manageable size. The summarized sections are then passed to the comparison agent."}, {"title": "Engineering Infrastructure", "content": "The system overview for LAW is illustrated in Figure 2. In addition to the tools described previously, it also uses the following modules:\nUser Query We compose user queries with two parts: (1) the entity of interest; and (2) the task to be executed. The base entities are Fund X, Trust X, and Custodian X. We also combine the base entities, e.g. Fund X and Custodian Y to find the contract for this particular relationship. The possible tasks to apply on the entities include: (1) Explore all contracts; (2) Find {master agreements, master dates, termination dates, parties, clause X}; (3) {Summarize, Compare} clause X. These tasks are motivated by legal use cases.\nCaching To reduce runtime latency, our system batch pre-processes data extraction. This includes features related to the involved parties or dates. The extracted data is stored in a CSV file on the backend disk, acting as a cache. This cached data helps avoid latency especially for multi-step reasoning.\nSection Search The large volume of legal text cannot be directly stored in our cache when retrieving contract sections. Instead, our contracts are segmented and indexed in an OpenSearch distributed datastore provided by AWS. For each contract, we retrieve the top 20 most relevant sections using the BM25 ranking algorithm. The ranking algorithm looks at the presence of the target clause in both the section texts as well as its indexed title (\u00a74.2).\nCode Generation Agent Our system prompts the agent to generate Python code that can resolve the user's query (\u00a76). The prompt includes tool names, descriptions, and examples similar to Chain-of-Thought (Wei et al., 2022) and Self-Refine (Madaan et al., 2023). The prompt specifies instruction preferences, such as outputs to display for particular tools, and execution preferences such as not printing outputs or leaving incomplete todo tags. LAW employs generated a three-tier system to generate and validate code:\n\u25ba Syntax Validation: Performs pre-execution checks to verify code syntax, types, and security constraints.\n\u25ba Hallucination Detection: Ensures generated code only calls tools that exist in LAW's toolset with valid parameter signatures.\n\u25ba Runtime Validation: Implements specialized error handling that captures and categorizes execution failures for targeted remediation.\nThis verification framework enables LAW's orchestrator to maintain a feedback loop, providing specific correction suggestions to the code generation agent when errors occur."}, {"title": "Experiments", "content": "Dataset Curation We labeled a dataset of 720 user queries as described in \u00a76. The tasks can be divided into two types: retrieval and analytical. Retrieval queries correspond to retrieving information about entities of interest from contracts. Queries that involve the exploration of all contracts, the extraction of dates, and parties fall into this category. Analytical queries require deeper insight, going beyond what can be extracted directly in the contracts. Queries that involve summarizing or comparing clauses across different contracts pertain to this category. We generated 20 queries for each combination of task and entity for retrieval queries and 10 for analytical queries. We randomly populated the entities of interest from the universe of '40Act funds. The ground truth answers are generated using hand-coded scripts that leverage the same tools and text agents that the proposed system has access to. This procedure makes the evaluation agnostic to the implementation of the tools and focuses exclusively on LAW's ability to generate code that correctly orchestrates the tools and the text agents.\nBaseline setup Our baseline seeks to understand if gpt-3.5-turbo, as is, can be prompted to answer queries on contracts. For Explore all contracts and Find master agreement queries, we simulate a noisy RAG framework by providing a set of four correct contracts and four distractor contracts. We reformulate user queries into a set of sequential True/False scenarios where the goal of the baseline is to determine if the candidate contract is associated with the entity, or is a master agreement, respectively. This choice was implemented as most contracts exceed the context limit of gpt-3.5-turbo. For other queries, we choose four relevant contracts and prompt gpt-3.5-turbo to extract the desired pieces of information. In essence, we narrowed the search space and provided relevant context for the baseline, where the provided context is sufficient for answering the queries. The context limit adds significant constraint on being able to provide in-context examples as demonstrations.\nResults Table 1 compares LAW against the baseline. LAW shows remarkable performance across retrieval to analytical queries. Among retrieval queries, for a true-false formulation of Explore all contracts, the baseline performs reasonably at 71.8% compared to 94.4% achieved by LAW. This similar performance is seen for Find master agreements. The baseline starts performing poorly at 36.2% when asked to lift the master date in contracts with a variety of dates. Moreover, the baseline quickly deteriorates for queries that require multi-hop reasoning, such as Find termination dates, where LAW surpasses the baseline by 92.9%. We observe that the baseline tends to hallucinate immensely, showing a near-compulsion to conjure fictional dates. Specifically, this operation depends on the LLM finding the term for the duration of the contract and adding it to the master's effective date. Finally, when asked to extract parties, the baseline often uses the question as an entity hint, but fails at lifting the complete list of entities from a dense agreement. For analytical queries, the baseline underperforms on clause summarization because of an inability to understand which sections are relevant to a given clause. Compare clause requires understanding the trend of how a clause changed across multiple contracts. This capability of comparing clauses cannot be performed using the baseline setup, as an agentic workflow with a larger context length is required."}, {"title": "Conclusion", "content": "We present LAW, a novel legal agentic workflow, achieving the successful completion of complex legal tasks. In contrast to fine-tuning an open-source LLM, our agentic invention is applicable for both closed and open-source models, leverages legal domain-specific tools and text agents that are modifiable and reusable, and orchestrates comprehensive plans. LAW has achieved remarkable performance, demonstrating robustness across retrieval and analytical queries and out-performing the baseline. Thus, our framework successfully enables automated workflows for varying contracts that govern the critical custody business. Future work can focus on applying LAW to non-English contracts, and explore additional agents grounded in other specific domains."}, {"title": "Disclaimer", "content": "This paper was prepared for informational purposes by the Artificial Intelligence Research group of JP-Morgan Chase & Co. and its affiliates (\"JPMorgan\") and is not a product of the Research Department of JPMorgan. JPMorgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful."}, {"title": "Domain Details", "content": "A.1 Form 485BPOS Regulatory Context\nForm 485BPOS is a document in the investment company industry, specifically used by mutual funds and other registered investment companies. It is filed with the U.S. Securities and Exchange Commission (SEC) under the Investment Company Act of 1940, often referred to as the '40 Act. It serves several key functions:\n1. Registration: It is used to register new mutual funds or update existing registrations.\n2. Prospectus Updates: '40Act Funds use Form 485BPOS to file updated prospectuses, which contain essential information for investors about the fund's investment objectives, risks, performance, and fees.\n3. Regulatory Compliance: It ensures that funds comply with SEC disclosure requirements and regulations subject under the Investment Company Act of 1940.\nFiling 485BPOS transcribes routine annual updates and other changes that become effective immediately upon filing. Currently, Form 485BPOS is 1.2% of the total available EDGAR filings4.\nA.2 Motivation and Impact\nThe contracts analyzed by LAW govern critical relationships between custodian banks and '40Act funds, which are investment vehicles for trillions of retail investors, especially for retirement income. Proper governance of key clauses is paramount for the health of these funds, custodian banks, and the broader financial ecosystem. Traditionally, an immeasurable amount of time has been spent on finding, analyzing, and comparing key clauses in these contracts.\nLAW shows potential for application to other contract types and legal documents. It also holds relevance for non-U.S. funds (e.g., USCITs) operating under similar regulations, demonstrating the broad applicability of our approach.\nA.3 Dataset Examples\n485BPOS Contracts in EDGAR are highly variable in length, format, content, and type. These include master agreements, amendments, separate appendixes, or the list of funds or entities that are captured by an agreement. See below for several example filings used in our dataset.\nA.4 Domain Complexity\nAnalyzing lengthy legal contracts is difficult as there are no obvious headings, a plethora of legal concepts that are exceedingly difficult to digest, and no obvious categorization of paragraphs. These highly unstructured and dense legal documents encompass and describe in minute detail with different nuances in different formats the following contractual principles such as standard of care regimes, gross negligence, fiduciary responsibilities, breach of contract, liability for direct damages, etc. These principles are presented in dense legal language, unstructured streams of text in different formats. Therefore, retrieving accurate numerical or textual values and analyzing/comparing them across tens of thousands of documents is a highly complex task. Within the legal domain, these retrieval and analytical tasks constitute as one of the most sophisticated and time-consuming tasks, especially in a highly unstructured database such as EDGAR, where no structured labels exist for the contracts."}, {"title": "List of Key Clauses", "content": "A full list of key clauses is found in Table 2. The term Indemnification refers to protective clauses that govern when losses occur with a third party. In the complex legal and financial landscape, recovery and punitive measures when accidents or losses occur is extremely important - for example, if a third party vendor's software breaks, is it the client or the provider's responsibility to recuperate those losses. Clauses governing Force majeure events are often related to indemnification clauses, which refer to"}, {"title": "System Design & Implementation", "content": "Framework Our system is based on FlowMind's (Zeng et al., 2023) framework and code recipe, extending it to a robust agentic legal framework with Fund Custody Services specific APIs. The code generation agent is responsible for mimicking planning by generating a series of function calls, akin to thinking steps, that breaks the question into steps semantically linked to our tool calls.\nTools and Agents LAW incorporates tools for direct extraction, multi-hop reasoning, and text analysis. By allowing LAW the flexibility to reuse statements, pass previous information from a function call directly into another, and iterate over retrieved items, it can go beyond single-step reasoning. Text-based agents employ zero-shot prompting. The summarize and compare tools utilize specialized text agents to yield useful analytics, enhancing the system's capability to handle complex queries. Specifically, the summary agent's task is to succinctly summarize a particular clause, restricting output to facts present in the text, such as preserving key terms, entities, dates, etc.\nLong-Context Contracts To handle long clauses and contracts, our system breaks the text into smaller chunks. These are then processed by separate \"sub-agent\" spawns to summarize, after which the results are concatenated into a complete summary. This approach ensures comprehensive analysis of extensive legal texts that would not fit into a standard context window.\nFramework Extensibility While LAW currently demonstrates strong performance with its existing toolset, extending the framework to new tasks requires careful consideration. Adding new tools involves:\n\u25ba Task Analysis: Identifying atomic operations that can be modularized into reusable components.\n\u25ba Tool Development: Creating focused tools with clear inputs/outputs and comprehensive unit tests.\n\u25ba Integration Testing: Verifying the tool's interaction with the code generation agent and other components.\nThe modular nature of LAW allows new tools to be added without modifying existing components. However, several challenges we faced included:\n\u25ba Ensuring tool reliability across diverse contract formats.\n\u25ba Maintaining clear boundaries between tool responsibilities.\n\u25ba Balancing tool specificity with reusability.\n\u25ba Managing increased complexity in the orchestration logic."}, {"title": "Experimental Details", "content": "D.1 Dataset Creation and Validation\nWe collaborated with business end-users to identify useful pieces of information to extract and sections/titles that they often examine. For validation, we conducted a small pilot study with human users on shorter contracts, obtaining performance comparable to that reported in the paper.\nD.2 Evaluation\nOur evaluation methodology included comparing outputs against hard-coded scripts as a ground truth. We simulated a noisy RAG framework by obtaining a small subset of relevant and irrelevant documents, mimicking a scaled version of the 485BPOS dataset. In this framework, we evaluate the system's ability to ignore irrelevant contracts while effectively extracting information from the relevant ones. The experiments employed a few-shot prompting strategy.\nD.3 Error Analysis\nThe main source of error in obtaining termination dates was the hallucination of non-existent dates, which particularly affected baseline performance. We noted that generating baseline output for clause changes across contracts was not feasible due to context limitations when communicating pairs of clauses.\nE Potential LAW Queries\n\u25ba Find the termination dates of all contracts from custodian Goldman Sachs.\n\u25ba Find the master dates of contracts between Trust Investor Counselor Series Fund Inc and Custodian State Street Bank and Trust.\n\u25ba Compare the fees and expenses clause of the previous contracts."}, {"title": "Title and Section Retrieval Metrics", "content": "The following are the training parameters for T5-Base and T5-Large models.\n\u25ba Training epochs: 3\n\u25ba Per device batch size for train and eval: 2\n\u25ba Learning rate: 0.01"}, {"title": "Sample Generated Code", "content": "Query: Compare the authorized persons clauses for Fund BNY Mellon International Equity Income Fund.\nCustomized suffix: Only compare subsequent clauses of five sampled non-empty contract sections. Ensure that there are also contracts for this entity choice.\nCode generated:"}]}