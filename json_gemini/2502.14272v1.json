{"title": "Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models", "authors": ["Yanggan Gu", "Junzhuo Li", "Sirui Huang", "Xin Zou", "Zhenghua Li", "Xuming Hu"], "abstract": "Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by comparing pairwise responses, overlooking the extent of difference between responses. This limitation hinders student SLMs from capturing the nuanced preferences for multiple responses. In this paper, we propose a Preference-Aligned Distillation (PAD) framework, which models teacher's preference knowledge as a probability distribution over all potential preferences, thereby providing more nuanced supervisory signals. Our insight in developing PAD is rooted in the demonstration that language models can serve as reward functions, reflecting their intrinsic preferences. Based on this, PAD comprises three key steps: (1) sampling diverse responses using high-temperature; (2) computing rewards for both teacher and student to construct their intrinsic preference; and (3) training the student's intrinsic preference distribution to align with the teacher's. Experiments on four mainstream alignment benchmarks demonstrate that PAD consistently and significantly outperforms existing approaches, achieving over 20% improvement on AlpacaEval 2 and Arena-Hard, indicating superior alignment with human preferences. Notably, on MT-Bench, using the GEMMA model family, the student trained by PAD surpasses its teacher, further validating the effectiveness of our PAD.", "sections": [{"title": "Introduction", "content": "Recently, small language models (SLMs) have demonstrated impressive performance across a variety of tasks (Grattafiori et al., 2024; Riviere et al., 2024; Jiang et al., 2023). Compared to large language models (LLMs) such as GPT4 (OpenAI, 2024), SLMs, with their fewer parameters, offer greater efficiency for deployment in diverse applications. However, their smaller parameter number limits their capacity to capture the nuances of human preferences, posing challenges in generating responses that align with human values, such as providing harmless replies to extreme or sensitive questions (Tunstall et al., 2024).\nUnlike SLMs, LLMs exhibit superior alignment with human preferences (OpenAI, 2024; Georgiev et al., 2024). Consequently, existing works leverage LLMs as teachers to distill preference knowledge into student SLMs (Bai et al., 2022; Cui et al., 2023; Tunstall et al., 2024; Wang et al., 2024; Yuan et al., 2024). All these works typically encode preference knowledge in teacher LLMs by comparing pairwise responses. For example, Bai et al. (2022) uses teacher-annotated responses to train a reward model, which guides the student via reinforcement learning. Similarly, Tunstall et al. (2024) employs a teacher model for preference annotation but instead applies Direct Preference Optimization (Rafailov et al., 2023) to optimize the student model."}, {"title": "Background", "content": "This section reviews two topics: 1) Preference modeling in preference learning theory, and 2) The generation process of language models under the reinforcement learning framework.\nPreference Modeling Given a prompt $x \\in X$, the language model $\\pi$ generates pairs of responses $(y_1, y_2) \\sim \\pi(y | x)$. A possible preference can be denoted as $y_1 > y_2 | x$, where $y_1$ and $y_2$ represent the preferred and dispreferred responses. Preferences are assumed to be generated based on a reward model $r(y | x)$, which assigns a continuous reward $r$ to each response $y$. For simplicity, we omit $x$ and use $r(y)$ to denote $r(y | x)$.\nThe pairwise preference probability $p(y_1 > y_2 | x)$ can be modeled using the Bradley-Terry (BT) framework (Bradley and Terry, 1952) as follows:\n$p(y_1 > y_2 | x) = \\frac{exp(r(y_1))}{exp(r(y_1)) + exp(r(y_2))}$                                                                    (1)\nNow, consider a more generalized scenario with a list of $n$ responses, denoted as $Y_n = \\{y_i\\}_{i=1}^n$, and the corresponding list of reward $R_n = \\{r_i\\}_{i=1}^n$. A possible preference ranking $\\tau_n = y^{(1)} > \\ldots > y^{(n)} | x$, where $y^{(i)}$ denotes the response ranked at the $i$-th position. Using the Plackett-Luce ranking model (Plackett, 1975; Luce, 2012), the preference probability is defined as:\n$p(\\tau_n) = \\prod_{i=1}^n \\frac{exp(r(y^{(i)}))}{\\sum_{j=i}^n exp(r(y^{(j)}))}$                                                                 (2)\nText Generation as a Markov Decision Process (MDP) The text generation process can be modeled as an MDP, which is represented by the triple (S, V, u), where the state space S represents all"}, {"title": "Self-Derived Log-Likelihood Rewards", "content": "This section introduces how we derive a reward function from language models without any reference model, providing the theoretical foundation for the framework proposed in the next section.\nInverse Reinforcement Learning (IRL) To induce the token-level reward model u, we follow the maximum-entropy IRL framework (Ziebart et al., 2008; Chan and van der Schaar, 2021), where the Q-value function at step t is defined as:\n$Q(y_t | y_{<t}, x) = u(y_t | y_{<t}, x) + \\log \\sum_{y_{t+1}} \\exp[Q(y_{t+1} | y_{<t}, x)]$                                                                   (3)\nFollowing Hao et al. (2022), we parameterize the Q-function as $Q(\\cdot) = f_{\\pi}(\\cdot)$, where $f_{\\pi}(\\cdot)$ represents the output logits of the language model $\\pi$. The reward function $u$ at each step $t$ is then defined as\n$u(y_t | y_{<t}, x) = f_{\\pi}(y_t | y_{<t}, x) - \\log \\sum_{y_{t+1} \\in V} \\exp[f_{\\pi}(y_{t+1} | y_{<t}, x)]$                                                      (4)\nWe further define $f_t := f_{\\pi}(y_t | y_{<t},x)$ and $Z_t := \\sum_{y_t \\in V} \\exp(f_{\\pi}(y_t | y_{<t-1}, x))$ for simplicity, which allows us to write that $u(y_t | y_{<t}, x) = f_t - \\log Z_{t+1}$. Please note that at last step, i.e., $t = |y|$, we have $\\log Z_{|y|+1} = 0$ according to the definition of the Q-value.\nCumulative Log-Likelihood Reward Given the token-level reward function u, the sequence-level reward is naturally defined by cumulating the token-level rewards:\n$r(y | x) = \\sum_{t=1}^{|y|} u(y_t | y_{<t}, x) = \\sum_{t=1}^{|y|} (f_t - \\log Z_{t+1}) = \\sum_{t=1}^{|y|} (f_t - \\log Z_{t}) + \\log Z_1 - \\log Z_{|y|+1} = \\sum_{t=1}^{|y|} \\log p_{\\pi} (y_t | y_{<t}, x) + \\log Z_1 = \\log p_{\\pi} (y | x) + \\log Z_1$        (5)\nwhere $p_{\\pi}(y_t | y_{<t}, x)$ is the probability of token $y_t$ given the previous sequences $(y_{<t}, x)$. Please note that $\\log Z_1$ does not depend on the particular sequence y.\nNormalized Log-Likelihood Reward By combining the Plackett-Luce model in Eq. 2 with the cumulative reward in Eq. 5, the probability for preference $\\tau_n$ is given by:\n$p(\\tau_n) = \\prod_{i=1}^n \\frac{\\exp (\\log p_{\\pi} (y^{(i)} | x))}{\\sum_{j=i}^n \\exp (\\log p_{\\pi} (y^{(j)} | x))}$                                                                     (6)\nWhen modeling preferences, the term $\\log Z_1$ can be eliminated due to the translation invariance property of the softmax function. Therefore, the cumulative reward simplifies to:\n$r(y | x) = \\frac{1}{|y|} \\log p_{\\pi} (y | x)$                                                                                (7)\nwhere $1/|y|$ is a length-normalized term to avoid bias towards longer sequences (Meng et al., 2024; Gu et al., 2024).\nIn other words, the reward of a language model can be formalized as the average log-likelihood, which naturally reflects the inherent preferences of the language model. Specifically, the higher the probability the model assigns to generating a response y, the greater the associated reward 3."}, {"title": "PAD: Preference-Aligned Distillation", "content": "This section outlines our PAD, which involves three key training phases (\u00a74.1-4.3), followed by the introduction of a preference decomposition strategy to accelerate the training process (\u00a74.4).\nDiverse Response Generation As the first step, taking prompt x as input, we directly sample n responses $Y_n$ from the student model $\\pi_{stu}$ through repeated sampling. To enhance response diversity, we apply a higher decoding temperature of 0.8. This approach offers two key advantages. First, enabling the generation of higher-quality responses. Existing works have shown that as the number of repeated samples increases, the likelihood of the model generating better answers across various tasks, such as mathematics and coding (Wang et al., 2023; Rozi\u00e8re et al., 2024; Brown"}, {"title": "Reward Calculation and Calibration", "content": "Given a prompt x and its corresponding list of responses $Y_n$ from the previous step, we calculate the rewards for both the teacher and student models for each response $y_i \\in Y_n$ using Equation (7). These rewards, denoted as $r^{tch}(y_i)$ and $r^{stu}(y_i)$, represent the models' average log-likelihood for each response. However, language models often suffer from miscalibration, where the assigned likelihoods do not accurately reflect the actual quality of the sequences (Zhao et al., 2023). For instance, phrases such as \"pros and cons\" and \"cons and pros\" convey the same meaning, but the former may be more frequent in the training data, leading the model to assign it a higher probability. This miscalibration poses a challenge: if the teacher's reward is miscalibrated, aligning the student model to the teacher may propagate this issue.\nTo address this, we leverage insights from Ren et al. (2023a) and Ren et al. (2023b), who demonstrate that Multiple-Choice Question (MCQ) selection probabilities better capture response quality than sequence likelihoods. We introduce the MCQ selection probability to calibrate the teacher model's reward. Specifically, each response $y_i \\in Y_n$ is randomly mapped to a choice within a pre-\ndefined set $C_n$ (e.g., $C_3 = \\{'A', 'B', 'C'\\}$), and we compute the (token-level) probability of selecting each choice:\n$p_{sel}(y_i) = P(C_i | Y_n, C_n, x)$                                                                                                     (8)\nwhere $c_i$ corresponds to the choice associated with response $y_i$.\nWe then calibrate the reward for each response by combining the normalized log-likelihood reward with the selection probability:\n$\\hat{r}^{tch} (y) = (1 - \\alpha) r^{tch}(y) + \\alpha \\log P_{sel}(y)$                                                                             (9)\nwhere the reward calibration ratio $\\alpha \\in [0, 1]$ is a hyperparameter that balances the influence of the original reward and the MCQ selection probability."}, {"title": "Preference Distillation", "content": "Based on different ways of modeling teacher preferences, we employ two losses to distillation: the vanilla preference loss $L_{VPD}$, and the probabilistic preference loss $L_{PPD}$.\nVanilla Preference Distillation (VPD) Following Rafailov et al. (2023); Song et al. (2024), the preference is modeled as a unique ranking. Specifically, we obtain ranking $\\tau_n$ of the responses $Y_n$ by sorting them according to their rewards $\\hat{r}^{tch}$. The student model is then trained with negative log-likelihood (NLL) loss to maximize the probability of teacher preference using Eq. 6.\n$L_{VPD} = - \\sum_{i=1}^n \\log \\frac{\\exp (\\beta r^{stu} (y^{(i)}))}{\\sum_{j=i}^n \\exp (\\beta r^{stu} (y^{(j)}))}$                                                                         (10)\nwhere $\\beta$ is a hyperparameter that controls the scaling of the reward difference."}, {"title": "Preference Decomposing Strategy", "content": "In our PAD, the number of sampled responses, i.e., the sample size n, plays a pivotal role. A larger n allows for a more macro comparison among responses, reduces the variance introduced by sampling, and increases the likelihood of generating high-quality responses (Brown et al., 2024). However, as n increases, the computational cost of both sampling and forward propagation also rises. Particularly when modeling preference distributions, the complexity grows factorially, making the computation unfeasible when n becomes large.\nTo reduce the computational cost, we propose a preference decomposition strategy. This strategy breaks down the preference of a large batch of responses into the preferences of multiple smaller batches, allowing the training process to be split"}, {"title": "Experiment", "content": "We evaluate two model families in our main experiments: 1) GEMMA-2 Models (Riv-"}, {"title": "Analysis", "content": "We analyze the impact of our proposed Preference Decomposing Strategy and Reward Calibration. To assess the generalization capability of PAD, we further investigate its performance when the teacher and student models belong to different families. A more detailed analysis is provided in Appendix C.\nEffect of Preference Decomposing Strategy We investigated the impact of the iterative distillation process on performance and training time using the preference decomposing strategy. Table 2 presents the effects of varying iteration counts and sample sizes. For a sample size of 4, decomposing the sampling process into two iterative steps does not reduce training time, as the low complexity of modeling the distribution with fewer samples renders"}, {"title": "Related Work", "content": "Traditional Knowledge Distillation Knowledge distillation (KD), introduced by Hinton et al. (2015), primarily aims at model compression by training a smaller student model to mimic the output behavior of a larger teacher model (Kim and Rush, 2016; Liang et al., 2021; Zhang et al., 2023; Gu et al., 2024; Agarwal et al., 2024). Kim and Rush (2016) extended KD to machine translation by training students on sequences generated by teachers in order to imitate teacher behavior.\nPreference Knowledge Distillation Motivated by the observation that large models have achieved a high degree of alignment with human values and preferences, many efforts focus on distilling preference knowledge from large models to smaller ones (Bai et al., 2022; Cui et al., 2023; Lee et al., 2024; Yuan et al., 2024; Tunstall et al., 2024; Yang et al., 2024). Bai et al. (2022) first introduced this concept, also known as Reinforcement Learning from AI Feedback (RLAIF), where teacher models annotate response pairs from the student to create a preference dataset for training a reward model. Tunstall et al. (2024) further utilized teacher-annotated preferences with Direct Preference Optimization (DPO) (Rafailov et al., 2023), streamlining the training of student models. These approaches follow the \"Teacher-as-Annotator\" paradigm. The annotated preference datasets generated through this paradigm can be directly employed with methods such as DPO, SimPO (Meng et al., 2024), and PRO (Song et al., 2024), enabling preference optimization of student models. However, a significant limitation of these methods lies in their reliance on unique ranking, which constrains their ability to model nuanced preferences. In contrast, our PAD treats modeling preference knowledge as a distribution over all possible preferences, enabling nuanced alignment for the student and teacher models."}, {"title": "Conclusion", "content": "In this paper, we introduced the Preference-Aligned Distillation (PAD) framework, which models the teacher's preference knowledge as a probability distribution over all potential preferences. This supervisory signal enables the student model to capture subtle distinctions between responses. Experimental results on the GEMMA-2 and LLAMA-3 model families show that PAD outperforms both traditional knowledge distillation and existing preference distillation methods across four benchmark tasks, highlighting its capacity for learning in-depth human preferences."}, {"title": "Limitations", "content": "Our research has several limitations. Firstly, the generalization capability is insufficient as we have not conducted experiments on larger-scale teacher and student models, primarily due to limited computational resources. Secondly, sampling multiple responses consumes more computational overhead. However, because SLMs have relatively smaller parameter sizes, this overhead remains comparatively modest. Thirdly, our method requires token-level probabilities, which are unavailable in some black-box models."}, {"title": "Decomposing Probabilistic Preference Distillation", "content": "Substituting Eq. 13 into the KLD:\n$D_{KL}(P_{tch}(\\tau_n) || P_{mix}(\\tau_n)) =  \\\\ = \\sum_{\\tau_n} P_{tch}(\\tau_n) \\log \\frac{P_{tch}(\\tau_n)}{P_{mix}(\\tau_n)} =  \\\\ = \\sum_{\\tau_n} \\left( \\prod_{i=1}^k P_{tch}(\\tau_n^{(i)}) \\right) \\log \\left( \\frac{\\prod_{i=1}^k P_{tch}(\\tau_n^{(i)})}{\\prod_{i=1}^k P_{mix}(\\tau_n^{(i)})} \\right) = \\\\ \\text{interchange summations,} = \\\\= \\sum_{\\tau_n} \\left( \\prod_{i=1}^k P_{tch}(\\tau_n^{(i)}) \\right) \\left( \\sum_{i=1}^k \\log \\frac{P_{tch}(\\tau_n^{(i)})}{P_{mix}(\\tau_n^{(i)})} \\right) = \\\\ notice that for a fixed i, the logarithm term only depends on $\\tau_n^{(i)}$, and the product can be separated, = \\\\ = \\sum_{i=1}^k \\left( \\sum_{\\tau_n^{(i)}} P_{tch}(\\tau_n^{(i)}) \\log \\frac{P_{tch}(\\tau_n^{(i)})}{P_{mix}(\\tau_n^{(i)})} \\prod_{j \\neq i} \\sum_{\\tau_n^{(j)}} P_{tch}(\\tau_n^{(j)}) \\right) \\\\ based on the independence assumption of sub-preferences, $\\sum_{\\tau_n^{(j)}} P_{tch}(\\tau_n^{(j)}) = 1$ for each j, \\\\= \\sum_{i=1}^k \\left( \\sum_{\\tau_n^{(i)}} P_{tch}(\\tau_n^{(i)}) \\log \\frac{P_{tch}(\\tau_n^{(i)})}{P_{mix}(\\tau_n^{(i)})} \\right) = \\\\Therefore, the KLD can be decomposed as:\\\\D_{KL}(P_{tch}(\\tau_n) || P_{mix}(\\tau_n)) =  \\\\ \\sum_{i=1}^k D_{KL}(P_{tch}(\\tau_n^{(i)}) || P_{mix}(\\tau_n^{(i)}))\nThe JSD Loss (Eq. 12) used in PPD is the average of two KLDs in different directions, making JSD also decomposable."}, {"title": "Training", "content": "We individually search the learning rates for dif-"}, {"title": "Formulation", "content": "and PAD, MiniLLM's reward function does not explicitly encode preference.\nReference Model Requirement. Both DPO and MiniLLM necessitate the simultaneous use of two models (current/reference or teacher/student) for reward computation. In contrast, PAD requires only the log-likelihood of the current model, eliminating the need for a reference model. This simplification not only streamlines the theoretical framework but also reduces computational cost in practice.\nAlignment with the Generation Stage. The reward functions in DPO and MiniLLM generally do not align with the probability distribution used during the final text-generation phase. In contrast, PAD's reward function is directly aligned with the model's log-likelihood, ensuring consistency between the training and inference.\nEmpirical Evaluation\nBeyond the theoretical distinctions outlined in Ta-"}]}