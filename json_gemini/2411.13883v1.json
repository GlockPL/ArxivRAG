{"title": "When Online Algorithms Influence the Environment: A Dynamical Systems Analysis of the Unintended Consequences", "authors": ["Prabhat Lankireddy", "Jayakrishnan Nair", "D Manjunath"], "abstract": "We analyze the effect that online algorithms have on the environment that they are learning. As a motivation, consider recommendation systems that use online algorithms to learn optimal product recommendations based on user and product attributes. It is well known that the sequence of recommendations affects user preferences. However, typical learning algorithms treat the user attributes as static and disregard the impact of their recommendations on user preferences. Our interest is to analyze the effect of this mismatch between the model assumption of a static environment, and the reality of an evolving environment affected by the recommendations. To perform this analysis, we first introduce a model for a generic coupled evolution of the parameters that are being learned, and the environment that is affected by it. We then frame a linear bandit recommendation system (RS) into this generic model where the users are characterized by a state variable that evolves based on the sequence of recommendations. The learning algorithm of the RS does not explicitly account for this evolution and assumes that the users are static. A dynamical system model that captures the coupled evolution of the population state and the learning algorithm is described, and its equilibrium behavior is analyzed. We show that when the recommendation algorithm is able to learn the population preferences in the presence of this mismatch, the algorithm induces similarity in the preferences of the user population. In particular, we present results on how different properties of the recommendation algorithm, namely the user attribute space and the exploration-exploitation tradeoff, effect the population preferences when they are learned by the algorithm. We demonstrate these results using model simulations.", "sections": [{"title": "1 Introduction", "content": "Online learning algorithms learn an environment through a sequence of actions and feedbacks from the environment to these actions. Most online learning algorithms make the key assumption that the characteristics that describe the environment, and the statistics of its response to the actions do not change as the learning progresses, i.e., the environment is static. Specifically, there is an implicit assumption that the environment does not respond to the actions of the learner. We argue that that is a strong assumption for these algorithms. Our interest in this paper is to analyze the consequence of this mismatch. Specifically, we consider the case of a learning algorithm updating its parameters assuming a stationary environment while the environment is evolving in response to the actions of the learning process, i.e., unbeknownst to it, the learner is influencing the environment that it is learning. Thus, we have an unintended consequence on the environment. This is perhaps best illustrated by a 'YouTube session' where a user starts off seeking videos on specific topics but the recommendations distract from the original intent and take the user through an entirely different sequence of videos. And all this while, the recommendation system would be updating the interest profile of the user in response to the new clicks. Thus the learning algorithm (recommendation system) and the environment (user preferences) are evolving together. In this paper, we will analyze this coupled evolution of the state of the learner and the state of the environment.\nLearning in non-stationary environments has received attention in recent literature, especially in multi-armed bandits where reward distributions of the arms are assumed non-stationary. In these models, the objective is typically to develop a learning algorithm that performs optimally in some sense, in the presence of an environment that changes. For example, in (Besbes, Gur, and Zeevi 2014), the expected reward from any arm can change at any point and any number of times but the changes are independent of the algorithm and the reward sequence. In rotting and recharging bandits, respectively (Levine, Crammer, and Mannor 2017) and (Kleinberg and Immorlica 2018), the arms have memory of when they were last recommended and the reward distribution depends on the delay since the last use of the arm. A generalization of rotting and recharging bandits is considered in (Fiez, Sekar, and Ratliff 2018) where the mean rewards from the arms vary according to a Markov chain.\nThe change of user preferences in response to recommendations has been well documented (Flaxman, Goel, and Rao 2016), (Mansoury et al. 2020). This has inspired some work to explicitly model this interaction and exploit it to improve the learning, e.g., (Shah, Blanchet, and Johari 2018) and more recently (Agarwal and Brown 2024). These works focus on optimally learning user preferences as they change, which is clearly different from our goal.\nThere is some literature that studies the effect of algorithms on the environment, albeit in a non online learning setting. The performative prediction literature (Perdomo et al. 2020) studies learning the environment when the ob-"}, {"title": "Organization and Summary of Results", "content": "The rest of the paper is organized as follows. We first introduce a generic model of interaction between an online learning algorithm and its environment in which the environment changes in response to the learners actions. We provide a result that allows us to understand the asymptotic behavior of the model by analyzing a dynamical system, when suitable assumptions are made on the model. Then, we consider the particular case of the contextual linear bandits problem in the setting where the environment is influenced by the algorithm. This is followed by a discussion on the equilibrium points of the dynamical system of aforementioned linear bandits setting and various results obtained from the simulation of the same. The proofs for all the theorems are provided in the appendix.\nIn the rest of the paper the term 'learner' will refer to the online learning algorithm that assumes that the environment is stationary. Also, we will use the terms 'learner', 'algorithm' and 'learning algorithm' interchangeably."}, {"title": "2 A Model of a Generic Online Learning System", "content": "We consider a discrete time interaction sequence between an environment E and a learner L that is trying to learn E; time is indexed by t. The learner believes that the environment is characterized by a state \u03c6* \u2208 E\u03c6, where E\u03c6 is the set of all possible states that the learner knows. The learning of E by L is achieved by finding \u03c6* that minimizes objective function \\(L_O\\), i.e.,\n\\(\\varphi^* = \\min_{\\varphi} L_O(\\varphi).\\)\nHowever, the environment is characterized by \u03c8t \u2208 E\u03c8 at time t, where E\u03c8 is the set of all possible states of the environment. E\u03c6 and E\u03c8 are subsets of real-numbered coordinate spaces and are not necessarily identical.\nTo learn E, L responds to a signal from E at time t with an action. This action elicits a reward from E even as E changes state to \u03c8t \u2208 E\u03c8 that depends on the action by L. The sequence of past signals, actions, and rewards are used to determine the action at time t; this history is captured by L in its state variable \u03c6t \u2208 E\u03c6. In the following, we make"}, {"title": "3 Unintended Consequences in a Linear Bandits Recommendation System", "content": "We consider a setting where a multi-type population of users receives suggestions from a recommendation system RS"}, {"title": "Model Description", "content": "There are N users in the population and are given recommendations from a pool of K products. RS associates each user n \u2208 [N] with a user attribute un \u2208 \\(\\mathbb{R}^p\\) and each product k\u2208 [K] with a product attribute wk \u2208 \\(\\mathbb{R}^q\\), where p and q are positive integers, and these attributes are used to make predictions. The set of all user attributes span \\(\\mathbb{R}^p\\) which tells us that N\u2265 p, and similarly the set of all product attributes span \\(\\mathbb{R}^q\\) which gives us K > q. At each time t, a user Ut \u2208 [N] arrives at RS and receives a recommendation At \u2208 [K]. We define Ct \u2208 \\(\\mathbb{R}^{pq}\\) as the context at time t, which is given by\n\\(C_t = u_{U_t} \\otimes w_{A_t}.\\)\nThe goal of RS is to learn the population. RS believes that the population is characterized by a parameter \u03b8 \u2208 \\(\\mathbb{R}^{pq}\\), which does not change with time, and that the reward is a linear function of \u03b8 and the context Ct. RS believes that \u03b8 minimizes a least squares objective given by\n\\(\\theta^* = \\min_{\\theta} E \\left[ (R_t - C_t^T \\theta)^2 \\right].\\)\nTo learn the population, RS maintains a state \u03b8t \u2208 \\(\\mathbb{R}^{pq}\\) that is updated at every time t. Contrary to the learner's assumption, the state of the population is given by \\(\\Psi_t = [\\Psi_{1,t} \\Psi_{2,t} ... , \\Psi_{N,t}]^T\\), where \\(\\Psi_{n,t} \\in \\mathbb{R}^{q}\\) is called the state of the user n at time t. Moreover, the state of the population changes with time, and that change at time t depends on the context Ct at that time. The following describes this interaction in the framework of the generic model described in the previous section."}, {"title": "4 Long Term Consequence on the Population", "content": "The following result ensures the existence of an equilibrium point to the previously obtained o.d.e."}, {"title": "5 Numerical Experiments", "content": "We demonstrate the long term effect of the parameters that are controlled by RS over the population using simulations. We do this by simulating a model using a set of simple, non-trivial parameters, and then study what happens when a specific parameter is varied. The following describes the initial simulation setup."}, {"title": "Initial Simulation Parameters", "content": "There are 3 users in the population and there are 3 products available for recommendation to RS i.e., N = 3 and K = 3 respectively. Each user arrives at RS with equal probability i.e., \u03bbn = 1/3 for all n \u2208 [N]. The dimension of the user attribute space p is taken to be p = 3 and the dimension of product attribute space satisfies q = 2. RS assumes that users have attributes, given by u1 = [1,2,3]T, u2 = [2,3,1]T, u3 = [3,1,2]T, and the product attributes are given by w1 = [cos(1\u00b0), sin(1\u00b0)]T, w2 = [cos(45\u00b0), sin(45\u00b0)]T, w3 = [cos(89\u00b0), sin(89\u00b0)]T. The initial states of the users are given by \u03c81(0) = [2,0]T, \u03c82(0) = [0,2]T, \u03c83(0) = [2,2]T. The regularization parameter in Eq. (8) is set to \u03b6 = 0.001, and the exploration-exploitation parameter of the recommendation algorithm is set to \u03b1 = 8. Each simulation is run for T = 1000 time steps, where \u03c4 is time in the o.d.e. timescale."}, {"title": "6 Discussion", "content": "In conclusion, we reiterate that the focus of this work is to understand the long term consequences of the interaction of the algorithm-dependent actions of a learning algorithm and the environment. Towards this, we first developed a general framework for modeling and analysis of such interactions.\nWe then applied this to a linear bandits based recommendation system and demonstrated several consequences that depend on the system parameters and the initial values.\nApplying the framework to other online learning settings is an obvious next step. One can also relax the assumptions made by the linear bandit model. An immediate extension is to allow the RS and the users to have different views of the product attributes. Two-timescale stochastic approximation theory can be applied to analyze the case when the evolution of the learner and the environment happen at different timescales. Using constant at and \u03b2t is also a natural next question."}, {"title": "A Proof of Theorem 1", "content": "From Eq. (3), we get\n\\(\\varphi_{t+1} = \\varphi_t + a_t (E[\\Phi(A_t, R_t, \\Psi_t) | \\varphi_t, \\Psi_t] + M_{t+1})\\)\nwhere \\(M_{t+1} := \\Phi(A_t, R_t, \\Psi_t) - E[\\Phi(A_t, R_t, \\Psi_t) | \\varphi_t, \\Psi_t]\\) is a martingale difference sequence that satisfies E[Mt/\u03c6t, \u03c8t] = 0. Similarly, Eq. (1) can be expressed as\n\\(\\Psi_{t+1} = \\Psi_t + a_t (E[\\Phi(A_t, R_t, \\Psi_t) | Q_t, \\psi_t] + M'_{t+1})\\)\nwhere \\(M'_{t+1} := \\Phi(A_t, R_t, \\psi_t) - E[\\Phi(A_t, R_t, \\Psi_t) | \\varphi_t, \\psi_t]\\).\nWe now use the following theorem from (Borkar 2009) to proceed."}, {"title": "B Proof of Theorem 2", "content": "The platform state is given by \\(\\varphi_t = (B_t, S_t)\\) and the environment's state is given by \\(\\Psi_t = (Y_{1,t}, Y_{2,t},..., Y_{N,t})\\). The evolution of the platform state \\(\\varphi_t = (B_t, S_t)\\) can be described by the function \\(\\Phi = (\\Phi_B, \\Phi_S)\\), where\n\\(\\Phi_B(\\varphi_t, \\Psi_t) = C_{U_t, A_t} R_t - B_t\\)\n\\(\\Phi_S(\\varphi_t, \\Psi_t) = C_{U_t, A_t} C_{U_t, A_t}^T - S_t\\)\nand the evolution of the environment's state \\(\\Psi_t\\) can be described by the function \\(\\Psi = (\\Psi_1, \\Psi_2,..., \\Psi_N)\\), where for any n \u2208 [N] we have\n\\(\\Psi_n(\\varphi_t, \\Psi_t) = \\mathbb{I}_{\\{U_t = n\\}} (w_{A_t} - Y_{n,t})\\).\nIf assumptions A1, A2 and A3 hold, then Theorem 1 tells us that Eq. (4) asymptotically tracks the system. We now show that all A1, A2 and A3 hold.\nTo show Al holds, we must first obtain expressions for E[\u03a6|\u03c6\u03c4, \u03c8t] and E[\u03a8|\u03c6t, \u03c8t]. Before that, we obtain expressions for \\(C_{U_t, A_t}\\) and Rt. From Eq (6), we get"}, {"title": "C Proof of Lemma 1", "content": "From Eq. (14), we can substitute y out to get the following\n\\(b = (V \\otimes W) \\Lambda_k diag(p)((I_N \\otimes W)^T (I_N \\otimes W) \\hat{p}\\)\n\\(s = (V \\otimes W) \\Lambda_k diag(p) (V \\otimes W)^T\\)\nWe can break down the expression for b as a sum of multiple expressions as follows."}, {"title": "F Proof for Theorem 4", "content": "From Lemma 2, then Eq. 19 holds when N = p, which can be rewritten as\n\\(0 = y_n - \\Sigma_{k=1}^K w_k \\frac{exp(a w_k^T y_n)}{\\Sigma_{l=1}^K exp(a w_l^T y_n)}\\).\nThe above can be expressed as the derivative of a function f: conv(W) \u2192 \\(\\mathbb{R}\\) evaluated at \\(\\hat{y}_n\\), given by\n\\(f(y) = \\frac{\\|y\\|^2}{2} - \\frac{1}{a} log(\\Sigma_{k=1}^K exp(a w_k^T y))\\)\nThe second term is the LogSumExp function, which behaves like a smooth max operator. As a \u2192 \u221e, this term gets closer to the max operator, which gives us"}, {"title": "G Proof for Theorem 5", "content": "From Eq. (14) and Eq. (15), we get\n\\(y_n = \\Sigma_{k=1}^K w_k \\frac{exp(a w_k^T y_n)}{\\Sigma_{l=1}^K exp(a w_l^T y_n)}  \\forall n \\in [N]\\)\nLet W denote the convex hull of all the product attributes {w1, w2,..., wK}. Consider the function f : W \u2192 W that satisfies"}]}