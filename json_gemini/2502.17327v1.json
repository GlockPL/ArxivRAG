{"title": "AnyTop: Character Animation Diffusion with Any Topology", "authors": ["Inbar Gat", "Sigal Raab", "Guy Tevet", "Yuval Reshef", "Amit H. Bermano", "Daniel Cohen-Or"], "abstract": "Generating motion for arbitrary skeletons is a longstanding challenge in computer graphics, remaining largely unexplored due to the scarcity of diverse datasets and the irregular nature of the data. In this work, we introduce AnyTop, a diffusion model that generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input. Our work features a transformer-based denoising network, tailored for arbitrary skeleton learning, integrating topology information into the traditional attention mechanism. Additionally, by incorporating textual joint descriptions into the latent feature representation, AnyTop learns semantic correspondences between joints across diverse skeletons. Our evaluation demonstrates that AnyTop generalizes well, even with as few as three training examples per topology, and can produce motions for unseen skeletons as well. Furthermore, our model's latent space is highly informative, enabling downstream tasks such as joint correspondence, temporal segmentation and motion editing.", "sections": [{"title": "INTRODUCTION", "content": "Character animation is a fundamental task in computer animation, playing a crucial role in industries such as film, gaming, and virtual reality. Animating 3D characters is a complex and time-consuming task that requires manual high-skill effort. Typically, animation pipelines involve a unique skeleton for each character, defining its motion span, over which the animation is carefully crafted.\nIn recent years, neural network-based approaches have simplified the animation process, showing impressive results in tasks such as motion generation and editing [Dabral et al. 2023; Holden et al. 2016; Tevet et al. 2023; Zhang et al. 2024a]. However, most existing methods cannot handle different skeletons and focus on a single topology [Kapon et al. 2023; Shafir et al. 2024], target skeletons that differ only in bone proportions [Tripathi et al. 2025; Yang et al. 2023], or rely on skeletal homeomorphism [Aberman et al. 2020].\nWhile effective within their scopes, these methods overlook the broader opportunities presented by diverse character animation, which require handling a wide variety of skeletal topologies. Conversely, methods designed to handle multiple skeletons often lack scalability, relying on topology-specific adjustments such as additive functional blocks for each skeleton [Li et al. 2024] or entirely distinct instances of the model [Li et al. 2022; Raab et al. 2024].\nThere are two main reasons keeping arbitrary skeleton animation generation largely under-explored. First, the irregular nature of the data, with skeletons varying in the number of joints and their connectivity, challenges standard methods for processing and analysis. Second, the lack of datasets encompassing diverse topologies presents significant challenges for data-driven approaches.\nIn this work, we introduce AnyTop, a diffusion framework designed to generate motions for arbitrary skeletal structures, as illustrated in fig. 1. AnyTop is carefully designed to handle any skeleton in a general manner with no need for topology-specific adjustments. AnyTop is based on a transformer encoder, specifically adapted for graph learning. While many works embed an entire pose in one tensor [Han et al. 2024; Xie et al. 2023], we embed each joint independently at each frame [Aberman et al. 2020; Agrawal et al. 2024], enabling capturing both joint interactions within the skeleton and universal joint behaviors across diverse skeletal structures. AnyTop applies attention along both the temporal and skeletal axes.\nNotably, the skeletal attention is between all joints. This is in contrast to previous art, and is made possible thanks to our topological conditioning scheme; we integrate graph characteristics [Park et al. 2022; Ying et al. 2021a], such as joint parent-child relations, into the attention maps. Consequently, each joint has access to information from all skeletal parts while also being able to prioritize topologically closer joints. Furthermore, to bridge the gap between similarly behaved parts in different skeletons, AnyTop incorporates textual descriptions of joints into the latent feature representation.\nAnyTop is trained on Truebones Zoo dataset [Truebones Motions Animation Studios 2022], which includes motion captures of diverse skeletal structures. We contribute a processed version, aligned with the popular HumanML3D [Guo et al. 2022] representation, which will be made publicly available. Using quantitative and qualitative evaluations, we show that AnyTop outperforms current art."}, {"title": "RELATED WORK", "content": "Skeletal variability in generative motion models. We refer to four types of skeletal variability (table 1). The naming draws from terminology in the graph domain, hence we interchangeably use the terms joint and vertex, as well as edge and bone. A single skeleton type refers to identical skeletons - that is, skeletons with the same vertices, connectivity, and edge lengths. Isomorphic skeletons correspond to isomorphic graphs, sharing vertices and edges but potentially differing in edge proportions. Homeomorphic skeletons may vary in structure, yet correspond to homeomorphic graphs, i.e., use topologies obtained from the same primal graph by subdivision of edges. Specifically, homeomorphic skeletons share the same number of kinematic chains and end-effectors. Finally, non-homeomorphic skeletons vary in their structure and have no common primal graph.\nMost motion generative methods focus on a single skeletal structure [Karunratanakul et al. 2023; Petrovich et al. 2021; Raab et al. 2023]. Others train on isomorphic skeletons [Villegas et al. 2021; Zhang et al. 2023b], including works that use the SMPL [Loper et al. 2015] body model [Jang et al. 2024; Petrovich et al. 2022a; Tripathi et al. 2025] and SMAL [Zuffi et al. 2017] body model or its derivatives [Rueegg et al. 2023; Yang et al. 2023]. A smaller portion of generative works support homeomorphic skeletons [Cao and Yang 2024; Lee et al. 2023; Ponton et al. 2024; Studer et al. 2024; Zhang et al. 2024d]. Among these works, some [Aberman et al. 2020] require a designated encoder and decoder per skeleton, and some [Zhang et al. 2024b] offer a unified framework for all skeletons.\nOnly a handful of works can handle non-homeomorphic skeletons. Martinelli et al. [2024] performs motion retargeting by learning a shared manifold for all skeletons, and decoding it to motions using learned skeleton-specific tokens. The learned tokens capture the skeletal information of characters in the dataset, limiting the model's ability to generalize to skeletons unseen during training. Its results are shown exclusively on bipeds, leaving the applicability to other character families (e.g., quadrupeds, millipedes) unexplored. WalkTheDog [Li et al. 2024] uses a latent space that encodes motion phases and accommodates non-generative motion matching.\nA different class of generative models bypasses the handling of the skeletal structure by generating motion directly from point clouds [Mo et al. 2025], shape-handles [Zhang et al. 2023a] or meshes [Muralikrishnan et al. 2024; Song et al. 2023; Ye et al. 2024; Zhang et al. 2024c]. These works demonstrate great flexibility in target character structure, but overlook the advantage of skeletons, which are more compact and semantically meaningful, easier to manipulate via rig-based animation, and compatible with physics engines [Tevet et al. 2024] and inverse kinematics systems. Some works [Wang et al. 2024] perform automatic rigging after the generation, but automatic rigging often necessitates manual adjustments.\nFinally, methods that support arbitrary skeletons [Li et al. 2022; Raab et al. 2024] involve a separate training process for each skeleton, exhibiting scaling issues and lacking Cross-skeleton generalization.\nAnyTop addresses training on non-homeomorphic skeletons and is the only skeletal-based approach capable of generating natural, smooth motions on a diverse range of characters, including bipeds (e.g., raptor, bird), quadrupeds (e.g., dog, bear), multi-legged arthropods (e.g., spider, centipede), and limbless creatures (e.g., snakes). To the best of our knowledge, our work is the only one capable of accepting an input topology, including unseen ones, and generating motions based on that topology.\nTransformer-based Graph Learning. Early versions of deep networks on graphs relied on convolutional architectures [Kipf and Welling 2016]. The emergence of transformers has sparked a new avenue of research, integrating graphs and transformers. GAT [Veli\u010dkovi\u0107 et al. 2018] replace the graph-convolution operation with a self-attention module, where attention is restricted to neighboring nodes. Rong et al. [2020] iteratively stack self-attention layers alongside graph convolutional ones to account for long-range interactions between nodes. Unlike transformers in the language and imaging domains, and due to the irregular structure of graphs, these earlier works do not use positional encoding.\nSubsequent works [Dwivedi and Bresson 2021; Kreuzer et al. 2021] linearize the graphs into an array of nodes and add absolute positional encoding to each node. However, linearization is unnatural to the graph structure, requiring a reconsideration of the approach. Encoding relative positional information has been explored to maintain positional precision while adhering to the graph's structure. Works using it [Park et al. 2022; Shaw et al. 2018; Ying et al. 2021b] integrate relative positional encoding into the attention map based on relative measures, such as shortest path distance between nodes or edge type.\nThe aforementioned approaches are discriminative, applied to tasks such as regression and segmentation. AnyTop leverages the relative positional encoding approach for generative tasks and tailors it to the motion domain. In particular, our work redefines edge types"}, {"title": "METHOD", "content": "AnyTop is a diffusion model synthesizing motions for multiple different characters with arbitrary skeletons. Given a skeletal structure for input, it generates a natural motion sequence with high fidelity to ground-truth characters. AnyTop is based on a transformer encoder, specifically adapted for graph learning, as depicted in fig. 2."}, {"title": "Preliminaries", "content": "Motion Representation. We represent motion as a 3D tensor $X \\in \\mathbb{R}^{N\\times J\\times D}$, where N and J are the maximum number of frames and joints across all motions in the dataset, and D is the number of motion features per joint. As motions vary in duration and skeletal structure, we pad the original number of frames and joints of each motion to match the maximum values N and J, respectively. We adopt a redundant representation, where each joint j (except the root) consists of its root-relative position $p_j \\in \\mathbb{R}^{3}$, 6D joint rotation $r_j \\in \\mathbb{R}^{6}$ [Zhou et al. 2018], linear velocity $v_j \\in \\mathbb{R}^{3}$, and foot contact label $fc_j \\in \\{0, 1\\}$. Altogether a joint is represented by $\\{p_j, r_j,v_j, fc_j\\} \\in \\mathbb{R}^{13}$, hence D = 13. For the root joint, features include its rotational velocity, linear velocity and height, which are concatenated and zero-padded to match the size D. Our representation is inspired by Guo et al. [2022]; however, our approach maintains features at the joint level by representing each joint as a separate tensor, resulting in J tokens per frame. In contrast, Guo et al. concatenate features from all joints into one tensor, resulting in a single token per frame.\nSkeletal structure Representation. In the context of 3D motion, topology is a directed, acyclic, and connected graph (DAG). Adding geometric information to this graph makes it a skeleton. We use the terms \"topology\" and \"skeleton\" interchangeably throughout this work, clarifying any distinction when necessary. A rest-pose is the character's natural pose, represented by (G, O), where G is a DAG defining the topological hierarchy and $O \\in \\mathbb{R}^{J\\times3}$ is a set of 3D offsets, specifying each joint's parent-relative position. In our work, we represent a skeleton by $S = \\{P_S,R_S, D_S, N_s\\}$. The first term, $P_S \\in \\mathbb{R}^{J\\times D}$, is the rest-pose, converted to the format of individual poses in the motion sequence. The second term, $R_S \\in \\mathbb{R}^{J\\times J}$, is the joints relations, where $R_S[i, j]$ holds the relation type between i and j. We allow six types of relations, which are child, parent, sibling, no-relation, self and end-effector. Self and end-effector are valid only in case i = j, and end-effector specifies if the joint is a leaf in $G_S$. The third term, $D_S \\in \\mathbb{R}^{J\\times J}$, represents the graph distances, where $D_S[i, j]$ holds the topological distance between i and j in $G_S$, up to a maximal distance $d_{max}$. The topological conditions, $R_S$ and $D_S$, are illustrated in fig. 3. Finally, $N_S$ is the joints' textual descriptions, which are typically included in 3D asset formats (e.g., bvh, fbx)."}, {"title": "Architecture", "content": "AnyTop is a generative Denoising Diffusion Probabilistic Model (DDPM) [Ho et al. 2020]. At each denoising step $t \\in [1, T]$ it gets a noisy motion $X_t$ and a skeleton $S = \\{P_S,R_S, D_S, N_s\\}$ as input, and predicts the clean motion $X_0$ [Tevet et al. 2023] rather than the noise $\\epsilon_t$.\nAnyTop consists of two primary components, illustrated in fig. 2. The first is an Enrichment Block, which integrates skeleton-specific information into the noised motion. The second is a Skeletal Temporal Transformer Block, which employs attention across both skeletal and temporal axes while embedding topological information into the skeletal attention maps."}, {"title": "Topological Conditioning Scheme", "content": "We extend transformers for graph-based learning by incorporating both graph topology and node interaction information through our Skeletal Attention mechanism. Inspired by discriminative works in the graphs domain [Ying et al. 2021b], AnyTop introduces a novel method for generative tasks, specifically tailored to the motion domain. We integrate graph properties directly into attention maps, enabling the structural characteristics of the graph to influence the learning process. Our work uses two types of node affinity, the topological distance, $D_s$, and relations, $R_s$, as detailed in section 3.1. We incorporate the graph information into the attention maps [Park et al. 2022], by learning distinct query and key embeddings for distances, denoted by $E_D, E'_D \\in \\mathbb{R}^{d_{max}\\times F}$, and embeddings for relation, denoted by $E_R, E'_R \\in \\mathbb{R}^{6\\times F}$, where $E'_D$ and $E'_R$ denote embeddings that relate to queries and keys, respectively, and F is the latent feature size. These embeddings are used to form two new attention maps, $a^D$ and $a^R$ defined for a given pair of joints $i, j\\in [J]:$\n$a^D_{ij} = q_i E_D[D_{ij}] + k_j \\cdot E'_D[D_{ij}]$, (1)\n$a^R_{ij} = q_i E_R[R_{ij}] + k_j \\cdot E'_R[R_{ij}]$, (2)\nwhere $q_i, k_j$ denote the i'th joint query and j'th joint key, respectively, and $[\\cdot]$ denotes an index in the embedding matrix. Finally, we incorporate graph information by adding the two attention maps to the standard attention map and scaling their sum:\n$a_{ij} = \\frac{q_i k_j + a^D_{ij} + a^R_{ij}}{\\sqrt{F}}$ (3)\nThe final attention score is computed by applying the standard row-wise softmax to $a_{ij}$."}, {"title": "Training", "content": "Data Sampling and Augmentations. We train AnyTop using minibatches sampled with a Balancing Sampler to address the imbalanced nature of the data (described in section 6.1) and mitigate the dominance of specific skeletons. To further enhance generalization, we apply skeletal augmentations to the data samples, including randomly removing 10% to 30% of the joints and adding new joints at the midpoint of existing edges. Further details on our data augmentation are provided in Appendix B .\nTraining Objectives. Given a motion $X_0$ of skeleton S, its noised counterpart $X_t$, with diffusion step $t\\sim [1, T]$, our model predicts the clean motion, $\\hat{X_0} = AnyTop(X_t, t, S)$. Our main objective is defined by the simple formulation [Ho et al. 2020], namely,\n$L_{simple} = E_{t\\sim[1,T]} ||X_0 - \\hat{X_0}||^2$. (4)\nThe Mean Squared Error (MSE) over rotations does not directly correlate to their distance in the rotation space, hence we apply a geodesic loss [Huang et al. 2017; Tripathi et al. 2025] over the learned rotations. Let $r, \\hat{r} \\in \\mathbb{R}^{N\\times J\\times 6}$ denote the 6D rotations of $X_0$ and $\\hat{X_0}$ respectively. The geodesic loss is defined as follows:\n$L_{rot} = \\sum_{n=1}^N \\sum_{j=1}^J \\arccos{\\frac{Tr(GS(r_{n,j})(GS(\\hat{r}_{n,j}))^T) - 1}{2}}$ (5)\nwhere GS is the Gram-Schmidt process, used to convert 6D rotations to rotation matrices [Zhou et al. 2019], and Tr is the matrix Trace operation. Overall, the final training objective is\n$L = L_{simple} + \\lambda_{rot} L_{rot}$. (6)"}, {"title": "ANALYSIS", "content": "Latent Space Analysis. In this section, we examine AnyTop's latent space and show that it features a unified manifold for joints across all skeletons. We use DIFT [Tang et al. 2023], a framework designed for detecting"}, {"title": "Generalization Forms", "content": "We identify three forms of generalization in our generated motions. In-skeleton Generalization. dubbed in-gen, refers to generalization within a specific skeleton, featured as both temporal composition combining motion segments from dataset instances, and spatial composition - introducing novel poses by combining skeletal parts of ground truth poses. Notably, spatial composition is enabled by our per-joint encoding, which provides the flexibility required for such diversity. In fig. 6 and in our supp. video, we showcase AnyTop's in-gen and highlight how other methods, which embed the entire pose, fail to achieve a comparable variety.\nCross-skeleton generalization. dubbed cross-gen, is expressed through shared motion motifs across different characters. This form of generalization enables the adaptation of motion behaviors originally performed by other skeletons, as shown in fig. 7 and our supp. video. When motions must strictly align with typical behaviors, the training dataset can be restricted accordingly.\nUnseen-skeleton generalization. extends to skeletons not encountered during training, and illustrated in fig. 8 and the video."}, {"title": "APPLICATIONS", "content": "AnyTop enables various downstream tasks; we demonstrate two."}, {"title": "EXPERIMENTS", "content": "Dataset and Preprocessing\nThe Truebones Zoo [Truebones Motions Animation Studios 2022] dataset comprises motion captures featuring 70 diverse skeletons, including mammals, birds, insects, dinosaurs, fish, and snakes. The number of motions per skeleton ranges from 3 to 40, adding up to 1219 motions and 147,178 frames in total. The dataset includes variations in orientation, root definition, and scale. Additionally, the skeletons vary in joint order, naming conventions, and connectivity standards. To address these variations, we have performed comprehensive preprocessing of the data, including aligning all motions to the same orientation and average bone length, centering the first frame at the origin, and ensuring it is located on the ground.\nSkeletal Subsets. In addition to experimenting with the full dataset, we categorize the skeletons into four groups based on their motion dynamics and train AnyTop on these subsets, alongside a model trained on the entire dataset. The four skeletal categories are Quadrupeds, Bipeds, Flying, and Insects. These subsets allow us to constrain cross-gen to characters with similar behavior.\nImplementation details\nWe use T = 100 diffusion steps, L = 4 STT layers, and latent dimension F = 128. We train the model using a single NVIDIA RTX A6000 GPU for 24 hours.\nEvaluation\nBenchmark. To evaluate AnyTop, we introduce a benchmark comprising 30 skeletons randomly selected from those with cumulative frame counts ranging between 600 and 1200.\nMetrics. We report four metrics that measure different aspects of the generated motions, following Li et al. [2022]; Raab et al. [2024]. The metrics are calculated separately for each skeleton, and the mean and standard deviation across all tested skeletons are reported in the form mean\u00b1std."}, {"title": "Baselines", "content": "To the best of our knowledge, no current works address such a diverse range of skeletal structures within a single model. Hence, we compare AnyTop to adaptations of two baselines. The first is MDM [Tevet et al. 2023], originally designed for a single humanoid skeleton. MDM uses per-frame embedding, so to match its representation format, we concatenate all joint features for each character, and pad them to a length of J \u00d7 D. For fairness, we also concatenate the vectorized rest-pose embedding Ps along the temporal axis as frame 0. Since MDM accepts textual conditions, we use the skeleton's name (e.g., Cat, Dragon) as the input text.\nThe second baseline is SinMDM [Raab et al. 2024], designed to be trained on a single motion sequence. We modify it to enable training on multiple sequences of the same character, resulting in a separate model for each skeleton."}, {"title": "Quantitative Results", "content": "Table 2 shows a quantitative comparison of AnyTop and the baselines. AnyTop outperforms MDM in all categories and SinMDM in all but coverage, which is expected since SinMDM is trained separately for each skeleton. Note the significant gap in diversity metrics, where the table shows AnyTop generalizes well, while the others struggle to do so. We also report the models' parameter count, showing ours uses fewer parameters, enabling lower computation and faster inference.\nQualitative Results\nOur supp. video reflects the quality of our results.\nUnseen skeleton. We present two unseen skeleton motions. One is a komodo dragon, generated by the Bipeds model. The second is a Cat, generated by a model trained on Quadrupeds, excluding the cat. Figure 8 demonstrates AnyTop generalizes well to unseen skeletons, while adapted MDM under the same settings generates static and jittery motions."}, {"title": "Ablation", "content": "In table 3, we explore three key components of AnyTop's architecture. First, the results confirm that without access to topological information, the model struggles to prioritize joints based on their hierarchical relations. Omitting the incorporation of D and R leads to degradation in all metrics. Next, excluding the rest pose Ps produces inferior results, reinforcing the idea that Ps encodes vital information about joint offsets and bone lengths. Lastly, we examine cross-skeletal prior sharing via the addition of joint name embeddings. While cross-gen improves motion diversity, it introduces a tradeoff, as generated motions may exhibit motifs absent in the skeleton's ground truth, reducing coverage. Results show that removing joint name embeddings increases coverage but severely sacrifices diversity and cross-skeleton generalization."}, {"title": "CONCLUSION, LIMITATIONS AND FUTURE WORK", "content": "We have presented AnyTop, a generative model that synthesizes diverse characters with distinct motion dynamics using a skeletal structure as input. It uses a transformer-based denoising network, integrating graph information at key points in the pipeline. Our evaluation shows a highly informative latent space and notable generalization, even for characters with few or no training samples.\nOne limitation of our method stems from imperfections in the input data. Despite our cleaning procedure, certain data artifacts remain unresolved. Another limitation is that our data augmentation process is computationally expensive with O(J2) complexity.\nIn the future, we plan to use AnyTop for skeletal retargeting, multi-character interaction, editing, and various control modalities such as text-based and music-driven animation. Another potential direction is editing animations by simply modifying joint labels in the text descriptions. Finally, future work could further explore DIFT features in the motion domain."}, {"title": "IMPLEMENTATION DETAILS", "content": "The maximum topological distance we allow in D is $d_{max} = 6$, and our Temporal Attention is applied on temporal windows of length W = 31. For our model inputs, we allow maximum number of joints J = 143. During training, we use cropped sequences of N = 40 frames. To enable our model handle higher frame positions and generate longer sequences, we incorporate positional encoding relative to the cropping index. For training, we used batch size of 16 when training on the entire dataset, and a batch size of 8 to train in the data subsets."}, {"title": "DATA", "content": "Truebones Zoo dataset. In addition to the data misalignment issues discussed in the main paper, the dataset also contains vulnerabilities such as excessive dummy joints, qualitative artifacts like foot sliding and floating, and 20% of the frames involve skeletons connected"}, {"title": "Topological Conditioning Scheme", "content": "We extend transformers for graph-based learning by incorporating both graph topology and node interaction information through our Skeletal Attention mechanism. Inspired by discriminative works in the graphs domain [Ying et al. 2021b], AnyTop introduces a novel method for generative tasks, specifically tailored to the motion domain. We integrate graph properties directly into attention maps, enabling the structural characteristics of the graph to influence the learning process. Our work uses two types of node affinity, the topological distance, D s , and relations, R s , as detailed in section 3.1. We incorporate the graph information into the attention maps [Park et al. 2022], by learning distinct query and key embeddings for distances, denoted by E D , E \u2032 D \u2208 R dmax \u00d7F , and embeddings for relation, denoted by E R , E \u2032 R \u2208 R 6\u00d7F , where E \u2032 D and E \u2032 R denote embeddings that relate to queries and keys, respectively, and F is the latent feature size. These embeddings are used to form two new attention maps, a D and a R defined for a given pair of joints i, j \u2208 [J]: a Dij = q i ED [D ij ] + k j \u00b7 E \u2032 D [D ij ], (1) a Rij = q i ER [R ij ] + k j \u00b7 E \u2032 R [R ij ], (2) where qi, kj denote the i'th joint query and j'th joint key, respectively, and [\u00b7] denotes an index in the embedding matrix. Finally, we incorporate graph information by adding the two attention maps to the standard attention map and scaling their sum: aij = qi kj + a D ij + a R ij \u221a F (3) The final attention score is computed by applying the standard row-wise softmax to aij."}, {"title": "Training", "content": "Data Sampling and Augmentations. We train AnyTop using minibatches sampled with a Balancing Sampler to address the imbalanced nature of the data (described in section 6.1) and mitigate the dominance of specific skeletons. To further enhance generalization, we apply skeletal augmentations to the data samples, including randomly removing 10% to 30% of the joints and adding new joints at the midpoint of existing edges. Further details on our data augmentation are provided in Appendix B . Training Objectives. Given a motion X 0 of skeleton S, its noised counterpart Xt, with diffusion step t \u223c [1, T ], our model predicts the clean motion, \u02c6 X 0 = AnyTop(Xt, t, S). Our main objective is defined by the simple formulation [Ho et al. 2020], namely, Lsimple = Et\u223c[1,T ] \u2225X 0 \u2212 \u02c6 X 0 \u2225 2 . (4) The Mean Squared Error (MSE) over rotations does not directly correlate to their distance in the rotation space, hence we apply a geodesic loss [Huang et al. 2017; Tripathi et al. 2025] over the learned rotations. Let r, \u02c6 r \u2208 R N\u00d7J\u00d76 denote the 6D rotations of X 0 and \u02c6 X 0 respectively. The geodesic loss is defined as follows: Lrot = N J X X arccos Tr GS(rnj)(GS(\u02c6 rnj) T ) \u2212 1 , (5) 2 n=1 j=1 where GS is the Gram-Schmidt process, used to convert 6D rotations to rotation matrices [Zhou et al. 2019], and Tr is the matrix Trace operation. Overall, the final training objective is L = Lsimple + \u03bbrot Lrot. (6)"}, {"title": "ANALYSIS", "content": "Latent Space Analysis. In this section, we examine AnyTop's latent space and show that it features a unified manifold for joints across all skeletons. We use DIFT [Tang et al. 2023], a framework designed for detecting"}, {"title": "GENERALIZATION FORMS", "content": "We identify three forms of generalization in our generated motions. In-skeleton Generalization. dubbed in-gen, refers to generalization within a specific skeleton, featured as both temporal composition combining motion segments from dataset instances, and spatial composition - introducing novel poses by combining skeletal parts of ground truth poses. Notably, spatial composition is enabled by our per-joint encoding, which provides the flexibility required for such diversity. In fig. 6 and in our supp. video, we showcase AnyTop's in-gen and highlight how other methods, which embed the entire pose, fail to achieve a comparable variety. Cross-skeleton generalization. dubbed cross-gen, is expressed through shared motion motifs across different characters. This form of generalization enables the adaptation of motion behaviors originally performed by other skeletons, as shown in fig. 7 and our supp. video. When motions must strictly align with typical behaviors, the training dataset can be restricted accordingly. Unseen-skeleton generalization. extends to skeletons not encountered during training, and illustrated in fig. 8 and the video."}, {"title": "APPLICATIONS", "content": "AnyTop enables various downstream tasks; we demonstrate two."}, {"title": "EXPERIMENTS", "content": "Dataset and Preprocessing The Truebones Zoo [Truebones Motions Animation Studios 2022] dataset comprises motion captures featuring 70 diverse skeletons, including mammals, birds, insects, dinosaurs, fish, and snakes. The number of motions per skeleton ranges from 3 to 40, adding up to 1219 motions and 147,178 frames in total. The dataset includes variations in orientation, root definition, and scale. Additionally, the skeletons vary in joint order, naming conventions, and connectivity standards. To address these variations, we have performed comprehensive preprocessing of the data, including aligning all motions to the same orientation and average bone length, centering the first frame at the origin, and ensuring it is located on the ground. Skeletal Subsets. In addition to experimenting with the full dataset, we categorize the skeletons into four groups based on their motion dynamics and train AnyTop on these subsets, alongside a model trained on the entire dataset. The four skeletal categories are Quadrupeds, Bipeds, Flying, and Insects. These subsets allow us to constrain cross-gen to characters with similar behavior. Implementation details We use T = 100 diffusion steps, L = 4 STT layers, and latent dimension F = 128. We train the model using a single NVIDIA RTX A6000 GPU for 24 hours. Evaluation Benchmark. To evaluate AnyTop, we introduce a benchmark comprising 30 skeletons randomly selected from those with cumulative frame counts ranging between 600 and 1200. Metrics We report four metrics that measure different aspects of the generated motions, following Li et al. [2022]; Raab et al. [2024]. The metrics are calculated separately for each skeleton, and the mean and standard deviation across all tested skeletons are reported in the form mean\u00b1std."}, {"title": "Baseline", "content": "To the best of our knowledge, no current works address such a diverse range of skeletal structures within a single model. Hence, we compare AnyTop to adaptations of two baselines. The first is MDM [Tevet et al. 2023], originally designed for a single humanoid skeleton. MDM uses per-frame embedding, so to match its representation format, we concatenate all joint features for each character, and pad them to a length of J \u00d7 D. For fairness, we also concatenate the vectorized rest-pose embedding Ps along the temporal axis as frame 0. Since MDM accepts textual conditions, we use the skeleton's name (e.g., Cat, Dragon) as the input text. The second baseline is SinMDM [Raab et al. 2024], designed to be trained on a single motion sequence. We modify it to enable training on multiple sequences of the same character, resulting in a separate model for each skeleton."}, {"title": "Quantitative Results", "content": "Table 2 shows a quantitative comparison of AnyTop and the baselines. AnyTop outperforms MDM in all categories and SinMDM in all but coverage, which is expected since SinMDM is trained separately for each skeleton. Note the significant gap in diversity metrics, where the table shows AnyTop generalizes well, while the others struggle to do so. We also report the models' parameter count, showing ours uses fewer parameters, enabling lower computation and faster inference. Qualitative Results Our supp. video reflects the quality of our results. Unseen skeleton We present two unseen skeleton motions. One is a komodo dragon, generated by the Bipeds model. The second is a Cat, generated by a model trained on Quadrupeds, excluding the cat. Figure 8 demonstrates AnyTop generalizes well to unseen skeletons, while adapted MDM under the same settings generates static and jittery motions."}, {"title": "Ablation", "content": "In table 3, we explore three key components of AnyTop's architecture. First, the results confirm that without access to topological information, the model struggles to prioritize joints based on their hierarchical relations. Omitting the incorporation of D and R leads to degradation in all metrics. Next, excluding the rest pose Ps produces inferior results, reinforcing the idea that Ps encodes vital information about joint offsets and bone lengths. Lastly, we examine cross-skeletal prior sharing via the addition of joint name embeddings. While cross-gen improves motion diversity, it introduces a tradeoff, as generated motions may exhibit motifs absent in the skeleton's ground truth"}]}