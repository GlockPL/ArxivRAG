{"title": "Empirical Evaluation of the Implicit Hitting Set Approach for Weighted CSPs", "authors": ["Aleksandra Petrova", "Javier Larrosa", "Emma Roll\u00f3n"], "abstract": "SAT technology has proven to be surprisingly effective in a large variety of domains. However, for the Weighted CSP problem dedicated algorithms have always been superior. One approach not well-studied so far is the use of SAT in conjunction with the Implicit Hitting Set approach. In this work, we explore some alternatives to the existing algorithm of reference. The alternatives, mostly borrowed from related boolean frameworks, consider trade-offs for the two main components of the IHS approach: the computation of low-cost hitting vectors, and their transformation into high-cost cores. For each one, we propose 4 levels of intensity. Since we also test the usefulness of cost function merging, our experiments consider 32 different implementations. Our empirical study shows that for WCSP it is not easy to identify the best alternative. Nevertheless, the cost-function merging encoding and extracting maximal cores seems to be a robust approach.", "sections": [{"title": "Introduction", "content": "The Weighted CSP problem (WCSP) is a framework for discrete optimization with many practical applications [7, 24, 25, 4, 7, 24, 25, 4] that has attracted the interest of researchers for decades [8, 19, 1, 3]. In this paper, we focus on the Implicit Hitting Set Approach (IHS) for WCSP solving. The idea of the IHS algorithms is to iteratively grow a set of unsatisfiable pieces of the problem (called cores) and find if it is possible to solve the problem by avoiding (i.e, hitting) them. The algorithm terminates when the best way to hit the identified cores incidentally also hits the unidentified cores.\nThe motivation for our work is that IHS is surprisingly effective for the MaxSAT problem [11, 12, 5], and such success has been lifted to several generalization frameworks such as Pseudo-boolean optimization [22, 23], MaxSMT [14] and Answering Set Programming [21]. Although MaxSAT and WCSP are"}, {"title": "Preliminaries", "content": "A Constraint Satisfaction Problem (CSP) is a pair (X, C) where X is a set of variables taking values in a finite domain, and C is a set of constraints. Each constraint depends on a subset of variables called scope. Constraints are boolean functions that forbid some of the possible assignments of the scope variables. A solution is an assignment to every variable that satisfies all the constraints. Solving CSPs is known to be an NP-complete problem [16].\nA Weighted CSP (WCSP) is a CSP augmented with a set F of cost functions. A cost function $f \\in F$ is a mapping that associates a cost to each possible assignment of the variables in its scope. The cost of a solution is the sum of costs given by the different cost functions. The WCSP problem consists of computing a solution of minimum cost $w*$.\nThe IHS approach for WCSP is defined in terms of vectors. The cost of vector $v = (v_1, v_2, ..., v_m)$ is $cost(v) = \\sum_{i=1}^{m} v_i$. In the (partial) order among same-size vectors, $\\tilde{u} \\leq v$, holds iff for each component i we have that $u_i \\leq v_i$. If $\\tilde{u} < v$ we say that $\\tilde{u}$ dominates $v$. We say that a set of vectors V dominates a vector $\\tilde{u}$, noted $\\tilde{u} < V$, if there is some $v \\in V$ that dominates $\\tilde{u}$. Further, we say that a set of vectors V dominates a set of vectors U, noted $U \\leq V$, if V dominates every vector of U. Given a set of vectors U, a vector $\\tilde{u} \\in U$ is maximal if it is not dominated by any other element of U. The set of maximal vectors in U is noted $U^\\lor$. The set of vectors in U with cost less than w is noted $U(<w)$. If u is not dominated by V we say that it hits V. The minimum cost hitting vector MHV of V is a vector that hits V with minimum cost. It is not difficult to see that MHV reduces to the classic minimum hitting set problem [15], which is known to be NP-hard.\nIn the following, we will consider an arbitrary WCSP (X, C, F) with m cost functions $F = \\{f_1, f_2,..., f_m\\}$. A cost vector $v = (v_1, v_2,..., v_m)$ is a vector where each component $v_i$ is associated to cost function $f_i$, and value $v_i$ must be a cost occurring in $f_i$. Cost vector v induces a CSP (X, C \\cup F_v) where $F_v$"}, {"title": "IHS-based WCSP solving", "content": "The IHS approach relies on the following observation that establish a lower bound and an upper bound condition in terms of cores and solutions,\nObservation 1. Consider a solution vector $\\hat{h}$ and a set of cores $K \\subseteq Cores$. Then, $MHV(K) \\leq w* < cost(\\hat{h})$.\nAll the algorithms discussed in this paper will aim at finding a solution $\\hat{h}$ and a (possibly small) set of cores K such that the two bounds meet (that is, $MHV(K) = cost(\\hat{h})$). This condition corresponds to $\\hat{h}$ being optimal and K being the proof of its optimality. We will refer to this (termination) condition as TC.\nConsider the set of all cores with costs less than $w*$. We define a goal core as a maximal core in that set. That is, the set of goal cores is GC = $Cores(<w*)^\\lor$. The following observation rephrases the lower bound part of TC as having a set of cores K that dominates all maximal goal cores,\nObservation 2. A set of cores K satisfies $MHV(K) = w*$ if and only if $GC < K$, where GC denotes the set of goal cores.\nTherefore, IHS algorithms must compute a set K that dominates every goal core (lower bound condition of TC) and an optimal solution (upper bound part of TC)."}, {"title": "Baseline Algorithm", "content": "The algorithm proposed in [13] appears at the left of Algorithm 1. It is a loop that maintains three variables: a working set of cores K, a lower bound lb, and an upper bound ub of the optimum. At each iteration, the algorithm computes in $\\hat{h}$ the MHV of K and solves the CSP that it induces. If it is satisfiable the algorithm will stop, else $\\hat{h}$ is improved into core k, which is added to K, and the algorithm goes on. The details for ImprCore() will be discussed in Subsection 4.2. For the moment, just note that it returns a core vector k such that $\\hat{h} \\leq k$. ImprCore() may find solution vectors during its execution. If their cost is smaller than the upper bound, the upper bound will be accordingly updated. Because the emphasis of this algorithm is in the lower bound (the upper bound is only updated if better solutions are found incidentally) we will refer to it as IHS-lb.\nIt is worth noting at this point that each iteration of IHS can be divided into two parts: i) the computation of the hitting vector $\\hat{h}$ which requires solving an NP-hard optimization problem and ii) if k is a core, its transformation into a larger core k which requires solving a sequence of CSPs, which are NP-complete decision problems. In this paper, we restrict ourselves to the usual choice of"}, {"title": "Algorithmic Alternatives", "content": "In the following, we describe some alternatives to alleviate the time spend com-puting hitting vectors."}, {"title": "Computation of Hitting Vectors", "content": "One way to decrease the workload of each iteration is to rely on non-optimal hitting vectors. As suggested in [23], we can replace optimal hitting vectors by hitting vectors of bounded cost. The right side of Algorithm 1 shows this idea. At each iteration, a hitting vector $\\hat{h}$ with cost less than ub is obtained. If $\\hat{h}$ is a solution the upper bound is updated, else it is improved and added to K. Since the emphasis of this algorithm is in the upper bound, we will refer to it as IHS-ub.\nThe main advantage of IHS-ub compared to IHS-lb is that iterations are likely to be faster. There are several reasons for this. On the one hand, it is much more efficient to find a bounded hitting vector which is a decision problem, than finding an optimal hitting vector which is an optimization problem. On the other hand, only in the last call of CostBoundedHV() the problem will be"}, {"title": "Cost-unbounded Hitting Vectors", "content": "Although obtaining cost-bounded hitting vectors can be done much more effi-ciently in practice than obtaining optimal hitting vectors, the problem remains NP-complete and, therefore, may still be time-consuming. As suggested in [11] and subsequently applied in [22], one way to avoid expensive calls is by remov-ing the cost-bound requirement. Obtaining a low-cost hitting vector without requiring the cost to be below a bound can easily be done with an incomplete algorithm.\nWe consider a greedy algorithm that starts from $\\hat{h}$ = 0 and makes a sequence of increments dictated by some greedy criterion until $\\hat{h}$ hits every vector in K. Since we want to hit as many cores as possible with the lowest cost, our algorithm selects the increment that minimizes the corresponding ratio. This algorithm is similar to a well-studied greedy algorithm for vertex covering [9].\nIf the resulting hitting vector $\\hat{h}$ is a core, then the algorithm can do the usual core improvement adding k to K as it would have happened with either IHS-lb or IHS-ub. If $\\hat{h}$ is a solution there are two cases. If its cost is less than ub, the upper bound is updated as it would have happened with IHS-ub. Alternatively, if its cost is more than or equal to ub, then there is no use for k and the iteration has been useless. To avoid the algorithm entering infinite loops, [11] suggests forcing the following iteration not to use the greedy algorithm. In our case, depending on whether the next iteration computes an optimal hitting vector or a cost-bounded hitting vector we will denote the algorithm IHS-grdlb or IHS-grdub."}, {"title": "Improving Cores", "content": "The different alternatives considered so far aimed at alleviating the time spent in computing hitting vectors. Now we address the task of improving cores. Adding to K cores with high values in their components is beneficial because they will increase the set of cores that they dominate and they will likely con-tribute towards the lower bound part of the Termination Condition. However, computing high-cost cores is more time consuming, and the right trade-off must be found.\nWe are restricting ourselves to algorithms that depart from a hitting vector $\\hat{h}$ and make a sequence of greedy increments to its components until some stopping criteria are achieved while preserving the core condition. In the following, we consider four alternatives.\n\u2022 Maximal Cores. The strongest (and most time consuming) criterion is"}, {"title": "Empirical Results", "content": "The experiments reported ran on nodes with 4 cores 16Gb Dell PowerEdge R240 with Intel Xeon E-2124 of 3.3Ghz. MinCostHV() and CostBoundedHV() were modeled as 0-1 integer programs and solved with CPLEX [10]. Induced CSPs were encoded as CNF SAT formulas and solved with CaDiCaL [6].\nFor the experiments, we used several benchmarks aiming at a heterogeneous sample of instances. Table 1 summarizes the features of each group of instances. All instances are pre-processed and made virtually arc consistent (VAC) [8]. Un-less indicated otherwise, we use the cost-function merging formulation proposed in [18] where clusters of cost functions are heuristically determined using a tree decomposition and (virtually) merged into a single cost function. Also, our im-plementation may compute several disjunctive cores in the same iteration as proposed in [13]. All executions had a time out of one hour. We conducted the empirical evaluation over the following benchmarks:"}, {"title": "Results", "content": "Our first analysis is about the impact of using or not using cost-function merg-ing. Table 2 reports the relative time performance gain of doing cost-function merging. For each benchmark, speed-up is the solving time ratio of its best performing algorithm out of the 16 alternatives with and without cost-function merging. We observe that cost-function merging is consistently useful producing significant speed-ups that in some cases are over 250. The only case where cost-function merging is not advantageous is with Grid instances where its use causes none of the 16 algorithms to solve any instance (not even with a larger time limit of 4 hours). Interestingly, without cost-function merging IHS-ub cores can solve all the instances with maximal and cost-bounded cores. The most prob-able reason is that the grid structure is so regular that the tree-decompostion used to decide which functions to merge is not appropriate. Because this result is so conclusive, and for the sake of clarity, in the following every table reports results with cost-merging except for Grid where the reported results are without cost-merging.\nTables 3 and 4 report for each one of the 12 problem classes and each one of the 16 algorithms, the relative performance with respect to time and space, respectively. Each table entry is the ratio w.r.t. the best-performing algorithm on that benchmark. For example, in Table 3, a 1 identifies the fastest algorithm while a value of r indicates that the algorithm is r times slower than the best.\nOur next analysis is about the impact that each algorithm has on the solv-ing time (Table 3). Our first observation is that different algorithms have very different running times, but no algorithm dominates the others. In some bench-marks, that difference is so extreme that the best approach solves all instances while other approaches are not able to solve any of the instances within the time limit. If we look at problem classes where every instance is solved with all (or nearly all) algorithms and compare speed-up ratios, we still see that the best approach is at least 4 times faster than the worst and the different can go up to several orders of magnitude. We also observe that some benchmarks are very"}, {"title": "Conclusion and Future Work", "content": "We have presented a large empirical evaluation of 32 alternative implementations of the Implicit Hitting Set approach for WCSP solving. Although our current implementations of IHS are only competitive with the state-of-the-art Toulbar2 solver in selected instances, our results show how different is the performance of different alternatives, and we believe that it indicates that the approach is very general and has potential.\nWe covered a variety of alternatives, but many known improvements that have been found useful in other paradigms remain to be adapted to the WCSP framework and tested. For example, we want to consider in the future reduced cost fixing or weight-aware cost extraction [23]. It is reasonable to expect that the IHS will also benefit from them in the WCSP paradigm.\nMore importantly, we believe that all the components of the algorithms that we have tested can be improved. We want to evaluate alternative ways to solve induced CSPs and a natural option is to replace the SAT solver by a constraint programming solver. We also want to evaluate alternative ways to find cost-bounded hitting vectors and a natural option would be to replace CPLEX by a Pseudo-Boolean optimization solver or a SAT solver with one of the many efficient encodings of Pseudo-Boolean constraints. Finally, we want to evaluate alternatives to the greedy algorithm and we believe that local search is a promising direction."}]}