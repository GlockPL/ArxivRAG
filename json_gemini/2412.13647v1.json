{"title": "G-VEval: A Versatile Metric for Evaluating\nImage and Video Captions Using GPT-40", "authors": ["Tony Cheng Tong", "Sirui He", "Zhiwen Shao", "Dit-Yan Yeung"], "abstract": "Evaluation metric of visual captioning is important yet not\nthoroughly explored. Traditional metrics like BLEU, ME-\nTEOR, CIDEr, and ROUGE often miss semantic depth, while\ntrained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-\nbased metrics also struggle with aligning to nuanced human\npreferences. To address these issues, we introduce G-VEval,\na novel metric inspired by G-Eval and powered by the new\nGPT-40. G-VEval uses chain-of-thought reasoning in large\nmultimodal models and supports three modes: reference-free,\nreference-only, and combined, accommodating both video\nand image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more\ntransparent and consistent framework for both human experts\nand evaluation metrics. It is designed to address the lack of\nclear criteria in existing datasets by introducing distinct di-\nmensions of Accuracy, Completeness, Conciseness, and Rel-\nevance (ACCR). Extensive results show that G-VEval out-\nperforms existing methods in correlation with human annota-\ntions, as measured by Kendall tau-b and Kendall tau-c. This\nprovides a flexible solution for diverse captioning tasks and\nsuggests a straightforward yet effective approach for large\nlanguage models to understand video content, paving the way\nfor advancements in automated captioning.", "sections": [{"title": "1 Introduction", "content": "Visual captioning, the task of generating descriptive text\nfrom visual content, represents a crucial intersection be-\ntween computer vision and natural language processing.\nThis field primarily addresses the complex challenge of en-\nabling machines to interpret and articulate visual data. Re-\ncently, researchers have integrated large language models\n(LLMs) to enhance the capabilities of visual captioning sys-\ntems, resulting in more precise and generalizable captions\nthat benefit numerous downstream applications. These ad-\nvanced systems, known as large vision-language models\n(LVLMs), now assist the visually impaired, improve educa-\ntional technologies, and enhance the autonomy of robotics\n(Li et al. 2020; Zhang et al. 2021)."}, {"title": "2 Related Work", "content": "In this section, we review existing metrics for evaluating im-\nage and video captioning, categorizing them into untrained\nmetrics, trained metrics, and advanced language model-\nbased metrics. Our goal is to highlight the strengths and lim-\nitations of each approach, ultimately establishing the need\nfor a more versatile and robust metric like G-VEval."}, {"title": "2.1 Untrained Metrics", "content": "Untrained metrics primarily rely on n-gram matching be-\ntween generated captions and reference captions. These met-\nrics, including BLEU, ROUGE, METEOR, and CIDEr, are\npopular due to their simplicity and ease of implementation.\nBLEU calculates the precision of n-grams in the gen-\nerated caption against reference captions (Papineni et al.\n2002). While widely used in both machine translation and\nimage captioning, BLEU struggles with synonyms and var-\nied sentence structures, which can lead to lower scores for\nhigh-quality captions generated by advanced models like V-\nLLMs. ROUGE focuses on recall by comparing overlap-\nping n-grams, word sequences, and word pairs between the\ngenerated and reference captions (Lin 2004). This metric is\ncommonly used in text summarization and image captioning\nbut shares similar limitations with BLEU regarding semantic\ndepth and synonym handling. METEOR combines precision\nand recall while incorporating synonym matching, stem-\nming, and paraphrase detection (Banerjee and Lavie 2005).\nIt offers a more nuanced evaluation compared to BLEU and\nROUGE but still relies heavily on word-level matches.\nCIDEr uses TF-IDF weighting for n-grams, emphasizing\nconsensus among multiple references (Vedantam, Zitnick,\nand Parikh 2015). It is specifically designed for image cap-\ntioning evaluation but can falter when the generated captions\nuse different wording than the references, especially with\nsynonyms. Despite their widespread use, these untrained\nmetrics often fail to align well with human judgment, par-\nticularly for captions generated by models that use advanced\nlanguage understanding, such as V-LLMs. While these met-\nrics can be applied to video captioning tasks, they do not ac-\ncount for visual content, limiting their effectiveness in this\ndomain."}, {"title": "2.2 Trained Metrics", "content": "Trained metrics leverage pre-trained embeddings or human-\nlabeled data, offering greater flexibility in language under-\nstanding. Embedding-based metrics, such as BERTScore,\nevaluate the similarity between generated and reference cap-\ntions using contextual embeddings from BERT (Zhang et al.\n2019). MoverScore enhances BERTScore with soft align-\nments and advanced aggregation methods (Zhao et al. 2019).\nThese metrics are more flexible with synonyms and sentence\nsegmentation but do not consider visual content, which lim-\nits their alignment with human preferences.\nTo address this gap, researchers have developed metrics\nbased on the cross-modal embeddings of vision-language\nmodels. CLIP-Score measures the similarity between gen-\nerated captions and image content using the CLIP model\n(Hessel et al. 2021). CLIP-ViT-B-32 and CLIP-ViT-L-\n14 are commonly used versions, encoding images into\n512-dimensional and 768-dimensional vectors, respectively.\nHowever, CLIP's encoding lacks the detail needed for fine-\ngrained visual captioning, and its applicability to video cap-\ntioning is limited. The only notable work in this area is EM-\nScore, which evaluates video captioning via coarse-grained\nand fine-grained embedding matching (Shi et al. 2022).\nSupervised metrics, such as PAC-S and Polos, are trained\non datasets derived from human evaluations, showing high\ncorrelation with human preferences. PAC-S uses contrastive\nlearning and human-labeled data to evaluate captions, em-\nphasizing positive augmentation (Sarto et al. 2023). Polos,\ndeveloped using multimodal metric learning from human\nfeedback, is effective in aligning with human judgments\n(Wada et al. 2024). However, their dependence on training\ndata can lead to weak performance in zero-shot settings, lim-\niting their broader applicability across diverse datasets."}, {"title": "2.3 Advanced Language Model-Based Metrics", "content": "Advanced language model-based metrics leverage the ca-\npabilities of large language models to provide more robust\nevaluations. CLAIR is an example of such a metric, us-\ning LLMs with simple prompts to evaluate image captions\n(Chan et al. 2023). While CLAIR shows strong performance\nin human correlation, it is limited to reference-only evalu-\nation and lacks interpretability due to its reliance on sim-\nple prompts. Additionally, CLAIR has not been extended to\nvideo captioning tasks, which restricts its broader applica-\nbility.\nG-Eval, although not a visual captioning evaluation met-\nric, presents a more structured approach to utilizing LLMs\nfor evaluation tasks, specifically in the context of summa-\nrization (Liu et al. 2023). Unlike CLAIR, G-Eval calculates\nthe expected value of the output from LLMs such as GPT-\n3 and GPT-4, addressing the challenges associated with the\nprobabilistic nature of LLMs. While G-Eval claims to use\nchain-of-thoughts (CoT) reasoning by including evaluation\nsteps in the prompt, it often produces single-digit outputs,\nlacking a genuine in-context reasoning process, which may\nlimit the effectiveness of CoT."}, {"title": "3 Methodology", "content": "G-VEval leverages GPT-40, a large language model with vi-\nsion capabilities, to evaluate model performance in image\nand video captioning tasks. G-VEval provides a framework\nthat generates evaluation scores highly aligned with human\npreferences by using prompts. The prompt consists of five\nmodules: 1) Evaluation Criteria; 2) Evaluation Steps: utiliz-\ning the Chain-of-Thought (CoT) to guide the LLM in a step-\nby-step manner, enhancing performance; 3) Score Function:\nformatting and restricting the output of the LLM; 4) Refer-\nence: attaching reference captions from human annotators as\nground truth; 5) Original Visual Content: original image or"}, {"title": "3.1 Evaluation Criteria", "content": "The evaluation criteria provide clear instructions to define\nthe task of evaluation. The criteria are designed to ensure\nconsistency across different captioning tasks, whether for\nimages or videos.\nGeneral Evaluation Criteria for Image and Video Cap-\ntioning. For both image and video captioning tasks, where\nan overall score is required, the evaluation criteria are stan-\ndardized to ensure a uniform approach across different tasks:\nScore (from 0 to 100) - selection of important content from\nthe references and the visual content. The generated caption\nshould accurately describe the important aspects of the vi-\nsual content while including the essential information from\nthe references. Annotators were instructed to penalize cap-\ntions that contained redundancies and excess information.\nThis general approach applies universally to evaluate the\noverall quality of captions, ensuring that both image and\nvideo content are assessed with the same rigor and consis-\ntency.\nACCR Evaluation Criteria for Video Captioning. While\ntraditional video captioning metrics provide a single score\nfor overall quality, our framework introduces a more gran-\nular approach with the ACCR evaluation criteria. ACCR\nstands for Accuracy, Completeness, Conciseness, and Rel-\nevance, which are the four dimensions used to comprehen-\nsively assess the quality of video captions:\n- Accuracy. Does the caption correctly describe the enti-\nties and actions shown in the video without errors or hallu-\ncinations?\nCompleteness. Does the caption cover all significant\nevents and aspects of the video, including dynamic actions\nand possible scene transitions?\nConciseness. Is the caption clear and succinct, avoiding\nunnecessary details and repetition?\n- Relevance. Is the caption pertinent to the video content,\nwithout including irrelevant information or questions?\nThe ACCR dimensions allow for a detailed and multidi-\nmensional evaluation of video captions, offering more than\njust an overall score. By breaking down the evaluation into\nthese four critical areas, ACCR provides a nuanced under-\nstanding of caption quality, which is essential for improv-\ning the performance of video captioning systems. Addition-\nally, by separating the evaluation dimensions, we reduce the\nvariance in evaluation scores. This approach forces both the\nevaluation metrics and human annotators to assess captions\nfrom the same angles, leading to more consistent and fair\nevaluations by minimizing inter-human variance."}, {"title": "3.2 Evaluation Steps", "content": "The Evaluation Steps leverage the Chain-of-Thought (CoT)\napproach, guiding GPT-40 to perform the task in a struc-\ntured, step-by-step manner. This method significantly en-\nhances the performance of the LLM by providing detailed\nintermediate steps for the evaluation task. According to the\nChain-of-Thought paper by Wei et al. (Wei et al. 2022), this\ntechnique improves the model's reasoning capabilities.\nThe example below demonstrates the evaluation steps\nused for image captioning, generated by GPT-4-Turbo.\nThese steps ensure that the LLM considers all relevant as-\npects of the content when generating its evaluations. A full\nset of evaluation steps, including those for video captioning,\nis provided in the appendix.\nEvaluation steps for images.\n1: Carefully observe the provided image to understand its\nmain content.\n2: Read the reference captions carefully to identify the im-\nportant information they highlight.\n3: Compare the generated caption to both the reference\ncaptions and the visual content of the image.\n4: Assess how well the generated caption covers the main\npoints of the visual content and the reference captions,\nand how much irrelevant or redundant information it\ncontains.\n5: Assign an integer score from 0 to 100, considering both\nthe alignment with the image and the inclusion of key\npoints from the references.\nThis structured approach ensures that all relevant aspects\nof the image and its captions are thoroughly evaluated, con-\ntributing to a more accurate and reliable scoring process. For\nvideo captioning tasks, similar steps are used, with adapta-\ntions to account for the temporal dynamics of video content."}, {"title": "3.3 Score Function", "content": "The score function formats and restricts the output of the\nLLM, ensuring consistency and clarity in the evaluation pro-\ncess. G-VEval supports two scoring settings depending on\nthe specific requirements of the task:\nScoring Setting. In this setting, the score ranges from\n0 to 100, providing a fine-grained evaluation scale. This ap-\nproach is particularly useful when a more detailed assess-\nment is needed, allowing for a broader range of possible\nscores.\n- Rating Setting. Alternatively, the score can be restricted\nto an integer between 1 and 5, offering a simpler and more\nstraightforward evaluation. This setting can be beneficial in\nscenarios where a coarser granularity is sufficient.\nFor the purposes of this paper, we primarily utilize the\nscoring setting (0 to 100), as it has demonstrated superior\nperformance in terms of human correlation in our prelimi-\nnary experiments.\nResponse Format: You should first give a detailed reason\nfor your score, ending with a sentence like this: The final\nscore is ${{score}}$. Note that the score should be an in-\nteger from 0 to 100, and should be wrapped in dollar signs\n($).\nUnlike G-Eval, where only a final score is provided, we\nensure that GPT-40 outputs a detailed reason for the score,"}, {"title": "3.4 Handling Probabilistic Outputs", "content": "G-VEval handles probabilistic outputs by calculating the ex-\npected value of the score using the log probabilities (log-\nprobs) provided by GPT-40. GPT-40 generates text in an\nautoregressive manner, where each token's probability dis-\ntribution is conditioned on the previously generated tokens.\nThis process allows us to derive the expected score by con-\nsidering the probabilistic distribution over possible outputs.\nThe expected score, denoted as E(s), is calculated using\nthe formula:\n$E(s) = \\sum_{i=1}^{m} i \\times p(i),$  (1)\nwhere m represents the maximum possible score, and p(i) is\nthe probability of each score i. The probability p(i) is com-\nputed as\n$p(i) = \\sum_{j=1}^{n}p(i|R_j) \\times p(R_j).$  (2)\nHere p(i| Rj) is the probability of score i given the reason\nRj, and p(Rj) is the probability of the reason Rj, with n\nbeing the total number of possible reasons.\nThe expected score can then be expressed as\n$E(s) = E_{R_j}(E_s(s|R_j)).$ (3)\nThis formula highlights that E(s) can be approximated by\nEs(s|Rj) when the variance of Es(s|Rj) is close to zero,\nas demonstrated in our experimental results. The detailed\nderivation of these equations is provided in the appendix for\ninterested readers."}, {"title": "3.5 Reference Captions and Original Visual\nContent", "content": "In G-VEval, reference captions are critical for providing\nground truth against which generated captions are evaluated.\nThese captions, provided by different annotators, often em-\nphasize different aspects of the visual content. Therefore, for\nboth image and video captioning tasks, we integrate all refer-\nence captions associated with the same visual input to form a\ncomprehensive ground truth. This integrated version ensures\nthat no significant detail is overlooked during evaluation.\nFor the reference-free and combined-reference settings,\nvisual content is included in the prompt. For image caption-\ning tasks, the original image is directly uploaded to GPT-40\nfor evaluation. However, video captioning presents unique\nchallenges since GPT-40 does not support direct video en-\ncoding. To address this, we sample three frames from each\nvideo clip: the first frame, the last frame, and the frame lo-\ncated at the midpoint of the video. These frames are then\ncombined into a single 1536x512 image, with each frame\nlabeled to indicate its position in the sequence (Frame 1,\nFrame 2, Frame 3). This setup aids\nthe model's spatial and temporal understanding, allowing it\nto interpret the sequence of events across frames effectively."}, {"title": "4 Experiments", "content": "In this section, we evaluate the performance of our G-VEval\nmetric compared to other metrics in the tasks of image\nand video captioning. We first conduct pre-experiments to\nestablish the optimal settings for G-VEval and then test\non the Flickr8k-Expert and Flickr8k-CF datasets for image\ncaptioning, as well as the VATEX-EVAL and MSVD-Eval\ndatasets for video captioning. Additionally, we conduct an\nablation study to assess the impact of different prompt com-\nponents on the performance of G-VEval."}, {"title": "4.1 Pre-Experiments", "content": "The pre-experiments aim to compare the effectiveness of\ntwo settings in G-VEval: the scoring setting (0 to 100) and\nthe rating setting (1 to 5). This comparison helps determine\nwhich setting provides better alignment with human judg-\nment and more reliable results.\nWe conducted experiments on the Flickr8k-Expert\ndataset, applying both the scoring and rating settings. For\neach setting, we calculated the variance of $E_s(s|R_j)$ and\nmeasured the correlation with human judgments using\nKendall's tau-b and tau-c metrics. The observed variance\nof $E_s(s|R_j)$ for both settings is low (0.014 for scoring and\n0.0087 for rating), indicating consistent outputs.\nThese low variances in both settings suggest that\n$E(S) = E_{R_j}(E_s(s|R_j))$ can be effectively approximated by\n$E_s(s|R_j)$. This approximation forms the foundation for our\nexperimental approach, as it allows us to calculate $E_s(s|R_j)$\ndirectly in subsequent experiments. A detailed proof of this\napproximation will be provided in the appendix.\nDespite the small variance in both settings, the scoring\nsetting significantly outperformed the rating setting in terms"}, {"title": "4.2 Image Captioning Performance", "content": "We tested our G-VEval metric against other metrics on the\nFlickr8k-Expert and Flickr8k-CF datasets (Hodosh, Young,\nand Hockenmaier 2013). These datasets are described in de-\ntail below.\nFlickr8k-Expert consists of 8,000 images, each anno-\ntated by three experts with five captions. This dataset pro-\nvides high-quality reference captions for evaluating caption-\ning models.\nFlickr8k-CF (CrowdFlower) includes the same 8,000\nimages as Flickr8k-Expert but with captions annotated by\ncrowdworkers. This dataset offers a different perspective on\ncaptioning quality due to the varied skill levels of annotators.\nTable 2 shows the performance of various metrics on the\nFlickr8k-Expert and Flickr8k-CF datasets. Among the three\nG-VEval settings, the ref-free setting consistently achieves\nthe highest human judgment correlation scores, establishing\na new state-of-the-art. This can be attributed to its ability\nto evaluate captions purely based on visual content, avoid-\ning potential biases from incomplete or misleading reference\ncaptions.\nThe combined setting, which integrates both reference\ncaptions and the original image, performs relatively worse\nthan the ref-free setting. As illustrated in Figure 3, this oc-\ncurs when reference captions fail to capture critical aspects\nlike the \"forest\" setting, introducing noise and slightly low-\nering the correlation score. Nevertheless, the combined set-\nting still achieves high scores, though it underscores the im-\nportance of visual content in evaluation.\nG-VEval's ref-only setting, relying solely on reference\ncaptions, performs lower than the combined setting, empha-\nsizing the need for visual context in accurate evaluations.\nMoreover, G-VEval significantly outperforms CLAIR,\nparticularly in Kendall Te, indicating that G-VEval is less\nprone to extreme scores, resulting in more stable and con-\nsistent evaluations. This is further demonstrated in Figure\n1, where G-VEval delivers a more balanced and accurate\nevaluation compared to CLAIR, which tends to give extreme\nscores."}, {"title": "4.3 Video Captioning Performance", "content": "We evaluate G-VEval on the VATEX-EVAL and MSVD-\nEval datasets to assess its effectiveness in video captioning\ntasks. The evaluation settings for each dataset are described\nbelow.\nVATEX-EVAL is a comprehensive dataset proposed by\nEMScore (Shi et al. 2022). It includes diverse video clips\nwith corresponding captions, allowing for robust evaluation\nof captioning quality. In this evaluation, we use three set-\ntings: No Ref, 1 Ref, and 9 Refs. The No Ref setting corre-\nsponds to G-VEval-ref-free, while the 1 Ref and 9 Refs set-\ntings correspond to our combined evaluation setting, where\nboth the reference captions and visual content are used to\ngenerate the scores.\nMSVD-Eval is our newly proposed dataset, created to\nenhance the evaluation of video captioning systems. It con-\nsists of 150 video clips selected from the MSVD validation\nset (Chen and Dolan 2011), with candidate captions gener-\nated by Video-LLaMA (Zhang, Li, and Bing 2023). These\ncaptions were selected to include both typical failure cases\nand acceptable captions of LLM-generated video captions.\nExperts evaluated these captions across four key ACCR di-\nmensions: Accuracy, Completeness, Conciseness, and Rel-\nevance, ensuring a comprehensive and detailed assessment\nframework. For comparison with other metrics, we also pro-\nvide an overall score (Avg.) by averaging the ACCR scores.\nTable 3 shows the performance of various metrics on the\nVATEX-EVAL dataset under the No Ref, 1 Ref, and 9 Refs\nsettings. Table 4 presents the overall human judgment corre-\nlation scores for various metrics on the MSVD-Eval dataset,\ncompared with the averaged human scores. Meanwhile, Ta-\nble 5 details the performance of G-VEval across the individ-\nual ACCR dimensions.\nThe results from VATEX-EVAL indicate that G-VEval's\ncombined setting, which leverages both reference captions\nand visual content, provides a substantial improvement over\ntraditional metrics, particularly in the 9 Refs setting. In the\nMSVD-Eval dataset, G-VEval achieves the highest human\njudgment correlation across all ACCR dimensions, further\nvalidating its capability to handle the nuanced evaluation\nof both typical failures and high-quality outputs in LLM-\ngenerated video captions.\nThe ACCR dimensions offer a significant advantage by\nfocusing human experts on specific aspects of caption qual-\nity, thereby reducing inter-human variance and improving\nthe reliability of the evaluation. This structured approach al-"}, {"title": "4.4 Ablation Study", "content": "To understand the impact of different components of our\nG-VEval prompt, we conducted an ablation study on the\nFlickr8k-Expert dataset under the reference-only setting.\nThis study examines how the performance of G-VEval\nchanges when certain key elements of the evaluation process\nare removed or altered. Specifically, we tested the following\nsettings:\n- G-VEval (full setting). The original prompt with Chain-\nof-Thought (CoT) evaluation steps, in-context reasoning,\nand expected score calculation.\nG-VEval w/o expected score. In this setting, instead of\ncalculating the expected value $E(s|R_j)$ from the probabilis-\ntic outputs of GPT-40, we directly use the single score s pro-\nvided by GPT-40 without considering the probabilistic dis-\ntribution of possible scores.\nG-VEval w/o CoT prompt. This setting removes the CoT\nevaluation steps from the prompt, testing the impact of los-\ning the guided, step-by-step reasoning process."}, {"title": "5 Discussion", "content": "G-VEval leverages the advanced capabilities of GPT-40,\nparticularly in language understanding and visual content\ninterpretation, through the use of Chain-of-Thought (CoT)\nprompting. This approach allows G-VEval to effectively uti-\nlize a large multimodal pre-trained dataset and a transformer\nmodel with billions of parameters. Unlike traditional n-gram\nmatching methods, G-VEval comprehends visual content\nfrom multiple perspectives provided by reference captions,\nenabling a deeper evaluation of candidate captions by com-\nparing their meaning with the visual content. This deeper\nlevel of understanding allows G-VEval to outperform tra-\nditional n-gram matching methods in aligning with human\npreferences.\nWhen compared to metrics that use pre-trained embed-\ndings, such as BERTScore (Zhang et al. 2019), G-VEval\nbenefits from GPT-40's comprehensive embeddings for both\nlanguage and visual content. Although the exact details of\nGPT-40's multimodal embedding model are not publicly\navailable, it is likely influenced by models like BLIP-2's Q-\nformer (Li et al. 2023), which achieves performance compa-"}, {"title": "6 Conclusion", "content": "We introduced G-VEval, an innovative evaluation metric de-\nsigned for image and video caption evaluation. G-VEval\nharnesses the deep multimodal understanding capabilities\nof GPT-40, utilizing the Chain-of-Thought reasoning and\nexpected score calculations based on decoding probability\ndistributions. This metric supports three evaluation modes\nand excels in scenarios where n-gram and embedding-based\nmetrics fall short, particularly in zero-shot and reference-\nfree contexts. Through extensive experiments, G-VEval has\ndemonstrated state-of-the-art performance and achieved the\nsuperior correlation with human evaluations compared to es-\ntablished metrics.\nThe introduction of MSVD-Eval further enriches the\nlandscape of video caption evaluation by offering a\ndataset that emphasizes multi-dimensional assessment cri-\nteria through the ACCR framework, focusing on Accuracy,\nCompleteness, Conciseness, and Relevance. This approach\nsignificantly enhances the reliability and consistency of the\nevaluation process by focusing human expert assessments\non the same aspects.\nLooking ahead, we aim to develop an online benchmark\nplatform based on G-VEval, where researchers can evaluate\ntheir image and video captioning models. This platform will\npave the way for future research in image and video cap-\ntioning tasks, providing a valuable resource for advancing\nthe field and improving the quality of automated captioning\nsystems."}, {"title": "A Appendix A: Proof of E(s) Approximation", "content": "We want to demonstrate that the expected value E(s) can\nbe approximated by Es(s|Rj) under the condition that\nVars(sRj) is small. This can be approached using a Tay-\nlor series expansion around the mean E(s).\nWe can derive from (1) and (2) in the main pa-\nper that $E(s) = \\sum_{i=1}^{m}\\sum_{j=1}^{n}i \\times p(i|R_j) \\times p(R_j) =$\n$\\sum_{j=1}^{n}P(R_j)\\sum_{i=1}^{m}i\\times p(i|R_j) =$\n$\\sum_{j=1}^{n} E_s(S|R_j) \\times p(R_j) =$\n$E_{R_j}(E_s(s|R_j))$\nGiven:\n$E(s|R_j) = \\mu_j$ is the expected value of S given condition\nRj.\n1. Overall Expected Value $E_{R_j}(E_s(S/R_j))$:\n$E_{R_j}(E_s(S/R_j)) = \\sum_j P(R_j)\\mu_j$\nLet this overall expected value be \u03bc:\n$\\mu = \\sum_j P(R_j)\\mu_j$\n2. Expectation of the Square of Conditional Expectations\n$E((E(s|R_j))^2)$:\n$E((E(s|R_j))^2) = \\sum_j P(R_j)\\mu_j^2$\n3. Variance of Conditional Expectations $Var(E(s|R_j))$:\n$Var(E(s|R_j)) = E((E(s|R_j))^2) - (E(E(s|R_j)))^2$\nSubstitute the expressions:\n$Var(E(s|R_j)) = \\sum_j P(R_j)\\mu_j^2 - (\\sum_j P(R_j)\\mu_j)^2$\n4. Approximating $\u03bc_j \\approx \\mu$ when $Var(E(s|R_j))$ is close to\n0:\nIf $Var(E(s|R_j))$ is close to 0, then:\n$\\sum_j P(R_j)\\mu_j^2 \\approx (\\sum_j P(R_j) \\mu_j)(\\sum_j P(R_j)\\mu_j)$"}, {"title": "Appendix B: G-VEval Settings and Sample\nPrompts", "content": "Here, we provide sample prompts for different G-VEval set-\ntings."}, {"title": "B.1 Image Captioning Evaluation", "content": "Reference-Only Setting\n\"You will be given one sentence of visual caption gen-\nerated from one image.\nYour task is to rate the generated caption on one met-\nric.\n Please make sure you read and understand these refer-\nence captions carefully. Please keep these references\nopen while reviewing, and refer to them as needed.\nEvaluation Criteria:\nScore is from 0 to 100 - selection of important content\nfrom the references. The generated caption should in-\nclude the important information in the references. An-\nnotators were instructed to penalize captions which\ncontained redundancies and excess information.\nEvaluation Steps:\n1. Read the reference captions carefully.\n2. Compare the generated caption to the reference\ncaptions and identify the main points of the visual\ncontent.\n3. Assess how well the generated caption covers the\nmain points of the visual content, and how much ir-\nrelevant or redundant information it contains.\n4. Assign an integer score from 0 to 100, please re-\nmember it.\"\nResponse Format:\nYou should first give detailed reason for your score,\nand ending with sentence like this: The final score is\n${{score}}$.\nNote that the score should be an integer from 0 to 100,\nand should be wrapped in the dollar signs ($).\nReference-Free Setting\n\u201cYou will be given one sentence of visual caption gen-\nerated from one image.\nYour task is to rate the generated caption on one met-\nric.\nEvaluation Criteria:\nScore is from 0 to 100 - selection of important con-\ntent from the image. The generated caption should\naccurately describe the important aspects of the im-\nage. Annotators were instructed to penalize captions\nwhich contained redundancies and excess informa-\ntion.\nEvaluation Steps:\n1. Carefully observe the image provided.\n2. Identify the main points of the visual content in the\nimage.\n3. Assess how well the generated caption covers the\nmain points of the visual content, and how much ir-\nrelevant or redundant information it contains.\n4. Assign an integer score from 0 to 100, please re-\nmember it.\nImage is attached\nResponse Format:\nYou should first give detailed reason for your score,\nand ending with sentence like this: The final score is\n${{score}}$.\nNote that the score should be an integer from 0 to 100,\nand should be wrapped in the dollar signs ($).\u201d\nCombined Setting\n\"You will be given one sentence of visual caption gen-\nerated from one image.\nYour task is to rate the generated caption on one met-\nric.\n Please make sure you read and understand these refer-\nence captions carefully. Please keep these references\nopen while reviewing, and refer to them as needed.\nEvaluation Criteria:\nScore is from 0 to 100 - selection of important content\nfrom the references and the image. The generated cap-\ntion should accurately describe the important aspects\nof the image while including the essential information"}, {"title": "B.2 Video Captioning Evaluation", "content": "Reference-Only Setting\n\"You will be given a caption generated from a com-\nplete video. For evaluation purposes, you are provided\nwith reference captions that describe specific frames\nor the overall video content.\nYour task is to rate the generated caption on one met-\nric.\n Please make sure you read and understand these refer-\nence captions carefully. Please keep these references\nopen while reviewing, and refer to them as needed.\nEvaluation Criteria:\nScore is from 0 to 100 - selection of important content\nfrom the references. The generated caption should in-\nclude the important information in the references. An-\nnotators were instructed to penalize captions which\ncontained redundancies and excess information.\nEvaluation Steps:\n1. Read the reference captions carefully.\n2. Compare the generated caption to the reference\ncaptions and identify the main points of the video con-\ntent.\n3. Assess how well the generated caption covers the\nmain points of the video content, and how much irrel-\n4. Assign an integer score from 0 to 100, please re-\nmember it.\""}, {"title": "B.3 Video"}]}