{"title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "authors": ["Yung-Sung Chuang", "Linlu Qiu", "Cheng-Yu Hsieh", "Ranjay Krishna", "Yoon Kim", "James Glass"], "abstract": "When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector-Lookback Lens-is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.", "sections": [{"title": "Introduction", "content": "Despite the utility and impressive capabilities of large language models (LLMs), their tendency to generate hallucinations, i.e., content that deviates from facts or contextually relevant information (Ji et al., 2023), presents a significant challenge in their deployment. In this work, we focus on the scenarios where the model is provided with the correct facts within the input context but still fails to generate accurate outputs, a phenomenon we term contextual hallucination. Despite the simplicity of this setup, LLMs struggle with contextual hallucinations, frequently producing errors in tasks such as summarization and document-based question answering (e.g., Table 1), which can cause serious issues in applications such as retrieval-augmented generation (RAG) (Lewis et al., 2020), even when correct documents are retrieved.\nMost prior studies that propose methods to combat hallucination focus on the scenario without any input context, where the hallucinations arise from the LLMs' parametric knowledge. These works detect and mitigate hallucinations by generally using the LLM's representations, such as hidden states (Burns et al., 2023; Azaria and Mitchell, 2023), MLP outputs (Zhang et al., 2024; Simhi et al., 2024), attention block outputs (Zhang et al., 2024; Simhi et al., 2024) and attention head outputs (Li et al., 2024; Chen et al., 2024b; Simhi et al., 2024). In contrast, the provided contextual information plays a key role in detecting contextual hallucinations. Insofar as attention (more so than other model internals) provides a human-meaningful measure of how much weight is given to the context during generation, this motivates the use of signals from the attention maps for hallucination detection and mitigation.\nTo leverage signals from attention maps, we start by hypothesizing that contextual hallucinations are related to the extent to which an LLM attends to the provided contextual information. Concretely, we propose a simple feature called lookback ratio, which is computed as the ratio of attention weights on the given context versus the newly generated tokens. At each time step, we calculate this lookback ratio for each attention head, and train a linear classifier, which we call the Lookback Lens, to detect contextual hallucinations based on the lookback ratio features, as illustrated in Figure 1. The Lookback Lens performs on par with, and sometimes even surpasses, more complex feature-based detectors that utilize hidden states from LLMs or text-"}, {"title": "Contextual Hallucinations Detection", "content": "To detect contextual hallucinations in LLMs, we introduce a lookback ratio, a measure based on the attention distribution of a transformer model. Given a transformer with L layers, each with H heads, the model processes an input sequence of context tokens  \\(X = {X_1,X_2,...,X_N}\\) of length N followed by a set of newly generated tokens \\(Y = {Y_1, Y_2,..., Y_{t\u22121}}\\) to generate the next token \\(Y_t\\). For time step t, and for each head, we calculate the ratio of attention weights focused on the context tokens versus the newly generated tokens. Formally, for each head h in layer l, we define:\n\\(A_{t}^{h,l}(context) = \\frac{1}{N} \\sum_{i=1}^{N} a_{h,i}\\)\n\\(A_{t}^{h,l}(new) = \\frac{1}{t-1} \\sum_{j=N+1}^{N+t-1} a_{h,j}\\)\nwhere \\(a_{h,i}\\) and \\(a_{h,j}\\) are softmax-ed attention weights assigned to context tokens X and new tokens Y respectively. The lookback ratio \\(LR_t^{h,l}\\) for head h in layer l at time step t is then calculated as:\n\\(LR_t^{h,l} = \\frac{A_{t}^{h,l}(context)}{A_{t}^{h,l}(context) + A_{t}^{h,l}(new)}\\)\nTo utilize these lookback ratios as input features in detecting hallucinations, we concatenate the lookback ratios across all heads and layers into a feature vector for the time step t:\n\\(v_t = [LR_t^{1,1}, LR_t^{1,2}, ..., LR_t^{L,H}]\\).\nGiven a text span of interest {\\(Y_t, Y_{t+1}\\), \u2026, \\(Y_{t+T\u22121}\\)}, we average the corresponding lookback ratio vectors {\\(v_t, v_{t+1}\\), ..., \\(v_{t+T\u22121}\\)} into a single vector \\(\\overline{v}\\). We then employ a logistic regression classifier F to predict if the span is factual (1) or hallucinated (0) based on the averaged lookback ratio vector.\n\\(P(y=1|\\overline{v}) = F(\\overline{v}) = \\sigma(w \\overline{v} + b)\\),\nwhere \u03c3 denotes the sigmoid function, w is the weight vector, and b is the bias term of the classifier.\nDefining Span The Lookback Lens predicts the probability of hallucinations over spans. We consider two ways to obtain spans for a given sequence: predefined spans or sliding window.\n1) Predefined Spans: When the hallucinated and non-hallucinated span annotations are available, we directly train the classifier to differentiate between them. This is a clean setting where all spans are either hallucinated or non-hallucinated.\n2) Sliding Window: In practice, we do not have any predefined spans during decoding, thus we need a sliding window setup that iterates over all possible spans. Specifically, we process the sentences into fixed-sized chunks and train the classifier to predict a label of 0 if any hallucinated content exists within a chunk, and 1 otherwise. Here,"}, {"title": "Experimental Setup", "content": "Data Training the Lookback Lens requires labels for hallucinated and non-hallucinated examples. To obtain these examples, we first prompt LLaMA-2-7B-Chat (Touvron et al., 2023) to greedy decode responses for 1,000 summarization examples from the CNN/DM dataset (See et al., 2017) and 2,655 QA examples from the Natural Questions (Kwiatkowski et al., 2019) following the setup of Liu et al. (2024). More details are shown in Appendix A. Although being prompted to generate correct responses, the decoded responses will contain both hallucinated and non-hallucinated information as the LLaMA model is still not perfect. Then, we employed GPT-4O (OpenAI, 2024) to verify the truthfulness of these responses and provide span-level annotations on hallucinated segments (detailed prompts in Appendix B). Additionally, we performed a pilot study of human annotation on a subset of 70 examples of the summarization task (details in Appendix C), confirming a 97% consistency rate between GPT-4o annotations and human judgments, and validating the reliability of the automated annotations. We show LLaMA-2-7B-Chat's results on both tasks, as evaluated by GPT-40, in Table 1. The results show that the generated summaries from LLaMA-2-7B-Chat still exhibit hallucinations about half of the time, highlighting the challenge of summarization tasks.\nBaselines We compare our detection method against several baselines: 1) Text-based entailment classifier: We fine-tune the DeBERTa-v3-base (He et al., 2021) model on the same dataset of CNN/DM and NQ as a natural language entailment (NLI) task. Additionally, we include the results from a state-of-the-art entailment model (Vectara, 2023) trained on a huge amount of annotated NLI data (see details in Appendix E. 2) Hidden states-based classifier: We train classifiers using the"}, {"title": "Results", "content": "Our results are presented in Table 2. We consider both predefined span segmentation and sliding window with a window size of 8. We include the two-fold validation setting on the source task and the out-of-domain transfer setting on the target task, with the tasks either question answering (QA) or summarization (Sum.). We find that the Lookback Lens achieves slightly better performance than the hidden states-based classifier and significantly outperforms the NLI models (SoTA and our impl.). The advantage of the Lookback Lens over the hidden states-based classifier is more significant in the sliding window settings, as shown in the right-hand side of Table 2.\nAdditionally, we observe that the hidden states-based classifier tends to overfit the training sets during the two-fold validation, and present a substantial performance drop when transferred to out-of-domain tasks. In contrast, Lookback Lens, while not always fitting the training set perfectly, consistently exhibits better performance when applied to out-of-domain tasks. This contrast highlights the effectiveness and generalizability of the lookback ratio features we extract from the attention maps."}, {"title": "Contextual Hallucinations Mitigation", "content": "To mitigate the impact of contextual hallucinations identified by the Lookback Lens, we introduce a classifier-guided decoding strategy to guide the generation toward more contextually accurate outputs. This approach serves as a robustness test of the Lookback Lens' ability to handle various text generation scenarios. While prior studies on controllable text generation adjust the output probabilities using classifiers based on the output tokens (Yang and Klein, 2021), our method fundamentally differs by not using the tokens themselves but rather their attention maps during generation.\nWe propose Lookback Lens Guided Decoding, which incorporates Lookback Lens (F) into the decoding process. Since all tokens in the vocabulary share the same attention pattern during one decoding step, F cannot directly influence one-step token choice. Instead, F can evaluate multiple-token chunks, as each chunk causes different attention patterns in multiple decoding steps.\nGiven the context and partially generated text, we independently sample a set of k candidate chunks {\\(C_1, C_2,...,C_k\\)} at the same decoding step t. For each chunk \\(C_j\\), the associated lookback ratios are averaged to form a feature vector \\(v_j\\). As shown in Figure 2, we select the best candidate \\(C^*\\) predicted by F and append to the generation,\n\\(C^* = arg \\underset{C_j\\in{C_1,C_2,...,C_k}}{max} F(v_j).\\)\nWe repeat this process until it generates the EOS token or reaches the maximum length."}, {"title": "Experimental Setup", "content": "We evaluate Lookback Lens Guided Decoding on three tasks that involve generating texts conditioned on given contexts, including summarization with XSum (Narayan et al., 2018), QA with"}, {"title": "Cross-model Transfer", "content": "One benefit of using the lookback ratio to capture higher-level model patterns for hallucination detection is its potential to better transfer across models. A classifier trained with one model's lookback ratio could potentially be applied to another model without retraining, provided correlation between"}, {"title": "Discussions and Ablations", "content": "In this section, we further conduct various experiments and ablation studies on the Lookback Lens and its corresponding classifier guided decoding.\nEffect of Chunk Size In Section 3.3 (Table 3), we experiment with chunk size = 8. Here, we study the effect of varying chunk sizes, from 4, 8, to 16. We see that there is a slight trend that Lookback Lens guided decoding prefers shorter chunk size for NQ and longer chunk size for XSum. However, in general the improvements are consistent across different chunk sizes, thus reducing the need to optimize for chunk sizes.\nPredictive Power of Different Heads In the aforementioned experiments, we utilize all attention heads to train the Lookback Lens. We are thus interested in how the predictive power is distributed among different heads in making predictions. That is, how much performance can we recover if we only utilize a subset of heads? To answer this, we use the coefficients in the linear classifier of the Lookback Lens (in Section 2) to estimate the importance of each head in detecting hallucinations."}, {"title": "Related Work", "content": "Hallucinations in LLMs. Simhi et al. (2024) defined close-book hallucination vs open-book hallucination for settings of relying on parametric knowledge vs knowledge in context. We term open-book hallucination as contextual hallucination for better clarity. Previous studies in hallucinations primarily focus on close-book hallucinations (Chen et al., 2023; Min et al., 2023; Chern et al., 2023) and their detection (Azaria and Mitchell, 2023; Simhi"}, {"title": "Conclusion", "content": "We introduce the Lookback Lens, a lightweight classifier designed to detect contextual hallucinations by utilizing the lookback ratio, which is computed solely from attention weights. This classifier not only effectively identifies contextual hallucinations but also mitigates them through Lookback Lens Guided Decoding from the LLM. Remarkably, the method is transferable across various tasks, and even across models after mapping their attention heads. This research opens up new possibilities for leveraging attention map information to combat hallucinations in large language models."}, {"title": "Limitations", "content": "Despite the effectiveness of the Lookback Lens and its decoding, there are several limitations to consider.\n\u2022 First, the performance upper bound of Lookback Lens Guided Decoding is limited by the sampling capabilities of the LLM itself. If the LLM fails to sample the correct chunk among the eight candidates, the Lookback Lens cannot correct the error.\n\u2022 Second, although the Lookback Lens is a lightweight classifier with negligible inference time, the requirement to sample multiple candidates from the LLM increases the total inference time. We argue that Lookback Lens Guided Decoding is a preliminary approach that demonstrates the feasibility of integrating the Lookback Lens into the decoding process, as well as a robustness test for the Lookback Lens to handle various text generation scenarios. However, other options, such as intervening in the attention map mechanism based on Lookback Lens signals, could potentially achieve faster inference, and we leave this for future work.\n\u2022 Lastly, the Lookback Lens relies on annotated examples of around 1k-2k to train the classifier. While other end-to-end methods (Chuang et al., 2024) can mitigate close-book hallucinations without training data, they lack interpretability due to the absence of a detection step. Nevertheless, we believe that requiring 1,000 annotated examples is a feasible setting."}, {"title": "Ethics Statement", "content": "In this research, we used publicly available datasets and we did not collect any personal information. All datasets and models are used in accordance with their intended use and licenses. Our method is designed to improve the factuality of large language models (LLMs), which can have a positive impact on various applications, such as question-answering systems, summarization systems, and other applications that rely on LLMs. When deployed, however, our approach still carries the issues stemming from LLMs, which means that there is a risk that the LLM can produce biased, harmful, or offensive output. Therefore, caution should be exercised before implementing similar approaches in real-world applications."}, {"title": "Data Creation for Lookback Lens", "content": "Our experimental setup aims to evaluate the ability of Lookback Lens to detect hallucinations in large language models with attention maps. We consider the summarization task and question-answering (QA) task in data creation.\nFor the summarization task, we sampled 1,000 examples from the CNN/DM dataset (See et al., 2017). For QA, we use 2,655 examples from the Natural Questions (Kwiatkowski et al., 2019) from the setup of Liu et al. (2024) to mix the gold document with irrelevant documents. To keep our focus more on LLM hallucinations rather than being distracted by assessing LLMs' long-context utilization ability, we limited context to three documents per question where the gold document containing the answer was placed in the middle, surrounded by two irrelevant documents.\nWe prompt LLaMA-2-7B-Chat (Touvron et al., 2023) to generate correct responses by greedy decoding for both tasks to ensure that both hallucinated and non-hallucinated examples derive from the same source distribution. The max length of generation is set to 256 tokens, or until the EOS token is generated.\nAfter the annotation was collected, we extract hallucinated and non-hallucinated spans, as well as the corresponding attention map lookback ratio, from the LLaMA-2-7B-Chat model, to train the Lookback Lens classifiers.\nIn the predefined span setting, three types of spans are considered as non-hallucinated spans: 1) the text segment before the first hallucinated span in the response 2) the text segment after the last hallucinated span in the response 3) the response annotated as non-hallucinated. All the annotated hallucinated spans are used as negative data to train the Lookback Lens.\nIn the sliding window setting, we consider all the possible fixed sized chunk with size = 8. If a chunk is overlapping with any of the annotated hallucinated spans, then it is considered as hallucinated, otherwise it is non-hallucinated.\nWhy not use existing data? Initially, we considered using the HaluEval dataset (Li et al., 2023), which was created by prompting GPT-3.5 (OpenAI, 2022) to generate \u201challucinated examples\" against human-annotated non-hallucinated responses, on summarization, QA, and dialogue tasks. However, we have concerns that their method introduces a bias by creating fundamentally different data distri-\""}, {"title": "Evaluation Prompt for GPT-40", "content": "We show the templates used to prompt GPT-40 (gpt-40-2024-05-13) in annotating the truthfulness of a response and the span-level hallucination segment prediction in Table 9 and Table 10, respectively for CNN/DM and Natural Questions.\nThis prompt is used for 1) collecting the data to train the Lookback Lens in Table 1, and 2) evaluating the XSum summarization task in Sections 3, 4, and 5."}, {"title": "Pilot Study of Human Evaluation on GPT-40 Evaluation", "content": "To assess the quality of GPT-40's evaluation in Appendix B, we conducted a pilot study using 70 examples from XSum data, involving the authors and colleagues who participated voluntarily as evaluators. Evaluators are all native English speakers. The human evaluators are provided with the document, ground truth summary, summary from LLaMA-2-7B-Chat, and the judgment from GPT-40. And the task is to give a binary judgment on whether the judgment from GPT-40 is correct. The screenshot of our interface is shown in Figure 4. Every evaluator is informed and consents that the annotation result will only be used to report statistics in this paper. Our pilot study shows that 68 out of 70 (97%) GPT-40 outputs are correct, justifying the use of GPT-40 as an automatic evaluator."}, {"title": "Evaluation Prompt for MT-Bench", "content": "We show the evaluation prompt for MT-Bench (hallucination) in Table 11. We follow standard practice for MT-Bench (original) evaluation\u2074 and show evaluation prompts in Table 12. We evaluate MT-bench (original) with their default"}, {"title": "Model Details", "content": "State-of-the-art NLI Model We give further detail on the pretrained SoTA NLI model 5 used as our topline hallucination detector. Specifically, the model is based on DeBERTa-V3-base (He et al., 2021) and further finetuned on a range of NLI and summarization datasets with examples annotated with factual consistency, including FEVER (Thorne et al., 2018), Vitamin C (Schuster et al., 2021) and PAWS (Zhang et al., 2019b). Roughly 731k data examples can be collected from the training set of the above three datasets. The model is reported to have superior performance when evaluated on TRUE (Honovich et al., 2022)"}, {"title": "Dataset Details", "content": "The datasets we used in the paper have the following details:\n\u2022 CNN/DM: sampled 1000 examples from the testing set. Apache-2.0 license. https://huggingface.co/datasets/abisee/cnn_dailymail\n\u2022 Natural Questions: Apache-2.0 license. Testing set: 2655 examples from https://github.com/nelson-liu/lost-in-the-middle. NQ-train: sampled 2499 examples from its training set, using the positive document provided by https://github.com/facebookresearch/DPR\n\u2022 XSum: 1000 examples sampled from the testing set. MIT license. https://github.com/EdinburghNLP/XSum\n\u2022 MT-bench: 80 examples. Apache-2.0 license. https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge"}]}