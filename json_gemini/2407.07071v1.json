{"title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "authors": ["Yung-Sung Chuang", "Linlu Qiu", "Cheng-Yu Hsieh", "Ranjay Krishna+", "Yoon Kim", "James Glass"], "abstract": "When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these look- back ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector-Lookback Lens-is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retrain- ing) to a larger 13B model. We further apply this detector to mitigate contextual hallucina- tions, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.", "sections": [{"title": "1 Introduction", "content": "Despite the utility and impressive capabilities of large language models (LLMs), their tendency to generate hallucinations, i.e., content that deviates from facts or contextually relevant information (Ji et al., 2023), presents a significant challenge in their deployment. In this work, we focus on the scenarios where the model is provided with the cor- rect facts within the input context but still fails to generate accurate outputs, a phenomenon we term contextual hallucination. Despite the simplicity of this setup, LLMs struggle with contextual halluci- nations, frequently producing errors in tasks such as summarization and document-based question an- swering (e.g., Table 1), which can cause serious issues in applications such as retrieval-augmented generation (RAG) (Lewis et al., 2020), even when correct documents are retrieved.\nMost prior studies that propose methods to com- bat hallucination focus on the scenario without any input context, where the hallucinations arise from the LLMs' parametric knowledge. These works detect and mitigate hallucinations by generally us- ing the LLM's representations, such as hidden states (Burns et al., 2023; Azaria and Mitchell, 2023), MLP outputs (Zhang et al., 2024; Simhi et al., 2024), attention block outputs (Zhang et al., 2024; Simhi et al., 2024) and attention head out- puts (Li et al., 2024; Chen et al., 2024b; Simhi et al., 2024). In contrast, the provided contex- tual information plays a key role in detecting con- textual hallucinations. Insofar as attention (more so than other model internals) provides a human- meaningful measure of how much weight is given to the context during generation, this motivates the use of signals from the attention maps for halluci- nation detection and mitigation.\nTo leverage signals from attention maps, we start by hypothesizing that contextual hallucinations are related to the extent to which an LLM attends to the provided contextual information. Concretely, we propose a simple feature called lookback ratio, which is computed as the ratio of attention weights on the given context versus the newly generated to- kens. At each time step, we calculate this lookback ratio for each attention head, and train a linear clas- sifier, which we call the Lookback Lens, to detect contextual hallucinations based on the lookback ratio features, as illustrated in Figure 1. The Look- back Lens performs on par with, and sometimes even surpasses, more complex feature-based detec- tors that utilize hidden states from LLMs or text-"}, {"title": "2 Contextual Hallucinations Detection", "content": "To detect contextual hallucinations in LLMs, we introduce a lookback ratio, a measure based on the attention distribution of a transformer model. Given a transformer with L layers, each with H heads, the model processes an input sequence of context tokens X = {X1,X2,...,xN} of length N followed by a set of newly generated tokens Y = {Y1, Y2,..., Yt\u22121} to generate the next token Yt. For time step t, and for each head, we calcu- late the ratio of attention weights focused on the context tokens versus the newly generated tokens. Formally, for each head h in layer l, we define:\n$A_{t}^{l,h}(context) = \\frac{1}{N} \\sum_{i=1}^{N} a_{t,i}^{l,h}$\n$A_{t}^{l,h}(new) = \\frac{1}{t-1} \\sum_{j=N+1}^{N+t-1} a_{t,j}^{l,h}$\nwhere $a_{t,i}^{l,h}$ and $a_{t,j}^{l,h}$ are softmax-ed attention weights assigned to context tokens X and new to-"}, {"title": "2.1 Lookback Lens", "content": "kens Y respectively. The lookback ratio $LR_{t}^{l,h}$ for head h in layer l at time step t is then calculated as:\n$LR_{t}^{l,h} = \\frac{A_{t}^{l,h}(context)}{A_{t}^{l,h}(context) + A_{t}^{l,h}(new)}$\nTo utilize these lookback ratios as input fea- tures in detecting hallucinations, we concatenate the lookback ratios across all heads and layers into a feature vector for the time step t:\n$v_{t} = [LR_{t}^{1,1}, LR_{t}^{1,2}, ..., LR_{t}^{L,H}].$\nGiven a text span of interest {yt, Yt+1, \u2026, Yt+T\u22121}, we average the corresponding lookback ratio vec- tors {vt, Vt+1, ..., Vt+T\u22121} into a single vector \\(\\bar{v}\\). We then employ a logistic regression classifier F to predict if the span is factual (1) or hallucinated (0) based on the averaged lookback ratio vector.\n$P(y = 1|\\bar{v}) = F(\\bar{v}) = \\sigma(w\\bar{v} + b)$,\nwhere \u03c3 denotes the sigmoid function, w is the weight vector, and b is the bias term of the classifier.\nDefining Span The Lookback Lens predicts the probability of hallucinations over spans. We con- sider two ways to obtain spans for a given sequence: predefined spans or sliding window.\n1) Predefined Spans: When the hallucinated and non-hallucinated span annotations are avail- able, we directly train the classifier to differentiate between them. This is a clean setting where all spans are either hallucinated or non-hallucinated.\n2) Sliding Window: In practice, we do not have any predefined spans during decoding, thus we need a sliding window setup that iterates over all possible spans. Specifically, we process the sen- tences into fixed-sized chunks and train the classi- fier to predict a label of 0 if any hallucinated con- tent exists within a chunk, and 1 otherwise. Here,"}, {"title": "2.2 Experimental Setup", "content": "Data Training the Lookback Lens requires labels for hallucinated and non-hallucinated examples. To obtain these examples, we first prompt LLaMA- 2-7B-Chat (Touvron et al., 2023) to greedy de- code responses for 1,000 summarization exam- ples from the CNN/DM dataset (See et al., 2017) and 2,655 QA examples from the Natural Ques- tions (Kwiatkowski et al., 2019) following the setup of Liu et al. (2024). More details are shown in Appendix A. Although being prompted to gener- ate correct responses, the decoded responses will contain both hallucinated and non-hallucinated in- formation as the LLaMA model is still not perfect. Then, we employed GPT-4o (OpenAI, 2024) to ver- ify the truthfulness of these responses and provide span-level annotations on hallucinated segments (detailed prompts in Appendix B). Additionally, we performed a pilot study of human annotation on a subset of 70 examples of the summarization task (details in Appendix C), confirming a 97% consis- tency rate between GPT-4o annotations and human judgments, and validating the reliability of the auto- mated annotations. We show LLaMA-2-7B-Chat's results on both tasks, as evaluated by GPT-4o, in Table 1. The results show that the generated sum- maries from LLaMA-2-7B-Chat still exhibit hallu- cinations about half of the time, highlighting the challenge of summarization tasks.\nBaselines We compare our detection method against several baselines: 1) Text-based entail- ment classifier: We fine-tune the DeBERTa-v3- base (He et al., 2021) model on the same dataset of CNN/DM and NQ as a natural language entailment (NLI) task. Additionally, we include the results from a state-of-the-art entailment model (Vectara, 2023) trained on a huge amount of annotated NLI data (see details in Appendix E. 2) Hidden states- based classifier: We train classifiers using the"}, {"title": "2.3 Results", "content": "Our results are presented in Table 2. We consider both predefined span segmentation and sliding win- dow with a window size of 8. We include the two-fold validation setting on the source task and the out-of-domain transfer setting on the target task, with the tasks either question answering (QA) or summarization (Sum.). We find that the Lookback Lens achieves slightly better performance than the hidden states-based classifier and significantly out- performs the NLI models (SoTA and our impl.). The advantage of the Lookback Lens over the hid- den states-based classifier is more significant in the sliding window settings, as shown in the right-hand side of Table 2.\nAdditionally, we observe that the hidden states- based classifier tends to overfit the training sets during the two-fold validation, and present a sub- stantial performance drop when transferred to out- of-domain tasks. In contrast, Lookback Lens, while not always fitting the training set perfectly, consis- tently exhibits better performance when applied to out-of-domain tasks. This contrast highlights the effectiveness and generalizability of the lookback ratio features we extract from the attention maps."}, {"title": "3 Contextual Hallucinations Mitigation", "content": "To mitigate the impact of contextual hallucinations identified by the Lookback Lens, we introduce a classifier-guided decoding strategy to guide the gen- eration toward more contextually accurate outputs. This approach serves as a robustness test of the Lookback Lens' ability to handle various text gener- ation scenarios. While prior studies on controllable text generation adjust the output probabilities using classifiers based on the output tokens (Yang and Klein, 2021), our method fundamentally differs by not using the tokens themselves but rather their attention maps during generation.\nWe propose Lookback Lens Guided Decoding, which incorporates Lookback Lens (F) into the de- coding process. Since all tokens in the vocabulary share the same attention pattern during one decod-"}, {"title": "3.1 Lookback Lens Guided Decoding", "content": "ing step, F cannot directly influence one-step to- ken choice. Instead, F can evaluate multiple-token chunks, as each chunk causes different attention patterns in multiple decoding steps.\nGiven the context and partially generated text, we independently sample a set of k candidate chunks {C1, C2,...,Ck} at the same decoding step t. For each chunk Cj, the associated lookback ratios are averaged to form a feature vector \\(\\bar{v}_{j}\\). As shown in Figure 2, we select the best candidate C* predicted by F and append to the generation,\n$C^{*} = \\underset{C_{j} \\in {C_{1}, C_{2},..., C_{k}}}{argmax} F(\\bar{v}_{j}).$\nWe repeat this process until it generates the EOS token or reaches the maximum length."}, {"title": "3.2 Experimental Setup", "content": "We evaluate Lookback Lens Guided Decoding on three tasks that involve generating texts condi- tioned on given contexts, including summariza- tion with XSum (Narayan et al., 2018), QA with"}, {"title": "4 Cross-model Transfer", "content": "One benefit of using the lookback ratio to capture higher-level model patterns for hallucination detec- tion is its potential to better transfer across models. A classifier trained with one model's lookback ra- tio could potentially be applied to another model without retraining, provided correlation between"}, {"title": "5 Discussions and Ablations", "content": "In this section, we further conduct various experi- ments and ablation studies on the Lookback Lens and its corresponding classifier guided decoding.\nEffect of Chunk Size In Section 3.3 (Table 3), we experiment with chunk size = 8. Here, we study the effect of varying chunk sizes, from 4, 8, to 16. We see that there is a slight trend that Lookback Lens guided decoding prefers shorter chunk size for NQ and longer chunk size for XSum. However, in general the improvements are consistent across different chunk sizes, thus reducing the need to optimize for chunk sizes.\nPredictive Power of Different Heads In the aforementioned experiments, we utilize all atten- tion heads to train the Lookback Lens. We are thus interested in how the predictive power is distributed among different heads in making predictions. That is, how much performance can we recover if we only utilize a subset of heads? To answer this, we use the coefficients in the linear classifier of the Lookback Lens (in Section 2) to estimate the impor- tance of each head in detecting hallucinations."}, {"title": "6 Related Work", "content": "Hallucinations in LLMs. Simhi et al. (2024) de- fined close-book hallucination vs open-book hal- lucination for settings of relying on parametric knowledge vs knowledge in context. We term open- book hallucination as contextual hallucination for better clarity. Previous studies in hallucinations pri- marily focus on close-book hallucinations (Chen et al., 2023; Min et al., 2023; Chern et al., 2023) and their detection (Azaria and Mitchell, 2023; Simhi"}, {"title": "Classifier guided generation.", "content": "Classifier guided generation aims to control attributes like topic or sentiment in text generation. PPLM (Dathathri et al., 2019) uses gradient ascent to adjust LM prob- abilities via attribute classifiers. FUDGE (Yang and Klein, 2021) uses an attribute predictor on partial sequences to modify LM probabilities. Our method uniquely guides generation using classifiers on at- tention maps, setting it apart from prior approaches."}, {"title": "Self-attention and model behavior.", "content": "The atten- tion mechanism, initially introduced in RNN- based encoder-decoder for neural machine trans- lation (Bahdanau et al., 2015; Luong et al., 2015), was later adopted in the Transformer model's self-attention module (Vaswani et al., 2017), en- abling greater parallelization. Self-attention's in- terpretability has led researchers to use it for un- derstanding model behaviors (Clark et al., 2019; Hao et al., 2021; Vashishth et al., 2019). Our work demonstrates that attention maps in LLMs are effective for detecting contextual hallucinations, providing a lightweight and interpretable solution compared to complex hidden representation meth- ods (Zhang et al., 2024; Chen et al., 2024b)."}, {"title": "7 Conclusion", "content": "We introduce the Lookback Lens, a lightweight clas- sifier designed to detect contextual hallucinations by utilizing the lookback ratio, which is computed solely from attention weights. This classifier not only effectively identifies contextual hallucinations but also mitigates them through Lookback Lens"}, {"title": "Limitations", "content": "Despite the effectiveness of the Lookback Lens and its decoding, there are several limitations to con- sider.\n\u2022 First, the performance upper bound of Look- back Lens Guided Decoding is limited by the sampling capabilities of the LLM itself. If the LLM fails to sample the correct chunk among the eight candidates, the Lookback Lens can- not correct the error.\n\u2022 Second, although the Lookback Lens is a lightweight classifier with negligible inference time, the requirement to sample multiple can- didates from the LLM increases the total in- ference time. We argue that Lookback Lens Guided Decoding is a preliminary approach that demonstrates the feasibility of integrating the Lookback Lens into the decoding process, as well as a robustness test for the Lookback Lens to handle various text generation scenar- ios. However, other options, such as inter- vening in the attention map mechanism based on Lookback Lens signals, could potentially achieve faster inference, and we leave this for future work.\n\u2022 Lastly, the Lookback Lens relies on annotated examples of around 1k-2k to train the classi- fier. While other end-to-end methods (Chuang et al., 2024) can mitigate close-book halluci- nations without training data, they lack inter- pretability due to the absence of a detection step. Nevertheless, we believe that requiring 1,000 annotated examples is a feasible setting."}, {"title": "Ethics Statement", "content": "In this research, we used publicly available datasets and we did not collect any personal information. All datasets and models are used in accordance with their intended use and licenses. Our method is designed to improve the factuality of large lan- guage models (LLMs), which can have a positive impact on various applications, such as question- answering systems, summarization systems, and other applications that rely on LLMs. When de- ployed, however, our approach still carries the is- sues stemming from LLMs, which means that there is a risk that the LLM can produce biased, harmful, or offensive output. Therefore, caution should be exercised before implementing similar approaches in real-world applications."}, {"title": "A Data Creation for Lookback Lens", "content": "Our experimental setup aims to evaluate the ability of Lookback Lens to detect hallucinations in large language models with attention maps. We consider the summarization task and question-answering (QA) task in data creation.\nFor the summarization task, we sampled 1,000 examples from the CNN/DM dataset (See et al., 2017). For QA, we use 2,655 examples from the Natural Questions (Kwiatkowski et al., 2019) from the setup of Liu et al. (2024) to mix the gold docu- ment with irrelevant documents. To keep our focus more on LLM hallucinations rather than being dis- tracted by assessing LLMs' long-context utilization ability, we limited context to three documents per question where the gold document containing the answer was placed in the middle, surrounded by two irrelevant documents.\nWe prompt LLaMA-2-7B-Chat (Touvron et al., 2023) to generate correct responses by greedy de- coding for both tasks to ensure that both halluci- nated and non-hallucinated examples derive from the same source distribution. The max length of generation is set to 256 tokens, or until the EOS token is generated.\nAfter the annotation was collected, we extract hallucinated and non-hallucinated spans, as well as the corresponding attention map lookback ratio, from the LLaMA-2-7B-Chat model, to train the Lookback Lens classifiers.\nIn the predefined span setting, three types of spans are considered as non-hallucinated spans: 1) the text segment before the first hallucinated span in the response 2) the text segment after the last hallucinated span in the response 3) the response annotated as non-hallucinated. All the annotated hallucinated spans are used as negative data to train the Lookback Lens.\nIn the sliding window setting, we consider all the possible fixed sized chunk with size = 8. If a chunk is overlapping with any of the annotated halluci- nated spans, then it is considered as hallucinated, otherwise it is non-hallucinated.\nWhy not use existing data? Initially, we consid- ered using the HaluEval dataset (Li et al., 2023), which was created by prompting GPT-3.5 (OpenAI, 2022) to generate \u201challucinated examples\" against human-annotated non-hallucinated responses, on summarization, QA, and dialogue tasks. However, we have concerns that their method introduces a bias by creating fundamentally different data distri-"}]}