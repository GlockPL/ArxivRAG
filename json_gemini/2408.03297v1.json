{"title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models", "authors": ["Ruizhe Zhang", "Yongxin Xu", "Yuzhen Xiao", "Runchuan Zhu", "Xinke Jiang", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks. However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses. To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors in the intricate and realistic retrieval scenarios. To this end, we propose a Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving controllable knowledge selection in real retrieval scenarios. Concretely, we explore and simulate error types across diverse context combinations and learn how to avoid these negative signals through preference optimization methods. Simultaneously, by adjusting the balance between response length and the proportion of preference data representing different behavior patterns, we enhance the adherence capabilities and noise robustness of LLMs in a balanced manner. Experimental results show that KaPO outperforms previous methods for handling knowledge conflicts by over 37%, while also exhibiting robust generalization across various out-of-distribution datasets.", "sections": [{"title": "I. Introduction", "content": "Large Language Models (LLMs) (Taylor et al. 2022; Zhao et al. 2023b) have been widely applied in various fields, such as natural language processing, question-answering systems, and text generation, giving rise to numerous AI applications (Kaplan et al. 2020; Vu et al. 2024). These models exhibit outstanding performance in many tasks, primarily due to their large-scale parameters and extensive pre-training data (Ziegler et al. 2020; Wang et al. 2023d). However, because of the static nature of the training data, LLMS may generate seemingly coherent but actually unreliable information, a phenomenon known as \"hallucination\u201d (Ji et al. 2023a; Cao et al. 2020; Ji et al. 2023b), due to outdated knowledge and a lack of long-tail knowledge (He, Zhang, and Roth 2022; Kandpal et al. 2023; Jiang et al. 2024).\nTo mitigate the hallucination problem in LLMs, the Retrieval-Augmented Generation (RAG) paradigm has been widely adopted (Izacard et al. 2022; Asai et al. 2023b,a). A common retrieve-and-read framework retrieves relevant information from external knowledge bases, such as Wikipedia, and integrates it with the reader model (i.e., LLMs) to enhance the accuracy of the responses (Gao et al. 2024; Yu et al. 2023a; Lewis et al. 2021). This method leverages rich information from reliable knowledge bases to compensate for the limitations in the internal knowledge of LLMs. However, because the knowledge acquired during LLM pretraining is static, the latest and correct knowledge from external sources can conflict with the parametric knowledge within the model, leading to knowledge conflicts (Xu et al. 2024; Jin et al. 2024). Research indicates that the behavior of LLMs in the face of knowledge conflicts is uncertain, influenced by model parameters and context (Longpre et al. 2022; Xie et al. 2024). In many cases, even when RAG retrieves correct information, LLMs may not necessarily adhere to it when addressing the conflict (Wu, Wu, and Zou 2024). Therefore, enhancing the capability of LLMs to handle knowledge conflicts is a crucial step in improving the performance of RAG.\nIn response to the aforementioned issue, a mainstream approach is to construct specific instruction-tuning datasets to optimize the knowledge prioritization of LLMs in contexts with varying degrees of relevance (Li et al. 2022; Xue et al. 2023). However, due to the lack of explicit contrastive target training, this prioritization has not been deeply integrated into the model's self-awareness, resulting in the probability mass still being concentrated on undesirable responses (Tajwar et al. 2024). In real-world scenarios, the context of RAG is complex, and even with relevant contexts, the model may still select irrelevant information from other content to respond (Shi et al. 2023a; Jiang et al. 2024). Therefore, our insights stem from the error types observed in real-world scenarios involving RAG. We propose introducing negative signals and optimizing knowledge selection in different contexts through preference learning. In the process of learning from human feedback, Direct Preference Optimization (DPO) reparameterizes the reward function to directly fit human preference data. Due to its simplicity and stability, it has been widely adopted and has achieved remarkable results in aligning with human values and intentions. Inspired by this, in this work, we consider utilizing the negative gradient terms in the DPO contrastive objectives to push down the likelihood of undesirable responses in various contexts, further enhancing the adherence capability and noise robustness of LLMs. Although seemingly straightforward, implementing this intuition will encounter the following challenges:\nC1. How to more accurately simulate real-world RAG scenarios and introduce more comprehensive and fine-grained negative signals? At the input level, due to factors such as data storage and retriever performance, the context within the retrieval recall window in actual retrieval environments is highly complex (Yao et al. 2024; Gao et al. 2023). At the output level, comprehensive negative signals are crucial for shifting probability mass from undesired responses to desired responses (Tajwar et al. 2024). Therefore, simulating common error scenarios based on complex input context combinations to construct reasonable chosen-rejected answer pairs is a significant challenge.\nC2. How to avoid preference imbalance issues in preference optimization? When constructing preference datasets, the length imbalance of chosen-rejected response pairs can lead to length utilization issues, where the fine-tuned model tends to generate longer sentences rather than learning preferences for specific behavior patterns (Park et al. 2024a; Meng, Xia, and Chen 2024). Additionally, the imbalance in preference data representing different behavior patterns (e.g., directly answering \"I don't know\" by ignoring the context versus finding answers from the context) can significantly impact the behavior patterns of the fine-tuned model (Liao et al. 2024). In the process of preference optimization, avoiding behavior pattern imbalances caused by data discrepancies is crucial for simultaneously enhancing the model's adherence capability and noise robustness.\nBy jointly considering the above issues, we propose KaPO, a Knowledge-aware Preference Optimization strategy, which constructs comprehensive and balanced preference relations to optimize LLMs' knowledge selection in different contexts. Specifically, our main contributions are summarized as follows:\n\u2022 In the process of constructing the preference dataset, we first simulated real-world RAG scenarios at the input level. We perform refined noise classification based on the relevance between the knowledge context and the question topic, and explore combination methods with evidence to form conflicting context and irrelevant context. At the output level, we simulate two common error types in different context relevance scenarios: Contextual Ignorance and Contextual Overinclusion, and develop training strategies to avoid these errors. This is a general paradigm for constructing knowledge conflict datasets, which can be generalized to various model architectures (Response to Challenge C1).\n\u2022 Secondly, we propose a rewriting strategy to address length imbalance and a data ratio balancing strategy to address behavior pattern imbalance, using DPO to optimize LLMs' adherence capability and noise robustness. These strategies not only eliminate length biases and imbalances in behavior pattern distribution but also enhance the exhaustiveness of the model's responses. This prevents the degradation of conversational abilities that can occur when training on datasets with shorter answers, such as in reading comprehension tasks (Response to Challenge C2).\n\u2022 We validated the training effectiveness of our method on multiple models and datasets and tested its generalization ability in out-of-distribution (OOD) scenarios. The results indicate that our method not only improves the performance of models on test sets but also enhances their adaptability and robustness when confronted with unknown data."}, {"title": "II. Related Work", "content": "Knowledge Conflicts. Numerous studies have explored the behavior of LLMs in knowledge conflict scenarios, providing valuable insights for our work. Longpre et al. (2022) discovered that large Pre-trained Language Models often prefer to ignore contextual information in favor of the parametric knowledge when facing knowledge conflicts. Recently, with the emergence of LLM such as ChatGPT (OpenAI 2022, 2023) and PaLM (Chowdhery et al. 2022), researchers re-examined this issue. Wu, Wu, and Zou (2024) highlighted that this tendency to disregard context is strongly influenced by the model's prior token probability and the deviation degree of the conflicting knowledge. Specifically, conflict knowledge that significantly deviates from the model's existing knowledge is less likely to be adopted, and knowledge with higher prior probabilities is more challenging to update. Kassner and Sch\u00fctze (2019) demonstrated that LLMs are susceptible to being misled by task-irrelevant context. Furthermore, Tan et al. (2024) indicated that the model's contextual preferences are closely related to the semantic completeness of the context and its relevance to the question.\nSeveral studies aim to improve the adherence of LLMs to context when faced with knowledge conflicts. For instance, Knowledge Aware Fine-Tuning (KAFT) (Li et al. 2022) attempts to enhance models' ability to utilize external knowledge by constructing challenging counterfactual knowledge from standard training datasets, as well as incorporating irrelevant knowledge to improve models' noise resistance. However, as previously mentioned, the applicability of this approach in real-world RAG scenarios is limited. Additionally, decoding-based methods (Jin et al. 2024; Chen, Zhang, and Choi 2022), such as Context-Aware Decoding (CAD) (Shi et al. 2023b), adjust the output probabilities of LLMs during token generation in a manner similar to contrastive decoding, conditioned on the relevant context. However, this approach may impact the semantic coherence of long responses. Moreover, prompt-based methods employ sophisticated designed prompts to ensure that LLMs adhere to the provided context (Si et al. 2023; Zhou et al. 2023). Nevertheless, related research indicates that merely modifying prompts does not significantly alter LLM's internal prior token probabilities (Wu, Wu, and Zou 2024), potentially limiting the effectiveness of this approach.\nRetrieval-Augmented Generation. RAG incorporates the external knowledge retrieval component via prompt engineering to achieve more factual consistency, enhancing the reliability and interpretability of LLMs' responses (Lewis et al. 2021). Some studies have made improvements to the retrieve-and-read framework by generating intermediate contexts using the parameter knowledge acquired during the pretraining phase, thereby enhancing the quality of the final response. These intermediate contexts may include commonsense knowledge (Liu et al. 2022), domain-specific knowledge (Luo et al. 2023), and chain-of-thought(COT) reasoning process (Wei et al. 2023; Kojima et al. 2023; Li et al. 2023). Furthermore, Zhao et al. (2023a), Wang et al. (2023a) and Yu et al. (2023b) have utilized retrieved knowledge to edit the intermediate contexts of the COT process, thereby updating conflicting knowledge. However, these intermediate contexts generated by LLMs may contain hallucinations or other inaccurate information, potentially misleading the retrieval or reader models. Additionally, the frequent interactions with LLMs result in inefficiencies in real-world applications (Jiang et al. 2024).\nKnowledge editing. Knowledge editing is a classic method for updating model knowledge, focusing on identifying how models store factual knowledge and designing effective strategies to update parametric knowledge stored in pre-trained models (Cao, Aziz, and Titov 2021; Onoe et al. 2023; Meng et al. 2023). Jang et al. (2022) proposed a continual learning framework aimed at updating outdated knowledge while preserving stable knowledge that is unaffected by temporal changes. However, these strategies may unintentionally affect unrelated parameters or cause inconsistencies within the model's internal knowledge (Pinter and Elhadad 2023; Xu et al. 2024; Wang et al. 2023b). Moreover, in the constantly evolving context of RAG scenarios, the sequential training method for injecting new knowledge proves impractical."}, {"title": "III. Methodology", "content": "A. Task Definition\nGiven an LLM and an input natural language question q, we ask \u0398 to generate a response a = \u0398(q), which represents the parametric knowledge for q. Besides, a typical retrieve-and-read framework can be expressed as y = \u0398(q|\u03c4), where y is the output of \u0398 based on \u03c4. Context \u03c4 is a permutation of $D_j, j = 1, 2, . . ., K$, which represents a set of documents retrieved based on q. Assume S = {ai},i = 1,2,..., N constitutes the set of correct answer, each of which is derived from the retrieved document $D_i$. Note that K is not necessarily equals with N, because some retrieved documents may not contain any answer for q which are known as noises.\nIt's clearly that a and ai are independent. Knowledge conflict appears when a \u2260 S, and at this time response y of \u0398(q|\u03c4) can be uncertain. To simplify the discussion, we limit N to a maximum of 1, which means context \u03c4 contains at most one document $D_1$ from which the answer can be derived. Our purpose is to make sure y = a\u2081 when |S| = 1 and y = a when |S| = 0. In other word, LLM \u0398 should use appropriate external knowledge when there exists a document which contains the necessary knowledge regardless of conflicting with parameter knowledge, while use its parameter knowledge when retrieved documents are all irrelevant.\nB. Contradictory Knowledge\nConstructing knowledge that conflicts with LLM's parameter knowledge is crucial to condition |S| = 1. For question q in RAG scenarios, this conflict is reflected in conflicting answers ai which are inconsistent with LLM's parameter answer a. It is important to note that these conflicting answers a\u017c do not necessarily have to be correct, nor is the LLM's parameter answer a always incorrect. In our approach, both answers can be incorrect to the question as long as they conflict with each other. The key to knowledge conflict lies in the conflict itself, regardless of correctness. This addresses a common misconception in previous work, where researchers often ensured that one answer was correct and the other incorrect (Tan et al. 2024; Wu, Wu, and Zou 2024), which not only increased the difficulty of data filtering but also overlooked some knowledge conflict scenarios.\nSpecifically, we first extract world knowledge acquired during the pretraining phase of the large model, marked as parameter answer a. We encourage LLM to abstain from answering when uncertain and normalize all instances of refusal. Additionally, we refine the response formats for other parametric knowledge. The revised results are presented in Table 1.\nFor a given question q and LLM's parameter answer a, there are two potential sources of conflicting answers. The first source is the realistic answer ar to the question. The second source is a fabricated answer ac generated using GPT-4 that deviates from the realistic answer ar. The latter is often referred to as a counterfactual answer, which we require to be as plausible as possible. Thus, for a question q and LLM's parameter answer a, we can obtain at least one conflicting answer, ensuring that this answer is not excessively far-fetched. The few-shot prompts are structured as follows:\nC. Context Formulation\nIn this section, we illustrated how to formulate context \u03c4 based on different kinds of knowledge conflict.\nTo align with the RAG scenario, we utilized the SQUAD2.0 dataset (Rajpurkar, Jia, and Liang 2018), a reading comprehension dataset encompassing multiple general domains, with a substantial corpus of documents and associated QA tasks. Notably, unlike corpora such as TriviaQA (Joshi et al. 2017), which are collected from Wikipedia, SQUAD2.0 is annotated by humans to determine whether a document can derived to an answer for a specific question. Previous research has highlighted that treating a relevant yet non-informative document as a reference external knowledge source can impair LLM's adherence capabilities (Li et al. 2022). Following the chunk-size commonly used in RAG tasks, we set the length of context \u03c4 to K = 4.\nFor scenarios with |S| = 1, we initially select pertinent documents from SQUAD2.0 based on the conflicting knowledge. For question q and realistic answer ar, we directly select the corresponding document Dr from the original dataset. For question q and counterfactual answer ac, we replace all occurrences of ar with ac in Dr. Subsequently, we select one relevant document on the same topic and two relevant documents on different topics based on semantic similarity. We ensure that these latter three documents are incapable of answering the question q. These four documents are then shuffled to constitute the conflicting context \u03c4.\nFor scenarios with S = 0, we distinguish between hard and easy irrelevant documents. Hard documents, derived from human annotations, consist of two documents that are on related topics but cannot answer the question. Easy documents are randomly selected, consisting of two documents on unrelated topics. These four documents are then shuffled to constitute the irrelevant context \u03c4.\nD. Error Type Analyse\nAs previously mentioned, we expect LLMs to utilize contextual knowledge when encountering conflicting context, while relying on parameter knowledge when faced with irrelevant context. These two modes of handling context reflect the model's adherence capability and noise robustness, respectively. In practical RAG scenarios, deficiencies in these capabilities manifest as two distinct error types: one in which the LLM incorrectly uses irrelevant contextual information to construct answers, termed Contextual Overinclusion; and another where the LLM disregards the context entirely and relies exclusively on its parameter knowledge, termed Contextual Ignorance. These errors can occur with both types of contexts as illustrated in Table 1. To address these issues, we have meticulously designed a dataset comprising positive and negative sample pairs to specifically target and mitigate these errors.\nErrors in Adherence Capability. The ideal behavior of the LLM demonstrating adherence capability, as shown by positive samples, is to answer using the conflicting knowledge present in the context. When contextual overinclusion happens with conflicting context, LLM often utilizes inappropriate information from the context due to insufficient noise robustness or hallucination issues. For instance, in the example presented in Table 1, LLM chooses noisy information marked in red. To address this error, we constructed negative samples by using a prompt mechanism to guide GPT-4 to generate incorrect answers from conflicting contexts. To ensure the quality of the generated data, we adhered to stringent validation criteria: (1) The generated answers must be derived from the context, ensuring that the error is unequivocally attributable to contextual overinclusion; (2) The generated answers should be as plausible as possible and distinctly different from the conflicting answers, thereby ensuring the high quality of the data.\nWhen contextual ignorance occurs in conflicting context, LLM may either fail to recognize the utility of the context or, even upon recognizing it, may opt to disregard the conflicting answer in favor of relying on its parameter knowledge. For instance, in the example shown in Table 1, LLM answers the question without utilizing supplemental knowledge. To simulate this error, we constructed negative samples by extracting LLM's response to the query in the absence of any contextual support.\nErrors in Noise Robustness. It is evident that positive sample for noise robustness is to use LLM's parameter knowledge to respond when encountering irrelevant contexts. When contextual overinclusion occurs in irrelevant contexts, LLM may fail to recognize the context as irrelevant or may be influenced by hallucinations, leading it to use information from the context instead of disregarding it. Similar to contextual overinclusion in conflicting contexts, we constructed corresponding negative samples by using GPT-4 to extract incorrect answers from irrelevant contexts. The prompts used to generate contextual overinclusion and contextual ignorance are structured as follows:\nE. Training Method\nOur training consists of two phases. First, we perform instruction tuning using the conflicting knowledge and contexts constructed in Section B and Section C to enhance the LLM's adherence capability and noise robustness in RAG task scenarios. Next, we utilize the preference dataset constructed in Section D for DPO training to further improve the LLM's ability to avoid the two types of errors, while ensuring that its final responses align with user preferences.\nInstruction Tuning. Instruction tuning is a multi-task learning framework that enables the use of human-readable instructions to guide the output of LLMs. Given a source text and task-specific instructions, the model is trained to generate a sequence of tokens representing the desired output structure and its corresponding labels. Reviewing our definition of adherence capability and noise robustness, we would like to get a finetuned model Oft from original LLM \u0398 that satisfies the following criteria:\n$|S| = 1$: Oft(q|\u03c4) = a1, where D\u2081 \u2192 a1\n$|S|= 0$: Oft(q|\u03c4) = a, where \u0398(q) = \u03b1\nNote that although the presence of the answer in \u03c4 was distinguished during dataset construction, the LLM does not possess this prior knowledge. The model must independently determine the context type and formulate a response during the RAG task. The instruction prompts we used are outlined below:\nDirect Preference Optimization. As previously discussed, LLMs may exhibit errors contextual overinclusion and contextual ignorance in real-world RAG scenarios. To further enhance adherence capability and noise robustness, we propose a Knowledge-aware Preference Optimization(KaPO) training strategy. This strategy employs three types of preferences between positive and negative samples in two different contextual settings to conduct DPO training on the LLM. Details of preference pairs construction can be found in Section D. Using this approach, we train the LLM to avoid these errors and improve its ability to utilize different contexts.\nDuring the selection of data ratios for DPO, we also identified two preference imbalances that impact training effectiveness.\n\u2022 Length Imbalance. Some studies suggest that reward hacking observed in RLHF can also negatively impact DPO training (Gao, Schulman, and Hilton 2022; Park et al. 2024b). We observed that in our previously constructed dataset, for the same preference pair, the positive sample was often the better-formatted and longer response, while the negative sample was a shorter conflicting answer. Due to the tendency of LLMs to be influenced by length bias during DPO (Singhal et al. 2024), they might prefer generating longer responses, which overall manifests as a greater tendency to refuse answering rather than providing a conflicting answer. To mitigate this issue, we standardized the format for all positive and negative samples, aligning their lengths to ensure that the average length $len_{win}$ approximately equals $len_{loss}$.\n\u2022 Error Type Imbalance. Given that the preference pairs associated with error contextual overinclusion in irrelevant context exhibit a tendency towards \"rejecting the use of contextual knowledge\u201d, while the preference pairs related to error contextual ignorance in conflicting context guide the LLM to \u201cutilize contextual knowledge without rejecting it\", we realized that the ratio of these contrasting preference pairs could significantly influence training efficacy. During KaPO training, we ensured that the proportion $R_{error}$ of these two types of data was maintained at approximately 1:1. Furthermore, we validated the importance of this ratio $R_{error}$ in subsequent experiments.\""}, {"title": "IV. Experiments", "content": "In this section, we conduct a series of experiments on three datasets to answer the following research questions:\n\u2022 RQ1 (Section B): Does KaPO outperform other approaches for resolving knowledge conflict across various base models and datasets?\n\u2022 RQ2 (Section C): What impact does each training phase and different knowledge types have on the overall performance?\n\u2022 RQ3 (Section D): Does KaPO training conducted in general domains remain effective in out-of-distribution (OOD) scenarios?\n\u2022 RQ4 (Section E): How sensitive is KaPO to hyperparameters data ratio Rerror and context length windows K?\nA. Experimental Setup\nDatasets We constructed the KaPO training dataset based on SQUAD 2.0 (Rajpurkar, Jia, and Liang 2018). The test datasets comprise the following three types: (1) SQUAD 2.0-Eval, a validation set partitioned using the same construction method. (2) Open-source counterfactual datasets: RGB (Chen et al. 2023) and KNOT (Liu et al. 2024) are two general-domain QA datasets containing counterfactual knowledge and contexts. We augmented these datasets with irrelevant contexts for testing purposes. Notably, RGB is a Chinese dataset. (3) Domain-specific dataset: CMB (Wang et al. 2023c) is a multi-task QA dataset in the medical domain, encompassing 269,359 questions across four clinical medicine specialties of physicians, nurses, medical technicians, and pharmacists. Due to quantity constraints, we Given the extensive size of the CMB dataset, we randomly sample 4,000 questions for testing.\nBase Model We selected the following two types of LLM as the base model and explored the gains brought by KaPO: Baichuan7B-chat (Yang et al. 2023) and Llama2-13B-chat (Touvron et al. 2023).\nCompared Methods In order to explore the advantages of the KaPO, we compare the KaPO results against four other models: (1) Base Model (Base) answers user questions based on supplementary external knowledge, which can be considered a fundamental retrieve-and-read framework under RAG (Lewis et al. 2021). We use Baichuan7B-chat and Llama2-13B-chatas base models. (2) Prompt-based Method (Prompt) employs meticulously designed prompts to enhance the model's capability to adhere to external knowledge (Zhou et al. 2023). (3) Finetuning: KAFT (Li et al. 2022) employs instruction fine-tuning to improve the LLM's adherence to contexts of varying relevance. (4) Decode-Based Method: CD2 uses a contrastive decoding-like method to adjust the probabilities of output tokens. (5) Chain-of-Thought: Chain of Thought (COT) (Wei et al. 2023) is a common method to enhance the performance of LLMs in downstream tasks. COT-VE (Zhao et al. 2023a) extends COT by guiding LLM to identify conflicting knowledge and modify its responses accordingly.\nMetrics We designed statistical metrics to evaluate the two capabilities of LLMs. For adherence capability, we utilized the conflicting contexts from the test set as supplementary knowledge, measuring the proportion $R_{Ad}$ of LLM responses that align with the conflicting knowledge within these contexts. For the RGB and KNOT datasets, the conflicting knowledge exclusively consists of counterfactual knowledge. For noise robustness, we employed the irrelevant contexts from the test set as supplementary knowledge, examining the proportion $R_{RO}$ of LLM responses that correspond with the model's parameter knowledge.\nB. Performance Comparison\nTo answer RQ1, we conduct experiments and report results of the two metrics on Squad2.0-Eval, RGB and KNOT with two LLM turbos, as illustrated in Table2. From the reported results, we can find the following observations:\nComparison of KaPO and other methods. Firstly, it is evident that our mothed, KaPO, outperforms the baseline methods across all metrics. For instance, the $R_{Ad}$ and $R_{RO}$ scores see an improvement of approximately 37.07%-85.33% and 80.91%-295.61% for the Squad2.0-Eval dataset with Baichuan2-7B-Chat. Moreover, compared to KAFT, KaPO uses more complicated contexts and comprehensive negative signals to enhance LLM's adherence capability and noise robustness."}, {"title": "V. Conclusion", "content": "In this paper, we propose KaPO, a Knowledge-aware Preference Optimization strategy to enhance LLM's adherence capability and noise robustness to external knowledge. Specifically, we abstract and simulate two common error types in scenarios with varying contextual relevance: Contextual Ignorance and Contextual Overinclusion. Based on instruction-tuning, we utilize negative gradient terms in the DPO comparative objectives to reduce the likelihood of undesired responses. Furthermore, by aligning data lengths and balancing data ratios, we effectively mitigate preference imbalances inherent in DPO. Experimental evaluations across diverse datasets and two base models substantiate the efficacy and generalization capability of KaPO. In the future, we will explore how the composition and proportion of different types of contexts affect the ability of LLMs to utilize external knowledge."}]}