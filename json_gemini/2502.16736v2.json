{"title": "AUKT: Adaptive Uncertainty-Guided Knowledge Transfer with Conformal Prediction", "authors": ["Rui Liu", "Peng Gao", "Yu Shen", "Ming Lin", "Pratap Tokekar"], "abstract": "Knowledge transfer between teacher and student models has proven effective across various machine learning applications. However, challenges arise when the teacher's predictions are noisy, or the data domain during student training shifts from the teacher's pretraining data. In such scenarios, blindly relying on the teacher's predictions can lead to suboptimal knowledge transfer. To address these challenges, we propose a novel and universal framework, Adaptive Uncertainty-guided Knowledge Transfer (AUKT), which leverages Conformal Prediction (CP) to dynamically adjust the student's reliance on the teacher's guidance based on the teacher's prediction uncertainty. CP is a distribution-free, model-agnostic approach that provides reliable prediction sets with statistical coverage guarantees and minimal computational overhead. This adaptive mechanism mitigates the risk of learning undesirable or incorrect knowledge. We validate the proposed framework across diverse applications, including image classification, imitation-guided reinforcement learning, and autonomous driving. Experimental results consistently demonstrate that our approach improves performance, robustness and transferability, offering a promising direction for enhanced knowledge transfer in real-world applications.", "sections": [{"title": "1. Introduction", "content": "In many machine learning applications, knowledge transfer from teacher models to student models is a valuable strategy that allows improved performance and practical deployment in real-world scenarios. The teacher models, composed of larger networks or access to multi-modalities, often exhibit superior performance but come with significant computational costs, making them unsuitable for resource-constrained environments. In contrast, student models are more lightweight or designed to work with reduced data modalities during testing, making them practical for real-world applications. The teacher-student framework allows the teacher models to provide predictions or insights that guide the student models' learning process. This can involve transferring knowledge from a larger network to a smaller one, adapting multimodal teacher models to unimodal student models, or using pretrained imitation learning (IL) policies to bootstrap reinforcement learning (RL) agents.Traditional teacher-student frameworks often assume that the teacher's predictions are always reliable, using them as guidance for the student's learning. However, a key challenge arises when domain shifts occur between the data used for pretraining the teacher and the data available for training the student. For instance, in transferring knowledge from large language models (LLMs) to student models for downstream tasks, the student's domain data may differ significantly from the pretraining data of the LLMs. Addressing this disparity typically requires retraining or fine-tuning the teacher model on the student's domain, but this process might be computationally expensive or impractical. For pretrained teacher models, domain shifts can introduce error in the teachers' predictions, undermining their reliability. Blindly relying on the teacher in these scenarios will lead to suboptimal knowledge transfer, as the student may overfit to incorrect or uncertain guidance. This over-reliance restricts the student's ability to explore independently and adapt effectively to new or noisy environments. These limitations are especially problematic in real-world applications, where dynamic and uncertain conditions are commonplace. Addressing such issues is crucial for improving the performance and transferability of knowledge transfer systems.Conformal prediction (CP) is a non-parametric, distribution-free, and model-agnostic framework that provides reliable prediction sets with statistical coverage guarantees, enabling robust decision-making under data or model uncertainty. Leveraging CP, we propose a novel framework, Adaptive Uncertainty-Guided Knowledge Transfer (AUKT), to address key challenges in knowledge transfer systems. AUKT quantifies the teacher's prediction uncertainty and uses it to dynamically determine the extent to which the student should rely on the teacher's guidance. Specifically, when the teacher exhibits high confidence in its predictions, the student prioritizes the teacher's guidance. Conversely, when the confidence is low, the student reduces its reliance on the teacher and explores the data more independently, potentially discovering new patterns or adapting better to noisy or domain-shifted data. AUKT is a general framework applicable to diverse teacher-student architectures, including model compression (larger to smaller networks), modality reduction (multimodal to unimodal networks), and imitation-to-reinforcement learning (pretrained policies to RL agents). We validate AUKT across diverse tasks, including image classification, imitation-guided reinforcement learning, and autonomous driving.Here, we ask the question: Can we improve the student model's performance beyond standard knowledge transfer methods when there are domain shifts between the teacher's pretraining data and the student's training data, or when the teacher's predictions are noisy? Additionally, there can be cases of teacher underperformance where the pretrained teacher model performs worse than the student model trained from scratch on the student domain data, likely due to significant domain shifts. In such cases, we investigate: Can the teacher model still provide useful knowledge to improve the student model's performance beyond what is achievable when training from scratch? This investigation especially differentiates AUKT from previous knowledge transfer methods, which often assume that the teacher's performance exceeds that of the student model.Unlike traditional teacher-student frameworks, our approach offers several unique advantages. We summarize the key contributions of our work as follows:\n\u2022 We propose AUKT, a novel framework that leverages conformal prediction to efficiently quantify the uncertainty in the teacher's predictions for knowledge transfer systems. By adaptively adjusting the teacher's influence based on its confidence, AUKT ensures effective student learning without over-relying on unreliable guidance, particularly in domain-shifted scenarios.\n\u2022 AUKT demonstrates the ability to still leverage useful \"dark knowledge\" from the teacher model to improve student performance, even when the teacher is underperforming due to significant domain shifts. This is a notable distinction from conventional knowledge transfer methods, which assume that the teacher always performs better than the student.\n\u2022 AUKT is universal and domain-agnostic, making it applicable to diverse tasks. We validate AUKT across a range of applications, including image classification, imitation-guided reinforcement learning and autonomous driving, demonstrating improvements in performance, as well as enhanced robustness and transferability compared to conventional knowledge transfer methods."}, {"title": "2. Related Work", "content": "2.1. Knowledge Transfer\nKnowledge transfer methods aim to adapt information from one network, modality, or learning paradigm to another. For instance, Hinton (2015); Jin et al. (2023); Sun et al. (2024) focused on transferring soft probabilities from the teachers' logits to guide student models, whereas Romero et al. (2014); Zagoruyko & Komodakis (2016); Passalis & Tefas (2018); Kim et al. (2018) emphasized transferring intermediate features from the teacher to the student. These approaches primarily address knowledge transfer from larger teacher networks to smaller student networks. Cross-modality transfer has also been extensively explored. Wang et al. (2023) introduced a prototype-based distillation method to handle missing modalities in medical image segmentation, where a multi-modality teacher guides a single-modality student. Similarly, Feng et al. (2023) distilled knowledge from an RGB-Thermal teacher model to a Thermal-only student model for semantic scene understanding. Shen et al. (2023) proposed Auxiliary Modality Learning (AML), where a teacher model with access to multiple modalities transfers knowledge to a student model that operates on reduced modalities during testing. Additionally, knowledge transfer across different learning paradigms has been investigated, such as in (Hu et al., 2023; Bhaskar et al., 2024), where pretrained imitation learning (IL) policies were used to guide reinforcement learning (RL) agents.\nA limitation of most existing knowledge transfer frameworks is their reliance on static guidance, which assumes the teacher's predictions are consistently reliable and always superior to the student's. This assumption often fails under significant domain shifts, where the teacher's predictions may become unreliable, ultimately hindering the student's learning. In contrast, AUKT introduces a principled approach to uncertainty-aware guidance that adaptively adjusts the teacher's influence based on its prediction uncertainty, ensuring that the student learns effectively without over-relying on potentially misleading guidance.\n2.2. Uncertainty-Aware Learning\nQuantifying uncertainty in machine learning systems has become a critical aspect, especially in safety-critical domains. For example, Edupuganti et al. (2020) quantified uncertainty in MRI reconstruction with deep learning models, while Kwon et al. (2020) employed Bayesian neural networks for uncertainty estimation in medical image classification. Similarly, Michelmore et al. (2020) evaluated uncertainty in end-to-end Bayesian controllers for autonomous driving, (Gao et al., 2023; Gao & Zhang, 2021) leverage uncertainty quantification in human-robot interaction, and Zhao et al. (2024) utilized Monte Carlo dropout for real-time uncertainty quantification in object detection for autonomous vehicles.\nDespite the progress in uncertainty-aware learning, most prior works focus on quantifying uncertainty in standalone models, with less exploration of its integration into teacher-student knowledge transfer frameworks. In this context, uncertainty remains underexplored as a mechanism to modulate the interaction between teacher and student models dynamically. To bridge this gap, our approach leverages conformal prediction as a principled mechanism for uncertainty quantification. By integrating uncertainty awareness into the learning process, our method adaptively adjusts the teacher's guidance, enabling the student to learn effectively while reducing the risk of overfitting to unreliable teacher predictions.\n2.3. Conformal Prediction\nConformal prediction (CP) is a non-parametric, distribution-free, and model-agnostic framework designed to provide reliable prediction sets with statistical coverage guarantees. In machine learning systems, CP has primarily been utilized for post-hoc uncertainty calibration due to its computational efficiency. For instance, Angelopoulos et al. (2020) introduced an algorithm that adapts any image classifier to output predictive sets containing the true label with a user-specified probability. Mossina et al. (2024) proposed a computationally lightweight approach to quantify predictive uncertainty in semantic image segmentation using CP. Similarly, Lu et al. (2022) applied CP to deep learning models for grading the severity of spinal stenosis in lumbar spine MRI, while Karimi & Samavi (2023) leveraged CP to measure uncertainty in deep learning.\nDespite these advancements, the application of CP in dynamic decision-making frameworks, such as adaptive knowledge transfer, remains underexplored. In this work, we extend CP to the domain of knowledge transfer, utilizing it as a foundation for adaptive, uncertainty-guided learning. By quantifying the teacher model's prediction uncertainty, we enable the student model to dynamically adjust its reliance on the teacher, effectively balancing the integration of pretrained knowledge with independent exploration."}, {"title": "3. Approach", "content": "3.1. Preliminaries\nIn our framework, we leverage Conformal Prediction (CP) to quantify the teacher's prediction uncertainty and guide knowledge transfer. CP is a distribution-free method that provides prediction sets or intervals with guaranteed coverage levels, independent of the underlying model (Angelopoulos & Bates, 2021; Shafer & Vovk, 2008).\nCP uses a nonconformity score $s$ to measure how unusual a prediction is for a new test input, based on a calibration set, a held-out dataset used to compute the empirical distribution of nonconformity scores. The score $s$ can be defined in various ways, such as residuals (e.g., $|\\bar{y} - \\hat{y}(x)|$ for regression) or confidence scores (e.g., $1 - p(\\hat{y}(x))$ for classification), where $y$ is the ground truth and $\\hat{y}(x)$ is the model's prediction for an input in the calibration set. Additional examples can be found in (Angelopoulos & Bates, 2021; Shafer & Vovk, 2008). From the calibration set, the $(1 - \\alpha)$ quantile $q_{1-\\alpha}$ of the nonconformity scores is computed, representing the threshold below which $1-\\alpha$ of the data falls, where $\\alpha$ is the allowable error rate. A prediction set for a test input $x_{test}$ includes all labels $y$ satisfying $s(x_{test}, y) \\leq q_{1-\\alpha}$, ensuring a coverage probability of $1 - \\alpha$, assuming the calibration and test data are from the same distribution (Angelopoulos & Bates, 2021).\n3.2. Problem Definition\nWe consider a knowledge transfer problem where a student model learns from a teacher model. Conventional knowledge transfer methods often assume consistently superior teacher performance. However, knowledge transfer becomes challenging when domain shifts occur, leading to uncertainty in the teacher model's prediction and suboptimal knowledge transfer.\nTo address these challenges, we propose an Adaptive Uncertainty-Guided Knowledge Transfer (AUKT) framework to improve the robustness and transferability of knowledge transfer systems. Formally, let the teacher domain data be denoted as $D_T$ with distribution $P_T$, and the student domain data as $D_S$ with distribution $P_S$. We represent the teacher model pretrained on $D_T$ as $f_T : X \\rightarrow Y$, and the student model as $f_S : X \\rightarrow Y$, where $X$ is the input space and $Y$ is the output space. In this framework, we do not retrain or finetune the teacher model $f_T$ on the student domain $D_S$, as this can be computationally prohibitive. Instead, we aim to transfer the knowledge from $f_T$ to $f_S$, with the goal of improving the performance of $f_S$ on $D_S$ compared to conventional knowledge transfer approaches or training the student model from scratch. This is particularly important when $D_S$ differs from $D_T$ (i.e., $P_S \\neq P_T$), making $f_T(x)$ uncertain for input $x \\in D_S$.\n3.3. Adaptive Uncertainty-Guided Knowledge Transfer\nOur approach aims to improve knowledge transfer by leveraging the prediction uncertainty of the teacher model to guide the student's learning process, especially in the presence of domain shifts. Given a student domain $D_S$, we split it into three subsets: the student training set $D_S^{train}$, used to train the student model $f_S$; the calibration set $D_{cal}$, used to transform any heuristic measure of uncertainty from the pretrained teacher model $f_T$ into a rigorous one; and the testing set $D_{test}$ for validate the model performance. The student training set $D_S^{train}$ also serves as the testing set for $f_T$ in CP-based uncertainty quantification. This is because in the knowledge transfer process, both the teacher and student models take the same input data, with the student learning from the teacher's predictions. We assume that $D_S^{train}$ and $D_{cal}$ come from the same distribution, satisfying the assumption of CP. AUKT is a general framework, we show how to use it in both supervised and imitation-guided reinforcement learning settings with minimal changes.\n3.3.1. Supervised Learning\nGiven a pretrained teacher model $f_T$ and the calibration set $D_{cal}$, for each sample $(x_i, y_i) \\in D_{cal}$, we compute a nonconformity score $s_i$, where $s_i$ can be residuals (e.g., $|y_i -\\hat{y}(x_i)|$ for regression) or confidence scores (e.g., $1 - P_{\\hat{y}(x_i)}$ for classification), as described in Section 3.1, depending on the task. Using a predefined coverage level $1 - \\alpha$, we compute the $(1 - \\alpha)$-quantile of these scores, denoted as\n$q_{1-\\alpha} = \\text{Quantile}_{1-\\alpha}(s_1, s_2, ..., s_{|D_{cal}|})$,\nwhich serves as a threshold for constructing prediction sets. For a test input $x_{test}$, we define the prediction set for the teacher model $f_T$ as: $C(x_{test}) = \\{y \\in Y : s(x_{test}, y) \\leq q_{1-\\alpha}\\}$, with the probability of the true label $y_{test}$ falling within $C(x_{test})$ satisfies $P(y_{test} \\in C(x_{test})) \\geq 1 - \\alpha$. To quantify uncertainty, we use the size of the prediction set $C(x)$ for an input $x \\in D_S^{train}$. Specifically, we define the teacher model's uncertainty as:\n$u_T(x) = g(|C(x)|),$ (1)\nwhere $g$ is a function that maps the size of the prediction set (e.g., the number of classes for classification or the interval length for regression) to an uncertainty value. Then we define the loss function of the adaptive uncertainty-guided knowledge transfer, which is\n$L = \\lambda_1L_S + w(x) \\cdot \\lambda_2L_T,$(2)\nwhere $L_S$ is the student's task loss (e.g., Cross Entropy loss), $L_T$ is the teacher guidance loss (e.g., KL divergence between student and teacher logits), $\\lambda_1$ and $\\lambda_2$ are the coefficients, and $w$ is an uncertainty-based weight derived from the teacher model's uncertainty $u_T$:\n$w(x) = h(u_T(x)),$(3)\nwhere $h$ is a function mapping uncertainty to the weight $w$. $w$ approaches 1 when the teacher's predictions are confident and decreases toward 0 as the uncertainty increases. We show the algorithm of AUKT in the Appendix A.1. We also provide a theoretical analysis to explain why AUKT outperforms standard knowledge transfer methods, please see the Appendix A.2.\nOur approach dynamically adjusts the student's reliance on the teacher based on prediction uncertainty. When the teacher is confident, the student follows its guidance, but as uncertainty rises, the student explores more independently. This balance between exploiting teacher knowledge and encouraging exploration helps the student avoid over-reliance"}, {"title": "3.3.2. Imitation-Guided Reinforcement Learning", "content": "Consider a Markov Decision Process (MDP) defined by the tuple $\\{S, A, P, R, \\gamma\\}$, where $S$ is the state space, $A$ is the action space, $P$ is the transition dynamics, $R$ is the reward function, and $\\gamma$ is the discount factor. We focus on off-policy RL methods due to their higher sample efficiency, leveraging past experiences and expert demonstrations.\nTo quantify uncertainty, we define a nonconformity score $s(s, a) = -\\log \\pi(a|s)$, measuring how \u201catypical\u201d an action $a$ is in state $s$ for a policy $\\pi$. Given a teacher model $f_T$ (e.g., a pretrained imitation policy), we first collect calibration set $D_{cal} = \\{(s_i, a_i)\\}_{i=1}^{|D_{cal}|}$ by rolling out $f_T$ in the student domain. For a predefined coverage level $1 - \\alpha$, we compute the $(1-\\alpha)$-quantile $q_{1-\\alpha}$ of the nonconformity scores on $D_{cal}$. For a test state $s_{test}$, we construct a conformal prediction set $C(s_{test}) = \\{a \\in A : s(s_{test}, a) \\leq q_{1-\\alpha}\\}$, ensuring $P(a_{test} \\in C(s_{test})) \\geq 1 - \\alpha$. Unlike prior setups where only the teacher model is calibrated, here we calibrate both teacher model $f_T$ and student model $f_S$. Since $f_T$ is pretrained, its quantile $q_{f_\\alpha}$ remains static, while the student's quantile $q_\\alpha$ is updating online during training. Using the corresponding prediction sets, the teacher and student uncertainties are defined as $u_T(s) = g(|C_T(s)|)$ and $u_S(s) = g(|C_S(s)|)$, respectively.\nSimilar to supervised learning, the total loss function is defined as: $L = L_S + w(s) \\cdot L_T$, where $L_S$ is the student's task loss (e.g., $\\mathbb{E} [-\\log f_S(a|s) \\cdot A(s, a)]$ with $A(s, a)$ as the advantage function), which corresponds to the RL objective. $L_T = \\mathbb{E} [KL(f_S(\\cdot|s) || f_T(\\cdot|s))]$ is the KL divergence between the student's policy and the teacher's, serving as the teacher guidance loss. The weight $w$, defined as: $w(s) = h(u_T(s), u_S(s))$, is similar to Eq. 3 but incorporates the uncertainties of both teacher and student."}, {"title": "4. Experiments", "content": "To validate our approach, we conduct experiments across a diverse range of applications, including image classification, imitation-guided reinforcement learning, and autonomous driving.\n4.1. Image Classification\nIn image classification, we evaluate the effectiveness of our framework in enhancing predictive performance compared to traditional knowledge transfer approaches. This evaluation is performed under different levels of domain shifts and noise. Specifically, we aim to address the two primary research questions: (1) Can we improve the performance of the student model beyond standard knowledge transfer techniques? (2) When the teacher model is underperformance, can it still provide useful \u201cdark knowledge\u201d to enhance the performance of the student model beyond what is achievable by training from scratch?\nExperimental Settings. We conduct our experiments on the CIFAR-100 dataset, which consists of 60K images, 50K for training and 10K for testing, across 100 distinct categories. In this section, we focus on knowledge transfer under two distinct settings: Homogeneous Structure, where both the teacher and student models share the same type of architecture (e.g., ResNet-32x4 and ResNet-8x4), and Heterogeneous Structure, where the teacher and student models are of different architectures (e.g., ResNet-32x4 and ShuffleNet-V1). We evaluate a wide range of neural network architectures, including ResNet, WRN, VGG, ShuffleNet-V1/V2, and MobileNet-V2. We introduce two levels of domain shifts. In Level 1, the domain shift is relatively mild with Gaussian noise of zero mean and a standard deviation of 0.03. In Level 2, the shift is more pronounced with noise standard deviation increased to 0.05, which may lead to underperformance of the teacher model, where its performance is worse than that of the student model trained from scratch on the student training set. For further details on these two levels of domain shifts, please refer to the Appendix A.3.1.\nTo quantify the uncertainty of the pretrained teacher model $f_T$ using conformal prediction, we utilize the RAPS algorithm as described in (Angelopoulos et al., 2020) with an error rate of $\\alpha = 0.1$. For more details on the RAPS algorithm, please refer to (Angelopoulos et al., 2020). Given an input image $x$, we obtain the prediction set $|C(x)|$ where the teacher model provides a set of candidate predictions. The prediction uncertainty of $f_T$ in Eq. 1 is then defined as $u_T(x) = \\frac{|C(x)| - 1}{K-1}$, where $K$ is the total number of classes, which is 100 for the CIFAR-100 dataset. From this uncertainty definition, $u_T$ takes values between 0 and 1. If $f_T$ is confident with its prediction with a single class, $u_T$ will be 0. Conversely, if the prediction is uncertain and contains multiple classes in the prediction set, $u_T$ is larger than 0. To incorporate this uncertainty into the knowledge transfer process, we define the uncertainty-based weight in Eq. 3 as $w = 1$ if $u_T = 0$, $w = 0$ if $u_T > 0$. This weight helps adjust the influence of the teacher's prediction on the student model, ensuring that confident predictions are more heavily relied upon while uncertain predictions have less influence. We follow the same experimental settings as in previous work for the coefficients $\\lambda_1$ and $\\lambda_2$, as well as other training details. Please refer to the Appendix A.3.3 for more details."}, {"title": "3.3. Imitation-Guided Reinforcement Learning", "content": "on uncertain predictions and generalize effectively, particularly in cases of domain shifts where the teacher's pretrained knowledge may not transfer well. This ensures robust and reliable learning outcomes.\n3.3.2. IMITATION-GUIDED REINFORCEMENT LEARNING\nConsider a Markov Decision Process (MDP) defined by the tuple $\\{S, A, P, R, \\gamma\\}$, where $S$ is the state space, $A$ is the action space, $P$ is the transition dynamics, $R$ is the reward function, and $\\gamma$ is the discount factor. We focus on off-policy RL methods due to their higher sample efficiency, leveraging past experiences and expert demonstrations.\nTo quantify uncertainty, we define a nonconformity score $s(s, a) = -\\log \\pi(a|s)$, measuring how \u201catypical\u201d an action $a$ is in state $s$ for a policy $\\pi$. Given a teacher model $f_T$ (e.g., a pretrained imitation policy), we first collect calibration set $D_{cal} = \\{(s_i, a_i)\\}_{i=1}^{|D_{cal}|}$ by rolling out $f_T$ in the student domain. For a predefined coverage level $1 - \\alpha$, we compute the $(1-\\alpha)$-quantile $q_{1-\\alpha}$ of the nonconformity scores on $D_{cal}$. For a test state $s_{test}$, we construct a conformal prediction set $C(s_{test}) = \\{a \\in A : s(s_{test}, a) \\leq q_{1-\\alpha}\\}$, ensuring $P(a_{test} \\in C(s_{test})) \\geq 1 - \\alpha$. Unlike prior setups where only the teacher model is calibrated, here we calibrate both teacher model $f_T$ and student model $f_S$. Since $f_T$ is pretrained, its quantile $q_{f_\\alpha}$ remains static, while the student's quantile $q_\\alpha$ is updating online during training. Using the corresponding prediction sets, the teacher and student uncertainties are defined as $u_T(s) = g(|C_T(s)|)$ and $u_S(s) = g(|C_S(s)|)$, respectively.\nSimilar to supervised learning, the total loss function is defined as: $L = L_S + w(s) \\cdot L_T$, where $L_S$ is the student's task loss (e.g., $\\mathbb{E} [-\\log f_S(a|s) \\cdot A(s, a)]$ with $A(s, a)$ as the advantage function), which corresponds to the RL objective. $L_T = \\mathbb{E} [KL(f_S(\\cdot|s) || f_T(\\cdot|s))]$ is the KL divergence between the student's policy and the teacher's, serving as the teacher guidance loss. The weight $w$, defined as: $w(s) = h(u_T(s), u_S(s))$, is similar to Eq. 3 but incorporates the uncertainties of both teacher and student."}, {"title": "4. Experiments", "content": "To validate our approach, we conduct experiments across a diverse range of applications, including image classification, imitation-guided reinforcement learning, and autonomous driving.\n4.1. Image Classification\nIn image classification, we evaluate the effectiveness of our framework in enhancing predictive performance compared to traditional knowledge transfer approaches. This evaluation is performed under different levels of domain shifts and noise. Specifically, we aim to address the two primary research questions: (1) Can we improve the performance of the student model beyond standard knowledge transfer techniques? (2) When the teacher model is underperformance, can it still provide useful \u201cdark knowledge\u201d to enhance the performance of the student model beyond what is achievable by training from scratch?\nExperimental Settings. We conduct our experiments on the CIFAR-100 dataset, which consists of 60K images, 50K for training and 10K for testing, across 100 distinct categories. In this section, we focus on knowledge transfer under two distinct settings: Homogeneous Structure, where both the teacher and student models share the same type of architecture (e.g., ResNet-32x4 and ResNet-8x4), and Heterogeneous Structure, where the teacher and student models are of different architectures (e.g., ResNet-32x4 and ShuffleNet-V1). We evaluate a wide range of neural network architectures, including ResNet, WRN, VGG, ShuffleNet-V1/V2, and MobileNet-V2. We introduce two levels of domain shifts. In Level 1, the domain shift is relatively mild with Gaussian noise of zero mean and a standard deviation of 0.03. In Level 2, the shift is more pronounced with noise standard deviation increased to 0.05, which may lead to underperformance of the teacher model, where its performance is worse than that of the student model trained from scratch on the student training set. For further details on these two levels of domain shifts, please refer to the Appendix A.3.1.\nTo quantify the uncertainty of the pretrained teacher model $f_T$ using conformal prediction, we utilize the RAPS algorithm as described in (Angelopoulos et al., 2020) with an error rate of $\\alpha = 0.1$. For more details on the RAPS algorithm, please refer to (Angelopoulos et al., 2020). Given an input image $x$, we obtain the prediction set $|C(x)|$ where the teacher model provides a set of candidate predictions. The prediction uncertainty of $f_T$ in Eq. 1 is then defined as $u_T(x) = \\frac{|C(x)| - 1}{K-1}$, where $K$ is the total number of classes, which is 100 for the CIFAR-100 dataset. From this uncertainty definition, $u_T$ takes values between 0 and 1. If $f_T$ is confident with its prediction with a single class, $u_T$ will be 0. Conversely, if the prediction is uncertain and contains multiple classes in the prediction set, $u_T$ is larger than 0. To incorporate this uncertainty into the knowledge transfer process, we define the uncertainty-based weight in Eq. 3 as $w = 1$ if $u_T = 0$, $w = 0$ if $u_T > 0$. This weight helps adjust the influence of the teacher's prediction on the student model, ensuring that confident predictions are more heavily relied upon while uncertain predictions have less influence. We follow the same experimental settings as in previous work for the coefficients $\\lambda_1$ and $\\lambda_2$, as well as other training details. Please refer to the Appendix A.3.3 for more details."}, {"title": "5. Conclusions", "content": "We propose AUKT, a novel framework that leverages conformal prediction to efficiently quantify uncertainty in the teacher's predictions for knowledge transfer. By dynamically adjusting the teacher's influence based on confidence, AUKT ensures effective student learning, especially under domain shifts where traditional methods struggle due to over-reliance on uncalibrated teachers. Our approach selectively utilizes reliable guidance while avoiding misleading supervision and can still extract useful \u201cdark knowledge\u201d even when the teacher underperforms, unlike conventional methods that assume a superior teacher. AUKT is universal and domain-agnostic, making it applicable across diverse tasks. We validate AUKT on image classification, imitation-guided reinforcement learning, and autonomous driving, demonstrating improved performance, robustness, and transferability. For limitations and future work, please see the Appendix A.6."}, {"title": "A. Appendix", "content": "A.1. Adaptive Uncertainty-Guided Knowledge Transfer (AUKT) Algorithm\nAlgorithm 1 Adaptive Uncertainty-Guided Knowledge Transfer (AUKT)\nRequire:\n1: Pretrained teacher model $f_T$", "f_S$\n2": "Calibration set $D_{cal"}, "from the student domain\n3: Training set $D_{train}^S$ from the student domain\nEnsure: Trained student model $f_S$\n4: Step 1: Calibrate Conformal Teacher Predictor\n5: Compute nonconformity scores $s_i = s(x_i, y_i)$ for each sample $(x_i, y_i)$ from the calibration set $D_{cal}$\n6: Compute the $(1 - \\alpha)$-quantile $q_{1-\\alpha}$ of the nonconformity scores\n7: Step 2: Train Student Model\n8: for each training sample $(x_j, y_j) \\in D_{train}^S$ do\n9: Compute the teacher's prediction $f_T(x_j)$\n10: Construct the conformal prediction set $C(x_j)$ and compute $w(x_j)$ as in Eq. 3\n11: Update the student model using the loss $L$ as in Eq. 2\n12: end for\n13: return Trained student model $f_S$\nA.2. Theoretical Analysis\nWe provide a theoretical analysis comparing the proposed AUKT with standard knowledge transfer to explain why AUKT outperforms conventional methods, particularly under domain shifts. In AUKT, we leverage CP to quantify the teacher's prediction uncertainty and dynamically adjust its guidance to the student based on its prediction uncertainty. The adaptive uncertainty-guided loss function is follows:\n$L = \\lambda_1L_S + w(x) \\cdot \\lambda_2L_T,$(4)\nwhere $L_S(f_S(x), y)$ is the student's supervised task loss (e.g., mean square error, cross-entropy), $L_T(f_S(x), f_T(x))$ is the teacher guidance loss (e.g., KL divergence, mean squared error), $\\lambda_1$ and $\\lambda_2$ are the coefficients, and $w$ is an uncertainty-based weight derived from the teacher model's uncertainty $u_T$ as in Eq. 3. The weight $w$ approaches 1 when the teacher's predictions are confident and decreases toward 0 as the uncertainty increases, ensuring that the student relies more on the teacher's predictions when the uncertainty is low, and less when the uncertainty is high.\nFrom the perspective of gradient modulation, taking the gradient of $L$ with respect to the student model parameter $\\theta_S$:\n$\\nabla_{\\theta_S} L = \\lambda_1\\nabla_{\\theta_S} L_S + w \\lambda_2 \\nabla_{\\theta_S} L_T,$(5)\nwhen the teacher's prediction is confident, $w \\rightarrow 1$, the gradient from teacher guidance loss is backpropagated to update the student model. Conversely, when the teacher's prediction is noisy, $w \\rightarrow 0$, gradients from noisy $L_T$ are suppressed, preventing the student from learning incorrect patterns. However, standard knowledge transfer employs a fixed-weighted gradient update, $\\nabla_{\\theta_S} = \\lambda_1\\nabla_{\\theta_S}L_S + \\lambda_2\\nabla_{\\theta_S} L_T$, even if the teacher's prediction is noisy, the gradient from teacher guidance is backpropogated to update the student model, leading to suboptimal knowledge transfer.\nThen we analyze from the perspective of critical point. To find the critical point of $f_S(x)$, we set the gradient of $L$ with respect to $f_S(x)$ to zero:\n$\\frac{\\partial L}{\\partial f_S(x)} = \\lambda_1 \\frac{\\partial L_S}{\\partial f_S(x)} + w \\lambda_2 \\frac{\\partial L_T}{\\partial f_S(x)} = 0.$(6)\nRearranging, the critical point $f_S(x)$ satisfies:\n$\\frac{\\partial L_S}{\\partial f_S(x)} = -"]}