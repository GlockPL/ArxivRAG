{"title": "Argument Mining in Data Scarce Settings: Cross-lingual Transfer and Few-shot Techniques", "authors": ["Anar Yeginbergen", "Maite Oronoz", "Rodrigo Agerri"], "abstract": "Recent research on sequence labelling has been\nexploring different strategies to mitigate the\nlack of manually annotated data for the large\nmajority of the world languages. Among oth-\ners, the most successful approaches have been\nbased on (i) the cross-lingual transfer capabili-\nties of multilingual pre-trained language mod-\nels (model-transfer), (ii) data translation and la-\nbel projection (data-transfer) and (iii), prompt-\nbased learning by reusing the mask objective to\nexploit the few-shot capabilities of pre-trained\nlanguage models (few-shot). Previous work\nseems to conclude that model-transfer outper-\nforms data-transfer methods and that few-shot\ntechniques based on prompting are superior to\nupdating the model's weights via fine-tuning.\nIn this paper, we empirically demonstrate that,\nfor Argument Mining, a sequence labelling task\nwhich requires the detection of long and com-\nplex discourse structures, previous insights on\ncross-lingual transfer or few-shot learning do\nnot apply. Contrary to previous work, we show\nthat for Argument Mining data-transfer obtains\nbetter results than model-transfer and that fine-\ntuning outperforms few-shot methods. Regard-\ning the former, the domain of the dataset used\nfor data-transfer seems to be a deciding factor,\nwhile, for few-shot, the type of task (length and\ncomplexity of the sequence spans) and sam-\npling method prove to be crucial.", "sections": [{"title": "Introduction", "content": "Transfer learning and pre-trained language models\nare closely related as the knowledge learned for one\nor more tasks in one specific language can be ap-\nplied to other tasks or languages (Wang et al., 2023).\nIn this paper, we analyze how this feature can be\napplied in scenarios where not much data is acces-\nsible as it is the case of argument mining in the\nclinical domain. In data-transfer approaches, data\ncan be translated and the required annotations pro-\njected to train supervised models. Model-transfer\nmethods avoid the long process of generating the\ntraining data by applying multilingual pre-trained\nlanguage models to learn the annotations in one\nlanguage and generate the predictions in a differ-\nent one (Pikuliak et al., 2021; Garc\u00eda-Ferrero et al.,\n2022a; Chen et al., 2023). Alternatively, by few-\nshot prompting there is a possibility to reach com-\nparable results by providing a few examples from\nthe problem at hand to pre-trained language models\n(Ma et al., 2022). In sequence labelling tasks, these\nmethods have shown to be effective with a minimal\nloss in performance based on a very few annotated\nexamples.\nThese few-shot methods have widely been tested\non popular benchmark datasets, such as in those\nfor Named Entity Recognition (NER) (CoNLL\n2003 (Tjong Kim Sang and De Meulder, 2003),\nOntoNotes 5.0 (Weischedel et al., 2013), MIT-\nMovie (Liu et al., 2013)) concluding that model-\ntransfer outperform data-transfer methods and that\nfew-shot techniques based on prompting are supe-\nrior to updating the model's weights via fine-tuning.\nHowever, such conclusions have been based on re-\nsults obtained on sequence labelling tasks for which\nthe sequence spans are commonly short and quite\nhomogeneous in terms of the structure and content\nof the label words.\nIn this paper we explore whether these conclu-\nsions still hold for Argument Mining, a task in Nat-\nural Language Processing (NLP) aimed at extract-\ning long and complex discourse structures from\ntext. Argument Mining usually involves two dis-\ntinct subproblems: (1) argument component de-\ntection, focusing on locating the spans of argu-\nments and identifying their types (e.g., claims and\npremises), and (2) classification of argument rela-\ntions, which involves classifying the relationship\nbetween two argument components as supporting\nor attacking.\nIn order to do so, we use AbstRCT (Mayer et al.,\n2021) a corpus of medical abstracts annotated for"}, {"title": "Related Work", "content": "In this section, we review the closest work to the\npaper's main topics, namely, Argument Mining,\ncross-lingual transfer and few-shot learning."}, {"title": "Argument Mining", "content": "The are a number of different theoretical ap-\nproaches to describe the argument structures that\ncan be inferred from text analysis. For instance,\nToulmin (1958) identified different functional roles\nin arguments (evidence, warrant, backing, qualifier,\nrebuttal, and claim) based on how the conclusion\nis drawn from evidence in the text. Furthermore,\nFreeman (2011) investigated how to transfer argu-\nments via diagramming techniques of the informal\nlogic tradition. Others (Dung, 1995) tried to cre-\nate a graph-based representation of argumentation\nby applying non-monotonic reasoning in Artificial\nIntelligence (AI) and logic programming. Finally,\nPeldszus and Stede (2013) introduced a diagram\nstructure with models of the textual representation\nof arguments and globally optimized argumenta-\ntive relations. They argued that support and attack\nrelations are sufficient to describe the overall re-\nlationships between argument components. More-\nover, they identified five different types of argument\ngraphs based on the connections that exist between\nthem, namely, one claim having relations with mul-\ntiple premises, a claim followed by another claim,\netc.\nIn Natural Language Processing Argument Min-\ning (AM) is focused on automatically identifying\nthe argument components and classifying the rela-\ntions that may exist between them. Following the\ntheoretical models proposed, a number of empiri-\ncal approaches have been developed in the last few\nyears. Thus, Stab and Gurevych (2017) tackled\nAM in two different steps. First, they try to locate\nthe span argumentative text and classify the type of\ncomponent at token level. Second, they classify the\nrelations linking the identified argument spans. In\naddition to the two step system to address AM, they\nalso generate Persuasive Essays, perhaps the most\npopular NLP dataset manually annotated with argu-\nment structures (Stab and Gurevych, 2017). Later\non, Eger et al. (2017) introduced an end-to-end\nAM system based on a bi-directional sequence-to-\nsequence model.\nOther work includes Toledo-Ronen et al. (2020),\nwhich provides an in detail analysis at argument\nlevel of various multilingual datasets, while Rocha\net al. (2018) experimented with cross-lingual argu-\nmentative relation identification from English to\nPortuguese.\nFinally, Mayer et al. (2020) introduced the first\ndataset of English medical abstracts annotated for\nargument component detection and argument re-\nlation classification. Subsequently, Mayer et al.\n(2021) introduced a Transformer-based solution\nwith Gated Recurrent Units (GRU) and Conditional\nRandom Field (CRF) classification layers."}, {"title": "Few-shot Learning Approaches for Sequence Labelling", "content": "The availability of pre-trained language models al-\nlows to apply supervised methods with less amount\nof annotated data which is why some research in\ndifferent NLP tasks has focused on few-shot train-\ning (Hofer et al., 2018; Fritzler et al., 2019; Li\net al., 2022), namely, learning supervised models\nwith very few manually annotated samples. The\nrise of prompt-based models (Radford et al., 2019;\nBrown et al., 2020) further increased the interest\nin learning the task describing the classification\nobjective. This usually involves transforming tradi-\ntional classification tasks into cloze tasks using tex-\ntual templates and a predefined set of label words,\nhighlighting the importance of template design in\nprompt-based learning.\nIn this line of work, Schick and Sch\u00fctze (2021)\npresented a semi-supervised training approach\nthat reformulates input instances into cloze-style\nphrases. Cui et al. (2021) proposed a template-\nbased method for Named Entity Recognition\n(NER) by generating templates for each entity from\na given example. However, template-based ap-\nproaches are better suited to sentence-level tasks\nwhere the complexity of the templates remain man-\nageable. As an alternative, EntLM (Ma et al.,\n2022) proposed a template-free few-shot learn-\ning approach for sequence labelling tasks. Their\nmethod is based on computing a set of label words\nfrom the input text and replacing the entity-specific\ntokens with these label words in the training sam-\nple. EntLM obtains state-of-the-art results which\nis why we use it in this paper as the representative\nof few-shot learning for argument component de-\ntection. Huang et al. (2022) and Das et al. (2022)\npropose few-shot learning for NER involving con-\ntrastive learning via prompt-based meta-learning.\nHowever, their methods require large amounts of\ndata to first train the model before adapting it with\na handful of examples for various label sets."}, {"title": "Cross-lingual Sequence Labelling", "content": "Previous work on cross-lingual sequence tagging\nmainly focuses on tasks such as part-of-speech\n(POS) tagging, named-entity-recognition (NER)\n(Gaddy et al., 2016; Yang et al., 2017; Agerri et al.,\n2018; Chen et al., 2018; Liu et al., 2020), and Opin-\nion Target Extraction (OTE) (Agerri and Rigau,\n2019). Garc\u00eda-Ferrero et al. (2022a) compared\nmodel-transfer and data-transfer approaches on a\nvariety of sequence labelling tasks, datasets, and\nlanguages. They conclude that model-transfer us-\ning pre-trained multilingual language models such\nas XLM-ROBERTa-large (Conneau et al., 2019)\noutperform data-transfer methods.\nCloser to our work, Eger et al. (2018) gener-\nated parallel German and Chinese versions from\nEnglish by applying manual and automatic transla-\ntion and label projection to experiment with data-\ntransfer approaches based on cross-lingual embed-\ndings. They concluded that, while machine trans-\nlated data degraded results when used for training\na supervised model for the target language, results\nwere promising enough to continue working on that\nresearch direction. Thus, Sousa et al. (2021) trans-\nlated Persuasive Essays into Portuguese for further\ncross-lingual experimentation. However, it should\nbe noted that current model-transfer, few-shot and\nsupervised techniques based on multilingual pre-\ntrained language models are clearly superior to the\nmethods used at the time, which makes the purpose\nof our work rather relevant."}, {"title": "Data", "content": "The starting point for experimentation on argument\nmining in data scarce settings is AbstRCT, a dataset\nof Randomized Controlled Trials (RCT) manually\nannotated with argument components and relations\n(Mayer et al., 2021). The original AbstRCT con-\nsists of abstracts of clinical trials in English col-\nlected from the MEDLINE database and manually\nannotated with two types of argument components:\nClaims and Premises. A \u2018claim' is a concluding\nstatement about the outcome of the study. In the\nmedical domain it typically refers to a judgement\nregarding a possible diagnosis or a treatment. A\n'premise' corresponds to an observation or measure-\nment in the study (ground truth), which supports\nor attacks another argument component, usually a\nclaim. It is important to stress that premises are\nobserved facts, therefore, credible without further\nevidence.\nThe training set consists of 350 abstracts that\ncover the neoplasm disease, 50 more abstracts\nabout neoplasm are used for development, while\nthe three evaluation sets are composed of: 100\nabstracts about neoplasm, 100 abstracts about glau-\ncoma and finally a mixed set of 100 abstracts with\n20 abstracts for each of the diseases in the Ab-\nSRCT dataset (i.e. neoplasm, glaucoma, hyperten-\nsion, hepatitis and diabetes). The number of the"}, {"title": "Sampling Data for Few-shot Learning", "content": "The main objective of Few-Shot Learning (FSL) is\nto generalize while learning from a small portion\nof data. In order to perform FSL, the data is sam-"}, {"title": "Experimental Setup", "content": "An important feature of AM with respect to other\nsequence labelling tasks is that arguments are con-\nsiderably long and composed by a variety of word\ntypes.\nThe experiments are based on the three different\ntechniques that we will be comparing to establish\nwhich one is the optimal one for AM in data-scarce\nsettings: (i) data-transfer, (ii) model-transfer and\n(iii), few-shot learning for sequence labelling.\nResults are reported using F1 macro-averaged\nscore calculated at sequence level, namely, the F1-\nscore is computed for each argument component\nfollowing the usual method for sequence labelling\ntasks as formulated for Named Entity Recognition\n(Tjong Kim Sang and De Meulder, 2003)."}, {"title": "Data-Transfer and Model-Transfer", "content": "Data-transfer involves generating training data in\nthe target language by translating and projecting\nthe annotations from the original English language\nto Spanish, French and Italian. This process was de-\nscribed in Section 3. The translated and projected\ntraining data is then used to fine-tune pre-trained\nencoder language models.\nInitially, we separately fine-tune multilingual\nBERT (Devlin et al., 2019), on the training sets\nof English, Spanish, French, and Italian AbstRCT\ncorpora and evaluate the resulting models for each\nof the languages in a monolingual setting.\nWe also tested data-transfer in a multilingual set-\nting by fine-tuning multilingual BERT on the train-\ning sets for the 4 languages. Finally, both mono-\nlingual and multilingual settings were evaluated\nusing both post-processed and manually corrected\nversions of the data (French, Italian and Spanish).\nModel-transfer is facilitated by pre-trained mul-\ntilingual language models such as mBERT by en-\nabling them to label sequences in languages on\nwhich they have not been explicitly trained on, re-\nlying on their multilingual or crosslingual abilities.\nThus, model-transfer allows to perform AM for\nlanguages for which no annotated data is avail-\nable by training in English and generating predic-\ntions in the target language (French, Italian and\nSpanish). In our experiments, this amounts to fine-\ntuning mBERT using English data and evaluating\nits performance on test data from the other three\nlanguages."}, {"title": "Few-shot Learning", "content": "Few-shot learning exploits limited annotated exam-\nples to train models, striking a balance between\ndata scarcity and task complexity.\nMa et al. (2022) proposed a template-free\nmethod for few-shot prompting for Named En-\ntity Recognition (NER) by tackling it as a Lan-\nguage Model (LM) task with an Entity-oriented\nLM (EntLM) objective to solve the NER task. This\navoids generating a new template corpus for each\nexample in the data. We use this method in our\nexperiments as it represents current state-of-the-art,\nat the time of writing, for sequence labelling in\nfew-shot settings. Their approach consists of first\nretrieving class-specific words called label words"}, {"title": "Model-transfer and Data-transfer", "content": "Table 4 displays the F1-scores derived from the\nargument component detection experiments us-\ning full in-domain data across all the experiments.\nThe rows corresponding to the monolingual data-\ntransfer category present the results obtained from\ntraining and evaluating in the corresponding lan-\nguage. Similarly, multilingual data-transfer refers\nto the merged training set consisting of all 4 lan-\nguages and evaluating each language separately.\nCross-lingual refers to model-transfer, namely,\ntraining in English and evaluating in the other 3\nlanguages. The last column corresponds to the av-\nerage between all the results per language across all\ntest sets. For a fair comparison, the average of the\ncross-lingual model transfer includes the F1-score\nof the monolingual English.\nResults show that, contrary to previous work\non crosslingual transfer (Garc\u00eda-Ferrero et al.,\n2022b), monolingual data-transfer clearly outper-\nforms cross-lingual model-transfer for argument\ncomponent detection. Another interesting point is\nthat multilingual data-transfer obtains the overall"}, {"title": "Few-shot", "content": "Figure 1 reports the results of few-shot using both\nsampling methods (k-shot and k-percent) for the\ndata trained by means of both EntLM and fine-\ntuning techniques.\nThe first point to mention is that data-transfer\nalso outperforms the few-shot prompting approach\nfor sequence labelling proposed by EntLM. Further-\nmore, and quite surprisingly, fine-tuning remains\ncompetitive with respect to EntLM with the k-shot\nsampling while it is quite superior when tested on\nthe percentage sampling. We hypothesized that k-\npercentage sampling produces better performance\ndue to the higher proportion of outside tokens. In\nfact, when fine-tuned with 20% and 50% of the data\nperformance is comparable to that of data-transfer\nand model-transfer results."}, {"title": "Error Analysis", "content": "In general, fine-tuning the model on the complete\ndataset often results in misclassifications with a ten-\ndency to assign Claim labels in place of Premise.\nAdditionally, dealing with long sequences poses\nchallenges in accurately identifying both bound-\naries and classes for the system. This pattern per-\nsists in zero-shot results, and it can be attributed to\nan inherent imbalance in the data, particularly in\nterms of the disparity between the number of Claim\nand Premise labels and the length of arguments in\nthe sequences.\nEach sequence predominantly corresponds to a\nsingle argument type, and instances where a se-\nquence contains compound arguments, or when the\nargument span is only a proportion of the input,\nare less frequent. Consequently, in such exam-\nples, the most prevalent error involves misidenti-\nfying Claim as Premise and recognizing only one\nargument component in sequences with multiple\ncomponents. These errors tend to occur more sys-\ntematically in classifications under the zero-shot\nsetting.\nIn k-shot scenarios, the model consistently strug-\ngles to accurately identify both the correct spans\nand class labels. Furthermore, as the number of k\ndecreases, there is an increase in randomness in the\nassigned classes for each token, meaning that each\ntoken in a sequence may be classified differently.\nIn particular, it is notable in the 5- and 10-shot.\nUnder k-shot the model struggles to predict B- to-\nkens. Whereas in the k-percent the opposite occurs,\nnamely, the model learns to predict the beginning\nof the sequence and fails to predict O sequences\ncorrectly. Nevertheless, it is observed that as the\namount of data increases, the quality of the pre-\ndicted outcomes improves.\nThe described errors persist consistently in the\ncase of EntLM. Additionally, when dealing with\nsmaller training sets, the trained model tends to\nassign a single argument type to all examples in a\ndocument. As the value of k increases, the random-\nness in predictions also grows proportionally. In\nother words, a larger amount of data leads to more\nunpredictable assigning of labels by the model on\nthe token level.\nA potential explanation for such behavior may\nbe the selection of the label words. The concept\ninvolves computing label-specific words to later\nsubstitute them for few-shot learning. Given that\nthe length of an argument is usually long enough,\none selected label word may not represent the ar-\ngument type correctly."}, {"title": "Concluding Remarks", "content": "In this paper, we address the argument component\nidentification task in the clinical domain in a sce-\nnario of lack of manually annotated data for lan-\nguages other than English. We address the prob-\nlem by applying cross-lingual transfer and prompt-\nbased learning strategies in the AbstRCT corpus.\nExperimentation was facilitated by the generation\nof multilingual dataset by machine-translating and\nprojecting the annotations of the original English\nAbstRCT into French, Italian, and Spanish.\nThe results of our experiments show that for long\nand structurally complex sequence labelling, as it\nis the case of component identification in Argu-\nment Mining, data-transfer is a better strategy than\nmodel-transfer (RS1). Thus, fine-tuning mBERT\nin monolingual and multilingual settings showed\nresults on an average of around 60 F1-scores for\nthree test sets, outperforming any other approach,\nbe that model-transfer or few-shot learning.\nFurthermore, we have addressed the question of\nhow much data is required to obtain similar results\nto those using the full data for training (RS2) by\nperforming experiments in a few-shot learning ap-\nproach. Thus, corpus splits of different granularity\n(5, 10, 20, and 50 shot or percentage) were used\nin the experimentation with EntLM and mBERT.\nThe models in general perform better when trained\nwith data sampled using the k-percent method (in\ncomparison to k-shot) and by fine-tuning a pre-\ntrained language model (instead of using a prompt-\ning method such as EntLM). Finally, empirical re-\nsults indicate that by fine-tuning the multilingual\nmodel mBERT with 20% of the data performance\nis competitive with data- and model-transfer ap-\nproaches."}, {"title": "Limitations", "content": "Our evaluation focuses on Argument Mining, and\nit would be interesting to compare it with other\nsequence labelling tasks where the spans are also\ncomplex and heterogeneous. Furthermore, we ex-\nperiment only in the medical domain, which may\naffect the results on the data-transfer method. We\nnote, however, that our results clearly contradict\nprevious results on model-transfer vs data-transfer\npreviously obtained for other sequence labelling\ntasks (Garc\u00eda-Ferrero et al., 2022b). Furthermore,"}]}