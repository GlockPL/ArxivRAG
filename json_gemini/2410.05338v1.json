{"title": "Distributed Inference on Mobile Edge and Cloud: An Early Exit based Clustering Approach", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "abstract": "Recent advances in Deep Neural Networks (DNNs) have demonstrated outstanding performance across various domains. However, their large size is a challenge for deployment on resource-constrained devices such as mobile, edge, and IoT platforms. To overcome this, a distributed inference setup can be used where a small-sized DNN (initial few layers) can be deployed on mobile, a bigger version on the edge, and the full-fledged on the cloud. A sample that has low complexity (easy) could be then inferred on mobile, that has moderate complexity (medium) on edge, and higher complexity (hard) on the cloud. As the complexity of each sample is not known beforehand, the following question arises in distributed inference: how to decide complexity so that it is processed by enough layers of DNNs. We develop a novel approach named DIMEE that utilizes Early Exit (EE) strategies developed to minimize inference latency in DNNs. DIMEE aims to improve the accuracy, taking into account the offloading cost from mobile to edge/cloud. Experimental validation on GLUE datasets, encompassing various NLP tasks, shows that our method significantly reduces the inference cost (> 43%) while maintaining a minimal drop in accuracy (< 0.3%) compared to the case where all the inference is made in cloud.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, Deep Neural Networks (DNNs) have significantly increased in scale, resulting in outstanding performance [1], particularly in Natural Language Processing (NLP) [2] tasks. However, this growth in scale necessitates substantial computational resources, which restricts their deployment on resource-constrained platforms like mobile and edge devices. To address these challenges, various strategies have been proposed, including model pruning, weight quantization, knowledge distillation, early exits, and cloud offloading [3].\nMethods such as model pruning [4], [5] weight quantization [6], [7] and knowledge distillation [8], [9] tend to lower the model size by different methods, significantly reducing the accuracy of the models. These methods mostly compress the model that can fit in the memory of the mobile device but affect the optimality of the backbone. Some models also provide smaller versions of their large-sized models [10], [11] to fit them in resource-constrained devices.\nAs mobile and edge devices often lack the capability to perform inference on large models due to resource constraints such as limited space and memory, cloud offloading leverages high-capacity services and extensive computing resources, allowing the deployment of full-fledged DNNs for inference. However, offloading samples to the cloud incurs additional costs due to the physical distance from mobile terminals. Moreover, not all samples require the same amount of computation since real-world datasets comprise a mixture of easy and hard samples.\nTo address this, we utilize distributed inference: deploying initial layers of the DNN on the mobile device, a larger model with more layers on the edge device, and the full model on the cloud. Given the varying complexity of real-world samples, it is advantageous to utilize mobile, edge, and cloud resources based on the complexity of incoming samples. As the complexity of the incoming samples is unknown, the question arises how to identify it. We address this challenge of identifying the complexity of the sample so that one can decide whether the sample is to be inferred at the mobile, edge, or cloud. Further, a sample inferred in the cloud can have better accuracy but involves higher offloading costs. On the other hand, inferring all the samples on mobile can degrade accuracy. Hence, the decision of where to infer has to account for both accuracy and offloading cost.\nRecently, Early Exit (EE) strategies have gained attention for adaptive inference [12], [13], where inference can be made at classifiers attached at the intermediary layer. The primary goal of EE strategies is to reduce inference latency by letting the sample exit from the intermediary layer if the prediction confidence at that layer exceeds a predefined threshold. The initial layers of DNNs extract low-level features sufficient for easy samples, while deeper layers are required for more complex features necessary for harder samples. Allowing easier samples to exit early reduces computational demand and increases inference speed. EE strategies perform inference based on sample complexity, making them ideal for distributed inference scenarios.\nOur approach optimizes resource usage across mobile devices, edge devices, and the cloud by distributed inference using an early exit DNN. Three variants of the DNN are deployed: a few initial layers on the mobile device, a higher number of layers on the edge device, and a full-fledged DNN deployed on the cloud. The number of layers in every device is decided based on the available resources on the mobile and edge devices and is further discussed in Section III-C. Since EE models are equipped with exit classifiers that can provide predictions on the input sample anytime, each device can independently classify incoming samples. In Figure 1, we show the inference process of our method where the easier samples are processed at the mobile device, moderate are offloaded to the edge, and only the hard ones are offloaded to the cloud.\nTo assess sample complexity during inference, first, we create a pool of easy, moderate and hard samples utilizing the exit points of the DNN. The pool is created during training where if a sample exits at the layer before the final layer on a mobile device, it is considered an easy sample. If a sample exits before the final layer on the edge device but after the final layer on mobile, it is classified as a moderate sample. Samples that are inferred after the final layer on the edge are considered hard samples, requiring more layers. This method gives us easy, moderate, and hard pools during training. This method effectively defines the complexity of incoming samples. During inference, we utilize the pools created during training to decide the complexity of the incoming sample. Specifically, our method analyses that the incoming sample resembles which group closely to decide its complexity on the fly using the word embeddings on the mobile device.\nIn our method named DIMEE: Distributed Inference on Mobile, Edge and Cloud: An Early Exit Approach, due to distributed inference, each sample is provided with an appropriate amount of computational resources as per its complexity. The major advantage of our method is that it decides on-the-fly about the computational requirements of an incoming sample without requiring it to first pass through the mobile device. This reduces the burden on individual devices and also solves the issue of inference of large models on mobile devices. Also, our method better models the accuracy-efficiency trade-off i.e., the efficiency is significantly improved in our method while maintaining the accuracy similar to that of the final layer. This approach effectively balances processing and communication costs, ensuring efficient and accurate processing of samples by dynamically determining whether to process them locally, at the mobile or edge, or offload them to the cloud based on sample complexity.\nFor our backbone model, we adopt the well-used BERT-base/large [10] backbone. This choice becomes an ideal test-bed for our method, given its efficiency and competitive accuracy compared to the state-of-the-art models. We conduct experiments on multiple NLP tasks to showcase the effectiveness of our method as detailed in Section IV. Our experimental results on sentiment classification, entailment classification and natural language inference tasks demonstrate that DIMEE is robust to different cost structures which means it can incorporate devices with varying processing power and communication methods such as 3G, 4G, 5G and Wi-Fi. Specifically, our method achieves a significant reduction in cost (> 43%) with only a minimal drop in accuracy (< 0.3%) when compared to the scenario where all the samples exit from the final layer.\nOur key contributions are as follows:\n\u2022 We utilize early exits for distributed inference to enable early inferences for easy samples on mobile devices,\n\u2022 moderate samples are inferred at the edge device and only hard samples are offloaded to the cloud.\n\u2022 Our method is robust to various cost changes and does not lose accuracy when the mobile devices, edge devices or the communication network is changed.\n\u2022 The minimal loss in accuracy is attributed to the fact that in our method, the optimality of the backbone is not lost i.e., we do not reduce any parameter from the backbone.\n\u2022 We experimentally validate that our method minimizes performance degradation (sometimes even improves) while significantly reducing the costs as compared to the previous baselines."}, {"title": "II. RELATED WORKS", "content": "In this section, we discuss the previous works on split computing and early exits to use DNNs on mobile devices.\nCloud offloading Neurosurgeon, as introduced the [14], explores the strategies for optimizing the splitting of DNNs based on cost considerations associated with selecting a specific splitting layer. In a similar vein, BottleNet [15] incorporates a bottleneck mechanism within split computing. This approach entails deploying a segment of the DNN on a mobile device to encode the input sample into a more compact representation before transitioning to the cloud for further processing. On the same setup, multiple training strategies have been proposed for training the encoder situated on the edge device such as BottleNet++ [16] employs cross-entropy-based training approaches in the context of split computing, Matsubara [17] performs knowledge distillation-based training, CDE [18] and Yao [19] perform reconstruction-based training and Matsubara [20] perform head-network distillation training method to effectively encode the input to offload efficiently.\nEarly Exit DNNs Early Exit DNNs have found applications across diverse tasks. In the context of image classification, BranchyNet [12] and several preceding studies utilize classification entropy metrics at different intermediate layers to determine whether early inference can be made with sufficient confidence. Approaches like SPINN [21] and SEE [22] incorporate early exit DNN architectures, primarily aimed at handling service disruptions in real-time inference scenarios.\nBesides early exiting, works like FlexDNN [23] and Edgent [24] focus mainly on on the appropriate DNN depth. Other works such as DynExit [25], focus on deploying the early-exit DNN in hardware. It trains and deploys the DNN on a Field Programmable Gate Array (FPGA) hardware.\nIn NLP domain, DeeBERT [13], ElasticBERT [26], CeeBERT [27], LeeBERT [28] and PABEE [29] have applied the early exit DNNs specifically for the BERT backbone. DeeBERT uses separate training to train the Early exit DNN while ElasticBERT uses the joint training strategy. CeeBERT [27] optimizes the threshold choice using multi-armed bandits. PABEE proposes a patience-based exiting criteria while LeeBERT additionally uses knowledge distillation during training. DeeCAP [30] and MuE [31] extend early exit ideas to image captioning models."}, {"title": "III. PROBLEM SETUP", "content": "We start with a Pre-trained Language model such as BERT and attach exit classifiers after all the layers of the backbone. In the following, we discuss the setup in detail."}, {"title": "A. Training exit classifiers", "content": "Let D represent the distribution of the dataset with a label class C used for backbone fine-tuning. Let us assume that there are l layers in the backbone. For any input sample, (x, y) ~ D and the ith exit, the loss can be computed as:\n$L_i(\\theta) = L_{CE}(f_i(x, \\theta), y)$ (1)\nHere, $f_i(x, \\theta)$ is the output of the classifier at the ith layer, where $\\theta$ denotes the set of learnable parameters, and $L_{CE}$ is the cross-entropy loss. We learn all the classifiers simultaneously hence the overall loss function could be written as $L = \\sum_{i=1}^l L_i$. This loss simultaneously optimizes all the exits. Also, let $P_i(c)$ denote the estimated probability class $c \\in C$ and $C_i$ denote the confidence in the estimate at the ith layer i.e., $C_i := max_{c \\in C} P_i(c)$. Subsequently, the model is ready for inference."}, {"title": "B. Preparation of dataset pool", "content": "After training, we divide the dataset into three pools based on their complexity i.e. the easy pool, the moderate pool and the hard pool. To create the pools of the datasets, we use the training and validation dataset. The complete procedure is given in Algorithm 1.\nIt initializes three empty lists $P_e$ easy pool, $P_m$ moderate pool and $P_h$ the hard pool. As a sample arrives, it is passed through the backbone, and if the sample exits before layer m"}, {"title": "C. Layer distribution", "content": "Let us assume that the mobile device contains DNN's first m layers and the edge has DNN's first n layers where 1 < m \u2264 n \u2264 l where the cases m = 1 is when there is no mobile device, m = n means there is either no mobile or edge device. If n = l, it means there is no edge device. We discuss the impact of the values of m and n. These values are important as they model the overall cost and are user-defined. These are used to decide the quantity of workload on different devices, i.e., mobile, edge or cloud. A higher value of m means more layers are deployed on the mobile device and the processing cost e.g. battery depletion will be high, however since more layers are in the mobile device there will be a lower chance of a sample being offloaded reducing the latency cost. If the value of n is high, then there will be fewer samples being offloaded to the cloud reducing the latency costs, however, it will increase load on the edge device. If both m and n are kept small then since less number of layers will inferred earlier, more samples will be offloaded to the cloud increasing the offloading cost and the charges of the cloud platform."}, {"title": "D. Choice of threshold a", "content": "The threshold a used to decide the early exiting not only models the accuracy-efficiency trade-off but also impacts the cost. The cost is affected as this threshold is used to divide the dataset into three different pools. These pools are very important as they model the assignment of a sample to different devices. Hence it is very crucial to set the threshold properly. We first define the different types of costs that we consider, 1) Processing cost is the cost to process the sample through one layer of the DNN in the mobile and edge denoted as $A_m$ and $A_e$ respectively. 2) Offloading cost from mobile to edge and mobile to cloud denoted as $o_e$ and $o_c$ respectively. We also assume that there is a constant cost $y$ charged by the cloud platform for each sample. To choose the threshold $a$, we define a reward function as:\nr(a) =$\\begin{cases}\nC_i - A_mi  if  C_i \\geq a  and  i < m \\\\\nC_i - A_e  - o_e  if  C_i \\geq a  and  m < i < n \\\\\nC_i - \\gamma  - o_c  otherwise\n\\end{cases}$\n(2)\nThe reward function could be interpreted as, if the sample exits at mobile device then the reward will be confidence gained subtracted by the cost of processing the sample till the ith layer on the mobile device. Similarly, for the edge device, the reward will be the same with an additional cost of offloading. Finally, if the sample is offloaded to the cloud, the reward will be the inference at the final layer subtracted by the cost of cloud platform and offloading cost. The expected reward function could be written as:\n$E[r(a)] = E[C_i \u2013 A_m | mob. inference]P[mob. inference]$\n$+ E[C_i \u2013 A_e \u2013 o_e | edge inference]P[edge inference]$\n$+ E[C_i - \\gamma \u2013 o_c | cloud inference]P[cloud inference]   (3)\nNow the objective is to maximize $E[r(a)]$ and could be expressed as $max_{a \\in S} E[r(a)]$ where the set S is the possible choices for the a values. Note that P[mob. inference], P[edge inference] and P[cloud inference] is the probability that the sample will be inferred at mobile, edge and cloud respectively and depend on the value of a."}, {"title": "E. Post-Deployment Inference", "content": "Fixed: After storing the values $P_e$, $P_m$ and $P_h$ consisting of embeddings of easy, moderate and hard samples respectively. We calculate the average of these values and name it as $P_e^a$, $P_m^a$ and $P_h^a$ respectively. The sample can be classified as easy, moderate or hard using the average values as in K-means clustering algorithm, as a sample arrives, the distance of the incoming sample is calculated from $P_e^a$, $P_m^a$ and $P_h^a$ and classifies the sample as easy, moderate or hard based on the minimum distance of the sample from the mean values of different pools. After this the easy samples are inferred locally at the mobile device incurring only processing cost, moderate samples are offloaded directly to the edge without any computation on mobile incurring small offloading cost and processing cost and the hard samples are directly offloaded to the cloud with higher offloading cost as well as cost charged by the cloud platform.\nAdaptive: In fixed inference, the pools are created using the validation dataset, however during test time there might be a shift in the dataset distribution. For such cases, we dynamically update the pool averages such that the distribution shift can be properly captured. In this setup, as the sample arrives, it is classified as easy, moderate or hard in a similar way but this time the average is recalculated based on the incoming sample's complexity. For instance, if a sample is easy, then the value $P_e^a$ is recalculated. In this manner, the shift is captured and the trade-off of accuracy-cost is not affected."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we provide all the experimental details of the paper and experimentally validate our method."}, {"title": "A. Dataset", "content": "We used the GLUE [2] datasets for the evaluation of our method. We evaluate our method on three types of tasks viz. sentiment classification, entailment classification and natural language inference. The datasets used are:\n1) MRPC: Microsoft Research Paraphrase Corpus is a semantic equivalence classification dataset containing sentence pairs extracted from online news sources. 2) QQP: Quora Question Pairs is also a semantic equivalence classification dataset but the sentence pairs are extracted from the community question-answering website Quora. 3) SST-2: Stanford-Sentiment Treebank is a sentiment classification dataset. 4) CoLA: Corpus of Linguistic Acceptability with a task of linguistic acceptability of a sentence. 5) QNLI: Question-answering natural language inference is a dataset with a labelling task indicating whether the answer logically entails the question's premise. 6) MNLI: Multi-Genre Natural Language Inference also contains sentence pairs as premise and hypothesis, the task is to classify them as entailment, contradiction or neutral."}, {"title": "B. Baselines", "content": "We compare the model against various baselines that are detailed below:\n1) BERT model: In this baseline, we report the results of the original BERT backbone. We assume that the BERT model is deployed on the mobile device and only processing cost is incurred. This is the main baseline for us.\n2) Random: In this baseline, the incoming sample is randomly assigned to one of the given devices i.e. mobile, edge or the cloud. This is created to show that our assignment based on the pooling of samples makes a significant difference.\n3) Early-Exit: is the baseline where we assume that the model is deployed completely on the mobile device. This baseline shows that splitting the model also helps due to the presence of hard samples.\n4) AdaEE: This method is an adaptive method that uses multi-armed bandits to learn the optimal threshold to decide offloading in an edge-cloud co-inference setup.\n5) I-SplitEE: This method learns the optimal splitting layer based on the accuracy-cost trade-off in an online setup. The method uses multi-armed bandits to learn the optimal layer in an edge-cloud co-inference setup where the test dataset contains distortions.\n6) Ours-F: is our method that uses a fixed pool average and does not update it during inference.\n7) Ours-D: is our method that dynamically updates the pool averages and covers any domain shift occurring during inference.\nWe use the same hyperparameters for all the baselines as given in their respective codebases. The cost for all the baselines is calculated using our cost structure which is very similar to most of the previous methods.\nFollowing this, we detail the training and inference procedure. There are three key phases in our experimental setup."}, {"title": "C. Training the backbone", "content": "To evaluate our method, we use the widely accepted BERT-base/large model. We add a linear output layer after each intermediate layer of these models. We split the dataset into three parts: 80% for training, 10% for validation and 10% for test. We closely follow the training procedure as described in the paper [27]. We train the backbone using the train split. We run the model for 5 epochs. We also perform a grid search over a batch size of {8,16,32} and learning rates of {1e-5, 2e-5, 3e-5, 4e-5, 5e-5} with Adam [39] optimizer. We apply an early stopping mechanism and select the model with the best performance on the validation set."}, {"title": "D. Pool creation and cost", "content": "We create the pool using the validation split of the dataset. The values of m and n are chosen using the cost structure and we choose m = 3,6 and n = 6,12 for BERT-base and large respectively. The set S of thresholds is chosen as ten equidistant values from 1/C to 1.0 where C denotes the number of classes. The reason for not choosing any value below 1/C as any threshold below this value is extraneous due to the definition of the confidence values. The value a is chosen by solving the Equation 2."}, {"title": "E. Inference", "content": "During inference, we use a batch size of 1 as data arrives sequentially. As a sample arrives, the word embedding of a sample is calculated on the mobile device. Then the distance of the word embedding of the sample is calculated against pool averages and the sample is assigned to the closest pool average. If the closest pool is the easy pool, then the sample is inferred on the mobile device. If the closest is the moderate pool, then the sample is offloaded to the edge device. Else, the sample is offloaded to the cloud.\nAll the experiments were conducted on NVIDIA RTX 2070 GPU with an average runtime of ~ 3 hours and a maximum runtime of ~ 10 hours for the MNLI dataset."}, {"title": "V. RESULTS", "content": "In Table I and II, we show the main results of our paper, our method outperforms all the existing baselines both in terms of cost as well as accuracy for both BERT-base and large models. The reduction in call is larger for the BERT-large model which is intuitive as the large variant is more overparameterized.\nThe BERT model has a higher cost as all the samples are required to pass through the final layer and there are no exits attached. Due to this the accuracy of this model is comparable to our method. In the random assignment of samples to any of the devices, the loss in accuracy is due to the fact that sometimes even the hard samples are assigned to the mobile device, while the increase in cost is due to the fact that easy samples are sometimes assigned to the cloud. The vanilla early exit method gets a lower accuracy as the threshold chosen for exiting is not chosen based on any optimization algorithm but only through a validation set. This signifies the importance of the choice of threshold using the reward function 2. AdaEE also has lower accuracy as the method mostly works better under domain change scenarios, but in our case the domain shift is minor, it simply reduces to an early exit model with dynamic learning of threshold. Due to this dynamic learning of threshold, it outperforms vanilla early exiting. Finally, the I-SplitEE model also has lower accuracy again due to the case that, it works better in domain shift scenarios. In terms of cost, these models are higher as they require all the samples to be processed on the mobile device before offloading.\nOur method outperforms all the baselines, the higher accuracy comes from the appropriate assignment of the sample to various devices and a smaller cost as compared to other methods since all the complexity of the sample is decided based on the word embedding that does not require much processing on mobile reducing the processing cost to a larger extent while in other methods, this cost is really high as it is not removed for any sample.\nAlso, note that for some datasets our method even outperforms the vanilla BERT inference, this is because of the overthinking issue during inference. This issue occurs when an easy sample is passed through the complete backbone leading to the extraction of irrelevant features which in turn results in a wrong prediction as pointed out in [29]."}, {"title": "VI. ABLATION STUDY AND DISCUSSION", "content": "In this section, we perform ablation studies and also discuss the choice of layers using the computational powers of mobile and edge and offloading costs."}, {"title": "A. Individual device inference", "content": "We stated that our method uses a distributed inference method between mobile, edge and cloud. In Figure 2, we show the effect on cost and accuracy when all the samples are inferred on one of the given devices. It means that instead of distributing the inference, performing the inference on a single device. In figure 2a, we plot the accuracies of the individual devices and our model. Since the cloud contains the full-fledged DNN, it has the highest accuracy; however, our method sometimes outperforms the full-fledged DNN due to the overthinking issue in DNNs. In terms of cost, we know that the highest cost will be of the cloud. Hence, the cost in figure 2b is given in terms of the percentage of cost saved as compared to the cloud. Our method has a slightly higher cost than only mobile setup as it involves offloading of samples. Also, note that for easier tasks such as sentiment classification, most of the samples are inferred on the mobile device while for harder tasks such as entailment classification, more samples are offloaded hence a higher cost."}, {"title": "B. Cost Variations", "content": "In Figure 3, we show the variation in accuracy and cost if we alter one of the given costs. In the left figure in Figure 3, we alter $A_m$ i.e., the processing cost of mobile device, while keeping other costs constant. The accuracy is not affected in this case as we are increasing the processing cost that forces more samples to offload to the edge and cloud having more layers hence accuracy slightly improves. Similarly, if the processing cost $A_e$ is increased, accuracy again slightly improves. As we increase the offloading cost of the cloud $o_c$, we observe that there is a drop in accuracy. This is expected as higher offloading costs for the cloud will set lower thresholds such that most of the samples are inferred locally or at the edge and do not offload to the cloud which in turn lowers the accuracy. Note that in this setup, other costs are kept constant. Also, our method is robust to changes in different types of costs i.e., the loss in accuracy is minimal when cost values vary."}, {"title": "VII. CONCLUSION", "content": "We address the inference of large DNNs on mobile devices using the complexity of the input samples. We propose a method that utlizes early exits to decide the complexity of samples. It minimizes the cost of inference by assigning appropriate amount of resources required to infer the incoming sample between mobile, edge and cloud. If the task is easy and require less computation then most of the samples are inferred locally while if the task is hard, then most of the samples are offloaded maintaining accuracy. Our method is robust to changes in cost values i.e., various mobile and edge devices. Experiments on various NLP tasks show the significance of our work where the drop in accuracy is (< 0.3%) while reducing the cost (> 43%) as compared to final layer on the cloud."}]}