{"title": "Challenging the Performance-Interpretability Trade-off: An Evaluation of Interpretable Machine Learning Models", "authors": ["Sven Kruschel", "Nico Hambauer", "Sven Weinzierl", "Sandra Zilker", "Mathias Kraus", "Patrick Zschech"], "abstract": "Machine learning is permeating every conceivable domain to promote data-driven decision support. The focus is often on advanced black-box models due to their assumed performance advantages, whereas interpretable models are often associated with inferior predictive qualities. More recently, however, a new generation of generalized additive models (GAMs) has been proposed that offer promising properties for capturing complex, non-linear patterns while remaining fully interpretable. To uncover the merits and limitations of these models, this study examines the predictive performance of seven different GAMs in comparison to seven commonly used machine learning models based on a collection of twenty tabular benchmark datasets. To ensure a fair and robust model comparison, an extensive hyperparameter search combined with cross-validation was performed, resulting in 68,500 model runs. In addition, this study qualitatively examines the visual output of the models to assess their level of interpretability. Based on these results, the paper dispels the misconception that only black-box models can achieve high accuracy by demonstrating that there is no strict trade-off between predictive performance and model interpretability for tabular data. Furthermore, the paper discusses the importance of GAMs as powerful interpretable models for the field of information systems and derives implications for future work from a socio-technical perspective.", "sections": [{"title": "1 Introduction", "content": "Machine learning (ML) has made significant advancements in recent years, allowing for the automation of many tasks related to predictive decision-making (Janiesch, Zschech, & Heinrich, 2021). Promising examples can be found in various fields such as e-commerce (Ghavamipoor & Hashemi Golpayegani, 2020), transportation (Balster, Hansen, Friedrich, & Ludwig, 2020), industrial maintenance (Landwehr, K\u00fchl, Walk, & Gn\u00e4dig, 2022; Zschech, Heinrich, Bink, & Neufeld, 2019), and business process monitoring (Kratsch, Manderscheid, R\u00f6glinger, & Seyfried, 2021; Oberdorf et al., 2022). However, many advanced ML models, such as boosted decision trees and deep neural networks, suffer from black-box characteristics. This means that the mathematical functions they learn between input features and target values are so complex that it is practically impossible for humans to understand how these models generate a prediction for a given data instance (Bauer, Hinz, van der Aalst, & Weinhardt, 2021). This lack of transparency can lead to a lack of trust in using these models for high-stakes decision tasks, such as in healthcare, finance, and criminal justice (Rudin, 2019; Thiebes, Lins, & Sunyaev, 2021).\nTo address the lack of transparency, interpretable ML models have been developed (Barredo Arrieta et al., 2020). The structure of interpretable models is constrained in some way to provide a better understanding of how predictions are generated (Rudin, 2019). Traditional representatives are linear models and decision trees, which are easy to analyze and comprehend, but often too limited to capture more complex relationships. As a remedy, more advanced models have been proposed to mitigate the trade-off between predictive performance and interpretability. Of particular interest are generalized additive models (GAMs), which are currently experiencing a renaissance in the debate about comprehensible decision support (Barredo Arrieta et al., 2020; Nori, Jenkins, Koch, & Caruana, 2019; Zschech, Weinzierl, Hambauer, Zilker, & Kraus, 2022).\nIn general, GAMs are a type of ML model that allows for the estimation of non-linear relationships between predictor variables (i.e., features) and a response variable (i.e., target). They are an extension of linear models, in which the linearity assumption is relaxed, allowing for more flexible and powerful modeling capabilities (Hastie & Tibshirani, 1986). More specifically, the relationship between each feature and the target is modeled separately in so-called shape functions, and the results are combined in an additive manner. This allows the model to capture arbitrary relationships while remaining fully interpretable, which provides crucial benefits for model analysis and debugging purposes (Lou, Caruana, & Gehrke, 2012)."}, {"title": "2.2 Generalized Additive Models", "content": "In this study, we focus on the specific model family of GAMs as a particularly powerful class of intrinsically interpretable ML models that balance transparency with high accuracy. To achieve this goal, GAMs build relationships between input features and the target by summing up several distinct non-linear mappings, known as shape functions\u00b9 (Lou et al., 2012). As such, they combine the simplicity of linear models with the flexibility of non-linear mappings by replacing static model coefficients with flexible shape functions to capture more complex relationships between input features and the prediction target.\nMore formally, this model structure can be expressed as follows. Let $D = (X,y)$ denote a training dataset, where $X = (x_1,...,x_n) \\in R^{N\\times n}$ is the feature matrix comprising N samples and n features and y denotes the target variable. For a regression task, y is a real value ($y \\in R^{N}$), while for a binary classification task, y is a binary value ($y \\in \\{1,0\\}^{N}$). A generalized additive model is then defined in the following form by taking a feature matrix X as input and calculating a prediction \u0177 as output\n$g(\\hat{y}) = f_1(x_1) +\\ldots+f_n(x_n),$ (1)\nwhere g(\u00b7) is called link function\u00b2 and $f_i(\u00b7)$ is the shape function for a feature x\u1d62 (Lou et al., 2012)."}, {"title": "2.3 Variants of Generalized Additive Models", "content": "The original GAM refers to a model in which the shape functions represent regression splines (Hastie & Tibshirani, 1986). Splines are a class of mathematical functions defined over intervals, typically composed of piecewise segments joined at specific points known as knots. They serve as flexible tools for approximating complex relationships in data, employing a variety of functions, such as polynomial functions or radial basis functions, to achieve smoothness and high flexibility in modeling. To address unnecessary model complexity and avoid overfitting, spline-based GAMs can also be enriched with advanced techniques such as shrinkage and automated smoothness estimation techniques to encourage smoother and more parsimonious models (Wood, 2017). Moreover, some authors integrate spline-based GAMs into ensemble learning strategies in order to further enhance their predictive performance (e.g., De Bock, Coussement, & Van den Poel, 2010).\nA second stream of research focuses on tree-based learning approaches for fitting shape functions in GAMs (Caruana et al., 2015). To this end, Lou et al. (2012) argue that splines are often too smooth for real-world applications and that higher predictive performance is achievable by using tree-based step functions. More specifically, the authors suggested using bagged and boosted tree ensembles to increase model flexibility and to fit more detailed shape functions. The authors further extended their approach by incorporating pairwise interaction terms, which can still be visualized as two-dimensional heatmaps (Lou, Caruana, Gehrke, & Hooker, 2013). Subsequently, the authors made their algorithm publicly available as an easy-to-use implementation known as explainable boosting machine (EBM) (Nori et al., 2019).\nAnother stream of research has shifted the focus to the development of GAM extensions inspired by tailored artificial neural networks. For example, Agarwal et al. (2021) proposed neural additive model (NAM) in which feature-wise shape functions are learned via deep neural networks consisting of multiple hidden layers. To fit the rapid changes in the target variables of real-world data, the authors introduce exp-centered hidden units. These units allow to capture sharp changes in the output.\nYang et al. (2021b) proposed generalized additive models with structured interactions (GAMI-Net). The basic architecture is similar to that of a NAM, using a simple GAM-based structure in the form of a disentangled feed-forward network with multiple additive sub-networks. To reduce unnecessary model complexity and avoid overfitting, GAMI-Net integrates sparsity constraints to select the most relevant features to receive a compact model. Furthermore, GAMI-Net has a similar strength to EBM in that it is able to incorporate pairwise interactions between individual features, which can lead to better predictive performance. At the same time, this functionality requires further model constraints such as heredity and marginal clarity constraints to retain structural interpretability and avoid mutual absorption between main effects and pairwise interactions.\nA different architectural design was sought with the proposal of explainable neural network (xNN) (Vaughan, Sudjianto, Brahimi, Chen, & Nair, 2018) and its successor enhanced explainable neural network (ExNN) (Yang, Zhang, & Sudjianto, 2021a). Similar to GAMI-Net, both models are based on multiple sub-networks and regularization terms to retain sparsity and receive a compact model. However, instead of using a simple GAM structure, both models are based on the structure of additive index models by adopting the idea of a projection pursuit regression (Friedman & Stuetzle, 1981). This structure generally violates the principle of univariate feature mappings due to an additional projection layer that fully connects all input features to the following sub-networks. As a result, each feature can possibly have a partial contribution to all corresponding shape functions, to achieve a higher level of predictive performance.\nMore recently, Kraus et al. (2024) proposed another novel GAM variant, called interpretable generalized additive neural networks (IGANN). In the first step, the model initializes shape functions linearly and then incrementally adapts to potential non-linearities. That is, IGANN is based on the concept of linear modeling and only resorts to non-linearities if the underlying data requires it. For the incremental adaptation to non-linear shapes, IGANN incorporates the principle of gradient boosting. Specifically, it uses a boosted ensemble of tailored sparsified neural networks, where each network represents an extreme learning machine and acts as a weak learner to gradually improve the performance."}, {"title": "2.4 Comparative Evaluation Studies", "content": "All of the GAM-based models presented above offer innovative algorithmic concepts and components to balance model accuracy and transparency. While the main principle remains almost identical for all models, some of the approaches differ greatly in their incorporated learning principles and model constraints. Therefore, it is worthwhile to evaluate and compare their merits and limitations for different prediction tasks and datasets.\nA few authors have conducted experiments to compare their proposed GAM extensions to traditional ML baselines and competing models. For instance, Lou et al. (2012) compared spline-based models with tree-based GAMs as well as linear/logistic regression (LR) and random forest (RF) as lower and upper bound baselines. Similarly, Yang et al. (2021b) evaluated GAMI-Net against several benchmark models, including EBM, splines, LR, RF, extreme gradient boosting (XGB), and multi-layer perceptron (MLP), using a large number of datasets. Agarwal et al. (2021) benchmarked NAM against EBM and traditional approaches, albeit with a limited number of datasets.\nBeyond that, only a few studies have assessed the characteristics of different intrinsically interpretable models so far. For instance, Chang, Tan, Lengerich, Goldenberg, and Caruana (2021) examined a series of GAMs, such as spline-based models like penalized cubic regression splines and tree-based models like EBM, using benchmark datasets from real-world applications and simulations. They evaluated the models both qualitatively and quantitatively. Hohman, Head, Caruana, DeLine, and Drucker (2019) incorporated GAMs into a visual analytics tool to examine how data scientists interact with shape functions. Meanwhile, Kaur et al. (2020) explored how data science experts use and evaluate interpretable models, including both intrinsically and post-hoc interpretable models like SHAP to compare the results of both paradigms.\nIn summary, when considering the focus of related work, it currently lacks a cross-model comparison to evaluate the merits and limitations of different GAMs from a broader and more neutral perspective. Likewise, it lacks a thorough analysis that examines the performance gap between these advanced interpretable models and commonly applied black-box models. Especially for application-oriented sciences such as the IS community,"}, {"title": "3 Research Approach", "content": "We perform a series of computational experiments, which we describe in the following. In the first part, we focus on the assessment of the predictive performance, for which we outline the selection of evaluated models (Section 3.1), our collection of datasets (Section 3.2), and our experimental set-up and evaluation pipeline (Section 3.3). Thereafter, we consider the models' visual outputs to assess their level of functional interpretability (Section 3.4)."}, {"title": "3.1 Selection of Models", "content": "For our comparative evaluation study, we consider seven different variants of GAMs, for which publicly accessible implementations are available. This includes (i) penalized B-splines (P-Splines) (Eilers & Marx, 1996), (ii) penalized thin plate regression splines with shrinkage (TP-Splines) (Wood, 2003), (iii) EBM (Nori, Jenkins, Koch, & Caruana, 2021), (iv) NAM (Agarwal et al., 2021), (v) GAMI-Net (Yang et al., 2021b), (vi) ExNN (Yang et al., 2021a), and (vii) IGANN (Kraus et al., 2024). The implementations are provided by the respective authors of the proposed GAM variants. For P-Splines, we use the Python package pyGAM (Serv\u00e9n, 2021). For TP-Splines, we adopt models from the mgcv package implemented in R (Wood, 2023) using a Python wrapper (Chang et al., 2021). Additionally, we also experiment with other spline-based models from the mgcv package, including penalized cubic regression splines with and without shrinkage and different configurations. However, since they only achieve subordinate predictive performance compared to TP-Splines, they are excluded from further model comparison (see Appendix E).\nFurthermore, we include several baseline models for a broader comparison, which are commonly used in the ML and IS community for prediction tasks using tabular data. Specifically, we include linear models (logistic/linear regression, LR) and decision trees (DTs) as common representatives of intrinsically interpretable models. To consider widely known black-box models, we include RF, XGB, CatBoost, MLP, and TabNet (De Caigny, De Bock, & Verboven, 2024; Schoormann, Strobel, M\u00f6ller, Petrik, & Zschech, 2023). For the implementation of the models, we use the corresponding Python packages from the scikit-learn library, except for XGB (Chen & Guestrin, 2016), CatBoost (Prokhorenkova, Gusev, Vorobev, Dorogush, & Gulin, 2018), and TabNet (Arik & Pfister, 2021), where we use Python implementations from the respective developers/authors."}, {"title": "3.2 Selection of Datasets", "content": "For an extensive comparison of models, we utilize a variety of benchmark datasets. We align our dataset selection with previous evaluation studies (e.g., Roy et al., 2019; Yang et al., 2021b), using datasets from recognized public repositories such as the UCI Machine Learning Repository\u00b3 and Kaggle4. These repositories offer diverse, high-quality datasets for benchmarking purposes, allowing for a thorough evaluation of the different models. After reviewing the repositories, we select twenty diverse datasets that cover a broad range of real-world applications relevant to various business, organizational, and societal issues. To ensure a balanced distribution between different prediction tasks, we use ten datasets tailored for predicting categorical outcomes (binary classification task, CLS) and an equal number for predicting numerical values (regression task, REG). Moreover, we limit our experiments to medium-sized datasets with up to 150,000 samples to maintain a manageable level of computational complexity. This is particularly important given the extensive hyperparameter tuning in our model evaluation procedure (cf. Section 3.3)."}, {"title": "3.3 Experimental Set-up and Evaluation Pipeline", "content": "To provide a fair comparison, we integrate all datasets and models into a shared environment to run the experiments under the same conditions.5 The predictive performance is measured in terms of commonly used evaluation metrics. For classification tasks, we primarily focus on the area under the receiver operating characteristic curve (AUROC) to assess the models' ability to correctly rank positive and negative instances. For regression tasks, we consider the root mean squared error (RMSE) to quantify the average magnitude of errors between the models' predicted values and the actual values. To determine the best-performing models, we calculate the average ranks across all datasets, with higher ranks (i.e., first, second, third, etc.) indicating better"}, {"title": "3.4 Assessment of Interpretability", "content": "To comprehensively analyze the behavior of all models and assess their level of functional interpretability, our investigations are divided into three parts, offering different perspectives.\nIn the first part, we consider each model individually and examine the GAMs' general ability to externalize their inner workings through human-understandable feature plots to verify their degree of intrinsic interpretability. More specifically, we follow the notion that an intrinsically interpretable model must be able to provide transparency at the level of individual predictions (i.e., local interpretability), but also at the level of the entire model (i.e., global interpretability), in order to enable a full understanding of how a model works (Du et al., 2019). To this end, we look into the GAMs' entire output as well as specific shape functions and assess the extent to which each model is able to visually reveal how input features are processed in order to produce a"}, {"title": "4 Results", "content": null}, {"title": "4.1 Assessment of Predictive Performance", "content": "This section presents the evaluation results for assessing the predictive performance. First, we consider the setting with default model configurations, and then we continue with the results of the models with tuned hyperparameters. Finally, we also consider the performance differences between both settings. The best overall performance for each dataset is highlighted in bold, whereas the best result among the interpretable models is underlined.5 Moreover, the combination of certain datasets with TP-Splines results in some anomalies where the model does not converge on all five folds, so a subset of folds is used to compute the aggregated performance results. These cases are highlighted in italics.\nDefault setting. Table 3 summarizes the prediction results of the default setting for the classification and regression tasks. The results show that the best prediction scores for each dataset are spread across a variety of models. That is, there is no single model that achieves the best results across all datasets. However, there are some remarkable tendencies. In particular, it can be seen that the intrinsically interpretable GAMs collectively deliver the highest performance in 6 out of 20 datasets. Specifically for classification, this is the case for 4 out of 10 datasets, and for regression, this is the case for 2 out of 10 datasets.\nIn contrast, the black-box models perform best in 14 out of 20 datasets. Nevertheless, the performance difference between the best black-box models and the best interpretable models is only marginal. For example, for the classification tasks, the smallest difference in performance between the best interpretable model and"}]}