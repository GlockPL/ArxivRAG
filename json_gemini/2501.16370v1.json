{"title": "Advanced Physics-Informed Neural Network with Residuals for Solving Complex Integral Equations", "authors": ["Mahdi Movahedian Moghaddam", "Kourosh Parand", "Saeed Reza Kheradpisheh"], "abstract": "In this paper, we present the Residual Integral Solver Network (RISN), a novel neural network architecture designed to solve a wide range of integral and integro-differential equations, including one-dimensional, multi-dimensional, ordinary and partial integro-differential, systems, and fractional types. RISN integrates residual connections with high-accurate numerical methods such as Gaussian quadrature and fractional derivative operational matrices, enabling it to achieve higher accuracy and stability than traditional Physics-Informed Neural Networks (PINN). The residual connections help mitigate vanishing gradient issues, allowing RISN to handle deeper networks and more complex kernels, particularly in multi-dimensional problems. Through extensive experiments, we demonstrate that RISN consistently outperforms PINN, achieving significantly lower Mean Absolute Errors (MAE) across various types of equations. The results highlight RISN's robustness and efficiency in solving challenging integral and integro-differential problems, making it a valuable tool for real-world applications where traditional methods often struggle.", "sections": [{"title": "1 Introduction", "content": "Integral and integro-differential equations are foundational tools in many fields of science and engineering, modeling a wide range of phenomena from physics and biology to economics and engineering systems [1-3]. These equations describe processes that depend not only on local variables but also on historical or spatial factors, making them essential for understanding systems with memory effects, hereditary characteristics, and long-range interactions [4-7]. Despite their importance, solving integral and integro-differential equations is a challenging task due to the complexity of their integral operators, especially when extended to multi-dimensional or fractional forms [2, 8].\nClassical numerical methods, such as finite difference [9, 10], finite element [11, 12], and spectral methods [13\u201315], have long been used to approximate solutions to these equations. However, these methods often suffer from several limitations. For example, they require fine discretization of the problem domain, leading to high computational costs and memory usage, especially for high-dimensional problems [16-19]. Additionally, these methods can struggle with non-local operators, singular kernels, or fractional derivatives, which introduce further complexities [20]. Even though more recent approaches, like Gaussian quadrature and collocation methods, provide enhanced accuracy for certain cases, their applicability is often restricted to simpler, well-behaved systems [5, 8, 20-22].\nIn recent years, deep learning has emerged as a powerful tool to overcome the limitations of traditional statistical methods [23, 24]. Physics-informed neural networks (PINNs) represent a major advance in this direction by directly integrating the governing physical laws with loss of neural activity [25-27]. This allows PINNs to learn solutions to differential equations using observational data while ensuring consistency with underlying physics [28, 29]. PINNs have shown promising results in solving various types of differential equations, including ordinary and partial differential equations [30-32]. However, PINNs are not without their limitations. The depth of the network often exacerbates issues such as vanishing gradients, and the absence of residual connections can make training deep networks unstable, especially when dealing with highly complex or multi-dimensional integral and integro-differential equations [33-36].\nTo overcome these challenges, we propose the Residual Integral Solver Network (RISN), a novel extension of the PINN framework specifically designed to solve integral and integro-differential equations. RISN enhances the original PINN architecture by incorporating residual connections, a proven technique in deep learning that significantly improves gradient flow, reduces training instability, and allows for deeper network architectures. These residual connections are crucial for ensuring stable training, particularly when handling multi-dimensional or highly non-linear systems [25, 32, 33]. Furthermore, RISN leverages high-accuracy numerical techniques, such as Gaussian quadrature for efficient and precise calculation of integral terms, and fractional operational matrices to handle fractional derivatives with minimal error [21, 37]. This combination of deep learning with classical numerical methods allows RISN to achieve high accuracy and stability across a wide range of equation types, including those that pose challenges for traditional methods and standard PINNs."}, {"title": "2 Literature review", "content": "The solution of integral and integro-differential equations plays a critical role across various scientific disciplines, including physics, biology, and engineering. These equations model systems with local and non-local interactions, which makes them essential for understanding phenomena ranging from heat conduction and population dynamics to quantum mechanics and fluid dynamics. Traditionally, methods such as finite difference and finite element approaches have been employed to solve these equations [38, 39]. Although these techniques have been widely successful, they face significant computational challenges, particularly when applied to high-dimensional problems [40-42].\nThe finite difference method (FDM) approximates differential operators by discretizing the domain, making it simple to implement for many problems. However, FDM struggles with large-scale problems due to the need for fine grid discretization, which leads to high computational cost and memory usage [43]. Finite element methods (FEM), while more versatile and effective for irregular domains, encounter similar issues in high-dimensional systems, where the complexity of mesh generation and element assembly increases exponentially with the number of dimensions [43, 44]. The inefficiency of these methods in handling complex multi-dimensional and fractional equations has spurred the development of alternative approaches, including machine learning-based methods [44, 45].\nIn recent years, Artificial Neural Networks (ANNs) have emerged as a powerful alternative for solving complex mathematical equations, including Fredholm and Volterra integral equations. Early work by [46] demonstrated the use of ANNs in solving Fredholm integral equations, highlighting their potential for improving computational efficiency and accuracy."}, {"title": "3 Methodology", "content": "The Residual Integral Solver Network (RISN) is a novel neural network architecture to address integral and integro-differential equations efficiently. Inspired by traditional neural networks, RISN integrates residual connections to mitigate the vanishing gradient problem and enable deeper network training. The model is structured to handle both differential and integral operators by incorporating these connections, which not only improve training stability but also enhance the accuracy of the solutions. By combining classical numerical techniques like Gaussian numerical quadrature and fractional operational matrices with the residual structure, RISN is capable of solving a wide range of complex mathematical problems, including integral and fractional integro-differential equations. This structure is chosen for solving integral equations due to its ability to maintain stability and accuracy in deep networks. This section provides a detailed explanation of the RISN structure and the numerical techniques employed to achieve high precision in solving these equations."}, {"title": "3.1 The model structure", "content": "The RISN is an advanced neural network architecture designed to address integral and integro-differential equations by integrating residual connections. The structure of RISN is inspired by traditional neural networks but enhances them with residual connections to improve training efficiency and solution accuracy.\nResidual connections, a critical component in our model, allow for the preservation of gradient flow during backpropagation, thereby preventing the vanishing gradient problem typically seen in deep networks. This feature is particularly important in multi-dimensional and fractional integro-differential equations, where the complexity of the solution requires a stable and accurate gradient propagation mechanism.\nThe architecture of the RISN, as illustrated in Figure 1, is designed to solve integral and integro-differential equations by combining residual deep neural networks with classical numerical methods. This figure provides an overview of the data flow and interaction between different parts of the network. The equation (1) represents the target system, where F(u)(x), D(u)(x), and I(u)(x) correspond to the differential, integral, and source terms, respectively.\n```latex\nF(u)(x) + D(u)(x) + I(u)(x) = S(x),\n```\n(1)\nThe model begins with the training data, which feeds into different branches corresponding to the components of the equation. The source and function terms are handled through the combination of F(u)(x) + S(x). For the differential operator D(u)(x), two approaches are used: automatic differentiation for ordinary derivatives"}, {"title": "3.2 Numerical techniques for integral and fractional Differential operators", "content": "In this work, two key numerical techniques have been employed to enhance the accuracy and efficiency of solving integral and fractional integro-differential equations: Gaussian numerical quadrature and fractional operational matrices. Both methods, though well-established in the field of numerical analysis, were first integrated into the"}, {"title": "3.2.1 Gaussian numerical quadrature", "content": "Gaussian quadrature, particularly well-suited for evaluating definite integrals, is leveraged in this model to compute the integral terms that arise in the solution of integro-differential equations. As described in the work of PINNIES, this method offers a high degree of accuracy with relatively few evaluation points, making it a computationally efficient choice. In the context of RISN, Gaussian quadrature allows for the precise calculation of integral terms, which are critical to solving the targeted problems. Its inclusion ensures that the integral operators are handled with minimal numerical error, thus enhancing the overall accuracy of the model."}, {"title": "3.2.2 Fractional operational matrices", "content": "Fractional differential operators are essential when dealing with integro-differential equations that exhibit non-local behavior, such as systems with memory effects. The method of using fractional operational matrices, initially introduced in the context of PINNs by PINNIES, allows for a structured and efficient way of handling these operators. By representing fractional derivatives in matrix form, the RISN model can compute fractional derivatives with high precision, seamlessly integrating this approach into the neural network architecture. This not only improves the accuracy of fractional derivative calculations but also streamlines the process within the network's training and inference stages.\nIn these problems, fractional derivatives, which generalize the concept of integer-order differentiation, allow for a more accurate representation of processes with memory effects or non-local dependencies. RISN, combined with operational matrices, efficiently handles these derivatives by leveraging its residual connections to maintain stability in training.\nThe combination of these two techniques-Gaussian numerical quadrature and fractional operational matrices\u2014within RISN reflects the practical benefits of merging classical numerical methods with advanced machine learning models. While these techniques do not significantly differ from their original implementation in the PINN framework, their application within RISN ensures that the model can solve a wide range of integral and integro-differential equations more effectively.\nThe RISN model leverages residual connections to enhance the training process and accuracy of solving integral and integro-differential equations. By integrating residual connections with a structured loss function, RISN improves solution fidelity and robustness, making it a powerful tool for addressing complex mathematical problems. However, the proposed method may require additional tuning when dealing with highly complex problems."}, {"title": "4 Experimental results", "content": "In this section, we present the results obtained from solving various types of integral and integro-differential equations using the Residual Integral Solver Network (RISN). The examples provided cover a range of equations, including one-dimensional and multi-dimensional problems, as well as systems of equations. The structure of this section is as follows: first, we outline the setup for each example, including the equation type, domain, and kernel functions. Next, we present the results for each test case and compare the performance of RISN with traditional methods, such as PINN-based models.\nThe RISN model used in these experiments consists of a fully connected neural network with seven hidden layers, each containing 20 neurons. The Tanh activation function is applied in each layer, and the model is optimized using the L-BFGS optimizer with a learning rate of 0.01. The training points size and Gaussian quadrature order are set to 50, and the loss function incorporates residuals for the target equation, as well as terms for initial and boundary conditions, with appropriate weighting parameters \u03bb. Additionally, Gaussian quadrature (specifically Gauss-Legendre) is employed for accurate numerical quadrature, and fractional operational matrices are used for handling fractional derivatives where necessary.\nThe proposed method was implemented in Python 3.11 using the PyTorch framework (Version 2.3.1), which facilitates automatic differentiation and efficient handling of large-scale computations. All experiments were performed on a personal computer equipped with an Intel Core i7-3520M CPU and 16GB of RAM, running on the Ubuntu Linux distribution. This setup ensured that the model training and testing processes were performed efficiently, with sufficient computational resources to handle the complexity of the equations solved by the RISN model.\nThe performance of the RISN model was evaluated using the Mean Absolute Error (MAE) as the primary error metric, ensuring a clear assessment of the model's accuracy in solving the various types of integral and integro-differential equations."}, {"title": "4.1 Integral equations", "content": ""}, {"title": "4.1.1 One-dimensional integral equations", "content": "In this section, we address one-dimensional integral equations of the form:\n```latex\n\u03bau(x) = S(x) + \\int_{\\Delta} K(x,t)(u(t)) dt,\n```\nwhere S(x) is the source term, K(x, t) represents the kernel of the integral operator, and ((x) characterizes the linearity of the problem. The constant parameter \u03ba\u2208 R, if \u03ba = 0 is a first-kind integral equation (IE); otherwise, it is classified as a second-kind IE. The domain of the problem, denoted as \u2206 = [a, b], applies to all equation types. For the Fredholm operator, we assume the interval a, b = 0,1, and for the Volterra operator, we take g(x), h(x) = 0, x for x \u2208 \u0394. In the case of the Volterra-Fredholm"}, {"title": "4.1.2 Multi-dimensional integral equations", "content": "In this section, we assess the proposed approach for solving multi-dimensional integral equations. We specifically examine a two-dimensional integral equation structured as follows:\n```latex\n\u03bau(x,y) = S(x, y) + \\int_{\\Delta} \\int_{\\Delta} K(x, y, s, t)u(s, t) dt ds,\n```"}, {"title": "4.1.3 System of integral equations", "content": "A system of integral equations involves multiple unknown functions that are interconnected and appear within the integral terms. For an integer M\u2265 2, such a system can be expressed mathematically as:\n```latex\n\u03bau_i(x) = S_i(x) + \\sum_{A i=1} \\int_{\\Delta} K_{i,l}(x, t)u_l(t) dt.\n```\n(5)\nThese systems often combine different types of integral equations previously discussed. For example, when K = 0, the system reduces to a set of first-kind integral equations."}, {"title": "4.2 Integro-differential equations", "content": "In this section, we present the results of solving various types of integro-differential equations using the proposed RISN model and compare its performance with the traditional PINN approach. The section is divided into subsections, each addressing a specific category of equations, including ordinary integro-differential equations, partial integro-differential equations, systems of integro-differential equations, and fractional integro-differential equations. For each category, we first introduce the specific equation, followed by an analysis of the results obtained using RISN in comparison with the classical PINN method. The goal is to highlight RISN's superior performance and higher accuracy across different types of equations."}, {"title": "4.2.1 Ordinary integro-differential equations", "content": "In the first experiment, we examine the following form of an ordinary integro-differential equation:\n```latex\n\u03ba \\frac{d^\\nu}{dx^\\nu} u(x) = S(x) + \\int_{\\Delta} K(x,t)(u(t)) dt,\n```\nwhere v \u2208 Z+ represents the order of differentiation. For all variations of this problem, two boundary conditions are applied, with the exact solution providing the necessary data. The remaining setups will follow the methodology outlined in the previous section on one-dimensional integral equations."}, {"title": "4.2.2 Partial integro-differential equations", "content": "In the following experiment, we consider a two-dimensional unknown function described by the partial integro-differential equation:\n```latex\n\\frac{\\partial}{\\partial t} u(x,t) = S(x, y) + \\int_{\\Delta_t} K(x,t,s)((u(x, s)) ds,\n```"}, {"title": "4.2.3 System of integro-differential equations", "content": "Integro-differential systems involve multiple interconnected unknown functions, each appearing within integral terms. For an integer M \u2265 2, such a system can be expressed mathematically as:\n```latex\n\u03ba \\frac{d}{dx^\\nu} u(x) = S(x) + \\sum_{\\Lambda=1} \\int_{\\Delta} K_{i,l}(x, t)u_l(t) dt.\n```\n(7)\nThese systems represent a combination of the integral equation types previously discussed. In this section, we examine the case where M = 2, resulting in the following system of equations:\n```latex\nu (x) = S_1(x) + \\int_{\\Delta} [K_{1,1}(x, t)u_1(t) + K_{1,2}(x,t)u_2(t)] dt,\n```\n```latex\nu (x) = S_2(x) + \\int_{\\Delta} [K_{2,1}(x, t)u_1(t) + K_{2,2}(x,t)u_2(t)] dt,\n```\nsubject to boundary conditions when v > 0.\nTo approximate the unknown functions, we utilize two separate neural networks, represented as u\u2081 = MLP1(X) and u2 = MLP2(X) where X \u2208 RN\u00d71. Each network operates with its own set of weights, denoted by 0."}, {"title": "4.2.4 Fractional integro-differential equations", "content": "Fractional integro-differential equations combine fractional derivatives and integrals, making them ideal for modeling complex systems that exhibit both local and non-local behaviors, such as those with memory effects, hereditary characteristics, or anomalous diffusion. A typical form of such an equation is:\n```latex\nD^\\alpha u(t) = f(t, u(t)) + \\int_{0}^{t} K(t,\u03c4)u(\u03c4)d\u03c4, 0 < \u03b1 < 1.\n```\nIn this equation, Da denotes the Caputo fractional derivative of order a. The function u(t) is the unknown to be determined, while f(t, u(t)) is a known function depending on both time t and u(t). The integral term fo K(t, \u03c4)u(t)d\u03c4 captures the memory effects of the system, with K(t,T) acting as a kernel function that reflects how the system's past states u(\u03c4) influence its present behavior at time t.\n```latexD^{0.75} u(t) = -(\\frac{e^x}{6}) u(x) + \\frac{5}{\u0393(3.25)} x^{2.25} + \\int_{0}^{x} e^{x t} u(t) dt,\n```\n(9)\nwith the initial condition\n```latexu(0) = 0.\n```\n(10)\nThe exact solution to this equation is given by u(x) = x\u00b3. To compare the accuracy of the two models, we compute the MAE for both the classic PINN and the proposed RISN models."}, {"title": "4.3 Sensitivity analysis", "content": "In this section, we perform a sensitivity analysis to evaluate the effectiveness of the proposed RISN compared to the traditional PINN. The goal is to investigate how varying certain vital factors such as the number of hidden layers affects the model's accuracy and stability when solving the fractional integro-differential equation 9. Additionally, we analyze the training behavior of both models by comparing their loss convergence over several epochs. By doing so, we aim to highlight the improvements in training stability and error minimization introduced by the residual-based architecture of RISN, ultimately demonstrating its advantages over the standard PINN framework.\nThe analysis of these figures demonstrates that the RISN model, which leverages residual learning, consistently outperforms the classic PINN model in terms of both training stability and final error performance. The RISN model's ability to reduce oscillations in the loss curve and maintain a lower MAE across various hidden layer configurations indicates its suitability for solving fractional integro-differential equations more effectively than traditional PINNs. This suggests that incorporating residual connections into PINNs can significantly improve the learning process, leading to better convergence and accuracy in solving complex mathematical problems."}, {"title": "5 Results and Discussion", "content": "In this section, we present and analyze the numerical results obtained from solving various types of integral and integro-differential equations using the proposed Residual Integral Solver Network (RISN), comparing its performance with the classical Physics-Informed Neural Network (PINN). The evaluation includes a range of equations, from\none-dimensional and multi-dimensional integral equations to systems of equations and fractional integro-differential problems.\nOverall, RISN consistently demonstrates superior accuracy across all problem types, as reflected in the Mean Absolute Error (MAE) comparisons. For example, in both one-dimensional and multi-dimensional cases, RISN significantly outperforms PINN, reducing the error by up to an order of magnitude in some instances. This improvement is primarily attributed to incorporating residual connections, which enhance the model's ability to handle deeper networks and more complex kernels, particularly in multi-dimensional and high-dimensional problems.\nAdditionally, RISN's advantage is evident in problems involving singular kernels and non-local operators, where traditional PINNs often struggle with stability and convergence. The enhanced gradient flow provided by the residual connections in RISN mitigates issues like the vanishing gradient problem, allowing for more stable and accurate solutions, especially in multi-dimensional scenarios.\nIn cases where the complexity of the equation increased, such as with fractional integro-differential equations, RISN's ability to integrate classical numerical methods, like Gaussian quadrature and fractional operational matrices, played a crucial role in maintaining high accuracy and efficiency. This combination of deep learning with robust numerical techniques enabled RISN to perform consistently better than traditional PINNS.\nIn conclusion, the experimental results demonstrate that RISN provides a more accurate solution framework for a wide range of integral and integro-differential equations and proves to be more stable and efficient in handling complex, high-dimensional problems. These findings suggest that RISN can be a valuable tool for solving real-world problems where traditional methods fall short."}, {"title": "6 Conclusion", "content": "In this paper, we introduced the Residual Integral Solver Network (RISN), a novel approach for solving a wide range of integral and integro-differential equations, including one-dimensional, multi-dimensional, ordinary, systems of equations, and fractional problems. Through the integration of residual connections and high-accurate numerical techniques, such as Gaussian quadrature and fractional derivative operational matrices, RISN demonstrated significant improvements in both accuracy and stability compared to traditional Physics-Informed Neural Networks (PINN).\nThe experimental results showed that RISN consistently outperforms PINN, particularly in multi-dimensional and complex problems where traditional approaches often face challenges related to gradient vanishing and convergence issues. Residual connections allowed for deeper network architectures and better gradient flow, leading to lower Mean Absolute Errors (MAE) across all test cases.\nRISN's ability to handle singular kernels, non-local operators, and fractional derivatives with greater precision makes it a valuable tool for solving complex mathematical problems encountered in real-world applications. The combination of advanced deep learning architectures with well-established numerical methods positions RISN as a robust framework for addressing a diverse set of integral and integro-differential equations.\nHowever, RISN also has certain limitations. As with other machine learning-based methods, the performance of RISN is sensitive to the choice of hyperparameters, such as learning rate, batch size, and network depth. Fine-tuning these parameters can be computationally expensive, particularly for larger or more complex equations. Additionally, although RISN has proven effective for multi-dimensional problems, further optimization may be needed to handle even higher-dimensional systems or equations with extreme nonlinearity.\nIn terms of future work, there are several promising directions to explore. One potential area is the integration of RISN with adaptive mesh techniques, which could further enhance its ability to solve high-dimensional problems efficiently. Additionally, expanding RISN to more specialized domains, such as stochastic integro-differential equations, could open up new applications. Finally, further improvements to the architecture, such as incorporating advanced optimization techniques or exploring other forms of regularization, may lead to even more accurate and efficient solutions."}]}