{"title": "Audio-Driven Emotional 3D Talking-Head Generation", "authors": ["Wenqing Wang", "Yun Fu"], "abstract": "Audio-driven video portrait synthesis is a crucial and useful technology in virtual human interaction and film-making applications. Recent advancements have focused on improving the image fidelity and lip-synchronization. However, generating accurate emotional expressions is an important aspect of realistic talking-head generation, which has remained underexplored in previous works. We present a novel system in this paper for synthesizing high-fidelity, audio-driven video portraits with accurate emotional expressions. Specifically, we utilize a variational autoencoder (VAE)-based audio-to-motion module to generate facial landmarks. These landmarks are concatenated with emotional embeddings to produce emotional landmarks through our motion-to-emotion module. These emotional landmarks are then used to render realistic emotional talking-head video using a Neural Radiance Fields (NeRF)-based emotion-to-video module. Additionally, we propose a pose sampling method that generates natural idle-state (non-speaking) videos in response to silent audio inputs. Extensive experiments demonstrate that our method obtains more accurate emotion generation with higher fidelity.", "sections": [{"title": "I. INTRODUCTION", "content": "Audio-driven video portraits have become an important technology across a variety of applications, including digital humans, virtual reality, and the entertainment industry. Achieving realistic talking-head synthesis requires not only accurate lip synchronization and high image fidelity but also the accurate generation of emotional expressions. Recently, many works have been proposed to synthesize audio-driven video portraits [21], [6], [25], [8], [37], [33], [28]. However, they often introduce artifacts, produce unrealistic images, or fail to capture the details of the target person. For instance, Wav2Lip [28] demonstrates high performance in lip-synchronization, but it is unable to generate the details or the overall facial movements of the target person, which results in decreased image quality. DaGAN [21] adopts a depth-conditioned generative adversarial network (GAN) for talking-head generation, but its GAN-based renderer struggles with unstable training and challenges in modeling image details. FACIAL [6] employs a GAN integrated with facial implicit attribute learning, but it also suffers from training instability and difficulties in generating intricate details. VideoReTalking [8] directly edits the video frames with the synthesized expressions to create lip-synchronized videos. However, it tends to produce unrealistic results with limited generalization capabilities.\nWith the adaptation of NeRF [26] in the talking-head generation, NeRF-based methods [7], [10], [30], [27], [9] have demonstrated better performance in producing high-fidelity talking-head videos with accurate rendering of image details. Furthermore, NeRF-based methods AD-NeRF [19], GeneFace[34], and GeneFace++ [17] manage to improve the lip-synchronization accuracy with realistic image rendering. Despite these advancements, the emotional aspect of generating vivid talking-head videos has been overlooked.\nMore recent works have begun focusing on generat-ing emotion-aware talking-head videos [18], [22], [13], [15], [32], [11], [31]. EAT [18] employs emotion adaption models to enable controlled emotional video generation. FlowVQTalker [32] synthesizes emotional talking heads using normalizing Flow and Vector-Quantization modeling. EAMM [22] utilizes emotional videos and pose videos to assist the generation of emotional video portraits. FG-EmoTalk [31] generates emotional talking heads using disentangled expression latent code and extracted human facial features. PC-AVS [36] uses pose control to generate talking heads. EVP [15] synthesizes emotional video portraits by disentangling speech content and emotional features.\nWhile these works made improvements in generating emotional expressions, they struggle with several challenges in terms of generating vivid emotions. 1) Emotion accuracy. Despite previous methods are able to generate some forms of emotional expressions, their generated emotional expressions are often partially inaccurate or invisible. For instance, some might have corresponding brow expressions but incorrect mouth expressions [18], which can diminish the realness of the synthesized videos. 1) Identity-preservation. Previous methods did not take into account preserving the target identity in the process of learning emotional representation. This leads to the loss of identity and facial distortions, which can reduce the realism and fidelity of the generated videos. 3) Idle state. Given silent audio, other works generate extra lip movements or unnatural body motions (either have extra movement or are entirely static). This limitation can undermine their ability to produce natural idle-state videos in many applications, such as conversational agents and digital humans.\nWe propose a system EmoGene to handle these three challenges. To address the emotion accuracy challenge, we propose a motion-to-emotion module. This module is composed of a landmark deformation model (LMD), which is a neural network trained on an emotion-labeled video dataset MEAD [12] to accurately generate emotional facial landmarks from the given emotion text label and neutral facial landmarks. This enables accurate and apparent emotion generation. For the identity-preservation challenge, we utilize an emotion-to-video module that consists of NeRF models to preserve the target person's identity and render high-fidelity videos based on emotional landmarks. To address the idle-"}, {"title": "II. RELATED WORK", "content": "Audio-driven emotional video portrait generation leverages a generative model to synthesize the emotional talking-head videos given the corresponding audio. It is related to the previous works on audio-driven video portrait generation and emotion-aware video portrait generation.\nAudio-driven video portrait generation. Generating audio-driven video portraits has gained lots of attention in the past few years [28], [30], [21], [6], [25], [37], [33], [8]. Cheng et al. [8] introduces a system to edit video expressions based on the input audio. Zhang et al. [6] proposes a method to synthesize talking face animation by learning the integrated phonetic, context, and identity information. By utilizing a pre-trained lip synchronization model, [28] manages to generate accurate lip-synchronization. However, it only generates the lip region, which can limit its realness and generalization ability. Zhang et al. [35] proposes a talking face generation framework guided by normalizing-flow, but its generation quality depends on the accurate foreground masks. Wav2NeRF [30] leverages wavelet transform and audio-visual cross-modality representations to generate talking heads. Yi et al. [19] developed the method to synthesize frames with the re-rendered 3D face animation, which often exhibits unrealistic expressions in its animation results. Following the introduction of NeRF [26], using NeRF to render the talking-head videos attracted more attention in the research community [19], [34], [17], due to their ability to render delicate details. Notably, Ye et al. proposed Gene-Face++ [17], which is a remarkable audio-driven talking-head generation system that enables realistic rendering and robust lip-synchronization.\nEmotion-aware video portrait generation. Building on the concepts above, emotion-aware video portrait generation aims to produce lifelike emotional expressions by incorporating emotional features into the video generation process. To achieve its goal, this task requires the generated video to not only exhibit corresponding audio-lip movements but also display vivid emotions. Previous works explored the different methods to enable controllable emotional expression generation [18], [22], [32], [11], [31], [16], [15]. Gan et al. developed EAT [18], an emotional talking-head generation framework based on an audio-to-expression transformer and emotional adaption networks. EVP [15] proposes a method to synthesize emotional video portraits with disentangled emotion and content features. Ji et al. developed EAMM [22] framework to utilize augmented emotional source videos to generate one-shot emotional talking heads. Tan et al. [32] produces emotional talking heads by leveraging normalizing Flow and Vector-Quantization modeling. Sung et al. [11] generates talking heads capable of expressing laughter using FLAME parameters and vertices. Furthermore, Sun et al. [31] developed a framework that employs a disentanglement scheme to isolate emotion latent code and utilizes self-supervised learning to generate emotional talking heads. Additionally, StyleTalk [16] utilizes a speaking style to synthesize video portraits, while Li et al. [24] proposes a two-staged system that synthesizes video portraits with facial expressions."}, {"title": "III. METHOD", "content": "We present our EmoGene framework in this section. EmoGene consists of 3 parts: 1) audio-to-motion: a VAE model that converts the audio features into neutral facial landmarks; 2) motion-to-emotion: a landmark deformation model that transforms the neutral landmarks into emotional landmarks; 3) emotion-to-video: NeRF models that render high-fidelity emotional talking-head video (Figure 1).\nA. Audio-to-Motion\nWe utilize a VAE model [23], [17] to generate audio-driven diverse facial landmarks, as shown in Figure 2. In this module, the VAE is conditioned on the audio features and ground truth (GT) facial landmarks and guided by the lip-synchronization discriminator to learn to generate diverse facial landmarks given an audio input.\nEncoder and decoder. Our encoder and decoder are structured as convolutional neural networks. Leveraging a WaveNet-based architecture [5], the convolutional layers are gradually increased to enhance the receptive field. This allows the encoder and decoder to generate sequences with different lengths efficiently.\nTraining process. In the training process, we utilize Deep 3D Face Reconstruction [4] to extract the ground truth 3DMM facial landmarks from the video, and we selected 68 landmark points to generate detailed facial motion. Then, we extract the HuBERT [14] and pitch features of the given audio. Furthermore, we employ the Monte-Carlo Evidence Lower Bound loss [29] to increase the efficiency of the training process. To guide the VAE training, we evaluate the audio-lip synchronization of the generated landmarks using a SyncNet [3] based pre-trained lip-synchronization discriminator. Hence, the VAE training loss is:\n$L_{VAE} = E [||l - \\hat{l}||_2 + KL(z \\| 2) + L_{sync}(a, l)]$.  (1)\nIn this setup, $l$ is the ground truth facial landmark and $a$ represents the input audio. The latent encoding of $l$ is denoted by $z = Encoder(l,a)$, and the latent code $z$ is sampled with"}, {"title": "B. Motion-to-Emotion", "content": "Without conditioning on emotion, the audio-to-motion module can only generate neutral landmarks. To enable the generation of emotionally expressive facial landmarks, we develope the motion-to-emotion module (Figure 3). This module transforms neutral landmarks into emotional landmarks by employing a landmark deformation model, which generates the emotional landmark deformation differences for computing the emotional facial landmarks.\nLandmark deformation model. To deform the neutral landmarks to obtain emotional expressions, we utilize a neural network with 3 fully connected (FC) layers which takes the neutral landmarks and emotional embedding as inputs to generate the emotional landmark deformation differences. The landmark deformation model is defined as:\n$LD(x) = FC(ReLU(FC(ReLU(FC(x)))))$,  (2)"}, {"title": "III. METHOD", "content": "We present our EmoGene framework in this section. EmoGene consists of 3 parts: 1) audio-to-motion: a VAE model that converts the audio features into neutral facial landmarks; 2) motion-to-emotion: a landmark deformation model that transforms the neutral landmarks into emotional landmarks; 3) emotion-to-video: NeRF models that render high-fidelity emotional talking-head video (Figure 1).\nAudio-to-Motion.  We utilize a VAE model [23], [17] to generate audio-driven diverse facial landmarks, as shown in Figure 2. In this module, the VAE is conditioned on the audio features and ground truth (GT) facial landmarks and guided by the lip-synchronization discriminator to learn to generate diverse facial landmarks given an audio input. Encoder and decoder.  Our encoder and decoder are structured as convolutional neural networks. Leveraging a WaveNet-based architecture [5], the convolutional layers are gradually increased to enhance the receptive field. This allows the encoder and decoder to generate sequences with different lengths efficiently. Training process.  In the training process, we utilize Deep 3D Face Reconstruction [4] to extract the ground truth 3DMM facial landmarks from the video, and we selected 68 landmark points to generate detailed facial motion. Then, we extract the HuBERT [14] and pitch features of the given audio. Furthermore, we employ the Monte-Carlo Evidence Lower Bound loss [29] to increase the efficiency of the training process. To guide the VAE training, we evaluate the audio-lip synchronization of the generated landmarks using a SyncNet [3] based pre-trained lip-synchronization discriminator. Hence, the VAE training loss is: $LVAE=E[||l\u2212^l||2+KL(z||z)+Lsync(a,l)]. (1) In this setup, l is the ground truth facial landmark and a represents the input audio. The latent encoding of l is denoted by z=Encoder(l,a), and the latent code z is sampled with"}, {"title": "B. Motion-to-Emotion", "content": "Without conditioning on emotion, the audio-to-motion module can only generate neutral landmarks. To enable the generation of emotionally expressive facial landmarks, we develope the motion-to-emotion module (Figure 3). This module transforms neutral landmarks into emotional landmarks by employing a landmark deformation model, which generates the emotional landmark deformation differences for computing the emotional facial landmarks. Landmark deformation model.  To deform the neutral landmarks to obtain emotional expressions, we utilize a neural network with 3 fully connected (FC) layers which takes the neutral landmarks and emotional embedding as inputs to generate the emotional landmark deformation differences. The landmark deformation model is defined as: $LD(x)=FC(ReLU(FC(ReLU(FC(x))))), (2)"}, {"title": "where DL represents the landmark deformation model and x denotes the concatenated embedding of the neutral landmarks and emotional embedding. To generate the corre- sponding emotional landmarks, we concatenate the neutral landmarks with the emotional landmark deformation differ- ences as:", "content": "IE = l \u0394l, (3) where le is the generated emotional facial landmarks and Al is emotional landmark deformation differences from the landmark deformation model. Training process.  During the training process, we extract the ground truth facial landmarks from the video and utilize an emotion embedder to construct the emotion embedding from the corresponding emotion label. We concatenate the"}, {"title": "C. Emotion-to-Video", "content": "After we obtain the emotional landmarks from the motion-to-emotion module, we then utilize the emotion-to-video module, which consists of NeRF-based models, to synthesize emotional talking-head frames conditioned on the generated emotional landmarks (Figure 4)."}, {"title": "Emotion landmark-conditioned Head-NeRF. To render high-quality talking-head videos, we utilize a NeRF-based model [9], [17] conditioned on the emotional landmarks to dynamically represent a talking head. Different from the original NeRF [26], this model incorporates not only 3D location x and the viewing direction d but also 3D emotional landmarks IE. Based on these parameters, the model function F is defined as:", "content": "F : (x,d,le) \u2192 (c, \u03c3), (5) where c represents the predicted RBG color and o denotes the predicted volume density (for the emitted ray r(t) = o+ td with the camera origin o) in the 3D neural radiance field. To render each pixel of the image frame, we aggregate the color c along the ray by following the differentiable volume rendering equation [26]: C(r,le) = \u222b f (r(t),le) \u00b7 c(r(t),le,d) \u00b7 T(t)dt, (6) where C is the pixel color for the ray r. tn and tf denote the near and far bounds of the ray. T(t) is the accumulated transmittance along the ray with these bounds:"}, {"title": "T(t) = exp(- \u222b \u03c3(r(s), IE))ds),", "content": "(7) where s is an intermediate point along the path of the ray from the near bound tn to the current point t."}, {"title": "Here, do denotes the canonical space's view direction, and the head pose P\u2208 R3\u00d74 with a translation vector and rotation matrix.", "content": "Ftorso: (x,Chead;do,P,le) \u2192 (c,\u03c3).  (8)  Training process. To train our NeRF models, we extract the facial landmarks and images from the input video frames. We then utilize the landmark-image pairs to train the NeRF models. To decrease the L2 reconstruction error between the generated images and the ground truth images, the training loss for the NeRF models is:"}, {"title": "LNeRF = E [||C(r, le) \u2013 CGT ||2] .", "content": "(9)"}, {"title": "D. Pose Sampling Method for Idle State", "content": "We propose a pose sampling algorithm to generate natural body and lip movements for the idle-state video. As shown in Figure 6, we first identify the starting idle poses in the original pose tensor, then we replicate these idle poses to construct a collection of idle pose segments with random lengths and fixed gaps (number of non-idle poses) between them (using Algorithm 1). To create a new pose tensor with the idle state, we insert the idle segments into the original tensor (as shown in Figure 5).\nDetermining the minimum and maximum idle lengths. To generate idle segments, we developed a method to determine the minimum and maximum lengths of contiguous idle segments. Given a pose tensor X {X1,..., Xi, Xi+1,...,xn\u22121}, this method computes the cosine similarities between each contiguous pair of poses at indices i and i+1 as:\nCosineSimilarity; = \\frac{XiXi+1}{||Xi || || Xi+1 ||} (10)"}, {"title": "A pair of consecutive poses is considered to belong to an idle segment if their cosine similarity is greater or equal to 1, since a cosine similarity of 1 indicates that two poses are aligned and have maximum similarity. Then, the method calculates the lengths of the idle segments to obtain the minimum and maximum idle segment lengths. Generating idle segments with random lengths and fixed gaps. As shown in Algorithm 1, this algorithm gen- erates the (start, end) pairs of the idle segments, where start and end are the starting and ending indices of the idle segments in the pose tensor. To synthesize a natural idle-state", "content": "Non-Idle Poses with Fixed Lengths (Gaps) Idle Segments with Random Lengths New Pose Tensor  Fig. 5: New pose tensor reconstruction. To construct the new pose tensor, we insert the idle segments after their corresponding non-idle pose tensors. Idle Pose Segment Original Pose Tensor An Idle Pose  Fig. 6: Idle pose segment reconstruction. We construct each idle segment by identifying and replicating an idle pose of the original pose tensor. motion, this algorithm iterates through the pose tensor and randomly determines the length of each idle segment within the constraints of the minimum and maximum idle segment lengths. This random sampling method creates variety in the idle motion, which helps to generate natural idle-state videos. After determining each idle segment, the algorithm keeps a fixed gap before the start of the next idle segment. This algorithm generates idle motion periods that mimic natural waiting or resting states, which can increase the realness of the idle-state videos. Inserting idle segments into pose tensor.  To enable the idle state, we create a new pose tensor by inserting the idle segments after their corresponding non-idle pose tensors (Figure 5). This produces natural idle-state motions and smooth motion-stillness transitions for the given silent audio, which makes the rendered videos more realistic."}, {"title": "Algorithm 1 Generate idle segments with random lengths and fixed gaps.", "content": "function GENERATESEGMENTS(n, min_len, max_len, fixed_gap)\nsegments \u2190\ncurrent_position \u2190 0\nwhile current_position < n do\nl \u2190 random between min_len and min(max_len, n-\ncurrent_position)\nstart current_position, end \u2190 start+1-1\nif end + fixed_gap > n then break\nend if\nsegments.append((start, end))\ncurrent_position \u2190 end + 1 + fixed_gap\nend while\nreturn segments\nend function"}, {"title": "IV. EXPERIMENTS", "content": "A. Experimental Settings\nDatasets. We trained the audio-to-motion module on VoxCeleb2 [2], which contains 6,112 identities with over 1 million utterances, to learn a generalized audio-to-motion mapping. For learning the landmark deformation model, we utilized the MEAD dataset [12], which consists of labeled emotional talking videos of 60 identities with 8 emotions (surprise, sad, neutral, happy, hear, contempt, disgust, angry) for each utterance. Based on identity, we split the dataset into training and testing sets. To train the NeRF models, we adopted the MEAD videos and collected videos, and each training video is around 3 to 6 minutes with 512x512 resolution and 25 FPS.\nComparisons. The 3 notable works we compare with EmoGene are: 1) EAT [18], which leverages a pre-trained transformer and emotion adaptation models to generate emotional expressions; 2) Wav2Lip [28], which adopts a pre-trained sync-expert to synchronize lip movements; 3) GeneFace++ [17], which uses auxiliary audio features and an efficient NeRF to render talking-head videos.\nImplementation details. EmoGene is trained on 1 NVIDIA RTX A6000. The landmark deformation model and VAE take around 40K to converge (about 14 hours). The NeRF models are trained for 400K iterations in total (about 12 hours).\nB. Quantitative Evaluation\nEvaluation metrics. We adopt the SSIM, PSNR, and FID [20] to measure the image quality and fidelity of the gener- ated videos. We employ the landmark distance (LMD) [1] to measure audio-lip synchronization and emotional expression accuracy.\nEvaluation results. From the results (Table I), we observe that EmoGene outperforms other methods in achieving high image quality with the best SSIM and PSNR scores. Furthermore, our method archives the second-best LMD score for generating emotional expressions. However, Wav2Lip only generates the lip region but not the entire face. This narrower generation scope of Wav2Lip might enable it to have most of its generated image landmarks overlapped with the ground truth image landmarks, which can lead to an inflated LMD score."}, {"title": "C. Qualitative Evaluation", "content": "For qualitative evaluation, we show the keyframes of two emotion-specific clips in Figure 7. We observe that although Wav2Lip and GeneFace++ show good lip-synchronization, they are not able to generate emotional expression. While EAT is able to synthesize regional emotional expressions, it struggles to generate accurate emotional expressions across all facial regions, including brows, eyes, and mouth, which can lead to unnatural expressions. Furthermore, the generated images of EAT contain distortions of facial features, which results in identity loss and reduced image fidelity. For instance, during its generation of emotional expressions, its generated facial structures exhibit visible differences in over- all facial structures compared to the target person. In compar- ison, EmoGene shows accurate facial emotional expressions and lip-synchronization with high identity-preservation.\nIn addition, we show the effectiveness of our pose sam-pling method for generating natural idle-state videos. We present the keyframes of the video clip generated by Emo- Gene (with pose sampling) and Wav2Lip (without pose sampling) in Figure 8. We observe that given the silent audio, the pose sampling algorithm allows EmoGene to have more stable and natural body and lip movements. In contrast, Wav2Lip exhibits larger head movements and extra lip motions, despite having silent audio as input.\nUser study. We have 20 evaluators to evaluate the gener- ated videos. For each of the 4 methods, we produce 3 video clips across 8 emotions, resulting in a total of 96 videos. We employ the Mean Opinion Score (MOS) for the evalua-"}, {"title": "tions, with ratings spanning from 1 (Bad) to 5 (Excellent). Evaluators are instructed to rate each video according to 4 criteria: 1) emotional accuracy; 2) lip synchronization; 3) video realness; and 4) video quality.", "content": "We observe from the results (Table II) that: 1) EmoGene outperforms the other methods in emotional accuracy, video realness, and video quality. 2) EmoGene has lower perceived lip synchronization accuracy compared to other methods. This could be due to the emotional landmark deformation process of EmoGene, which might potentially affect its lip-synchronization accuracy when generating emotional land- marks."}, {"title": "D. Ablation Study", "content": "We conduct an ablation study in this section to demon- strate the essential role of the landmark deformation model in EmoGene.\nLandmark deformation model. We test the setting of w/o landmark deformation model. In this setting, we remove the emotional landmark deformation from the neutral landmarks to evaluate how the absence of emotional deformation affects the performance of our framework. The results of this setting are shown in Table III. Due to the removal of emotional deformations, the NeRF is solely conditioned on the neutral landmarks, which do not exhibit any emotional cues. Thus, the conditioning on neutral landmarks restricts the NeRF from generating emotional expressions. Consequently, we observe a notable decrease in PSNR, LMD, and FID scores, which shows the critical role of the landmark deformation model in generating emotional expressions in the framework."}, {"title": "V. CONCLUSION", "content": "We present EmoGene to synthesize accurate emotional video portraits. A landmark deformation model is proposed to synthesize the robust emotional landmarks to condition the NeRF model to render vivid emotional expressions. Furthermore, a pose sampling method is introduced to gen- erate natural idle-state videos given silent audio. Extensive experiments show that our method produces more accurate emotional expressions with preserved identity and natural movements.\nLimitations. 1) The performance of our method is limited by the emotional training data, in terms of diversity and size. 2) The landmark deformation can potentially impact the lip-synchronization accuracy of the generated videos.\nFuture directions. For future works, one direction is to expand the emotional training dataset with a broader range of emotional expressions. To improve lip-synchronization while generating a vivid emotion generation, future works can also consider refining the emotion transformation process by integrating other methods, such as expressive GAN-based models, and conducting extensive user studies to obtain more insights into the user-perceived emotional generation quality and image fidelity of the emotional video portraits."}]}