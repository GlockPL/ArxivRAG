{"title": "deepTerra - AI Land Classification Made Easy", "authors": ["Andrew Keith Wilkinson"], "abstract": "deepTerra is a comprehensive platform designed\nto facilitate the classification of land surface fea-\ntures using machine learning and satellite imagery.\nThe platform includes modules for data collection,\nimage augmentation, training, testing, and predic-\ntion, streamlining the entire workflow for image\nclassification tasks. This paper presents a detailed\noverview of the capabilities of deepTerra, shows\nhow it has been applied to various research areas,\nand discusses the future directions it might take.", "sections": [{"title": "Introduction", "content": "Land classification using satellite imagery has emerged\nas a critical tool in diverse applications, including urban\nplanning, environmental monitoring, and resource man-\nagement [1]. However, deploying machine learning models\nfor such tasks often involves overcoming challenges such\nas data collection, dataset augmentation, and model opti-\nmization. To address these needs, deepTerra offers an in-\nte grated suite of tools that simplifies the end-to-end work-\nflow of image classification via machine learning.\ndeepTerra is designed to streamline the processes of\nsatellite image acquisition, data preparation, and training\nof state-of-the-art convolutional neural network (CNN) ar-\nchitectures. By supporting efficient labeling, data aug-\nmentation, and hyperparameter tuning, deep Terra enables\nresearchers and practitioners to develop robust models\nwith minimal effort. The platform supports a variety of\nCNN architectures, such as ResNet, MobileNet, and Effi-\ncientNet, making it versatile for a wide range of classifi-\ncation tasks.\nThis paper provides an overview of the capabilities of\ndeepTerra, detailing its modules for data collection, aug-\nmentation, training, and prediction. Through case stud-\nies like garbage detection, private pool identification, and\nbeehive localization, the paper demonstrates the flexibil-\nity and effectiveness of the tool in addressing real-world\nproblems."}, {"title": "deepTerra Overview", "content": "deepTerra is an integrated suite of tools that support the\nvarious stages that make up image classification via ma-\nchine learning. These comprise the following modules:\n\u2022 Data Collection: This module provides tools to\nextract suitable image patches from pre-existing im-\nges or download satellite imagery from sources like\nGoogle Earth. It also includes features for label-\ning and organizing datasets efficiently. When geo-\ngraphic coordinates are available, they are automati-\ncally recorded and associated with each image patch\nfor precise spatial referencing.\n\u2022 Image Augmentation: Collecting and labeling a\nlarge dataset of images for training a reliable model\ncan be challenging and time-consuming [2]. Image\naugmentation offers an effective solution by expand-\ning the dataset through the application of common\ngeometric transformations, such as rotation, flipping,\nand shifting. These techniques maximize the utility\nof existing data, improving the model's robustness\nand performance.\n\u2022 Training: Once a suitable labeled training dataset\nhas been prepared, the model training process can\nbegin. The tool supports a variety of popular CNN\narchitecture families, including VGG, ResNet, Incep-\ntion, and MobileNet. It provides sensible default hy-\nperparameter settings while allowing users to fine-\ntune these as needed. Training progress is displayed\nthrough graphical summaries and detailed metrics,\nsuch as accuracy and F1 score.\n\u2022 Test and Predict: After training a model, the\ntest and predict module can be applied. The test-\ning phase evaluates the model's performance on ex-\nisting labeled data, providing an objective measure\nof accuracy. The prediction phase focuses on novel,\nunlabeled data-the ultimate goal of the preceding\nsteps. Results are presented both in summary and\ndetailed formats, with options to export them as CSV\nor JSON files. If geographic coordinates are included,\nGoogle Maps links can be generated to reference spe-\ncific image patches. Advanced features allow for the\ncreation of heatmaps [3] and interactive Google Map\noverlays in HTML format.\nEach of the deepTerra modules are accessed through the\nhome screen, as shown in Figure 1. The following sections"}, {"title": "Data Collection", "content": "The data collection module provides various ways to im-\nport and organize image patches.\n\u2022 Image patches can be created by simply loading a\nlarge existing image file, which is then automatically\nsplit into patches suitable for the machine learning\nprocess (see Figure 2). The image patches can be op-\ntionally classified by left or right clicking to indicate\npositive or negative. Figure 3 shows a satellite image\nbroken into patches and a set of images specified as\n\"garbage\" (positive, blue patches) or \"not_garbage\"\n(negative, red patches).\n\u2022 Alternatively, a latitude/longitude coordinate can be\nsupplied for the north-east corner of a 1km\u00b2 area,\nafter which the google earth engine can be used to\ndownload and organize all the image patches for this\nlocation. This is stored in an organized directory and\nconsists of 1296 200x200 pixel high-resolution images.\nThe geographical coordinates are calculated tagged\nwith each image patch."}, {"title": "Data Augmentation", "content": "Collecting and classifying satellite images is often a time-\nconsuming and challenging task. Well-curated datasets\nfor many problem domains are difficult to find, and even\nwhen a suitable source is identified, the number of avail-\nable images is often insufficient to train a robust model.\nData augmentation addresses this limitation by applying\nvarious transformations to the existing dataset, effectively\nincreasing the number of unique training samples [4].\ndeepTerra supports several standard geometric augmen-\ntation techniques, including:\n\u2022 Rotation: Rotate each image by a random number\nof degrees\n\u2022 Shifting: Shift an image along the vertical or hori-\nzontal axes by a random amount; effectively shifting\nthem up, down, left, or right.\n\u2022 Zooming: Scaling images up or down by a random\namount.\n\u2022 Flipping: Generating mirrored versions of images\nalong the horizontal or vertical axis.\nThese transformations can be applied to any image\ndataset to expand the number of training samples while in-\ntroducing variations that generate unique instances. Cru-\ncially, the augmented images retain the essential charac-\nteristics of the originals, ensuring their relevance for train-\ning. If any transformation creates \"dead\" space, a config-\nurable fill mode can be applied to seamlessly fill the gaps,\nmaintaining the integrity of the augmented images."}, {"title": "Training", "content": "Training is the process through which a model learns to\nidentify objects by analyzing example images. It involves\nutilizing a labeled dataset in conjunction with a selected\nmachine learning architecture to process the images and\nextract the critical features necessary for accurately rec-\nognizing target objects [5].\ndeepTerra supports the training process through the fol-\nlowing key features:"}, {"title": "Importing a dataset", "content": "Image datasets can be imported from three main sources:\n1. Local Folder: A folder on the local machine where\neach subdirectory corresponds to a target machine\nlearning category (label). Each subdirectory should\ncontain a set of images in either PNG or JPEG for-\nmat.\n2. Local Compressed Archive: A .tgz archive stored on\nthe local file system. The archive should follow the\nsame folder and image organization as the local folder\nstructure.\n3. Remote Compressed Archive: A .tgz archive available\nfor download via a URL."}, {"title": "Selecting a training architecture", "content": "Training is conducted using a specific machine learning\narchitecture. deepTerra simplifies this process by offering\neasy access to a diverse range of convolutional neural net-\nwork (CNN) architectures, widely regarded as the \"gold\nstandard\" for AI-driven image classification tasks [6].\nUsers can simply select an architecture suitable for their\ntask from a convenient drop-down menu. This also makes\nit easy to perform comparative analyses, enabling users to\nevaluate and identify the best-performing architecture for\ntheir specific needs.\nThe following CNN architectures are supported:\n\u2022 ResNet50\n\u2022 Inception V3\n\u2022 DenseNet\n\u2022 EfficientNet V2\n\u2022 Inception-ResNet V2\n\u2022 VGG19\n\u2022 MobileNet V3\n\u2022 NASNet\n\u2022 Xception\n\u2022 ConvNet"}, {"title": "Tuning hyperparameters", "content": "Once an architecture is selected, key parameters can be\noptimized to balance various aspects of the machine learn-\ning process, such as memory efficiency, training speed, and\nmodel generalization. deepTerra simplifies this process by\nproviding sensible default settings tailored to the selected\ndataset and architecture. Users also have the flexibility\nto override these defaults with their own custom values,\nensuring the system adapts to specific requirements and\npreferences.\nThe following parameters are currently supported:\n\u2022 maximum number of epochs\n\u2022 batch size\n\u2022 early stopping\n\u2022 dropout\n\u2022 optimizer\n\u2022 learning rate\n\u2022 activation function\n\u2022 pre-existing weights (imagenet by default)\n\u2022 Training/validation set split"}, {"title": "Controlling the training run", "content": "Once a dataset is loaded and an architecture selected,\nthe training process can be initiated. As the training\nprogresses, performance metrics are visualized through a\ngraphical summary, complemented by a detailed text con-\nsole. These outputs provide insights into each training\nepoch, including accuracy and loss statistics.\nThe training process offers flexibility, allowing users to\npause, resume, stop, or reset the run at any time, ensuring\ncomplete control over the workflow."}, {"title": "Assessing the results", "content": "Once a training run is completed, the results can be thor-\noughly reviewed. deepTerra provides a detailed summary,\nincluding a confusion matrix and key performance met-\nrics such as accuracy, precision, recall, F1-Score, and\nMatthew's Correlation Coefficient (MCC). These metrics\noffer a comprehensive initial assessment of the model's\nperformance, helping users gauge its effectiveness and re-\nliability."}, {"title": "Test and Prediction", "content": "The test and prediction tool addresses two closely related\nscenarios:\n1. Testing on Labeled Datasets: Performing predictions\non test datasets to evaluate how well the model gen-\neralizes to data that differs from the training set.\n2. Predicting on Unlabeled Data: Making predictions\non novel, unlabeled datasets to generate insights or\nclassifications for new data."}, {"title": "Testing the Model", "content": "The training process includes evaluating the model using\na randomly selected subset of the training data. While\nthis provides an initial measure of the model's accuracy,\nit is tied to the dataset used for training and may reflect\nbiases present in the data collection process. To ensure\nthe model's ability to generalize to broader datasets, it\nis valuable to use an independently collected dataset for\nevaluation.\nThe testing module supports this by allowing users to\nload a separate dataset and select a previously trained\nmodel. During testing, predictions are generated for the\ndataset, with progress and sample-level results displayed\nin a detailed output console."}, {"title": "Using the Model for Predictions", "content": "The ultimate goal of training and testing a model is to de-\nploy it for real-world predictions. While prediction also re-\nquires datasets, these are typically unlabeled consisting\nsolely of images without predefined classes. The data col-\nlection module can assist in gathering and organizing such\ndatasets, just as it does for training datasets (see Section\n2.1).\nUnlike testing, where performance metrics like confu-\nsion matrices can be calculated, predictions on unlabeled\ndatasets do not allow for such evaluations because the true\nclass of each image is unknown. However, the tool pro-\nvides a summary of the prediction results, including the\nnumber of instances predicted for each class and detailed\ninformation for each prediction (see Section 2.4.3).\nAfter completing a prediction run, the raw results are\ndisplayed in the main window, appearing similar to those\nfrom the testing variant of the tool (as shown in Figure\n6). The results can also be saved in several formats for\nfurther analysis or integration into workflows:"}, {"title": "Details Window", "content": "This subtool provides additional information and func-\ntionality, enhancing the utility of both testing and pre-\ndiction workflows. This presents the raw test/prediction\nresults in a more structured way, as a table of results,\nas shown in Figure 8. The following data points are dis-\nplayed:\n\u2022 filename of the image on disk or tgz archive file\n\u2022 predicted class based on model used\n\u2022 actual class for test runs, or chosen class for predic-\ntions where this has been defined\n\u2022 confidence of the prediction, expressed as a percent-\nage\n\u2022 significance of the prediction, expressed as a percent-\nage of pixels in the image that contribute positively\nto the prediction\n\u2022 clickable thumbnail of the image (see below)\n\u2022 location of the image expressed as a geographical lat-\nitude/longitude coordinate (if available)\n\u2022 html link to location of the image on a google map\n(if available)\nIf present, the confusion matrix and accuracy stats are\ndisplayed. It is also possible to save the current dataset\nstatus if it has been edited."}, {"title": "Result Manipulations", "content": "The results from predictions can be manipulated to focus\non specific criteria, enabling more targeted analysis. Key\nmanipulations include:\n\u2022 Filtering by Confidence: Predictions can be filtered\nbased on their confidence level the calculated prob-"}, {"title": "Image Details and Heatmap Visualization", "content": "Each image thumbnail functions as a button that opens\na file details window upon being clicked (see Figure 9).\nThis popup provides comprehensive information about the\nselected image and includes several interactive features:\n\u2022 Prediction Toggle: Users can toggle the recorded pre-\ndiction for the image, enabling adjustments when cre-\nating labeled datasets from a prediction run. This\nfeature is particularly useful for refining datasets for\ntraining or testing purposes.\n\u2022 Enlarged Image View: The popup displays an en-\nlarged representation of the image, aiding users in\nvisually confirming or assigning labels to images with\ngreater accuracy.\n\u2022 Heatmap Visualization: A \"Show Heatmap\" button\n(see Figure 10) allows users to view a heatmap rep-\nresentation associated with the image. This visual\nexplanation highlights the areas of the image that\ncontributed most to the prediction, offering valuable\ninsights into the model's decision-making process and\nhelping diagnose potential issues in predictions.\n\u2022 Geophysical Location: A \"Show on Google Maps\"\nbutton is available to display the geographical loca-\ntion of the image patch, where applicable. This fea-\nture overlays the location on Google Maps, providing\na visual reference for the origin of the image.\nThese features collectively enhance the usability and in-\nterpretability of the tool, empowering users to refine their\ndatasets and better understand model behavior."}, {"title": "Case Studies", "content": "deepTerra has been applied to a variety of situations, in-\ncluding garbage detection, private swimming pool identi-\nfication, beehive locality, and various others. Full details\nof these case studies can be found in [7]."}, {"title": "Garbage Dump Detection", "content": "Garbage dump recognition presents various challenges, in-\ncluding:\n\u2022 High dimensionality of data\n\u2022 Utilizing high-resolution satellite image sources\n\u2022 Irregular, difficult-to-categorize garbage targets\n\u2022 Obstructions by trees and vegetation\n\u2022 Rapidly evolving heat characteristics\n\u2022 Variation according to historical snapshots\ndeepTerra was used to help produce and organize a\ndataset of a few hundred verified and labeled garbage/not\ngarbage images. The image augmentation tool boosted\nthis to a dataset of a few thousand. A sample image is\nreproduced in figure 11."}, {"title": "The Private Pools Project", "content": "The main aim of this case study was to demonstrate\nthe benefits of image augmentation. The study focused\non identifying private swimming pools using the VGG19\nmodel, chosen for its simplicity and suitability for this\ntask. A model was trained on non-augmented and aug-\nmented datasets, and tested on a independently collected\ndataset. The non-augmented model gave an accuracy of\n72% whilst the augmented model one of 81%. This helps\nillustrate that image augmentation can improve accuracy\nof a model."}, {"title": "Beehive Project", "content": "Here the aim was to apply the tool to very small and\ndifficult to collect datasets. For this, the target objects to\nbe detected were beehives. A sample images is shown in\nFigure 13"}, {"title": "Cats vs Dogs", "content": "Whilst deepTerra has been designed to operate on satel-\nlite images, it can work equally well as an arbitrary image\nclassifier. To demonstrate this, the popular cats vs dogs\ndataset [10] was used to train a model to distinguish be-\ntween cats and dogs. A subset of 1000 images (500 cat, 500\ndog) was used to train a ResNet50 model. When applied\nto a separate test dataset, this achieved an impressive ac-\ncuracy of 98%, failing to correctly categorize just 3 images\nfrom the 200 image test data set."}, {"title": "Future Plans", "content": "As technology evolves, deepTerra is committed to advanc-\ning its capabilities to address emerging needs and chal-\nlenges. Future development efforts focus on integrating\ncutting-edge features and expanding the tool's versatility\nacross various domains. Below are some of the planned\nenhancements being explored:\n\u2022 Multicategory Classification: Expanding the tool to\nhandle datasets with multiple classes, enabling clas-\nsification tasks across a broader range of categories\nand use cases.\n\u2022 Advanced AI Techniques: Implementing innovative\napproaches such as hybrid models, which combine\nmultiple architectures for enhanced performance, and\nincremental learning, allowing the system to adapt\ncontinuously as new data becomes available.\n\u2022 Multispectral Imaging: Incorporating multispectral\ndata analysis to detect features like heat signatures\nand other spectral characteristics, broadening the\nrange of applications.\n\u2022 Image Segmentation: Adding capabilities for image\nsegmentation, enabling pixel-level classification and\nprecise delineation of regions within images.\n\u2022 Historical Analysis: Introducing tools for analyzing\nhistorical datasets, allowing users to identify trends,\nmonitor changes over time, and make informed pre-\ndictions based on past data.\n\u2022 Real-time Processing for UAVs: Developing a real-\ntime module optimized for mobile platforms, such as\nunmanned aerial vehicles (UAVs), to support on-the-\nfly image detection and analysis in dynamic environ-\nments."}]}