{"title": "Causal Discovery in Linear Models with Unobserved Variables and Measurement Error", "authors": ["Yuqin Yang", "Mohamed Nafea", "Negar Kiyavash", "Kun Zhang", "AmirEmad Ghassami"], "abstract": "The presence of unobserved common causes and the presence of measurement error are two of the most limiting challenges in the task of causal structure learning. Ignoring either of the two challenges can lead to detecting spurious causal links among variables of interest. In this paper, we study the problem of causal discovery in systems where these two challenges can be present simultaneously. We consider linear models which include four types of variables: variables that are directly observed, variables that are not directly observed but are measured with error, the corresponding measurements, and variables that are neither observed nor measured. We characterize the extent of identifiability of such model under separability condition (i.e., the matrix indicating the independent exogenous noise terms pertaining to the observed variables is identifiable) together with two versions of faithfulness assumptions and propose a notion of observational equivalence. We provide graphical characterization of the models that are equivalent and present a recovery algorithm that could return models equivalent to the ground truth.", "sections": [{"title": "1 Introduction", "content": "Causal structure learning, also known as causal discovery, from observational data has been studied extensively in the literature. The majority of work assume that there are no unobserved variables in the system that can affect more than one other variable, i.e., no unobserved common causes, and that variables are measured without error. This leads to identification of the underlying structure up to Markov equivalence class in general (Spirtes et al., 2000; Chickering, 2002), and complete identification of the structure under further model assumptions such as assuming a linear non-Gaussian model (LiNGAM, (Shimizu et al., 2006, 2011)). However, in majority of real-world settings, the researcher will not be able to observe all the relevant variables and hence cannot rule out the presence of unobserved common causes, and further many of the variables may have been measured with error. This necessitates approaches for causal discovery capable of dealing with these two important challenges."}, {"title": "2 Model Description", "content": "Notations We use upper-case letters with subscript for variables, upper-case letters without subscript for vectors and bold upper-case letters for matrices. For two vectors or variables X, Y, we use [X, Y] to represent the horizontal concatenation of X and Y, and [X; Y] to represent the vertical concatenation of X and Y.\nWe start with a formal definition of the model that we consider in this work.\nDefinition 1 (General linear LV-SEM-ME) A general linear LV-SEM-ME consists of two sets of variables V and X. Variables in V can be arranged in a causal order, and each variable Vi \u2208 V is generated as a linear combination of a subset Pa(Vi) \u2282 V (called its direct parents), plus an exogenous noise term Nv\u2081, where {Nv;}v;\u2208v are jointly independent. Further, V can be partitioned into three sets Y, Z and H. Variables in Y are observed (without error). Variables in Z are measured with error, where each variable Xi \u2208 X is a noisy measurement of one corresponding variable Zi \u2208 Z plus an exogenous noise term Nx\u2081 (which we call measurement error of Zi). Variables in H are neither observed nor measured (called unobserved variables). We refer to this model as linear latent variable SEM with measurement error (linear LV-SEM-ME). We study the identifiability of linear LV-SEM-MEs in a setup where the independent exogenous noise terms that causally (directly or indirectly) affect each observed variable can be distinguished from each other. That is, the mixing matrix of the linear system that transforms exogenous noise terms to observed variables is recoverable up to permutation and scaling of the columns. This can be satisfied, for example, if all independent exogenous noise terms are non-Gaussian. We note that measurement error challenge is essentially a special case of unobserved variable challenge, in which we observe a measurement of an underlying unobserved variable of interest. Yet, the observed measurement variable usually have special properties (such as not being affected by other variables) that can be leveraged to improve the identification power. Hence, our point of view in this work is to allow for coexistence of the challenges of unobserved common causes and measurement error, while leveraging the properties of the measurement variables to improve identification.\nWe study the identifiability of linear LV-SEM-ME under two faithfulness assumptions. The first assumption prevents existence of zero total causal effects of a variable on its descendants, which we refer to as conventional faithfulness assumption as it is widely assumed in the literature. The second assumption prevents additional parameter cancellation or proportionality among specific edges, which we refer to as LV-SEM-ME faithfulness assumption. Both assumptions are mild in that their violations are zero-probability events. We show that under conventional faithfulness assumption, the model can be identified up to an equivalence class characterized by an ordered grouping of the variables which we call ancestral ordered grouping (AOG)."}, {"title": "3 Identification Analysis", "content": "In this section we study identification for our model of interest. We start by looking at the LV-SEM and SEM-ME separately in Subsection 3.1; we consider identification in presence of both unobserved variables and measured variables in Subsection 3.2, which is our main identification result. In both subsections, we study identification under two faithfulness assumptions where, as will be discussed shortly, the first one,"}, {"title": "3.1 Identifiability of SEM-ME and LV-SEM", "content": "3.1.1 Identification Assumptions\nIn the following we present two assumptions for the identifiability of SEM-ME and LV-SEM, namely separability assumption and faithfulness assumption. Note that for the identifiability of LV-SEM-ME, we need one extra assumption which we will present in Section 3.2.\nSeparability assumption. We first deduce the mixing matrix that transforms independent exogenous noise terms to the observed variables [X; Y]. To simplify the deduction, we rewrite Equation (1b) as follows, by considering cogent variables (ZC and Y) as a single vector:\n$\\begin{bmatrix}\nZ^L \\\\\nV^C\n\\end{bmatrix} = \\begin{bmatrix}\nB^L \\\\\nB^C\n\\end{bmatrix}H + \\begin{bmatrix}\nD \\\\\nC\n\\end{bmatrix}V^C + \\begin{bmatrix}\n0 \\\\\nN_{VC}\n\\end{bmatrix}$\nwhere VC = [ZC;Y] represent the vector of cogent variables, and Nyc = [Nzc; Ny] represent their corresponding exogenous noise terms. B is partitioned into [BL; BC] according to [ZL; VC]. From Equations (1a) and (2), we can write variables in [ZL; VC] as linear combinations of the exogenous noise terms:\n$\\begin{bmatrix}\nZ^L \\\\\nV^C\n\\end{bmatrix} = W^* \\begin{bmatrix}\nN_H \\\\\nN_{VC}\n\\end{bmatrix}$ where $W^* = \\begin{bmatrix}\nB^L + D(I \u2013 C)^{-1}B^C & D(I \u2013 C)^{-1} \\\\\n(I \u2013 C)^{-1}B^C & (I \u2013 C) ^{-1}\n\\end{bmatrix}$,\nand I represent the pc \u00d7 pc identity matrix. Lastly, combined with Equation (1c), the overall mixing matrix W can be written as\n$\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix} = \\begin{bmatrix}I \\\\\nW^* \\end{bmatrix} \\begin{bmatrix}\nN_H \\\\\nN_{VC} \\\\\nN_{XL} \\\\\nN_{XC}\n\\end{bmatrix} = W \\begin{bmatrix}\nN_H \\\\\nN_{VC} \\\\\nN_{XL} \\\\\nN_{XC}\n\\end{bmatrix}$\nWe note that because mleaf variables have no exogenous noise, each column in W* either has at least two non-zero entries, or has one non-zero entry where the non-zero entry is not in X."}, {"title": "4.1 AOG Recovery", "content": "The following property can be implied from the definition of AOG, which shows that under conventional faithfulness assumption, we can identify the AOG of the model only based on the support of the mixing matrix W*.\nProposition 4 Under Assumptions 2 and 4,\n(a) One mleaf variable and one measured cogent variable belong to the same ancestral ordered group if and only if the two rows in W* corresponding to these variables have the same support. Further, for any cogent variable Vi and its descendant Vj, the row support of Vi must be a subset of the row support of Vj.\n(b) One unobserved variable and one cogent variable belong to the same ancestral ordered group if and only if the two columns in W* corresponding to the exogenous noise terms of these variables have the same support. Further, for any cogent variable Vi and its ancestor Vj, the column support of Nvi must be a subset of the column support of Nv;.\nThe proof of Proposition 4 directly follow from the definition of AOG, and hence are omitted.\nEquipped with Proposition 4, we propose an iterative algorithm for recovering the AOG in Definition 7 from W*. The pseudo-code of the proposed method is presented in Algorithm 1. In the first iteration, the Algorithm randomly chooses a row in W* with the fewest number of non-zero entries and finds all other rows with the same support. Denote the selected rows as Zj, and columns corresponding to these non-zero entries as N\u2081. Each of the selected row may either correspond to a cogent variable or an mleaf variable, and each of the column in N\u2081 may either correspond to the exogenous noise of an cogent variable or an unobserved confounder. The task is to decide whether there exists a cogent variable (and its associated exogenous noise). We first select the columns in N\u2081 with the fewest number of non-zero entries in W*. Denote this subset as NJ. Then noises in N\u2081 \\NJ must correspond to unobserved variables and are assigned to separate groups in Cunobserved.\nWe check whether any of the rows in Zy can be a cogent variable. If there is one observed variable in ZJ then it must correspond to the cogent variable. If all variables are unobserved, then we look at the submatrix Wo of W* with the rows corresponding to the (column) support of any variable in NJ, and columns corresponding to the (row) support of ZJ. If ZJ includes a cogent variable, then its corresponding exogenous noise must be in NJ, and the remaining noises are unobserved confounders. Since all noises in Nj have the same number of non-zero entries, they must have the same support. Further, any row that includes this exogenous noise must be a descendant of the cogent variable, and must include all the non-zero columns of the cogent variable under Assumption 2. This implies that Wo does not include any non zero entry. Therefore, if Wo includes any zero entry, then none of the rows in Zy would correspond to a cogent variable. The rows in Zy and the columns in Nj all belong to separate groups in Cmleaf and Cunobserved as they correspond to mleaf variables and unobserved variables. If Wo does not include any zero entry, then under minimality assumption, one of the rows in ZJ correspond to a cogent variable. Therefore all noises"}, {"title": "4.2 Model Recovery in the AOG and DOG Equivalence Class", "content": "In this section, we present our algorithm for recovering the models in the equivalence class using W*. The psuedo-code of the algorithm is presented in Algorithm 2.\nAOG Equivalence class. Recall from Section 3.2 that all members in the AOG equivalence class can be enumerated by switching the center with any mleaf variables in the group and/or switching the exogenous noises of the center with the noises of any unobserved variables in the group. Therefore, given the mixing matrix W*, Algorithm 2 first recovers the AOG of the true model using Algorithm 1. Then, it enumerates all possible choices of centers and the noises for each group. Note that groups with only mleaf variables or noises of unonserbed variables (i.e., in Cmleaf or Cunobserved) only have one choice. Therefore, we only need to consider all the groups that include cogent variables (i.e., in Ccogent). Denote each single selection of the centers in these groups as row, and the noises as col. The next step is to recover the model parameters B, C, D based on W* and the selected row and col following Equation (3). Denote the variables not in row as rowC, and the noises not in col as colC. The selected centers in row correspond to VC, and the selected noises in col correspond to Nyc in (3). Similarly, variables in row and noises in col correspond to ZL and NH, respectively. Therefore C, BC, D, BL can be calculated following lines 6-9 in Algorithm 2. Finding model parameters B, C, D for all possible choices of row and col gives us all models in the AOG equivalence class."}, {"title": "5 Conclusion", "content": "We studied the problem of causal discovery when the challenges of unobserved common causes and mea- surement error may coexist in the model. We defined a class of models called linear LV-SEM-ME which can be used to model both unobserved variables and measurement error. Further, we characterized the extent of identifiability of this model under separability assumption and two versions of the faithfulness assumption. Specifically, the first version, referred to as the conventional faithfulness requires only pairwise information about the variables, while the second version, referred to as the LV-SEM-ME faithfulness requires joint information. Nevertheless violation of both versions occur with zero probability. We showed that, under conventional faithfulness and LV-SEM-ME faithfulness, the model can be learned up certain model classes that we called AOG and DOG equivalence classes, respectively. We provided graphical characterization of the models that are equivalent and presented a recovery algorithm that could return models equivalent to the ground truth. A remaining challenge in our proposed methodology is its reliance on the accuracy of the provided estimate of the mixing matrix of the underlying model. Hence, devising accurate approaches for estimating the mixing matrix is an important direction of future research. Moreover, our proposed method- ology is based on the assumption of linearity of the data generating process. Relaxing this assumption is another important extension of our work which we leave to future work."}, {"title": "A.1 Proof of Proposition 2", "content": "The proof includes two parts. We first show the sufficiency: For an unobserved variable Hi in the ground truth model M, if it has an mleaf child that satisfies the condition described in Proposition 2, then M is not minimal, i.e., there exists an alternative model M' without H\u2081 that has the same mixing matrix and satisfies Assumption 1. Next, we show the necessity: If M is not minimal, then there must exist an unobserved variable Hi and one of its mleaf child Zj such that the described condition is satisfied.\nA.1.1 Proof of sufficiency\nSuppose there exists latent variable H\u2081 and a mleaf child Z; of H\u2081 in M such that the condition described in Proposition 2 holds. In the following we construct the alternative model M' that includes all variables in M except for Hi. The idea is to consider NH\u2081 as the exogenous noise term of Zj. Further, for any other child Vk of Hi, replace the edge H\u2081 \u2192 Vk in M by edges from Zj (and parents of Zj) to Vk in M'.\nThe structural equation of Zj in M can be written as\n$Z_j = \\sum_{\u03b9:V_\u03b9\\in Pa_M(Z_j) \\cap V_C} a_{j\u03b9}V_\u03b9 + \\sum_{l_j:H_i\\notin Pa_M(Z_j)} b_{jl_j} H_{l_j} + b_{ji}N_{H_i}$\nConsider NH\u2081 as the exogenous noise term of Zj in M'. The structural equation of Z; in M' is\n$Z_j = \\sum_{\u03b9:Z_\u03b9\\in Pa_M(Z_j) \\cap V_C} a_{j\u03b9}Z_\u03b9 + \\sum_{l_j:H_i\\notin Pa_M(Z_j) \\cap H \\{H_\u03b9\\}} b_{jl_j} H_{l_j} + b_{ji}N_{H_i}$\nFor any other children Vk of H\u2081 in M, the structural equation of V in M can be written as\n$V_k = \\sum_{\u03b9\u03ba:Z_\u03b9\\in Pa_M(V_k) \\cap V_C} a_{\u03ba\u03b9}Z_\u03b9 + \\sum_{lk:H_{ik}\\in Pa_M(V_k) \\cap H} b_{klk} H_{lk} + N_{Vk}.$\nBy considering N\u2081\u2081 in Equation (7) to be the exogenous noise of Zj, the structural equation of Zm in M' can be written as\n$V_k = \\sum_{\u03b9\u03ba:Z_\u03b9\\in Pa_M(V_k) \\cap V_C} a_{\u03ba\u03b9}Z_\u03b9 + \\sum_{lk:H_{ik}\\in Pa_M(V_k) \\cap H \\{H_\u03b9\\}} b_{klk} H_{lk} + N_{Zm} + b_{ki}b_ji \\left( Z_j- \\sum_{\u03b9:Z_\u03b9\\in Pa_M(Z_j) \\cap V_C} a_{ji}Z_\u03b9 - \\sum_{l_j:H_i\\notin Pa_M(Z_j) \\cap H \\{H_\u03b9\\}} b_{jl_j} H_{l_j}\\right)$\nSince Hi and Z; satisfy the condition in Proposition 2, Vk cannot be an ancestor of Zj or variables in \u0420\u0430\u043c(Zj) in M, otherwise we have Vk \u2208 Uz\u2208Pa(Z;) A\u043f\u043c(Z = \u0410\u043f\u043c(Z\u0458) \u2286 An\u043c(Vk). This implies that M' is still acyclic. Further, since \u0410\u043f\u043c(Zj) \u2286 \u0410nm(Vk), there are no additional ancestors introduced to Vk in M' compared with M. Lastly, we note that there might be edge cancellations in (4). In particular, the coefficient of the direct edge from a variable in Z\u2208 Pam(Vk) \u2229 VC to Vk may change in M' if Z\u2208 \u0420\u0430\u043c(Zj) and hence may be cancelled out. However, Z is still an ancestor of Vk in M', as there is"}, {"title": "A.2 Enumerating all models in the AOG and DOG equivalence classes by different choice of centers", "content": "We first show that an LV-SEM-ME can be uniquely deduced given the mixing matrix W* of the LV-SEM- ME, and a choice of the centers (and their corresponding exogenous noises) in each group. This has been described in Algorithm 2, where the matrices B, C, D can be found through matrix calculation.\nIn this following, given the ground-truth model M, we will show how to deduce the structural equations of the variables in the alternative model M', where M and M' has the same mixing matrix and the same (ancestral ordered or directed ordered) grouping of the variables. Specifically, we consider the case when"}, {"title": "A.3 Proof of the identification result under conventional faithfulness", "content": "In this subsection, we provide the proof of the result regarding the AOG equivalence class in LV-SEM-ME, i.e., Theorem 3(a). Note that the proof of the result for SEM-ME and LV-SEM, i.e., Theorem 1, can be deduced from it.\nTo show that the extent of identifiablity of an LV-SEM-ME under conventional faithfulness is the AOG equivalence class, we need to show that, for the ground-truth model M, any other model M' in the AOG equivalence class of M, and any model M\" that has the same mixing matrix but does not belong to the AOG equivalence class of M:\n(1.a) M' satisfies conventional faithfulness.\n(1.b) M' is consistent with any causal order among the ancestral ordered groups that is consistent with M.\n(1.c) M\" violates conventional faithfulness.\nRecall that for each cogent variable V\u2081, the ancestral ordered group of V\u00bf includes its mleaf child Z; if Vi is measured and all other parents of Zj are also ancestors of V\u00bf, and unobserved parent H\u012b if all other children of H\u2081 are also descendants of Vi.\nProof of (1.a). We note that it suffices to show (a) when the choices of centers (and/or the corresponding exogenous noise) of M' only differs from the choices of M in one group. This is because if there are p differences in the choices of centers, then we can always find a finite sequence of models Mo \u2192 M1 \u2192 \u2192 Mp, where Mo = M, Mp = M', and for each Mkp, kp \u2208 [p], the choices of centers only differs from the choices of centers of Mkp\u22121 in one group. If (a) hold for models with one difference of the choices, then following the sequence of the models, (a) must also hold for Mp.\nWe prove by contradiction. Suppose V is an ancestor of V' in M' and the total causal effect from V to V' is zero. Note that the total causal effect from V to V' is equal to the sum of path products from the exogenous noise of V to V'. Suppose the exogenous noise of V in M' is the exogenous noise of Vo in M. Since M satisfies Assumption 2, Vo is not an ancestor of V' in M. This means that the added edges in M' introduces additional ancestors to V'.\nNote that if V' = Zj, then we compare the addition of ancestors to Z; in M' with the ancestors of Vj in M as both are the center variable in the corresponding model. Similarly, we compare the addition of ancestors to V; in M' with the ancestors of V\u00bf in M.\nHowever, as we described in Appendix A.2, all added edges in M' can be categorized as follows:\n\u2022 If V' = Zj. According to (ii), all added edges in M' are from parents of Zj in M to Zj. However, parents of Zj are all ancestors of Vi in M. Therefore no additional ancestors are introduced.\n\u2022 If V' = Vk\u2081. According to (iii), all added edges in M' are also from parents of Zj in M to Zj. Since there is Vi is a parent of Vk\u2081 in M, parents of Zj are all ancestors of Vk\u2081 in M.\n\u2022 If V' = Vk\u2081. According to (iv), all added edges in M' are also from parents of V\u00bf or Zj in M to Zj. Since Vi is an ancestor of Vk\u2081 in M, parents of Vi or Zj are all ancestors of Vk\u2081.\nIn conclusion, the added edges in M' does not introduce any additional ancestors to V', which leads to a contradiction. Therefore M' satisfies Assumption 2."}, {"title": "A.4 Proof of the identification result under LV-SEM-ME faithfulness", "content": "In this subsection, we provide the proofs of all results regarding the DOG equivalence class in LV-SEM-ME, i.e., Proposition 3, and 6, and Theorem 3(b). These proof of the results for for SEM-ME and LV-SEM, i.e., Proposition 1 and Theorem 2 can be deduced from it.\nTo show that the extent of identifiablity of an LV-SEM-ME under LV-SEM-ME faithfulness is the DOG equivalence class, the proof includes two parts.\nFirst, we show that, for the ground-truth model M, any other model M' in the DOG equivalence class:\n(2.a) M' does not add extra edge compared with M.\n(2.b) M' does not remove any edge compared with M.\n(2.c) M' satisfies LV-SEM-ME faithfulness.\nTherefore we cannot distinguish M' from M. Next, any other model M\" that is in the AOG equivalence class of M but not the DOG equivalence class:\n(2.d) M\" adds at least one extra edge compared with M.\n(2.e) M\" does not remove any edge compared with M, and there is at least one added edge that is not removed.\n(2.f) M\" violates LV-SEM-ME faithfulness.\nTherefore we can distinghuish M' from M.\nA.4.1 Model within the same DOG equivalence class\nSimilar to (1.a) in the AOG proof, we only need to show the result if M' only differs from M in the choices of centers (and/or the corresponding exogenous noise) in one group. If there are p differences in the choices of centers between M and M', then we can always find a finite sequence of models Mo \u2192 M\u2081 \u2192 \u2192 Mp, where M\u2081 = M, Mp = M', and for each Mkp, kp \u2208 [p], the choices of centers only differs from the choices of centers of Mkp\u22121 in one group.\nIn the following, we show (2.a) - (2.c) together for the one difference in the choices of centers. Specifically, for (2.b), we show that if one edge is cancelled out, then M violates LV-SEM-ME faithfulness. Further, for (2.c), we show that a single change still preserves LV-SEM-ME faithfulness. Following the sequence of the models, all three properties hold for M'.\nProof of (2.a). Following the description in Appendix A.2, after replacing the center V\u2081 in M with Zj, and replacing the exogenous noise Nv; in M with NH\u2081, where Z; and H\u2081 belong to the same direct ordered group as Vi in M, all added edges in M' can be categorized as follows:\n\u2022 For Vi: No edges are added.\n\u2022 For Zj: According to (ii), all added edges in M' are from parents of Zj (excluding V\u00bf) in M to Zj. However, according to Condition 1(a), since Zj belongs to the same direct ordered group as Vi, Pa(Zj) \\ {V} is a subset of Pa(Vi). Therefore no edges are added.\n\u2022 For Vk\u2081: According to (iii), all added edges in M' are from parents of Z; in M (excluding Vi) to Vki. Still, according to Condition 1(b), since Zj belongs to the same direct ordered group as Vi, Pa(Zj) is a subset of Pa(Vk\u2081). Therefore no edges are added."}]}