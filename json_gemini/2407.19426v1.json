{"title": "Causal Discovery in Linear Models with Unobserved Variables and Measurement Error", "authors": ["Yuqin Yang", "Mohamed Nafea", "Negar Kiyavash", "Kun Zhang", "AmirEmad Ghassami"], "abstract": "The presence of unobserved common causes and the presence of measurement error are two of the most limiting challenges in the task of causal structure learning. Ignoring either of the two challenges can lead to detecting spurious causal links among variables of interest. In this paper, we study the problem of causal discovery in systems where these two challenges can be present simultaneously. We consider linear models which include four types of variables: variables that are directly observed, variables that are not directly observed but are measured with error, the corresponding measurements, and variables that are neither observed nor measured. We characterize the extent of identifiability of such model under separability condition (i.e., the matrix indicating the independent exogenous noise terms pertaining to the observed variables is identifiable) together with two versions of faithfulness assumptions and propose a notion of observational equivalence. We provide graphical characterization of the models that are equivalent and present a recovery algorithm that could return models equivalent to the ground truth.", "sections": [{"title": "1 Introduction", "content": "Causal structure learning, also known as causal discovery, from observational data has been studied extensively in the literature. The majority of work assume that there are no unobserved variables in the system that can affect more than one other variable, i.e., no unobserved common causes, and that variables are measured without error. This leads to identification of the underlying structure up to Markov equivalence class in general (Spirtes et al., 2000; Chickering, 2002), and complete identification of the structure under further model assumptions such as assuming a linear non-Gaussian model (LiNGAM, (Shimizu et al., 2006, 2011)). However, in majority of real-world settings, the researcher will not be able to observe all the relevant variables and hence cannot rule out the presence of unobserved common causes, and further many of the variables may have been measured with error. This necessitates approaches for causal discovery capable of dealing with these two important challenges."}, {"title": "2 Model Description", "content": "Notations We use upper-case letters with subscript for variables, upper-case letters without subscript for vectors and bold upper-case letters for matrices. For two vectors or variables X, Y, we use [X, Y] to represent the horizontal concatenation of X and Y, and [X; Y] to represent the vertical concatenation of X and Y.\nWe start with a formal definition of the model that we consider in this work.\nDefinition 1 (General linear LV-SEM-ME) A general linear LV-SEM-ME consists of two sets of variables V and X. Variables in V can be arranged in a causal order, and each variable \\(V_i \\in V\\) is generated as a linear combination of a subset \\(Pa(V_i) \\subset V\\) (called its direct parents), plus an exogenous noise term \\(N_{v_i}\\), where \\({N_{v_i}}\\)_{v_i \\in V}\\) are jointly independent. Further, V can be partitioned into three sets Y, Z and H. Variables in Y are observed (without error). Variables in Z are measured with error, where each variable \\(X_i \\in X\\) is a noisy measurement of one corresponding variable \\(Z_i \\in Z\\) plus an exogenous noise term \\(N_{X_i}\\) (which we call measurement error of \\(Z_i\\)). Variables in H are neither observed nor measured with error. We refer to variables in H, Y, Z, and X as unobserved variables, observed variables, measured variables, and measurements, respectively.\nWe define a measured leaf variable (mleaf variable) as a measured variable in Z that has no other children besides its noisy measurement. We define a cogent variable as a variable in \\(Z \\cup Y\\) that is not an mleaf. As mentioned earlier, we study the problem of recovering the linear LV-SEM-ME from observations of (Y,X). We add the following two restrictions on the model for the sake of model identifiability based on observational data.\n\u2022 Firstly, as discussed in (Zhang et al., 2018; Yang et al., 2022), for any mleaf variable \\(Z_i\\), the exogenous noise term \\(N_{Z_i}\\) is not distinguishable from its measurement error \\(N_{X_i}\\). Specifically, for any two models that only differ in \\(Z_i\\) and \\(X_i\\) for some mleaf variable \\(Z_i\\) but have the same sum \\(N_{Z_i} + N_{X_i}\\),"}, {"title": "3 Identification Analysis", "content": "In this section we study identification for our model of interest. We start by looking at the LV-SEM and SEM-ME separately in Subsection 3.1; we consider identification in presence of both unobserved variables and measured variables in Subsection 3.2, which is our main identification result. In both subsections, we study identification under two faithfulness assumptions where, as will be discussed shortly, the first one,"}, {"title": "3.1 Identifiability of SEM-ME and LV-SEM", "content": "3.1.1 Identification Assumptions\nIn the following we present two assumptions for the identifiability of SEM-ME and LV-SEM, namely separability assumption and faithfulness assumption. Note that for the identifiability of LV-SEM-ME, we need one extra assumption which we will present in Section 3.2.\nSeparability assumption. We first deduce the mixing matrix that transforms independent exogenous noise terms to the observed variables [X; Y]. To simplify the deduction, we rewrite Equation (1b) as follows, by considering cogent variables (ZC and Y) as a single vector:\n\n\\[\\begin{bmatrix}\nZ^L \\\\\nV^C\n\\end{bmatrix} = \\begin{bmatrix}\nB^L \\\\\nB^C\n\\end{bmatrix}H + CV^C + \\begin{bmatrix}\n0 \\\\\nN_{VC}\n\\end{bmatrix},\\qquad(2)\\]\n\nwhere \\(V^C = [Z^C;Y]\\) represent the vector of cogent variables, and \\(N_{VC} = [N_{Z^C}; N_Y]\\) represent their corresponding exogenous noise terms. B is partitioned into \\([B^L; B^C]\\) according to \\([Z^L; V^C]\\). From Equations (1a) and (2), we can write variables in \\([Z^L; V^C]\\) as linear combinations of the exogenous noise terms:\n\n\\[\\begin{bmatrix}\nZ^L \\\\\nV^C\n\\end{bmatrix} = W^* \\begin{bmatrix}\nN_H \\\\\nN_{VC}\n\\end{bmatrix},\\qquad\\text{where } W^* = \\begin{bmatrix}\nB^L + D(I - C)^{-1}B^C & D(I - C)^{-1} \\\\\n(I - C)^{-1}B^C & (I - C)^{-1}\n\\end{bmatrix},\\qquad (3)\\]\n\nand I represent the \\(p_c \\times p_c\\) identity matrix. Lastly, combined with Equation (1c), the overall mixing matrix W can be written as\n\n\\[\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix} = \\begin{bmatrix}\nW^* & \\begin{bmatrix}\nI \\\\\n0\n\\end{bmatrix} \\\\\n& I\n\\end{bmatrix} \\begin{bmatrix}\nN_H \\\\\nN_{VC} \\\\\nN_{XL} \\\\\nN_{XC}\n\\end{bmatrix} = W\\begin{bmatrix}\nN_H \\\\\nN_{VC} \\\\\nN_{XL} \\\\\nN_{XC}\n\\end{bmatrix} , \\qquad (4)\\]\n\nWe note that because mleaf variables have no exogenous noise, each column in \\(W^*\\) either has at least two non-zero entries, or has one non-zero entry where the non-zero entry is not in X."}, {"title": "3.1.2 Identification Under Conventional Faithfulness", "content": "We first present a graphical characterization of equivalence under conventional faithfulness, called Ancestral ordered grouping (AOG) equivalence, and then formally show that this notion of equivalence is the extent of identifiability in Theorem 1.\nDefinition 3 (Ancestral ordered grouping (AOG)) The AOG of a SEM-ME (resp. LV-SEM) is a partition of \\(Z^L \\cup V^C\\) (resp. \\(H \\cup V^C\\)) into distinct sets. This partition is described as follows:\n(1) Assign each cogent variable in \\(V^C\\) to a distinct group.\n(2) (i) SEM-ME: For each mleaf variable \\(Z_i \\in Z^L\\), if it has one measured parent \\(Z_j \\in Z \\cap V^C\\) such that \\(Z_j\\) has no other parents, or all other parents of \\(Z_j\\) are also ancestors of \\(Z_i\\), assign \\(Z_i\\) to the same group as \\(Z_j\\). Otherwise, assign \\(Z_i\\) to a separate group (with no cogent variable).\n(ii) LV-SEM: For each unobserved variable \\(H_j \\in H\\), if it has one cogent child \\(V_i \\in V^C\\) such that all other children of \\(H_j\\) are also descendants of \\(V_i\\), assign \\(H_j\\) to the same group as \\(V_i\\). Otherwise, assign \\(H_j\\) to a separate group (with no cogent variable).\nDefinition 4 (AOG equivalence class) The AOG equivalence class of a linear SEM-ME (resp. LV-SEM) is a set of models where the elements of this set all have the same mixing matrix (up to permutation and scaling) and same ancestral ordered groups.\nGraphical characterization. It was shown in (Yang et al., 2022) that all models in the AOG equivalence class are consistent with the same set of causal orders among the groups (i.e., if a causal order on the groups"}, {"title": "3.1.3 Identification Under LV-SEM-ME Faithfulness", "content": "We first present a graphical characterization of equivalence under LV-SEM-ME faithfulness, called Direct ordered grouping (DOG) equivalence, and then formally show that this notion of equivalence is the extent of identifiability in Theorem 2.\nCondition 1 (SEM-ME edge identifiability) For a given edge from a measured cogent variable \\(Z_i\\) to an mleaf variable \\(Z_l\\), at least one of the following two conditions is satisfied: (a) \\(Pa(Z_l) \\backslash \\{Z_i\\}\\) is not a subset of Pa(Zi). That is, there exists another parent \\(V_j\\) of \\(Z_l\\), which is not a parent of Zi. (b) Pa(Zi) is not a subset of \\(\\bigcap_{V_k \\in Ch(Z_l) \\backslash \\{Z_l\\}} Pa(V_k)\\). That is, there exists a child \\(V_k\\) of \\(Z_l\\) and a parent \\(V_j\\) of \\(Z_i\\) such that \\(V_j\\) is not a parent of \\(V_k\\).\nCondition 2 (LV-SEM edge identifiability) For a given edge from an unobserved variable \\(H_i\\) to a cogent variable \\(V_i\\), there exists another cogent child \\(V_j\\) of \\(H_i\\), such that at least one of the following two conditions is satisfied: (a) \\(V_i\\) is not a direct parent of \\(V_j\\). (b) \\(Pa(V_i)\\) is not a subset of \\(Pa(V_j)\\). That is, there exists an observed (or unobserved) parent \\(V_k\\) (or \\(H_k\\)) of \\(V_i\\) that is not a parent of \\(V_j\\).\nDefinition 5 (Direct ordered grouping (DOG)) The DOG of a linear SEM-ME (resp. LV-SEM) is a partition of \\(Z^L \\cup V^C\\) (resp. \\(H \\cup V^C\\)) into distinct sets. This partition is described as follows:\n(1) Assign each cogent variable in \\(V^C\\) to a distinct group."}, {"title": "3.2 Identifiability of LV-SEM-ME", "content": "In this subsection, we provide identification results for a system in which the challenges of unobserved confounders and measurement error can co-exist. For this case, we need an extra assumption, called minimality, described below.\nAssumption 4 (Minimality) We assume the linear LV-SEM-ME M is minimal, that is, there does not exist any other linear LV-SEM-ME M' such that M' has strictly fewer unobserved variables than M, the same observability indicators of the variables, and the same mixing matrix as M up to permutation and scaling of the columns.\nMinimality assumption asserts that the ground-truth model has fewer (or equal) unobserved variables than any other models that has the same mixing matrix and observability indicator. This assumption is required since we cannot infer the number of unobserved variables without prior knowledge of the system. Recall from Equation (3) that the number of columns of \\(W^*\\) is the sum of the number of cogent variables and unobserved variables. However, the number of each type of variable is not known apriori under only separability assumption.\nMinimality assumption is always required when unobserved variables are present in the system. Specifically, it is often assumed that the ground-truth model either has the fewest number of edges (Adams et al., 2021), or the fewest number of unobserved variables (Salehkaleybar et al., 2020). Our minimality condition falls into the latter case, and in Proposition 2 below, we show that the minimality assumption has a equivalent graphical characterization.\nProposition 2 (Minimality) Under Assumption 2, a linear LV-SEM-ME is not minimal if and only if there exists an unobserved variable \\(H_i\\) and a mleaf child \\(Z_j\\) of \\(H_i\\), such that for any other child \\(V_k\\) of \\(H_i\\), \\(An(Z_j) \\subseteq An(V_k)^4\\). This is equivalent to the following. Any (observed or unobserved) parent of \\(Z_j\\) is also an ancestor of \\(V_k\\).\nWe note that there is a similarity between the condition in Proposition 2 and the condition in the definition of AOG in Definition 3. The reason for this similarity is that both intend to characterize the situation in which the location of an exogenous source is unknown. The former condition considers the case where this source belongs to a unobserved variable, while the latter considers the case where this source belongs to a measured cogent variable.\nEquipped with minimality assumption, we are ready to present our main identification result for the LV-SEM-ME model. Similar to Subsection 3.1, we provide results under Assumptions 1 and 3 to emphasize the trade-off between the strength of the assumption and the resulting extent of identifiability.\nDefinition 7 (AOG and DOG of LV-SEM-ME) The DOG (resp. AOG) of an LV-SEM-ME consists of a partition of the variables in \\(H \\cup Z^L \\cup V^C\\) described as follows:"}, {"title": "4 Algorithm", "content": "In this section, we present recovery algorithms for the introduced LV-SEM-ME model. We first present AOG recovery algorithm (Algorithm 1) in Section 4.1. The algorithm returns the AOG of the underlying model, and it is used in both AOG equivalence class and DOG equivalence class recovery algorithms. We then show how to recover all models in the AOG and DOG equivalence classes in Section 4.2 based on the recovered AOG (Algorithm 2)."}, {"title": "4.1 AOG Recovery", "content": "The following property can be implied from the definition of AOG, which shows that under conventional faithfulness assumption, we can identify the AOG of the model only based on the support of the mixing matrix \\(W^*\\).\nProposition 4 Under Assumptions 2 and 4,\n(a) One mleaf variable and one measured cogent variable belong to the same ancestral ordered group if and only if the two rows in \\(W^*\\) corresponding to these variables have the same support. Further, for any cogent variable \\(V_i\\) and its descendant \\(V_j\\), the row support of \\(V_i\\) must be a subset of the row support of \\(V_j\\).\n(b) One unobserved variable and one cogent variable belong to the same ancestral ordered group if and only if the two columns in \\(W^*\\) corresponding to the exogenous noise terms of these variables have the same support. Further, for any cogent variable \\(V_i\\) and its ancestor \\(V_j\\), the column support of \\(N_{V_i}\\) must be a subset of the column support of \\(N_{V_j}\\).\nThe proof of Proposition 4 directly follow from the definition of AOG, and hence are omitted.\nEquipped with Proposition 4, we propose an iterative algorithm for recovering the AOG in Definition 7 from \\(W^*\\). The pseudo-code of the proposed method is presented in Algorithm 1. In the first iteration, the Algorithm randomly chooses a row in \\(W^*\\) with the fewest number of non-zero entries and finds all other rows with the same support. Denote the selected rows as \\(Z_J\\), and columns corresponding to these non-zero entries as \\(N_1\\). Each of the selected row may either correspond to a cogent variable or an mleaf variable, and each of the column in \\(N_1\\) may either correspond to the exogenous noise of an cogent variable or an unobserved confounder. The task is to decide whether there exists a cogent variable (and its associated exogenous noise). We first select the columns in \\(N_1\\) with the fewest number of non-zero entries in \\(W^*\\). Denote this subset as \\(N_J\\). Then noises in \\(N_1 \\backslash N_J\\) must correspond to unobserved variables and are assigned to separate groups in \\(C_{unobserved}\\).\nWe check whether any of the rows in \\(Z_J\\) can be a cogent variable. If there is one observed variable in \\(Z_J\\) then it must correspond to the cogent variable. If all variables are unobserved, then we look at the submatrix \\(W_0\\) of \\(W^*\\) with the rows corresponding to the (column) support of any variable in \\(N_J\\), and columns corresponding to the (row) support of \\(Z_J\\). If \\(Z_J\\) includes a cogent variable, then its corresponding exogenous noise must be in \\(N_J\\), and the remaining noises are unobserved confounders. Since all noises in \\(N_J\\) have the same number of non-zero entries, they must have the same support. Further, any row that includes this exogenous noise must be a descendant of the cogent variable, and must include all the non-zero columns of the cogent variable under Assumption 2. This implies that \\(W_0\\) does not include any non zero entry. Therefore, if \\(W_0\\) includes any zero entry, then none of the rows in \\(Z_J\\) would correspond to a cogent variable. The rows in \\(Z_J\\) and the columns in \\(N_J\\) all belong to separate groups in \\(C_{mleaf}\\) and \\(C_{unobserved}\\) as they correspond to mleaf variables and unobserved variables. If \\(W_0\\) does not include any zero entry, then under minimality assumption, one of the rows in \\(Z_J\\) correspond to a cogent variable. Therefore all noises"}, {"title": "4.2 Model Recovery in the AOG and DOG Equivalence Class", "content": "In this section, we present our algorithm for recovering the models in the equivalence class using \\(W^*\\). The psuedo-code of the algorithm is presented in Algorithm 2.\nAOG Equivalence class. Recall from Section 3.2 that all members in the AOG equivalence class can be enumerated by switching the center with any mleaf variables in the group and/or switching the exogenous noises of the center with the noises of any unobserved variables in the group. Therefore, given the mixing matrix \\(W^*\\), Algorithm 2 first recovers the AOG of the true model using Algorithm 1. Then, it enumerates all possible choices of centers and the noises for each group. Note that groups with only mleaf variables or noises of unonserbed variables (i.e., in \\(C_{mleaf}\\) or \\(C_{unobserved}\\)) only have one choice. Therefore, we only need to consider all the groups that include cogent variables (i.e., in \\(C_{cogent}\\)). Denote each single selection of the centers in these groups as row, and the noises as col. The next step is to recover the model parameters B, C, D based on \\(W^*\\) and the selected row and col following Equation (3). Denote the variables not in row as rowC, and the noises not in col as colC. The selected centers in row correspond to \\(V^C\\), and the selected noises in col correspond to \\(N_{VC}\\) in (3). Similarly, variables in rowC and noises in colC correspond to \\(Z^L\\) and \\(N_H\\), respectively. Therefore C, BC, D, BL can be calculated following lines 6-9 in Algorithm 2. Finding model parameters B, C, D for all possible choices of row and col gives us all models in the AOG equivalence class."}, {"title": "5 Conclusion", "content": "We studied the problem of causal discovery when the challenges of unobserved common causes and mea- surement error may coexist in the model. We defined a class of models called linear LV-SEM-ME which can be used to model both unobserved variables and measurement error. Further, we characterized the extent of identifiability of this model under separability assumption and two versions of the faithfulness assumption. Specifically, the first version, referred to as the conventional faithfulness requires only pairwise information about the variables, while the second version, referred to as the LV-SEM-ME faithfulness requires joint information. Nevertheless violation of both versions occur with zero probability. We showed that, under conventional faithfulness and LV-SEM-ME faithfulness, the model can be learned up certain model classes that we called AOG and DOG equivalence classes, respectively. We provided graphical characterization of the models that are equivalent and presented a recovery algorithm that could return models equivalent to the ground truth. A remaining challenge in our proposed methodology is its reliance on the accuracy of the provided estimate of the mixing matrix of the underlying model. Hence, devising accurate approaches for estimating the mixing matrix is an important direction of future research. Moreover, our proposed method- ology is based on the assumption of linearity of the data generating process. Relaxing this assumption is another important extension of our work which we leave to future work."}, {"title": "A Proofs", "content": "A.1 Proof of Proposition 2\nThe proof includes two parts. We first show the sufficiency: For an unobserved variable \\(H_i\\) in the ground truth model M, if it has an mleaf child that satisfies the condition described in Proposition 2, then M is not minimal, i.e., there exists an alternative model M' without \\(H_i\\) that has the same mixing matrix and satisfies Assumption 1. Next, we show the necessity: If M is not minimal, then there must exist an unobserved variable \\(H_i\\) and one of its mleaf child \\(Z_j\\) such that the described condition is satisfied.\nA.1.1 Proof of sufficiency\nSuppose there exists latent variable \\(H_i\\) and a mleaf child \\(Z_j\\) of \\(H_i\\) in M such that the condition described in Proposition 2 holds. In the following we construct the alternative model M' that includes all variables in M except for \\(H_i\\). The idea is to consider \\(N_{H_i}\\) as the exogenous noise term of \\(Z_j\\). Further, for any other child \\(V_k\\) of \\(H_i\\), replace the edge \\(H_i \\rightarrow V_k\\) in M by edges from \\(Z_j\\) (and parents of \\(Z_j\\)) to \\(V_k\\) in M'.\nThe structural equation of \\(Z_j\\) in M can be written as\n\n\\[Z_j = \\sum_{l: V_l \\in Pa_M(Z_j) \\cap V^C} a_{jl}V_l + \\sum_{lj: H_{lz} \\in Pa_M(Z_j) \\cap H} b_{jlj}H_{lj} + b_{ji}N_{H_i} (5)\\]\n\nConsider \\(N_{H_i}\\) as the exogenous noise term of \\(Z_j\\) in M'. The structural equation of \\(Z_j\\) in M' is\n\n\\[Z_j = \\sum_{l: Z_l \\in Pa_M(Z_j) \\cap V^C} a_{jl}Z_l + \\sum_{lj: H_{lz} \\in Pa_M(Z_j) \\cap H \\\\{H_i\\}} b_{jlj}H_{lj} + b_{ji}N_{H_i}. (6)\\]\n\nFor any other children \\(V_k\\) of \\(H_i\\) in M, the structural equation of \\(V_k\\) in M can be written as\n\n\\[V_k = \\sum_{\u03b1_{kl}Z_l + \\sum b_{klk}H_{lk} + N_{V_k}. (7)\\]\nl: Z_l \\in Pa_M(V_k) \\cap V^C\\)\nlk: H_{ik} \\in Pa_M(V_k) \\cap H\\)\n\nBy considering \\(N_{H_i}\\) in Equation (7) to be the exogenous noise of \\(Z_j\\), the structural equation of \\(Z_m\\) in M' can be written as\n\n\\[\\begin{aligned}\nV_k &= \\sum_{l: Z_l \\in Pa_M(V_k) \\cap V^C} \u03b1_{kl}Z_l + \\sum_{lk: H_{ik} \\in Pa_M(V_k) \\cap H \\\\{H_i\\}} b_{klk}H_{lk} + N_{Z_m}+\\\\\n& b_{kib}Z_j = \\sum_{l: Z_l \\in Pa_M(V_k) \\cap V^C} \u03b1_{kl}Z_l - \\sum_{l: Z_l \\in Pa_M(Z_j) \\cap V^C} b_{kib}\u03b1_{jl}Z_l - \\sum_{lj: H_{iz} \\in Pa_M(Z_j) \\cap H \\\\{H_i\\}} b_{jl.j} H_{lj} \\end{aligned} (8)\\]\n\nSince \\(H_i\\) and \\(Z_j\\) satisfy the condition in Proposition 2, \\(V_k\\) cannot be an ancestor of \\(Z_j\\) or variables in \\(Pa_M(Z_j)\\) in M, otherwise we have \\(V_k \\in \\bigcup_{Z \\in Pa(Z_j)} An_M(Z = An_M(Z_j) \\subseteq An_M(V_k)\\). This implies that M' is still acyclic. Further, since \\(An_M(Z_j) \\subseteq An_M(V_k)\\), there are no additional ancestors introduced to \\(V_k\\) in M' compared with M. Lastly, we note that there might be edge cancellations in (4). In particular, the coefficient of the direct edge from a variable in \\(Z \\in Pa_M(V_k) \\cap V^C\\) to \\(V_k\\) may change in M' if \\(Z \\in Pa_M(Z_j)\\) and hence may be cancelled out. However, Z is still an ancestor of \\(V_k\\) in M', as there is"}, {"title": "A.2 Enumerating all models in the AOG and DOG equivalence classes by different choice of centers", "content": "We first show that an LV-SEM-ME can be uniquely deduced given the mixing matrix \\(W^*\\) of the LV-SEM- ME, and a choice of the centers (and their corresponding exogenous noises) in each group. This has been described in Algorithm 2, where the matrices B, C, D can be found through matrix calculation.\nIn this following, given the ground-truth model M, we will show how to deduce the structural equations of the variables in the alternative model M', where M and M' has the same mixing matrix and the same (ancestral ordered or directed ordered) grouping of the variables. Specifically, we consider the case when"}, {"title": "A.3 Proof of the identification result under conventional faithfulness", "content": "In this subsection, we provide the proof of the result regarding the AOG equivalence class in LV-SEM-ME, i.e., Theorem 3(a). Note that the proof of the result for SEM-ME and LV-SEM, i.e., Theorem 1, can be deduced from it.\nTo show that the extent of identifiablity of an LV-SEM-ME under conventional faithfulness is the AOG equivalence class, we need to show that, for the ground-truth model M, any other model M' in the AOG equivalence class of M, and any model M\" that has the same mixing matrix but does not belong to the AOG equivalence class of M:\n(1.a) M' satisfies conventional faithfulness.\n(1.b) M' is consistent with any causal order among the ancestral ordered groups that is consistent with M.\n(1.c) M\" violates conventional faithfulness."}, {"title": "A.4 Proof of the identification result under LV-SEM-ME faithfulness", "content": "In this subsection, we provide the proofs of all results regarding the DOG equivalence class in LV-SEM-ME, i.e., Proposition 3, and 6, and Theorem 3(b). These proof of the results for for SEM-ME and LV-SEM, i.e., Proposition 1 and Theorem 2 can be deduced from it."}]}