{"title": "Neural Networks Remember More: The Power of Parameter Isolation and Combination", "authors": ["Biqing Zeng", "Zehan Li", "Aladdin Ayesh"], "abstract": "Catastrophic forgetting is a pervasive issue for pre-trained language models(PLMs) during continual learning, where models lose previously acquired knowledge when sequentially trained on a series of tasks. The model's ability to remain old tasks is referred to as stability, while its adaptability to new tasks is called plasticity. Therefore, the key to solving this problem is to find a trade-off between the plasticity and stability of the model. To address this issue, in this paper, we propose a novel method to achieve a balance between model stability and plasticity, thereby mitigating catastrophic forgetting. More specific, our proposed approach leverages parameter isolation and subsequent combination strategy. Initially, in training stage, the model adapts on each downstream task via parameter isolation method to prevent potential inference among different tasks. We then combine all trained parameters which containing acquired knowledge by the task arithmetic method and finally apply to the backbone model. Empirical evaluations on continual language learning benchmarks substantiate the effectiveness of our approach, revealing a marked enhancement over existing state-of-the-art approaches.", "sections": [{"title": "1 Introduction", "content": "Pre-trained Language Models (PLMs) have shown outstanding performance on a diverse range of downstream Natural Language Processing(NLP) tasks [1]. In real-world application, PLMs are often deployed in dynamic environment, necessitating continual adapting on the new data while preserving previous learned knowledge. However, an intractable issue known as catastrophic forgetting [2] arises during continual learning, where models may drastically forget previously acquired knowledge when adapting to new tasks.\nThere is a series of work focus on mitigating catastrophic forgetting. For instance, rehearsal-based methods alleviate the problem through retraining the model with historical data which is cached during the previous learning process [3, 4]. Nevertheless, as the model size grows, retraining the model multiple times is infeasible because of the expensive computational cost. Moreover, access to historical data may be restricted in some situations due to privacy and security. In addition to rehearsal-based methods, parameter isolation is another popular method for mitigating catastrophic forgetting, which alleviate potential inference among different tasks by allocating a separate set of parameters for each task [5, 6]. However, conventional parameter isolation methods are only appropriate for task-incremental Continual learning problems because they require a task-id to select the proper modules during the testing phase.\nMotivated by the above issues, our proposed method employs parameter isolation and combination strategy to replace conventional methods. More specifically, we employ Parameter-Efficient Fine-Tuning(PEFT) methods [7] (Adapter [8] and LORA [9] in our experiments) to adapt downstream tasks, which introduce a small set of external parameters and only fine-tune these parameters while backbone model counterpart is kept frozen. Subsequently, after training on all downstream tasks, we use task arithmetic method [10] to integrate the knowledge that the model has obtained, thereby overcoming the limitation of previous parameter isolation methods that require task-id during testing stage. Besides mitigating catastrophic forgetting, we would like to facilitate knowledge transfer [6] among diverse tasks. In our work, we show that simply initializing the current PEFT modules with those of previous tasks effectively improves the knowledge transfer between learned and new tasks.\nTo verify the effectiveness of our proposed method, we conduct extensive experiments on standard continual learning benchmarks. The results show that our method not only outperforms existing rehearsal-based methods, but also improves upon previous state-of-the-art rehearsal-free methods. For example, the EPI method [11] perform an average accuracy of 76.3% in the full-shot setting, while our approach obtains superior performance(77.2%).\nTo summarize, our contributions are as follows:\n\u2022 We develop a novel method for mitigating catastrophic forgetting based on parameter isolation and combination strategy, and the effectiveness is validated through extensive experiments.\n\u2022 Our proposed approach achieves satisfactory performance without the need to save historical data compared to the previous rehearsal-based methods, thereby reducing the storage and computational consumption."}, {"title": "2 Related Works", "content": "Continual learning is a scenario that the model learn from a stream of data over time, distinguishing it from traditional methods that train on the stationary dataset. In continual learning, LLMs commonly encounter the problem of catastrophic forgetting. This occurs because as the model adapts to new tasks, its parameters tend to deviate from the optimal values that were previously established for old tasks. This phenomenon significantly affects the performance and reliability of models in practical applications, especially when dealing with multiple tasks and datasets [12]. To address this issue, various strategies have been proposed in the past. Here, we discuss three widely used methods, which include replay [13, 14], parameter regularization [15-17], and parameter isolation [18-20] methods.\nReplay, also known as rehearsal, is based on the idea of training models by sup-plementing the training data of current task with representative previous data [14]. However, these approaches are not without risk, as they may lead to privacy leakage. Further more, as the model scales up, there is a corresponding increase in the required storage and computing resources.\nParameter regularization restricts the update of model weights through adding a regularization term to the loss function that penalizes large changes to the network's parameters [16]. Although these methods alleviate the problem of forgetting to some extent, they may also reduce the model's ability to adapt to new tasks [17].\nParameter isolation methods avoid interference between different tasks by assigning certain parts of the model exclusively to specific tasks [20]. However, these methods are only applicable to task-incremental learning scenarios as they often require a task-id to select the correct model when testing."}, {"title": "2.2 Task Arithmetic", "content": "In our study, we employ Task Arithmetic [10], a groundbreaking approach to combines all parameters corresponding to each individual task after training. Task Arithmetic represents an innovative paradigm to guiding model behavior, focusing on the use of task vectors. The task vector specifies a direction within the weight space of a pre-trained model, and adjusting the model along this direction enhances its performance on the specific task. These vectors are obtained by subtracting the weights of the pre-trained model from those of a fine-tuned model. Subsequently, we can leverage simple arithmetic operations, termed task arithmetic, on task vectors to edit a model. For instance, by adding task vectors, we can combine diverse models to create a more effective multi-task model."}, {"title": "3 Methodology", "content": "Fig 1 summarises the approach presented in this paper. In the training stage, we assign a new PEFT module for each task and fine-tune model on training dataset while all of parameters of backbone model are kept frozen. After training on a task, we obtain and save the task vector by subtracting the initialization weights of the PEFT module from the tuned parameters.\nDuring testing time, we combine all acquired knowledge by adding task vectors according to task arithmetic method. Consequently, we apply the integrated task vector to the original pre-trained model and test its performance on each task's dataset.\nIn the subsequent sections, we begin with an in-depth discussion of the parameter isolation with PEFT methods in Section 3.1. Moving forward, we illustrate the parameter combination method for testing in Section 3.2. Finally, Section 3.3 will delve into various knowledge transfer methods."}, {"title": "3.1 Parameter Isolation with PEFT", "content": "One of the reasons for catastrophic forgetting in continual learning is the interference between tasks [21, 22]. As the network parameters are adjusted to optimize the loss on the new task, they are often shifted away from their optimal values for the previously learned tasks. Therefore, a direct approach is to assign distinct parameters to each task, thereby preventing potential inference between them. However, assigning a distinct pre-trained model to each task would incur extremely high storage costs. Therefore we employ parameter-efficient fine-tuning [23] framework as an alternative. This approach adds a new component to the backbone model whenever a new task needs to be learned, while sharing the powerful pre-trained model across tasks.\nIn the following, we briefly review Adapter and LoRA method. Adapter [8] are designed to make more general architectural adjustments, repurposing a pre-trained model for a specific downstream task. The adapter tuning strategy introduce a bottleneck structure to neutral network, including a couple of up/down project matrix. LORA [9] allows us fine-tune a model indirectly by optimizing rank decomposition matrices of the dense layer's change during adaption. In a word, Adapter and LORA utilize a shared pre-trained model, the weights denoted as \u0398, across various tasks. Additionally, they allocate a small number of external parameters for each task, represented as \u03a6n, where n \u2208 {1, 2, . . ., N}. The optimization objective for each task is of the form:\n\u03a6\u03b7= argmin L\u03b7(\u0398, \u03a6\u03b7)  (1)\nIn\nA pivotal feature of PEFT methods is their ability to effectively train models for specific tasks using a small set of external parameters integrated with a powerful PLM [23]. These approach not only significantly cut down the storage costs by parameter isolation methods, it also ensures high performance for individual tasks."}, {"title": "3.2 Parameter Combination with Task Arithmetic", "content": "After training, we save the fine-tuned weights and subsequently calculate the task vectors. Denote a task vector for a specific task as:\n\u03c4i = \u03a6i \u2212 \u03a6pre  (2)"}, {"title": "3.3 Knowledge Transfer with Initialization", "content": "Although completely separating parameters per task would eliminate any inference among tasks, it would also block the positive transfer among tasks [12]. Given a series of learned PEFT module \u03a6 = {\u03a61, \u03a62, . . . , \u03a6i}, we explore different strategies to leverage the knowledge acquired from previous tasks. Our goal is to enhance and accelerate the learning process for the current task.\nIn our experiments, we demonstrate that initializing a new PEFT module with well-trained parameters is an effective strategy for facilitating knowledge transfer. A desirable starting point also significantly aids in accelerating the convergence of the training process for PEFT methods. As a result, we opt to initialize using previously learned modules instead of random initialization. In our experiments, we explore two initialization strategies for the PEFT modules. First, we initialize PEFT module with the parameters of the previous module, formalized as:\n\u03a6i \u2190 \u03a6i\u22121  (5)\nSecond, we initialize the module using the average weights of the previously tuned modules, calculated as:\n\u03a6i \u2190 1(i-1) \u2211\u03a6i (6)\nt=1\nThe experimental results show that the effects of these two methods are similar(The former strategy is slightly better than the latter)."}, {"title": "4 Experiments", "content": "Following the previous setting [4, 24], we assess the effectiveness of our approach using a well-established continual learning benchmark. The benchmark is composed of five diverse datasets: AG News (news), Yelp (business reviews), Amazon (product reviews), Yahoo!Answer (Q&A), and DBPedia (encyclopedic articles). Collectively,"}, {"title": "4.1.2 Baselines.", "content": "We employ the subsequent continual learning methods as baselines:\n\u2022 Fine-tune. Fine-tuning the model with PEFT method on all tasks. Due to the severe degree of forgetting that occurs, this approach also represents the lower bound of continual learning.\n\u2022 Replay. Following the conventional replay method, we cache some data during training process and re-train the model periodically.\n\u2022 MTL. Training the model on all tasks simultaneously. Multi-task learning represents the upper bound of continual learning.\n\u2022 EPI [11]. A parameter isolation method via prefix tuning [25] and using an non-parametric task identifier during testing. This method and ours leverage PEFT methods to implement the parameter isolation strategy similarly. However, our approach differs in that we create an integrated model by consolidating all weights of PEFT modules, which eliminates the need for task identification during the testing phase."}, {"title": "4.1.3 Implement Details.", "content": "We utilize the Roberta-large [26] as the PLM in our experiments. We set the default adapter bottleneck to 32, default lora rank to 16 and the scaling term A to 0.25 while combing task vectors."}, {"title": "4.2 Main Results", "content": "In Table. 1, we compare the performance of our proposed method with established baselines across five benchmark datasets: AG News, Yelp, Amazon, Yahoo, and DBPedia. In our replay approach, we store 50 samples per class, which is equivalent to 2.5% of the entire training dataset. The results presented in the table show that our approach achieves a new state-of-the-art (SOTA) result, outperforming previous methods. Furthermore, our approach, which does not utilize experience replay or task-ID, significantly reduces the gap to the upper bound(75.60%). This demonstrates the effectiveness of our method in improving the performance of continual learning. Another key observation from the results is that the accuracy of our method remains stable across different orderings of the five datasets. This insensitivity to task sequence changes highlights the robustness and generalization capabilities of our approach. Overall, our method shows promise in improving the performance and stability of continual learning across various benchmark datasets."}, {"title": "4.3 Analysis", "content": "To further inspect the proposed methods, we investigate the following research questions."}, {"title": "4.3.1 Impact of module initialization on knowledge transfer", "content": "Ablation experiments were conducted to examine our proposed initialization method, and the results are presented in Table. 3. The experiments are implemented with 2 different datasets order. The experimental results show that, compared to not implement initialization, our method significantly improves the model's performance. Besides, we could find that the effect of these two methods are similar. Specifically, the \"pre initialization\" strategy is slightly better than the \"mean initialization\"."}, {"title": "4.3.2 Influence of different PEFT setting on model performance", "content": "To investigate the performance of different PEFT setting, we implement the experiments with various bottleneck dimension(for Adapter method) and LoRA rank(for LORA method) to compare their effect. Fig. 2 and Fig. 3 presents average accuracy across the full setting of five datasets with different PEFT method settings. The similar results show that moderate dimension size more closer to the stability-plasticity"}, {"title": "5 Conclusion", "content": "In summary, in our work, we propose a novel method that leverages parameter isolation and combination to mitigate catastrophic forgetting for pre-trained language model(PLM), with its efficacy supported by extensive experimental evidence. Our approach exhibits superior performance to previous methods without the requirement for storing historical data and task identification, leading to reduced storage and computational costs. Besides, we introduce two initialization method to facilitate the knowledge transfer between tasks."}]}