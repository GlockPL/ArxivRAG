{"title": "Soft-Label Integration for Robust Toxicity Classification", "authors": ["Zelei Cheng", "Xian Wu", "Jiahao Yu", "Shuo Han", "Xin-Qiang Cai", "Xinyu Xing"], "abstract": "Toxicity classification in textual content remains a significant problem. Data with labels from a single annotator fall short of capturing the diversity of human perspectives. Therefore, there is a growing need to incorporate crowdsourced annotations for training an effective toxicity classifier. Additionally, the standard approach to training a classifier using empirical risk minimization (ERM) may fail to address the potential shifts between the training set and testing set due to exploiting spurious correlations. This work introduces a novel bi-level optimization framework that integrates crowdsourced annotations with the soft-labeling technique and optimizes the soft-label weights by Group Distributionally Robust Optimization (GroupDRO) to enhance the robustness against out-of-distribution (OOD) risk. We theoretically prove the convergence of our bi-level optimization algorithm. Experimental results demonstrate that our approach outperforms existing baseline methods in terms of both average and worst-group accuracy, confirming its effectiveness in leveraging crowdsourced annotations to achieve more effective and robust toxicity classification.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are rapidly being adopted in applications such as conversations [1, 2], AI-assisted programming [3], and education [4]. However, despite impressive capabilities, the interaction between humans and LLMs can generate harmful, biased, or factually incorrect content [5, 6]. For example, users may ask LLMs to generate toxic content, such as hate speech, misinformation, or violent threats, which can have severe consequences for individuals and communities. Recent studies on jailbreaking LLMs also show that adversarial prompts can elicit toxic responses from models [7, 6, 8, 9]. Therefore, there is a pressing need to develop a robust toxicity classification model that can effectively identify and mitigate harmful content generated by LLMs."}, {"title": "2 Related Work", "content": "Bi-level Optimization. Bi-level optimization [20] has attracted significant attention due to its ability to handle hierarchical decision-making tasks including meta learning [21-25], neural architecture search [26-28], sample re-weighting [29, 30, 24], label denoising [31], etc. For example, in meta-learning [25], bi-level optimization provides a way to learn the initial parameters of a model which leads to fast adaptation and good generalization for various learning tasks. In this work, we formulate the toxicity classification from multiple annotations as a bi-level optimization problem where we alternate between minimizing the empirical risk minimization (ERM) loss on training samples with learned soft labels and optimizing the soft-label weights against the out-of-distribution (OOD) risk.\nLearning from Partial Labels. Training a classifier from partial labels implicitly requires determining the ground truth from multiple annotations. We categorize existing methods into two types: pre-training label identification and online label identification.\nPre-training label identification. Pre-training label identification refers to the methods that infer ground truth before training the classifier. Some baseline methods such as Majority Voting (MV) [32] and Participant-Mine Voting (PM) [33, 34] directly infer a true label from crowdsourced multiple labels [35], with MV assuming equal annotator quality and PM accounting for worker quality differences. However, both MV and PM assume annotator quality is instance-independent, which is often not the case due to varying cultural and educational backgrounds. Probabilistic models [36-38] like Snorkel use statistical dependencies to infer true labels but can be limited by non-independent annotators like GPT-4 and GPT-4 Turbo.\nPost-training label identification. Post-training label identification refers to the methods that train a set of models that approximate the annotators' labels and utilize aggregation methods (e.g., majority voting) to handle these labels such as [39].\nOnline label identification. Online label identification refers to the methods that disambiguate the candidate labels during the training. There are generally two categories of methods. The first one is average-based methods [40-42] which treats each candidate label equally in the model training phase and minimizes the average loss over all candidate labels, assuming equal likelihood for each, which is unrealistic. The second one is identification-based methods which directly maximizes the probability of exactly one candidate label [43-45]. Lv et al. [45] introduce PRODEN, which iteratively identifies pseudo labels and minimizes the corresponding loss. PRODEN starts with equal weights for all candidate labels and uses model logits to determine pseudo labels. However, incorrect initial assumptions can lead to local minima.\nDistributionally Robust Optimization. Distributionally robust optimization (DRO) optimizes the worst-case loss in an uncertainty set of test distributions [46-50]. Sagawa et al. [46] propose GroupDRO to learn a robust model to minimize the loss of the worst group when the dataset has group annotations. Oren et al. [48] propose topic-CVaR to optimize the loss over the worst-case mixture of text topics. When such group distributions are not available, conditional value at risk (CVaR) [51, 52] constructs new distributions by reweighting the training samples and minimizes the supreme loss over these distributions. In this work, we leverage the GroupDRO technique to learn a robust soft-label weight estimator."}, {"title": "3 Proposed Technique", "content": "3.1 Problem Setup and Assumption\nConsider a toxicity classification task with C classes, with a training dataset $\\mathcal{D}_{tr} := \\{(\\mathbf{X}^{(i)}, \\tilde{\\mathbf{y}}_i)\\}$. Here, $\\mathbf{X}^{(i)}$ represents the input text, and $\\tilde{\\mathbf{y}}_i$ denotes the associated labels annotated by workers or experts. Each text instance in the training set is annotated by M workers, resulting in a set of possible labels $\\tilde{\\mathbf{y}}_i := \\{\\mathbf{y}_i^m\\}_{m=1}^M$, where $\\mathbf{y}_i^m \\in [C] := \\{1,2,...,C\\}$. We assume that the correct ground-truth"}, {"title": "3.2 Technical Overview", "content": "Recall our goal is to train an optimal classifier that does not depend on spurious correlations, a naive approach might involve using existing out-of-distribution (OOD) risk loss functions, such as distributionally robust optimization (DRO). However, a significant issue arises from the absence of ground-truth labels in the training set. Training a robust model directly using DRO on the clean validation set could result in limited available data, potentially compromising the overall performance. Considering these, we propose a bi-level formulation to address these challenges. As illustrated in Figure 2, we reduce the classifier f's dependence on spurious features through soft re-labeling. In this example, we identify $x_1$ and $x_2$ as the core and spurious features, respectively, and aim to train a classifier that does not rely on the spurious feature $x_2$. Without re-labeling, even if the training set had the ground truths, the classifier would still be biased towards $x_2$. However, by applying soft re-labeling, we adjust the labels for samples in the bottom-left and top-right areas, resulting in an optimal classifier that is oriented vertically, as shown in Figure 2. This adjustment ensures that the newly trained classifier f does not depend on $x_2$. Motivated by these, we formulate the task of learning soft labels to remove the spurious features as a bi-level optimization problem:\n$\\underset{w}{\\text{minimize}}\\ \\mathcal{R}(\\mathcal{D}_{val}, \\theta^{*}(w))\\ \\text{subject to}\\ \\theta^{*}(w) \\in \\underset{\\theta}{\\text{arg min}}\\ \\mathcal{L}(\\mathcal{D}_{tr}, \\theta; w)$ (1)\nwhere w is the soft-label weight vector which indicates the importance of each annotator. The outer objective function can be any OOD risk loss function (i.e., group DRO loss). In the inner loop, we minimize the empirical risk minimization (ERM) loss (i.e., cross-entropy loss) on training samples with learned soft labels, resulting in a model denoted as $\\theta^{*}(w)$. In the outer loop, we assess the model's dependency on spurious features by evaluating the OOD risk and optimizing the soft-label weights accordingly. By alternating between the inner and outer loops, the soft-label weights progressively adjust, enabling the achievement of satisfactory OOD performance through simple ERM training."}, {"title": "3.3 Technical Details", "content": "We design a bi-level optimization process consisting of an inner-loop optimization and an outer-loop optimization to simultaneously update the learned soft-label weight w and model parameters $\\theta$. We begin by addressing the parameterization of the soft-label weight function w in Eqn. (1). Although we could parameterize w as an m-dimensional vector, it does not account for the relationship between the feature and label as annotated by the worker. Thus, we capture the weight of annotated labels $\\tilde{\\mathbf{y}}$ for the sample $\\mathbf{X}^{(i)}$ through a neural network $\\Theta_w: \\mathbf{X}^{(i)} \\rightarrow \\mathbf{v}^{(i)} \\in \\mathbb{R}^m$. After obtaining the normalized soft-label weights $\\mathbf{v}^{(i)}$ through the softmax function, the final soft-label $\\bar{\\mathbf{y}}_i$ is determined by taking the weighted sum of the one-hot vectors $\\mathbf{e}$ in the potential label set $\\tilde{\\mathbf{y}}_i$, where the weights are explicitly provided by $\\mathbf{v}^{(i)}$. With the soft-labels computed, we can now turn to the outer-level optimization. Motivated by [53], we initiate by pseudo-updating the parameter vector $\\theta$, thereby establishing a relationship between w and the optimized parameters $\\theta'$. Specifically, $\\theta'$ approximate $\\theta^{*}(w)$ through one-step inner loop gradient descent. We then update w to make the induced $\\theta'$ minimize the outer loss $\\mathcal{R}$. Regarding the inner-loop optimization, $\\theta$ is directly updated to minimize $\\mathcal{L}$. We provide the full algorithm in Algorithm 1. Detailed explanations of the optimization process are provided below."}, {"title": "3.4 Theoretical Analysis", "content": "Finally, we can prove the convergence of our bi-level optimization algorithm under moderate assumptions. The convergence analysis follows from a similar idea as the proof in [54]. We first introduce the following necessary assumptions.\nAssumption 3.1 (Smoothness of $\\mathcal{R}$). The OOD risk function $\\mathcal{R}$ is Lipschitz-smooth with a constant L.\nAssumption 3.1 is a common assumption in the analysis of bi-level optimization [54, 55, 23, 56]. Additionally, we assume that the gradients of $\\mathcal{L}$, $\\mathcal{R}$ and their inner product are bounded.\nAssumption 3.2 (Lower bound of the inner product of the gradients). We assume that the following inequality holds with some constant k for every time step t\n$\\nabla_{\\theta}\\mathcal{R}(\\theta_{t+1})^T \\nabla_{\\theta}\\mathcal{L}(\\theta_t; w_{t+1}) \\geq k||\\nabla_{\\theta}\\mathcal{L}(\\theta_t; w_{t+1})||^2$ (5)\nAssumption 3.3 (Bounded gradients of $\\mathcal{L}$ and $\\mathcal{R}$). The gradients of $\\mathcal{L}$ and $\\mathcal{R}$ are bounded by $\\sigma$. $\\nabla_w\\nabla_{\\theta}\\mathcal{L}(\\theta; w)$ is bounded by $\\sigma'$.\nUnder the above assumptions, we further provide Theorem 6 to show the convergence of our bi-level optimization method. The proof of Theorem 3.4 can be found in Appendix A."}, {"title": "Theorem 3.4 (Convergence).", "content": "Under Assumption 3.1 and Assumption 3.2 and setting the step size $\\mu \\leq \\frac{2k}{L}$, our bi-level optimization algorithm can ensure that the risk function $\\mathcal{R}$ monotonically decreases with respect to the time step t, i.e.,\n$\\mathcal{R}(\\theta_{t+1}) \\leq \\mathcal{R}(\\theta_t)$ (6)\nThe equality in Eqn. (6) holds if the gradient of the risk function $\\mathcal{R}$ with respect to w becomes 0 at some time step t, i.e., $\\nabla_w\\mathcal{R}(\\theta_t) = 0$.\nTheorem 3.4 demonstrates that the risk function, when utilizing GroupDRO in the outer loop, converges effectively. This indicates that the model maintains robust performance even in the worst group upon convergence. Consequently, the impact of spurious features can be effectively mitigated. Additionally, we prove the convergence rate of our bi-level optimization method as $O(\\frac{1}{T})$. The details of the proof are in Appendix B."}, {"title": "Theorem 3.5 (Convergence rate).", "content": "Let the total number of training steps as T and set the step size $\\alpha = \\frac{k_1}{\\sqrt{T}}$ for some constant $k_1$ where $0 < k_1 < \\frac{k}{L}$ and $\\mu = \\frac{2k}{L}$. For some constant $k_2$. Under Assumption 3.1 and Assumption 3.3, we have\n$\\frac{1}{T} \\sum_{10, we have to train $O(\\frac{1}{\\epsilon^2})$ steps.\nFurthermore, as the training step increases, the gradient of the risk function with respect to w is gradually close to 0. If the risk function $\\mathcal{R}$ is convex with respect to w, it essentially means that w gradually converges to the optimal $w^{*}$ that minimizes the risk function."}, {"title": "4 Evaluation", "content": "In this section, we start with the experimental setup, including the datasets, baselines, and metrics. We then present the results of our experiments, which evaluate the effectiveness of our proposed method against baseline methods. Finally, we conduct an ablation study to compare the performance of our method with alternative design choices. We release the data and code in https://github.com/chengzelei/crowdsource_toxicity_classification."}, {"title": "4.1 Experiment Setup", "content": "Datasets. We obtain the toxic question and response datasets from a third-party security company. The toxic question dataset is classified into 15 categories based on the OpenAI usage policy retrieved in 2023 as shown in Table 4. The response classification task is a binary classification problem, where the responses are labeled as toxic or non-toxic. Each data point is associated with three human annotations and three LLM-generated annotations (GPT-4, GPT-4 Turbo, and Claude-2). To better reflect the real-world scenario where the source or the number of annotators is limited, we have six datasets: Q-H, Q-L, Q-A, R-H, R-L, and R-A, where Q-H and R-H are annotated by humans, Q-L and R-L are annotated by LLMs, and Q-A and R-A are annotated by all annotators.\nFor each classification task, we have a large training set with crowdsourced annotations (i.e., 6,941 samples for toxic question classification and 28,194 samples for toxic response classification) and a testing set containing 2,000 samples with ground truth. The validation set with ground truth includes a small number of samples (i.e., 1,000 samples) from the training set. Additionally, the company assigned 15 topics utilizing Latent Dirichlet Allocation (LDA) [57]. We further construct the groups based on both topics and true labels. The details of the groups can be found in Appendix C.2.\nIn addition, we conduct our experiments on the public HateXplain dataset [58]. It contains three classes - \"hatespeech\u201d, \u201coffensive\u201d, \u201cnormal\u201d. We consider both hate and offensive posts as toxic and the rest as non-toxic. Each record includes a post and three human annotations. The true labels are determined as the majority vote of three human annotations following [59]. We further utilize GPT-4, GPT-4 Turbo, and Claude 2 to label these comments. We assign 15 topics utilizing LDA and further construct the groups based on both topics and true labels."}, {"title": "4.2 Main Results", "content": "Compare with baseline methods. In Table 1, we show the average accuracy and worst-group accuracy of our method and the baseline methods on the datasets from the third-party security company. As shown in the table, our method outperforms all baseline methods in terms of both average accuracy and worst-group accuracy across two classification tasks. Baseline methods do not consider the out-of-distribution risk and therefore show worse performance regarding worst-group accuracy. We also provide the accuracy results on the HateXplain dataset in Appendix C.4. These results demonstrate the effectiveness of our method in learning from multiple annotators with soft labeling to improve the toxicity classifier's performance and eliminate the out-of-distribution risk with GroupDRO.\nCompare with human and proprietary LLM annotations. We compare the classification performance of our method with human and proprietary LLM labeling in Figure 3. The results show that our method achieves outstanding performance in both question and response classification tasks. The accuracy of our method for question classification is comparable to GPT-4 Turbo, the state-of-the-art LLM, and significantly outperforms any human annotator. For response classification, our method surpasses all annotations, including GPT-4 Turbo, by a large margin. Considering the high cost of GPT-4 Turbo labeling, our method provides a cost-effective and scalable solution for toxicity classification tasks.\nTime complexity comparison with baseline methods. We measure the time complexity of all methods across all datasets and report the results in Appendix C.5. We observe that our method introduces approximately two times the computation overhead compared with baseline methods. The additional computation overhead originates from the pseudo-update of the model parameter $\\theta$ and the update of the soft-label weights w. Note that we utilize a smaller model (i.e., ROBERTa-base) to"}, {"title": "4.3 Ablation Study", "content": "We conduct an ablation study to demonstrate the superiority of our design with alternative designs and compare the performance of our method with fewer annotators.\nLearning with fewer annotators. We assess the performance of our method with fewer annotators and compare it with other methods in Figure 4. The figure first shows that our method still outperforms all baseline methods in two classification tasks in terms of both average accuracy and worst-group accuracy when only human annotations or LLM annotations are available. This demonstrates that our method is robust and effective in learning from fewer annotators, providing a cost-effective solution for toxicity classification tasks.\nWe also observe that the annotation quality of LLMs and humans varies for different tasks. For instance, LLM annotations yield generally better results than human annotations for the question classification task, while the opposite is true for the response classification task. This finding aligns with the result in Figure 3. Thus, baseline methods may be particularly sensitive to the quality of the annotators. Specifically, for the response classification task, the classification performance of baselines is much lower when all annotators are present compared to when only human annotators are present. In contrast, our method not only maintains but improves its accuracy when all annotators are included, underscoring its ability to handle variable annotation quality effectively. Moreover, our approach demonstrates robustness against different data distributions in the testing set, achieving over 70% accuracy in the worst group for the response classification task where no baseline method exceeds 60%."}, {"title": "5 Discussion and Conclusion", "content": "In this work, we introduce a novel bi-level optimization framework that incorporates soft-labeling techniques alongside GroupDRO to tackle the OOD risk of toxicity classification with crowdsourced annotations. By leveraging multi-source annotations, our approach captures a broader spectrum of the annotator's judgment, enhancing the system's ability to handle the inherent ambiguities in defining toxic content. We present a theoretical analysis of convergence and demonstrate its superior performance over toxic question and response datasets. We hope that our work will inspire further research in developing ethically aware and technically robust AI-driven moderation tools.\nOur work suggests several promising directions for future research. First, it would be interesting to investigate the extension of our toxicity classification framework to multi-modal contents, where toxicity may manifest not just in text but through images, videos, and their combinations, presenting unique challenges and requiring novel adaptation strategies. Second, while our model leverages annotations from multiple sources to enhance the accuracy of toxicity classification, it remains dependent on the quality and representativeness of these annotations. Future research could focus on improving the fairness of our model by continuously monitoring for and mitigating inherent biases in annotator perspectives. This would involve regular audits, updates to training data, and adjustments to model parameters to bolster both the effectiveness and fairness of the system. Finally, the versatility of our framework could extend beyond toxicity classification to other large language model safety applications, such as LLM alignment through reinforcement learning from feedback (RLHF). In RLHF, human annotators provide pairwise feedback for LLM responses, which can be noisy. Our bi-level optimization framework could be adapted to assess the quality of this feedback and select the most reliable inputs for fine-tuning LLMs."}, {"title": "A Proof of Theorem 3.4", "content": "First, we provide the following lemma to demonstrate the property of Lipschitz-smoothness.\nLemma 1 ([60]). If function g(x) is Lipschitz-smooth with a constant L, then we have the following inequality:\n$g(x_2) \\leq g(x_1) + \\nabla g(x_1)^T (x_2-x_1) + \\frac{L}{2}||x_2-x_1||^2, \\forall x_1,x_2$ (8)\nProof. Let's define a function h(t) as $h(t) = g(x_1+t(x_2 - x_1))$ where 0 \u2264 t \u2264 1. The first-order derivative of h(t) is\n$h'(t) = \\nabla g(x_1 + t(x_2 - x_1))^T(x_2 - x_1)$ (9)\nIf g(x) is Lipschitz-smooth with constant L, we have\n$h'(t) \u2013 h'(0)$\n$=(\\nabla g(x_1+t(x_2 \u2212 x_1)) \u2013 \\nabla g(x_1))^T(x_2 - x_1)$\n$=(\\nabla g(x_1+t(x_2-x_1)) - \\nabla g(x_1)) (tx_1 + tx_2)$\n$=\\frac{1}{t}(\\nabla g(x_1+t(x_2 - x_1)) - \\nabla g(x_1))^T (t(x_1 + x_2 - x_1))$ (10)\n$\\leq \\frac{L}{t}||\\nabla g(x_1+t(x_2-x_1)) - \\nabla g(x_1) ||^2$\n$=\\frac{L}{t}||t(x_2-x_1)||^2$\n$=tL||x_2 - x_1||^2$\nNote that $g(x_2) = h(1) = h(0) + \\int_{0}^{1} h'(t)dt$ and g(x1) = h(0). Given that h'(t) \u2264 h'(0)+tL||x2-x1||2, we further have\n$g(x_2) = h(1) = h(0) + \\int_{0}^{1} h'(t)dt$\n$\\leq h(0) + \\int_{0}^{1} [h'(0) +tL||x_2 - x_1||^2]dt$\n$= h(0) + h'(0)+\\frac{L}{2}||x_2 - x_1||^2$ (11)\n$= h(0) + h'(0) + \\frac{L}{2}||x_2 - x_1||^2$\n$= g(x_1) + \\nabla g(x_1)^T(x_2\u2212x_1) + \\frac{L}{2}||x_2 - x_1||^2$\nWe can now prove the convergence in Theorem 3.4.\nProof. Given the assumption that $\\mathcal{R}$ is Lipschitz-smooth with a constant L, following Lemma 1, we have\n$\\mathcal{R} (\\theta_{t+1}) \u2013 \\mathcal{R} (\\theta_{t}) \\leq \\nabla_{\\theta}\\mathcal{R} (\\theta_{t})^T (\\theta_{t+1} \u2013 \\theta_{t}) + \\frac{L}{2} (\\theta_{t+1} \u2013 \\theta_{t})||^2$ (12)\nRecall that the update rule in Eqn. (4) tells us $\\theta_{t+1}-\\theta_t = -\\mu\\nabla_{\\theta}\\mathcal{L}(\\theta_t; w_{t+1})$. Inserting in Eqn. (12), we have\n$\\mathcal{R} (\\theta_{t+1}) \u2013 \\mathcal{R} (\\theta_{t}) \\leq -\\mu\\nabla_{\\theta}\\mathcal{R} (\\theta_{t+1})^T \\nabla_{\\theta}\\mathcal{L} (\\theta_{t}; w_{t+1}) + \\frac{L\\mu^2}{2} ||\\nabla_{\\theta}\\mathcal{L} (\\theta_{t}; w_{t+1})||^2$ (13)"}, {"title": "Under Assumption 3.2, we further have", "content": "$\\mathcal{R} (\\theta_{t+1}) \u2013 \\mathcal{R} (\\theta_{t}) \\leq -\\mu k || \\nabla_{\\theta}\\mathcal{L} (\\theta_{t}; w_{t+1})||^2 \u2013 \\frac{L\\mu^2}{2} || \\nabla_{\\theta}\\mathcal{L} (\\theta_{t}; w_{t+1})||^2$ (14)\nIf we set the step size $\\mu \\leq \\frac{2k}{L}$, we can ensure that $\\mathcal{R} (\\theta_{t+1}) \u2013 \\mathcal{R} (\\theta_{t}) \\leq 0$.\nAdditionally, if $\\nabla_w\\mathcal{R}(\\theta_t) = 0$, it implies that the algorithm converges and $\\mathcal{R}(\\theta_{t+1}) = \\mathcal{R}(\\theta_t)$."}, {"title": "B Proof of Theorem 3.5", "content": "Proof. Based on the update rule of $\\theta$ in Eqn. (4), we have\n$\\mathcal{R} (\\theta_{t+1}) \u2013 \\mathcal{R} (\\theta_{t})$\n$=\\mathcal{R} (\\theta_{t} \u2212 \\mu\\nabla_{\\theta}\\mathcal{L} (\\theta_{t}; w_{t+1})) \u2013 \\mathcal{R} (\\theta_{t\u22121} \u2212 \\mu\\nabla_{\\theta}\\mathcal{L} (\\theta_{t\u22121}; w_{t}))$\n$= [\\mathcal{R} (\\theta_{t} \u2212 \\mu\\nabla_{\\theta}\\mathcal{L} (\\theta_{t}; w_{t+1})) \u2013 \\mathcal{R} (\\theta_{t\u22121} \u2212 \\mu\\nabla_{\\theta}\\mathcal{L} (\\theta_{t\u22121}; w_{t+1})]$\n$+ [\\mathcal{R} (\\theta_{\u22121} \u2013 \\mu\\nabla_{\\theta}\\mathcal{L} (\\theta_{t\u22121}; w_{t+1})) \u2013 \\mathcal{R} (\\theta_{t\u22121} \u2212 \\mu\\nabla_{\\theta}\\mathcal{L} (\\theta_{t\u22121}; w_{t}))]$ (15)\nLet's define a function $F(\\theta, w) = \\theta \u2013 \\mu\\nabla_{\\theta}\\mathcal{L}(\\theta; w)$. The above equation can be transformed as\n$\\mathcal{R}(\\theta_{t+1}) \u2013 \\mathcal{R} (\\theta_{t})$\n$= [\\mathcal{R} (F (\\theta_{t}, w_{t+1})) \u2013 \\mathcal{R} (F (\\theta_{t\u22121}, w_{t+1}))] + [\\mathcal{R} (F (\\theta_{t\u22121}, w_{t+1})) \u2013 \\mathcal{R} (F (\\theta_{t\u22121}, w_{t}))]$ (16)\nFor the first term, note that $\\mathcal{R}$ is Lipschitz-smooth with a constant L under Assumption 3.1. By Lemma 1, we have\n$\\mathcal{R}(F(\\theta_{t}, w_{t+1})) \u2013 \\mathcal{R} (F (\\theta_{t\u22121}, w_{t+1}))$\n$< \\nabla_{F}\\mathcal{R}(F (\\theta_{t\u22121}, w_{t+1})) (F(\\theta_{t}, w_{t+1}) \u2013 F (\\theta_{t\u22121}, w_{t+1}))$\n$+ \\frac{L}{2}|| (F(\\theta_{t}, w_{t+1}) - F (\\theta_{t\u22121}, w_{t+1})) ||^2$ (17)\nWe observe that\n$||F (\\theta_{t}, w_{t+1}) \u2013 F (\\theta_{t-1}, w_{t+1}) ||$\n$=|| [\\theta_{t} \u2212 \\mu\\nabla_{\\theta}\\mathcal{L} (\\theta_{t}, w_{t+1})] - [\\theta_{t\u22121} \u2212 \\mu\\nabla_{\\theta}\\mathcal{L} (\\theta_{t-1}, w_{t+1})] ||$\n$=|| [\\theta_{t} - \\theta_{t-1}] \u2013 \\mu [\\nabla_{\\theta}\\mathcal{L} (\\theta_{t}, w_{t+1}) \u2013 \\nabla_{\\theta}\\mathcal{L} (\\theta_{t-1}, w_{t+1})] ||$\n$=||\\mu\\nabla_{\\theta}\\mathcal{L} (\\theta_{t}, w_{t+1}) + \\nabla_{\\theta}\\mathcal{L} (\\theta_{t\u22121}, w_{t}) \u2013 \\nabla_{\\theta}\\mathcal{L} (\\theta_{t-1}, w_{t+1}) ||$ (18)\nUnder Assumption 3.3, the gradient of $\\mathcal{L}$ is bounded by $\\sigma$, by the triangle inequality, we have\n$||F (\\theta_{t}, w_{t+1}) \u2013 F (\\theta_{t-1}, w_{t+1}) || \\leq 3\\mu\\sigma$ (19)\nUnder Assumption 3.3, the gradient of $\\mathcal{R}$ is also bounded by $\\sigma$. Combining with Eqn. (19), we can derive the upper bound of $\\mathcal{R} (F (\\theta_{t}, w_{t+1})) \u2013 \\mathcal{R} (F (\\theta_{t\u22121}, w_{t+1}))::$\n$\\mathcal{R} (F (\\theta_{t}, w_{t+1})) \u2013 \\mathcal{R} (F (\\theta_{t\u22121}, w_{t+1})) \\leq 3\\mu\\sigma^2 + \\frac{9}{2} L\\mu^2\\sigma^2$ (20)\nFor the second term, under Assumption 3.1, $\\mathcal{R}$ is Lipschitz smooth with a constant L. By Lemma 1, we have\n$\\mathcal{R} (F(\\theta_{t-1}, w_{t+1})) \u2013 \\mathcal{R} (F (\\theta_{t-1}, w_{t}))$\n$\\leq \\nabla_{w}\\mathcal{R}(F(\\theta_{t-1}, w_{t})) (w_{t+1} - w_{t}) + \\frac{L}{2} ||w_{t+1} - w_{t}||^2$ (21)\nRecall that the update rule of w is $w_{t+1} = w_{t} - \\alpha\\nabla_{w}\\mathcal{R}(F(\\theta_{t}, w_{t}))$. Thus, we have\n$\\mathcal{R} (F (\\theta_{t-1}, w_{t+1})) \u2013 \\mathcal{R} (F (\\theta_{t-1}, w_{t}))$\n$\\leq \u2212 \\alpha\\nabla_{w}\\mathcal{R} (F (\\theta_{t\u22121}, w_{t})) \\nabla_{w}\\mathcal{R} (F(\\theta_{t}, w_{t})) + \\frac{L\\alpha^2}{2} ||\\nabla_{w}\\mathcal{R} (F(\\theta_{t}, w_{t})) ||^2$\n$= (\\frac{L\\alpha^2}{2} - \\alpha) ||\\nabla_{w}\\mathcal{R} (F(\\theta_{t}, w_{t})) ||^2$\n$+ \\alpha(\\nabla_{w}\\mathcal{R} (F(\\theta_{t}, w_{t})) \u2013 \\nabla_{w}\\mathcal{R} (F (\\theta_{t\u22121}, w_{t}))) \\nabla_{w}\\mathcal{R} (F (\\theta_{t}, w_{t}))$ (22)"}, {"title": "Under Assumption 3.3,", "content": "$\\nabla_w\\nabla_{\\theta"}, "mathcal{L}(\\theta, w)$ is bounded by $\\sigma'$ and $\\mathcal{L}$ has $\\sigma$-bounded gradients. Then we can derive the upper bound of $\\nabla_{w}\\mathcal{R}(F(\\theta, w))$ based on the chain's rule:\n$||\\nabla_{w}\\mathcal{R}(F(\\theta,w))|| = ||\\nabla_{w}F(\\theta,w)^T\\nabla_{F}\\mathcal{R}(F(\\theta,w))||$\n$= ||\\mu\\nabla_{w}\\nabla_{\\theta}\\mathcal{L}(\\theta,w)^T\\nabla_{F}\\mathcal{R}(F(\\theta,w))||$\n$\\leq \\mu\\sigma\\sigma'$ (23)\nTherefore, we further have\n$\\mathcal{R}(F(\\theta_{t-1}, w_{t+1})) \u2013 \\mathcal{R} (F (\\theta_{t\u22121}, w_{t}))$\n$< (\\frac{L\\alpha^2}{2} - \\alpha) ||\\nabla_{w}\\mathcal{R} (F(\\theta_{t}, w_{t})) ||^2$\n$+ \\alpha(\\nabla_{w}\\mathcal{R} (F(\\theta_{t}, w_{t})) \u2013 \\nabla_{w}\\mathcal{R} (F (\\theta_{t\u22121}, w_{t})))^T\\nabla_{w}\\mathcal{R} (F(\\theta_{t}, w_{t}))$\n$\\leq (\\frac{L\\alpha^2}{2} - \\alpha) ||\\nabla_{w}\\mathcal{R} (F(\\theta_{t}, w_{t})|| + \\frac{9}{2} (24)\nCombining Eqn. (20) and Eqn. (24), we can derive that\n$\\mathcal{R} (\\theta_{t+1}) \u2013 \\mathcal"]}