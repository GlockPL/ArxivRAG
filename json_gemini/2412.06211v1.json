{"title": "MSCrackMamba: Leveraging Vision Mamba for Crack Detection in Fused Multispectral Imagery", "authors": ["Qinfeng Zhu", "Yuan Fang", "Lei Fan"], "abstract": "Crack detection is a critical task in structural health monitoring, aimed at assessing the structural integrity of bridges, buildings, and roads to prevent potential failures. Vision-based crack detection has become the mainstream approach due to its ease of implementation and effectiveness. Fusing infrared (IR) channels with red, green and blue (RGB) channels can enhance feature representation and thus improve crack detection. However, IR and RGB channels often differ in resolution. To align them, higher-resolution RGB images typically need to be downsampled to match the IR image resolution, which leads to the loss of fine details. Moreover, crack detection performance is restricted by the limited receptive fields and high computational complexity of traditional image segmentation networks. Inspired by the recently proposed Mamba neural architecture, this study introduces a two-stage paradigm called MSCrackMamba, which leverages Vision Mamba along with a super-resolution network to address these challenges. Specifically, to align IR and RGB channels, we first apply super-resolution to IR channels to match the resolution of RGB channels for data fusion. Vision Mamba is then adopted as the backbone network, while UperNet is employed as the decoder for crack detection. Our approach is validated on the large-scale Crack Detection dataset Crack900, demonstrating an improvement of 3.55% in mIoU compared to the best-performing baseline methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Bridges, roads, and buildings are critical infrastructures that require regular structural health monitoring, and crack detection plays a vital role in ensuring their safety [1]. Over time, factors such as weather, seismic activity, and general wear can degrade the integrity of these structures, leading to crack formation and posing significant safety risks. Early and accurate detection of these cracks allows for timely maintenance, preventing further damage, reducing repair costs, and ensuring the safety of the infrastructure. Traditional crack detection methods typically rely on manual inspections, which are time-consuming and inefficient. With the rapid advancements in deep learning technologies, computer vision has found widespread applications across various industries. Consequently, automated crack detection methods based on computer vision have attracted substantial attention from researchers.\nBy analyzing data collected from sensors, deep learning-based methods can provide a more efficient and accurate assessment of the structural condition [2].\nCurrently, adopting deep learning networks to analyze structural information collected by visual sensors has become the most mainstream approach for crack detection due to its efficiency and ease of implementation [2]. By processing data collected from imaging devices such as cameras, deep learning methods can effectively detect and assess cracks. To enhance the crack detection capabilities of deep learning models, the fusion of infrared (IR) channels and red, green and blue (RGB) channels has demonstrated significant potential in recent years [3]. Studies have shown that the thermal information provided by IR channels can effectively improve the accuracy of crack detection [4].\nSemantic segmentation is an essential approach for addressing crack detection problems. It assigns a unique label to each pixel in an image, enabling pixel-level understanding and facilitating the identification and localization of important features such as cracks within the image [5]. Therefore, semantic segmentation allows for precise localization of cracks and provides clear delineation of their patterns.\nSince the introduction of convolutional neural networks (CNNs), semantic segmentation has witnessed significant advancements. Networks such as U-Net [6], DeepLab [7], and fully convolutional networks (FCNs) [8] have achieved impressive accuracy across various image segmentation tasks. These networks are designed to effectively integrate both shallow and deep features of an image, enabling them to capture fine-grained details as well as high-level semantic information. In recent years, the introduction of vision transformer (ViT) [9] has further improved segmentation performance [10]. ViT transforms the image into a sequence of patches, applying multi-head attention to each patch, which endows the network with a global receptive field, leading to enhanced contextual understanding of the image.\nDespite the significant advancements in semantic segmentation networks, crack detection tasks still face numerous challenges. CNN-based methods often struggle in dealing with cracks of varying scales. This is primarily due to the limited receptive field of CNNs [11], which makes it challenging for the network to capture both detailed and global information in images with complex morphology and significant scale variation [12]. Although ViT can enhance the network's global perception capabilities, its quadratic complexity in calculating attention between patches leads to a significant increase in computational resource requirements when processing high-resolution images, resulting in limited efficiency during training and inference [9].\nIn addition, in the context of crack detection using fused multispectral images, a common challenge is the resolution difference between RGB and IR images. Due to the characteristics of the sensors, IR images typically have lower resolution compared to RGB images [4]. To enable semantic segmentation networks to properly process multispectral data, a common approach is to downsample RGB images to match the resolution of IR images, allowing for the fusion of both image modals into a multi-channel input, as shown in Fig. 1. However, this approach results in a loss of detail in RGB images, which negatively affects the accuracy of crack detection.\nTo address these challenges, this paper proposes MSCrackMamba, a two-stage crack detection architecture that combines a super-resolution method with the recently introduced Mamba architecture [13]. In the first stage, a self-supervised super-resolution method is used to upsample low-resolution IR images, aligning the resolution of IR and RGB images. The RGB and IR images are then concatenated to form a six-channel input. In the second stage, the recently proposed Vision Mamba [14] is selected as the backbone network, with UperNet [15] serving as the decoder, to train on the six-channel multispectral data. Vision Mamba provides a global receptive field with linear complexity, making it well-suited for crack detection from multispectral images [16]. The main contributions of this work are as follows:\n1.  We propose a two-stage MSCrackMamba, a novel paradigm designed for crack detection tasks using RGB and IR images.\n2.  To the best of our knowledge, this is the first application of Vision Mamba to crack detection in fused multispectral images.\nThe remainder of this paper is organized as follows: Section II provides a detailed description of MSCrackMamba; Section III presents our experiments and results; Section IV concludes the paper and suggests future directions."}, {"title": "II. METHODOLOGY", "content": "MSCrackMamba is a two-stage framework designed for crack detection, with its overall architecture illustrated in Fig. 2. The primary objective of the first stage is to align the resolution of RGB and IR channels without losing the fine details of RGB images. The second stage aims to use Vision Mamba for semantic segmentation of the multi-channel multispectral data.\nAs shown in Stage 1 of Fig. 2, we propose a super-resolution approach to super-resolve IR channels, aligning their resolution with that of the RGB channels. To achieve this, we select the state-of-the-art (SOTA) super-resolution network, Fusion-Net [17]. This network is based on detail injection and utilizes a deep convolutional neural network to improve the quality of the fusion by using the difference between the ground truth and the images to be upsampled as input, thus preserving details effectively.\nWe adopt a self-supervised training paradigm to super-resolve IR images [18]. We define the higher-resolution RGB data collected by the sensor as $P_{RGB}$ and the lower-resolution three-channel IR data as $P_{IR}$. Specifically, we first downsample $P_{IR}$ to obtain the downsampled $P'_{IR}$. We then apply a super-resolution network to super-resolve $P'_{IR}$, using $P_{IR}$ as the ground truth, thereby obtaining a trained super-resolution model M. Subsequently, we use M to further super-resolve $P_{IR}$, resulting in $P'_{IR}$ with resolution $R_{P'_{IR}}$ matching the resolution $R_{P_{RGB}}$ OF $P_{RGB}$.\nAfter obtaining $P'_{IR}$ with the resolution aligned with $P_{RGB}$, we concatenate the three-channel $P_{RGB}$ with the three-channel $P'_{IR}$ to form the six-channel $P_{Fuse}$, which serves as the input for the subsequent semantic segmentation network.\nMamba is initially designed for large language models [13], capable of performing full-context understanding with linear complexity. It is built upon a State Space Model (SSM), which offers a robust framework for capturing dependencies in sequential data. Unlike traditional recurrent networks, which sequentially update hidden states and are prone to forgetting earlier information, SSMs maintain a continuous latent state evolution, allowing predictions to integrate information from the entire sequence. The general form of SSMs can be expressed as:\n$h'(t) = Ah(t) + Bx(t)$ (1)\n$y(t) = Ch(t) + Dx(t)$ (2)\nwhere h(t) represents the latent state, x(t) the input, and A, B, C, D define the dynamics. To adapt SSMs for discrete input, Mamba leverages the Structured State Space for Sequences (S4), employing zero-order hold discretization:\n$h_k = \\overline{A}h_{k-1} + \\overline{B}x_k$ (3)\n$y_k = \\overline{C}h_k + \\overline{D}x_k$ (4)\nwhere $\\overline{A} = e^{\\Delta t A}$, $\\overline{B} = (\\overline{A} \u2013 I)A^{-1}B$, with $\\Delta t$ as the sampling interval. This discretization enables Mamba to process sequential data efficiently. Additionally, Mamba incorporates a gating mechanism that adaptively controls the propagation or suppression of specific inputs, enabling the model to focus on salient features while minimizing computational overhead.\nBuilding on the concept of ViT, which serializes images into patches, Mamba has quickly been adapted for use in the visual domain [14, 19]. Its structured state space enables comparative efficient multi-directional scanning, allowing it to learn the positional relationships of patches without relying on computationally expensive multi-head attention required by ViT. This design has demonstrated strong potential in various applications, including remote sensing [12, 20]."}, {"title": "III. EXPERIMENTS", "content": "We selected a recent large-scale multimodal crack detection dataset, Crack900 [4], to validate the MSCrackMamba architecture. The dataset contains 914 finely annotated RGB and IR images of masonry structure cracks, which were captured using a FLIR E85 IR camera at the ancient city walls in Suzhou, China. The IR sensor resolution is 384\u00d7288, while the RGB sensor resolution is 1280\u00d7960. The dataset was randomly divided into a training set (80%) and a test set (20%), resulting in 731 training images and 183 validation images.\nWe used the $mIoU$ (mean Intersection over Union) metric to evaluate the segmentation accuracy, which is calculated using the following formula:\n$mIoU = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{TP_i}{TP_i + FP_i + FN_i}$ (5)\nwhere N represents the total number of classes, which is generally 2 in the crack detection dataset, including crack and background classes. $TP_i$ represents the number of true positive pixels for class i, $FP_i$ represents the number of false positive pixels for class i, and $FN_i$ represents the number of false negative pixels for class i. A higher mIoU score indicates better overall segmentation performance [21].\nIn order to ensure a fair comparison, our network parameter settings were kept as consistent as possible with those used in the benchmark networks of the article introducing Crack900, as shown in TABLE I. The only difference is that we increased the patch size from 256\u00d7256 to 512\u00d7512. This adjustment was made because we super-resolved the IR channels to match the resolution of the RGB channels (1280\u00d7960), allowing the network to crop at a larger scale. Data augmentation techniques were employed to enhance the generalization ability of the network [22], including Random Flip, Random Rotate, and Random Crop. All experiments were conducted using two 4090D GPUs (24G), with a batch size of 8 per GPU.\nWe adopted the pretraining-finetuning approach in our experiments to ensure optimal segmentation performance. This is a common training strategy in semantic segmentation tasks. It involves first training the encoder-decoder structure on a large-scale image dataset and then fine-tuning it on the downstream segmentation task. The pretrained encoder can more effectively extract features from images, thereby improving performance in downstream tasks.\nAll experiments were retrained to eliminate the influence of training equipment and parameters, ensuring a fair comparison. TABLE II summarizes the experimental results. The mIoU score achieved using the MSCrackMamba architecture was 76.96%, which represents a significant improvement of 3.55% compared to the best-performing combination of ConvNeXt-t and UperNet. This demonstrates the effectiveness of the introduced MSCrackMamba architecture.\nExamples visualized segmentation results are shown in Fig. 4, where we compared MSCrackMamba with the previously best-performing combination of ConvNeXt and UperNet. It can be observed that the earlier methods exhibited noticeable false positive errors (as shown in Fig. 4(b), (c), and (d)) and false negative errors (as indicated in Fig. 4(a) and (e)). In contrast, the MSCrackMamba architecture demonstrated a significantly better capacity for capturing the shape of the cracks, although occasional minor false positive errors are still present (e.g., Fig. 4(e)).\nIn our ablation experiments, we aimed to verify the effectiveness of the two-stage strategy and/or pretraining. In these tests, we used VisionMamba as the backbone and UperNet as the decoder. The results, as shown in TABLE III, indicate that both the two-stage strategy and pretraining contribute to improving segmentation performance. When they were combined, an improvement of 4.42% in mIoU was observed compared to the baseline without either. Due to the strong performance of the Mamba-based backbone, even without the two-stage training strategy or pretraining, the segmentation results still outperformed previous works."}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "This paper introduces MSCrackMamba, a two-stage framework specifically designed for crack detection in multispectral images. The first stage ensures resolution alignment between multispectral channels while preserving the fine details of RGB channels through super-resolution of IR channels. The second stage involves the implementation of VisionMamba, leading to linear complexity and more effective capture of global contextual relationships in multispectral images. Experiments have been carried out to quantitatively investigate the contribution of each stage for enhancing crack detection. By integrating super-resolution techniques with the VisionMamba network, MSCrackMamba achieved significant performance improvements on the large-scale multispectral Crack900 dataset, outperforming the best baseline (from typical CNN and ViT-based networks) by 3.55%.\nThe directions for future research include:\n1.  More accurate super-resolution methods: While the current two-stage approach achieves multichannel resolution alignment, the self-supervised super-resolution process may introduce distortion, leading to misalignment of features and thus affecting segmentation accuracy. Future work can focus on enhancing the accuracy of super-resolution to mitigate such issues.\n2.  Lightweight VisionMamba backbone: Although Mamba has linear computational complexity, the VisionMamba backbone still employs four directional scans, which is computationally intensive. Studies [20] suggest that reducing the number of scan directions has minimal impact on performance. Future research could explore lightweight design strategies [28], such as alternating scan directions akin to Vision LSTM [29, 30], to make the architecture more efficient.\n3.  End-to-end training: The current two-stage approach is operationally time-consuming. Future work could investigate end-to-end networks that allow resolution alignment and semantic segmentation to be trained simultaneously, enhancing efficiency.\n4.  Enhanced multispectral channel fusion: In this study, we utilized six-channel data, limiting the spectral resolution. However, in future applications, higher spectral resolutions, such as hyperspectral data, may be encountered. In such cases, the VisionMamba backbone may not be suitable. Therefore, future research should focus on optimizing the fusion of multispectral channels [31, 32] to enable the network to adaptively adjust weights according to the features of each channel, improving the model's adaptability to complex environments."}]}