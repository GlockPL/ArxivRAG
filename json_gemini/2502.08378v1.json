{"title": "Learning Humanoid Standing-up Control across Diverse Postures", "authors": ["Tao Huang", "Junli Ren", "Huayi Wang", "Zirui Wang", "Qingwei Ben", "Muning Wen", "Xiao Chen", "Jianan Li", "Jiangmiao Pang"], "abstract": "Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems, such as fall recovery. Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes. To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures. HoST effectively learns posture-adaptive motions by leveraging a multi-critic architecture and curriculum-based training on diverse simulated terrains. To ensure successful real-world deployment, we constrain the motion with smoothness regularization and implicit motion speed bound to alleviate oscillatory and violent motions on physical hardware, respectively. After simulation-based training, the learned control policies are directly deployed on the Unitree G1 humanoid robot. Our experimental results demonstrate that the controllers achieve smooth, stable, and robust standing-up motions across a wide range of laboratory and outdoor environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Can humanoid robots stand up from a sofa, walk to a table, and pick up coffee, seamlessly like humans? Fortunately, recent advancements in humanoid robot hardware and control have enabled significant progress in bipedal locomotion [38, 26, 28, 54] and bimanual manipulation [5, 24, 9, 16], allowing robots to navigate environment and interact with objects effectively. However, the fundamental capability-standing-up control [43, 17]-remains underexplored. Most existing systems assume the robots start from a pre-standing posture, limiting their applicability to many scenes, such as transition-ing from a seated position or recovering after a loss of balance. We envision that unlocking this standing-up capability would broaden the real-world applications of humanoid robots. To this end, we investigate how humanoid robots can learn to stand up across diverse postures in real environments.\nA classical approach for this control task involves tracking handcrafted motion trajectories through model-based motion planning or trajectory optimization [17, 18, 22, 43]. Although effective in generating motions, these methods require extensive tuning of analytical models and often perform subopti-mally in real-world settings with external disturbances [29, 23] or inaccurate actuator modeling [15]. Besides, real-time optimization on the robot makes these methods computa-tionally intensive, prompting workarounds such as reduced optimization precision or offload computations to external machines [34, 8], though both are with practical limitations.\nReinforcement learning (RL) offers an alternative effective framework for humanoid locomotion and whole-body con-trol [36, 13, 4, 53], benefiting from minimal modeling assump-tions. However, compared to these tasks that partially decou-ple upper- and lower-body dynamics, RL-based standing-up control involves a highly dynamic and synergistic maneuver on both halves of the body. This complex maneuver features time-varying contact points [17], multi-stage motor skills [29], and precise angular momentum control [11], making RL ex-ploration challenging. Although predefined motion trajectories can guide RL exploration, they are typically limited to ground-specific postures [35, 36, 51, 12], leaving the scalability to other postures unclear. Conversely, training RL agents from scratch with wide explorative strategies on the ground can lead to violent and abrupt motions that hinder real-world deployment [46], particularly for robots with many actuators and wide joint limits. In summary, learning posture-adaptive, real-world deployable standing-up control with RL remains an open problem (see Table I).\nIn this work, we address this problem by proposing HoST, an RL-based framework that learns humanoid standing-up con-trol across diverse postures from scratch. To enable posture-adaptive motion beyond the ground, we introduce multiple terrains for training and a vertical pull force during the initial stages to facilitate exploration. Given the multiple stages of the task, we adopt multi-critic RL [33] to optimize distinct reward groups independently for a better reward balance. To ensure real-world deployment, we apply smoothness regularization and motion speed constraints to mitigate oscillatory and vio-lent motions. Our control policies, trained in simulation [31] with domain randomization [48], can be directly deployed on the Unitree G1 humanoid robot. The resulting motions, tested in both laboratory and outdoor environments, demonstrate high smoothness, stability, and robustness to external disturbances, including forces, stumbling blocks, and heavy payloads.\nWe overview the real-world performance of our controllers in Fig. 1 and summarize our core contributions as follows:"}, {"title": "II. RELATED WORK", "content": "Classical approaches to standing-up control rely on tracking handcrafted motion trajectories through model-based opti-mization [17, 18, 22, 43]. While effective, these methods are computationally intensive, sensitive to disturbances [29, 23], and require precise actuator modeling [15], limiting their real-world applicability. In contrast, RL-based methods learn control policies with minimal modeling assumptions, either by leveraging predefined motion trajectories to guide explo-ration [35, 36, 51, 12] or employing exploratory strategies to learn from scratch [46]. However, none of these methods have demonstrated real-world standing-up motion across diverse postures. Our proposed RL framework addresses these limita-tions by achieving posture adaptivity and real-world deploy-ability without predefined motions, enabling smooth, stable, and robust standing-up across a wide range of laboratory and outdoor environments."}, {"title": "III. PROBLEM FORMULATION", "content": "We formulate the problem of humanoid standing up as a Markov decision process (MDP; [37]) with finite horizon, which is defined by the tuple M = (S,A,T,R, \u03b3). At each timestep t, the agent (i.e., the robot) perceives the state $s_t \\in S$ from the environment and executes an action $a_t \\in A$ produced by its policy $\\pi_{\\theta}(\\cdot|s_t)$. The agent then observes a successor state $s_{t+1} \\sim T(\\cdot|s_t, a_t)$ following the environment transition function T and receives a reward signal $r_t \\in R$. To solve the MDP, we employ reinforcement learning (RL; [45]), whose goal learn an optimal policy $\\pi_{\\theta}$ that maximizes the expected cumulative reward (return) $E_{\\pi_{\\theta}} [\\Sigma_{t=0}^T \\gamma^t r_t]$ the agent receives during the whole T-length episode, where $\\gamma \\in [0,1]$ is the discount factor. The expected return is estimated by a value function (critic) V. In this paper, we adopt Proximal Policy Optimization (PPO; [42]) as our RL algorithm because of its stability and efficiency in large-scale parallel training.\nWe hypothesize that the proprioceptive states of robots provide sufficient information for standing-up control in our target environments. We thus include the proprioceptive information read from robot's Inertial Mea-surement Unit (IMU) and joint encoders into the state $s_t = [w_t, r_t, p_t, \\dot{p_t}, a_{t-1}, \\beta]$, where $w_t$ is the angular velocity of robot base, $r_t$ and $p_t$ are the roll and pitch, $\\dot{p_t}$ and $\\dot{p_t}$ are positions and velocities of the joints, $a_{t-1}$ is the last action, and $\\beta \\in (0,1]$ is a scalar that scale the output action. Given the contact-rich nature of the standing-up task, we implicitly enhance contact detection by feeding the policy with the previous five states [15].\nWe employ a PD controller for torque-based robot actuation. The action $a_t$ represents the difference between the current and next-step joint positions, with the PD target computed as $p_q = p_t + \\beta a_t$, where each dimension of $a_t$ is constrained to [-1,1]. The action rescalar B restricts the action bounds to regulate the motion speed implicitly. This is essential to constrain the standing-up motion and will be discussed in later sections. The torque at timestep $t$ is computed as:\n$\\tau_t = K_p \\cdot (p_t - P_t) - K_d \\cdot \\dot{p_t},$"}, {"title": "IV. METHOD", "content": "This section introduces HOST (Humanoid Standing-up Con-trol), a reinforcement learning (RL)-based framework for learning humanoid robots to stand up across diverse postures, as summarized in Fig. 2. This control task is highly dynamic, multi-stage, and contact-rich, posing challenges for conven-tional RL approaches. We first outline the key challenges addressed in this work in Section IV-A, then describe the core components of the framework in the following sections."}, {"title": "A. Key Challenges & Overview", "content": "The standing-up task involves multiple motor skills: righting the body, kneeling, and rising. Learning a control policy for these stages is challenging without explicit stage separation [25, 19]. We address this by dividing the task into three stages and activating corresponding reward functions at each stage. The complexity of these skills requires multiple reward functions, which can complicate policy optimization. To mitigate this, we employ multi-critic RL [33], grouping reward functions to balance objectives effectively.\nDespite multi-critic RL, exploration remains difficult due to the robot's high degrees of freedom and wide joint limits. Drawing inspiration from human infant skill development [6], we facilitate explo-ration by applying a curriculum-based vertical pulling force.\nWith only reward functions, the agent tends to learn violent and jerky motions, driven by high torque limits and numerous actuators. Such behaviors are impractical for real-world deployment. To ad-dress this, we introduce an action rescaler $\\beta$ to gradually tighten action output bounds, implicitly limiting joint torques and motion speed. Additionally, we incorporate smoothness regularization [20] to mitigate motion oscillation."}, {"title": "B. Reward Functions & Multiple Critics", "content": "Considering the multi-stage nature of the task, we divide the task into three stages: righting the body $h_{base} < H_{stage1}$, rising the body $h_{base} > H_{stage2}$, and standing $h_{base} > H_{stage2}$, indicated by the height of the robot base $h_{base}$. Corresponding reward functions are activated at each stage. We then classify reward functions into four groups: (1) task reward $r^{task}$ that specifies the high-level task objectives, (2) style reward $r^{style}$ that shapes the style of standing-up motion, (3) regularization reward $r^{regu}$ that further regularizes the motionw, and (4) post-task reward $r^{post}$ that specify the desired behaviors after successful standing up, i.e., stay standing. The overall reward function is expressed as follows:\n$r_t = w^{task} \\cdot r^{task} + w^{style} \\cdot r^{style} + w^{regu} \\cdot r^{regu} + w^{post} \\cdot r^{post},$\nwhere $w$ with superscript represents the corresponding reward weight. Each reward group contains multiple reward functions. A comprehensive list of all reward functions and groups is provided in Table VI.\nHowever, we observe that using a single value function (critic) presents significant challenges in learning effective standing-up motions. Besides, the large number of reward functions makes hyperparameter tuning computationally inten-sive and difficult to balance. To address these challenges, we implement multiple critics (MuC; [33, 50, 52]) to estimate returns for each reward group independently, where each reward group is regarded as a separate task with its own assigned critic $V_{\\phi i}$. These multiple critics are then integrated into the PPO framework for optimization as follows:\n$\\mathcal{L}(\\phi i) = E[||r_i + \\gamma V_{\\phi i} (s_t) - V_{\\phi i} (s_{t+1})||^2],$ where $r_i$ is the total reward and $V_{\\phi i}$ is the target value function of reward group i. Each critic independently computes its advantage function $A_{\\phi i}$, estimated through GAE [41]. These individual advantages are then aggregated into an overall weighted advantage: $A = \\Sigma w_A^2 \\frac{\\mu_{A_{\\phi i}}}{\\sigma_{A_{\\phi i}}} A_{\\phi i}$, where $\\mu_{A_{\\phi i}}$ and $\\sigma_{A_{\\phi i}}$ are the batch mean and standard deviation of each advantage. The critics are updated simultaneously with the policy network $\\pi_{\\theta}$ according to:\n$\\mathcal{L}(\\theta) = E [min (a_t (\\theta) A_t, clip(a_t(\\theta), 1 \u2013 \\epsilon, 1 + \\epsilon)A_t)],$"}, {"title": "C. Force Curriculum as Exploration Strategy", "content": "The primary exploration challenges emerge during the tran-sition from falling to stable kneeling, a stage that proves difficult to explore effectively through random action noise alone. While human infants are likely to learn motor skills with external supports [6], it inspires us to design environmental assistance to accelerate the exploration. Specifically, we apply an upward force F on the robot base, which is largely set at the start of training. This force takes effect only when the robot's trunk achieves a near-vertical orientation, indicating a successful ground-sitting posture. The force magnitude de-creases progressively as the robot can maintain a target height at the end of the episode."}, {"title": "D. Motion Smoothness", "content": "Humanoid robots often feature many DoFs, each equipped with wide position limits and high-power actuators. This configuration often results in violent motions after RL training, characterized by violent ground hitting and rapid bouncing movements. While setting low action bounds could mitigate this behavior, it might prevent the robot from exploring effective standing-up motions. To this end, we introduce an action rescaler $\\beta$ to scale the action output, implicitly controlling the bound of the maximal torques on each actuator. This scale coefficient gradually decreases like vertical force reduction.\nTo prevent motion os-cillation, we adopt the smoothness regularization method $L2C2$ [20] into our multi-critic formulation. This method applies regularization to both the actor-network $\\pi_{\\theta}$ and critics $V_{\\phi i}$ by introducing a bounded sampling distance between consecutive states $s_t$ and $s_{t+1}$:\n$\\mathcal{L}_{L2C2} = \\lambda_{\\pi}D(\\pi_{\\theta}(s_t), \\pi_{\\theta}(\\hat{s_t})) + \\lambda_{V} \\Sigma_i D(V_{\\phi i} (s_t), V_{\\phi i} (\\hat{s_t})),$ where $D$ is a distance metric, $\\lambda_{\\pi}$ and $\\lambda_{V}$ are weight coeffi-cient, $\\hat{s_t} = s_t + (s_{t+1} - s_t) \\cdot u$ is the interpolated state given a uniform noise u ~ U(\u00b7). We combine this objective function with ordinary PPO objectives to train our control policies."}, {"title": "E. Training in Simulation & Sim-to-Real Transfer", "content": "We use Isaac Gym [31] simulator with 4096 parallel envi-ronments and the 23-DoF Unitree G1 robot to train standing-up control policies with the PPO [42] algorithm.\nTo model the diverse starting postures in the real world, we design 4 terrains to diversify the starting postures: (1) ground that is flat, (2) platform that supports the trunk of robot, (3) wall that supports the trunk of the robot, and (4) slope with a benign inclination that supports the whole robot. We visualize these terrains and examples of their corresponding scenes in the real world in Fig. 3.\nTo enhance real-world deploy-ment, we employ domain randomization [48] to bridge the physical gap between simulation and reality. The random-ization parameters, detailed in Table II, include body mass, base center of mass (CoM) offset, PD gains, torque offset, and initial pose, following [2, 28]. Notably, the CoM offset is critical, as it enhances controller robustness against real-world CoM position noise, which may arise from insufficient torques or discrepancies between simulated and real robot models."}, {"title": "F. Implementation Details", "content": "Our implementation of PPO is based on [39]. The actor and critic networks are structured as 3-layer and 2-layer MLPs, respectively. Each episode has a rollout length of 500 steps. For smoothness regularization, the weight coefficients A and Ay are set to 1 and 0.1, respectively. The PD controller operates at 200 Hz in simulation and 500 Hz on the real robot to ensure accurate tracking of the PD targets, while the control policies run at 50 Hz."}, {"title": "V. SIMULATION EXPERIMENTS", "content": "While the design of evaluation metrics for humanoid standing-up control remains an open question [44], we aim to make a step forward by proposing the following metrics:\nSuccess rate $E_{succ}$: The episode is considered successful if the robot's base height, $h_{base}$, exceeds a target height $h_{targ}$ and is maintained for the remainder of the episode, indicating stable standing."}, {"title": "VI. REAL ROBOT EXPERIMENTS", "content": "We evaluate our method in both laboratory and outdoor en-vironments corresponding to simulation terrains, using HoST-w/o-L2C2 as the baseline to examine the effect of smoothness regularization during deployment.\nMotion oscillations are observed in all scenes without smoothness reg-ularization, often leading to standing-up failures. In contrast, our method produces smooth and stable motions, especially on 10.5\u00b0 slope."}, {"title": "VII. CONCLUSION", "content": "Our proposed framework, HoST, advances humanoid standing-up control by addressing the limitations of existing methods, which either neglect hardware constraints or rely on predefined motion trajectories. By leveraging reinforcement learning from scratch, HoST enables the learning of posture-adaptive standing-up motions across diverse terrains, ensuring effective sim-to-real transfer. The multi-critic architecture, along with smoothness regularization and implicit speed con-straints, optimizes the controllers for real-world deployment. Experimental results with the Unitree G1 humanoid robot demonstrate smooth, stable, and robust standing-up motions in a variety of real-world scenarios. Looking forward, this work paves the way for integrating standing-up control into existing humanoid systems, with the potential of expanding their real-world applicability."}, {"title": "VIII. LIMITATIONS AND FUTURE DIRECTIONS", "content": "While our method demonstrates strong real-world perfor-mance, we acknowledge several key limitations that should be addressed in the near future.\nAlthough proprioception alone is sufficient for many postures, some failures were observed during outdoor tests, such as standing from a seated position and colliding with surroundings. Integrating percep-tual capabilities will help address this issue.\nWe observe that training with both supine and prone postures has negatively impacted perfor-mance due to interference between sampled rollouts. Ad-dressing this issue could further enhance capabilities like fall recovery and improve overall system generalization.\nAlthough in-tegration with existing humanoid systems is not demonstrated in this paper, we envision that standing-up control can be effectively incorporated into current humanoid frameworks to extend real-world applications."}, {"title": "APPENDIX", "content": "We conducted our experiments using the Unitree G1 humanoid robot, which has a mass of 35 kg, a height of 1.32 m, and 23 actuated degrees of freedom (6 per leg, 5 per arm, and 1 in the waist). The robot is equipped with a Jetson Orin NX for onboard computation and uses an IMU and joint encoders to provide proprioceptive feedback.\nThe curriculum adjustment condition is consistent for both the vertical force and action bound: the head height hhead must reach a target height Hhead by the end of each episode. Initially, the vertical force F is set to 200 N, and the action bound $\\beta$ is set to 1. Upon reaching the target head height, the vertical force decreases by 20 N, and the action bound decreases by 0.02. The lower bounds for the vertical force and action bound are 0 N and 0.25, respectively.\nThe first stage involves righting the body, where we set Hstage1 to 0.45 m. The second stage involves rising the body, with Hstage2 set to 0.65 m.\nEach policy is evaluated on each terrain with 5 repetitions of 250 episodes each, totaling 1250 episodes. We report the mean and standard deviation of performance."}]}