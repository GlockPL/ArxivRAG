{"title": "RELCON: RELATIVE CONTRASTIVE LEARNING FOR A MOTION FOUNDATION MODEL FOR WEARABLE DATA", "authors": ["Maxwell A. Xu", "Jaya Narain", "Gregory Darnell", "Haraldur Hallgrimsson", "Hyewon Jeong", "Darren Forde", "Richard Fineman", "Karthik J. Raghuram", "James M. Rehg", "Shirley Ren"], "abstract": "We present RelCon, a novel self-supervised Relative Contrastive learning ap- proach that uses a learnable distance measure in combination with a softened contrastive loss for training an motion foundation model from wearable sensors. The learnable distance measure captures motif similarity and domain-specific se- mantic information such as rotation invariance. The learned distance provides a measurement of semantic similarity between a pair of accelerometer time-series segments, which is used to measure the distance between an anchor and vari- ous other sampled candidate segments. The self-supervised model is trained on 1 billion segments from 87,376 participants from a large wearables dataset. The model achieves strong performance across multiple downstream tasks, encom- passing both classification and regression. To our knowledge, we are the first to show the generalizability of a self-supervised learning model with motion data from wearables across distinct evaluation tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Advances in self-supervised learning (SSL) combined with the availability of large-scale datasets have resulted in a proliferation of foundation models (FMs) in computer vision (Oquab et al., 2023), NLP (OpenAI et al., 2023), and speech understanding (Yang et al., 2024). These models provide powerful, general-purpose representations for a particular domain of data, and support generaliza- tion to a broad set of downstream tasks without the need for finetuning. For example, the image representation contained in the DINOv2 (Oquab et al., 2023) model was trained in an entirely self- supervised way and achieves state-of-the-art performance on multiple dense image prediction tasks such as depth estimation and semantic segmentation, by decoding a frozen base representation with task-specific heads. In contrast to these advances, the times-series have not yet benefited from the foundation model approach, with a few exceptions (Abbaspourazad et al., 2024; Das et al., 2023). This is particularly unfortunate for problems in mobile health (mHealth) signal analysis, which encompasses data modalities such as accelerometry, PPG, and ECG (Rehg et al., 2017), as the col- lection of mHealth data from participants can be time-consuming and expensive. However, recent advances in self-supervised learning for mHealth signals (Abbaspourazad et al., 2024; Yuan et al., 2024; Xu et al., 2024) have shown promising performance, raising the question of whether it is now feasible to train foundation models for mHealth signals.\nIn this paper, we demonstrate, for the first time, the feasibility of adopting a foundation model ap- proach for the analysis of accelerometry data across tasks. Accelerometry is an important mHealth signal modality that is used in human activity recognition (HAR) (Haresamudram et al., 2022), phys- ical health status assessment (Xu et al., 2022), energy expenditure estimation (Stutz et al., 2024), and gait assessment (Apple, 2021), among many other tasks. We use a novel method for self- supervised learning combined with pretraining on a large-scale accelerometry dataset. We show that this approach yields an effective representation for accelerometry data which is useful for multiple downstream tasks, including HAR and the estimation of gait metrics such as stride velocity and dou- ble support time. Crucially, we obtain state-of-the-art performance on these tasks without finetuning and also exceed the performance of fully-supervised models trained on smaller datasets."}, {"title": "2 RELATED WORK", "content": "Time-series FM: We define foundation models to be representations that are pre-trained on broad- scale data and are capable of solving multiple diverse downstream tasks without fine-tuning (Bom- masani et al., 2021) (e.g., each task uses frozen weights with supervised training of a light-weight prediction head). We believe we are the first to train a model for accelerometry data which meets these criteria. The closest related work is Yuan et al. (2024), which uses data from the UK Biobank to train an accelerometry representation. However, this work does not exhibit multi-task downstream performance, since they are all Human Activity Recogintion (HAR) variants, making it is less clear how well their representation captures the data domain. In contrast, we test our FM on multiple di- verse tasks including HAR, workout classification, and gait analysis regression. Other notable FMs are Abbaspourazad et al. (2024); Das et al. (2023), but these do not model accelerometry.\nTime-series SSL: Our RelCon architecture is a novel contrastive learning approach for time-series data. The closest related method is REBAR (Xu et al., 2024). We adopt REBAR's motif-based approach to generating positive pairs but their model was non-specific to accelerometry and thus only used a simple exact-motif-matching mechanism, which would not be invariant to changes in sensor position and other invariances. Additionally, due to its \"hard\" contrastive loss, it suffers from sensitivity to false positives and negatives because only one candidate is set to be positive and all other are set to be negative. Other prior works on SSL primarily adopt either data augmentations or masked auto-encoder approaches, including other prior works on HAR (Yuan et al., 2024; Hare- samudram et al., 2022; 2024; Straczkiewicz et al., 2021). Additional works have addressed SSL for signals such as PPG and ECG, for applications including health condition predictions and sleep staging (Abbaspourazad et al., 2024; Song et al., 2024; Thapa et al., 2024; McKeen et al., 2024; Yuan et al., 2024; Kiyasseh et al., 2021; Diamant et al., 2022; Jeong et al., 2023b; Das et al., 2023).\nFor motion wearables, most SSL approaches have focused on human activity recognition. Hare- samudram et al. (2022) benchmarked a number of SSL approaches on accelerometer data, showing generalizability across datasets and sensor positions. Models for other motion-based tasks \u2013 in- cluding walking speed prediction (Shrestha & Won, 2018; Soltani et al., 2021), gait classification (Slemen\u0161ek et al., 2023; Brand et al., 2024), and health monitoring (Takallou et al., 2024) have focused on physics-based models, supervised learning, or SSL training for a single task.\nSoft Label Learning Approaches: A feature of our RelCon approach is the use of soft labels to obtain more fine-grained characterizations of similarity. Most prior self-supervised contrastive"}, {"title": "3 METHODOLOGY", "content": "The RelCon methodology has two key components. The first includes several innovations to learn a better distance measure to capture accel semantic information in Section 3.1. The second component is a novel relative contrastive loss that encodes relative order relationships, presented in Section 3.2."}, {"title": "3.1 LEARNABLE DISTANCE MEASURE", "content": "Following prior work (Xu et al., 2024), we train a neural network to learn a distance measure to identify whether two sequences have similar temporal motifs (Sch\u00e4fer & Leser, 2022) and are se- mantically similar. After training, the architecture is frozen and used as a static function to determine the relative similarities of candidate samples in the RelCon approach. The distance measure archi- tecture is defined in Eq. 1 below:\n$d(X_{anc}, X_{cand}) := ||X_{anc|cand} - X_{anc}||_2$ (1)\n$X_{anc|cand} = = ((CrossAttn(X_{anc|X_{cand}})W_o+b_o) + \\mu_{cand}) \\sigma_{cand}$ (2)\n$CrossAttn (x_{anc} X_{cand}) = \\sum_{X_{cand} \\in X_{cand}} sparsemax (sim (f_q(x_{anc}), f_k (x_{cand}))) f_v (x_{cand})$ (3)\n$f_{q/k/v} (X_{anc/cand}) = DilatedConvNet_{q/k/v} (x_{cand}) $ (4)\nwhere $X \\in R^{T\\times D}$ and $x, \\mu, \\sigma \\in R^D$ with $T$ as the time length and $D$=3 for our 3-axis accelerometry signals. The distance between an anchor sequence and a candidate sequence, $d(X_{anc}, X_{cand})$, is defined as the reconstruction accuracy to generate the anchor from the candidate. The distance measure is strictly dependent on the motif similarities between the anchor and candidate that are captured in the dilated convolutions in $f_{q/k}$ (Xu et al., 2024).\nPrior work with this distance measure only captured a simple exact-motif-matching mechanism because the original masked reconstruction training task can be solved by exact-matching the non- masked regions. To enhance the distance measure, we introduce 3 key innovations:\n1. Use Accel-specific augmentations (Tang et al., 2020) during training to learn a motif-matching mechanism that is invariant to Accel-semantic-preserving transformations.\nDuring training, we define $X_{cand} := Aug(X_{anc})$, making the candidate an augmented version of the original sequence, using augmentations from Accel SimCLR (Tang et al., 2020). This helps the model learn to reconstruct the original anchor from semantically similar but altered candidates and prevents the exact-match solution, such as recognizing running signals even with changes like an upside-down wearable device or increased noise from a loose fitting device.\n2. Replace the softmax in the cross-attention with a sparsemax formulation (Martins & Astudillo, 2016) in Eq. 3 to encourage precise motif comparison.\nSparsemax returns the euclidean projection of the unnormalized logits onto the probability simplex, encouraging sparsity (Martins & Astudillo, 2016). This prevents diffuse attention distributions that compare minor, irrelevant motifs within the anchor and candidate. Sparsemax ensures the model reconstructs with distinct features, enabling our measure to capture class-specific information.\n3. Modify the reversible instance normalization (Kim et al., 2021) to normalize an anchor based upon the candidate, to preserve relative magnitude information.\nThe anchor sequence and final reconstruction output are normalized (in Eq. 4) and unnormalized (in Eq. 2) using candidate sequence statistics. When an anchor and candidate have drastically different magnitudes, it is more likely they represent different fundamental motions and should have worse reconstruction error. Reversible normalization helps preserve this effect."}, {"title": "3.2 RELATIVE CONTRASTIVE LOSS: ALL PAIRS ARE POSITIVE", "content": "Normalized temperature cross entropy loss (NT-Xent is standard in contrastive learning (Eq. 5). It learns a class discriminative embedding space by pulling the anchor and positive instances closer together in the embedding space and pushing all negatives away from the anchor, as in:\n$l(X_{anc}, X_{pos}, S_{neg}) = -log \\frac{exp(sim(X_{anc}, X_{pos})/\\tau)}{\\sum_{X_{neg} \\in S_{neg}} exp(sim(X_{anc}, X_{neg})/\\tau) + exp(sim(X_{anc}, X_{pos})/\\tau)}$ (5)\nTraditionally, NT-Xent uses strict labels that define absolute positive and negative sets. However, we would like to use the learned distance measure to modify the loss function to capture relative ordering among the candidates. That is, if $d(X_{anc}, X_i) > d(X_{anc}, X_j)$ the loss learns to preserve that relative ordering in the embedding space.\nTo do this, we redefine $S_{neg} := f_{neg}$ in Eq. 6. Now the set of negatives is replaced with a function that creates different negative sets, depending on the current positive pairing. This enables us to construct our Relative Contrastive Loss function in Eq. 7. This loss function iterates across all candidates, $X_i \\in S_{cand}$, and sets each candidate as positive, $X_{pos} := X_i$ before calculating the negative set. All candidates with a larger distance measure than this newly defined positive are defined to be the negative set, $S_{neg} := f_{neg} (X_{anc}, X_i, S_{cand})$, and then NT-Xent loss is calculated for this iteration.\n$f_{neg} (X_{anc}, X_{pos}, S) = \\{X \\in S : d(X_{anc}, X) > d(X_{anc}\\}$ (6)\n$L_{RelCon} = \\sum_{X_i \\in S_{cand}} (X_{anc}, X_{pos := X_i, S_{neg} := f_{neg} (X_{anc}, X_i, S_{cand}))$ (7)\nIn RelCon, our pool of candidates originates from 2 sources: 1) sampling within the user, across time and 2) sampling within the batch. In (1), we choose c random subsequences from the same user as the anchor sequence to be contrasted. In our experiments, c=20. This sampling method helps capture within-person temporal dynamics, such as how a user's motion signals may indicate fatigue over time. Sampling within the batch in (2) allows the model to learn similarities and differences across other users. An visualization of our RelCon loss procedure can be found in Fig. 1."}, {"title": "4 EXPERIMENTAL DESIGN", "content": null}, {"title": "4.1 FOUNDATION MODEL PRETRAINING", "content": "We trained models on Inertial Movement Unit (IMU) sensors from the Apple Heart & Movement Study (AHMS) (MacRae, 2021). AHMS is an ongoing research study designed to explore the links between physical activity and cardiovascular health, which is sponsored by Apple and conducted in partnership with American Heart Association and Brigham and Women's Hospital. To be eligible for the study, participants must among other eligibility criteria \u2013 be at least 18 years of age (at least 19 in Alabama and Nebraska; at least 21 in Puerto Rico), reside in the United States, have access to an Apple Watch, and provide informed consent electronically in the Apple Research app.\nThe training data included a subset of study data with 87,376 participants recorded over one day, with a 10/3/3 train/val/test split. Following prior convention (Reyes-Ortiz et al., 2015), we use 2.56 seconds of the raw 100 Hz 3-axis x,y,z accelerometry signal of the IMU sensor in the wearable device as input to our embedding model. Each model was pre-trained with 8 x A100 GPUs for 24 hours. Models iterated over a total of 1 billion samples, and a total of ~30k unique participant-days.\nWe evaluated RelCon along with other commonly used methods in SSL with Accel for comparison:\n\u2022 Accel SimCLR (Tang et al., 2020; Chen et al., 2020) contrasts positive pairs formed by accel- specific augmentations (i.e. 3D rotation) and has been shown to have state-of-the-art performance in the accel domain (Haresamudram et al., 2022).\n\u2022 Augmentation Prediction (Yuan et al., 2024; Haresamudram et al., 2022) predicts whether an accel-specific augmentation was applied to each sample.\n\u2022 Accel REBAR (Xu et al., 2024) uses a contrastive loss where positive pairs are identified by a learned motif-similarity. Instead of using the original version of REBAR designed for generalized time-series data, we utilize it with the accel-focused innovations proposed in Section 3.1.\nFor each of these methods, we used a 1D ResNet-34 encoder backbone that used average pooling to generate a 256-dimensional embedding vector (3.9M parameters)."}, {"title": "4.2 DOWNSTREAM EVALUATION", "content": "First, in order to assess generalizable of our learned embedding, we evaluate our models with our Task Diversity Evaluation with two different datasets, in which we compare the RelCon foundation model against a set of diverse downstream tasks and to self-supervised models trained on our data with three other methods that have seen strong performance accel data: SimCLR, AugPred, and REBAR. We use the embeddings with linear regression for various gait metrics, and with linear probe classification on both the subsequence and workout-level.\nNext, in order to compare against prior work, we evaluate our RelCon foundation model with our Benchmarking Evaluation with four additional classification datasets. Specifically, we compare against another large-scale pre-trained accel foundation model (Yuan et al., 2024) and against an accel SSL benchmarking study (Haresamudram et al., 2022), with comparisons to seven distinct self-supervised learning methods. These datasets help evaluate model generalizability under data distribution shifts, including differing sensor positions and inference window lengths.\nIn total, we compare against 6 different downstream datasets across 4 different types of downstream tasks. We compare our RelCon foundation model against 11 models total: 3 pre-trained from scratch in the Task Diversity Evaluation and 8 from the prior literature in the Benchmarking Evaluation."}, {"title": "4.2.1 TASK DIVERSITY EVALUATION DATASETS AND SET-UP", "content": "Gait Metric Regression: We used a dataset including gait mat collection to evaluate models on gait metric regression (Apple, 2021). All participants completed proctored overground walking tasks with two mobile devices with IMU sensors at each side of the body in different locations (i.e. at the hip, in a front or back pocket, or in a waist bag). We used the Design Cohort A participants, which included 359 participants with an average age of 74.7, who provided informed consent. Participants were instructed to conduct 4 different walking tasks: 1) one lap at self-selected speed, 2) four laps at an instructed slow speed, and 3) as many laps as possible within 6 minutes. Each walking task was conducted along a 12-meter straight-line course, with an 8-meter pressure mat placed centrally and various statistics about each participant's gait were calculated: step count, walking speed, step length, double support time, and walking asymmetry. Models were evaluated on predicting double support time (DST) and stride velocity.\nEach 2.56s subsequence was matched to the lap aggregated target (e.g. total number of steps or average walking speed). The participants were assigned into 50% train and 50% test randomly based on participant ID, where every lap for a given participant falls into the same split. The training split was used to train linear regression probes on embeddings from the self-supervised models. Metrics were selected following a prior report (Apple, 2021): mean squared error (MSE), std dev of squared error (SDSE), mean absolute error (MAE), std dev of absolute error (SDAE), and Pearson's Correlation Coefficient. Mean and std devs were calculated by aggregating predictions across all inputs of a given user and are related to bias and variance respectively. Correlation is used in order to assess how each user's average gait metric corresponds to ground truth values. Ranges for each metric were calculated by retraining the linear regression probe five times with different set seeds.\nActivity Classification: We evaluated activity classification performance using self-reported activ- ity labels gathered from data in AHMS. We used a subset of data with ~2k total users across 14 workouts (full list in Table 2). The 14 selected workouts captured a range of diverse activities (e.g. kickboxing and rowing) that are non-trivial to separate (i.e. outdoor cycling vs. indoor cycling). For evaluation, workouts were class balanced so each included ~22 hours of data, for a total of 310 workout hours. We used a 4/1/5 train/val/test split based on participant ID. We ensured the subset of data used for classification did not overlap with the pre-training dataset, preventing any data leakage.\nEmbeddings are generated for each 2.56s-long subsequence. We evaluate classification on both the subsequence-level as well as the workout-level. At the workout-level, we predict workout classes across each of 2.56s subsequences within the workout, and select the most frequently predicted workout. Including two prediction scales allow us to evaluate provides additional context about how information learned by the embedding interacts with time aggregation. We report F1, Kappa, Accuracy, and macro AUC metrics following previous work (Haresamudram et al., 2022; Yuan et al., 2024; Xu et al., 2024). Ranges per metric are obtained by retraining the probe five times and calculating the mean and standard deviation."}, {"title": "5 RESULTS AND DISCUSSION", "content": null}, {"title": "5.1 TASK DIVERSITY EVALUATION RESULTS", "content": "Gait metric regression: Table 1 shows gait metric regression performance of each SSL method. RelCon had the strongest consistent performance across both gait metrics, double support time and stride velocity. While methods based on prior work with Accel SimCLR (Haresamudram et al., 2022; Tang et al., 2020) were not developed with a focus on gait, they still achieve strong perfor-"}, {"title": "7 REPRODUCIBILITY", "content": "We will release the code for our RelCon foundation model publicly upon acceptance. This will include the RelCon training methodology, architecture, as well as the reproducible evaluation code from our \"Benchmarking Evaluation\u201d task, which utilizes public datasets. This can be used by the broader research community by providing code to easily re-train the RelCon approach in addition to providing a unified benchmarking task to guide model development."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 MODEL IMPLEMENTATION DETAILS", "content": "SimCLR: We follow the approach described by Haresamudram et al. (2023), following the implementation located here: https://github.com/ubicompsoartutorial/soar_ tutorial/tree/main/simclr. Then we use a batch-size of 64, temperature of 1, and train for 1e5 steps.\nAug Pred: We follow the approach described by Yuan et al. (2024), following the imple- mentation located here: https://github.com/OxWearables/ssl-wearables?tab= readme-ov-file. Then we use a batch-size of 64 and train for 1e5 steps.\nRelCon: We use the augmentations utilized by the aforementioned SimCLR model, batch-size of 64, temperature of 1, candidate set size of 20, and train for 1e5 steps.\nREBAR: We follow the approach described by Xu et al. (2024), following the implementation located here: https://github.com/maxxu05/rebar. Then we use the prior SimCLR aug- mentations, a batch-size of 64, temperature of 1, candidate set size of 20, and train for 1e5 steps."}, {"title": "A.2 EXTRA AHMS CLASSIFICATION RESULTS", "content": null}, {"title": "A.3 DETAILS AND PREPROCESSING ON THE AHMS PRE-TRAINING DATASET", "content": "Please refer to original paper on AHMS for further details on the dataset (Shapiro et al., 2023). Specifically, Figure 8 in that work includes visualizations that show general subject demographic distributions including age, body mass index, and self-reported race and ethnicity.\nPreprocessing: We use the raw 100hz accelerometry data as is, without specific preprocessing techniques, such as filtering or downsampling. We choose to not filter the data as prior work has discouraged filtering in a daily monitoring setting (Campbell et al., 2020), and may prevent our deep learning model from modeling subtle nuances within the signal. Additionally, we would like to develop our methods to be robust to noise via augmentations, such as additive gaussian noise augmentations. We purposely do not attempt to filter out periods of low accelerometry activity. This is because small, minor changes in the accelerometer signal have been shown to still be informative, being able to predict heart rate (Moebus et al., 2024)."}, {"title": "A.4 JUSTIFICATION FOR USING SINGLE-SENSOR-ACCELEROMETER-ONLY DATA", "content": "Motion information can be analyzed with multiple sensor streams, whether it be through multiple accelerometer sensors strapped in different locations (Jain et al., 2022) or via extra IMU-based sensors, such as the gyroscopic sensor (Garc\u00eda-de Villa et al., 2023). Many prior state-of-the-art human activity recognition, supervised machine learning models will exploit the full sensor suite that includes multi-modality and multi-location sensors (Essa & Abdelmaksoud, 2023; Suh et al., 2023). We recognize the value of a foundation model that can incorporate a multi-modality and/or multi-location stream of data, as it would enable for greater insights on human motion and physiology, and we are interested in investigating this in future work.\nHowever, for our foundation model, we strive for broad generalizability in order to ensure that our learning approach and model is applicable across various settings. This includes low resource settings that only have accelerometer sensors available, as gyroscopic sensors are quite power hungry (Group, 2017). Accelerometer sensors are thus the most common sensor for monitoring human motion (Huang et al., 2023). Additionally, we would like our model to also be applicable for real- world field settings, in which multi-location sensors are uncommon for daily usage due to their bulkiness and discomfort. As such, our foundation model is able to be benchmarked against a broad range of datasets (i.e. our AHMS classification, our Gait Metric Data, HHAR, Motionsense, PAMAP2, Opportunity), which each utilize different sensor hardwares, but all include at least one 3-axis accelerometer sensor."}, {"title": "A.5 ELABORATING ON TABLE 1", "content": "Negatives in a SimCLR-based well-studied problem, with many recent works proposing novel methodologies to address this (Huynh et al., 2022; Jin et al., 2023; Chien & Chen, 2024). Addi- tionally, the accelerometry SimCLR we are benchmarking (Tang et al., 2020) makes no distinction to model within-user interactions, by treating every subsequence as independent, and hence does not model within-user interactions. Both SimCLR and augmentation prediction creates instances from the original sequence via augmentations, and so they will be resistant to False Positives.\nSimilar to REBAR, RelCon explicitly models within and between-user interactions by explicitly comparing an anchor against candidates from within-user across time and across other users. How- ever, unlike REBAR, we model the relative positions of our candidates, rather than a binary com- parison that treats all negative instances as the same. Then, RelCon is more likely to be resistant to False positives and False negatives due to the enhanced comparison that captures the nuanced differences between candidates."}, {"title": "A.6 COMPUTATIONAL COMPLEXITY OF RELCON", "content": "During training, RelCon needs to compare the relative distances of each candidate from the an- chor with our distance function. The distance function utilizes a highly parallelizable transformer function with complexity of $O(T^2 \\times d)$(Vaswani et al., 2017) and a convolution to embed the in- puts with a complexity of $O(k \\times T \\times d^2)$ (Vaswani et al., 2017). T=256 the subsequence length, d=64 the embedding dimension, and k=15 the kernel size. Therefore, because we calculate this"}, {"title": "A.7 EVALUATION DATASET DESCRIPTIONS", "content": null}, {"title": "A.7.1 COMPARISON TO A LARGE-SCALE PRE-TRAINED ACCEL MODEL (YUAN ET AL., 2024)", "content": "Yuan et al. (2024) has released their code publicly here, which contains exact data split and genera- tions: https://github.com/OxWearables/ssl-wearables\nOpportunity: 4-fold leave-one-subject-out cross validation with the sitting, standing, walking, and lying labels.\nPAMAP2: 9-fold leave-one-subject-out cross validation with the lying, sitting, standing, walking, ascending stairs, descending stairs, vacuum cleaning, and ironing classes. This is the wrist-specific accelerometry data."}, {"title": "A.7.2 COMPARISON TO AN ACCEL SSL BENCHMARKING STUDY (HARESAMUDRAM ET AL., 2022)", "content": "Although Haresamudram et al. (2022) has not released their code publicly, we contacted the authors directly to ensure that we matched their exact splits and classes evaluated. Table 4 in our paper is then constructed by drawing from Table 3 and 4 in the original paper. Specifically, HHAR is drawn from Table 4, PAMAP2 from Table 3/4, and MotionSense from Table 3 (Haresamudram et al., 2022). Note that in Haresamudram et al. (2022), Augmentation Prediction is referred to as \"Multi-Task Self Supervision\".\nHHAR: 5-fold leave-subject-out cross validation with the bike, sit, stairs down, stairs up, stand, and walk classes.\nMotionsense: 5-fold leave-subject-out cross with the downstairs, upstairs, sitting, standing, walk- ing, and jogging labels.\nPAMAP2: 5-fold leave-subject-out cross validation with the lying, sitting, standing, walking, run- ning, cycling, nordic walking, ascending stairs, descending stairs, vacuum cleaning, ironing, and rope jumping classes. This is the ankle-specific accelerometry data."}, {"title": "A.8 GENERALIZING TO TIME LENGTHS BEYOND 2.56 SECONDS", "content": "2.56s is a common accelerometry subsequence size for motion tasks (Reyes-Ortiz et al., 2015; Chen & Xue, 2015; Wang et al., 2007; McQuire et al., 2021; Mandong & Munir, 2018). Additionally, we have shown that our approach can easily be used for time-series with other, differing lengths. In our comparisons against Yuan et al. (2024), we utilize a subsequence length of 10 seconds in order to match their evaluations, and we still show strong performance. In our comparisons against Haresamudram et al. (2022), we utilize a subsequence length of 2 seconds in order to match their evaluations. This highlights that our model is robust to varying input lengths and is generalizable across input data configurations, as would be desirable from a foundation model. This flexibility is enabled by the final temporal-global-average-pooling layer that we have at the end of our architec- ture. Additionally, in our AHMS workout-level classification task, we show how our method can be used with variable-length time-series that can last up to 10 minutes long by aggregating predictions across windows."}, {"title": "A.9 COMPARISONS OF RELCON VS. SOFTER OR HARDER LOSS FUNCTIONS", "content": "Our relative contrastive loss in Eq. 7 is particularly interesting because the relative contrastive loss is able to have a nice balance of softness. If we increase the softness in our loss function by utilizing a metric learning loss function (Kim et al., 2019), then this will hurt performance on the gait metric regression tasks. However, if we increase the hardness to a binary contrastive loss (i.e. REBAR),"}, {"title": "A.10 COMPARISONS OF RELCON VS. YUAN ET AL. (2024) WITH SAME ENCODER BACKBONE", "content": "We have re-trained our RelCon FM with the ResNet-18 backbone with a final encoding dimension- ality of 1024, matching Yuan et al. (2024), and we show the results in the Table 7 below. Out of the 3/4 evaluations, RelCon continues to have stronger performance."}]}