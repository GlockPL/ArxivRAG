{"title": "Learning from Massive Human Videos for Universal Humanoid Pose Control", "authors": ["Jiageng Mao", "Siheng Zhao", "Siqi Song", "Tianheng Shi", "Junjie Ye", "Mingtong Zhang", "Haoran Geng", "Jitendra Malik", "Vitor Guizilini", "Yue Wang"], "abstract": "Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through a comprehensive pipeline: data mining from the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train a large humanoid model, UH-1, which takes text instructions as input and outputs corresponding actions to control a humanoid robot. Extensive simulated and real-world experiments validate that our scalable training approach leads to superior generalization in text-based humanoid control, marking a significant step toward adaptable, real-world-ready humanoid robots.", "sections": [{"title": "1. Introduction", "content": "Scalability is crucial in deep learning. Recent advances in computer vision have demonstrated that scaling up training data leads to more powerful foundation models for visual recognition [26, 41, 44] and generation [3, 51]. In robotics, researchers follow a similar paradigm and build foundation models for robotic manipulation [4, 5, 24, 40] by collecting massive robotic demonstrations. Nevertheless, in contrast to images and videos that are abundant and easily accessible, collecting large-scale robotic demonstrations is expensive and time-consuming, which limits the scalability of current robot learning methods. This raises the question: Can we use videos as demonstrations to improve the scalability of robot learning?\nTo address this challenge, many efforts have been made, such as learning affordances [2, 15, 28], flows [67, 69], and world models [68] from natural videos, which enable more generalizable robotic manipulation. However, when it comes to humanoid robots, learning such action representations from videos remains an open problem. Unlike robotic arms, humanoid robots have distinct kinematic structures and more degrees of freedom (DoFs), making them harder to control. Existing works [8, 9, 16, 17, 30, 47, 48] leverage large-scale reinforcement learning to learn robust humanoid control policies, but they only focus on limited robotic skills such as locomotion or jumping, making them less generalizable for handling everyday tasks. Other works [13, 19, 20, 53] control humanoid robots through teleoperation, but they require human labor to collect robotic data, which is less scalable. In contrast to these previous works, learning a universal action representation from massive videos will greatly improve the scalability of humanoid robot learning and enable more generalizable humanoid pose control.\nTo bridge this gap in humanoid robot learning, we introduce Humanoid-X, a large-scale dataset curated from a massive and diverse collection of videos for universal humanoid pose control. Humanoid-X utilizes natural language as an interface to connect human commands and humanoid actions, so humans can talk to their humanoid robots to control their actions. The natural language representations are extracted from videos via captioning tools and are used to describe the actions of humanoid robots. For action representations, Humanoid-X leverages both robotic keypoints for high-level control and robotic target DoF positions for direct position control. To extract humanoid actions from human videos, we first reconstruct 3D humans and their motions from videos. Then, we leverage motion retargeting to transfer motions from 3D humans to humanoid robots, resulting in robotic keypoints for high-level humanoid pose control. Finally, we learn a universal RL-based control policy that maps keypoints to low-level humanoid target DoF positions that can be deployed in real robots. We collect over 160,000 human-centric videos from academic datasets and the Internet, covering diverse action categories. We further transform these videos into text-action pairs, resulting in over 20 million humanoid actions with corresponding text descriptions. Humanoid-X paves the way for developing more generalizable and scalable humanoid robotic control guided by natural language.\nOn top of the Humanoid-X dataset, we further investigate how to learn a universal humanoid pose control model using large-scale text-action pairs. We introduce Universal Humanoid-1 (UH-1), a large humanoid model for universal language-conditioned humanoid pose control. UH-1 leverages the scalability of the Transformer architecture to handle vast amounts of data efficiently. We begin by discretizing 20 million humanoid actions into action tokens, creating a vocabulary of motion primitives. Then, given a text command as input, the Transformer model auto-regressively decodes a sequence of these tokenized humanoid robotic actions. For cases where the action representation involves robotic keypoints, we transform these into robotic DoF positions using an additional action decoder. Finally, we utilize a proportional-derivative (PD) controller to convert the DoF positions into motor torques, enabling us to control humanoid robots and deploy them in the real-world.\nTo validate the effectiveness of the Humanoid-X dataset and the UH-1 model, we conducted extensive experiments across both simulated and real humanoid platforms. Our results reveal that leveraging vast amounts of video data enables our model to seamlessly translate textual commands into diverse and contextually accurate humanoid actions. Notably, the UH-1 model demonstrates strong robustness, proving reliable in real-world deployment. To summarize, our key contributions are as follows:\n*   We introduce Humanoid-X, a pioneering large-scale dataset tailored for learning universal humanoid control from massive Internet video data.\n*   We introduce UH-1, a powerful, scalable model for language-conditioned control of humanoid poses. Our approach supports two flexible control modes that are interchangeable, depending on task requirements. We also provide extensive ablation study for our design choices.\n*   Our experiments confirm that training on massive video data enables a level of generalizability in humanoid control that was previously unattainable."}, {"title": "2. Related Works", "content": "Robot Learning from Internet Data. Many endeavors have been made to learn scalable robot learning policies from non-robotic data, especially Internet videos. The key idea is to learn valuable representations from massive visual data and transfer them to robotic tasks. The learned representations include pre-trained visual features from videos [36, 39, 46, 65] and transferable action representations such as affordances [1, 2] and object-centric flows [67, 69]. Other works [12, 38, 68] attempt to learn world models from Internet videos. However, most of these works focus on robotic manipulation. Since robot arms have totally different kinematic structures from humanoid robots, the learned visual and action representations for robotic manipulation are not transferable to humanoid robot control. In contrast, we investigate how to learn universal pose control for humanoid robots from massive videos.\nHumanoid Robot Learning. Extensive work has been dedicated to learning policies that enable robust control of humanoid robots. Some works focus on humanoid locomotion using large-scale reinforcement learning [8, 16, 17, 30, 48] or imitation learning [49, 57]. Other works learn humanoid manipulation via imitation learning [29, 71]. Notably, some works [9, 13, 19\u201321] learn humanoid teleoperation by transferring motions from 3D humans to humanoid robots. However, these works rely on well-calibrated motion capture data, limiting their generalization ability to unseen motions. In contrast, our method operates as a fully autonomous agent that learns from massive Internet videos and performs generalizable humanoid pose control based on arbitrary text commands.\n3D Human Motion Generation. Many works are attempting to generate diverse 3D human motions via Transformers [22, 72] or diffusion models [31, 54, 60, 66, 74]. Also, some works [14, 34, 35, 42, 43, 58, 64, 70] are trying to generate realistic motions to animate physics-based virtual characters. However, humanoid robots are essentially different from digital humans in many aspects: (1) they have different joint structures and degrees of freedom; (2) humanoid robots cannot access privileged information like linear velocities, which is readily available when controlling virtual humans; (3) humanoid robots have physical constraints such as motor torque limits, whereas 3D virtual humans do not have these limitations. An alternative solution for generalizable humanoid pose control is to first generate 3D human motions and then retarget them to humanoid robots [19, 23]. Compared to these approaches, our UH-1 model offers a more streamlined solution by directly mapping text commands into executable humanoid actions without intermediate steps. Furthermore, unlike human motion generation models trained on expensive motion capture data, learning from massive videos significantly enhances the generalization ability of our method."}, {"title": "3. Humanoid-X Dataset", "content": "3.1. Overview\nTo scale up humanoid robot learning using massive human videos, we introduce Humanoid-X, the largest humanoid robot dataset to date compiled from a vast and diverse collection of videos for universal humanoid pose control. Humanoid-X consists of 163,800 motion samples covering a comprehensive set of action categories. Each motion sample in the dataset contains 5 data modalities: an original video clip V, a text description T of the action in the video, a sequence of SMPL [33]-based human poses Phuman estimated from the video, a sequence of humanoid keypoints Probot for high-level robotic control, and a sequence of humanoid actions Arobot representing target DoF positions for low-level robotic position control. Humanoid-X encompasses over 20 million frames, totaling approximately 240 hours of data. Beyond its extensive scale across multiple data modalities, which is essential for scalable humanoid policy training, Humanoid-X also features a large and diverse text-based action vocabulary, as shown in Fig. 3 (c). This diversity supports universal and text-conditioned humanoid pose control. In the next section, we will discuss how to obtain these motion samples (V,T, Phuman, Probot, Arobot) from massive videos.\n3.2. Learning from Massive Videos\nTo process large-scale, in-the-wild raw video data, we developed a fully automated data annotation pipeline comprising five modules, as illustrated in Fig. 2. The pipeline includes (1) a video processing module that mines and extracts video clips V from noisy Internet videos, (2) a video captioning model that generates text description of human actions T, (3) a human pose detection module that estimates parametric 3D human poses Phuman from video clips, (4) a motion retargeting module to generate humanoid robotic keypoints Probot by transferring motions from humans to humanoid robots, and (5) a goal-conditioned reinforcement learning policy to learn physically-deployable humanoid actions Arobot by imitating humanoid keypoints.\nVideo Mining and Processing. The first step of our approach is to collect a large number of human-centric videos that encompass a wide variety of action types. To this end, we mine massive informative video clips from 3 sources: academic datasets for digital human research [6, 11, 18, 32, 56, 61, 75], datasets for video action understanding [7, 55], and Internet videos from YouTube. To collect Internet videos, we designed over 400 unique search terms covering a range of human activities from daily tasks to professional sports, and then utilized the Google Cloud API* to retrieve the top 20 videos for each specified search term.\nOriginal videos are often noisy, including segments with no humans, multiple humans, or a stationary individual, which makes them unsuitable for humanoid control. To obtain meaningful video clips, we begin by downsampling each video to a standardized 20 frames per second (FPS) to ensure consistency across the dataset. Next, we employ an object detector [50] for single-human detection, selecting frames with precisely one visible person. Following detection, we apply motion detection by calculating the pixel-wise grayscale difference between consecutive frames to keep frames showing significant movement. We then compile sequences of at least 64 consecutive frames that satisfy the above single-human motion criterion into video clips, resulting in 163,800 video clips V in total.\nVideo captioning. Language bridges human commands and humanoid actions. To associate humanoid actions with semantic meaning and enable language-conditioned humanoid control, we employ a video captioning model [10] to generate fine-grained action descriptions T from videos:\nT = Fcaption(V), (1)\nwhere Fcaption is the video captioning model. To avoid irrelevant text descriptions, we carefully design prompts to guide the model to describe human actions instead of physical appearance, resulting in action-centric text descriptions.\n3D Human Pose Estimation. Humanoid robots inherently share a similar skeleton with humans, which allows for learning control policies for humanoid robots based on human motion data. To this end, we first need to extract human poses from videos. To accurately track and estimate human poses in video clips, we adopt a video-based 3D human parametric model estimator [27], which estimates SMPL [33]-based humans and camera parameters for each frame. We further extract global human motions, i.e., root translations, using the estimated camera parameters. The process can be formulated as:\nPhuman(\u03b2, \u03b8, troot) = Fpose(V), (2)\nwhere Fpose is the human pose estimation model. Finally, we obtain per-frame 3D human pose: Phuman(\u03b2, \u03b8, troot), where \u03b2 controls the human shapes, \u03b8 controls the joint rotations, and troot controls the global root translations.\nMotion Retargeting from Humans to Humanoid Robots. Since humans and humanoid robots have similar skeletons, we can track the human joint positions across frames and map them to the corresponding joints in a humanoid robot, resulting in humanoid keypoints Probot for high-level control. In particular, we chose 12 joints that exist in both humans and humanoid robots: left and right hips, knees, ankles, shoulders, elbows, and wrists. The joint positions Pjoints can be obtained via forward kinematics Ffk:\nPjoints = Ffk(Phuman(\u03b2, \u03b8, troot)). (3)\nSince humans have different shapes from humanoid robots, following [20], we first optimize the human shape parameters \u03b2 to ensure that resized human shapes closely resemble those of a humanoid robot. Specifically, we first obtain joint positions in the humanoid robot under a standard T-shaped pose: Probot. Then, under the same T-shaped pose, we optimize \u03b2 to make human joint positions Pjoints the same as the corresponding humanoid joint positions Probot:\nmin\u03b2 ||PTjoints \u2212 Probot||2, (4)\ns.t. PTjoints = Ffk(Phuman(\u03b2, \u03b8T, troot)), (5)\nwhere \u03b8T denotes the standard T pose. For each frame of human pose, we replace the original \u03b2 with the optimal \u03b2\u2032 in Phuman, and following Eq. 3 we can obtain the adjusted joint positions P\u2032joints. Finally, we directly set humanoid robotic keypoints as the adjusted human joint positions:\nProbot := P\u2032joints (6)"}, {"title": "4. UH-1 for Universal Humanoid Pose Control", "content": "To effectively control humanoid robots, we also extract the motor DoF positions qrobot in the humanoid robot via inverse kinematics Fik:\nqrobot = Fik(Probot). (7)\nWe use the Adam optimizer [25] to solve the inverse kinematics problem. A smoothing term is added to the optimization to regularize changes in qrobot across frames.\nGoal-conditioned Humanoid Control Policy. The retargeted humanoid keypoints Probot and DoF positions qrobot accurately reflect humanoid motions, but they cannot be directly deployed to the real robot. This is because they lack the necessary safety guarantees and robustness needed to handle real-world variability and constraints effectively. To address this, we develop a goal-conditioned control policy \u03c0 that adapts these motions while ensuring safe and reliable deployment on the physical robot:\n\u03c0: G \u00d7 O \u2192 Arobot. (8)\nThe inputs to the policy \u03c0 include two parts: the goal space G and the observation space O. The goal space G contains humanoid keypoints Probot, DoF positions qrobot, and root movement goals derived from troot. The observation space O contains robot proprioception information such as root orientation, angular velocity, and current motor DoF positions. The output action space Arobot are target DoF positions of each joint for controling the humanoid robot, which can be further transformed into motor torque signals through a proportional-derivative (PD) controller.\nWe train the control policy, \u03c0, using large-scale reinforcement learning with PPO [52] for policy optimization. The reward function includes multiple terms: motion rewards to encourage imitation of the retargeted humanoid keypoints Probot and DoF positions qrobot; root tracking rewards to follow target root orientations and linear velocities from troot; and stability rewards to help the robot maintain balance and prevent falls during movement. The resulting policy \u03c0 and robotic actions Arobot enable the humanoid robot to operate safely in the physical world while maintaining the desired motions.\nFinally, we collect a large number of motion samples (V,T, Phuman, Probot, Arobot) from massive videos. In the next section, we investigate how to train a universal humanoid pose control policy using massive motion samples.\n4. UH-1 for Universal Humanoid Pose Control\nLearning from massive videos enables us to distill a universal humanoid pose control policy from large-scale motion samples (V,T, Phuman, Probot, Arobot). We introduce UH-1, a large language-conditioned humanoid model that takes natural language commands T and generates corresponding humanoid robotic actions {Probot, Arobot}:\n\u03c0UH\u22121: T \u2192 {Probot, Arobot}, (9)\nwhere \u03c0UH\u22121 denotes the UH-1 model. Notably, as illustrated in Fig. 5, our model can either generate high-level humanoid keypoints Probot, which are then fed into the goal-conditioned policy \u03c0 to control the humanoid robot in closed-loop, or generate robotic actions Arobot for direct open-loop control. Our model bridges the gap between semantic language commands and physically deployable robotic actions, enabling more generalizable humanoid robotic control using text instructions. For simplicity, in the following section, we use Arobot as an example to illustrate our method; Probot can be generated in the same manner.\nWe adopt the Transformer [63] as our main model architecture due to its scalability to large-scale data. As shown in Fig. 4, to enable efficient learning, we first train an action tokenizer using [62] to discretize humanoid motions into a vocabulary of action tokens. Then, we train the Transformer to auto-regressively decode action tokens, resulting in executable humanoid actions.\nUH-1 Action Tokenizer. We follow [62] and map T frames of actions Arobot = [a1, ..., aT] into a sequence of discrete action tokens Ztoken = [z1, ..., zT/K] via an encoder Fencode and quantization Fquant:\nZtoken = Fquant(Fencode(Arobot)), (10)\nwhere Fencode and Fquant are standard operations in [62]. The action tokens Ztoken come from a shared action vocabulary, and each token can be viewed as a motion primitive that is learned and shared across all data samples. Notably, different from language tokenization, humanoid actions won't change much in adjacent frames. To maintain the temporal smoothness in humanoid actions, we encode a short clip with K frames of actions [aiK, ..., a(i+1)K] into a single action token zi, rather than encoding each frame individually. This approach not only preserves smooth transitions but also eases the learning process.\nThe decoder of VQ-VAE Fdecode tries to reconstruct the original action sequence with the latent embeddings associated with the action tokens:\nArobot = Fdecode(Ztoken). (11)\nWe denote the reconstructed action sequence as A\u2032robot = [a\u20321, ..., a\u2032T]. The reconstruction loss is formulated as\nLrecon = \u2211T(di \u2212 ai|+|(di+1 \u2212 ai) \u2212 (ai+1 \u2212 ai)|), (12)\nwhere the first term is the L1 reconstruction loss in [62] and the second term encourages the first-order similarity of original and reconstructed action sequences. Additionally, we add regularization terms on latent embeddings as in [62].\nUH-1 Transformer. We formulate the task of language-conditioned humanoid pose control as auto-regressively decoding action tokens Ztoken conditioning on text commands T. Formally, let Ztoken = [z1, ..., zT/K] denote the target action token sequence, where zi is the current step to predict, and z1:i\u22121 represent the preceding context of action tokens, and l denote the text embedding by encoding the text command T with the CLIP [45] encoder. The UH-1 Transformer is then trained to model the conditional probability distribution P(zi|z1:i\u22121, l). A special [End] token is incorporated into the vocabulary to signal the termination of sequence generation. During training, we first tokenize each Arobot into Ztoken using Eq. 3. Then, we feed the language embedding l into the UH-1 transformer, and the transformer auto-regressively decodes action tokens. The learning objective is to minimize the negative log-likelihood over the whole training dataset D:\nLlearn =\u2211Z \u2208 D \u2211T/K i=1 log\u03a0 P(zi|z1:i\u22121, l). (13)\nDuring inference, using Eq. 11, the generated action tokens are decoded into Arobot for controlling the humanoid robot."}, {"title": "5. Experiments", "content": "In this section, we conduct extensive experiments to investigate the following research questions: (1) Universal Pose Control with UH-1: Does our UH-1 model enable universal humanoid robot pose control based on text commands? (2) Scalability and Generalization with Humanoid-X: Does the large-scale Humanoid-X dataset facilitate scalable training and improve the generalization ability of our UH-1 model? (3) Real-World Deployment of UH-1: Can our UH-1 model be deployed on real humanoid robots to enable reliable robotic control in real-world environments?\n5.1. Universal Humanoid Pose Control with UH-1\nWe conduct extensive experiments to validate the generalization ability of the UH-1 model. An alternative solution to text-to-humanoid action generation is a two-stage pipeline: generating 3D human motions first and then retargeting the human motions to humanoid robots. To this end, we compare our method with two important baselines for text-to-human motion generation: Motion Diffusion Model (MDM) [59] and Text-to-Motion GPT (T2M-GPT) [73]. For fair comparisons, We choose the commonly used HumanML3D [18] benchmarks and transform the humans in this dataset into humanoid robots, resulting in a new benchmark called HumanoidML3D. Similarly, we adopt the same motion retargeting method as in this paper to transform the human motions generated by the baselines into humanoid actions. We adopt the metrics in [18] to evaluate the humanoid motions from different aspects: (1) Quality: The Frechet Inception Distance (FID) evaluates the dissimilarity between feature distributions of generated and ground truth humanoid poses. (2) Diversity: The Diversity metric evaluates the variability within the generated humanoid pose distribution, calculated as the average Euclidean distance between 300 randomly sampled pairs of humanoid poses. (3) Reliability: The Multi-modal Distance (MM Dist) measures the Euclidean distance between motions and corresponding texts, and the R Precision assesses the accuracy of text and humanoid pose matches in the Top 3 rankings.\nTab. 1 shows the results of our UH-1 model compared against the baselines. The results indicate that UH-1 attains the highest performance across nearly all metrics, showing an over 23% reduction in the critical FID metric, while also maintaining comparable performance on the Diversity metric. The first-order similarity loss proposed in this paper greatly enhances the quality and reliability of the generated outputs. The results suggest that UH-1 is a streamlined model and performs better than the two-stage methods.\n5.2. Scalable Learning with Humanoid-X\nIn this section, we investigate whether scaling up training data with the large-scale Humanoid-X dataset can improve the generalization ability of our model. To explore this, we first pre-trained our UH-1 model on the Humanoid-X dataset and then finetuned and evaluated the performance on the HumanoidML3D benchmark. Tab. 2 shows the performance comparison with training only on HumanoidML3D. We found that pre-training on the Humanoid-X dataset greatly improves the quality, reliability, and diversity of humanoid actions, with an FID improvement from 0.445 to 0.379, a MM Dist score improvement from 3.249 to 3.232, and a Diversity improvement from 10.157 to 10.221.\nIn addition, we also study how scaling up training data affects the model performance. To this end, we train our UH-1 model on varying proportions of the Humanoid-X dataset, specifically 1%, 10%, 25%, 50%, 75%, and 100%. The results shown in Fig. 7 indicate that scaling up training data from 1% to 100% leads to a significant performance improvement in all metrics (FID from 0.689 to 0.463 and Diversity from 5.900 to 6.149). This suggests that by learning from massive videos, we successfully scale up the training data of humanoid robots and attain better performance.\n5.3. Real-World Deployment of UH-1\nTo investigate whether our UH-1 model, trained on the Humanoid-X dataset, can generate reliable humanoid actions that are physically deployable on humanoid robots, we designed 12 distinct language commands, as shown in Tab. 3, and evaluated them on a real humanoid robot. We use UNITREE H1-2\u2020 as our test embodiment. For the experiments, we evaluated each language command 10 times and controlled the robot in different places. Notably, for text-to-humanoid actions, we found that open-loop control can only work for upper-body control, so in this control mode, we use a pre-trained locomotion policy for controlling the lower-body of the humanoid robot. Fig. 6 shows the demos of real-robot experiments. Tab. 3 measures the task success rate for each language command. Our experimental results demonstrate that our UH-1 model can be reliably deployed on the real humanoid robot, achieving a success rate of nearly 100% across all evaluated language instructions."}, {"title": "5.4. Empirical Studies", "content": "Analysis of two control modes. UH-1 can either produce high-level humanoid keypoints for a goal-conditioned, closed-loop policy or directly generate robotic actions for open-loop control. To investigate the effectiveness of these two control modes, we randomly generate 100 keypoint sequences and 100 action sequences for each task, as illustrated in Fig. 8, and apply them in simulated robot control. The findings indicate that both modes can achieve an average success rate exceeding 89%, suggesting that text-to-action open-loop control with a separate locomotion policy is sufficient for most tasks. Moreover, the text-to-keypoint control mode, benefiting from the whole-body control policy, demonstrates slightly better robustness.\nAblation study on the action tokenizer. We conduct an ablation study to investigate the impact of different vocabulary sizes of the UH-1 action tokenizer on model training. We selected the vocabulary sizes of 512, 1024, and 2048, and reported the model performances on the Humanoid-X dataset. As illustrated in Fig. 9, increasing the vocabulary size up to 2048 leads to an improvement in FID metric from 0.539 to 0.463 and brings an improvement in Diversity metric from 6.050 to 6.149. This indicates that increasing the number of motion primitives learned in the action tokenizer results in more diverse humanoid motion generation. Due to the limited computational resources, we didn't try a larger vocabulary. We will leave this for future works.\nAblation study on the model architecture. A key consideration for generation tasks is selecting the appropriate model architecture, such as the Transformer or diffusion model. To explore this, we trained a text-controlled humanoid motion diffusion model on the Humanoid-X dataset and compared its performance with the original Transformer-based UH-1 model. The results in Tab. 4 show that the Transformer architecture used in UH-1 is more scalable to large-scale training data and achieves better performance, with a lower FID and MM Dist score compared to the diffusion-based model."}, {"title": "6. Conclusion", "content": "We introduce Humanoid-X, a large-scale dataset that facilitates scalable humanoid robot learning from massive videos. On top of Humanoid-X, we trained a large humanoid model, UH-1, for generalizable humanoid pose control based on language commands. Extensive experiments demonstrate that scalable training enables UH-1 to generate generalizable and reliable humanoid actions following language commands, and the UH-1 model can be effectively deployed on the real humanoid robot.\nLimitations. In this paper, we only study the humanoid pose control. Humanoid manipulation is not in the scope of this paper. In future works, we plan to investigate learning humanoid loco-manipulation from Internet videos."}, {"title": "Appendix", "content": "A Ethics Statement\nThis paper presents Humanoid-X, a large scale dataset that facilitates scalable humanoid robot learning from massive videos, and UH-1, a large humanoid model for generalizable humanoid pose control based on language commands. The Internet videos that Humanoid-X and UH-1 involve in the dataset and the pipeline are strictly for academic research and are not intended for commercial use. On the privacy protection side, we apply face anonymization to all human subjects in the Internet videos involved in Humanoid-X and UH-1, making sure that the videos do not include any personal information. In addition, we will not release the original Internet videos to protect copyright. In summary, we believe that Humanoid-X and UH-1 do not raise ethical concerns.\nB. Details on Humanoid-X Data Collection\nIn this section, we will introduce more details on the whole data collection pipeline of the Humanoid-X dataset, including data source distribution, video mining and processing, video captioning, 3D human pose estimation, motion retargeting from humans to humanoid robots, and the goal-conditioned humanoid control policy.\nB.1. Data Source Distribution\nHumanoid-X consists of massive motion samples with diverse sources, and the detailed source of the data in our Humanoid-X dataset is shown in Tab. 1. Humanoid-X consists of 163.8K motion samples, spanning 240.3 hours of video footage, containing 20.7M frames of human and robotic motion data, with a vocabulary size of 3206 words. Each motion video sample is expanded to the 5 data modalities (V,T, Phuman, Probot, Arobot) of the motion sample in our Humanoid-X dataset. The subsections below introduce details on the dataset building and data processing pipeline.\nB.2. Video Mining and Processing\nTo collect a dataset of videos featuring single-person movements, we first designed specific motion categories and then generated search prompts based on these categories. Using the phrase \"single person\" in searches often produced irrelevant results since the majority of the video titles would not specify whether the video is single person using the exact word \"single person\". So, activity-based terms were created to ensure relevant data retrieval. These categories included martial arts tutorials, fitness and exercise drills, sports techniques, dance practice, music performance tutorials, everyday movement patterns, animal-inspired movements, and rehabilitation exercises.\nMartial arts tutorials included search terms for techniques, drills, and demonstrations across disciplines like Wushu, Taekwondo, Karate, and Kung Fu. Examples of generated terms are \"karate front kick training,\" \"taekwondo spinning hook kick demonstration,\" and \"wushu staff spin practice.\" Fitness and exercise drills focused on isolated movements like \"yoga handstand practice,\" and \"calisthenics planche progression tutorial,\u201d.\nSports techniques targeted individual actions in activities like baseball, tennis, archery, running, and parkour, with examples including \u201ctennis serve technique tutorial\" and \"running stride form analysis.\u201d Dance practice emphasized solo routines in styles such as salsa, hip hop, ballet, modern dance, and improvisation, using terms like \"salsa basic turn solo\" and \"ballet arabesque demonstration.\" Music performance tutorials captured movements involved in playing instruments such as guitar, violin, piano, and drums, with terms like \"guitar strumming while standing solo\" and \"violin bowing technique while standing demonstration.\"\nEveryday movement patterns focus on practical motions during daily activities, using terms like \u201cpicking up an object while balancing,\" \"loading a dishwasher with proper form,", "squatting to tie shoelaces,\u201d. Animal-inspired movements were included to capture dynamic motion patterns with terms like \"bear crawl coordination movement,\" \"frog jump exercise,\u201d and \u201cflamingo balance on one leg.": "ehabilitation and mobility exercises targeted balance, flexibility, and strength, focusing on slow and deliberate movements such as \"dynamic torso twist warm-up\" and \"hip flexor stretch technique breakdown.\"\nBy designing categories and generating search terms from these, we ensured the collected videos focused on single-person movements while covering a wide range of activities.\nAfter collection of videos from the designed searching prompts, we designed a pipeline for detecting and extracting video segments featuring single-person movements. The process begins with the YOLOv8 model [50", "person\". Frames containing exactly one detected person are selected, ensuring the focus remains solely on single-person actions. Once a single-person frame is identified, a region of interest (ROI) is extracted using the bounding box of the detected individual from YOLOv8 detection result. To determine existence of motion, the pipeline calculates frame-to-frame differences in the grayscale ROI, assessing movement levels using predefined thresholds. This ensures only frames with significant motion are retained, while static or irrelevant segments are excluded.\nTo further refine the selection regarding motion, the pipeline employs a batch-based filtering process, analyzing sequences of frames to identify consistent motion patterns over time. Small movement threshold is applied to frame-to-frame and a larger threshold is applied to the frame batch, enabling the detection of subtle and significant activities by allowing relative small motions for several frames as long as large motion is detected in frames' batch. Such design would benefit continuity of the clips by keeping frames in between large motions. Frames that meet these criteria are grouped into chunks representing continuous motion, and only chunks exceeding a minimum duration are considered for clip generation.\nThe output clips are processed to maintain consistent quality and playback speed. Frames within each chunk are down-sampled for efficiency, interpolated for smooth transitions, and standardized to 20 FPS. The resulting clips focus exclusively on single-person actions, discarding distractions such as multiple individuals or irrelevant frames. This approach ensures a precise and diverse dataset of single-person motion segments, suitable for applications in motion analysis, action recognition, and training of computer vision models. By integrating object detection, motion analysis, and sequence processing, the pipeline achieves high accuracy and relevance in isolating meaningful single-person movements.\nB.3. Video Captioning\nFor video captioning, we implemented a video captioning pipeline using Video LLaMA [10": "with a video processing framework which extracts visual information from input videos by sampling a fixed number of eight frames at regular intervals.\nThe prompts used for video captioning are designed to produce concise and action-focused descriptions. The main prompt directs the model to describe the actions of a person in the video in a single sentence, explicitly avoiding mentions of the person's appearance. We used the query \"Please describe what the human is doing in the video in one sentence.\" with guidance of rules shown above. Such a query would guarantee a concise description of motion without any irrelevant information being collected.\nB.4. 3D Human Pose Estimation\nThe SMPL generation pipeline is designed to estimate 3D human pose and shape parameters from video frames. This process involves several key steps, including detecting the subject in video frames, estimating pose and shape parameters, and generating a 3D mesh representation. VIBE model [27"}]}