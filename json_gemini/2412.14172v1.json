[{"title": "Learning from Massive Human Videos for Universal Humanoid Pose Control", "authors": ["Jiageng Mao", "Siheng Zhao", "Siqi Song", "Tianheng Shi", "Junjie Ye", "Mingtong Zhang", "Haoran Geng", "Jitendra Malik", "Vitor Guizilini", "Yue Wang"], "abstract": "Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through a comprehensive pipeline: data mining from the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train a large humanoid model, UH-1, which takes text instructions as input and outputs corresponding actions to control a humanoid robot. Extensive simulated and real-world experiments validate that our scalable training approach leads to superior generalization in text-based humanoid control, marking a significant step toward adaptable, real-world-ready humanoid robots.", "sections": [{"title": "1. Introduction", "content": "Scalability is crucial in deep learning. Recent advances in computer vision have demonstrated that scaling up training data leads to more powerful foundation models for visual recognition [26, 41, 44] and generation [3, 51]. In robotics, researchers follow a similar paradigm and build foundation models for robotic manipulation [4, 5, 24, 40] by collecting massive robotic demonstrations. Nevertheless, in contrast to images and videos that are abundant and easily accessible, collecting large-scale robotic demonstrations is expensive and time-consuming, which limits the scalability of current robot learning methods. This raises the question: Can we use videos as demonstrations to improve the scalability of robot learning?\nTo address this challenge, many efforts have been made, such as learning affordances [2, 15, 28], flows [67, 69], and world models [68] from natural videos, which enable more generalizable robotic manipulation. However, when it comes to humanoid robots, learning such action representations from videos remains an open problem. Unlike robotic arms, humanoid robots have distinct kinematic structures and more degrees of freedom (DoFs), making them harder to control. Existing works [8, 9, 16, 17, 30, 47, 48] leverage large-scale reinforcement learning to learn robust humanoid control policies, but they only focus on limited robotic skills such as locomotion or jumping, making them less generalizable for handling everyday tasks. Other works [13, 19, 20, 53] control humanoid robots through teleoperation, but they require human labor to collect robotic data, which is less scalable. In contrast to these previous works, learning a universal action representation from massive videos will greatly improve the scalability of humanoid robot learning and enable more generalizable humanoid pose control.\nTo bridge this gap in humanoid robot learning, we introduce Humanoid-X, a large-scale dataset curated from a massive and diverse collection of videos for universal humanoid pose control. Humanoid-X utilizes natural language as an interface to connect human commands and humanoid actions, so humans can talk to their humanoid robots to control their actions. The natural language representations are extracted from videos via captioning tools and are used to describe the actions of humanoid robots. For action representations, Humanoid-X leverages both robotic keypoints for high-level control and robotic target DoF positions for direct position control. To extract humanoid actions from human videos, we first reconstruct 3D humans and their motions from videos. Then, we leverage motion retargeting to transfer motions from 3D humans to humanoid robots, resulting in robotic keypoints for high-level humanoid pose control. Finally, we learn a universal RL-based control policy that maps keypoints to low-level humanoid target DoF positions that can be deployed in real robots. We collect over 160,000 human-centric videos from academic datasets and the Internet, covering diverse action categories. We further transform these videos into text-action pairs, resulting in over 20 million humanoid actions with corresponding text descriptions. Humanoid-X paves the way for developing more generalizable and scalable humanoid robotic control guided by natural language.\nOn top of the Humanoid-X dataset, we further investigate how to learn a universal humanoid pose control model using large-scale text-action pairs. We introduce Universal Humanoid-1 (UH-1), a large humanoid model for universal language-conditioned humanoid pose control. UH-1 leverages the scalability of the Transformer architecture to handle vast amounts of data efficiently. We begin by discretizing 20 million humanoid actions into action tokens, creating a vocabulary of motion primitives. Then, given a text command as input, the Transformer model auto-regressively decodes a sequence of these tokenized humanoid robotic actions. For cases where the action representation involves robotic keypoints, we transform these into robotic DoF positions using an additional action decoder. Finally, we utilize a proportional-derivative (PD) controller to convert the DoF positions into motor torques, enabling us to control humanoid robots and deploy them in the real-world.\nTo validate the effectiveness of the Humanoid-X dataset and the UH-1 model, we conducted extensive experiments across both simulated and real humanoid platforms. Our results reveal that leveraging vast amounts of video data enables our model to seamlessly translate textual commands into diverse and contextually accurate humanoid actions. Notably, the UH-1 model demonstrates strong robustness, proving reliable in real-world deployment. To summarize, our key contributions are as follows:\nWe introduce Humanoid-X, a pioneering large-scale dataset tailored for learning universal humanoid control from massive Internet video data.\nWe introduce UH-1, a powerful, scalable model for language-conditioned control of humanoid poses. Our approach supports two flexible control modes that are interchangeable, depending on task requirements. We also provide extensive ablation study for our design choices.\nOur experiments confirm that training on massive video data enables a level of generalizability in humanoid control that was previously unattainable."}, {"title": "2. Related Works", "content": "Robot Learning from Internet Data. Many endeavors have been made to learn scalable robot learning policies from non-robotic data, especially Internet videos. The key idea is to learn valuable representations from massive visual data and transfer them to robotic tasks. The learned representations include pre-trained visual features from videos [36, 39, 46, 65] and transferable action representations such as affordances [1, 2] and object-centric flows [67, 69]. Other works [12, 38, 68] attempt to learn world models from Internet videos. However, most of these works focus on robotic manipulation. Since robot arms have totally different kinematic structures from humanoid robots, the learned visual and action representations for robotic manipulation are not transferable to humanoid robot control. In contrast, we investigate how to learn universal pose control for humanoid robots from massive videos.\nHumanoid Robot Learning. Extensive work has been dedicated to learning policies that enable robust control of humanoid robots. Some works focus on humanoid locomotion using large-scale reinforcement learning [8, 16, 17, 30, 48] or imitation learning [49, 57]. Other works learn humanoid manipulation via imitation learning [29, 71]. Notably, some works [9, 13, 19-21] learn humanoid teleoperation by transferring motions from 3D humans to humanoid robots. However, these works rely on well-calibrated motion capture data, limiting their generalization ability to un-"}, {"title": "3. Humanoid-X Dataset", "content": "3.1. Overview\nTo scale up humanoid robot learning using massive human videos, we introduce Humanoid-X, the largest humanoid robot dataset to date compiled from a vast and diverse collection of videos for universal humanoid pose control. Humanoid-X consists of 163,800 motion samples covering a comprehensive set of action categories. Each motion sample in the dataset contains 5 data modalities: an original video clip V, a text description T of the action in the video, a sequence of SMPL [33]-based human poses $P_{human}$ estimated from the video, a sequence of humanoid keypoints $P_{robot}$ for high-level robotic control, and a sequence of humanoid actions $A_{robot}$ representing target DoF positions for low-level robotic position control. Humanoid-X encompasses over 20 million frames, totaling approximately 240 hours of data. Beyond its extensive scale across multiple data modalities, which is essential for scalable humanoid policy training, Humanoid-X also features a large and diverse text-based action vocabulary, as shown in Fig. 3 (c). This diversity supports universal and text-conditioned humanoid pose control. In the next section, we will discuss how to obtain these motion samples (V,T, $P_{human}$, $P_{robot}$, $A_{robot}$) from massive videos.\n3.2. Learning from Massive Videos\nTo process large-scale, in-the-wild raw video data, we developed a fully automated data annotation pipeline comprising five modules, as illustrated in Fig. 2. The pipeline includes (1) a video processing module that mines and extracts video clips V from noisy Internet videos, (2) a video captioning model that generates text description of human actions T, (3) a human pose detection module that estimates parametric 3D human poses $P_{human}$ from video clips, (4) a motion retargeting module to generate humanoid robotic keypoints $P_{robot}$ by transferring motions from humans to humanoid robots, and (5) a goal-conditioned reinforcement learning policy to learn physically-deployable humanoid actions $A_{robot}$ by imitating humanoid keypoints.\nVideo Mining and Processing. The first step of our approach is to collect a large number of human-centric videos that encompass a wide variety of action types. To this end, we mine massive informative video clips from 3 sources: academic datasets for digital human research [6, 11, 18, 32, 56, 61, 75], datasets for video action understanding [7, 55], and Internet videos from YouTube. To collect Internet videos, we designed over 400 unique search terms covering a range of human activities from daily tasks to professional sports, and then utilized the Google Cloud API* to retrieve the top 20 videos for each specified search term.\nOriginal videos are often noisy, including segments with no humans, multiple humans, or a stationary individual, which makes them unsuitable for humanoid control. To obtain meaningful video clips, we begin by downsampling each video to a standardized 20 frames per second (FPS) to ensure consistency across the dataset. Next, we employ an object detector [50] for single-human detection, selecting frames with precisely one visible person. Following detection, we apply motion detection by calculating the pixel-wise grayscale difference between consecutive frames to keep frames showing significant movement. We then compile sequences of at least 64 consecutive frames that satisfy the above single-human motion criterion into video clips, resulting in 163,800 video clips V in total.\nVideo captioning. Language bridges human commands and humanoid actions. To associate humanoid actions with semantic meaning and enable language-conditioned humanoid control, we employ a video captioning model [10] to generate fine-grained action descriptions T from videos:\nT = $F_{caption}(V)$,\nwhere $F_{caption}$ is the video captioning model. To avoid irrelevant text descriptions, we carefully design prompts to guide the model to describe human actions instead of physical appearance, resulting in action-centric text descriptions.\n3D Human Pose Estimation. Humanoid robots inherently share a similar skeleton with humans, which allows for learning control policies for humanoid robots based on human motion data. To this end, we first need to extract human poses from videos. To accurately track and estimate human poses in video clips, we adopt a video-based 3D human parametric model estimator [27], which estimates SMPL [33]-based humans and camera parameters for each frame. We further extract global human motions, i.e., root translations, using the estimated camera parameters. The process can be formulated as:\n$P_{human} (\\beta, \\theta, t_{root} ) = F_{pose}(V)$,\nwhere $F_{pose}$ is the human pose estimation model. Finally, we obtain per-frame 3D human pose: $P_{human} (\\beta, \\theta, t_{root})$, where $\u03b2$ controls the human shapes, \u03b8 controls the joint rotations, and $t_{root}$ controls the global root translations.\nMotion Retargeting from Humans to Humanoid Robots. Since humans and humanoid robots have similar skeletons, we can track the human joint positions across frames and map them to the corresponding joints in a humanoid robot, resulting in humanoid keypoints $P_{robot}$ for high-level control. In particular, we chose 12 joints that exist in both humans and humanoid robots: left and right hips, knees, ankles, shoulders, elbows, and wrists. The joint positions $P_{joints}$ can be obtained via forward kinematics $F_{fk}$:\n$P_{joints} = F_{fk}(P_{human} (\\beta, \\theta, t_{root}))$.\nSince humans have different shapes from humanoid robots, following [20], we first optimize the human shape parameters $\u03b2$ to ensure that resized human shapes closely resemble those of a humanoid robot. Specifically, we first obtain joint positions in the humanoid robot under a standard T-shaped pose: $P_{robot}^T$. Then, under the same T-shaped pose, we optimize $\u03b2$ to make human joint positions $P_{joints}^T$ the same as the corresponding humanoid joint positions $P_{robot}^T$:\n$\\min_{\\beta} ||P_{joints}^T - P_{robot}^T||^2$,\ns.t. $P_{joints}^T = F_{fk} (P_{human} (\\beta, \\theta^T , t_{root}))$,\nwhere $\u03b8^T$ denotes the standard T pose. For each frame of human pose, we replace the original $\u03b2$ with the optimal $\u03b2'$ in $P_{human}$, and following Eq. 3 we can obtain the adjusted joint positions $P'_{joints}$. Finally, we directly set humanoid robotic keypoints as the adjusted human joint positions:\n$P_{robot} := P'_{joints}$"}, {"title": "4. UH-1 for Universal Humanoid Pose Control", "content": "Learning from massive videos enables us to distill a universal humanoid pose control policy from large-scale motion samples (V,T, $P_{human}$, $P_{robot}$, $A_{robot}$). We introduce UH-1, a large language-conditioned humanoid model that takes natural language commands T and generates corresponding humanoid robotic actions {$P_{robot}$, $A_{robot}$} :\n$\\pi_{UH-1} : T \\mapsto \\{P_{robot} , A_{robot} \\}$,\nwhere $\u03c0_{UH\u22121}$ denotes the UH-1 model. Notably, as illustrated in Fig. 5, our model can either generate high-level humanoid keypoints $P_{robot}$, which are then fed into the goal-conditioned policy \u03c0 to control the humanoid robot in closed-loop, or generate robotic actions $A_{robot}$ for direct open-loop control. Our model bridges the gap between semantic language commands and physically deployable robotic actions, enabling more generalizable humanoid robotic control using text instructions. For simplicity, in the following section, we use $A_{robot}$ as an example to illustrate our method; $P_{robot}$ can be generated in the same manner.\nWe adopt the Transformer [63] as our main model architecture due to its scalability to large-scale data. As shown in Fig. 4, to enable efficient learning, we first train an action tokenizer using [62] to discretize humanoid motions into a vocabulary of action tokens. Then, we train the Transformer to auto-regressively decode action tokens, resulting in executable humanoid actions."}, {"title": "5. Experiments", "content": "In this section, we conduct extensive experiments to investigate the following research questions: (1) Universal Pose Control with UH-1: Does our UH-1 model enable universal humanoid robot pose control based on text commands? (2) Scalability and Generalization with Humanoid-X: Does the large-scale Humanoid-X dataset facilitate scalable training and improve the generalization ability of our UH-1 model? (3) Real-World Deployment of UH-1: Can our UH-1 model be deployed on real humanoid robots to enable reliable robotic control in real-world environments?\n5.1. Universal Humanoid Pose Control with UH-1\nWe conduct extensive experiments to validate the generalization ability of the UH-1 model. An alternative solution to text-to-humanoid action generation is a two-stage pipeline: generating 3D human motions first and then retargeting the human motions to humanoid robots. To this end, we compare our method with two important baselines for text-to-human motion generation: Motion Diffusion Model (MDM) [59] and Text-to-Motion GPT (T2M-GPT) [73]. For fair comparisons, We choose the commonly used HumanML3D [18] benchmarks and transform the humans in this dataset into humanoid robots, resulting in a new benchmark called HumanoidML3D. Similarly, we adopt the same motion retargeting method as in this paper to transform the human motions generated by the baselines into humanoid actions. We adopt the metrics in [18] to evaluate the humanoid motions from different aspects: (1) Quality: The Frechet Inception Distance (FID) evaluates the dissimilarity between feature distributions of generated and ground truth humanoid poses. (2) Diversity: The Diversity metric evaluates the variability within the generated humanoid pose distribution, calculated as the average Euclidean distance between 300 randomly sampled pairs of humanoid poses. (3) Reliability: The Multi-modal Distance (MM Dist) measures the Euclidean distance between motions and corresponding texts, and the R Precision assesses the accuracy of text and humanoid pose matches in the Top 3 rankings.\nTab. 1 shows the results of our UH-1 model compared against the baselines. The results indicate that UH-1 attains the highest performance across nearly all metrics, showing an over 23% reduction in the critical FID metric, while also maintaining comparable performance on the Diversity metric. The first-order similarity loss proposed in this paper greatly enhances the quality and reliability of the generated outputs. The results suggest that UH-1 is a streamlined model and performs better than the two-stage methods.\n5.2. Scalable Learning with Humanoid-X\nIn this section, we investigate whether scaling up training data with the large-scale Humanoid-X dataset can improve the generalization ability of our model. To explore this, we first pre-trained our UH-1 model on the Humanoid-X dataset and then finetuned and evaluated the performance on the HumanoidML3D benchmark. Tab. 2 shows the performance comparison with training only on HumanoidML3D. We found that pre-training on the Humanoid-X dataset greatly improves the quality, reliability, and diversity of humanoid actions, with an FID improvement from 0.445 to 0.379, a MM Dist score improvement from 3.249 to 3.232, and a Diversity improvement from 10.157 to 10.221.\nIn addition, we also study how scaling up training data affects the model performance. To this end, we train our UH-1 model on varying proportions of the Humanoid-X dataset, specifically 1%, 10%, 25%, 50%, 75%, and 100%. The results shown in Fig. 7 indicate that scaling up training data from 1% to 100% leads to a significant performance improvement in all metrics (FID from 0.689 to 0.463 and Diversity from 5.900 to 6.149). This suggests that by learning from massive videos, we successfully scale up the training data of humanoid robots and attain better performance.\n5.3. Real-World Deployment of UH-1\nTo investigate whether our UH-1 model, trained on the Humanoid-X dataset, can generate reliable humanoid actions that are physically deployable on humanoid robots, we designed 12 distinct language commands, as shown in Tab. 3, and evaluated them on a real humanoid robot. We use UNITREE H1-2\u2020 as our test embodiment. For the experiments, we evaluated each language command 10 times and controlled the robot in different places. Notably, for text-to-humanoid actions, we found that open-loop control can only work for upper-body control, so in this control mode, we use a pre-trained locomotion policy for controlling the lower-body of the humanoid robot. Fig. 6 shows the demos of real-robot experiments. Tab. 3 measures the task success rate for each language command. Our experimental results demonstrate that our UH-1 model can be reliably deployed on the real humanoid robot, achieving a success rate of nearly 100% across all evaluated language instructions."}, {"title": "6. Conclusion", "content": "We introduce Humanoid-X, a large-scale dataset that facilitates scalable humanoid robot learning from massive videos. On top of Humanoid-X, we trained a large humanoid model, UH-1, for generalizable humanoid pose control based on language commands. Extensive experiments demonstrate that scalable training enables UH-1 to generate generalizable and reliable humanoid actions following language commands, and the UH-1 model can be effectively deployed on the real humanoid robot."}, {"title": "Appendix", "content": "A Ethics Statement\nThis paper presents Humanoid-X", "single person\" in searches often produced irrelevant results since the majority of the video titles would not specify whether the video is single person using the exact word \"single person\". So, activity-based terms were created to ensure relevant data retrieval. These categories included martial arts tutorials, fitness and exercise drills, sports techniques, dance practice, music performance tutorials, everyday movement patterns, animal-inspired movements, and rehabilitation exercises.\nMartial arts tutorials included search terms for techniques, drills, and demonstrations across disciplines like Wushu, Taekwondo, Karate, and Kung Fu. Examples of generated terms are \"karate front kick training,\" \"taekwondo spinning hook kick demonstration,\" and \"wushu staff spin practice.\" Fitness and exercise drills focused on isolated movements like \"yoga handstand practice,\" and \"calisthenics planche progression tutorial,\u201d.\nSports techniques targeted individual actions in activities like baseball, tennis, archery, running, and parkour, with examples including \u201ctennis serve technique tutorial\" and \"running stride form analysis.\u201d Dance practice emphasized solo routines in styles such as salsa, hip hop, ballet, modern dance, and improvisation, using terms like \"salsa basic turn solo\" and \"ballet arabesque demonstration.\u201d Music performance tutorials captured movements involved in playing instruments such as guitar, violin, piano, and drums, with terms like \"guitar strumming while standing solo\" and \"violin bowing technique while standing demonstration.\u201d\nEveryday movement patterns focus on practical motions during daily activities, using terms like \u201cpicking up an object while balancing,\" \u201cloading a dishwasher with proper form,\" and \u201csquatting to tie shoelaces,\u201d. Animal-inspired movements were included to capture dynamic motion patterns with terms like \"bear crawl coordination movement,\" \"frog jump exercise,\u201d and \u201cflamingo balance on one leg.\u201d Rehabilitation and mobility exercises targeted balance, flexibility, and strength, focusing on slow and deliberate movements such as \"dynamic torso twist warm-up\" and \"hip flexor stretch technique breakdown.\"\nBy designing categories and generating search terms from these, we ensured the collected videos focused on single-person movements while covering a wide range of activities.\nAfter collection of videos from the designed searching prompts, we designed a pipeline for detecting and extracting video segments featuring single-person movements. The process begins with the YOLOv8 model [50": "which detects objects in each frame and identifies detected humans based on the class label corresponding to \"person\". Frames containing exactly one detected person are selected", "10": "with a video processing framework which extracts visual information from input videos by sampling a fixed number of eight frames at regular intervals.\nThe prompts used for video captioning are designed to produce concise and action-focused descriptions. The main prompt directs the model to describe the actions of a person in the video in a single sentence", "Please describe what the human is doing in the video in one sentence.\" with guidance of rules shown above. Such a query would guarantee a concise description of motion without any irrelevant information being collected.\nB.4. 3D Human Pose Estimation\nThe SMPL generation pipeline is designed to estimate 3D human pose and shape parameters from video frames. This process involves several key steps, including detecting the subject in video frames, estimating pose and shape parameters, and generating a 3D mesh representation. VIBE model [27": "is used to infer SMPL parameters", "as": "n$t_z = \\frac{f"}, {"as": "n$T_{root"}, "begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix}$  (2)\nwhere t = (tx,ty) corresponds to the 2D translations from the camera parameters, and tz is the computed depth.\nB.5. Motion Retargeting\nOur motion retargeting process mainly consists of two tasks: the optimization of human shape parameters \u03b2 to fit human shapes to those of a humanoid robot, and solve the humanoid motor DoF positions qrobot from adjusted human joint positions with inverse kinematics.\nOptimization of human shape parameters \u03b2. Given the forward kinematics of human body models in Eq. (3), we optimize the human shape parameters \u03b2 with the Adam optimizer [25"], "L(\\beta)$": "n$L(\\beta) = ||P_{joints"}, {"theta^T": "t_{root"}, {"\u03b2": "n$\\forall i \\in \\{1", "L_{ik}$": "n$L_{ik"}, {"robot}": "S_{root"}, {"q_{robot}": "S_{root"}, {"L_S$": "n$L_s(q_{robot}) = \\sum_{i=1}^{n-2} (2q_{robot}[i] - q_{robot}[i - 1] - q_{robot}[i + 1])$, (8)\nwhere n is the number of frames of one motion sample trajectory, with the index ranging from 0 to n \u2212 1. We use the Adam optimizer [25] to solve the inverse kinematics problem, where the weight of smoothing term \u03bb = 0.05.\nB.6. Goal-conditioned Control Policy\nWe use massively parallel simulation to train our goal-conditioned humanoid RL control policy with Isaac Gym. In this subsection, we will introduce our training data, our policy, our training rewards and training parameters.\nTraining Data. We selectively used a portion of the CMU MoCap dataset in AMASS [37], in the form of SMPL models. We exclude motions that involve physical interactions with others, heavy objects, or rough terrain. We retarget from the training data to humanoid robot motion with the method introduced above, including humanoid keypoint joint positions $P_{robot}$, humanoid robot DoF positions $q_{robot}$ and humanoid robot root states $S_{root}$. We can estimate the corresponding linear or angular velocities of humanoid DoFs and humanoid root joint from the humanoid motion data across frames.\nRL Control Policy. Our goal is to track the root movement goal for the whole body and the target expression goal for upper body, and our training data is introduced above. The humanoid control policy is defined with Eq. (8). The goal space can be formulated as $\\mathbb{G} = \\mathbb{G}_e \\times \\mathbb{G}_m$, where $\\mathbb{G}_e$ includes joint angles and keypoint translations from the retargeting process above and the goal space for robot movement control $\\mathbb{G}_m = (v, rpy, h)$ where v \u2208 $\\mathbb{R}^3$ is the linear velocity, rpy \u2208 $\\mathbb{R}^3$ is the robot pose in terms of row/pitch/yaw and h is the body height. The observation $\\mathbb{O}$ includes robot proprioception information $o_t = [\\omega_t, r_t, p_t, \\Delta y, q_t, \\dot{q}_t, a_{t-1}]^T$ where $\\omega_t$ is robot root angular velocity, rt, pt is roll and pitch, $\\Delta y = y_t - \\widehat{y}$ is the difference between current and desired yaw angle, qt and $\\dot{q}_t$ is the joint position and angular velocity and at \u2208 $\\mathbb{R}^{27}$ is the target position of the joint proportional-derivative (PD) controllers.\nTraining Rewards. In each step, the reward from the environment consists of motion rewards, root tracking rewards and regularization terms. To protect the fragile ankle roll joints on the robot hardware, we set the actions of the two joints to zero every simulation step. Motion rewards include DoF position reward and keypoint position reward, and root tracking rewards include root linear velocity reward, root roll & pitch reward and root yaw reward.\nThe imitation rewards, including motion rewards and root tracking rewards, are listed in Tab. 2, where qtar, q \u2208 $\\mathbb{R}^9$ are the target and actual upper body DoF positions, ttar, t \u2208 $\\mathbb{R}^{18}$ are the target and actual upper body keypoint positions, $V_{tar}$, v \u2208 $\\mathbb{R}$ are the target and actual root velocity, $\\Omega_{tar}$, $\\Omega_{\\Phi \\Theta}$ are the target and actual body roll and pitch.\nThe regularization rewards are listed in Tab. 3, where $h_{feet}$ is feet height, $t_{air}$ denotes the duration for which each foot remains in the air, $1_{new contact}$ means new foot contact with the ground, $F_f, F^z_f, F_{th}$ are foot contact force in horizontal plane and along the z-axis, and the contact force threshold respectively, $\\dot{q}, \\ddot{q}$ are joint velocity and acceleration, at is action at timestep t, $1_{collision}$ denotes self-collision, $q_{max}, q_{min}$ are limits for joint positions, and gxy is gravity vector projected on horizontal plane.\nTraining Parameters. We use PPO with hyperparameters listed in Tab. 4 to train the policy.\nC. Details on Humanoid-X Dataset\nIn this section, we will introduce the Humanoid-X dataset. We will introduce the data format and structure and show several examples of the dataset.\nC.1. Data Format and Structure\nFor each motion sample in Humanoid-X, we expand them to"}]