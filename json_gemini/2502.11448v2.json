{"title": "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection", "authors": ["Weidi Luo", "Shenghong Dai", "Xiaogeng Liu", "Suman Banerjee", "Huan Sun", "Muhao Chen", "Chaowei Xiao"], "abstract": "The rapid advancements in Large Language Models (LLMs) have enabled their deployment as autonomous agents for handling complex tasks in dynamic environments. These LLMs demonstrate strong problem-solving capabilities and adaptability to multifaceted scenarios. However, their use as agents also introduces significant risks, including task-specific risks, which are identified by the agent administrator based on the specific task requirements and constraints, and systemic risks, which stem from vulnerabilities in their design or interactions, potentially compromising confidentiality, integrity, or availability (CIA) of information and triggering security risks. Existing defense agencies fail to adaptively and effectively mitigate these risks. In this paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent safety, which features adaptive safety check generation, effective safety check optimization, and tool compatibility & flexibility. Extensive experiments demonstrate that AGrail not only achieves strong performance against task-specific and system risks but also exhibits transferability across different LLM agents' tasks.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Model (LLM) powered agents have demonstrated remarkable capabilities in tackling complex tasks in our daily life (Liu et al., 2024a; Zheng et al., 2024a; Zhou et al., 2024; Xie et al., 2024; Mei et al., 2024a; Hua et al., 2024a; Lin et al., 2024; Zhang et al., 2024a; Mei et al., 2024b; Gu et al., 2024a), as well as in specialized fields such as chemistry (Yu et al., 2024; Bran et al., 2023; Boiko et al., 2023; Ghafarollahi and Buehler, 2024) and healthcare (Abbasian et al., 2024; Shi et al., 2024; Yang et al., 2024; Tu et al., 2024; Li et al., 2024). LLM agents generate instructions (e.g., code) as actions to interact with the environment, enabling them to complete specific tasks effectively (Yao et al., 2023).\nMore advanced LLM agents (Zhu et al., 2023b,a; Park et al., 2023; Shinn et al., 2023) are equipped with memory capabilities, enabling them to store information gathered from the environment and utilize memory to inform and enhance future actions (Wang et al., 2024).\nMeanwhile, recent studies (He et al., 2024) have shown that LLM agents fail to adequately consider their potential vulnerabilities in different real-world scenarios. Generally, the risks of an LLM agent can be categorized into two groups illustrated in Figure 1 : Task-specific risks refer to risks explicitly identified by the agent administrator based on the agent's intended objectives and operational constraints within a given task. For example, according to the guard request of the EICU-AC dataset, these risks include unauthorized access to diagnostic data and violations of privacy regulations (Xiang et al., 2024). Systemic risks arise from vulnerabilities in an LLM agent's interactions, potentially compromising confidentiality, integrity, or"}, {"title": "2", "content": "safety checks in response to manually specific trusted contexts. Effective Safety Check Optimization: Our framework iteratively refines its safety checks to identify the optimal and effective set of safety checks for each type of agent action during test-time adaptation (TTA) by two cooperative LLMs. Tool Compatibility & Flexibility: In addition to leveraging the internal reasoning ability for guardrail, our framework can selectively invoke customized auxiliary tools to enhance the checking process of each safety check. These tools may include environment security assessment tools to provide an environment detection process.\nWe evaluate AGrail with a focus on real-world agent outputs, rather than LLM-generated synthetic environments and agent outputs (Zhang et al., 2024b). Our evaluation includes task-specific risks described in the Mind2Web-SC and EICU-AC datasets (Xiang et al., 2024), as well as systemic risks such as prompt injection attacks from AdvWeb (Xu et al., 2024) and EIA (Liao et al., 2025). Furthermore, we constructed the Safe-OS benchmark, which consists of three attack scenarios carefully designed to assess the robustness of online OS agents against systemic risks. To ensure a comprehensive evaluation, Safe-OS also includes benign data to assess the impact of defenses on normal task performance. In our main experiment, AGrail demonstrates strong performance. Using Claude-3.5-Sonnet, our framework preserved 96% of benign actions while achieving 0% Attack Success Rate (ASR) against prompt injection. It reduced ASR to 3.8% and 5% for environmental and system sabotage on Safe-OS, 0% ASR on AdvWeb, and averaged 17% ASR on EIA across action generation and action grounding. AGrail establishes universal safety principles that adapt to diverse tasks while ensuring effective risk detection, serving as a strong guardrail for future LLM agent security research."}, {"title": "Related Work", "content": "LLM-based Agent An LLM agent is an autonomous system that follows language instructions to perform complex tasks using available tools (Su, 2023). Pilot studies have explored applications across domains like chemistry (Yu et al., 2024; Boiko et al., 2023; Ghafarollahi and Buehler, 2024), healthcare (Abbasian et al., 2024; Shi et al., 2024; Yang et al., 2024), and daily life (Liu et al., 2024a; Zheng et al., 2024a; Zhou et al., 2024; Gou et al., 2024; Gu et al., 2024b). The memory module"}, {"title": "3 Safe-OS", "content": "In this section, we will introduce our motivation behind creating the Safe-OS benchmark and provide an overview of its data and associated risk types.\n3.1 Motivation\nThe development of Safe-OS is motivated by two key challenges: (1) Risk evavluation in online execution setting of LLM agents. As intelligent assistants, LLM agents autonomously interact with environments in real-world applications, making real-time evaluation of their security crucial. However, existing benchmarks (Zhang et al., 2024b, 2025) primarily rely on LLM-generated data, which often includes test cases that do not fully reflect real-world scenarios. This gap highlights the need for a benchmark that accurately assesses LLM agents' safety in dynamic and realistic environments. (2) The challenge of environment-dependent malicious actions. Unlike explicit attacks (Zeng et al., 2024; Yuan et al., 2024a; Xiang et al., 2024; Liu et al., 2024b; Xu et al., 2024; Liao et al., 2025; Li and Liu, 2024; Debenedetti et al., 2024) that can be classified as harmful without additional context, certain actions appear benign but become dangerous depending on the agent's environment. These"}, {"title": "3.2 Overview of Safe-OS benchmark", "content": "Considering the complexity of the OS environment and its diverse interaction routes\u2014such as process management, user permission management, and file system access control-OS agents are exposed to a broader range of attack scenarios. These include Prompt Injection Attack: Manipulating information in environment to alter the agent's actions, leading it to perform unintended operations (e.g., modifying agent output). System Sabotage Attack: Directing the agent to take explicitly harmful actions against the system (e.g., corrupting memory, damaging files, or halting processes). Environment Attack: An attack where an agent's action appears harmless in isolation but becomes harmful when considering the environment situation (e.g., rename file resulting in data loss). To address this challenge, we propose Safe-OS, a high-quality, carefully designed, and comprehensive dataset designed to evaluate the robustness of online OS agents. These attacks are carefully designed based on successful attacks targeting GPT-4-based OS agents. Additionally, our dataset simulates real-world OS environments using Docker, defining two distinct user identities: one as a root user with sudo privileges, and the other as a regular user without sudo access. Safe-OS includes both normal and harmful scenarios, with operations covering both single-step and multi-step tasks. More details of Safe-OS are described in Appendix A."}, {"title": "4 Methodology", "content": "4.1 Preliminary\nWe aim to identify the best set of safety checks,\n\u03a9* \u2286 \u03a9, that best align with predefined safety\ngoals in safety criteria Ic while incorporating op-\ntional guard requests Z\u00b9. Formally, the search\nspace of safety checks to satisfy safety goals is de-"}, {"title": "4.2 Safety Criteria", "content": "Safety criteria Z refer to the objective safety standards that an LLM agent must comply with when executing user tasks. For example, in LLaMA-Guard, the safety criteria depend on the content categories (AI, 2025). Similarly, for LLM agent, safety criteria also include different safety categories that specify the scope of safety checks. We built upon the safety criteria of LLM agent summarized by He et al. (2024), developing universal safety criteria that can be applied across various agents. These universal safety criteria primarily consist of three safety categories: Information Integrity, Information Confidentiality and Information Availability, see details in Appendix B.3. Our framework not only supports the universal safety criteria but also allows for manually designed safety criteria to enhance its effectiveness for specific tasks."}, {"title": "4.3 Overview of Our Framework", "content": "As shown in Figure 2, our framework employs two identical LLMs as Fand a memory module as"}, {"title": "5 Experimental Setup", "content": "In this section, we introduce our experimental setup for evaluating the performance of our method on different LLM agents across task-specific risks and systemic risks. Check more details in Appendix C.\n5.1 Models\nIn our primary experiments on the Safe-OS dataset, we use GPT-4-Turbo as the OS agent's foundation model for prompt injection attacks and GPT-40 for other attacks. For defense agencies, we adopt Claude-3.5-Sonnet and GPT-40 as the foundation models. The Guardrail-based baseline integrates four advanced agencies: LLaMA-Guard 3 (MetaAI,"}, {"title": "5.2 Datasets", "content": "We conduct our experiments on the following datasets focus on real-world agent outputs with Safe-OS ensure realistic and adaptive evaluation:\nMind2Web-SC and EICU-AC. Mind2Web-SC and EICU-AC (Xiang et al., 2024) are benchmarks for evaluating LLM agents' safety on task-specific risk. Mind2Web-SC assesses web agents like See-Act in enforcing web safety rules, while EICU-AC evaluates EHRAgent (Shi et al., 2024) in restricting access to ICU patient data based on user roles.\nAdvWeb. AdvWeb (Xu et al., 2024) generates and injects imperceptible adversarial strings to mislead web agents into executing incorrect actions. We utilized 80 data based on AdvWeb from four domains: housing, finance, medical, and cooking.\n\u0395\u0399\u0391. \u0395\u0399\u0391 (Liao et al., 2025) is a black-box attack that manipulates web environments by injecting hidden malicious HTML elements, tricking web agents into leaking user data. We sampled 100 instances where EIA successfully attacked SeeAct through action grounding and generation-based attacks. Additionally, from cases where EIA failed to attack SeeAct, we manually filtered out maliciously injected content to curate 30 clean benign samples."}, {"title": "5.3 Baseline", "content": "In our experiments, we categorize baselines into two types: model-based defense agencies and guardrail-based defense agencies. For model-based defenses, we configure advanced LLMs, such as GPT-40, with a customized CoT prompt (Xiang et al., 2024) under zero-shot and one-shot settings. For guardrail-based defenses, we set up LLaMA-Guard3 with guard requests as safety categories and evaluate GuardAgent under a two-shot setting, which represents its strongest setup. We also incorporate ToolEmu (Ruan et al., 2024) as a baseline,"}, {"title": "5.4 Evaluation Metric", "content": "In our experiments, we utilize two evaluation metrics to assess the quality of safety evaluations performed by all defense agencies on agent actions.\nPredictive Performance Metrics We utilize classical statistical metrics, including accuracy, precision, recall, and F1-score with ASR, as our predictive performance metrics to comprehensively evaluate the trade-off performance of defense agencies in handling normal and harmful agent actions.\nAgreement Metrics To further enhance the evaluation quality of defense agencies, we customize different agreement metrics for each task to precisely determine whether the risk detection process generated by the defense agencies aligns with the actual risks in ground truth for each dataset. See more implementation details in Appendix C.2."}, {"title": "6 Result and Analysis", "content": "We hearby report the results analyses and associated case studies of AGrail in this section.\n6.1 Main Result\nAGrail demonstrates strong performance in both task-specific and systemic risk detection. As shown in Tables 1 and 2, AGrail consistently ranks second across specific tasks (e.g., Mind2Web-SC and EICU-AC), regardless of using GPT-40 or Claude-3.5-Sonnet. In systemic risk detection on Safe-OS and AdvWeb, AGrail based on Claude-3.5-Sonnet achieves 0% ASR against prompt injection attacks"}, {"title": "6.2 Ablation Study", "content": "In the In-Distribution (ID) setting, we split the Mind2Web-SC dataset into a training set and a test set with an 8:2 ratio. In Out-Of-Distribution (OOD) setting, we split the Mind2Web-SC dataset based on domains with a 3:1 ratio for training and test sets and conduct experiments in three random seeds.\nOOD and ID Analysis In our experiments, we randomly set three groups of seeds. We first train the memory on the training set and then freeze the memory for evaluation on the test set. From the results in Table 3, we observe that in both ID and OOD setting, training the memory on the training set and then freezing it leads to better performance compared to the setting without memory. Moreover, this trend holds for both a stronger LLM (Claude-3.5-Sonnet) and a weaker LLM (GPT-40-mini), demonstrating the effectiveness of memory of AGrail. Additionally, AGrail enables generalization inference using cases stored in memory. Under the ID and OOD setting, we further evaluate the performance during TTA and found that it also outperforms the setting without memory, validating the importance of the memory module during TTA. See more details in Appendix D.1.\nSequence Analysis To investigate the impact of input data sequence on AGrail during TTA, we conduct experiments by setting three random seeds to shuffle the data sequence. In Table 4, the results indicate the effect of data sequence across different fundation models of AGrail. For Claude 3.5 Sonnet, accuracy shows minimal variation in this settings, suggesting that its performance remains largely stable regardless of data sequence. In contrast,"}, {"title": "6.3 Case Study", "content": "Error Analysis In our error analysis, we aim to evaluate whether defense agencies correctly identify the risks of agent actions that correspond to the ground truth in the data, rather than aimless thinking about potential risks. To achieve this, we manually annotated the accurate reasons for unsafety in the environment attacks in Safe-OS to serve as the ground truth. We then assessed the defense agency's capability to identify ground truth in prompt injection and environment attacks in Safe-OS, as well as prompt injection attacks in AdvWeb and EIA. From Figure 3, AGrail outperforms baselines by increasing an average of 46.2% in agreement rate and even achieves a 100% agreement rate on OS prompt injection. This demonstrates that AGrail can effectively identify correct risks of agent actions to block unsafe agent actions."}, {"title": "7 Conclusion", "content": "In this work, we introduce Safe-OS, a carefully designed, high-quality and comprehensive dataset for evaluating the robustness of online OS agents. We also propose AGrail, a novel lifelong framework that enhances LLM agent robustness by detecting risks in an adaptive fashion and identify effective safety policies for those risks. Our approach outperforms existing defense agencies by reducing ASR while maintaining effectiveness of LLM agents. Experiments demonstrate strong generalizability and adaptability across diverse agents and tasks."}, {"title": "Limitation", "content": "Our limitations are twofold. First, our current framework aims to explore the ability of existing LLMs to guardrail the agent. In our paper, we use off-the-shelf LLMs as components of our framework and incorporate memory to enable lifelong learning. Future work could explore training the guardrail. Second, due to the scarcity of existing tools for LLM agent security, our framework primarily relies on reasoning-based defenses and invokes external tools only when necessary to minimize unnecessary tool usage. Future work should focus on developing more advanced tools that can be directly plugged to our framework and further strengthen LLM agent security."}]}