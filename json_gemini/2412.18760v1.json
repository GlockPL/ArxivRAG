{"title": "Data clustering: an essential technique in data science", "authors": ["Wong Hauchi", "Daniil Lisik", "Tai Dinh"], "abstract": "This paper provides a comprehensive exploration of data clustering, emphasizing its methodologies and applications across different fields. Traditional techniques, including partitional and hierarchical clustering, are discussed alongside other approaches such as data stream, subspace and network clustering, highlighting their role in addressing complex, high-dimensional datasets. The paper also reviews the foundational principles of clustering, introduces common tools and methods, and examines its diverse applications in data science. Finally, the discussion concludes with insights into future directions, underscoring the centrality of clustering in driving innovation and enabling data-driven decision making.", "sections": [{"title": "1. Introduction", "content": "CLUSTERING is a fundamental technique in data science, which organizes data into meaningful groups, or clusters, based on their intrinsic similarities [1]. Unlike classification, which relies on predefined labels, clustering is an unsupervised learning approach, allowing data scientists to uncover hidden structures within datasets. This technique is widely used to handle complex, high-dimensional datasets and is pivotal for understanding the inherent organization of data [2].\nIn today's data-driven world, where organizations and researchers deal with enormous volumes of data, clustering serves as a critical tool for summarizing and analyzing this information. By grouping similar data points together, clustering aids in simplifying complex datasets, enabling better exploration and visualization, and laying the groundwork for further data analysis [3]. Furthermore, clustering finds applications in a wide array of fields. The objects to be clustered may include words, text, images, records, graph nodes, or any collection where items can be described by a set of features [4]. As a versatile tool, clustering has been widely employed as an intermediate step in numerous data mining and machine learning tasks. As illustrated in Figures 1 and 2, clustering is recognized as a fundamental branch of machine learning and serves as a pivotal task in the broader field of data science.\nClustering has been applied across diverse domains, including natural sciences, social sciences, economics, education, engineering, and health science, to name a few [5]. Additionally, clustering is closely connected to other data science processes, such as regression and classification. For instance, clustering can enhance regression analysis by identifying distinct groups within data, which may then inform separate regression models for each cluster. Similarly, clustering can complement classification by identifying potential class labels or refining existing ones based on patterns in the data. By serving as both a foundational and complementary tool, clustering facilitates a deeper understanding of data, enabling more accurate predictions, targeted interventions, and efficient resource allocation."}, {"title": "2. Basics of Data Clustering", "content": "Clustering is an unsupervised learning process that partitions a dataset D = {X1,X2, ..., Xn} where each data point xi \u2208 Ra intok clusters C = {C1, C2, ..., Ck}. The goal is to group similar data points into the same cluster while ensuring that points in different clusters are dissimilar. Formally, the clustering task can be expressed as the optimization of a clustering objective F(C), where:\n\\sum_{j=1}^{k} Cj = D, C\u00a1n Cj = \u00d8 for i \u2260 j (1)\narg min F (C) (2)\nEquation (1) indicates that clusters are non- overlapping and exhaustive, whereas equation (2) indicates that each cluster C\u00a1 is defined such that points within the same cluster are more similar to each other (intra-cluster similarity) than to those in different clusters (inter-cluster dissimilarity). In what follows, we revisit some of the most commonly used types of clustering discussed in the literature."}, {"title": "2.1. A Taxonomy of Clustering Types", "content": "Partitional (partitioning) clustering, also known as nonhierarchical or flat clustering, partitions a dataset into a predefined number of clusters k, without constructing hierarchical structures. Figure 3 shows the general workflow of partitional clustering algorithms. The primary goal is to optimize an objective function, such as minimizing intra-cluster distances or maximizing inter-cluster separation [6].\nPartitional clustering algorithms can be broadly categorized into hard partitional clustering and soft partitional clustering. Hard clustering, also referred to as crisp clustering, assigns each data point to exactly one non-overlapping cluster. Algorithms like K-means [7], K-modes [8], and K-prototypes [8] fall under this category. In contrast, soft (or fuzzy) clustering allows data points to belong to multiple clusters simultaneously. Each point is assigned a membership value ranging from 0 (no membership) to 1 (full membership), indicating its degree of association with each cluster [9]. This flexibility is particularly useful for datasets with overlapping or ambiguous boundaries.\nHierarchical clustering constructs a dendrogram to represent relationships among data points, using either an agglomerative (bottom-up) or divisive (top- down) approach [2]. Figure 4 shows the workflow of hierarchical clustering algorithms. In agglomerative clustering, each object begins as a separate cluster, and the two closest clusters are iteratively merged based on a similarity measure until only one cluster remains. Conversely, divisive clustering starts with all objects in a single cluster and recursively splits them into smaller clusters. The dendrogram illustrates the merging or splitting process, and the optimal number of clusters can be determined by cutting the dendrogram at a level that balances the number of clusters and their homogeneity.\nDensity-based clustering identifies clusters as dense regions of data points separated by sparser areas. Figure 5 shows the workflow of density-based clustering. Unlike partitional or hierarchical methods, it does not require specifying the number of clusters in advance. Instead, clusters are formed based on the density of points within a defined neighborhood. This approach is particularly effective for discovering clusters of arbitrary shapes and handling noise in datasets. A prominent example is the DBSCAN algorithm, which groups points with sufficient density while labeling outliers as noise [10].\nModel-based clustering is a statistical approach that assumes data are generated from a mixture of underlying probability distributions, with each distribution corresponding to a specific cluster [11]. Figure 6 shows the workflow of model-based clustering algorithm. This method relies on a generative model, where clusters are represented by parametric probability distributions, such as Gaussian distributions in the widely used Gaussian Mixture Model (GMM). The clustering process involves estimating the parameters of these distributions and assigning data points to the cluster that maximizes their likelihood. By leveraging statistical principles, model-based clustering can capture complex data structures, determine the optimal number of clusters using model selection criteria (e.g., BIC or AIC), and handle overlapping clusters effectively.\nSubspace clustering focuses on identifying clusters within specific subspaces of a high- dimensional dataset, rather than the full- dimensional space [12]. Figure 7 shows the workflow of subspace clustering. The key idea is that meaningful clusters may exist in subsets of dimensions where data points share strong similarities, while other dimensions may introduce noise. By searching for clusters in these relevant subspaces, this approach is particularly effective for high-dimensional data where traditional methods often struggle. Subspace clustering has proven valuable in domains such as bioinformatics and text analysis, where data often exhibit localized patterns in specific subsets of features.\nGraph-based clustering represents data as a graph, where nodes correspond to data points and weighted edges reflect the similarity between them [13]. Figure 8 shows the workflow of graph-based clustering. Clusters are formed as densely connected subgraphs, with sparse or minimal connections to nodes outside the cluster. The goal of graph-based clustering algorithms is to partition the graph by analyzing its edge structure, typically maximizing the total weight or number of edges within clusters while minimizing inter-cluster connections. This approach is particularly effective for identifying non- convex clusters and handling complex data structures.\nData stream clustering dynamically groups similar data points in real time as they flow continuously into a system. Designed to handle high- velocity, large-volume, and unbounded data streams, it incrementally updates clusters to ensure efficient and adaptive processing. The general framework of data stream clustering, illustrated in Figure 9, comprises several key steps. It begins with clustering the initial batch of data to establish a baseline configuration. A sliding window model is then employed to focus on the most recent data, ensuring that older, less relevant points are discarded. Clusters are represented through statistical summaries, such as centers or spreads, which are continuously updated to reflect new data. As new points arrive, they are incrementally assigned to existing clusters or used to form new ones, depending on their similarity. The framework also incorporates mechanisms to detect and respond to concept drift, adjusting the clustering model to accommodate shifts in the data distribution. Periodic maintenance is performed to refine clusters, which may involve merging, splitting, or repositioning them as needed. This iterative process ensures that the clustering remains adaptive, providing an up-to-date representation of the evolving data stream for downstream analysis.\nGenetic algorithm-based clustering utilizes genetic algorithms (GAs) to improve clustering performance [14]. GAs are search heuristics inspired by the process of natural selection, operating on a population of potential solutions and evolving them through selection, crossover, and mutation [15]. Figure 10 shows the framework of GA-based clustering. In clustering, each potential solution (or chromosome) represents a partitioning of the dataset into clusters, where each cluster contains a subset of data points. By iteratively optimizing cluster assignments through evolutionary processes, GA- based methods offer robust solutions and reduce the risk of being trapped in local optim.\nEnsemble clustering combines multiple clustering outputs, either from different algorithms or multiple runs, to produce a single consensus partition of the original dataset [16]. By leveraging the diversity and complementary strengths of individual clustering algorithms, this approach enhances overall clustering accuracy and robustness. The workflow of ensemble clustering for categorical data is illustrated in Figure 11. A typical ensemble framework consists of two main steps: (1) generating a set of base clustering results and (2) integrating these results into a final consensus clustering using a consensus function. Both the quality of the base clusterings and the effectiveness of the consensus function play a critical role in determining the success of the ensemble."}, {"title": "2.2. Evaluation Metrics for Clustering", "content": "Evaluating clustering performance is critical to understanding the quality and reliability of the generated clusters. Clustering evaluation metrics can be broadly categorized into internal and external validation metrics. Internal metrics assess the quality of clusters based on the dataset's intrinsic properties, focusing on factors such as compactness, separation, and density. These metrics do not require ground truth labels and evaluate the clustering structure directly. In contrast, external metrics compare the clustering results to ground truth labels, if available, to measure how well the clusters align with predefined categories or known partitions. Together, these metrics provide complementary insights into the effectiveness of a clustering algorithm."}, {"title": "3. Data Clustering Algorithms and Tools", "content": "In the world of machine learning, many clustering algorithms have been developed in order to approach existing problem users are facing from different perspective. These algorithms vary in their methodologies, assumptions, and the types of data they handle most effectively. Some focus on minimizing distances between data points, while others prioritize density or hierarchical relationships. As data science continues to evolve, more and more algorithms have become integral tools in fields ranging from market segmentation to anomaly detection. In the following, some of the most popular clustering algorithms will be introduced, showing their key features and applications."}, {"title": "3.1 Popular Clustering Algorithms", "content": "K-means [7] is a popular algorithm of partitioning methods used for clustering data into groups based on similarity. The Lloyd's algorithm (Algorithm 1) is the most common implementation of K-means. It begins by initializing k cluster centroids, which are typically chosen randomly. Each data point is then assigned to the nearest centroid based on a distance metric, usually Euclidean distance, forming k clusters. Once the assignments are made, the centroids are updated by calculating the mean position of all points in each cluster. These steps of assignment and update are repeated iteratively until the centroids stabilize or a stopping criterion is met, such as a maximum number of iterations or minimal movement of centroids. While K-means is simple and computationally efficient, it requires the number of clusters (k) to be specified beforehand and can be sensitive to outliers and the initial placement of centroids. Additionally, it works best when the clusters are roughly spherical and of similar size.\nK-modes [8] is another partitioning methods of clustering technique, extending the K-means algorithm. Instead of using the mean to define cluster centroids, K-modes relies on the mode, which is the most frequently occurring value for each feature. The algorithm begins by randomly selecting k data points as the initial centroids. It then assigns each data point to the cluster whose centroid has the minimum dissimilarity, often measured by the number of mismatched categorical attributes. After the assignments, the centroids are updated by determining the mode for each feature within the cluster. This process of assignment and update is repeated iteratively until the centroids stabilize or a predefined stopping criterion is met. The K-modes algorithm is computationally efficient and well-suited for categorical data, but it requires pre-specifying the number of clusters (k) and can be sensitive to the initial choice of centroids.\nK-prototypes [8] is a clustering algorithm designed for mixed-type data, combining K-means and K-modes. It uses an adapted distance function that treats each data type separately, updating numeric variables with their means and categorical variables with their modes. It efficiently handles mixed data in one algorithm, rather than running separate clustering processes for each data type."}, {"title": "3.2. Tools and Frameworks", "content": "Data clustering tools are essential for analyzing and grouping data into meaningful subsets or clusters based on similarity. These tools enable us to uncover hidden patterns, reduce dimensionality, and gain insights that would be difficult to identify manually. Clustering is widely used in fields like customer segmentation, anomaly detection, and image analysis, making these tools indispensable in modern data science. By automating the process of grouping similar data points together, they allow for more efficient and scalable analysis, even with large datasets.\nIn Python, clustering is straightforward with Scikit-learn\u00b2. It is a Python-based machine learning library that excels in simplicity and versatility. It offers efficient implementations of clustering methods such as K-means , DBSCAN and hierarchical clustering, making it a favorite for data scientists and engineers. With its clean API and seamless integration with other Python libraries like NumPy and Pandas, Scikit-learn is ideal for building and testing clustering models during prototyping or moderate-scale production tasks. For instance, K-means clustering can be performed in a few lines of code. A typical workflow involves generating or preprocessing data, fitting the K-means model, and visualizing the results. With the K-means class, data points can be grouped into clusters, and their centroids are identified, allowing for intuitive visualization.\nR5 is a powerful statistical programming language, particularly doing well for tasks requiring deep exploratory analysis. Its libraries, such as cluster and factoextra, enable the implementation of clustering techniques with a strong emphasis on visualization. Users can perform K-means clustering and generate elegant visualizations that highlight the distribution of clusters. This makes R a popular option for academic research and projects where statistical rigor and clarity of insights are paramount.\nTensorFlow is primarily designed for deep learning, but also supports custom clustering methods, often integrated with neural networks. It is preferred in scenarios where clustering involves highly scalable or complex data architectures. Its ability to handle large datasets with GPU acceleration makes it suitable for advanced tasks like deep clustering. With TensorFlow, clustering can integrate directly with large-scale and deep learning workflows. TensorFlow's flexibility enables creating custom clustering models, leveraging its tensor-based computations and GPU support. For instance, users can build scalable K-means solutions or incorporate clustering into neural network pipelines, handling vast datasets efficiently.\nMATLAB\u00ae stands out as a high-level language tailored for numerical computation and visualization. Its built-in functions for clustering, such as K-means, hierarchical clustering, and Fuzzy C-means, allow researchers to develop algorithms rapidly. MATLAB is particularly popular in academia and industries requiring precise numerical modeling, such as engineering and finance. In MATLAB, clustering workflows are built around precision and visualization. As an example, MATLAB's hierarchical clustering tools allow users to visualize cluster relationships using dendrograms. By coupling hierarchical clustering with MATLAB's ability to plot and group data, users can derive meaningful insights into the structure of your data.\nOrange Data Mining is an open-source tool for data visualization and analysis, catering to both beginners and experts in data science. With its modular and user-friendly interface, Orange allows users to build workflows for tasks like data preprocessing, visualization, classification, clustering, and more using a simple drag-and-drop approach. Its extensibility through Python scripting and add-ons makes it a versatile option for advanced machine learning and statistical analysis. For clustering, Orange offers widgets for widely used techniques such as K-means hierarchical clustering, and DBSCAN, enabling users to configure parameters, visualize clusters, and assess clustering quality with ease. Moreover, Orange includes advanced visualization tools, such as dendrograms for hierarchical clustering and scatterplots for exploring and interpreting cluster assignments."}, {"title": "3.3. Challenges in Practical Implementation", "content": "Data clustering plays a crucial role in various fields of modern society by uncovering hidden patterns and structures within data, clustering enables organizations and researchers to derive valuable insights from complex datasets. Many of the challenges associated with data clustering arise because it is an unsupervised learning technique. Unlike supervised learning, where the model is trained on labeled data with a clear target outcome, clustering works without predefined labels or ground truth. This lack of guidance introduces several complexities that make clustering inherently more challenging.\nDespite these challenges, clustering remains an indispensable tool for data exploration and analysis, provided these obstacles are addressed effectively. In this section, explore some of the most common challenges in the practical implementation of data clustering will be discussed.\nHigh-dimensional data poses significant challenges for clustering because of the curse of dimensionality. In high-dimensional spaces, distances between points lose their interpretability, as most points tend to become equidistant from one another. This makes it difficult for distance-based algorithms like K-means to effectively group data into meaningful clusters. For example, in a low- dimensional space, Euclidean distance may work well to measure similarity, but as the number of dimensions increases, this metric becomes less reliable. Nowadays, techniques like Principal Component Analysis (PCA), t-SNE, and UMAP are often used to reduce dimensionality, capturing the most relevant features while maintaining the overall structure of the data. Alternatively, algorithms such as Spectral Clustering or distance metrics like cosine similarity may offer better performance in high- dimensional datasets.\nLarge datasets clustering with millions or billions of data points can be computationally expensive in terms of both time and memory. Algorithms like Hierarchical clustering, which require building a complete dendrogram, scale poorly as the dataset size increases. Similarly, iterative methods like K-means can become resource-intensive when dealing with massive data volumes. To address this, techniques like Mini-Batch K-means, which processes subsets of data iteratively, are designed to solve the problem. Additionally, distributed computing frameworks like Apache Spark allow for parallel processing of large datasets, reducing computational overhead. Preprocessing steps, such as feature selection or dimensionality reduction, can also help to manage dataset size before clustering.\nNoise and outliers in the dataset can significantly affect the performance of clustering algorithms. For instance, in K-means, outliers can pull centroids away from their ideal positions, leading to distorted clusters. Algorithms like DBSCAN handle noise better by identifying and marking outliers as separate entities rather than forcing them into clusters. However, DBSCAN struggles when clusters have varying densities, which can still be problematic in noisy datasets. Preprocessing methods, such as outlier detection techniques (e.g., Isolation Forest or Z-score) are developed in order to help clean the data before clustering. For particularly noisy datasets, robust clustering methods or denoising approaches, like filtering or smoothing techniques, may be necessary.\nIncomplete datasets with a lot of missing data are another major challenge in clustering, as most algorithms require complete data to compute distances between points [21]. Missing numerical values can disrupt calculations of centroids or means, while missing categorical data can cause errors in assignments. Various imputation techniques are used to address this issue, such as replacing missing numerical values with the mean or median and filling categorical gaps with the mode. Predictive modeling can also be employed to estimate missing values based on correlations in the data. In cases where missing data is excessive, certain features or records may need to be removed entirely. While K-prototypes is designed to cluster mixed-type data (both numeric and categorical), it does not automatically handle missing values more effectively than other clustering methods. Effective handling of missing data typically requires explicit strategies, such as imputation or specialized distance metrics, regardless of the clustering algorithm.\nSelecting an appropriate clustering algorithm is critical because different algorithms work best under specific conditions. For example, K-means performs well for spherical clusters but struggles with clusters of varying density or irregular shapes. On the other hand, DBSCAN can identify clusters of arbitrary shapes and handle noise but is sensitive to its parameter settings, particularly when clusters have varying densities. Hierarchical Clustering provides a more detailed, dendrogram-based view of clusters but is computationally expensive and unsuitable for large datasets. Choosing the right algorithm requires a thorough understanding of the data structure together with aim of task, which can often be explored using visualization techniques like t-SNE or UMAP. Experimenting with multiple algorithms and comparing their results using metrics such as silhouette score or Davies-Bouldin index can also aid in selecting the best approach before further analysis.\nParameter selection is another challenge when it comes to data clustering. Most clustering algorithms require the user to specify hyperparameters that significantly affect the clustering outcome. For instance, K-means requires defining the number of clusters (k) upfront, which can be challenging without prior knowledge of the data [19, 25]. DBSCAN requires setting the epsilon (\u03b5) parameter, which defines the maximum distance between points, and the minimum number of points (minPts) to form a cluster. Similarly, Agglomerative Hierarchical Clustering depends on the choice of linkage criteria (e.g., single, complete, or average linkage), which affects the final clusters. Tuning these parameters often involves trial-and-error methods using techniques like grid search or Bayesian optimization. Visualization methods, undergoing together with elbow method or silhouette analysis, which are commonly used to determine optimal parameters. Domain expertise can also provide valuable guidance in setting these parameters correctly."}, {"title": "4. Applications of Data Clustering in Data Science", "content": "Clustering plays a vital role in exploratory data analysis by uncovering hidden patterns and grouping similar data points, enabling a deeper understanding of the underlying structure within datasets [3]. By organizing data into clusters, analysts can visualize relationships, anomalies, and trends more effectively. Tools such as dendrograms, scatterplots, and heatmaps often accompany clustering methods to provide intuitive representations, allowing users to gain insights and guide further analysis or decision-making."}, {"title": "4.1. Exploratory Data Analysis and Visualization", "content": "Clustering plays a vital role in exploratory data analysis by uncovering hidden patterns and grouping similar data points, enabling a deeper understanding of the underlying structure within datasets [3]. By organizing data into clusters, analysts can visualize relationships, anomalies, and trends more effectively. Tools such as dendrograms, scatterplots, and heatmaps often accompany clustering methods to provide intuitive representations, allowing users to gain insights and guide further analysis or decision-making."}, {"title": "4.2. Clustering for Structured and Unstructured Text Data", "content": "Clustering is widely applied to both structured and unstructured text data, enabling effective organization and analysis. For structured data, methods such as K-means K-modes and K-prototypes are widely used for numerical, categorical and mixed data types. In contrast, for unstructured text data like documents or social media posts, techniques often involve converting text into numerical representations, such as TF-IDF vectors or embeddings, before applying clustering algorithms [30]. These approaches are used in applications like document categorization, topic modeling, and trend detection, helping to simplify complex datasets and uncover meaningful insights."}, {"title": "4.3. Image-Based Clustering with Embedding Methods", "content": "Recent advancements in large language models (LLMs) have transformed image-based clustering by enabling feature extraction through embeddings [27]. These embeddings represent high-dimensional image features in a compact vector format, capturing semantic relationships effectively. Once features are extracted, traditional clustering techniques like K-means or DBSCAN are applied in the embedding space to group similar images. This approach is commonly used in tasks like image retrieval, object categorization, and anomaly detection in visual data."}, {"title": "4.4. Applications in Bioinformatics and Personalized Medicine", "content": "Clustering has become a cornerstone in bioinformatics, enabling tasks such as gene expression analysis, protein structure prediction, and patient stratification [31]. In personalized medicine, clustering is used to group patients based on genetic markers, symptoms, or treatment responses, aiding in the design of tailored therapeutic strategies. By identifying distinct biological patterns, clustering facilitates a deeper understanding of complex biological systems and supports precision healthcare initiatives [28, 29]."}, {"title": "4.5. Sentiment Analysis and Community Detection", "content": "In natural language processing, clustering is applied to sentiment analysis by grouping text data, such as customer reviews or social media posts, based on their emotional tone or underlying themes [33, 34]. Similarly, in social network analysis, clustering is instrumental in community detection, uncovering subgroups of interconnected individuals or entities within a larger network [32]. These applications provide valuable insights into behavioral trends, opinion mining, and the dynamics of social ecosystems."}, {"title": "5. Conclusion", "content": "Data clustering is an important and commonly used technique in data science, serving as a cornerstone for organizing complex datasets into coherent groups based on intrinsic similarities. As demonstrated in this paper, clustering not only simplifies high-dimensional data but also provides a robust framework for identifying latent structures and patterns that are critical for exploratory analysis and downstream tasks. Its flexibility and applicability across a broad spectrum of data types and domains underscore its enduring significance in solving intricate data challenges.\nThe integration of clustering with emerging technologies, particularly large language models (LLMs), represents a significant advancement in the field. LLMs, with their ability to generate high- quality embeddings and capture nuanced patterns in data, enhance clustering by providing more meaningful and compact feature representations. These embeddings are particularly useful for unstructured data, such as text and images, enabling traditional clustering algorithms, such as K-means to operate effectively in reduced-dimensional spaces. This synergy between clustering and LLMs opens new avenues for applications ranging from image- based analysis to text mining, where extracting semantic relationships and organizing information is essential."}]}