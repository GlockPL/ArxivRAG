{"title": "FaceSpeak: Expressive and High-Quality Speech Synthesis from Human Portraits of Different Styles", "authors": ["Tian-Hao Zhang", "Jiawei Zhang", "Jun Wang", "Xinyuan Qian", "Xu-Cheng Yin"], "abstract": "Humans can perceive speakers' characteristics (e.g., identity, gender, personality and emotion) by their appearance, which are generally aligned to their voice style. Recently, vision-driven Text-to-speech (TTS) scholars grounded their investigations on real-person faces, thereby restricting effective speech synthesis from applying to vast potential usage scenarios with diverse characters and image styles. To solve this issue, we introduce a novel FaceSpeak approach. It extracts salient identity characteristics and emotional representations from a wide variety of image styles. Meanwhile, it mitigates the extraneous information (e.g., background, clothing, and hair color, etc.), resulting in synthesized speech closely aligned with a character's persona. Furthermore, to overcome the scarcity of multi-modal TTS data, we have devised an innovative dataset, namely Expressive Multi-Modal TTS (EM2TTS), which is diligently curated and annotated to facilitate research in this domain. The experimental results demonstrate our proposed FaceSpeak can generate portrait-aligned voice with satisfactory naturalness and quality.", "sections": [{"title": "Introduction", "content": "Human voices contain munificent information in aspects such as age, gender, emotional nuances, physical fitness, and speaker identity. These vocal characteristics are intrinsically related to an individual's physical and psychological makeup, offering a unique profile of the speaker. For example, the emotional content conveyed through speech is often mirrored in facial expressions. This inherent correlation between voice and image sparked research exploration in various fields, including emotion recognition, speaker verification, face-speech retrieval, and speech separation.\nIn recent years, growing research interest has focused on the controllability of synthesized speeches. To this end, one solution is to introduce an auxiliary reference input to model the style features of speech. For example, PromptTTS and InstructTTS leverage textual descriptions to control the speech synthesis style. In contrast, the input text description still needs human crafting efforts and expertise, while some individuals may struggle to accurately express their intended synthesis goals. Other works employ vision as a reference. For example, visualTTS generates temporal synchronized speech sequences for visual dubbing, and MM-TTS transfers multi-modal prompts (i.e., text, human face, pre-recorded speech) into a unified style representation to control the generation process. Despite both works using images (real human faces) as the reference, thus cannot adapt to nonphotorealistic portraits which are widespread in digital assistants, video games, and virtual reality scenarios.\nAlthough previous attempts have been made to generate speech based on visual cues, they have faced notable limitations. One such limitation is the predominant reliance on real-face datasets, which lack diversity in image styles necessary for comprehensive speech synthesis guidance. This significantly restricts the potential applications of synthesizing speech from portrait images. Additionally, prior methods often employ entangled embeddings of facial images to guide speech synthesis, potentially introducing extraneous information that hampers performance. Moreover, relying on entangled visual features can further constrain the flexibility of the synthesis system, as the synthesized speech in this case can only be controlled by a single image. However, decoupling identity and emotion features can enable the control of speech synthesis by using different images providing identity and emotion information separately, greatly increasing the diversity and flexibility of multi-modal TTS.\nIn this paper, we tackle the aforementioned issues through a novel multi-modal speech synthesis process. As shown in Fig. 1, given the text of the content and the images in different styles (e.g., fantasy art and cartoon), our aim is to generate high-quality vivid human speech that is aligned with the characteristics indicated by vision. Our key contributions are summarized as follows.\n1. We introduce EM2TTS, a pioneering multi-style, multi-modal TTS dataset. It is designed and re-annotated through a collaborative multi-agent framework which leverages chatGPT for crafting intermediate textual descriptions, PhotoMaker for generating human portraits from text, and DALL-E to establish multi-modal coherence. It provides large-scale and diverse style images that enable the training model to generate high-quality and image-coherent speech, thus facilitates the state-of-the-art (SOTA) multi-modal TTS development.\n2. We propose a novel speech synthesis method given human portrait prompt, namely FaceSpeak, to generate speech that is aligned with the characteristics indicated by the visual input. To our best knowledge, this is the first multi-modal expressive speech synthesis work that allows input any-style images. In particular, we disentangle the identity and expression features from facial images, ensuring that the synthesized audio aligns with the speaker's characteristics, while mitigating the impact of irrelevant factors in the images and enhancing the flexibility and diversity of synthesis systems.\n3. Extensive experiments demonstrate that our proposed FaceSpeak can synthesize image-aligned, high-quality, diverse, and expressive human speech. Its superior performance is also validated through the numerous subjective and objective evaluations."}, {"title": "Related work", "content": "Existing TTS works utilizing prompts leverage a multitude of modalities, including reference speech, textual descriptions, and human faces. We list the most related works in Table 1.\nSpeech prompt: Traditional TTS system extracts features from a reference speech to obtain the desired voice with unique vocal characteristics. For example, Meta-StyleSpeech, which is built on FastSpeech2, fine-tunes the gain and bias of textual input based on stylistic elements extracted from a speech reference, facilitating effective style-transferred speech synthesis. YourTTS, based on VITS, proposes modifications for zero-shot multi-speaker and multilingual training, resulting in good speaker similarity and speech quality. GenerSpeech proposes a multi-level style adapter and a generalizable content adapter to efficiently model style information. Mega-TTS employs various techniques (e.g., VQ-GAN, codec-LM) to extract different speech attributes (e.g., content, timbre, prosody, and phase), leading to successful speech disentanglement. Despite the impressive results, they are still limited by the availability of pre-recorded reference speech with a clean background. Moreover, the synthesized speech is often limited by the intrinsic attributes of the reference speech.\nText prompt: In contrast to traditional TTS systems that require users to have acoustic knowledge to understand style elements such as prosody and pitch, the use of text prompts is more user-friendly as text descriptions offer a more intuitive and natural means of expressing speech style. For example, PromptTTS utilizes the BERT model as a style encoder and a transformer-based content encoder to extract the corresponding representations from the text prompt to achieve voice control. InstructTTS"}, {"title": "Proposed EM2TTS Dataset", "content": "The widely-used TTS datasets, such as LibriTTS and VCTK, predominantly exhibit a single-modal nature, comprising solely audio recordings without corresponding textual or visual labels. Existing multi-modal TTS datasets, including ESD, EmovDB2, and Expresso, lack visual labels for character profiles. While IEMOCAP, MEAD, CMU-MOSEI and RAVDESS datasets provide labels with limited aspects such as emotion and facial expressions, no existing TTS datasets provide face data with diverse image styles.\nTo address the above limitations, we re-design and annotate an expansive multi-modal TTS dataset, termed EM2TTS. It is enriched with diverse text descriptions and a wide range of facial imagery styles. Due to the distinct data characteristics and varying levels of annotation completion, we have delineated the dataset into two distinct emotional subsets. Please see Appendix for more details.\nEM2TTS-MEAD\nFor the MEAD dataset with real human face recordings, we initially applied a random selection strategy to extract video frames. Then, following steps are designed to proceed the data: 1) Automatic text generation: we generate corresponding text labels that encompass gender, emotion and its intensity, utilizing the raw data from the MEAD dataset. In particular, our methodology draws from the MMTTS framework, which correlates emotion intensity levels with specific degree words, as shown in Figure 2. 2) Image style transfer: we leverage the innovative image style conversion model, PhotoMaker, which excels in producing a variety of portrait styles representing the same individual, guided by character images and textual prompts. For each speaker exhibiting varying emotion intensities, we have created four styles of images (i.e., fantasy art, cinematic, neonpunk, and line art), enriching the visual diversity of the dataset.\nNevertheless, a significant challenge arises in discerning the correlation between a speaker's appearance and their timbre, particularly with hard samples. These cases involve a mismatch between the physical attributes and the vocal characteristics of the speaker (e.g., a physically strong person's voice may sound similar to that of a slim woman). Consequently, models trained exclusively on EM2TTS-MEAD face difficulties in aligning with intuitive expectations of human perception.\nEM2TTS-ESD-EmovDB\nThe unimodal emotional speech datasets ESD and EmovDB lack accompanying speaker images, thus performing style transfer based on real human face is unfeasible. Therefore, we design the next steps to process them: 1) Manual annotation: we explore a human expert to label age, gender, and characteristics by listening to each speech data; 2) Text expansion: we use the Large Language Model (LLM) model e.g., ChatGPT to expand the label words into texts with varying contents but similar meanings; 3) Text-driven image generation: the enriched texts were then fed into DALL-E-3, a text-to-image model capable of generating a multitude of images in distinct styles. We assigned these generated images to the corresponding speeches, resulting in style-free images that emphasize the character's emotions, surpassing the limitations of the recorded images."}, {"title": "Proposed method", "content": "Let us denote Ii, xi and ti as the visual image of arbitrary style, the corresponding voice signal, and the co-speech content of character i, respectively. As depicted in Fig. 3, our proposed FaceSpeak algorithm aims to generate a speech waveform si corresponding to the imagined voices of characters given the input text ti and images of different styles (either real IR or generated IF).\nThe proposed FaceSpeak consists of two sub-modules: 1) Multi-style image feature decomposition module that receives different styles of portrait images for feature extraction and decouples emotion and speaker information. With this module, we can get the disentangled identity and emotion embeddings extracted from the portrait visual features; 2) Expressive TTS module receives the identity and emotion embeddings as control vectors for generating high-quality speech that matches the portrait images. Detailed descriptions are given below.\nMulti-style image feature disentanglement\nTo enhance the coherence of identity and emotion between the input image and the synthesized speech, it is crucial to mitigate the influence of extraneous visual elements (e.g., background, clothing, distracting objects) in the extracted visual features. The FaRL model, leveraging the multi-modal pre-trained CLIP model on large-scale datasets of face images and correlated text, ensures the extraction of predominantly face-related visual features with robust generalization capabilities. Thus, we apply it on real images and corresponding multi-style images to extract high-dimensional intermediate visual representations:\ne_i = FaRL(I_i)\nwhere $e_i \\in R^{512}$ contains both the emotion and speaker information of the portrait.\nOur subsequent objective is to decouple the emotion and the speaker information within $e_i$. Let us define the Identity Adapter Module (IAM) and the Expression Adapter Module (EAM) to learn the mapping from $e_i$ to the identity embedding $a_i$ and emotion embedding $\\beta_i$, respectively:\na_i = IAM(e_i) = FC (GeLU(FC(e_i)))\n\\beta_i = EAM(e_i) = FC (GeLU(FC(e_i)))\nSubsequently, we used the emotion classification model following to bias the $\\beta_i$ features toward emotion characteristics. Furthermore, we incorporated the emotion classification model after introducing Gradient Reverse Layer (GRL) following $a_i$, which aims to minimize the sentiment information retained in $a_i$. We utilize the Cross-Entropy loss to constrain the two emotion classification models, formulated as follows:\nL_{emo} = CrossEntropy (CLS(\\beta_i), L_e)\nL_{grl} = CrossEntropy(GRL(CLS(a_i)), L_e)\nwhere CLS represents the classification layer, and $L_e$ denotes the emotion categorization label of the input image $I_i$. GRL inverts the sign of the incoming gradient during the back-propagation phase. By this strategic reversal, IAM learns to remove or minimize features that are correlated with emotion, emphasizing the identity aspects of the input. To enhance the decoupling of identity embedding $a_i$ and emotion embedding $\\beta_i$, we propose a feature decoupling approach that leverages Mutual information (MI) minimization. This strategy effectively reduces the correlation between identity and emotion representations, enabling a more robust and accurate analysis of each aspect.\nMutual information based decoupling: MI is a fundamental concept in information theory that quantifies the statistical dependence between two random variables $V_1$ and $V_2$, calculated as:\nI(V_1; V_2) = \\sum_{v_1\\in V_1} \\sum_{v_2\\in V_2} p(v_1, v_2) log(\\frac{p(v_1, v_2)}{p(v_1)p(v_2)})\nwhere $p(v_1, v_2)$ is the joint probability distribution between $v_1$ and $v_2$, while $p(v_1)$ and $p(v_2)$ are their marginals. However, it is still a challenge to obtain a differentiable and scalable MI estimation. In this work, we use vCLUB, an extension of CLUB, to estimate an upper bound on MI, which allows efficient estimation and optimization of MI when only sample data are available and a probability distribution is not directly available. Given the sample pairs of identity embedding and emotion embedding ${(a_i, \\beta_i)}_i^N$, where $N$ denotes the number of samples, the MI can be computed as:\nL_{mi} = \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N [log q_{\\theta}(\\beta_i|a_i) - log q_{\\theta}(\\beta_j|a_i)]\nwhere $q_{\\theta}(\\beta|a_i)$ is a variational approximation which can make vCLUB holds a MI upper bound or become a reliable MI estimator. At each iteration during the training stage, we first obtain a batch of samples ${(a_i, \\beta_i)}$ from IAM and EAM, then update the variational approximation $q_{\\theta}(\\beta|a_i)$ by maximizing the log-likelihood $L_o = \\sum_{i=1}^N log q_{\\theta}(\\beta|a_i)$. The updated $q_{\\theta}(\\beta|a_i)$ can be used to calculate the vCLUB estimator. Finally, we sum the decoupled identity embedding and the emotion embedding obtained as the ultimate control embedding $p_i$ for speech synthesis.\nExpressive TTS\nWe use VITS2, one of the SOTA TTS models, as our speech synthesis backbone. As shown in Figure 3, The VITS2 model consists of a Posterior Encoder and a Text Encoder that generate posterior distribution and prior distribution based on the input speech and text, respectively; a transformer-based Flow module to refine the latent representation produced by the posterior encoder; a Monotonic Alignment Search (MAS) module to estimate an alignment between input text and target speech; a Duration Predictor module to predict the duration of each phoneme; a Decoder to reconstructs the speech from the latent representation generated by the posterior encoder. In particular, we injected the control embedding from the portrait images into the Posterior Encoder, Decoder, Flow module, and Duration Predictor to generate the speech corresponding to the portrait images, which are highlighted in green in Figure 3. In the training stage, identity embedding $a_i$ and emotion embedding $\\beta_i$ are decoupled from the same image and the final loss can be expressed as:\nL = L_{vits} + \\lambda_1L_{mi} + \\lambda_2L_{emo} + \\lambda_3L_{grl}\nwhere $\\lambda_1$, $\\lambda_2$ and $\\lambda_3$ are the hyper-parameters to balance the individual losses. During inference, $a_i$ and $\\beta_i$ can come from the same image or be provided by different images separately, and the inference process of Expressive TTS is consistent with work.\nDataset and Experimental setup\nIn the evaluation of our method, we conduct separate experiments on intra-domain and out-of-domain data, respectively."}, {"title": "Synthetic quality on real portraits", "content": "We first evaluate the quality of the speech synthesized by FaceSpeak given a real portrait as the prompt. We conduct a Mean Opinion Score (MOS) (Sisman and Yamagishi 2021) with 95% confidence intervals to assess speech quality. Naturalness-MOS (NMOS) and Similarity-MOS (SMOS) evaluate the speech naturalness and image-speech similarity. Specifically, we generate 50 speech samples for each model, which are rated by 20 volunteers on a scale of 1 to 5, with higher scores indicating better results (\u2191). We also perform an AXY preference test (Skerry-Ryan and Battenberg 2018) to verify the style transfer effect based on image prompt. In the test, \"A\" is the reference speech that is stylistically consistent with the image prompt, \"X\" and \"Y\" are the speech generated by the compared model or our proposed FaceSpeak. The participants decide whether the speech style of \"X\" or \"Y\" is closer to that of image prompt where the scale range of [-3,3] indicating \"X\" is closer to \"Y\" is closer. Table 2 displays the subjective results including NMOS, ISMOS and ESMOS. Our FaceSpeak achieves better results on both speech naturalness and style similarity, showing the effectiveness of DSP module on speaker representation extraction by using IAM and EAM. As shown in Table 3, the results of AXY test indicate that listeners prefer FaceSpeak synthesis against the compared models. The generated data and the method significantly improves the style extraction ability, allowing an arbitrary reference sample to guide the stylistic synthesis of arbitrary content text.\nAs shown in the Table 4, we further objectively measure the quality of speech through MCD, emotion and gender classification accuracy, and speaker similarity. MCD measures the spectral distance between the reference and synthesized speech and FaceSpeak achieved a MCD result of 3.32. We measure speaker similarity (SS) between two speech samples in Resemblyzer 3, our result is 0.95. A hubert-based pre-trained model 4 is used for gender classification (Accgen) and emotion2vec is used to predict the emotion category (Accemo) of speech. On the intra-domain data, we obtained Accgen for 99.40% and Accemo for 60.92%. For out-of-domain data, the results for Accgen and Accemo are 92.42% and 31.32%, respectively."}, {"title": "Synthetic quality on multi-style virtual portraits", "content": "In evaluating the quality of FaceSpeak's speech synthesis based on multi-style virtual portraits, the models we compare are whether or not trained with the virtual portrait images of our proposed multi-style dataset EM2TTS, respectively. Since models trained with EM2TTS will have \u201cseen\u201d multi-style portraits in the domain, our evaluation will only be performed on out-of-domain multi-style portraits. As illustrated in Table 5, the model trained with the virtual portrait images performs better in all evaluation metrics, confirming the effectiveness of our methods."}, {"title": "Results of decoupled identity and emotion information", "content": "TSNE results of decoupled features: Fig. 4 (a) visualizes the embeddings extracted from FaRL in emotion, which are randomly distributed. By applying the EAM module, as shown in Fig. 4 (b), the learned embeddings with the same emotion are more clustered, while the others are more discriminative. Fig. 5 (a) shows the embeddings extracted from FaRL in identity, which are also randomly distributed. By applying the IAM module, from Fig. 5 (b) to (d), it is observed that embeddings from different speakers are more distinguished when using both the GRL and vCLUB strategies. These intuitively verify the effectiveness of our decoupled identity and emotion characteristics.\nSpeech synthesis controlled by combined portraits: By decoupling identity and emotion information in an image, we can use different image combinations to control the synthesized speech. As shown in Figure 6, we define X as the image that provides identity embedding and Y as the image that provides emotion embedding, ensuring that X and Y have different genders and emotions. We let listeners discriminate the synthesized speech by deciding whether it is matched to the X-image or Y-image in terms of identity and emotion, respectively. As a result, 98.6% of the speech are determined to correctly match the X image on identity, while 92.1% of the speech are determined to correctly match the Y image on emotion, which proves that our proposed FaceSpeak speech synthesis system can reliably control the synthesis by combining different images, greatly improving the diversity and flexibility."}, {"title": "Conclusion", "content": "In this paper, we introduce FaceSpeak, a pioneering approach for multi-modal speech synthesis which extracts key identity and emotional cues from diverse character images to drive the TTS module to synthesize the corresponding speech. To tackle the problem of data scarcity, we introduced an innovative EM2TTS dataset, which is meticulously curated and annotated to support and advance research in this emerging field. Additionally, novely methods are proposed for decoupling emotional and speaker-specific features, enhancing both the adaptability and fidelity of our system. Experimental results confirm that FaceSpeak can generate high-quality, natural-sounding speech that authentically align with the visual attributes of the character."}]}