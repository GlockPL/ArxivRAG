{"title": "The Value of AI-Generated Metadata for UGC Platforms: Evidence from a Large-scale Field Experiment", "authors": ["Xinyi Zhang", "Chenshuo Sun", "Renyu Zhang", "Khim-Yong Goh"], "abstract": "AI-generated content (AIGC), such as advertisement copy, product descriptions, and social media posts,\nis becoming ubiquitous in business practices. However, the value of AI-generated metadata, such as title,\nremains unclear on user-generated content (UGC) platforms. To address this gap, we conducted a large-\nscale field experiment on a leading short-video platform in Asia to provide about 1 million users access to\nAI-generated titles for their uploaded videos. Our findings show that the provision of AI-generated titles\nsignificantly boosted content consumption, increasing valid watches by 1.6% and watch duration by 0.9%.\nWhen producers adopted these titles, these increases jumped to 7.1% and 4.1% respectively. This viewership-\nboost effect was largely attributed to the use of this generative AI (GAI) tool increasing the likelihood of\nvideos having a title by 41.4%. The effect was more pronounced for the groups more affected by metadata\nsparsity. Mechanism analysis revealed that AI-generated metadata improved user-video matching accuracy\nin the platform's recommender system. Interestingly, for a video for which the producer would anyway have\nposted a title, adopting the AI-generated title will decrease its viewership on average, implying that AI-\ngenerated titles may be of lower quality than human-generated ones. However, when producers chose to co-\ncreate with GAI and significantly revised the AI-generated titles, the videos outperformed their counterparts\nwith either fully AI-generated or human-generated titles, showcasing the benefits of human-AI co-creation.\nThis study highlights the value of AI-generated metadata and human-AI metadata co-creation in enhancing\nuser-content matching and content consumption for UGC platforms.", "sections": [{"title": "1. Introduction", "content": "Generative AI (GAI) and AI-generated content (AIGC) have demonstrated significant values and\npotentials across industries by efficiently producing high-quality content, such as generating adver-\ntising copy, improving customer service, and enhancing media production.\u00b9 These GAI tools help"}, {"title": "2. Literature Review", "content": "Our paper speaks to three streams of literature: (1) GAI and its collaborations with humans; (2)\nplatform operations; and (3) data augmentation and recommender system.\nGAI and its Collaborations with Humans. Our work is most closely connected to research\non the economic impact of GAI. Emerging literature has examined its economic impact across var-\nious fields including labor market (Liu et al. 2024), firm innovation (Cheng et al. 2022), marketing\n(Chen and Chan 2023, Su et al. 2024, Reisenbichler et al. 2022), artwork (Zhou and Lee 2024), and\nknowledge sharing (Burtch et al. 2023).\nOur contribution to this literature is threefold. First and most importantly, the nascent litera-\nture that examines the effect of GAI tools on user engagement has mostly focused on the context"}, {"title": "3. Field Setting, Experiment Design, and Data", "content": "We collaborated with one of the largest short-video platforms in Asia (Platform A), which has over\n300 million daily active users. Like TikTok, users on the platform can be either content producers\nor viewers. Producers post short videos on Platform A to enhance viewership/or engagement and\nattract new followers, aiming to increase advertising opportunities and revenue. Users visit Platform\nA either to be entertained by videos that catch their interest (organic browse) or to search for\nspecific videos related to a topic (search-oriented browse).\nViewers can consume videos and engage with others for free on Platform A. They engage with\nproducers mainly through viewership, but they can also like videos, leave comments, forward\ncontent to others both on and beyond Platform A, and follow producers for long-term video con-\nsumption and engagement. The platform generates revenue primarily through online advertising,\ni.e., disseminating advertising videos to viewers. Therefore, accurately matching the content with\nviewers to improve video consumption and engagement is crucial to Platform A's business model.\nContent distribution on Platform A is through two primary channels: organic recommendations\nand search-query-oriented recommendations. Organic recommendations generate a personalized\nvideo feed based on a user's viewing history and preference, catering to those browsing without\nactive searches. Search-query recommendations, on the other hand, respond directly to users' text-\nbased searches, tailoring content to match users' specific queries. Here, the video title, which can\ninclude hashtags and descriptions, is the only content metadata used by the recommender system\nto improve content relevance."}, {"title": "3.1. Research Context", "content": "We collaborated with one of the largest short-video platforms in Asia (Platform A), which has over\n300 million daily active users. Like TikTok, users on the platform can be either content producers\nor viewers. Producers post short videos on Platform A to enhance viewership/or engagement and\nattract new followers, aiming to increase advertising opportunities and revenue. Users visit Platform\nA either to be entertained by videos that catch their interest (organic browse) or to search for\nspecific videos related to a topic (search-oriented browse).\nViewers can consume videos and engage with others for free on Platform A. They engage with\nproducers mainly through viewership, but they can also like videos, leave comments, forward\ncontent to others both on and beyond Platform A, and follow producers for long-term video con-\nsumption and engagement. The platform generates revenue primarily through online advertising,\ni.e., disseminating advertising videos to viewers. Therefore, accurately matching the content with\nviewers to improve video consumption and engagement is crucial to Platform A's business model.\nContent distribution on Platform A is through two primary channels: organic recommendations\nand search-query-oriented recommendations. Organic recommendations generate a personalized\nvideo feed based on a user's viewing history and preference, catering to those browsing without\nactive searches. Search-query recommendations, on the other hand, respond directly to users' text-\nbased searches, tailoring content to match users' specific queries. Here, the video title, which can\ninclude hashtags and descriptions, is the only content metadata used by the recommender system\nto improve content relevance."}, {"title": "3.2. Experiment Design", "content": "To causally examine the values of AI-generated metadata, we conducted a field experiment on the\nvideo posting page to simulate the metadata input change that feeds into the video recommender\nsystem. This experiment lasted from July 20th to August 21st, 2023. Producers involved in our\nexperiment were randomly assigned to the control and treatment groups. Treatment group pro-\nducers could access an AI-generated title in the title-setting box on the video posting page after\nthey uploaded the video (see Figure 3(b)), and there is a notification next to the AI-generated title\nindicating that the provided title is generated by AI. In contrast, control group producers could\nnot access such a tool to generate titles via AI (see Figure 3(a)). In addition, in 2023, external\nGAI tools for generating video titles were unlikely to be widely used since major language models\ndid not support video processing, and few video platforms offered such features. Thus, the risk\nof contamination, where the control group was unintentionally influenced by the experimental in-\ntervention, was limited. Treatment producers had the flexibility to either delete, amend, or fully"}, {"title": "3.3. Data and Variables", "content": "Due to some technical issues, Platform A only stored the AI-generated titles between August\n8th and August 21st during the experiment. Thus, our dataset was segmented into two periods:\n(1) pre-treatment period: July 10th to July 19th and (2) treatment period: August 8th to August\n21st. Our study included 2,048,033 producers who posted at least one video during our treatment\nperiod, with 1,024,940 in the treatment group and 1,023,093 in the control group. During the\ntreatment period, producers in the treatment group uploaded 5,377,560 videos, while those in the\ncontrol group posted 5,361,424 videos. During the pre-treatment period, only 60.7% of videos had\ntitles, which indicates that metadata sparsity is prevalent on Platform A. For each producer, we\nobtained data for video viewership outcomes, producer characteristics, and video characteristics.\nTo accommodate variations in video posting times during the treatment period, we calculated the\ncumulative viewership outcomes for each video over the first two weeks after its posting (Zeng et al.\n2023)."}, {"title": "3.4. Randomization Check", "content": "To verify the randomization effectiveness, we compared treatment producers (N=1,024,940) and\ncontrol producers (N=1,023,093) on their pre-treatment video engagement outcomes, producer\ncharacteristics, and video attributes. The results of pairwise t-tests in Table 3 show no significant\ndifferences between treatment and control groups on these observable attributes. These results\nconfirm that the treatment and control producers in our sample were comparable, suggesting\nthat any difference between conditions after the experiment started should be attributed to our\nexperimental manipulation\u2014that is, whether producers had access to and/or adopted AI-generated\ntitles."}, {"title": "4. Effects of AI-generated Metadata on Content Consumption", "content": "Our investigation began by examining the effects of AI-generated metadata (i.e., titles) on the\nproducers' content consumption outcomes of posted videos. Motivated by past studies (Huang et\nal. 2021; Sun et al. 2019), we aimed to study two types of causal effects: (1) the effect of treatment\n(i.e., access to AI-generated titles) on video viewership (intention-to-treat effect, ITT); and (2) the\neffect of treatment-induced adoptions (i.e., adoption of AI-generated titles) on video viewership\n(local average treatment effect, LATE). Our unit of analysis was at the producer-video level to\ncapture changes in viewership outcomes for each video uploaded by producers."}, {"title": "4.1. Effects of Having Access to AI-generated Metadata on Content Consumption", "content": "We used the ordinary linear squares (OLS) regression specification with robust standard errors to\ncausally estimate the effects of having access to AI-generated titles on viewership outcomes:\nOutcomeij = Bo + \u03b2\u2081 Treat; + \u03b22 Controlsij + eij\n(1)\nwhere Treat is a binary indicator equal to 1 if the producer i was in the treatment group,\nControlsij includ all prior-mentioned producer-, video-, and day-level attributes, and eij is the\nerror term. Outcomeij represented our two viewership metrics, include ValidWatchij (the number\nof valid watches) and WatchDurationij (viewers' total watch duration). All continuous variables\nin Outcomeij were log-transformed, incremented by 1 to account for zero viewership outcomes,\nfollowing the semi-log approach in Cole and Sokolyk (2018). Highly-skewed control variables were\nalso log-transformed.\nThe model estimation results in Table 4 show that AI-generated titles boosted content consump-\ntion. Specifically, column (1) indicates an increase of 1.6%16 in valid watches in the treatment\ngroup compared to the control group (\u03b2\u2081 = 0.016, p-value < 0.01). Results in column (2) indicate\nthat treatment group videos enhanced watch duration by 0.9% from the control group (\u03b2\u2081 = 0.009,\np-value < 0.01). Given our sample covers 2% of total platform users, who posted over 10 million\nvideos during our experiment, this result translates to billions of additional valid watches and bil-\nlions of extra minutes in watch duration across the platform, demonstrating significant economic\nbenefits."}, {"title": "4.2. Results of Adopting AI-generated Metadata on Viewership Outcomes", "content": "To identify the effect of adopting AI-generated metadata, we cannot simply compare producers who\nadopted AI-generated titles with those who did not, because omitted variables (e.g., producers'\ninherent capability to generate titles) may drive both producers' decision to adopt AI-generated\ntitles and their subsequent video viewership outcomes. Instead, we used the random assignment\nof producers to the treatment group (Treat\u2081) as an instrumental variable (IV) for the adoption\ndecision of AI-generated titles (Huang et al. 2021, Sun et al. 2019). We employed the following\ntwo-stage least squares (2SLS) regression specification:\nAdoptij = Yo + \u00a51Treat; + \u00a52Controlsij + Eij\n(2)\nOutcomeij = lo + l\u2081Adopti; + 13Controlsij + Nij\n(3)\nwhere Eij and Nij are error terms. In Equation (2), Adoptij is a binary indicator for whether\nproducer i adopts an AI-generated title for video j, and is instrumented with Treat. In Equation\n(3), Adoptij refers to the instrumented Adoptij, i.e., the fitted value of Adoptij from Equation\n(2). B\u2081 is the coefficient of interest indicating LATE. Treat; is a valid IV for two reasons. First,\nit satisfies the relevance assumption as only the treatment group can access AI-generated titles,\nsignificantly influencing adoption. This is evidenced by a high first-stage F-statistic of 1,700,000.\nSecond, it satisfies the exclusion restriction because the treatment assignment is random and should\nnot correlate with other observed or unobserved covariates. Moreover, title generation occurs after\nvideo upload and just before posting, removing direct influence on video production.\nThe main effect results are presented in Table 8. The positive coefficients of Adoptij indicate that\nadopting AI-generated titles increased valid watches by 7.1% (p-value<0.01), and watch duration\nby 4.1% (p-value<0.01). These findings align with our ITT results but reflect a greater magnitude of"}, {"title": "4.3. Mechanism: AI-Generated Metadata Facilitates User-Content Matching", "content": "So far, we have shown that AI-generated titles significantly increased viewership outcomes. Next,\nwe explore the mechanism behind this effect. Given the importance of metadata in user-content\nmatching of recommender systems, we hypothesized that AI-generated titles improved video view-\nership by enhancing user-video matching accuracy. To illustrate, AI-generated titles probably help\nrecommender systems better interpret video content. This enhanced interpretation should enable\nthe system to more accurately predict which users are more likely to engage (e.g., view/like/share\nvideos or follow the producer) and recommend/match the video to these specific users. This im-\nproved user-video matching accuracy translates into the higher consumption outcomes observed in\nSections 4.1 and 4.2.\nTo evaluate user-video matching accuracy, we used the Area Under the ROC Curve (AUC),\na widely applied metric in recommender system studies (Chen et al. 2024, Bi et al. 2024). AUC\nmeasures how well the model predicts viewer engagement behaviors by comparing the predicted and\nactual viewer engagement outcomes. A higher AUC indicates more accurate user-video matching.\nTo calculate AUC, we collected a proprietary dataset from the platform's recommender system,\ndocumenting video recommendations from November 1st to November 30th, 2023 for videos posted\nduring our experiment.19 It included 93,618,096 records with details on predicted engagement\nprobabilities (i.e., like videos, share videos, and follow producers) and actual viewer behaviors for\neach user-video pair.\nWhile our main analysis focuses on viewership outcomes (e.g., valid watch and watch duration),\nthis additional dataset does not include predictions for these measures. Instead, it focuses on"}, {"title": "5. AI vs. Human-Generated Titles", "content": "Beyond the impact of AI-generated metadata on content consumption, we next explored whether\nand how human content producers can co-create with AI-generated metadata to further enhance\nconsumption outcomes, to address whether content producers should be offered the option to\nmodify AI-generated metadata."}, {"title": "5.1. Effect of Human-AI Co-creation on Content Consumption", "content": "We began our exploration by comparing the effectiveness of AI-generated titles to human-generated\ntitles. While Section 4 demonstrated that AI-generated titles boosted video viewership by ad-\ndressing title sparsity, their impact on videos that already have human-generated titles remained\nunclear. To investigate this, a direct comparison between titled videos in the treatment and con-\ntrol groups would be misleading. This is because some videos in the treatment group may only"}, {"title": "5.2. Effect of Human-AI Co-creation on Lexical Richness", "content": "To better understand how increased human input boosted video viewership, we used lexical rich-\nness, a key linguistic concept in language studies, signaling information quality, as our alternative\ndependent variable. We followed prior research (Qiao et al. 2020) and measured it through multiple\ndimensions, including lexical density (LexicalDensityij), lexical variation (LexicalVariationij),\nand entropy (Entropyij). Lexical richness is an important proxy for cognitive effort in text crafting\nand a signal of information quality. For example, Goes et al. (2014) used lexical density to measure\nthe informational value of reviews. Lexical density is the proportion of content words (such as\nnouns, verbs, and adjectives) to the total number of words in a text. Lexical variation is the ratio\nof unique words to the total number of words. Entropy quantifies text unpredictability, computed\nas:\nEntropy = -\u2211 Pk log Pk\n(5)"}, {"title": "6. Additional Analyses and Robustness Tests", "content": "This section is devoted to further discussions and analyses to supplement our main results. The\ndetailed regression results are relegated to Appendix D.\nViewership Diversity. In the main text, we focus on the economic impact of AI-generated\ntitles on content consumption, particularly consumption quantity (e.g., number of valid watch).\nHowever, their implications can be multifold, encompassing both quantity and diversity. This\nsection analyzed how the access to AI-generated titles affects video viewership diversity using the\nHerfindahl-Hirschman Index (HHI), a widely used measure of market concentration in economic\nand antitrust analyses (Narayanan et al. 2009). HHI is calculated by squaring the market share\nof each entity and summing the results, with values ranging from close to 0 to 1. A lower HHI\nindicates a more competitive environment, while a higher HHI signals dominance by one or a few\nlarge entities. In our analysis, we used valid watches to calculate HHI. The index ranged from 1/N\nto 1, where N was the total number of videos in our context. As shown in Table 19, the treatment\ngroup with access to AI-generated titles had a significantly lower HHI of 0.0002 compared to\n0.0003 in the control group, representing a 50% reduction in platform-level HHI. This indicates\na substantial increase in viewership diversity, aligning with our earlier finding that AI-generated\ntitles disproportionately benefited low-skilled producers. These results are consistent with recent\nGAI studies (Zhou and Lee 2024) and contribute to the growing body of research on GAI's impact\non socioeconomic inequality (Capraro et al. 2024).\nChannel Analysis. In Section 3.1, we have discussed that organic and search-oriented recom-\nmendations are two main video recommendation channels on Platform A. Building on this, we\nnext analyzed viewership outcomes for each channel separately and replicated the main analysis in\nEquation (1). As shown in Table 20 and Table 21, the positive coefficients of Treat are qualitatively\naligned with the results in Table 4. These results indicate that AI-generated titles improve content\nconsumption across both channels, reinforcing the effectiveness of AI-generated titles in enhancing\nuser-video matching.\nImpact on Content Production. One potential explanation for the boosted viewership is that\naccess to AI-generated titles changes users' video production behavior. For example, producers with\naccess to AI-generated titles might spend more time refining each video's content and producing\nfewer, but higher-quality, videos. To examine this possibility, we conducted a producer-level t-test\ncomparing both the total number of videos and the average time gaps (in hours) between videos"}, {"title": "7. Conclusion and Discussion", "content": "Previous research has shown that AI-generated content, such as advertisements, effectively en-\ngages users by enhancing content quality. However, the value of AI-generated content that does\nnot directly interact with users, such as metadata, remains less understood. To address this gap,\nwe conducted a randomized field experiment on a short-video platform where AI-generated titles,"}, {"title": "7.1. Practical Implications", "content": "Our results shed light on several important managerial implications. First, our study demonstrates\nthat AI-generated metadata can significantly boost content discovery by improving user-content\nmatching through mitigating metadata sparsity. Therefore, we encourage platform owners to invest\nin GAI tools that generate metadata, which can address operational challenges related to sparse\nmetadata. While much of the focus has been on GAI's ability to create user-facing content (e.g., ad-\nvertisements and articles), our results emphasize the equally crucial role of AI-generated metadata\nin improving platform operations and boosting content discovery. This is relevant for platforms\nwhere content consumption is primarily driven by recommendations, such as UGC platforms and\ne-commerce sites.\nSecond, by demonstrating that AI-generated metadata disproportionately benefits low-skilled\nproducers and hedonic-content videos, our work reveals the importance of tailoring platform strate-\ngies to support those most affected by metadata sparsity. Platforms should consider focusing their\nefforts on these segments when deciding whether to scale up the implementation of AI-generated\nmetadata tools and how to maximize their effectiveness. For example, platforms can prioritize\nrolling out these tools to groups more affected by metadata sparsity, such as novice producers, to\ngenerate the most immediate and noticeable impact on content consumption.\nThird, while GAI tools streamline video title generation, our results show that the quality of these\nAI-generated titles often falls short of human-generated ones. Therefore, rather than automatically\nintegrating AI-generated titles into their recommender systems, platforms are encouraged to display"}, {"title": "7.2. Limitations and Future Research", "content": "The limitations of our work open up interesting avenues for future research. First, while we fo-\ncus on the value of AI-generated metadata in improving user-content matching, we only explore\ncontent-based metadata. Future research could explore the role of user-related metadata, such as\nAI-generated user profiles. GAI can generate synthetic user profiles based on minimal inputs or\ndemographic similarities, which may help the system to better predict user preferences and im-\nprove matching accuracy. Additionally, examining how different types of AI-generated metadata\n(e.g., content and user metadata) interact or complement each other also deserves exploration.\nSecond, in our study, the GAI algorithm generated titles solely based on video content, without\nincorporating producer attributes or their historical content. Future research could explore how to\ndesign and improve AI-generated metadata to induce stronger effects. One potential direction is to\nincorporate producer-specific data, such as frequently used keywords from past videos or audience\nengagement patterns, to generate personalized AI-generated titles."}, {"title": "A. Pearson Correlation Matrix of Focal Variables", "content": "Pearson Correlation Matrix of Focal Variables\n(1) Treat\n(2) Adopt\n(3) Valid Watch\n(4) WatchDuration\n(5) Utilitarian\n(6) LowSkill\n(7) Follower\n(8) KOL\n(9) Experience\n(10) Following\n(11) Multihome\n(12) Female\n(13) Public Visible\n(14) Video Duration"}, {"title": "B. Propensity Score Matching (PSM) Results for Titled Videos", "content": "From the total of 10,738,984 videos, we first excluded 3,444,235 videos which do not have titles from\nboth the treatment and control groups. Additionally, we removed 2,226,922 titled videos (51.56%)\nin our treatment group that do not receive AI-generated titles due to algorithmic limitations. The\nfiltered sample consisted of 2,092,219 videos in the treatment group and 2,975,608 in the control\ngroup. Next, we used the Propensity Score Matching (PSM) method to identify a sample that\nwas similar in observed characteristics during the pre-treatment period. The matching followed\na two-step procedure: first, we ran a logit regression using the pre-treatment variables (i.e., all\nthe moderating and control variables mentioned in Section 3.3) and obtained predicted propensity\nscores for each unit. Second, we employed a one-to-many radius matching algorithm where all\ncontrol units for which the propensity scores fall within a pre-defined radius (also known as caliper)\nfrom the propensity scores of the treatment units are matched. This ensures multiple matches for\neach treated unit. We then applied weights in the subsequent analysis to account for this one-\nto-many structure. Next, we obtained a new sample after discarding unmatched units (3,885,089\nvideos are matched). To evaluate the matching quality, we performed t-tests of equality of means\nbefore and after matching to verify whether our matching has successfully balanced the attributes\nbetween the treatment and control group videos. The results in Table 17 show that the mean\ndifferences between the groups were no longer statistically significant, indicating that the matching\nprocess successfully reduced bias associated with observable attributes."}, {"title": "C. PSM Results for Videos with Low Textual Similarity", "content": "From the 2,092,219 titled videos in the treatment group and 2,975,608 in the control group (as\ndescribed in Online Appendix B), we first excluded 1,532,819 treatment videos with cosine sim-\nilarity to AI-generated titles higher than 20%. This filtering resulted in 559,400 videos in the\ntreatment group and 2,975,608 in the control group. Using the Propensity Score Matching (PSM)\nmethod with a radius matching algorithm, we identified a matched sample based on pre-treatment\ncharacteristics (i.e., all moderating and control variables in Section 3.3), following the detailed\nprocedure in Online Appendix B. After discarding unmatched units, the final matched sample\nincluded 3,533,720 videos. Post-matching t-tests of equality of means, shown in Table 18, indicate\nthat the mean differences between the treatment and control groups were no longer statistically\nsignificant, confirming that the matching process effectively reduced bias."}, {"title": "D. Additional Analyses and Robustness Tests", "content": "This section is devoted to further discussions and analyses to supplement our main results. The\ndetailed regression results are relegated to Appendix D."}, {"title": "D.1. Results for Viewership Diversity Analysis", "content": "For analyses reported in the main text (as explained in Section 4), we focused on the influence\nof AI-generated titles on video viewership. Here, we extend this analysis to explore their impact\non viewership diversity. We first divided the total videos in our sample into two groups: videos\nproduced by treatment group users and videos by control group users. We then calculated HHI\nbased on valid watches to measure viewership concentration for each group, defined as:\nHHI =\u2211 sj^2\n(6)\nwhere s; represents the market share of video j within its respective group, calculated as:\ns_i = V_j /\u2211 V_i\n(7)\nwhere V; is the number of valid watches for video j, and the denominator is the total number of\nvalid watches across all N videos within the group."}, {"title": "D.2. Results for Channel Analysis", "content": "In Section 4.3, we suggested that the positive effect of AI-generated titles on video viewership is\nattributed to enhanced recommendation accuracy. Theoretically, if true, this effect should benefit\nboth recommendation channels on Platform A (organic and search-oriented recommendations). To\ntest this, we used detailed video consumption data to identify the recommendation source for each\nwatch. Next, we aggregated the valid watch counts for each video by channel and replicated the\nanalysis in Equation (1) for each channel. The results, presented in Table 20 and 21, show a 0.3%\nincrease in both valid watches and watch duration for treatment group videos in the search-oriented\nchannel, and a stronger 1.5% increase in valid watches with a 0.9% increase in watch duration in\nthe organic channel. These findings validate the effectiveness of AI-generated titles in enhancing\nviewership metrics across both channels."}, {"title": "D.3. Results for Video Production Comparison", "content": "In Section 4.3, we suggested that the positive effect of AI-generated titles on video viewership is\nattributed to enhanced recommendation accuracy. An alternative explanation for the increased\nviewership could be a supply-side shift in production behavior. Specifically, treatment producers\nwith access to AI-generated titles might have altered their production behavior by focusing on\nproducing fewer but potentially higher-quality videos. This shift could theoretically enhance per-\nceived video quality, thereby increasing viewers' content consumption. To examine this possibility,\nwe conducted a producer-level t-test comparing the total number of videos and average time gaps\n(in hours) between videos produced by each producer in the treatment and control groups during\nthe treatment period. As shown in Table 22, there is no statistically significant difference in the\nnumber of videos produced (p-value > 0.1) and the time gaps between videos (p-value > 0.1) across\nthe two groups. These results suggest that the increase in viewership in the treatment group is\nunlikely to be driven by a change in producers' video production behavior."}, {"title": "D.4. Results for Alternative Operationalization of Variables", "content": "To ensure the robustness of our main findings, we employed alternative operationalizations for\nkey variables in our analysis. Given that the observed positive effect of AI-generated titles may\nvary depending on how viewership and adoption are defined, testing these alternative definitions\nallows us to verify the consistency and generalizability of our results. First, for viewership, we used\nthe number of watches(Watch\u017cj), complete watches (CompleteWatchij), and the number of likes\n(Likeij) per video as alternative dependent variables. These measures capture different aspects\nof viewer engagement and may offer additional insights into how AI-generated titles impact a\nrange of interactive forms of engagement. The ITT and LATE results, shown in Tables 23-26, are"}]}