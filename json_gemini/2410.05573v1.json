{"title": "TaeBench: Improving Quality of Toxic Adversarial Examples", "authors": ["Xuan Zhu", "Dmitriy Bespalov", "Liwen You", "Ninad Kulkarni", "Yanjun Qi"], "abstract": "Toxicity text detectors can be vulnerable to adversarial examples - small perturbations to input text that fool the systems into wrong detection. Existing attack algorithms are time-consuming and often produce invalid or ambiguous adversarial examples, making them less useful for evaluating or improving real-world toxicity content moderators. This paper proposes an annotation pipeline for quality control of generated toxic adversarial examples (TAE). We design model-based automated annotation and human-based quality verification to assess the quality requirements of TAE. Successful TAE should fool a target toxicity model into making benign predictions, be grammatically reasonable, appear natural like human-generated text, and exhibit semantic toxicity. When applying these requirements to more than 20 state-of-the-art (SOTA) TAE attack recipes, we find many invalid samples from a total of 940k raw TAE attack generations. We then utilize the proposed pipeline to filter and curate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically, we demonstrate that TaeBench can effectively transfer-attack SOTA toxicity content moderation models and services. Our experiments also show that TaeBench with adversarial training achieve significant improvements of the robustness of two toxicity detectors.", "sections": [{"title": "1 Introduction", "content": "Toxicity text detection systems are popular content moderators for flagging text that may be considered toxic or harmful. These toxicity detectors are frequently used in safety-concerned applications like LLM-based chatbots and face persistent threats from malicious attacks designed to circumvent and exploit them. Recent literature includes a suite of text adversarial attacks that generate targeted adversarial examples from seed inputs, fooling a toxicity detection classifier into predicting \"benign\" outputs, while the examples are semantically toxic. These targeted toxic adversarial examples (TAE) are critical in pinpointing vulnerability of state-of-the-art (SOTA) toxicity safeguard models or services. However, running existing TAE attacks directly against a new model is time consuming (Table A2), needs expert-level attack knowledge, and also results in many low-quality examples (see Table 1). This quality issue hinders using TAE attacks to sanity check the real-world toxicity detection services or using them as data augmentation strategies to perform effective adversarial training of toxicity detection models.\nWe, therefore, propose an annotation pipeline to conduct quality control of generated TAE. We define a successful TAE as a perturbed text input (from a seed) that fools a target toxicity model into producing \"benign\" outputs, is semantically toxic, is grammatically appropriate, and is natural like human-generated text (since non-natural TAE are easy to detect by a language model). Our quality annotation, therefore, focuses on three criteria: (1) the generated TAE are indeed semantically \"toxic\"; (2) these examples include few grammar issues; and (3) these examples are natural as human-generated text. For each criterion, we propose automated and human annotation-based strategies to measure and constrain these criteria. Following this, we run more than 20 \u03a4\u0391\u0395 recipes derived from 6 SOTA TAE attack"}, {"title": "2 Toxic Adversarial Examples (TAE) and Attack Recipes", "content": "This paper focuses on the TAE proposed by Bespalov et al. (2023) [3]. The main motivation of TAE attacks is that a major goal of real-world toxicity detection is to identify and remove toxic language. Adversarial attackers against toxicity detectors will focus on designing samples that are toxic in nature but can fool a target detector into making benign prediction (aka TAE). TAE attacks search for an adversarial example x' from a seed input x by satisfying a targeted goal function as follows: $G(F,x') = {F(x') = b; F(x) \\neq b}$ (Eq-1). Here b denotes the \"0:benign\" class. $F : X \\rightarrow Y$ is a given target toxicity text classifier.\nAdversarial attack methods design search strategies to transform a seed x to x' via transformation, so that x' fools F by achieving the fooling goal $G(F,x')$, and at the same time fulfilling a set of constraints. Therefore literature has split each text adversarial attack into four components: (1) goal function, (2) transformation, (3) search strategy, and (4) constraints between seed and its adversarial examples [24]. This modular design allows pairing the TAE goal function (Eq-1) with popular choices of other three components from the literature to obtain a large set of TAE attack recipes."}, {"title": "2.1 Running > 20 SOTA Recipes for a Large Unfiltered TAE Pool", "content": "The research community is still lacking a systematic understanding of the adversarial robustness of SOTA toxicity text detectors. Two major challenges exist: (1) running TAE attack recipes is quite time consuming; and (2) many generated TAE samples are invalid or ambiguous (see Table 1). For instance, Table A2 shows that the average runtime cost for running ToxicTrap [3] attack recipes against a binary toxicity classifier from 185k seed samples takes about 29.9 hours. It takes 6.6 hours to attack a multi-class toxicity detector from 2.5k seeds. To this end, we aim to develop a standardized dataset of toxic adversarial examples that covers a wide range of possible attack recipes and are of high quality.\nOur first step is to select a total of 25 TAE attack recipes to generate a large pool of raw TAE samples (see Section 4 for details of two seed datasets and three proxy toxicity detection models). Specifically, we use 20 variants of attack recipes proposed in ToxicTrap [3] that combine different transformation, constraint, and search strategy components. In addition to these ToxicTrap attack recipes, we select 5 algorithms from literature: DeepWordBug [11], TextBugger [23], A2T [30], PWWS [28], and TextFooler [22]. These algorithms were proposed to attack general language classifiers. We adapt these five attacks by replacing their goal functions with Eq-1. These 25 attack recipes cover a wide range of popular transformations, constraints, and search methods (details in Table A1).\nTransformation. The attack recipes use different character or word transformation components. We also include the recipes"}, {"title": "3 Improving TAE Quality with an Annotation Pipeline", "content": "As shown in Table 1, many examples generated by TAE attack recipes suffer from low-quality issues. We, therefore, propose an automatic pipeline to quality control raw TAE samples."}, {"title": "3.1 LLM Judge and Small Models based Automated Quality Controls", "content": "Our quality filter pipeline includes four steps: TAE deduplication. The attack recipes in Section 2.1 can lead to duplicates depending on seed inputs and recipe similarity. Our filtering is based on exact match and we obtain 50.7% unique TAE examples shown in (Table 2).\nPoor grammar detection. We then filter out samples that have poor grammar (such as bad noun plurality and noun-verb disagreement) using LanguageTool2.\nRemoving text of low naturalness. Next we remove samples with low text naturalness using an English acceptability classifier [26]. This classifier is fine-tuned from Huggingface TDA-BERT using a 3k labeled data we collect through human annotation. The human annotation guidelines on what defines \"text naturalness\" are in Section 3.2. We fine- tune the model with 2,370 labeled texts, and evaluate it with 593 held-out texts, following training setup in Section C.3. Table A3 shows that the F1 score (88.9%) of fine-tuned TDA- BERT improves 18% compared to F1 (70.5%) from pretrained TDA-BERT.\nLLM judge for Removing non-toxic invalid TAE samples. Now we design model-based automated strategy to keep only those TAE samples that are semantically toxic. We propose an ensemble approach for toxicity label filtering by combining :(1) in-context learning (ICL) prompted Mistral (Mistral-7B-Instruct-v0.1) [21] and (2) a fine-tuned toxigen-RoBERTa classifier [16] (via \"AND\"). For (1), Mistral ICL, we run a series of experiments to select the best ICL prompt formatting according to [17] and build 5-shot ICL prompting by selecting demonstrations from our TAE dataset (see the prompt in Table A5). The accuracy of best Mistral ICL prompting is 76%. For (2), we fine-tune Toxigen-Roberta with 3.2k human annotated data (see annotation guideline in Section C.2 and training set up in Section C.3) and achieve a F1 score of 94% (Table A4)."}, {"title": "3.2 Human Evaluation to Annotate TAE on Toxicity and Naturalness", "content": "We use human annotators to curate the toxicity and text naturalness of subsets of generated TAE examples. Three human annotators are asked to review the toxicity and three annotators are asked to annotate the text naturalness. The final label is assigned by unanimous vote, where a fourth adjudicator resolves any disagreements. (1) Toxicity is defined as \"issues that are offensive or detrimental, including hate speech, harassment, graphic violence, child exploitation, sexually explicit material, threats, propaganda, and other content that may cause psychological distress or promote harmful behaviors.\" (2) Text naturalness is defined as \"text that could be plausibly written by a human even if it includes 'internet language' that is outside 'school grammar'\"\nWe provide human annotation guidelines and examples in Section A6. We use the above human annotations to curate TAE samples in three different steps: (a) To curate fine- tuning training and test data for TDA-BERT model for filtering text naturalness. (b) \u03a4\u03bf curate fine-tuning training and test data for Toxigen-RoBERTa model for filtering toxicity labels. (c) To verify the quality of filtered TAE samples. We randomly sample 200 \u03a4\u0391\u0395 examples from each quality filtering step in our annotation pipeline shown in Table 2. The human annotated samples are then used to estimate the ratios of toxic and natural examples in data."}, {"title": "4 TaeBench and TaeBench+", "content": ""}, {"title": "4.1 TAE Generation with Proxy Models and Seeding Datasets", "content": "Running TAE attacks needs a set of text inputs that are toxic as seeds (denoted as x in Eq-1 of Section 2.1). We use the following two datasets as seeds for our TAE attacks.\nJigsaw: A dataset derived from the Wikipedia Talk Page dataset [1]. Wikipedia Talk Page allows users to comment, and the comments are labeled with toxicity levels. Comments that are not assigned any of the six toxicity labels are categorized as \"non toxic\". We can use this data for both binary and multi-label toxicity detection tasks.\nOffensive Tweet:Davidson et al. (2017) [7] used a crowd-sourced hate speech lexicon from Hatebase.org to collect tweets containing hate speech keywords. Each sample is labeled as one of three classes: those containing hate speech, those containing only offensive language, and those containing neither. This data is for multi- class toxicity detection.\nBesides, to generate TAEs we also need target toxicity detection models against which to run the attack recipes. Now we use one important property of adversarial attacks."}, {"title": "4.2 TaeBench: a Large Set of Quality Controlled TAE Samples", "content": "In Table 2, we pass a total of 936,742 raw TAES through the proposed quality filtering pipeline. We are able to select 264,672 examples (28.30% as of the original examples) as the filtered set, and we call it TaeBench.\nTo validate the quality of each step of filtering, we conduct human-in-the-loop annotations by randomly sampling 200 TAES from each filtering step to evaluate the ratios of toxic and natural examples. In Table 2, human validation shows that, after filtering, the toxicity ratios are improved by 5.64% in the selected examples (94.17%) compared to unfiltered examples (88.53%). The text naturalness ratios are improved by 6.36%, from (79.63%) in the unfiltered examples to (85.99%) in the selected examples."}, {"title": "4.3 TaeBench+: Benign Seeds Derived Adversarial Examples", "content": "TAE are semantic-toxic samples that fool toxicity detection models into making benign predictions. Essentially they are false negative predictions (assuming \"toxic\" is the positive class). Related, it is also interesting to understand and search for those semantic- benign samples that fool a target model into making toxic predictions. These samples belong to false positive inputs. We call them \"benign adversarial examples (BAE)\" in the rest of this paper.\nTo search for BAE, we design its goal function as: $G(F,x') = {F(x') \\neq b; F(x) = b}$ (Eq-2), where b denotes the benign class. Starting from benign seeds (F(x) = b), we perturb x into x' by pushing the prediction of x' to not be benign anymore. We can reuse the TAE attack recipes by keeping their transformation, search and constraint components intact, and replace the goal function into the above equation (Eq-2).\nEmpirically, we run the 25 BAE attacks following the same setup as TaeBench, obtaining 102,667 raw BAE examples (searching for BAE seems harder than searching for TAE). Table A8 shows how we conduct automated filtering following the same workflow as obtaining TaeBench. Differently, in the label-toxicity filtering step, we keep those benign-labeled BAE samples. Finally, we add the filtered BAE examples to create TaeBench+, a new variation of the TaeBench dataset. We provide the additional benefits of"}, {"title": "5 Example Use Cases of TaeBench and TaeBench+", "content": ""}, {"title": "5.1 Benefit I: Benchmark Toxicity Detectors via Transfer Attacks", "content": "To evaluate the efficacy of the filtered TAE examples, we conduct transfer attack experiments to benchmark four SOTA toxicity classifiers: detoxify (detoxify-unbiased) [15], Llama Guard\u00b3 [19], OpenAI Moderation API4, and Nemo Guardrails (with GPT-3.5-turbo) [27]. Using TaeBench in transfer attacks can save resources and minimize the effort needed to generate TAE examples plus with data quality guarantees. Also the transfer attack set up is indeed a (major) real-world use case of using TAE. In this black-box transfer attack setup, TAE are constructed offline (like what we have done using many existing TAE attack recipes to attack local proxy models), then get them used to attack a target victim model or service.\nWe use attack success rate (ASR = $\\frac{\\text{# of successful attacks}}{\\text{# of total attacks}}$) to measure how successful a set of transfer attack TAE examples are at attacking a victim model. In Table 4, we report ASR obtained from the test splits of TaeBench (data details in Table 3). The ASR from TaeBench is essentially the false negative rate (FNR) calculated as dividing the number of predicted false negative by the size of used TaeBench samples.\nWe observe even the best performing model (NeMo Guardrails) exhibits ASR (FNR) of 8.94% and 7.31% from the TaeBench-Jigsaw- test and TaeBench-OffensiveTweet-test. Then OpenAI-Moderation achieves ASR (FNR) of 21.68% and 36.41%. Furthermore, we use Table A9 to showcase the change of ASR (FNR) from using Jigsaw seed toxic samples to using TaeBench Jigsaw test. The FNR increases from seed to TaeBench indicating the effectiveness of generated TAE examples."}, {"title": "5.2 Benefit II: Improve Toxicity Detectors w. Adversarial Training", "content": "We also showcase how a vanilla adversarial training with TaeBench can help increase the adversarial robustness of a toxicity detector even against unseen attacks. Here, adversarial training introduces the TAE adversarial data into the training of a DistilBERT or detoxify model together with the Jigsaw Binary train split. More dataset details are in Table 3.\nIn Table 5, we report the impacts of using TaeBench for adversarial training. We train DistilBERT/detoxify models using (a) Jigsaw- train only (No TAE); (b) Jigsaw-train + extra unfiltered TAE samples (TAE-Unfiltered); and (c) Jigsaw-train + TaeBench. We sample the unfiltered TAE data such that TAE-Unfiltered has the same size as TaeBench to have a fair comparison on model performance by removing the impact of data set size. We observe that the model trained with Jigsaw- train + TaeBench achieves significantly lower ASR (14.58% and 23.25% FNR for DistilBERT and detoxify, respectively), being more robust than the model trained with no adversarial training (74.99% and 54.28% ASR/FNR). It is also better than or close to the one trained with random sampling augmentation (16.55% and 22.92% ASR/FNR). We also observe that these three data augmentations impact the classification metrics from the Jigsaw test set very minimally (<2% impact on F1 and AUC scores in Table 5). Training setups are described in Section C.3."}, {"title": "5.3 Variation: Adding TaeBench+", "content": "In Table 5, when we augment the training data with TaeBench+, the model achieves the lowest ASR (FNR) of 12.66% and 22.80% on TaeBench-test for DistilBERT and detoxify, respectively. We further oversample the benign adversarial examples in TaeBench+ during data augmentation. We name it as balanced TaeBench+ and it aims to balance off the size difference between toxic and benign adversarial examples in TaeBench+. This revised augmentation helps the trained model achieve the lowest or second lowest ASR (FPR) of 53.02% and 3.97% on the (TaeBench+)-test- benign samples. ASR of running the BAE examples is essentially false positive rate (FPR)."}, {"title": "6 Connecting to Related Works", "content": "Literature has included no prior work on the quality control of adversarial examples from toxicity text detectors. Literature includes just a few studies on adversarial examples for toxicity text classifiers. One recent study [18] tried to deceive Google's perspective API for toxicity identification by misspelling the abusive words or by adding punctuation between letters. Another recent study [3] proposed the concept of \"toxic adversarial examples\" and a novel attack called ToxicTrap attack.\nQuality control of Text Adversarial Examples. Performing quality control of data sets used by deep learning (whether in training or during testing) is essential to ensure and enhance the overall performance and reliability of deep learning systems [10, 29, 14]. [25] proposed a set of language constraints to filter out undesirable text adversarial examples, including limits on the ratio of words to perturb, minimum angular similarity and the Part-of-Speech match constraint. The study investigated how these constraints were used to ensure the perturbation generated examples preserve the semantics and fluency of original seed text in two synonym substitution attacks against NLP classifiers. This study found the perturbations from these two attacks often do not preserve semantics, and 38% generated examples introduce grammatical errors. Two related studies from [8, 5] also revealed that word substitution based attack methods generate a large fraction of invalid substitution words that are ungrammatical. Both papers focus on only word substitution- based attacks attacking the general NLP classification cases, and both did not show the benefit of filtered examples.\nAdversarial Examples in Natural Language Processing. Adversarial attacks create adversarial examples designed to cause a deep learning model to make a mistake. First proposed in the image domain by [13], adversarial examples provide effective lenses to measure a deep learning system's robustness. Recent techniques that create adversarial text"}, {"title": "7 Conclusion", "content": "In this paper, we present a model-based pipeline for quality control in in the generation of TAE. By evaluating 20+ TAE attack recipes, we curate a high-quality benchmark TaeBench. We demonstrate TaeBench's effectiveness in assessing the robustness of real-world toxicity content moderation models, and show that adversarial training using TaeBench improves toxicity detectors' resilience against unseen attacks."}, {"title": "A Limitations", "content": "While our study represents a pioneering attempt at implementing quality control for TAEs, it faces certain limitations. First, the TAEs used in our research are derived from attacks on two seed datasets, Jigsaw and OffensiveTweet. We acknowledge that additional toxic datasets exist but are not utilized due to the high computational and time costs of TAE generation.\nSecondly, we perform human annotation only a subset of the generated TAEs to calculate the quality score, and recognize that a larger scale annotation could yield more precise quality metrics. However, in our work we emphasize that data annotation is expensive and requires skilled annotators given the sensitive nature of the content in TAEs. Additionally, as the field lacks extensive studies on the quality of annotating TAEs, we develop straightforward yet effective annotation guidelines, contributing valuable insights to ongoing research in this area."}, {"title": "B Risks and Ethical Considerations", "content": "Our research aims to enhance the quality of large volumes of TAEs through a combined model- and annotation-based filtering process. We develop an efficient pipeline that employs models fine-tuned on a subset of TAES annotated by a specially trained human team. Before beginning their work, annotators are informed about the nature of the toxic data they will be working with, and written consent is obtained. It's important to note that while our approach significantly reduces the presence of low-quality TAEs, it does not eliminate all such instances, though minimizing them is our primary objective."}, {"title": "C Appendix on Methods", "content": ""}, {"title": "C.1 Human Annotators", "content": "We use an internal annotator team based in United States to perform the annotation jobs. We disclose the disclaimer of potential risk that contents may contain racism, sexuality, or other undesired contents. We obtain consent from the annotators. The data annotation protocol is approved by our ethics review board. Annotation guidelines are listed in Table A6."}, {"title": "C.2 Human Annotation of Training Data", "content": "We use human annotation to create training data to fine-tune TDA-BERT and toxigen- ROBERTa respectively. TDA-BERT training data are labeled on naturalness, while toxigen- ROBERTa is labeled on toxicity. Annotation guidelines and examples for toxicity and naturalness are in Appendix A6. In each case, we stratified-sample a total of 3.4k generated TAEs from each recipe. (i.e. We remove the 3.4k TAE examples before passing the remaining 940k TAE examples to our filtering pipeline to create TaeBench.) Three human annotators are asked to review the toxicity and naturalness. The final label is assigned by unanimous vote, where a fourth adjudicator resolves any disagreements. Then we remove the UNSURE class in both annotation jobs, and split the remaining labeled data into train (80%) and test (20%) sets to fine-tune the models."}, {"title": "C.3 Training Configuration", "content": "Below we list our model training configurations:\nFine-tuning TDA-Bert. We train the TDA-BERT model up to 10 epochs (with early stopping) using the default AdamW optimizer with learning rate as 1-e05 and weight decay as 0.01. The training job is run using a batch size as 32 on an NVIDIA A10G GPU (same below).\nFine-tuning Toxigen. We fine-tune the Toxigen-ROBERTa model up to 5 epochs (with early stopping) using AdamW optimizer with learning rate as 1-e05, weight decay as 0.01, 5 warm up steps, and a batch size as 16.\nTraining DistillBERT and detoxify. We train the DistilBERT and detoxify models up to 5 epochs using AdamW optimizer with learning rate as 2.06-e05, the \u201ccosine with restarts learning rate\" scheduler, and 50 warm up steps."}, {"title": "C.4 On Three Local Proxy Models for Text Toxicity Detection", "content": "Our proxy models try to cover three different toxicity classification tasks: binary, multilabel, and multiclass; over two different transformer architectures: DistillBERT and BERT; and across two datasets: the large-scale Wikipedia Talk Page dataset Jigsaw data and the Offensive Tweet for hate speech detection dataset. Table 3 lists two datasets' statistics.\nOur three local proxy models (toxicity text detectors) cover two transformer architectures. We use \"distilbert-base-uncased\" pre-trained transformers model for DistilBERT architecture. For BERT architecture, we use \"GroNLP/hateBERT\" pre-trained model. All texts are tokenized up to the first 128 tokens. The train batch size is 64 and we use AdamW optimizer with 50 warm-up steps and early stopping with patience 2. The models are trained on NVIDIA T4 Tensor Core GPUs and NVIDIA Tesla V100 GPUs with 16 GB memory, 2nd generation Intel Xeon Scalable Processors with 32GB memory and high frequency Intel Xeon Scalable Processor with 61GB memory."}, {"title": "D Appendix on Results", "content": ""}]}