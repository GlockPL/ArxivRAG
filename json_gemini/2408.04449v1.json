{"title": "RiskAwareBench: Towards Evaluating Physical Risk\nAwareness for High-level Planning of LLM-based\nEmbodied Agents", "authors": ["Zihao Zhu", "Bingzhe Wu", "Zhengyou Zhang", "Baoyuan Wu"], "abstract": "The integration of large language models (LLMs) into robotics significantly en-\nhances the capabilities of embodied agents in understanding and executing complex\nnatural language instructions. However, the unmitigated deployment of LLM-based\nembodied systems in real-world environments may pose potential physical risks,\nsuch as property damage and personal injury. Existing security benchmarks for\nLLMs overlook risk awareness for LLM-based embodied agents. To address\nthis gap, we propose RiskAwareBench, an automated framework designed to as-\nsess physical risks awareness in LLM-based embodied agents. RiskAwareBench\nconsists of four modules: safety tips generation, risky scene generation, plan\ngeneration, and evaluation, enabling comprehensive risk assessment with mini-\nmal manual intervention. Utilizing this framework, we compile the PhysicalRisk\ndataset, encompassing diverse scenarios with associated safety tips, observations,\nand instructions. Extensive experiments reveal that most LLMs exhibit insuffi-\ncient physical risk awareness, and baseline risk mitigation strategies yield limited\nenhancement, which emphasizes the urgency and cruciality of improving risk\nawareness in LLM-based embodied agents in the future.", "sections": [{"title": "1 Introduction", "content": "One of the long-term goals of AI and robotics is to enable embodied agents to understand natural\nlanguage instructions and perform complex tasks [14]. Recent advances in large language models\n(LLMs) have demonstrated a profound capacity for understanding, reasoning, and planning leading to\nsignificant enhancements across various domains [21]. LLMs have acquired an extensive repository\nof world knowledge and task execution strategies by learning from vast amounts of multimodal data.\nConsequently, contemporary research is investigating the application of LLMs within the realm of\nrobotics [27], positioning them as the brain for embodied AI systems, which equips the agents with\nhigh-level task plans, enabling robots to exhibit a human-like understanding and decision-making\nproficiency in task execution [3, 19, 4, 22, 9]."}, {"title": "2 Related Work", "content": "Nevertheless, deploying embodied agents into the real physical world carries potential safety hazards\nthat could pose risks to the environment, property, and even human safety, dubbed as physical risks.\nFor instance, in a kitchen scenario equipped solely with metal utensils, where a housekeeping robot\ninstructed to \"heat food with the microwave and utensils\", we empirically find that most LLMs fail to\nrecognize the implicit danger that \u201cmicrowaving metal can lead to the production of electric arcs,\npotentially damaging the microwave or even starting a fire\u201d. As a result, these robots may formulate\nhigh-level plans with potential physical risks, such as \u201cplace a metal plate inside the microwave\u201d\nand \"turn on the microwave\u201d. Therefore, prior to the practical deployment of LLM-based embodied\nagents, assessing their awareness of physical risks among the high-level task plans is crucial for\nachieving safe embodied intelligence.\nRecent benchmarks have been proposed to evaluate the safety of LLMs. For example, Safety-\nBench [28] presents a comprehensive benchmark for evaluating the safety of LLMs using multiple-\nchoice questions covering various safety concerns. ToolEmu [16] introduces a framework for scalable\ntesting of LLM agents by emulating tool execution and evaluating safety risks. ASSERT [10] empha-\nsizes the robustness of LLMs through Automated Safety ScEnario Red Teaming, which generates a\nsuite of prompts to evaluate the model's performance under various robustness settings. R-Judge [26]\nfocuses on evaluating the behavioral safety of LLM agents within interactive environments by analyz-\ning agent interaction records. Although existing studies have made significant strides in evaluating\nthe safety and robustness of LLMs, they often overlook the specific aspect of physical risk awareness\nfor embodied agents operating in diverse environment.\nConsidering the limitations of current safety evaluations, in this paper, we propose RiskAwareBench,\nwhich aims to fill this gap by providing an automated framework for benchmarking the capability\nof LLM-based embodied agents to identify and mitigate potential physical risks in real-world\nenvironments. This framework comprises four key modules: safety tip generation module, scene\ngeneration module, plan generation module, and evaluation module. The safety tip generation module\nis responsible for generating safety tips and corresponding explanations for common environments.\nThe scene generation module creates the detailed scene information and observation as well as natural\nlanguage instructions for embodied agents based on the specific safety tip. The format of scene\nobservation varies, depending on the type of LLM that serves as the core of the embodied agent. In\nthe plan generation module, the LLM-based embodied agent generates high-level plans based on\nscene observations and instructions for guiding the downstream low-level control. The evaluation\nmodule includes risk evaluation and effectiveness evaluation, where the former evaluates whether the\ntask planning contains implicit physical risks and identifies specific dangerous steps, while the latter\nevaluates whether the plans can efficiently complete the task as instructed. All the aforementioned\nmodules are driven by large foundation models, enabling the automated evaluation of physical risk\nawareness for embodied agents, thereby reducing the requirement for manual intervention.\nUtilizing RiskAwareBench, we collect the PhysicalRisk dataset, which encompasses safety tips, scene\nobservations, and robotic instructions across various scenarios. Moreover, we conduct extensive\nexperiments to assess the physical risk awareness of LLM-based embodied agents. The baseline\nexperiments explore the influence of various popular LLMs with different sizes. The findings indicate\nthat most LLMs lack physical risk awareness. Furthermore, we introduced several baseline risk\nmitigation strategies to enhance the risk awareness of LLM-based embodied agents. The results show\nminimal improvement, underscoring the urgency of advancing physical risk awareness in embodied\nintelligent systems.\nIn summary, our contributions are as follows: 1) We reveal the potential physical risks of deploying\nLLM-based embodied intelligent systems in the real world. 2) We propose a benchmark framework\nthat enables automatic evaluation of physical risk awareness of LLM-based embodied agents. 3)\nBased on the proposed framework, we construct a dataset that contains various scenarios along with\nrisky scenes and instructions. 4) Extensive experiments are conducted to compare various popular\nLLMs as the high-level planner of embodied agents."}, {"title": "2.1 LLMs in Embodied Task Planning", "content": "Building on the demonstrated prowess of LLMs in intricate reasoning and contextual generalization,\nrecent years have seen a growing interest in leveraging LLMs for embodied task planning [25, 23]."}, {"title": "2.2 Safety Evaluation of LLM Agents", "content": "The evaluation of safety in LLM Agents has garnered significant attention, with various studies\nproposing benchmarks and frameworks to assess different safety aspects of these models. SAFE-\nTEXT [8] highlights the susceptibility of state-of-the-art LLMs to generating unsafe text and their\ndifficulty in rejecting unsafe advice, emphasizing the need for further research in commonsense\nphysical safety. SafetyBench [28] provides a comprehensive benchmark for evaluating the safety\nof LLMs using multiple-choice questions across seven distinct safety categories. This benchmark\nfacilitates evaluation in both Chinese and English, revealing substantial room for improving the\nsafety of current LLMs despite the advantages shown by models like GPT-4. ToolEmu [16] offers a\nscalable testing framework by emulating tool execution and evaluating safety risks associated with\nLLM agents. This framework identifies potential failures and quantifies associated risks, providing a\nquantitative risk analysis of current LM agents. However, it primarily focuses on tool interactions\nrather than physical risk awareness in real-world scenarios. ASSERT [10] emphasizes the robustness\nof LLMs through Automated Safety ScEnario Red Teaming, generating prompts to evaluate model\nperformance under various robustness settings. This approach provides a fine-grained analysis of\nmodel performance across different safety domains but does not specifically address the physical risks\nencountered by embodied agents. R-Judge [26] benchmarks the behavioral safety of LLM agents\nwithin interactive environments by analyzing agent interaction records. It evaluates the proficiency of\nLLMs in judging safety risks and highlights the importance of salient safety risk feedback. Despite its\ncomprehensive evaluation, R-Judge centers on interaction records rather than proactive physical risk\nidentification. While these studies have advanced LLM safety evaluations, they often overlook physi-\ncal risk awareness for LLM-based embodied agents, which RiskAwareBench specifically addresses\nby benchmarking their ability to identify and mitigate real-world physical risks."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 LLM-based Embodied AI", "content": "LLM-based embodied AI refers to the integration of Large Language Models (LLMs) within the realm\nof robotics to create embodied agents, which are capable of interacting with the physical environment"}, {"title": "3.2 Physical Risk", "content": "The high-level plans generated by the planning module, while indispensable for guiding the robot's\nactions, can inadvertently introduce physical risks if not meticulously managed. Physical risk refers\nto the propensity for the robot to inflict harm upon humans, property, or the environment as a\nconsequence of executing these plans. This risk can emanate from a multitude of sources, including\nbut not limited to, collisions with objects, mishandling of fragile items, non-standard operations on\nhazardous goods, combinations of incompatible objects, and etc. Given the critical implications of\nphysical risks, this paper focuses on evaluating the risk awareness of LLM-based embodied agents to\nensure their future safe deployment in real-world environments."}, {"title": "4 RiskAwareBench", "content": "To comprehensively evaluate the physical risk awareness of LLM-based embodied agents, we\nintroduce an automated evaluation framework, named RiskAwareBench. Then, with the help of\nRiskAwareBench, we construct a PhysicalRisk dataset, which contains various risky scenes and\ncorresponding task instructions which imply potential physical risks. In the following sections, we\nwill describe the framework of RiskAwareBench and the construction of the PhysicalRisk dataset in\ndetail."}, {"title": "4.1 Overall Framework", "content": "As illustrated in Figure 2, the framework of RiskAwareBench consists of four key modules, including\nsafety tip generation module, risky scene generation module, plan generation module, and evaluation\nmodule. First, the safety tip generation module is responsible for generating safety tips based on the\nrisky scenes and task instructions. Then, the risky scene generation module creates diverse scenes and\ntask instructions that may lead to potential physical risks. Afterwards, the plan generation module\ngenerates high-level plans to complete the instructed task based on the observation of risky scenes.\nFinally, the evaluation module evaluates riskiness and effectiveness of the generated plans. The\ndetailed description of each module is as follows."}, {"title": "4.2 Safety Tips Generation Module", "content": "The safety tips generation module plays a pivotal role in the RiskAwareBench framework by proac-\ntively identifying and communicating potential safety hazards to prevent physical risks in various\nenvironments. This module is inspired by the notion that in real life, adherence to safety standards and\nprecautions, as established through expert consensus and experiential learning, significantly reduces\nthe likelihood of hazards. For instance, the user manual of a microwave oven explicitly warns against\nheating metal objects, guiding users away from potential hazards. Drawing on this principle, the\nsafety tip generation module is designed to generate comprehensive safety tips for given scenes. Our\napproach encompasses two distinct strategies for generating these safety tips: summarizing safety\ntips from existing materials and generating new safety tips. The prompts used in this module are\npresented in Appendix A.1."}, {"title": "Strategy 1: Summarizing Safety Tips from Existing Materials", "content": "Recognizing the vast array of\nsafety manuals and guidelines available online, the first strategy involves a two-step process. Initially,\nthe module conducts a targeted search to locate relevant safety instructions pertaining to the given\nscene. Subsequently, the module extracts valuable safety tips from these documents based on the\nunderstanding abilities of LLMs. This process not only ensures the provision of established safety\nmeasures but also filters out irrelevant information, thereby optimizing the relevance and utility of the\nsafety tips generated."}, {"title": "Strategy 2: Generating New Safety Tips", "content": "The second strategy is employed particularly in sce-\nnarios where existing online materials are sparse or insufficient. In such cases, the module uses the\nexisting safety tips extracted with strategy 1 and leverages the in-context learning capabilities of\nLLMs to generate new safety tips that are tailored to the specific needs and contexts of the scenario at\nhand. This innovative approach allows for the comprehensive generation of safety tips, filling the\ngaps left by existing materials and ensuring comprehensive coverage of potential physical risks."}, {"title": "4.3 Risky Scene Generation Module", "content": "Following the generation of safety tips, it is critical to contextualize these precautions within environ-\nments where the physical risks they address may occur. The Risky scene generation module is devised\nto automatically create such environments, along with detailed instructions for embodied agents to"}, {"title": "Objects", "content": "At the core of every scene are the objects, the tangible entities that populate the envi-\nronment. These include items directly related to the generated safety tips, like the microwave and\nmetal utensils for the tip \"Do not heat metal items in a microwave.\" To mimic the complexity of\nreal-world environments and to add layers of challenge, objects not associated with the safety tip\nare also integrated, such as tables, cabinets, glassware, sinks, and faucets. Each object is uniquely\nidentified with a name_id format (e.g.,, spoon_1, spoon_2), allowing for distinction between identical\nobjects within the scene."}, {"title": "Positions of Objects", "content": "The spatial arrangement of objects is a crucial aspect of scene generation,\ndictating how the objects relate to one another within the space. This is described using the format\n<Object_1, Relation, Object_2>, where the Relation is a preposition defining spatial relationships,\nlike \"in, above, or beside\". An example could be \u201cmicrowave_1 above cabinet_1,\" establishing a\nclear and structured layout of the scene."}, {"title": "Attributes of Objects", "content": "To enhance the diversity and authenticity of the scenes, attributions are\nassigned to objects. These characteristics can include material properties (e.g.,, the chair is made of\nwooden), states (e.g.,, the door of the microwave is opened), or contents (e.g.,, the glass is filled\nwith orange juice). These attributions not only enrich the scene's description but also influence the\ndecision-making process of the agent, as different attributes can present varying levels of risk."}, {"title": "Task Instructions", "content": "Apart from above necessary scene information, this module is also responsible\nfor generating task instructions for embodied agents to follow. The instructions should be described\nin natural language, mimicking tasks that users might perform in their daily lives. The instructions\nneed be explicit and challenging to test decision-making skills in complex situations. Crucially, since\nthe framework's objective is to assess the agent's risk awareness, the instructions are designed to\npotentially lead the agent to plan actions with inherent physical risks. An example might be \u201cPlace\nthe metal tray inside the microwave and set the timer for 3 minutes on high power.\u201d This instrustion,\nwhile straightforward, invites the agent to engage in an action that poses a marked safety hazard,\nthereby evaluating its risk assessment capabilities."}, {"title": "Textual Observation", "content": "The textual observation synthesizes the scene's elements into a coherent\nnatural language description. This format is particularly suited for agents whose core is text-only\nLLMs. For example, a textual observation might be \"In the kitchen, you see microwave_1 mounted\nabove cabinet_1. On the countertop lies spoon_1 with a metal bowl_1, . . .\u201d This detailed narrative\nenables the agent to comprehend the scene's complexity and identify the risks associated with the\nenvironment and the tasks to be performed."}, {"title": "Visual Observation", "content": "To extend the framework's utility to multimodal LLMs, visual observations\nare generated using text-to-image diffusion models. These models translate the detailed information\nof the scene into a visual context, akin to the perspective a robot's camera might capture. This visual\ndata encapsulates the scene's spatial dynamics and the interplay of various elements, providing a rich\nvisual context for agents that process image input."}, {"title": "4.4 Plan Generation Module", "content": "The plan generation module is a crucial component of the framework, where the LLM serves as\nthe decision-maker within the emobodied agent, responsible for generating high-level plans from"}, {"title": "4.5 Evaluation Module", "content": "The evaluation module is the critical final step in the framework, where the high-level plans generated\nby the previous module are evaluated for risk and effectiveness. Drawing inspiration from related\nworks that have successfully demonstrated the capabilities of LLMs in achieving human-paralleled\naccuracy in assessing complex tasks. we adopts LLMs as an evaluator, which enables automatic\nevaluation. The prompts used in this module are presented in Appendix A.4."}, {"title": "Risk Evaluation", "content": "While LLM-based embodied agents have a powerful understanding of language\nand context, may not inherently possess an awareness of physical risks. Hence, it is imperative to\nassess where these plans could lead to hazardous outcomes, especially considering the potential\nphysical hazards embedded within the scenes and instructions through scene generation module.\nWith carefully designed prompts, the evaluator can output not only whether there is a risk, but also\nwhich steps are risky."}, {"title": "Effectiveness Evaluation", "content": "The effectiveness evaluation aims to assess the quality of the generated\nplans in terms of robot executableness. Due to the LLM Hallucinations, the LLM-based embodied\nagents may generate plans than include actions outside the robot's skill set, resulting in execution\nfailures, which means that the plans are ineffective. On the contrary, if all the plans are valid and\nexecutable according to the skill set, the plans are considered effective."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setup", "content": "PhysicalRisk Dataset. Utilizing the RiskAwareBench framework's first two modules, we have\nmeticulously compiled the PhysicalRisk dataset, which is an extensive collection of samples from\ndiverse environments such as kitchen, bathroom, laboratory, factory, and etc. Each sample is a\ncomprehensive unit consisting of an environment name, associated safety tip with an explanation,\ncorresponding detailed scene information (including the list of objects, their positions, properties,\nand various modalities of scene observation), and task instruction that could violate the safety tip.\nThe PhysicalRisk dataset encompasses a comprehensive collection of 4,605 samples, distributed\nacross 15 distinct environments. In particular, we generate a total of 307 safety tips, each paired\nwith a constructed risky scene encompassing objects, positions, and attributes. For every risky scene,\nwe formulate one textual observation and four visual observations. Furthermore, we create multiple\ndistinct task instructions for each safety tip. As a result, this methodology has produced a total of 921\ntextual risk scenarios and 3,684 multimodal scenarios, consequently augmenting the thoroughness\nand dependability of our evaluation.\nBaseline LLMs. To comprehensively evaluate the risk awareness of LLMs as high-level planners\nfor embodied agents, we assessed two categories of LLMs: unimodal text-based LLMs and\nmultimodal LLMs. We included LLMs of varying scales, encompassing both open-source and\nproprietary models. The unimodal text-based LLMs include gpt-3.5-turbo-1106 [11],\nLlama-3-8b-chat-hf and Llama-3-70b-chat-hf [20], Qwen1.5-7B-Chat\nand\nQwen1.5-72B-Chat [2], Qwen2-72B-Instruct [24], Mistral-7B-Instruct-v0.1 [6]\nand Mistral-8x7B-Instruct-v0.1 [7]. The multimodal LLMs include gpt-40 [12] and\ngpt-40-mini [13], claude-3-haiku [1], gemini-1.5-flash [15]. All LLMs were evaluated on\nthe text subset of the PhysicalRisk dataset, while only the multimodal LLMs were further assessed\non the multimodal subset.\nEvaluation Metrics. As described in Section 4.5, the evaluation module is divided into risk\nevaluation and effectiveness evaluation. We use the Task Risk Rate (TRR) to measure the risk"}, {"title": "5.2 Main Results", "content": "Tables 1 and 2 present the main results of RiskAwareBench. Based on the results, we derive the\nfollowing key findings.\nAll LLMs exhibit poor risk awareness. Table 1 presents the performance of well-known unimodal\ntext-based LLMs and multimodal LLMs on the textual cases of the PhysicalRisk dataset. Meanwhile,\nTable 2 displays the results of multimodal LLMs on the multimodal portion of the dataset. We find\nthat the Task Risk Rate (TRR) for all LLMs exceeds 90%, indicating that current implementations\nof LLMs as decision-makers for embodied agents lack robust risk awareness. For instance, gpt-40\nexhibited a TRR of 93.950%, while Llama-3-70b-chat-hf showed a TRR of 95.662%. Such high\nTRR values underscore the inadequate performance of LLMs in generating safe high-level plans.\nBigger LLMs achieve better risk awareness in general. We further analyze the impact of different\nmodel sizes on risk awareness. The results suggest that within the same model series, bigger models\ngenerally exhibit better risk awareness. This trend is evident when comparing models of different\nscales within the same family. For example, Llama-3-70b-chat-hf has a TRR of 95.662%, which is\nslightly lower than the 96.005% TRR of Llama-3-8b-chat-hf. Similarly, the Qwen1.5-72B-Chat\nmodel, with a TRR of 94.977%, outperforms its smaller counterparts such as Qwen1.5-7B-Chat,\nwhich has TRR of 95.548%. This finding holds true for the Mixtral series as well. Such results\nindicate that scaling laws are also valid for the risk awareness of LLMs.\nClosed-source LLMs show relatively better risk awareness than open-source LLMs. Despite\nthe overall poor performance of all LLMs in risk awareness, a comparison between closed-source and"}, {"title": "Visual observation is more challenging than textual observation", "content": "By comparing the results in\nTable 1 and Table 2, we find that, for the same model, the task risk rate is higher when embodied\nagents receive textual observations compared to visual observations. For example, the TPP of gpt-40\nin Table 1 is 93.950%, while in Table 2, the TRR is 97.516%. Similar results can also be found for\nclaude-3-haiku and gemini-1.5-flash.\nIn summary, such findings underscore the challenges and potential directions for improving the risk\nawareness of LLM-based planners under embodied settings. While larger and closed-source models\nshow relative improvements, there is a clear need for further research and development to enhance\nthe safety and effectiveness of these models in high-level planning tasks."}, {"title": "5.3 Case Study", "content": "Figure 3 showcases examples of safety tips, scene information, task instructions, and observations\ngenerated for a kitchen environment. Constructing such detailed and contextually accurate scene\ninformation manually is a challenging and time-consuming task. However, leveraging carefully\ndesigned prompts, LLMs can accurately generate diverse and detailed scene information automat-\nically, which significantly enhances the efficiency and scalability of creating realistic and varied\nenvironments for evaluating risk awareness.\nFigure 4 shows high-level plans generated by LLM-based embodied agents under three different\nsettings. The left image shows plans without any risk mitigation strategy. Notably, steps 10-13\ninvolve a risk operation: heating eggs in a microwave. If executed in the real world, these steps could\npose significant risks. The middle image shows plans with an implicit risk mitigation strategy, which\nstill includes risk steps. The right one, however, shows plans with an explicit risk mitigation strategy,\nwhere the LLM provides a warning to the user. These examples highlight the weak risk awareness\nof LLM-based embodied agents and underscores the necessity of advanced strategies to mitigate\npotential risks."}, {"title": "6 Risk Mitigation Strategies", "content": "To enhance the risk awareness of LLM-based embodied agents, we propose two fundamental prompt-\nbased risk mitigation strategies: the implicit risk mitigation strategy (RM-Implicit)and the explicit\nrisk mitigation (RM-Explicit) strategy. Given the robust instruction comprehension capabilities of\nLLMs, these strategies aim to incorporate additional safety constraints into the input prompts to guide\nthe model in generating risk-free plans. Below, we detail each strategy and present the experimental\nresults."}, {"title": "Implicit risk mitigation strategy (RM-Implicit)", "content": "In the RM-Implicit strategy, we incorporate\ngeneral safety guidelines into the prompt to implicitly remind the model to consider potential risks\nwhen generating plans. The added prompt is structured as follows:\n\"When generating the plans, you should consider the safety of the plans to avoid\nany potential risks or hazards in the real world. The risks include object damage,\nhuman injury, environmental damage, generation of toxic substances and pollutants,\nor any other negative consequences that may arise from the actions performed by\nthe robot. The risks may come from various factors, including but not limited to\ncollisions with objects, improper handling of fragile objects, unsafe movements\nin dynamic environments, non-standard operations on objects, or combinations\nof incompatible objects. When you think one step may cause a potential risk, you\nshould avoid generating such a step in the plans and add \"Warning: \" at the\nbeginning of the step to point out the risk.\u201d\nThis strategy leverages the model's inherent understanding of general safety principles to mitigate\nrisks without specifying detailed instructions for each scenario."}, {"title": "Explicit risk mitigation strategy (RM-Explicit)", "content": "Contrary to the RM-Implicit strategy, the RM-\nExplicit strategy involves incorporating more detailed and scenario-specific safety rules into the\nprompt, explicitly requiring the model to consider these risks. Specifically, we directly include the\nsafety tips generated by the safety tip module for the current scenario into the prompt, ensuring that\nthe model adheres to these safety tips when generating plans."}, {"title": "Mitigation results", "content": "Table 3 presents the task risk rate of different risk mitigation strategies in\ncomparison to the baseline. The results reveal significant variations in the effectiveness of different risk\nmitigation strategies across various LLMs. For gpt-40, both RM-Implicit and RM-Explicit strategies\nmarkedly enhance the model's risk awareness, with TRR values of 49.206% and 43.915%, respectively.\nThis improvement can be attributed to the superior instruction comprehension capabilities of gpt-40,\nwhich allows it to better integrate and act upon the provided safety guidelines. In contrast, for the\nless advanced gpt-3.5-turbo-1106, the instruction comprehension ability is limited, resulting in\ninconsistent improvements in risk awareness using these prompt-based strategies. The RM-Implicit\nstrategy achieves a TRR of 79.365%, whereas the RM-Explicit strategy results in a TRR of 90.651%.\nThese findings suggest that while prompt-based strategies can be beneficial, their effectiveness\nis contingent upon the underlying model's capacity to interpret and follow complex instructions.\nOverall, these strategies provide limited enhancement, underscoring the importance of developing\nmore effective Risk Mitigation Strategies in the future. Further research is necessary to devise\nadvanced methods that can consistently improve the risk awareness of LLM-based embodied agents\nacross different model architectures."}, {"title": "7 Conclusion", "content": "In this paper, we propose RiskAwareBench, an innovative framework designed to evaluate the physical\nrisk awareness of LLM-based embodied AI system. Through the creation of the PhysicalRisk dataset\nand a series of comprehensive experiments, we have demonstrated that current LLMs, despite their\nadvanced language processing capabilities, often lack the necessary risk awareness to operate safely\nin real-world environments. The high Task Risk Rates (TRR) across various models, both open-\nsource and closed-source, highlight a significant gap in safety that must be addressed. Moreover,\nthe risk mitigation strategies proposed in this paper represent initial steps towards enhancing safety.\nHowever, the limited improvements observed suggest that these strategies are not sufficiently robust\nto address the complex risks inherent in physical environments. This calls for more sophisticated\nand context-aware approaches that can dynamically adapt to the nuances of different scenarios. As\nLLMs merge with robotics, safety must be prioritized to ensure that technological advancement does\nnot come at the expense of human well-being or environmental integrity. The RiskAwareBench\nframework and the associated dataset provide a valuable tool for the AI and robotics community to\nfurther explore and enhance the safety of LLM-based embodied AI systems."}]}