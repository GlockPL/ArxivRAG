{"title": "RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents", "authors": ["Zihao Zhu", "Bingzhe Wu", "Zhengyou Zhang", "Baoyuan Wu"], "abstract": "The integration of large language models (LLMs) into robotics significantly enhances the capabilities of embodied agents in understanding and executing complex natural language instructions. However, the unmitigated deployment of LLM-based embodied systems in real-world environments may pose potential physical risks, such as property damage and personal injury. Existing security benchmarks for LLMs overlook risk awareness for LLM-based embodied agents. To address this gap, we propose RiskAwareBench, an automated framework designed to assess physical risks awareness in LLM-based embodied agents. RiskAwareBench consists of four modules: safety tips generation, risky scene generation, plan generation, and evaluation, enabling comprehensive risk assessment with minimal manual intervention. Utilizing this framework, we compile the PhysicalRisk dataset, encompassing diverse scenarios with associated safety tips, observations, and instructions. Extensive experiments reveal that most LLMs exhibit insufficient physical risk awareness, and baseline risk mitigation strategies yield limited enhancement, which emphasizes the urgency and cruciality of improving risk awareness in LLM-based embodied agents in the future.", "sections": [{"title": "1 Introduction", "content": "One of the long-term goals of AI and robotics is to enable embodied agents to understand natural language instructions and perform complex tasks [14]. Recent advances in large language models (LLMs) have demonstrated a profound capacity for understanding, reasoning, and planning leading to significant enhancements across various domains [21]. LLMs have acquired an extensive repository of world knowledge and task execution strategies by learning from vast amounts of multimodal data. Consequently, contemporary research is investigating the application of LLMs within the realm of robotics [27], positioning them as the brain for embodied AI systems, which equips the agents with high-level task plans, enabling robots to exhibit a human-like understanding and decision-making proficiency in task execution [3, 19, 4, 22, 9]."}, {"title": "2 Related Work", "content": "Nevertheless, deploying embodied agents into the real physical world carries potential safety hazards that could pose risks to the environment, property, and even human safety, dubbed as physical risks. For instance, in a kitchen scenario equipped solely with metal utensils, where a housekeeping robot instructed to \"heat food with the microwave and utensils\", we empirically find that most LLMs fail to recognize the implicit danger that \u201cmicrowaving metal can lead to the production of electric arcs, potentially damaging the microwave or even starting a fire\u201d. As a result, these robots may formulate high-level plans with potential physical risks, such as \u201cplace a metal plate inside the microwave\u201d and \"turn on the microwave\u201d. Therefore, prior to the practical deployment of LLM-based embodied agents, assessing their awareness of physical risks among the high-level task plans is crucial for achieving safe embodied intelligence.\nRecent benchmarks have been proposed to evaluate the safety of LLMs. For example, Safety-Bench [28] presents a comprehensive benchmark for evaluating the safety of LLMs using multiple-choice questions covering various safety concerns. ToolEmu [16] introduces a framework for scalable testing of LLM agents by emulating tool execution and evaluating safety risks. ASSERT [10] emphasizes the robustness of LLMs through Automated Safety ScEnario Red Teaming, which generates a suite of prompts to evaluate the model's performance under various robustness settings. R-Judge [26] focuses on evaluating the behavioral safety of LLM agents within interactive environments by analyzing agent interaction records. Although existing studies have made significant strides in evaluating the safety and robustness of LLMs, they often overlook the specific aspect of physical risk awareness for embodied agents operating in diverse environment.\nConsidering the limitations of current safety evaluations, in this paper, we propose RiskAwareBench, which aims to fill this gap by providing an automated framework for benchmarking the capability of LLM-based embodied agents to identify and mitigate potential physical risks in real-world environments. This framework comprises four key modules: safety tip generation module, scene generation module, plan generation module, and evaluation module. The safety tip generation module is responsible for generating safety tips and corresponding explanations for common environments. The scene generation module creates the detailed scene information and observation as well as natural language instructions for embodied agents based on the specific safety tip. The format of scene observation varies, depending on the type of LLM that serves as the core of the embodied agent. In the plan generation module, the LLM-based embodied agent generates high-level plans based on scene observations and instructions for guiding the downstream low-level control. The evaluation module includes risk evaluation and effectiveness evaluation, where the former evaluates whether the task planning contains implicit physical risks and identifies specific dangerous steps, while the latter evaluates whether the plans can efficiently complete the task as instructed. All the aforementioned modules are driven by large foundation models, enabling the automated evaluation of physical risk awareness for embodied agents, thereby reducing the requirement for manual intervention.\nUtilizing RiskAwareBench, we collect the PhysicalRisk dataset, which encompasses safety tips, scene observations, and robotic instructions across various scenarios. Moreover, we conduct extensive experiments to assess the physical risk awareness of LLM-based embodied agents. The baseline experiments explore the influence of various popular LLMs with different sizes. The findings indicate that most LLMs lack physical risk awareness. Furthermore, we introduced several baseline risk mitigation strategies to enhance the risk awareness of LLM-based embodied agents. The results show minimal improvement, underscoring the urgency of advancing physical risk awareness in embodied intelligent systems.\nIn summary, our contributions are as follows: 1) We reveal the potential physical risks of deploying LLM-based embodied intelligent systems in the real world. 2) We propose a benchmark framework that enables automatic evaluation of physical risk awareness of LLM-based embodied agents. 3) Based on the proposed framework, we construct a dataset that contains various scenarios along with risky scenes and instructions. 4) Extensive experiments are conducted to compare various popular LLMs as the high-level planner of embodied agents."}, {"title": "2.1 LLMs in Embodied Task Planning", "content": "Building on the demonstrated prowess of LLMs in intricate reasoning and contextual generalization, recent years have seen a growing interest in leveraging LLMs for embodied task planning [25, 23]."}, {"title": "2.2 Safety Evaluation of LLM Agents", "content": "The evaluation of safety in LLM Agents has garnered significant attention, with various studies proposing benchmarks and frameworks to assess different safety aspects of these models. SAFETEXT [8] highlights the susceptibility of state-of-the-art LLMs to generating unsafe text and their difficulty in rejecting unsafe advice, emphasizing the need for further research in commonsense physical safety. SafetyBench [28] provides a comprehensive benchmark for evaluating the safety of LLMs using multiple-choice questions across seven distinct safety categories. This benchmark facilitates evaluation in both Chinese and English, revealing substantial room for improving the safety of current LLMs despite the advantages shown by models like GPT-4. ToolEmu [16] offers a scalable testing framework by emulating tool execution and evaluating safety risks associated with LLM agents. This framework identifies potential failures and quantifies associated risks, providing a quantitative risk analysis of current LM agents. However, it primarily focuses on tool interactions rather than physical risk awareness in real-world scenarios. ASSERT [10] emphasizes the robustness of LLMs through Automated Safety ScEnario Red Teaming, generating prompts to evaluate model performance under various robustness settings. This approach provides a fine-grained analysis of model performance across different safety domains but does not specifically address the physical risks encountered by embodied agents. R-Judge [26] benchmarks the behavioral safety of LLM agents within interactive environments by analyzing agent interaction records. It evaluates the proficiency of LLMs in judging safety risks and highlights the importance of salient safety risk feedback. Despite its comprehensive evaluation, R-Judge centers on interaction records rather than proactive physical risk identification. While these studies have advanced LLM safety evaluations, they often overlook physical risk awareness for LLM-based embodied agents, which RiskAwareBench specifically addresses by benchmarking their ability to identify and mitigate real-world physical risks."}, {"title": "3 Preliminaries", "content": null}, {"title": "3.1 LLM-based Embodied AI", "content": "LLM-based embodied AI refers to the integration of Large Language Models (LLMs) within the realm of robotics to create embodied agents, which are capable of interacting with the physical environment"}, {"title": "3.2 Physical Risk", "content": "The high-level plans generated by the planning module, while indispensable for guiding the robot's actions, can inadvertently introduce physical risks if not meticulously managed. Physical risk refers to the propensity for the robot to inflict harm upon humans, property, or the environment as a consequence of executing these plans. This risk can emanate from a multitude of sources, including but not limited to, collisions with objects, mishandling of fragile items, non-standard operations on hazardous goods, combinations of incompatible objects, and etc. Given the critical implications of physical risks, this paper focuses on evaluating the risk awareness of LLM-based embodied agents to ensure their future safe deployment in real-world environments."}, {"title": "4 RiskAwareBench", "content": "To comprehensively evaluate the physical risk awareness of LLM-based embodied agents, we introduce an automated evaluation framework, named RiskAwareBench. Then, with the help of RiskAwareBench, we construct a PhysicalRisk dataset, which contains various risky scenes and corresponding task instructions which imply potential physical risks. In the following sections, we will describe the framework of RiskAwareBench and the construction of the PhysicalRisk dataset in detail."}, {"title": "4.1 Overall Framework", "content": "As illustrated in Figure 2, the framework of RiskAwareBench consists of four key modules, including safety tip generation module, risky scene generation module, plan generation module, and evaluation module. First, the safety tip generation module is responsible for generating safety tips based on the risky scenes and task instructions. Then, the risky scene generation module creates diverse scenes and task instructions that may lead to potential physical risks. Afterwards, the plan generation module generates high-level plans to complete the instructed task based on the observation of risky scenes. Finally, the evaluation module evaluates riskiness and effectiveness of the generated plans. The detailed description of each module is as follows."}, {"title": "4.2 Safety Tips Generation Module", "content": "The safety tips generation module plays a pivotal role in the RiskAwareBench framework by proactively identifying and communicating potential safety hazards to prevent physical risks in various environments. This module is inspired by the notion that in real life, adherence to safety standards and precautions, as established through expert consensus and experiential learning, significantly reduces the likelihood of hazards. For instance, the user manual of a microwave oven explicitly warns against heating metal objects, guiding users away from potential hazards. Drawing on this principle, the safety tip generation module is designed to generate comprehensive safety tips for given scenes. Our approach encompasses two distinct strategies for generating these safety tips: summarizing safety tips from existing materials and generating new safety tips. The prompts used in this module are presented in Appendix A.1.\nStrategy 1: Summarizing Safety Tips from Existing Materials. Recognizing the vast array of safety manuals and guidelines available online, the first strategy involves a two-step process. Initially, the module conducts a targeted search to locate relevant safety instructions pertaining to the given scene. Subsequently, the module extracts valuable safety tips from these documents based on the understanding abilities of LLMs. This process not only ensures the provision of established safety measures but also filters out irrelevant information, thereby optimizing the relevance and utility of the safety tips generated.\nStrategy 2: Generating New Safety Tips. The second strategy is employed particularly in scenarios where existing online materials are sparse or insufficient. In such cases, the module uses the existing safety tips extracted with strategy 1 and leverages the in-context learning capabilities of LLMs to generate new safety tips that are tailored to the specific needs and contexts of the scenario at hand. This innovative approach allows for the comprehensive generation of safety tips, filling the gaps left by existing materials and ensuring comprehensive coverage of potential physical risks."}, {"title": "4.3 Risky Scene Generation Module", "content": "Following the generation of safety tips, it is critical to contextualize these precautions within environments where the physical risks they address may occur. The Risky scene generation module is devised to automatically create such environments, along with detailed instructions for embodied agents to"}, {"title": "Objects.", "content": "At the core of every scene are the objects, the tangible entities that populate the environment. These include items directly related to the generated safety tips, like the microwave and metal utensils for the tip \"Do not heat metal items in a microwave.\" To mimic the complexity of real-world environments and to add layers of challenge, objects not associated with the safety tip are also integrated, such as tables, cabinets, glassware, sinks, and faucets. Each object is uniquely identified with a $name\\_id$ format (e.g.,, $spoon\\_1$, $spoon\\_2$), allowing for distinction between identical objects within the scene."}, {"title": "Positions of Objects.", "content": "The spatial arrangement of objects is a crucial aspect of scene generation, dictating how the objects relate to one another within the space. This is described using the format $<Object\\_1, Relation, Object\\_2>$, where the Relation is a preposition defining spatial relationships, like \"in, above, or beside\". An example could be \u201cmicrowave_1 above cabinet_1,\" establishing a clear and structured layout of the scene."}, {"title": "Attributes of Objects.", "content": "To enhance the diversity and authenticity of the scenes, attributions are assigned to objects. These characteristics can include material properties (e.g.,, the chair is made of wooden), states (e.g.,, the door of the microwave is opened), or contents (e.g.,, the glass is filled with orange juice). These attributions not only enrich the scene's description but also influence the decision-making process of the agent, as different attributes can present varying levels of risk."}, {"title": "Task Instructions.", "content": "Apart from above necessary scene information, this module is also responsible for generating task instructions for embodied agents to follow. The instructions should be described in natural language, mimicking tasks that users might perform in their daily lives. The instructions need be explicit and challenging to test decision-making skills in complex situations. Crucially, since the framework's objective is to assess the agent's risk awareness, the instructions are designed to potentially lead the agent to plan actions with inherent physical risks. An example might be \u201cPlace the metal tray inside the microwave and set the timer for 3 minutes on high power.\u201d This instrustion, while straightforward, invites the agent to engage in an action that poses a marked safety hazard, thereby evaluating its risk assessment capabilities.\nNext, this module need to produce observations about the simulated scene, which serve as the input for the agents' subsequent planning process. These observations are carefully designed to be compatible with various types of embodied agents, ensuring broad applicability of the RiskAwareBench framework. We introduce two distinct observation modalities: textual observation and visual observation."}, {"title": "Textual Observation.", "content": "The textual observation synthesizes the scene's elements into a coherent natural language description. This format is particularly suited for agents whose core is text-only LLMs. For example, a textual observation might be \"In the kitchen, you see microwave_1 mounted above cabinet_1. On the countertop lies spoon_1 with a metal bowl_1, . . .\u201d This detailed narrative enables the agent to comprehend the scene's complexity and identify the risks associated with the environment and the tasks to be performed."}, {"title": "Visual Observation.", "content": "To extend the framework's utility to multimodal LLMs, visual observations are generated using text-to-image diffusion models. These models translate the detailed information of the scene into a visual context, akin to the perspective a robot's camera might capture. This visual data encapsulates the scene's spatial dynamics and the interplay of various elements, providing a rich visual context for agents that process image input."}, {"title": "4.4 Plan Generation Module", "content": "The plan generation module is a crucial component of the framework, where the LLM serves as the decision-maker within the emobodied agent, responsible for generating high-level plans from"}, {"title": "4.5 Evaluation Module", "content": "The evaluation module is the critical final step in the framework, where the high-level plans generated by the previous module are evaluated for risk and effectiveness. Drawing inspiration from related works that have successfully demonstrated the capabilities of LLMs in achieving human-paralleled accuracy in assessing complex tasks. we adopts LLMs as an evaluator, which enables automatic evaluation. The prompts used in this module are presented in Appendix A.4.\nRisk Evaluation. While LLM-based embodied agents have a powerful understanding of language and context, may not inherently possess an awareness of physical risks. Hence, it is imperative to assess where these plans could lead to hazardous outcomes, especially considering the potential physical hazards embedded within the scenes and instructions through scene generation module. With carefully designed prompts, the evaluator can output not only whether there is a risk, but also which steps are risky.\nEffectiveness Evaluation. The effectiveness evaluation aims to assess the quality of the generated plans in terms of robot executableness. Due to the LLM Hallucinations, the LLM-based embodied agents may generate plans than include actions outside the robot's skill set, resulting in execution failures, which means that the plans are ineffective. On the contrary, if all the plans are valid and executable according to the skill set, the plans are considered effective."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experimental Setup", "content": "PhysicalRisk Dataset. Utilizing the RiskAwareBench framework's first two modules, we have meticulously compiled the PhysicalRisk dataset, which is an extensive collection of samples from diverse environments such as kitchen, bathroom, laboratory, factory, and etc. Each sample is a comprehensive unit consisting of an environment name, associated safety tip with an explanation, corresponding detailed scene information (including the list of objects, their positions, properties, and various modalities of scene observation), and task instruction that could violate the safety tip. The PhysicalRisk dataset encompasses a comprehensive collection of 4,605 samples, distributed across 15 distinct environments. In particular, we generate a total of 307 safety tips, each paired with a constructed risky scene encompassing objects, positions, and attributes. For every risky scene, we formulate one textual observation and four visual observations. Furthermore, we create multiple distinct task instructions for each safety tip. As a result, this methodology has produced a total of 921 textual risk scenarios and 3,684 multimodal scenarios, consequently augmenting the thoroughness and dependability of our evaluation.\nBaseline LLMs. To comprehensively evaluate the risk awareness of LLMs as high-level planners for embodied agents, we assessed two categories of LLMs: unimodal text-based LLMs and multimodal LLMs. We included LLMs of varying scales, encompassing both open-source and proprietary models. The unimodal text-based LLMs include gpt-3.5-turbo-1106 [11], Llama-3-8b-chat-hf and Llama-3-70b-chat-hf [20], Qwen1.5-7B-Chat and Qwen1.5-72B-Chat [2], Qwen2-72B-Instruct [24], Mistral-7B-Instruct-v0.1 [6] and Mistral-8x7B-Instruct-v0.1 [7]. The multimodal LLMs include gpt-40 [12] and gpt-40-mini [13], claude-3-haiku [1], gemini-1.5-flash [15]. All LLMs were evaluated on the text subset of the PhysicalRisk dataset, while only the multimodal LLMs were further assessed on the multimodal subset.\nEvaluation Metrics. As described in Section 4.5, the evaluation module is divided into risk evaluation and effectiveness evaluation. We use the Task Risk Rate (TRR) to measure the risk"}, {"title": "5.2 Main Results", "content": "Tables 1 and 2 present the main results of RiskAwareBench. Based on the results, we derive the following key findings.\nAll LLMs exhibit poor risk awareness. Table 1 presents the performance of well-known unimodal text-based LLMs and multimodal LLMs on the textual cases of the PhysicalRisk dataset. Meanwhile, Table 2 displays the results of multimodal LLMs on the multimodal portion of the dataset. We find that the Task Risk Rate (TRR) for all LLMs exceeds 90%, indicating that current implementations of LLMs as decision-makers for embodied agents lack robust risk awareness. For instance, gpt-40 exhibited a TRR of 93.950%, while Llama-3-70b-chat-hf showed a TRR of 95.662%. Such high TRR values underscore the inadequate performance of LLMs in generating safe high-level plans.\nBigger LLMs achieve better risk awareness in general. We further analyze the impact of different model sizes on risk awareness. The results suggest that within the same model series, bigger models generally exhibit better risk awareness. This trend is evident when comparing models of different scales within the same family. For example, Llama-3-70b-chat-hf has a TRR of 95.662%, which is slightly lower than the 96.005% TRR of Llama-3-8b-chat-hf. Similarly, the Qwen1.5-72B-Chat model, with a TRR of 94.977%, outperforms its smaller counterparts such as Qwen1.5-7B-Chat, which has TRR of 95.548%. This finding holds true for the Mixtral series as well. Such results indicate that scaling laws are also valid for the risk awareness of LLMs.\nClosed-source LLMs show relatively better risk awareness than open-source LLMs. Despite the overall poor performance of all LLMs in risk awareness, a comparison between closed-source and"}, {"title": "5.3 Case Study", "content": "Figure 3 showcases examples of safety tips, scene information, task instructions, and observations generated for a kitchen environment. Constructing such detailed and contextually accurate scene information manually is a challenging and time-consuming task. However, leveraging carefully designed prompts, LLMs can accurately generate diverse and detailed scene information automatically, which significantly enhances the efficiency and scalability of creating realistic and varied environments for evaluating risk awareness.\nFigure 4 shows high-level plans generated by LLM-based embodied agents under three different settings. The left image shows plans without any risk mitigation strategy. Notably, steps 10-13 involve a risk operation: heating eggs in a microwave. If executed in the real world, these steps could pose significant risks. The middle image shows plans with an implicit risk mitigation strategy, which still includes risk steps. The right one, however, shows plans with an explicit risk mitigation strategy, where the LLM provides a warning to the user. These examples highlight the weak risk awareness of LLM-based embodied agents and underscores the necessity of advanced strategies to mitigate potential risks."}, {"title": "6 Risk Mitigation Strategies", "content": "To enhance the risk awareness of LLM-based embodied agents, we propose two fundamental prompt-based risk mitigation strategies: the implicit risk mitigation strategy (RM-Implicit)and the explicit risk mitigation (RM-Explicit) strategy. Given the robust instruction comprehension capabilities of LLMs, these strategies aim to incorporate additional safety constraints into the input prompts to guide the model in generating risk-free plans. Below, we detail each strategy and present the experimental results."}, {"title": "Implicit risk mitigation strategy (RM-Implicit).", "content": "In the RM-Implicit strategy, we incorporate general safety guidelines into the prompt to implicitly remind the model to consider potential risks when generating plans. The added prompt is structured as follows:\n\"When generating the plans, you should consider the safety of the plans to avoid any potential risks or hazards in the real world. The risks include object damage, human injury, environmental damage, generation of toxic substances and pollutants, or any other negative consequences that may arise from the actions performed by the robot. The risks may come from various factors, including but not limited to collisions with objects, improper handling of fragile objects, unsafe movements in dynamic environments, non-standard operations on objects, or combinations of incompatible objects. When you think one step may cause a potential risk, you should avoid generating such a step in the plans and add \"Warning: at the beginning of the step to point out the risk.\u201d\"\nThis strategy leverages the model's inherent understanding of general safety principles to mitigate risks without specifying detailed instructions for each scenario."}, {"title": "Explicit risk mitigation strategy (RM-Explicit).", "content": "Contrary to the RM-Implicit strategy, the RM-Explicit strategy involves incorporating more detailed and scenario-specific safety rules into the prompt, explicitly requiring the model to consider these risks. Specifically, we directly include the safety tips generated by the safety tip module for the current scenario into the prompt, ensuring that the model adheres to these safety tips when generating plans."}, {"title": "Mitigation results.", "content": "Table 3 presents the task risk rate of different risk mitigation strategies in comparison to the baseline. The results reveal significant variations in the effectiveness of different risk mitigation strategies across various LLMs. For gpt-40, both RM-Implicit and RM-Explicit strategies markedly enhance the model's risk awareness, with TRR values of 49.206% and 43.915%, respectively. This improvement can be attributed to the superior instruction comprehension capabilities of gpt-40, which allows it to better integrate and act upon the provided safety guidelines. In contrast, for the less advanced gpt-3.5-turbo-1106, the instruction comprehension ability is limited, resulting in inconsistent improvements in risk awareness using these prompt-based strategies. The RM-Implicit strategy achieves a TRR of 79.365%, whereas the RM-Explicit strategy results in a TRR of 90.651%. These findings suggest that while prompt-based strategies can be beneficial, their effectiveness is contingent upon the underlying model's capacity to interpret and follow complex instructions. Overall, these strategies provide limited enhancement, underscoring the importance of developing more effective Risk Mitigation Strategies in the future. Further research is necessary to devise advanced methods that can consistently improve the risk awareness of LLM-based embodied agents across different model architectures."}, {"title": "7 Conclusion", "content": "In this paper, we propose RiskAwareBench, an innovative framework designed to evaluate the physical risk awareness of LLM-based embodied AI system. Through the creation of the PhysicalRisk dataset and a series of comprehensive experiments, we have demonstrated that current LLMs, despite their advanced language processing capabilities, often lack the necessary risk awareness to operate safely in real-world environments. The high Task Risk Rates (TRR) across various models, both open-source and closed-source, highlight a significant gap in safety that must be addressed. Moreover, the risk mitigation strategies proposed in this paper represent initial steps towards enhancing safety. However, the limited improvements observed suggest that these strategies are not sufficiently robust to address the complex risks inherent in physical environments. This calls for more sophisticated and context-aware approaches that can dynamically adapt to the nuances of different scenarios. As LLMs merge with robotics, safety must be prioritized to ensure that technological advancement does not come at the expense of human well-being or environmental integrity. The RiskAwareBench framework and the associated dataset provide a valuable tool for the AI and robotics community to further explore and enhance the safety of LLM-based embodied AI systems."}]}