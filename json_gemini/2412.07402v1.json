{"title": "Non-Progressive Influence Maximization in Dynamic Social Networks", "authors": ["Yunming Hui", "Shihan Wang", "Melisachew Wudage Chekol", "Stevan Rudinac", "Inez Maria Zwetsloot"], "abstract": "The influence maximization (IM) problem involves identifying a set of key individuals in a social network who can maximize the spread of influence through their network connections. With the advent of geometric deep learning on graphs, great progress has been made towards better solutions for the IM problem. In this paper, we focus on the dynamic non-progressive IM problem, which considers the dynamic nature of real-world social networks and the special case where the influence diffusion is non-progressive, i.e., nodes can be activated multiple times. We first extend an existing diffusion model to capture the non-progressive influence propagation in dynamic social networks. We then propose the method, DNIMRL, which employs deep reinforcement learning and dynamic graph embedding to solve the dynamic non-progressive IM problem. In particular, we propose a novel algorithm that effectively leverages graph embedding to capture the temporal changes of dynamic networks and seamlessly integrates with deep reinforcement learning. The experiments, on different types of real-world social network datasets, demonstrate that our method outperforms state-of-the-art baselines.", "sections": [{"title": "1 Introduction", "content": "Influence maximization (IM) is a fundamental concept in the domain of social network analysis and is pivotal in applications ranging from viral marketing to idea propagation within social networks [27]. At its core, IM seeks to identify a small and fixed-size subset of nodes (seed set) within a social network to maximize the spread of influence, e.g. opinions, behaviours or products.\nIn the IM problem, nodes in a social network have two states: active or inactive. Seed nodes are initially activated. Influence spreads through edges, allowing active nodes to activate connected inactive ones. The influence of the seed set is measured by the number of activated nodes or the duration all nodes remain active. For instance, in social media"}, {"title": "2 Related Work", "content": "In this section, we first discuss existing non-progressive diffusion models that are commonly used in IM problems. Then, a general overview of existing solutions to the IM problems is given."}, {"title": "2.1 Non-progressive Diffusion Models", "content": "There are two kinds of non-progressive diffusion models in social IM problems [27]. The most commonly used are Susceptible-Infected-Susceptible (SIS) [20] and its variants which are originally designed to describe the epidemic spread. In SIS models, nodes have two states: susceptible (inactive) or infected (active). Infected nodes infect (activate) neighbourhood susceptible nodes with probability \u00df. Infected nodes recover to susceptible state with probability \u03bc and can be reinfected. Minority studies on non-progressive IM problems also use Voter model [29] and its variants to describe the influence diffusion. These non-progressive models have been extend to dynamic networks [14, 37]. The main change is to add time constraints to the diffusion process.\nThe epidemic model, Susceptible-Infected-Recovered (SIR) [20], is also called non-progressive but refers to the epidemic spread. In SIR, infected nodes will turn into the recovered state and cannot infect other nodes nor be infected again. However, Kempe et al. stated that nodes can switch between active and inactive state multiple times under the non-progressive setting in their study [19] where first proposed the non-progressive IM. Thus, IM problems using SIR models are not considered non-progressive in this paper.\nWe argue that neither model is suitable for describing non-progressive influence diffusion in social networks for the following reasons. The SIS (Susceptible-Infected-Susceptible) model, originally developed to describe the spread of diseases, assumes a fixed recovery probability \u00b5, even when applied to epidemic models on dynamic networks. However, this assumption does not align well with the nature of influence diffusion in social networks. In social networks, disappearance of influence is not a fixed probability; rather, it is often sustained through frequent interactions with advertisers or like-minded individuals [5, 35]. These interactions can reinforce certain beliefs or desires, thereby motivating individuals to continue spreading particular ideas or messages. As a result, the time length that an active node returns to the inactive state should not be a fixed probability. Instead, this should vary depending on the nature and frequency of interactions with other nodes in the network. Similarly, in the Voter model, a node simply mimics the state of its randomly selected neighbours in order to achieve consensus in the network. On the other hand, IM is concerned with the spread of propagation rather than the consistency of opinions."}, {"title": "2.2 Solutions to IM Problems", "content": "To the best of our knowledge, existing research in IM has not yet utilized dynamic non-progressive diffusion models to analyze influence diffusion in social networks, i.e., no existing solution to the dynamic non-progressive IM problem. Thus, we give an overview of solutions to other types of IM problems."}, {"title": "2.2.1 Non-deep learning solutions", "content": "Typical non-deep learning solutions to the IM problems are greedy algorithms that approximate the optimal seed set by iteratively selecting nodes that yield the highest marginal gain in influence spread calculated using Monte Carlo simulation [1, 8, 16, 24]. Some methods [23, 45] solve the IM problem by analysing the node importance based on graph statistics such as degree or PageRank. These methods perform well on small scale social networks. However, since they do not scale to increasingly large social networks, deep learning-based solutions are becoming mainstream."}, {"title": "2.2.2 Deep Learning Solutions", "content": "Graph embedding (GE) is used widely in deep learning based IM methods due to the high dimensionality and sparsity of networks. GE densely represents nodes as points in a low-dimensional vector space and reflect the relationships among nodes [6]. GE approaches can be broadly classified as static or dynamic. Static GE [17, 33] aims to capture the static topological structure and relationships between nodes. In addition to these static features, dynamic GE methods [34, 42] also capture the temporal evolution of the graph.\nThe FastCover algorithm proposed by Ni et al. [32] reduces the IM problem to a budget-constrained d-hop dominating set problem (kdDSP) and design a multi-layer GNN which can capture the diffusion process to solve the problem. Zhang et al. [44] proposed a self-labeling mechanism and designed a GCN with adjustable number of layers for different sizes of networks to balance scalability and performance. Kumar et al. [21] transfers the IM problem to a pseudo-deep learning regression problem. They use a big real-world dataset to train a Graph Neural Network (GNN) based regressor using the influence of node under SIR and IC model as the labels.\nDeep reinforcement learning (DRL) [26] offers a new direction for solving the IM problem. DRL effectively addresses the issue of lacking labeled datasets and excels at solving combinatorial optimization (CO) problems [31], including IM. Li et al. [25] first use RL to solve the static progressive IM problem and proposed a framework named DISCO, that composes GNN and DQN. Based on DISCO, Wang et al. [41] proposed the IMGER. They use graph attention networks for graph embedding. Chen et al. [7] proposed ToupleGDD, which is an end-to-end reinforcement learning framework where model is trained on several small randomly generated graphs which makes it not graph-specific. All these methods are designed for static progressive IM problem.\nSome most recent studies [11, 36] use deep learning to solve the dynamic progressive IM problem. While they describe a dynamic network as a series of network snapshots which results in information loss. We describe dynamic social networks using continuous time 2, allowing a more accurate assessment of node influence. To the best of our knowledge, there are no deep learning solutions developed for non-progressive IM problems."}, {"title": "3 Problem Definition", "content": "We first define the dynamic non-progressive influence maximization problem formally.\nDEFINITION 1 (DYNAMIC NON-PROGRESSIVE IM PROBLEM). Given a dynamic network G and a dynamic non-progressive diffusion model, the goal of dynamic non-progressive influence maximization problem is to find a k-size subset of nodes"}, {"title": "whose influence is maximum, i.e:", "content": "\\max_{S \\subset V} I(S)\nwhere |S| = k, V is the node set of G, and I(S) is the influence of the seed set S determined by the diffusion model.\nAs discussed in Section 2.1, existing diffusion models have limitations in accurately capturing non-progressive influence diffusion in dynamic social networks. To address this, we propose a new dynamic non-progressive diffusion model, termed Social-SIS. This model extends the well-recognized continuous-time SIS model [13] by introducing a key modification: nodes return to the susceptible (inactive) state based on the influence of their neighboring nodes, rather than following a fixed probability.\nDynamic social networks can be represented using both the discrete and continuous dynamic graph model [43]2. Fennell et al. [13] proved that the SIS model defined on dynamic networks described using the continuous dynamic graph model can more accurately model the diffusion process. In this paper, we therefore decide to use the continuous model to capture temporal features of the dynamic social networks, so that the influence of the seed set is evaluated more accurately.\nThe formal definition of Social-SIS is presented below and detailed in Algorithm 1."}, {"title": "Network Definition", "content": "A dynamic network existing from Ts to Te (specific time points) is described using continuous dynamic graph model as G = (V, &(t)), where V is the node set that includes all users appear in this network. The number of nodes is denoted as N = |V|. &(t) is the edge set where each edge is denoted as e(vs, ve, t), where us and ve are the start and end node of edge e respectively. t is the timestamp indicating when the edge is valid."}, {"title": "Influence Diffusion", "content": "All the nodes in the seed set S are always active. For each edge (vs, ve, t) in &(t), us will try to activate ve at time t if and only if us is in the active state at time t. Activation is successful with a probability of \u00b5. We modify the continues-time SIS model by introducing tact. If ve is activated successfully, it will stay in the active state for tact. Notably, ve can be activated even if it is active, and such a successful activation will prolong the time it remains active."}, {"title": "Influence Calculation", "content": "Following [30], we introduce how to calculate the influence of a seed set under the Social-SIS. act_stats is defined to record the active time intervals for each node. Specifically, for node v, act_stats(v) =\n{(act_s, act_e),..., (act_s, act_e)} representing the k time intervals when node v is in active state. A node may already be activated when another node tries to activate it. We deal with this by consecutively adding up the active periods. Hence \\(t_{new\\_act\\_s} = \\max(t, v_{act\\_e})\\) and ends at \\(t_{new\\_act\\_e} = \\min(T_e, t_{new\\_act\\_s} + t_{act})\\). The influence I of a seed set S is measured by the average length of time that all nodes are in the active state. Specifically, it is defined as follows:\n\\[\nI(S) = \\frac{\\sum_{v \\in V} \\sum_{i=0}^{j} v_{act\\_e} - v_{act\\_s}}{N}\n\\]"}, {"title": "4 Methodology", "content": "In this section, we describe how we solve the proposed dynamic non-progressive IM problem using dynamic graph embedding and deep reinforcement learning. The framework of our method, DNIMRL, is shown in Figure 1. The proposed dynamic non-progressive problem is formulated as a Markov Decision Process (MDP) [4] and the Double DQN method [38] is employed to learn the optimal policy."}, {"title": "4.1 Problem Formulation", "content": "The goal of the defined IM problem is to find a fixed-size subset of users whose influence (measured by a dynamic non-progressive diffusion model) is maximum in the dynamic network. We formulate the problem as a finite and discrete Markov Decision Process (MDP) [4]. The key concepts of this MDP are defined as follows.\n\u2022 The state \\(s_t\\) at any time step t represents the original dynamic network and the current seed set \\(S_t\\). The seed set is represented using a one-hot vector. The length of the one-hot vector is equal to the total number of nodes in the graph and each element is a binary indicator signifying whether the corresponding node is in the current seed set."}, {"title": "4.2 Double DQN with Graph Embedding", "content": "We employ the Double DQN [38] to learn and optimize the policy for selecting seed nodes. Double DQN extends the popular Deep Q-Network (DQN) method, which uses a neural network to approximate the Q-value function (i.e. quantifies the expected future rewards for taking a specific action in a given state). Double DQN uses two separate neural networks with same structure: one for selecting actions and another for evaluating those actions. This separation helps mitigate the overestimation of action values often observed in DQN. It enables the model to handle environments with high-dimensional state spaces and leads to more reliable and stable learning outcomes [38]. In this paper, we propose to integrate graph embedding within the learning procedure of Double DQN."}, {"title": "4.2.1 Network Architecture", "content": "We first introduce the neural network we designed to approximate the Q-value function Q(s, a), which estimates the expected return (total cumulative reward) for taking action a in a particular state s. The architecture of the neural network is shown in Figure 2(a). It consists of two primary components: a dynamic graph embedding module and a influence estimator module. The dynamic graph embedding module has two main purposes. The first one is to convert the dynamic network into low-dimensional dense embedding vectors. Second, the embedding vectors are used to capture both the temporal dynamics and structure of the network, serving as the foundational input for the subsequent value calculation process. These embedding vectors are then fed into the influence estimator module designed to approximate the Q-value."}, {"title": "Dynamic Graph Embedding Module", "content": "This module is designed based on the TGNs proposed by Rossi et al. [34], which is a state of the art dynamic graph embedding method. In the original design, TGNs focus on generating embedding vectors for each node at individual time steps. In contrast, we place greater emphasis on capturing the evolving history of each node. This perspective is more pertinent to the problem of IM, as it allows for a more comprehensive understanding of how nodes influence each other over time. Therefore, we modify the original framework of the TGNs to enhance the dynamic graph embedding module (Figure 2(b)). Here, we describe our-proposed modifications. For a detailed description of the graph embedding module, please refer to Appendix B.\nThis module consists of two key components: memory and embedding. The memory component is designed to memorize long-term dependencies for each node in the graph. It consists of |V| vectors, each representing a node, which are initialized as zero vectors. The memory vector of each node is updated upon the generation of an edge related"}, {"title": "Influence Estimator Module", "content": "From the dynamic graph embedding module, we have generated node embeddings that can represent the dynamics of node interconnections within the dynamic network. In the following, we describe how the Q-value function Q(s, a) is approximated based on node embeddings (shown in Figure 2(c)).\nNode embeddings, produced by the dynamic graph embedding module, first undergo two linear transformations to yield two matrices \\(M_1\\) and \\(M_2\\). To perform the linear transformation, node embeddings are processed through two different MLPs. Specifically, for node embedding z: \\(M_1 = \\text{MLP}_1(z)\\) and \\(M_2 = \\text{MLP}_2(z)\\). These two linear transformations are designed to extract and enhance the representation of features pertinent to the influence maximization problem from varied perspectives, thereby providing a richer informational foundation for Q-value computation.\n\\(M_1\\) and \\(M_2\\) then undergo a dot product operation to produce a new matrix \\(M_3\\). This operation measures the similarity between node representations. The higher similarity between a node pair indicates that their behavior is more similar in the network, and therefore they are more likely to influence each other. So, \\(M_3\\) can be used to represent the potential influence of each node on others. To normalize the potential influence of all other nodes on a single node into values between 0 and 1, the sigmoid function (\\(\\sigma\\)) is applied to each row of \\(M_3\\), i.e. \\(M_3 = \\sigma(M_1 \\cdot M_2)\\). Following this, the state s and action a are represented as a one-hot vector (denoted as \\(S_A\\)), where nodes already in the seed set and the node selected by the action a are set to 1, with the rest are set to 0. A dot product operation between \\(M_3\\) and \\(S_A\\) yields a new Matrix M = \\(M_3 \\cdot S_A^T\\). By doing so, each element in M represents the quantified contribution of each node towards influence maximization under the given action and state. Therefore, summing all elements in M provides the Q-value for the action under the specified state, i.e. Q(s, a) = \\(\\sum_{i} M_i\\). In summary, the Q-value Q(s, a) is approximated using the node embeddings z as follows:\n\\[\nQ(s, a) = \\sum_{i}^M M_i\n\\]\n\\[\nM = \\sigma(\\text{MLP}_1(z) \\cdot \\text{MLP}_2(z)^T) \\cdot S_A^T\n\\]"}, {"title": "4.2.2 Training Strategy", "content": "In the Double DQN, two neural networks both using the architecture detailed in Section 4.2.1, the Q-network and the target network, are employed. The Q-network Q(s, a|0), where @ is the parameter of the Q-network, directly interacts with the environment, making real-time decisions based on the state inputs. The target network Q(s, all) provides a stable benchmark for evaluating the decisions made by the Q-network. The target network is updated less frequently by copying the Q-network's parameter after a fixed number of episodes. The loss function is designed to measure the discrepancy between the predicted Q-values by the Q-network and the more stable Q-value targets provided by the target network. The exact equation for the loss function is:\n\\[\nL(\\theta) = (r + \\gamma Q(s_{t+1}, \\arg \\max_{a'} Q(s_{t+1}, a'|\\theta)|\\theta) - Q(s_t, a_t|\\theta))^2\n\\]\nwhere r is the reward received for taking action a in state s, s' is the subsequent state after taking action a. y is the discount factor, which balances the importance of immediate versus future rewards.\nTwo techniques are used in training. First, replay buffer is used to store experience data during interactions between the agent and the environment, allowing these data to be used during training to improve learning efficiency. Second, e-greedy is used to balance exploration and exploitation by allowing random action selection under a small probability. The specific training strategy and the detailed algorithm are presented in Appendix C."}, {"title": "5 Experiments", "content": "All experiments are conducted on a machine with an Intel Xeon Platinum 8360Y (2.4 GHz, 18 cores), 128 GiB DDR4 RAM, and a NVIDIA A100 (40 GiB HBM2 memory), running Linux release 8.6."}, {"title": "5.1 Experiment Setup", "content": "Three real-world datasets are used. 1) Bitcoinalpha [22]. Bitcoinalpha is a who-trusts-whom network of people who trade using Bitcoin on a platform called Bitcoin Alpha. In the dataset each node represents a Bitcoin user and each edge represents a credit rating record from the rater to the ratee. 2) Bitcoinotc [22]. This dataset is similar to Bitcoinalpha, but collected from another platform named Bitcoin OTC. 3) Facebook [40]. This dataset is collected from New Orleans regional network in Facebook. In the dataset, each node represents a user and each edge represents a friendship between users.\nBitcoinalpha, and Bitcoinotc are from SNAP3 without modification. Facebook is from NR\u2074 and loops (edges that start and end at the same node) are removed. The statistical information of the three used datasets is shown in Table 1.\nThe three datasets were carefully selected so that they increase in size and have different densities (edge to node ratio |8|/|V|) to measure the applicability of the method on different types of datasets."}, {"title": "5.1.2 Baselines", "content": "To the best of our knowledge, currently there are no existing baseline methods tailored for dynamic non-progressive IM problems. Thus, for a fair comparison, we have opted to include five methods designed for addressing other types of IM problems. The five different methods cover all types of IM problems (see Table 2 for an overview of their key features). Among them, CELF [24] and INDDSN [1] are the two best-known greedy algorithms designed for static and dynamic IM problems, respectively. These two greedy algorithms are chosen to show the accuracy of our method. ToupleGDD [7] is one of the state-of-the-art (SOTA) methods designed for the static progressive problem, which also employs RL. KTIM [45] is the only SOTA method for the dynamic progressive IM problem which also defines dynamic networks using the continuous model. For non-progressive methods, we chose the SOTA method TSGC [10] from limited number of methods.\nTo make these static methods applicable for the experiments, we transform the dynamic networks into static networks by removing the timestamps of edges and suppressing duplicate edges. The progressive methods can be applied directly (on dynamic networks), as they use the topology of the network only."}, {"title": "5.1.3 Hyperparameter Setting", "content": "For the diffusion model, the activation success probability u is set to 0.5 (i.e. the probability of each activation to be random) as the four datasets we used are anonymized, which provides no information about the users. As there is randomness in the calculation of the seed set influence, the calculation of the influence will be repeated 2,000 times and take the average for a fair comparison. The hyperparameters for baselines are fine-tuned on each dataset according to original papers.\nFor our method, we first give the parameters related to dynamic graph embedding module. The dimension of both node memory and embedding is 64. The number of graph attention layers is 1 and the number of heads used in each attention layer is 2. The batch size for updating memory is 200. These hyperparameters follow those provided by TGNs [34], except for the dimension of node memory and embedding, which are fine-tuned by ourselves. For the RL part, we fine-tuned the following hyperparameters: the batch size of transactions for updating Q-network is 16, the Q-network is updated every 1 episode. The target network is updated every 20 episodes. The value of e for epsilon-greedy starts at 1 and decays to 0.2, decreasing by 0.98 every episode. The y in loss function is set to 0.95, following the standard practice. The learning rate, which is shared by the two modules, is optimized to 0.001."}, {"title": "5.2 Quantitative & Scalability Analysis", "content": "In order to compare the performance of the proposed method with baselines, the influence of the seed set selected by these methods is evaluated. The influence of a seed set is calculated using Equation 1. The higher the influence of the selected seed set, the higher the performance of the method. As the activation interval tact is an important parameter in our diffusion model, we also consider multiple settings for each dataset. For all datasets, tact is set to 1 month, 3"}, {"title": "5.3 Ablation Study", "content": "In our Double DQN method, we performed two linear transformations to the node embedding to extract and enhance the representation of influence maximization related features, i.e. the generation of M\u2081 and M2 in Figure 2(c). To show the importance of this step, given that it requires considerable time, we compare the performance of the method using the proposed network architecture with that using the reduced network architecture. In the reduced network architecture, the two linear transformations are removed and instead using the embeddings to calculate M3 directly, i.e. \\(M_3 = \\sigma(z \\cdot z^T)\\).\nAs the results from Table 4 demonstrate, the influence of the seed sets selected using the reduced network has a clear reduction compared to the seed sets selected using the proposed network. However, the increase of the running time"}, {"title": "5.4 Qualitative Analysis", "content": "We further illustrate the validity of the DNIMRL method. In Bitcoinalpha, we identify and compare the seed sets chosen by the DNIMRL and CELF. The seed set size is fixed at 10. In Figure 4(a)&(b), we visualise the number of active nodes in different time windows.\nFirst, it can be seen that the number of nodes activated by CELF seeds is significantly lower than the number of nodes activated by DNIMRL seeds in the time windows shown in the orange box. This also contributed to the slightly lower performance of CELF compared to DNIMRL. We analyse the causes as follows.\nHere are the seed sets chosen by the two algorithms respectively. DNIMRL: {3149, 892, 2388, 14, 10, 1536, 246, 18, 20, 1423}, CELF: {7, 0, 27, 6, 2, 17, 40, 64, 9, 12}. The smaller the node's index, the earlier an edge is connected to it in the network. It can be seen that the greedy algorithm tends to select nodes that appear early because such nodes have better connectivity with other nodes. However, the nodes that these seeds can connect to may have a high degree of overlap and may only have a short-term influence when nodes can return to an inactive state. In contrast, we found an interesting seed node selected by DNIMRL, No. 3149, which appears quite late in the dynamic network. We further analyse all the nodes activated by No. 3149 and visualize the number of these activated nodes over time in Figure 4 (c). It can be seen that although the active time of this seed node is limited, its influence period falls within the orange box of 4(a)&(b). Therefore, we believe that selecting this node as seed contributed significantly to DNIMRL's excellent performance during this period. We also calculated the centrality of seed No. 3149, which is lower than other seed nodes. Still, it cooperates with other seed nodes and enhances the overall influence by activating many nodes in this time period. This demonstrates that deep RL algorithms can fully explore the action space and trade-offs between different actions to select the best seed set, which leads to the superiority of our method.\nIn Figure 4(a)&(b), it can also be seen that the number of active nodes spiked in earlier time windows but then dropped sharply. It indicates information diffusion is very active in the early stage of this dynamic network, and its influence is shrinking afterwards. On the other hand, since the number of active nodes does not drop over time in the progressive setting, such a dynamic phenomenon is not able to be captured. This example further illustrates the importance of studying the dynamic non-progressive setting for IM."}, {"title": "5.5 Validation of Social-SIS", "content": "As discussed in Section 2.1, to align with the IM properties of dynamic social networks, the proposed Social-SIS model aims to make the duration for which an active node remains active dependent on its interactions with other nodes, rather than relying solely on a fixed probability. To experimentally demonstrate this, in Table 5, we calculate and present the percentage of successfully activated nodes that were already active. The results show that at least 50% of successful activations are performed on already activated nodes. This implies that a significant number of activated nodes are influenced by interactions with active neighboring nodes. It shows Social-SIS's ability to capture fluent interactions among nodes."}, {"title": "6 Conclusion", "content": "In this paper, we investigate the dynamic non-progressive IM problem. We model the problem as a Markov Decision Process and employ a deep reinforcement learning algorithm to find the optimal seed set. In particular, we propose a novel node influence estimation module to validate the temporal influence of nodes based on dynamic graph embeddings. We also extend a dynamic non-progressive epidemic model to capture the influence diffusion in dynamic social networks. The experiments show that our proposed method works well on different types of social networks and has the ability to scale to large dynamic networks. It would also be interesting to find new training strategies to obtain a model that can be applied to multiple social networks."}, {"title": "A Discrete and continuous dynamic graph models", "content": "In this section, we illustrate the discrete and continuous dynamic graph models used to describe dynamic networks which are defined based on the models [43] commonly used in the AI avenue. Graph is a data structure composed of nodes (vertices) and edges (links) used to model entities and their relationships. Dynamic graph is a special type of graph whose structure (nodes or edges) changes over time.\nIn the continuous dynamic graph model, a dynamic network is modeled as a sequence of interactions (edges) between nodes. These interactions and specific occurrence time (usually in second) are recorded. Specifically, we define G = (V, &(t)), where G is a dynamic graph representing a dynamic network exists from time Ts to time Te (both Ts and Te are specific time points). V is the node set that includes all users that have appeared in this network. The number of nodes is denoted as N = |V|. 8(t) is the edge set where each edge is denoted as e(vs, ve, t), where us and we are the start and end node of edge e respectively. t is the timestamp indicating when the edge is valid.\nIn the discrete dynamic model model, the existence time (from time Ts to time Te) of a dynamic network is divided into n intervals (length usually a day or a week) {TI\u2081, ..., TIk, ..., TIn}, where TIk starts form (k \u2212 1) * \\(\\frac{T_e-T_s}{n}\\) and ends at k * \\(\\frac{T_e-T_s}{n}\\). All the users and interactions appeared in one interval TIk form a static graph Gk (snapshot of the dynamic network). That is, the dynamic network is represented by a sequence of static graphs G = {G1, ..., Gk, ..., Gn}.\nCompared with the discrete model, the continuous model provides a higher time resolution. This detailed time recording allows the model to capture more accurately the dynamic changes of the network over short periods of time."}, {"title": "B Dynamic Graph Embedding Module", "content": "This module is designed based on the TGNs proposed by Rossi et al. [34], which is a state of the art dynamic graph embedding method. In the original design, TGNs focus on generating representations for each node at individual time steps. In contrast, we place a greater emphasis on capturing the evolving history of each node. This perspective is more pertinent to the problem of IM, as it allows for a more comprehensive understanding of how nodes influence each other over time. Therefore, we extend the original framework of the TGNs to enhance its ability to form the dynamic graph embedding module (Figure 2(b)) which consists of two key components: memory and embedding.\nThe memory component is designed to memorize long term dependencies for each node in the graph. It consists of |V| vectors for each node, which are initialized as zero vectors. The memory vector of each node is updated upon the generation of an edge related to that node. The set of edges is divided into n batches, with each batch independently updating the memory. For a batch, the specific update process is as follows:"}, {"title": "Message Function msg", "content": "For each edge involving node i, a message is computed to update its memory. For an edge (us, ve, t), two messages will be generated: \\(m_{u_s}(t) = s_{u_s}(t^-)||s_{v_e}(t^-)||\\Delta t\\) and \\(m_{v_e}(t) = s_{v_e}(t^-)||s_{u_s}(t^-)||\\Delta t\\), where \\(m_{u_s}(t)\\) is the message generated for node us at time t, \\(s_{u_s}(t^-)\\) is the latest memory of node us before time t, \\(\\Delta t = t - t^-\\), and || represents the concatenation operation between two vectors."}, {"title": "Message Aggregator agg", "content": "Due to the large number of edges, there may be multiple messages about node v within a batch. For efficiency reasons, multiple messages for a node in a batch [\\(m_v(t_1), m_v(t_2), ..., m_v(t_b)\\)] (t1 t2 < ... < t\u2081) are aggregated by remaining most recent messages, i.e. \\(m_v(t_b)\\)."}, {"title": "Memory Updater mem", "content": "The memory vector of a node is updated upon each message involving the node itself using GRU [9]. Specifically, \\(s_v(t) = GRU(m_v(t), s_v(t^-))\\)."}, {"title": "h(0) = si", "content": "h(1) = MLP(1)(h(1-1) ||h1), $(1)),"}, {"title": "\u0125l) = MultiHeadAttentionl (q, Kl), Vl)),", "content": "ql) = hl\u22121)||$(),"}, {"title": "Kl) = Vl) = Cl,", "content": "Cl) = [hl\u22121)||(t\u2081), ..., hl-1)||(tn)]."}, {"title": "C Training Strategy", "content": "In following, we illustrate our training strategy based on Double DQN (cf. Algorithm 2). We make use of the two technologies:"}, {"title": "Replay buffer", "content": "collects and stores the agent's experiences at each time step, defined by tuples of states, actions, rewards, and subsequent states, i.e. (s, at, rt, St+1). Training with mini-batches of experiences randomly sampled from this buffer helps to decorrelate the experiences and mitigate the risks of overfitting and instability in learning."}, {"title": "Epsilon greedy", "content": "manages the trade-off between exploration of the environment and exploitation"}]}