{"title": "FunctionChat-Bench: Comprehensive Evaluation of Language Models' Generative Capabilities in Korean Tool-use Dialogs", "authors": ["Shinbok Lee", "Gaeun Seo", "Daniel Lee", "Byeongil Ko", "Sunghee Jung", "Myeongcheol Shin"], "abstract": "This study investigates language models' generative capabilities in tool-use dialogs. We categorize the models' outputs in tool-use dialogs into four distinct types: Tool Call, Answer Completion, Slot Question, and Relevance Detection, which serve as aspects for evaluation. We introduce FunctionChat-Bench, comprising 700 evaluation items and automated assessment programs. Using this benchmark, we evaluate several language models that support function calling. Our findings indicate that while language models may exhibit high accuracy in single-turn Tool Call scenarios, this does not necessarily translate to superior generative performance in multi-turn environments. We argue that the capabilities required for function calling extend beyond generating tool call messages; they must also effectively generate conversational messages that engage the user.", "sections": [{"title": "Introduction", "content": "Function calling, a way to connect language models with external tools, is a significant advancement that enhances the utility of AI systems. Language models that support function calling have been fine-tuned to generate a JSON object that adheres to the function specification when a function needs to be called. Given this key feature, the JSON objects generated by the language models receive significant attention in related research and evaluation.However, In tool-use dialogs, the capabilities required of a language model are not limited to generating call messages for which the tool is the recipient; they must also encompass the generation of conversational messages for which the user is the recipient. This realization has led to the creation of a new dataset designed to better simulate real-world scenarios where both interactions between AI and tools, as well as between users and AI, occur.In this paper, we introduce FunctionChat-Bench, which aims to provide a more comprehensive evaluation of language models' generative capabilities to handle diverse types of inputs, including both single-turn utterances and conversation history of multi-turn dialogs. We also describe experiments conducted using this benchmark and analyze these results."}, {"title": "Generative Capabilities Associated with Language Models' Tool Usage", "content": ""}, {"title": "Definitions of Output Types", "content": "The outputs generated by language models in tool-use dialogs can be broadly categorized into two types: those that directly communicate with the user and those that interact with tools. The former is represented on the left side of the figure 1 and is referred to in this paper as a conversational type output. The latter is shown on the right side of the figure 1 and is referred to in this paper as a tool call type output. The conversational type output can be further divided into three subcategories. Thus, we can categorize them into four types:\n\u2022 Tool Call. The output containing a tool call object that has the function name and its arguments.\n\u2022 Answer Completion. The output that delivers the result from a specific function to the user.\n\u2022 Slot Question. A question or a request about missing information, which is essential for tool call.\n\u2022 Relevance Detection. The output corresponding to a response to either a user's general chat unrelated to tool usage, or a request outside of the accessible functions."}, {"title": "Related Works", "content": "Recently, there has been a significant increase in research focusing on the tool utilization ability within language models. The development of new benchmarks for tool usage further underscores the growing interest in evaluating the proficiency of these models in handling tools.APIBench (Patil et al., 2023), GPT4Tools (Yang et al., 2023), RestGPT (Song et al., 2023), Tool-Bench (Qin et al., 2023), among others, concentrate on evaluation data and systematic assessment methods for measuring the tool usage capabilities of language models.It is encouraging that API-bank (Zheng et al., 2023) and ToolTalk (Farn and Shin, 2023) have considered over multiple user utterances as input for their evaluation data. MetaTool (Huang et al., 2024) is distinctive in that it focuses more on evaluating whether language models have tool usage awareness rather than generating arguments. Similarly, BFCL (Yan et al., 2024) is characterized by including function relevance detection among its research interests, which aims to determine how the model will react when the provided function is not suitable to answer the user's question.The most critically evaluated aspect in recent benchmarks related to tool usage, including function calling, is the generation of the tool call type output, which is considered a core capability. However, the tool call type output is not a type that directly interacts with the user. The conversational type output, which communicates with the user in natural language, is a factor that significantly affects perceived performance. This type, which has been either excluded or only partially addressed so far, also needs to be included in the evaluation scope."}, {"title": "Dataset Design", "content": "We developed a novel benchmark, FunctionChat-Bench, that not only evaluates the tool call type output but also comprehensively assesses the conversational type output. It contains two subsets of the evaluation dataset: a single call dataset and a dialog dataset."}, {"title": "Single Call Dataset", "content": "Evaluation items in the single call dataset are defined by the following conditions:\n1.  The user's single turn utterance must contain all of the necessary information for function invocation, leading directly to a tool call.\n2.  A suitable function for carrying out the user's request must be provided in the available tool list.\nUnder these conditions where a tool call object must be generated, language models are evaluated whether they select the suitable function and correctly extract information to appropriately generate arguments.It was considered that the success of the model's output generation could be influenced by the length of the available tool list or the similarity between the functions provided therein. Therefore, the tool list was constructed in three types based on the number of functions provided: 1, 4, and 8. Among these, for cases where 4 or 8 functions were provided, the lists were further differentiated by randomly selecting the functions or by selecting functions that were similar in terms of domains or operations."}, {"title": "Dialog Dataset", "content": "Unlike the environment addressed in the single call dataset, language models cannot continuously generate only tool call type outputs when interacting with a user in real chat. After a function invocation, the results must be conveyed to the user, and if the user has not provided all the essential information for the function call, the model must request the missing information. Furthermore, the user may introduce topics unrelated to the function calling feature during the conversation, and the necessary external tools to process the user's request may not be provided in the available function list.The dialog dataset was designed to evaluate whether language models can appropriately handle a diverse range of input scenarios. It broadens the scope of evaluation to encompass not only tool call type outputs but also conversational outputs directed towards the user. This is because, even if the tool call messages are accurately conveyed, users directly experience the model's performance during interactions.The dialog dataset consists of 45 Korean dialogs, each containing between 3 to 8 turns to be generated by the model, with a median of 4 and an average of 4.44 turns. In total, it includes 200 turns of model generation, each serving as an evaluative item. Each turn is annotated as one of the four defined output types: Tool Call, Answer Completion, Slot Question, and Relevance Detection. All dialogs contain 70 Tool Calls, with each dialog containing at least one function calling."}, {"title": "System Prompt", "content": "For details on the system prompt applied to our evaluation data, see Appendix A."}, {"title": "Evaluation Methods", "content": "When evaluating language models, there is a general consensus that having a large number of evaluation items is beneficial, regardless of the specific domain under evaluation. This is because a substantial quantity of evaluation items helps ensure that the assessment is not biased and remains reliable. Simultaneously, a large number of evaluation items naturally leads to the need for automating the evaluation process and quantifying the results.In the evaluation of language models' function calling capabilities, several methods have been introduced for automatically assessing a substantial volume of items and quantifying the results (Farn and Shin, 2023; Yan et al., 2024). These methods primarily measure the accuracy of function selection and argument extraction from the Tool Call type output generated by the model. An exact match approach is commonly employed for this purpose, and cosine similarity is also used as a supplementary metric.However, when constructing evaluation datasets composed of functions from various domains that include diverse parameters, and when assessing capabilities in languages other than English, as well as not merely responding to single-turn queries but also broadly evaluating generative abilities within multi-turn dialogs that involve context and history, relying solely on exact match or cosine similarity as evaluation methods might be perceived to be insufficient.First, the ground truth that serves as a match target is not an absolute and unique answer. Parameters such as numbers, integers, or booleans have permissible arguments that do not deviate significantly from the ground truth. However, the spectrum for string type arguments is broad, ranging from elements that must match the user-provided information exactly, like telephone numbers and email addresses, to elements like note titles or message content and search queries, where variations in expression are allowed. Secondly, these methods do not account for semantic equivalence across different languages. Thirdly, it is challenging to adequately assess whether the conversational messages delivered by the language model to the user are appropriately generated.To address these limitations, we chose a method using a powerful large language model (LLM) as a judge (Zhou et al., 2023; Zheng et al., 2023). Our expectation for the LLM judge is to determine whether each turn generated by language models is a successful output or a failure. In Section 2.1, we defined the four types of outputs generated by language models. Based on these definitions, we evaluate their generative capabilities. We established the criteria that precisely match the four types of outputs. We provided well-refined evaluation metrics and expected responses, known as ground truth, to assist the LLM judge in determining the pass or fail status of submissions."}, {"title": "Experiments", "content": "With FunctionChat-Bench, a dataset specifically designed to comprehensively evaluate the generative capabilities in Korean Dialogs, we assessed eight language models that support function calling functionality. Initially, we collected outputs from each model for 500 evaluation items in the single call dataset and 200 evaluation items in the dialog dataset. Subsequently, the model-generated outputs were inserted into evaluation prompts along with specific criteria and reviewed by an LLM judge. The judge used was the gpt-4-0125-preview model. After completing its assessment, the LLM judge determined the pass or fail status for each item, and finally, the overall pass scores or rates were calculated for each model. Additionally, to facilitate an easy review of the evaluation items, model outputs, and the LLM judge's reasoning and final decisions, a separate report file in TSV format was generated. This entire process was fully automated through the evaluation system we implemented.Even when using the most robust models as judges, LLMs are not perfectly reliable at reasoning for passing judgment. Therefore, after the automatic evaluation, the LLM judge's reasoning and decisions were qualitatively reviewed by a human judge. During this process, incorrect judgments that did not meet the evaluation criteria and principle were identified. These judgment errors were ultimately adjusted (Appendix C). The main results of our experiments are presented in Tables 3 and 4."}, {"title": "Analysis", "content": ""}, {"title": "Statistics", "content": "The FunctionChat-Singlecall exclusively evaluates the tool call type output. It allows for the assessment of performance in generating accurate tool call object, driven by appropriate function selection and proper argument generation. When designing this dataset, the initial hypotheses were as follows: First, the accuracy in generating a tool call object was expected to decrease as the number of candidates in the available tool list increased. Second, Whenever more than two functions were available in the tool list, the higher the similarity in domain or operations between these functions, the lower the accuracy would be. After experimenting with various models, it was found that these hypotheses were partially correct and partially incorrect.Regarding the first hypothesis, the trend of decreasing the accuracy as the number of candidates increases was not distinctly evident within the range of 1 to 8 candidates. Although it was expected that the exact type, where only one target function is presented in the tool list, would be easier, it could not be generalized. Particularly, Gemini demonstrated higher accuracy as the number of provided function candidates increased from 1, 4, to 8. In relation to the second hypothesis, the similarity of available functions showed a fair correlation with the accuracy. The close group generally scored lower than the random group. Especially, gpt-4-turbo exhibited a significant difference of 10 points between the scores in the 8.random type and the 8.close type (Table 3).The FunctionChat-Dialog provides a broader and more diverse range of inputs for evaluation compared to the FunctionChat-Singlecall. In addition to inputs that ask for the generation of tool call type outputs, it also includes inputs that necessitate the generation of conversational type outputs such as slot filling questions and tool call rejections based on relevance detection. Furthermore, while the inputs in the FunctionChat-Singlecall consist solely of a user's single-turn utterance, the inputs in the FunctionChat-Dialog are of a multi-turn discourse format. This includes messages exchanged between the user and AI, as well as messages between the AI and tools.When designing this dataset, there was a hypothesis that while single-turn environments might show high accuracy in Tool Call, this does not necessarily imply superior overall generative capabilities in multi-turn environments. Comparisons between Table 3 and 4 demonstrate that this hypothesis holds true. Notably, Solar achieved a high score of 83.6 in evaluations using the single call dataset, but it exhibited significantly lower performance in the areas of Slot Question and Relevance Detection"}, {"title": "Error Types", "content": "Through our experiments, we obtained a substantial volume of output data from various models. This data is also valuable as it provides significant insights into the generative capabilities of language models in tool-use dialogs. We conducted a detailed review of this data to analyze the types of errors exhibited by the language models, and we present these findings along with examples of actual outputs.\n\u2022 Errors in Tool Call.\nErrors related to not creating a tool call object include the following: Language models incorrectly indicate that a tool call will be made, but it is not actually called. A function is not called despite being available, and the model falsely claims it is unsupported. Even when all necessary information is provided by the user, the model redundantly asks for it again (e.g., \"You want to know Junhyuk's birthday! What is Junhyuk's name?\"). Additionally, cases where optional or undefined parameters were requested, and incomplete objects that included only the function name without the argument fields, are also considered errors.Regarding errors in function selection, these include generating a function name not listed in the available function pool or selecting an incorrect function from the pool. Due to the similarity in domain or action, a similar function is chosen (e.g., choosing add_contact instead of update_contact), or a function is selected based on the similarity in argument terminology (e.g., due to the movie title \"Gift in Cell No.7\", the incorrect function gift_search_product is chosen instead of the correct get_movie_details). On the other hand, there is a tendency among GPT models to solve tasks in a familiar manner without primarily referencing the received function specifications (e.g., despite the function specification allowing send_message to be called with only a name, the model attempts to search for a contact using search_contact).The errors associated with extracting information for generating arguments also varied. A well-known error type is fabricating arguments by inventing information that was not mentioned. Sometimes, only part of an argument is invented (e.g., the user provides only month and day, but an arbitrary year is invented to generate a fitted format argument). Conversely, there are errors where clearly stated information is omitted. Functionary often extracts only the first part of a phrase needed to construct a argument (e.g., extracting only \"in Cell No.7\" from the movie title \"Gift in Cell No.7\"). This leads to a distortion of meaning, resulting in a failure.Additionally, there are errors in not following the specified format. Parameters designated as integers are generated in numbers (\"year\": 2012.0 or \"num_people\": 3.0), integers or numbers are generated as strings, and strangely, positive numbers are converted to negatives.Errors such as converting \"\uc0bc\ub9cc\uce60\ucc9c (meaning 37000)\" as 30700 or \"\uc2ed\uc624\ud504\ub85c (meaning 15%)\" as 10% appear to stem from a lack of understanding Korean. Other errors also seem related to the handling of Korean tokens, such as adding a number of spaces or newline characters, or removing all spaces entirely when extracting relatively long strings.\n\u2022 Errors in Answer Completion.\nIn this part, the language model's task is to convey the result of a function call to the user. However, there have been instances where the model does not fulfill this role, instead producing messages that are completely unrelated to the context of the input or mirroring parts of the conversation history. This tendency was observed in the Solar model, which seems to lack the capability for Answer Completion (See Example #1 in Figure 3).Meanwhile, one reason for integrating external tools with a language model is to overcome the limitations of pre-training and to utilize up-to-date and factual information. However, there have been cases where the model alters the results provided by function calls. This tendency was particularly evident in the Gemini model. Instead of conveying the results of function calls to the user, it generates arbitrary answers based on its own old knowledge, thereby delivering false information (See in Example #2 in Figure 3). This is considered a critical penalty.\n\u2022 Errors in Slot Question.\nThe most prominent error type in this part is clear: language models hallucinate arbitrary values to fill required parameters and generate function call messages when the user has not provided the necessary information (See Example #1 in Figure 4). Conversely, there are errors where the model redundantly requests information already provided by the user, and cases where it falsely generates a completion message even though the tool call has not yet occurred (See Examples #2 and #3 in Figure 4). Additionally, there were instances where the language model completely malfunctioned, producing outputs with strange content or formats.\n\u2022 Errors in Relevance Detection.\nIn this part, language models fail to properly detect inputs that are unrelated to tool-use, and instead, it excessively associates all outputs with tool usage. This error is observed as a pattern attempting to link to one of the available functions (e.g., to the input \"The weather is nice!\" it outputs \"Which area's weather would you like to know about?\"), or actually calling it (e.g., to the input \"I'm so busy these days.\" it calls the add_task function with hallucinated arguments).On the other hand, language models also fail to correctly detect the relevance of some inputs to the available tools, even if they are related to tool-use. This manifests as pretending to have features that are not available or fabricating functions that are not provided. For example, in response to the input \"Can you order some pizza for me?\", it might reply with \"Sure! What kind of pizza would you like to order?\" or fabricate and call a function named order_pizza with parameters for size and toppings. It can also manifest as calling an incorrect function that is available, instead of indicating that the proper function to address the user's request does not exist."}, {"title": "Conclusion", "content": "This paper presents FunctionChat-Bench, the novel benchmark for language model's generative capabilities associated with function calling. It contains two subsets of an evaluation dataset (Singlecall and Dialog), and an automated evaluation program.Our dataset uniquely targets not only the tool call objects generated by the language model but also the conversational messages that interact with users. Additionally, we have devised an evaluation methodology. It employs an advanced LLM as a judge to determine the pass or fail status of submitted outputs, utilizing refined evaluation rubrics. The composition and design of our dataset, along with our evaluation methodology, will provide valuable insights. These will focus on what and how to evaluate in order to measure or enhance the function calling capabilities of language models.With FunctionChat-Bench, we evaluated eight language models that support function calling. Through our experiments, we observed that the strengths and weaknesses of language models can vary significantly depending on several factors, such as the number of functions provided, the similarity between the provided functions, whether the input is single-turn or multi-turn, and the type of output required. Researchers can also utilize our publicly available evaluation dataset and program to comprehensively assess the function calling capabilities of API-accessible language models or their own implementations. Additionally, we conducted a detailed analysis of the outputs generated by the models used in our experiments, providing a rich description of their error types along with actual examples. We believe that this will inspire improvements in the technology for integrating tools with language models by providing valuable information on areas that need enhancement."}, {"title": "Limitations", "content": "Our research attempted a comprehensive evaluation of language models' function calling capabilities; however, the evaluation dataset we designed is in some respects narrow and relatively easy. Language models that support function calling are capable of parallel function calling, allowing them to call multiple functions in one turn. However, our dataset does not include scenarios involving multiple function calls; it only addresses single function calls. Additionally, it lacks complex scenarios that require planning the sequence in which multiple functions should be called. Overall, while the study delves into the basic aspects of the function calling feature, it is not suitable for evaluating advanced capabilities that utilize this feature."}, {"title": "System Prompt", "content": "Our dataset includes user inputs and function or parameter descriptions in the function specification, all written in Korean. This indirectly assesses the understanding and generation capabilities for Korean, a relatively low-resource language. Commonly, language models tend to convert information extracted from Korean inputs into English vocabulary when generating arguments in the tool call object. This tendency can sometimes affect the successful execution of tasks requested by the user (consider cases where search terms are translated into English for querying a Korean database, or message contents are changed to English). Therefore, we have attempted to more customarily control the operation of the language model by specifying in the system prompt that outputs are encouraged to be generated in Korean, along with several other detailed aspects.The system prompt used in FunctionChat is as follows. Both the original Korean version and the English translation version are presented."}, {"title": "Rubrics for LLM evaluation", "content": "The common elements of evaluation prompt are as follows:\nYou are evaluating a response submitted for a specific function call task against a set of standards. Below is the data:\n[BEGIN DATA]\n* * *\n[Available Functions]\n{tools}\n[Query]:\n{query}\n[Ground Truth]:\n{ground_truth}\n[Submission]:\n{response}\n* * *\n[Criterion]:\n* * *\n[END DATA]\nDoes the submission meet the criteria? Begin by explaining your reasoning step by step in Korean, without immediately revealing the outcome. Subsequently, on a separate line, clearly indicate whether it is a \"pass\" or \"fail\". For clarity, repeat your final decision once more (without quotes or punctuation, literally).\nThese format refers to Zhou et al. (2023). The placeholders {tools}, {query} and so on will be replaced by specific details from the actual case being evaluated. As in the data from Zhou et al. (2023), The \"Query\" and \"Submission,\" which correspond to the input and output of the model being evaluated, are included in the prompt. Specifically, since the FunctionChat evaluation system targets function calling models, the \"Available Functions\" section has been added. Additionally, to enhance the alignment between the llm judge and the human who designed the evaluation, \"Ground Truth\" has been incorporated. The prompt for evaluating Tool Call type outputs additionally includes \"Acceptable Arguments,\" which serve as supplementary indicators for \"Ground Truth.\"The \"Criterion\" has been established in four different categories according to our definition of the output types for the fc model. While presented a 6-scale Likert score, we have provided criteria for a pass or fail status. Subsequently, the four criteria applied to each type of output will be detailed.\n\u2022 Tool Call\n[Criterion]: Accuracy in selecting the proper function, and generating the function name and argument values\nDetermine if the [Submission] is a \"pass\" or \"fail\". You are given a [Ground Truth] for each [Query], so you can refer to this for evaluating the response.\n\"pass\"\n- Selected the appropriate function and accurately named it.\n- All keys in arguments match those presented in [Ground Truth].\n- Each argument value matches the type specified in [Available Functions].\n- Each value in arguments was created appropriately, as presented in [Ground Truth]. For string types, a [Submission] passes if its argument matches or refers to the same content or subject as [Ground Truth] or [Aacceptable arguments], even without an exact text match.\n- If 'Only ground truth is allowed.' appears under [Aacceptable arguments], it means that only when the argument value exactly matches [Ground Truth] will it be considered a pass.\n\"fail\"\n- Selection error: Did not select a function or selected a different function than the one presented in [Ground Truth]. -\nFunction name error: Failed to accurately create the function name as presented in [Ground Truth] (different spelling).\n- Argument key error: Created a key not presented, or different from those presented in [Ground Truth].\n- Argument value type error: The type of the created argument value is inappropriate (not created as the type specified in the description of [Available Functions]).\n- Logical error in argument value: The created argument value exceeds the permissible range as per [Ground Truth] and [Acceptable Arguments].\n\u2022 Answer Completion\n[Criterion]: Appropriateness of completing the answer without altering semantics, based on context\nDetermine if the [Submission] is a \"pass\" or \"fail\". In this submission, the role of the assistant is to convey the result returned by a specific function to the user. Instead of directly passing on data in JSON format, it should be paraphrased into conversational human utterance. It's important that the paraphrased content does not semantically differ from the tool's content. You are provided with a [Ground Truth] for each [Query], which you can use to evaluate the response. However, the [Ground Truth] is not the absolute and only answer. A slightly more concise response is also acceptable.\n\u2022 Slot Question\n[Criterion]: Occurrence of a proper slot filling question\nEvaluate whether it's a \"pass\" or a \"fail\".\nIn the [Query], a user asks the assistant a question or makes a request, and in [Available Functions], there exists a suitable function to perform this task, but there is a lack of required parameter values needed to call the function. In this [Submission], the assistant is required to ask the user for any additional information necessary to invoke the appropriate function and complete the task. You are given a [Ground Truth] for each [Query], so you can refer to this for evaluating the response.\n\"pass\"\n- Appropriate questions for slot filling were asked. (It is not an issue if the 'function call' or 'tool call' item is null.)\n\"fail\"\n- Tool call missing required information.\n- Tool call with incorrect information: Hallucinated values not found in the [Query].\n\u2022 Relevance Detection\n[Criterion]: Detecting the relevance of the [Query] to the function call functionality or [Available Functions]\nEvaluate whether it's a \"pass\" or a \"fail\".\nIn the [Query], it presents a scenario that does not necessitate a tool call. You are given a [Ground Truth] for each [Query], so you can refer to this for evaluating the response.\n\"pass\"\n- For user statements that didn't require a function call, the model leveraged its available knowledge to interact naturally.\n- When there's a need for an external tool or real-time information beyond the capabilities of the language model, and the [Available Functions] don't cover these needs, it's explained that the feature isn't provided. Therefore, it's clarified that the question cannot be answered or the request cannot be fulfilled.\n\"fail\"\n- A tool was called improperly or unnecessarily.\n- The task was not rejected despite requiring an external tool or real-time information not covered by the language model and [Available Functions], leading to an inaccurate claim that it could be performed or had been performed.\nC Misalignment in Judgment between Human and LLM\nMisalignment cases include instances where the LLM judged a submission as \"pass\" but it was actually a \"fail\" (False Positives, FP), and where the LLM judged as \"fail\" but it was actually a \"pass\" (False Negatives, FN). The quantity and proportion of items that were adjusted through qualitative review are as shown in Table 5."}]}