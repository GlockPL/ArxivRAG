{"title": "Improving Interpretability and Accuracy in Neuro-Symbolic Rule Extraction Using Class-Specific Sparse Filters", "authors": ["Parth Padalkar", "Jaeseong Lee", "Shiyi Wei", "Gopal Gupta"], "abstract": "There has been significant focus on creating neuro-symbolic models for interpretable image classification using Convolutional Neural Networks (CNNs). These methods aim to replace the CNN with a neuro-symbolic model consisting of the CNN, which is used as a feature extractor, and an interpretable rule-set extracted from the CNN itself. While these approaches provide interpretability through the extracted rule-set, they often compromise accuracy compared to the original CNN model. In this paper, we identify the root cause of this accuracy loss as the post-training binarization of filter activations to extract the rule-set. To address this, we propose a novel sparsity loss function that enables class-specific filter binarization during CNN training, thus minimizing information loss when extracting the rule-set. We evaluate several training strategies with our novel sparsity loss, analyzing their effectiveness and providing guidance on their appropriate use. Notably, we set a new benchmark, achieving a 9% improvement in accuracy and a 53% reduction in rule-set size on average, compared to the previous SOTA, while coming within 3% of the original CNN's accuracy. This highlights the significant potential of interpretable neuro-symbolic models as viable alternatives to black-box CNNs.", "sections": [{"title": "Introduction", "content": "Interpretability in deep neural models has gained a lot of interest in recent years, e.g., [Li et al., 2022; Zhang et al., 2021; R\u00e4uker et al., 2023]. This is well placed, as some applications such as autonomous vehicles [Kanagaraj et al., 2021], disease diagnosis [Sun et al., 2016], and natural disaster prevention [Ko and Kwak, 2012] are very sensitive areas where a wrong prediction could be the difference between life and death. The above tasks rely heavily on good image classification models such as Convolutional Neural Networks (CNNS) [LeCun et al., 1989] which are not interpretable. Hence these applications could benefit greatly by using models that balance interpretability with decent accuracy.\nSpecifically, in the context of image classification using CNNs, there has been a lot of effort towards improving their interpretability by extracting a rule-set from the convolution layers, that explains the underlying decision-making logic of the model [Townsend et al., 2021; Townsend et al., 2022; Padalkar et al., 2024a; Padalkar et al., 2024b; Padalkar et al., 2024c]. The rule-set along with the CNN upto the last convolutional layer forms the NeSy model, where the final classification is done by the rule-set. This neuro-symbolic approach, where the neural component (CNN) and the symbolic component (rule-set) are used in conjunction for the classification task, has shown promise in areas such as interpretable Covid-19 and pleural effusion detection from chest X-ray images [Ngan et al., 2022].\nThe core of these neuro-symbolic methods lies in the binarization of the last-layer filter outputs using a threshold calculated post-training. This process converts each training image into a binarized feature vector derived from the final convolutional layer of the CNN. Subsequently, a decision-tree algorithm or rule-based machine learning algorithm is employed to generate symbolic rules from these binarized vectors. The classification is then performed by extracting the outputs of the last-layer filters, binarizing each filter's output using the"}, {"title": "Background", "content": "The filters in the CNN are matrices that capture patterns in the images. It has been shown that filters in the later layers of a CNN capture high-level concepts such as objects or object parts [Zhou et al., 2015]. Hence, this line of research has emerged, wherein the final decision-making rules are represented in terms of these filter outputs that learn high-level concept(s) in the images.\nNeSyFOLD [Padalkar et al., 2024a] has emerged as a leading framework, extracting a logic program (specifically, stratified answer set program [Baral, 2003]) from the CNN. Figure 1 illustrates the rule-extration pipeline of NeSyFOLD. The binarized outputs of the last convolutional layer, stored in the binarization table, serve as input to the FOLD-SE-M algorithm [Wang and Gupta, 2024]. FOLD-SE-M then generates the raw rule-set where the truth value of that predicate is determined by the corresponding filter's binarized output i.e. 0 or 1 (Fig. 1, bottom-right). Then the filters are matched to the concepts that they have learnt from the images using a semantic labelling algorithm that uses the images' semantic segmentation masks (which are masks of the images with every pixel labelled with the concept that it belongs to) to determine what concept(s) the filters are most activated by in the images. The predicates in the raw rule-set are then labelled as those concept(s). Then, during test-time, the images are passed through the CNN and their respective filter outputs are binarized using the threshold computed after training. Based on the binary filter activation values, the truth value of the predicates in the rule-set is determined and thus the classification is made based on the rule that evaluates to true.\nFOLD-SE-M [Wang and Gupta, 2024] is a Rule-Based Machine Learning (RBML) algorithm that generates rules from tabular data, encoding them as stratified answer set programs. This means that there are no cycles through negation in the rules. It uses special abx predicates to represent exceptions, where x is a unique identifier. The algorithm incrementally generates literals for default rules to cover positive examples while avoiding negative ones. It recursively learns exceptions by swapping positive and negative examples. FOLD-SE-M includes two hyperparameters: ratio, which limits the ratio of false positives to true positives covered by a predicate, and tail, which sets a threshold for the minimum number of examples covered by a rule. Compared to decision-tree classifiers, FOLD-SE-M has been shown to produce fewer, more interpretable rules with higher accuracy [Wang and Gupta, 2024].\nThe NeSyFOLD framework when used with a CNN trained using a class-specific sparse filter learning technique such as Elite BackProp (EBP) [Kasioumis et al., 2021] achieves state-of-the-art results [Padalkar et al., 2024a]. EBP is designed to associate each class with a small number of highly responsive \"Elite\" filters. This is achieved by employing a loss function (along with the cross-entropy loss) that penalizes filters with a lower probability of activation for any class, while reinforcing those with higher activation probabilities during training. As a result, only a few filters learn the representations for each class. The number of elite filters assigned to each class is controlled by the hyperparameter K. This reduces the number"}, {"title": "Methodology", "content": "We now introduce our novel sparsity loss for learning class-specific filters and effective binarization of those filter outputs to address the limitation of EBP discussed above."}, {"title": "Calculating the Sparsity Loss", "content": "The P matrix stores the probability of activation for each filter across all classes. It is a 2D matrix of shape (C, F), where C is the total number of classes and F is the total number of filters in the last convolutional layer. We indicate 2 different methods to compute the P matrix.\n1. Extract the feature maps generated by each filter from the last convolutional layer for all training images.\n2. For each image, compute the normalized feature map output for each filter:\n$Norm_{i, j} = \\frac{\\sum_{h,w} |f_{i,j,h,w}|}{H \\cdot W} $\ni i \u2208 [1, \u039d], j \u2208 [1, F]\nwhere H and W are the spatial dimensions of the feature map f, N is the number of training images, and F is the number of filters.\n3. Accumulate these norms into a class-filter matrix D of shape (C, F) by summing over all images of the same class such that each row represents a class and each column represents the cumulative norm value for each filter.\n4. For each class, identify the top K filters with the highest cumulative activations, where K is a hyperparameter.\n5. Calculate probability P[i][j] for each filter j in class i:\n$P[i][j] = \\begin{cases} 1, & \\text{if } j \\in \\text{ Top-K filters for } i \\\\ 0, & \\text{otherwise} \\end{cases}$ (1)\nCompute the P matrix by randomly initializing K filters for each class as 1 and every other filter as 0."}, {"title": "Threshold Tensor Calculation", "content": "The threshold tensor is computed to determine the activation thresholds for each filter. The steps are:\n1. For each training image, calculate the L2-norm of the filter feature maps from the last convolutional layer:\n$Norm_{i,j} = || f_{i,j}||_2$, i\u2208 [1,N], j \u2208 [1, F]\n2. Compute the mean (\u00b5j) and standard deviation (\u03c3j) of these norms for each filter across all images.\n3. Calculate the threshold for each filter:\n$Threshold_j = h_1 \\cdot \\mu_j + h_2 \\cdot \\sigma_j$ (2)\nwhere h\u2081 and h\u2082 are hyperparameters."}, {"title": "Sparsity Loss Computation", "content": "During training, the sparsity loss is calculated as follows:\n1. Compute the L2-norms of the filter feature maps for every input image n:\n$Norm_{n,j} = || f_{n,j} ||_2, n \\in [1, \u039d], j\\in [1, F]$ (3)\n2. Subtract the precomputed threshold from each filter norm:\n$Adjusted_{n,j} = Norm_{n,j} \u2013 Threshold_j$ (4)\n3. Apply the sigmoid function to the adjusted norms so that the outputs are in range [0, 1]:\n$Sigmoid_{n,j} = \\sigma(Adjusted_{n,j}) = \\frac{1}{1+ e^{-Adjusted_{n,j}}}$ (5)\n4. Retrieve the filter activation probabilities for the target class of each image using the P matrix:\n$ClassProbabilities_{n,j} = P[class \\text{ of } n][j]$\n5. Define the target activations for the filters for each image:\n$Target_{n.j} = \\begin{cases} 1, & \\text{if ClassProbabilities}_{n,j} = 1 \\\\ 0, & \\text{otherwise} \\end{cases}$ (6)\n6. Compute the Binary Cross-Entropy (BCE) loss between the predicted sigmoid activations and the target activations:\n$L_{sparsity} = - \\frac{1}{N \\cdot F} \\sum_{n=1}^{N} \\sum_{j=1}^{F} \\left[ Target_{n,j} \\cdot \\log(Sigmoid_{n,j}) + (1 \u2013 Target_{n,j}) \\cdot \\log(1 \u2013 Sigmoid_{n,j}) \\right]$ (7)"}, {"title": "Total Loss", "content": "The sparsity loss is combined with the cross-entropy loss to form the total Loss L:\n$L = \\alpha \\cdot L_{cross-entropy} + \\beta \\cdot L_{sparsity}$ (8)\nwhere \u03b1 and \u03b2 are hyperparameters controlling the trade-off between classification accuracy and filter sparsity.\nThe most critical step in calculating the sparsity loss is assigning each filter a target value of either 1 or 0 (Equation (6)). This assignment frames the problem as a binary classification task, where each filter is categorized as either \"active\" or \"inactive.\u201d The Binary Cross-Entropy (BCE) loss is then applied to optimize this classification which is designed for such tasks. Thus, as the training progresses, the filter outputs gradually converge toward binary values (0 or 1). This ensures minimal information loss when the outputs are finally rounded to binarize them for rule-extraction."}, {"title": "Extracting the Rule-Set and Inference", "content": "Once the training is complete, the NeSyFOLD pipeline is employed wherein each image in the train set is passed through the CNN and the last layer filter feature maps are obtained. Then Equations (3) to (5) are applied in order to obtain the sigmoid values which are rounded to the nearest integer (i.e. 0 or 1). Thus, each image is converted to a binarized vector and the FOLD-SE-M algorithm is used on the full binarized train set to obtain the rule-set.\nAt test-time, the input images are passed through the CNN to obtain the outputs of the last convolutional layer filters. The rounded sigmoid values are then computed as described previously. These values are used as truth values for the predicates in the rule-set. The final classification is made by employing the FOLD-SE-M toolkit's internal interpreter to determine which rule is activated based on these truth values."}, {"title": "Experiments and Results", "content": "We conducted experiments to address the following research questions:\nQ1: How does altering various steps in the sparsity loss computation affect the performance of the NeSy model?\nQ2: What is the maximum performance gain that can be achieved w.r.t. accuracy and rule-set size compared to NeSyFOLD-EBP, using the sparsity loss?\nQ3: How well does this approach scale as the number of classes increases?\nQ4: What effect does the sparsity loss have on the representations learned by the CNN filters?"}, {"title": "[Q1, Q2, Q3] Training Strategies (TS), Performance and Scalability:", "content": "We evaluate various training strategies, each by varying a key step in the computation of the sparsity loss. First, we explain the setup of our experiments:\nSetup: We evaluate performance using three key metrics: (1) the accuracy of the NeSy model (comprising the CNN and the extracted rule-set), (2) the fidelity of the NeSy model with respect to the original CNN, and (3) the total number of predicates in the rule-set (referred to as the rule-set size). Fidelity is defined as the proportion of predictions made by the NeSy model that match those of the original CNN, calculated by dividing the number of matching predictions by the total number of images in the test set. A smaller rule-set size improves interpretability [Lage et al., 2019], hence we use rule-set size as a metric of interpretability.\nDatasets: We evaluate our approach on the same datasets as NeSyFOLD-EBP (NeSyFOLD with Elite BackProp (EBP)) [Padalkar et al., 2024a], ensuring a fair comparison. We used the Places [Zhou et al., 2017] dataset which contains images from various indoor and outdoor \u201cscene\" classes such as \"bathroom\", \"bedroom\", \"desert road\", \"forest road\" etc. We created multiple subsets from this dataset of varying number of classes. P2 includes images from the bathroom and bedroom classes, P3.1 is formed by adding the kitchen class images to P2. P5 is created by adding dining room and living room images to P3.1, and P10 further includes home office, office, waiting room, conference room, and hotel room images in addition to all classes in P5. Additionally, P3.2 comprises desert road, forest road, and street images, while P3.3 contains desert road, driveway, and highway images. Each class has 5k images of which we made a 4k/1k train-test split for each class and we used the given validation set as it is. We also used the German Traffic Sign Recognition Benchmark (GTSRB) [Stallkamp et al., 2012] dataset which consists of images of various traffic signposts. This dataset has 43 classes of signposts. We used the given test set of 12.6k images as it is and did an 80: 20 train-validation split which gave roughly 21k images for the train set and 5k for the validation set.\nHyperparameters: We employed a VGG16 CNN pretrained on ImageNet [Deng et al., 2009], training for 100 epochs with batch size 32. The Adam [Kingma and Ba, 2015] optimizer was used, accompanied by class weights to address data imbalance. L2 Regularization of 0.005 spanning all layers, and a learning rate of 5 \u00d7 10-6 was adopted. A decay factor of 0.5 with a 10-epoch patience was implemented. Images were resized to 224 \u00d7 224, and hyperparameters h1 and h2 (eq. (2)) for calculating threshold for binarization of kernels, were set at 0.6 and 0.7 respectively. The \u03b1 and \u03b2 used in the final loss calculation (Eq. (8)) were set to 1 and 5 respectively. The K value to find Top-K filters per class as described in Method 1, step 4 of computing the filter probability matrix (P) is set to 5 for P2, P3.1, P3.2 and P3.3 and 20 for P10 and GT43 as they have a higher number of classes. The ratio and tail hyperparameter values for rule extraction using FOLD-SE-M were set to 0.8 and 5 \u00d7 10-3 respectively.\nNext, we discuss the various training strategies using this sparsity loss function\nTS 1: This strategy involves training the CNN for 50 epochs with the sparsity loss term set to 0. After the 50th epoch we compute the P matrix using Method 1, Top-K filters per class and the thresholds for all the filters. Then, we train for 50 more epochs with the sparsity loss term in effect. The intuition behind this approach is that training the CNN without sparsity constraints in the initial phase should allows the filters to naturally specialize and differentiate themselves. This specialization should result in a more informed selection of the Top-K filters, whose targets are set to 1 during the subsequent training phase. This two-step process aims to improve both accuracy and the quality of the learned representations.\nTS 2: Here we compute the filter thresholds, P matrix using Method 1, and Top-K from the initial ImageNet pretrained weights of the CNN. The sparsity loss is employed from the start along with the cross-entropy loss. This is done to understand how enforcing sparsity constraints from the beginning of training affects the accuracy and rule-set size.\nTS 3: To understand how the choice of the Top-K filters affects the performance, we initialize P using the random initialization method (Method 2). So, Top-K filters for each class are chosen as these randomly assigned filters and the sparsity loss is employed from the start along with the cross-entropy loss.\nTS 4: In this strategy, we use the same configuration as TS 3, but the cross-entropy loss term is set to 0. Consequently, only the sparsity loss is optimized throughout the training. This"}, {"title": "[Results: Q1, Q2] Performance Comparison among various Training Strategies and Maximum Gain:", "content": "We present the accuracy of the NeSy model (blue) and the rule-set size (red) for all training strategies and NeSyFOLD-EBP (NE) in Table 1. Table 2 reports the original CNN model's accuracy (blue) alongside the NeSy model's fidelity with respect to the CNN (red). Accuracy and fidelity are average percentages, while rule-set size represents the average rule-set size, all computed over 5 runs per dataset and rounded to the nearest integer. MS shows the average over all datasets for each strategy. When discussing the performance of each strategy, we omit specifying that it pertains to the NeSy model generated by that strategy, unless explicitly referring to the original CNN model.\nTable 1 shows that TS2, TS3, and TS4 outperforms NE in both accuracy and rule-set size. TS1 achieves accuracy comparable to NE while generating a smaller rule-set, on average. TS5 performs the worst w.r.t. accuracy but generates the smallest rule-sets. Ideally, the accuracy-to-rule-set-size ratio should be maximized, favoring high accuracy with smaller rule-sets for better interpretability.\nTS3 stands out as the best overall, achieving 9% higher accuracy and generating a rule-set that is 53% smaller than NE on average. TS3 does even better than the trained CNN on P2, P3.1, P3.2 and P3.3 w.r.t. accuracy.\nIn TS3, K filters are randomly assigned a probability of 1 per class, with the rest set to 0 in the P matrix at the start of training. Interestingly, this approach outperforms TS2, which achieves a 8% higher accuracy and a 33% reduction in rule-set size compared to NE. Recall that in our experiments the CNN was initialized with pretrained ImageNet weights, as is standard practice.\nIn TS2, the Top-K filters are selected at the start of the training based on their activation strength using feature map norms. However, the poorer performance compared to TS3 may stem from the pretrained ImageNet weights, which are obtained through conventional training with a cross-entropy loss. This process might not yield an optimal filter selection for the sparsity loss and hence the random selection of Top-K filters in TS3 leads to better performance.\nThis trend is even more pronounced in TS1, where the CNN is initially trained for 50 epochs using only the cross-entropy loss. Afterward, the Top-K filters and the P matrix are computed, and the sparsity loss is activated alongside the cross-entropy loss for another 50 epochs. However, since the initial filter selection is based on a CNN trained solely with cross-entropy loss, the chosen Top-K filters might not be optimal for the sparsity loss, thus leading to a suboptimal performance.\nIn TS4, the CNN is trained solely with the sparsity loss, without the cross-entropy loss, using randomly initialized Top-K filters. This leads to a remarkable observation: even without cross-entropy, the NeSy model achieves a 1% accuracy gain and a 61% reduction in rule-set size compared to NE. The absence of cross-entropy constraints allows the sparsity loss to better separate filters in the latent space, optimizing their utility as features in the binarization table. As a result, FOLD-SE-M can effectively generate rules that segregate classes using fewer filters per class, reducing the rule-set size while maintaining high accuracy.\nIn TS5, the thresholds are not calculated, and Top-K filters are randomly selected. The sigmoid is applied directly to the norms of the filters without subtracting thresholds beforehand. As norms are always positive, the minimum sigmoid value becomes 0.5, limiting the representation power. This means that the non-Top-K (both poorly and highly activating) filters can only be assigned a value of 0.5 at the minimum, making them harder to distinguish from the Top-K filters. In contrast, subtracting the threshold enables some filter norms to become negative, allowing the sigmoid to push irrelevant filters closer to 0. This facilitates true binarization, where non-relevant filters are suppressed, and preferably the Top-K filters remain closer to 1. The lack of threshold subtraction in TS5 compromises its performance especially as the number of classes increases.\nNote that the accuracy of the original CNN model as denoted in Table 2 (blue) is similar for NE and TS1-TS3. TS4 doesn't employ the cross-entropy loss so the accuracy of the CNN is naturally low. In TS5 the accuracy drops as the number of classes increases. Also note that the gap between the original CNN model and the NeSy model in terms of accuracy is 12% in NE but it drops to just 3% and 2% for TS3 and TS2 respectively, suggesting that the information loss caused by post-training binarization is greatly reduced.\nThe NeSy models generated in both TS2 and TS3 have the highest average fidelity which is 7% higher than NE. TS4 shows the lowest fidelity because the CNN is not trained for classification. So, the NeSy model generated by TS4 is a stand alone-model that does not follow the original model at all but does give a higher accuracy than the baseline (NE) on average. TS5 shows poor fidelity because of the limited representational capability as discussed earlier."}, {"title": "[Results: Q3] Scalability:", "content": "TS2, TS3, and TS4 perform better than NE for P10 and GT43 which are datasets with 10 and 43 classes respectively, showing better scalability. In fact, TS2 and TS3 show a 19% and 17% increase in accuracy and show a 46% and 55% decrease in rule-set size respectively for P10. Similarly for GT43, TS2 and TS3 show a 6% and 10% increase in accuracy and a 32% and 57% decrease in rule-set size."}, {"title": "[Q4] Filter Representations:", "content": "For the P3.1 dataset, which includes the bathroom, bedroom, and kitchen classes, we analyzed the extracted rule-sets for TS1-TS5. For each class, under each training strategy, we identified the top filter by examining the rule-set and selecting the filter that appeared as the first non negated predicate in the first rule for each class. We then overlaid the feature maps of these filters onto the top three images that activated them the most. Figure 2 displays these overlays, showing three images per filter, per class, and per training strategy. Each row corresponds to a training strategy, while each set of three columns represents the top three images that most activate the chosen filter for a specific class. For instance, filters 145, 295, and 38 were selected for TS3 for the bathroom, bedroom, and kitchen classes, respectively. The"}, {"title": "Related Works", "content": "Extracting knowledge from neural networks is a well studied topic [Andrews et al., 1995; Tickle et al., 1998] but only recently extracting rules from CNNs by binarizing the filter outputs has gained popularity [Townsend et al., 2021; Townsend et al., 2022; Padalkar et al., 2024a]. However, none of these approaches facilitate binarization of filter outputs like our method. Our approach addresses the performance gap between the NeSy model and the trained CNN.\nThere has been a lot of effort in sparsifying the weights of neural networks [Ma and Niu, 2018]. However, our work is concerned with sparsifying the activations or outputs of the filters. Other approaches such as Dropout [Srivastava et al., 2014], Sparseout [Khan and Stavness, 2019] and Dasnet [Yang et al., 2019] are techniques where the activations are masked during training to induce activation sparsity. Our sparsity loss does not involve any masking. Methods such as ICNN [Zhang et al., 2018], ICCNN [Shen et al., 2021] and learning sparse class-specific gate structures [Liang et al., 2020] deal with class-specific filters but unlike our approach they do not focus on binarization of the filter outputs. Some more approaches induce sparsity in the activations layer-wise i.e. a certain percentage of activations in each layer is retained [Georgiadis, 2019; Kurtz et al., 2020]. We on the other hand induce class-wise sparsity.\nPrevious research on interpreting CNNs has primarily focused on visualizing the outputs of CNN layers, aiming to establish relationships between input pixels and neuron outputs. Zeiler et al. [Zeiler and Fergus, 2014] employ output activations as a regularizer to identify discriminative image regions. Other studies [Denil et al., 2014; Selvaraju et al., 2017; Simonyan et al., 2013], utilize gradients to perform input-output mapping. Unlike these visualization methods, our approach is useful for methods that generate a rule set using filter outputs."}, {"title": "Conclusion and Future Work", "content": "In this work, we have addressed the issue of information loss due to the post-binarization of filters in rule-extraction frameworks such as NeSyFOLD. We presented a novel sparsity loss that helps in learning class-specific sparse filters and binartizes the filter outputs during training itself to mitigate post-binarization information loss. We evaluated five training strategies employing the sparsity loss and compared their performance with the baseline, NeSyFOLD-EBP.\nAs general guidelines for researchers, we recommend using TS2 and TS3 when high fidelity to the original model is required, alongside a balance between accuracy and interpretability. TS4 is best suited for scenarios where minimizing the rule-set size is a priority. Evidently TS2, TS3 and TS4 all outperform the baseline both w.r.t. accuracy and the rule-set size.\nFinally, we demonstrated that interpretable neuro-symbolic methods can achieve accuracy levels within 3% - 4% of the original CNN model, without compromising on interpretability. This establishes these methods as viable and interpretable alternatives to black-box CNN models.\nCurrently, our approach is tailored to CNN models, and it would be intriguing to investigate whether a similar sparsity loss function could be adapted for Vision Transformers. Although the challenge lies in the absence of filters that directly capture concepts, sparse autoencoders could be used to extract relevant concepts from attention layers [Cunningham et al., 2023]. Another promising direction is integrating the symbolic rule-set into the training loop to further mitigate information loss by leveraging soft decision tree like structure for gradient backpropogation [Irsoy et al., 2012]. We aim to advance the development of interpretable neuro-symbolic models that match or even surpass the performance of black-box neural models, continuing our quest for models that combine interpretability and accuracy."}]}