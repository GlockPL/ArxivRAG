{"title": "SPARSE AUTOENCODERS DO NOT FIND CANONICAL UNITS OF ANALYSIS", "authors": ["Patrick Leask", "Bart Bussmann", "Michael Pearce", "Joseph Bloom", "Curt Tigges", "Noura Al Moubayed", "Lee Sharkey", "Neel Nanda"], "abstract": "A common goal of mechanistic interpretability is to decompose the activations of neural networks into features: interpretable properties of the input computed by the model. Sparse autoencoders (SAEs) are a popular method for finding these features in LLMs, and it has been postulated that they can be used to find a canonical set of units: a unique and complete list of atomic features. We cast doubt on this belief using two novel techniques: SAE stitching to show they are incomplete, and meta-SAEs to show they are not atomic. SAE stitching involves inserting or swapping latents from a larger SAE into a smaller one. Latents from the larger SAE can be divided into two categories: novel latents, which improve performance when added to the smaller SAE, indicating they capture novel information, and reconstruction latents, which can replace corresponding latents in the smaller SAE that have similar behavior. The existence of novel features indicates incompleteness of smaller SAEs. Using meta-SAEs - SAEs trained on the decoder matrix of another SAE - we find that latents in SAEs often decompose into combinations of latents from a smaller SAE, showing that larger SAE latents are not atomic. The resulting decompositions are often interpretable; e.g. a latent representing \u201cEinstein\" decomposes into \u201cscientist\u201d, \u201cGermany\", and \"famous person\". Even if SAEs do not find canonical units of analysis, they may still be useful tools. We suggest that future research should either pursue different approaches for identifying such units, or pragmatically choose the SAE size suited to their task. We provide an interactive dashboard to explore meta-SAEs:\nhttps://metasaes.streamlit.app/", "sections": [{"title": "INTRODUCTION", "content": "Mechanistic interpretability aims to reverse-engineer neural networks into human-interpretable algorithms (Olah et al., 2020; Meng et al., 2022; Geva et al., 2023; Nanda et al., 2023; Elhage et al., 2021). A key challenge of mechanistic interpretability is identifying the correct units of analysis fundamental components that can be individually understood and collectively explain the network's function. Ideally, these units would be unique, with no variations (Bricken et al., 2023); complete, encompassing all necessary features (Elhage et al., 2022); and atomic or irreducible, indivisible into smaller components (Engels et al., 2024). We refer to a set of units with all of these properties as canonical.\nInitially, researchers hoped that individual MLP neurons (Meng et al., 2022; Olah et al., 2020) and attention heads (Wang et al., 2022; Olsson et al., 2022) could serve as these units. However, these"}, {"title": "SPARSE AUTOENCODERS", "content": "Sparse dictionary learning is the problem of finding a decomposition of a signal that is both sparse and overcomplete (Olshausen & Field, 1997). Lee et al. (2007) initially applied the sparsity constraint to deep belief networks, with SAEs later being applied to the reconstruction of neural network activations (Bricken et al., 2023; Cunningham et al., 2023). In the context of large language models, SAEs decompose model activations x \u2208 Rn into sparse linear combinations of learned directions, which are often interpretable and monosemantic.\nAn SAE consists of an encoder and a decoder:\nf(x) := \u03c3(Wencx + benc), (1)\nx(f) := Wdecf + bdec. (2)"}, {"title": "RELATED WORK", "content": "SAEs for Mechanistic Interpretability. SAEs have been demonstrated to recover sparse, monose-mantic, and interpretable features from language model activations (Bricken et al., 2023; Cunning-ham et al., 2023; Templeton, 2024; Gao et al., 2024; Rajamanoharan et al., 2024a;b), however their application to mechanistic interpretability is nascent. After training, researchers often interpret the meaning of SAE latents by examining the dataset examples on which they are active, either through manual inspection using features dashboards (Bricken et al., 2023) or automated interpretability techniques (Gao et al., 2024). SAEs have been used for circuit analysis (Marks et al., 2024) in the vein of (Olah et al., 2020; Olsson et al., 2022); to study the role of attention heads in GPT-2 (Kissane et al., 2024); and to replicate the identification of a circuit for indirect object identification in GPT-2 (Makelov et al., 2024). Transcoders, a variant of SAEs, have been used to simplify circuit analysis and applied to the greater-than circuit in GPT-2 (Dunefsky et al., 2024). While these applications highlight SAEs as valuable tools for understanding language models, it remains unclear whether they identify canonical units of analysis.\nRepresentational Structure. Language models trained on the next-token prediction task learn rep-resentations that model the generative process of the data. For example, Li et al. (2022) found that transformers trained by next-move prediction to play the board game Othello explicitly represent the board state; and Gurnee & Tegmark (2023) used linear probes to predict geographical and temporal information from language model activations. SAEs learn these structures as well, for example, the activations of a cluster of GPT-2 SAE latents form a cycle when reconstruction activations for weekday name tokens (Engels et al., 2024). Bricken et al. (2023) find evidence of convergent global structure by applying a 2-dimensional UMAP transformation to decoder directions of SAEs of dif-ferent sizes. This results in a rich structure of latent projections with regions of latents relating to similar concepts close to each other.\nTuning Dictionary Size. Previous work on tuning dictionary size has mixed findings regarding how SAEs scale with dictionary size. For instance, Templeton (2024) observed that larger SAEs learn latents absent in smaller ones, such as specific chemical elements. Conversely, Bricken et al. (2023) found similar latents across various SAE sizes, noting that latents in smaller SAEs sometimes split into multiple latents as the dictionary size increases (Appendix A.3 includes such examples taken from our SAEs). Currently, the effect of dictionary size on the learned latents has not been systematically studied.\nModel Stitching. Model stitching is a method by which layers from one neural network are \"stitched\" or swapped into another neural network. Lenc & Vedaldi (2015); Bansal et al. (2021) propose that model stitching can be used to quantify representation similarity across different neu-ral network architectures and training regimes by demonstrating that if two networks trained on the same task can be stitched together with minimal loss in performance, it suggests a high degree of alignment in their intermediate representations."}, {"title": "SAE STITCHING", "content": "Templeton (2024) finds that a larger SAE has latents that activate on certain specific individual chemical elements that a smaller SAE did not represent. Conversely, Bricken et al. (2023) observes that smaller SAEs learn latents activating on broad mathematical text, while larger SAEs learn latents activating on more specific mathematical categories. On the one hand, this suggests that training large SAEs is necessary to learn latents relating to all relevant concepts. On the other hand, this means that some of the capacity of larger SAEs is used to represent similar information as smaller SAEs, but with more latents.\nTo compare latents of SAEs with different dictionary sizes, we introduce SAE stitching. In SAE stitching, we add or replace latents in one SAE with latents from another and observe the effect on the reconstruction performance. We find that larger SAEs learn both finer-grained versions of latents from smaller SAEs (reconstruction latents) and entirely new latents that capture additional information (novel latents). SAE stitching allows us to identify these two categories of latents in larger SAEs."}, {"title": "STITCHING OPERATION", "content": "The output of an SAE can be expressed as a sum of the contributions from the individual latents:\nx := \u2211di=0 Wdec fi (x) + bdec (4)\nwhere d is the size of the SAE dictionary, Wdeci is an individual decoder direction, fi(x) is the activation value for the latent i, and bdec is the decoder bias term. To stitch latents from one SAE into another, we modify the reconstruction as follows:\nx := abdec + (1 \u2212 a)bdec + \u2211lo \u2208 Lo Wdeclo fo,lo (x) + \u2211l1\u2208L1 Wdecl1 f1,l1 (x) (5)\nwhere a = LoL, Wdeclo and Wdecl1 represent individual decoder directions and Lo and L\u2081 are the set of latents we include from the respective SAEs. Unlike with model stitching (Bansal et al., 2021), SAE decoder directions are privileged and require no transformations to stitch.\nWe experiment with stitching on eight SAEs trained on the residual stream of layer 8 of GPT 2 Small (Radford et al., 2019) with dictionary sizes ranging from 768 to 98,304 and two of the Gemma Scope SAEs (Lieberum et al., 2024) trained on Gemma 2 2B (Team et al., 2024) with dictionary size 16,384 and 32,768. For the full list of SAE properties and training details see Appendix A.5."}, {"title": "NOVEL AND RECONSTRUCTION LATENTS", "content": "Using SAE stitching, we want to find out whether latents in larger SAEs are just more fine-grained versions of latents of smaller SAEs, or whether they represent novel information that is missed by the smaller SAEs. If we stitch a latent from a larger SAE into a smaller SAE and the reconstruction"}, {"title": "META-SAES", "content": "In Section 4, we demonstrated through SAE stitching that increasing dictionary size leads to larger SAEs learning not only novel features, but also reconstruction latents that encode similar information to the latents in smaller SAEs. We found that some of these reconstruction latents have high cosine similarity with multiple latents in the smaller SAE, see Appendix A.4. This suggests that these smaller SAE latents are composing into more complex latents, such as in the example of a latent representing \"blue\" and another latent representing \u201csquare\u201d combining in a \"blue square\"-latent (see Figure 2). If large SAE features are indeed compositions rather than atomic, it may be possible to decompose them into more fundamental units."}, {"title": "EVALUATING META-SAE DECOMPOSITIONS", "content": "We follow the lead of (Bills et al., 2023; Bricken et al., 2023; Cunningham et al., 2023; Rajamanoha-ran et al., 2024b) in evaluating neural network and SAE latents using automated interpretability with LLMs.\nFirst, we generate explanations of SAE latents by presenting GPT-40-mini with a list of input se-quences that activate an SAE latent to varying degrees, and prompting it to generate a natural lan-guage explanation of the feature consistent with the activations. Second, we collect all of the SAE latents on which a meta-SAE latent is active, and prompt again with the explanation and a number of top activating examples of each of the SAE latents, asking the model to provide an explanation of the common behavior of the SAE latents, which becomes the meta-SAE latent explanation.\nWe evaluate the meta-SAE latent explanations in a zero-shot multiple-choice-question setting. For a given latent we prompt GPT-40-mini with the explanations of the meta-SAE latents that are active on that latent, and ask it to choose which of 5 SAE latent explanations most relate to the explanations of the meta-SAE latents. One of these SAE latent explanations is of the correct latent, with the remaining 4 explanations corresponding to random latents from the SAE. On a random sample of 1,000 SAE latents, GPT-40-mini chose the correct answer of the five options 73% of the time."}, {"title": "COMPARISON TO SMALLER SAE LATENTS", "content": "In Section 4, we hypothesised that latents in larger SAEs can be described as the composition of latents in smaller SAEs. We find that meta-SAEs learn similar latents to similar sized SAEs trained on the original reconstruction problem. Plots of the maximum cosine similarity between meta-SAE latents and latents from SAEs of different sizes are shown in Figure 6. We validate this by replac-ing meta-SAE decoder directions with the most similar SAE decoder direction, and retrained the encoder. This results in only a small decrease in meta-SAE reconstruction performance (Appendix Figure 20). This suggests that larger SAE latents are indeed composed of latents from smaller SAEs."}, {"title": "CONCLUSION", "content": "Our findings challenge the idea that SAEs can discover a canonical set of features. Through SAE stitching, we demonstrated that smaller SAEs are incomplete, missing information that novel fea-tures in larger SAEs capture. Moreover, our meta-SAE experiments showed that, due to the sparsity penalty, latents in larger SAEs are often not atomic but compositions of interpretable meta-latents. These findings suggest that there is no single SAE width at which it learns a unique and complete dictionary of atomic features that can be used to explain the behavior of the model.\nThese results imply that rather than converging on a unique, complete, and irreducible set of features, SAEs of different sizes offer varying granularities and compositions of features. This indicates that the choice of SAE size should be guided by the specific interpretability task at hand, accepting that no single SAE configuration provides a universal solution. However, our methods neither identify canonical units of analysis, nor the size of dictionary to use for a given task. Furthermore, our work only studies two LLMs and does not include very large SAEs, such as in Templeton (2024). We also acknowledge that the use of SAEs in mechanistic interpretability is nascent, and whilst early results are encouraging, associating the learned latents of SAEs with interpretable concepts is still an open problem. In conclusion, our research suggests that alternative methods are required for identifying canonical units, and that SAE practitioners should embrace a pragmatic approach towards choosing dictionary size when using SAEs on interpretability tasks such as probing, unlearning and steering."}, {"title": "GLOSSARY OF TERMS", "content": "Active Latents (L0): For an input x and SAE activation function f(x), the number of non-zero elements in f(x). Typically measured as average L0 across a batch: L0 = \u2211i || f(xi)||\u03bf.\nCanonical Unit: Hypothetical, fundamental building blocks of a LLMs computation that are unique, complete, and atomic.\nCross-Entropy Degradation: The increase in cross-entropy loss when replacing the model acti-vations with the reconstruction of the SAE.\nDecoder Directions: The columns of the decoder matrix Wdec that map from latent to input space. Two decoder directions with high cosine similarity suggest related features."}, {"title": "SAE VARIANTS", "content": "ReLU SAEs (Bricken et al., 2023) use the L1-norm S(f) := ||f||1 as an approximation to the L0-norm for the sparsity penalty. This provides a gradient for training unlike the L0-norm, but suppresses latent activations harming reconstruction performance (Rajamanoharan et al., 2024a). Furthermore, the L1 penalty can be arbitrarily reduced through reparameterization by scaling the decoder parameters, which is resolved in Bricken et al. (2023) by constraining the decoder directions to the unit norm. Resolving this tension between activation sparsity and value is the motivation behind more recent architecture variants.\nTopK SAEs (Gao et al., 2024; Makhzani & Frey, 2014) enforce sparsity by retaining only the top k activations per sample. The encoder is defined as:\nf(x) := TopK(Wencx + benc) (7)\nwhere TopK zeroes out all but the k largest activations in each sample. This approach eliminates the need for an explicit sparsity penalty but imposes a rigid constraint on the number of active latents"}, {"title": "EXAMPLE LATENTS", "content": "Figure 7 shows a histogram of the maximum decoder cosine similarity for each latent in GPT2-1536 over all latents in GPT2-768. On the right-hand-side, there is a cluster of latents with high cosine similarity.\nBricken et al. (2023) use the cosine similarity between latent activations as a measure for latent similarity. However, we use decoder cosine similarity due its lower computational cost and because it captures the latent's effect on the reconstruction. Empirically, we find a high correlation between these two metrics at values of decoder similarity relevant to our stitching experiments (see Figure 8)."}, {"title": "LATENT FAMILIES", "content": null}, {"title": "OPEN SOURCE SAE WEIGHTS", "content": "All GPT-2 Small SAEs were trained on the layer 8 residual stream, which was chosen in line with Gao et al. (2024). They were trained for 300M tokens on the OpenWebText dataset, which was processed into sequences of a maximum of 128 tokens for input into the language models. All models were trained using the Adam optimizer with a learning rate of 4 \u00d7 10-4, \u03b2\u2081 = 0.9, and \u03b22 = 0.99. The batch size used was 4096 and all were trained with a sparsity penalty of 8 \u00d7 10-5. The GPT-2 SAEs are available on Neuronpedia at Redacted URL. We also use two of the Gemma Scope SAEs (Lieberum et al., 2024) trained on Gemma 2 2B (Team et al., 2024) with dictionary size 16384 and 32768. We used the TransformerLens (https://transformerlensorg.github.io/ TransformerLens/) implementations of GPT-2 and Gemma 2 2B. CELR is the cross entropy loss recovered from either zero or mean ablation."}, {"title": "STITCHING EXPERIMENTS", "content": null}, {"title": "METASAE ADDITIONAL FIGURES", "content": null}, {"title": "INTERPRETABILITY EXPERIMENTS", "content": "In this paper, we demonstrate that larger SAEs may learn more narrow, composed concepts in or-der to improve sparsity rather than just learning concepts that are missing in smaller SAEs. Here, we provide some experimental results on sparse probing Gao et al. (2024) and concept removal benchmarks (Anonymous, 2024)1."}, {"title": "SPARSE PROBING", "content": "Similarly to Gao et al. (2024), we use sparse probes to evaluate the presence of known ground-truth features in our SAEs. If we expect a specific feature to be discovered by an SAE, then a metric for autoencoder quality is simply checking whether these features are present as latents. We do this by"}]}