{"title": "CLIP-RLDRIVE: HUMAN-ALIGNED AUTONOMOUS DRIVING VIA CLIP-BASED REWARD SHAPING IN REINFORCEMENT LEARNING", "authors": ["Erfan Doroudian", "Hamid Taghavifar"], "abstract": "This paper presents CLIP-RLDrive, a new reinforcement learning (RL)-based framework for improving the decision-making of autonomous vehicles (AVs) in complex urban driving scenarios, particularly in unsignalized intersections. To achieve this goal, the decisions for AVs are aligned with human-like preferences through Contrastive Language-Image Pretraining (CLIP)-based reward shaping. One of the primary difficulties in RL scheme is designing a suitable reward model, which can often be challenging to achieve manually due to the complexity of the interactions and the driving scenarios. To deal with this issue, this paper leverages Vision-Language Models (VLMs), particularly CLIP, to build an additional reward model based on visual and textual cues. CLIP's ability to align image and text embeddings provides important features for translating human-like instructions into reward signals to guide the AV's decision-making process. In addition, two RL algorithms are applied, Proximal Policy Optimization (PPO) and Deep Q-Network (DQN), to train an agent in complex unsignalized intersection environments. The performance of these algorithms is compared with and without the CLIP-based reward model, which emphasizes the effect of CLIP on the agent's ability to learn and optimize its behavior in a way that aligns with desired driving actions. Our obtained results demonstrate that the CLIP-based DQN achieves a 96% success rate with only a 4% collision rate, significantly outperforming CLIP-based PPO, which achieves a success rate of 38% with a 54% timeout rate. This is suggestive of the superior effectiveness of the proposed framework in aligning AV behavior with human-like driving standards.", "sections": [{"title": "1 INTRODUCTION", "content": ""}, {"title": "1.1 BACKGROUND", "content": "The successful deployment of autonomous vehicles (AVs) into road systems, particularly in urban settings, demands both effective navigation and control capabilities, as well as the ability to emulate human driving behaviors [1]. To achieve this level of human-like performance, social cognition is needed in terms of understanding complex social interactions while navigating and interpreting human-like decision-making and social dynamics. This human-centric approach extends beyond environmental perception and serves as a much-needed step forward to make interaction with human drivers, pedestrians, and other road users safe and reliable [2]. One of the main obstacles regarding the large-scale commercialization of fully autonomous systems from industries such as Waymo and Tesla is the handling of edge cases [3], which refer to rare or unconventional situations that fall outside the scope of the scenarios typically encountered during training or testing. These scenarios are particularly difficult because they include complex interactions or unexpected conditions that deviate from standard traffic rules or patterns, such as adverse weather conditions, irregular behaviors of road users, and ambiguous traffic situations. As a result, traditional decision-making systems struggle to generalize or adapt effectively due to their rigid design. In contrast, a human driver outperforms in these edge cases due to the unique human ability to use intuition, social intelligence, and reasoning. Hence, it is critical to be able to connect the rule-based decision-making systems and the intuitive interpretation ability inherent in human driving behaviors [4].\nAlthough recent progress observed in Reinforcement Learning (RL), deep learning, and neural networks (NNs) have contributed to improved decision-making abilities of AVs in complex and crowded urban environments [4, 5, 6, 7], they still struggle with long-tail scenarios due to the limited scope of patterns within their training datasets [8]. On the other hand, the recent developments in Large Language Models (LLMs) have presented a transformational opportunity for improving RL frameworks. LLMs can be used to generate context-aware instructions or explanations that provide AVs with additional guidance in complex traffic scenarios [9]. These models can interpret human preferences, predict consequences, and even explain the rationale behind decisions, and therefore build a hybrid method that improves the raw data-driven learning through human-like reasoning. Hence, the decision-making process of AVs can be enhanced to include both the optimal performance and also socially and ethically aligned actions that are interpretable to other road users as well [3, 8, 10, 11].\nIn RL, the agent's objective is to maximize cumulative returns, which makes the design of the reward function a critical and often challenging task [12]. Because the optimal policy is inherently defined by this reward function, sparse or delayed rewards in real scenarios can significantly reduce the learning of the agents [13]. In addition, because RL agents depend only on reward signals to optimize their actions, slow or limited feedback can also prevent good learning progress as well. To remedy this drawback, additional guidance in the form of a shaping reward can be introduced, which supplements the environment's natural reward signal and thus improves learning speed and performance [14, 15]. This process originates from experimental psychology, where it involves reinforcing all actions that contribute toward the desired behavior [13]. While reward shaping can accelerate learning by providing additional guidance, it may also cause the agent to optimize an augmented reward function $R'$ instead of the original reward $R$. Hence, the resulting policy might be suboptimal for the intended objective, which is because the agent might end up with behaviors that maximize the shaping reward in ways that are different from, or even in conflict with, the original task goal defined by $R$. For example, in Ref. [16], the experimented bicycle agent returned in a circle to stay upright rather than reach its goal, which illustrated how reward shaping can lead agents to prioritize the shaping reward over the original objective.\nSeveral techniques have been proposed to address the potential issues with reward shaping. These methods are specifically designed to provide the agent's behavior alignment with the original task objectives, which also prevents it from becoming disproportionately influenced by the additional shaping rewards. Recently, leveraging pre-trained foundation models to generate reward signals for RL adjustment has become a needed approach in the development of LLMs [17]. Some approaches only require a small amount of natural language feedback instead of a whole dataset of human preferences [18]. Although the use of language models has become popular to compute a reward function from a structured environment representation [19], many RL tasks are indeed visual and demand using Vision-Language Models (VLMs) instead."}, {"title": "1.2 RELATED WORKS", "content": "LLMs have demonstrated promising performance across a diverse range of natural language tasks [20, 21, 22]. They can be an effective method for reward shaping by utilizing human feedback in the system's learning process, where this feedback helps define the task and guide the agent's behavior [23]. Reinforcement Learning from Human Feedback (RLHF) is a technique where human feedback is used to guide the training process of an RL agent, especially when designing a reward function is difficult. Bai et al. [17] explored the use of RLHF to calibrate LLMs and showed its ability to align AI behavior with human preferences for helpfulness and harmlessness while improving NLP performance through iterative feedback and preference modeling. However, collecting high-quality human feedback can be expensive and laborious.\nTherefore, the use of pre-trained models to determine the reward signal has shown as an effective strategy in research on LLMs. LLMs' human-like reasoning features are suggestive of their suitability in agent decision-making [24]. For example, Yu et al. [25] introduced a novel paradigm that utilized LLMs to generate reward functions from high-level language instructions, which as a result, enabled intuitive and flexible interaction with robots. Additionally, Cui et al. [3] employed a hybrid decision-making framework combining LLM-based and rule-based systems to balance adaptability and safety. They demonstrated the ability of their framework to manage edge cases, such as snowy intersections and animal crossings, with superior contextual awareness compared to traditional methods. The GPT-Driver [26] framework also introduced a novel perspective on motion planning for AVs by utilizing LLMs such as GPT-3.5 to address the limitations of traditional methods. Their proposed method reformulated motion planning as a language modeling task by converting heterogeneous inputs such as perception data and ego-states into language tokens. Such addition can allow the model to generate waypoint-based trajectories through natural language descriptions [27]. However, designing a suitable reward function is still complex for various types of tasks.\nMany RL tasks are visual and need to use Vision-Language Models (VLMs) instead. Rocamonde et al. [28] introduced a method using CLIP as a zero-shot reward model for reinforcement learning, which helps agents learn complex tasks such as humanoid movements from simple text prompts. This approach removes the demand for manual reward design. In Ref. [29] Language Reward Modulated Pretraining (LAMP) on RLBench was evaluated using a robotic manipulation benchmark setup, which showed significant improvements in sample efficiency and the ability to deliver tasks such as Pick Up Cup and Push Button. In addition, several key challenges in traditional RL methods were introduced in Ref. [29], such as dependency on handcrafted rewards and challenges in adapting to diverse downstream tasks. In Ref. [24], by assigning VLMs to compare agent observations based on task descriptions, their proposed method eliminated the need for human-labeled rewards or access to low-level ground-truth states.\nThe reviewed literature indicates that while LLMs have recently been suggestive of an enhanced decision-making process, the combination of VLMs and RL is still largely unexplored, especially considering the visual interactions of agents within their environment. In particular, our paper, compared to the existing literature, proposes a pre-trained CLIP on a custom dataset using transfer learning techniques to improve the model's performance in the specified intersection scenario. In addition, the present study aims to develop and evaluate a novel framework that combines RL with VLMs to improve AV decision-making in unsignalized intersections, with a focus on mimicking human-like behaviors. The challenge of managing AV behavior at intersections without traffic signals requires an understanding of contextual, human-like decision-making processes. In addition, this study aims to enable the AV to make safe and efficient decisions in response to instantaneous visual inputs by using CLIP [30] as a VLM to build a reward model. Therefore, the three main contributions in this paper can be summarized in the following:\n\u2022 A CLIP-based reward model is introduced for guiding AV behavior. By optimizing it, this research uses CLIP's contextual understanding to provide instantaneous guidance to the AV to align its actions with safe and human-inspired behaviors.\n\u2022 A set of data (visual scenario, description) pair is collected to modify the CLIP using our dataset. To optimize the training process, this study utilizes transfer learning by optimizing only the upper layer of CLIP while keeping the remaining layers unchanged. This approach reduces computational demands, which provides efficient training with a limited dataset and enhances the practicality of the framework.\n\u2022 This research comprises CLIP's recommendations as a secondary reward signal, combined with the environment's basic reward function, within the RL framework. This dual reward structure guarantees that the AV's actions both maximize traditional rewards and align with CLIP's human-like guidance to improve decision-making quality."}, {"title": "2 PROBLEM FORMULATION", "content": "The problem of training RL agent in an unsignalized intersection is formulated as a POMDP by the tuple $M := (S, A, \\Omega, T, O, R, \\gamma)$, which is a form of fully observable Markov Decision Process (MDP) that the agent perceives an observation $o \\in \\Omega$, while it lacks complete access to the actual state of the environment. $S$ is a finite set of states, $A$ is a finite set of actions, $\\Omega$ is a finite set of observations, $T$ is a transition probability defined as $T : S \\times A \\times S \\rightarrow [0, 1]$, $O$ is an observation function defined as $O : S \\times A \\times \\Omega \\rightarrow [0, 1]$, $R$ is a reward function defined as $R : S \\times A \\times S \\rightarrow R$, and $\\gamma$ is the discount factor. A sequence of states and actions is called a trajectory $\\tau = (s_0, a_0, s_1, a_1, ...)$, where $s_i \\in S$, $a_i \\in A$, and the length of the trajectory $|\\tau|$ is considered finite. Thus, the primary objective is to identify a policy $\\pi(s, a)$ that maximizes the discounted sum of future rewards over an infinite time horizon $\\pi^* : S \\rightarrow A$.\n$\\pi^* := \\arg \\max_\\pi E_{\\tau \\sim \\pi} \\sum_{t=0}^{T} \\gamma_t(s_t, \\pi(s_t))$\nwhere $E$ denotes the expectation operator, and $\\gamma \\in [0, 1)$ is the discount factor. To find the optimal policy $\\pi^*$, Deep Q-Network (DQN) [33], and Proximal Policy Optimization (PPO) [34] are utilized in this paper as a value-based, and policy-based deep reinforcement learning (DRL) method, respectively."}, {"title": "2.1 DEEP Q-NETWORK", "content": "In RL, the state-action value function $Q^{\\pi}(s, a)$ under a policy $\\pi$ represents the expected cumulative reward when starting from state $s$, taking action $a$, and following policy $\\pi$ thereafter, formulated as follows:\n$Q^{\\pi}(s, a) = E_{\\pi} [ \\sum_{t=0}^{\\infty} \\gamma_t r_t(s_t, \\pi(s_t)) | s_0 = s, a_0 = a ]$\nThe optimal policy maximizes the state-action value function, i.e., $\\pi^*(s) = \\arg \\max_a Q^*(s, a)$, and the optimal state-action value function $Q^*(s, a)$ that satisfies the Bellman equation, can then be derived as:\n$Q^*(s, a) = E [r + \\gamma \\max_{a'} Q^*(s', a') | s_0 = s, a_0 = a]$\nTo approximate $Q^*(s,a)$, DQN uses a neural network $Q(s, a; \\theta)$ parameterized by $\\theta$. Then it minimizes the mean squared Bellman error between the current Q-values and the target Q-values:\n$L(\\theta) = E [(y - Q(s, a; \\theta))^2]$\nwhere the target $y$ is defined as:\n$y = r + \\gamma \\max_{a'} Q_{target}(s', a'; \\theta^-)$\nand the target network $Q_{target}(s', a'; \\theta^-)$, is introduced to stabilize training by providing fixed Q-values as a reference during updates, where $\\theta^-$ represents the target network parameters, which the weights from the main network $\\theta$ are copied to the target network periodically. Also, the network parameters $\\theta$ are updated using stochastic gradient descent (SGD):\n$\\theta_{i+1} \\leftarrow \\theta_i - \\alpha \\nabla_{\\theta} L(\\theta)$"}, {"title": "2.2 PROXIMAL POLICY OPTIMIZATION", "content": "Unlike DQN, PPO is a policy-based method that directly optimizes the policy by maximizing the expected cumulative rewards. The policy $\\pi_\\theta(a|s)$ in PPO is parameterized by a neural network with parameters $\\theta$. This network outputs a probability distribution over actions for each state $s$. For discrete action spaces, the output is a softmax distribution:\n$\\pi_{\\theta}(a|s) = softmax(Z_a)$\nwhere $Z_a$ is the logit corresponding to action $a$. PPO optimizes a surrogate objective function to improve training stability while preventing large policy updates. This approach introduces a clipping mechanism that ensures the policy does not change too drastically during each training step. The objective function is shown as follows:\n$L(\\theta) = E_t [min (r_t(\\theta) A_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)A_t)]$\nwhere $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio, comparing the new policy $\\pi_{\\theta}$ to the old policy $\\pi_{\\theta_{old}}$, $\\epsilon$ is a small clipping parameter that constrains the probability ratio $r_t(\\theta)$ within the range $[1 - \\epsilon, 1 + \\epsilon]$, and $A_t$ is an estimator of the advantage function at time $t$. The advantage function is defined as follows:\n$A_t = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + \\cdot \\cdot \\cdot + (\\gamma \\lambda)^{T-t+1} \\delta_{T-1}$\nwhere $\\delta_t = r_t + V(s_{t+1}) - V(s_t)$, $r_t$ is the reward at time $t$, $V(s)$ is the value function at state $s$, $\\gamma$ is a discount factor and $\\lambda$ helps to balance the bias-variance trade-off in advantage estimation. The objective function encourages the policy to improve (when $A_t > 0$) while preventing large updates that might degrade performance. This balance makes PPO simpler and more effective compared to methods, e.g., Trust Region Policy Optimization (TRPO) [35]. In addition to optimizing the policy, PPO trains a value function $V(s)$ to approximate the expected return from state $s$. The value function is updated by minimizing the following loss:\n$\\phi_{k+1} = \\arg \\min_\\phi \\frac{1}{2} \\sum_{T \\in D_k} \\sum_{t=0}^T (V_{\\phi}(s_t) - R_t)^2$\nwhere $\\phi$ represents the parameters of the value function $V_\\phi$, which is a neural network, $D_k$ is a dataset of trajectories collected during iteration $k$, and $R_t$ is reward-to-go at time $t$ computed as:\n$R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdot \\cdot \\cdot + \\gamma^{T-t} r_T$\nwhich is the actual observed cumulative reward from state $s_t$ onward."}, {"title": "2.3 ACTION AND OBSERVATION SPACES", "content": "Highway-env offers different types of observation spaces based on the complexity and information needs of the environment setup. In this study, the grayscale observation is used to represent the environment as a grayscale image (similar to a top-down view), where the positions of vehicles and road boundaries are visualized as pixel intensities. In a driving scenario, it's challenging to determine the speed or direction of other vehicles with just a single snapshot. As a result, the four most recent frames are stacked together to capture temporal information, providing the model with a sense of motion and changes in the environment. This stacked representation was fed into a Convolutional Neural Network (CNN) architecture within the DQN framework as depicted in Fig. 2. The agent must drive a vehicle by controlling its acceleration chosen from the set of abstract meta-actions in"}, {"title": "2.4 RL WITH CLIP FEEDBACK", "content": "This section presents how VLMs, particularly CLIP, is used in this paper to improve the AV's decision-making process in various intersection scenarios. CLIP provides the model with associated visual inputs and natural language instructions, which can be used to guide the agent's behavior based on contextual understanding. The approach leverages transfer learning, which is particularly useful given the relatively small dataset available for refinement and modifications. Transfer learning facilitates the use of features learned by CLIP, which have been pre-trained on large, diverse datasets. Transferring these learned features to our dataset enables the model to improve performance, even with limited training data. This process avoids updating the entire model; most of the model's weights are unchanged, and only specific layers are refined, which causes faster training times and reduces GPU usage. To do that, a dataset including 500 images from different frames of the intersection scenarios was collected. These images represented the diverse situations the AV could encounter during training. Alongside the images, 500 corresponding instructions were defined as single-sentence text prompts, serving as labels for each image. These instructions are based on the vehicle's current visual input. After adjusting CLIP, the trained model was integrated into the AV's decision-making system. Hence, CLIP was used as a reward model to train AV in an unsignalized intersection scenario. In each training step, the current environment state of the AV, which is represented by its visual input, is passed through the vision encoder of the CLIP model. Simultaneously, the three possible actions in the environment A passes again through the text encoder. These actions are encoded as text embeddings corresponding to each potential behavior the AV might adopt. By utilizing the enhanced CLIP model, the agent receives rewards from CLIP's reward model whenever its action aligns with the instruction output by CLIP for the current scenario. This allows the agent to align its behavior with the predefined instructions and helps it make decisions that are consistent with the desired driving behaviors at the unsignalized intersection. To calculate the reward, the CLIP model calculates the cosine similarity between the visual representation of the current state and the text embedding of the corresponding natural language prompt, which describes the expected human behavior in a similar scenario as follows:\n$R_{CLIP} = \\frac{E_{image} \\cdot E_{text}}{||E_{image}|| ||E_{text}||}$\nwhere $E_{image}$ is the embedding vector from the image encoder, and $E_{text}$ is the embedding vector generated by the text encoder. To ensure that the basic reward structure from the simulation environment does not dominate the training process, CLIP's reward is multiplied by a weight determined during calibration and refinement. This scaling factor allows us to balance the influence of CLIP's reward with the basic reward from the simulation. The basic reward structure provided by the simulation environment includes three primary components as follows:\n$R_{Basic} = \\begin{cases} r_{speed} & \\text{if the agent maintains an efficient speed,} \\\\ r_{collision} & \\text{if the ego-vehicle collides with another vehicle,} \\\\ r_{destination} & \\text{if the agent successfully reaches its destination,} \\\\ 0 & \\text{otherwise.} \\end{cases}$\nwhere $r_{speed} \\in R^+$ represents the reward for maintaining an efficient speed while driving, $r_{collision} \\in R_-$ denotes the penalty for a collision with another vehicle, and $r_{destination} \\in R^+$ is the reward for successfully reaching the destination. In this study, the values were chosen as $r_{speed} = 1$, $r_{collision} = -5$, and $r_{destination} = 2$. Finally, the weighted reward from CLIP is then combined with the reward from the basic reward structure provided by the simulation environment:\n$R_{Final} = R_{Basic} + W_c \\times R_{CLIP}$\nwhere $W_c = 1.2$ is the corresponding weight, and $R_{CLIP}$ is the reward by CLIP reward model."}, {"title": "3 EXPERIMENTS", "content": "In this section, the performance of the proposed framework is evaluated using DQN and PPO as RL algorithms and CLIP as a VLM to build a reward model. An overview of the experimental setup is also provided, including the environment configurations, hyperparameters, and training protocols used for DQN and PPO. Subsequently, the results are presented based on several evaluation metrics to compare the performance of the two RL algorithms with and without the CLIP-based reward function. In addition, the influence of CLIP on agent behavior at unsignalized intersections will be evaluated based on key factors such as collision rate, success rate, and alignment with human-like decision-making behaviors through a series of simulation experiments."}, {"title": "3.1 TRAINING EVALUATION", "content": "Training sessions on environmental configuration were conducted. The simulations were conducted for 8,000 steps to train the proposed models using DQN and PPO algorithms separately. The aim was to observe and analyze the behavior of the AVs in an unsignalized four-way intersection. Several key factors were measured, including the success rate, collision rate, and timeout rate (instances when an the AV fails to reach its destination). The primary objective of these simulations was to assess the overall generalization ability of the trained policies with CLIP used as a reward model."}, {"title": "3.2 POST-TRAINING EVALUATION", "content": "The post-training evaluation involves deploying the trained agent utilizing CLIP-based DQN, CLIP-based PPO, vanilla DQN, and PPO in an intersection setting to assess its navigation abilities across 100 testing episodes. The trained agent employs its learned policy to make real-time decisions at every time step. The agent starts in an initial state and chooses the best action based on its policy. Following this, it receives visual observation and reward and transitions to the next state. This iterative process goes on until the agent arrives at a terminal state during the episode.\nThe evaluation continues until the episode terminates, either due to a collision, successful arrival at the destination, or truncation. If a collision occurs during the episode, the collision counter c is incremented by one, and the episode is terminated. In contrast, if the agent successfully reaches its destination, the success counter s is incremented by one. During the evaluation, the agent's speed at each time step is appended to a list v, which provides the possibility for calculating the average speed at the end of the evaluation process. At the end of all episodes, the algorithm outputs the normalized metrics, which also facilitates a generalized assessment of the trained agent's behavior across multiple scenarios."}, {"title": "4 CONCLUSION", "content": "In this paper, a novel method was proposed to integrate VLMs, specifically CLIP, with RL algorithms such as PPO and DQN to improve the decision-making capabilities of AVs navigating through unsignalized four-way intersections, which is a complex environment to find the optimal policy with traditional approaches. The obtained results in this paper demonstrated by introducing a reward mechanism based on VLMs, the AV is facilitated to align its actions more closely with human-like decision-making patterns to address the limitations of rule-based systems. In addition, the conducted simulation experiments demonstrated that the agent achieved higher rewards when guided by this enhanced reward function, compared to using only the conventional reward function. This paper emphasizes the prospects of combining VLMs with RL techniques to develop more adaptive and context-aware AV systems. Despite the promising outcomes, future research needs to improve its evaluations of more generalized urban driving conditions with a more extended dataset. Furthermore, a human-in-the-loop framework could be developed to enhance the realism of the evaluation process. By including virtual reality environments and immediate human interaction, subjective human factor analysis could be performed to better understand how humans perceive and respond to AV behavior. This approach would allow for the refinement and modification of algorithms based on direct feedback for other practical applications."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Trajectories", "content": "Figures 10, 11, and 12 depict the trajectories of the AV and HVs in three intersection scenarios using an agent trained with the CLIP-based DQN. In each scenario, the AV starts from the same initial position (south), while HVs approach the intersection from different directions. These representations demonstrate the AV's decision-making process and emphasize its altruistic behavior. Also, the bottom component in each representation corresponds to the temporal progression of the trajectories and the intensity of the gradient shows the temporal progress of the vehicles. In Fig. 10, the AV begins its trajectory from the south side of the intersection, and an HV approaches from the east. The AV slows down to allow the HV to pass safely through the intersection. Once the intersection is clear, the AV accelerates and executes a left turn. In Fig. 11, and 12, the HV starts from north, and west, respectively. Similar to Scenario 1, the AV yields to the HV to maintain a cautious approach, and then it speeds up and completes the left turn."}, {"title": "A.2 CLIP Implementation Details", "content": "Different image encoder architectures are available for CLIP. In this paper, the experiments are conducted with the ViT. Specifically, the ViT-B/32 model on our dataset for 15 epochs was modified using an Adam optimizer [38]. For the purpose of transfer learning, the gradients were updated only for the last layer of the model and left the earlier layers frozen. The hyperparameters used in this setup are listed in Table 3."}, {"title": "A.3 Simulation Environment", "content": "The environment is intersection-v1 from highway-env with discrete actions (i.e., speed up or slow down) and continuous observations (i.e., grayscale frames). To train the agents, the Stable-Baselines3 were implemented with DQN and PPO for a total of 8,000 timesteps. Table 4 provides an overview of the key configuration parameters for the environment, detailing aspects such as observation shape, action type, and simulation frequency. The evaluation process for the trained agents is outlined in Algorithm 2, which describes the method used to assess the performance of the agents based on metrics such as collisions, successful arrivals, and average speed."}, {"title": "A.4 Evaluation of Adjusted CLIP Predictions", "content": "Figure 13 illustrates the performance of the CLIP-based model in predicting driving instructions for some random intersection scenarios. For each scenario, the green bar represents the probability of the correct label, while the red bars indicate the probabilities of the incorrect labels. The results demonstrate the model's ability to align the intersection scene with the appropriate driving action."}]}