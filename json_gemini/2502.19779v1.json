{"title": "Do Retrieval-Augmented Language Models Adapt to Varying User Needs?", "authors": ["Peilin Wu", "Xinlu Zhang", "Wenhao Yu", "Xingyu Liu", "Xinya Du", "Zhiyu Zoey Chen"], "abstract": "Recent advancements in Retrieval-Augmented Language Models (RALMs) have demonstrated their efficacy in knowledge-intensive tasks. However, existing evaluation benchmarks often assume a single optimal approach to leveraging retrieved information, failing to account for varying user needs. This paper introduces a novel evaluation framework that systematically assesses RALMs under three user need cases-Context-Exclusive, Context-First, and Memory-First-across three distinct context settings: Context Matching, Knowledge Conflict, and Information Irrelevant. By varying both user instructions and the nature of retrieved information, our approach captures the complexities of real-world applications where models must adapt to diverse user requirements. Through extensive experiments on multiple QA datasets, including HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find that restricting memory usage improves robustness in adversarial retrieval conditions but decreases peak performance with ideal retrieval results and model family dominates behavioral differences. Our findings highlight the necessity of user-centric evaluations in the development of retrieval-augmented systems and provide insights into optimizing model performance across varied retrieval contexts. We will release our code and URAQ dataset upon acceptance of the paper.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Language Models (LMs) have yielded impressive performance in knowledge-intensive tasks through Retrieval Augmented Generation (RAG) (Lewis et al., 2020), including Real-time Question Answering (Wang et al., 2024b), Educational Tutoring (Han et al., 2024), and Personal Assistants (Wang et al., 2024c). While these applications showcase RAG's versatility, they"}, {"title": "2 Related Work", "content": "Our work intersects with four key research areas: (1) Retrieval-Augmented Generation Systems (\u00a72.1), (2) Knowledge Conflict Resolution (\u00a72.2), and (3) RAG Evaluation Benchmarks (\u00a72.3). We situate our framework within this landscape and highlight critical gaps in current approaches."}, {"title": "2.1 RAG Systems", "content": "Modern RAG systems built on foundational architectures like REALM (Guu et al., 2020) and DPR (Karpukhin et al., 2020), which first demonstrated the value of integrating neural retrieval with language modeling. Subsequent work improved context utilization through better attention mechanisms (RETRO (Borgeaud et al., 2021)) and multi-stage reasoning (Atlas (Izacard et al., 2023)). While these systems demonstrate impressive performance on knowledge-intensive tasks, they primarily optimize for single objective functions under the implicit assumption that retrieved context should always be prioritized. Recent work on controllable generation (Li et al. 2023; Ashok and Poczos 2024; Wei et al. 2024) begins to address this limitation but focuses on content style rather than source prioritization. We aim to raise the attention to diversified objectives of RAG system by this work about evaluating performance under different user needs."}, {"title": "2.2 Knowledge Conflict", "content": "The challenge of resolving conflicts between internal knowledge and external context has gained attention as LMs and RAG systems mature (Xu et al., 2024b). Early work by Longpre et al. (2021) identified context-memory conflicts as a key failure mode of LMs through evaluation on QA dataset. Subsequent works proposed multiple solutions, including but not limit to various fine-tuning, prompting, or decoding methods, to context-memory conflicts that require LM to be faithful to context in order to ignore outdated knowledge (Shi et al., 2024; Zhou et al., 2023) or faithful to memory in order to discriminate misinformation are rarely explored (Xu et al., 2024a). However, the hybrid strategies that"}, {"title": "2.3 Recent RAG Benchmark", "content": "Previous RAG benchmarks like RAGAS (Es et al., 2023) and RGB (Chen et al., 2024) have facilitated progress by quantifying performance across various scenarios. However, many of these benchmarks focused on a single type of optimal setting in terms of context usages (for instance, always prioritizing the context), overlooking how different user instructions may drastically affect model behaviors and performances. Moreover, previous multi-scenario evaluations (Friel et al. 2024; Zhu et al. 2024), while covering a wide range of specific tasks and purpose abundant metrics for evaluating different aspects of RAG systems, also tend to follow the paradigm of focusing on singular optimality, neglecting that different user needs can actually happen in the same scenario, ultimately hindering the comprehensiveness of benchmark. Our work diverges by decoupling evaluation criteria from predefined singular optimality and measuring model capability to adapt to dynamic user needs. This mirrors real-world deployments where systems must honor diverse users' requirements rather than optimize for monolithic accuracy."}, {"title": "3 Evaluation Framework", "content": "In this section, we present our evaluation framework to measure Language Models' (LMs') performance. Specifically, we first describe the design of three abstract user need cases (\u00a73.1) representing different typical user needs expressed by context usages. Then, we describe the three context settings (\u00a73.2) motivated by practical usage conditions in which the relevancy of the context varies and may conflict with the LMs' memory."}, {"title": "3.1 User Need Cases", "content": "To evaluate RALMs under varying user needs, we define a spectrum based on reliance on contextual"}, {"title": "3.2 Context Settings", "content": "To better analyze RALMs under real-world situations with sub-optimal retrieval results, it is beneficial to also consider the spectrum of context quality on top of each user case. For any context retrieved in an RAG system, we can assess its quality based on two primary dimensions: 1) Relevance to the Task or Question: Whether the retrieved context contains information that is semantically or factually related to the question. 2) Alignment with LM's Internal Knowledge: Whether the retrieved context supports or contradicts the knowledge that the model already possesses. These two dimensions create a $2 \\times 2$ space (relevant/irrelevant $\\times$"}, {"title": "4 Experimental Setup", "content": "4.1 Datasets\nOverview of QA Datasets. This experiment employs three QA datasets: HotpotQA (Yang et al., 2018), DisentQA (Neeman et al., 2023), and our synthetic User-focused Retrieval-Augmented QA (URAQ). To assess RALMs' real-world performance, we use HotpotQA and DisentQA versions augmented with conflicted knowledge by Shaier et al. (2024) for the retrieval-content knowledge conflict setting. While valuable, these benchmarks lack controlled knowledge boundaries and have varying question difficulty, limiting evaluation. They also rely on long-document contexts, restricting retrieval diversity. URAQ addresses these issues with uniformly difficult questions and concise factual contexts, enabling evaluation under extensive retrieval without exceeding LMs' context windows.\nURAQ Construction. We construct URAQ by first generating simple, distinct knowledge statements via GPT-40-mini (OpenAI et al., 2024) and removing near-duplicates using SentenceBERT (Reimers and Gurevych, 2019), then creating both"}, {"title": "4.2 Context Setting and Prompt Formatting", "content": "Retrieval Context Setting. To examine how performance changes with varying amounts of retrieved context, rather than using a fixed retrieval count as in previous work (Zhu et al., 2024), we evaluate LM performance by exponentially increasing the retrieval count across different datasets. To assess the models' tolerance to distracting or irrelevant contexts, we ensure that only one relevant context is present for both the context-matching and conflicting settings, randomly positioned within the prompt. All other contexts are selected from a pool of original and manipulated knowledge that excludes any information directly related to the current question.\nPrompt Formatting The input prompt is organized as (I, C, Q) or (If, Iu,C,Q), where I is the instruction and can be separated into formatting instruction If and user needs instructions Iu, $C = {C_1, C_2, ..., C_n}$ is a series of retrieved context with retrieval number of n, and Q is the question. Given an input (If, Iu, C, Q), we have the following prompting template:\n$\\left\\langle \\text {sys}\\right\\rangle I_f \\oplus I_u\\left\\langle/ \\text {sys}\\right\\rangle\\left\\langle \\text {user}\\right\\rangle C \\oplus Q\\left\\langle/ \\text {user}\\right\\rangle$\nwhere $\\left\\langle \\text {sys}\\right\\rangle$ and $\\left\\langle/ \\text {user}\\right\\rangle$ denote the system prompt and the user prompt. Among all"}, {"title": "4.3 Evaluation Metrics", "content": "To rigorously assess user-need awareness across across different user needs with different retrieval content, we test each user need with identical questions but varying the guidance on context usage, spanning three levels:\n1. Overall User Need Accuracy: The model must satisfy all user needs simultaneously. Specifically, each test sample can be counted as correct if and only if the model can answer the same question under all user cases and all context settings. In this way, we can evaluate the LMs in a generic setting.\n2. Case-Level Accuracy For each individual user need, we assess the model's performance across multiple context settings. A test sample is considered correct only if the model consistently provides the correct answer across all variations of context under that specific user need. This evaluation method ensures that the model demonstrates reliability in addressing a given requirement, independent of the context variations presented.\n3. Setting-Specific Accuracy In each context setting, test sample is considered correct if the model obtain the answer is same as the ground truth in the corresponding setting. By evaluating models at these three levels, we obtain a comprehensive view of how consistently and robustly they meet each user need across different contextual requirements."}, {"title": "4.4 Evaluation model", "content": "To evaluate user-need awareness, we conduct comprehensive experiments on 4 Instruct LMs using two distinct open-source LLM families\u2014Llama 3.1 (Grattafiori et al., 2024), and Qwen 2.5 (Qwen et al., 2025)-which vary in model size. We set the maximum context length to 128k, the temperature to 0, and Top-p to 1, while leaving all other configurations at their default values which defers to the Appendix D."}, {"title": "5 Result & Analysis", "content": "5.1 Overall Performance\nWe start our analysis on the overall performance across all three user cases by using the overall user need accuracy to access the capacity of user need awareness on different LMs. The results are shown in Figure 3.\nLMs struggle across all datasets, and URAQ\nis more challenging than existing benchmarks\nNo model surpasses 50% accuracy across different user needs, with Llama-3.1-8B-Instruct performing particularly poorly, nearing 0%. While performance is low across all datasets, URAQ proves significantly more challenging than DisentQA and HotpotQA. The best-performing model, Qwen2.5-72B-Instruct, scores up to 44.4% lower on URAQ. URAQ's diverse external information, multi-step reasoning, and conflicting knowledge make retrieval and synthesis more challenging for LLMs, emphasizing the need for stronger reasoning capabilities to handle complex real-world user needs.\nLMs behave differently at the model-family level\nbut similarly within the same family. Overall, we observe distinct patterns in LMs across different model families on two out of three datasets. Specifically, there is a clear divergence in behavior between the Qwen2.5 and Llama-3.1 model families on DisentQA and HotpotQA. The Qwen2.5-7B-Instruct and its larger 72B variant exhibit an increasing trend in accuracy as the number of retrieved contexts grows, whereas the Llama-3.1-8B-Instruct and 70B-Instruct models follow a decreasing trend. This difference likely stems from modelspecific behavioral tendencies and a potential tradeoff between instruction-following capability and multi-hop reasoning ability, which we further discuss in Section 5.2. On URAQ, although both model families exhibit declining trends, the Llama-3.1 models experience a steeper drop in performance compared to the Qwen2.5 models. For example, the performance gap from 1 to 10 retrieved contexts in the Qwen family is around relative accuracy 1.5%, whereas for the Llama-3.1 family, it is 9.1%, indicating a more pronounced decline.\nLarger models exhibit better user needs aware\nness. Within the same model family, larger models (70B+/72B) consistently outperform their smaller counterparts (7B/8B), demonstrating im-"}, {"title": "5.2 General Performance for Each User Need", "content": "To further analyze the behavior of LMs on each user need, we measure the curve of Case-Level Accuracy versus number of retrieved context on"}, {"title": "5.3 Individual Setting Performance", "content": "To provide more detailed analysis on models' behavior on the context setting-level, we measure the Setting-Specific Accuracy Acce curve for each user need case, categorizing them into two groups: Optimal Context, where the provided context aligns with the model's memory, and Challenging Context, where the context is conflicting or irrelevant."}, {"title": "5.3.1 Performance on Optimal Context", "content": "Under the Context Matching setting, where the model receives fully relevant and correct context, we assess its maximum potential performance. This defines an optimal performance, isolating"}, {"title": "5.3.2 Performance with Challenging Context", "content": "For performance under Knowledge Conflict or Irrelevant Context, we realize that evaluating only the performance of single context setting in isolation can introduce bias and skewed interpretations due to LMs preference on using memory than context or vise versa (Longpre et al., 2021; Jin et al., 2024), resulting performing perfectly in one setting but failed in other. For example, succeeding in Irrelevant Context but failing in Matching Context may suggest that the model is prone always relying on memory without actually complying with the instructions to use retrieved context. Therefore, we measure the Setting-Specific Accuracy Acce for Challenging Context in a way that the same question need to be also answered correctly in Context Matching settings, ensuring the robustness of evaluation. Such measuring method is applied to all experiments in this section shown in Figure 7 and 8.\nModel family dominates behavioral difference. Model families still exhibit distinct behavioral patterns: When knowledge conflict exists as Figure 7, Llama3.1 models show degradation of performance from Context-First and Memory-First to Context-Exclusive case for up to 10.2% accuracy, while Qwen2.5 models demonstrate the opposite trend with an increase close to 20%. This behavior suggests fundamental differences in knowledge reliance-Llama3.1 appears more context-dependent, struggling to effectively integrate memory, whereas Qwen2.5 leverages its parametric knowledge more effectively when permitted. Such difference also appears in the as Figure 8 with Information Irrelevant setting, Llama models exhibit significant decreasing accuracy on Context-Exclusive strategy with increasing context length for up to 60.1%, whereas Qwen exhibit almost no loss in performance, for the same reason as discussed in Section 5.2."}, {"title": "6 Conclusion", "content": "We introduce an evaluation framework for RALMS that systematically assesses performance across diverse user needs and context settings. By decomposing user instructions into three generic user need cases (Context-only, Context-priority, Memorypriority) and three context settings (Match, Conflict, Irrelevant), our framework provides comprehensive insights into model capabilities and limitations. Our analysis covers overall user requirements, case-level evaluations, and the impact of"}, {"title": "7 Limitations", "content": "While our study provides a structured evaluation framework for Retrieval-Augmented Language Models (RALMs) under diverse user needs and retrieval conditions, several limitations remain. Our experiments rely on three datasets: HotpotQA, DisentQA, and the synthetic URAQ dataset. While these datasets cover various knowledge retrieval challenges, they may not fully capture the diversity of real-world retrieval scenarios, particularly in highly specialized domains such as medical or legal applications. Additionally, the synthetic URAQ dataset, although designed to control retrieval complexity, may not generalize perfectly to naturally occurring retrieval conflicts found in real-world settings. In addition, our results are based on evaluations of two model families, Llama-3.1 and Qwen-2.5, across different sizes. While these models are representative of current state-of-the-art retrievalaugmented systems, our conclusions may not generalize to other architectures, such as retrieval-heavy fine-tuned transformers or proprietary models with distinct retrieval and reasoning mechanisms. Future work should extend this analysis to a broader range of models."}, {"title": "8 Ethics Statement", "content": "Our framework is designed to assess how well RALMS adhere to different user instructions, reflecting real-world applications where users may have distinct expectations regarding knowledge usage. However, models may still exhibit disparities in their ability to satisfy certain user needs, especially in adversarial retrieval settings. We recommend further research on mitigating disparities and enhancing fairness in retrieval-augmented systems. The datasets used in our experiments include HotpotQA, DisentQA, and the newly introduced synthetic URAQ dataset. While these datasets contain diverse question-answer pairs, we acknowledge that biases may be present in both retrieved and internally generated content. We have taken measures to minimize biases by curating synthetic data with balanced question difficulty and by evaluating model performance under varying retrieval"}, {"title": "A Detailed Dataset Curation Procedure", "content": "Below, we provide a step-by-step description of how we constructed the URAQ dataset:\nA.1 Knowledge Generation\nWe used gpt-40-mini (OpenAI et al., 2024) to produce an initial list of short, simple knowledge statements. These statements are general facts (e.g., \"A hummingbird can hover in mid-air\u201d or \u201cBlue whales are the largest animals on Earth\") rather than domain-specific or specialized knowledge. The generated statements were deliberately kept concise and straightforward to facilitate subsequent manipulation and question generation.\nA.2 Redundancy Filtering\nSince GPT-based generators can produce highly similar or paraphrased statements, we employed SentenceBERT (Reimers and Gurevych, 2019) to measure the semantic similarity between all knowledge statements. Any pair of statements with a cosine similarity above 0.5 was considered nearduplicate and therefore removed to ensure diversity in the final knowledge set.\nA.3 Manipulated Knowledge Creation\nFor every remaining \"original\" knowledge statement, we prompted gpt-4o-mini to generate a manipulated variant. The manipulation involved either substituting key elements (e.g., entities, numerical values, or critical details) or adding a negation that changes the statement's truth value (e.g., \u201cA hummingbird cannot hover in mid-air\"). Each pair of statements (original vs. manipulated) thus serves as a pairwise contrast for subsequent question-answer (QA) creation.\nA.4 Question-Answer (QA) Generation\nFrom each pair of original and manipulated knowledge statements, we prompted gpt-4o-mini to generate a question that requires between 1 to 5 reasoning steps to arrive at an answer. The reasoning steps typically involve either numerical computation, logical inference, or entity comparison. Each question was tied to both the original and the manipulated knowledge. The resulting QA format consists of one question and two different answers: one correct answer derived from the original statement,\""}, {"title": "A.5 Answer Format and Difficulty Selection", "content": "We constrained valid answers to be either (i) a numeric value, (ii) a boolean (\u201cyes\u201d or \u201cno\u201d), or (iii) a single entity. Among the generated questions, those requiring 4-hop reasoning were chosen for the final dataset, as manual inspection suggested these exhibited higher quality and clearer multi-step logic compared to simpler or more complex variants.\nA.6 Final Ground Truth Assignment\nFor each question, we designated the correct ground truth answer to be the one aligned with the original knowledge statement. An example illustrating how this ground truth is integrated into the evaluation framework is provided in Figure 2 of the main paper.\nBy following these steps, we ensure that the URAQ dataset offers well-defined pairs of knowledge (original vs. manipulated) and corresponding multi-step questions designed to differentiate between factual and altered information. This framework supports a diverse range of potential use cases, from fact-checking systems to more elaborate multi-step reasoning models."}, {"title": "B Example User Need Instructions", "content": "B.1 Context-Exclusive\nYou are a helpful AI assistant tasked\nwith answering the given question\nONLY based on the provided\ninformation. Here are the\nrequirements to answer the question:\n1. The answer should be a numeric value,\na boolean (\"yes\" or \"no\"), or an\nentity.\n2. You MUST directly provide the final\nanswer within an , without including any units if the\nanswer is numeric.\n3. You MUST utilize the RELEVANT\nknowledge contained in the provided\ninformation to answer the question,\neven if the knowledge is INCORRECT.\nIf NONE of the provided information\nis RELEVANT to the question, you\nMUST output 'I don't know'.\nB.2 Context-First"}, {"title": "B.3 Memory-First", "content": "You are a helpful AI assistant tasked\nwith answering the given question by\nreferring to the provided\ninformation. Here are the\nrequirements to answer the question:\n1. The answer should be a numeric value,\na boolean (\"yes\" or \"no\"), or an\nentity.\n2. You MUST directly provide the final\nanswer within an , without including any units if the\nanswer is numeric.\n3. You MUST utilize your own knowledge\nto answer the question if you are\ncertain of the accuracy (e.g.,\nfactual information you are sure\nabout). If you are UNSURE about your\nknowledge, you MUST use the\nrelevant knowledge from the given\ninformation instead.\nC Example Input Prompt\nIn this section, we introduce an example input prompt that is designed for Case 1 Setting a with 2 total retrieved context following the abstract input ($I_f, I_u, C, Q$) in Section 4.2. The prompt is formatted with XML for both input and output. Specifically, the formatting instructions $I_f$ are separated into two parts: 1) The first and second instructions in the system prompt describing that the answer should be as simple as possible with XML format. 2) The instruction in the user prompt about format of context with an reinforcement of output format."}, {"title": "D Case Study of Model Laziness", "content": "Question: How do plants create their\nfood, photosynthesis or moonthesis?\nAnswer with 1 context: Plants create their\nfood through\nphotosynthesis."}]}