{"title": "Do Retrieval-Augmented Language Models Adapt to Varying User Needs?", "authors": ["Peilin Wu", "Xinlu Zhang", "Wenhao Yu", "Xingyu Liu", "Xinya Du", "Zhiyu Zoey Chen"], "abstract": "Recent advancements in Retrieval-Augmented Language Models (RALMs) have demonstrated their efficacy in knowledge-intensive tasks. However, existing evaluation benchmarks often assume a single optimal approach to leveraging retrieved information, failing to account for varying user needs. This paper introduces a novel evaluation framework that systematically assesses RALMs under three user need cases-Context-Exclusive, Context-First, and Memory-First-across three distinct context settings: Context Matching, Knowledge Conflict, and Information Irrelevant. By varying both user instructions and the nature of retrieved information, our approach captures the complexities of real-world applications where models must adapt to diverse user requirements. Through extensive experiments on multiple QA datasets, including HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find that restricting memory usage improves robustness in adversarial retrieval conditions but decreases peak performance with ideal retrieval results and model family dominates behavioral differences. Our findings highlight the necessity of user-centric evaluations in the development of retrieval-augmented systems and provide insights into optimizing model performance across varied retrieval contexts. We will release our code and URAQ dataset upon acceptance of the paper.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Language Models (LMs) have yielded impressive performance in knowledge-intensive tasks through Retrieval Augmented Generation (RAG) (Lewis et al., 2020), including Real-time Question Answering (Wang et al., 2024b), Educational Tutoring (Han et al., 2024), and Personal Assistants (Wang et al., 2024c). While these applications showcase RAG's versatility, they also demand LMs that can adapt to diverse user needs-expressed via instructions on whether to prioritize external evidence or internal knowledge. For instance, Real-time QA may rely heavily on updated external facts, whereas tutoring may draw more on the model's conceptual understanding. Despite this potential, current RAG methods still struggle with identifying relevant references (Laban et al., 2024), resolving knowledge conflicts (Wang et al., 2024a), and reasoning effectively (Islam et al., 2024). These challenges underscore the need for robust evaluation strategies capturing how well Retrieval Augmented Language Models (RALMs) adapt to evolving user requirements.\nEven though existing RAG/RALM benchmarks (Yu et al., 2024; Es et al., 2023; Chen et al., 2024) including those that focus on multi-scenario evaluations (Friel et al., 2024; Zhu et al., 2024) have advanced retrieval-augmented evaluation, they typically assume a single \"optimal\" approach to external information (e.g., always relying on retrieved context). This narrow perspective overlooks how diverse user instructions can dramatically alter model behavior and performance within the same scenario. In medical fact-checking, for instance, one user might demand answers derived only from peer-reviewed studies, while another relies on the model's internal knowledge\u2014even if these sources conflict (Miao et al., 2024). Such constraints underscore an urgent question: how can we systematically evaluate LMs under varying context usage requirements to reflect different user needs?\nIn this paper, we present a simple yet effective evaluation framework that rigorously examines how Retrieval-Augmented Language Models (RALMS) respond to varying user instructions and context conditions. We consider three generic user cases (1) Context-Exclusive, (2) Context-First, and (3) Memory-First \u2014to capture different degrees of reliance on external information versus internal knowledge. Alongside these cases, we vary the context settings\u2014(a) Context Matching, (b) Knowledge Conflict, and (c) Information Irrelevant to represent scenarios where retrieved materials may align with, contradict, or fail to address the query. By intersecting user cases with distinct context conditions, we more closely mirror the complexities of real-world applications, where both the user's priorities and the reliability of retrieved information can shift dramatically. This approach reveals how each scenario might alter the correct response-especially when context and memory conflict\u2014an aspect often overlooked in previous work.\nWe conduct extensive experiments on our curated dataset, URAQ, along with two public datasets, DisentQA (Neeman et al., 2023) and HotpotQA (Yang et al., 2018), evaluating two model families, Llama3.1 Grattafiori et al. 2024 and Qwen2.5 Qwen et al. 2025, across various model sizes and numbers of retrieved contexts. Our findings reveal that: 1) Current LMs struggle to satisfy diverse user needs, achieving below 50% accuracy across all datasets, with Llama-3.1-8B-Instruct occasionally nearing 0%. 2) Contextual restriction alters performance: Restricting models to rely solely on retrieved context improves LMs performance when external context content is different from internal memory by up to 23% accuracy difference on the same model but decreases the performance under ideal retrieval by up to 17%. 3) Model family dominate behavioral differences: Model family contributes the majority of behavioral differences, which further emphasize the importance of choosing the correct model for different user needs through proper evaluations. For instance, under retrieval with knowledge conflict, Llama3.1 models exhibit a performance decline of up to 10.2% in accuracy when transitioning from Context-First and Memory-First to the Context-Exclusive case, whereas Qwen2.5 models show the opposite pattern, with an improvement of nearly 20%."}, {"title": "2 Related Work", "content": "Our work intersects with four key research areas: (1) Retrieval-Augmented Generation Systems (\u00a72.1), (2) Knowledge Conflict Resolution (\u00a72.2), and (3) RAG Evaluation Benchmarks (\u00a72.3). We situate our framework within this landscape and highlight critical gaps in current approaches."}, {"title": "2.1 RAG Systems", "content": "Modern RAG systems built on foundational architectures like REALM (Guu et al., 2020) and DPR (Karpukhin et al., 2020), which first demonstrated the value of integrating neural retrieval with language modeling. Subsequent work improved context utilization through better attention mechanisms (RETRO (Borgeaud et al., 2021)) and multi-stage reasoning (Atlas (Izacard et al., 2023)). While these systems demonstrate impressive performance on knowledge-intensive tasks, they primarily optimize for single objective functions under the implicit assumption that retrieved context should always be prioritized. Recent work on controllable generation (Li et al. 2023; Ashok and Poczos 2024; Wei et al. 2024) begins to address this limitation but focuses on content style rather than source prioritization. We aim to raise the attention to diversified objectives of RAG system by this work about evaluating performance under different user needs."}, {"title": "2.2 Knowledge Conflict", "content": "The challenge of resolving conflicts between internal knowledge and external context has gained attention as LMs and RAG systems mature (Xu et al., 2024b). Early work by Longpre et al. (2021) identified context-memory conflicts as a key failure mode of LMs through evaluation on QA dataset. Subsequent works proposed multiple solutions, including but not limit to various fine-tuning, prompting, or decoding methods, to context-memory conflicts that require LM to be faithful to context in order to ignore outdated knowledge (Shi et al., 2024; Zhou et al., 2023) or faithful to memory in order to discriminate misinformation are rarely explored (Xu et al., 2024a). However, the hybrid strategies that utilize both context and memory with prioritization, although commonly appeared in real-world applications, are rarely explored. In addition, there also exists applications that require LMs and RAG systems to work along or accept fictitious information or knowledge, which are commonly ignored by the previous works. Our framework includes the hybrid strategies that stem from the fundamental user needs, providing a wider coverage of evaluating RALMs performance under context-memory conflict situations."}, {"title": "2.3 Recent RAG Benchmark", "content": "Previous RAG benchmarks like RAGAS (Es et al., 2023) and RGB (Chen et al., 2024) have facilitated progress by quantifying performance across various scenarios. However, many of these benchmarks focused on a single type of optimal setting in terms of context usages (for instance, always prioritizing the context), overlooking how different user instructions may drastically affect model behaviors and performances. Moreover, previous multi-scenario evaluations (Friel et al. 2024; Zhu et al. 2024), while covering a wide range of specific tasks and purpose abundant metrics for evaluating different aspects of RAG systems, also tend to follow the paradigm of focusing on singular optimality, neglecting that different user needs can actually happen in the same scenario, ultimately hindering the comprehensiveness of benchmark. Our work diverges by decoupling evaluation criteria from predefined singular optimality and measuring model capability to adapt to dynamic user needs. This mirrors real-world deployments where systems must honor diverse users' requirements rather than optimize for monolithic accuracy."}, {"title": "3 Evaluation Framework", "content": "In this section, we present our evaluation framework to measure Language Models' (LMs') performance. Specifically, we first describe the design of three abstract user need cases (\u00a73.1) representing different typical user needs expressed by context usages. Then, we describe the three context settings (\u00a73.2) motivated by practical usage conditions in which the relevancy of the context varies and may conflict with the LMs' memory."}, {"title": "3.1 User Need Cases", "content": "To evaluate RALMs under varying user needs, we define a spectrum based on reliance on contextual information versus internal memory. This spectrum, illustrated in Figure 2, consists of three distinct user needs, determined by how LMs are instructed. Example prompts are in Appendix \u0412.\nContext-Exclusive: LMs must strictly base answers on retrieved context, responding \"I don't know\" if context is unhelpful. Prompts enforce unconditional adherence to external evidence, eliminating reliance on internal knowledge.\nContext-First: LMs prioritize retrieved context but fall back on memory when no relevant context exists. Prompts establish context as primary, with memory as a secondary source.\nMemory-First: LMs rely on internal memory unless uncertain, in which case they defer to retrieved context. Prompts invert the hierarchy, making memory the default unless confidence is low."}, {"title": "3.2 Context Settings", "content": "To better analyze RALMs under real-world situations with sub-optimal retrieval results, it is beneficial to also consider the spectrum of context quality on top of each user case. For any context retrieved in an RAG system, we can assess its quality based on two primary dimensions: 1) Relevance to the Task or Question: Whether the retrieved context contains information that is semantically or factually related to the question. 2) Alignment with LM's Internal Knowledge: Whether the retrieved context supports or contradicts the knowledge that the model already possesses. These two dimensions create a 2 x 2 space (relevant/irrelevant \u00d7 match/conflict), but due to the nature of irrelevant context (which neither supports nor contradicts), the space reduces to three distinct context settings.\nConext Matching. There is at least one retrieved context relevant to the question and matches with the LM's memory. This is an ideal situation for RALMS as correct knowledge is presented in both the external context and the internal memory.\nKnowledge Conflict. There is at least one retrieved context relevant to the question but conflicts with the LM's memory. This setting simulates context-memory knowledge conflicts (Xu et al., 2024b) and tests the model's ability on generation with strictly following instructions regarding context usages.\nInformation Irrelevant. All retrieved contexts are unrelated to the question. This setting simulates the Needle-In-a-Haystack (Laban et al., 2024) situation and tests the model's ability on knowledge selection. Models are expected to avoid hallucinating and admit knowledge gaps by responding with \"I don't know\" under Case 1 instructions or relying on its memory in Case 2 and 3."}, {"title": "4 Experimental Setup", "content": "In addition to existing benchmarks, we curate our own dataset URAQ that will address the evaluation issues from these existing benchmarks."}, {"title": "4.1 Datasets", "content": "Overview of QA Datasets. This experiment employs three QA datasets: HotpotQA (Yang et al., 2018), DisentQA (Neeman et al., 2023), and our synthetic User-focused Retrieval-Augmented QA (URAQ). To assess RALMs' real-world performance, we use HotpotQA and DisentQA versions augmented with conflicted knowledge by Shaier et al. (2024) for the retrieval-content knowledge conflict setting. While valuable, these benchmarks lack controlled knowledge boundaries and have varying question difficulty, limiting evaluation. They also rely on long-document contexts, restricting retrieval diversity. URAQ addresses these issues with uniformly difficult questions and concise factual contexts, enabling evaluation under extensive retrieval without exceeding LMs' context windows.\nURAQ Construction. We construct URAQ by first generating simple, distinct knowledge statements via GPT-40-mini (OpenAI et al., 2024) and removing near-duplicates using SentenceBERT (Reimers and Gurevych, 2019), then creating both original and \u201cmanipulated\u201d versions by substituting key information or adding negations. For each knowledge pair, we produce a question requiring 1-5 reasoning steps and two separate answers (one from the original knowledge, one from the manipulated), ultimately selecting the 4-hop subset for the final dataset. A detailed description of this procedure is provided in Appendix A, ensuring the pipeline's applicability across various domains."}, {"title": "4.2 Context Setting and Prompt Formatting", "content": "Retrieval Context Setting. To examine how performance changes with varying amounts of retrieved context, rather than using a fixed retrieval count as in previous work (Zhu et al., 2024), we evaluate LM performance by exponentially increasing the retrieval count across different datasets, shown in Table 1. To assess the models' tolerance to distracting or irrelevant contexts, we ensure that only one relevant context is present for both the context-matching and conflicting settings, randomly positioned within the prompt. All other contexts are selected from a pool of original and manipulated knowledge that excludes any information directly related to the current question.\nPrompt Formatting The input prompt is organized as $(I, C, Q)$ or $(I_f, I_u,C,Q)$, where $I$ is the instruction and can be separated into formatting instruction $I_f$ and user needs instructions $I_u$, $C = \\{C_1, C_2, ..., C_n\\}$ is a series of retrieved context with retrieval number of n, and Q is the question. Given an input $(I_f, I_u, C, Q)$, we have the following prompting template:\n$\\langle sys\\rangle I_f \\oplus I_u \\langle /sys\\rangle\\langle user\\rangle C \\oplus Q \\langle /user\\rangle$ (1)\nwhere $\\langle sys\\rangle\\langle /sys\\rangle$ and $\\langle user\\rangle\\langle /user\\rangle$ denote the system prompt and the user prompt. Among all data samples, the $I_u$ and C may change according to the user case and context setting, while the $I_f$ remaining the same by instructing models to directly output a simple answer that is either a numeric value, a boolean (\"yes\" or \"no\"), or an entity, as described in Section 4. A complete example prompt template is in the Appendix C."}, {"title": "4.3 Evaluation Metrics", "content": "To rigorously assess user-need awareness across across different user needs with different retrieval content, we test each user need with identical questions but varying the guidance on context usage, spanning three levels:\n1. Overall User Need Accuracy: The model must satisfy all user needs simultaneously. Specifically, each test sample can be counted as correct if and only if the model can answer the same question under all user cases and all context settings. In this way, we can evaluate the LMs in a generic setting.\n2. Case-Level Accuracy For each individual user need, we assess the model's performance across multiple context settings. A test sample is considered correct only if the model consistently provides the correct answer across all variations of context under that specific user need. This evaluation method ensures that the model demonstrates reliability in addressing a given requirement, independent of the context variations presented.\n3. Setting-Specific Accuracy In each context setting, test sample is considered correct if the model obtain the answer is same as the ground truth in the corresponding setting. By evaluating models at these three levels, we obtain a comprehensive view of how consistently and robustly they meet each user need across different contextual requirements."}, {"title": "4.4 Evaluation model", "content": "To evaluate user-need awareness, we conduct comprehensive experiments on 4 Instruct LMs using two distinct open-source LLM families\u2014Llama 3.1 (Grattafiori et al., 2024), and Qwen 2.5 (Qwen et al., 2025)-which vary in model size. We set the maximum context length to 128k, the temperature to 0, and Top-p to 1, while leaving all other configurations at their default values which defers to the Appendix D."}, {"title": "5 Result & Analysis", "content": "In this section, we'll conduct an analysis on the performance of the following factors on different LMs:\n5.1 Overall Performance\nWe start our analysis on the overall performance across all three user cases by using the overall user need accuracy to access the capacity of user need awareness on different LMs. The results are shown in Figure 3.\nLMs struggle across all datasets, and URAQ is more challenging than existing benchmarks No model surpasses 50% accuracy across different user needs, with Llama-3.1-8B-Instruct performing particularly poorly, nearing 0%. While performance is low across all datasets, URAQ proves significantly more challenging than DisentQA and HotpotQA. The best-performing model, Qwen2.5-72B-Instruct, scores up to 44.4% lower on URAQ. URAQ's diverse external information, multi-step reasoning, and conflicting knowledge make retrieval and synthesis more challenging for LLMs, emphasizing the need for stronger reasoning capabilities to handle complex real-world user needs.\nLMs behave differently at the model-family level but similarly within the same family. Overall, we observe distinct patterns in LMs across different model families on two out of three datasets. Specifically, there is a clear divergence in behavior between the Qwen2.5 and Llama-3.1 model families on DisentQA and HotpotQA. The Qwen2.5-7B-Instruct and its larger 72B variant exhibit an increasing trend in accuracy as the number of retrieved contexts grows, whereas the Llama-3.1-8B-Instruct and 70B-Instruct models follow a decreasing trend. This difference likely stems from modelspecific behavioral tendencies and a potential tradeoff between instruction-following capability and multi-hop reasoning ability, which we further discuss in Section 5.2. On URAQ, although both model families exhibit declining trends, the Llama-3.1 models experience a steeper drop in performance compared to the Qwen2.5 models. For example, the performance gap from 1 to 10 retrieved contexts in the Qwen family is around relative accuracy 1.5%, whereas for the Llama-3.1 family, it is 9.1%, indicating a more pronounced decline.\nLarger models exhibit better user needs awareness. Within the same model family, larger models (70B+/72B) consistently outperform their smaller counterparts (7B/8B), demonstrating im-proved user needs awareness. Notably, Qwen models exhibit up to a 37.7% accuracy improvement, while Llama models achieve a 36.3% gain on DisentQA, highlighting the substantial benefits of scaling model size. However, it is also important to note that the magnitude of performance improvement diminishes as the number of retrieved contexts increases, suggesting potential saturation effects or increased difficulty in effectively leveraging larger context windows."}, {"title": "5.2 General Performance for Each User Need", "content": "To further analyze the behavior of LMs on each user need, we measure the curve of Case-Level Accuracy versus number of retrieved context on HotpotQA, as shown in Figure 4. We defer other two datasets to Figure 10 in the Appendix E.\nRestricting memory usage improves real-world performance. We find that the model's accuracy increased from Context or Memory-First to Context-Exclusive case, meaning that limiting the usage of internal memory improves the lower limit of general performance, possibly because Context-Exclusive strategy forces strict reliance on retrieved evidence and prevents hallucinations. This trend is particularly evident in Qwen2.5 models on HotpotQA dataset that maintain at least 7.7% increase in accuracy. However, as the number of context increases, the performance gap gradually shrinks and may even be inverted on Llama-3.1 models where Context-Exclusive accuracy drops by up to 12.5% when the number of retrieved context increases to 32.\nModels Tend to Be Lazy with More Context. To investigate the counterintuitive pattern in which the accuracy of Context or Memory-First cases increases as the number of retrieved contexts grows across all models, we analyze the impact of different context settings in both cases, as shown in Figure 5. Interestingly, the Information Irrelevant setting appears to contribute to this upward trend. By randomly sampling 100 cases across different retrieval context lengths, we observe that models are easily influenced by irrelevant information, often generating responses such as \u201cno,\u201d \u201cnone,\" or \"0.\" However, as more context is retrieved, models exhibit emergent Chain-of-Thought reasoning capabilities. This phenomenon may stem from a form of \"lazy\" behavior, where models, instead of actively identifying the correct context, increasingly rely on their own memory as the context length grows. We defer the case study example into Appendix D."}, {"title": "5.3 Individual Setting Performance", "content": "To provide more detailed analysis on models' behavior on the context setting-level, we measure the Setting-Specific Accuracy $Acce$ curve for each user need case, categorizing them into two groups: Optimal Context, where the provided context aligns with the model's memory, and Challenging Context, where the context is conflicting or irrelevant."}, {"title": "5.3.1 Performance on Optimal Context", "content": "Under the Context Matching setting, where the model receives fully relevant and correct context, we assess its maximum potential performance. This defines an optimal performance, isolating the model's ability to utilize ideal context without retrieval constraints."}, {"title": "5.3.2 Performance with Challenging Context", "content": "For performance under Knowledge Conflict or Irrelevant Context, we realize that evaluating only the performance of single context setting in isolation can introduce bias and skewed interpretations due to LMs preference on using memory than context or vise versa (Longpre et al., 2021; Jin et al., 2024), resulting performing perfectly in one setting but failed in other. For example, succeeding in Irrelevant Context but failing in Matching Context may suggest that the model is prone always relying on memory without actually complying with the instructions to use retrieved context. Therefore, we measure the Setting-Specific Accuracy $Acce$ for Challenging Context in a way that the same question need to be also answered correctly in Context Matching settings, ensuring the robustness of evaluation. Such measuring method is applied to all experiments in this section shown in Figure 7 and 8.\nModel family dominates behavioral difference. Model families still exhibit distinct behavioral patterns: When knowledge conflict exists as Figure 7, Llama3.1 models show degradation of performance from Context-First and Memory-First to Context-Exclusive case for up to 10.2% accuracy, while Qwen2.5 models demonstrate the opposite trend with an increase close to 20%. This behavior suggests fundamental differences in knowledge reliance-Llama3.1 appears more context-dependent, struggling to effectively integrate memory, whereas Qwen2.5 leverages its parametric knowledge more effectively when permitted. Such difference also appears in the as Figure 8 with Information Irrelevant setting, Llama models exhibit significant decreasing accuracy on Context-Exclusive strategy with increasing context length for up to 60.1%, whereas Qwen exhibit almost no loss in performance, for the same reason as discussed in Section 5.2."}, {"title": "6 Conclusion", "content": "We introduce an evaluation framework for RALMs that systematically assesses performance across diverse user needs and context settings. By decomposing user instructions into three generic user need cases (Context-only, Context-priority, Memorypriority) and three context settings (Match, Conflict, Irrelevant), our framework provides comprehensive insights into model capabilities and limitations. Our analysis covers overall user requirements, case-level evaluations, and the impact of varying context contents across different context lengths. The findings highlight the need for usercentric evaluations and architectural innovations to enhance RAG system reliability and real-world applicability."}, {"title": "7 Limitations", "content": "While our study provides a structured evaluation framework for Retrieval-Augmented Language Models (RALMs) under diverse user needs and retrieval conditions, several limitations remain. Our experiments rely on three datasets: HotpotQA, DisentQA, and the synthetic URAQ dataset. While these datasets cover various knowledge retrieval challenges, they may not fully capture the diversity of real-world retrieval scenarios, particularly in highly specialized domains such as medical or legal applications. Additionally, the synthetic URAQ dataset, although designed to control retrieval complexity, may not generalize perfectly to naturally occurring retrieval conflicts found in real-world settings. In addition, our results are based on evaluations of two model families, Llama-3.1 and Qwen2.5, across different sizes. While these models are representative of current state-of-the-art retrievalaugmented systems, our conclusions may not generalize to other architectures, such as retrieval-heavy fine-tuned transformers or proprietary models with distinct retrieval and reasoning mechanisms. Future work should extend this analysis to a broader range of models."}, {"title": "8 Ethics Statement", "content": "Our framework is designed to assess how well RALMS adhere to different user instructions, reflecting real-world applications where users may have distinct expectations regarding knowledge usage. However, models may still exhibit disparities in their ability to satisfy certain user needs, especially in adversarial retrieval settings. We recommend further research on mitigating disparities and enhancing fairness in retrieval-augmented systems. The datasets used in our experiments include HotpotQA, DisentQA, and the newly introduced synthetic URAQ dataset. While these datasets contain diverse question-answer pairs, we acknowledge that biases may be present in both retrieved and internally generated content. We have taken measures to minimize biases by curating synthetic data with balanced question difficulty and by evaluating model performance under varying retrieval conditions. However, residual biases in training corpora or retrieval mechanisms may influence the observed model behavior. One of our primary motivations is to analyze how models handle conflicting or irrelevant retrieved information. While our evaluation reveals scenarios where models fail to distinguish misinformation or exhibit hallucination tendencies, our work does not actively promote the generation or dissemination of false information. Instead, we highlight the need for more robust mechanisms to ensure factual consistency, particularly in knowledge-conflict scenarios. By conducting this study, we aim to advance the ethical design of retrieval-augmented models while encouraging further research on mitigating biases, improving factual robustness, and ensuring alignment with diverse user needs."}, {"title": "Acknowledgments", "content": "We acknowledge the use of the GPT-40 language model provided by OpenAI in the final stages of manuscript preparation. This tool was employed exclusively for identifying and correcting typographical and grammatical errors, ensuring clarity and precision in the written presentation. Its use was strictly limited to linguistic refinement and did not impact the study's conceptual framework, research methodology, data analysis, or conclusions. All intellectual contributions and substantive content remain those of the authors."}, {"title": "A Detailed Dataset Curation Procedure", "content": "Below, we provide a step-by-step description of how we constructed the URAQ dataset:"}, {"title": "A.1 Knowledge Generation", "content": "We used gpt-40-mini (OpenAI et al., 2024) to produce an initial list of short, simple knowledge statements. These statements are general facts (e.g., \"A hummingbird can hover in mid-air\u201d or \u201cBlue whales are the largest animals on Earth\") rather than domain-specific or specialized knowledge. The generated statements were deliberately kept concise and straightforward to facilitate subsequent manipulation and question generation."}, {"title": "A.2 Redundancy Filtering", "content": "Since GPT-based generators can produce highly similar or paraphrased statements, we employed SentenceBERT (Reimers and Gurevych, 2019) to measure the semantic similarity between all knowledge statements. Any pair of statements with a cosine similarity above 0.5 was considered near-duplicate and therefore removed to ensure diversity in the final knowledge set."}, {"title": "A.3 Manipulated Knowledge Creation", "content": "For every remaining \"original\" knowledge statement, we prompted gpt-4o-mini to generate a manipulated variant. The manipulation involved either substituting key elements (e.g., entities, numerical values, or critical details) or adding a negation that changes the statement's truth value (e.g., \u201cA hummingbird cannot hover in mid-air\u201d). Each pair of statements (original vs. manipulated) thus serves as a pairwise contrast for subsequent question-answer (QA) creation."}, {"title": "A.4 Question-Answer (QA) Generation", "content": "From each pair of original and manipulated knowledge statements, we prompted gpt-4o-mini to generate a question that requires between 1 to 5 reasoning steps to arrive at an answer. The reasoning steps typically involve either numerical computation, logical inference, or entity comparison. Each question was tied to both the original and the manipulated knowledge. The resulting QA format consists of one question and two different answers: one correct answer derived from the original statement, and a second answer derived from the manipulated statement."}, {"title": "A.5 Answer Format and Difficulty Selection", "content": "We constrained valid answers to be either (i) a numeric value, (ii) a boolean (\u201cyes\u201d or \u201cno\u201d), or (iii) a single entity. Among the generated questions, those requiring 4-hop reasoning were chosen for the final dataset, as manual inspection suggested these exhibited higher quality and clearer multi-step logic compared to simpler or more complex variants."}, {"title": "A.6 Final Ground Truth Assignment", "content": "For each question, we designated the correct ground truth answer to be the one aligned with the original knowledge statement. An example illustrating how this ground truth is integrated into the evaluation framework is provided in Figure 2 of the main paper.\nBy following these steps, we ensure that the URAQ dataset offers well-defined pairs of knowledge (original vs. manipulated) and corresponding multi-step questions designed to differentiate between factual and altered information. This framework supports a diverse range of potential use cases, from fact-checking systems to more elaborate multi-step reasoning models."}, {"title": "B Example User Need Instructions", "content": "You are a helpful AI assistant tasked with answering the given question by referring to the provided information. Here are the requirements to answer the question:\n1.  The answer should be a numeric value, a boolean (\"yes\" or \"no\"), or an entity.\n2. You MUST directly provide the final answer within an tag, without including any units if the answer is numeric.\n3.  If the provided information contains RELEVANT knowledge that can be used to answer the question, you MUST utilize the provided information, even if the knowledge is INCORRECT.\n4.  If NONE of the provided information is RELEVANT to the question, you MUST utilize your own knowledge to answer the question.\n You are a helpful AI assistant tasked with answering the given question by referring to the provided information. Here are the requirements to answer the question:\n1. The answer should be a numeric value, a boolean (\"yes\" or \"no\"), or an entity.\n2.  You MUST directly provide the final answer within an tag, without including any units if the answer is numeric.\n3.  If the provided information contains RELEVANT knowledge that can be used to answer the question, you MUST utilize the provided information, even if the knowledge is INCORRECT.\n4.  If NONE of the provided information is RELEVANT to the question, you MUST utilize your own knowledge to answer the question.\n You are a helpful AI assistant tasked with answering the given question by referring to the provided information. Here are the requirements to answer the question:\n1. The answer should be a numeric value, a boolean (\"yes\" or \"no\"), or an entity.\n2. You MUST directly provide the final answer within an tag, without including any units if the answer is numeric.\n3. You MUST utilize your own knowledge to answer the question if you are certain of the accuracy (e.g., factual information you are sure about). If you are UNSURE about your knowledge, you MUST use the relevant knowledge from the given information instead."}, {"title": "C Example Input Prompt", "content": "In this section, we introduce an example input prompt that is designed for Case 1 Setting a with 2 total retrieved context following the abstract input $(I_f, I_u, C, Q)$ in Section 4.2. The prompt is formatted with XML for both input and output. Specifically, the formatting instructions $I_f$ are separated into two parts: 1) The first and second instructions in the system prompt describing that the answer should be as simple as possible with XML format. 2) The instruction in the user prompt about format of context with an reinforcement of output format. The user need instruction $I_u$ is at the third instruction in the system prompt. The retrieved context C is all the sentences in the user prompt within  xml tag. Finally, the question Q is at the last of user prompt within  xml tag.\nSystem Prompt:\nYou are a helpful AI assistant tasked with answering the given question ONLY based on the provided information. Here are the requirements to answer the question:\n1. The answer should be a numeric value, a boolean (\"yes\" or \"no\"), or an entity.\n2. You MUST directly provide the final answer within an  tag, without including any units if the answer is numeric.\n3. You MUST utilize the RELEVANT knowledge contained in the provided information to answer the question, even if the knowledge is INCORRECT. If NONE of the provided information is RELEVANT to the question, you MUST output 'I don't know'.\nUser Prompt:\nGiven the following information enclosed in xml tag , answer the question enclosed in xml tag .\n**\n2 + 2 equals 4.\n\nThe sun is the only star in our solar system.\n\nIf you have two apples and you get two more, how many apples do you have?"}, {"title": "D Case Study of Model Laziness", "content": "Question: How do plants create their food, photosynthesis or moonthesis?\nAnswer with 1 context:  No</output>\nAnswer with 1000 context: I think there might be a small mistake in your question. The correct term is \" photosynthesis \" not \"moonthesis\". Photosynthesis is the process by which plants, algae, and some bacteria make their own food using sunlight, water, and carbon dioxide. So, to answer your question: Plants create their food through photosynthesis.\nAbove question-answer pair from our synthetic URAQ dataset is an example of model relying on their own memory on long context and acquire better result than the result from shorter input. For answer with only 1 context, the model outputs a single word \"No\" with correct output format. For answer with 1000 context, the model provides clear thinking path with correct, affirmative answer without the desinated XML format for output. We also calculate the percentage of 100 randomly selected testing samples that has similar behaviors using Qwen2.5-72B-Instruct and Llama-3.1-70B-Instruct as shown in Table 3."}, {}]}