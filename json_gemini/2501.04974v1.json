{"title": "SensorQA: A Question Answering Benchmark for Daily-Life Monitoring", "authors": ["Benjamin Reichman", "Xiaofan Yu", "Lanxiang Hu", "Jack Truxal", "Atishay Jain", "Rushil Chandrupatla", "Tajana \u0160imuni\u0107 Rosing", "Larry Heck"], "abstract": "With the rapid growth in sensor data, effectively interpreting and interfacing with these data in a human-understandable way has become crucial. While existing research primarily focuses on learning classification models, fewer studies have explored how end users can actively extract useful insights from sensor data, often hindered by the lack of a proper dataset. To address this gap, we introduce SensorQA, the first human-created question-answering (QA) dataset for long-term time-series sensor data for daily life monitoring. SensorQA is created by human workers and includes 5.6K diverse and practical queries that reflect genuine human interests, paired with accurate answers derived from sensor data. We further establish benchmarks for state-of-the-art AI models on this dataset and evaluate their performance on typical edge devices. Our results reveal a gap between current models and optimal QA performance and efficiency, highlighting the need for new contributions. The dataset and code are available at: https://github.com/benjamin-reichman/SensorQA.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, the number of connected Internet-of-Things (IoT) devices has grown exponentially, with an estimated 40 billion devices expected by 2030 [41]. These devices generate vast amounts of sensor data, which, unlike text or video, are not easily interpretable by humans due to their raw and complex nature. While existing machine learning algorithms can classify sensor data into predefined categories [6, 32, 33, 47, 49, 50], they fall short in providing an intuitive way for humans to interact with and extract meaningful insights from this data. For example, answering a question like \"How good was my work-life balance last week?\" is straightforward for humans, but current technologies require multiple steps: (1) selecting the appropriate sensor data, (2) understanding the difference between work and relaxing activities, and (3) researching online to understand what qualifies as a healthy work-life balance. In everyday life, people are more interested in gaining insights related to their health and well-being, rather than identifying specific activities at a given moment.\nQuestion Answering (QA) is an ideal framework for modeling natural interactions between humans and sensors: users ask questions and receive accurate answers based on the sensor data. While QA has been extensively studied in the language and vision domains [5, 10, 13, 20, 35, 38, 39], few studies have explored sensor applications using available sensor data. Early QA systems such as AI therapist [31] and DeepSQA [48] focus on mental health diagnosis and human activity monitoring. However, their QA dataset is generated by template-based searches, which presents limited diversity and practical value. The recent rise of Large Language Models (LLMs) offers the potential for handling more diverse queries and sophisticated reasoning, such as captioning IMU signals related to activity [16, 29] or analyzing smartwatch data for medical diagnosis [12, 21, 51]. However, these models are currently constrained to dealing with short durations of sensor data, typically around 10 seconds, or low-dimensional sensor data such as step counts per day.\nIn summary, the progress of QA interactions for sensing applications is limited by the absence of suitable datasets and benchmarks. An ideal dataset would include diverse, practical samples, such as raw sensor signals of varying durations collected in real-world settings, along with rich QA pairs that align with users' genuine interests. To the best of the authors' knowledge, no such benchmark has been proposed.\nOur Contribution In this work, we introduce SensorQA, the first human-created dataset and benchmark for QA interactions between humans and long-term time-series sensor data. The creation of SensorQA emphasizes realistic QA scenarios that closely resemble everyday life. On the sensor data side, SensorQA focuses on daily activity monitoring using commonly available mobile devices for sensing, such as smartphones and smartwatches, with extensive sensor data collected from 60 users intermittently over a period of up to three months. For the QA part, we visualize activity schedules and present these graphs to human workers on the Amazon Mechanical Turk (AMT) platform. Workers are tasked with generating questions based on their practical interests and writing down ground-truth answers according to the activity graphs. To encourage question diversity, we design multi-timescale activity graphs using 14 different activity label subsets. As a result, SensorQA contains 5.6K QA pairs covering different sensor time scales from one day to multiple weeks, with six question categories and seven answer categories.\nThe contributions of this work are summarized as follows:\n\u2022 We present SensorQA, a human-created QA benchmark with naturally collected sensor data and diverse QA pairs, aimed at real-world scenarios.\n\u2022 We benchmark state-of-the-art baselines on SensorQA using typical edge devices, highlighting the challenges in QA accuracy and efficiency.\n\u2022 We open-sourced SensorQA and our code to encourage further contributions in this area."}, {"title": "RELATED WORK", "content": "Question Answering using sensor data Question Answering has attracted extensive interests in the field of Natural Language Processing (NLP), with a wide range of datasets designed to address various language tasks [5, 8, 10, 19, 20, 22, 26, 38, 53, 56]. Recent benchmarks integrate multiple modalities in QA, such as VQAv2 [13] for visual question answering, ScienceQA [28] for science-related questions, and PathVQA [17] for pathology image-based questions, among others. In sensing, researchers have developed multiple QA benchmarks for remote sensing [27, 40, 42, 46] and clinical diagnosis using low-dimensional sensor data [12, 21, 51]. DeepSQA [48] is currently the only QA dataset and benchmark for time-series data from IoT sensors, specifically for human activity monitoring. The dataset is generated automatically by a rule-based search algorithm. As a result, the questions in DeepSQA lack linguistic and content diversity, and, more importantly, may not reflect the practical interests of users. For example, as shown in Fig. 1 (left), one predefined question template compares the frequency of activities at two different times, which may not provide meaningful insights in real-world scenarios.\nMultimodal reasoning using sensor data Recent works have explored multimodal reasoning that connects sensor data with natural language. IMU2CLIP [30], mmCLIP [6], and TENT [57] align textual and sensor data embeddings using contrastive learning, similar to CLIP [36]. FM-Fi [47] leverages vision-based models for radio-frequency sensing. The most relevant works, AnyMAL [29] and OneLLM [16], enable more advanced reasoning beyond activity classification. Both works connect sensor embeddings to an LLM via an adapter module, with the LLM fine-tuned on IMU data and text descriptions from the Ego4D dataset [14], as shown in Fig. 1 (right). However, all these methods are limited to simple reasoning tasks over short, fixed-duration signals.\nIn summary, our SensorQA dataset significantly differs from prior benchmarks in two aspects: (i) On the QA side, SensorQA, created by humans, is highly diverse in terms of both the questions posed and the answers that are provided, better reflecting the needs of a user, (ii) On the sensor side,"}, {"title": "SENSORQA DATASET", "content": "In this section, we describe the collection process for the SensorQA dataset. The real-world scenario that SensorQA models is a long-term, in-the-wild sensor data collection scenario where users follow their daily routines without needing to focus on the sensing device. Throughout this process, users may pose arbitrary questions of personal interest about sensor data, whether focusing on a single day or over several weeks, and expect accurate answers derived from the collected data. To capture such real-world scenarios, we carefully design and implement protocols for both the sensor data (Sec. 3.1) and the QA data (Sec. 3.2) collection for developing SensorQA."}, {"title": "Sensor Data Collection", "content": "We choose to utilize a pre-existing dataset, the ExtraSensory dataset\u00b9 [44, 45] as the source of the sensor data for SensorQA. We select ExtraSensory for its natural, in-the-wild collection setting and its massive scale. In contrast to the vast majority of sensor datasets [7, 33, 52] that are collected in a heavily controlled environment, ExtraSensory emphasizes real-life settings. This dataset uses easily accessible sensors (IMU, compass, location, audio, and phone state sensors on smartphones and smartwatches), with participants encouraged to maintain their natural routines throughout the collection period ranging from two days to three months. In contrast to Ego4D [14] which relies on data captured by a head-mounted camera, ExtraSensory imposes no restrictions on device placement, whether they be on desks, in pockets, or held in hand. These real-life collection protocols make ExtraSensory an ideal base for the SensorQA dataset. Moreover, ExtraSensory contains raw sensor measurements from 60 subjects and has more than 300K minutes of data, tagged by 51 activity or context labels after cleaning. The massive scale of ExtraSensory allows us to explore a wide range of real-life activities and personal routines during QA. We also note that the data collection protocol for SensorQA can be extended to any future sensor application."}, {"title": "QA Data Collection", "content": "We use Amazon Mechanical Turk (AMT) [1], a crowdsourcing platform, to generate question-answer pairs through paid tasks completed by human workers. AMT is extensively utilized in the field of NLP for dataset creation [13, 39]. Unlike template-based question and answer generation in [48] or simply using the narration text provided in the sensor dataset [16, 29], our approach leverages the unique value of human-generated content, ensuring all QA pairs reflect genuine human interests and needs.\nWe carefully design our QA data collection process to ensure practical and diverse QA generation. Our objective is to produce a high-quality QA dataset that encompasses a variety of Q&A types and time durations, thereby challenging the Al agent to perform effectively in real-world scenarios. To achieve this, we use two key strategies: (i) we develop multi-time scale activity graphs to facilitate the generation of both short-term and long-term questions, and (ii) we divide the context labels from the ExtraSensory dataset into subsets to encourage queries covering a wide range of aspects. Next, we detail each strategy.\nCreating multi-time scale activity graphs Collecting a dataset on AMT requires visualizing the sensor data for the workers and asking them to generate relevant Q&A pairs. However, visualizing raw sensor signals presents a unique challenge due to the inherent unreadability of sensor data by humans [48]. Given our primary interest in understanding the underlying daily activities, we opt to visualize the activity or context labels over time and provide these graphs to the workers. These visual representations, depicted in Fig. 2, resemble Gantt Charts, with the x-axis showing wall-clock time from 00:00 AM to 23:59 PM and the y-axis representing separate days. Different activity labels are shown by bars with distinct colors. These graphs offer an intuitive visualization of daily activities along with the specific timestamps. While the temporal scale of questions is determined by individual crowdsourcing workers, we recognize that the time scale depicted in activity graphs can implicitly influence their approach. For instance, when presented with a weekly activity graph, workers tend to ask more high-level and qualitative questions like \"How frequently do I exercise?\" rather than basic quantitative inquiries such as \"What did I do at 10:00 AM?\" Motivated by this understanding, we have developed graphs at different temporal granularities to prompt questions across various scales:\n\u2022 Daily graph with timetable We generate activity graphs for each user on a single day, accompanied by a table listing the start time, end time, and duration of all activities occurring on that day. This daily graph prompts workers to generate basic quantitative questions about specific times and activities on a given day, as shown in Fig. 2a and 2b, where we omit the detailed timetable due to space limitation.\n\u2022 Multi-day graph We also create graphs depicting a user's activities over multiple days. The multi-day graph encourages workers to focus on the general and high-level activity patterns, leading to the generation of qualitative reasoning questions as exemplified in Fig. 2c and 2d."}, {"title": "DATASET ANALYSIS", "content": "In this section, we provide quantitative and qualitative analysis of the SensorQA to better understand its characteristics. Examples of the collected Q&As in SensorQA are displayed in Fig. 2. SensorQA contains 5,648 question-answer pairs, with an average length of 10.43 words per question and 10.48 words per answer. The dataset has a total of 118,051 tokens, of which 1,709 are unique and primarily related to daily activities. The repetition of words makes it more challenging for Al agents to answer questions accurately, as they must differentiate between similar questions based on the specifics of the sensor data.\nTo closely inspect the diversity of SensorQA, we profile the question and answer categories. We manually label 200 pairs, then train two BERT models [11] to classify the question and answer categories, respectively. The final profiling results are displayed in Table 3. SensorQA includes six distinct question categories and seven answer categories. The distribution of questions and answers is imbalanced, with a notable focus on time-related aspects of activities, as seen in the high number of questions in the \"Time Compare\" and \"Time Query\" categories. This pattern aligns with practical user interests but has not been observed in previous QA datasets for human activities [29, 30, 48]. In addition to time-related queries, SensorQA covers a wide range of other aspects, including action, location, counting, and existence, demonstrating its diversity and practicality."}, {"title": "BENCHMARK RESULTS", "content": "In this section, we benchmark state-of-the-art AI models on the SensorQA dataset and reveal the gap between existing models and ideal performance."}, {"title": "Benchmark Setup", "content": "We establish comprehensive baselines using three distinct modality combinations: text-only, vision+text, and sensor+text, to identify the impact of each modality. We use few-shot learning (FSL) for closed-source models like GPT, and apply LoRA fine-tuning (FT) [18] for open-source models like Llama. We randomly split 80% of the data in SensorQA for training and 20% for testing. More details are included in the github repository\u00b2.\nBaselines We begin by evaluating the generative pretrained models using few-shot QA examples in the prompt.\n\u2022 GPT-3.5-Turbo [54] and GPT-4 [3] are text-only baselines that only use the questions as input.\n\u2022 GPT-4-Turbo [3] and GPT-40 [3] are vision+text baselines that use both the questions and activity graphs.\n\u2022 IMU2CLIP-GPT4 [30] is the state-of-the-art sensor+text GPT baseline. It first uses a trained CLIP model to retrieve the most relevant text for each chunk of IMU signal, then combines the text into a storyline and provides it to GPT-4, along with the question, for answer generation. We train the CLIP model using ExtraSensory [44, 45].\nWe also selected the following open-source models, which are tested after they are either trained or finetuned. We started from the official code release and used the default hyperparameters. All Llama-based backbones are Llama-2 7B [43] unless specified otherwise.\n\u2022 T5 [37] and Llama [43] are widely used language models, serving as text-only baselines.\n\u2022 Llama-Adapter [55] is a vision+text framework that combines vision inputs (activity graphs) with a Llama model via a pretrained transformer-based adapter."}, {"title": "Results", "content": "QA performance Table 4 presents the results of all baselines on both the full-answer and short-answer versions of SensorQA. Rouge, Meteor, and Bleu scores assess n-gram precision between generated and reference answers, while exact-match accuracy focuses on correctness.\nThe results show that existing AI models perform poorly on SensorQA, with the best-performing baseline Llama-Adapter achieving an accuracy of only 28%. This highlights the significant challenge SensorQA poses for current models, as SensorQA is designed to be realistic and diverse. Notably, the Sensor+Text baselines perform worse than even the Text-only baselines, indicating that current models struggle with effective sensor-text fusion, particularly with SensorQA's long-duration sensor data. This fusion challenge seems to confuse the models, resulting in worse performance than using only the text of the question. These results emphasize the need for new approaches to effectively integrate sensor data and text in realistic applications like SensorQA.\nOverall, finetuning open-source models outperforms few-shot learning on GPT baselines. For instance, Llama-Adapter achieves 0% exact-match accuracy without fine-tuning, highlighting the importance of fine-tuning on the target dataset to better adapt models for specific tasks.\nQA performance per category Different Q&A types present varying difficulties for the models. Fig 3 shows exactmatch accuracy by category using Llama [43], one of the strongest baselines. Existence questions achieve the highest accuracy since they require only a Yes/No response. However, Llama performs only slightly better than random guessing, with an accuracy of 58%. The most challenging category involves time-related queries, such as duration and timestamp questions, which makes SensorQA unique compared to prior datasets [29, 30, 48]. It is critical for future approaches to accurately extract time information from sensor data and incorporate it into responses.\nEfficiency results Fig. 4 presents model memory requirements and average answer generation latency on the NVIDIA Jetson TX2 [2]. Non-LLM models generally require less memory; however, DeepSQA and T5-Base encounter out-of-memory (OOM) issues with large multi-day timeseries inputs, so their latency results are omitted. Non-LLM methods also show poor QA performance. LLM-based models, while more accurate, demand substantial memory for their billion-level parameters, resulting in an average generation latency of over 57 seconds, which is impractical. Optimizations in memory and efficiency are essential to develop a viable conversational AI agent for edge deployment. We hope SensorQA encourages new contributions in this area."}, {"title": "CONCLUSION", "content": "As IoT and wearable devices proliferate, the ability to interact with sensor data through conversational AI becomes increasingly crucial. In this work, we introduce SensorQA, a question-answering dataset created by humans to foster natural language interactions between humans and wearable sensors in daily life monitoring. SensorQA is built on sensor data from ExtraSensory [44, 45] and 5.6K QA pairs collected from AMT, featuring practical scenarios and diverse queries. Benchmarking results on state-of-the-art AI models demonstrate the gap between existing solutions and ideal performance, both in QA and efficiency."}]}