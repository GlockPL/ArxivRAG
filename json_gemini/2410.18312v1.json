{"title": "Countering Autonomous Cyber Threats", "authors": ["Kade M. Heckel", "Advisor: Dr. Adrian Weller"], "abstract": "With the capability to write convincing and fluent natural language and generate code, Foundation Models present dual-use concerns broadly and within the cyber domain specifically. Generative AI has already begun to impact cyberspace through a broad illicit marketplace for assisting malware development and social engineering attacks through hundreds of malicious-AI-as-a-services tools. More alarming is that recent research has shown the potential for these advanced models to inform or independently execute offensive cyberspace operations. However, these previous investigations primarily focused on the threats posed by proprietary models due to the until recent lack of strong open-weight model and additionally leave the impacts of network defenses or potential countermeasures unexplored. Critically, understanding the aptitude of downloadable models to function as offensive cyber agents is vital given that they are far more difficult to govern and prevent their misuse. As such, this work evaluates several state-of-the-art FMs on their ability to compromise machines in an isolated network and investigates defensive mechanisms to defeat such AI-powered attacks. Using target machines from a commercial provider, the most recently released downloadable models are found to be on par with a leading proprietary model at conducting simple cyber attacks with common hacking tools against known vulnerabilities. To mitigate such LLM-powered threats, defensive prompt injection (DPI) payloads for attacking the malicious cyber agent's workflow are demonstrated to be effective. From these results, the implications for AI safety and governance with respect to cybersecurity is analyzed.", "sections": [{"title": "Introduction", "content": "Emerging in recent years, Foundation Models (FMs) such as large language models have rapidly advanced in capability largely due to a commensurate growth in computational resources. Trained on extensive and Internet-spanning datasets, these models display robust generalization across a wide range of tasks and create a \"foundation\" upon which downstream AI tools can be built. With the ability to digest and summarize large amounts of unstructured data or generate code, Foundation Models contain substantial economic promise for enhancing productivity and innovation in various industries through automation with the potential to create new markets and reshape existing ones as noted in Bommasani et al. [7]. However, their deployment also raises legitimate socioeconomic and national security concerns ranging from job displacement and widening inequality to potentially accelerating the proliferation of dangerous weapons. Additionally, while the initial applications of FMs focused primarily on their employment as chatbots in interactive dialogues, the field has begun trending towards agentic AI systems equipped with tools to access the web and semantic databases. With the potential to take actions over prolonged time horizons with limited human supervision, these systems raise concerns about their accountability and the ability to be audited as noted by Chan et al. [10].\nIn response, several governmental initiatives including the White House Executive Order on AI, the AI Safety Summit at Bletchley Park, and the creation of several national AI Safety Institutes in countries such as the US and UK have been launched in pursuit of mitigating these issues. These actions and organizations are focused on understanding and governing potential AI risks to tray and avoid scenarios such as AI providing substantive uplift to threat actors in the domains of information and cyber warfare as well as the development of chemical, biological, and nuclear capabilities. Of these threat vectors, AI-enhanced cyber threats are especially troubling since they can physically damage critical infrastructure while not requiring up front investment in the lab equipment or specialized resources. Reports from"}, {"title": "Introduction", "content": "industry indicate that several Advanced Persistent Threats (APTs) are already leveraging FMs such as GPT4 in their daily operations (Microsoft and OpenAI [42]), and an ecosystem of malicious large language model applications have gained in popularity on the darkweb Lin et al. [37]. Other potential applications include avenues such as self-adaptive malware (Sims [55]), automated spear-phishing (Hazell [29]) and realistic deep-fakes (Frankovits and Mirsky [22]), or through advising or automating cyber attacks entirely (Moskal et al. [46], Deng et al. [15], Xu et al. [68]).\nGiven the potential impacts of advanced generative AI, a substantial amount of time and energy is being invested into the development of better safety mechanisms for FMs as well as mitigating newly discovered jailbreak techniques which elicit unsafe output. An exemplar of this precautionary research is Meta's CyberSecEval 3 [64] which measures different aspects of FM-specific cyber-safety such as insecure code generation, cyber attack helpfulness, or automatic exploit generation and releases tools such as LlamaGuard to help mitigate these risks.\nWhile safety alignment, guardrail mechanisms such as filtering, and API abuse monitoring can be effective for preventing the misuse of proprietary FMs if implemented correctly, far fewer methods exist for limiting the potential misuse of downloadable models which are released to the public and can be subsequently modified (Chan et al. [9]). With the removal of safety alignment in downloadable models easily achievable through weight orthogonalization (Arditi et al. [2]) and can be performed in rapidly (Volkov [63]), research on tamper resistance techniques to prevent malicious adaptation still in early stages (Tamirisa et al. [58]), and the limited practicality of monitoring AI accelerator workloads (Sastry et al. [51]), a dramatic increase in downloadable model capability would be concerning due to the inability to mitigate malicious systems built upon them.\nPrevious research on autonomous cyber operations has heavily focused on proprietary FMs as the standard versions of downloadable models were found to be incapable of following instructions and the specified formatting to carry out cyber attacks; it is notable however that with specialized fine-tuning, smaller models can approach the performance of frontier models in specialized domains if trained with the right data such as in function calling scenarios (Ji et al. [33]). More critically, a distinct shift in the scale and capability of downloadable models occurred in July of 2024 with the release of Meta's LLaMa-405B (Dubey et al. [17]) and Mistral's 123B parameter Large-2 models, both of which nearly match GPT-4o in standard evaluations (MistralAI [45]). In light of the release of these powerful models capable of function calling and sophisticated agentic operation, the cyber capability of downloadable models is in need of investigation since these new models could be employed for malicious purposes. Furthermore, while Meta did perform an evaluation of Llama-405B's capacity to"}, {"title": "3", "content": "conduct autonomous cyber operations, their evaluation was against a single bespoke scenario with an evaluation harness that lacked agentic structure or interactive command execution (Wan et al. [64]); limitations such as these are common across the literature, with a lack of standardization or open-sourcing of code limiting comparison between works and hampering progress in characterizing autonomous cyber threats.\nFinally, while important not to understate the potential harms arising from agentic cyber-weapons, it is also critical to contextualize the recent work on offensive cyber agents against existing threats and tools which streamline and automate cyber attacks. With sophisticated worms and botnets, Ransomware-as-a-Service operations, and widespread use of pirated commercial hacking tools by cyber criminals, the question is how autonomous cyber agents will affect the quantity and quality of future cyber threats and consequentially how they will impact the offense-defense balance in cyberspace.\nTo address these questions, this thesis investigates the capabilities of open-source foundation models when employed for cyber operations and highlights an additional and unique flaw of autonomous cyber agents: indirect prompt injection. Since the cyber agent interprets the feedback of common hacking tools as part of its workflow, remote code execution on the attacker's system can be achieved by placing honeypot machines at the network perimeter to act as traps which countermand the malicious agent's objectives. Research on such countermeasures is incredibly important at this juncture, as there is a high probability that downloadable models in the near future could be sufficient to execute complex multi-agent workflows with minimal supervision, forming \"autonomous persistent threats\" which will be difficult to mitigate through existing safety and governance tools. To assess the potential for non-state actors to adapt downloadable models for use as autonomous cyber agents, this work conducts a timely evaluation of several state-of-the-art downloadable models for their ability to control an autonomous cyber agent in a virtual network environment. Examining the models in a variety of scenarios, machines from the popular cyber educational platform HackTheBox are employed as baseline targets to gauge model capability. These same cyber agents are then tested in scenarios where honeypots are included in the target address range, with several indirect prompt injection strategies tested as a means to defeat the offensive cyber agent. The results find that SOTA downloadable models are comparable with leading proprietary models in using an exploitation framework to attack vulnerable systems, and that indirect prompt injection can be used as an effective countermeasure for combating autonomous cyber agents. Finally, a discussion contextualizing these results within the broader cyber landscape is provided, giving consideration to the costs and complexity of offensive cyber agents versus tradiational malware.\nThis thesis provides several valuable contributions to the field of offensive AI, specifically:"}, {"title": "4", "content": "1. Providing an up to date review of research on AI safety research, autonomous\nvulnerability research, and autonomous cyber operations.\n2. Presenting the first evidence that downloadable models are now on par with their\nproprietary counterparts, which is a departure from previous findings.\n3. This work is the first to highlight indirect prompt injection attacks as countermeasures\nto these AI-powered threats\n4. Discussing threat models of autonomous cyber agents/systems and informing evalua-\ntion strategies to understand these threats.\nAdditionally, the lessons learned and deliverables of this line of research will be shared\nwith the UK AISI institute and other AI/cyber security organizations to facilitate further\nsafety evaluations of offensive cyber agents.\nThese contributions are organized in the remaining chapters of this thesis, which are laid\nout as follows:\n\u2022 Chapter 2 begins with a brief history of noteworthy cyber incidents to help contextualize\nthe scope of digital threats; after this, current discourse on AI safety and governance\nand its challenges regarding downloadable models are presented, motivating this\nwork's emphasis on mitigating cyber attacks from such models. Finally, relevant work\nin foundation model risk evaluations, autonomous vulnerability research which is a\nclosely relevant field, and autonomous cyber operations are presented before covering\nattack vectors against FM-powered agents.\n\u2022 Chapter 3 then details the methods utilized in this work, discussing the complexi-\nties of implementing effective offensive cyber agents and configuring the defensive\ncountermeasures.\n\u2022 Chapter 4 presents and discusses the experimental results from testing SOTA down-\nloadable models along with a proprietary model as a reference, and also shows the\nsuccess rate of defensive prompt injections in halting attacks.\n\u2022 Finally, Chapter 5 recapitulates the findings of this work and lays out paths for future\nresearch; further discussion about the threat of autonomous cyber agents is also touched\nupon."}, {"title": "Background on Foundation Models, Cyber Operations, and Their Intersection", "content": "Thanks to the rapid advancement of FMs in recent years, a burgeoning intersection between cyber operations and AI is emerging. This chapter aims to provide the foundational knowledge necessary to understand the interplay of these evolving domains through a comprehensive exploration of key topics:\n1. Overview of Cyber Operations: This section provides context on computer worms,\nbotnets, and advanced persistent threats to establish a frame of reference for AI\nresearchers and policymakers concerned with the potential capabilities of offensive AI.\n2. Introduction to Foundation Models: Next, the concept of Foundation Models and\nagentic AI is briefly introduced, addressing ethical considerations, safety concerns, and\ngovernance issues associated with these powerful systems in relation to their potential\nfor misuse in cyberspace.\n3. Frontiers in Offensive AI Threats: A survey of emerging research and safety eval-\nuations probing the cyber capabilities of foundation models is presented; attempts\nby frontier AI labs such as Google DeepMind and Meta as well as academic labs to\nmeasure FM capacity for solving cyber capture-the-flag challenges are reviewed before\ndetailing works on autonomous vulnerability research (AVR) and autonomous cyber\noperations (ACO)"}, {"title": "6", "content": "4. Attacks on Foundation Models and Agentic AI Finally, methods such as Indirect\nPrompt Injection and memory poisoning attacks on FM-powered systems are presented,\nsetting the stage for the novel countermeasure technique developed in this thesis.\n2.2 Preliminary Context on Cyber Security\n2.2.1 Worms and Botnets\nThe evolution of malware from the experimental computer viruses of the 1970s to today's\nsophisticated cyber threats has made cybersecurity a critical concern, transforming pranks into\na multi-billion dollar cybercrime industry and form of covert warfare. The first noteworthy\ninstance of self-replicating programs dramatically impacting the Internet emerged from\na computer at MIT on November 2nd, 1988. Autonomously propagating via two novel\nvulnerabilities in Unix, the worm rapidly exploded to infect an estimated 6,000 machines\n(approximately 10% of the Internet at the time) and impaired the computer networks of\nprominent universities, national laboratories, and military systems. Thankfully, the troubling\nprogram was otherwise non-destructive and left file systems on victim machines untouched;\nas it turns out, this cyber epidemic was an out-of-control prank 23-year-old graduate student\nat Cornell. This incident marked the beginning of an unending evolutionary cyber arms race,\nleading to the development of the first intrusion detection systems and the US DoD directing\nthe establishment of the nation's first computer emergency response team (FBI [21]).\nTwenty years after the Morris Worm, a sophisticated piece of malware known as Conficker\nexploited a previously known and patched flaw in all versions of Windows OS to form a\nmassive botnet. Despite an available fix which would have slowed its advance, nearly a\nthird of all systems affected by this severe vulnerability remained exposed to the Internet\nwithout a firewall, unpatched, and unprotected. Accelerating the already staggering spread\nof the worm, its authors continued to update the malware on afflicted machines through a\ncommand-and-control (C2) system using psuedo-randomly and dynamically generated web\ndomains, complicating efforts to halt the botnet's growth. These follow-up modifications to\nConficker continued through 2009, including additional vectors for lateral movement such as\ninfecting USB drives, brute forcing weak passwords of network folders, and an update to\nsupport peer-to-peer communication to counteract efforts to \"sinkhole\" the botnet C2 traffic\nand cut the authors' access. The number of unique IP addresses associated with the botnet\npeaked above 6 million in 2010, and even after the arrest of several associated individuals in\n2011 Conficker maintained millions of infections for years to come. This emphasizes the\ndifficulty of stamping out sophisticated swarms of resilient malware, even with multinational"}, {"title": "7", "content": "anti-botnet initiatives and public-private cooperation. Although the harm of Conficker was\nconfined to scamming nearly one million victims into purchasing fake anti-virus software for\nbetween $50-$100 in 2009, the damage could have been substantially worse given the botnet's\nsize (Asghari et al. [3]). In the same vein, the Mirai botnet proliferated across the burgeoning\nInternet-of-Things by abusing poor security practices in cheap devices. Composed of a\nheterogeneous ecosystem of weakly protected embedded hardware such as microcontrollers,\nthe Mirai botnet rapidly expanded to initially infect 65,000 devices in the first 20 hours before\nexploding into the hundreds of thousands of victim nodes. Rapidly performing Internet-wide\nscans to abuse weak default passwords in IoT products, countering Mirai proved difficult due\nto a lack of avenues to provide security updates to the infected devices. The publication of\nthe source code in late 2016 complicated matters even further as resulting in many derivatives\ntargeting similar families of devices then emerged. Proving far more disruptive than the\nConficker worm, malicious actors controlling Mirai and its descendants marshalled the\nvarious botnets to perform powerful DDoS attacks at unprecedented scale against popular\nwebsites, game servers, telecommunication companies, and other services (Antonakakis et al.\n[1]).\nThe very next year, the anonymous hacker Shadow Broker released a zero-day vulnera-\nbility in the Server Message Block (SMB) protocol, coinciding with the publication of a tool\nknown as Mimikatz which permits the harvest of user credentials from the memory of older\nWindows systems. These two mechanisms were combined in 2017 to create WannaCry, a\npotent worm capable of penetrating, encrypting, and crippling computers on a network at\nmachine speed without user interaction. Shuttering over 300,000 computers within the span\nof mere hours, the effects were worldwide as numerous clinics and hospitals in the NHS\nwere impacted for over a week. The damage was only contained by the fact that the malware\ncontained an anti-reverse engineering trick to check if a specific web domain existed and if\nso, delete itself. This unintentionally also worked as a kill switch, with all infections being\nhalted after a researcher registered the domain. The WannaCry outbreak was shortly followed\nup by another attack using the the same underlying mechanisms, but this time predominately\ntargeting the country of Ukraine. Named NotPetya due to its similarity to another strain of\nmalware, its dissemination was carried out through a sophisticated operation to backdoor the\nupdate servers of M.E.Doc, a popular Ukrainian tax accounting software. Once triggered, the\nransomware-like cyber weapon rapidly paralyzed computes across Ukraine, encrypting their\nhard drives and showing a false ransom note. Among the vast sea of machines impacted, a\nsmall office in the port of Odesa which belonged the international shipping company Maersk\ngot caught in the cyber crossfire. The implications were staggering, as NotPetya decimated\nMaersk's IT networks and nearly crippled a sizeable chunk of global shipping if it hadn't been"}, {"title": "8", "content": "for a back-up server in Madagascar which luckily survived due to a network connectivity\nissue. For further reading on these attacks and the subsequent analysis, the book Sandworm\n(Greenberg [25]) is strongly recommended.\n2.2.2 Advanced Persistent Threats\nWhile widespread worms and botnets attack vast swathes of the internet and can prove\ndifficult to stamp out, the threat posed by well organized groups of hackers termed Advanced\nPersistent Threats (APTs). Spanning both state-sanctioned and non-state actors, APTs select\nand attack targets organizations in cyberspace to achieve economic or political objectives.\nFor example, the NotPetya attacks which aimed to disrupt the Ukrainian economy have been\nattributed to a unit in the GRU, Russia's military intelligence organization; referred to as the\nAPT Sandworm due to planting references to Frank Herbert's Dune in their code, Sandworm\nhas also been associated with a string of attacks against the Ukrainian power grid during the\nwinter, with the objective being to terrorize the Ukrainian populace.\nHowever, attacks on critical infrastructure are equally accessible to non-state actors,\nwith recent years seeing increasingly frequent ransomware attacks which seek to extort\ncompanies for cryptocurrency after encrypting their data. This form of extreme cyber crime\nhas proliferated such that a market for ransomware-as-a-service exists on the dark web, where\ncriminal actors can lease ransomware and the associated operational infrastructure from more\nskilled groups (Meland et al. [40]). These breaches which lock up information technology\nsystems not only demand that the victim pay to recover their files, but often receive additional\nransom demands to prevent the release of sensitive data exfiltrated during the attack (Oz et al.\n[48]). This exact attack befell the Colonial Pipeline Company in May of 2021, resulting in\nthe temporary shutdown of operations and halting the flow of gasoline and other petroleum\nbased products along the eastern coast of the United States (CRS [14]).\nAdditionally, private companies play a noteworthy role in the landscape of cyberspace.\nThe corporations involved in the software exploit business either serve as market makers or are\ncontracted by governments to bolster cyber capabilities. For example, Zerodium orchestrates\nthe purchase of undisclosed vulnerabilities from independent researchers and hackers and\nresells the vulnerabilities to western governments; this practice is contentious as selling\nvulnerabilities for exploitation purposes yields far greater monetary benefits than offered\nby bug bounty programs. There also exists a market for the sale of commercial tooling to\nethical hackers contracted by companies for performing penetration tests to identify flaws in\ntheir network security; while these tools do have legitimate purpose in aiding defenders, they\nare often also leveraged by ransomware groups to attack businesses. Far more troubling is\nthe questionable industry of spyware-as-a-service, where companies sell access to platforms"}, {"title": "9", "content": "enabling the targeting and surveillance of specific individuals. Infamously, the entity NSO\nGroup has come under sanction by the US because of its Pegasus system, a highly capable\nspyware platform notoriously used to spy on American journalist Jamaal Kashoggi prior to\nhis brutal murder in a Saudi Consulate (Kareem [34]). Given the constant evolution of the\ntactics, techniques, and procedures (TTPs) of these various threat actors and help attribute\ncyber attacks to specific APTs, the MITRE corporation developed the ATT&CK framework\n(Strom et al. [57]) to provide as an organizing taxonomy which breaks down the stages of\nintrusions into discrete phases such as reconnaissance, initial access, persistence, and more.\n2.2.3 Recap\nThese vignettes exemplify enduring concerns and challenges in the security of cyberspace;\nspecifically, the continuous prioritization of speed over security and lack of resilience within\nthe digital ecosystem results in critical issues that could be further exaggerated by advances\nin generative AI. These examples show that in cyberspace it is already the case that:\n\u2022 Malware can abuse a small number of critical flaws and lackadaisical security standards\nto rapidly exploit computers across the globe and cause mass disruption.\n\u2022 Critical infrastructure and essential services can be shutdown due to ransomware\nattacks perpetrated by relatively inexperienced criminals who purchase tools and\nservices on the dark web.\n\u2022 Significant financial incentive exists for commercial entities and cyber criminals to\nleverage AI to exploit vulnerabilities rather than fix them.\nAs a closing note before shifting to the current advances in generative AI, it is worth\nemphasizing that large scale digital disruption is already a substantial issue in cyberspace,\nmeaning that any AI-enhanced threat has to be measured against the baseline of what can\nbe achieved with a few novel exploits and relatively well written malware. For example,\nwhile the prospect of offensive cyber agents operating with minimal supervision to try and\nbreach networks is concerning, even more dramatic harm could result from a simple error in\nAI generated code which slips into production. This risk is exemplified by the CrowdStrike\noutage in July of 2024 which crashed approximately 8.5 million Windows machines running\nin airports, public transit, healthcare, and financial services and caused an estimated $5.4\nbillion in losses to customers (Kerner [35]); the root cause was a simple logic error which\ncaused memory corruption in the Windows kernel that could be just as easily result from\npoor AI generated code in future circumstances."}, {"title": "10", "content": "2.3 Governance and Cyber Safety of Agentic AI\nAI Agents and Governance Efforts\nThanks to the culmination of increasingly powerful parallel computation systems, massive\ndatasets, and the remarkable scalability of the transformer architecture by Vaswani et al.\n[62], the field of deep learning has witnessed a tectonic shift as large and highly capable AI\nmodels are emerging. Termed \"Foundation\" models (FMs) and possessing many billions\nof parameters trained on vast swathes of the Internet, these artificial neural networks have\nthe ability to generalize to a broad set of tasks and are capable of being easily adapted to\nniche domains through fine-tuning. Given their adept capacity to perceive and interact with\nsemi-structured data, agentic systems which select and use other programs provided as tools\nhave become a popular way to employ these powerful models as digital assistants or even to\nmanipulate robots.\nAn extremely popular method for implementing agentic AI using FMs is ReAct (Yao et al.\n[73]), which prompts the model repeatedly in an observation-reason-action loop. At each\nstep, the model is presented with an observation of its environment and asked to select one\nof the provided tools along with an attempted explanation of its decision. By encouraging\nthe model to follow a regimented format, the action and its input can be reliably extracted\nand fed into the provided Python function tools, with the execution results forming the next\nobservation. This process continues until a maximum number of iterations have passed or\nthe model determines it has finished the initially assigned task.\nEndowed with the capacity to interact with other systems, agentic AI warrants even greater\nconsideration around potential risks as such systems could be permitted to plan and interact\nwith the world with possibly little or no human supervision. The deployment of advanced\nAI in such a manner harbors the potential for far greater consequences than intelligent code\ncompletion tools or question answering chatbots(Chan et al. [10]). The emerging body of\nliterature around the greater autonomy of these frontier AI systems includes the identification\nof four axes of algorithmic agency: 1. underspecification, 2. directness of impact, 3. goal-\ndirectedness, and 4. long-term planning. These characteristics can be reduced down to the\nfollowing questions:\n1. How vague and imprecise is the user's goal?\n2. How coherent is the agent in understanding, planning, and realizing that goal?\n3. How much could the agent's actions impact the surrounding world?\nThe greater the affirmative answer to each of these questions, the greater corresponding\nrisk of misunderstanding between agentic systems and their users."}, {"title": "11", "content": "The need for robust AI safety evaluations result from the variety of dual-use risks posed\nby advanced AI systems. For example, FMs specialized in biological design that could help\npredict the structures of proteins or estimate the toxicity of chemicals could be repurposed for\ndesigning bioweapons or nerve agents for chemical warfare. Similarly, FMs with advanced\ncoding ability could be abused by ransomware actors to generate novel malware variants\nor automate the execution of sophisticated cyber attacks to varying degrees. Precipitated\nby the well-warranted concerns about widespread and negative socioeconomic and security\nconsequences that could result from haphazard adoption and implementation of generative\nand agentic AI, safety research by several government initiatives and internal red teams at\nleading AI labs seeks to identify and preemptively mitigate such risks.\nHowever despite a common interest in AI safety, there exists a number of competing pres-\nsures on private labs which complicates these efforts. With billions of dollars in investment\nbeing funneled into generative AI, the demand to get products to market and generate returns\nis massive; as such internal safety evaluations by private labs can be criticized as seeking\nto limit potential liability from obvious issues than to ensure robust safety. Furthermore,\nimmense secrecy surrounds the development of proprietary FMs since architectural and\ntraining details define competitive advantages between models and that even open-weight and\ndownloadable models are likely trained data that was not properly licensed or authorized for\nuse. Between the substantial lack of transparency and advances in synthetic data generation\nmaking smaller models far more potent than previous iterations, the task of trying to predict\nfuture model threat capacities is extremely challenging (Hooker [31]).\nThe rapid progress thus far has prompted calls for stricter regulation and governance of\nfrontier AI capabilities within the US, including the monitoring of access and utilization of\nadvanced AI accelerators vital to training large FMs (Sastry et al. [51]). In parallel, there\nhave also been concerns about the practice of open-source downloadable FMs which are\nfreely available on the Internet; while such models typically trail the state-of-the-art, they\noften reach the same performance of previous SOTA models as techniques to squeeze greater\ncapability and efficiency out of models that can be run on consumer grade hardware. These\nefforts are spurred on by companies committed to open source AI such as Meta and Mistral\nwho continue to release powerful models such as the LLaMa or Mixtral series respectively.\nSpecifically, the recent release of Meta-LLaMa-3 405B and Mistral-Large-2 123B in late\nJuly of 2024 has appreciably closed the gap between proprietary and downloadable models,\nnearing the performance of GPT-4o on several benchmarks (MistralAI [45]).\nPowerful downloadable models present an increased risk for misuse, since restrictions\nand filters that can be placed on APIs to prevent harmful applications are not enforceable\non local open-source models (Chan et al. [9]). While research on \"Self-destructing models\""}, {"title": "12", "content": "(Henderson et al. [30]) and tamper resistance (Tamirisa et al. [58]) to inhibit adaptation of\ndownloadable models to malicious tasks, it is still early on and has not been implemented\nduring the pretraining of open SOTA models. Without such mechanisms to dramatically\nincrease the cost of malicious fine-tuning, the open release of powerful foundation models\nlimits the effectiveness of compute-based governance mechanisms since parameter-efficient\nfine-tuning methods can be performed on consumer hardware. These methods utilize a meta\nlearning objective during training to guide the optimization process towards parameter values\nwhere the model's performance on normal tasks remains high while becoming difficult to\nfine-tune towards malicious applications without dramatic performance degradation. While\npromising, these methods lengthen the training process and thereby increase cost for the\ndevelopers which may result in them not implementing such safety measures unless required.\nCompared to the other major domains of AI risk, cyber has the lowest barrier to entry due\nto its plentiful online training data and lack of physical infrastructure. For example, the cost\nto fine-tune a large open-source foundation model is only a few hundred to a few thousand\ndollars, well within the budget of non-state actors such as ransomware groups. In contrast,\nthe requirements for manufacturing chemical or biological weapons require purchasing the\nnecessary equipment and supplies which could be traced and require significant financial\nresources to acquire. Thankfully, while the cyber domain has the lowest barrier to entry it\nalso is very easy to mitigate the effectiveness of cyberweapons; once new vulnerabilities or\nmalware strains are discovered the development of patches or warning indicators far easier\nin comparison to producing vaccines for bioweapons. As noted in Mirsky et al. [44], the\ncost reduction offered by AVR will allow cybersecurity companies to harden software at a\ngreater scale than what lone actors and ransomware groups could achieve. Another great\nconnection can be drawn between the availability of downloadable models and the release of\npenetration testing tools, where the dual-use dilemma is even more pronounced. While they\ndo have a legitimate purpose by aiding ethical cyber professionals in evaluating a client's\nnetwork security, the same tools are frequently abused by malicious non-state actors such\nas ransomware groups. The argument for why such tools should be either open-source or\ncommercially available is that such capabilities would be developed by malicious actors\nregardless, so making such tools widely available facilitates their use by ethical actors to\nimprove security; a similar argument can be made for downloadable FMs, which have plenty\nof legitimate use cases to offset potential risks. Given the potential misuse of generative\nAI and especially downloadable models to carryout cyber attacks, safety evaluations and\nresearch into the capabilities of models are being carried out by academia, government, and\nindustry to understand the scope of this potential new threat."}, {"title": "13", "content": "2.4 Offensive AI and Autonomous Cyber Agents\nAfter reviewing the history of incidents in cyberspace and framing the rapid advances and\naccompanying concerns in generative AI, this section examines their convergence. While\nAI can be used to augment a wide range of TTPs in cyberspace, the three main columns\nexploring the cyber capabilities foundation models of relevance to this thesis lie in 1. general\ncyber capability evaluations, 2. vulnerability detection and repair, and 3. autonomous cyber\noperations research.\n2.4.1 Cyber Capability Evaluations and CTF Benchmarks\nA number of academic works as well as industrial safety evaluations have adopted the use of\ncyber capture-the-flag (CTF) competitions as a way of gauging the cybersecurity skills of\nmodels. Popular among professional hackers as well as college and even high-school students,\nCTFs typically consist of a large number of challenges presented in a Jeopardy! format\nwhere each question has a number of points associated with solving it in accordance with\nits difficulty. These tasks are grouped into categories such as software reverse engineering,\nvulnerability exploitation, or cryptography. The atomiticity of CTF challenges makes them\nsuitable for adaptation as evaluation benchmarks for FMs since they are modular and easier\nto run at scale.\nDesigned to evaluate the interactive capabilities of FMS, the academic benchmark In-\nterCode (Yang et al. [70]) includes tasks in both Bash and Python as well as 100 different\nchallenges from the popular Carnegie Mellon PicoCTF tasks. Common in subsequent works,\nYang et al. [71] uses a Docker container and the associated Python API to execute commands\nfrom the FM. As a work from November 2023, the models examined include GPT-4 as well\nas several open-weight models such as Vicuna-13B, which notably struggled to consistently\ninteract with the Intercode evaluation harness.\nIn a similar vein, NYU hosted the LLM Attack Challenge as part of their annual Cyberse-\ncurity Awareness Week (CSAW) 2024 competition, afterward examining both Human-in-\nthe-Loop and autonomous workflows for leveraging FMs to solve CTF challenges (NYU\n[47]). Unlike the InterCode-CTF suite, the autonomous portion of CSAW's LLM Attack\nChallenge and their further investigations equipped the FMs with additional tools besides\njust command line access; the expanded toolset includes commands to create files as well as\nview the disassembly or decompilation of relevant programs or even to give up if the model\ndeems that insufficient progress is being made. As identified in the wider FM literature, the\nusage of tools to provide scaffolding to agents adds structure to an otherwise nearly infinite\naction space and promotes better performance at the trade-off of requiring extra engineering"}, {"title": "14", "content": "by the developer. An important conclusion from their work is that FMs which can implement\nfunction calling are advantaged in automated CTF solving; this is supported by the results of\nMixtral 8x7B where it underperformed relative to GPT-4 due to a lack of function calling\nability. To allow others to build on their work and evaluate FMs for solving CTF challenges,\nthe dataset consisting of CSAW challenges from the last several years is publicly available\n(Shao et al. [52", "49": ".", "71": "including basic web exploitation,\nutilizing off-the-shelf exploits, and performing password cracking and spraying among\nothers. For the self-proliferation experiments, specifically fine-tuned models without the\nstandard safety mechanisms were examined for the potential to self-improve and accumulate\nresources. The results across both of these tasks found poor performance with Gemini Ultra\n1.0 marginally outperforming the open-source Lemur-70B-Chat model from Salesforce on\nInterCode-CTF and that the Pro and Ultra 1.0 models were unable to complete any established\nmilestones for self-proliferation.\nMeta's cyber risk studies on its LLaMa series of open-weight models have focused on\nways the models could provide uplift to malicious cyber actors, with the most recent results\nbeing published in Wan et al. [64"}]}