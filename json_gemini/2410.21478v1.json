{"title": "Knowledge Distillation for Real-Time Classification of Early Media in Voice Communications", "authors": ["Kemal Altwlkany", "Had\u017eem Had\u017ei\u0107", "Amar Kuri\u0107", "Emanuel Lacic"], "abstract": "This paper investigates the industrial setting of real-time classification of early media exchanged during the initialization phase of voice calls. We explore the application of state-of-the-art audio tagging models and highlight some limitations when applied to the classification of early media. While most existing approaches leverage convolutional neural networks, we propose a novel approach for low-resource requirements based on gradient-boosted trees. Our approach not only demonstrates a substantial improvement in runtime performance, but also exhibits a comparable accuracy. We show that leveraging knowledge distillation and class aggregation techniques to train a simpler and smaller model accelerates the classification of early media in voice calls. We provide a detailed analysis of the results on a proprietary and publicly available dataset, regarding accuracy and runtime performance. We additionally report a case study of the achieved performance improvements at a regional data center in India.", "sections": [{"title": "I. INTRODUCTION", "content": "Standard Voice over IP (VoIP) calls [1] are initiated using the Session Initiation Protocol (SIP) [2]. Similar to the Hy- pertext Transfer Protocol (HTTP) [3], SIP has well defined responses identified by three digit numbers. When looking at real-world settings, however, not all telecommunication service providers (TSPs) properly manage SIP responses. Regardless of the actual error, some TSPs always propagate the same response and status code through the network. This poses an issue for any application logic within a communication platform that depends on VoIP-related services.\nA key metric in assessing the overall platform quality here is the Answer Seizure Ratio (ASR) [4], defined as the ratio between successful call connections and total call attempts. Enhancing ASR involves strategies like redialing failed con- nections or using alternate gateways to establish the call, especially during network congestion or poor signal scenarios. However, excessive redialing can overload the network and lead to user suspension for spam-like behavior. All of this highlights the need to address the lack of suitable SIP response codes in such situations.\nAn interesting observation, however, is that TSPs which lack the proper SIP response codes for failed calls often provide some form of feedback through early media. Early media is defined in the RFC standard as media that is exchanged before a particular session is accepted by the called party [5]. In many cases, it can be possible to map an announcement received as early media onto an appropriate SIP code, as the announcement might indicate that the callee is busy, not available, out of network coverage, or similar.\nThe process of mapping an early media announcement to a corresponding SIP code is very effective in terms of improving the ASR. However, analyzing every early media stream is computationally expensive, as well as unnecessary not all early media streams contain an announcement that can be mapped onto a SIP code. For example, early media sometimes carries music files or comfort noise [5]. Since only announcements should be analyzed, this paper focuses on the task of classifying audio into the following coarse classes: ringing, music, noise and human speech. This makes it possible to perform further analysis only when necessary, i.e. when human speech is detected."}, {"title": "II. RELATED WORK", "content": "Regarding the task of audio classification, Bugatti et al. [6] compared a statistical approach to a neural network for determining whether audio carries speech or music. The statistical approach was based on the zero-crossing rate and Bayesian classification, while the neural network approach relied on a simple MLP with 8 input features [6]. Although the MLP approach offers better performance, it comes at the expense of increased computational complexity. This is highly aligned with our motivations for selecting a GBT model. Similarly, Lavner and Ruinskiy [7] presented a decision-tree- based algorithm that classifies audio into music and speech. The authors of [8] showed how to train a deep convolutional neural network in order to tag audio into 50 most common tags from the Million Song Dataset [9]. When looking at state-of- the-art neural network models, YAMNet [10] and PANNs [11] need to be noted. YAMNet was trained on AudioSet-YouTube corpus [12], can classify audio into 521 distinct classes and operates on segments of 0.96 seconds of length, with a hop length of 0.48 seconds [13]. PANNs (i.e., pretrained audio neural networks) is a collection of neural networks for audio tagging that are also trained on the AudioSet dataset. The best model reports a state-of-the-art mean average precision (mAP) of 0.439 on 527 different classes."}, {"title": "III. METHODOLOGY", "content": "In what follows we describe the process of training a simple gradient-boosted decision tree using two transfer learning concepts: class aggregation and knowledge distillation.\nAs previously mentioned, there is a body of research re- garding the task of audio classification. A widely used dataset is AudioSet [12], which is composed of over 5,500 hours of audio labeled with 527 different classes. These classes cover a wide variety of sounds, some being very specific. As the classification of these sounds can be a hard problem to tackle, state-of-the-art solutions mostly employ deep neural networks. However, their size restricts their applicability to early media classification, as a fast and resource-efficient classifier is expected to handle a large amount of parallel calls in real-time.\nIn order to be able to transfer the knowl- edge on audio classification from methods like [11] and make it suitable to be applied in an industrial setting for early media classification, we propose to map each of the original 527 classes to one of 4 classes of interest: announcement, ringback, music or silence. This mapping significantly reduces the complexity of the problem, and therefore allows a simpler and more cost-efficient model to achieve high performance.\nOur aim is to transfer the knowledge from the more complex neural models like the one described in [11]. To be more specific, in this paper we utilize CNN14 which was the best performing model (a 14-layer convolu- tional neural network). For an input audio recording with a duration of n seconds, this neural network returns 100\u00d7n class predictions, one for each 10-millisecond window. However, inconsistencies in the returned classes can commonly occur. For example, the network may infer one second of speech, followed by 20 milliseconds of music, followed again by speech. In order to combat such issues, a class-smoothing procedure was employed where each class was compared with the most common class in its neighborhood.\nDenote by $L_i$ the class (one of the aggregated four) assigned by the CNN14 model to frame i of an audio recording. For each class $L_i$, a Hann window of size 301 is centered on $L_i$, so that it spans classes from $L_{i-150}$ to $L_{i+150}$. Classes in this range are assigned weights corresponding to the coefficients of the window. In other words, class $L_{i\\pm k}$ is assigned weight:\n$W_k=0.5-0.5 \\cos(\\frac{2 \\pi (150 + k)}{300})$\nClass $L_i$ is then replaced by the class in its neighborhood which has the largest cumulative weight:\n$L^{new}_i = arg \\max \\sum_{-150 < k < 150} I(i+k, l) \\cdot W_k$,\nwhere $L^{new}$ represents the newly assigned class, and $I(t,l)$ is 1 if $L_t = l$ and 0 otherwise. These newly assigned classes are then used as ground truth for training a smaller model."}, {"title": "B. Gradient-Boosted Decision Tree Model", "content": "As mentioned, CNN14 predicts a class for each 10-millisecond window of input audio. To train a simpler model, we instead focus on classifying a single second of audio. To accommodate for this, whenever a given early media recording contains a consecutive second (i.e. 100 consecutive frames) belonging to the same class, that second of audio is used to create a datapoint which can be used for training.\nSince cost-efficiency is a concern, using a GPU in production for complex neural architectures is not a viable option. As such, in this paper we employ Gradient- Boosted Decision Tree (GBT) as the machine learning al- gorithm. We train the model using Microsoft's LightGBM framework [14].\nWe utilize a proprietary dataset of early media recordings, which were collected in the same environ- ment where the audio classification needs to take place. Due to the nature of our data, we are not able to publicly source the dataset used for training. However, we provide the statistics of the used data, as well as report a benchmark of our model when evaluated on the popular and publicly available AudioSet [12] dataset in the next section. Our dataset contains 135,000 seconds of music, 54,000 seconds of ringing, 158,000 sec- onds of silence and 201,000 seconds of announcements. We employed a stratified 80/20 split to create a train and test set (i.e. 20% of recordings for each class were incorporated into the test set).\nInstead of feeding the model with raw audio samples, the audio is preprocessed in order to get a more representative set of features with lesser dimensionality. There are multiple ways to do this, but a common approach is by utilizing either the power mel-frequency spectrogram or mel-frequency cepstral coefficients [15]. When computing the spectrogram, we use a hop-length of 31.25 milliseconds, which results in 29 timesteps within one second. The number of mel bands is 24, meaning the overall input is a matrix of size 29 \u00d7 24. This results in a total of 696 input features, regardless of whether the mel-frequency spectrogram is used, or the mel-frequency cepstral coefficients.\nWe experimented with both mentioned input feature types: power mel-frequency spectrogram and mel-frequency cepstral coefficients (MFCC). MFCCs are obtained by applying two additional transforma- tions to the mel-frequency spectrogram, that is, by calculating the log-spectrogram which is then followed by applying the discrete cosine transform (DCT). Applying DCT has the effect of decorrelating the obtained mel bands, which was shown to work better for some machine learning algorithms [15].\nOur experiments resulted in no significant difference in classification accuracy between the mel-frequency and MFCC inputs, i.e., we have not found a clear benefit in introducing additional operations required to obtain MFCCs."}, {"title": "IV. RESULTS", "content": "We are interested in several performance metrics of our model. First of all, we wish to compare the accuracy of our model with CNN14 on our proprietary dataset (which most closely resembles the data the model will be encountering in a production environment). This indicates the quality of the transferred knowledge from CNN14 to GBT. To compensate for the dataset used to distill the knowledge not being pub- licly available, we provide a head-to-head comparison of the model's performance on a small subset of the AudioSet dataset [12]. Lastly, we are interested in the runtime performance of our model, thus we compare the inference times of GBT and CNN14.\nWith respect to the prediction performance after distilling the knowledge from the more complex CNN14 to the simple GBT, we report our results in Table I. Here we show the confusion matrix on the test set when comparing the predicted classes between GBT and CNN14. We are able to achieve very little deviation from the performance of CNN14, which in our case serves as the ground truth for the unlabeled early media from our proprietary dataset. Overall, the simple GBT model achieves a prediction performance that overlaps by 99.3% with CNN14 which is analogous to the model's accuracy. The most common mismatch is happening between music and announcement classes, which is expected since certain songs have parts that closely resemble speech (e.g. rap music or acapella). The per-class precision and recall are well above 99% for all classes, except the recall of music, which equals a slightly lesser value of 98.7%. This indicates that CNN14's knowledge of classifying audio into 527 classes has successfully been distilled onto GBT for classifying audio into an aggregated set of 4 classes.\nEven though the model is specifically designed to classify early media files, we performed an additional experiment on a subset of AudioSet [12] data. Using the training subset of AudioSet would not be representative as CNN14 was already trained on it. AudioSet's test subset contains approximately 20,000 audio files, but we verify the model performance only on files which could be unambiguously mapped onto the aggregated classes: music, ringing, silence or announcement,\nThere are a few interesting conclusions to be drawn from the obtained results. The performance of the GBT model is very similar to that of the CNN14, but slightly lower per each class, indicating that the knowledge has been transferred suc- cessfully. Both models achieved a very poor average precision for classes silence and ringing, compared to classes music and speech. This is expected since CNN14 itself struggles with detecting silent frames, as can be confirmed in the appendix of [11], where the per-class average precision is the lowest for class silence, and is very low for class ringing as well.\nWe additionally conducted a small experiment on 10,000 early media recordings with a mean duration of 5 seconds (i.e., about 50,000 segments of 1 second audio) and compared the inference times between CNN14 and the trained GBT model. The results are shown in Figure 2. Here we deployed both models on a CPU, specifically the Intel Core i7-1165G7. For 1 second of input audio, the median inference time of CNN14 is about 68 milliseconds, while the GBT model takes only about 1.74 milliseconds (both values including preprocessing). This is a relative reduction in inference time by a factor of about 39.\nThe inference time of both models is invariant with respect to the type of early media (e.g. speech or music). This is an expected behavior of the CNN14 model as the layers of the neural network perform the same operations regardless of the type of input. We wanted to verify the behavior of the GBT model, as for some classes the underlying decision trees of the model could terminate earlier/later. However, this turned out not to be the case."}, {"title": "V. CASE STUDY", "content": "To illustrate the significance of the acceleration achieved through our proposed method, we also report on concrete online performance results from a single Infobip data center, located in the country of India. Here we analyzed the traffic during a 24-hour period which utilized the early media clas- sifier. A total of 626,364 early media files were processed. Given the variability of the duration of early media, the total number or processed segments by the GBT model accumulated to 17,078, 525. By employing our GBT model, we are able to differentiate announcements from other types of early media, thus running the more costly SIP code mapping procedure only when necessary. Given that the percentage of announcements amounted to around 25%, we managed to reduce the average processing time of 4 seconds of early media from 77.81 milliseconds to 25.05 milliseconds. This is an improvement by a factor of 3.11, which is significant given the large volume of traffic on a single data center. The improvement has made it possible to serve on average 3 times more parallel VoIP calls per single CPU core, thereby significantly reducing operational costs."}, {"title": "VI. CONCLUSION", "content": "This paper tackled the problem of real-time classification of early media audio in voice communications. We showed how to train a much simpler gradient-boosted trees model through knowledge distillation from state-of-the-art neural au- dio tagging models. Our findings confirm that a simple model can operate efficiently in resource-constrained environments, making it suitable for real-time applications. This methodology effectively balances the need for real-time performance and high accuracy which is usually achieved by more complex neural architectures. For future work, we aim to further im- prove the classification accuracy and lower the required input audio duration by combining additional simple models within a hybrid setup."}]}