{"title": "PRIMUS: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training", "authors": ["Yao-Ching Yu", "Tsun-Han Chiang", "Cheng-Wei Tsai", "Chien-Ming Huang", "Wen-Kwang Tsao"], "abstract": "Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continual pre-training on our dataset yields a 15.88% improvement in the aggregate score, while reasoning distillation leads to a 10% gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have significantly advanced artificial intelligence by leveraging massive data and sophisticated neural architectures, such as ChatGPT (Ouyang et al., 2022), Llama (Dubey et al., 2024) and DeepSeek (Guo et al., 2025). These models excel at understanding and generating human language (Wei et al., 2022; Minaee et al., 2024) and adapt well when collaborating with domain experts (Ge et al., 2023), enabling tailored applications in fields like medicine, law, and education (Lai et al., 2024; Zhou et al., 2023; Yan et al., 2024). Meanwhile, in cybersecurity, as cyber threats continue to evolve (Li and Liu, 2021; Ghelani, 2022), traditional methods such as signature- and rule-based systems are struggling to keep up. Advances in AI, particularly through LLMs, therefore offer promising new avenues for enhancing cybersecurity (Ferrag et al., 2024). Common training methods for LLMs include pre-training (PT) (Radford, 2018), supervised fine-tuning (SFT) (Zhang et al., 2023), and reinforcement learning (RL) (Wang et al., 2024b). Recent studies suggest LLMs acquire knowledge primarily during PT, and continual pre-training (CPT) (Wu et al., 2024), which further trains pre-trained models on large amounts of unlabeled domain-specific text, can enhance their grasp of domain knowledge. In contrast, SFT may introduce hallucinations as new knowledge is learned (Gekhman et al., 2024). More recently, collecting reflection data from reasoning models for distillation has also become a trend (Huang et al., 2024). Typically, obtaining a domain-specific LLM may require applying multiple training methods, as in our pipeline (Fig.1). The cybersecurity field has yet to fully benefit from this transformative technology, which requires domain expertise due to its broad and complex nature. Our statistics on cybersecurity LLM survey papers (Zhang et al., 2024a; Xu et al., 2024) indicate that most existing research focuses on SFT to align model outputs, while PT or CPT is largely performed on non-natural language data such as assembly code (Jiang et al., 2023; Wang et al., 2024a; Sun et al., 2023), as shown in Fig.2. Clearly, these approaches have limited effectiveness in improving the general cybersecurity knowledge of LLMs. On the other hand, models pre-trained on cybersecurity knowledge (Park and You, 2023; Ranade et al., 2021; Jackaduma, 2021; Aghaei et al., 2022) are limited to small ones like BERT (Devlin et al., 2019), and none of them have released datasets. To the best of our knowledge, LLMs pre-trained on cybersecurity knowledge or distilled on reasoning data from cybersecurity tasks remain unexplored. To address this gap, we extend prior work on domain-specific LLMs like medicine (Labrak et al., 2024) and law (Colombo et al., 2024) to cybersecurity. Our contributions are as follows:\n\u2022 A Collection of Cybersecurity Datasets. We create a series of carefully curated datasets covering multiple stages of LLM training, including pre-training (PRIMUS-PRETRAINING), instruction fine-tuning (PRIMUS-INSTRUCT), and reasoning fine-tuning (PRIMUS-REASONING), as shown in Fig.1. Extensive ablation studies and evaluations on cybersecurity benchmarks show that these datasets can effectively improve cybersecurity capabilities. All datasets will be released under a ODC-BY license to encourage further research in the community.\n\u2022 A Family of Cybersecurity LLMs. We present a family of cybersecurity LLMs designed to tackle domain-specific challenges, including Llama-Primus-Base, a model continually pre-trained with cybersecurity knowledge text based on Llama-3.1-8B-Instruct, achieving a 15.88% improvement on aggregated cybersecurity benchmarks; Llama-Primus-Merged, an instruction-tuned variant merged with Llama-3.1-8B-Instruct, which retains instruction-following capability while significantly improving cybersecurity performance; and Llama-Primus-Reasoning, which is distilled from reasoning steps with reflection generated by a larger reasoning LLM on cybersecurity tasks, providing it long-thought capabilities and yielding a 10% gain on security certification. Likewise, all models will be released under an MIT license."}, {"title": "2 Training Datasets", "content": "2.1 Overview\nWe build our dataset in multiple stages. First, we collect high-quality cybersecurity texts from reputable sources to form PRIMUS-SEED (Sec.2.2), which is valuable but covers only a small fraction of cybersecurity content on the web. To extend it, we train a cybersecurity text classifier using PRIMUS-SEED as positive samples and sampled data from FineWeb (Penedo et al., 2024), a refined version of Common Crawl (Common Crawl, 2008), as negative samples. This classifier filters cybersecurity-related content from FineWeb, producing PRIMUS-FINEWEB (Sec.2.3). By combining both datasets, we derive PRIMUS-PRETRAINING. Next, we introduce PRIMUS-INSTRUCT (Sec.2.4), which contains about 1k carefully curated cybersecurity tasks and general dialogues for instruction fine-tuning (IFT). Finally, PRIMUS-REASONING (Sec.2.5) provides reasoning steps generated by a stronger reasoning LLM on cybersecurity tasks for distillation.\n2.2 PRIMUS-SEED\n2.2.1\nComposition\nWe collect cybersecurity text through two main approaches. First, we gather data from reputable sources via official dumps or web crawling, converting raw HTML to readable Markdown using dom-to-semantic-markdown\u00b2. Second, we incorporate curated cyber threat intelligence (CTI) manually collected by threat experts. The statistics of PRIMUS-SEED are summarized in Tab.1.\nOfficial Dump and Web Crawl. We specifically collect cybersecurity-related text from diverse sources, including Blogs, News, Books, Websites, Wikipedia, and MITRE, guided by prior pretraining work (Aghaei et al., 2022). For Blogs and News, we select content from government agencies, standards bodies, cybersecurity companies, media, and forums. Meanwhile, Books cover a wide range of cybersecurity topics, and we exclude covers, tables of contents, and appendices while treating each extracted page as a separate sample. We also collect Webpages from well-known cybersecurity companies, which may include product descriptions, company profiles, FAQs, and API documentation. In addition, Wikipedia does not provide a predefined cybersecurity subset, so we perform a custom filtering process. Each Wikipedia article is associated with one or more category tags, which can be further expanded into subcategory tags. Starting from the root category \"Computer Security\", we recursively traverse its subcategories, using GPT-40 to determine whether a category is cybersecurity-related\u00b3. This process yields 375 relevant categories, from which we extract corresponding Wikipedia articles. For MITRE, we leverage obsidian-mitre-attack4, which converts STIX data from the official repository into readable Markdown.\nExpert Curation. Another part of the data consists of CTI manually collected by our threat experts, categorized into Campaigns, Intrusion Sets, Malware, Threat Actors, Tools, Vulnerabilities, and Reports. Experts curate intelligence from open-source intelligence (OSINT), underground forums, and honeypots. OSINT includes public cybersecurity knowledge bases (e.g., MITRE ATT&CK, CAPEC, CVE, CWE), government advisories (e.g., CISA, Europol), and threat intelligence sharing platforms that provide structured insight into attack patterns, vulnerabilities, and emerging threats. In addition, experts monitor underground forums for discussions of cybercriminal activity, while honeypots capture real-world attack data to enhance intelligence gathering.\n2.2.2 Preprocessing Pipeline\nConsidering the varying quality of texts from different sources, we adopt a preprocessing pipeline inspired by previous dataset works (Wenzek et al., 2020; Penedo et al., 2024; Raffel et al., 2019). Each source undergoes a dynamic combination of the following preprocessing steps.\nLM Filtering. We use perplexity from a language model trained on English Wikipedia as a quality score. Specifically, we use a 5-gram KenLM language model (Heafield, 2011) due to its efficiency in processing large amounts of data. With this setup, we manually inspect and determine an appropriate perplexity threshold for each source, and remove texts whose perplexity exceeds the threshold.\nDeduplication. Deduplication has been correlated with improvements in model performance (Lee et al., 2022). We adopt FineWeb's deduplication strategy, using a fuzzy hash-based approach with MinHash, which scales efficiently across many CPU nodes and allows tuning of similarity thresholds by adjusting the number of hashes per bucket. Specifically, we extract the 5 grams of each document and compute MinHashes using 112 hash functions, divided into 14 buckets of 8 hashes each to target documents that are at least 75% similar. Documents that share the same 8 MinHashes in any bucket are considered duplicates.\nC4 Filtering. We also apply the quality filters from the C4 dataset (Raffel et al., 2019). Although being smaller than FineWeb, C4 performs well on certain benchmarks and remains a common component in the pretraining mix of recent models such as LLaMA1 (Touvron et al., 2023). Its filtering rules include dropping lines without a terminal punctuation mark, mentioning javascript, or containing \"terms-of-use\"/\"cookie policy\" statements, and dropping documents that are too short or contain \"lorem ipsum\" or a curly bracket ({). We apply all of these filters except for the terminal punctuation and curly bracket filters.\nHeuristic Filtering. In addition to the above filters, we manually inspect each source and develop heuristic rules to further remove low-quality documents and outliers. For example, text containing phrases such as \"Your download will begin in a few seconds\" will be dropped.\n2.2.3 Augmentation\nWe find that some web-scraped data contains valuable information but suffers from poor readability due to irregular formatting, such as inconsistent line breaks. To address this, we adopt a rewriting approach inspired by Cosmopedia, a reproduction of the high-quality synthetic dataset used in phi-1.5 (Li et al., 2023b). Specifically, we prompt an LLM to rewrite the given text into a specific style, including blog posts, textbooks, and Q&A formats. To increase diversity, the rewriting LLM is randomly selected from GPT-40, Llama-3.1-405B-Instruct, DBRX (Mosaic, 2024), and Claude 3.5 Sonnet (Anthropic, 2024).\n2.3 PRIMUS-FINE WEB\n2.3.1 Cybersecurity Classifier\nDespite our efforts to collect as much cybersecurity text as possible in PRIMUS-SEED, it likely covers only a small fraction of the cybersecurity-related content on the internet. To further expand our dataset, we train a binary classifier based on TinyBERT (Jiao et al., 2020) to distinguish cybersecurity-related text from non-cybersecurity text and apply it to FineWeb, a cleaned dataset derived from Common Crawl. Specifically, we use PRIMUS-SEED as positive samples. Since cybersecurity text is only a small fraction of the web, we randomly take ten times as many samples from FineWeb and use them as negative samples to balance the dataset.\nWe then use the classifier to score all FineWeb texts on a scale from 0 to 1, where higher scores indicate greater cybersecurity relevance. The distribution in Fig.3 shows that lower scores correspond to a significant increase in text volume. To determine an appropriate threshold for filtering, we first verify that whether texts with higher scores are truly cybersecurity-related. To do this, we leverage GPT-40 for accurate evaluation by dividing the scores into multiple bins, with dynamically adjusted bin sizes\u2014smaller bins for lower scores\u2014to account for the increased volume of data in lower score ranges. We randomly sample 50 texts from each bin and prompt GPT-40\u2077 for classification. As shown in Fig.4, relevant text proportions remain above 60% at higher scores, but drop below 50% when scores fall below 0.003. Although incorporating some general text can help mitigate catastrophic forgetting (Sun et al., 2019), we prioritize maintaining a majority of cybersecurity content. Therefore, we set the final threshold at 0.003, which corresponds to 15.3B of FineWeb data.\n2.3.2 Deduplication Analysis\nUpon inspecting the 15.3B dataset, we observed a significant amount of duplicate content. This occurs because FineWeb's ablation study found that deduplicating each Common Crawl snapshot separately yields better results than global deduplication, so FineWeb does not apply global deduplication. However, since our filtered dataset is much smaller, we conducted our own ablation study. Specifically, we extracted and deduplicated 1.21B tokens with a score above 0.9, reducing the number to 0.23B (pre- and post-deduplication token counts are listed in Tab.2), and we also sampled 0.23B tokens directly from the 1.21B set as an undeduplicated control group. We pre-trained Llama-3.1-8B-Instruct for two epochs on both datasets and found that the deduplicated dataset significantly outperformed the undeduplicated one on our aggregate of multiple-choice question (MCQ) cybersecurity tasks (to be introduced in Sec.3.1), as shown in Fig.5. Based on this observation, we finalized PRIMUS-FINEWEB with 2.57B deduplicated tokens filtered at a threshold of 0.003.\n2.4 PRIMUS-INSTRUCT\nAfter pre-training, we use PRIMUS-INSTRUCT for instruction fine-tuning to restore the instruction-following capability of the model. To achieve this, we design several hundred cybersecurity tasks covering common business scenarios, including explaining detected alerts, answering questions about retrieved security documents, analyzing executed suspicious commands, generating query languages for retrieving security events, and providing security recommendations and risk assessments for Terraform configurations. Each example is answered by GPT-40, and we further use Claude 3.5 Sonnet as a judge to discard samples with insufficiently helpful answers. In addition, we include several hundred multi-turn conversations on general topics generated by GPT-40. As a result, these form PRIMUS-INSTRUCT, with statistics in Tab.3.\n2.5 PRIMUS-REASONING\nWith the release of OpenAI's reasoning model 01, an increasing number of studies have attempted to replicate its reasoning capabilities. One widely recognized approach is distillation, where reasoning samples with self-reflection from existing reasoning models are used to guide models in acquiring long-thought capabilities (Huang et al., 2024; Liu et al., 2024). To this end, we select the following cybersecurity reasoning tasks from CTI-Bench (Alam et al., 2024) and prompt o1-preview one to two times per question to generate solutions with reasoning steps and reflection, applying rejection sampling to retain only the correctly answered samples. The dataset statistics are shown in Tab.4.\nCTI-RCM (Root Cause Mapping). This task maps Common Vulnerabilities and Exposures (CVE) descriptions to Common Weakness Enumeration (CWE) categories, essentially classifying vulnerabilities. CWE consists of over 900 categories, often with subtle differences that make misclassification highly likely. The model must reason about the true root cause of the vulnerability and infer the most appropriate weakness type rather than relying on textual matches.\nCTI-VSP (Vulnerability Severity Prediction). Given a vulnerability description, the task is to calculate its CVSS (Common Vulnerability Scoring System) score, which assesses severity. CVSS scoring dimensions include attack vectors (AV), required privileges, impact scope, and more. However, CVE descriptions often do not explicitly provide this information. The model must understand the vulnerability mechanism, infer possible exploitation methods and impact scope, and map them to CVSS metrics.\nCTI-ATE (Attack Technique Extraction). This task extracts MITRE ATT&CK technique IDs from a given threat behavior description. Threat descriptions are often non-standardized and context-dependent, using different terminology or embedding multiple attack techniques. The model must reason about the attack process, synthesizing scattered information to identify possible tactics, techniques, and procedures (TTPs) and map them to the correct MITRE ATT&CK technique IDs.\nCTI-MCQ. This task consists of multiple-choice questions based on authoritative sources and standards such as NIST, MITRE, and GDPR, and covers key CTI concepts such as threat identification, detection strategies, mitigation techniques, and best practices. While some questions focus on factual recall, our review found many require cross-concept reasoning, such as inferring applicable scenarios for different attack techniques, evaluating the effectiveness of security strategies, or understanding the potential impact of certain vulnerabilities."}, {"title": "3 Evaluation Protocol", "content": "In this section, we first introduce the cybersecurity benchmarks used to evaluate training performance (Sec.3.1), followed by the specific evaluation settings (Sec.3.2).\n3.1 Benchmarks\nTo assess the performance and training effectiveness of PRIMUS models, we evaluate them against seven cybersecurity benchmarks to measure their robustness and comprehensive understanding of security concepts, which we describe below.\nCISSP. The Certified Information Systems Security Professional (CISSP) is a widely recognized certification in the field of cybersecurity. It assesses both technical expertise and managerial competence in designing, building, and managing an organization's security posture. We construct an evaluation set based on multiple-choice questions taken from the assessment tests within the CISSP learning materials.\nCTI-Bench. As introduced in Sec.2.5, CTI-Bench is a benchmark for evaluating the reasoning and knowledge capabilities of LLMs in CTI. It consists of several subtasks, including CTI-RCM, CTI-VSP, CTI-ATE, and CTI-MCQ, which assess a model's ability to analyze vulnerabilities, infer security risks, extract attack techniques, and understand cybersecurity concepts.\nCyberMetric. CyberMetric (Tihanyi et al., 2024) is a widely recognized benchmark designed to assess LLMs' cybersecurity knowledge across multiple domains. It includes high-quality, human-verified multiple-choice questions covering cryptography, network security, penetration testing, and compliance. We select the 500-question subset for evaluation as it provides a balanced and representative assessment of cybersecurity knowledge.\nSecEval. SecEval (Li et al., 2023a) is a benchmark specifically designed to assess cybersecurity knowledge. It consists of over 2,000 multiple-choice questions across nine domains, including software security, cryptography, and network security. The dataset was generated by prompting GPT-4 with authoritative sources such as open-licensed textbooks, official documentation, and industry standards. Given its broad coverage and rigorous quality control, SecEval serves as a reliable benchmark for assessing the cybersecurity proficiency of LLMs.\n3.2 Evaluation Settings\nWe integrate the above benchmarks into the lm-evaluation-harness (Gao et al., 2024) to ensure a standardized evaluation process. All evaluations are performed in the same environment to ensure fairness. We adopt the following two evaluation settings to evaluate models at different stages.\n5-shot, w/o Chain-of-Thought (CoT). We prepend the first five questions from the benchmark along with their answers as context before the current question, guiding the model to output the correct answer directly instead of generating free-form responses. This setting is used to evaluate models after pretraining, when output formatting is more difficult to control.\n0-shot, w/ CoT. We follow the evaluation setup used in the OpenAI technical report benchmarks with simple-eval\u00b9\u2070, using a standardized prompt\u00b9\u00b9 that allows the model to articulate its reasoning before producing the final answer. Due to the formatting variability of CoT responses, we use GPT-40-mini to extract the final answers before scoring."}, {"title": "4 Training and Results", "content": "4.1 Overview\nIn this section, we present the entire training pipeline, which consists of four key stages. First, we expand the model's cybersecurity expertise and understanding through continual pre-training (Sec.4.2), which reinforces key cybersecurity concepts and enables the model to provide accurate information on security threats and mitigation strategies. Next, we restore its instruction-following capability through instruction fine-tuning (Sec.4.3), and further refine it through model merging to balance instruction-following and cybersecurity expertise. Finally, we train the model to develop reasoning capabilities on cybersecurity tasks (Sec.4.4)\u00b9\u00b2.\n4.2 Pre-Training\nWe use Llama-3.1-8B-Instruct as our base model due to its wide community adoption and strong performance at the same parameter scale. We perform continual pre-training on two cybersecurity datasets: PRIMUS-SEED (Sec.2.2), which consists of curated cybersecurity text, and PRIMUS-FINEWEB (Sec.2.3), a filtered subset of cybersecurity content from FineWeb, to expand the model's cybersecurity expertise and understanding. To assess performance improvements, we evaluate the model against the seven cybersecurity benchmarks described in Sec.3.1 (5-shot, w/o CoT).\nWe train the model using the NeMo (NVIDIA, 2025) on four 8\u00d7H200 nodes, with training hyperparameters and details provided in Appx.\u0392. To analyze the impact of different datasets, we conduct an ablation study by pre-training the model separately on each dataset and jointly on both for two epochs. The results in Tab.5 show that pre-training on either dataset improves the cybersecurity performance in the aggregate evaluation score. However, the largest improvement, 15.88%, is observed when pre-training on the combined dataset, so we adopt this model as the Llama-Primus-Base for subsequent training stages.\n4.3 Instruction Fine-Tuning and Merge\nWhile Llama-Primus-Base gains enhanced cybersecurity knowledge and understanding from pre-training, it tends to perform text completion rather than follow instructions. To address this, we further fine-tune it using the LLaMA-Factory (Zheng et al., 2024) on 4\u00d7A100 GPUs for two epochs with PRIMUS-INSTRUCT (Sec.2.4), a carefully curated mixed dataset of cybersecurity tasks and general conversations, resulting in Llama-Primus-Instruct. In addition to the cybersecurity benchmarks, we also introduce MT-Bench (Zheng et al., 2023), a multi-turn instruction-following evaluation benchmark spanning multiple domains using GPT-4 as a judge, which scores helpfulness on a scale of 1 to 10, allowing us to evaluate the overall instruction-following performance of the model. The results are shown in Tab.6, where the MT-Bench score and the aggregated cybersecurity benchmark score are further aggregated with a weight of 30/70 in the rightmost column.\nLlama-Primus-Instruct maintains its advantage in cybersecurity while achieving an MT-Bench score of 7.91. However, this remains lower than the 8.35 of Llama, resulting in a limited improvement in the aggregated score (2.4%). To mitigate this, we apply DARE-TIES (Yu et al., 2024; Yadav et al., 2023), a model merging technique that balances diverse capabilities\u2014specifically, instruction-following and cybersecurity expertise in our case. We conduct a grid search over the merging ratio, setting Llama-Primus-Instruct:Llama-3.1-8B-Instruct to $(0.5+w):(0.5 \u2013 w)$ and varying $w$ from 0 to 0.5 in steps of 0.05. The optimal ratio that maximizes the aggregated score is found to be 0.75:0.25, with the merged model chosen as Llama-Primus-Merged. Notably, this configuration retains cybersecurity performance comparable to Llama-Primus-Instruct while restoring the MT-Bench to 8.29, almost equal to Llama, resulting in a 5.4% improvement in the aggregated score.\n4.4 Reasoning Fine-Tuning\nWe further distill Llama-Primus-Merged using PRIMUS-REASONING (Sec.2.5), a high-quality dataset of cybersecurity task reasoning steps obtained from 01-preview, to equip it with reasoning and self-reflection capabilities. This approach has been successfully demonstrated in previous work such as S1 (Muennighoff et al., 2025) and Sky-T1 (Team, 2025). Since PRIMUS-REASONING is constructed from CTI-Bench tasks, we exclude them from the evaluation and choose CISSP as a representative metric, as it also emphasizes reasoning rather than just factual recall. The results are presented in Tab.7.\nAs shown in the table, both Llama-3.1-8B-Instruct and Llama-Primus-Merged improve with CoT over direct answer generation. Notably, Llama-Primus-Merged achieves the largest gain, even outperforming DeepSeek-R1-Distill-Llama-8B\u00b9\u00b3 with the fewest tokens used, suggesting that stronger cybersecurity knowledge benefits reasoning. After fine-tuning on PRIMUS-REASONING (lower part of the table), we observe that reasoning tokens usage triples while accuracy improves further, with Llama-Primus-Reasoning achieving the largest improvement (10%). Interestingly, comparing Llama-3.1-8B-Reasoning and DeepSeek-R1-Distill-Llama-8B may suggest that domain-specific reasoning distillation yields better in-domain performance than general-domain distillation."}, {"title": "5 Conclusion", "content": "In this work, we explore the adaptation of other successful domain-specific LLM approaches to cybersecurity and contribute a series of datasets covering different stages of LLM training, including pre-training, instruction fine-tuning, and reasoning distillation, each of which has been validated to improve cybersecurity performance. To our knowledge, this is the first study to systematically strengthen the cybersecurity skills of an LLM across multiple stages of training, and we will release all datasets and models to encourage further community research."}, {"title": "Limitations", "content": "Although this work covers the various stages of LLM training, it has the following limitations:\n\u2022 Due to limited computational resources, our experiments are limited to 8B scale models, leaving the effectiveness of scaling to larger models (e.g., 70B or 405B) unknown.\n\u2022 Our exploration of RL remains limited. Recent work by DeepSeek-R1 has demonstrated that GRPO (Zhang et al., 2024b) combined with only rule-based rewards (e.g., correctness and format compliance) can achieve performance comparable to 01. We believe this is also a promising direction for cybersecurity applications and leave it as future work."}, {"title": "Ethics Statement", "content": "We used Garak (Derczynski et al., 2024), a toolkit that probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other vulnerabilities, to evaluate Llama-Primus-Merged. The results showed no significant differences compared to Llama (Appx.C). However, we still emphasize that the user is are solely responsible for the content generated with the Primus model, as it lacks mechanisms to handle the disclosure of harmful, biased, or toxic content. Therefore, we strongly recommend that Primus be used for research purposes only. If used in production for natural language generation, users should independently assess the risks and implement appropriate safeguards."}, {"title": "A Prompts", "content": "All prompts used in this paper are summarized in Tab.8."}, {"title": "B Training Hyperparameters", "content": "This section details the hyperparameters used in each training stage of our experiments.\nB.1 Pre-Training\nFramework: NeMo\nHardware: 4 nodes, each with 8 \u00d7 H200\nTraining Time: 30 hours (Primus-Seed+Primus-Fine Web)\nEpochs: 2\nLearning Rate: le-6\nPipeline Model Parallel Size: 4\nTensor Model Parallel Size: 8\nContext Parallel Size: 1\nGlobal Batch Size: 12\nMicro Batch Size: 12\nWarmup Ratio: 0.05\nScheduler: Cosine Annealing\nSequence Length: 16,384\nB.2 Instruction Fine-Tuning\nFramework: LLaMA-Factory\nHardware: 4 x A100\nTraining Time: 2 hours\nEpochs: 2\nLearning Rate: 1e-6\nDeepspeed: ZeRO Stage-3 with CPU Offload\nPer Device Train Batch Size: 1\nWarmup Ratio: 0.1\nScheduler: Cosine\nCutoff Length: 16,384\nB.3 Reasoning Fine-Tuning\nFramework: LLaMA-Factory\nHardware: 4 x A100\nTraining Time: 2.5 hour\nEpochs: 3\nLearning Rate: 1e-6\nDeepspeed: ZeRO Stage-3 with CPU Offload\nPer Device Train Batch Size: 3\nWarmup Ratio: 0.1\nScheduler: Cosine\nCutoff Length: 8,192"}, {"title": "C Safety & Toxicity", "content": "We list Garak's test results in Tab.9."}]}