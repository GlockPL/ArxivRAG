{"title": "SalNAS: Efficient Saliency-prediction Neural Architecture Search with self-knowledge distillation", "authors": ["Chakkrit Termritthikun", "Ayaz Umer", "Suwichaya Suwanwimolkul", "Feng Xia", "Ivan Lee"], "abstract": "Recent advancements in deep convolutional neural networks have significantly improved the performance of saliency prediction. However, the manual configuration of the neural network architectures requires domain knowledge expertise and can still be time-consuming and error-prone. To solve this, we propose a new Neural Architecture Search (NAS) framework for saliency prediction with two contributions. Firstly, a supernet for saliency prediction is built with a weight-sharing network containing all candidate architectures, by integrating a dynamic convolution into the encoder-decoder in the supernet, termed SalNAS. Secondly, despite the fact that SalNAS is highly efficient (20.98 million parameters), it can suffer from the lack of generalization. To solve this, we propose a self-knowledge distillation approach, termed Self-KD, that trains the student SalNAS with the weighted average information between the ground truth and the prediction from the teacher model. The teacher model, while sharing the same architecture, contains the best-performing weights chosen by cross-validation. Self-KD can generalize well without the need to compute the gradient in the teacher model, enabling an efficient training system. By utilizing Self-KD, SalNAS outperforms other state-of-the-art saliency prediction models in most evaluation rubrics across seven benchmark datasets while being a lightweight model. The code will be available at https://github.com/chakkritte/SalNAS.", "sections": [{"title": "1. Introduction", "content": "Deep learning models have achieved state-of-the-art performance in several domains, including saliency prediction. The goal of saliency prediction is to provide visual attention that highlights certain areas that are visually meaningful in an image. Visual attention or saliency prediction represents an important mechanism of the human visual system, enabling the interpretation of the most relevant information in the visual scene. The procedure of the saliency prediction is as follows: Given an input image, the prediction produces a saliency map, indicating its important regions by marking it with high intensity. Thus, the resulting visual attention plays an important role in many real-world applications, including object detection, segmentation, driver focus of attention, and radiologist gaze prediction. Early research on saliency prediction mainly relies on hand-crafted features. The limitation of these hand-crafted features is their ineffectiveness towards complex scenes, i.e., natural images with multiple objects or multiple salient regions.\nRecently, deep learning has been used for its powerful feature representation. To overcome the limitation in complex scenes, the recent works extract the hierarchical features that can predict the saliency map in a multi-scale fashion. The hierarchical feature is extracted using a Fully Convolutional Network (FCN) where the convolutional layers are stacked into a hierarchical structure. The model architectures consist of an encoder-decoder framework where the encoder is used as the backbone, and the entire framework can be trained in an end-to-end fashion. However, the architectures of these decoders are designed manually, which could provide high accuracy but are not well-optimized for other properties, especially for latency and power consumption.\nTo address such problems, neural architecture search [19] can be used in searching for the best architectures for the best fitting towards the ground truth and the computational resources. One can argue that most of the architectures of the (pre-trained) encoders were optimized with NAS, at one point. However, the architectures are optimized for classification tasks. Therefore, their performance is not tailored for saliency prediction. In addition, optimizing only the encoder with NAS will require the post-training where, for every optimal choice of the encoder, each pair of encoder and decoder will need to be trained again for saliency prediction.\nIn this paper, we propose to build a framework where the encoder and decoder are optimized together with NAS, termed SalNAS. Our framework not only allows both the encoder and decoder to be trained jointly, but also optimized for various properties, e.g., accuracy, number of parameters, and computational complexity. We build a supernet that employs dynamic convolution in both the encoder and decoder, which allows the customization of the kernel dimensions for each candidate pair.\nThe supernet assembles all the candidate architectures as subnets whose weights are shared as inspired by [20], resulting in a weight-sharing NAS. Thus, the best architecture suitable for different computing platforms can be obtained by a search algorithm. Our resulting SalNAS architecture is highly efficient, but the small neural network still struggles to generalize effectively across diverse datasets due to the limited number of parameters. Therefore, we propose a Knowledge Distillation (KD) method called Self-KD that regularizes the knowledge from the student SalNAS with the weighted average information between the ground truth and the teacher model. The teacher model is a neural network whose weights are the average of the best-performing students obtained from different epochs. The best-performing students are chosen via cross-validation. In addition to KD, our learning loss is derived from saliency evaluation metrics, whose values are in different ranges [24]. Thus, we also propose to adjust the value of the loss functions to the same range (0, 1), which results in a new learning loss that improves the overall learning capabilities. Together with Self-KD, our model SalNAS offers the best architecture transfer and achieves the highest performance. Therefore, our contributions are as follows:"}, {"title": "2. Related Work", "content": "Saliency prediction can typically be divided into two categories based on their simulated attention processes: bottom-up (free-viewing) and top-down (task-driven). Our work aims to anticipate computationally efficient saliency prediction on raw images to simulate free-viewing bottom-up visual attention. Thus, we mainly focus on the related works for free-viewing computationally efficient saliency prediction models."}, {"title": "2.1. Compact Saliency Prediction", "content": "Early research in saliency prediction relies on manually extracting high-dimensional features to identify important areas that represent attention. A work [25] utilized a combination of multi-scale low-level features, including color, intensity, and orientation, to compute a saliency map. The work [26] computes a saliency map using a graph-based approach using Markov chains. Other examples of these works are [27, 28]. However, they fail to produce saliency prediction for complex scenes. Deep learning [29] has been shown to overcome complex scenes by training with large-scale annotated data. The architecture contains a pair of encoders and decoders [29, 11, 12, 13, 14, 15]. Given input images, the encoders perform the latent feature extraction; meanwhile, the decoder converts these features into visual attention, e.g., MSI [11], EML-Net [12], SalED [13], PKD [14], and FastSal [15]. To maximize the efficiency, the following pre-trained models can be used as the encoder, i.e., EfficientNet [22], OFA-595 [20], and ResNet [30]. FastSal [15] utilizes a lightweight backbone, i.e., MobileNetV2 [31], and EfficientNet-B0 [22], to achieve the fast saliency prediction offering low latency for lightweight computing devices. Another work [32] proposed a real-time saliency prediction by introducing a modified U-Net architecture and location-dependent fully connected layers to build a fast saliency model suitable for edge devices. PKD [14] proposed to distill knowledge from a bigger network (teacher) to a smaller (student) network with higher efficiency."}, {"title": "2.2. Neural Architecture Search", "content": "Conventional Neural Architectures find candidate architectures by employing (i) a supernet, a large-size network containing millions of subnets, and (ii) an architecture searching, a technique for finding candidate subnets, either by reinforcement learning [33, 34] or evolutionary search [35, 36]. Yet, these techniques require high latency. To reduce the latency, recent works [37, 38] proposed gradient-based optimization to search for subnets in a supernet which is a directed acyclic graph, termed DARTS. Recently, Saliency-aware NAS [39] proposed to employ the saliency information to further improve the training in a gradient-based NAS similar to DARTS. The architecture and weights of the directed acyclic graph are optimized in association with the intermediate results of the saliency map. Nevertheless, the directed acyclic graph is extremely long - each layer contains multiple configurations, e.g., candidate operations and parameters, which results in high memory and latency issues in training.\nHigh-efficiency approach. To solve the memory and latency problem in the conventional NAS, the high-efficiency approaches [40, 41, 20, 42] employ a supernet, a weight-sharing sequential network that replaces the directed acyclic graph. One-shot approaches [40, 41] use a sequential network as a new supernet, whose parameters between subnets are shared. Once-for-All [20] further improves the efficiency by proposing a progressive shrinking that prunes the supernet for the candidate subnets compatible with various hardware platforms. However, additional tuning is required. This problem is solved by BigNAS [42] that concurrently trained all subnets in a single-stage model, allowing high-quality subnets to be sliced without post-tuning. Uniform sampling is generally used for its simplicity, but affecting the final accuracy. This issue is solved by an attentive sampling [21]."}, {"title": "2.3. Knowledge Distillation (KD)", "content": "Knowledge distillation focuses on transferring the knowledge from the more complex and bigger teacher models to improve the generalization performance of the students. It has shown to be beneficial in many applications, e.g., [14], [15], [43]. KD can be used in NAS, which automates the design of neural architecture. For example, the work [44] proposed a novel framework of KD and NAS by introducing KD loss to facilitate model search and distillation by using an ensemble-based teacher network. Another work [45] utilizes NAS, which divides the search space into blocks for efficient training and utilizes a novel distillation approach to supervise architecture search in a block-wise pattern.\nOn the other hand, self-knowledge distillation focuses on regularizing the knowledge itself, that is, by forcing the deep learning to produce more meaningful and consistent predictions, e.g., in a class-wise manner [46]. This technique also enables better knowledge transition in learning, such as in [47] that proposed a progressive self-knowledge distillation (PS-KD) to distill the model's knowledge as a soft target. MixSKD [48] utilized two techniques, i.e., image mixture and self-knowledge distillation to mutually distill knowledge between random pair of images. A simple dropout sampling can be utilized to distill the posterior distribution of multiple models [49]. In the same light, our Self-KD regularizes the knowledge itself for better generalization. Yet, unlike these works, our technique chooses the teacher model with incremental performance via cross-validation. This can be done without computing the teacher model's gradients; thus, our technique requires only an additional feed-forward pass when computing back-propagation for the student model."}, {"title": "3. Proposed Method", "content": "We proposed a new framework for saliency prediction by incorporating two methods: (i) SalNAS, a supernet for saliency prediction, and (ii) Self-KD, a teacher-less distillation. The proposed architecture of SalNAS consists of an encoder-decoder structure. It utilizes a search space derived from AlphaNet [50] and is adjusted specifically for saliency prediction. Then, we propose a self-knowledge distillation, termed Self-KD, to improve the generalization of SalNAS. To train the SalNAS supernet, a combination loss that incorporates different saliency evaluation metrics is presented. SalNAS is first presented in Section 3.1. Self-KD is then proposed in Section 3.2. Then, the learning loss is discussed in Section 3.3."}, {"title": "3.1. SalNAS", "content": "Let $S a l_{i}$ denote the architectural configurations of subnets in a supernet to be used for saliency prediction. Let $W_{o}$ be the weight-sharing of the supernet. Our strategy is to minimize the validation loss function $L_{val}$ by solving the optimization problem:\n$\\min \\sum L_{val}(S(W_{o}, S a l_{i}))$\n$\\qquad\\qquad W_{o}$\n$\\qquad S a l_{i}$\n$\\qquad\\qquad\\qquad\\qquad \\qquad\\qquad (1)$\n\nThe architecture for each $S a l_{i}$ is a FCN consisting of the dynamic encoder and decoder module. Specifically, we consider $S a l_{i}:=$ Decoder(Encoder(X)). S is a selection scheme that selects part of the model from the supernet to form a sub-network with architectural configuration $S a l_{i}$.\nThe dynamic convolutional layers can adapt to variable dimensions such as width, depth, kernel size, and input resolutions, unlike the vanilla convolutional layers with fixed feature and kernel sizes [51, 52, 20]. Suppose we have an elastic input with shape $X \\in \\mathbb{R}^{C_{i n} \\times H \\times W}$, where $C_{i n}$ denote the selected input channel; H and W denote the height and width of the elastic input, respectively. The dynamic convolutional layers are defined as follows:\n$\\varphi_{c o n v}(X)=W\\left[: C_{\\text {out }},: C_{\\text {in }}\\right] \\otimes X\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(2)$\nwhere $[:]$ is a slice operation denoted in a Python-like style; $W \\in \\mathbb{R}^{C_{\\text {out }} \\times C_{\\text {in }} \\times K \\times K}$ denote the parameter of a convolutional layer; $C_{i n}, C_{o u t}, C_{o u t}$, and K represent the input channel number, output channel number, selected output channel number, and kernel size.\nSpecifically, the encoder has dynamic layers with four variable dimensions, i.e., resolution, depth, expansion ratio, and kernel size. Each layer in the proposed dynamic encoder contains an efficient, dynamic MobileNet block (MB) based on MobileNetV3 [53]. Each dynamic MB block consists of dynamic pointwise convolution, dynamic depthwise separable convolution, squeeze-and-excite, and dynamic pointwise convolution. Then the decoder employs dynamic convolutional layers where a NAS-based approach is used to regenerate the output from the encoder to match the input resolution stage-wise, as shown in Fig. 1.\nThe dynamic convolution in the proposed supernet enables efficient architecture searching. That is, our supernet can contain subnets with various configurations (e.g. sizes and weight-sharing) that can be trained as a single supernet and optimized for different hardware platforms without retraining and modifying the network's weights. This reduces the computational time needed for architecture search and allows efficient inference across multiple devices."}, {"title": "3.1.2. Architecture space", "content": "The proposed method utilized AlphaNet [50] trained on ImageNet for the encoder part of the FCN network. AlphaNet is based on an efficient neural network design. It uses weight-sharing NAS to build a supernet (which contains many architectures as its subnets) and jointly trains a supernet and its subnets. AlphaNet utilizes an adaptive selection of alpha-divergence to avoid over or underestimating teacher models in a KD framework, inplace KD. We selected over $10^{19}$ subnets from AlphaNet supernet and modified it for the saliency prediction encoder. We utilized the search space dimension of AlphaNet. A summary of our proposed search space dimension is shown in Table 1. We have three blocks: Standard Convolution (Conv), MobileNetV3 convolution (MBConv), and Input resolution. The reason for selecting MBConv block is to maintain consistency with the search space of AlphaNet. Moreover, changing the encoder architecture would require re-training on ImageNet dataet. Each MBConv block has various widths, depths, kernels, and expansion ratios. For instance, the smallest subnet has the lowest computational complexity and parameters (0.51 billion FLOPS and 4.97 million parameters), while the largest subnet with the highest computational complexity and parameters (4.18 billion FLOPS and 19.38 million parameters). Furthermore, the decoder part of the proposed method adaptively adjusts the decoder module according to the equivalent encoder without human intervention."}, {"title": "3.2. Self-knowledge distillation", "content": "Traditional KD methods are challenging due to extensive testing for pairing teacher and student models. Teacherless KD minimizes this by using the student model from the previous epoch or mini-batch as the teacher for the current one, eliminating the need for a separate teacher model. Although it may not always outperform teacher-based methods, it significantly reduces model pairing and training time.\nTo enhance the generalization performance, we train the model in a KD framework, consisting of a teacher model and a student model. Unlike the traditional KD whose optimizer needs parameters from both models, our proposed Self-KD is a teacherless KD. That is, inspired by Stochastic Weight Averaging (SWA) [54], Self-KD improves model generalization by averaging parameters at each epoch, stabilizing training, and improving performance on unseen datasets. Our teacher model is an averaged network whose weights are the average between the best-performing models (local minima) chosen via cross-validation. This results in better generalization-due to the use of validation data as well as lower computation-since our approach does not need the gradient computation for the teacher model. For an input x and the output ($p_{\\text {avg}}$) from the teacher model $S_{\\text {avg}}(x)$ is used to augment the ground truth (gt) as follows:\n$\\mathrm{gt}=p_{\\text {avg}}+\\left(\\mathrm{gt}-p_{\\text {avg}}\\right) \\times \\alpha \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (3)$\nwhere $p_{\\text {avg}}$ represents the augmented information from the averaged network's prediction; alpha ($\\alpha$) weights the belief towards the augmented information versus ground truth. Note that we set $\\alpha=1$ for the first epoch to omit the augment information; after the first epoch, $\\alpha$ can be set to an appropriate value $\\in(0,1)$ for the calculation in Eq. 3.\nDuring the validation phase, the averaged model $S_{\\text {avg}}$ is updated, if the validation loss is lower than the previous one. The update is done by averaging the model parameters with the previously chosen parameters of the best-performing student model S. Otherwise, the parameters of the averaged model $S_{\\text {avg}}$ are retained. This process is illustrated and described in Algorithm 1. Although our approach requires slightly more computation time than the baseline training without distillation, it requires less training time than PKD which involves the same teacher and student network. Additionally, Self-KD involves fewer networks compared to PKD."}, {"title": "3.3. Loss function", "content": "We used three popular saliency prediction metrics to construct the learning loss functions, namely, KLD, CC, and NSS. The KLD and CC correspond to distribution-based metrics for saliency prediction; meanwhile, NSS corresponds to location-based metrics of saliency prediction [24]. Our learning loss function is defined as follows:\n$L_{\\text {train }}=K L D(P, G)+C C(P, G)+N S S(P, F)\\qquad\\qquad(4)$\nwhere the loss formulations related to KLD, CC, and NSS are denoted as KLD(P,G), CC(P,G), and NSS (P, F), respectively. These losses compare the difference between the prediction P and the related ground truth values G, as well as the fixation locations F.\nEmploying evaluation metrics in formulating the loss function is intuitive and is a common practice. Nevertheless, the range of these values is different by definition according to to [24], e.g., KLD(P,G) and CC(P,G) can take the value between 0 and 1, but $N S S(P, F)$ can take any real, negative value. Therefore, we propose to adjust each evaluation metric to be employed as a loss function, described in the following.\nKLD(P, G) loss measures the divergence at each pixel location, it is defined as follows:\n$K L D(P, G)=\\sum G_{i} \\log \\frac{G_{i}}{P_{i}}+\\frac{G_{i}}{P_{i}+\\epsilon}\\qquad\\qquad\\qquad\\qquad\\qquad(5)$\nwhere $G_{i}$ and $P_{i}$ denote the ground truth and the predicted saliency value at $i^{\\text {th }}$ pixel location; $\\epsilon$ is a small constant to prevent numerical instability.\nCC(P, G) loss measures the correlation between the predicted and the ground truth saliency map P and G, is defined as:\n$C C(P, G)=1-\\frac{\\operatorname{covar}(P, G)}{\\sigma(P) \\times \\sigma(G)} \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(6)$\nwhere the covariance between P and G covar(P, G) is normalized by the standard deviations of P and G, denoted as $\\sigma(P)$ and $\\sigma(G)$, respectively.\nNSS (P, F) loss is computed as the average normalized saliency at a fixated location. Our NSS loss is defined as\n$N S S(P, F)=1-\\left[\\frac{1}{N} \\sum\\left(\\hat{P}_{i} \\times F_{i}\\right)\\right] \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(7)$\nwhere $\\hat{P}$ represents the normalized saliency map P with a mean of zero and a standard deviation of one; F denotes the binary map of fixation locations. N denotes the total number of human eye fixations. We scale NSS to the range of 0 to 1 by using a logistic function $\\sigma$. The logistic function $\\sigma$ is defined as:\n$\\sigma(x)=\\frac{1}{1+e^{-x}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(8)$\nThis adjustment makes the metric more usable and interpretable and ensures the loss function is differentiable."}, {"title": "4. Experimental Results", "content": "The baseline models used in our experiments, which can be categorized as compact, deep, and hybrid for a fair comparison, are introduced in Section 4.1. The detailed implementation of our work and the settings during training are described in Section 4.2 and 4.3. Our evaluation is performed across several practical scenarios with the protocols and datasets provided in Section 4.4. The performance analysis against the state-of-the-art saliency prediction is in Section 4.5, while the comparison against state-of-the-art backbones is analyzed in the transfer learning setting Section 4.6. The qualitative analysis is provided in Section 4.7, and the real-time performance is provided in Section 4.8."}, {"title": "4.1. Baseline", "content": "In the following experiments, we divided the baseline models into three categories, for fair comparison: compact, deep, and hybrid models. Compact model category refers to the small and efficient models; deep model category refers to the large-size models, while hybrid model category refers to the group of models with the combination of architectures, e.g., CNN and Vision transformer. These baseline models are used as references in performance analysis and the real-time processing saliency prediction in Section 4.5 and 4.8, respectively.\nIn performance analysis Section 4.5, we employed the following baseline models to compare against state-of-the-art saliency prediction. This selection is driven by the availability of SALICON test set results and source code. The compact model is FastSal [15]; the deep model includes SimpleNet [55], EML-NET [12], MSI-Net [11], SalFBNet [56], DINet [57], SalED [13], DeepGaze II-E [58], ACSalNet [59], FastFixation [60], SalDA [61], and TempSal [62]; the hybrid model includes TranSalNet [63].\nFor the analysis of the performance gained by Self-KD training, we employed the following baseline models, according to the above categorization: the compact model include EEEA-Net-C2 [64], OFA595 [20], MobileNetV3 [53], and the deep model includes EfficientNet-B4 [22] and TResNet-M [65]. We employ a similar selection for the architecture transfer section.\nFor real-time processing performance Section 4.8, state-of-the-art saliency prediction models are considered. The baseline models, which can be put into the above categorization, are as follows: the compact model includes EEEA-Net-C2, OFA595, and EfficientNet-B0, the deep model includes SimpleNet, SalFBNet, EfficientNet-B4, EfficientNet-B7, while the hybrid model includes TranSalNet."}, {"title": "4.2. Implementation details", "content": "Our proposed method is implemented using PyTorch. We utilized AlphaNet search space for the encoder module for training supernet, while the decoder automatically adjusted dimensions according to the encoder. Furthermore, we used mixed precision during supernet training with a maximum input resolution size of 384 \u00d7 288. For hyperparameter configuration, our approach aligns with the setting of AlphaNet. SGD optimizer is utilized with a momentum of 0.9, a learning rate (LR) of 0.1, the LR scheduler is cosine annealing, and batch size is reduced to 64 in accordance with our hardware memory specifications. Supernet is first trained on the SALICON dataset [66] and then fine-tuned on other datasets such as CAT2000 [67], MIT1003 [67], OSIE [68], DUT-OMRON [69], PASCAL-S [70], and FIWI [71]."}, {"title": "4.3. LR scheduler and Combination loss", "content": "We utilized a cosine annealing LR scheduler, the learning rate starts at the initial value and decreases to a minimum value following a cosine annealing schedule. $T_{0}$ is used to recalibrate the learning rate to its highest value, which allows the model to avoid getting stuck in local minima, thus allowing the model to continue learning and improving performance."}, {"title": "4.5. Performance analysis", "content": "We trained our SaINAS on the SALICON training set and submitted the results to the SALICON benchmark, which is a saliency prediction challenge. The goal of the challenge is to evaluate the performance of visual saliency prediction of natural scene images. The SALICON test dataset comprises 5000 images with no ground truths released. Our SalNAS-XL is a subnet with the maximum computational complexity, parameters, and performance from our proposed supernet SalNAS. We compared the result of SalNAS-XL with thirteen recent state-of-the-art saliency prediction models, including SimpleNet, EML-NET, MSI-Net, SalFBNet, FastSal, DINet, SalED, DeepGaze II-E, ACSalNet, FastFixation, SalDA, TempSal, and TranSalNet.\nFrom Table. 5, it can be observed that few saliency models, such as DeepGaze II-E, EML-NET, SalED, TempSal, and TransSalNet, outperformed SalNAS in various metrics: sAUC, NSS, CC, KLD, and SIM. One reason for the enhanced performance of these state-of-the-art models is the utilization of ResNet-50 and DenseNet backbones, which have significantly high model parameters ranging from 23 to 104 million, compared with our SalNAS with 20.98 million parameters."}, {"title": "Analysis of performance gain by Self-KD training.", "content": "We compared Self-KD against the non-distillation (baseline), Pseudoknowledge Distillation (PKD) [14], and PS-KD training [47]. We show the performance across different backbones, i.e., our SalNAS-XL and five other backbones from state-of-the-art saliency prediction-EEEA-Net-C2, OFA595, MobileNetV3, EfficientNet-B4, and TResNet-M. These backbones can be separated into (i) the small-size backbones with the number of parameters < 10M and (ii) the medium-to-large-size backbones with the number of parameters > 20M.\nFrom Table 6, Self-KD outperformed the baseline, PKD, and PS-KD training for both the small-size and the medium-to-large-size backbones, across all metrics, except for NSS. In addition to superior performance, Self-KD requires less training time compared to PKD. Furthermore, compared to the baseline training, Self-KD only requires 10% higher training time and 10-20% higher training time for the small-size and the medium-to-large-size backbones, respectively.\nWe explored other efficient training strategies, for instance, sandwich rule and inplace distillation to validate the performance of the proposed SalNAS method. In the sandwich rule, each mini-batch of data samples smallest subnet, biggest subnet, and two randomly sampled subnet models. The gradient from all sampled subnet models is aggregated before updating the weights of the single-stage model. The inplace distillation involves soft labels generated by the biggest possible subnet model to supervise all other subnet models. Fig. 3 illustrates the performance of the proposed Self-KD method compared to inplace distillation and the sandwich rule methods, with the CC metric considered for evaluation. From the results, it can be observed that the Self-KD method outperforms other efficient training strategies including Inplace distillation and sandwich rule.\nFurthermore, comprehensive hyperparameter tuning experiments are conducted to explore the impact of varying $\\alpha$ values on the model performance across diverse saliency datasets. The results show that the $\\alpha$ value between 0.4 and 0.6 gives good performance across different datasets."}, {"title": "4.6. Architecture transfer", "content": "This section evaluates the performance of the pre-trained backbones after fine-tuning with six different datasets, namely MIT1003, CAT2000, PASCAL-S, DUT-OMRON, OSIE, and FIWI datasets. We compared results with state-of-the-art backbones, as shown in Table 7 and Table 8. All the backbones were pre-trained with the SALICON training dataset. For our work, we used a pre-trained SALICON subnet SalNAS-XL from our proposed supernet and then fine-tuned it. From the table, our SalNAS-XL wins 25 times out of 30 trials (six datasets evaluated over five metrics), confirming the effectiveness of our strategies."}, {"title": "4.7. Qualitative analysis", "content": "We employ the SALICON validation dataset to provide the visualization results, shown in Fig. 4. We compared SalNAS-XL with state-of-the-art saliency prediction models, i.e., TranSalNet, TResNet-M, and state-of-the-art backbone, EfficientNet-B4. For each prediction, a CC score is provided, indicating the performance of each model. A higher CC score shows better alignment of ground truth with the prediction. Our SalNAS-XL provides the precise area of attention with similar magnitudes to the ground truth. We have chosen two sets of sample images. The first set comprises images that the SalNAS-XL model can predict accurately. The second set consists of images for which the SalNAS-XL model's predictions are less accurate than others, as follows: rows 1 to 3 demonstrate the performance of SalNAS-XL against other state-of-the-art. The results show that SalNAS-XL provides saliency maps that closely align with the ground truth; the CC scores further support this observation, indicating that higher CC scores correspond to better prediction. Furthermore, for rows 4 to 6, SalNAS-XL provides a lower CC score compared to other models, indicating a weaker alignment between the predicted saliency map and the ground truth.\nIn addition, a closer look at row 5 shows that the ground truth depicts a single attention peak at the dog's face, while the SalNAS-XL saliency map represents additional peaks. Other models show better alignment with the ground truth. Similarly, in the sample from row 6, the ground truth displays multiple peaks, while SalNAS-XL fails to produce multiple peaks compared to the other models."}, {"title": "4.8. Real-time processing performance", "content": "Table 9 illustrates the real-time processing performance of various state-of-the-art saliency prediction models. All results are obtained using the following hardware specifications 13th Gen Intel(R) Core (TM) i9-13900KF CPU, DDR5 64 GB RAM, and NVIDIA GeForce RTX 4090 GPU. Real-time processing performance is evaluated using seven assessment measures: computational complexity, parameters, model size, carbon emission, power consumption, latency, and throughput. We considered running each image for latency calculation 200 times on an Apple M2 system on a chip (SoC), where each batch size is one and takes the average latency value. To calculate the carbon footprint of our computing infrastructure in Thailand (THA), we monitored the energy utilization of the CPU, GPU, and RAM during our experiments by forwarding 200 times a batch of size 64 to the model. To accurately assess its environmental effect, the resulting data is analyzed using CodeCarbon, a Python package that enables real-time power monitoring and estimates carbon emissions associated with software applications.\nWe evaluated two subnets, namely SalNAS-XS (small-size) and SalNAS-XL (large-size), derived from our proposed supernet SalNAS, as illustrated in Fig. 5, in comparison with state-of-the-art methods. Our SalNAS-XS is compared against three state-of-the-art backbones-EEEA-Net-C2, OFA595, and EfficicnetNet-B0\u2014with input image resolution of 256x192. Our SalNAS-XL is compared against the state-of-the-art backbones-EfficicnetNet-B4 and EfficicnetNet-B7\u2014and state-of-the-art saliency prediction, TranSalNet, with the input resolution of 384\u00d7288. Table 9 shows that SalNAS-XS offers competitive computational costs in computational complexity, carbon emission, and power consumption. Meanwhile, SalNAS-XL outperforms all the state-of-the-art saliency prediction backbone, by a significant margin in the number of FLOPS, latency, power consumption, carbon emission, parameters, model size, and throughput."}, {"title": "4.9. Limitation", "content": "The proposed work has two limitations. Firstly, high computational cost associated with model search across sub-networks. Search strategies such as reinforcement learning and evolutionary algorithms require high computational costs, which consequently affect time and energy efficiency. These algorithms can be improved by optimizing the neural architecture generators to dynamically produce diverse components while keeping the dimensionality of the problem low. This can result in a wider and more cost-effective search. These generators can assist in developing optimal models customized for specific deployment scenarios, consequently improving search efficiency and flexibility. Secondly, Self-KD lacks external supervision, making it difficult to handle complex tasks without a teacher model. It may not be suitable for all models or applications, particularly where high accuracy is required. Traditional KD is recommended in such cases for better student model performance. However, this paper's SalNAS model overcomes this limitation by using weight-sharing, treating the SalNAS-XL model as a teacher for smaller subnets. Thus, Self-KD with NAS is recommended for performance gains."}, {"title": "5. Conclusion", "content": "We proposed a novel supernet SalNAS, a weight-sharing network that includes all candidate architectures by incorporating a dynamic convolution into the encoder-decoder design for saliency prediction. Our proposed SalNAS"}]}