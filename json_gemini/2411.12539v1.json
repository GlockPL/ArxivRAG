{"title": "Predicting Customer Satisfaction by Replicating the Survey Response\nAbstract", "authors": ["Etienne Manderscheid", "Matthias Lee"], "abstract": "For many call centers, customer satisfaction\n(CSAT) is a key performance indicator (KPI).\nHowever, only a fraction of customers take the\nCSAT survey after the call, leading to a biased\nand inaccurate average CSAT value, and missed\nopportunities for coaching, follow-up, and rec-\ntification. Therefore, call centers can benefit\nfrom a model predicting customer satisfaction\non calls where the customer did not complete\nthe survey. Given that CSAT is a closely moni-\ntored KPI, it is critical to minimize any bias in\nthe average predicted CSAT (pCSAT). In this\npaper, we introduce a method such that pre-\ndicted CSAT (pCSAT) scores accurately repli-\ncate the distribution of survey CSAT responses\nfor every call center with sufficient data in a live\nproduction environment. The method can be\napplied to many multiclass classification prob-\nlems to improve the class balance and minimize\nits changes upon model updates.", "sections": [{"title": "1 Introduction", "content": "Many machine learning applications use classifiers\nupdated periodically by developers. Without spe-\ncial control mechanisms, these updates can shift\nthe relative balance of output classes, causing un-\nintended effects. For the case of predicting CSAT,\nwe developed a control mechanism to address this\nissue, taking care to mitigate the risks posed by\nsampling noise. This paper explains our method\nand strategies for handling sampling noise, and\naims to help developers seeking to replicate one or\nmore target class distribution(s).\nCustomer satisfaction (CSAT) is critical for call\ncenter performance assessment, yet often measured\nthrough surveys completed by a small subset of cus-\ntomers-averaging 8% in our dataset. This limited\nresponse rate can skew perceived performance, as\nnon-responding customers' satisfaction remains un-\nknown. Predicting CSAT for all calls, even those\nwithout survey responses, can mitigate this bias\nManderscheid and Lee (2023).\nEnsuring these pCSAT scores do not introduce\nfurther bias is crucial. This paper introduces a\nmethod to more accurately replicate the distribu-\ntion of survey CSAT responses in a live production\nenvironment, addressing limitations identified in\nprior work and providing more accurate metrics for\ncall center performance."}, {"title": "2 Related Work", "content": "Predicting CSAT using machine learning models\nhas gained attention, especially in call center con-\nversations. The challenge is not only predicting\naccurate scores but also ensuring these predictions\nreplicate the true distribution of survey responses.\nThis section reviews relevant studies and method-\nologies applied to similar problems, focusing on\ndistribution replication and ordinal classification\n(since CSAT is measured on a 1-5 scale)."}, {"title": "2.1 Predicting Customer Satisfaction", "content": "Previous research explored various approaches to\npredict CSAT from contact center conversations.\nBockhorst et al. (2017) developed a system using\nASR-generated call transcripts, non-textual data,\nand sentiment scores to predict a Representative\nSatisfaction Index (RSI) with rank scoring and iso-\ntonic regression models. Similarly, Auguste et al.\n(2019) used the Net Promoter Score (NPS) with\nbinary classification (promoters vs. detractors) for\npredicting customer satisfaction in chat conversa-\ntions, achieving moderate improvements with a\nmacro F1 score of 53.8%.\nOther studies examined predicting CSAT from\nraw audio signal features such as acoustic, emo-\ntional, and prosodic features (Park and Gates, 2009;\nZweig et al., 2006; Vaudable and Devillers, 2012;\nDevillers et al., 2010).\nThis work builds on a previously developed\nmethod for predicting CSAT scores using ASR-\ngenerated (Automated Speech Recognition) call"}, {"title": "2.2 Replicating Class Distribution", "content": "Our threshold fitting approach replicates the survey\nCSAT distribution, crucial for maintaining class\nproportions in predictions. Research on maintain-\ning class distribution intersects with imbalanced\nlearning and ordinal regression, using techniques\nlike resampling, re-weighting, and threshold adjust-\nment to handle class imbalances. Model calibration\ncan be a helpful addition to these methods, but is\nnot a replacement, as model calibration focuses\non adjusting predicted probabilities to better re-\nflect true likelihoods, which does not imply that the\nclass distribution will be faithfully replicated if the\ndecision thresholds are incorrect.\nRe-sampling and Re-weighting: These tech-\nniques adjust training processes to account for class\nimbalances, ensuring minority classes are repre-\nsented. However, they are not well suited to repli-\ncating an exact class balance, as the effects of these\ntraining set adjustments are difficult to predict.\nThreshold Optimization in Multiclass and Or-\ndinal Classification: Threshold optimization is\ncritical in contexts requiring precise class predic-\ntions, such as multiclass and ordinal classification\ntasks. Kotsiantis et al. (2006) discuss methods to\nadjust decision thresholds for imbalanced datasets,\nbalancing sensitivity and specificity to represent\nminority classes. Ferri et al., 2002 introduce meth-\nods to optimize decision thresholds to minimize\nmisclassification costs. Their work is relevant in\ncontexts where different misclassifications have dif-\nferent costs, making threshold adjustment crucial.\nWhile these approaches are closely related to ours\nby adjusting model thresholds to reflect true class\ndistributions, they focus on binary and multiclass\nclassification without emphasizing ordinal classes\nas ours does.\nCardoso and da Costa (2007) proposed a data\nreplication method for ordinal classification, han-\ndling ordinal data by replicating instances to indi-\nrectly optimize thresholds for ordinal predictions.\nThis study aligns with our work, emphasizing main-"}, {"title": "3 Data & Methods", "content": "We used conversational transcripts generated from\nour Automatic Speech Recognition engine. The\naccuracy (1 - Word Error Rate) was > 85%."}, {"title": "3.0.1 Transcripts", "content": null}, {"title": "3.0.2 Calls", "content": "We used approximately 892K call center calls with\na CSAT survey score and a model-assigned pCSAT\nprobability ranging from June 24, 2023 to June 17,\n2024."}, {"title": "3.0.3 Trials", "content": "To rule out effects due to chance or periodicity, we\nran the experiment 7 times using different training\nand test periods. The last of those trials corresponds\nto a production deployment of the model, and the\nother trials were simulated for the purposes of this\nanalysis. Each trial consists of a 60 day training\nperiod and 120 day test period. A 30 day period\nseparates the start of one trial from the start of\nthe next (thus trials overlap). We expected and\nobserved no differences between the deployed and\nsimulated trials since the pipeline is the same."}, {"title": "3.0.4 Call Centers", "content": "We excluded call centers with fewer than 5 high\nand 5 low CSAT calls over the 60-day training pe-\nriod to avoid very high sampling noise. To better\nunderstand the impact of sampling noise, we fur-\nther categorized call centers heuristically based on\nthe number of survey CSAT responses in the 60-\nday training period."}, {"title": "3.1 Threshold Optimization Procedure", "content": null}, {"title": "3.1.1 Model and Mapping", "content": "The model is a large language model (LLM) that\npredicts CSAT with binary outputs: high or low\nCSAT. Details on the model and how it was trained\nare provided in Manderscheid and Lee, 2023. The\nmodel also provides the probability of both classes,\nreferred to as \"proba\" for the low CSAT class. The\nmapping function (Figure 1) uses this probability to\noutput a pCSAT score on a 1-5 scale. The mapping\nhas four parameters representing decision thresh-\nolds (i.e. class boundaries): $t_{1,2}, t_{2,3}, t_{3,4}$, and\n$t_{4,5}$. For example, $t_{3,4}$ is the probability threshold\nseparating a pCSAT of 3 from 4."}, {"title": "3.1.2 Product Requirements", "content": "Our approach is based on meeting product require-\nments, ranked by importance:"}, {"title": "3.1.3 Parameter Estimation", "content": "Jointly estimate the four thresholds: To find the\noptimal parameters, our process iterates through\ndifferent combinations of thresholds to find the set\nthat minimizes the loss.\nTo meet all three product requirements, we cre-\nated the following loss function:\n$Loss = \\Delta_{\\%p,c} + \\Delta_{avgp,c} + MSE_{p,c}$\nwhere p and c are short for pCSAT and CSAT, re-\nspectively, and:\n$\\Delta_{\\%p,c} = | (\\% \\text{ of pCSAT > 4})-(\\% \\text{ of CSAT > 4}) |$\n$\\Delta_{avgp,c} = |avg\\_pcsat \u2013 avg\\_csat|$\n$MSE_{p,c} = MSE(pcsat, csat)$\nWe preferred a random search to a grid search\nto save computation time. We used 5000 iterations\nfor random search, but found 98.7% convergence\nby 500 iterations."}, {"title": "3.1.4 Optimization Steps:", "content": "1. Compute the Number of Calls for Each Survey\nCSAT Level:\n$n_{csati} = \\text{number of calls with survey CSAT}$\n$\\text{where class } i \\in (1, 2, 3, 4, 5)$\n2. Calculate the Average Survey CSAT:\n$\\text{avg\\_csat = } \\frac{\\Sigma_{i=1}^{5}(n_{csati} \\cdot i)}{\\Sigma_{i=1}^{5} n_{csati}}$\n3. Initialize the loss:\n$best\\_loss = 1000.0$\n4. Random Search for Optimal Thresholds: Per-\nform a random search through the possible\nthresholds to find the set that minimizes the\nloss. For j in range(5000):\n(a) Generate 4 uniform random values and\nsort them:\n$t_{12} > t_{23} > t_{34} > t_{45} \\sim U(0, 1)$\n(b) Compute the Number of Calls for Each\npCSAT Level:\n$n_{pcsat_{i}} = \\text{number of calls with pCSAT}$\n$\\text{where class } i \\in (1, 2, 3, 4, 5)$\n(c) Calculate the Average pCSAT:\n$\\text{avg\\_pcsat} = \\frac{\\Sigma_{i=1}^{5}(n_{pcsat_{i}} \\cdot i)}{\\Sigma_{i=1}^{5} n_{pcsat_{i}}}$\n(d) Compute the Delta Between Average pC-\nSAT and CSAT:\n$\\Delta_{pcsat\\_csat} = avg\\_pcsat - avg\\_csat$\n(e) Compute $\\Delta_{\\%p,c}$\n(f) Normalize both CSAT Vectors to unit\nlength:\n$pcsat = \\text{normalized}([n_{pcsat_{1}},..., n_{pcsat_{5}}])$\n$csat = \\text{normalized}([n_{csat_{1}},..., n_{csat_{5}}])$\n(g) Calculate the Mean Squared Error Be-\ntween the Normalized Vectors ($MSE_{p,c}$)\n(h) Compute the loss:\n$Loss = \\Delta_{\\%p,c} + \\Delta_{avgp,c} + MSE_{p,c}$\n(i) Update the loss and best thresholds if the\ncurrent loss is lower."}, {"title": "3.2 Experimental Conditions", "content": "We evaluated the loss under five conditions:\n1. Baseline: Naive model output (evaluated over\ntest period)\n2. Global Threshold: Thresholds are fitted on a\nsingle, global pool (evaluated over test period)\n3. Call Center Threshold: Individual threshold-\ning for each call center (evaluated over test\nperiod)\n4. Train Period: We apply the same call center-\nspecific parameters as used in the Call Center\nThreshold condition, but apply them to the\ntraining period instead of the test period. As\nwe expect a near-zero difference of means,\nthis serves to validate our parameter estima-\ntion method.\n5. Bootstrap (Train Period): This approach\nis similar to the \"train\" condition, but the\nkey difference is that we repeatedly resam-\nple the training set and measure the difference\nof means over these samples. This method\nhelps us estimate how much of the loss in the\n\"Call center thresholding\" condition is due to\nsampling noise, and attribute the rest to differ-\nences between train and test distributions, i.e.\nmodel drift.\nWe note that the first 3 conditions are test condi-\ntions, i.e. evaluated on the test set, whereas the\nlast 2 are train conditions which help us understand\nsources of error."}, {"title": "4 Results", "content": "The effectiveness of different methods for predict-\ning customer satisfaction (CSAT) scores was evalu-\nated through various experimental conditions. The\nresults are summarized in Figures 2-5, and detailed\nobservations are as follows:"}, {"title": "4.1 Loss", "content": "Overall loss, depicted in Figure 2, combines the\ndifference of means, difference of percent satisfied,\nand MSE. We see that the Baseline method consis-\ntently has the highest loss, and the Train condition\nhas the lowest. The train condition does not have\n0 error because it is not usually possible to find 4\nparameters to zero all 3 terms that make up the loss\nsimultaneously."}, {"title": "4.3 Difference of Means", "content": "Figure 4 focuses on the average absolute differ-\nence between mean Predicted CSAT and survey\nCSAT. The Baseline method consistently lags other\nmethods, showing our methods create a substan-\ntial improvement. The Global Threshold method\nperforms best at the lowest response volumes (<\n200), whereas the Call Center Threshold method\noutperforms other methods from 200 calls onwards,\nconsistent with its requirement of small sampling\nnoise.\nAs expected, the Train and Bootstrap conditions\nshow very low percentages, further validating the\nparameter estimation and highlighting the minimal\nimpact of sampling noise after 500 calls."}, {"title": "4.4 MSE", "content": "The MSE, shown in 5, measures the vector simi-\nlarity between the pCSAT and CSAT distributions.\nThe Baseline method exhibits by far the highest\nMSE values across all call volumes, so much so\nthe figure requires a logarithmic scale. It exceeds\n0.1 for all survey response volumes. The Global\nThreshold method is consistently best at lowering\nthe MSE, though the gap with Call Center Thresh-\nold narrows as survey responses increase."}, {"title": "5 Discussion", "content": "Some but not all initial targets were achieved:\n\u2022 Difference in Percentage of Satisfied Calls:\nNot achieved. The methods were not able to\nimprove over Baseline or meet the target of\nless than 1%. The fact the fitting thresholds\ndoes not improve the output class distribution\nsuggests that the baseline classifier outputs\nmay be already well distributed. This is a\nlikely explanation because this metric treats\nCSAT as binary (call satisfied if CSAT >= 4)\nand the baseline classifier is trained on binary\nCSAT data. Further work may be required to\nyield improvements.\n\u2022 Difference of Means: The Call Center\nThreshold method consistently achieved the\ntarget (difference of means less than 0.1) for\ncall centers with survey responses greater than\n500-1000. This method also achieved signifi-\ncant improvements over Baseline for all call\ncenter bins.\n\u2022 Mean Squared Error (MSE): Both the\nGlobal Threshold and Call Center Threshold\nmethods significantly improved the MSE over\nBaseline, indicating improved alignment be-\ntween pCSAT and actual CSAT distributions.\nIn this case, there was no quantitative target,\nbut the improvement is over 10X.\nHaving learned from the varying performance\nof the methods across different survey response\nvolumes, we are now considering implementing\na hybrid approach. Specifically, using the Global"}, {"title": "6 Limitations", "content": "While our thresholding method demonstrates sub-\nstantial improvements, several limitations must be\nacknowledged:\n\u2022 Sampling Noise: As highlighted, small call\ncenters with low survey response volumes suf-\nfer from high sampling noise, limiting the ef-\nfectiveness of our approach for sample sizes\nunder 500, especially call center thresholding.\n\u2022 Temporal Stability: Although our method\nshows promise in maintaining low loss over\nat least 4 months, we did not examine the\ntimecourse of errors over those 4 months or\nbeyond. Long-term drift could be a concern\nand warrants further investigation."}, {"title": "7 Ethics Statement", "content": "In developing and implementing this method, we\nhave adhered to ethical standards to ensure fairness,\ntransparency, and accountability:\n\u2022 Bias Mitigation: Previously, we have sam-\npled subpopulations of users and evaluated\ninternally to ensure the pCSAT is not biased\nagainst specific groups. This approach takes\na further step to reduce bias in pCSAT scores\nby ensuring a more accurate reflection of cus-\ntomer satisfaction across different call centers.\nHowever, continuous evaluation and improve-\nment are necessary to address any emergent bi-\nases, and our near-term plans include quantifi-\nable and verifiable explainability for AI CSAT\nwhich will help our users pinpoint the causes\nof low pCSAT, including any bias."}]}