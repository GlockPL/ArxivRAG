{"title": "Online Planning in POMDPs with State-Requests", "authors": ["Rapha\u00ebl Avalos", "Eugenio Bargiacchi", "Ann Now\u00e9", "Diederik M. Roijers", "Frans A. Oliehoek"], "abstract": "In key real-world problems, full state information is sometimes available but only at a high cost, like activating precise yet energy-intensive sensors or consulting humans, thereby compelling the agent to operate under partial observability. For this scenario, we propose AEMS-SR (Anytime Error Minimization Search with State Requests), a principled online planning algorithm tailored for POMDPs with state requests. By representing the search space as a graph instead of a tree, AEMS-SR avoids the exponential growth of the search space originating from state requests. Theoretical analysis demonstrates AEMS-SR's \u025b-optimality, ensuring solution quality, while empirical evaluations illustrate its effectiveness compared with AEMS and POMCP, two SOTA online planning algorithms. AEMS-SR enables efficient planning in domains characterized by partial observability and costly state requests offering practical benefits across various applications.", "sections": [{"title": "1 Introduction", "content": "The Partially Observable Markov Decision Process (POMDP) is a powerful framework that models sequential decision-making in scenarios where the environment's true state is inaccessible. Often, this partial observability is an inherent characteristic of the environment, perhaps due to noise or the unavailability of suitable sensors. However, in numerous instances, determining the true state of the system is feasible but entails a considerable cost. For example, consider a scenario involving a battery-powered robot that lacks the necessary power to employ highly accurate sensors continuously, and thus, is also equipped with power-efficient yet less precise sensors. Additionally, the concept of requesting state information can extend to scenarios with privacy implications, such as the use of surveillance cameras in public spaces for crowd control. In these cases, the decision to activate cameras involves weighing the benefits of state access against potential privacy costs.\nWe can think of these settings as situations where the agent has the option to consult an oracle (like a precise sensor or a human expert) at every step to obtain the state against a cost. In the context of our battery-powered robot, the cost could represent the electricity cost of activating the accurate sensor. We refer to this setting as POMDPs with State Requests (POMDP-SR), where the agent, for a cost, can eliminate all uncertainty regarding its current state before selecting each action.\nA naive approach to handling POMDP-SRs is converting them to equivalent POMDPs, as detailed in Section 3. However, such a method overlooks the unique characteristics of POMDP-SRs, potentially leading to suboptimal performance of conventional POMDP planning techniques. This is largely because in the conversion to an equivalent POMDP, the number of time-steps effectively doubles, and the observation space expands considerably. For methods relying on tree search, such as POMCP (Silver & Veness, 2010) and AEMS (Ross & Chaib-Draa, 2007), the transformation into an equivalent POMDP introduces an exponential increase in the search tree, significantly impeding their efficiency. Extensions that build on sparse samplings, such as DESPOT (Somani et al., 2013), theoretically"}, {"title": "2 Background", "content": "POMDPs Partially Observable Markov Decision Processes (\u00c5str\u00f6m, 1965) are defined as a tuple\nP = \u3008S, \u03a9, \u0391, P, O, R, \u03b3\u3009 where S is the set of states, \u03a9 is the sets of observations, A is the set\nof actions, P: S \u00d7 A \u2192 \u2206s is probability transition function with As being the simplex over the\nstate space, O: S \u00d7 A \u2192 \u0394\u03a9 is the probability observation function, R: S \u00d7 A \u2192 R is the reward\nfunction, and \u03b3\u2208 [0,1) is the discount factor. At each time step the agent selects an action based\non its observation-action history h\u2208 (\u0391\u00b7\u03a9)*. Due to the exponential growth of the history space\nin the number of time-steps dealing with histories might not be practical. Beliefs, defined as the\nprobability distribution over current states b\u2208 B = \u2206s, are sufficient statistics of the history for\ncontrol (\u00c5str\u00f6m, 1965) and a more compact alternative. Beliefs are computed recursively: after\ntaking an action a in belief b and receiving an observation o the next belief is defined for any next\nstate s'e S as follows, with \u03b7 being the normalizing factor.\n$b'(s') = \\eta \\sum_{s\\sim b}P(s' | s, a) \\cdot O(o | s', a)$\nA policy \u03c0: \u0392\u2192 A is a mapping from beliefs to actions and is associated with a Value V (b). We\ndenote as \u03c0* and V* the optimal policy and its value. For finite horizon, V is a piece-wise linear\nand convex (PWLC) function of the belief (Sondik, 1971), and can therefore be represented as a set\n\u0393 of a-vectors which corresponds to the slopes of the PWLC function.\nWe refer to beliefs as corner beliefs when the probability of being in a states is 1 and 0 for the\nother states. In clear contexts, we directly use the state s to reference such beliefs. The support of a\nbelief, supp(b), is the set of states with non-zero probability. POMDP planning methods generally\nfall into two categories: offline and online approaches. Offline methods precompute comprehensive\nplans for all scenarios but suffer from computational demands and scalability issues. In contrast,\nonline methods provide real-time computational capabilities for determining optimal actions within\ntime constraints.\nAEMS Anytime Error Minimization Search (AEMS) (Ross & Chaib-Draa, 2007) is an online al-\ngorithm that, following the stochastic shortest path approach of AO* (Nilsson, 1982), builds a tree\nT from the current belief bo. The algorithm maintains an upper bound Ur(b) and a lower bound\nLT(b) of the value V*(b). At each step, AEMS expands the node that is believed to have the highest\nreduction potential for the error at the root. Let F(T) be the set of fringe nodes (nodes without\nchildren) in T, \u00ea(b) = U(b) \u2013 L(b) be the gap between the upper and lower bounds of the value,\ndy(b, bo) be the number of actions that separate b and bo in the tree T, ho be the history from bo\nto b, and P(hbobo, \u3160\u315c) be the probability of reaching b from bo by following the policy \u3160\u315c that\nselects the action maximizing the upper bound. AEMS expands the fringe node that maximizes the\nheuristic of Eq. 1.\n$\\gamma(T) = arg \\underset{b \\epsilon F(T)}{max} \\gamma^{dy(b,b_0)} P(h_{b_0}^{b}|b_0, \\pi_T) \\hat{e}(b)$"}, {"title": "3 Framework", "content": "POMDP-SR We define the POMDP with State Request as a tuple PSR = <P, c\u3009 where P is a\nPOMDP and c > 0 is the associated cost to request the state. At each timestep, the agent first\ndecides whether to request the state, which is immediately revealed if requested and then selects an\naction. The decision is binary: i to request and i to not request the state.\nA property of POMDP-SR that may not be immediately apparent is that even in cases where the\noptimal actions in an MDP and a POMDP align, the POMDP-SR's optimal action might differ. This\narises from the fact that the agent operates under the anticipation of potential future state requests.\nIn such contexts, a suboptimal action in the MDP and the POMDP can become the optimal one\nin the POMDP-SR when combined with a future request the state, achieving a return that is sub-\noptimal for the MDP but significantly better than the one of a POMDP. An illustrative example\ndemonstrating this aspect of POMDP-SR is elaborated in the Appendix. This highlights how the\nintegration of state requests fundamentally shifts the dynamics of decision-making in POMDPs.\nEquivalent POMDP A POMDP-SR PSR = <P, c> can be transformed into an equivalent POMDP\nP' = <S', \u03a9', A', A', P', O', R'', \u03b3' with variable action space and with P', O', R' only defined over\nlegal actions. While the comprehensive technicalities of this transformation are detailed in the\nAppendix, the core concept is to separate the state request action from the environmental action\ndoubling the number of timesteps. Additionally, the state space is expanded by integrating a binary\nindicator, which functions to signal the phase in which the agent is operating. For any state\nse S, s\u00ba indicates the request the state phase, and s\u00b9 is the environment action phase. We denote\nsimilarly the beliefs containing only states of one type (i.e. 60, 61).\nEquivalent POMDP Complexity Transforming a POMDP-SR into its equivalent POMDP en-\nables the use of classic POMDP planning algorithms, but this approach may prove inefficient. One\ninefficiency arises from the lack of support for variable action spaces in classic implementations,"}, {"title": "4 Online planning: AEMS-SR", "content": "In this section, we present our new method Anytime Error Minimization Search for POMDP-SRS\n(AEMS-SR) which adapts AEMS to our framework. As outlined in section 3, the introduction of\nstate requests leads to an exponential increase in the size of the search tree. This growth is primarily\ndue to two factors: the doubling of timesteps and the generation of a new subtree for each state in\nthe belief's support. Consequently, each expansion of belief in a POMDP-SR, illustrated by the red\nbox in Fig. 1a, adds (1 + |supp(b)|) \u00b7 |A|\u00b7 |\u03a9| nodes. This is in stark contrast to classic POMDPS,\nwhere only |A|\u00b7 |\u03a9| nodes are added per expansion, as illustrated by the blue box in Fig. la.\nUpon examining the search tree in POMDP-SR scenarios, illustrated in Figure la, we observe that\nmany nodes are similar. This redundancy is particularly pronounced in cases involving position\nuncertainty and potential action failure, leading to a significant overlap in subsequent beliefs. As a\nresult, the tree often contains identical subtrees that are redundantly expanded, impairing search\nefficiency. The challenge of repetitive subtree expansions is not unique to POMDP-SR; it is a known\nissue in both POMDPs and MDPs. Techniques like transposition tables (Childs et al., 2008) have\nbeen used to address this problem, offering computational trade-offs that can be beneficial in certain\nenvironments but are less practical for continuous spaces such as beliefs. To deal with these, AEMS-\nSR employs a rooted cyclic graph, denoted as G, with the current belief bo as its root, for the search\nreplacing the conventional tree structure. A rooted cyclic graph is defined as a regular cyclic graph\nwhere every node can be reached from its root, and where the root does not have any parents. This\nshift to a cyclic graph necessitates the development of novel heuristic and algorithmic solutions to\nadeptly manage the added complexities of cyclicity."}, {"title": "4.1 AEMS-Loop", "content": "We first introduce AEMS-Loop, the extension of AEMS to cyclic graphs, and theoretically prove its\ncompleteness and \u025b-optimiality, meaning that the algorithm will always return a solution that is\nE-close to the optimal solution given enough time. Similar to other online tree search algorithms, we\nrely on upper and lower bounds, denoted as U(b) and L(b), of the optimal value function V*(b) that\nare computed offline. These values are propagated in the graph G to the parents using the following\nequations, allowing expansions to reduce the error gap at the root: eg(bo) = U\u00e7(bo) \u2013 Lg (bo).\n$U_G(b,a) = R(b,a) + \\gamma \\sum_{o \\epsilon \\Omega}P(o|b,a) U_G(\\tau(b, a, o))$\n$L_G (b,a) = R(b,a) + \\gamma \\sum_{o \\epsilon \\Omega}P(o|b,a)L_G(\\tau(b, a, o))$\n$U_G(b) =\\begin{cases}\nU(b) & \\text{if } b \\epsilon F(G) \\\\\nmax_a U_G (b, a) & \\text{otherwise}\n\\end{cases}$\n$L_G (b) =\\begin{cases}\nL(b) & \\text{if } b \\epsilon F(G) \\\\\nmax_a L_G (b, a) & \\text{otherwise}\n\\end{cases}$\nWorking with a cyclic graph introduces the possibility of multiple paths, and potentially an infinite\nnumber, between the root bo and any fringe node b\u2208 F(G). We define as I\u00e7 (bo, b) the set of\npaths in G that start on bo and end up on b. A path h\u2208 \u03a6\u03c2 (bo, b) is a sequence of beliefs, action\nand observation (bi, ai, Oi+1)i<T. Based on a policy \u03c0, each path has an associated probability\nP(h/bo, \u03c0) = \u03a0-01P(Oi+1|bi, \u03b1\u03ca)\u03c0(ai|bi) corresponding to the probability of observing the path h\nwhile starting from bo and following \u03c0. Additionally, we define (bo, b) as the sum over all possible\npaths between the root bo and a fringe b \u2208 F(G) of the probability of observing the path discounted"}, {"title": "4.2 Algorithm", "content": "This subsection explains how AEMS-SR (Alg. 1), a practical implementation of AEMS-Loop adapted\nto POMDP-SR, works in practice. While the graph structure enables consolidating identical belief\nnodes to prevent redundant work, fully implementing this strategy would require comparing each\nnew belief against all existing beliefs in the current graph. Such an approach would lead to scalability\nissues similar to those that render graphs less practical in general MDPs and POMDPs. To maintain\ntractability while still achieving our main goal of mitigating the exponential growth in the search\ntree, we restrict the capacity for multiple parents to corner beliefs.\nAEMS-SR starts with a graph G containing only the root belief bo. The algorithm then iterates\nthrough the following steps until the time limit is reached: a) find the fringe belief b(G) \u2208 F(G) that\nmaximizes Eq. 7, this corresponds to Alg. 2; b) update the graph G by expanding the belief b(G), for\nthe request the state action i we reuse the existing corner beliefs creating a graph similar to Fig. 1b;\nc) update the upper and lower bound value to match Eq.2-3."}, {"title": "5 Bounds", "content": "AEMS-SR requires a lower and upper bound L, U for Eq. 2, 3 and 6. Those bounds, computed\noffline, are represented as a set \u0393of |A| a-vectors, one for each action a and denoted as aa. Evaluation\nof the bounds on a belief b is given by maxa\u0454\u0433 <b, \u0430). Typically, the lower bound is derived from\nBlind policies (Hauskrecht, 1997) that consistently select the same action. For the upper bound,\nthe two main algorithms are QMDP (Cassandra et al., 1997) and FIB (Hauskrecht, 2000). As the\nagent retains the option not to request the state, the ability to request the state cannot reduce the\nexpected return but it may potentially increase it. Therefore, ensuring the validity of the upper\nbounds in POMDP-SRs is crucial.\nQ-MDP is constructed by assuming that the uncertainty about the state will disappear after one\nstep and corresponds to solving the underlying MDP. The a-vector aa is the fixed point of Eq. 12,\nand aa(s) corresponds to the Q-Value Q(s, a) of the underlying MDP.\n$\\alpha_a(s) = R(s, a) + \\gamma \\sum_{s' \\epsilon S}P(s'|s, a) \\underset{\\alpha_a \\epsilon \\Gamma}{max} \\alpha_a'(s')$", "subsections": [{"title": "Lemma 3.", "content": "The Q-MDP upper bound of a standard POMDP P is an upper bound for the equivalent\nPOMDP P' of PSR.\nProof. In the underlying MDP M' of the Equivalent POMDP, the optimal policy will avoid state\nrequests due to the penalty and the full observability. Consequently, the optimal policies of both\nMDPs only differ by the inclusion of non-request actions, which carry zero reward and thus do not\naffect the expected return. Hence, for any se S, the value of the optimal policy in Mats matches\nthe one in M' at s\u00ba. Therefore QMDP is an upper bound for the values of PSR."}, {"title": "Fast Informed Bound", "content": "(FIB) considers the partial observability at the next step which provides a\ntighter upper bound compared to Q-MDP. The associated a-vectors are constructed by iteratively\napplying the operator associated to Eq. 13.\n$\\alpha_a(s) = R(s, a) + \\gamma \\sum_{o \\epsilon \\Omega} \\underset{\\alpha_a \\epsilon \\Gamma}{max} \\sum_{s' \\epsilon S}P(s', o |s, a)\\alpha_a' (s')$"}, {"title": "Lemma 4.", "content": "The Fast Informed Bound upper bound of a standard POMDP P is not guaranteed to\nbe an upper bound for PSR's equivalent POMDP P'.\nProof. Consider a POMDP P with two states, a uniform probability transition function, a unique\nobservation, and two actions such that R(81,a\u2081) = R(82,A2) = 1, R(81,a2) = R(82,41) = \u22121.\nQ-MDP returns (12 + 1, 12 \u2013 1) and (12 \u2013 1,12 + 1) as a-vectors, which correspond to the\nuncertainty of the initial state +1 and observing to the following states. Conversely, FIB returns\n(1,-1) and (-1,1) as a-vectors corresponding to the first reward followed by an expected future\nreturn of 0. Let us now consider the POMDP-SR PSR = <P, c\u3009 and the policy that always request\nthe state to then select the optimal action. This policy has an expected discounted return equal to\n1.Setting the cost c to 0.1 proves that FIB is not an upper bound for POMDP-SR."}, {"title": "FIB-SR", "content": "Adapting FIB to POMDP-SR involves introducing an additional a-vector, ac, corre-\nsponding to the action of requesting the state. FIB-SR alternates between updating a-vectors for\nenvironmental actions using Eq. 13 with \u0393 = {aa, da \u2208 A} ac and updating ac using Eq. 14. This\nprocess reflects paying the cost c to observe the state and then selecting the environmental action.\n$\\alpha_c(s) = -c + \\underset{a \\epsilon A}{max} \\alpha_a(s)$"}, {"title": "Improving the bounds during learning", "content": "In traditional POMDPs, maintaining offline-computed bounds unchanged during online phases is\nstandard practice. While updating these bounds could enhance the algorithm's efficiency over suc-\ncessive time steps and episodes, this approach is generally not feasible. The primary obstacle is the\nneed to store additional alpha vectors, which would diminish the efficiency of computing bounds\nfor new beliefs and increase memory demands. Conversely, in POMDP-SRS, corner beliefs present\nan opportunity to update bounds efficiently. For every corner belief s in the graph G and action a,\nU\u00e7(s, a) provides a tighter bound than aa(s) computed offline. Therefore, by replacing aa(s) with\nthe value of U\u00e7(s, a), we can update the upper bound without requiring additional memory. The\nsame approach applies to lower bounds, resulting in a practical and efficient solution."}]}, {"title": "6 Experiments", "content": "Many existing POMDP benchmarks, like RockSample (Smith & Simmons, 2004), feature partial\nobservability that can be permanently eliminated with a single state request, thereby rendering the\nproblem trivial. We evaluate AEMS-SR on Tag, where partial observability is restored at the next\ntimestep, and on RobotDelivery, a new benchmark tailored to POMDP-SR.\nRobot Delivery is a grid-world environment (Fig. 2) featuring a main room (width 3, length 2n+1)\nwith, at the top, n corridors, each two units long and leading to a package pickup point. At the\nbeginning of each episode, the agent starts at (A) and a package is in one of the pickup-points, with\nequal probability. Its mission is to collect the package and deliver it to point (D), receiving a reward\nof 1 for each successful delivery. After each delivery, there is a probability e that no new packages\nwill spawn. Package spawning occurs with probability 1 - t into the waiting area (W) and with\nprobability t in one of the pickup points. If a package is in the waiting area, it has a probability t\nof being transferred to a pickup point. The waiting area forces the agent to time its state request as\nrequesting the state when the package is in (W) would force the agent to request it again. The agent"}, {"title": "7 Related Work", "content": "Heuristic search Other heuristic search algorithms are notable in the realm of online planning for\nPOMDPs. The approach by Satia & Lave (1973) employs a branch and bound strategy and utilizes\na heuristic similar to AEMS, wherein fringe beliefs are weighted by their likelihood of observation.\nA key distinction, however, is that all non-dominated actions are deemed equally probable. The\nBI-POMDP algorithm (Washington, 1997) aligns more closely with AO*, and therefore AEMS(-\nSR), focusing only on fringe nodes accessible with a greedy policy which selects the action that\nmaximizes the upper bound, akin to our Equation 5. Unlike AEMS, BI-POMDP does not impose\nadditional weighting on the probability of reaching a particular fringe node and instead prioritizes\nnode expansion based on maximizing the error gap. In this work, we decided to extend over AEMS\nbecause it was shown to be more efficient (Ross et al., 2008). However, our approach is not limited\nto AEMS and could be applied to other heuristic search algorithms.\nState requests in POMDPs have seen growing research interest. Bellinger et al. (2021) developed\nthe AMRL framework, where agents incur a cost to request the next state. This framework, unlike\nour POMDP-SR, delays state access and doubles the action space instead of separating the two\ndecision steps. Their AMRL-Q algorithm, based on Q-learning (Watkins & Dayan, 1992), focuses\non state-conditioned policies rather than histories or beliefs, making it sub-optimal. ACNO-MDPS\n(Nam et al., 2021) differ from AMRL by not providing observations without state requests, thus\nsimplifying belief updates. They propose two RL methods: 'observe-before-planning', combining\ninitial MDP learning with subsequent POMCP application, and 'observe-while-planning', where\nPOMCP or DVRL (Igl et al., 2018) in its deep learning variant, make decisions on state requests\nand environmental actions. Krale et al. (2023) further investigate ACNO-MDPs, focusing on timing\nstate requests through heuristics. While these approaches offer valuable insights, they contrast with\nour methodology of planning with a pre-known model and striving for an e-optimal solution."}, {"title": "8 Conclusion", "content": "To address environments where the agent can obtain full state information before each action at\na cost, we introduce the POMDP with State Requests framework. Within this framework, we\npresent AEMS-SR, a principled algorithm that effectively tackles the exponential growth challenge\nin POMDP-SR tree-based search by employing a cyclic graph structure. Our theoretical analysis\nproved that AEMS-SR is complete and \u025b-optimal. Empirical evaluation in RobotDelivery a novel\nbenchmark designed for POMDP-SR and Tag demonstrates AEMS-SR's superior performance\ncompared to established algorithms, AEMS and POMCP, in POMDP-SR settings. In future work,\nwe aim to develop policies ng tuned to the specificity of POMDP-SR, and we plan to investigate\nthe potential application of AEMS-Loop to other subclasses of POMDPs."}, {"title": "A Equivalent POMDP", "content": "In this section, we explain the technicalities of the transformation of a POMDP-SR PSR = <P,c>\ninto an equivalent POMDP P' = <S',N', \u0391', \u0391', P', O', R', \u03b3'\u3009 with variable action space and with\nP', O', R' defined only over legal actions.\n\u2022 The state space S' = {0,1} \u00d7 S augments the original state space with a binary indicator i,\nwhich equals 0 when the agent needs to decide whether to request the state and 1 for selecting\nthe environmental action. To ease notations, we add the binary indicator in sup-script s\u00b2.\n\u2022 The observations space \u03a9' = Nu S\u222a {o*}; o* is a special observation associated with not\nrequesting the state.\n\u2022 The action space A' = {\u012b, 1} \u222a A includes additional actions: 1 for requesting the state, and\n\u012b not to request it. The set of legal actions is returned by A which is defined as follows\nA(s) = {\u012b, 1}, A(s\u00b9) = A. We note that, since at each time step the states in the support\nof the belief have always the same binary indicator i, A can be extended to beliefs.\n\u2022 The transition function P' is defined for all s, s' \u2208 S as follows, P\u2032(s'|s\u00b9,a) = P(s'|s, a)\nand P' (s'\u00b9|s\u00ba, a) = 1s(s'), P\u2032 (s'i|s\u00b2, a) = 0, with 1 the indicator function,\n\u2022 The observation function O' : S \u00d7 A \u2192 \u2206\uff61 is defined for legal actions as follows, O' (s'\u00ba, a) =\nO(s', a), O' (s'\u00b9,\u0131) is a dirac distribution centered in s', and O'(s'\u00b9,\u012b)is a dirac distribution\ncentered in o*\n\u2022 The reward function R' is defined for legal actions as follows: R'(s\u00ba,a) = \u22121,(a)c/\u221a\u221a7,\nR'(s\u00b9, a) = R(s, a)\n\u2022 ' = \u221a7"}, {"title": "B Example of Optimal Action Divergence between MDP, POMDP and POMDP-SR", "content": "As discussed in the Framework section, the optimal action in a POMDP-SR scenario may deviate\nfrom that in an MDP or POMDP, even when their optimal actions are aligned. This divergence\nis attributed to the capability of requesting state information in future steps. To illustrate this\nconcept, we present a toy environment, depicted in Figure 3, where such a divergence occurs.\nIn our example environment, states producing the same observations are enclosed within dashed\nboxes. Initially, at state so, both MDP and POMDP strategies suggest executing action 1, which\nleads to expected returns of 10 and 5, respectively. However, in the POMDP-SR, the calculation"}, {"title": "C Notations", "content": "We starts by restating some definitions and notations.\n\u2022 We denote trees as T and graphs as G.\n\u2022 F(G) is the set of nodes in graph G that does not have any children. We use the same\nnotation for trees F(T).\n\u2022 Corner beliefs corresponds to beliefs where one of the state has probability 1. We use the\nnotation s to refer both to the state and the associated corner belief.\n\u2022 \u0424g (bo, b) is the set of paths in the graph G that starts on bo and finishes on b.\n\u2022 Paths h are sequences of beliefs, action and observations (bi,ai, Oi+1)i<T with T = d(h)\nbeing its length. We write paths that start in b\u2081 and ends in b\u2082 as hb2.\n\u2022 P(h|bo, \u03c0) = \u03a0=0\u00b9P(0i+1|bi, \u03b1\u03ca)\u03c0(ai|bz) corresponds to the probability of observing the path\nh while starting at the root belief bo and following the policy \u03c0.\n\u2022 \u03a8(bo, b) = \u03a3\u03b5\u03a6\u03c2 (bo,b) yd(h) P(h/bo, \u03c0) corresponds to the sum over all possible paths be-\ntween the root belief bo and a belief b of the probability of observing the path discounted\nby its length.\n\u2022 For any belief be Band lower bound L, we defined the error gap as e(b) = V*(b) \u2013 L(b)\n\u2022 For any belief be B, lower bound L and upper bound U, we defined the approximate error\ngap as \u00ea(b) = U(b) \u2013 L(b) \u2265 e(b)"}, {"title": "D Proofs", "content": "Theorem 5. In any rooted graph G with root bo where values are computed according to Equation\n3 using a lower bound value function L, bounded bellow, with error e(b) = V*(b) \u2013 L(b), the error\non the root belief state is bounded by: eg(bo) = V*(bo) \u2013 LG(bo) \u2264 \u03a3beF(G) \u03a8\u03c0* (bo, b)e(b). where\ne(b) = V*(b) \u2013 L(b).\nProof. Consider an arbitrary node b\u2208 G\\F(G) in a graph G that is not a fringe, and a\narg maxa Q* (b, a) the optimal action. By definition \u03b3\u2208 [0, 1). If y = 0, then L(b) = V*(b) = r(b,a)\nand eg(b) = 0 which conclude the proof. Therefore let us focus on \u03b3\u03b5 (0,1). By definition of Lg we\nhave L\u00e7(b,a) < Lg(b)."}, {"title": "E Algorithm to compute \u03a8 and bo", "content": "Algorithm 3 presents the pseudo code to compute\nand . The algorithm recursively traverse\nthe graph by following ng and by maintaining two sets, one for the visited corner beliefs and one for\nthe fringe beliefs."}, {"title": "F Experiment Supplementary Details", "content": "POMCP Our implementation of POMCP is an adaptation of the one provided by https://github.\ncom/Svalorzen/AI-Toolbox to handle the variable action space of the extended POMDP. This\nallows us to avoid resorting to deterrent penalties for illegal actions, which can hinder learning.\nAEMS Our implementation of AEMS is also tailored for the POMDP-SR structure. This prevents\nthe need to double the state space, as done in the extended POMDP formulation, ensuring a fair\nevaluation as doubling the state space would slow the belief update computation.\nTag The original Tag environment has 870 states, while our implementation has 842. The difference\narises from the number of terminal states; in the original implementation, they differentiate based on\nwhich tile the prey was successfully tagged, while we reduced them to a unique state. The evaluation\nis conducted by performing 400 runs on 10 initial states (for a total of 4000 runs). The initial states\nare kept identical for all algorithms."}, {"title": "G Additional experiments", "content": "Table 3 shows results for the same RobotDelivery instances as in the main paper, but using the\nQ-MDP upper bound. Generally, both upper bounds yield similar average results. A notable\nexception is observed in R-3-1, where AEMS with improved bounds achieves an average return\nof 1.04, suggesting a shift away from the exit-directly strategy in certain instances. This further\nunderscores the benefit of improving bounds during the online phase.\nTable 4 presents the results for a modified RobotDelivery version with a movement failure probability\nf = 0.2, also using the Q-MDP upper bound for ease of comparison. AEMS's performance remains\nunaffected by this change, with variations in return due to the longer expected exit time. AEMS-SR\nshows lower returns compared to the f = 0.1 scenario. This outcome was expected given that the\nincreasing f results in expending the support of the beliefs and on increasing the necessary time to\npickup and deliver packages. Nonetheless, AEMS-SR still outperforms AEMS and POMCP, both\nconsistently adopting the exit-immediately strategy.\nTable 5 details results for the RobotDelivery environment with a state request cost of c = 0.25,\nmaintaining other parameters as in the main paper. Given our use of a greedy policy to approximate\n\u03c0*, increasing the cost requires a more significant reduction in the no-request action's upper bound\nfor AEMS-SR to consider state requests. This makes the problem more difficult. In contrast to\nPOMCP and AEMS, which consistently opt for immediate exits, AEMS-SR still manages to deliver\npackages in R-3-1 without bounds improvement and in all R-3 instances if updating the offline\nbounds."}]}