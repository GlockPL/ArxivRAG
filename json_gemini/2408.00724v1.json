{"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "authors": ["Yangzhen Wu", "Zhiqing Sun", "Shanda Li", "Sean Welleck", "Yiming Yang"], "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth. We study compute-optimal inference: designing models and inference strategies that optimally trade off additional inference-time compute for improved performance. As a first step towards understanding and designing compute-optimal inference methods, we assessed the effectiveness and computational efficiency of multiple inference strategies such as Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their variants on two different Tree Search algorithms, involving different model sizes and computational budgets. We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off. These results highlight the potential benefits of deploying smaller models equipped with more sophisticated decoding algorithms in budget-constrained scenarios, e.g., on end-devices, to enhance problem-solving accuracy. For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on MATH500 while using 2\u00d7 less FLOPs. Our findings could potentially apply to any generation task with a well-defined measure of success.", "sections": [{"title": "1. Introduction", "content": "Scaling laws of neural networks [Hestness et al., 2017, Rosenfeld et al., 2019] have been established across a range of domains, including language modeling [Kaplan et al., 2020, Hoffmann et al., 2022, OpenAI, 2023], image modeling [Henighan et al., 2020, Yu et al., 2022, Peebles and Xie, 2023], video modeling [Brooks et al., 2024], reward modeling [Gao et al., 2023], and board games [Jones, 2021]. These studies have demonstrated how model performance is influenced by both the size of the model and the amount of training computation. However, there is limited knowledge on how varying the compute during inference affects model performance after the model has been trained.\nTo improve the task performance of large language models (LLMs), inference techniques typically involve additional computation as a performance maximization step at inference time [Nye et al., 2021, Wei et al., 2022, Wang et al., 2022b, Yao et al., 2023, Chen et al., 2024b]. This cost must be taken into account for compute-optimal inference. For example, a Monte Carlo Tree Search (MCTS) method [Jones, 2021] may improve task performance, but potentially require much more compute than simply sampling solutions multiple times. Generally speaking, we need a comprehensive understanding of how various inference-time methods (e.g., Best-of-N, Majority Voting) trade off between performance and cost. To improve our understanding, this paper presents a thorough empirical evaluation with careful analysis over various configurations of representative LLMs and inference algorithms.\nSpecifically, we explore how to select an optimal size for the language model and an effective inference strategy (e.g., Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their Tree Search variants) to maximize performance (i.e., accuracy) with a given compute budget. We control the inference computation (FLOPs) of a fixed model by utilizing more tokens through the language"}, {"title": "2. Related Works", "content": "Mathematical Reasoning with LLMs. Large language models have made significant progress in recent years, and have exhibited strong reasoning abilities [Brown et al., 2020, Hoffmann et al., 2022, Chowdhery et al., 2022, Lewkowycz et al., 2022]. Mathematical problem solving is a key task for measuring LLM reasoning abilities [Cobbe et al., 2021a, Hendrycks et al., 2021b]. [Ling et al., 2017] first developed the method of producing step by step solutions that lead to the final answer. Later, [Cobbe et al., 2021b] extended the work by finetuning the pre-trained language model on a large dataset to solve math word problems, a verifier is trained for evaluating solutions and ranking solutions. Nye et al. [2021] train models to use a scratchpad and improve their performance on algorithmic tasks. Wei et al. [2022] demonstrate that the reasoning ability of a language model can be elicited through the prompting. Subsequent research [Kojima et al., 2022, Lewkowycz et al., 2022, Zhou et al., 2022] in reasoning tasks has also highlighted the efficacy of rationale augmentation. We choose problem solving in mathematics as the task to study the compute-optimal strategy since it allows us to accurately evaluate the problem solving ability of LLMs.\nInference Strategies of LLM Problem Solving. A variety of inference (also called decoding) strategies have been developed to generate sequences with a trained model. Deterministic methods such as greedy decoding and beam search [Teller, 2000, Graves, 2012] find highly probable sequences, often yielding high quality results but without diversity. Sampling algorithms (e.g., temperature sampling [Ackley et al., 1985]) can produce a diverse set of results which are then aggregated to achieve higher accuracy (e.g., via Majority Voting [Wang et al., 2022a]). Recent methods combine search algorithms with modern LLMs, including breadth-first or depth-first search [Yao et al., 2023], Monte-Carlo Tree Search (MCTS) [Zhang et al., 2023, Zhou et al., 2023, Liu et al., 2024, Choi et al., 2023], and Self-evaluation Guided Beam Search [Xie et al., 2023]. All of these methods show that using search at inference time can lead to performance gains in various tasks.. However, the trade-off for the improved performance is the use of compute to perform the search. Analyzing the trade-off between compute budget and LLM inference performance remains understudied. In this paper, we systematically analyze the trade-off between compute budget and problem-solving performance, and propose a tree search method that is empirically Pareto-optimal. .\nProcess Reward Models. Process reward models (PRMs) have emerged as a technique to improve the reasoning and problem-solving capabilities of LLMs. These models assign rewards to the intermediate steps of the LLM generated sequences. PRMs have been shown effective in selecting reasoning traces with a low error rate, and for providing rewards in reinforcement learning-style algorithms [Uesato et al., 2022, Polu and Sutskever, 2020, Gudibande et al., 2023]. Ma et al. [2023] applies the PRM to give rewards on the intermediate steps and guide the multi-step reasoning process. The PRM can be either trained on human labeled data [Lightman et al., 2023a] or model-labeled synthetic data [Wang et al., 2023]. In our work, we use the PRM as the reward model for selecting generated solutions, and for selecting which partial solutions to explore in tree search."}, {"title": "3. Compute-Optimal Inference for Problem-Solving", "content": "3.1. Problem Formulation\nWe explore the following question: Given a fixed FLOPs budget, how should one select an optimal model size for the policy model, and an effective inference strategy to maximize performance (i.e., accuracy)? We are the first to formulate this problem and study the inference time scaling law, setting our work apart from previous scaling law studies (Fig. 2).\nTo address this, we represent the problem-solving error rate $E(N,T; S)$ as a function of the number of model parameters $N$, the number of generated tokens $T$ and the inference strategy $S$. The"}, {"title": "3.2. Inference Strategies", "content": "3.2.1. Sampling-based Methods\nWe consider the following sampling-based inference strategies which are popularly used in practice:\n\u2022 Greedy Search. This strategy generates tokens one at a time by selecting the highest probability token at each step, without considering future steps. It is computationally efficient but often suboptimal in terms of diversity.\n\u2022 Best-of-N. This strategy, also known as rejection sampling, samples multiple solutions and chooses the one with the highest score given by the reward model.\n\u2022 Majority Voting. In this strategy, multiple model outputs are generated, and the final answer to the problem is determined by the most frequently occurring answer in all the outputs.\n\u2022 Weighted Majority Voting. This strategy is a variant of Majority Voting in which the votes are weighted based on the scores given by the reward model.\nTheoretical Analysis. Before diving into more sophisticated inference algorithms (e.g., tree search), we present theoretical results on the asymptotic behavior of voting-based strategies given infinite compute in Theorems 1 & 2. Informally, we show that the accuracy of standard/Weighted Majority"}, {"title": "3.2.2. Monte Carlo Tree Search (MCTS)", "content": "Monte Carlo Tree Search (MCTS) has proven effective in domains such as board games where strategic decision-making is required [Silver et al., 2016, 2017, Jones, 2021]. Recent work has shown that adapting MCTS to the context of LLMs can enhance the text generation process [Zhang et al., 2023, Zhou et al., 2023, Liu et al., 2024, Choi et al., 2023, Chen et al., 2024a, Tian et al., 2024, Chen et al., 2024a]. In this context, MCTS is often paired with a value model to score and guide the exploration steps. For additional background, we provide a review of MCTS in Appendix B.\nRecent work in MCTS or its variants (e.g., Tree of Thoughts [Yao et al., 2023]) mainly focus on improving the performance (e.g., accuracy) on the studied tasks. However, generic comparisons of MCTS with conventional methods like Best-of-N and Majority Voting in terms of computational budget, measured in generated tokens or processing time, are either scarce or indicating inference-time issues. For example, MCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods. Specifically, a significant portion of the paths in the search tree are used to estimate and select nodes, and these paths do not necessarily become a part of the final candidate solution, although MCTS ensures that the sampled solutions comprise high-quality intermediate steps. In contrast, sampling methods generate multiple solutions in parallel and independently, and all the generated sequences are included in the candidate solutions. However, the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor steps or exploiting promising ones.\nThis highlights the need for developing a new tree search method that can achieve a comparable (or better) performance as MCTS, and that is computationally less costly, just like Weighted Majority Voting and best-of-N. This need motivates the development of our new method named Reward Balanced Search (REBASE), as is introduced next."}, {"title": "3.2.3. Reward Balanced Search (REBASE)", "content": "The REBASE tree search method, illustrated in Fig. 3, inherits the exploitation and pruning properties of tree search, while using the reward model alone to estimate the nodes' qualities without additional computation for estimating values by sampling children. The efficiency is achieved by constraining the total expansion width of the tree at a certain depth. REBASE balances the expansion width among the nodes at the same depth based on the rewards given by the Process Reward Model (PRM). The details are provided below:\nNotations. We view the fine-tuned LLM as a policy $\u03c0_\u03b8$ which generates the solution step by step. Given a question $x$ and the first $k$ steps of a solution $r_1\u00b7\u00b7\u00b7r_k$, the $(k + 1)$-th step is sampled from $\u03c0_\u03b8(\u00b7|xr_1\u00b7\u00b7\u00b7r_k)$. REBASE effectively generates a solution tree during inference, in which the root node the question $x$ and other nodes corresponds to solution steps. When generating solution trees, we generate children of $r_k$ by sampling from $\u03c0_\u03b8(\u00b7|xr_1\u00b7\u00b7\u00b7r_k)$. Here we slightly abuse notations and use the corresponding question/solution step to denote a node. The reward of a node $r_k$ is generated by the PRM: $R(r_k) := R(qr_1\u00b7\u2026\u2026r_k)$.\nInitialization. Given the question $x$, balance temperature $T_\u266d > 0$, and sampling number of solutions $N$, we sample $N$ instances of the first step for the question, yielding all the nodes of depth 1 in the search tree. We let the sampling budget of depth 0, $B_o$, to $N$ at initialization.\nReward modeling and update. In the $i$-th iteration, the PRM assigns the rewards to all the nodes at depth $i$. After that, the algorithm examines whether the solutions up to depth $i$ are complete. Supposing there are $C_i$ completed solutions, we update the sampling budget using $B_i \u2190 B_{i\u22121} \u2212 C_i$. If $B_i = 0$, the process ends, and we obtain $N$ solutions.\nExploration balancing and expansion. For all of the nodes $n_j$ with reward $R(n_j)$ in the depth $i$ of"}, {"title": "4. Experiments", "content": "4.1. Setup\nDatasets. We conduct experiments on two mathematical problem-solving datasets to investigate the scaling effects of computation and our REBASE method for both challenging and simpler problems. Specifically, MATH [Hendrycks et al., 2021a] and GSM8K [Cobbe et al., 2021b] are datasets containing high school mathematics competition-level problems and grade-school level mathematical reasoning problems, respectively. Following [Lightman et al., 2023b, Wang et al., 2024, Sun et al., 2024], we use the MATH500 subset as our test set.\nPolicy model (generators). To study the inference compute scaling effect, we choose Pythia [Biderman et al., 2023] as our base models since various model sizes are available in the Pythia family. For tree search, we use math-specialized Llemma models [Azerbayev et al., 2024]. We further finetune these models on the MetaMath dataset [Yu et al., 2024] using full parameter supervised fine-tuning (Full-SFT), The detailed finetuning configuration is given in the Appendix. Additionally, we test the Mistral-7B [Jiang et al., 2023] to expand our findings across different models and architectures.\nReward Model. All of the experiments use the same Llemma-34B reward model, which we finetuned on the synthetic process reward modeling dataset, Math-Shepherd [Wang et al., 2024]. We added a reward head to the model, enabling it to output a scalar reward at the end of each step.\nInference Configuration. We use sampling and tree search methods to generate multiple samples and select the answer through Best-of-N, Majority Voting, or Weighted Voting. Each configuration has been run multiple times to calculate the mean and variance, thereby mitigating the randomness and ensuring the reliability of our conclusions.\n4.2. Main Results of Compute-Optimal Inference\nIn order to compare the compute budgets of different models, we plot the figures with the number of FLOPs used per question during inference. We compute the inference FLOPs based on the standard formula from [Kaplan et al., 2020]."}, {"title": "Scaling law of compute-optimal inference", "content": "The scaling effect of inference computation budget across different model sizes is shown in Fig. 1. We note that the error rate first decreases steadily and then starts to saturate. Where smaller model first takes advantage since it can generate more samples given limited budget, larger models are preferable with more FLOPs due to the saturation of small model performances. As highlighted in the right panel of Fig. 1, the optimal model size can be different under various computation budgets. We performed a regression analysis on inference FLOPs $C$ and corresponding model sizes $N$ to establish a relationship between a given computational budget and its optimal model size. The resulting regression equation, $log_{10}(C) = 1.19log_{10}(N) + 2.03$, enables us to estimate the optimal inference model size for a specified computational constraint."}, {"title": "Llemma-7B model achieves competitive accuracy to Llemma-34B model with lower compute budget", "content": "Fig. 4 & 5 show the curves of error rates versus total number of inference FLOPs per question. Inference methods with different model sizes are plotted in the same diagram. We found that Llemma-7B costs approximately 2\u00d7 less total FLOPs than Llemma-34B under the same method (Sampling, MCTS, REBASE) and task (MATH, GSM8K) while achieving competitive accuracy. This result suggests that, with the same training dataset and model family, training and inference with a smaller model could be more favorable in terms of compute budget if multiple sampling or search methods are employed."}, {"title": "4.3. Comparing REBASE to Other Baselines", "content": "REBASE is Pareto-optimal. While MCTS undeperforms Sampling (Fig. 4), from Fig. 4, 5, 6, and 7, we found that REBASE consistently outperforms the Sampling method in all settings, when fixing the model and the evaluation task. Table 1 shows that REBASE can achieve competitive accuracy with even a lower compute budget than the sampling method. This finding is novel, and differs from previous tree search works which typically improve the performance at the cost of higher computational expense compared to sampling [Chen et al., 2024a, Xie et al., 2023]. Table 2 shows that given the same compute budget (sampling 32 solutions for the 7B model and 8 solutions for 34B model), using REBASE yields higher accuray than sampling.\nWeaker models gain more from Tree Search. For example, our proposed REBASE leads to 5.3%, 3.3%, and 2.6% performance gains on MATH for Mistral-7B, Llemma-7B, Llemma-34B, respectively. The order of accuracy increase is inversely related to the model's corresponding greedy search on"}, {"title": "5. Conclusions & Limitations", "content": "Conclusions. In this work, we conducted a comprehensive empirical analysis of compute-optimal inference for problem-solving with language models. We examined the scaling effect of computation during inference across different model sizes and found that while increased computation generally leads to higher performance, the optimal model size varies with the available compute budget. When the computation budget is limited, smaller models can be preferable. Specifically, a small language model can achieve competitive accuracy to a model four times larger while using approximately half the total FLOPs under the same inference methods (Sampling, MCTS, REBASE) and tasks (MATH, GSM8K). Additionally, we introduce our novel tree search method, REBASE, which is more compute-optimal than both sampling methods and Monte Carlo Tree Search (MCTS). REBASE typically achieves higher accuracy while using several times less computation than sampling methods. Our results underscore the potential of deploying smaller models equipped with sophisticated inference strategies like REBASE to enhance problem-solving accuracy while maintaining computational efficiency.\nLimitations. Our experiments specifically targeted mathematical problem-solving tasks. Additionally, our models were primarily trained using the MetaMath dataset. Investigating the impact of different training datasets on the performance and efficiency of compute-optimal inference strategies for mathematical problem-solving would be a valuable direction for future research."}, {"title": "A. Omitted Proofs", "content": "A.1. Proof of Theorem 1\nProof. Recall that we assume the answer must be shorter than L tokens. Let $A = {v | |v| < L}$ be the set of all possible answers. Let $\u03c0(y | x)$ be the probability of the language model $\u03c0$ outputting the answer $y$ to the question $x$ after marginalizing over the \u201creasoning paths\u201d, i.e.,\n$\u03c0(y | x) = \\sum_{r\u2208V*}\u03c0(rey|x)$.\nGiven an input x, Assume that $y* = arg \\max_{y\u2208A}\u03c0(y|x)$, $y' = arg \\max_{y\u2208A\\{y*}}\u03c0(y|x)$, and denote\n$\u03b4 = \u03c0(y*|x) \u2013 \u03c0(y'|x)$.\nFor any y, denote by $f_n(y)$ the number of times that the model answers y in the first n samples. Let $E_n$ be the event that Majority Voting with n samples does not output $y*$. We note that $E_n$ happens only if there exists y\" such that $f_n(y\") \u2265 f_n(y*)$. Therefore, by union bound,\n$P(E_n) \u2264P(\u2203 y\" \u2208 A\\{y*}, f_n(y\") \u2265 f_n(y*))$\n$\u2264 \\sum_{y\"\u2208A\\{y*}}P(f_n(y\") \u2265 f_n(y*))$\n$<|A|P(f_n(y') \u2265 f_n(y*))$\nNote that $f_n(y*) \u2013 f_n(y')$ can be viewed as a sum of n i.i.d. random variables, which take value 1 with probability $\u03c0(y*|x)$, \u22121 with probability $\u03c0(y'|x)$, and 0 otherwise. Thus, their expectations are all $\u03b4 = \u03c0(y*|x) \u2013 \u03c0(y'|x)$. By Hoeffding's inequality, we have\n$P(f_n(y') \u2265 f_n(y*)) \u2264 exp(-\\frac{n\u03b4^2}{2})$\nThus,\n$P(E_n) \u2264 |A| exp(-\\frac{n\u03b4^2}{2})$\nBy Borel-Cantelli lemma, we have\n$P (\\lim \\sup E_n) = 0$,\nwhich implies the following is true almost surely:\n$\u2203N \u2208 N*, such that for any n > N, y* = arg \\max_{y\u2208A} f_n (y)$\nHence\n$\\lim_{n\u2192+\u221e} acc^{MV}({(x,y)}; \u03c0) = I [y = y*]$   (almost surely).\nRecall the definition of $y*$, the above shows the theorem is true for a dataset with a single example ${(x, y)}$. For general datasets D with m examples, one can apply the above argument to each examples and combine the results to conclude the proof of the almost-sure convergence."}, {"title": "A.2. Proof of Theorem 2", "content": "Proof. The proof is similar to the proof of Theorem 1. We only need to set\n$\u03c0(y | x) = \\sum_{r\u2208V*}\u03c0(rey|xi)\u03c1(xirey)$.\nThen the technique in the proof of Theorem 1 immediately applies."}, {"title": "B. MCTS Details", "content": "In this section, we present additional background on the Monte Carlo Tree Search (MCTS) algorithm. The MCTS process can be formulated as the following steps:\nSelection. The process begins at the root node. Here, the algorithm recursively selects the child node that offers the highest Upper Confidence Bound applied to Trees (UCT) value, continuing until a node is reached that has not been expanded. The UCT is calculated using the formula\n$UCT(s) = Q(s) + C \\sqrt{\\frac{\\ln (V(Parent(s)))}{N(s)}}$\nwhere $Q(s)$ denotes the quality score of node s, N(s) is the number of visits to node s, Parent(s) denotes the parent node of s, and C is a constant determining the level of exploration.\nExpansion and evaluation. Upon reaching a non-terminal node s, the node is expanded by generating multiple child nodes. Each child node c is then evaluated using a value function V (c), which predicts the potential quality of continuing the sequence from node c."}, {"title": "C. Hyper-parameters", "content": "Finetuning All the hyperparameters for model fine-tuning can be found in Table 3. We preprocess the MetaMath Dataset to make the solutions in a stepwise format.\nInference For all the inference strategies, the temperature of the LLM is set to 1.0. Max tokens for the output is 1024 and max tokens for one step is 256. For REBASE, we chose the balance temperature (the softmax temperature in the REBASE algorithm) as T\u2081 = 0.1. For MCTS, we set C in the UCT value as 1 and we expand 4, 8, 16 children for the root, 2 children for other selected nodes with total 32, 64, 128 expansions respectively."}, {"title": "D. Supplementary Figures", "content": "We append the figures about Majority Voting and Majority Voting v.s. Weighted Majority Voting (Fig. 8,9,10, 11) in this section. The experiments show that although the gap between Majority Voting and Weighted Majority Voting on sampling is huge. This gap becomes much smaller if we apply REBASE. This phenomenon can be caused by the selection ability of tree search like REBASE. Once REBASE already samples solutions with high rewards, conducing Weighted Majority Voting gains less since the sampled solutions may all have relatively high and stable rewards compared with those of sampling."}]}