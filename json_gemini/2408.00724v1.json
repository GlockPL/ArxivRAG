{"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "authors": ["Yangzhen Wu", "Zhiqing Sun", "Shanda Li", "Sean Welleck", "Yiming Yang"], "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth. We study compute-optimal inference: designing models and inference strategies that optimally trade off additional inference-time compute for improved performance. As a first step towards understanding and designing compute-optimal inference methods, we assessed the effectiveness and computational efficiency of multiple inference strategies such as Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their variants on two different Tree Search algorithms, involving different model sizes and computational budgets. We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off. These results highlight the potential benefits of deploying smaller models equipped with more sophisticated decoding algorithms in budget-constrained scenarios, e.g., on end-devices, to enhance problem-solving accuracy. For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on MATH500 while using 2\u00d7 less FLOPs. Our findings could potentially apply to any generation task with a well-defined measure of success.", "sections": [{"title": "1. Introduction", "content": "Scaling laws of neural networks [Hestness et al., 2017, Rosenfeld et al., 2019] have been established across a range of domains, including language modeling [Kaplan et al., 2020, Hoffmann et al., 2022, OpenAI, 2023], image modeling [Henighan et al., 2020, Yu et al., 2022, Peebles and Xie, 2023], video modeling [Brooks et al., 2024], reward modeling [Gao et al., 2023], and board games [Jones, 2021]. These studies have demonstrated how model performance is influenced by both the size of the model and the amount of training computation. However, there is limited knowledge on how varying the compute during inference affects model performance after the model has been trained.\nTo improve the task performance of large language models (LLMs), inference techniques typically involve additional computation as a performance maximization step at inference time [Nye et al., 2021, Wei et al., 2022, Wang et al., 2022b, Yao et al., 2023, Chen et al., 2024b]. This cost must be taken into account for compute-optimal inference. For example, a Monte Carlo Tree Search (MCTS) method [Jones, 2021] may improve task performance, but potentially require much more compute than simply sampling solutions multiple times. Generally speaking, we need a comprehensive understanding of how various inference-time methods (e.g., Best-of-N, Majority Voting) trade off between performance and cost. To improve our understanding, this paper presents a thorough empirical evaluation with careful analysis over various configurations of representative LLMs and inference algorithms.\nSpecifically, we explore how to select an optimal size for the language model and an effective inference strategy (e.g., Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their Tree Search variants) to maximize performance (i.e., accuracy) with a given compute budget. We control the inference computation (FLOPs) of a fixed model by utilizing more tokens through the language"}, {"title": "2. Related Works", "content": "Mathematical Reasoning with LLMs. Large language models have made significant progress in recent years, and have exhibited strong reasoning abilities [Brown et al., 2020, Hoffmann et al., 2022, Chowdhery et al., 2022, Lewkowycz et al., 2022]. Mathematical problem solving is a key task for measuring LLM reasoning abilities [Cobbe et al., 2021a, Hendrycks et al., 2021b]. [Ling et al., 2017] first developed the method of producing step by step solutions that lead to the final answer. Later, [Cobbe et al., 2021b] extended the work by finetuning the pre-trained language model on a large dataset to solve math word problems, a verifier is trained for evaluating solutions and ranking solutions. Nye et al. [2021] train models to use a scratchpad and improve their performance on algorithmic tasks. Wei et al. [2022] demonstrate that the reasoning ability of a language model can be elicited through the prompting. Subsequent research [Kojima et al., 2022, Lewkowycz et al., 2022, Zhou et al., 2022] in reasoning tasks has also highlighted the efficacy of rationale augmentation. We choose problem solving in mathematics as the task to study the compute-optimal strategy since it allows us to accurately evaluate the problem solving ability of LLMs.\nInference Strategies of LLM Problem Solving. A variety of inference (also called decoding) strategies have been developed to generate sequences with a trained model. Deterministic methods such as greedy decoding and beam search [Teller, 2000, Graves, 2012] find highly probable sequences, often yielding high quality results but without diversity. Sampling algorithms (e.g., temperature sampling [Ackley et al., 1985]) can produce a diverse set of results which are then aggregated to achieve higher accuracy (e.g., via Majority Voting [Wang et al., 2022a]). Recent methods combine search algorithms with modern LLMs, including breadth-first or depth-first search [Yao et al., 2023], Monte-Carlo Tree Search (MCTS) [Zhang et al., 2023, Zhou et al., 2023, Liu et al., 2024, Choi et al., 2023], and Self-evaluation Guided Beam Search [Xie et al., 2023]. All of these methods show that using search at inference time can lead to performance gains in various tasks.. However, the trade-off for the improved performance is the use of compute to perform the search. Analyzing the trade-off between compute budget and LLM inference performance remains understudied. In this paper, we systematically analyze the trade-off between compute budget and problem-solving performance, and propose a tree search method that is empirically Pareto-optimal. .\nProcess Reward Models. Process reward models (PRMs) have emerged as a technique to improve the reasoning and problem-solving capabilities of LLMs. These models assign rewards to the intermediate steps of the LLM generated sequences. PRMs have been shown effective in selecting reasoning traces with a low error rate, and for providing rewards in reinforcement learning-style algorithms [Uesato et al., 2022, Polu and Sutskever, 2020, Gudibande et al., 2023]. Ma et al. [2023] applies the PRM to give rewards on the intermediate steps and guide the multi-step reasoning process. The PRM can be either trained on human labeled data [Lightman et al., 2023a] or model-labeled synthetic data [Wang et al., 2023]. In our work, we use the PRM as the reward model for selecting generated solutions, and for selecting which partial solutions to explore in tree search."}, {"title": "3. Compute-Optimal Inference for Problem-Solving", "content": "We explore the following question: Given a fixed FLOPs budget, how should one select an optimal model size for the policy model, and an effective inference strategy to maximize performance (i.e., accuracy)? We are the first to formulate this problem and study the inference time scaling law, setting our work apart from previous scaling law studies (Fig. 2).\nTo address this, we represent the problem-solving error rate $E(N,T; S)$ as a function of the number of model parameters $N$, the number of generated tokens $T$ and the inference strategy $S$. The"}, {"title": "3.1. Problem Formulation", "content": "computational budget $C$ is a deterministic function $FLOPs(N, T; S)$, based on $N$ and $T$. Our goal is to minimize $E$ under the test-time compute constraint $FLOPs(N,T, S) = C$:\n$(N_{opt}(C), T_{opt}(C); S) = \\underset{(N,T,S) \\text{ s.t. } FLOPS(N,T,S)=C}{\\arg \\min} E(N,T; S)$\nwhere $N_{opt}(C)$ and $T_{opt}(C)$ denote the optimal allocation of a computational budget $C$.\nHere, the inference computation (FLOPs) for a fixed model can be modulated by generating more tokens with the policy model, e.g., by sampling additional candidate solutions and subsequently ranking them using a reward model. We primarily consider sampling and tree-search approaches with reranking or Majority Voting as the means to consume more tokens, including Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their variants on tree search methods."}, {"title": "3.2. Inference Strategies", "content": "We consider the following sampling-based inference strategies which are popularly used in practice:\n\u2022 Greedy Search. This strategy generates tokens one at a time by selecting the highest probability token at each step, without considering future steps. It is computationally efficient but often suboptimal in terms of diversity.\n\u2022 Best-of-N. This strategy, also known as rejection sampling, samples multiple solutions and chooses the one with the highest score given by the reward model.\n\u2022 Majority Voting. In this strategy, multiple model outputs are generated, and the final answer to the problem is determined by the most frequently occurring answer in all the outputs.\n\u2022 Weighted Majority Voting. This strategy is a variant of Majority Voting in which the votes are weighted based on the scores given by the reward model.\nTheoretical Analysis. Before diving into more sophisticated inference algorithms (e.g., tree search), we present theoretical results on the asymptotic behavior of voting-based strategies given infinite compute in Theorems 1 & 2. Informally, we show that the accuracy of standard/Weighted Majority"}, {"title": "3.2.1. Sampling-based Methods", "content": "Voting converges with infinite samples, and the limit only depends on the distribution modeled by the language model (and the reward model). This theoretical finding is also aligned with our empirical findings shown in Sec. 4.2. The proofs are presented in Appendix A.\nLet $V$ be a finite vocabulary and $V^*$ its Kleene closure, i.e., the set of all strings. Given a problem $x$, we say a language model answers $y$ to this problem if the model outputs $r\\circ e$ where $r \\in V^*$ can be any \u201creasoning path\" and $e \\in V$ denotes a special token that marks the end of reasoning. We further assume that the answer string is always shorter than $L$ tokens, i.e., $|y| < L$ for some fixed $L \\in N^*$ where $|y|$ denotes the length of $y$. For a language model $\\pi$, denote by $\\pi(v|w)$ the probability of generating $v$ given input (prompt) $w$. For a reward model $\\rho$, denote by $\\rho(v)$ the score it assigns to the string $v$. We use $\\mathbb{I}$ to denote the indicator function."}, {"title": "Notations and assumptions.", "content": "Consider a dataset $\\mathbb{D} = \\{(x_i, y_i)\\}_{i=1}^m$ where $x_i$ and $y_i$ denotes input and true answer, respectively. For a language model $\\pi$, denote by $acc_{MV}(\\mathbb{D}; \\pi)$ the accuracy on $\\mathbb{D}$ using Majority Voting with $n$ samples. Following the notations and assumptions defined above, we have:\n$\\lim_{n\\to+\\infty} acc_{MV}(\\mathbb{D}; \\pi) = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{I} [y_i = \\underset{y\\leq L} {\\arg \\max} \\sum_{r\\circ e\\in V^*} \\pi(r\\circ e |x_i)] ~(\\text{almost surely});$\n$\\text{and } \\mathbb{E} [acc_{MV}(\\mathbb{D}; \\pi)] = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{I} [y_i = \\underset{y\\leq L} {\\arg \\max} \\sum_{r\\circ e\\in V^*} \\pi(r\\circ e |x_i)] - O(c^{-n});$\nfor some constant $c > 1$."}, {"title": "Theorem 1.", "content": "Consider a dataset $\\mathbb{D} = \\{(x_i, y_i)\\}_{i=1}^m$. For a language model $\\pi$ and a reward model $\\rho$, denote by $acc_{WV}(\\mathbb{D}; \\pi, \\rho)$ the accuracy on $\\mathbb{D}$ using Weighted Majority Voting with $n$ samples. Following the notations and assumptions defined above, we have:\n$\\lim_{n\\to+\\infty} acc_{WV}(\\mathbb{D}; \\pi, \\rho) = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{I} [y_i = \\underset{y\\leq L} {\\arg \\max} \\sum_{r\\circ e\\in V^*} \\pi(r\\circ e |x_i) \\rho(x_i r\\circ e)] ~(\\text{almost surely});$\n$\\text{and } \\mathbb{E} [acc_{WV}(\\mathbb{D}; \\pi, \\rho)] = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{I} [y_i = \\underset{y\\leq L} {\\arg \\max} \\sum_{r\\circ e\\in V^*} \\pi(r\\circ e |x_i) \\rho(x_i r\\circ e)] - O(c^{-n});$\nfor some constant $c > 1$."}, {"title": "Theorem 2.", "content": "Theorems 1 & 2 state the convergence of the accuracy with increasing number of samples, indicating that the performance gains of using more samples will saturate for any fixed models. The limit is determined by the likelihood of generating the correct answers through all possible reasoning paths (and the likelihood should be viewed as a weighted sum for Weighted Majority Voting). This motivates us to consider inference algorithms that search for \u201cgood\u201d reasoning paths, such as the tree-search-based variants detailed in Sec. 3.2.2 & 3.2.3.\nRemarks."}, {"title": "3.2.2. Monte Carlo Tree Search (MCTS)", "content": "MCTS or its variants (e.g., Tree of Thoughts [Yao et al., 2023]) mainly focus on improving the performance (e.g., accuracy) on the studied tasks. However, generic comparisons of MCTS with conventional methods like Best-of-N and Majority Voting in terms of computational budget, measured in generated tokens or processing time, are either scarce or indicating inference-time issues. For example, MCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods. Specifically, a significant portion of the paths in the search tree are used to estimate and select nodes, and these paths do not necessarily become a part of the final candidate solution, although MCTS ensures that the sampled solutions comprise high-quality intermediate steps. In contrast, sampling methods generate multiple solutions in parallel and independently, and all the generated sequences are included in the candidate solutions. However, the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor steps or exploiting promising ones.\nMonte Carlo Tree Search (MCTS) has proven effective in domains such as board games where strategic decision-making is required [Silver et al., 2016, 2017, Jones, 2021]. Recent work has shown that adapting MCTS to the context of LLMs can enhance the text generation process [Zhang et al., 2023, Zhou et al., 2023, Liu et al., 2024, Choi et al., 2023, Chen et al., 2024a, Tian et al., 2024, Chen et al., 2024a]. In this context, MCTS is often paired with a value model to score and guide the exploration steps. For additional background, we provide a review of MCTS in Appendix B.\nThis highlights the need for developing a new tree search method that can achieve a comparable (or better) performance as MCTS, and that is computationally less costly, just like Weighted Majority Voting and best-of-N. This need motivates the development of our new method named Reward Balanced Search (REBASE), as is introduced next."}, {"title": "3.2.3. Reward Balanced Search (REBASE)", "content": "The REBASE tree search method, illustrated in Fig. 3, inherits the exploitation and pruning properties of tree search, while using the reward model alone to estimate the nodes' qualities without additional computation for estimating values by sampling children. The efficiency is achieved by constraining the total expansion width of the tree at a certain depth. REBASE balances the expansion width among the nodes at the same depth based on the rewards given by the Process Reward Model (PRM). The details are provided below:\nWe view the fine-tuned LLM as a policy $\\pi_{\\theta}$ which generates the solution step by step. Given a question $x$ and the first $k$ steps of a solution $r_1\\cdots r_k$, the $(k + 1)$-th step is sampled from $\\pi_{\\theta}(\\cdot|xr_1\\cdots r_k)$. REBASE effectively generates a solution tree during inference, in which the root node the question $x$ and other nodes corresponds to solution steps. When generating solution trees, we generate children of $r_k$ by sampling from $\\pi_{\\theta}(\\cdot|xr_1\\cdots r_k)$. Here we slightly abuse notations and use the corresponding question/solution step to denote a node. The reward of a node $r_k$ is generated by the PRM: $R(r_k) := R(qr_1\\cdot\\ldots r_k)$.\nInitialization. Given the question $x$, balance temperature $T_b > 0$, and sampling number of solutions $N$, we sample $N$ instances of the first step for the question, yielding all the nodes of depth 1 in the search tree. We let the sampling budget of depth 0, $B_0$, to $N$ at initialization.\nReward modeling and update. In the $i$-th iteration, the PRM assigns the rewards to all the nodes at depth $i$. After that, the algorithm examines whether the solutions up to depth $i$ are complete. Supposing there are $C_i$ completed solutions, we update the sampling budget using $B_i \\leftarrow B_{i-1} - C_i$. If $B_i = 0$, the process ends, and we obtain $N$ solutions.\nExploration balancing and expansion. For all of the nodes $n_j$ with reward $R(n_j)$ in the depth $i$ of"}, {"title": "Notations.", "content": "the tree, we calculate the expansion width of the $n_j$ as:\n$W_j = \\text{Round} \\Big(B_i \\cdot \\frac{\\exp \\left(R(n_j) / T_b\\right)}{\\sum_k \\exp \\left(R(n_k) / T_b\\right)}\\Big)$.\nThen we sample $W_j$ children for $n_j$ for all the nodes in depth $i$, and start the next iteration."}, {"title": "4. Experiments", "content": "We conduct experiments on two mathematical problem-solving datasets to investigate the scaling effects of computation and our REBASE method for both challenging and simpler problems. Specifically, MATH [Hendrycks et al., 2021a] and GSM8K [Cobbe et al., 2021b] are datasets containing high school mathematics competition-level problems and grade-school level mathematical reasoning problems, respectively. Following [Lightman et al., 2023b, Wang et al., 2024, Sun et al., 2024], we use the MATH500 subset as our test set.\nTo study the inference compute scaling effect, we choose Pythia [Biderman et al., 2023] as our base models since various model sizes are available in the Pythia family. For tree search, we use math-specialized Llemma models [Azerbayev et al., 2024]. We further finetune these models on the MetaMath dataset [Yu et al., 2024] using full parameter supervised fine-tuning (Full-SFT), The detailed finetuning configuration is given in the Appendix. Additionally, we test the Mistral-7B [Jiang et al., 2023] to expand our findings across different models and architectures.\nAll of the experiments use the same Llemma-34B reward model, which we finetuned on the synthetic process reward modeling dataset, Math-Shepherd [Wang et al., 2024]. We added a reward head to the model, enabling it to output a scalar reward at the end of each step."}, {"title": "4.1. Setup", "content": "We use sampling and tree search methods to generate multiple samples and select the answer through Best-of-N, Majority Voting, or Weighted Voting. Each configuration has been run multiple times to calculate the mean and variance, thereby mitigating the randomness and ensuring the reliability of our conclusions."}, {"title": "Policy model (generators).", "content": "In order to compare the compute budgets of different models, we plot the figures with the number of FLOPs used per question during inference. We compute the inference FLOPs based on the standard formula from [Kaplan et al., 2020]."}, {"title": "Reward Model.", "content": "REBASE is Pareto-optimal. While MCTS undeperforms Sampling (Fig. 4), from Fig. 4, 5, 6, and 7, we found that REBASE consistently outperforms the Sampling method in all settings, when fixing the model and the evaluation task. Table 1 shows that REBASE can achieve competitive accuracy with even a lower compute budget than the sampling method. This finding is novel, and differs from previous tree search works which typically improve the performance at the cost of higher computational expense compared to sampling [Chen et al., 2024a, Xie et al., 2023]. Table 2 shows that given the same compute budget (sampling 32 solutions for the 7B model and 8 solutions for 34B model), using REBASE yields higher accuray than sampling.\nWeaker models gain more from Tree Search. For example, our proposed REBASE leads to 5.3%, 3.3%, and 2.6% performance gains on MATH for Mistral-7B, Llemma-7B, Llemma-34B, respectively. The order of accuracy increase is inversely related to the model's corresponding greedy search on"}, {"title": "Inference Configuration.", "content": "The scaling effect of inference computation budget across different model sizes is shown in Fig. 1. We note that the error rate first decreases steadily and then starts to saturate. Where smaller model first takes advantage since it can generate more samples given limited budget, larger models are preferable with more FLOPs due to the saturation of small model performances. As highlighted in the right panel of Fig. 1, the optimal model size can be different under various computation budgets. We performed a regression analysis on inference FLOPs $C$ and corresponding model sizes $N$ to establish a relationship between a given computational budget and its optimal model size. The resulting regression equation, $log_{10} (C) = 1.19log_{10} (N) + 2.03$, enables us to estimate the optimal inference model size for a specified computational constraint.\nLlemma-7B model achieves competitive accuracy to Llemma-34B model with lower compute budget. Fig. 4 & 5 show the curves of error rates versus total number of inference FLOPs per question. Inference methods with different model sizes are plotted in the same diagram. We found that Llemma-7B costs approximately 2\u00d7 less total FLOPs than Llemma-34B under the same method (Sampling, MCTS, REBASE) and task (MATH, GSM8K) while achieving competitive accuracy. This result suggests that, with the same training dataset and model family, training and inference with a smaller model could be more favorable in terms of compute budget if multiple sampling or search methods are employed."}, {"title": "4.2. Main Results of Compute-Optimal Inference", "content": "those datasets. This suggests that weaker models, as indicated by their lower greedy search accuracy, benefit more from tree search methods like REBASE.\nREBASE saturates later than sampling with higher accuray. From Fig. 6 and Fig. 7, we observe that both sampling and REBASE saturate early in GSM8K and relatively late in MATH, which we attribute to the difference of the difficulty level. This can be explained through the LLM may assign high probability to the true answer in easy problems than those of harder problems, as suggested by Theorems 1 & 2 with their proofs A. On MATH (Fig. 6), we see that REBASE finally saturates with a higher accuracy than sampling. We hypothesize the reason is that REBASE samples the truth answer with higher probability than sampling. And as demonstrated by Theorems 1 & 2, the upper bound becomes higher."}, {"title": "Scaling law of compute-optimal inference.", "content": "In this work, we conducted a comprehensive empirical analysis of compute-optimal inference for problem-solving with language models. We examined the scaling effect of computation during inference across different model sizes and found that while increased computation generally leads to higher performance, the optimal model size varies with the available compute budget. When the computation budget is limited, smaller models can be preferable. Specifically, a small language model can achieve competitive accuracy to a model four times larger while using approximately half the total FLOPs under the same inference methods (Sampling, MCTS, REBASE) and tasks (MATH, GSM8K). Additionally, we introduce our novel tree search method, REBASE, which is more compute-optimal than both sampling methods and Monte Carlo Tree Search (MCTS). REBASE typically achieves higher accuracy while using several times less computation than sampling methods. Our results underscore the potential of deploying smaller models equipped with sophisticated inference strategies like REBASE to enhance problem-solving accuracy while maintaining computational efficiency."}, {"title": "4.3. Comparing REBASE to Other Baselines", "content": "Our experiments specifically targeted mathematical problem-solving tasks. Additionally, our models were primarily trained using the MetaMath dataset. Investigating the impact of different training datasets on the performance and efficiency of compute-optimal inference strategies for mathematical problem-solving would be a valuable direction for future research."}, {"title": "5. Conclusions & Limitations", "content": "Recall that we assume the answer must be shorter than $L$ tokens. Let $\\mathcal{A} = \\{v \\mid |v| < L\\}$ be the set of all possible answers. Let $\\tilde{\\pi}(y | x)$ be the probability of the language model $\\pi$ outputting the answer $y$ to the question $x$ after marginalizing over the \u201creasoning paths\u201d, i.e.,"}, {"title": "Conclusions.", "content": "$\\tilde{\\pi}(y | x) = \\sum_{r\\circ e\\in V^*} \\pi(r\\circ e |x).$"}, {"title": "Limitations.", "content": "Given an input $x$, Assume that $y^* = \\arg \\max_{y \\in \\mathcal{A}} \\tilde{\\pi}(y|x), y' = \\arg \\max_{y \\in \\mathcal{A} \\backslash \\{y^*\\}} \\tilde{\\pi}(y|x)$, and denote\n$\\delta = \\tilde{\\pi}(y^*|x) - \\tilde{\\pi}(y'|x)."}, {"title": "A. Omitted Proofs", "content": "For any $y$, denote by $f_n(y)$ the number of times that the model answers $y$ in the first $n$ samples. Let $E_n$ be the event that Majority Voting with $n$ samples does not output $y^*$. We note that $E_n$ happens only if there exists $y''$ such that $f_n(y'') \\geq f_n(y^*)$. Therefore, by union bound,\n$\\mathbb{P}(E_n) \\leq \\mathbb{P}(\\exists y'' \\in \\mathcal{A} \\backslash \\{y^*\\}, f_n(y'') \\geq f_n(y^*))$\n$\\leq \\sum_{y'' \\in \\mathcal{A} \\backslash \\{y^*\\}} \\mathbb{P}(f_n(y'') \\geq f_n(y^*))$\n$< |\\mathcal{A}| \\mathbb{P}(f_n(y') \\geq f_n(y^*))$\nNote that $f_n(y^*) - f_n(y')$ can be viewed as a sum of $n$ i.i.d. random variables, which take value 1 with probability $\\tilde{\\pi}(y^*|x)$, $-1$ with probability $\\tilde{\\pi}(y'|x)$, and 0 otherwise. Thus, their expectations are all $\\delta = \\tilde{\\pi}(y^*|x) - \\tilde{\\pi}(y'|x)$. By Hoeffding's inequality, we have\n$\\mathbb{P}(f_n(y') \\geq f_n(y^*)) \\leq \\exp \\left(-\\frac{n \\delta^2}{2} \\right).$\nThus,\n$\\mathbb{P}(E_n) \\leq |\\mathcal{A}| \\exp \\left(-\\frac{n \\delta^2}{2} \\right) ~~~ \\xrightarrow[n \\to +\\infty]{} ~~~ \\sum_{n=1}^{+\\infty} \\mathbb{P}(E_n) < +\\infty.$\nBy Borel-Cantelli lemma, we have\n$\\mathbb{P} (\\limsup_{n \\to \\infty} E_n) = 0,$\nwhich implies the following is true almost surely:\n$\\exists N \\in \\mathbb{N}^*, \\text{ such that for any } n > N,  y^* = \\arg \\max_{y \\in \\mathcal{A}} f_n (y).$\nHence\n$\\lim_{n \\to +\\infty} acc_{MV}(\\{(x, y)\\}; \\pi) = \\mathbb{I} [y = y^*] ~~~~~~ (\\text{almost surely}).$\nRecall the definition of $y^*$, the above shows the theorem is true for a dataset with a single example $\\{(x, y)\\}$. For general datasets $\\mathbb{D}$ with $m$ examples, one can apply the above argument to each examples and combine the results to conclude the proof of the almost-sure convergence."}, {"title": "A.1. Proof of Theorem 1", "content": "Next, we prove the asymptotic result on $\\mathbb{E} [acc_{MV}(\\{\\mathbb{D}\\}; \\pi)]$. We slightly abuse notation for simplicity as follows: We let $y^*(x_i) = \\arg \\max_{y \\in \\mathcal{A}} \\tilde{\\pi}(y|x_i), y' = \\arg \\max_{y \\in \\mathcal{A} \\backslash \\{y^*(x_i)\\}} \\tilde{\\pi}(y|x_i)$, and let\n$\\delta_{min} = \\min_{(x_i,Y_i) \\in \\mathbb{D}} [\\tilde{\\pi}(y^*(x_i)|x_i) - \\tilde{\\pi}(y'(x_i)|x_i)].$\nWe denote by $E_n(x_i)$ the event that Majority Voting with $n$ samples does not output $y^*(x_i)$ given input $x_i$. Then it's easy to see that\n$\\mathbb{P}(E_n(x_i)) \\leq |\\mathcal{A}| \\exp \\left(-\\frac{n \\delta_{min}^2}{2} \\right) = O(c^{-n}),$\nwhere $c > 1$ is a constant (which does not depend on $i$).\nNote that if $acc_{MV}(\\{(x_i, Y_i)\\}; \\pi) = 1$, we have $y_i = y^*(x_i)$ unless $E_n(x_i)$ happens. In other words,\n$acc_{MV}(\\{(x_i, Y_i)\\}; \\pi) \\leq \\mathbb{I} [y_i = y^*(x_i)] + \\mathbb{I} [E_n(x_i)]$\n$|\\mathbb{E} [acc_{MV}(\\{(x_i, Y_i)\\}; \\pi)] - \\mathbb{I} [Y_i = Y^*(x_i)]| < \\mathbb{P}(E_n(x_i)) = O(c^{-n}).$\nTaking a summation over the entire dataset $\\mathbb{D}$ yields\n$\\Big|\\mathbb{E} \\big[ ace_{MV}(\\mathbb{D}; \\pi) \\big] - \\frac{1}{m} \\sum_{i=1}^m \\mathbb{I} [Y_i = Y^*(x_i)]\\Big| < \\frac{1}{m} \\sum_{i=1}^m \\mathbb{P}(E_n(x_i)) = O(c^{-n}),$\nwhich concludes the proof."}, {"title": "A.2. Proof of Theorem 2", "content": "The proof is similar to the proof of Theorem 1. We only need to set\n$\\tilde{\\pi}(y | x) = \\sum_{r\\circ e\\in V^*} \\pi(r\\circ e |x_i) \\rho(x_i r\\circ e).$\nThen the technique in the proof of Theorem 1 immediately applies."}, {"title": "A.1. Proof of Theorem 1", "content": "In this section, we present additional background on the Monte Carlo Tree Search (MCTS) algorithm. The MCTS process can be formulated as the following steps:\nThe process begins at the root node. Here, the algorithm recursively selects the child node that offers the highest Upper Confidence Bound applied to Trees (UCT) value, continuing until a node is reached that has not been expanded. The UCT is calculated using the formula\n$\\text{UCT}(s) = Q(s) + C \\sqrt{\\frac{\\ln (V(\\text{Parent}(s)))}{N(s)}}$\nSelection."}, {"title": "B. MCTS Details", "content": "Upon reaching a non-terminal node $s$, the node is expanded by"}]}