{"title": "Can Al grade your essays? A comparative analysis of large language models and teacher ratings in multidimensional essay scoring", "authors": ["Kathrin Se\u00dfler", "Babette B\u00fchler", "Maurice F\u00fcrstenberg", "Enkelejda Kasneci"], "abstract": "The manual assessment and grading of student writing is a time- consuming yet critical task for teachers. Recent developments in generative AI, such as large language models, offer potential solu- tions to facilitate essay-scoring tasks for teachers. In our study, we evaluate the performance and reliability of both open-source and closed-source LLMs in assessing German student essays, compar- ing their evaluations to those of 37 teachers across 10 pre-defined criteria (i.e., plot logic, expression). A corpus of 20 real-world essays from Year 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, 01, LLaMA 3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring capabilities. Closed-source GPT models outperform open-source models in both internal consis- tency and alignment with human ratings, particularly excelling in language-related criteria. The novel 01 model outperforms all other LLMs, achieving Spearman's r = .74 with human assessments in the overall score, and an internal consistency of ICC = .80. These findings indicate that LLM-based assessment can be a useful tool to reduce teacher workload by supporting the evaluation of es- says, especially with regard to language-related criteria. However, due to their tendency for higher scores, the models require further refinement to better capture aspects of content quality.", "sections": [{"title": "1 Introduction", "content": "The correction and evaluation of student texts is a tedious yet cru- cial process for teachers, especially in subjects where a significant amount of assessment-relevant text is produced. One of the rela- tively few studies of teachers' working hours in German schools revealed, for example, that correcting students' texts is enormously time-consuming [33]. The study found that, after teaching itself (29%) and the preparation and follow-up of lessons (22%), proof-reading (including feedback to content etc.) accounted for the third-highest proportion of working time for secondary school teachers, at 14%. Notably, the study did not differentiate between subjects, hence teachers of less correction-intensive subjects were also in- cluded in this proportion. For example, a typical German teacher with 25 students, three German classes, and four class tests per year plus one practice essay per test would need to correct around 600 essays annually, averaging more than two per working day and that's just for the subject German.\nDespite the time and effort teachers invest in correcting student texts, the feedback provided is often delayed, and extremely het- erogeneous [31]. Currently, Germany, like many other countries, is also struggling with a systematic shortage of teachers [26], which is expected to worsen in the future. In this context, supporting time-consuming tasks like essay scoring is crucial, and technical solutions can offer valuable assistance to alleviate the workload on teachers.\nThe rapid advancements of Artificial Intelligence (AI) and Large Language Models (LLMs), such as GPT-4 and o1 by OpenAI [37, 38], LLAMA 3 by Meta [1], or Mistral AI [22] open various new pos- sibilities, also within the educational context [23]. These models possess the capability to process, analyze, and generate natural language with high proficiency, offering the potential to enhance teaching and learning processes. For instance, LLMs can support lesson preparation through automated question generation [8], facilitate teacher collaboration via conversational AI [20], and sup- port the evaluation and feedback generation for student errors [7]. Moreover, the abilities of LLMs extend across various educational domains, including language learning [32], mathematics [36], and life science education [6]. By integrating these advanced AI tools, educators can devote more time to other important tasks, enhance the quality and consistency of student assessments, and foster more interactive and personalized learning experiences.\nThe challenges mentioned above, coupled with ongoing advance- ments in Al, raise the question of whether LLMs can serve as efficient alternatives or at least provide valuable support in the evaluation of student essays. Although AI-generated feedback is progressively being integrated into educational applications, there remain substantial gaps in our understanding of the quality and ef- fectiveness of this feedback. While some initial approaches employ AI to provide feedback on student texts [19, 47] and others focus on Automated Essay Scoring (AES) [40, 44], a detailed, criteria-based evaluation is still missing. Previous research, mostly evaluating texts based on a holistic score [40, 45] or concentrating on few general and task-agnostic criteria [29, 34], often falls short of fully capturing the complexity and details inherent in student writing, particularly within the context of German language lessons. Also, the limited availability of suitable data resources and the uncer- tainty around a clear and consistent ground truth-typically based on teacher evaluations, which are often subjective and re- quire multiple raters-arise as further challenges. For instance, the widely used ASAP dataset [18] relies on only two raters for most of the essays. While this setup is standard for many datasets, it could limit the robustness of the annotations when evaluating highly subjective dimensions like essay quality. This uncertainty also ex- tends to the ratings of automated essay scoring system. Only few studies have examined the consistency of LLMs across multiple runs when evaluating a text [17]. Finally, existing research lacks a qualitative analysis of the reasoning processes of LLMs to im- prove the understanding of which essays aspects have an influence on the model assessment.\nTo leverage LLMs to their full potential in educational settings, we address these gaps by evaluating the quality of open- and closed-source LLMs in detail by assessing student text based on ten predefined language- and content-related criteria. Therefore, we conducted a user study involving 37 German teachers from different schools and educational levels, collecting on average 5.45 (SD = 0.92) ratings per 20 German real-world student essays. Specif- ically, we compare the human ratings with those generated by GPT-3.5, GPT-4 [37], 01 [38], LLAMA 3-70B [1] and Mixtral 8x7B [22], thoroughly analyzing their strengths and limitations across different evaluation categories and their reasoning process through inter-category correlation examination. We underpin our results by including LLM reliability measurements through comparing repeated assessments. Our analysis adopts a multidimensional ap- proach, extending beyond the mere holistic score to examine biases and performance variations in different assessment aspects and gain in-depth insights into the capabilities of the tested models. This comprehensive evaluation aims to understand how well LLMs align with human evaluations and to identify areas where they excel or require improvement. To achieve these goals, we target the following research questions:\n\u2022 RQ1: How reliably do open-source and closed-source LLMs perform in essay evaluation, and how do their assessments correlate with real-world teacher evaluations of German- language texts?\n\u2022 RQ2: What are the strengths and limitations of using LLMs to assess multidimensional aspects of essay quality beyond providing a basic holistic score?\n\u2022 RQ3: How do different evaluation criteria influence the rea- soning process for the overall essay scores of humans and LLMs?"}, {"title": "2 Related Work", "content": "When evaluating student texts, a scale can be established with 'holis- tic' at one end and 'analytical' at the other [46]. This distinction relates to the nature of the judgment process: holistic approaches typically rely on implicit criteria that are difficult to articulate, while analytical methods are guided by explicit criteria, often structured in detailed rubrics. Despite the scarcity of studies-particularly regarding the assessment of German school texts-research sug- gests no significant differences in objectivity between these two approaches [16, 46]. Both holistic and analytical assessments are subject to variability between different evaluators [9, 46], though there is also substantial agreement between the judgments derived from each approach [42]. Analytical assessments tend to be more rigorous and less prone to judgment errors [46]. Note, however, that the study situation for German texts is very sparse and that the various studies were carried out in completely different age groups and text types in different years and are therefore not comparable. From the learners' perspective, criteria-based feedback is key to improving their writing skills, therefore, in this study, we focus on analytical feedback on student texts."}, {"title": "2.2 LLMs for text assessment", "content": "Traditional Automated Essay Scoring. Automated Essay Scoring (AES) techniques have been developed since 1966 [44]. Primarily, they exploited statistical features to analyze the text [24]. Then, with the introduction of deep learning methods, it became possible to concentrate on more advanced syntactic and semantic features and to analyze the style and content of a text [4], applying LSTMs or Transformer-based models like BERT [12]. For example, BERT has been employed to extract features to train a regression model [13], directly output a class label [49, 54], or RoBERTa and Bi-LSTM have been combined to assess essays [5]. Further, hand-crafted features in combination with existing LSTM or BERT structures have been shown to boost the performance [51]. But overall, the models up to 2022 concentrate more on language than on content and do not emphasize cohesion and coherence of the essays enough [44]. Also, many works rely on statistical features to determine one holistic final score and concentrate on English data.\nShort Answer Grading on German data. To adapt for German essays, [45] applied BERT, focusing on Automatic Short Answer Grading (SAG) (with less than 100 words on average). They achieved a Pearson correlation coefficient of .75, indicating a strong align- ment between the model's predictions and human ratings. Similarly, [40] examined the inter-rater reliability between SBERT and human ratings for German corpora, also concentrating on SAG. Although they simplified the task by reducing the grading to a binary deci- sion and training the model on a specific dataset, the transformer model yielded an average precision of only 71.4%. Unlike free-text essay scoring, however, the evaluation of short answers typically relied on a predefined ground truth for each question. This distinc- tion raised concerns about the suitability of transformer models for more complex tasks such as the assessment of student texts using expert ratings. There is an absence of studies focusing on the assessment of standard German essays that go beyond holistic scoring.\nApplying GPT models on English Essay data. The rise of LLMs in 2022, has opened up new opportunities for automatic essay grading. One of the first studies was conducted by [11], which compared the ratings of three teachers on 400 English text fragments (averaging 150 words) with the ratings of GPT-3 [39]. The three teachers and the model rated the texts - half written by humans and half gener- ated by GPT-2 [43] - on a five-point Likert scale in four categories: grammaticality, cohesion, sympathy, and relevance. The ratings of the teachers and the model only correlated strongly for the criterion relevance, while the other criteria showed a weak positive corre- lation. [29] leveraged GPT-3 to evaluate over 12,000 essays from the TOEFL11 dataset [10] across four dimensions: task response, coherence and cohesion, lexical resources, and grammatical range. They compared GPT-3's performance with 45 linguistic features and discovered that while the linguistic features alone outperformed GPT-3, the combination of both approaches yielded the best results. Specifically, GPT-3 achieved an accuracy of 54% compared to pro- fessional human evaluators but demonstrated an 89% agreement within a deviation of 1 to 2 points. [14] reported similar findings with GPT-3.5, showing perfect agreement with human raters on only 30% of exams. However, GPT-3.5 maintained a 70% accuracy within a 10% range around the true score, indicating reasonable but not perfect alignment with human judgments.\nIn the context of discourse coherence analysis, [34] identified a 56% exact match and a 97% adjacent match between human eval- uators and GPT-4 [37]. Additionally, both [2] and [41] employed GPT-4 for open-ended question grading. [2] found a strong corre- lation between human and LLM scores, while [41] demonstrated that the model's feedback closely resembled expert feedback. [27] compared GPT-3.5 and GPT-4 for automated scoring in science education, revealing that prompting strategies such as few-shot learning and chain-of-thought (CoT) [53] can enhance accuracy. All these studies rely on closed-source GPT models and focus ex- clusively on English text data. Moreover, many provide only holistic scores or assess a limited set of artificial criteria. There is a notable lack of detailed analyses exploring how LLM-generated scores differ from human judgments across multidimensional criteria.\nApplying open-source LLMs on Essay data. [48] investigated the use of open-source LLMs for generating feedback. They evaluated different prompting strategies for zero- or few-attempt learning to determine how well Mistral 7B [21] could generate essay feed- back. This approach of comparing different prompting strategies appeared promising for future research. The study showed that com- bining automated essay scoring (AES) with feedback generation could improve scoring performance, although the overall impact of AES on feedback quality was minimal. Both, LLM-based scores and manual scores, were used to evaluate the usefulness of the feedback. However, using another LLM to evaluate a feedback-generating LLM raises concerns, such as the risk of perpetuating the biases inherent in the models and the lack of the nuanced understanding of a human expert, such as a teacher. Furthermore, the study did not report the qualifications or backgrounds of the 12 human raters, which is a serious omission. Understanding these raters' expertise is essential to assessing the quality of manual feedback rating. It raises the question of how amateur raters might differ from an LLM in terms of feedback quality.\nAcknowledging the limited research in open-source LLMs for automated essay scoring, the scarcity of non-English datasets, and the absence of multidimensional, didactic-based evaluation criteria, our study aims to conduct a comprehensive evaluation of various closed- and open-source LLMs in multidimensional essay assess- ment and gather new insights into their reasoning processes. In addition, almost no study uses authentic teacher ratings in combi- nation with real learner texts, which impairs the ecological validity of previous studies."}, {"title": "3 Methodology", "content": "In our study, we aim to analyze the performance of LLMs in eval- uating student texts according to ten pre-defined criteria. In the following, we describe the details of the study, including the essays and scoring criteria used, the participants, the application of the LLMs, and the metrics employed for analysis."}, {"title": "3.1 Student Essay Dataset", "content": "The text corpus comprises N = 20 real-world student texts from pupils in Year 7 (n = 10) and Year 8 (n = 10) at two secondary schools (one \"Gymnasium\u201d and one \u201cRealschule\") in Germany. These essays were written as part of a performance assessment. Pupils in the seventh grade wrote a narrative with descriptive elements based on the ballad \"The Sorcerer's Apprentice\" (in German \"Der Zauber- lehrling\") by J. W. Goethe. Those in the eighth grade wrote narra- tives inspired by two paintings by Edward Hopper (\"Nighthawks\" and \"Gas\"). To ensure representativeness despite a small corpus, we selected narratives since they are taught in all German school types."}, {"title": "3.2 Teacher Essay Scoring", "content": "The 20 student essays were presented to N = 37 teachers with the task of reading the texts and assessing them according to 10 pre-defined criteria using a six-point Likert scale, ranging from Not true at all to Fully applies. The six-point Likert scale was selected to align with the German grading system, which also employs six levels, enabling teachers to provide consistent and familiar ratings. In addition, the even number of possible choices eliminates the risk of a tendency towards the center [52].\nSince no universally standardized criteria exist for essay evalua- tion for any type of text in German, the criteria for this study were derived from real-world teacher feedback specific to these essay types and were later refined and formalized by an expert in German didactic. The advantage of this approach, apart from the increase in ecological validity, is that teacher are familiar with the categories, which in turn has a positive influence on the reliability of their ratings. Half of the resulting categories asses the content (i.e., In- troduction, Main Part, and Conclusion), while the others focus on the writing style and language (e.g., Verbal images and Descriptive elements, Literal speech & inner monologue and Spelling & punctu- ation), with one category assessing overall judgment, as detailed in Table 1. The teachers submitted their assessments anonymously via an online portal. Each teacher was asked to rate three texts; however, there are some missing values, resulting in a total of 1,090 human ratings across the 20 texts and the 10 assessment categories. Each essay received between three and seven ratings from different teachers, with an average of 5.45\u00b10.92 ratings per essay. We employ the average of the teacher ratings as our ground truth per essay and evaluation criteria."}, {"title": "3.3 LLM Essay Scoring", "content": "To automatically evaluate student essays based on the ten pre-defined criteria, we selected different LLMs, to compare the perfor- mance of various foundation models. For closed-source models, we used GPT-3.5 (gpt-3.5-turbo-0125), GPT-4 (gpt-40-2024-05-13) [37] and o1 (01-preview) [38] as representative examples. These GPT models were integrated into our evaluation pipeline via the OpenAI API. For open-source LLMs, we chose LLaMA 3-70B [1] and Mixtral 8x7B [22]. Initial experiments included smaller variants of these models; however, their performance was inferior to their larger counterparts. Therefore, the smaller models were excluded from further analysis. Both open-source models were executed locally in half-precision using two NVIDIA A100 80GB GPUs to ensure efficient processing.\nAll LLMs were prompted using a zero-shot approach, instructing them to evaluate one text at a time. For each essay, a new conver- sation was initiated to maintain the independence of evaluations. The prompt structure was designed to align with the predefined evaluation criteria, ensuring consistency across all model assess- ments. This methodology allowed us to systematically compare the performance of different LLMs in a controlled and unbiased manner.\nThe prompt, illustrated in Figure 2, specifies the model's role as a teacher, provides the context of an essay written by a 13-year-old student, outlines the specific task of analyzing the essay according to predefined criteria, and defines the desired output format as JSON. This prompt was consistently used throughout all experiments to ensure uniformity. While variations in input prompts can lead to different results [48], specific prompt engineering was not the focus here."}, {"title": "4.1 Reliability of model predictions", "content": "We compare multiple runs of the same prompt to assess the reliabil- ity of each model's evaluations in addressing RQ1. Consequently, we obtain ten ratings for each data point, similar to human raters.\nIt is evident that the closed-source models demonstrate a fair level of consistency in their ratings across individual runs, as indi- cated by ICC showing moderate to good reliability (.73.84) [25]. In contrast, the open-source models LLaMA 3 and Mixtral, display significant variability, with low values and poor agreement between multiple runs. This inconsistency needs to be taken into account in the subsequent analysis."}, {"title": "4.2 Correlation Analysis Between LLM Evaluations and Teacher Assessments", "content": "Continuing our examination of RQ1, we compare the assessment of LLMs and humans by computing the Spearman correlation coef- ficients r for all evaluation criteria in Table 3. 01 demonstrates the strongest correlation with human ratings, achieving significance in nine out of ten categories. Specifically, it is the only model demon- strating a high, significant correlation of .74 to teacher ratings in the overall category. GPT-4 maintains significant correlation in seven out of ten categories, with a moderate agreement with human raters in most of the categories, while GPT-3.5 reaches significant correla- tions in four categories. This again shows the improvement across subsequent model versions. In contrast, Mixtral shows only weak or non-significant correlations across all criteria. LLaMA 3 exhibits nearly zero correlation to teacher ratings and, in some cases, even negative correlations, underscoring its inconsistency with human evaluations.\nFigure 3 visually shows the strong correlation between 01 and human ratings, especially for language-related features such as spelling (r=0.814), use of literal speech (r=.805), and verbal imagery (r=.738). Additionally, the overall rating exhibits a robust correlation despite having an intercept of 2.3. However, two content-related categories - Introduction, and Main Part - demonstrate only weak or moderate correlations. In these areas, o1 is less effective at dis- tinguishing between higher and lower-quality essays."}, {"title": "4.3 Rating Comparison between LLM and Human Evaluation", "content": "To address RQ2, we first compare the distribution of the ratings in the Overall category made by teachers and the LLMs, as shown in Table 4. The human raters exhibit a lower average rating than the GPT models, indicating a stricter assessment approach. The LLAMA 3 model presents a similar average to the human raters. However, the variance in ratings across essays is significantly higher for human raters than for all LLMs, with the LLaMA 3 and Mixtral displaying particularly low variance. This suggests that the open- source models assess essays more uniformly, clustering ratings around the midpoint, whereas human raters and the closed-source models provide a wider differentiation between essay qualities. Additionally, GPT-4, 01, and Mixtral provide significantly higher ratings than humans. The low rater correlation of both LLaMA 3 and Mixtral (Section 4.1) contributes to these observations by resulting in more average values and reduced variance. This highlights a limitation in the consistency and alignment of certain LLMs with human evaluative standards."}, {"title": "4.4 Analysis of Rating Distributions Across Evaluation Categories", "content": "To gain further insights into the differing ratings between human raters and LLMs, we compare the average ratings for each criterion in Figure 4. Generally, the GPT models exhibit higher mean ratings across all individual criteria, whereas human raters tend to be stricter in their evaluations. LLaMA 3 displays average ratings similar to those of the human raters. In contrast, the Mixtral model consistently shows significantly higher average ratings across all categories. Consequently, the differences observed in the overall ratings between LLMs and humans are reflected in each category's judgments. This consistency indicates that the final ratings align with the more detailed evaluations of human raters and LLMs.\nIt is important to note the variances presented in Table 5. While the LLaMA 3 models exhibit average ratings comparable to hu- man raters, their variances are considerably smaller, consistently clustering around the midpoint. Similarly, the Mixtral model also demonstrates reduced variance, aligning with the pattern observed in LLaMA 3. In contrast, GPT-3.5, o1, and human raters display higher variances, indicating a broader differentiation in their evalu- ations. The low ICC values for both LLAMA 3 and Mixtral models contribute to these observations, as their limited agreement among ratings results in more average values and diminished variability.\ndifferences between human and GPT-4 ratings for criteria Conclu- sion, Verbal Images & Descriptive Elements, Literal Speech & Inner Monologue, Expression & Sentence Structure, and Spelling & Punctu- ation. Except for Conclusion, all these criteria are language-related. Similarly, 01 only exhibits a non-significant difference for Spelling & Punctuation and Expression, both language-related criteria. Since LLMs are trained on vast amounts of text data encompassing various writing styles and formalities, they are optimized for recognizing patterns and stylistic elements. Consequently, these surface-level aspects of a text, including grammar and sentence structure, can be efficiently analyzed and evaluated by LLMs in a manner sim- ilar to humans. Remarkably, they accurately assess learner texts despite likely limited exposure to student writings during training, demonstrating their effective generalization for educational writing evaluation.\nIn contrast, the discrepancy between human ratings and GPT-4 and o1 is significant for criteria Heading, Introduction, Main Part, and Plot Logic, all of which are content-related categories. This gap may arise from several factors. Although LLMs can generate coherent texts, at the moment they still lack the deep semantic understanding that humans possess. This limitation manifests in flawed logical reasoning concerning contextual details and maintaining logical consistency. Additionally, during the training process, model bi- ases can be inherited and learned by the LLMs, resulting in milder ratings. Notably, their are differences between model versions, as previously indicated in Table 4 and Figure 4, which is surprising"}, {"title": "4.5 Impact of Evaluation Categories on Overall Essay Scoring", "content": "To compare how strongly individual evaluation categories impact the overall rating for teachers and LLMs (RQ3), we correlate single criteria scores with the overall rating for teachers and each model. Overall, it is noteworthy that the correlations between all features of GPT-3.5 and GPT-4 are higher than those for human judgments. This can be attributed to the auto-regressive nature of LLMs, where each new token generation is conditioned on the previous output, implicitly incorporating prior evaluations into the context. This effect is particularly pronounced in the smaller GPT-3.5 model, where correlations between ratings are nearly always p \u2265 0.8. The high inter-criteria correlations may indicate that the assessment of each criterion relies more on preceding ratings than solely on the original text.\nIn contrast, o1 exhibits fairly lower correlations between cri- teria, which may stem from its larger number of parameters and enhanced context understanding, allowing it to focus more on the input text than previous ratings. Compared to the previous GPT models, human raters and o1 demonstrate lower inter-criteria cor- relations, suggesting that their evaluations are less influenced by preceding ratings and more independently based on each criterion's merits.\nConversely, the LLaMA 3 and Mixtral models consistently show low correlations across all criteria, highlighting a distinct evaluation pattern. This lower inter-criteria correlation suggests that the open- source model assess each criterion more independently, potentially reflecting a different internal reasoning process or a less integrated evaluation strategy compared to closed-source models.\nWhen examining which evaluations criteria seem to highly im- pact overall scores, both human raters and LLMs agree that criteria Plot Logic and Expression & Sentence Structure are highly relevant to the overall grade, whereas criterion Heading is less relevant. Major differences are observed in how criterion Main Part influences the overall grade for teachers compared to GPT-4, while o1 is able to in- clude this information more strongly. Specifically, for teachers, the Main Part is a major determinant of the overall assessment, whereas GPT-4 places greater emphasis on criterion Spelling & Punctuation and Verbal image, which is not as decisive for human evaluators. This discrepancy highlights a potential risk associated with using LLMs for assessing student texts: the models may inherently focus more on linguistic surface features, such as Spelling & punctua- tion. Consequently, these surface-elements could be weighted more heavily in the overall assessment than is appropriate, potentially undermining the evaluation of deeper content quality. In addition, Main Part and Plot Logic are particularly influential criteria for the teachers, but precisely here Table 3 shows weak correlations and Table 5 significant differences between GPT-4, 01 and the humans. This shows one of the major challenges we are currently facing in improving machine-generated feedback."}, {"title": "5 Discussion", "content": "This study aimed to evaluate the performance and reliability of open-source and closed-source LLMs in assessing student essays, specifically focusing on German-language texts."}, {"title": "5.1 Closed-source LLMs provide reliable essay assessments", "content": "Our analysis revealed that closed-source models, particularly 01, exhibit higher reliability when run multiple times and stronger correlations with human assessments compared to open-source models like LLAMA 3 and Mixtral. The high reliability of GPT mod- els aligns with previous findings in the literature [17]. 01 demon- strated strong correlations in eight out of ten evaluation categories, aligning closely with human judgments in language-related aspects.\nConversely, open-source models like LLaMA 3 and Mixtral showed minimal to no correlation with human ratings, often rating essays around the midpoint with low variance. This limited differentia- tion suggests that these models are less effective in distinguishing between varying levels of essay quality, likely due to their lower ICC scores indicating poor internal consistency. This inconsistency in ratings, when run multiple times, makes them unsuitable for employment in real-world scenarios. [48] find a similar lack of align- ment with human evaluations with LLaMA 2 [50], while they were able to increase the performance of Mistral by applying prompt engineering techniques like CoT prompting [53]."}, {"title": "5.2 LLMs align best with Human Ratings in Language-Related Criteria", "content": "The comparison of overall ratings on individual criteria revealed that GPT-4, 01, and Mixtral generally provide higher average scores than human raters, which is in line with previous research [2, 30]. The lower variance in overall scores observed in LLaMA 3 and Mixtral models indicates a tendency to rate essays uniformly, likely due to averaging highly inconsistent values of several assessments, which limits their ability to differentiate effectively between high and low-quality writing. This finding is consistent with our initial observation of low internal consistency for these models, suggesting that they lack detailed evaluation capabilities.\nMoreover, the Mann-Whitney U tests indicated that the dis- tribution of GPT-4 ratings aligns closely with human judgments in language-related criteria but diverges significantly in content- related aspects, which have the highest influence on the overall grade for real-world teachers. This indicates a better alignment in broadly applicable linguistic features but a bigger discrepancy for content criteria, which are highly dependent on the context (i.e., grade level and text type) and on the particular standards and conventions of the German school system. This may stem from the model's lack of exposure to similar data during pre-training. Further alignment through fine-tuning or few-shot learning might mitigate this issue by providing the models with a frame of reference. Other- wise, deploying these tools in real-world educational settings could lead to inconsistent assessments and reduced usability."}, {"title": "5.3 Inter-Criteria Correlations emphasize Consistent Reasoning in GPT Models", "content": "The correlation analysis revealed that GPT models exhibit higher inter-criteria correlation among evaluation categories compared to human raters. This phenomenon can be attributed to the auto-regressive nature of LLMs, where each generated token is influ- enced by preceding outputs, thereby creating a more integrated and consistent reasoning process across different evaluation dimen- sions. Specifically, GPT-3.5 and GPT-4 showed strong correlations (p \u2265 0.86 resp. p \u2265 0.75) between criteria, suggesting that its as- sessments are highly dependent on prior ratings. While this may enhance consistency, it also raises concerns about the model's abil- ity to independently evaluate each criterion based solely on the original text.\nIn contrast, 01 demonstrated lower and more diverse correlations than GPT-3.5 and GPT-4, likely due to its larger parameter set and enhanced context understanding, which allow for a more balanced focus between the input text and prior evaluations. This results in a reasoning process that better reflects human evaluative practices, particularly in content-related categories. The open-source models LLAMA 3 and Mixtral, however, maintained weaker correlations across criteria, indicating a fragmented evaluation approach that lacks the cohesive reasoning seen in GPT models. These patterns suggest that the architectural and training differences between closed-source and open-source models significantly impact their evaluation strategies and reliability.\nWhile for humans content-related criteria like main part and plot logic have the highest influence on the overall rating, GPT-4 sets a greater focus on language aspects like expression and spelling. This discrepancy points, again, to a basic limitation where LLMs may prioritize surface-level linguistic features over deeper content analysis, potentially skewing the overall assessment towards as- pects like spelling and punctuation rather than substantive content quality."}, {"title": "5.4 Implications for Automated Essay Assessment", "content": "The findings of this study underscore both the potential and the lim- itations of using LLMs for automated essay assessment. GPT-4 and 01, despite their high alignment with human ratings in language- related criteria, exhibit a bias towards higher overall scores. This highlights the need for careful calibration and potential alignment strategies, such as top-down prompting or bottom-up fine-tuning on domain-specific datasets, to mitigate biases and enhance the model's ability to assess essays more accurately.\nMoreover, the poor performance of open-source models like LLaMA 3 and Mixtral suggests that these models are not yet ready for reliable use in educational settings without significant improve- ments in their consistency and alignment with human standards. The low ICC scores and minimal correlation with human ratings indicate that these models lack the necessary reliability for detailed, multidimensional assessments required in educational contexts."}, {"title": "5.5 Trust and Perceived Usefulness", "content": "[55] found in their meta-study on the perceived usefulness of au- tomated writing assessments with Chinese students that students' trust in the automated system is an important factor in the per- ceived usefulness of the system. Moreover, [35] showed that it is important for teachers to understand how the decisions are made to increase trust in a system. Additionally, [28] found that emotional trust in products could be negatively influenced by the 'AI' label. Against this background, it is important not to waste the potential that LLMs have for the assessment of student texts by overesti- mating their functionality while their development is still at the beginning.\nBuilding on these previous findings, the high correlation and re- liability of GPT-4 and o1 could foster greater trust among educators and students, enhancing the perceived usefulness of such systems. However, transparency in model limitations and ongoing efforts to align LLM assessments with human evaluative standards are essential to implementing the full potential of these technologies without limitating user confidence."}, {"title": "5.6 Limitations and Future Directions", "content": "This study has several limitations that should be acknowledged. First, while no prompt engineering was employed in this research, future work on open-source models could benefit from sophisticated strategies such as Chain-of-Thought Engineering [53], as demon- strated in [48]. The novel 01 series automatically incorporates CoT reasoning, which likely contributes to its superior performance. Implementing these techniques may enhance the performance and alignment of LLMs with human ratings by enabling more detailed and context-aware evaluations.\nThe scope of this study was limited to a specific set of models, namely GPT-3.5, GPT-4, 01, LLaMA 3, and Mixtral. Incorporating other models such as Claude [3] or Gemini [15] in future research would offer a more comprehensive evaluation of LLM performance across different architectures and training paradigms. Furthermore, this study focused solely on one type of essay. Exploring other essay formats, such as argumentative essays, and using a more extensive and diverse dataset would enhance the generalizability and robustness of the findings. Different essay types may present unique challenges and require distinct evaluative criteria, providing a more holistic understanding of LLM capabilities in automated assessment. Also, the inherent variance observed in multiple runs of LLMs, specifically for open-source models, suggests that real- world applications should incorporate mechanisms to aggregate multiple scores to mitigate the influence of outliers and enhance the robustness. Additionally, the absence of a gold standard due to the inherent variability among human raters poses a challenge for automated systems. Future studies should investigate methods to account for the fuzzy nature of human evaluations and integrate them into LLM training and assessment frameworks.\nFinally, we observed a clear trend in OpenAI's closed-source mod- els: each new version shows improved reliability and a stronger correlation with human assessments, which was also shown by [27]. Given the rapid advancements in this field and our findings, we anticipate continued enhancements in LLMs, making them increas- ingly effective tools for supporting automated essay evaluation in educational settings."}]}