{"title": "Gesture-Aware Zero-Shot Speech Recognition for Patients with Language Disorders", "authors": ["Seungbae Kim", "Daeun Lee", "Brielle Stark", "Jinyoung Han"], "abstract": "Individuals with language disorders often face significant communication challenges due to their limited language processing and comprehension abilities, which also affect their interactions with voice-assisted systems that mostly rely on Automatic Speech Recognition (ASR). Despite advancements in ASR that address disfluencies, there has been little attention on integrating non-verbal communication methods, such as gestures, which individuals with language disorders substantially rely on to supplement their communication. Recognizing the need to interpret the latent meanings of visual information not captured by speech alone, we propose a gesture-aware ASR system utilizing a multimodal large language model with zero-shot learning for individuals with speech impairments. Our experiment results and analyses show that including gesture information significantly enhances semantic understanding. This study can help develop effective communication technologies, specifically designed to meet the unique needs of individuals with language impairments.", "sections": [{"title": "Introduction", "content": "Language disorders, such as aphasia, arise from damage to brain regions responsible for language production and comprehension. Aphasia is most often caused by acquired brain injuries, like stroke, and persists chronically in at least 30% of cases (Broca et al. 1861; Wasay, Khatri, and Kaul 2014). This means that many are living with aphasia - indeed, there are nearly two million with aphasia in the USA alone (Simmons-Mackie and Cherney 2018). Individuals with language disorders face significant communication challenges due to difficulties in processing and understanding language, resulting in impaired social interactions and reduced quality of life (El Hachioui et al. 2017). As voice-assisted technologies like Siri and Alexa become integral to daily activities, the inability of individuals with language disorders to interact effectively with these systems exacerbates their communication barriers, leading to frustration and further marginalization (Rohlfing et al. 2021).\nAutomatic Speech Recognition (ASR) systems are designed to transcribe spoken language into text, serving as a cornerstone for many voice-driven technologies. While recent advancements have made ASR systems more adept at handling disfluencies such as stuttering, their performance heavily depends on clear and consistent audio input (Radford et al. 2023). For individuals with language disorders, speech distortions, substitutions, and disjointed delivery pose significant challenges to accurate ASR transcription (Sanguedolce, Naylor, and Geranmayeh 2023). As a result, current ASR solutions often fall short in addressing the needs of this population.\nTo enhance the robustness of ASR, researchers have explored Audio-Visual Speech Recognition (AVSR) systems that combine auditory and visual information (Cheng et al. 2023b). These systems leverage visual features such as lip movement (Hu et al. 2023; Cheng et al. 2023a) and facial expressions (Zadeh et al. 2016; Busso et al. 2008) to improve speech recognition accuracy. However, these approaches are not ideal for individuals with language disorders, who often experience concomitant motor speech disorders and facial hemiplegia, which can result in 'masked' facial expressions (Multani et al. 2017; Duffy et al. 2012).\nIn contrast, individuals with language disorders frequently rely on non-verbal communication, such as gestures, to compensate for their verbal limitations (Stark and Oeding 2023). Iconic gestures, in particular, serve as powerful tools for conveying meaning when spoken language is insufficient (de Kleine et al. 2023; van Nispen, van de Sandt-Koenderman, and Krahmer 2017; Stark and Cofoid 2022; Stark and Oeding 2023). Unlike speech or facial expressions, iconic gestures offer a visual representation of concepts, enabling individuals with language impairments to express ideas more effectively (Lee et al. 2023). However, current ASR and AVSR systems fail to consider the latent semantic information encoded in these gestures, leaving a critical gap in understanding the full context of communication for individuals with language disorders.\nTo address the limitations of current ASR and AVSR systems, we introduce a gesture-aware zero-shot speech recognition framework specifically designed for individuals with language disorders. Our approach harnesses the capabilities of multimodal large language models (LLMs) to incorporate linguistic, acoustic, and gestural information, enabling a deeper understanding of the speaker's intended meaning. Unlike traditional systems that rely solely on spoken or visual cues like lip movements, our method emphasizes the integration of iconic hand gestures-gestures that visually represent concepts to enrich the transcription process. This allows the system to produce transcripts that not only reflect spoken content but also capture the latent meanings, conveyed through gestures, which are crucial for individuals with impaired speech.\nThe core of our method lies in leveraging a zero-shot framework to align and synthesize multimodal inputs. Our system processes disfluent or incomplete speech signals while simultaneously analyzing visual data to identify and interpret gestures. By fusing these streams of information, the system generates semantically enriched transcripts that bridge the gaps left by speech alone. This zero-shot design eliminates the need for task-specific training, making the approach adaptable to a wide range of scenarios and linguistic contexts. This adaptability is particularly advantageous for language-disordered populations, where speech patterns and gestures vary widely between individuals.\nOur experiments show that our proposed model successfully generates transcripts by incorporating significant gestural information to ascertain the latent intent of individuals with language disorders, which is not conveyed through speech alone. This findings indicate that incorporating non-verbal information can effectively aid in advancing the creation of more inclusive and effective communication technologies designed specifically for the distinctive requirements of individuals with language impairments."}, {"title": "Related Work", "content": "Assistive technologies play a critical role in supporting clinicians to deliver assessments and therapies, as well as in facilitating communication for individuals with aphasia. In speech-language pathology, such tools are often referred to as augmentative and alternative communication (AAC) systems (Beukelman and Mirenda 1998). ASR technology has shown promise in enhancing communication by enabling real-time, accurate feedback to individuals with aphasia (Ballard et al. 2019; Barbera et al. 2021). This is particularly valuable as individuals with aphasia frequently face challenges with self-monitoring (Oomen, Postma, and Kolk 2001; Sampson and Faroqi-Shah 2011) and often benefit from external cues to improve their performance (Tompkins, Scharp, and Marshall 2006; Conroy, Sage, and Lambon Ralph 2009; Schwartz et al. 2016). Additionally, ASR can improve communication efficiency and quality by compensating for writing or grammatical impairments through speech-to-text conversion and delivering feedback during therapy (Ballard et al. 2019; Barbera et al. 2021). Consequently, ASR-based applications have gained popularity in delivering speech-language services for individuals with aphasia and other neurogenic conditions, such as Parkinson's disease (Hoover and Carney 2014; McCrocklin 2016; Strik et al. 2009).\nHowever, despite advancements in ASR, reliance on voice-based systems alone often results in transcription errors when processing disfluent or incomplete speech, a hallmark of language impairments (Jefferson 2019; Le et al. 2016). These inaccuracies can hinder communication and reduce the overall effectiveness of these tools, further exacerbating barriers faced by individuals with language disorders. To address these challenges, AVSR systems have emerged, combining visual inputs\u2014such as lip movements and facial expressions\u2014with audio to enhance recognition accuracy (Gabeur et al. 2022; Afouras et al. 2018; Dupont and Luettin 2000; Ma, Petridis, and Pantic 2021; Noda et al. 2015; Mroueh, Marcheret, and Goel 2015; Feng et al. 2017). However, AVSR systems often fall short for individuals with language and speech disorders. Speech disorders such as dysarthria and apraxia frequently disrupt articulation, rendering lip movements unreliable. Moreover, neutral or ambiguous facial expressions common in these populations provide limited contextual insight (Tong, Sharifzadeh, and McLoughlin 2020; Salama, El-Khoribi, and Shoman 2014)."}, {"title": "Gesture into Assistive Technology", "content": "Gestures play a vital role in communication, often conveying critical information that complements, enhances, or even substitutes spoken language (Kita 2009; Kendon 1994; McNeill, Pedelty, and Levy 1990). This holds true for individuals with and without language impairments, as gestures are co-produced with speech across languages and cultures, aiding both speakers and listeners (Kita 2009). Remarkably, even congenitally blind speakers, who have never observed gestures, use gestures as frequently as sighted individuals, emphasizing their fundamental role in communication (Iverson and Goldin-Meadow 1998). For speakers, gestures illustrate abstract concepts, emphasize key points, and provide additional information that speech alone may struggle to convey (McNeill, Pedelty, and Levy 1990; Kita 2000; Goldin-Meadow and Alibali 2013). For listeners, gestures offer visual cues that complement verbal messages, enhancing understanding in noisy settings, during non-native language processing, and when speech is ambiguous (Cook and Tanenhaus 2009; Goldin-Meadow 1999). Notably, listeners often recall information received via gestures as if it were spoken, underscoring their semantic importance (Cassell 2000; Kelly et al. 1999).\nAmong gestures, representational gestures-those that visually represent objects or actions\u2014are particularly valuable for enhancing communication (McNeill, Pedelty, and Levy 1990). Iconic gestures, a subtype of representational gestures, depict concrete referents (e.g., a kicking motion to signify \"kick\") (Novack and Goldin-Meadow 2017). Other representational gestures include pantomimes, which occur without speech, and deictic gestures, such as pointing, which may emphasize or add meaning to speech(Kendon 1994). Additionally, emblems like a \"thumbs up\" convey culturally specific meanings independent of speech (Kendon 1994). However, not all gestures are communicative. Non-communicative movements, such as tucking hair behind the ear, do not contribute to the conveyed message and can be irrelevant to the communicative process."}, {"title": "Challenges in Gesture Interpretation for Assistive Technology", "content": "Accurately interpreting the wide variety of gestures individuals use is a significant challenge, as gestures vary in form and meaning, with some being non-communicative. Even gestures conveying the same intent, such as cutting, can differ-for example, an \u201cenacting\u201d gesture mimics the action of cutting with a flat hand, while a \"handling\" gesture simulates gripping an invisible knife (Poggi 2008; Hassemer and Winter 2018). This variability complicates consistent interpretation, particularly in zero-shot settings. While recent computer vision methods focus on generating gestures from text inputs for avatars or video sequences (Ginosar et al. 2019; Ahuja, Lee, and Morency 2022; Liu et al. 2022), they do not address gesture understanding or intent recognition, relying instead on predefined datasets. Existing ASR and AVSR systems also fall short, as they lack the ability to incorporate conversational context or personalized knowledge, both critical for interpreting gestures tied to specific topics or individual habits. For instance, one person may favor \"handling\" gestures while another prefers \"enacting.\" Additionally, task-specific training required by many systems is impractical for real-time applications, as it necessitates frequent retraining to accommodate new users and scenarios. These limitations underscore the need for adaptable systems capable of real-time, personalized gesture interpretation."}, {"title": "Method", "content": "Given that the dataset provides video recordings of persons with language disorders, the proposed model aims to improve the transcription of audio from individuals with language disorders, focusing on enhancing contextual understanding, incorporating gesture, and resolving ambiguities in speech. It consists of three interconnected components: Speech Recognition, Gesture Recognition, and Contextual Rewriting. Figure 1 provides a schematic overview of the system.\nSpeech Recognition The first step in the pipeline is audio processing using an ASR system. This component takes an audio signal A as input and outputs a preliminary transcript $T_{ASR}$, which represents the system's initial interpretation of spoken words. Let the ASR model be represented by $f_{ASR}$ as follows,\n$T_{ASR} = f_{ASR}(A), T_{ASR} = {w_1, w_2,..., w_n}$ (1)\nwhere $w_i$ is the i-th word in the transcript. Consider an audio input from a patient saying, \u201cI um... tomato.\" The ASR model may generate a raw transcript as incomplete speech. This transcript lacks clarity and completeness, as it fails to convey the full meaning intended by the speaker. The limitations arise because ASR systems rely solely on the audio signal and are unable to incorporate accompanying non-verbal cues, such as gestures, that could provide additional context.\nGesture Recognition The second component focuses on identifying gestures from the video frames corresponding to the speech. The input is a sequence of video frames $V = {v_1, v_2, ..., v_m}$, where each frame $v_i$ captures a snapshot of the speaker's hand or body movements. A gesture recognition model, $f_{Gesture}$, processes these frames to detect meaningful gestures as follows,\n$G = f_{Gesture}(V), G = {g_1, g_2,..., g_k}$ (2)\nwhere $g_i$ represents a detected gesture and its associated meaning. Due to the scarcity of labeled datasets for iconic gestures, the framework employs a multimodal large language model to perform zero-shot gesture recognition. This approach allows the model to infer the meanings of gestures based on generalizable patterns learned from other modalities. Iconic gestures, which represent concrete referents, are of particular interest because they often clarify or complement the spoken language.\nIf the video captures the speaker making a back-and-forth motion with a flat hand, the gesture recognition model identifies this as a cutting motion and assigns it the label g=\"cut\". Iconic gestures, however, can vary significantly between individuals. For instance, one person might perform a \"handling\" gesture by mimicking the act of holding and moving an invisible knife, while another might use an \u201cenacting\" gesture, simulating the motion of cutting without mimicking the act of holding. The proposed model is designed to handle such variability by recognizing these different representations and mapping them to a unified semantic meaning, ensuring that the gesture's intent is accurately interpreted regardless of individual differences in gesture style.\nContextual Rewriting The third component integrates the outputs from the ASR and gesture recognition models to generate a contextually enriched and semantically accurate transcript. This process is handled by a LLM which takes the initial transcript $T_{ASR}$, the detected gestures G as inputs. The final transcript, $T_{Final}$, is generated as follows:\n$T_{Final} = f_{LLM}(T_{ASR}, G), T_{Final} = {w'_1, w'_2,..., w'_j}$ (3)\nwhere $w'_i$ represents a word in the final transcript. The model directly uses the recognized gestures as the sole non-verbal input, streamlining the process while focusing on the integration of multimodal signals.\nFor example, if the ASR transcript reads, \"I um... tomato,\u201d and the gesture recognition model identifies the label g=\"cut\" from a cutting motion, the LLM rewrites the transcript as \"I cut tomato.\" This correction ensures that the final output reflects both the spoken and gestural communication, improving the system's ability to understand and represent the speaker's intent accurately. This approach enables the generation of accurate and enriched transcripts, even in cases where speech alone might be ambiguous or incomplete. By leveraging the co-speech gesture input, the system enhances its interpretive power without requiring additional external context."}, {"title": "Experiments", "content": "We collected the dataset from AphasiaBank (MacWhinney et al. 2011; Forbes, Fromm, and MacWhinney 2012), a shared database created by clinical experts for aphasia research; the corpus information is summarized in Table 1. The dataset includes video recordings of the language evaluation test process between a pathologist and a subject, which also contains human-annotated transcriptions and subjects' demographic information.\nFor our study, we selected the \"Peanut Butter Sandwich Task\" as the focal activity where people are asked to explain the procedure for making a sandwich. This task has been demonstrated to associate with high rates of iconic gesturing (Stark and Cofoid 2022; Stark and Oeding 2023; Pritchard et al. 2015; Illes 1989).\nWe performed a detailed analysis of the dataset to examine the distribution and characteristics of gestures used by participants during this task."}, {"title": "Results", "content": "We conducted experiments to evaluate the performance of ASR models in generating initial transcripts. These experiments are essential for assessing the quality of the ASR output, which serves as the input for our gesture-aware contextual rewriting model. Additionally, we performed a case study on selected samples to highlight the improvements made by our proposed approach compared to the initial ASR outputs."}, {"title": "Speech Recognition Models", "content": "The foundation of a successful gesture-aware ASR system lies in obtaining high-quality initial transcripts from the speech recognition stage. To this end, we compared state-of-the-art ASR models on the AphasiaBank dataset to determine their performance in generating accurate transcripts. The results, summarized in Table 3, highlight the average Word Error Rate (WER) for each model, calculated by comparing their output with the ground truth human-annotated transcripts. WER serves as a critical metric for evaluating ASR systems, representing the proportion of errors (insertions, deletions, and substitutions) in the generated transcript relative to the total number of words in the ground truth.\nAmong the models tested, Whisper (Radford et al. 2023) demonstrated the best performance, achieving the lowest WER of 0.557. This indicates that Whisper is particularly effective in handling speech from participants with aphasia, despite challenges such as disfluencies, atypical speech patterns, and background noise. Wave2Vec (Baevski et al. 2020) performed moderately well with a WER of 0.630, while Seamless (Barrault et al. 2023), designed for multilingual ASR, showed a higher WER of 0.891, indicating potential limitations in its robustness to the specific speech characteristics in this dataset.\nTo further improve the accuracy of the initial transcripts, we applied a confidence-based filtering approach to Whisper's outputs. By utilizing Whisper's token-level confidence scores, we excluded tokens with a confidence score below 0.2, thereby retaining only the most reliable portions of the transcript. This refinement reduced Whisper's WER from 0.557 to 0.519, further solidifying its position as the most accurate ASR model in our evaluations.\nThese results highlight the importance of selecting a robust ASR model for datasets featuring non-standard speech patterns. Whisper, particularly with confidence-based filtering, provides a strong baseline for generating initial transcripts. The reduced WER achieved by Whisper demonstrates its ability to capture key elements of speech even under challenging conditions, thereby enhancing the reliability of the downstream contextual rewriting process."}, {"title": "A Case Study", "content": "To evaluate the effectiveness of the proposed multimodal model in capturing and incorporating the latent meaning conveyed by iconic gestures, we conducted a case study comparing its outputs with those of the Whisper ASR model. Table 4 presents the results, showcasing the original transcripts in the dataset, the transcript generated by Whisper, and the transcript generated by our multimodal approach. The goal of this comparison was to determine whether our model can enhance transcript quality by inferring and integrating the semantic meaning of gestures observed during speech.\nAs shown in the table, Whisper provides a literal transcription of the spoken words but fails to capture the contextual or semantic information conveyed by the speaker's gestures. For example, in the first example, the original input includes the layering gesture, indicating an action of stacking or placing items on top of each other. Whisper transcribes this as \"There's right there,\u201d which misses the gesture's implicit meaning. In contrast, our proposed model generates the enriched transcript, \"You put things right there, doing a layering action,\u201d effectively incorporating the gesture's semantic context into the spoken narrative.\nIn the second example, the speaker performs a folding motion, which is entirely absent from Whisper's transcription, resulting in the simple word \"right.\" However, our model identifies and integrates this gesture into the transcript, producing \u201cfolding it right,\u201d which provides a more comprehensive description of the speaker's intention.\nSimilarly, in the third and fourth examples, iconic gestures like cutting and eating are completely omitted from Whisper's transcription, leading to outputs that lack essential contextual details. The proposed model enriches these outputs by explicitly including the actions, resulting in transcripts like \"cutting banana\u201d and \u201cand eating,", "do this night stuff like oh spreading,": "s not fully accurate. This inaccuracy stems from the initial transcription provided by Whisper, which included the word \u201cnight\u201d leading our model to generate a sentence misaligned with the speaker's actual intent. This example underscores the importance of having accurate initial transcripts for improving the final output quality, especially in scenarios where subtle contextual nuances are critical.\nIt is important to note that the proposed model is capable of inferring and incorporating the speaker's underlying intention more effectively. By leveraging gestures alongside incomplete or disfluent speech, our approach captures the full semantic meaning of the speaker's communication. For example, when a speaker's verbal output is fragmented or ambiguous, the model uses the accompanying gestures to infer the intended message, producing transcripts that are both richer and more contextually accurate. Unlike Whisper, which relies solely on the spoken words and often misses crucial gestural context, our method bridges the gap between speech and gesture, offering a more comprehensive representation of the speaker's intent."}, {"title": "Concluding Remarks", "content": "In this paper, we proposed a novel approach utilizing multimodal LLMs to generate gesture-aware speech recognition transcripts for patients with language disorders. Our framework integrates verbal speech and iconic gestures, enabling the generation of enriched transcripts that capture the latent meaning conveyed through both modalities. Through extensive experimentation, we demonstrated that the proposed method effectively contextualizes incomplete or disfluent speech by incorporating gesture information, leading to more accurate and meaningful representations of the speaker's intent. These findings highlight the potential of our approach to significantly contribute to the field of speech and language therapy, offering innovative tools that can enhance the quality of life for individuals with language disorders by facilitating better communication and assessment methods."}, {"title": "Ethical Statement", "content": "Our dataset was obtained from AphasiaBank with the approval of the Institutional Review Board (IRB) and adheres to the data sharing guidelines set by TalkBank. This includes complying with the Ground Rules for all TalkBank databases, which are based on the American Psychological Association Code of Ethics (Association et al. 2002)."}, {"title": "Limitation & Future Work", "content": "While the results are promising, we recognize several limitations and outline our plans to extend this work further.\nOne primary limitation is the absence of a definitive ground truth for quantitative evaluation. Since our model generates transcripts by synthesizing speech and gesture data from scratch, traditional benchmarks, such as comparisons with standard speech recognition outputs, are insufficient. Moreover, existing original transcripts lack gesture annotations, making direct comparisons challenging. In future work, we aim to address this gap by collaborating with certified pathologists to conduct qualitative assessments, such as A-B preference tests, to evaluate the effectiveness of gesture-enriched transcripts in accurately conveying the speaker's intentions.\nTo support quantitative evaluations, we plan to develop novel metrics that assess transcript quality, including grammar accuracy, semantic consistency, and the integration of multimodal information. Such metrics will provide a more objective basis for assessing our model's performance and facilitate comparisons with other multimodal and unimodal approaches.\nAnother limitation of this study is its focus on structured gestures from a specific task, the Peanut Butter Sandwich Task. While this task offers a controlled context for testing our approach, it does not encompass the diversity of gestures and communication patterns seen in everyday scenarios. As part of our future work, we plan to expand the scope of our model to include tasks such as the Cinderella Story Recall Task (Bird and Franklin 1996), which involves unstructured and complex narrative gestures. This expansion will allow us to evaluate the adaptability and robustness of our model in handling varied linguistic and gestural contexts.\nIn summary, while this study establishes a strong foundation for gesture-aware speech recognition, we aim to refine and extend our methods through collaborative qualitative evaluations, the development of robust quantitative metrics, and broader task applications. These efforts will ensure that our approach continues to evolve, ultimately contributing to more effective communication tools and interventions for individuals with language disorders."}]}