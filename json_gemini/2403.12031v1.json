{"title": "ROUTERBENCH: A Benchmark for Multi-LLM Routing System", "authors": ["Qitian Jason Hu", "Jacob Bieker", "Xiuyu Li", "Nan Jiang", "Benjamin Keigwin", "Gaurav Ranganath", "Kurt Keutzer", "Shriyash Kaustubh Upadhyay"], "abstract": "As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and deliver a comparative analysis of various routing approaches through ROUTERBENCH, highlighting their potentials and limitations within our evaluation framework. This work not only formalizes and advances the development of LLM routing systems but also sets a standard for their assessment, paving the way for more accessible and economically viable LLM deployments. The code and data are available at https://github.com/withmartian/routerbench.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have exhibited remarkable capabilities in addressing a wide range of tasks across academic and industrial scenarios (Bubeck et al., 2023). This has motivated both researchers and practitioners to introduce new LLMs, designed for both generic and specialized use cases, on a near-daily basis \u00b9. However, the proliferation of LLMs presents a challenge for LLM application builders to identify the most suitable model for their applications. While some proprietary models such as GPT-4 are distinguished by their superior performance, they often incur high economic costs due to the expensive API prices.\nMany prior works focus on improving the capabilities of individual LLMs while maintaining low costs. Techniques such as prompting (Wei et al., 2023), quantization (Lin et al., 2023; Kim et al., 2023), and system optimization (Kwon et al., 2023) may reduce a single model's serving cost, yet with new models emerging daily, these approaches may not remain feasible or scalable in long term. Moreover, the diversity of choices of LLMs available at various price and performance tiers can be daunting for users attempting to select and optimize an appropriate model2.\nAn alternative solution aims to select to optimal LLM for each input through \"routing.\" (Chen et al., 2023; Shnitzer et al., 2023; \u0160akota et al., 2023). Routing offers several advantages over single-LLM optimization. First, it is a lightweight process, which treats each LLM as an input-output black box, avoiding the need to delve into intricate infrastructure details, thus making it flexible and broadly applicable. Second, routing systems benefit from the diversity of LLMs, while single-LLM methods may struggle to keep pace with the expanding LLMs landscape. Lastly, while single-LLM strategies often face a compromise between performance and other factors such as per-token costs, routing systems adeptly balance a spectrum of user demands (Chen et al., 2023; Lee et al., 2023; Lu et al., 2023).\nThe rise in routing-related research has improved cost-efficiency, enhanced performance, and broadened accessibility. Despite these advances, a comprehensive benchmark for evaluating routing techniques remains absent. We introduce ROUTERBENCH, the first comprehensive benchmark designed specifically for assessing router mechanisms in terms of inference dollar cost and performance."}, {"title": "2. Related Work", "content": "Various strategies have been proposed to optimize the cost and performance of current LLMs. We provide an overview of them with a focus on routing-related approaches.\nSingle LLM Enhancement Fine-tuning is used to improve models for specific tasks, which requires additional training and domain-specific data (Rafailov et al., 2023). Prompting mechanisms like Chain-of-Thought (CoT) (Wei et al., 2023; Zhou et al., 2023; Wang et al., 2023a) and Tree of Thoughts (ToT) (Yao et al., 2023) could bolster LLM performance without additional fine-tuning. Mixture-of-Experts (MoE) (Eigen et al., 2014; Shazeer et al., 2017; Fedus et al., 2022; Du et al., 2022; Shen et al., 2023; Si et al., 2023) is another line of work that explores routing within the model to enhance performance efficiently, which contains specialized \"experts\" and routes the input to the best expert. Nevertheless, these single-LLM enhancements are usually model and scenario specific, and could not benefit from the explosion in the number of LLMs.\nLLM Synthesis Beyond single LLM approaches, LLM synthesis utilizes the ensemble of multiple LLMs, integrating their outputs into an enhanced final result (Jiang et al., 2023b). Another approach has shown that a strategic combination of smaller models can match or even outperform larger models (Lu et al., 2024). However, these methods require at least two steps: text generation and synthesis, which increases costs and latency, creating challenges to applying this approach in production.\nRouting Unlike LLM Synthesis, routing can select the suitable model for specific input without performing inference on every candidate model. Routing can be classified into two categories, non-predictive routing and predictive routing. Non-predictive routing strategies retrieve outputs from LLMs and directly pick one without a model-assisted synthesis step."}, {"title": "3. Math Formulation for Router Evaluation", "content": "The primary challenge in assessing the performance of routing systems lies in balancing two conflicting objectives: maximizing efficiency and minimizing cost. To effectively compare routers, we have developed a framework that captures the multi-faceted nature with one metric."}, {"title": "3.1. Setup and Basic Operations", "content": "Consider a set of models $L = \\{LLM_1,...,LLM_m\\}$ a dataset $D$ consisting of examples $x_i \\in \\{X_1, ..., X_{|D|}\\}$. For each model $LLM_i$, we evaluate its performance by generating an output $o_i = LLM_i(x_i)$ for each example $x_i$. Each output $o_i$ has two associated quantities: the cost $c(o)$ of generating that output and the quality or performance $q(o)$ of the output itself. Through this process, we establish an expected cost $C_m$ and an expected quality $q_m$ for each model $LLM_m$ across the dataset $D$.\n$C_m = E[c(LLM_m(x))|x \\in D]$\n$q_m = E[q(LLM_m(x))|x \\in D]$\nA router $R$, define as a function, takes in a prompt $x$ and a set of parameters $\\theta$, subsequently selecting the most suitable model $LLM_i$ from a set $L$ to complete the prompt, i.e.\n$R_\\theta(x) \\rightarrow LLM_i \\in L$.\nThe parameters $\\theta$ typically include maximum price the user is willing to pay, the desired latency, or number of layers of neural networks for the router model, etc. More details of router parameters will be elaborated and discussed in Section 5.1.\nThe expected cost of a router $R_{\\theta_1}$ across dataset $D$ is defined as\n$C_{R_{\\theta_1}}(D) = E[c(R_{\\theta_1}(x))|x \\in D]$\nand the expected performance of a router $R_{\\theta_1}$ can be defined similarly.\nBy experimenting with various router parameters $\\theta_1,..., \\theta_k$, we obtain a series of data points $(C_{R_{\\theta_1}},q_{R_{\\theta_1}}),..., (C_{R_{\\theta_k}},q_{R_{\\theta_k}})$ which can be graphically represented in the cost-quality $(c - q)$ plane alongside the results of LLMs for comparative analysis.\nLinear Interpolation The initial operation we introduce within this framework is linear interpolation, which enables the computation of a weighted average between any two points on the cost-quality $(c - q)$ plane.\nAs illustrated by an example in the left of Figure 2, consider two routers, $R_{\\theta_1}$ and $R_{\\theta_2}$, we can formulate a third router, $R_{int}(R_{\\theta_1}, R_{\\theta_2}, t)$, based on the following principle: given a prompt $x$ select $t \\in [0, 1]$ such that:\n$R_{int(R_{\\theta_1}, R_{\\theta_2}, t)}(x) = \\begin{cases} R_{\\theta_1}(x), & \\text{w.p. } t \\\\ R_{\\theta_2}(x), & \\text{w.p. } 1 - t \\end{cases}$\nThrough the principle of linearity of expectation, we can deduce the expected cost of $R_{int(R_{\\theta_1}, R_{\\theta_2}, t)}(x)$ in terms of $LLM_1$ and $LLM_2$:\n$E[C_{R_{int}(x)}|X \\in D] = t \\cdot C_{R_{\\theta_1}} + (1 - t) \\cdot C_{R_{\\theta_2}}$\nand the expected performance of $R_{int(R_{\\theta_1}, R_{\\theta_2}, t)}(x)$ can be defined similarly.\nNotably, for two data points $(c_1, q_1)$ and $(c_2, q_2)$ corresponding to $R_{\\theta_1}$ and $R_{\\theta_2}$ respectively, $R_{int}(t)$ can precisely interpolate any point along the line segment connecting $(c_1, q_1)$ and $(c_2, q_2)$.\nExtrapolation To ensure all routers can enrich our mathematical framework, we also introduce the extrapolation operation, which enables all routers to extend to cost domain $[0, \\infty]$. For a given router $R_\\theta$, we can trivially add more cost to the system without adding performance (for example repeat LLM generation $k$ times and only take the last generation as final output) and thus extend the cost to $\\infty$. To extend the router to a smaller cost domain, we simply interpolate the null router (zero cost, zero performance) and $R_\\theta$. Thus we are able to achieve any cost level between $[0, \\infty]$ to when comparing routers with different domains.\nIt is essential to note that the routers discussed are functionally analogous to LLMs within this context, as both can be represented as coordinates in the cost-quality (c \u2013 q) plane."}, {"title": "3.2. Non-Decreasing Convex Hull", "content": "When working with multiple routers, it's feasible to construct any affine combination of points through linear interpolation among them. Specifically, for a set $S$ of points in the cost-quality $(c - q)$ plane, these affine combinations can target any point $(c, q)$ in $R^2$ lying within the convex hull formed by $S$. We identify $S_{ch} \\subseteq S$ as the subset of points that delineate the vertices of this convex hull.\nFurthermore, it's possible to configure a non-decreasing convex hull from $S_{ch}$, ensuring that for any two points $(c_1,q_1)$ and $(c_2, q_2)$ where $c_2 > c_1$, it follows that $q_2 \\geq q_1$. Intuitively, if the extra cost of $c_2 - c_1$ does not bring any performance improvement, it is advisable to simply extrapolate $(c_1, q_1)$ to the domain of $c_2$, and $(c_2, q_2)$ could be $(c_2, q_1)$.\nFor a given routing system $R_1$, constituted by LLMs and routers plotted in the $c-q$ plane for dataset $D$, we can conceptualize a new routing system $R_1'$. This involves constructing routers $R_{\\theta_1},..., R_{\\theta_k}$, yielding points $(c_1, q_1), ..., (c_k, q_k)$. By establishing a non-decreasing convex hull $S_{ndch}$ from these points and for any cost $c$ within the range $[c_{min}, c_{max}]$, optimal performance is attainable by interpolating between the two closest cost points. This process effectively creates a new routing system that spans the complete domain $[c_{min}, c_{max}]$.\nGiven the framework established, we define the Zero router ($R_{zero}$) as a router that selects LLMs from $\\{LLM_1,..., LLM_m\\}$ based on their collective non-decreasing convex hull. For a specified cost $c$, $R_{zero}$ provides a probabilistic mix of LLMs that maximizes expected output quality with a simple, mathematics-driven routing strategy. $R_{zero}$ serves as a basic benchmark for assessing the efficacy of other routing systems; a router is deemed significant only if it demonstrates superior performance compared to $R_{zero}$."}, {"title": "3.3. Comparing Different Routing Systems", "content": "Given the agnostic nature of our comparison framework towards the router's structure, routing systems can produce an assorted set of points on the $c - q$ plane that may be non-deterministic and non-parametric, complicating direct comparisons. Leveraging the methodologies delineated previously, we have the capacity to condense these disparate points into a streamlined function\u2014specifically, a non-decreasing convex hull\u2014and subsequently distill this representation into a singular metric that encapsulates the system's characteristics.\nRouting systems often generate multiple points on the cost-quality $(c q)$ plane, making it difficult to compare the underlying systems. However, our framework allows us to transform these non-parametric points into a simpler function, specifically a non-decreasing convex hull, which can be characterized by a single numerical value.\nLet's consider two different routing systems (for example KNN and MLP-based routers), $R_\\theta$ where $\\theta \\in \\Theta$, and $R_\\lambda$ where $\\lambda \\in \\Lambda$. To compare their effectiveness, we parametrize them by sampling from $\\Theta$, $\\Lambda$ to generate a set of points: $R_{\\theta_1},..., R_{\\theta_k}$, and $R_{\\lambda_1},..., R_{\\lambda_k}$. Then, we construct non-decreasing convex hull for both groups, $R_\\theta$ and $R_\\lambda$, defined on a shared domain $[c_{min}, c_{max}]$.\nWe define AIQ (Average Improvement in Quality) for one of the routing system as follows:\n$AIQ(R) = \\frac{1}{c_{max}-c_{min}} \\int_{c_{min}}^{c_{max}} q_{R_{ndch}} dc$\nWith the equation above, we can calculate AIQs for any group of routing systems to get a clear understanding of their relative performance, which is demonstrated in the right of Figure 2. Rather than performing complex graphic analysis, AIQ allows users to measure router performance in a straightforward way."}, {"title": "4. Benchmark Construction - ROUTERBENCH", "content": "To systematically assess router performance, we have developed a dataset, ROUTERBENCH. This comprehensive dataset consists of a broad spectrum of tasks, including commonsense reasoning, knowledge-based language understanding, conversation, math, coding and retrieval-augmented generation (RAG). ROUTERBENCH is constructed by leveraging existing datasets that are widely recognized and utilized in the evaluation of leading LLMs, such as GPT-4, Gemini (Team et al., 2023), and Claude (Anthropic, 2023). This approach ensures that ROUTERBENCH is representative of the diverse challenges and requirements pertinent to mainstream LLM performance evaluation."}, {"title": "4.1. Principles in benchmark construction", "content": "The construction of ROUTERBENCH is guided by the following principles:\n\u2022 Extensive Coverage: Our selection process identified a diverse array of fields where LLMs are widely utilized, aiming for wide-ranging applicability.\n\u2022 Practical Relevance: The benchmarks chosen are of considerable significance to the industry's current applications of LLM systems, presenting a balanced challenge to the state-of-the-art LLMs, that is not too difficult nor too simplistic.\n\u2022 Extensibility: ROUTERBENCH is designed for seamless integration of additional metrics, such as latencies and throughputs, ensuring adaptability to the evolving landscape of LLM."}, {"title": "4.2. Benchmark Dataset", "content": "For the initial release, we have curated a selection of 8 representative datasets from multiple different tasks. Detailed descriptions are in Appendix A.3.\n\u2022 Commonsense Reasoning: Hellaswag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021), and ARC Challenge (Clark et al., 2018)\n\u2022 Knowledge-based Language Understanding: MMLU (Hendrycks et al., 2021)\n\u2022 Conversation: MT-Bench (Zheng et al., 2023b)\n\u2022 Math: GSM8K (Cobbe et al., 2021)\n\u2022 Coding: MBPP (Austin et al., 2021)\nRAG Dataset: Additionally, we collected 657 question-ground truth pairs from various major news platforms as well as Wikipedia. To generate the pairs, we ask GPT-4 to come up with questions that can be answered based on the given factual article and the answers, thus generating a dataset to test routers' performance on retrieval-augmented generation tasks. This initiative is designed to assess the routers' performance in a complex \"compound system\" setting \u2013 determining whether routers can adeptly navigate when retrieval abilities are also in play. For instance, when dealing with news published after the GPT-4 knowledge cutoff, it is expected that routers will more frequently opt for models that possess the functionality to access and search the latest internet-based information (e.g., the sonar-medium-online model)."}, {"title": "4.3. Dataset Construction Process", "content": "For the compilation of our benchmark dataset, we perform inference with 14 different LLMs, with 3 of them specific to the RAG dataset\u00b3, including both open-source and proprietary models. This process was applied to each of the eight datasets and the RAG dataset enumerated in Section 4.2, which is also illustrated in Figure 1. The selected LLMs are as follows and more details are in Appendix A.1:\nOpen Source Model: Llama-70B-chat (Touvron et al., 2023), Mixtral-8x7B-chat (Madaan et al., 2023), Yi-34B-chat (AI et al., 2024), Code Llama-34B (Rozi\u00e8re et al., 2023), Mistral-7B-chat (Jiang et al., 2023a), WizardLM-13B (Xu et al., 2024)\nProprietary Model: GPT-4, GPT-3.5-turbo (OpenAI, 2023), Claude-instant-v1, Claude-v1, Claude-v2 (Anthropic, 2023), You.com API, sonar-small-online, sonar-medium-online.\nIn total, there are 405,467 samples in ROUTERBENCH, covering 11 models, 8 datasets, and 64 tasks."}, {"title": "4.4. A Pilot Study: The Oracle Router", "content": "We assessed the performance of various models across the eight datasets, with more details in (A.5 and 7) while aggregate results are illustrated in Figure 3. The Oracle represents the best possible router: the one that always routes to the best-performing LLM (if there are multiple of them, then route to the cheapest one).\nResult: We note that the Oracle router achieves near-optimal performance at a low cost, highlighting the potential for efficient routing among LLMs. Although proprietary models like GPT-4 offer superior performance, their higher cost compared to open-source alternatives is a significant drawback. Other factors such as overalignment could also hurt the generation quality of proprietary models such as Claude 2 (refer to Appendix C). The heatmap in Figure 3 illustrates that, despite WizardLM-13B and Mistral-7B achieving only about 50% accuracy across tasks, their affordability leads to frequent selection by the Oracle, which prioritizes them when they provide correct responses. Moreover, the surprising observation that GPT-4 is seldom chosen suggests the existence of less expensive LLMs that can deliver high-quality answers for most queries. This underscores the substantial opportunity for enhancing LLM systems through cost-effective routing without sacrificing quality."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Predictive Router", "content": "We propose a novel set of predictive routers, which do not require pre-generation of LLM outputs. Specifically, we introduce a router R : xi \u2192 LLM, constructed as follows: for an input xi, the performance score for LLM; is calculated via:\nperformance $score_{ij} = \\lambda \\cdot P_{ij} \u2013 cost_j$\n$P$ denotes the predicted performance of LLM; on sample $x_i$, with $\\lambda$ representing the willingness to pay (WTP) parameter that delineates the cost-performance trade-off. A higher $\\lambda$ indicates a preference for superior performance at a higher cost. We approximate total cost using the cost per token metric. The routing decision for the predictive router is thus formulated as selecting the LLM that optimizes the performance score.\nTo estimate $P$ for each input across models, we implemented two supervised regression approaches: k-nearest neighbors (KNN) and multi-layer perceptron (MLP) inspired by (Shnitzer et al., 2023). We allocated a fraction of the dataset for training a performance predictor for each task, assessing its efficacy on the remainder.\nSpecifically, the KNN router estimates performance $score_{ij}$ by identifying the $k$ nearest samples in the training set $D_{train}$ and opting for $LLM_i$ demonstrating optimal performance within this subset.\n$P_{KNN}(x_i) = \\frac{1}{k} \\sum_{x_j \\in NN_k(x_i, D_{train})} q(o_i^*)$\nWhere $NN_k(x_i, D_{train})$ signifies the subset of $k$ nearest neighbors to the sample $x_i$ within the training dataset $D_{train}$.\nSimilarly, for MLP router, we have trained a set of MLP models to predict performance\n$P_{MLP}(x_i) = f(W_n \\cdot \\sigma(...\\cdot \\sigma(W_1 \\cdot x_i + b_1)... + b_n)$\nThose series of KNN and MLP routers are trained with varying hyperparameters, and we present the experimental results derived from the optimal hyperparameter configurations."}, {"title": "5.2. Non-Predictive Routers", "content": "This category of routers generates answers from a sequence of Large Language Models (LLMs), evaluates these answers, and bases routing decisions on the evaluation outcomes. Drawing inspiration from (Chen et al., 2023; Wang et al., 2023b), we introduce a cascading router comprising of a total cost parameter $T$, and a sequence of $m$ LLMs, denoted as LLM; : text \u2192 text, ranked from the least to the most expensive in terms of computational cost and expected accuracy. A key component of its operation is a scoring function $g$: text \u2192 [0,1] paired with a threshold $t$ (the \"judge\"). Upon receiving a request, it is initially processed by LLM\u2081. If $g(0_1) > t$, the output $0_1$ is selected and the process terminates; otherwise, if the cumulative cost is still less than total cost $T$, the router proceeds to the next LLM in the sequence, and returns the current output if not.\nAlthough developing an effective scoring function $g$ for a specific task in a production setting presents challenges, within the context this paper, the router possesses perfect knowledge of the final score, enabling it to consistently select the most cost-effective model that yields a satisfactory response (akin to an oracle). To simulate real-world performance more accurately, we introduce an error parameter $\\epsilon \\in [0, 1]$. The adjusted scoring function $g_{\\epsilon}(o)$ is defined as:\n$g_{\\epsilon}(o) = \\begin{cases} 1 - g(o) & \\text{with probability } \\epsilon \\\\ g(o) & \\text{with probability } 1 - \\epsilon \\end{cases}$\nA variant of non-predictive router is overgenerate-and-rerank, which generates all potential outcomes from the LLM, assesses each, and outputs the optimal one as determined by a designated reward function. Although its practical application is limited due to significant costs, we will present its results for demonstration."}, {"title": "5.3. Main Results", "content": "Predictive Router With the KNN and MLP router design, we present the performances of predictive routers across all tasks (other than MT-Bench). The dataset for each task is randomly partitioned into two splits, where the routers are trained on 70% and evaluated on the rest 30%. We exclude MT-Bench in this set of experiments due to its limited size to perform such a train-test partition. As shown in Figure 4, both KNN routers and MLP routers achieve the level of performance to the best individual LLMs with lower or similar costs, demonstrating the effectiveness of the proposed routing solutions, despite their simplicity. However, none of the routing algorithms significantly outperform the baseline Zero router (The routers exhibit higher AIQ than the Zero router for MMLU and Winogrande, achieved comparable AIQ for Hellaswag and GSM8K, and underperform on ARC-Challenge and MBPP), the oracle router consistently exceeds all other routers and LLMs in performance, underscoring the room for further advancements in routing algorithms design.\nCascading Router We present results for cascading routers on MMLU, MBPP, and GSM8K in Figure 5. The results indicate that with each error rate, as the total cost $T$ increases, the cascading router's performance improves due to the availability of a larger budget for selecting more appropriate models. For lower error rates, the cascading router demonstrates superior performance compared to the Zero router, as evidenced by the higher AIQ value. The router with a zero error rate judge quickly approximates the performance of the Oracle at the same cost and achieves comparable results as the cost further increases. Figure 5 illustrates the cascading routers' effectiveness, showing they surpass both individual LLMs and the Zero router by a significant margin when the router's judge has an error rate of up to 0.1. This indicates the routing technique's potential when paired with an effective judge.\nHowever, as the judge's error rates increase, the performance of the cascading router may deteriorate rapidly, particularly when the error rate exceeds 0.2. Achieving a sufficiently low error rate for certain real-world tasks to benefit from cascading routers might be challenging. Additionally, the sequence in which LLMs are chosen plays a crucial role in performance and offers room for optimization (Chen et al., 2023). Our findings present a simulated upper limit for this method, highlighting the significant potential and the necessity of exploring the optimal implementation of cascading routers for specific applications."}, {"title": "5.4. RAG Results", "content": "Building on results above, we simultaneously compared various router types, including predictive and cascading routers, on the RAG dataset. We used the same setting for KNN routers and MLP routers while selecting error rate 0.2 for cascading routers. We randomly partitioned RAG dataset into two splits: 70% for training predictive routers and 30% for evaluating all routers. Figure 6 demonstrates that all routers performed comparably to the Zero Router. Nevertheless, further analysis revealed that routers with internet access significantly outperformed GPT-4 and GPT-3.5 Turbo on news platform data due to their real-time information retrieval capability. Specifically, the KNN router demonstrated a preference for routing news data to models with internet access, although it struggled with wiki data, leading to sub-optimal overall results. Our findings highlight the potential of model routing to enhance LLM applications within the \"Compound AI Systems\u201d scenario.\nContrary to previous findings, the cascading router underperformed in this experiment, likely due to increased cost discrepancies among models and the inefficacy of starting with the least expensive model. This underscores the necessity for optimized LLM candidate ranking in cascading router designs, a principle that has also been demonstrated in (Chen et al., 2023)."}, {"title": "6. Limitations and Future Work", "content": "ROUTERBENCH currently only focuses on performance and economic cost. It is meaningful to include more evaluation criteria, such as latency, throughput, and others to capture a more comprehensive understanding of router capabilities and limitations. There are also many LLMs and tasks that are not included in ROUTERBENCH due to the limitation of time, and future iterations of this benchmark would include datasets that cover more tasks to effectively evaluate the ever-growing capability of LLMs, and also to add newer LLMs as they are being released.\nOur current work only evaluates the efficacy of predictive and cascading routers, yet there exists considerable scope for investigating further router designs, as highlighted in Section 5.3. Delving into more advanced router designs is crucial for enhancing routing efficiency. It's noteworthy that our evaluation within the RAG context was limited to models possessing inherent retrieval capabilities. Addressing the challenge of implementing two-stage routing, which encompasses both retrievers and LLMs, remains critical. Such an approach could significantly refine router evaluations on standard RAG tasks, including HotpotQA (Yang et al., 2018) and NaturalQuestions (Kwiatkowski et al., 2019), by ensuring more precise assessments.\nFurthermore, although the seven datasets in ROUTERBENCH offer broad coverage across a variety of tasks, incorporating domain-specific tasks that require long-tail skills, like translation of low-resource languages, could reveal additional intriguing aspects of LLM routing. This enhancement would align the benchmark more closely with real-world application scenarios. Efforts to integrate such tasks in future versions are planned."}, {"title": "7. Conclusion", "content": "We present ROUTERBENCH, a benchmark specifically designed for the evaluation of router mechanisms within multi-LLM systems. By addressing the critical need for standardized evaluation in this domain, our benchmark provides a comprehensive dataset and a theoretical framework specifically designed for the nuanced analysis of router cost-efficiency and performance. The insights from our study shed light on the effectiveness of various routing strategies, emphasizing the necessity for advanced LLM routing systems and refined routing methods. This work establishes a robust and scalable benchmark for router evaluation and aims to facilitate future progress in the efficient and cost-effective deployment of Large Language Models."}, {"title": "A. Additional Dataset Details", "content": ""}, {"title": "A.1. Model Details & Cost Estimation", "content": "For all proprietary models, we calculate the cost of input and output results based on their API pricing. For open-source models, we utilize Together AI 5 to obtain results and reference costs. For the RAG experiment, we refer to the API pricing of You.com 6 and Perplexity 7 for cost estimation."}, {"title": "A.2. Dataset Generation for RAG", "content": "We designed two subtasks of different nature in dataset generation step to capture the complexity and heterogeneity of real-work RAG use cases.\n\u2022 Wikipedia: this subtask consists of 308 questions generated from wikipedia pages that focus on historic perspective or timeless content (e.g. wikipedia pages on history of pizza, or black holes).\n\u2022 News: this subtask consists of 349 questions generated from news articles of major new outlets published within 1 month of experiment.\nDifferent LLM and compound AI systems are expected to have different performance on those subtasks of different nature, and thus create room for routing."}, {"title": "A.3. Dataset Details", "content": "MMLU (Hendrycks et al., 2021): A benchmark that measures the knowledge acquired by models during pretraining and evaluates models in zero-shot and few-shot settings across 57 tasks, testing both knowledge and reasoning on different fields of human knowledge.\nHellaswag (Zellers et al., 2019): A dataset that challenges models to pick the best ending choice to a sentence given. It uses Adversarial Filtering(AF) to create a Goldilocks zone of complexity, wherein generations are largely nonsensical to humans but always make models struggle.\nGSM8K (Cobbe et al., 2021): A dataset of diverse grade school math word problems, testing a model's ability to perform multi-step mathematical reasoning.\nARC Challenge(Clark et al., 2018) A rigorous question answering dataset, ARC-Challenge includes complex, different grade-school level questions that require reasoning beyond simple retrieval, testing the true comprehension capabilities of models. Arc Challenge dataset contains those that both a retrieval and a co-occurrence method fail to answer correctly)\nWinogrande (Sakaguchi et al., 2021): A large-scale and increased harness dataset inspired by the original Winograd Schema Challenge(WSC) (Levesque et al., 2012) tests models on their ability to resolve pronoun ambiguity and their ability to understand the context with commonsense knowledge.\nMBPP (Austin et al., 2021): The benchmark is designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.\nMT-Bench (Zheng et al., 2023b): This dataset contains 3.3K expert-level pairwise human preferences for model responses generated by 6 models in response to 80 MT-bench questions, multi-run QA. The 6 models are GPT-4, GPT-3.5, Claude-v1, Vicuna-13B (Zheng et al., 2023a), Alpaca-13B (Taori et al., 2023), and LLaMA-13B (Touvron et al., 2023). The annotators are mostly graduate students with expertise in the topic areas of each of the questions. In this work, we only used the 80 questions to generate model responses for ROUTERBENCH."}, {"title": "A.4. More Details on Dataset Construction", "content": "Each sample in the benchmark dataset will have the following attributes:\n\u2022 sample_id: contain the information about the name of the sub-task, the split of dataset, and the index of the data in that dataset. Example: mmlu-astronomy.val.5\n\u2022 model_name: the model used to perform inference for this sample. Example: GPT-4\n\u2022 eval_name: the source data this specific sample comes from. Example: hellaswag.dev.v0\n\u2022 prompt: prompt sentence. Example: The following are multiple choice questions...\n\u2022 model_response: Model's output. Example: The answer is A)\n\u2022 performance: the result compared to true label. Example: True/False\n\u2022 cost: for proprietary model, we use API cost to calculate; for open source model, we use Together AI to call the model and use their cost as reference. Example: 0.00019\n\u2022 true label: the true label or gold response for this prompt. Example: True/False"}, {"title": "A.5. Evaluation Metrics", "content": ""}]}