{"title": "AI Conversational Interviewing: Transforming Surveys with LLMs as Adaptive Interviewers", "authors": ["Alexander Wuttke", "Matthias A\u00dfenmacher", "Christopher Klamm", "Max M. Lang", "Quirin W\u00fcrschinger", "Frauke Kreuter"], "abstract": "Traditional methods for eliciting people's opinions face a trade-off between depth and scale: structured surveys enable large-scale data collection but limit respondents' ability to express unanticipated thoughts in their own words, while conversational interviews provide deeper insights but are resource-intensive. This study explores the potential of replacing human interviewers with large language models (LLMs) to conduct scalable conversational interviews. Our goal is to assess the performance of A\u0399 Conversational Interviewing and to identify opportunities for improvement in a controlled environment. We conducted a small-scale, in-depth study with university students who were randomly assigned to be interviewed by either AI or human interviewers, both employing identical questionnaires on political topics. Various quantitative and qualitative measures assessed interviewer adherence to guidelines, response quality, participant engagement, and overall interview efficacy. The findings indicate the viability of AI Conversational Interviewing in producing quality data comparable to traditional methods, with the added benefit of scalability. Based on our experiences, we present specific recommendations for effective implementation.", "sections": [{"title": "1 Introduction", "content": "A structured survey is the most popular tool for learning people's thoughts (Groves, 2009; Kertzer and Renshon, 2022; Stantcheva, 2023). These surveys typically gather individual orientations through self-reports, asking respondents to select from predefined options on fixed questions. This method allows for efficient data collection across large populations, producing structured, tabular data that is straightforward to analyze and compare across respondents (Krosnick, 1999; Groves, 2009). Due to these benefits, structured surveys continue to hold a prominent position in both academic and commercial research. Despite their established utility, structured surveys with predefined response options have significant limitations (Schwarz and Hippler, 1987; Kash, 2013). Their static and impersonal nature often leads to respondent fatigue, which can diminish engagement and, consequently, the quality of responses (Krosnick, 1999; Jeong et al., 2023). More critically, the rigid format of these surveys constrains respondents from fully expressing their thoughts, restricting them from offering responses that researchers may not have anticipated (Chang et al., 2021; Esses and Maio, 2002; Reja et al., 2003; Baburajan et al., 2022; Duck-Mayr and Montgomery, 2023). This limitation hampers the discovery of new phenomena and prevents a comprehensive understanding of the full spectrum of people's attitudes. An alternative to structured surveys is conversational interviewing, sometimes called in-depth or semi-structured or qualitative interviewing (Adeoye-Olatunde and Olenik, 2021; Kallio et al., 2016; Adams, 2015). This approach involves interviewers engaging with respondents in a more open-ended format, allowing them to freely express their thoughts on topics of interest. The dynamic nature of in-depth interviews helps alleviate respondent fatigue and permits the exploration of opinions that might not fit within pre-defined response options. However, this approach requires skilled interviewers capable of conducting nuanced conversations, which limits its application to small sample sizes due to the associated costs.\nSo, survey research inherently faces a trade- off between depth and scale: researchers must choose between conducting in-depth explorations with small groups through personalized interviews or gathering extensive but uniform data via structured surveys. However, recent advances in natural language processing (Dubey et al., 2024; \u00dcst\u00fcn et al., 2024; Workshop et al., 2023; Costello et al., 2024) present new possibilities for addressing this dilemma. The conversational capabilities of instruction-finetuned large language models (LLMs) (Wei et al., 2022; Ouyang et al., 2022) have"}, {"title": "2 Related Work", "content": "To implement and evaluate AI Conversational Interviews this study combines insights from three distinct lines of work that have rarely been combined.\nAdvances in AI research have facilitated multiple ongoing commercial and academic projects that use LLM-powered chatbots for in-depth, qualitative, or semi-structured interviews, as they are interchangeably called (Chopra and Haaland, 2023; Weidmann et al., 2024). Although these implementations vary, they collectively highlight the potential of LLMs for conducting in-depth interviews. However, several critical questions regarding the precise implementation of AI interviewing remain unresolved.\nQualitative studies have extensively explored best practices for conducting in-person interviews (Adams, 2015), offering insights that can inform LLM-based interviewing. However, some of these guidelines must be adapted to meet the specific needs of LLM interviewing.\nStudies in survey methodology have extensively examined how different interview implementations influence responses. One line of research has focused on interviewer and mode effects (Mittereder et al., 2018; Malhotra and Krosnick, 2007). The presence of an interviewer significantly impacts respondents, often leading to greater engagement but also increasing the likelihood of socially desirable responses (Atkeson et al., 2014; West and Blom, 2016). In this vein, studies on conversational interviewing has shown that a more active and flexible interviewer who engages with questions from respondents can improve data quality (Schober and Conrad, 1997; Davis et al., 2024; Mittereder et al., 2018).\u00b9 Another important factor is the input mode. Responses to open-ended questions vary depending on whether they are submitted via text or speech. Text input typically requires more effort, which can result in shorter but more carefully considered responses (Gavras et al., 2022; H\u00f6hne et al., 2024). Consequently, whether AI interviewing uses text\n\u00b9Our method is similar to traditional \"conversational in- terviewing\" in that it enhances flexibility during the inter- view. However, AI Conversational Interviewing differs by highlighting the flexibility of the respondents rather than the interviewer."}, {"title": "3 Study Design and Implementation", "content": "Our study pursues two goals:\n1. Assess the performance of AI Conversational Interviewing (in comparison to human-led interviewing)\n2. Identify problems and opportunities for improvement of AI Conversational Interviewing\nWe conducted a small-N study among university students in a controlled environment. Ahead of data collection, we pre-registered our research questions, research design, and evaluation metrics (cf. OSF Registry).\nWe conducted both AI-led and human-led interviews as part of a class activity, where students were randomly assigned to serve as either interviewers or respondents in the respective conditions. Identical questionnaires were used in both interview settings. After the interview respondents filled out a structured questionnaire to evaluate their interview experience. In the AI interview condition, students monitored the interviewees in real-time to identify any technical issues."}, {"title": "3.1 Procedure", "content": "The study was embedded in a student seminar on survey methodology that was hosted via Zoom. Students were informed that they would participate in a pilot study of conversational interviewing. The seminar proceeded with a detailed script (cf. Appendix C.2), lasting about 120 minutes:\n1. Participants were informed about the upcoming procedure, the technical requirements were laid out, and they were asked for consent to participate and collect their data.\n2. As preparation for the upcoming tasks, an instructor gave a 10-minute presentation about scientific approaches to interview respondents, and rules for good interviewer behavior.\n3. Students were paired up and randomly assigned different roles:\n(a) Students participated in both a human- conducted and an AI-conducted interview, with the sequence randomly assigned\n(b) In the human-conducted interviews, students took on roles as either respondents or interviewers\n(c) In the AI-conducted interviews, students served as either respondents or observers, monitoring for any technical issues during the interview"}, {"title": "3.2 Model setup", "content": "We implemented a voice-assisted AI Conversational Interviewing pipeline with GPT-42 and a Chainlit-based user interface, using the following task-adapted prompts:\n1. the system instruction to act as an interviewer;\n2. the user instructions with specific guidelines, derived from the qualitative literature on human in-depth interviewing (Adams, 2015), specifying desirable and undesirable interviewer behavior (cf. Appendix C);\n3. a task questionnaire on politics and democracy, developed by a democracy researcher among the authors (cf. Appendix D).\n\u00b2GPT-4 turbo, version: 04/2024\n\u00b3https://chainlit.io/"}, {"title": "3.3 User interface", "content": "To enable voice-assisted interviewing, we developed a user interface based on Chainlit\u00b3, with customization for audio input and output as shown in Figure 2). Our voice-assisted implementation allowed respondents to choose between voice and text modes for both the model output (interviewer questions) and their input (responses). When respondents selected audio input, their speech was transcribed into text, which they could then review and edit before submitting their responses. This approach sought to blend the spontaneity and expressiveness of audio input with the precision and control offered by text-based refinements. For audio output, interviewer questions were displayed as text and could be delivered as voice upon the user's request. We utilized OpenAI Whisper (Radford et al., 2023) for text-to-speech transcriptions of model-generated text."}, {"title": "3.4 Interview Content", "content": "Human and AI in-depth interviews were conducted with an identical questionnaire in English (cf. Appendix D). The questionnaire concerned questions on politics and democracy (e.g. Let us talk about democracy. When you think about how democracy works right now in Western countries such as Germany, what are the good things that come to mind? or And what do you think \u201cpolitics\u201d is? How would you define this term?). Human-led interviews lasted 16 minutes, on average. AI-led interviews lasted 22 minutes, on average."}, {"title": "3.5 Evaluation Metrics", "content": "We computed a set of quantitative and qualitative measures, designed to evaluate the effectiveness, efficiency, and quality of AI-conducted interviews in comparison to traditional human-conducted interviews. Besides quantitative text-based metrics (Q), we evaluate indicators of participant engagement, response depth, and coherence (\u25c9). Additionally, we gathered survey feedback () on the interview experience from participants in both interview settings.\nInterviewer behavior: Human coding. We provided two research assistants with the interviewer guidelines, which outlined desirable and undesirable interviewer behaviors (cf. Appendix H). The research assistants then manually double-coded each conversational turn of the interviewer (e.g., a question) to identify any potential violations of these guidelines. In essence, we assessed whether the human and AI interviewers adhered to the instructions.\nInterview responses: Human coding. Two research assistants were provided with a detailed coding manual to assess the quality of the participants' responses (cf. Appendix G). They assessed factors such as whether a response directly addressed the question, whether the participant appeared engaged, and the specificity and detail of the response. In essence, we evaluated whether the interviews elicited insightful responses from participants.\nQ Interview responses: Computational analysis. We computed the Flesch Reading Ease scores on the transcribed interview data to evaluate response readability and length (Flesch, 1948). Additionally, we calculated the number of tokens per response to obtain a more granular measure of linguistic complexity and information density.\nStructured post-interview survey. After each interview, the respondents were asked to fill out a survey on their experience (cf. Appendix K).\n\u25c9 Real-time problem recording. During the AI in-depth interview, one student from each pair was assigned to observe the other student's interaction with the AI interviewer. The observer was given a form to document any technical difficulties or other issues the respondent encountered during the interview (cf. Appendix F)."}, {"title": "4 Findings", "content": "We collected data on six human-led and five AI- conducted in-depth interviews. Human-led interviews were audio-recorded and then transcribed.\nFigure 2 presents an example snippet from an AI conversational interview, showcasing how the interviewer engages in active listening by occasionally repeating the preceding answer, as instructed.\nQualitative inspection of the transcribed data shows that both the AI and human interviewers faithfully followed the provided questionnaire. Manual coding of all interviewer behavior shows that neither humans nor AI always acted in full accordance with the interview guidelines (Figure 6). Summarizing across all coded categories, we counted 72 violations per AI interview and 64 violations (-11.11%) per human interview, on average.\nWhile error rates of human and AI interviewers were at similar levels, the nature of the errors differed. Contrary to instructions, human interviewers often failed to engage in active listening, which involves restating the respondent's answer to ensure proper understanding. Specifically, 94 percent of guideline violations related to active listening were committed by human interviewers, compared to only 6 percent by the AI interviewer (cf. Appendix I)). Conversely, and in contrast to internal pre-tests, the AI interviewer predominantly failed to follow the instruction to 'ask follow-up questions when a respondent gives a surprising, unexpected, or unclear answer,' with 88 percent of violations of this rule attributed to the AI interviewer. These findings highlight the challenge of finding the right balance between asking too many and too few follow-up questions in any in-depth interviewing setting. Moreover, the fact that the interviewer model had previously succeeded in asking appropriate follow-up questions during internal tests serves as a reminder that even minor modifications to prompts can lead to unintended side effects.\nAnother guideline was to avoid any behavior that could bias the respondents' answers. However, despite the instruction to 'not take a position on whether their answers are right or wrong,' the AI interviewer occasionally judged the respondent, typically in an encouraging manner (e.g., 'Your definition of politics is quite insightful', 67 percent attributed to the AI interviewer). In contrast, human interviewers sometimes erred by guiding respondents through associations or suggestions for their answers, accounting for 75 percent of such violations. Overall, while no interviewer setting perfectly adhered to the guidelines, these findings suggest that AI interviewers demonstrate a similar level of effectiveness to human student interviewers in following instructions for in-depth interviewing. However, achieving optimal performance relies on fine-tuning and thoroughly testing model instructions.\nTurning from the interviewer's behavior to the participants' responses, we see that both interviewing settings succeeded in eliciting answers from respondents at substantial lengths. In the AI interviewer setting, the average response length was 52.39 words. In the human interview setting, the average response length was 32.81 words (\u2193-62.63%).\nWhile participants' answers to the AI interviewer were substantial in length, were they also meaningful in substance? The transcribed responses were given to human coders to rate response quality. While we observe minor differences across setting, overall, the ratings indicate a similar response quality (Figure 3). Responses in human and AI interviews were rated as similarly clear (i.e., easy to understand), empathetic (i.e., sensitive towards the interviewer), engaged (i.e., high level of enthusiasm or interest), complex (i.e., advanced vocabulary), grammatically correct (i.e., error-free), specific (i.e., detailed information), and adequate in tone (i.e., suitable for the context).\nOne particularly important outcome is the assessed relevance of the responses-whether they are useful and directly related to the question asked. Once again, no substantial differences in relevance were observed between AI and human interviews. While these estimates should be interpreted with caution due to the considerable imprecision associated with the small sample size, the findings suggest that engaging with an AI interviewer does not lead to a significant decline in response quality compared to a human interviewer. We interpret this as a proof-of-concept, underscoring the general viability of AI Conversational Interviewing. Our setup allowed for a close-up investigation of how our AI interviews unfolded in practice. Real-time problem recording during AI interviews showed that respondents interacted seamlessly with our user interface, which resembled familiar chat interfaces, indicating that no learning curve was necessary. Yet, occasionally, the latency of the GPT responses was criticized (e.g. \u201cSometimes the time it takes to produce an answer is unexpectedly long. But it is not really off putting.\u201d, \u201crun time is quite slow, it takes a couple (>5 seconds)\u201d). While this latency may reflect similar reaction times in human-to-human chat interactions, participants appeared to prefer shorter waiting times when they were aware they were interacting with an AI interviewer."}, {"title": "5 Discussion and Recommendations", "content": "Applying the questionnaire to a student sample with both human and AI interviewers demonstrates the general viability of AI Conversational Interviewing. When properly implemented, AI Conversational Interviewing can collect high-quality data. A comprehensive set of qualitative and quantitative metrics suggests that AI interviewing maintains quality comparable to that of human interviewing, but at significantly lower costs, thereby making in-depth interviews more scalable.\nAlthough these findings highlight the potential of AI Conversational Interviewing, the success of the method depends on its precise implementation. Based on our comprehensive analysis, we present five recommendations for the future development and employment of AI-driven in-depth interviews:\nLeverage existing knowledge. When specifying desired interviewer behavior, it is crucial to draw on established principles from survey methodology. These practices, developed through extensive research and practical experience, offer proven guidelines for effective implementation.\nContext-specific definition of desired interviewer behavior. It is crucial to make deliberate judgment calls to tailor the desired interviewer behavior to your specific research context. This may involve decisions on aspects such as the importance or frequency of follow-up questions, the depth of probing on certain topics, or the level of formality in the interview tone (for example, Weidmann et al. (2024) demonstrated the effectiveness of empathy prompting). Each research project may require a unique approach to AI interviewer behavior to ensure the collection of appropriate data.\nConsider user experience. The interface through which participants interact with the AI interviewer is crucial to the success of the interview. It is essential to rely on familiar and intuitive user interfaces that minimize cognitive load and technical barriers. Well-designed interfaces enable participants to focus on providing thoughtful responses rather than being distracted by technical difficulties.\nCareful prompting. The prompts provided to the AI interviewer are crucial to its performance. Conduct thorough pre-testing to ensure that the AI's behavior aligns with your established guidelines. It is important to consider the potential unintended side-effects of modifying prompts, as even minor adjustments can lead to significant changes in interviewer behavior or question interpretation (Tam et al., 2024; Sclar et al., 2024; Zhu et al., 2024).\nInput mode matters. Recognize that the chosen input mode (e.g., text or speech) will significantly influence participant behavior by eliciting different psychological responses. Response patterns may vary across several outcomes, sometimes in contrasting ways. For instance, spoken responses might be longer but less detailed, while written responses may be shorter yet more concise and thoughtfully constructed. The choice of input mode should be made with careful consideration of your research objectives and the type of data you aim to collect."}, {"title": "6 Conclusion", "content": "Our research contributes to the growing field of AI- supported interviewing by offering initial insights through an in-depth evaluation process. We assessed AI performance using a variety of quantitative and qualitative evaluation methods, documenting the challenges participants faced and comparing AI-conducted interviews with human-led ones. To ensure transparency, we have made our pipeline, questions, and data publicly available. Based on our preliminary findings, we propose five areas for consideration in future implementations: integrating established survey methodology principles, adapting AI behavior to different contexts, designing user-friendly interfaces, conducting comprehensive pre-testing, and being aware of input mode effects. While our results highlight the potential of AI Conversational Interviewing, it is important to recognize that outcomes are heavily dependent on the specific implementation methods used."}, {"title": "Limitations", "content": "Several limitations reflect our study's design of a close-up monitoring of AI interviewing in practice. The study's small sample limits the generalizability of the findings. Our decision to have students monitor the AI interviewing process impedes investigating whether the absence of a human being fosters respondents' proclivity to discuss sensitive topics which may be an additional advantage of AI Conversational Interviewing. Our participants were students with an interest in survey methodology which may have been more motivated than ordinary participants. Furthermore, the use of a closed model restricts the study's replicability compared to the transparency that could be achieved with an open-source model (Spirling, 2023). We chose GPT-4 because it was the state of the art at the time of the interviews and offered social science researchers the most accessible opportunity for application (Palmer et al., 2024). By showing the pitfalls of the best-performing model across several benchmarks, we aimed to provide a starting point for an open discussion on this type of model. For future research, we plan to compare the capabilities of different models, including strong open-source models such as Llama 3.1 (Dubey et al., 2024), to provide a more comprehensive and application-oriented view of AI interviewing techniques. Finally, our study concerned collecting data via AI Conversational interviews and not its analysis where researchers may rely on computational methods for text analysis (Baden et al., 2022; Banks et al., 2018; DiMaggio, 2015; Grimmer et al., 2022)."}, {"title": "Appendix", "content": ""}, {"title": "A Ethics", "content": "In conducting our study on democracy aspects with students, we prioritized several key ethical principles. Firstly, we ensured informed consent by providing all participants with comprehensive information about the study's purpose, methods, and potential risks before seeking their agreement to participate. This also included informing students in the AI interview condition that they would be interacting with an LLM. Secondly, we maintained strict privacy and confidentiality measures, including the anonymization of data and secure storage of all collected information, to protect student identities. Lastly, we are committed to transparency in our research process. We will openly share our methodology and acknowledge any limitations of our study, thereby enabling reproducibility and facilitating critical evaluation of our findings by the broader research community."}, {"title": "B Chat Interface", "content": "We used a standard chat interface (Fig. 5) for our AI-conducted interviews, a format now familiar to many. The conversation unfolded in a series of messages, with the interviewer's questions and the Al's responses clearly distinguished. The participants were able to see the AI's questions promptly, mimicking a real-time dialogue, and were able to provide their answers in a chat interaction. This setup allowed for a smooth flow of the interview, enabling us to focus on the content rather than the technology. The familiar chat format made the AI-driven interview process feel more natural and accessible, even for those new to AI interactions."}, {"title": "C Chat-GPT Model Prompts", "content": ""}, {"title": "C.1 Your role as an AI interviewer", "content": "You are a survey interviewer named 'InterviewGPT', an AI interviewer, wanting to find out more about people's views, you are a highly skilled Interviewer AI, specialized in conducting qualitative research with the utmost professionalism. Your programming includes a deep understanding of ethical interviewing guidelines, ensuring your questions are non-biased, non-partisan, and designed to elicit rich, insightful responses. You navigate conversations with ease, adapting to the flow while maintaining the research's integrity. You are a professional interviewer that is well trained in interviewing people and takes into consideration the guidelines from recent research to interview people and retrieve information. Try to ask question that are not biased. The following is really important: If they answer in very short sentences ask follow up questions to gain a better understanding what they mean or ask them to elaborate their view further. Try to avoid direct questions on intimate topics and assure them that their data is handled with care and privacy is respected."}, {"title": "C.2 Guidelines for asking questions", "content": "It is Important to ask one question at a time. Make sure that your questions do not guide or predetermine the respondents' answers in any way. Do not provide respondents with associations, suggestions, or ideas for how they could answer the question. If the respondents do not know how to answer a question, move to the next question. Do not judge the respondents' answers. Do not take a position on whether their answers are right or wrong. Yet, do ask neutral follow-up questions for clarification in case of surprising, unreasonable or nonsensical questions. You should take a casual, conversational approach that is pleasant, neutral, and professional. It should neither be overly cold nor overly familiar. From time to time, restate concisely in one or two sentences what was just said, using mainly the respondent's own words. Then you should ask whether you properly understood the respondents' answers. Importantly, ask follow-up questions when a respondent gives a surprising, unexpected or unclear answer. Prompting respondents to elaborate can be done in many ways. You could ask: \"Why is that?\u201d, \u201cCould you expand on that?\u201d, \u201cAnything else?\u201d, \u201cCan you give me an example that illustrates what you just said?\u201d. Make it seem like a natural conversation. When it makes sense, try to connect the questions to the previous answer. Try to elicit as much information as possible about the answers from the users; especially if they only provide short answers. You should begin the interview based on the first question in the questionnaire below. You should finish the interview after you have asked all the questions from the questionnaire below."}, {"title": "C.3 Questions", "content": "Please definitely ask and include the following questions in your interview, keep the order but do not read out the enumeration (Question X):\n1. Before we start with the questions on society and politics, please tell us the number of the breakout room that you are currently in.\n2. Let's start. Please note that there are no right or wrong answers. We are just interested in your views.\nWe begin with a hypothetical scenario where a group of people need to make decisions. We want to know what you think is the best way for this group to decide together. It's important to note that we're interested in the decision-making process itself, not in what the final decision should be. Imagine a group of 10 people are deciding where to have a dinner event. Seven people want to have the event at a Japanese sushi restaurant. Three people cannot eat sushi because they have fish allergies and they want to have the event at an Italian restaurant instead. They have discussed this issue for a while but have not come to a conclusion. How should the group decide what to do?\n1. Can you think of other ways to make decisions apart from the method you just described? What do you see as the strengths and weaknesses of these alternative approaches?\n2. Let's talk a bit about politics. On a scale from 1 (not interested at all) to 7 (very interested), how interested are you in politics?\n3. Can you elaborate and explain your level of interest in politics?"}, {"title": "4. And what do you think \"politics\" is? How would you define this term?", "content": "5. Think back to the last time you took part in an action that you considered \"political\", whether it was a small or significant act. If you're comfortable sharing, what was the most recent political activity you participated in?\n6. Consider a scenario where a 7-year-old boy decides to stop eating meat after watching a documentary on meat production, but his mother insists that he should continue to eat meat. Do you believe this situation raises a political issue within the family? Are they discussing politics?\n7. Can you think back and tell us about an instance where politics made you feel very disappointed or very satisfied?\n8. Now that we have talked a little bit about the meaning of \"politics\" would you reconsider your definition of \"politics\"?\n9. Let us talk about democracy. When you think about how democracy works right now in Western countries such as Germany, what are the good things that come to mind?\n10. And what are the bad things that come to your minds about democracy in the West?\n11. Generally speaking, what makes a country democratic? In your view, what are the most important elements of a democracy?\n12. The architect of Munich's Olympiapark for the 1972 Olympics aimed to create a democratic landscape that is open and accessible to all. In what way do you think public parks do or do not contribute to the principles of democracy in society?"}, {"title": "D In-depth Interviewing Questionnaire", "content": ""}, {"title": "Question 1", "content": "Before we start with the questions on society and politics, please tell us the number of your breakout room that you are currently in."}, {"title": "Question 2", "content": "Let's start. Please note that there are no right or wrong answers. We are just interested in your views.\nWe begin with a hypothetical scenario where a group of people need to make decisions. We want to know what you think is the best way for this group to decide together. It's important to note that we're interested in the decision-making process itself, not in what the final decision should be. Imagine a group of 10 people are deciding where to have a dinner event. Seven people want to have the event at a Japanese sushi restaurant. Three people cannot eat sushi because they have fish allergies and they want to have the event at an Italian restaurant instead. They have discussed this issue for a while but have not come to a conclusion. How should the group decide what to do?"}, {"title": "Question 3", "content": "Can you think of other ways to make decisions apart from the method you just described? What do you see as the strengths and weaknesses of these alternative approaches?"}, {"title": "Question 4", "content": "Let's talk a bit about politics. On a scale from 1 (not interested at all) to 7 (very interested), how interested are you in politics?"}, {"title": "Question 5", "content": "Can you elaborate and explain your level of interest in politics?"}, {"title": "Question 6", "content": "And what do you think \"politics\u201d is? How would you define this term?"}, {"title": "Question 7", "content": "Think back to the last time you took part in an action that you considered \"political\", whether it was a small or significant act. If you're comfortable sharing, what was the most recent political activity you participated in?"}, {"title": "Question 8", "content": "Consider a scenario where a 7-year-old boy decides to stop eating meat after watching a documentary on meat production, but his mother insists that he should continue to eat meat. Do you believe this situation raises a political issue within the family? Are they discussing politics?"}, {"title": "Question 9", "content": "Can you think back and tell us about an instance where politics made you feel very disappointed or very satisfied?"}, {"title": "Question 10", "content": "Now that we have talked a little bit about the meaning of \"politics\u201d would you reconsider your definition of \"politics\"?"}, {"title": "Question 11", "content": "Let us talk about democracy. When you think about how democracy works right now in West-"}, {"title": "ern countries such as Germany, what are the good things that come to mind?", "content": "Question 12\nAnd what are the bad things that come to your minds about democracy in the West?\nQuestion 13\nGenerally speaking, what makes a country democratic? In your view, what are the most important elements of a democracy?\nQuestion 14\nThe architect of Munich's Olympiapark for the 1972 Olympics aimed to create a democratic landscape that is open and accessible to all. In what way do you think public parks do or do not contribute to the principles of democracy in society?"}, {"title": "E Interviewer guidelines", "content": "based on\nAdams, W.C. (2015). Conducting Semi- Structured Interviews. In Handbook of Practical Program Evaluation (eds K.E. Newcomer, H.P. Hatry and J.S. Wholey). https://doi.org/10.1002/9781119171386.ch19\nGuidelines for In-Depth Interviews\n\u2022 Make sure that your questions do not guide or predetermine the respondents' answers in any way. Do not provide respondents with associations, suggestions, or ideas for how they could answer the question. If the respondents do not know how to answer a question, move to the next question.\n\u2022 Do not judge the respondents' answers. Do not take a position on whether their answers are right or wrong. Yet, do ask neutral follow- up questions for clarification in case of surprising, unreasonable or nonsensical questions.\n\u2022 You should take a casual, conversational approach that is pleasant, neutral, and professional. It should neither be overly cold nor overly familiar.\n\u2022 From time to time, restate concisely in one or two sentences what was just said, using mainly the respondent's own words. Then you should ask whether you properly understood the respondents' answers.\n\u2022 Importantly, ask follow-up questions when a respondent gives a surprising, unexpected or unclear answer. Prompting respondents to elaborate can be done in many ways. You could ask: \"Why is that?\u201d, \u201cCould you expand on that?\u201d, \u201cAnything else?\u201d, \u201cCan you give me an example that illustrates what you just said?\". Make it seem like a natural conversation. When it makes sense, try to connect the questions to the previous answer. Try to elicit as much information as possible about the answers from the users; especially if they only provide short answers You should begin the interview based on the first question in the questionnaire below. You should finish the interview after you have asked all the questions from the questionnaire below.\""}, {"title": "F Real-time problem recording", "content": "This appendix lists the issues that the observers have recorder during the AI in-depths interviews."}, {"title": "F.1 Issues 1", "content": "In this form, document technical issues during the interview\n\u2022\n\u2022 Problems with audio recording\n\u2022 Excessive latency of AI Interview (response times)\nResponses: Breakout room \"too\" instead of 2 small recurring problems with audio recording (not sure if it already runs, accidently stop in recording early) quickly resolved\nSome problems with the microphone: Some- times does not record., speech recognition sometimes recognises words incorrectly.\nlong loading times at the beginning\nSometimes the time it takes to produce an answer is unexpectedly long. But it is not really off putting.\nThe recording was not possible\nrun time is quite slow, it takes a couple (>5 seconds) voice recording does not get all spoken words in the sentence voice recoding also takes in the wrong word e.g. ai spoken \u2013> aA recorded the recording button didnt work good. stopped randomly mid sentence and had to be clicked quite often before finally starting to record on the last questions the recordings lagged a couple seconds answer time also decreased further\nDictation did not work\nAudio recording is a problem, sometimes respondent can not give answers with using audio, sometimes there are spelling mistakes."}, {"title": "F.2 Issues 2", "content": "In this form, document odd, unexpected, undesired interviewer behavior that is inconsistent with interview guidelines\nResponses: sometimes does not sound very human like\nrecording just stopped completely for a couple seconds and interviewee was kinda mad about it. bad ai system or cheap ass servers voice recoding suddenly capitalized letters\nThe AI seems not to be neutral.\nIt emphasises on the given answers and even adds points to the argument. no, this did not appear."}, {"title": "F.3 Issues 3", "content": "In this form", "proceed\nResponses": "sushi restaurant: a little unsure about follow-up question\na bit unsure how to answer the first questions"}]}