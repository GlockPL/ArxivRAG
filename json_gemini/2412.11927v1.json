{"title": "Explainable Procedural Mistake Detection", "authors": ["Shane Storks", "Itamar Bar-Yossef", "Yayuan Li", "Zheyuan Zhang", "Jason J. Corso", "Joyce Chai"], "abstract": "Automated task guidance has recently attracted attention from the AI research community. Procedural mistake detection (PMD) is a challenging sub-problem of classifying whether a human user (observed through egocentric video) has successfully executed the task at hand (specified by a procedural text). Despite significant efforts in building resources and models for PMD, machine performance remains nonviable, and the reasoning processes underlying this performance are opaque. As such, we recast PMD to an explanatory self-dialog of questions and answers, which serve as evidence for a decision. As this reformulation enables an unprecedented transparency, we leverage a fine-tuned natural language inference (NLI) model to formulate two automated coherence metrics for generated explanations. Our results show that while open-source VLMs struggle with this task off-the-shelf, their accuracy, coherence, and dialog efficiency can be vastly improved by incorporating these coherence metrics into common inference and fine-tuning methods. Furthermore, our multi-faceted metrics can visualize common outcomes at a glance, highlighting areas for improvement.", "sections": [{"title": "1 Introduction", "content": "The problem of automated, interactive task guidance has recently attracted attention in the AI research community (Bao et al., 2023; Wang et al., 2023a; Peddi et al., 2024; Bohus et al., 2024), stemming from significant efforts to build and learn from large-scale procedural video datasets (Zhou et al., 2018; Damen et al., 2018; Miech et al., 2019; Grauman et al., 2022). A successful task guidance agent can observe a human user through video and interact with them through language to guide them through completing a task. One key component of such an agent is procedural mistake detection (PMD): the ability to detect when the user's actions deviate from a procedural text, e.g., a recipe or instruction manual. To achieve this, a system must apply physical commonsense reasoning to anticipate the success conditions for actions in this procedural text, then verify them by extracting relevant physical state information from the visual scene.\nPrior work in PMD has explored two approaches: one thread of work has fine-tuned primarily vision-based classifiers without incorporating language (Wang et al., 2023a; Peddi et al., 2024), while another has applied foundational language models (LMs) and vision-and-language models (VLMs) to this problem (Du et al., 2023; Bao et al., 2023). However, both types of approaches have failed to achieve a viable level of accuracy in detecting mistakes. Qualitatively, Bao et al. (2023) find that while the web-scale multimodal pre-training of foundational VLMs enables coverage of a wide variety of procedures, they often produce noisy, vague, or otherwise insufficient information from visual scenes to facilitate PMD. This capability to extract and reason over key task-relevant visual information is crucial to PMD, but prior work has largely overlooked it, instead targeting binary or categorical classification tasks in their system design and quantitative evaluations. Consequently, the reasons for VLMs' decisions are opaque, hindering practical use\u00b9 and the identification of areas for improvement.\nTo promote the development of PMD systems with greater transparency and coherence, we propose the reformulated problem of explainable PMD: given a procedural text and egocentric video frame, VLMs must not only decide whether a mistake has occurred, but additionally explain their decision with visual information from the"}, {"title": "2 Problem Formulation and Dataset", "content": "In this section, we define the extended problem of explainable PMD in an approachable manner for VLMs, then we introduce a benchmark dataset we curated for evaluating explainable PMD."}, {"title": "2.1 Defining Explainable PMD", "content": "Formally, the inputs for PMD are a short procedural text $T$ and a single video frame $F$, which may or may not visualize the successful completion of"}, {"title": "2.2 Constructing a Dataset for PMD", "content": "We follow Du et al. (2023) in recasting Ego4D (Grauman et al., 2022), a large-scale egocentric video dataset for everyday activities with dense annotations for various aspects of the videos, into an offline mistake detection format, but expand the diversity of mistake types studied there. Ego4D's hand and object interactions data subset includes videos of physical actions being performed with various objects. Each video is annotated with narrations describing fine-grained procedures being performed, timestamps for when it begins and ends, and category labels for the verb and noun"}, {"title": "3 Evaluating Coherence in PMD", "content": "Next, we describe our application of a fine-tuned NLI model to calculate two coherence metrics for explainable PMD: relevance of questions and informativeness of answers to those questions."}, {"title": "3.1 Using NLI Models to Judge Success", "content": "As visualized in Figure 3, LMs fine-tuned for NLI can estimate the sufficiency of visual questions and answers in explaining whether a procedure was successfully completed. To implement this, we need an NLI model $f_e$ (which returns a probability that an input premise string entails a hypothesis string), a premise transformation $t_p$ (which converts a question $Q$ and answer $A$ into a declarative statement to add to the premise), and a hypothesis prompt template $t_h$ (which creates a hypothesis about the success of the procedure in $T$). We can then calculate the NLI model's probability for the success of procedure $T$ based on the explanation $E = (Q, A)$ as follows:\n$p_e(T|Q, A) = f_e(t_h(T)|\\{t_p(Q_i, A_i)|1 \\leq i \\leq n\\})$\nWe implement $f_e$ with BART (Lewis et al., 2020) fine-tuned on the large-scale MultiNLI dataset (Williams et al., 2017), applying softmax over its logits for entailment and contradiction to get an entailment probability. We follow Srinivasan et al. (2024) and prompt a foundational LM to implement $t_p$. For the procedural text $T$, we"}, {"title": "3.2 Relevance", "content": "A coherent decision in PMD should be supported by relevant questions about the state of the environment. We measure the relevance of a question $Q'$ to the success of a procedure $T$, given previous questions $Q$ and their answers $A$, as follows:\n$Rel(Q'|T, Q, A) =|p_e(T|Q \\cup Q', A \\cup \\text{``No''})\n- p_e(T|Q \\cup Q', A \\cup \\text{``Yes''}) |$\nThis definition quantifies how much impact the answer to the proposed question $Q'$ can have on the success probability (as estimated by the NLI model). If the success probability is similar for \u201cYes\u201d and \u201cNo\u201d answers, this suggests that $Q'$ would not reveal pertinent information (i.e., beyond that in $Q$ and $A$) about whether the procedure in $T$ was successfully executed by the user, and thus the relevance would be low. If the success probabilities vary widely depending on the answer, this suggests that $Q'$ can reveal important new information to help make the decision.\nExample-level relevance. For evaluation across a dataset, we summarize the relevance of a sequence of questions generated for a particular example of explainable PMD by taking the mean relevance of generated questions with respect to previous questions and answers:\n$\\frac{1}{n} \\sum_{i=1}^{n} Rel \\left(Q_{i} \\mid T,\\left\\{Q_{j}: j<i\\right\\},\\left\\{A_{j}: j<i\\right\\}\\right)$"}, {"title": "3.3 Informativeness", "content": "Since a relevant question does not guarantee an informative answer, and VLM errors in answering questions could unintentionally introduce conflicting information, it is necessary to evaluate the sufficiency of VLMs' answers in justifying a PMD decision. To achieve this, we measure the informativeness of a predicted answer $A'$ for a question $Q'$ to the success of a procedure $T$ (given previous questions and answers $Q$ and $A$) as follows:\n$Inf(A'|Q', T, Q, A) = 1 - H(p_e(T|Q \\cup Q', A \\cup A'))$\n$H$ is the binary Shannon entropy of the success probability $p_e$, calculated by $H(p) = -p \\text{log}_2 p - (1-p) \\text{log}_2 (1 - p), p \\in [0, 1]$. This definition for informativeness quantifies how much information an answer to a question gives us in determining the success of the procedure. As such, if the success probability given this answer $A'$ to $Q'$ is confident (i.e., very low or high), this indicates that $A'$ (along with $Q$ and $A$) are sufficient to make a decision, and thus informativeness will be high. On the other hand, a success probability closer to 50% suggests that the information gathered thus far is insufficient, yielding low informativeness. We express informativeness as a percentage (i.e., of 1, the maximum absolute value of bits here).\nReference-adjusted informativeness. We also wish to account for cases where the information gathered in questions and answers indicates the wrong PMD decision. To do so, we define the NLI model's PMD belief $y_e(T|Q, A)$ as 1 (mistake) if $p_e(T|Q, A) < 0.5$, else 0 (success). We then define the reference-adjusted informativeness:\n$Inf^* (A'|Q', T, Q, A, y^*) = \\begin{cases}Inf (A'|Q', T, Q, A), & \\text{if } y_e(T \\cup Q \\cup Q', A \\cup A') = y^*\\\\-Inf (A'|Q', T, Q, A), & \\text{else}\\end{cases}$\nThis makes informativeness negative if the NLI model judges the evidence gathered as indicating the wrong final answer for PMD.\nExample-level informativeness. For evaluation across a dataset, we summarize the (reference-adjusted) informativeness of a set of questions and answers generated for a particular example by taking the maximum value across the self-dialog:\n$\\max _{1<i<n} Inf^* \\left(A_{i} \\mid Q_{i}, T,\\left\\{Q_{j}: j<i\\right\\},\\left\\{A_{j}: j<i\\right\\}, y^{*}\\right)$"}, {"title": "4 Experimental Results", "content": "In this section, we present a broad set of experiments with VLMs in explainable PMD to validate our proposed metrics. First, we provide details for how VLMs are applied to the problem. We then introduce two inference-time interventions to encourage VLMs to select more coherent questions from generated candidates, as well as an approach to encourage them to generate more coherent candidates through preference optimization from our proposed coherence metrics. After evaluating and comparing these approaches, we show that our proposed metrics visualize and characterize common behaviors of VLMs, enabling a panoptic understanding of their capabilities in PMD."}, {"title": "4.1 Applying VLMs to Explainable PMD", "content": "Our experiments evaluate explainable PMD in InstructBLIP (Dai et al., 2023a), LLaVA 1.5-7B (Liu et al., 2023), and Llama 3.2-Vision-11B (Dubey et al., 2024). The models apply different architectures, training datasets, and/or training strategies to integrate vision into the model. While LLaVA and InstructBLIP were not trained on Ego4D data, Dubey et al. (2024) does not reveal whether Llama 3 was. Details and prompt templates can be found in Appendix D, while key implementation details about the iterative self-dialog are provided below.\nVQG. We first prompt the VLM to generate a series of questions given the procedural text and the dialog history with previous questions and answers (enabling deductive reasoning). To encourage logical but diverse questions, we apply greedy beam search decoding to generate 4 candidates, from which we select the most likely candidate that has not already been generated.\nVQA. After a question is generated, it is answered by the VLM given only the question and video frame. If the probability of the chosen answer (i.e., \u201cYes\u201d or \u201cNo\u201d) exceeds an answer sureness threshold of 60%, we append it to the dialog history, otherwise we append \"Unsure.\" These unsure answers are excluded from the sets of questions Q and answers A in the calculation of example-level relevance and informativeness (defined in Section 3). Effectively, they are assigned zero informativeness and excluded from previous questions and answers in metric calculations.\nSuccess classification. After each iteration of VQG and VQA, we prompt the VLM to judge whether the procedure has been successfully executed based on the video frame and entire dialog history. The VLM's decision is made using a mistake confidence threshold $\\tau$ (tuned on the validation data for each approach) on its mistake likelihood. The prompt and answer for this step are excluded from the dialog history in future iterations.\nStopping criteria. To prevent over-generating noisy information, which can degrade PMD accuracy, we implement an early stopping mechanism to determine whether to stop generating questions based on the success likelihood after each iteration. The self-dialog stops early (i.e., before $n$ questions have been generated) if the likelihood of success either stabilizes (i.e., changes by less"}, {"title": "4.2 Coherent Question Selection", "content": "While a typical beam search would rank candidate questions by their likelihood, an alternative approach is to re-rank candidates using the reference-free coherence metrics introduced in Section 3. This could encourage the selection of questions that are most likely to bring in new, salient, and helpful information. Furthermore, as prior work has shown how the in-context learning capability of LMs enables them to accurately predict physical states from just a few in-context demonstrations (Zhang et al., 2023), we will examine how supplementing candidate questions generated from the dialog history with additional questions generated through in-context learning approach impacts VLM performance.\nNext, we introduce these two approaches we use to augment the candidate question pool for more coherent candidates: coherence-based re-ranking and candidate generation through in-context learning. We then compare their performance on Ego4D-PMD with that of vanilla VLMs."}, {"title": "4.2.1 Coherence-Based Question Selection", "content": "We implement a coherence-based candidate question re-ranking approach as follows. Given a set of question candidates Q for procedural text T along with previous confidence-filtered questions Q and answers A, we can select the best question $Q^*$ by maximizing the product of relevance and potential informativeness across all $Q \\in Q$:\n$\\frac{\\text{Rel}(Q|T, Q, A)}{\\max _{A \\in \\{\\text{Yes}, \\text{No}\\}} \\text{Inf}(A|Q, T, Q, A)}$\nThis ranking prioritizes well-rounded questions that can yield impactful information for success classification which leads to the most confidence. $Q^*$ is then concatenated to the dialog history and answered by the VLM as described earlier."}, {"title": "4.2.2 In-Context Learning Augmentation", "content": "Applying in-context learning in explainable PMD is not straightforward, as it would require reasoning over multiple images and possibly long, variable-length dialogs. Instead, we use in-context learning to improve the text-based VQG step by providing examples of human-written questions."}, {"title": "4.2.3 Comparison of Results", "content": "We compare these approaches to vanilla VLMs based on mistake detection accuracy, mean example relevance and reference-adjusted informative-"}, {"title": "4.3 Coherent Question Generation", "content": "While the above approaches boost the coherence of VLM outputs for PMD, they have limitations. First, since they are training-free, they do not improve the internal coherence of VLMs, rather they only filter and augment their outputs. Further, these interventions take significantly more time and compute due to the need to evaluate the coherence of candidate questions (which requires prompting an NLI model) and generate questions twice (once based on the self-dialog history and once with in-context learning). In practical applications like task guidance, it may be advantageous to use specialized VLMs to improve reliability and speed. As such, we lastly explore whether VLMs can be fine-tuned to generate more coherent questions using our automated coherence metrics.\nSpecifically, we apply direct preference optimization (DPO; Rafailov et al., 2023) with quantized low-rank adaptation (QLoRA; Hu et al., 2022; Dettmers et al., 2024) to fine-tune a VQG adapter for LLaVA (the best performing model). We generate training data for preference optimiza-"}, {"title": "4.3.1 Comparison of Results", "content": "Table 2 displays the results of fine-tuning LLaVA for VQG, including the previous inference-time interventions to compare their relative impact on fine-tuned models. First, we observe that the relevance drastically increases from a maximum of 74.2% to 97.0%. Remarkably, this shows that VLMs are capable of learning to ask more coherent questions for explainable PMD, a task based on properties of the physical world.\nHowever, we find that accuracy and informativeness drop slightly from the best results in Table 1, suggesting that asking more coherent questions is not enough to improve performance globally. We suspect that this is due to an implicit difficulty trade-off of asking highly relevant questions, which should strongly indicate success if answered one way, otherwise a mistake. For elaborate procedures, highly relevant questions may cover multiple aspects or states of the scene. As such, answering these questions may be more difficult for VLMs, which struggle with complex questions. For example, while the generated question \"Is the soil around the tomato seedling with the gardening trowel in your hand?\" covers the success conditions of the procedure \"Put some soil around the tomato seedling with the gardening trowel in your hand,\" answering such a question requires understanding spatial relations between several objects (i.e., soil, tomato seedling, trowel, and hand). Future work may explore methods to encourage VLMs to generate simpler questions.\nMeanwhile, we find that fine-tuning reduces the average number of iterations taken to come"}, {"title": "4.4 Visualizing PMD Performance", "content": "An additional benefit of our automated coherence metrics is the ability to audit the global and local reasoning behaviors of VLMs under various strategies. In Figure 4, we visualize the distribution of decision error, relevance, and informativeness of four representative approaches to apply LLaVA. Specifically, we compare vanilla LLaVA (i.e., with likelihood-based question ranking) to variants successively equipped with coherence-based question ranking, in-context learning, and coherence-based fine-tuning with DPO.\nFor each example, decision error is calculated by how far the VLM's success likelihood was from being 100% confident in the ground truth PMD decision. Relevance and informativeness use the example-level forms defined in Section 3. The colors of points indicate combinations of decision error, relevance, and informativeness in VLM outputs, highlighting common outcomes; we describe several examples in Appendix E.7. In comparing the plots, we see a dramatic shift as each successive intervention is introduced. Specifically, we see that red and black/indigo points are virtually eradicated, indicating fewer incoherent explanations and complete failures.\nUltimately, when choosing an approach to use for explainable PMD, one should weigh accuracy, coherence, and explanatory dialog efficiency as appropriate for the setting. For example, online applications may benefit from shorter (i.e., faster) dialog, even at a cost of accuracy. Meanwhile, high-risk applications may prioritize high confidence (e.g., information gain) over other factors. Additional insights toward the fine-grained strengths and weaknesses of various approaches may be gained from analyzing these results by mistake type, or verb and noun categories."}, {"title": "5 Related Work", "content": "In Section 1, we contextualized this work with prior work in PMD. In this section, we additionally discuss two lines of work: multi-step reasoning in LMs and leveraging NLI models in NLP."}, {"title": "5.1 Multi-Step Reasoning in LMs", "content": "Foundational LMs have exhibited impressive capabilities in multi-step reasoning from prompting-based methods (Wei et al., 2022; Kojima et al., 2022). Later work strengthened LMs' reasoning with multiple paths (Wang et al., 2023b; Snell et al., 2024), tree-search (Yao et al., 2023; Hao et al., 2023; Putta et al., 2024; Tian et al., 2024; Chen et al., 2024a; Zhang et al., 2024a; Qi et al., 2024), and fine-tuning on data generated by stronger LMs (Wang et al., 2024; Gou et al., 2024). Related to task guidance, some work has investigated LMs' ability to reason about dependencies between steps of physical procedures (Bellos et al., 2024; Lal et al., 2024). In light of new challenges in visually-dependent reasoning (Dai et al., 2023b; Li et al., 2023b; Guan et al., 2024; Zhang et al., 2024b), recent work has strengthened VLMs by utilizing text-based LMs and other tools to generate intermediate questions and coordinate visual reasoning step by step (Srinivasan et al., 2024; Chen et al., 2024b; Zhou et al., 2024; Zong et al., 2024; Cheng et al., 2024). In this work, we similarly enabled VLMs to coordinate their visual reasoning through a self-dialog in the novel problem of explainable PMD, and we additionally proposed new coherence metrics to evaluate and improve their reasoning."}, {"title": "5.2 Leveraging NLI in Other NLP Tasks", "content": "NLI requires determining whether a premise text entails a hypothesis text, and has long been studied in the NLP community (Dagan et al., 2005). Once thought to be a grand challenge for commonsense reasoning, many human-annotated resources have been compiled for NLI, and thus significant progress has occurred in training specialized LMs for NLI (Storks et al., 2019). Prior work has leveraged this progress by using such NLI models to improve the competence, confidence, and coherence of LMs in other tasks, such as dialog systems (Dziri et al., 2019; Welleck et al., 2019), summarization (Roit et al., 2023), visual question answering (Srinivasan et al., 2024), and image captioning (Cascante-Bonilla et al., 2024). Our work extends this effort to a novel problem of explainable PMD, adopting two NLI-based coherence metrics."}, {"title": "6 Conclusion", "content": "In this work, we investigated the capabilities of foundational VLMs in a challenging new problem of explainable procedural mistake detection (PMD), where visual questions and answers must be generated to drive system decisions. To evaluate explanations, we defined two coherence metrics leveraging a fine-tuned NLI model. We then applied several interventions in VLMs to encourage coherent question selection (e.g., coherence-based question re-ranking) and generation (e.g., preference optimization from pairs of coherent and incoherent questions). Our results showed that vanilla VLMs do not generate coherent explanations off-the-shelf, but these interventions vastly improve their coherence, with the former also improving their overall PMD accuracy, and the latter improving the efficiency of generating and extracting information from explanations. Further, we showed that patterns in accuracy and coherence metrics illuminate common failures, e.g., visual hallucination. This work lays a foundation for future work harnessing VLMs for task guidance and other multimodal reasoning applications."}, {"title": "7 Limitations", "content": "Latency of self-dialog. One limitation of our explainable PMD problem formulation is the requirement of generating several pieces of information autoregressively, which would take several seconds in practical settings. This is not ideal for a problem like interactive task guidance, where responsiveness and the ability to intervene quickly to correct mistakes are important. However, rather than being applied frame by frame (which would likely not be feasible), we expect this process to be applied once at the end of procedure execution to verify the state of the environment, e.g., when the user asks a task guidance system to inform them of the next step of a recipe. Based on the results of our study, one could explore streamlined and specialized approaches to apply VLMs to a stream of video frames in a live online setting. For example, in preliminary experiments, we tried to generate questions once with a VLM, then answer them over a series of video frames, but we found this approach limited by the inability to adapt questions to previously gathered information, and the challenge of aggregating noisy VLM responses across time. We leave further investigation of such approaches for future work.\nInherent limitations of single video frames. Next, single video frames are limited in representing actions, which involve movement and state change. Our decision to focus on individual frames stemmed from preliminary experiments we performed with existing open-source VLMs for video understanding (Lin et al., 2024; Li et al., 2023a), which are still in early stages. There, we found that they often confused information from frames in different regions of the video, preventing them from judging the final states of objects and reconciling this information with success of procedures, and resulting in poor performance. As such, our choice to focus on single images simplified the problem for current VLMs, enabling our experiments to begin building a meaningful understanding of their capabilities in PMD. To minimize the dependence on multiple frames in detecting mistakes, we applied several careful preprocessing steps to Ego4D-PMD (as discussed thoroughly in Section 2.2 and Appendix A). It is also worth noting that by not incorporating modalities beyond text and images, e.g., audio, the VLMs we studied are inherently limited in their capturing of physical information (Yu et al., 2022; Zong et al.,"}]}