{"title": "Turing Representational Similarity Analysis (RSA): A Flexible Method for Measuring Alignment Between Human and Artificial Intelligence", "authors": ["Mattson Ogg", "Ritwik Bose", "Jamie Scharf", "Christopher Ratto", "Michael Wolmetz"], "abstract": "As we consider entrusting Large Language Models (LLMs) with key societal and decision-making roles, measuring their alignment with human cognition becomes critical. This requires methods that can assess how these systems represent information and facilitate comparisons to human understanding across diverse tasks. To meet this need, we developed Turing Representational Similarity Analysis (RSA), a method that uses pairwise similarity ratings to quantify alignment between Als and humans. We tested this approach on semantic alignment across text and image modalities, measuring how different Large Language and Vision Language Model (LLM and VLM) similarity judgments aligned with human responses at both group and individual levels. GPT-40 showed the strongest alignment with human performance among the models we tested, particularly when leveraging its text processing capabilities rather than image processing, regardless of the input modality. However, no model we studied adequately captured the inter-individual variability observed among human participants. This method helped uncover certain hyperparameters and prompts that could steer model behavior to have more or less human-like qualities at an inter-individual or group level. Turing RSA enables the efficient and flexible quantification of human-Al alignment and complements existing accuracy-based benchmark tasks. We demonstrate its utility across multiple modalities (words, sentences, images) for understanding how LLMs encode knowledge and for examining representational alignment with human cognition.", "sections": [{"title": "Introduction", "content": "Foundation Model (FM) reasoning (Brown et al., 2020; Wang et al., 2024) and perceptual skills (Marjieh, Sucholutsky, et al., 2023; Radford et al., 2021) may soon match or exceed human performance across a wide range of tasks (OpenAl et al., 2024; Wei et al., 2022; see Minaee et al., 2024 for review). The rapid pace of this progress, as exemplified by Large Language Models (LLMs) (Hoffmann et al., 2022; Kaplan et al., 2020), has initiated a discussion of whether and how these models should be integrated into every-day life or given additional responsibilities (Amodei, 2024). The increasing deployment of Al systems in critical roles (potentially replacing humans in those roles) requires scalable, generalizable methods for measuring how FMs represent knowledge about the world, and for evaluating how those representations and downstream behaviors compare to complex human behaviors (see Sucholutsky et al., 2023 for discussion). The recent transition from fundamentally narrow models of moderate size and complexity to increasingly general models that are larger and more complex compounds the classic opacity problem of deep neural networks: discerning how an LLM processes a given input or arrives at a decision has never been as challenging or as critical. Fortunately, cognitive science and psychological research has focused on this exact challenge in the context of biological intelligence and serves as a productive framework for studying artificial cognition and its alignment with human cognition.\nAs an exemplar of this emerging framework, Turing Experiments capitalize on the rich history of experimental psychology to measure the cognitive and behavioral alignment between human and artificial intelligence (Aher et al., 2023; Mei et al., 2024). In a Turing Experiment, an LLM is used to simulate a sample of the human population over repeated runs (Cava & Tagarelli, 2024; Mei et al., 2024), sometimes using simulated participant identities for each new run (Aher et al., 2023; Petrov et al., 2024), and is prompted to engage in classic psychology tasks (e.g., the prisoner's dilemma, the ultimatum game, or the Milgram shock experiment). While still very early, this line of work has shown that the GPT family of models (as well as different open- source models) can exhibit discrete personalities based on their prompting (Cava & Tagarelli, 2024) that influence behavior (Bose et al., 2024), that they can generate responses in behavioral tasks that fall within the range of human variability (Mei et al., 2024), and, finally, that larger, more recently developed language models better align with human behavior (Aher et al., 2023). This approach allows for an examination both of how similar LLM behavior is to human behavior in the aggregate (i.e, using the central tendency of a sample of simulated participants) as well as their ability to capture inter-individual variability. These initial studies suggest there could be value in using other paradigms and techniques to explore the knowledge and behavior of LLMs and how they align with human knowledge and behavior.\nA series of studies by Marjieh and colleagues (Marjieh et al., 2022; Marjieh, Rijn, et al., 2023; Marjieh et al., 2024) have begun to explore a new, more continuous approach to mapping LLM knowledge via representational distances either from model embeddings or model ratings. Their initial results found that LLMs can predict human similarity judgments across multiple perceptual domains based on text input alone. Dickson and colleagues (2024) asked similar questions regarding perception based on visual input, finding that different LLM models aligned with human ratings along some (but not all) perceptual dimensions. However, neither of these studies compared representations across text and image domains or undertook an evaluation of variability in the ratings of LLM participants.\nOne of the most productive methods for mapping the structure of how an individual represents information about the world is the use of pairwise ratings of similarity (or dissimilarity) with respect to a pair of stimuli (Shepard, 1980, 1987; Tversky, 1977). This class of tasks is adaptable to a wide array of domains and questions (e.g., \u201cHow similar are the words \u2018apple' and 'hand?'\u201d or \"How similar are these two images?\"), and is most useful when the experimenter does not have direct access to a participant's internal representations (i.e., neuronal activations or embeddings), as is the case in traditional psychophysics and cognitive science experiments as well as for many frontier LLMs. Ratings elicited by participants on each trial comprise a behavioral distance metric for the two stimuli that were presented. These ratings can be organized into a symmetrical matrix whose rows and columns correspond to the probe items used in each rating trial and can be analyzed using techniques like multi-dimensional scaling to visualize the geography of how different items relate to one another (Hout et al., 2013) or to test different hypotheses (Borg & Groenen, 1997). This approach has deeply informed a range of questions in human perception and cognition including object relations (Jiang et al., 2022; Ogg & Slevc, 2019) and semantic information (Carlson et al., 2014), as well musical pitch (Marjieh, Griffiths, et al., 2023) and timbre (McAdams et al., 1995; Thoret et al., 2021).\nRepresentational similarity analysis (RSA; Kriegeskorte & Kievit, 2013) builds on the use of distance, or dissimilarity, matrices (\u201cDSMs\u201d including from pairwise ratings) to quantify the similarity of representational spaces among diverse systems: across organisms (Kriegeskorte et al., 2008), individuals (Khaligh-Razavi & Kriegeskorte, 2014), models (Mehrer et al., 2020; Ogg & Skerritt-Davis, 2021), or biological substrates like different brain regions (Carlson et al., 2014; Giordano et al., 2023; Ogg et al., 2020). In RSA, the organized distance matrices are correlated with one another to quantify the agreement of the pairwise ratings (or distances) between each system. That is, RSA can be used to quantify how similarly two species (e.g., humans and primates; Kriegeskorte et al., 2008) process object images at different stages of the visual hierarchy, to align object representations from different neuroimaging modalities across time and cortical space (Cichy et al., 2014), or to investigate how the computations performed by layers of convolutional networks trained for visual object recognition relate to the computations of the ventral visual pathway (Cichy, Khosla, et al., 2016).\nRSA has also been used to understand the representations of neural network models (e.g., Mehrer et al., 2020, 2021; Ogg & Skerritt-Davis, 2021; see Sucholutsky et al., 2023 for review) by correlating distances derived from model embeddings. However, for current LLMs, these embeddings are not always accessible. Instead, the representational structure of these models can be distilled by formulating this analysis as a Turing Experiment where the model is queried with pairs of stimuli and asked to provide a similarity rating for each pair. A crucial advantage of RSA is that it enables comparison between any systems capable of producing comparable behavioral outputs, without requiring access to or assumptions about their internal representations. This makes it particularly valuable for comparing human and artificial intelligence, where internal processing mechanisms may be fundamentally different or inaccessible.\nHere, we integrate the concepts of Turing Experiments and RSA in Turing RSA to probe the knowledge and behavior of LLMs agents as a strategy for measuring alignment between artificial and biological intelligence across information domains (e.g., text and images). Using this method, we measure the relationship between LLM and human judgments for sets of well-studied probe objects via words and images. In the process, we demonstrate the flexibility to facilitate comparisons within and across different modalities (i.e., text and images), quantify individual variability among LLMs and humans, and evaluate of prompt effects among standard Turing Experiment formulations."}, {"title": "Results", "content": "Word Similarity Judgements\nTuring RSA was designed as a prompt-based task to map the high-level semantic representational space of LLMs, drawing inspiration from the power and ostensible simplicity of pairwise rating tasks (Figure 1A). Even without direct access to the model's embeddings or internal representations, the flexibility of the chat prompt interface allowed us to repeatedly query the model with questions asking the LLM to rate the similarity of two concepts (see Figure 1A and Example 1). This method also allowed us to quantify the representational similarity (or representational alignment) of two model systems, and to specifically assess how similar LLM semantic representations are to those of humans. Repeated runs for a given model allowed us to examine the variability of LLM responses by assigning the model a different simulated participant identity for it to assume on each new run (see Aher et al., 2023; Cava & Tagarelli, 2024; Mei et al., 2024; Petrov et al., 2024). The LLM was assigned these identities (for example, \"Ms. Olson\u201d) each time it was initialized for a given participant run: both when introducing the task and during the subsequent run of rating trials (see Example 1).\nTo validate this approach, we compared LLM responses to previously collected pairwise ratings of semantic relatedness for different object concepts. In the original study eight participants judged the semantic relatedness of all pairs of 67 object concepts, originally collected for comparison with neural responses to assess when semantic meaning emerges within the human ventral visual pathway (Carlson et al., 2014, see Supplemental Table 1). The same task instructions and word stimuli were presented to GPT-3.5, GPT-4, GPT-40-mini, GPT-40 and other smaller open-source language models to elicit similar ratings that could be compared to human ratings.\nGPT-40 ratings were most similar to the human participant responses among the models that were tested ($r_s$ = 0.740; all Bonferroni-corrected p < 0.05 unless otherwise stated, see Figure 1B and Supplemental Table 2 for more details and for the full set of comparisons, see Supplemental Figure 1 for group-level DSMs), showing much stronger alignment than GPT-3.5 ($r_s$ = 0.456) and GPT-4 ($r_s$ = 0.696) ratings. However, GPT-40-mini ($r_s$ = 0.736) aligned with human ratings almost as well as GPT-40. Among smaller language models and language model embeddings, Gemma2-9b ratings ($r_s$ = 0.658), Solar-10.7b ($r_s$ = 0.647), Llama3-8b ($r_s$ = 0.586), Phi-3-medium-14b ($r_s$ = 0.586), Mistral-7b ($r_s$ = 0.532), GloVe embeddings ($r_s$ = 0.643) and Ada embeddings ($r_s$ = 0.437) were closely aligned with human ratings. In many cases these smaller LLMs were more aligned with human ratings than GPT-3.5, but not as closely as GPT-4 ratings. Llama2 (Llama2-7b: $r_s$ = 0.245; Llama2-uncensored-7b: $r_s$ = 0.201) and BERT (bert-base-uncased: $r_s$ = 0.188) models had the lowest alignment with human ratings among the text models that were tested (the albert-xxlarge-v2 alignment of $r_s$ = -0.001 did not survive Bonferroni correction).\nThe stimuli in these experiments were organized into discrete object categories (Human, Animal, Natural, and Man-Made objects, see Supplemental Table 1), which allowed us to examine how these models represented different aspects of within- and between-category semantic structure. Most models were more aligned with human participant ratings of items within the same object category (GPT-3.5: $r_s$ = 0.472; GPT-4: $r_s$ = 0.780; GPT-40-mini: $r_s$ = 0.747; GPT-40: $r_s$ = 0.781; Gemma-7b: $r_s$ = 0.479; Gemma2-9b: $r_s$ = 0.691; Llama3-8b $r_s$ = 0.613; Phi-3- medium-14b $r_s$ = 0.498; Mistral-7b: $r_s$ = 0.482; Solar-10.7b: $r_s$ = 0.715; GloVe: $r_s$ = 0.801; Ada: $r_s$ = 0.660, all Bonferroni-corrected p < 0.05), compared to items from different categories (GPT-3.5: $r_s$ = 0.293; GPT-4: $r_s$ = 0.510; GPT-40-mini: $r_s$ = 0.602; GPT-40: $r_s$ = 0.598; Gemma-7b: $r_s$ = 0.389; Gemma2-9b: $r_s$ = 0.474; Llama3-8b $r_s$ = 0.377; Phi-3-medium-14b $r_s$ = 0.481; Mistral-7b: $r_s$ = 0.466; Solar-10.7b: $r_s$ = 0.470; GloVe: $r_s$ = 0.485; Ada: $r_s$ = 0.246, all Bonferroni-corrected p < 0.05; W = 312, p < 0.021, two-sided Wilcoxon Rank-Sum test for within vs between category text model alignment). In general, the models that were better aligned with human participant ratings were most aligned with respect to within-category structure and their between-category structure was very sparse (i.e., high performing models were more aligned when rating objects from the same category such as \u201ccow\u201d and \u201cgoat\u201d than when rating objects form different categories like \u201ccow\u201d and \u201cphone\u201d and these between category ratings tended to be near zero; see Supplemental Table 2 and Supplemental Figure 1). This indicates that there is some nuance in how human raters represent some between-category semantic relations that is not well captured by the LLMs.\nImage Similarity Judgements\nInitial experiments demonstrated that our Turing RSA task allowed us to behaviorally probe the structure of an LLM's semantic representations and compare those with humans. Next, we evaluated the generalizability of this method for comparing human and LLM behavioral judgments across domains. For this we used the images corresponding to the object words rated in the study by Carlson and colleagues (2014; obtained from Cichy, Pantazis, and"}, {"title": "Image Similarity Judgements", "content": "colleagues, 2016, although they originated from Kriegeskorte and colleagues, 2008) referred to as the \u201cCarlson-Image\u201d stimuli (the human behavior ratings were collected to analyze neural responses to these images). We ran an additional set of experiments using corresponding images from the THINGS database (Hebart et al., 2019; Hebart et al., 2023). This complement the Carlson-Image stimulus set, which comprises a cropped view of each object presented on a grey background, with more natural depictions of each object (including backgrounds) typical of the THINGS dataset (see Grootswagers and Robinson, 2021, for discussion). There was incomplete overlap in the object classes between these two stimulus sets, so these analyses were restricted to models or responses for the 55 object classes present in both the Carlson- Image stimulus set and the THINGS database (see Supplemental Table 1).\nWe adapted the Turing RSA task to elicit similarity ratings for pairs of images from the GPT-4- Vision and GPT-40 models (see Example 2). For a comparison with human behavioral ratings, we used cosine distances between Sparse POsitive Similarity Embeddings (SPOSE) generated for each object (Hebart et al., 2020). These embeddings were learned so as to accurately predict odd-one-out behavioral judgments for a large number of over 1,800 objects represented by a large set of images from the THINGS database. GPT ratings were also compared with representations derived from a popular high performing deep convolutional network (AlexNet) trained on either the ImageNet database (referred to as AlexNet-LSVRC2012) or a more ecologically representative dataset (referred to as AlexNet-Ecoset; both from Mehrer et al., 2021, see Supplemental Figure 1).\nWe found that GPT-40 predicted SPOSE model distances (i.e., a model of human visual image ratings) reasonably well ($r_s$ = 0.545 based on the Carlson-Image stimuli; $r_s$ = 0.606 based on the THINGS stimuli; all Bonferroni-corrected p < 0.05; Supplemental Table 3), but overall the image- processing models aligned slightly less with human behavior (GPT-4-Vision: $r_s$ = 0.518 based on the Carlson-Image stimuli; $r_s$ = 0.529 based on the THINGS stimuli; GPT-40-mini: $r_s$ = 0.533 based on the Carlson-Image stimuli; $r_s$ = 0.537 based on the THINGS stimuli) than their text-only counter parts (W = 75, p < 0.039, two-sided Wilcoxon Rank-Sum test between GPT-text model alignment with human data and GPT-vision model alignment and SPOSE model distances). For each model, alignment was slightly better for the THINGS stimuli, perhaps due to the more natural depictions of each image in that dataset (e.g., including natural backgrounds), which could have been a better match the training data of the visual GPT-4 and GPT-40 models. The AlexNet-Ecoset models were less well aligned than the GPT-40 model ratings, but otherwise aligned reasonably well with the model of human behavior based on THINGS stimuli ($r_s$ = 0.564) than the Carlson-Image stimuli ($r_s$ = 0.407), and overall better predicted SPOSE distances than AlexNet-LSVRC2012 (based on THINGS: $r_s$ = 0.428; based on Carlson-Image: $r_s$ = 0.372).\nRepresentational similarity analysis facilitates comparisons across input domains (like vision and language), which are useful for assessing the potential for modality-agnostic conceptual representations (which is considered to be an central feature of semantic knowledge; see Patterson et al., 2007 and Simanova et al., 2014 for discussion). For example, human ratings of these objects via text were well aligned with the SPOSE model distances, which were based on images ($r_s$ = 0.729). GPT-40 (and the other visual processing GPT-4 LLMs) were less well aligned across text and image domains (for example, GPT-40 text-ratings correlated with Carlson-Image ratings: $r_s$ = 0.478 and with THINGS images: $r_s$ = 0.527; see Supplemental Table 3). Notably, GPT- 40 ratings of these object words aligned better ($r_s$ = 0.686) with SPOSE ratings (which were derived based on images) than the GPT-40 model's ratings of images, as did Glove ($r_s$ = 0.678), and Gemma2-9b ($r_s$ = 0.612). In some cases, model alignment with human pairwise text ratings increased given the reduced 55-item stimulus set that accommodated the THINGS dataset classes, but where possible we defer to the results of the larger 67-item sample.\nSimilar to the text experiments, Human-LLM alignment for images was explored with respect to within and between category ratings. Again, models were decidedly more aligned with human ratings for within-category comparisons (within-category GPT-4-Vision: based on Carlson-Image: $r_s$ = 0.564; based on THINGS: $r_s$ = 0.589; within-category GPT-40-mini: based on Carlson-Image: $r_s$ = 0.458; based on THINGS: $r_s$ = 0.589; within-category GPT-40: based on Carlson-Image: $r_s$ = 0.645; based on THINGS: $r_s$ = 0.591, all Bonferroni-corrected p < 0.05) than for between- category comparisons (GPT-4-Vision for Carlson-Image: $r_s$ = 0.250; GPT-4-Vision for THINGS: $r_s$ = 0.220; GPT-40-mini for Carlson-Image: $r_s$ = 0.225; GPT-40-mini for THINGS: $r_s$ = 0.196; GPT-40 for Carlson-Image: $r_s$ = 0.309; GPT-40 for THINGS: $r_s$ = 0.358, all Bonferroni-corrected p < 0.05; W = 576, p < 0.001, two-sided Wilcoxon Rank-Sum test for within vs between category GPT vision model alignment). AlexNet models were less aligned to the SPOSE (human behavior) for both within-category (AlexNet-Ecoset for Carlson-Image: $r_s$ = 0.109; AlexNet-Ecoset for THINGS: $r_s$ = 0.469; AlexNet-LSVRC2012 for Carlson-Image: $r_s$ = 0.080; AlexNet-LSVRC2012 for THINGS: $r_s$ = 0.316, with only the THINGS analyses surviving multiple comparison correction) and between- category structure (AlexNet-Ecoset for Carlson-Image: $r_s$ = 0.051; AlexNet-Ecoset for THINGS: $r_s$ = 0.231; AlexNet-LSVRC2012 for Carlson-Image: $r_s$ = 0.060; AlexNet-LSVRC2012 for THINGS: $r_s$ = 0.024, with only the comparison with the AlexNet-Ecoset representations on the THINGS dataset surviving multiple comparison correction)."}, {"title": "Increasing Human-LLM Alignment Through Prompting and Hyperparameters", "content": "Increasing Human-LLM Alignment Through Prompting and Hyperparameters\nLLMs provide increasingly accurate proxies for human ratings at the group level, especially for text-based tasks. However, there is interest in methods to increase the similarity between the representational and behavioral spaces of LLMs and humans (see Sucholutsky, et al. 2023), and there is room for improvement for even the most well-aligned LLMs observed in our study (see Supplemental Tables 2 and 3). Thus, we undertook an additional set of experiments that explored ways to increase alignment between LLM and human behavior via changes to hyperparameters and model prompts. GPT models were the focus for these experiments because of their popularity and overall good alignment performance.\nFirst, we investigated increasing the alignment of image processing GPT-4 models. These models achieved modest alignment with human behavior, and the text-only versions of GPT-4 rated these object concepts more similarly to models of human visual semantic behavior. Therefore, relying more heavily on the text processing capabilities of GPT-40 may increase alignment with human behavior. To test this, a new set of GPT-4 participants was run for the image rating task, where each LLM participant first provided a description of each image. Then, the LLM participants were asked to make their pairwise ratings based on their own descriptions of the images they had just provided (see Figure 1B and Example 3). In other words, the images were first converted to text descriptions, and similarity judgments were made based on these text descriptions. The results of these experiments are also indicated in Supplemental Tables 2 and 3 (e.g., denoted \u201cGPT-4-Vision Descriptions\u201d or \u201cVis. Desc.\u201d). Rating text descriptions in this way increased alignment between each GPT-4 image processing model and the SPOSE model of human visual semantics for both stimulus sets (for GPT-4-Vision based on Carlson-Image: $r_s$ = 0.518 to 0.635; based on THINGS: $r_s$ = 0.529 to 0.613; for GPT-40-mini based on Carlson-Image: $r_s$ = 0.533 to 0.592; based on THINGS: $r_s$ = 0.537 to 0.665; for GPT-40 based on Carlson-Image: $r_s$ = 0.545 to 0.610; based on THINGS: $r_s$ = 0.606 to 0.653; W = 2, p < 0.001, two-sided Wilcoxon Rank-Sum test comparing LLM image ratings and SPOSE model alignment with LLM text description ratings and SPOSE model alignment). Ratings based on text-descriptions of the images also increased cross-modal alignment for each model's corresponding text-only word ratings, and this was especially notable for GPT-40-mini (for GPT-4-Vision based on Carlson-Image: $r_s$ = 0.458 to 0.552; based on THINGS: $r_s$ = 0.504 to 0.559 for GPT-40-mini based on Carlson-Image: $r_s$ = 0.490 to 0.586; based on THINGS: $r_s$ = 0.483 to 0.619; for GPT-40 based on Carlson-Image: $r_s$ = 0.478 to 0.526; based on THINGS: $r_s$ = 0.527 to 0.558; W = 1, p < 0.005, two- sided Wilcoxon Rank-Sum test).\nNext, we assessed whether different approaches to operationalizing LLM participants impacted alignment results. Aher et al. (2023) ascribed a surname and honorific to each new LLM participant, and other experiments have simply re-queried the model without explicitly assigning an identity, and still obtained a distribution of human-like responses (Cava & Tagarelli, 2024; Marjieh et al., 2024; Mei et al., 2024) or did not find substantial differences among individual-level prompts for improving alignment with human behavior (Petrov et al., 2024). \u03a4\u03bf examine what effect this has on LLM responses and the semantic representation distances elicited by our Turing RSA experiment, we re-ran our experiments for the GPT models with all surnames and honorifics removed and measured group-level representational alignment. The results of these experiments are reported in Supplemental Tables 2 and 3 (denoted by \u201c(Repeats)\u201d or \u201c(Rep.)\u201d). Removing surnames and honorifics increased alignment with human ratings for each of the text-only GPT models (increase in GPT-3.5 alignment from $r_s$ = 0.456 to $r_s$ = 0.521; increase in GPT-4 alignment from $r_s$ = 0.696 to $r_s$ = 0.708; increase in GPT-40-mini alignment from $r_s$ = 0.736 to $r_s$ = 0.746). GPT-40 run without surnames or honorifics produced the highest correlation between LLM and human responses observed in these studies (increase in GPT-40 alignment from $r_s$ = 0.740 to $r_s$ = 0.758). However, there were mixed results regarding whether this more minimal style of prompting improved alignment in the image rating experiments (see Supplemental Table 3).\nThe temperature hyperparameter increases or decreases the verbosity and randomness of a large language model's responses, and thus could influence LLM persona responses (as reported by Cava and Tagarelli, 2024). We explored the influence of this hyperparameter in our text- based experiments by re-running our GPT-3.5 and GPT-4 participants across a range of temperature settings: 0.01, 0.7 and 1.5 (combined with our results reported thus far which were run at 1.0). In general, this did not have a substantial or systematic influence on alignment (GPT- 3.5 $r_s$ = 0.451, 0.464, 0.456, 0.454; and GPT-4 $r_s$ = 0.687, 0.692, 0.696, 0.699 across temperatures of 0.01, 0.7, 1.0 and 1.5, respectively). However, we noted changes in the consistency of responses across participants in these temperature-sweep experiments, which we address in the following section."}, {"title": "Individual Variability in Human and LLM Participant Responses", "content": "Individual Variability in Human and LLM Participant Responses\nIndividual differences (or the variability observed between individuals) is a fundamental aspect of human behavior. Inter-individual variability was salient in our human behavioral data (for text-based ratings, Figure 2A), and overall lead to only modest agreement among human raters. Thus, an accurate encapsulation of human behavior by a cohort LLM participants would be able to achieve both a high alignment with human responses at the group level, and moderate inter- individual variability. Quantifying and matching these inter-individual differences with LLMs is critical for generating useful proxies of human behavior, but this has thus far been a less- explored dimension of human-Al alignment. For example, when viewing a cohort of GPT-40 participants' responses (Figure 2B) there is a stark contrast in terms of the homogeneity of LLM responses relative to human participants. To quantify this aspect of performance, alignment among the unique participants (human and simulated) within each experiment was calculated along with an intraclass correlation coefficient (ICC) among each set of participants. We compare these measures alongside group-level representational alignment in Figure 2."}, {"title": "Discussion", "content": "The performance of LLMs continues to rapidly improve, raising increasingly important questions about reliability, explainability and alignment with human objectives. If LLMs begin to be used widely as proxies for human behavior (potentially in simulations, as assistants or for human subjects testing), methods will be needed for assessing how human-like a given model's behavior can be across a wide array of scenarios and for increasing alignment between LLMs and humans. We developed a generalizable pairwise rating task, called Turing RSA, to probe the representational structure of LLMs that would otherwise be black-box interfaces. Experiments using this task found that GPT-40's ratings conveyed a representational structure that is highly (but not perfectly) aligned with human semantic representational structure (obtained through the same behavioral rating task), especially when compared to other smaller models and when relying primarily on text processing capabilities (regardless of the input modality). Also, despite being smaller, many of the recent generation of compact 8- to 14-billion parameter language models such as Llama-3, Phi-3, Gemma-2 and Solar were still well aligned with human semantic ratings (more even than the substantially larger GPT-3.5). However, the inter-individual variability observed among humans was difficult to reproduce among LLM participants. Group- level alignment between LLMs and human behavior could be increased by changing some prompts and hyperparameters. Finally, human participants' object ratings were more consistent across text and image modalities than cross-modal LLM ratings.\nThese studies extend prior work (Dickson et al., 2024; Marjieh et al., 2022, 2024; Marjieh, Rijn, et al., 2023) by examining multiple models, comparisons within and across stimulus domains, and by examining inter-individual variability all within the same framework using matched stimuli. Our use of a continuous subjective rating task rather than assessing accuracy or performance like many LLM evaluations (e.g., Zhou et al., 2024) provides a useful complement to standard practices. The pairwise rating method used here can help probe nuanced, high-level features of knowledge representations and relationships among concepts in a flexible manner.\nThere are a number of limitations to this study that are well-suited for future work. First, our main goal was to develop a generalizable method for querying LLM behavior as a tool to understand Human-LLM alignment, reliability and explainability. These experiments involved a set of well-studied stimuli (words, images etc.) and relied heavily on previously collected or publicly available data sets. However, these materials, like all stimulus sets, are not exhaustive, are limited in scope and may have other shortcomings and biases (see Grootswagers and Robinson, 2021 or Thoret et al., 2021 for discussion). This of course stems in large part from their being developed with human testing in mind, and thus would be subject to practical constraints on a human participant's time and patience. A more comprehensive set of stimuli to fully probe LLM behavior and knowledge will require additional development. This might include scaling up Turing RSA testing stimuli to operate over more sentences, paragraphs, audio clips or movies, as well as using stimuli that can better target expertise, emotions or personality dimensions.\nA related limitation is that our human comparison cohort comprised a relatively small sample of participants (n = 8). As discussed, pairwise rating tasks are burdensome to carry out due to the time and effort required of participants. Nonetheless"}]}