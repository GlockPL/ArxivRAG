{"title": "Evaluating graph-based explanations for Al-based recommender systems", "authors": ["Simon Delarue", "Astrid Bertrand", "Tiphaine Viard"], "abstract": "Recent years have witnessed a rapid growth of recommender systems, providing suggestions in numerous applications with potentially high social impact, such as health or justice. Meanwhile, in Europe, the upcoming AI Act mentions transparency as a requirement for critical Al systems in order to \"mitigate the risks to fundamental rights\". Post-hoc explanations seamlessly align with this goal and extensive literature on the subject produced several forms of such objects, graphs being one of them. Early studies in visualization demonstrated the graphs' ability to improve user understanding, positioning them as potentially ideal explanations. However, it remains unclear how graph-based explanations compare to other explanation designs. In this work, we aim to determine the effectiveness of graph-based explanations in improving users' perception of AI-based recommendations using a mixed-methods approach. We first conduct a qualitative study to collect users' requirements for graph explanations. We then run a larger quantitative study in which we evaluate the influence of various explanation designs, including enhanced graph-based ones, on aspects such as understanding, usability and curiosity toward the AI system. We find that users perceive graph-based explanations as more usable than designs involving feature importance. However, we also reveal that textual explanations lead to higher objective understanding than graph-based designs. Most importantly, we highlight the strong contrast between participants' expressed preferences for graph design and their actual ratings using it, which are lower compared to textual design. These findings imply that meeting stakeholders' expressed preferences might not alone guarantee \"good\" explanations. Therefore, crafting hybrid designs successfully balancing social expectations with downstream performance emerges as a significant challenge.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems have emerged as fundamental tools for delivering personalized services to users. These frameworks find application across diverse domains, ranging from commonplace ones like online library and e-shopping, to more contentious ones such as finance, law, education or health. In cases where recommendations carry significant implications for users, it becomes essential to provide additional explanatory mechanisms. In Europe, ongoing legal discussions highlight the likelihood of transparency becoming a requirement for high-risk Artificial Intelligence (AI) systems [14]. From the academic perspective, several works underscore a positive correlation between users' understanding of the model and their trust in such systems [26, 47, 49, 54, 58].\nExplanations aim to address the \"why\" question in relation to a recommender system or a specific prediction [58]. Emerging from two primary research fields, namely Computer Science (CS) and Human Computer Interaction (HCI), various forms of explanations, such as text [52], charts [18, 24], matrices [8], hybrid designs [4, 9, 28] or graphs [41, 53, 55], have been proposed to enhance user satisfaction toward AI systems. Nevertheless, determining whether an explanation qualifies as a \"good\" one is not straightforward. On the one hand, Miller [31] argues that \u201cgood\u201d explanations should be (i) contrastive, highlighting contrast with alternatives, (ii) selected, recognizing that people seldom expect the complete cause of an event and (iii) preferring causal over probabilistic reasoning. Meanwhile, authors in [26] suggest that it is essential to ensure that explanations align with stakeholders' desiderata, sometimes referred to as social expectations [20].\nWe hypothesize that graphs, as objects that can model selected relational data and exhibit causality, appears to align intuitively with Miller's criteria and may have the ability to fulfill various users' requirements. More specifically, the recommendation task can be naturally framed as a link prediction task in a bipartite graph (see Figure 1) that includes both users and items. The relevance of graph structures in problem-solving is not novel, as its roots trace back hundreds of years to Euler's solution to the K\u00f6nigsberg bridges problem [13]. More recently, within the HCI field, several works have explored the impact of graph visualization through extended user studies [6, 19]. However, only a few works [15, 24] have explored the application of such designs in the explanatory context of recommender systems. In the CS community, when graphs are considered as explanations, they are often evaluated from an algorithmic perspective, also referred to as functionally-grounded evaluation [10]. This setup involves the use of machine learning-oriented proxy metrics that operate without human oversight, contradicting the criteria outlined in [20, 26].\nIn this work, we seek to bridge the gap between the HCI field, where graph designs are seldom examined in the context of explainability, and the CS field, where graphs are ubiquitous in explainable recommender systems \u2013 either as components of models or explanation designs \u2013 but are primarily evaluated from an algorithmic standpoint. To achieve this, we conduct a qualitative user study to characterize users' needs in terms of graph-based designs. Specifically, we gather requirements from users with varying levels of expertise in Al systems. Leveraging this knowledge, we develop an enhanced graph-based design and compare it to two commonly used forms of explanations - textual and SHAP [28]-based explanations - through a quantitative user study. Through these studies, our goal is to answer the following research questions:"}, {"title": "2 RELATED WORK", "content": "2.1 Explainable AI for recommender systems\nExplainable recommendations offer user personalized item suggestions while explicitly clarifying why a particular item is being proposed. The design of explanations for recommendations has been studied for a long time [18, 47]. Numerous studies emphasized the correlation between understanding the system and placing trust in it [26, 47, 49, 54, 58], amplifying the significance of such explanations. While recommender systems can be crafted to naturally include interpretable elements (model-intrinsic) [59], our focus is on approaches involving post-hoc explanations (model-agnostic), also known as post-hoc interpretability [27, 31].\nDesigning such explanation is a task situated at the intersection of two research fields, namely Computer Science (CS) and Human Computer Interaction (HCI), each with its distinct focus and requirements.\n2.2 Explainable AI in Computer Science\nFrom the CS perspective, building explainable recommendations involves a variety of techniques and explanation types. This includes rule mining [40], approximation using simple models as with LIME [43], feature importance methods such as SHAP [28], attention maps [44] or graphs [41, 53, 55]. Concerning graphs, authors often highlight their natural ability to reveal user-item connectivity [53] or extract small subgraphs containing elements most influential for predictions [55]. In this context, justifications for using graphs as explanations for recommendations seem to align seamlessly with Miller's description of good explanations [31].\nHowever, the evaluation of such explanations in the CS field heavily relies on functionally-grounded approaches [10], i.e. algorithmic-oriented techniques that measure proxy metrics such as fidelity or correctedness [34], but lack human control. While such an evaluation approach offers advantages by saving time and avoiding ethical concerns potentially associated with human participation, authors in [10, 32] emphasize the limitations of these approaches in terms of \"real-world impact\" and advocate for their use solely after conducting user studies. To tackle this issue, recent work integrated both graph-based explanations and an evaluation framework through a user study [57]. However, the authors restricted their comparison to graph designs among themselves only, preventing a comprehensive assessment of the validity of graph explanations against alternative designs.\nIn this work, we go beyond the conventional algorithmic-oriented evaluation typically employed in the CS field and introduce both qualitative and quantitative user studies to evaluate the validity of graph-based explanations compared to other designs.\n2.3 Explainable AI in Human Computer Interaction\nIt is acknowledged that visualization can provide cognitive support through various mechanisms, e.g. helping in pattern discovery, summarizing large volumes of data, or reducing search time [50]. The HCI field extensively studies the influence of design choices, with several works emphasizing their significance in impacting users' understanding and ability to contextualize problems. In particular, numerous studies have explored the impact of graph visualizations and showed the influence of the overall setting on their performance. For instance, several works demonstrated that node-link representations were indeed advantageous over matrix representations, when faced with small graphs and when path-oriented tasks were involved [16, 17, 23, 37, 38]. For other tasks, such as weighted graph comparison [3] or suspicious node detection [29], matrix representations have proven to be a superior choice over node-link diagrams. More recently, authors in [9] analyzed hybrid visualizations that combine node-link and matrix representations. Their goal was to provide a comprehensive tool for analyzing real-world networks that are globally sparse but locally dense. They showed that in such configurations, their mixed model overcomes the limitations of using the node-link diagram alone. While all these approaches systematically involve user study evaluations, they are not specific to the explanatory context of recommender systems.\nFew works from the HCI field address both graphs and their evaluation as explanations. In an early study, authors in [36] carried out a user study to assess the impact of various interactive graph-based representations of a recommender system. They showed that 78% of users \"felt that the system provided a good explanation of collaborative filtering\". However, the graph representation itself was not challenged and the study only focused on the layout of the system (profile-based or not) and its interactivity. More recently, authors in [24] involved graphs to highlight user preferences for item-based explanations. However, their graphs consist in concentric circle diagrams or pathways between columns, which we argue do not fully leverage the capabilities of graphs. We draw a similar conclusion for [15], where authors show participants' preference for action-oriented over connection-oriented explanations, but where graphs are limited to the paths they can exhibit.\nIn this work, similar to the aforementioned studies, we maintain the evaluation process that involves user studies to assess design performance. However, we deviate from them by integrating our analysis directly into the explanatory context."}, {"title": "3 STUDY 1: QUALITATIVE EXPLORATION OF STAKEHOLDERS' EXPECTATIONS REGARDING GRAPH-BASED EXPLANATIONS", "content": "We seek to answer RQ1 by exploring stakeholders' expectations regarding graph-based explanations for AI-based recommender systems. To achieve this, we conduct a qualitative study involving participants with varying expertise levels in Al systems, employing a think-aloud case study to collect insights and remarks.\n3.1 Study design\nWe interviewed 12 participants. All participants were volunteers, recruited through an email campaign within the university with which the authors are affiliated. We conducted a 30-minute interview with each participant. To ensure data privacy, participants signed a consent form designed and approved in collaboration with the Data Protection Officer. Each interview was then divided into three parts.\nIn the first part of the session, we assessed the perceived expertise level of participants regarding AI-based systems in general and recommender systems specifically, using a preliminary questionnaire. From these answers, we obtained three distinct groups. The first group consists of Experts (4 participants); people who consider themselves really familiar with AI-based systems and either have a precise idea of what recommender systems are, or developed such algorithms. On the other side of the spectrum, another group includes non-Experts (2 participants); these users consider themselves slightly familiar with AI-based systems and are only interacting with recommender systems as users, or have a general idea of how these systems work. The last group lies in-between these two groups and includes Insiders (6 participants); people who are familiar with AI-based systems and have a general idea of how recommender systems work.\nDuring the second part of the interview, we evaluated participants' perceived understanding of AI-based systems. They answered questions about their anticipated risks for recommender systems, their comprehension of the recommendations when they receive some, and their understanding of the criteria influencing these recommendations.\nLastly, we introduced a task-oriented think-aloud scenario. In this scenario, we presented participants with an AI-based book recommendation. To increase participant engagement, we collected their book preferences prior to the interview and used this information to create a personalized context incorporated into the AI system, thus making the experiment more realistic. The system recommendation was presented within a graph-based explanation, featuring a subset of the participants' previously read books alongside other users and their readings. This explanation was displayed as a bipartite graph, where users were connected to the books they had read, with a distinct link indicating the system recommendation. Lastly, we provided information about book preferences using link weighting (see Figure 1 for an illustration). We did not disclose to the participants the details of the recommendation algorithm or how we had chosen the other visible users. We asked the participants to discuss the relevance of the recommendation to them, their understanding of the elements that led to this recommendation, and what kind of information was missing to better understand the recommendation.\n3.2 Results\nWe extracted key insights from the interviews, focusing on participants' understanding of the recommendation through the lens of graph-based explanation. Additionally, we performed an inductive content analysis [12] of the notes taken during the interviews, identifying main themes related to participants' desiderata toward graph-based explanation. These results are summarized in Table 1."}, {"title": "4 STUDY 2: INFLUENCE OF EXPLANATION DESIGN", "content": "To address RQ2, RQ3 and RQ4, we conduct a qualitative user study in which we investigate the influence of explanation design, among them a graph-based explanation, on various AI-based recommender system users. An example of our interface is shown in Figure 5 in the Appendix.\n4.1 Study design\n4.1.1 Recommender system. For this purpose, we use the setup introduced in our qualitative experiment (see Section 3): we build an AI-based book recommender system which goal is to provide the participant with a book suggestion given pre-selected reading preferences. We did not share the functioning details of the recommender system with the participants, but specified that the recommendation they were given was AI-based.\n4.1.2 Explanation design. We introduced three different explanation designs, that were detailed to each participant through a small paragraph, as well as a reading key. We used the well-known SHAP [28] approach to design a feature importance oriented explanation design. This choice stems from the popularity of this method in the Explainable AI (XAI) community as well as for its tight bounds with the users' expectations obtained from our first study, i.e. book characteristics are considered important for the quality of the explanation. We also presented a Text explanation, containing concise item-based sentences stating why this recommendation was made. This choice was influenced by recent studies indicating that text explanations, specifically with item-based wording, were perceived more persuasive than other visual formats [25] and were effective in providing the user with personalized feeling, which is correlated with trust in the system [56]. Finally, we improved the Graph design introduced in Figure 1 based on participants' feedback from our qualitative study. Specifically, we focused on item characteristics by projecting the initial bipartite graph into an item-oriented one. In this representation, each book is a node, and edges connect books that share common readers in our database. Additionally, we incorporated information about book characteristics (literary genre, author, publication year, etc) through node colors. Consequently, this final graph representation aligns more closely with users' expectations. We illustrate these designs in Figure 2.\n4.1.3 Experimental conditions. Overall, 66 participants were recruited through two channels: (i) students, doctoral students and researchers from the author's university and affiliated laboratories, and (ii) users of the Reddit forum \u201cSampleSize\u201d\u00b9 dedicated to user studies. All participants were asked to fill out an online form divided into two parts. First, as in the previous study, participants answered preliminary questions about their perceived expertise level toward AI-based systems in general and AI-based recommender systems in particular. We ended up with 11 non-Experts (16.7%), 32 Insiders (48.5%) and 23 Experts (34.8%). We then asked each participant to choose their favorite book selection from among 5 sets of 10 books and used our AI-based recommender system to suggest a new book. Each participant was shown all explanation designs for this recommendation (in a random order), and for each design, they were asked to answer a set of questions related to our evaluation measures. Finally, we collected participants' design preferences by explicitly asking them to rank the three designs. The whole study lasted around 10 minutes.\nIn summary, this study is constructed using a Mixed Factorial design, involving the participants' expertise level as a between-subject variable and the explanation design as a within subject variable. Therefore, in the following analysis, we assess three effects: (i) the influence of the within-subject variable on the measures, (ii) the influence of the between-subject variable on the measures, and (iii) the effect of the interaction between the two factors on the measures.\n4.1.4 Evaluation constructs. The same set of questions was used for each explanation design. These questions are built according to previous literature on explanation evaluation within AI-based frameworks [4, 21, 39, 42] and are shaped to fit our specific experimental design. All questions have the same structure; given an explanation design, we ask the participant \"Please evaluate your level of agreement with the following sentences.\". Each participant answers using a 5-point Likert scale ranging from Strongly disagree to Strongly agree. We organized the study around four global constructs (detailed below). We verified the internal validity of the questionnaire by computing Pearson correlations between answers within each construct and the total of each construct (at this stage we discarded results from one question), and its internal reliability using McDonald's \u03c9 [30]. Detailed questions and corresponding categories are provided in Table 2.\nSubjective understanding. We measured subjective understanding by asking participants if they understood the recommendation made by the system or if they were able to derive insights about its internal mechanics.\nObjective understanding. We measured objective understanding of the recommendations by asking participants about the features used by the system to provide the recommendation. These questions were specifically designed for the study but encompass different recommender systems elements that have been emphasized in our first study as well as in previous studies from the literature [4, 25], e.g. item-based influence, user-based influence or self-historical information influence.\nUsability. Usability was measured by asking participants how difficult the explanation was to read and understand. We also asked participants about how easily other people would learn to read this explanation [2].\nCuriosity. We measured the curiosity induced by the explanation design using questions adapted from [21, 39]. We asked the participants if they were curious to know why the recommender system did not provide another suggestion, and if the recommendation incited curiosity.\n4.2 Results\nWe used a repeated-measure analysis of variance (RM-ANOVA) to analyze the collected measures for each participant. Each measure consisted in the average of the ratings for the corresponding questions. All measures passed the sphericity assumption (constant variance across repeated measure), either using the traditional Mauchly's test or after applying Greenhouse-Geisser correction. We confirmed the homogeneity of variance for all levels of the repeated measures using Levene's test. When significant effects were observed, we conducted a post-hoc Bonferroni test for pairwise comparisons. Results are summarized in Figure 3.\n4.2.1 Text rather than graph design for higher objective understanding. We found a statistically significant main effect of explanation design on participants' objective understanding (p = .043). Post-hoc analysis with a Bonferroni adjustment revealed that text-based design led to significantly higher objective understanding than graph-based design (p = .046) (see Figure 3a), but that there was no statistically significant difference for the levels of objective understanding between graph and SHAP-based designs, nor between text and SHAP-based designs. The main effect of the expertise level on objective understanding was not statistically significant (p = .424), i.e., if we ignore the explanation design being evaluated, Experts, Insiders and non-Experts gave similar ratings considering the objective understanding measure. However, within the Expert group, objective understanding was statistically significantly higher for text-based design than for graph-based design (p = .022).\n4.2.2 Graph and text designs increase usability. There was a statistically significant main effect of the explanation design on usability (p < .001), but no statistically significant influence of the level of expertise on this measure (p = .398). Post-hoc tests with a Bonferroni adjustment revealed that SHAP-based design led to statistically significantly lower usability compared to graph (p = .043) or text-based (p < .001) designs (see Figure 3d). However, we did not find any statistically significant difference between graph and text-based designs (p = .418). Within expertise levels, SHAP-based design led to statistically significantly lower level of usability than text-based design, for both Insiders (p = .007) and Experts (p = .006). Among Experts, usability was also statistically significantly higher for text-based design than for graph-based design (p = .008).\n4.2.3 Higher expertise increases curiosity for graph and text designs. Our analysis determined that there was no statistically significant effect of explanation design on participants' curiosity (p = .514). However, the expertise level had a statistically significant influence on curiosity (p = .016). Post-hoc tests with a Bonferroni adjustment revealed that being Insider led to significantly lower levels of curiosity compared to being Expert (p = .02). Specifically, when faced with graph-based designs, Experts showed statistically significantly higher levels of curiosity than Insiders (p = .041). Similarly, when faced with text-based designs, Experts were significantly more curious than Insiders (p = .032) (see Figure 3c). However, we did not find any statistically significant difference in curiosity levels between non-Experts and other users. Furthermore, we did not find any statistically significant difference in ratings between participants when using SHAP-based design.\n4.2.4 No effect of explanation design or expertise level on subjective understanding. We did not observe any statistically significant influence of explanation design on participants' subjective understanding (p = .104). Similarly, the expertise level of participants did not have any statistically significant influence on their subjective understanding (p = .185) (see Figure 3b).\n4.2.5 Graph-based design is preferred, regardless of the expertise level. At the end of the questionnaire, participants were asked to rank explanation designs according to their preferences. A Chi-Square Goodness of Fit Test was performed to determine whether the design preferences were equally distributed across the 6 possible outcomes. Our results revealed that the obtained proportions significantly differed (p = .015). We display in Figure 4 the number of occurrences of each design at each ranking position. We show that participants preferred graph-based design more frequently than other designs. When graph-based design was not ranked in first position, it was ranked in second position more often than in third position. The second best-ranked design was text-based explanation. SHAP-based explanation was ranked in last position more than 35 times. Finally, we explored the relationship between participants' expertise level and design preference. We ran a Chi-Square test and did not find any statistically significant evidence of correlation between the two variables (p = .841)."}, {"title": "5 DISCUSSION", "content": "5.1 Graph vs. textual explanations\nAccording to Miller [31], a \"good\" explanation should be contrastive, selected, dialogic and causal. We argued that graphs inherently model causal and contrastive relationships between entities through their node-link structure. They achieve the selected criterion by potentially representing a subset of the original data, e.g. in the vicinity of a recommender system's prediction. Finally, the node or link attributes they may possess contribute to them being considered dialogic. Consequently, we aligned with previous studies that acknowledged the strong expressive power of graphs and the fact that they were especially well-suited for applications like recommender systems [1].\nInitial findings from our qualitative study affirmed these statements and emphasized the potential effectiveness of graph-based explanations. With the exception of one Expert who preferred textual explanations, participants found graph-based design \"very easily interpretable\" and highlighted the importance of showcasing \"relations between groups\". This was further confirmed quantitatively; when asked for their preferred explanation type, the majority of participants in our second study favored graph-based explanation over text or SHAP designs (see Figure 4). Specifically, users expressed a preference for item-centric graph explanations over user-centric ones, aligning with results from Kouki et al.'s work [24, 25], which compared textual and visual explanations, although not specifically graphs.\nHowever, despite their expressed preferences for graph-based design, our second study revealed that participants had significantly higher levels of objective understanding and usability when using textual designs. Such contrast between expressed desiderata and measured performance aligns with previous research [5, 48] where authors emphasized users' preference for visual explanations (though not specifically graphs) over texts, despite their poorer performance when using the former kind. A plausible explanation for this tendency may be grounded in the findings of a recent perception study [33] in which authors showed that interest and attractiveness of a stimuli can be well predicted by the complexity of this stimuli; in our case, the visual complexity of our graph explanation, compared to textual explanation could encourage participants to prefer this design over others. More recent research [7] explored this direction in the context of graph designs and further confirmed the close relationship between complex visual structure and interest.\nOverall, our findings suggest that depending solely on stakeholders' expressed desiderata for crafting graph-based explanation designs may not be sufficient. Considering both their preferences and measured performance when using such explanations could provide a more comprehensive understanding of what constitutes a good explanation.\n5.2 Impact of expertise level\nDespite its enhanced design tailored to the needs of users with various levels of expertise, our graph explanation did not lead to significantly higher ratings on the different measures. Nevertheless, noteworthy insights can be derived from this visual design. For example, we showed that high expertise level positively influenced participants' objective understanding when facing textual explanation vs. graph explanation, but we did not reveal any difference in ratings for non-Experts. This contrasts with the findings in [48] showing how lay users performed better with textual explanations vs. visual explanations. While authors evoke the plausible effects of confirmation bias, i.e. favoring elements confirming preconceived ideas, on lay users to explain their results, we did not notice such behavior in our qualitative study. An interpretation of our results could be that Experts may be more inclined than non-Experts to have preconceived notions, or false narratives about the internal functioning of the system, a behavior that has been particularly observed for experienced users by previous works [22, 51]. These a priori may be more contradicted by complex designs, such as SHAP or graph, compared to relatively vague explanations like the textual design we proposed.\nWe also demonstrated higher curiosity levels for Experts compared to Insiders, when using graph and text designs, which aligns with previous research [35] stating that curiosity is strongly associated with intrinsic interest and motivation. We hypothesize that Expert participants are more engaged in the proposed tasks due to their interest in the topic. However, this tendency is not observed with SHAP-based explanation, warranting further analysis for comprehensive understanding.\nOur analysis finally revealed that both Insiders and Experts considered text-based design more usable than SHAP-based design, but only the most experienced participants found text-based explanations more usable than graph-based ones. This could be attributed to users with moderate expertise being more persuadable by graphical design and experiencing some kind of over-trust [11]. Or it may be the result of Experts being more inclined to over-analyze graph explanations to align with their knowledge of recommender systems and network layouts. Additionally, we did not find any statistically significant difference between levels of expertise when measuring graph usability. This observation aligns with previous research in the biological field [46], showing that novice users could create and evaluate graph layouts as effectively as domain experts.\nIn summary, we highlighted how graph-based explanations were subject to different cognitive biases according to the epxertise level of the end-users. To address this limit, future work could involve further inclusion of cognitive factors [45] in explanations to further enhance perceived designs.\n5.3 Limitations\nIn our quantitative study, we compelled participants to choose from pre-defined user profiles based on a selection of representative books. This action further determined the algorithm recommendation and consequently the content of each explanation. While this approach allows us to use pre-computed answers, which significantly simplifies the questionnaire procedure, we acknowledge that it might hinder participants from fully recognizing their tastes within the recommender system's choices, potentially influencing their design preferences. To address this concern, we verified that the user profile chosen by participants was not correlated with their design preferences. For this purpose, we ran a Chi-Square test between the two variables and the results revealed that there is not enough evidence to suggest an association between the selected user profile and design preferences (p = .549).\nIn this work, we did not impose any time constraint on participants during their analysis of design, and the only time indication provided was that the questionnaire should take around 10 minutes to complete. Consequently, participants had sufficient time to explore and analyze each recommendation, which may not entirely replicate a real-world scenario where end users require an explanation to make a decision. Future research aimed at assessing the generalizability of our findings could involve time-constrained tasks tailored for specific contexts.\nMore generally, the results presented in this work are derived from data related to book recommendations. This simplistic use case facilitated the building of the study since it mitigated potential ethical concerns. While our results may have applicability in other domains, additional experiments would be necessary to confirm this."}, {"title": "6 CONCLUSION", "content": "In this work, we have conducted a qualitative study to understand the needs for graph-based explanation designs expressed by users with various levels of expertise. From these results, we built an enhanced graph-based design emphasizing several key recommendations such as a content-based approach obtained through bipartite graph projection or the focus on item similarity rather than user-similarity. We then conducted a quantitative study in which we investigated the influence of SHAP, textual and graph-based explanation designs regarding four constructs: objective understanding, subjective understanding, curiosity and usability. Our results revealed that text-based explanations significantly improved objective understanding to graph-based explanation, and that it was specifically marked for users with a higher level of expertise. On the other hand, we did not find any statistically significant evidence of influence of explanation design on subjective understanding. We also showed that both graph and text explanations were considered more usable than SHAP-based design, and that it was particularly true for users with middle to high level of expertise. Moreover, we highlighted the influence of expertise level on curiosity by showing how expert users were more curious than insiders toward graph and textual explanations. Lastly, we emphasized the discrepancy between participants' expressed preferences for graph-based explanations during qualitative interviews and further confirmed quantitatively, and their higher ratings regarding understanding or usability for textual designs. This outcome suggests that solely fulfilling stakeholders' desiderata may not suffice to achieve \"good\" explanations. Crafting hybrid designs achieving a balance between social expectation and effective downstream performance emerges as a significant challenge."}, {"title": "7 ETHICAL CONCERNS", "content": "To ensure participants' privacy, the qualitative study was designed and approved in collaboration with the Data Protection Officer (DPO) of the authors' affiliated school. Anonymized versions of interview notes for data analysis are securely stored, with access restricted to the authors. We did not collected any personal information for the quantitative study."}]}