{"title": "Evaluating graph-based explanations for Al-based recommender systems", "authors": ["Simon Delarue", "Astrid Bertrand", "Tiphaine Viard"], "abstract": "Recent years have witnessed a rapid growth of recommender systems, providing suggestions in numerous applications with potentially high social impact, such as health or justice. Meanwhile, in Europe, the upcoming AI Act mentions transparency as a requirement for critical AI systems in order to \"mitigate the risks to fundamental rights\". Post-hoc explanations seamlessly align with this goal and extensive literature on the subject produced several forms of such objects, graphs being one of them. Early studies in visualization demonstrated the graphs' ability to improve user understanding, positioning them as potentially ideal explanations. However, it remains unclear how graph-based explanations compare to other explanation designs. In this work, we aim to determine the effective-ness of graph-based explanations in improving users' perception of AI-based recommendations using a mixed-methods approach. We first conduct a qualitative study to collect users' requirements for graph explanations. We then run a larger quantitative study in which we evaluate the influence of various explanation designs, including enhanced graph-based ones, on aspects such as understanding, usability and curiosity toward the AI system. We find that users perceive graph-based explanations as more usable than designs involving feature importance. However, we also reveal that textual explanations lead to higher objective understanding than graph-based designs. Most importantly, we highlight the strong contrast between participants' expressed preferences for graph design and their actual ratings using it, which are lower compared to textual design. These findings imply that meeting stakeholders' expressed preferences might not alone guarantee \"good\" explanations. Therefore, crafting hybrid designs successfully balancing social expectations with downstream performance emerges as a significant challenge.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems have emerged as fundamental tools for delivering personalized services to users. These frameworks find application across diverse domains, ranging from commonplace ones like online library and e-shopping, to more contentious ones such as finance, law, education or health. In cases where recommendations carry significant implications for users, it becomes essential to provide additional explanatory mechanisms. In Europe, ongoing legal discussions highlight the likelihood of transparency becoming a requirement for high-risk Artificial Intelligence (AI) systems [14]. From the academic perspective, several works underscore a positive correlation between users' understanding of the model and their trust in such systems [26, 47, 49, 54, 58]."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Explainable AI for recommender systems", "content": "Explainable recommendations offer user personalized item suggestions while explicitly clarifying why a particular item is being proposed. The design of explanations for recommendations has been studied for a long time [18, 47]. Numerous studies emphasized the correlation between understanding the system and placing trust in it [26, 47, 49, 54, 58], amplifying the significance of such explanations. While recommender systems can be crafted to naturally include interpretable elements (model-intrinsic) [59], our focus is on approaches involving post-hoc explanations (model-agnostic), also known as post-hoc interpretability [27, 31].\nDesigning such explanation is a task situated at the intersection of two research fields, namely Computer Science (CS) and Human Computer Interaction (HCI), each with its distinct focus and requirements."}, {"title": "2.2 Explainable AI in Computer Science", "content": "From the CS perspective, building explainable recommendations involves a variety of techniques and explanation types. This includes rule mining [40], approximation using simple models as with LIME [43], feature importance methods such as SHAP [28], attention maps [44] or graphs [41, 53, 55]. Concerning graphs, authors often highlight their natural ability to reveal user-item connectivity [53] or extract small subgraphs containing elements most influential for predictions [55]. In this context, justifications for using graphs as explanations for recommendations seem to align seamlessly with Miller's description of good explanations [31].\nHowever, the evaluation of such explanations in the CS field heavily relies on functionally-grounded approaches [10], i.e. algorithmic-oriented techniques that measure proxy metrics such as fidelity or correctedness [34], but lack human control. While such an evaluation approach offers advantages by saving time and avoiding ethical concerns potentially associated with human participation,"}, {"title": "2.3 Explainable AI in Human Computer Interaction", "content": "It is acknowledged that visualization can provide cognitive support through various mechanisms, e.g. helping in pattern discovery, summarizing large volumes of data, or reducing search time [50]. The HCI field extensively studies the influence of design choices, with several works emphasizing their significance in impacting users' understanding and ability to contextualize problems. In particular, numerous studies have explored the impact of graph visualizations and showed the influence of the overall setting on their performance. For instance, several works demonstrated that node-link representations were indeed advantageous over matrix representations, when faced with small graphs and when path-oriented tasks were involved [16, 17, 23, 37, 38]. For other tasks, such as weighted graph comparison [3] or suspicious node detection [29], matrix representations have proven to be a superior choice over node-link diagrams. More recently, authors in [9] analyzed hybrid visualizations that combine node-link and matrix representations. Their goal was to provide a comprehensive tool for analyzing real-world networks that are globally sparse but locally dense. They showed that in such configurations, their mixed model overcomes the limitations of using the node-link diagram alone. While all these approaches systematically involve user study evaluations, they are not specific to the explanatory context of recommender systems.\nFew works from the HCI field address both graphs and their evaluation as explanations. In an early study, authors in [36] carried out a user study to assess the impact of various interactive graph-based representations of a recommender system. They showed that 78% of users \"felt that the system provided a good explanation of collaborative filtering\". However, the graph representation itself was not challenged and the study only focused on the layout of the system (profile-based or not) and its interactivity. More recently, authors in [24] involved graphs to highlight user preferences for item-based explanations. However, their graphs consist in concentric circle diagrams or pathways between columns, which we argue do not fully leverage the capabilities of graphs. We draw a similar conclusion for [15], where authors show participants' preference for action-oriented over connection-oriented explanations, but where graphs are limited to the paths they can exhibit.\nIn this work, similar to the aforementioned studies, we maintain the evaluation process that involves user studies to assess design performance. However, we deviate from them by integrating our analysis directly into the explanatory context."}, {"title": "3 STUDY 1: QUALITATIVE EXPLORATION OF STAKEHOLDERS' EXPECTATIONS REGARDING GRAPH-BASED EXPLANATIONS", "content": "We seek to answer RQ1 by exploring stakeholders' expectations regarding graph-based explanations for AI-based recommender systems. To achieve this, we conduct a qualitative study involving participants with varying expertise levels in Al systems, employing a think-aloud case study to collect insights and remarks."}, {"title": "3.1 Study design", "content": "We interviewed 12 participants. All participants were volunteers, recruited through an email campaign within the university with which the authors are affiliated. We conducted a 30-minute interview with each participant. To ensure data privacy, participants signed a consent form designed and approved in collaboration with the Data Protection Officer. Each interview was then divided into three parts.\nIn the first part of the session, we assessed the perceived expertise level of participants regarding AI-based systems in general and recommender systems specifically, using a preliminary questionnaire. From these answers, we obtained three distinct groups. The first group consists of Experts (4 participants); people who consider themselves really familiar with AI-based systems and either have a precise idea of what recommender systems are, or developed such algorithms. On the other side of the spectrum, another group includes non-Experts (2 participants); these users consider themselves slightly familiar with AI-based systems and are only interacting with recommender systems as users, or have a general idea of how these systems work. The last group lies in-between these two groups and includes Insiders (6 participants); people who are familiar with AI-based systems and have a general idea of how recommender systems work.\nDuring the second part of the interview, we evaluated participants' perceived understanding of AI-based systems. They answered questions about their anticipated risks for recommender systems, their comprehension of the recommendations when they receive some, and their understanding of the criteria influencing these recommendations.\nLastly, we introduced a task-oriented think-aloud scenario. In this scenario, we presented participants with an AI-based book recommendation. To increase participant engagement, we collected their book preferences prior to the interview and used this information to create a personalized context incorporated into the AI system, thus making the experiment more realistic. The system recommendation was presented within a graph-based explanation, featuring a subset of the participants' previously read books alongside other users and their readings. This explanation was displayed as a bipartite graph, where users were connected to the books they had read, with a distinct link indicating the system recommendation. Lastly, we provided information about book preferences using link weighting. We did not disclose to the participants the details of the recommendation algorithm or how we had chosen the other visible users. We asked the participants to discuss the relevance of the recommendation to them, their understanding of the elements that led to this recommendation,"}, {"title": "3.2 Results", "content": "We extracted key insights from the interviews, focusing on participants' understanding of the recommendation through the lens of graph-based explanation. Additionally, we performed an inductive content analysis [12] of the notes taken during the interviews, identifying main themes related to participants' desiderata toward graph-based explanation. These results are summarized in Table 1."}, {"title": "3.2.1 A high level of perceived understanding, based on three main criteria.", "content": "To the question \"To what extent do you understand the suggestions made to you by AI-based recommendation systems?\", the participants, whatever their level of expertise, said they understood the recommendations in most of the cases. In the few cases when participants find the recommendations unclear, they suggest that their profile diverges from a hypothetical \"basic\" user, or they propose that the algorithm's designers intentionally introduced a mechanism to propose a \"percentage of new things\" unknown to the users. When questioned about their understanding of the factors influencing a system to recommend a specific item, participants identified three main criteria. Both Insider and non-Expert participants reported that the recommendations are primarily built based on user characteristics, such as \"the person's gender, age\" or \"the person's interests\". Some Experts and a few Insiders believed that the \"similarity\" between their profile and the profiles of other users takes precedence in the recommendation algorithm's decision. According to this group, the cross-knowledge of a large number of users and their preferences enables the system to suggest new items. Lastly, some participants considered that a recommendation is primarily influenced by their personal history of interactions with the platform. These participants mentioned factors such as \u201cclicking on an ad\", the \"frequency of viewing\", or the \"purchase history\u201d as elements at the core of the system's suggestions."}, {"title": "3.2.2 Strong link between context knowledge and recommendation understanding.", "content": "When asked \"Do you think this recommendation is relevant? and why?\", participants unfamiliar with the item suggested by the system either expressed a lack of understanding of the recommendation (\u201cI do not know this book therefore I don't know if the recommendation is valuable.\u201d) or asked for additional information about the book's characteristics (e.g. author or literary genre) before answering. Conversely, a participant who is familiar with the recommendation or, after reviewing its related characteristics identifies familiar elements (other books by the same author), will immediately deem the recommendation relevant and give it credibility: \"The recommendation seems relevant [...] because I know the author, and it makes me want to read the book.\". In one case, the participant was familiar with the recommendation, had already read the suggested book, yet found it less relevant. This judgment was based on the perception that the recommendation did not align with the literary genre they prefer. In summary, regardless of their level of expertise, participants tend to draw parallels between the predicted book and their reading history, considering factors such as literary genre, period, or author. The analysis of similarities with other users is only taken into consideration at a later stage and appears to be optional for the positive or negative judgment of the recommendation. In one instance, the recommendation aligns with the participant's literary preferences (known and appreciated author) and is described as relevant, even if the participant does not see the connection with the elements provided in the explanation."}, {"title": "3.2.3 Graphs as objects modeling both similarities and popularity.", "content": "Participants' utilization of the graph-based explanation can be characterized along two dimensions. Firstly, Insiders and Experts follow the links between users and books to highlight similarities among users. These similarities are described through the sharing of literary tastes and represent the manifestation of their a priori understanding of how a recommender system operates. In such cases, the weight (thickness) associated with the links, especially when positive, plays an important role: \u201cIt visually strikes me a bit\u201d. Secondly, the graph-based explanation leads to an understanding of predictions in terms of popularity, i.e. participants focus on the quantity of users who have liked the recommendation. In these cases, the recommendation is seen as a consequence of the enthusiasm surrounding it, rather than its relevance to the individual user.\nA few remarks about the use of graphs by participants should be noted. The conclusions drawn regarding user similarities through graph analysis require time (a few seconds), even for Experts who, at the end of the interview, considered the graph easy to use. To derive insights about the mechanisms of the system, non-Experts tend not to analyze the graph through its links and users, but rather to focus on already-known item characteristics (genre or author). Finally, none of the participants mention graph-specific notions, such as clique, i.e. fully connected subset elements in a graph or density. Intuitively, such elements could have been used to deeply understand subgroups of users that led to the recommendation."}, {"title": "3.2.4 Graph-based explanations are considered interpretable but should contain item information.", "content": "Participants, regardless of their level of expertise, all agree that it would be useful to have more information about the characteristics of the suggested item, such as literary genre, period, etc. This would enable the creation of \"connections between different books\" and provide a \"general context\" to the decision: \"What I miss is the tool's knowledge about the world.\", \"I lack an idea of which types of books are similar to each other.\". To address this, a participant suggests using colors to highlight the proximity between groups of books. Furthermore, we noticed that the needs for additional item information align perfectly with the understanding of the recommendation; more than any other feature, participants require item characteristics that they can rely on to judge the relevance of a given recommendation.\nAn Expert and an Insider discussed the value of links representing a weak attraction to items, mentioning that they helped establish a \"contrast\" with other users, thereby enhancing their understanding of \"similarities in their profile with other users.\u201d. Some Insiders evoked adding numerical information on links, rather than playing on thickness. However, the relevance of information about disliked items, i.e. thin links in the graph, is not shared by all users: \"I don't really get the thin links in this graph.\", \"The fact that we have a thick line and a very thin one [...], side by side, confuses me a bit.\u201d. Experts find graph explanations \"very easily interpretable\" or \"sufficient\" and do not see the need for them to be \"transformed into natural language.\". One of them interprets them as the visual counterpart to the classic formula found on platforms using recommendation: \"Other users than you, who liked similar things to you, also liked...\". Moreover, they \"do not expect to have an exhaustive representation\" of the context of the recommendation. Yet, even Experts can have trouble distinguishing clear relations with other users; \"I don't see clearly whether I am close or not to other users. [...] I don't fully see the connection between me and other users.\"."}, {"title": "4 STUDY 2: INFLUENCE OF EXPLANATION DESIGN", "content": "To address RQ2, RQ3 and RQ4, we conduct a qualitative user study in which we investigate the influence of explanation design, among them a graph-based explanation, on various AI-based recommender system users. An example of our interface is shown in Figure 5 in the Appendix."}, {"title": "4.1 Study design", "content": ""}, {"title": "4.1.1 Recommender system.", "content": "For this purpose, we use the setup introduced in our qualitative experiment (see Section 3): we build an AI-based book recommender system which goal is to provide the participant with a book suggestion given pre-selected reading preferences. We did not share the functioning details of the recommender system with the participants, but specified that the recommendation they were given was AI-based."}, {"title": "4.1.2 Explanation design.", "content": "We introduced three different explanation designs, that were detailed to each participant through a small paragraph, as well as a reading key. We used the well-known SHAP [28] approach to design a feature importance oriented explanation design. This choice stems from the popularity of this method in the Explainable AI (XAI) community as well as for its tight bounds with the users' expectations obtained from our first study, i.e. book characteristics are considered important for the quality of the explanation. We also presented a Text explanation,"}, {"title": "4.1.3 Experimental conditions.", "content": "Overall, 66 participants were recruited through two channels: (i) students, doctoral students and researchers from the author's university and affiliated laboratories, and (ii) users of the Reddit forum \u201cSampleSize\" dedicated to user studies. All participants were asked to fill out an online form divided into two parts. First, as in the previous study, participants answered preliminary questions about their perceived expertise level toward AI-based systems in general and AI-based recommender systems in particular. We ended up with 11 non-Experts (16.7%), 32 Insiders (48.5%) and 23 Experts (34.8%). We then asked each participant to choose their favorite book selection from among 5 sets of 10 books and used our AI-based recommender system to suggest a new book. Each participant was shown all explanation designs for this recommendation (in a random order), and for each design, they were asked to answer a set of questions related to our evaluation measures. Finally, we collected participants' design preferences by explicitly asking them to rank the three designs. The whole study lasted around 10 minutes.\""}, {"title": "4.1.4 Evaluation constructs.", "content": "The same set of questions was used for each explanation design. These questions are built according to previous literature on explanation evaluation within AI-based frameworks [4, 21, 39, 42] and are shaped to fit our specific experimental design. All questions have the same structure; given an explanation design, we ask the participant \"Please evaluate your level of agreement with the following sentences.\". Each participant answers using a 5-point Likert scale ranging from Strongly disagree to Strongly agree. We organized the study around four global constructs (detailed below). We verified the internal validity of the questionnaire by computing Pearson correlations between answers within each construct and the total of each construct (at this stage we discarded results from one question), and its internal reliability using McDonald's \u03c9 [30]. Detailed questions and corresponding categories are provided in Table 2.\nSubjective understanding. We measured subjective understanding by asking participants if they understood the recommendation made by the system or if they were able to derive insights about its internal mechanics."}, {"title": "4.2 Results", "content": "We used a repeated-measure analysis of variance (RM-ANOVA) to analyze the collected measures for each participant. Each measure consisted in the average of the ratings for the corresponding questions. All measures passed the sphericity assumption (constant variance across repeated measure), either using the traditional Mauchly's test or after applying Greenhouse-Geisser correction. We confirmed the homogeneity of variance for all levels of the repeated measures using Levene's test. When significant effects were observed, we conducted a post-hoc Bonferroni test for pairwise comparisons. Results are summarized in Figure 3."}, {"title": "4.2.1 Text rather than graph design for higher objective understanding.", "content": "We found a statistically significant main effect of explanation design on participants' objective understanding (p = .043). Post-hoc analysis with a Bonferroni adjustment revealed that text-based design led to significantly higher objective understanding than graph-based design (p = .046), but that there was no statistically significant difference for the levels of objective understanding between graph and SHAP-based designs, nor between text and SHAP-based designs. The main effect of the expertise level on objective understanding was not statistically significant (p = .424), i.e., if we ignore the explanation design being evaluated, Experts, Insiders and non-Experts gave similar ratings considering the objective understanding measure. However, within the Expert group, objective understanding was statistically significantly higher for text-based design than for graph-based design (p = .022)."}, {"title": "4.2.2 Graph and text designs increase usability.", "content": "There was a statistically significant main effect of the explanation design on usability (p < .001), but no statistically significant influence of the level of expertise on this measure (p = .398). Post-hoc tests with a Bonferroni adjustment revealed that SHAP-based design led to statistically significantly lower usability compared to graph (p = .043) or text-based (p < .001) designs. However, we did not find any statistically significant difference between graph and text-based designs (p = .418). Within expertise levels, SHAP-based design led to statistically significantly lower level of usability than text-based design, for both Insiders (p = .007) and Experts (p = .006). Among Experts, usability was also statistically significantly higher for text-based design than for graph-based design (p = .008)."}, {"title": "4.2.3 Higher expertise increases curiosity for graph and text designs.", "content": "Our analysis determined that there was no statistically significant effect of explanation design on participants' curiosity (p = .514). However, the expertise level had a statistically significant influence on curiosity (p = .016). Post-hoc tests with a Bonferroni adjustment revealed that being Insider led to significantly lower levels of curiosity compared to being Expert (p = .02). Specifically, when faced with graph-based designs, Experts showed statistically significantly higher levels of curiosity than Insiders (p = .041). Similarly, when faced with text-based designs, Experts were significantly more curious than Insiders (p = .032) . However, we did not find any statistically significant difference in curiosity levels between non-Experts and other users. Furthermore, we did not find any statistically significant difference in ratings between participants when using SHAP-based design."}, {"title": "4.2.4 No effect of explanation design or expertise level on subjective understanding.", "content": "We did not observe any statistically significant influence of explanation design on participants' subjective understanding (p = .104). Similarly, the expertise level of participants did not have any statistically significant influence on their subjective understanding (p = .185) ."}, {"title": "4.2.5 Graph-based design is preferred, regardless of the expertise level.", "content": "At the end of the questionnaire, participants were asked to rank explanation designs according to their preferences. A Chi-Square Goodness of Fit Test was performed to determine whether the design preferences were equally distributed across the 6 possible outcomes. Our results revealed that the obtained proportions significantly differed (p = .015). We display in Figure 4 the number of occurrences of each design at each ranking position. We show that participants preferred graph-based design more frequently than other designs. When graph-based design was not ranked in first position, it was ranked in second position more often than in third position. The second best-ranked design was text-based explanation. SHAP-based explanation was ranked in last position more than 35 times. Finally, we explored the relationship between participants' expertise level and design preference. We ran a Chi-Square test and did not find any statistically significant evidence of correlation between the two variables (p = .841)."}, {"title": "5 DISCUSSION", "content": ""}, {"title": "5.1 Graph vs. textual explanations", "content": "According to Miller [31], a \"good\" explanation should be contrastive, selected, dialogic and causal. We argued that graphs inherently model causal and contrastive relationships between entities through their node-link structure. They achieve the selected criterion by potentially representing a subset of the original data, e.g. in the vicinity of a recommender system's prediction. Finally, the node or link attributes they may possess contribute to them being considered dialogic. Consequently, we aligned with previous studies that acknowledged the strong expressive power of graphs and the fact that they were especially well-suited for applications like recommender systems [1].\nInitial findings from our qualitative study affirmed these statements and emphasized the potential effectiveness of graph-based explanations. With the exception of one Expert who preferred textual explanations, participants found graph-based design \"very easily interpretable\" and highlighted the importance of showcasing \"relations between groups\". This was further confirmed quantitatively; when asked for their preferred explanation type, the majority of participants in our second study favored graph-based explanation over text or SHAP designs (see Figure 4). Specifically, users expressed a preference for item-centric graph explanations over user-centric ones, aligning with results from Kouki et al.'s work [24, 25], which compared textual and visual explanations, although not specifically graphs.\nHowever, despite their expressed preferences for graph-based design, our second study revealed that participants had significantly higher levels of objective understanding and usability when using textual designs. Such contrast between expressed desiderata and measured performance aligns with previous research [5, 48] where authors emphasized users' preference for visual explanations (though not specifically graphs) over texts, despite their poorer performance when using the former kind. A plausible explanation for this tendency may be grounded in the findings of a recent perception study [33] in which authors showed that interest and attractiveness of a stimuli can be well predicted by the complexity of this stimuli; in our case, the visual complexity of our graph explanation, compared to textual explanation could encourage participants to prefer this design over others. More recent research [7] explored this direction in the context of graph designs and further confirmed the close relationship between complex visual structure and interest.\nOverall, our findings suggest that depending solely on stakeholders' expressed desiderata for crafting graph-based explanation designs may not be sufficient. Considering both their preferences and measured performance when using such explanations could"}, {"title": "5.2 Impact of expertise level", "content": "Despite its enhanced design tailored to the needs of users with various levels of expertise, our graph explanation did not lead to significantly higher ratings on the different measures. Nevertheless, noteworthy insights can be derived from this visual design. For example, we showed that high expertise level positively influenced participants' objective understanding when facing textual explanation vs. graph explanation, but we did not reveal any difference in ratings for non-Experts. This contrasts with the findings in [48] showing how lay users performed better with textual explanations vs. visual explanations. While authors evoke the plausible effects of confirmation bias, i.e. favoring elements confirming preconceived ideas, on lay users to explain their results, we did not notice such behavior in our qualitative study. An interpretation of our results could be that Experts may be more inclined than non-Experts to have preconceived notions, or false narratives about the internal functioning of the system, a behavior that has been particularly observed for experienced users by previous works [22, 51]. These a priori may be more contradicted by complex designs, such as SHAP or graph, compared to relatively vague explanations like the textual design we proposed.\nWe also demonstrated higher curiosity levels for Experts compared to Insiders, when using graph and text designs, which aligns with previous research [35] stating that curiosity is strongly associated with intrinsic interest and motivation. We hypothesize that Expert participants are more engaged in the proposed tasks due to their interest in the topic. However, this tendency is not observed with SHAP-based explanation, warranting further analysis for comprehensive understanding.\nOur analysis finally revealed that both Insiders and Experts considered text-based design more usable than SHAP-based design, but only the most experienced participants found text-based explanations more usable than graph-based ones. This could be attributed to users with moderate expertise being more persuadable by graphical design and experiencing some kind of over-trust [11]. Or it may be the result of Experts being more inclined to over-analyze graph explanations to align with their knowledge of recommender systems and network layouts. Additionally, we did not find any statistically significant difference between levels of expertise when measuring graph usability. This observation aligns with previous research in the biological field [46], showing that novice users could create and evaluate graph layouts as effectively as domain experts.\nIn summary, we highlighted how graph-based explanations were subject to different cognitive biases according to the epxertise level of the end-users. To address this limit, future work could involve further inclusion of cognitive factors [45] in explanations to further enhance perceived designs."}, {"title": "5.3 Limitations", "content": "In our quantitative study, we compelled participants to choose from pre-defined user profiles based on a selection of representative books. This action further determined the algorithm recommendation and consequently the content of each explanation. While this approach allows us to use pre-computed answers, which significantly simplifies the questionnaire procedure, we acknowledge that it might hinder participants from fully recognizing their tastes within the recommender system's choices, potentially influencing their design preferences. To address this concern, we verified that the user profile chosen by participants was not correlated with their design preferences. For this purpose, we ran a Chi-Square test between the two variables and the results revealed that there is not enough evidence to suggest an association between the selected user profile and design preferences (p = .549).\nIn this work, we did not impose any time constraint on participants during their analysis of design, and the only time indication provided was that the questionnaire should take around 10 minutes to complete. Consequently, participants had sufficient time to explore and analyze each recommendation, which may not entirely replicate a real-world scenario where end users require an explanation to make a decision. Future research aimed at assessing the generalizability of our findings could involve time-constrained tasks tailored for specific contexts.\nMore generally, the results presented in this work are derived from data related to book recommendations. This simplistic use case facilitated the building of the study since it mitigated potential ethical concerns. While our results may have applicability in other domains, additional experiments would be necessary to confirm this."}, {"title": "6 CONCLUSION", "content": "In this work, we have conducted a qualitative study to understand the needs for graph-based explanation designs expressed by users with various levels of expertise. From these results, we built an enhanced graph-based design emphasizing several key recommendations such as a content-based approach obtained through bipartite graph projection or the focus on item similarity rather than user-similarity. We then conducted a quantitative study in which we investigated the influence of SHAP, textual and graph-based explanation designs regarding four constructs: objective understanding, subjective understanding, curiosity and usability. Our results revealed that text-based explanations significantly improved objective understanding to graph-based explanation, and that it was specifically marked for users with a higher level of expertise. On the other hand, we did not find any statistically significant evidence of influence of explanation design on subjective understanding. We also showed that both graph and text explanations were considered more usable than SHAP-based design, and that it was particularly true for users with middle to high level of expertise. Moreover, we highlighted the influence of expertise level on curiosity by showing how expert users were more curious than insiders toward graph and textual explanations. Lastly, we emphasized the discrepancy between participants' expressed preferences for graph-based explanations during qualitative interviews and further confirmed quantitatively, and their higher ratings regarding understanding or usability for textual designs. This outcome suggests that solely fulfilling stakeholders' desiderata may not suffice to achieve \"good\" explanations. Crafting hybrid designs achieving a balance between social expectation and effective downstream performance emerges as a significant challenge."}, {"title": "7 ETHICAL CONCERNS", "content": "To ensure participants' privacy, the qualitative study was designed and approved in collaboration with the Data Protection Officer (DPO) of the authors' affiliated school. Anonymized versions of interview notes for data analysis are securely stored, with access restricted to the authors. We did not collected any personal information for the quantitative study."}]}