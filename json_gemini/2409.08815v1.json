{"title": "Deep reinforcement learning for tracking a moving target in jellyfish-like swimming", "authors": ["Yihao Chen", "Yue Yang"], "abstract": "We develop a deep reinforcement learning method for training a jellyfish-like swimmer to effectively track a moving target in a two-dimensional flow. This swimmer is a flexible object equipped with a muscle model based on torsional springs. We employ a deep Q-network (DQN) that takes the swimmer's geometry and dynamic parameters as inputs, and outputs actions which are the forces applied to the swimmer. In particular, we introduce an action regulation to mitigate the interference from complex fluid-structure interactions. The goal of these actions is to navigate the swimmer to a target point in the shortest possible time. In the DQN training, the data on the swimmer's motions are obtained from simulations conducted using the immersed boundary method. During tracking a moving target, there is an inherent delay between the application of forces and the corresponding response of the swimmer's body due to hydrodynamic interactions between the shedding vortices and the swimmer's own locomotion. Our tests demonstrate that the swimmer, with the DQN agent and action regulation, is able to dynamically adjust its course based on its instantaneous state. This work extends the application scope of machine learning in controlling flexible objects within fluid environments.", "sections": [{"title": "1. Introduction", "content": "Machine learning for fluid mechanics has garnered considerable attention in recent years (Brunton et al. 2020; Karniadakis et al. 2021). Reinforcement learning, one of the machine learning methods, employs an agent that learns to interact with its environment by taking actions and learning from the rewards it receives, with the aim of maximizing the cumulative reward (Sutton & Barto 2018). It has been successfully applied in fluid mechanics, such as swimming strategy (Qiu et al. 2020), tracking (Mirzakhanloo et al. 2020), schooling strategy (Gazzola et al. 2016), and turbulence modeling (Bae & Koumoutsakos 2022).\nThe classic reinforcement learning uses the iteration method, dynamic programming, Monte-Carlo method and tabular method, whereas the deep reinforcement learning (DRL) uses a deep neural network (Garnier et al. 2021). The DRL is suitable for high-dimensional or non-linear tasks with very large number of states, as it avoids modeling complex system and exploits the feature extraction capabilities of deep neural network. It has been successfully applied to various control tasks such as walking (Haarnoja et al. 2019; Su & Gutierrez-Farewik 2023; Itahashi et al. 2024), flying (Reddy et al. 2016; Becker-Ehmck et al. 2020;"}, {"title": "2. Deep reinforcement learning", "content": "The overall workflow is sketched in figure 1. We first run multiple simulations to collect data of jellyfish-like swimming (figure 1a). The dataset is then used to train the neural network offline (figure 1d). The network is tested in various tracking tasks (figure 1f). Supplementary movies 1 and 2 illustrate how the swimmer tracks a moving target. Moreover, figures 1b,c,e show the jellyfish's state, action space and decision making process."}, {"title": "2.1. Data preparation", "content": "The 2D flow data for the deep reinforcement learning of jellyfish-like swimming are obtained from numerical simulations. The immersed boundary method (Peskin 2002; Tong et al. 2021) is used to treat the fluid-solid coupling at the moving boundary. A unit density, incompressible flow is governed by the Navier-Stokes equations\n\n$\\frac{\\partial u}{\\partial t} + u \\cdot \\nabla u = -\\nabla p + \\nu \\nabla^2 u + f, $  (2.1)\n\n$\\nabla \\cdot u = 0,$  (2.2)\n\nwhere $u, p, \\nu$ and $f$ denote the velocity, pressure, kinematic viscosity and body force exerted by a jellyfish-like swimmer.\nThe immersed boundary is represented by Lagrangian markers. A regularised delta function $\\delta_h$ (Peskin 2002) is employed to interpolate and spread $f$ between Eulerian and Lagrangian points, whose coordinates are denoted by $x$ and $X$, respectively. The Eulerian force in (2.1) is calculated as\n\n$f(x) = \\int_S F(X) \\delta_h(x \u2013 X) dX,$  (2.3)\n\nwhere $F$ denotes the Lagrangian force at $X$, and $S$ is the domain of the immersed boundary. The non-slip condition is satisfied by exerting $F$ on the immersed boundary. The velocity on the immersed boundary satisfies\n\n$\\int_D u(x) \\delta_h(x \u2013 X) dx = U_\\flat(X),$ (2.4)\n\nwhere $D$ denotes the entire fluid domain and $U_\\flat$ the velocity at Lagrangian points.\nThe simulations were conducted using the code IBAMR (Griffith et al. 2007), which is a distributed-memory parallel implementation of the immersed boundary method with adaptive mesh refinement for the Cartesian grid."}, {"title": "2.2. Deep Q-learning", "content": "A reinforcement learning system consists of two parts \u2013 the agent and the environment. The agent observes the status of environment and takes an optimal action based on this observation. The environment then changes and the agent receives a reward by evaluating the action. These steps loop until the end of a learning process.\nThe deep Q-learning (Mnih et al. 2015), a reinforcement learning method, uses the deep Q-network (DQN) to generate the optimal action. This neural network receives state $s_t$ at time $t$ as input, and outputs a value $Q(s_t, A_i)$ for each action $A_i$, representing the highest total reward for the agent after taking action $A_i$. This Q-value satisfies the Bellman optimality equation (Sutton & Barto 2018)\n\n$Q(s_t, A_i) = r_t + \\max_{A_j} Q(s_{t+\\Delta t}, A_j).$ (2.5)\n\nNamely, given state $s_t$ and action $A_i$, the maximum total reward at $t$ equals reward $r_t$ after taking this action plus the maximum total reward at the next state $s_{t+\\Delta t}$ among all possible actions $A_j$, where $\\Delta t$ denotes the time interval. For clarity, we define $t^* = t/\\Delta t$, along with current state $s_{t^*}$ and next state $s_{t^*+1}$. Note that the number of available actions is finite in the DQN. There are several difficulties in a reinforcement learning task \u2013 proper forms of state vector and reward function, overall convergence for a Bellman optimality equation, and gap between the convergence and actual performance of the agent."}, {"title": "2.3. Case setup and action regulation", "content": "In our 2D flow simulation, we adopt the bell-like geometry shown in figure 1b for jellyfish-like swimming. The outflow boundary condition is applied to all boundaries of $\\Omega$. The domain size is sufficiently large so that the boundary conditions do not influence the simulation results.\nIn the jellyfish-like locomotion, a pair of sinusoidal forces is applied to the swimmer's tips (marked in red), as illustrated in figure 1b. The force density (force per unit length)\n\n$F_i = \\hat{F}_i \\sin(2\\pi f t) \\tau_i, \\; i=1,2,$ (2.6)\n\nacting on the left and right halves of the swimmer are denoted as $F_1$ and $F_2$, respectively, with $\\hat{F}_1 = |F_i|$ and the unit tangent vector $\\tau_i$ on $S$. As sketched in figure 1c, $F_1$ and $F_2$ can be different, while they have the same frequency $f$ and zero initial phase. Other setup details are similar to those in Hoover & Miller (2015) except that the model for the force density due to the elastic deformation of the swimmer is improved, which is detailed in Appendix A. The simulation parameters are listed in table 1. All parameters are non-dimensionalized by the reference length $l$ = 1 m and reference time 1 s. The diameter $d_0$ of the jellyfish-like swimmer is defined as the distance between the left and right ends of the swimmer at rest.\nAs listed in table 2, we divide a period $t$ = 0 ~ $T$ of the sinusoidal force pair ($F_1$, $F_2$) applied to the swimmer into four quarters, with $T$ = 1/$f$. The force pair is non-dimensionalized by the reference force density 1 N/m. During each quarter, a DQN is employed to process the current state and produce ($F_1$, $F_2$). In order to reduce the training complexity, we restrict the choices for ($F_1$, $F_2$) to (0.003, 0.003), (0.001, 0.003), (0.003, 0.001), and (0,0). These choices are labeled as actions $A_i$ with $i$ = 0, 1, 2, and 3, as listed in table 2 and illustrated in figure 1c. To mimic the jellyfish locomotion and achieve high propulsion efficiency and maneuverability, forces are applied only during the first and third quarters of each period, except for A0 and A3 (with symmetric force magnitudes) which can last for the next quarter following specific actions. The actions are summarised in table 2. This action regulation is similar to the burst-and-coast strategy used in optimal intermittent swimming of fish (Li et al. 2021) and jellyfish-like swimming (Kang et al. 2023).\nTo evaluate the efficiency of model performance, we use the total time taken for the swimmer to complete a task. The task is considered ended when the swimmer reaches the target or the swimmer gets too close to the boundary of the simulation domain."}, {"title": "2.4. Training", "content": "As sketched in figure 1b, the swimmer's state is characterized by a 12-dimensional vector\n\n$s = (x_1, y_1, x_2, y_2, x_3, y_3, u_1, u_2, d, \\theta, \\Omega, \\eta),$ (2.7)\n\nwhere $(x_i, y_i), i = 1,2,3$ denote the coordinates of the swimmer's left, middle, and right endpoints referenced to its mass centre, $u_r = (u_1, u_2)$ the velocity of the mass centre relative to target, $d$ the distance between the mass centre and target, $\\theta$ the angle between the swimmer's symmetric axis and the line segment connecting the mass centre and target, $\\Omega$ the angular velocity of the symmetric axis, and $\\eta = 0, 1, 2, 3$ indicates the quarter of period. Note that the symmetric axis is defined as the line connecting the midpoint of the swimmer and its mass centre, and it does not necessarily imply that the swimmer geometry is symmetric all the time. The state vector in (2.7) encapsulates the information perceived by a swimming jellyfish when it chooses to navigate to a specific location within its natural environment, and the choice of state variables is important for the further training and tracking performance.\nWe choose a function $r(s, a) = -min(\\theta^2, 3) + A\\; clip(v, -0.1, 0.1) \u2013 Bd$ to characterize the reward received at state $s$ after taking action $a$, where $v$ is the projection of mass centre's velocity onto the line segment connecting the mass centre and target point, and constants $A$ and $B$ are tuned during the training; the clip function $clip(x, b, c)$ returns $x$ if $b \\leq x \\leq c$, $b$ if $x < b$, and $c$ if $x > c$. The DQN agent receives state and estimates $Q^*(s, a)$. The loss function is defined by\n\n$loss = (Q^* (s, a) \u2013 (r(s, a) + (1 \u2013 D) \\max_{A_j} Q^* (s', A_j)))^2,$ (2.8)\n\nwhere $D$ = 1 if the task is ended and 0 otherwise, $s'$ denotes the next state and its subscripts for time is omitted for clarity. This loss function optimizes the DQN parameters to satisfy the Bellman optimality equation in (2.5) with a slight modification. The term (1 \u2013 D) means that the Q-value at the penultimate state is equivalent to the reward obtained. More details on reinforcement learning and training are provided in Appendix B."}, {"title": "3. Tracking fixed target", "content": "We first examine the DQN performance in tracking a fixed target. In this case, the swimmer can adjust its movement direction gradually towards the target. As shown in figures 2a-c, we set three target points. The swimmer trajectories suggest that the DQN has learned how to make turns using the combination of symmetric and asymmetric actions.\nFigures 2d and e illustrate how the swimmer moves towards the target point, where the symmetric and asymmetric actions are marked in red and blue, respectively, on the swimmer trajectory. The first action with asymmetric forces causes the swimmer to turn right by slightly rotating its body, forming a pair of asymmetric wake vortices. Under the action regulation (see table 2), the subsequent actions with symmetric forces make the swimmer move straight to the target with a small angular velocity $\\Omega$ and a decreasing $\\theta$ (see figure 2f), and then make the orientation of its symmetric axis turns slowly to align with its velocity, with the decrease of $\\alpha$ (see figure 1b). The decreasing of both $\\theta$ and $\\alpha$ is ideal for tracking a fixed target, because the swimmer can adjust to proper orientation and then swim forward. This adjustment is critical, which will be shown in tracking of a moving target."}, {"title": "3.2. Effect of action regulation", "content": "The detailed process of vorticity formation during the right turn of a swimmer is illustrated in figure 2d, with the sequence of actions $A_1$, $A_3$, $A_0$ and $A_0$ in a period from $t$ = 0 to $T$. First, the force exerted on the right side is larger than that on the left in action $A_1$, generating a stronger vortex at the right trailing edge. This vortex then induces a leftward rearward velocity on the right half of the swimmer, effectively dragging the entire body towards the right.\nThe turning process depicted in figure 2d emphasizes the importance of action regulation. By applying forces only in the first and third quarters of each cycle, the swimmer can execute a swift turn within a single period, and the vortices generated in the two quarters have enough time to shed off from the trailing edges within the second and fourth quarters. Experimental studies showed that the starting vortex at the trailing edge for a impulsively started wing rapidly escapes to the wake within $t/T$ = 0.167 (Huang et al. 2001). This motion is similar to the flapping by the muscle of the jellyfish-like swimmer. Therefore, the time of $T/4$ is enough for these vortices to shed off, with minor influence on lateral motion of the swimmer.\nBy contrast, frequent exertion of forces can make the swimmer difficult to control. Without the action regulation, the asymmetric forces can be applied in consecutive quarters as the action sequence $A_1$, $A_1$, $A_0$ and $A_0$ in figure 3. As a result, the vortices generated in the consecutive quarters can interference with each other. In contrast to figure 2d, where the swimmer's geometry becomes nearly symmetric, the vortex (red patch highlighted in the blue circle at $t/T$ = 0.4) formed by the right side of the swimmer stays at the trailing edge when the next symmetric action is executed. This interaction between the previously formed vortex and the newly generated vortex leads to an asymmetric shape of the swimmer. The neighboring vortices (red and blue patches highlighted in the blue circle at $t/T$ = 0.7, 0.8 and 1) with opposite signs cancel out each other, which significantly reduces thrust and makes the motion difficult to control."}, {"title": "4. Tracking moving target", "content": "We then examine the DQN agent performance in tracking a moving target. In our training strategy, tracking a moving target is equivalent to tracking a fixed point at each individual time step, so the trained DQN agent is expected to be applicable.\nThe target trajectory (green curve) in figure 4a is part of a circle centered at (0.8, 0.8) with a radius of 0.4. The target point starts from (1.2,0.8) and moves counterclockwise with an angular velocity of $\\pi/120$ rad/s or 1.5\u00b0/s, which is a speed of 0.0105 in figure 4a.\nThe swimmer effectively tracks the moving target, indicating that the trained neural network is capable of adapting to dynamic scenarios. Figure 4a and supplementary movie 1 illustrate how the swimmer track a moving target in detail, where the red trajectory denotes the action with forces and the blue one denotes the action without force. Initially, the target is on the front right of the swimmer. In phase 1, the swimmer turns right and drifts for a certain period (marked by the blue trajectory) to align its symmetric axis direction with the velocity direction. It then swims straight toward the target (marked by the red trajectory). In phase 2, as the target moves to left, the swimmer chooses to drift again (marked by the blue trajectory), allowing it to slow down and adjust its symmetric axis direction towards the moving target. In phase 3, the swimmer at low speed already turns its symmetric axis toward the target and starts to move forward, and then makes further adjustments to reduce $\\theta$.\nThese observations suggest that the swimmer has acquired the skill of employing drifting (action A3) as a strategy to adjust its direction, enabling it to efficiently follow the moving target. Moreover, the large radius of the circular target trajectory provides the swimmer with enough space and time to adjust its swimming direction. Figure 1e sketches the decision process at $t$ = 0. The DQN takes the state vector as the input and gives the Q-value for each action, and the action with the largest Q-value is chosen.\nFigure 4b plots the temporal evolution of $\\Omega$ (angular velocity) and $\\theta$ (angle). In general, $\\theta$ and $\\Omega$ exhibit opposite signs for the majority of the time, indicating that the swimmer is actively adjusting its direction to swim towards the target. The delay in the change of $\\Omega$ relative to $\\theta$ arises from the fact that once an action is performed, it does not instantaneously influence the swimmer's motion. The swimmer learns to drift, waiting for the previous action to take effect, and then adjust its direction. Initially, the swimmer employs asymmetric forces to generate an angular velocity, swiftly adjusting its direction towards the target in a short time. Then, the swimmer swims straight forward with minor changes in $\\theta$. When $\\Omega$ becomes large, the swimmer finds the deviation and responds by producing a counter $\\Omega$ to reduce $\\theta$."}, {"title": "4.2. Figure-eight target trajectory tracking", "content": "The trajectory of the target in \u00a74.1 is a circle with a relatively large radius, moving at a low speed. This provides the swimmer with sufficient time and space to adjust its velocity direction to align with its symmetric axis.\nFigure 5 and supplementary movie 2 present a more challenging tracking task \u2013 the target moves along a figure-eight trajectory, characterized by a smaller radius and higher speed than those in the previous case. Specifically, the radius of each circle is 0.3 and the angular velocity of the target that moves along it is $\\pi/45$ rad/s or 4\u00b0/s. Consequently, the target's speed is 0.021, which is twice the speed of the target in figure 4a.\nGiven the delay between the swimmer's motion and action, the swimmer's trajectory does not strictly align with that of the target. Instead it goes through a process of deviating and adjusting its direction. Figure 6a plots the evolution of $\\Omega$ and $\\theta$. Similar to figure 4b, $\\theta$ and $\\Omega$ exhibit opposite sign for majority of the time. The event for the jumps of $\\theta$ around $t/T$ = 45 is that the swimmer reaches and surpasses the target as shown in figure 6b. These behaviors indicate that the swimmer's actions are based solely on its current state, without prior knowledge of the target's motion. Note that the primary objective is to track the target, and the swimmer accomplishes this task for the first time at station 5 in figure 5. Then, the swimmer restarts to track the moving target.\nThe trajectory of the swimmer for tracking the first circle (stations 1\u20135) aligns relatively well with that of the target. After reaching the target at station 5, the two trajectories begin to show notable discrepancy. Following a period of questionable maneuvers, the swimmer decelerates and manages to reorient itself correctly and catch up at stations 6\u20138. Difficulties arise at stations 8\u20139, where the swimmer's velocity turns excessively, leading to further deviation. The issues in this complex target trajectory tracking is discussed in detail in Appendix C. Nonetheless, the swimmer ultimately adjusts and catches up at stations 9\u201310. Overall, the swimmer still performs satisfactory tracking by completing a rough figure-eight trajectory.\nWe then examine the actions output by the agent. Figure 7 shows the actions in terms of $\\theta$ and $\\Omega$. They are divided into two groups, $d < d_0$ and $d \\geq d_0$. As shown in figure 7a, as the swimmer maintains a substantial distance from the target, the agent exhibits a pronounced preference for certain actions. Action A0, with symmetric forcing, predominates for large portion of the time when $\\theta$ is small. This preference is particularly evident when both $\\theta$ and $\\Omega$ are small (highlighted by the dashed box in figure 7a), reflecting the agent prefers straightforward progression with negligible self-rotation under these conditions.\nAction A3 ranks second in the preference, and spans a broader spectrum of $\\theta$, corresponding to states characterized by varying velocities. This preference implies that the DQN learns a strategy of drifting that enables the swimmer to naturally decelerate or adjust its trajectory by ceasing to apply forces, especially when moving at high speeds in a misaligned direction. Conversely, A1 and A2 are predominantly selected when $\\theta$ has finite positive and negative values, respectively, indicating the swimmer recalibrates its symmetry axis towards the target.\nAs shown in figure 7b, when the swimmer closes in on the target, however, the action selection becomes less predictable. Within the vicinity defined by $d < d_0$, the swimmer either approaches immediate proximity to or overshoots the target. In this critical range, determining the optimal action becomes increasingly challenging, as any maneuver can precipitate substantial alterations in $\\theta$, compounded by the lack of information regarding the target's trajectory. The agent's decision-making process, therefore, becomes more stochastic.\nThese observations highlight the current challenges of the DQN in handling complex scenarios where precise adjustments and quick responses are required to track a continuously moving target. Further fine-tuning of the training process is necessary to overcome these challenges, such as using an extra network to predict the target's trajectory and adding extra action regulations."}, {"title": "5. Conclusions", "content": "We develop a deep reinforcement learning method for tracking a moving target in jellyfish-like swimming. This control strategy, with a DQN agent, is based on the instantaneous state of a 2D jellyfish-like swimmer. The swimmer is a flexible object with a muscle model of torsional spring that has no torque introduced when deformed. We apply a pair of sinusoidal forces to the muscle part of the swimmer and adjust their magnitudes to control the swimmer's motion.\nThe data for the swimmer motion in fluid are obtained from the simulation using the immersed boundary method, and they are utilized to train and validate the DQN agent. To make swimming more natural and reduce the difficulty of training, we introduce an action regulation. This regulation mitigates the cancellation of wake vortices generated by the swimmer's flapping motion by suspending the application of forces during specific time periods.\nEquipped with the DQN agent and action regulation, the swimmer demonstrates the capability in tracking both fixed and moving targets. The action regulation reduces the influence from the flow induced by previous actions, enabling the swimmer to output action to control its movement only based on its current state.\nIn the basic test for tracking a fixed target, the swimmer with the DQN agent performs much more efficient than that with the baseline strategy outlined in Appendix B. In the challenging test for tracking a moving target, there is the inherent latency between the action for exerting forces and the response for rotating the swimmer's body, due to the hydrodynamic interactions between the shedding vortices and the swimmer's own locomotion. The swimmer is still able to dynamically adjust its course based on its instantaneous state. This resilience highlights the DQN agent's robust decision-making capabilities, enabling it to counteract external perturbations and maintain a focused pursuit trajectory.\nWhen confronted with more intricate target trajectories, such as the figure-eight curve, the agent's performance may exhibit certain limitations. This observation implies the complex interplay of factors that govern natural navigation strategies, particularly those employed by organisms in their native environments. For example, the jellyfish leverages a multifaceted array of sensory inputs, encompassing both internal state cues (e.g. velocity) and external environmental indicators (e.g. fluid dynamics) to navigate precisely towards designated locations.\nThe present study advances the capabilities of a 2D jellyfish-like swimmer beyond forward swimming, explores its control strategy, and broadens the application of reinforcement learning in fluid dynamics. Given that the swimmer's motion involves significant fluid-structure interaction, the agent may require additional information to take optimal actions. To enhance the tracking performance, the swimmer could leverage more information from its surrounding fluid flow, predict the target's position, and incorporate data from previous states. Besides, more complex network architectures, such as RNN (Rumelhart et al. 1986) and transformer (Vaswani et al. 2017), could be employed to capture more features and temporal dependencies from past and current states to achieve higher level of control strategy."}, {"title": "Appendix A. Elastic force model for the swimmer muscle", "content": "We present the beam force model for mimicking the muscle of a jellyfish-like swimmer. The model swimmer consists of 159 Lagrangian points. Each point is connected to its neighboring ones with linear springs with large stiffness to prevent significant displacement among the Lagrangian points. Every set of three neighboring points is connected with a beam that generates a restoring force on those three points when bent, in order to maintain the curvature of the structure.\nThe structure of the swimmer is sketched in figure 8. The springs and beams together constitute the muscle and geometry of our model swimmer. For the purpose of the present study, the beam needs to resist deformation such as bending, and does not resist translation and rotation so that the whole swimmer can turn without influence from potentially non-zero internal torque.\nThe elastic force density in Hoover & Miller (2015) is chosen as\n\n$F_E(q,t) = \\left(k_s \\frac{\\partial}{\\partial q} X(q,t) \\frac{\\partial}{\\partial q} X(q,t) - 1\\right) \\frac{\\partial}{\\partial q} \\tau(q,t) - k_b \\frac{\\partial^4}{\\partial q^4} (X(q,t) - X_b(q)),$ (A1)\n\nwhere $X(q,t)$ is the Cartesian coordinate of the Lagrangian material point $q$, $\\tau(q,t) = \\frac{\\partial X/\\partial q}{|\\partial X/\\partial q|}$ is the unit tangent vector, $k_b \\geq 0$ and $k_s \\geq 0$ are respectively the bending and stretching stiffnesses of the muscle, and $X_b$ characterizes the shape of the swimmer's body at rest. On the right-hand side (RHS) of (A1), the first term represents a linear spring connecting neighboring Lagrangian points, and the second term represents a linear beam applied on three neighboring points (see figure 8).\nIn the present study, the fiber model for the second term is changed from the linear beam to a torsional spring (Battista et al. 2017) to incorporate rotation, so that the muscle is more flexible than that in Hoover & Miller (2015). When the entire torsional spring rotates around an arbitrary point, the force acting on it rotates accordingly, while the torque applied to it remains zero. CFD simulations showed that this enhancement to the muscle model is pivotal for effectively controlling the turning of the jellyfish-like swimmer.\nThe forces in the torsional spring model are\n\n$F_L = k_b C (X_M \u2013 X_R) \\times e_z,$ (A2)\n\n$F_M = k_b C (X_R - X_L) \\times e_z,$ (A3)\n\n$F_R = k_b C (X_L \u2013 X_M) \\times e_z,$ (A4)\n\nwhere $e_z = e_x \\times e_y$ is the unit vector perpendicular to the x-y plane, $X_L, X_M$ and $X_R$ the displacement vector, $F_L, F_M$ and $F_R$ the Lagrangian forces of its left, middle and right points, respectively; $C = e_z \\cdot ((X_R \u2013 X_M) \\times (X_M \u2013 X_L) \u2013 (X_{R0} \u2013 X_{M0}) \\times (X_{M0} \u2013 X_{L0}))$ and $X_{L0}, X_{M0}$ and $X_{R0}$ the corresponding displacement vector at the initial time when the swimmer is at rest.\nThe torque referenced to the mass centre $X_c = (X_L + X_M + X_R)/3$, or any arbitrary point, is\n\n$(X_L \u2013 X_C) \\times F_L + (X_M \u2013 X_c) \\times F_M + (X_R \u2013 X_C) \\times F_R $\n$= X_L \\times F_L + X_M \\times F_M + X_R \\times F_R.$ (A5)\n\nSubstituting (A2)-(A 4) into (A 5) yields the vanishing torque. This torsional spring model refrains from imparting extraneous torques, notably anti-rotational forces, onto the swimmer's flexible structure. Therefore, it is a suitable model for the present study."}, {"title": "Appendix B. Details on reinforcement learning and training", "content": "The tracking task is formulated as a sequential decision problem solved using reinforcement learning. The decision-making is modeled as a Markov decision process where the agent makes decision solely based on the current state (Sutton & Barto 2018).\nThe trajectory of the swimmer can be written as\n\n$\\Gamma_t = (s_0, a_0, r_0, ..., s_{t^* -1}, a_{t^* -1}, r_{t^* -1}, s_{t^*}, a_{t^*}, r_{t^*}),$ (B 1)\n\nwhere $s_{t^*}, a_{t^*}$, and $r_{t^*}$ denote the state, the action taken by the agent, and the reward received at a given time $t = t^*\\Delta t$, respectively. In the present study, $\\Delta t = T/4$, corresponding to the time interval for force application. The agent takes action $a_0$ at $t$ = 0, and then it receives reward $r_0$ and its state changes from $s_0$ to $s_1$. The same process continues until time $t$.\nThe action $a_{t^*}$ taken by the agent is determined solely by the current state $s_{t^*}$ and the agent's policy $\\pi(\\Theta,\\cdot)$, which is implemented as a neural network with parameters $\\Theta$. The reward function, $r_{t^*} = r_{t^*} (s_{t^*}, a_{t^*})$, depends on the current state and action. The goal for our DQN agent is to maximize the expected discounted sum of rewards over time by adjusting $\\Theta$ (Sutton & Barto 2018). This can be expressed as an optimization problem\n\n$\\max_{\\pi} E_{s_0, a_0, s_1, a_1,...} \\sum_{t^*=0}^{\\infty} \\gamma^{t^*} r(s_{t^*}, a_{t^*}),$ (B2)\n\nwhere $E_{s_0, a_0, s_1, a_1,...}$ denotes the expected value taken over the trajectory of states and actions ($s_0, a_0, s_1, a_1, ...$) that the agent encounters. The action $a_{t^*}$ is selected according to policy $\\pi$, and $\\gamma \\in (0, 1"}]}