{"title": "See then Tell: Enhancing Key Information Extraction with Vision Grounding", "authors": ["Shuhang Liu", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Jun Du", "Qing Wang", "Jianshu Zhang", "Chenyu Liu"], "abstract": "In the digital era, the ability to understand visually rich documents integrating text, complex layouts, and imagery is critical. Traditional Key Information Extraction (KIE) methods primarily rely on Optical Character Recognition (OCR), which often introduces significant latency, computational overhead, and errors. Current advanced image-to-text approaches, which bypass OCR, typically yield plain text outputs without corresponding vision grounding. In this paper, we introduce STNet (See then Tell Net), a novel end-to-end model designed to deliver precise answers and relevant vision grounding. Distinctively, STNet utilizes a unique <see> token to observe pertinent image areas, aided by a decoder that interprets physical coordinates linked to this token. Positioned at the outset of the answer text, the <see> token allows the model to first see observing the regions of the image related to the input question - and then tell - providing articulated textual responses. To enhance the model's see capabilities, we collect extensive structured table recognition datasets. Then leveraging the advanced text processing prowess of GPT-4, we develop the TVG (TableQA with Vision Grounding) dataset, which not only provides text-based Question Answering (QA) pairs but additionally incorporates precise vision grounding of these pairs. Our approach demonstrates substantial advancements in KIE performance, achieving state-of-the-art results on publicly available datasets such as CORD, SROIE, and DocVQA. The code will also be made public available.", "sections": [{"title": "Introduction", "content": "Visually rich document is a type of medium that centers around text while also incorporating the layout and related visual imagery. In the digital information age, many documents are digitized and saved as images. Understanding document images plays a pivotal role across multiple domains such as document analysis (Cui et al. 2021), document retrieval (Mass and Roitman 2020), and office robotic process automation (Axmann and Harmoko 2020). This comprehension significantly bolsters the efficiency and accuracy of information processing. Key Information Extraction (KIE) aims to locate, analyze, and extract key entities (like names, dates, and numerical data) from documents. KIE has become a key part of the document understanding field.\nAs illustrated in Figure 1, existing methods for KIE can be broadly categorized into two groups. Traditional methods (Xu et al. 2020, 2021; Huang et al. 2022; Zhang et al. 2023b; Appalaraju et al. 2021) rely on Optical Character Recognition (OCR) engines to first extract text and coordinate information from document images. These OCR elements are then fed into a document model for analysis and classification. However, such OCR-based methods are heavily dependent on the OCR engine, resulting in additional latency and computational costs. Furthermore, errors originating from the OCR step can propagate to the document model, thereby deteriorating overall performance. Recent advancements in document understanding (Kim et al. 2022; Cao et al. 2023; Okamoto et al. 2024) have introduced end-to-end image-to-text paradigms. These methods enable document models to directly process document images, bypassing the need for explicit OCR engine. To achieve this, they leverage the Transformer architecture (Vaswani et al. 2017) to decode OCR results during the pre-training stage, endowing the document model with reading capabilities. These OCR-free methods demonstrate remarkable adaptability in various document understanding tasks, highlighting their powerful capabilities in understanding text embedded within images. Nevertheless, the KIE task differs from typical Visual Question Answering (VQA) tasks (Singh et al. 2019; Lu et al. 2022) due to the strong correspondence required between the extracted information and the visual content of the document. Therefore, it is essential to design a module that aligns the predicted plain text answers with the visual content present in the images.\nCurrently, most document KIE datasets, such as DocVQA (Mathew, Karatzas, and Jawahar 2021) and WikiTableQuestions (Pasupat and Liang 2015), predominantly offer simple plain text Question Answering (QA) pair annotations without vision grounding for each answer within the image context. With the rise of large language models (LLMs) like ChatGPT (Brown et al. 2020) and GPT-4 (OpenAI 2023), there has been a notable shift in research focus towards leveraging these LLMs for generating domain-specific instruction fine-tuning data, as evidenced by recent studies (Han et al. 2023; Zhang et al. 2023a). Motivated by these advancements, we present an automated processing pipeline that leverages GPT-4 to generate KIE data with robust vision grounding, specifically tailored for the document domain. This enhancement is expected to significantly improve the performance of KIE models by providing precise and dependable vision grounding.\nIn this work, we introduce a novel end-to-end model named STNet (See then Tell Net), which can simultaneously provide answers and corresponding vision grounding. Unlike existing methods, we explicitly design a <see> token to guide the model in identifying the relevant location within the image. Accordingly, we develop a specialized physical decoder to interpret the physical coordinate associated with the <see> token. In downstream tasks, we simply place the <see> token at the beginning of the answer text, thereby providing vision grounding for the answer. To further enhance the model's see capabilities, we collect a substantial number of highly structured table recognition datasets, such as PubTables1M (Smock, Pesala, and Abraham 2022) and iFLYTAB (Zhang et al. 2024). Leveraging the powerful text understanding capabilities of GPT-4, we construct a TVG (TableQA with Vision Grounding) dataset. This dataset not only provides the related plain text QA pairs but also includes the specific vision grounding of the QA pairs within the image. We validate STNet on publicly available datasets such as CORD (Park et al. 2019), SROIE (Huang et al. 2021), and DocVQA (Mathew, Karatzas, and Jawahar 2021), achieving state-of-the-art results. The main contributions of this paper are as follows:"}, {"title": "Related Work", "content": "Early KIE algorithms used rule-based methods, heavily relying on prior knowledge. These approaches were limited to fixed-format documents and lacked robustness for diverse real-world applications. The rapid development of deep learning has brought superior solutions to document understanding, which can be primarily categorized into OCR-based and OCR-free methods."}, {"title": "OCR-based Methods", "content": "OCR-based methods require an OCR engine to extract text and coordinate information from visually rich document images, which then serve as input for subsequent document models. The LayoutLM family (Xu et al. 2020) introduces a pre-training framework that combines text and layout features, with LayoutLMv2 (Xu et al. 2021) enhancing representation capabilities through spatial-aware self-attention and tasks like text-image alignment and text-image matching. LayoutLMv3 (Huang et al. 2022) further advances this approach by reducing visual feature extraction costs with patch encoding and introducing masked image modeling and word-patch alignment tasks. Besides the LayoutLM family, DocFormer (Appalaraju et al. 2021) integrates visual and spatial information into each Transformer layer using a self-attention encoder. GraphDoc (Zhang et al. 2023b) employs BERT and Swin Transformer for semantic and visual encoding, respectively, with an attention-based graph network for localized feature interaction. Despite their commendable performance, these models exhibit significant reliance on OCR tools, which inevitably leads to error propagation due to inherent inaccuracies. Moreover, the cascade structure complicates the model architecture and increases computational costs, hindering efficiency."}, {"title": "OCR-free Methods", "content": "OCR-free methods aim to eliminate dependence on OCR recognition modules, offering faster reasoning speeds and requiring fewer parameters. For example, Donut (Kim et al. 2022) uses the Swin Transformer to encode image patches and BART-like Transformers to generate text sequences, introducing a prompt mechanism to switch between tasks. This end-to-end approach simplifies the model architecture and achieves cost-effectiveness by directly mapping input images into structured outputs. Furthermore, Pix2Struct (Lee et al. 2023) extends these improvements by enlarging the pre-training dataset and incorporating more"}, {"title": "Methodology", "content": "STNet is an end-to-end image-to-text model that not only provides textual answers but also excels in offering vision grounding. As illustrated in Figure 2, it leverages the architecture of Donut (Kim et al. 2022) and consists of two primary modules: a vision encoder and a text decoder. The vision encoder is tasked with processing image features which are subsequently interpreted by the text decoder to formulate the answer sequence A. Our model introduces a novel <see> token at the beginning of A. This inclusion facilitates the attachment of a physical decoder designed specifically to extract the physical coordinates from the <see> token, thereby providing robust vision grounding. To further enhance the model's capability to see, we accumulate a large amount of structured table recognition data and construct the TVG dataset utilizing GPT-4. More details are elaborated in subsequent sections."}, {"title": "Vision Encoder", "content": "The vision encoder transforms the input document image I into a feature map $F \\in \\mathbb{R}^{H \\times W \\times D}$. This feature map is subsequently serialized into a set of embeddings $Z = \\{z_i \\in$"}, {"title": "Text Decoder", "content": "Similar to Donut, we utilize the BART (Lewis et al. 2020) decoder to generate the answer sequence A, which is conditioned on the Z and prompted by the question sequence Q. Drawing from the mechanisms of LLMs (OpenAI 2023), STNet is trained to predict the next token in the sequence. Consequently, the training objective is to minimize the negative log-likelihood of the target sequence.\n$L_{im} = \\min \\frac{1}{T} \\sum_{i=1}^{T} -\\log P(a_i \\vert Z,Q, a_{1:i})$"}, {"title": "Physical Decoder", "content": "We explore the fundamental human cognitive process of \u201csee then tell\u201d, where individuals first see \u2013 gathering visual information and contextual insights \u2013 and then tell \u2013 constructing responses. This sequence notably enhances the accuracy and relevance of interactions. To effectively mirror this intuitive cognitive pattern, our proposed method adopts a two-phase output strategy. The <see> token initiates the see phase to perceive location information related to Q in the document image, followed by the tell phase that outputs the answer text. We design a physical decoder that decodes the hidden states $H = \\{h_i \\in \\mathbb{R}^{D} \\vert i = 1,...,T\\}$ extracted from the final layer of our text decoder, specifically corresponding to the <see> token, allowing us to obtain the polygon coordinates p within the image context for vision grounding. To facilitate this prediction, we employ a quantization strategy utilizing a specialized vocabulary composed of 1,000 unique tokens, ranging from <0> to <999>, collectively denoted as $Loc \\in \\mathbb{R}^{1000 \\times D}$. For each coordinate $p_j$ within a polygon $p_i$, its associated hidden state $h_i$ undergoes a linear transformation, producing has a query against the vocabulary Loc. The final determination of $p_j$'s position is computed based on the expected location derived from the probability distribution over Loc, as provided by $h_i^\\prime$, divergent from previous direct classification methods (Chen et al. 2022) over a location vocabulary:\n$h^\\prime = Linear (h_i)$\n$b_j = softmax (h^\\prime Loc^T)$\n$E(p_j) = \\sum_{i=0}^{999} i b_j^i$\nHere, $b_j \\in \\mathbb{R}^{1000}$ represents the probability distribution for the position of $p_j$. The polygon regression loss of see is defined as follows:\n$L_{see} = \\frac{1}{8} \\sum_{j=1}^8 (E(p_j) - p_j^\\prime)^2$\nwhere $p^\\prime$ represents the ground truth label."}, {"title": "TVG Dataset", "content": "Current document datasets, such as DocVQA (Mathew, Karatzas, and Jawahar 2021) and WikiTableQuestion (Pasupat and Liang 2015), only offer plain text QA pairs, constraining the enhancement of STNet's ability to see. Inspired by recent studies (Xu et al. 2023; Han et al. 2023; Zhang et al. 2023a) which leverage LLMs for dataset construction, we propose a comprehensive GPT-4-based method, as depicted in Figure 3, to automatically construct QA datasets for document images. Tables, as a special form of document image, can be described using structured languages such as HTML, and high-quality table data can be readily sourced from online resources. To this end, we have collected a number of table recognition datasets including PubTables1M (Smock, Pesala, and Abraham 2022) and iFLYTAB (Zhang et al. 2024), each sample containing both images and their corresponding structured description statements. We design specific prompt templates and use structured table statements as input to GPT-4, guiding it to generate corresponding QA pairs. For each QA pair, we also require GPT-4 to provide the logical location of the answer (i.e., the row and column position in the table). Through post-processing, we can map the logical structure to physical information (coordinates), thereby linking the answers to their respective locations in the images. Consequently, we construct the TVG dataset, which includes table images I and the QA pairs {Q, A, p} generated via GPT-4. This dataset significantly enhances our model's training efficacy. More details can be found in our Appendix."}, {"title": "Implementation Details", "content": "Our proposed STNet utilizes specific hyperparameters: We set the input image resolution to 1280 \u00d7 960 and use random padding to maintain the original aspect ratio. The visual backbone's downsampling factor is configured to 32. The feature dimension D is established at 1024. The decoders consist of a stack of 4 identical layers, and the number of multi-heads is set to 16."}, {"title": "Pre-training Tasks", "content": "To bolster STNet's capability to perceive text locations essentially, its ability to see we have integrated a multi-task pre-training strategy. This strategy encompasses three distinct sub-tasks: OCR, Document Read, and VQA, as illustrated in Figure 4.\nOCR. We represent positions using polygons defined by four coordinate points, with each point mapped to a token ranging from <0> to <999> in Loc. STNet is designed to output text relevant to these positions based on such prompts, thereby emulating the process of an OCR engine.\nDocument Read. This task improves the model's ability to understand document structures by training it to generate text sequences in the conventional reading order. Each text block in the sequence is prefaced by a <see> token. The physical coordinates of the text are obtained by decoding the hidden states using a specialized physical decoder.\nVQA. Expanding beyond conventional VQA tasks, STNet is designed to not only generate a plaintext response but also identify the relevant text coordinates using the <see> token for vision grounding. This approach aligns with the requirements of downstream KIE tasks."}, {"title": "Training Strategy", "content": "In the initial pre-training phase, in addition to the previously constructed TVG dataset and its data sources the training sets from PubTables1M (Smock, Pesala, and Abraham 2022) and iFLYTAB (Zhang et al. 2024) we also employ a synthetic dataset comprising 2.2 million entries in both Chinese and English from SynthDog (Kim et al. 2022). PubTables1M, iFLYTAB, and SynthDog are used for OCR and document read training, while TVG is utilized for OCR and VQA tasks.\nAfter pre-training, STNet is fine-tuned on specialized datasets for KIE. Each dataset is tailored to meet the VQA task specifications and is combined with TVG at a 1:1 ratio. This blending ensures that the model retains its ability to see. For SROIE (Huang et al. 2021), given the requirement to extract four specific elements, we have designed four distinct questions, each targeting one element. In the case of CORD (Park et al. 2019), which features an intricate nested structure, we use \u201cparse the receipt\u201d as a prompt, following the answer output format pioneered by Donut, and prepend a <see> token to each answer text. For DocVQA (Mathew, Karatzas, and Jawahar 2021), the QA pairs are directly used as provided in the dataset annotations.\nWe use the Adam optimizer (Kingma and Ba 2015) with a learning rate of 5 \u00d7 10-5. The learning rate is linearly warmed up during the first 10% of the steps and then linearly decayed. The training is conducted on 4 Tesla V100 48GB GPUs with a total batch size of 28. The model is trained for 250 epochs on the SROIE and CORD datasets, and extended to 300 epochs for the DocVQA dataset.\nSTNet utilizes two types of loss: $L_{im}$ and $L_{see}$. The total loss is computed as a weighted sum of these components.\n$L_{total} = L_{im} + \\lambda L_{see}$\nAfter extensive evaluation, we set x = 0.001."}, {"title": "Inference", "content": "During the inference phase, we feed the question sequence Q to STNet as a prompt, guiding it to output the answer sequence A. Utilizing the hidden states H from the text decoder's last layer, we can decode the polygon information p, associated with the <see> token preceding each response in A. This allows for providing vision grounding of each answer, enhancing interpretability."}, {"title": "Experiments", "content": "Datasets and Evaluation Metrics\nSROIE. The SROIE (Huang et al. 2021) dataset comprises 973 scanned receipt images, divided into two subsets: 626 images for training and 347 for testing. Each receipt is annotated with four predefined target fields: company, date, address, and total. Additionally, segment-level text bounding boxes and their corresponding transcripts are provided to facilitate the extraction tasks. The primary objective is to accurately map each word to its appropriate field. For evaluating model performance on the test set, we employ two metrics: the field-level F1 score (Hwang et al. 2019) and Tree Edit Distance (TED)-based accuracy (Zhong, ShafieiBavani, and Jimeno-Yepes 2020). To ensure consistency, we adopt the results reported from Donut (Kim et al. 2022) and SeRum (Cao et al. 2023).\nCORD. The CORD (Park et al. 2019) dataset serves as a public benchmark comprising 800 training, 100 validation, and 100 testing receipts. The receipts are annotated with 30 types of entities under 4 categories: menu, void menu, subtotal, and total. A list of text lines with bounding boxes is provided. The evaluation task and metrics for the CORD dataset align with those used for the SROIE dataset.\nDocVQA. The DocVQA (Mathew, Karatzas, and Jawahar 2021) dataset comprises 50,000 questions derived from over 12,000 pages across a wide array of documents. The pages are allocated into training, validation, and test sets with an approximate ratio of 8:1:1. Due to the absence of ground truth in the test set, evaluations are performed on the validation set using the ANLS (Average Normalized Levenshtein Similarity), an edit-distance based metric."}, {"title": "Results", "content": "As illustrated in Table 1, we compare our approach with various prior methodologies. The STNet* model employs supervised training on the see process by mapping each answer text in the downstream dataset to its corresponding coordinates. In contrast, the original STNet model omits this step and directly infers the answer coordinates using a pre-trained physical decoder to ensure a fair comparison. It can be seen that our approach achieves a new state-of-the-art performance on both the CORD and SROIE datasets, significantly outperforming previous methods in both scenarios.\nOn the DocVQA dataset, due to the lack of answer text coordinate information, we only have results for the original STNet, which achieves the second-best performance, following only LayoutLMv2 (Xu et al. 2021). It is important to note that the DocVQA dataset features complex and densely packed textual content within images. LayoutLMv2 benefits significantly from the use of the advanced external Microsoft Read API\u00b9, which extracts text and bounding boxes from images prior to processing. This preprocessing step considerably simplifies the task.\nSTNet not only achieves state-of-the-art results among OCR-free methods but also provides vision grounding for the answers. This capability is particularly notable in datasets like DocVQA, which do not include answer text coordinate information. Figure 5 showcases the outcomes of text coordinate acquisition by our model."}, {"title": "Ablation Study", "content": "To demonstrate the effectiveness of each of our contributions, we design systems T1 through T4, which are evaluated on the SROIE dataset."}, {"title": "Conclusion", "content": "In this work, we introduce STNet, a novel end-to-end model that not only provides textual answers but also excels in offering vision grounding. STNet employs a \u201csee then tell\u201d strategy by placing a special <see> token before the answer text, then uses a physical decoder to parse the coordinates within the image for vision grounding. To enhance the model's training efficacy, we collect a number of table recognition datasets and develop a GPT-4-driven automated QA pair generation method, leading to the creation of the TVG Dataset, which comprises QA pairs with precise vision grounding. Experimental results on publicly available datasets such as CORD, SROIE, and DocVQA demonstrate that our STNet model achieves state-of-the-art performance in Key Information Extraction."}, {"title": "Appendix", "content": "TVG Dataset\nAs illustrated in Figure 3, we propose a comprehensive GPT-4-based method for the automatic construction of the TVG dataset. Each step of this process is detailed below."}, {"title": "Details of Data Source", "content": "Tables, as a unique form of document image, can be described using structured languages such as HTML. High-quality table data can be readily sourced from online resources. To this end, we have compiled several table recognition datasets, including PubTables1M (Smock, Pesala, and Abraham 2022) and iFLYTAB (Zhang et al. 2024).\nPubTables1M. PubTables1M is a large-scale table recognition dataset sourced from the PubMed Central Open Access (PMCOA) database. This dataset includes detailed annotations for projected row headers and bounding boxes for all rows, columns, and cells, including blank cells. Additionally, it introduces a novel canonicalization procedure aimed at correcting over-segmentation. This procedure ensures that each table is presented with a unique and unambiguous structural interpretation. Through these detailed annotations, we transform the tables into structured sequences in HTML format.\niFLYTAB. The iFLYTAB dataset comprises 12,104 training samples and 5,187 testing samples. It offers comprehensive annotations for each table image, including both physical coordinates and detailed structural information. This dataset includes not only axis-aligned digital documents but also images captured by cameras, which present more significant challenges than PubTables1M due to their complex backgrounds and non-rigid image deformations. Although it lacks textual annotations, we have addressed this limitation by using PaddleOCR (Li et al. 2022) for text recognition, subsequently converting the tables into HTML format."}, {"title": "Generation Prompt", "content": "As illustrated in Figure 7, we provide a standardized prompt template for QA data generation, utilizing structured HTML table sequences as input for GPT-4. The black text represents fixed components of the prompt, while the text within red brackets requires specific input. For instance, [Language] specifies the language in which the QA pairs should be generated. We instruct GPT-4 to generate five types of questions: specific extraction, simple reasoning, complex reasoning, numerical questions, and content summary.\nSpecific Extraction. Each specific extraction question should target a specific cell in the table. The answer should indicate the row <tr> and column <td> of the cell.\nSimple Reasoning. Each simple reasoning question should have an answer derived by reasoning from less than three cells in the table.\nComplex Reasoning. Each complex reasoning question should have an answer that requires reasoning from three or more cells in the table.\nNumerical Questions. Each numerical question should involve numerical calculations, such as sum, maximum, average, and minimum values. Provide the calculation process and the final result.\nContent Summary. Each content summary needs to provide a summary that describes the main content of the table and matches the table's content."}, {"title": "Post Process", "content": "As described in the aforementioned prompt template, for specific extraction questions, we require GPT-4 to provide not only the specific value from the cell but also the logical position of the cell, indicating its row and column number within the table. The detailed annotations in these table recognition datasets enable us to accurately locate the corresponding cell's real information, including its content and bounding box, based on the logical position. We only retain the QA pair when the value provided by GPT-4 matches the content of the located cell, with the bounding box serving as the required physical information p. This process ensures the generation of high-quality QA data {Q, A, p}. Ultimately, the TVG dataset we construct comprises 958,000 questions derived from 65,000 table images. It includes 244k specific extraction questions, 293k simple reasoning questions, 191k complex reasoning questions, 166k numerical questions, and 64k content summary. Some examples of them are illustrated in Figure 8."}]}