{"title": "Neural networks consisting of DNA", "authors": ["Michael te Vrugt"], "abstract": "Neural networks based on soft and biological matter constitute an inter-\nesting potential alternative to traditional implementations based on electric circuits.\nDNA is a particularly promising system in this context due its natural ability to store\ninformation. In recent years, researchers have started to construct neural networks\nthat are based on DNA. In this chapter, I provide a very basic introduction to the\nconcept of DNA neural networks, aiming at an audience that is not familiar with\nbiochemistry.", "sections": [{"title": "1 Introduction", "content": "Recent years have seen an increasing amount of work (some of which is also covered\nin this book) on implementing machine learning methods in physical systems, and\nthe concept of intelligent matter [18] is closely related to this idea. While many\napproaches of this type employ electronic, magnetic, or photonic systems, it is in\nprinciple a relatively natural idea to use soft and biological matter as a basis for\nphysical neural networks. After all, artificial neural networks are inspired by the\nbrain, and the brain is a soft matter system.\nDNA, the carrier of genetic information, naturally suggests itself for such ap-\nproaches. It is a soft matter system that has evolved specifically for the purpose\u00b9 of\nstoring and processing information. Moreover, there is an established tradition of"}, {"title": "2 Biochemistry for beginners", "content": "I have promised that this chapter does not require prior exposure to biochemistry,\nand consequently I will start with a brief introduction to what DNA is.\nDNA (short for deoxyribonucleic acid) is a biological macromolecule that stores\nthe genetic information. A macromolecule is (surprise!) a large molecule. Macro-\nmolecules often consist of many repeated subunits (monomers), in this case they are\nreferred to as polymers. DNA is a polymer whose subunits are nucleotides. Each\nnucleotide consists of a base, deoxyribose (a sugar), and a phosphate group. The\ndeoxyribose of one nucleotide binds to the phosphate group of the next one (this is\nreferred to as phosphodiester bond).\nWhat matters most for our purposes is the base, of which there exist five types:\nadenine (A), guanine (G), cytosine (C), thymine (T), and uracil (U). In DNA, one\nonly finds adenine, guanine, cytosine, and thymine. These can bind to each other via\nhydrogen bonds (base pairing). More specifically, adenine always binds to thymine\nand cytosine always binds to guanine (the bonding between G and C is a bit stronger).\nDNA is therefore typically found in a helix structure consisting of two strands (chains\nof nucleotides). The nucleotides within a strand are bound together by the phospho-\ndiester links, nucleotides in different strands are held together via base pairing.\nSince each base only binds to one specific other base, knowing the composition of\none strand allows to infer the composition of the other one (the strand are comple-\nmentary). For example, if the first strand is ACCCGAT, the second one has to be\nTGGGCTA.\nIn protein synthesis, DNA is converted into proteins (biological macromolecules\nthat perform a variety of functions). This occurs in several steps. First (transcription),\nDNA is copied to ribonucleid acid (RNA), specifically to so-called messenger RNA"}, {"title": "3 DNA Computing", "content": "The first DNA computer was realized in 1994 by Adleman [1], who used it to solve\nthe so-called \"directed Hamiltonian path problem\". Here, the aim is to find, for\ndirected graph and a given starting and end point, a path that visits each node exactly\nonce. This is a classical problem in computer science, since it is easy to pose, but\nvery difficult (\"NP-hard\") to solve. Adleman's procedure allowed, exploiting the\nparallelism of DNA computing, to solve this problem with a procedure where the\nnumber of steps grew only linearly with the number of vertices. This is a notable\nefficiency since the number of potential solutions increases combinatorially with the\nnumber of vertices, as a consequence of which traditional computing approaches\ndo not achieve such a scaling [2]. A discussion of the many developments and\napplications of DNA computing that emerged after Adleman's work can be found in\nreview articles [20, 9, 22, 31]. Of particular interest in the context of this book is the\ndevelopment of intelligent systems based on DNA (see Ref. [9] for a review).\nSome major advantages of DNA in the context of computing are [20]:\n\u2022\nParallelism: DNA computers can perform a large number of tasks in parallel,\nwhich can lead to extremely good performances compared to modern supercom-\nputers. For instance, Adleman's DNA computer already had a performance of\n100 Teraflops.\n\u2022\nStorage: DNA allows for extremely efficient data storage, requiring just 1 cubic\nnanometer for one bit of information. All the datat hat humanity has generated by\n2025 could be stored in the size of a ping-pong ball, in a manner that makes the\nstored data easy to maintain and to copy [16].\n\u2022\nEnergy Efficiency: DNA computers, being based on chemical reactions, do\nnot require any electricity and are very energy efficient compared to traditional\ncomputers.\nHowever, there are also significant disadvantages [20]:\n\u2022\nAccuracy: The biochemical processes involved in DNA computers are prone to\nerrors, with the error probability increasing exponentially with the number of\noperations."}, {"title": "4 The winner takes it all", "content": "The DNA-based implementation of neural networks discussed in this chapter is based\non so-called winner-take-all computation [23]. Here, the basic idea is that neurons\ncompete with each other. They inhibit other neurons while activating themselves.\nThereby, it can be ensured that only the neuron with the largest input stays active,\nwhile all the other ones become inactive. A CMOS implementation of a winner-\ntake-all function was proposed in Ref. [21]. How exactly this architecture can be\nimplemented in a DNA system, and how input signals can be encoded in DNA, will\nbe discussed in Section 6.\nIn the context of neural networks, a winner-take-all computation can connect the\npenultimate and final layer of a neural network, which have the same number of\nneurons. Let us denote the ith neuron in the penultimate layer by $s_i$ and the ith\nneuron in the final layer by $y_i$. The connection is set up in such a way that $y_i$ is one\nif $s_j$ is the largest value in the penultimate layer and zero otherwise.\nHow is this achieved? Thinking of one layer of a neural network as a vector x with\ncomponents $x_i$, the next layer is constructed by multiplying the vector by a matrix\nand then applying a nonlinear function to each component of the resulting vector.\nDenoting the elements of the weight matrix by $w_{ij}$, the jth component of the vector\nthat results from applying the weight matrix to the penultimate layer is\n$s_i = \\sum_j P_{ij}$\nwith the products\n$P_{ij} = w_{ij}x_j$.\nThe nonlinear function is then one that selects the largest component of the vector s:\n$Y_i =\\{\n\\begin{array}{ll}\n1 & \\text { if } s_i>s_j \\forall j \\neq i, \\\\\n0 & \\text { otherwise. }\n\\end{array}$\nSuch a setup allows the network to have memory, which is encoded in the weights,\nand to solve pattern recognition tasks. Suppose the network has been trained to\nremember two patterns corresponding to the vectors w\u2081 and w2 (for example, two"}, {"title": "5 DNA gates", "content": "A key technique in this context is toehold-mediated strand displacement [32, 33]\n(see Ref. [28] for a review). This process involves a single-stranded DNA (the input)\nand a double-stranded DNA, whose strands are referred to as gate and output. One\nstrand of the double-stranded DNA (the gate) has an overhanging piece (the toehold)\nthat the input can bind to. Via branch migration (a process by which one DNA strand\nis exchanged for another, see Ref. [14] for an introduction), the input strand then\ngradually starts binding to the gate strand and thereby replaces the output strand,\nwhich is thereby released. This process is more likely to start if the toehold is longer,\nand consequently, the toehold length can be used to control the reaction rate [25].\nSpecifically, the reaction rate increases exponentially with the toehold length [33].\n(In practice stochastic fluctuations are of course likely to be relevant here, what\nexactly their influence is has not been systematically investigated.)\nThis process is visualized in Fig. 1. (A more complex illustration can be found\nin Ref. [26].) Fig. 1(a) shows the initial configuration. The input strand consists of\nthree segments, labelled S1, T, and S2. Here, S\u2081 and S2 are recognition domains,\nwhich are relatively long (15 nucleotides), and T is the toehold domain, which is\nshorter (5 nucleotides). The other necessary ingredient is the gate-output-complex,\nwhich consists of two strands (gate and output) that are bound to each other. The\ngate has a recognition domain S, with a toehold domain T' before and after it, while\nthe output has two recognition domains S2 and S3 separated by a toehold domain T.\nHere, a prime indicates a complementary base sequence (e.g., if T =AGGAT, then\nT'=TCCTA). Due to the DNA base-pairing rules (see Section 2), the segments S\nand T' bind to the segments S2 and T of the output, respectively.\nSince the gate has two toeholds and the output has only one, the gate-output\ncomplex has a free toehold T'. This toehold is complementary to the free toehold T\nof the input. As a consequence, the input binds to the gate-output complex, leading to\na complex consisting of three DNA strands. Now, although the recognition domain\nS' of the gate is currently bound to the output, it could by the DNA base-pairing rules\nequally well bind to the recognition domain S2 of the input. Gradually, via branch\nmigration, the recognition domain of the output ceases to bind to the S2 recognition\ndomain of the output and instead binds to the corresponding domain of the input (as\nshown in Fig. 1(b)). In the final state, shown in Fig. 1(c), the gate is now bound to\nthe input, forming a gate-input complex, while the output has been released."}, {"title": "6 A DNA neural network", "content": "We have now assembled all ingredients that are necessary to understand a simple\nwinner-take-all neural network based on DNA, namely the one realized by Cherry\nand Qian [5]. What it is supposed to do is to recognize handwritten numbers based\non the general approach discussed in Section 4 (and it turns out that the DNA neural"}, {"title": "7 DNA reservoir computing", "content": "Finally, I will discuss a different approach to DNA-based machine learning. This\napproach, proposed in a theoretical study by Goudarzi et al. [12], uses reservoir\ncomputing [17, 24]. See the introductory chapter on this topic in this volume for an\nexplanation of how reservoir computing works and the chapters by Kathy L\u00fcdge/Lina\nJaurigue, Atreya Majumdar/Karin Everschor-Sitte, and Julian Jeggle/Raphael Wit-\ntkowski in this volume for other implementations."}, {"title": "8 Advantages and disadvantages of this approach", "content": "While it is certainly impressive that test tubes filled with some chemicals can be\nused for handwritten number recognition, it should of course be noted that this is\ncertainly not the most efficient approach if the recognition of handwritten numbers\nis our primary goal. If the numbers are primarily a proof of principle, what then can\nthese methods be useful for?\nDNA neural networks require the input signal to have the form of a DNA strand. In\ngeneral, this is a disadvantage since converting general input signals to DNA is quite\nan effort. This aspect can, however, turn into an advantage in contexts where the input\nsignal takes the form of DNA strands (or at least that of biomolecules) anyway. This\nwill primarily be the case in biomedical applications of neural networks. Suppose,\nfor example, that a neural network has been trained to recognize genetic dispositions\nfor a certain disease. If this network is implemented in DNA form, then one could\njust take a DNA sample from the patient, put it into a test tube, and then see a glowing\ntest tube indicating that the gene one looks for is (or is not) present.\nA further aspect to note here are the time scales involved here. Photonic neu-\nral networks (see the chapters by Kathy L\u00fcdge/Lina Jaurigue and by Lennart\nMeyer/Rongyang Xu/Wolfram Pernice) have the attractive feature that they oper-\nate with the speed of light, i.e., they are extremely fast. This can certainly not be said\nabout DNA computers. Their computational speed depends on how fast the chemical\nreactions take place, which can be of the order of many minutes. An advantage, in\ncontrast, is that DNA-based approaches are well suited for parallelization. These\n(dis)advantages are familiar from DNA computing in general.\nMoreover, there can be contexts where having a neural network that operates\nslower can actually be an advantage.3 A good example would be a network that\nprocesses temporal input signals, as is required, e.g., in speech recognition. In this\ncase, it is advantageous if the system's dynamics takes place on roughly the same time\nscales as the input signal rather than being significantly faster. Consider reservoir\ncomputing, where the employed physical system possesses a fading memory, as an\nexample - if, after the second word of a sentence to be processed, all memory of the\nfirst word has already vanished, the system cannot process the sentence as a whole.\nA particular advantage of DNA neural networks in this context is that the speed at"}, {"title": "9 Summary", "content": "In this chapter, I have provided a brief introduction to neural networks consisting\nof DNA, using as an example the winner-take-all network proposed in Ref. [5]. The\ninput data is provided as a DNA strand and is processed via biochemical reactions.\nOn this basis, it is possible to recognize handwritten digits using DNA. Moreover,\nI have briefly discussed a proposal for DNA-based reservoir computing [12]. Such\napproaches constitute a promising starting point for the development of intelligent\nmatter based on biological materials, and might also find applications in, for instance,\nmedical contexts where input data is already present in a biochemical form."}]}