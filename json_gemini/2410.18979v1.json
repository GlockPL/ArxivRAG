{"title": "PIXELGAUSSIAN: GENERALIZABLE 3D GAUSSIAN RECONSTRUCTION FROM ARBITRARY VIEWS", "authors": ["Xin Fei", "Wenzhao Zheng", "Yueqi Duan", "Wei Zhan", "Masayoshi Tomizuka", "Kurt Keutzer", "Jiwen Lu"], "abstract": "We propose PixelGaussian, an efficient feed-forward framework for learning generalizable 3D Gaussian reconstruction from arbitrary views. Most existing methods rely on uniform pixel-wise Gaussian representations, which learn a fixed number of 3D Gaussians for each view and cannot generalize well to more input views. Differently, our PixelGaussian dynamically adapts both the Gaussian distribution and quantity based on geometric complexity, leading to more efficient representations and significant improvements in reconstruction quality. Specifically, we introduce a Cascade Gaussian Adapter to adjust Gaussian distribution according to local geometry complexity identified by a keypoint scorer. CGA", "sections": [{"title": "1 INTRODUCTION", "content": "Novel view synthesis (NVS) seeks to reconstruct a 3D scene from a series of input views and gener-ate high-quality images from previously unseen viewpoints. High-quality and real-time reconstruc-tion and view synthesis are crucial for autonomous driving (Tonderski et al., 2023; Khan et al., 2024;Tian et al., 2024; Huang et al., 2024a), robotics perception (Wilder-Smith et al., 2024; Jiang et al.,2023a) and virtual or augmented reality (Yang et al., 2024; Zheng et al., 2024).\nNeRF-based methods ( Mildenhall et al., 2020; Hu et al., 2022; Liu et al., 2020; Neff et al.,2021) have achieved remarkable success by encoding 3D scenes into implicit radiance fields, yetsampling volumes for NeRF rendering is costly in both time and memory. Recently, Kerbl et al.(2023) proposed to represent 3D scenes explicitly using a set of 3D Gaussians, enabling much moreefficient and high-quality rendering via a differentiable rasterizer. Still, the original 3D GaussianSplatting requires separate optimization on each single scene, which significantly reduces inferenceefficiency. To tackle this problem, recent researches have aimed at generating 3D Gaussians directlyfrom a feed-forward network without any per-scene optimization (Charatan et al., 2023; Chen et al.,2024; Liu et al., 2024; Szymanowicz et al., 2024; Zheng et al., 2024). Typically, these approachesadhere to a paradigm where a fixed number of Gaussians is predicted for each pixel in the inputviews. The Gaussians derived from different views are then directly merged to construct the final3D scene representation. However, such a paradigm limits the model performance as the Gaussianssplats are uniformly distributed across images, making it difficult to capture local geometric detailseffectively. Additionally, as the number of input views increases, directly merging Gaussians candegrade reconstruction performance due to severe Gaussian overlap and redundancy across views.\nTo address this, we propose PixelGaussian, which enables dynamic adaption on both 3D Gaussian distribution and quantity. To be specific, we first uniformly initialize Gaussian positions followingChen et al. (2024) to accurately localize the Gaussian centers. To identify geometry complexityacross images, we then compute a relevance score map for each input view from image features inan end-to-end manner. Under the guidance of score maps, we construct a Cascade Gaussian Adapter(CGA), which leverages deformable attention (Xia et al., 2022) to control the pruning and splittingoperations. After CGA, more Gaussians are allocated to regions with complex geometry for precisereconstruction, while unnecessary and duplicate Gaussians across views are pruned to reduce redun-dancy and improve efficiency. Since these Gaussian representations still fall short in fully capturingthe image details, we further introduce a transformer-based Iterative Gaussian Refiner (IGR) to refine3D Gaussians through direct image-Gaussian interactions. Finally, we employ rasterization-basedrendering using the refined Gaussians to generate novel views at target viewpoints.\nWe conduct extensive experiments on ACID (Liu et al.) and RealEstate10K (Zhou et al., 2018)benchmarks for large-scale 3D scene reconstruction and novel view synthesis. PixelGaussian out-performs existing methods on different input views with a comparable inference speed. Notably,existing generalizable 3D Gaussian splatting methods (pixelSplat (Charatan et al., 2023) and MVS-plat (Chen et al., 2024)) fail to achieve good results when transferring to more input views whileour method demonstrates consistent performance. This is because existing pixel-wise methods gen-erate uniform pixel-aligned Gaussian predictions, and our model mitigates Gaussian overlap andredundancy across views by dynamically adjusting their distribution based on local geometry com-plexity. Visualizations and ablations further demonstrate that both CGA and IGR blocks are crucialin adapting Gaussian distribution, enabling the proposed PixelGaussian to capture geometry detailsand achieve better reconstruction accuracy."}, {"title": "2 RELATED WORK", "content": "Multi-View Stereo. Multi-View Stereo (MVS) aims to reconstruct a 3D representation from multi-view images of a given scene or object. Since accurate depth estimation is essential for reliable3D reconstruction from 2D inputs, most MVS methods (Gu et al., 2020; Ding et al., 2021; Yaoet al., 2018) require ground truth depth for supervision in training process. Additionally, point-basedMVS approaches generally separate the processes of depth estimation and point cloud fusion pro-cesses. Recently, inspired by efficient Gaussian representations proposed by Kerbl et al. (2023),Chen et al. (2024) introduces to directly predict depth for pixel-wise Gaussians from a cost volumestructure without requiring depth supervision, significantly improving model scalability and flexi-bility. Therefore, following a similar approach, we construct a lightweight cost volume to facilitatethepth estimation, which serves as an efficient initialization for 3D Gaussians in our PixelGaussian.\nPer-scene 3D Reconstruction. Neural Radiance Fields (NeRF) have revolutionized the field of3D reconstruction by representing scenes as implicit neural fields (Mildenhall et al., 2020). Subse-quent researches have focused on overcoming the limitations of the original NeRF to improve itsperformance and broaden its applicability. Some researches aim to improve the efficiency for novelview synthesis (Hu et al., 2022; Fridovich-Keil et al., 2022; Yu et al., 2021a; Liu et al., 2020;Neff et al., 2021). Moreover, several studies concentrate on capturing intricate geometry and tem-poral information to achieve accurate and dynamic reconstruction (Li et al., 2021; Du et al., 2021;Pumarola et al., 2020; Tian et al., 2023; Wang et al., 2022). Compared to implicit NeRF-basedmethods, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) represents a 3D scenario as a set of ex-plicit 3D Gaussians, enabling a rasterization-based splatting rendering process that is significantlymore efficient in both time and memory. Given that 3DGS still requires millions of 3D Gaussiansto represent a single scene, numerous studies have focused on achieving real-time rendering andminimizing memory usage (Fan et al., 2023; Katsumata et al., 2024; Lu et al., 2024). Additionally,some researches focus on enhancing the reconstruction quality of 3DGS by employing multi-scalerendering (Yan et al., 2024), advanced shading models (Jiang et al., 2023b) or incorporating physi-cally based properties for realistic relighting (Gao et al., 2023). However, these methods still requireper-scene optimization and rely on dense input views, which can be computationally expensive andlimit their scalability for large-scale or dynamic scenes.\nGeneralizable 3D Reconstruction. PixelNeRF (Yu et al., 2021b) pioneers the approach of pre-dicting pixel-wise features directly from input views to reconstruct neural radiance fields. Follow-ing methods incorporate volume or transformer architectures to improve the performance of feed-forward NeRF models (Chen et al., 2021a; Xu et al., 2024; Miyato et al., 2024; Sajjadi et al., 2022;Du et al., 2023). However, these feed-forward NeRF approaches typically demand substantial mem-ory and computational resources due to the expensive per-pixel volume sampling process (Wanget al., 2021a; Johari et al., 2022; Barron et al., 2021; Garbin et al., 2021; Reiser et al., 2021; M\u00fclleret al., 2022). With the advent of 3DGS, PixelSplat (Charatan et al., 2023) initiates a shift towardsfeed-forward Gaussian-based reconstruction. It takes sparse input views to directly predict pixel-wise 3D Gaussians by leveraging epipolar geometry to learn cross-view features. MVSplat (Chenet al., 2024) constructs a cost volume structure for depth estimation, which significantly boosts bothmodel efficiency and reconstruction quality. Additionally, MVSGaussian (Liu et al., 2024) furtherimproves model performance by introducing an efficient hybrid Gaussian rendering process. More-over, SplatterImage (Szymanowicz et al., 2024) and GPS-Gaussian (Zheng et al., 2024) predictpixel-wise 3D Gaussians for object-centric or human reconstruction.\nHowever, these feed-forward methods are constrained by the pixel-wise Gaussian predictionparadigm, which limits the model's performance as the Gaussian splats are uniformly distributedacross images. Such a paradigm inadequately captures intricate geometries, while also causingGaussian overlap and redundancy across views, ultimately resulting in severe rendering artifacts.In comparison, PixelGaussian consists of a Cascade Gaussian Adapter (CGA), allowing for dy-namic adaption on both Gaussian distribution and quantity. Visualizations demonstrate that CGAis capable of allocating more Gaussians in areas rich in geometric details, while reducing duplicateGaussians in similar regions across input views. Furthermore, we introduce an Iterative GaussianRefiner (IGR), enabling direct interaction between 3D Gaussians and local image features via de-formable attention. Experimental results show that IGR effectively leverages image features to guideGaussians in capturing the full information contained within the images, significantly enhancing themodel's ability to capture local intricate geometry."}, {"title": "3 PROPOSED APPROACH", "content": "In this section, we present our method to learn generalizable Gaussian representations from arbitraryviews. Given an arbitrary set of input images \\(I = \\{I_i\\}_{i=1}^{N_1} \\in \\mathbb{R}^{N \\times H \\times W \\times 3}\\) and correspondingcamera poses \\(C = \\{C_i\\}_{i=1}^{N_1}\\), our PixelGaussian aims to learn a mapping M from images to 3DGaussians for scene reconstruction:\n\\[M: \\{(I_i, C_i)\\}_{i=1}^{N_1}\\leftrightarrow \\{(\\mu_j, s_j, r_j, a_j, sh_j)\\}_{j=1}^{N_K},\\]\nwhere NK is the total number of 3D Gaussians, which adaptively varies depending on the scenecontext. Each Gaussian is parameterized by its position \\(\\mu_j\\), scaling \\(s_j\\), rotation \\(r_j\\), opacity \\(a_j\\) andspherical harmonics \\(sh_j\\).\nAs illustrated in Figure 2, we first use a lightweight cost volume for depth estimation and Gaussianposition initialization. We then introduce Cascade Gaussian Adapter (CGA), which dynamicallyadapts both Gaussian distribution and quantity based on local geometric complexity. Finally, weexplain how Iterative Gaussian Refiner (IGR) enables direct image-Gaussian interactions, furtherrefining Gaussian distribution and representations for enhanced reconstruction."}, {"title": "3.1 GAUSSIAN INITIALIZATION", "content": "Position Initialization. Following the instructions of MVSplat (Chen et al., 2024), we first extractimage features via a 2D backbone consisting of CNN and Swin Transformer (Liu et al., 2021).Specifically, CNN encodes multi-view images to corresponding feature maps, while Swin Trans-former performs both self-attention and cross-view attention to better leverage information crossviews. Then, we obtain the aggregated multi-view features \\(F = \\{F_i\\}_{i=1}^{N_1}\\)\nTo initialize Gaussian positions precisely, we construct a lightweight cost volume (Yao et al., 2018)for depth estimation, denoted as \\(\\Phi_{depth}\\). We then predict Gaussian centers as follows:\n\\[\\mu = P^{-1}(\\Phi_{depth}(F), C)\\]\nwhere \\(P^{-1}(.)\\) stands for unprojection operation.\nParameter Initialization. For each Gaussian center \\(\\mu_j\\), we randomly set corresponding scaling\\(s_j \\in \\mathbb{R}^3\\), rotation \\(r_j \\in \\mathbb{R}^4\\), opacity \\(a_j \\in \\mathbb{R}^1\\), spherical harmonics \\(sh_j \\in \\mathbb{R}^C\\) within a proper range.we then get the initial Gaussians set \\(G = \\{(\\mu_j, s_j, r_j, a_j, sh_j)\\}_{j=1}^{H W} \\in \\mathbb{R}^{H W \\times (11+C)}\\)."}, {"title": "3.2 CGA: CASCADE GAUSSIAN ADAPTER", "content": "After obtaining the initial Gaussian set G, we introduce Cascade Gaussian Adapter (CGA) drivenby a multi-view keypoint scorer \u03a8, as shown in Figure 3(a). CGA contains a set of context-aware"}, {"title": "3.3 IGR: ITERATIVE GAUSSIAN REFINER", "content": "Though CGA allows for a more optimal Gaussian distribution, the Gaussian representations still fallshort in capturing the full information contained in the images. Inspired by the efficiency demon-strated by GaussianFormer (Huang et al., 2024b) in occupancy prediction, we design a transformer-based Iterative Gaussian Refiner (IGA) to further extract local geometric information from inputviews, as shown in Figure 3(b). In this process, we leverage deformable attention to enable directimage-Gaussian interactions, enhancing the ability for 3D Gaussians to more accurately captureintricate geometry details in reconstruction and view synthesis.\nIGR is composed of B attention and refinement blocks. In Section 3.2, CGA adapts the originalGaussian set G to \\(G = G_K\\). To continue, we first sample and embed G into Gaussian queries Q.In each block, deformable attention is first applied between Gaussian queries Q and multi-viewfeatures F to update Gaussian representations. This is followed by a refinement stage where aresidual module further fine-tunes the queries. The overall process of IGR can be formulated as:\n\\[Q_b = \\Phi_{ref}(\\sum_{i=1}^N a_i \\cdot DA(Q_{b-1}, F_i, P(\\mu^{(b)}, C_i))), b = 1, 2, ..., B,\\]\nwhere DA(\u00b7), \\(\\Phi_{ref}\\)(\u00b7), P(\u00b7) denote the deformable attention function, refinement layer and projec-tion operation, \\(F_i, C_i, a_i\\) represents the image feature, camera parameters and contribution factor ofinput view \\(I_i\\), respectively. \\(Q_b (b = 1, 2, ..., B)\\) stands for output queries from the b \u2013 th IGR block,and \\(\\mu^{(b)}\\) is the Gaussian center of current stage. Initially, we set \\(Q_0 = Q\\).\nFinally, the refined Gaussian queries are decoded into Gaussian parameters \\(G_f\\) through a simpleMLP to ensure all parameters within proper range, and then can be used for rasterization-basedrendering at novel viewpoints.\n\\[G_f = \\{(\\mu_j^f, s_j^f, r_j^f, a_j^f, sh_j^f)\\}_{j=1}^{N_K} = MLP(Q_B).\\]\nOur full model takes ground-truth target RGB images at novel viewpoints as supervision, allowingfor efficient end-to-end training. The training loss is calculated as a linear combination of MSE andLPIPS (Zhang et al., 2018) losses, with loss weights of 1 and 0.05, respectively."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETTINGS\nDatasets. To assess the performance of our model, we conduct experiments on two extensive datasets: ACID (Liu et al.) and RealEstate10K (Zhou et al., 2018). The ACID dataset consists of video frames capturing natural landscape scenes, comprising 11,075 scenes in the training set and 1,972 scenes in the test set. RealEstate10K provides video frames from real estate environments, with 67,477 scenes allocated for training and 7,289 scenes reserved for testing. The model is trained with two reference views, and four novel views are selected for evaluation. During testing, however, we perform multiple experiments where 2, 3, and 4 views are selected for reference, while four novel views selected for evaluation in each scenario.\nImplementation Details. We set the resolutions of input images as 256x256. In Cascade Gaussian Adapter (CGA), we apply K = 3 stages of cascade Gaussian adaption. As for the splitting operation, the SplitNet generates M = 1 separate new Gaussians, whereas the pruning process uses reduction factors \\(\\gamma_a = \\gamma_s = 0.5\\) and opacity threshold \\(\\tau_{\\alpha} = 0.3\\). We use B = 3 blocks in Iterative Gaussian Refiner (IGR) to extract local geometry from input views. We implement our PixelGaussian with Pytorch and train the model on 8 NVIDIA A6000 GPUs for 300,000 iterations with Adam optimizer. More training details are provided in Section A.2."}, {"title": "4.2 MAIN RESULTS", "content": "Novel View Synthesis. As shown in Table 1 and Figure 4, our proposed PixelGaussian consistently outperforms previous NeRF-based methods and pixel-wise Gaussian feed-forward networks across all settings with 2, 3, and 4 reference views. Notably, as the number of input views increases, the re-construction performance of both pixelSplat (Charatan et al., 2023) and MVSplat (Chen et al., 2024) degrades significantly, while PixelGaussian shows a slight improvement. This is because previous methods directly merge multiple views by back-projecting pixel-wise Gaussians to 3D space based on depth maps. Without the capability to adapt the quantity and distribution of Gaussians dynamically, pixel-wise methods often produce duplicated Gaussians with significant overlap, and their spatial positioning is suboptimal. In contrast, PixelGaussian is able to optimize both the distribution and quantity of Gaussians via CGA, while IGR blocks facilitate direct interaction between Gaussian queries and local image features, resulting in more accurate reconstructions."}, {"title": "4.3 \u0395\u03a7\u03a1\u0395IMENTAL ANALYSIS", "content": "In this section, we further investigate and conduct experiments to demonstrate the effectiveness of our PixelGaussian. We first visualize the score maps S and Gaussian density maps. Then, we present the cascade adaption process of CGA. Finally, we conduct ablation studies on our model. These"}, {"title": "5 CONCLUSION AND DISCUSSIONS", "content": "In this paper, we have presented PixelGaussian to learn generalizable 3D Gaussian reconstruction from arbitrary input views. The core innovation of our approach is context-aware Cascade Gaussian Adapter (CGA), which dynamically splits Gaussians in regions with complex geometric details and prunes redundant ones. Further, we incorporate deformable attention within Iterative Gaussian Re-finer (IGR), facilitating direct image-Gaussian interactions to improve local geometry reconstruc-tions. Compared to previous uniform pixel-wise methods, PixelGaussian is able to dynamically adapt both Gaussian distribution and quantity guided by the complexity of local geometry details, allocating more to detailed regions and reducing redundancy across views, thus leading to better performance in reconstruction and view synthesis.\nDiscussions and Limitations. Although PixelGaussian can adjust the distribution of 3D Gaussians dynamically, the initial Gaussians are still derived from pixel-wise unprojection. When we initialize the Gaussian centers completely at random, the model fails to converge. Moreover, deformable attention in IGR consumes substantial computational resources when the number of Gaussians is extremely large, highlighting the need for a more efficient approach to represent 3D scenes with fewer Gaussians. Furthermore, PixelGaussian is unable to perceive the unseen parts of 3D scenes beyond the input views, suggesting the potential need to incorporate generative models."}, {"title": "A APPENDIX", "content": "A.1 PRELIMINARY\nA.1.1 3D GAUSSIAN SPLATTING\n3D Gaussian Splatting (Kerbl et al., 2023) represents a 3D scene as a set of explicit Gaussian prim-itives as follows:\n\\[\\mathcal{G} = \\{g_i | g_i = (\\mu_i, \\Sigma_i, \\alpha_i, sh_i)\\}_{i=1}^{N}\\]\nwhere each Gaussian has a center \\(\\mu_i\\), a covariance \\(\\Sigma_i\\), an opacity \\(\\alpha_i\\) and spherical harmonics \\(sh_i\\).Furthermore, given the scaling matrix S and rotation matrix R, we can calculate the covariancematrix:\n\\[\\Sigma = RSS^T R^T\\]\nAs explicit Gaussian primitives \\(\\mathcal{G}\\) can be rendered via an rasterization-based operation, such ap-proach is much more cheaper in both time and memory compared to implicit neural fields (Milden-hall et al., 2020; Barron et al., 2021; Tian et al., 2023; Chen et al., 2021a; Garbin et al., 2021) orvoxel-based representations( Sitzmann et al., 2019; Ji et al., 2017; Xie et al., 2019; Tatarchenkoet al., 2017; Choy et al., 2016).\nA.1.2 DEFORMABLE ATTENTION\nSince the introduction of ViT (Dosovitskiy et al., 2021), numerous efficient attention mechanismshave been proposed to further enhance the scalability and reduce the complexity of ViT (Liu et al.,2021; Chen et al., 2021b; Wang et al., 2021b; Yang et al., 2021; Zhang et al., 2021; Dong et al.,2022; Jaegle et al., 2021; Sun et al., 2022; Zhu et al., 2020; Yue et al., 2021; Chen et al., 2021c).Among them, Deformable Attention Transformer (DAT) (Xia et al., 2022) stands out for its ability toeffectively capture multi-scale features and adaptively focus on important regions while maintaininglightweight computational overheads. Therefore, we apply deformable attention in both the ScoreHypernetworks H (equation 4) and Iterative Gaussian Refiner (IGR) (equation 8) in our PixelGaus-sian model. We elaborate equation 8 as an example.\nGiven Gaussian centers \\(\\mu\\), corresponding queries Q, feature maps \\(F = \\{F_i\\}_{i=1}^{N}\\) and camera param-eters \\(C = \\{C_i\\}_{i=1}^{N}\\), we first project Gaussian centers to pixel coordinates to get reference points,denote as \\(R = \\{R_i\\}_{i=1}^{N}\\):\n\\[R_i = P(\\mu, C_i), (i = 1, 2, ..., N)\\]\nwhere P(\u00b7) denotes projection operation. Next, for Gaussian queries Q, we perform deformablesampling from feature maps F at reference points R using bilinear interpolation as follows:\n\\[\\hat{F_i}(R_i, (x, y)) = \\sum_{(p_x, p_y)} g(r_x, p_x) \\cdot g(r_y, p_y) \\cdot F_i(p_x, p_y)\\]\nwhere \\(g(x, y) = max(0, 1 - |a - b|)\\), \\((r_x, r_y), F_i\\) denote reference point and feature map, respec-tively. Then we can update queries Q via attention:\n\\[Q = \\sum_{i=1}^{N} \\alpha_i \\cdot (W_i \\cdot \\phi(F_i, R_i))\\]\n\\[W_i = softmax(Q \\cdot \\phi(F_i, R_i)^T)\\]\nwhere \\(\\alpha_i, (i = 1, 2, ..., N)\\) represents view weights as illustrated in equation 3. Following thisparadigm, we incorporate deformable attention in the score hypernetworks H to generate context-aware thresholds, and in IGR blocks to further refine Gaussian representations using image features."}, {"title": "A.2 MORE IMPLEMENTATION DETAILS", "content": "Training Details. As mentioned in Section 4.1, our model is trained on 8 NVIDIA A6000 GPUs,The batch size for a single GPU is set to 4, where each batch contains a large-scale 3D scene withtwo reference views and four inference views. Detailed settings of image backbone to generateaggregated features F, context-aware score hypernetworks H in Cascade Gaussian Adapter and"}, {"title": "A.3 MORE RESULTS", "content": "We provide more visualization comparisons of PixelGaussian and previous dominant pixel-wise methods (Charatan et al., 2023; Chen et al., 2024) on ACID (Liu et al.) and RealEstate10K (Zhou et al., 2018) benchmarks. Additionally, we provide visualizations of more score maps and Gaussian density maps from our PixelGaussian model. As shown in Figure 7 and Figure 8, PixelGaussian ben-efits from dynamic adaption of Gaussian distributions, resulting in superior reconstruction compared to previous pixel-wise methods."}]}