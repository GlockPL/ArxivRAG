{"title": "OpenDataLab: Empowering General Artificial Intelligence with Open Datasets", "authors": ["Conghui He", "Wei Li", "Zhenjiang Jin", "Chao Xu", "Bin Wang", "Dahua Lin"], "abstract": "The advancement of artificial intelligence (AI) hinges on the quality and accessibility of data, yet the current fragmentation and variability of data sources hinder efficient data utilization. The dispersion of data sources and diversity of data formats often lead to inefficiencies in data retrieval and processing, significantly impeding the progress of AI research and applications. To address these challenges, this paper introduces OpenDataLab, a platform designed to bridge the gap between diverse data sources and the need for unified data processing. OpenDataLab integrates a wide range of open-source AI datasets and enhances data acquisition efficiency through intelligent querying and high-speed downloading services. The platform employs a next-generation AI Data Set Description Language (DSDL), which standardizes the representation of multimodal and multi-format data, improving interoperability and reusability. Additionally, OpenDataLab optimizes data processing through tools that complement DSDL. By integrating data with unified data descriptions and smart data toolchains, OpenDataLab can improve data preparation efficiency by 30%. We anticipate that OpenDataLab will significantly boost artificial general intelligence (AGI) research and facilitate advancements in related AI fields.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving field of artificial intelligence, data has become an indispensable resource [6; 11; 14; 4]. However, the isolation between datasets and the incompatibility with data processing tools have been major technical barriers hindering the widespread application and advancement of AI technologies. To address these challenges, OpenDataLab has adopted an innovative technological architecture aimed at eliminating barriers between datasets and enhancing data accessibility and operability, thereby facilitating comprehensive advancements in AI.\nThe architectural design of OpenDataLab is meticulously constructed, as illustrated in Figure 1, and includes the following layers from bottom to top:\n\u2022 Diverse Data Sources: Encompassing data from research institutions, individual researchers, community shares, and collaborative constructions, ensuring comprehensiveness and diversity of data.\n\u2022 Data Pipeline Layer: Incorporates critical steps such as data collection, integration, cleaning, and compression, ensuring high quality and consistency of data."}, {"title": "2 Open Datasets", "content": ""}, {"title": "2.1 High-Quality, Diverse Datasets", "content": "OpenDataLab boasts over 6,500 open datasets, covering more than 30 data formats and supporting over 50 types of tasks, offering an extensive range of data topics. These datasets include over 6 billion images, 800 million video clips, 1 trillion tokens, 1 million 3D models, and 20,000 hours of audio, totaling over 80TB of data. This provides users with large-scale, high-quality, and diverse training samples."}, {"title": "2.2 Licensing Protection for Datasets", "content": "OpenDataLab's management of datasets includes not only the collection and organization of data but also the systematic organization and verification of dataset metadata. This metadata includes the original sources, official websites, relevant scholarly articles, publishers, and applicable licenses. Through this approach, OpenDataLab offers a free and convenient public data hosting service, ensuring transparency and traceability of data.\nMoreover, the platform supports a variety of open licenses to facilitate the broad use and sharing of data. These licenses include Creative Commons (CC), Open Data Commons (ODC), and Community Data License Agreement (CDLA). By clearly outlining the permissions, restrictions, and conditions of each license, OpenDataLab helps users understand and adhere to the specific legal and ethical guidelines required when using datasets, such as attribution (BY), share-alike (SA), non-commercial use (NC), and no derivatives (ND). This not only protects the rights of data creators but also provides clear usage guidelines for data users."}, {"title": "2.3 Growth and Updates of Datasets", "content": "OpenDataLab utilizes automated pipelines and community operations to achieve continuous growth and updates of datasets. Built on a robust big data infrastructure, the automated data collection pipelines facilitate the collection, processing, and uploading of dataset information, ensuring that high-quality and diverse dataset selections are promptly available to users.\nCommunity operations further enhance the platform's development by encouraging community contributions. A mutually beneficial relationship between dataset creators and users establishes an effective closed-loop data supply system. Regular community events, updates on newly released datasets, and active user interactions are key drivers of this process. The latest news can be viewed on the platform's website2."}, {"title": "3 Data Set Description Language (DSDL)", "content": ""}, {"title": "3.1 DSDL Design Goals and Core Architecture", "content": "Datasets are foundational to artificial intelligence research and applications, with their acquisition, dissemination, and utilization rates directly influencing the pace of technological advancement. Throughout the evolution of the AI field, the generation and release of massive datasets have been a key driver of progress. However, these datasets often adhere to diverse definition formats, leading to high costs in their dissemination, integration, and application. To address this issue, we have introduced a new dataset description language: DSDL (Data Set Description Language), with the primary design objectives of universality, portability, and extensibility:\n\u2022 Universality: DSDL provides a unified standard for data representation, applicable across various domains of AI datasets, not limited to specific tasks or fields. This language is designed to articulate datasets of different modalities and structures in a consistent format.\n\u2022 Portability: DSDL ensures high portability of dataset descriptions, allowing description files to be used across different systems and environments without modification, which is crucial for fostering a thriving development ecosystem.\n\u2022 Extensibility: DSDL supports the expansion of dataset descriptions without altering the core standards. Its scope can be extended through libraries or packages while maintaining the long-term stability of the core syntax."}, {"title": "3.2 DSDL User Base", "content": "DSDL serves AI researchers and developers at various levels:\nBeginners in AI can quickly understand and utilize mainstream datasets without needing to delve into the complexities of data formats and content, as DSDL provides clear metadata and annotation details.\nResearchers in specific fields benefit from a unified dataset function interface provided by DSDL, simplifying the use of multiple datasets.\nResearchers of large models can efficiently combine and process vast amounts of data across different tasks and modalities, accelerating the training and validation processes of models."}, {"title": "3.3 DSDL Standardized Datasets", "content": "We have standardized over 100 mainstream datasets, which are readily available for download on the OpenDataLab platform\u00b3.\nUsers can easily visualize datasets and explore specific examples, as detailed in the online tutorial.4. They can also train and deploy models using pre-configured files, significantly simplifying the process and lowering the barriers to entry, as shown in another tutorial.5 Additionally, users have the flexibility to combine and utilize standardized datasets as needed."}, {"title": "4 Open Data Platform and Data Toolkits", "content": ""}, {"title": "4.1 Platform Features", "content": "Search and Explore\nOpenDataLab provides not only an array of open data resources but also flexible and multi-dimensional dataset retrieval methods, catering to diverse search scenarios. Users can use the global search box for fuzzy searches across various dimensions and filter datasets by data type, label type, task type, and topics. This functionality enables users to identify similar datasets, fostering a more comprehensive collection. The platform also facilitates the discovery of the most recent datasets by offering sorting options based on popularity and update time. Additionally, a random recommendation function is available for broader exploration scenarios.\nPreview and Describe\nOpenDataLab employs Data Cards, comprising a README and a Metafile, and a standardized DSDL for dataset definition. These components allow users to grasp the dataset's essentials, facilitating informed decisions regarding its applicability.\nOpenDataLab offers an extensive array of data visualization techniques to support visual tasks such as image classification, segmentation, object detection, tracking, and OCR recognition. Additionally, it facilitates statistical and analytical visualization of distribution metrics, including file format, size, and resolution. This comprehensive approach provides users with an intuitive understanding of dataset attributes.\nHost and Download\nOpenDataLab offers free dataset hosting with multiple download options. Users can upload dataset files via a web browser or use the platform's Command Line Interface (CLI) or Python SDK. Datasets can be downloaded using web browser or code tools by network. Users are encouraged to identify the required dataset and verify its suitability through meta-information, introductory details, and visual examples which prevents the time-consuming download of large-scale data."}, {"title": "4.2 OpenDataLab CLI and SDK", "content": "OpenDataLab CLI and SDK are provided to platform users as sub-functions of the OpenXLab [1] dataset module. OpenDataLab CLI and SDK provide users with different usage methods, including search, query, view details, create data sets and data download functions, and support users to contribute data sets, search and download data resources.\nCommand Line Interface for Datasets\nThe OpenDataLab CLI offers command line tools for dataset publishers and users, with the command line structure specified as follows:\nopenxlab dataset <subcommand > [options and parameters]\n\u2022 <openxlab dataset>: refers to the name of the CLI tool of OpenDataLab.\n\u2022 <subcommand>: Specify the additional sub command to perform the operation, such as get or download.\n\u2022 <options and parameters>: Specify options or API parameter options used to control CLI behavior. The option values can be numbers, strings, JSON structure, etc.\nThe OpenDataLab CLI command line tool includes several functions that support users in hosting, listing, downloading, and managing datasets on OpenDataLab.\nPython SDK for Datasets\nThe OpenDataLab Python SDK offers users a programmatic approach to managing and operating datasets, enhancing flexibility across various system environments and data usage contexts. The features provided by the OpenDataLab Python SDK are nearly identical to those of the CLI, thereby simplifying the management and utilization of datasets."}, {"title": "4.3 Data Labeling Tools", "content": "LabelU - Labeling tools for ML\nLabelU is a data labeling tool library specially designed for the field of artificial intelligence and machine learning. It encapsulates over 15 labeling tools for image, audio, and video tasks.\nLabelU's image tools enable the labeling of boxes, points, lines, polygons, and 3D boxes. Its video and audio tools provide support for segmentation, classification, and transcription, thereby facilitating the precise processing of complex visual and auditory information.\nLabelU offers customizable tool configurations, allowing a flexible combination of multiple tools to suit diverse user needs. With its open-source code and local installation support, LabelU fosters convenient, precise, and efficient data labeling.\nLabelLLM - Labeling tools for AGI\nLabelLLM is an intelligent dialogue labeling tool designed to provide high-quality annotated data for large models. It offers capabilities for labeling and reviewing conversation data, thereby enabling precise processing of NLP and multi-modal data. The platform includes tools for classifying and annotating entire conversations or individual questions, as well as for sequencing each response within a conversation.\nLabelLLM allows for flexible tool combinations and task parameter configurations. It is applicable in a variety of contexts, such as question-answer collection, preference gathering, and dialogue evaluation necessary for large models. It provides an efficient toolkits for data preparation in the fine-tuning and RLHF stages of large model development."}, {"title": "5 Use Case", "content": "Using the object detection task as an illustrative example, we demonstrate the convenience offered by OpenDataLab throughout the entire research and development process, as depicted in the Figure 7."}, {"title": "5.1 Dataset Download", "content": "To train an object detection model, the initial step involves acquiring the relevant data. OpenDataLab streamlines this process, enabling users to effortlessly query, select, and download the desired dataset."}, {"title": "5.2 Dataset Merge and Model Training", "content": "The PASCAL VOC2007 and COCO2017 datasets can be downloaded using above command. However, their original data formats are not consistent, can not direct merging. We shall use DSDL to standardized the datasets for combined training. Our support extends to Pytorch training and the MMDetection framework by OpenMMLab7, with detailed usage instructions to follow.\nPytorch Implementation (Merge by SDK)\nFor Pytorch users, using the merged data set to train the model can directly merge the data through the SDK. Eliminate a lot of handwritten code alignment and merging process.\n\n\n\n\n\n\n\n# Your Traning Code"}, {"title": "5.3 Visualization and Labeling", "content": "LabelU equips users with tools for visual data analysis and label modification, fostering clear visualization of raw images and detailed annotations, and enabling customization of labeling tasks based on user requirements.\nConsequently, OpenDataLab provides an integrated solution for data download, training, and modification, thereby augmenting the efficiency and effectiveness of AI research."}, {"title": "6 Related Work", "content": ""}, {"title": "6.1 Unified Data Standards", "content": "Deep Lake[2] is a data lake optimized for deep learning, capable of interfacing with various storage systems and enhancing data operations with its advanced query language. It streamlines data handling for machine learning with features like version control, efficient data streaming, and a visualizer for large datasets.\nHugging Face[7] is a hub for accessing and sharing a wide array of datasets tailored for natural language processing tasks. It simplifies the process of finding, using, and contributing to datasets, offering tools for easy dataset manipulation and collaboration within the NLP community. While platforms like Hugging Face Data provide an extensive collection of datasets for natural language processing tasks, they do not address the unification of AI data formats. A standardized specification that enables interoperability among various tasks and even different modalities is crucial for advancing multimodal research.\nData Set Description Language (DSDL) is designed precisely for this purpose. DSDL not only standardizes diverse data types through a unified specification but also boasts robust language extensibility, progressively encompassing a broader spectrum of AI data types. This approach lays a solid foundation for multimodal AI research by facilitating interoperability and interconnectivity of data."}, {"title": "6.2 Data Platform", "content": "Data Tools The advancement of artificial intelligence has prompted an influx of open-source labeling tools, exhibiting a broad spectrum but often constrained by their application range, tool functionality, and interoperability. These constraints impede their adaptability to the swiftly progressing AI sector and its diverse needs. For instance, CVAT[5], an Opencv's web-based image labeling tool, excels in various image annotations but lacks in text and audio data support. Likewise, Labelme[13], a graphical interface CV annotation software from MIT, primarily supports single-task annotations, lacking multi-task integration.\nThe onset of the large model era has amplified the demand for fine-tuning and RLHF, necessitating tools capable of handling dialogue labeling and configuration scenarios. The existing tool, Open Assistant[10], is an open-source large model labeling tool. The functions of Open Assistant include marking prompts, adding reply messages, and editing assistant replies. However, its functionality is confined to text types, and it lacks in multi-modal data comprehension and task configuration capabilities, rendering it ill-equipped for the diverse multi-modal scenarios demanded by the large model industry.\nCommand Line Interface for Datasets\nThe Well-known open data platforms at home and abroad were selected to compare OpenData-Lab, Kaggle[9], Huggingface, Paperwithcode[3], ModelScop[1], and BAAI[12] from three aspects: platform functions, platform content, and platform standardization.\nEstablished in 2010, Kaggle is a globally recognized data science competition platform offering numerous public datasets and fostering community discussions. However, its scope of visual, audio data, and large model-related data remains limited. Hugging Face, introduced in 2016 amidst the rise of deep learning, is an open platform specializing in natural language processing. It houses a plethora of open-source models and diverse linguistic data, but lacks in other modalities besides text.\nThe growth of artificial intelligence has sparked the emergence of open AI platforms like ModelScope and BAAI. ModelScope, a product of Alibaba Damo Academy, operates as a model-as-a-service platform, offering developers comprehensive model services and large model data hosting. BAAI, under the Beijing Zhiyuan Artificial Intelligence Research Institute, serves as a data platform. However, both platforms are limited in data scale and richness of data information details.\nOpenDataLab, established in 2022, is an open AI data platform providing a superior data exploration and download experience. It features comprehensive search, filtering, and online visualization of media data annotation information, complemented by a user-friendly GUI design."}, {"title": "7 Conclusion", "content": "OpenDataLab, an open data platform, is committed to providing high-quality datasets for artificial intelligence, fostering data sharing, collaboration, and comprehensive support for global AI initiatives.\nIt supplies a broad spectrum of training data, pertinent to a range of models including language, multi-modal, video, three-dimensional, and speech. The data finds extensive applications across various industries such as autonomous driving, smart healthcare, and digital cities.\nOpenDataLab has introduced a Data Set Description Language (DSDL), enabling a unified representation of datasets across various formats and modalities. OpenDataLab also develops intelligent labeling tools and extensive multi-modal datasets to facilitate efficient cross-modal and cross-task data acquisition, labeling, and modification, thus enhancing data preparation efficiency by 30%.\nOpenDataLab aims to pioneer a new paradigm for data preparation in the AI industry, thereby catalyzing the progression of a new era of large-scale models."}]}