{"title": "Diffusion Models for Molecules: A Survey of Methods and Tasks", "authors": ["Liang Wang", "Chao Song", "Zhiyuan Liu", "Yu Rong", "Qiang Liu", "Shu Wu", "Liang Wang"], "abstract": "Generative tasks about molecules, including but not limited to molecule generation, are crucial for drug discovery and material design, and have consistently attracted significant attention. In recent years, diffusion models have emerged as an impressive class of deep generative models, sparking extensive research and leading to numerous studies on their application to molecular generative tasks. Despite the proliferation of related work, there remains a notable lack of up-to-date and systematic surveys in this area. Particularly, due to the diversity of diffusion model formulations, molecular data modalities, and generative task types, the research landscape is challenging to navigate, hindering understanding and limiting the area's growth. To address this, this paper conducts a comprehensive survey of diffusion model-based molecular generative methods. We systematically review the research from the perspectives of methodological formulations, data modalities, and task types, offering a novel taxonomy. This survey aims to facilitate understanding and further flourishing development in this area.", "sections": [{"title": "1 Introduction", "content": "Molecular generative tasks are critical for drug discovery and material design [S\u00e1nchez-Lengeling and Aspuru-Guzik, 2018; Du et al., 2022, 2024]. The ability to generate novel molecules with specific desired properties can significantly expedite the development of new pharmaceuticals and advanced materials, thereby addressing pressing challenges in healthcare and technology. However, traditional methods for these tasks are often labor intensive and time-consuming.\nDeep generative models, such as Variational Autoencoders (VAEs) [Kingma and Welling, 2014], Generative Adversarial Networks (GANs) [Goodfellow et al., 2014], Autoregressive models (ARs), and Normalizing Flows (NFs) [Rezende and Mohamed, 2015], have opened new avenues for automating molecular generative tasks, gaining prominence for their ability to explore vast chemical spaces efficiently. These models enhance the speed and accuracy of molecular discovery, making them indispensable tools in modern scientific research.\nDiffusion models [Sohl-Dickstein et al., 2015; Ho et al., 2020] have recently emerged as powerful generative models, showcasing remarkable performance in generating high-quality data across various domains. These models operate by simulating the gradual degradation of a data distribution and learning its reverse process to generate new samples [Luo, 2022]. Originally introduced for visual domains [Croitoru et al., 2022], these models excel at capturing data distributions through iterative processes. Their success in visual domains has inspired researchers to explore their potential for molecular generative tasks. By effectively modeling intricate molecular structures and properties, diffusion models have become central to molecular design. This has sparked a surge in research adapting diffusion models for molecular applications, highlighting their transformative potential in this area [Jo et al., 2022; Hoogeboom et al., 2022].\nDespite the rapid advancements and the proliferation of research on diffusion model-based molecular design, the area faces significant challenges. The diversity in diffusion model formulations, molecular data modalities, and generative task types has resulted in a fragmented research landscape. This diversity makes it difficult for researchers to navigate existing studies, hindering a comprehensive understanding of the area's progress and potential. The lack of a systematic and up-to-date survey exacerbates this issue, as researchers struggle to keep pace with the latest developments. This fragmentation not only limits the accessibility of existing advances, but also constrains the area's growth and innovation.\nTo address these challenges, this paper presents an comprehensive survey of diffusion model-based molecular generative methods. We provide a comprehensive and systematic review of the area, elucidating the design space of existing works according to method formulations, data modalities, and task types. By offering this novel taxonomy, we clarify the research landscape and facilitate easier navigation for researchers.\nThis survey seeks to bridge the gap between diverse studies, promoting a more cohesive understanding of the area and supporting its further development. By highlighting key advancements and identifying emerging opportunities for future research, we hope to contribute to the area's ongoing evolution and encourage future innovations.\nThis survey makes several key contributions to the area of molecular generative tasks using diffusion models:\n\u2022 We provide an up-to-date and systematic overview of the current state of research, addressing the need for a comprehensive understanding of the area.\n\u2022 We introduce a novel taxonomy that categorizes research efforts based on method formulations, data modalities, and task types, offering a structured framework.\n\u2022 By identifying opportunities in the existing literature, we pave the way for future directions, encouraging further innovation in diffusion model-based molecular design."}, {"title": "2 Formulations of Diffusion Models", "content": "This fundamental framework of diffusion models comprises two parts: the forward diffusion process and the reverse generation process, as shown in Figure 2. In the forward process, the model progressively adds noise to real data, eventually approaching a simple prior distribution. In the reverse process, the model learns to progressively restore the data distribution from noise. The reverse process is typically parameterized using neural networks.\nThis framework can be formulated in various ways, such as the Denoising Diffusion Probabilistic Models [Ho et al., 2020], Score Matching with Langevin Dynamics [Song and Ermon, 2019], the generalized Stochastic Differential Equations [Song et al., 2021], and other variants."}, {"title": "2.1 Denoising Diffusion Probabilistic Models (DDPMS)", "content": "DDPM [Sohl-Dickstein et al., 2015; Ho et al., 2020] is a classical diffusion model that performs step-by-step denoising using a fixed noise schedule. It employs two Markov chains for the forward and reverse processes.\nStarting with the original noise-free data xo, the forward process transforms it into a sequence of noisy data X1, X2, ..., XT using a forward transition kernel:\n$q(x_t|x_{t-1}) = \\mathcal{N} (x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I),$\nwhere $\\alpha_t \\in (0,1)$ for $t = 1,2,..., T$ are hyperparameters that define the noise ratio at each step. $\\mathcal{N}(x; \\mu, \\Sigma)$ denotes a Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$. A useful property of this Gaussian transition kernel is that $x_t$ can be directly derived from $x_0$ by:\n$q(x_t|x_0) = \\mathcal{N} (x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I),$\nwhere $\\bar{\\alpha}_t := \\prod_{i=1}^{t} \\alpha_i$. Thus, $x_t$ is given by $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, I)$. Typically, we set $\\bar{\\alpha}_T \\approx 0$ so that $q(x_T) \\approx \\mathcal{N}(x_T; 0, I)$, allowing the reverse diffusion process to start from a random Gaussian noise.\nThe reverse transition kernel is parameterized by the neural networks $\\mu_\\theta$ and $\\Sigma_\\theta$:\n$p_\\theta(x_{t-1}|x_t) = \\mathcal{N} (x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)),$\nwhere $\\theta$ denotes $p_\\theta$'s learnable parameters. The goal is to maximize the likelihood of the training sample $x_0$ by optimizing $p_\\theta(x_0)$. This is achieved by minimizing the variational lower bound of the negative log-likelihood $\\mathbb{E}[-\\text{log }p_\\theta(X_0)]$.\nDDPM simplifies the covariance matrix $\\Sigma_\\theta$ in Eq. (3) to a constant-scaled matrix $\\beta_tI$, where $\\beta_t = \\frac{1 - \\alpha_{t-1}}{1 - \\alpha_t}(1-\\alpha_t)$ varies across each step to control noise. Additionally, the mean $\\mu$ in Eq. (3) is expressed as a function of a learnable noise term:\n$\\mu_\\theta(x, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\bigg( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\bigg),$\nwhere $\\epsilon_\\theta$ is a network that predicts noise $\\epsilon$ for $x_t$ and $t$. According to the property in Eq. (2) and discarding the weight, DDPM simplifies the objective function to:\n$\\mathbb{E}_{t,x_0,\\epsilon} [||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)||^2].$\nEventually, samples are generated by removing noise from $x_T \\sim \\mathcal{N}(x_T; 0, I)$. Specifically, for $t = T, T - 1, ..., 1,$\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\bigg( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\bigg) + \\sigma_t z,$\nwhere $z \\sim \\mathcal{N}(0, I)$ for $t = T, ..., 2$, and $z = 0$ for $t = 1$.\nDDPM has been widely applied in generating molecules. Considering that DDPM is designed based on continuous data space, it is more commonly used for generating continuous 3D molecular structures, such as in EDM [Hoogeboom et al., 2022] and GeoDiff [Xu et al., 2022]. For discrete 2D molecular structures, the discrete version of DDPM, Discrete Denoising Diffusion Probabilistic Model (D3PM), is typically employed, as exemplified by DiGress [Vignac et al., 2023a]."}, {"title": "2.2 Score Matching with Langevin Dynamics (SMLDS)", "content": "SMLD [Song and Ermon, 2019], also known as score-based generative model (SGM), uses score matching theory to learn the score function of the data distribution, and combines it with Langevin dynamics for sampling. It comprises two main components: score matching and annealed Langevin dynamics (ALD). ALD generates samples iteratively using Langevin Monte Carlo, relying on the Stein score of a density function $q(x)$, defined as $\\nabla_x \\text{log }q(x)$. Since the true distribution $q(x)$ is often unknown, score matching [Hyv\u00e4rinen, 2005] approximates the Stein score with a neural network.\nFor efficiency, variants of score matching, such as denoising score matching [Vincent, 2011], are often used in practice. Denoising score matching processes observed data using the forward transition kernel $q(x_t|x_0) = \\mathcal{N}(x_t; x_0, \\sigma_t^2I)$, where $\\sigma_t$ is a seiries of increasing noise levels for $t = 1, ..., T$, and then jointly estimates the Stein scores for the noise density functions $q_{\\sigma_1}(x), q_{\\sigma_2}(x), ..., q_{\\sigma_T}(x)$. The Stein score is approximated by a neural network $s_\\theta(x, t)$ with $\\theta$ as its learnable parameters. Thus, the objective function is defined as follows:\n$\\mathbb{E}_{t,x_0,x_t} [||s_\\theta(x_t, t) - \\nabla_{x_t} \\text{log }q_{\\sigma_t}(x_t)||^2].$\nWith the Gaussian assumption of the forward transition kernel, the objective function can be rewritten as a tractable version:\n$\\mathbb{E}_{t,x_0,x_t} \\bigg[\\delta(t) \\bigg||s_\\theta(x_t, t) + \\frac{x_t - x_0}{\\sigma_t^2} \\bigg||^2\\bigg],$\nwhere $\\delta(t)$ is a positive weight that depends on the noise scale $\\sigma_t$. Once the score-matching network $s_\\theta$ is trained, the ALD algorithm is used for sampling. It begins with a sequence of increasing noise levels $\\sigma_1, ..., \\sigma_T$ and an starting point $x_{T,0} \\sim \\mathcal{N}(0,I)$. For $t = T,T - 1,...,0$, $x_t$ is updated through N iterations that compute the following steps:\n$x_{t,n} = x_{t,n-1} + \\frac{\\eta_t}{2}s_\\theta(x_{t,n-1},t) + \\sqrt{\\eta_t}z,$\nwhere $z \\sim \\mathcal{N}(0, I)$, $n = 1, . . ., N$, and $\\eta_t$ is the update step. After N iterations, the resulting $x_{t,N}$ becomes the starting point for the next N iterations. $x_{0, N}$ will be final sample.\nSMLD has also been applied to generating molecules, such as in MDM [Huang et al., 2023b] and LDM-3DG [You et al., 2024]. Subsequent studies demonstrat that SMLD and DDPM are theoretically equivalent and can both be regarded as discretizations of the SDE introduced in the next subsection."}, {"title": "2.3 Stochastic Differential Equations (SDEs)", "content": "Both DDPM and SMLD rely on discrete processes, requiring careful design of diffusion steps. Song et al. [2021] formulate the forward process as an SDE, extending the discrete methods to continuous time space. The reverse process is modeled as a time-reverse SDE, enabling sampling by solving it. Let w and \\bar{w} denote a standard Wiener process and its time-reverse, with continuous diffusion time $t \\in [0, T]$. A general SDE is:\n$dx = f(x,t)dt + g(t)dw,$\nwhere $f(x, t)$ and $g(t)$ are the drift coefficient and the diffusion coefficient for the diffusion process, respectively.\nThe corresponding time-reverse SDE is defined as:\n$dx = [f(x,t) - g(t)^2\\nabla_x \\text{log } q_t(x)] dt + g(t)d\\bar{w}.$\nSampling from the probability flow ordinary differential equation (ODE) has the same distribution as the time-reverse SDE:\n$dx = \\bigg[f(x,t) - \\frac{1}{2}g(t)^2\\nabla_x \\text{log } q_t(x)\\bigg] dt.$\nHere $\\nabla_x \\text{log } q_t(x)$ is the Stein score of the marginal distribution of $x_t$, which is unknown but can be learned with a similar method as in SMLD with the objective function:\n$\\mathbb{E}_{t,x_0,x} \\bigg[\\delta(t) \\bigg||s_\\theta(x, t) - \\nabla_{x_t} \\text{log } q_{\\sigma_t}(x_t|x_0)\\bigg||^2\\bigg].$\nDDPM and SMLD can be regarded as discretizations of two SDEs. Recall that $\\alpha_t$ is a defined in DDPM and $\\sigma_t$ denotes the noise level in SMLD. The SDE corresponding to DDPM is known as variance preserving (VP) SDE, defined as:\n$dx = -\\frac{1}{2}\\alpha(t)xdt + \\sqrt{\\alpha(t)}dw,$\nwhere $\\alpha(\\cdot)$ is a continuous function, and $\\alpha(t) = T(1 - \\alpha_t)$ as $T\\rightarrow\\infty$. For the forward process of SMLD, the associated SDE is known as variance exploding (VE) SDE, defined as:\n$dx = \\frac{d[\\sigma(t)^2]}{dt} dw,$\nwhere $\\sigma(\\cdot)$ is a continuous function, and $\\sigma(t) = \\sigma_T$ as $T\\rightarrow\\infty$. Inspired by VP SDE, sub-VP SDE is designed and performs especially well on likelihoods, given by:\n$dx = -\\frac{1}{2}\\alpha(t)xdt + \\alpha(t)\\sqrt{(1 - e^{-2\\int_0^t \\alpha(s)ds})} dw.$\nThe objective function in Eq. (13) involves a perturbation distribution $q_{\\sigma_t}(x_t|x_0)$ that varies for different SDEs (i.e., VP SDE, VE SDE, sub-VP SDE). After $s_\\theta(x, t)$ is trained, samples can be generated by solving the time-reverse SDE or the probability flow ODE with techniques such as ALD.\nBecause SDEs provide a continuous and flexible formulation that allows for improved control over generation processes, they have gradually replaced discrete-time formulations like DDPM and SMLD in molecular generative tasks [Jo et al., 2022; Bao et al., 2023; Huang et al., 2024a]."}, {"title": "2.4 More Variants", "content": "These three formulations establish the theoretical foundation of diffusion models and demonstrate excellent performance in generative tasks. Building on them, diffusion models have spawned many variants and extensions aimed at enhancing generation efficiency or expanding application scenarios. For example, Discrete Denoising Diffusion Probabilistic Models (D3PMs) [Austin et al., 2021] extend the DDPM to discrete data space, such as text or graphs. Latent Diffusion Models (LDMs) [Rombach et al., 2022] perform the diffusion process in latent space, significantly reducing computational complexity while maintaining generation quality. Consistency Models (CMs) [Song et al., 2023] focus on learning a single-step mapping from noise to data, enabling fast and high-quality sampling while maintaining consistency with the underlying data distribution. Diffusion Bridges (DBs) [Bortoli et al., 2021; Chen et al., 2022; Zhou et al., 2024a] extend diffusion models for generative tasks that connect different distributions, enabling efficient generation from one distribution to another.\nThese formulations propose innovative solutions tailored to different tasks, driving the widespread application in multi-modal generative tasks such as image, text, video, and graph [Yang et al., 2022; Liu et al., 2023a; Zhang et al., 2023a]."}, {"title": "3 Molecular Data Modalities", "content": "In molecular generative modeling, molecular data can be featurized in various modalities. The primary modalities are graphs in 2D topological space and conformations in 3D geometric space. In 2D graphs, nodes denote atoms, and edges represent bonds. In 3D conformations, the molecule is described as a point cloud, where the points are atoms with positional coordinates. Molecules can also be featurized in other ways, such as SMILES in 1D space and molecular descriptors. However, these featurizations are typically not generated using diffusion models and thus fall outside the scope of this survey.\nDifferent molecular generative methods often target distinct modalities, resulting in different advantages. As illustrated in Figure 3 and Table 1, existing methods are categorized into three groups based on the modality of generated molecular data: those generating molecules only in 2D space, only in 3D space, and in 2D and 3D joint space."}, {"title": "3.1 Generating Molecules in 2D Topological Space", "content": "Definition 1: 2D molecular graph. The molecular graphs have categorical node and edge types, represented by the spaces X and E, respectively, with cardinalities a and b. A molecular graph can be denoted as $M_{2D} = (X, A)$, where $X \\in \\mathbb{R}^{n \\times a}$ is the one-hot encoded atom feature matrix representing atom types, and $A \\in \\mathbb{R}^{n \\times n \\times b}$ is the one-hot encoded adjacency matrix indicating bond existence and bond types. Here, n denotes the number of atoms.\nThe primary challenges in generating molecular graphs is maintaining permutation invariance and modeling the complex dependencies between nodes (atoms) and edges (bonds). To address these challenges, graph neural networks (GNNs) are commonly employed as the backbone architecture. As illustrated in Figure 3, several methods, such as GDSS [Jo et al., 2022], CDGS [Huang et al., 2023a], and MOOD [Lee et al., 2023], directly apply diffusion models from continuous data spaces to the task of molecular graph generation.\nHowever, the discrete nature of atom and bond types presents further challenges. Recognizing this, DiGress [Vignac et al., 2023a] employs a discrete diffusion model, D3PM [Austin et al., 2021], specifically designed for discrete data spaces, to model molecular graphs. On smaller molecules, discrete models like DiGress achieve results comparable to continuous models but offer the advantage of faster training.\nThe drawback of generating molecules in 2D space is that it only produces atom types and binding topology, lacking 3D geometric structure. Molecules inherently exist in 3D space, and their 3D structure affects their quantum properties. This makes it unsuitable for quantum-property-based or structure-based drug design, which rely heavily on 3D structures.\nIn conclusion, while 2D molecular graph generation provides a useful framework for certain applications, its limitations highlight the necessity for further research into methods that can integrate 3D structural data to enhance the accuracy and applicability of molecular models in real-world scenarios."}, {"title": "3.2 Generating Molecules in 3D Geometric Space", "content": "Definition 2: 3D molecular conformation. A 3D molecular conformation is formally defined as $M_{3D} = (X, P)$, where $X \\in \\mathbb{R}^{n \\times a}$ is the one-hot encoded atom feature matrix representing atom types, $P \\in \\mathbb{R}^{n \\times 3}$ is the position coordinate matrix of atoms, and n denotes the number of atoms.\nMethods for generating molecules in 3D space focus on producing both the atom types and their geometric structures,"}, {"title": "3.3 Generating Molecules in 2D&3D Joint Space", "content": "The generation of molecular structures in 2D and 3D joint spaces offers a comprehensive approach to capturing both the topological and geometric properties of molecules. This dual featurization, also referred to as a complete molecular structure in some works [Huang et al., 2024a; Hua et al., 2023], is crucial for accurately depicting molecules.\nDefinition 3: Complete molecular structure. A molecule in this joint space is defined as $M = (X, A, P)$, where $X \\in \\mathbb{R}^{n \\times a}$ is the atom feature matrix, $A \\in \\mathbb{R}^{n \\times n \\times b}$ is the one-hot encoded adjacency matrix indicating bond existence and bond types, and $P \\in \\mathbb{R}^{n \\times 3}$ is the position coordinate matrix of the atoms. Here, n denotes the number of atoms in the molecule."}, {"title": "4 Molecular Generative Tasks", "content": "In this section, we explore various molecular generative tasks that leverage diffusion models, as outlined in Figure 3. These tasks are crucial for advancing molecular design and discovery, providing innovative solutions across different domains."}, {"title": "4.1 De Novo Generation", "content": "De novo generation involves creating novel molecular structures from scratch. This approach is essential for discovering new compounds without relying on existing molecular templates. It includes two main sub-tasks: unconditional and conditional generation.\nUnconditional generation. Unconditional generation focuses on producing molecules without specific constraints. Starting from a random noise vector, these models generate entirely new molecular structures, exploring the vast chemical space for novel applications.\nConditional Generation Conditional generation tailors molecule creation based on specific conditions, such as desired properties, targets, or fragments, allowing for more directed and efficient molecular design. By incorporating these conditions, diffusion models produce molecules that meet predefined criteria. Existing works can be further divided into four categories based on the type of conditions applied.\nProperty-based molecular generation, also known as inverse molecule design, aims to generate molecules with desired properties such as bioactivity and synthesizability. More specifically, the inverse molecule design can be further divided into single-property conditioning [Huang et al., 2023a; Hoogeboom et al., 2022; Xu et al., 2023; Huang et al., 2023b] and multiple-property conditioning [Vignac et al., 2023a; Liu et al., 2024; Bao et al., 2023]. Among them, MOOD [Lee et al., 2023] and CGD [Klarner et al., 2024] also focus on generating structurally novel molecules outside the training distribution, referred to as OOD molecule generation.\nTarget-based molecular generation, also known as structure-based drug design (SBDD), generates molecules based on the 3D structure of target binding pockets, aiming to enhance interaction with specific targets. IRDiff [Huang et al., 2024e] introduces interaction-based retrieval to generate target-specific molecules based on retrieved high-affinity ligand references.\nFragment-based molecular generation specifies the generation of molecules with particular fragments. DiffLinker [Igashov et al., 2024] focuses on linker design, generating linkers that connect fragments into a complete molecule.\nComposition-based molecular generation restricts the elemental composition of generated molecules, ensuring they meet specific compositional criteria [Yang et al., 2024]."}, {"title": "4.2 Molecular Optimization", "content": "Molecular optimization tasks aim to improve existing molecules for better performance or properties, differing from de novo generation by focusing on modifying known structures rather than creating new ones. Starting with an existing molecule, these tasks refine it to enhance its properties or performance. This task includes scaffold hopping, R-group design, and generalized optimization.\nScaffold hopping involves modifying molecular scaffolds to discover new compounds, transforming known scaffolds into new ones that retain biological activity [Torge et al., 2023].\nR-group design focuses on optimizing molecules by fine-tuning the properties of lead compounds through the adjustment of specific R-groups [Zhou et al., 2024b].\nGeneralized optimization is a flexible approach to optimize molecules without being restricted to specific strategies like scaffold hopping or R-group design. DiffSBDD [Schneuing et al., 2024] allows for a broader range of structural changes, as long as the optimized molecule maintains a certain level of similarity to the original structure. This flexibility enables the exploration of diverse pathways to improve properties."}, {"title": "4.3 Conformer Generation", "content": "Conformer generation involves predicting the 3D conformers of a molecule based on its 2D topological structure. This task is crucial for understanding the spatial arrangement of atoms within a molecule, which is essential for predicting molecular interactions, reactivity, and properties. Generated 3D conformers reflect the molecule's potential energy landscape and geometric constraints. GeoDiff [Xu et al., 2022] and Torsional Diffusion [Jing et al., 2022] employ diffusion models to generate 3D molecular conformers in Cartesian space and torsion angle space, respectively. DiSCO [Lee et al., 2024] further optimizes the predicted conformers with Diffusion Bridge."}, {"title": "4.4 Molecular Docking", "content": "Molecular docking tasks involve predicting how molecules interact with biological targets, a key step in drug discovery for assessing binding affinity and specificity. By analyzing a molecule and a target structure, DiffDock [Corso et al., 2023] predicts the binding pose with the diffusion model. Re-Dock [Huang et al., 2024c] further utilizes the diffusion bridge for flexible and realistic molecular docking, which predicts the binding poses of ligands and pocket sidechains simultaneously."}, {"title": "4.5 Transition State Generation", "content": "Transition state generation focuses on predicting the 3D structure of transition states in chemical reactions, using the reactants and products as inputs. This task is vital for understanding reaction mechanisms, as the transition state represents the highest energy point along the reaction pathway. Accurate modeling of these states provides insights into reaction kinetics and can aid in the design of catalysts and optimization of reaction conditions [Duan et al., 2023]."}, {"title": "5 Discussion and Future Direction", "content": "In this section, we discuss the current state and challenges in diffusion models for molecules and outline several promising directions for future research to advance this area.\nComplete data modality. Most existing works fall under the category of generating molecules in 3D space, neglecting 2D topology. Considering the complementary nature of 2D and 3D structures, generating molecules in a joint 2D and 3D space holds significant potential for producing more realistic molecules. This approach has proven effective in de novo generation [Hua et al., 2023; Huang et al., 2024a], but its broader potential in other generative tasks remains underexplored.\nSophisticated diffusion models. As summarized in Table 1, the diffusion models employed in existing works exhibit a wide variety of formulations. Regarding the time space, there is a shift from discrete-time methods to more generalized continuous-time SDEs. In terms of the data space, an open challenge lies in handling the discrete molecular components (e.g., atom and bond types) alongside the continuous components (e.g., coordinates). Moreover, advanced formulations and techniques, such as flow matching and efficient sampling, remain underutilized.\nChallenging generative tasks. Many existing works focus on fundamental tasks, like unconditional or single-conditional generation, with insufficient attention to more practical generative tasks, such as multi-conditional generation, molecular optimization, and docking. Furthermore, poor performance on large molecules in GEOM-Drugs compared to small molecules in QM9, highlights room for improvement. Additionally, extending molecular generation to complex while considering inter-molecular interactions, presents a another promising yet challenging direction.\nExpressive network architectures. Existing methods rely on relatively classical network architectures like EGNNs. Recent advances in more expressive equivariant neural networks offer new opportunities. Incorporating more powerful architectures into molecular diffusion models could further enhance their performance and effectiveness.\nRelationship between molecular generation and molecular representation. With the increasing recognition of diffusion models' ability to learn representations, exploring the relationship between molecular generation and molecular representation based on diffusion models emerges as a promising direction. MoleculeSDE [Liu et al., 2023b], SubgDiff [Zhang et al., 2024], and UniGEM [Feng et al., 2025] mark pioneering steps, but there remains significant room for further research."}, {"title": "6 Conclusion", "content": "In this survey, we provide an systematic overview of diffusion model-based molecular generative methods, addressing the growing need for clarity in this rapidly evolving area. By introducing a novel taxonomy that categorizes research based on method formulations, data modalities, and task types, we offer a structured framework to navigate the research landscape. Furthermore, we identify opportunities in the existing literature, highlighting promising directions for future exploration."}]}