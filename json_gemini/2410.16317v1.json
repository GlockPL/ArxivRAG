{"title": "A Survey on Physical Adversarial Attacks against Face Recognition Systems", "authors": ["Mingsi Wang", "Jiachen Zhou", "Tianlin Li", "Guozhu Meng", "Kai Chen"], "abstract": "As Face Recognition (FR) technology becomes increasingly prevalent in finance, the military, public safety, and everyday life, security concerns have grown substantially. Physical adversarial attacks targeting FR systems in real-world settings have attracted considerable research interest due to their practicality and the severe threats they pose. However, a systematic overview focused on physical adversarial attacks against FR systems is still lacking, hindering an in-depth exploration of the challenges and future directions in this field. In this paper, we bridge this gap by comprehensively collecting and analyzing physical adversarial attack methods targeting FR systems. Specifically, we first investigate the key challenges of physical attacks on FR systems. We then categorize existing physical attacks into three categories based on the physical medium used and summarize how the research in each category has evolved to address these challenges. Furthermore, we review current defense strategies and discuss potential future research directions. Our goal is to provide a fresh, comprehensive, and deep understanding of physical adversarial attacks against FR systems, thereby inspiring relevant research in this area.", "sections": [{"title": "I. INTRODUCTION", "content": "Face Recognition (FR) systems have made significant strides in both performance and scalability, driven by advanced deep learning techniques [1]\u2013[3]. These systems are now integral to various applications, including device unlocking [4], e-banking [5], and military operations [6]. FR systems primarily work by utilizing well-trained FR models to extract facial features from digital images or video frames captured by cameras, enabling the identification and authentication of individuals. However, security concerns have greatly intensified alongside the rapid development of FR technology. Recent studies [7]\u2013[9] have revealed that FR systems are vulnerable to adversarial attacks, where adversarial samples are deliberately crafted to deceive the FR systems.\nAdversarial attacks against FR systems can be categorized into digital and physical attacks based on their domain of execution. Digital adversarial attacks occur after camera capturing and involve directly manipulating digital pixels, typically by adding imperceptible perturbations to face images to generate adversarial samples that deceive the FR model. Numerous studies [8]\u2013[16] have proposed methods for generating subtle yet potent global perturbations across the entire face image. However, these global digital perturbations, while difficult to print and capture accurately, are impractical in real-world scenarios, where attackers are limited to modifying only the face without altering the surrounding background captured by the camera. This limitation has prompted researchers to explore more practical attack strategies in the physical world.\nIn contrast to digital adversarial attacks which perturb pixels globally, physical adversarial attacks focus on objects around faces in the real world, manipulating them before the image is captured by the camera to deceive FR systems without directly interacting with the FR model. Various techniques have been proposed by attackers to introduce localized patches or patterns to the face, such as adversarial stickers [17]\u2013[20], accessories [21]\u2013[24], infrared lights [25]\u2013[27], and illumination [28]\u2013[30]. Despite these advancements, there remains a significant gap in providing a precisely focused overview that clearly clarifies the unique challenges of this field, comprehensively analyzes existing attack methods, and prospectively explores potential future directions. To address this gap, we present a systematic and in-depth survey of current physical adversarial attack methods targeting FR systems in this paper."}, {"title": "A. Motivation", "content": "The main motivations for this survey are as follows:\n(i) FR, a contactless biometric technology widely applied in fields such as finance and public safety, is facing significant security challenges from adversarial attacks. Physical adversarial attacks, which have been proven effective in deceiving face recognition systems [31], are especially concerning in real-world environments, as they can compromise a wide range of already deployed security applications, posing significant risks to practical systems. Therefore, constructing a systematic survey of physical adversarial attacks against FR systems is highly valuable for both academics and industry.\n(ii) Physical adversarial attacks present unique challenges in real-world applications that distinguish them from digital adversarial attacks. Firstly, physical adversarial perturbations require a common real-world medium to function and should appear natural within the surrounding environment when applied. Secondly, ensuring effectiveness through virtual-to-real and real-to-virtual transformations is challenging. Thirdly, the difficulty and cost of implementing such attacks in real-world settings hinder their practicality. Fourthly, optimizing universal perturbations, which attain strong reusability across diverse input samples, remains a significant challenge. Finally, it remains challenging for physical adversarial attacks to achieve strong transferability across various custom-designed, commercial, and confidential FR systems deployed in real-world scenarios with black-box access. Thus, it is crucial to develop a thorough understanding of the challenges posed by the intrinsic attributes of physical adversarial attacks and effective solutions to address them.\n(iii) While numerous studies on physical adversarial attacks against FR systems have been proposed, a systematic overview of existing physical attacks is still needed to summarize current advancements and chart future research directions. Although several related surveys have been published, they do not specifically focus on face recognition systems. Consequently, these surveys lack up-to-date investigations, comprehensive analyses, and in-depth insights specifically examining physical adversarial attacks against FR systems. For instance, surveys like [32]\u2013[38] explore physical adversarial attacks within the broader field of computer vision. The survey [39] examines physical adversarial attacks in the context of surveillance systems. The study [40] investigates the application of physical adversarial attacks in face privacy protection. The work [41], while focused on FR systems, provides limited coverage of physical adversarial attacks and defenses but primarily emphasizes adversarial attacks and defenses in the digital domain. Additionally, while introducing attack and defense methods, this study focuses mainly on listing and categorization rather than a detailed analysis and comparison. Therefore, a more comprehensive and in-depth examination of physical adversarial attacks focusing exclusively on FR systems is needed."}, {"title": "B. Research Questions", "content": "This survey aims to provide an overview of the unique challenges in physical adversarial attacks against FR systems, the corresponding solutions proposed by existing attack methods, current defense mechanisms, and potential future research directions. Specifically, we answer the following questions:\n\u2022 RQ1: What are the unique challenges of physical adversarial attacks on FR systems? (answered in Section III)\n\u2022 RQ2: How do existing attack methods operate and address these inherent challenges? (answered in Section IV)\n\u2022 RQ3: How are current defense mechanisms conducted? (answered in Section V)\n\u2022 RQ4: What are the potential future directions in this field? (answered in Section VI)"}, {"title": "C. Collection Strategy", "content": "In this survey, we concentrate on physical adversarial attacks against FR systems. To gather a broad and representative range of studies, we employed a systematic search strategy using keywords such as \u201cphysical attack\u201d, \u201cphysical adversarial examples\", or \"face recognition\" in Google Scholar. In addition, we manually reviewed the references of all selected papers to ensure comprehensive coverage of the relevant literature. Given the critical importance and high complexity of security issues in this field, related research remains relatively scarce. Consequently, we collect a comprehensive set of 40 relevant papers from authoritative conferences and journals spanning from 2016 to 2024, specifically focusing on physical adversarial attacks in the context of face recognition. Consequently, our study presents a more up-to-date, extensive, and in-depth analysis of the physical adversarial attacks against FR systems, offering crucial insights into this fast-evolving domain.\nContributions. Our contributions are summarized as follows:\n\u2022 We provide a comprehensive review of existing physical adversarial attacks against face recognition systems, highlighting their substantial threat in real-world scenarios.\n\u2022 We explore the differences between physical and digital adversarial attacks based on their workflows and identify the unique challenges posed by physical attacks in terms of their key attributes.\n\u2022 We categorize existing attack methods by their physical medium, analyze how they address the identified challenges, and summarize their strengths, limitations, and practical implications of each category.\n\u2022 We conduct a thorough review of current defenses and discuss promising directions to facilitate future research.\nThe rest of the paper is structured as follows: Section II provides the background knowledge and benchmarks for physical adversarial attacks. Section III outlines the key differences between physical and digital adversarial attacks against FR systems, and reveals the unique challenges posed by physical adversarial attacks. Section IV categorizes existing physical adversarial attacks and summarizes their solutions to address the associated challenges. Section V reviews current defenses against such attacks. Section VI discusses potential future research directions. Finally, Section VII concludes the paper.\""}, {"title": "II. PRELIMINARIES", "content": ""}, {"title": "A. Notations and Terms", "content": "1) Notations: In Table I, we standardize and define the notations used throughout this paper.\n2) Technical terms:\na) Adversary's Knowledge:\n\u2022 White-box attack means the attacker has full access to the model's architecture, parameters, and gradients, enabling precise optimization of adversarial perturbations. However, this assumption is often impractical in real-world scenarios, where such access is typically restricted.\n\u2022 Black-box attack refers to a situation where the attacker can only query the target model using input samples and observe the outputs, without any internal access. This setup more closely reflects real-world conditions and presents greater challenges. Attackers often rely on transferring adversarial perturbations from a white-box surrogate model to execute attacks via transferability.\nb) Adversarial Specificity:\n\u2022 Targeted attack (impersonation attack) generates perturbations by minimizing the distance between the features of the original and target image, causing misclassification as a specific individual (e.g., an authorized user), potentially leading to identity theft or unauthorized access.\n\u2022 Non-targeted attack (dodging attack) evades recognition by maximizing the distance between the image's features and the correct classification boundary, inducing misclassification to arbitrary identity except the correct one.\nc) Perturbation Universality:\n\u2022 Universal Attack generates perturbations that can mislead diverse inputs, offering broad applicability. However, these universal adversarial perturbations are hard to optimize, resulting in a lower attack success rate.\n\u2022 Individual Attack generates unique perturbations for each input, making them highly effective for the corresponding sample. The main drawback is that this approach lacks flexibility and must be repeated for each new input."}, {"title": "B. Background", "content": "1) Face Recognition Systems: As illustrated in Fig. 1, a typical FR system consists of three key components: the camera, pre-processing module, and FR model. Specifically, a camera first captures the person's face in the physical world and converts it into a digital image, which is then processed by a pre-processing module that performs tasks such as denoising, alignment, and normalization to prepare the aligned face image for recognition. Finally, the processed image is input into an FR model, where feature vectors are extracted and used to identify the person based on their high-dimensional representations, completing the identification process.\nAdversarial attacks aim to introduce perturbations to face images, which cause the FR system to misidentify individuals, resulting in either a dodging attack (where no identity is recognized), or an impersonation attack (where the wrong identity is recognized). Digital adversarial attacks primarily target the pre-processing module and the FR model by injecting adversarial perturbations into the digital pixels of the image. In contrast, physical adversarial attacks occur during the interaction between a person and the camera in real-world environments, often carried out utilizing specialized physical mediums, such as adversarial glasses, hats, or stickers.\n2) Digital and Physical Adversarial Attacks: Given a face image x as input, the FR system recognizes its identity by matching features extracted by the feature extractor $f_0$. Digital adversarial attacks introduce perturbations to the pixel values of a digital image, resulting in an adversarial example $x^{adv}$ that appears visually similar to the original image x but leads the FR model to make incorrect predictions, defined as:\n$x^{adv} = x + \\delta$, (1)\nwhere $\\delta$ represents the global adversarial perturbations optimized by various digital attack techniques, such as PGD [42], L-BFGS [7], MI-FGSM [43], C&W [44], and Deepfool [45].\nDue to the difficulty of printing and capturing global perturbations of digital pixels in the physical world, physical attacks apply localized patches or patterns to the image. Such attacks are considered more practical and have attracted increasing attention from researchers. Physical adversarial attacks craft an adversarial sample $x^{adv}$ by adding adversarial perturbation p to the benign image x, causing the FR system to fail in feature matching. $x^{adv}$ is defined as follows, with a binary mask M specifying the location and shape of p:\n$x^{adv} = (1 \u2013 M) \u00b7 x + M \u00b7 p$. (2)\nwhere p denotes an adversarial patch (such as a sticker, mask, hat, or glasses), or an adversarial pattern (like infrared, projection, and light applied to the face) in the physical world."}, {"title": "C. Benchmark", "content": "We outline the benchmark for evaluating physical adversarial attacks against FR systems, including target models, benchmark datasets, evaluation settings in the physical world, and metrics to assess attack performance.\n1) Target Models: The study of security issues tends to lag, with existing physical adversarial attack methods focusing on classic models. Fig. 2 illustrates the evolution and advancements of various architectures in FR models. These developments highlight the growing complexity and performance of 2D FR models, including diverse architectures such as Convolutional Neural Networks (CNNs) like ResNet [72] and Inception [47], lightweight models like MobileNet [54] and MobileFaceNets [58], and advanced frameworks incorporating angular-based losses like ArcFace [57] and CosFace [60]. Moreover, 3D face recognition models such as PointNet [66] and DGCNN [68] improve spatial feature handling.\n2) Datasets: Benchmark datasets are crucial for evaluating the performance and generalization of physical adversarial attacks. As shown in Table II, these datasets are broadly categorized into two types: 2D and 3D face datasets. 2D Face Datasets provide large-scale collections of 2D face images that capture various variations in pose, lighting, expression, and demographics, essential for training and evaluating FR models against physical adversarial attacks. 3D Face Datasets introduce spatial information, enabling more accurate modeling of facial geometry and details like depth, pose, and expressions.\n3) Physical Evaluation Settings: In addition to using the above models and datasets to generate adversarial samples and assess their performance in digital scenarios, it is essential to validate their effectiveness in real-world environments. Physical evaluations are typically conducted in two settings, determined by the level of experimental control: controlled laboratory settings or uncontrolled real-world settings.\nIn laboratory settings, researchers build experimental frameworks to simulate real FR systems, allowing for systematic control of environmental factors such as lighting, camera positions, and distances to evaluate the robustness of physical adversarial attacks [17]\u2013[22], [24], [73]\u2013[91]. Specifically, attackers generate adversarial examples in the digital domain and use high-quality printers like the Canon SELPHY CP1300 or HP DeskJet 2677 to create physical adversarial artifacts, such as adversarial glasses, stickers, or masks. Subjects equipped with these physical artifacts are photographed by cameras such as the Canon EOS 650D, iPhone 11 Pro Max, Intel RealSense D415, or Logitech C270, and the captured images are then input into open-source models or commercial APIs like Face++ [92], Aliyun [93], Microsoft Azure Face API [94], Clarifai [95], Baidu [96] and Tencent [97] for evaluation.\nIn contrast to controlled experimental frameworks in laboratory settings, real-world settings utilize end-to-end FR systems to evaluate the attack robustness in uncontrolled conditions, facing challenges from unpredictable environments such as varying illuminations, dynamic backgrounds, and diverse camera angles [23], [98]\u2013[100]. More precisely, subjects wear adversarial artifacts to test smartphone unlocking features on devices like the iPhone 11, Samsung Galaxy S10, and Xiaomi Redmi K20 Pro, commercial applications with built-in camera modules such as Alipay and B612.\n4) Metrics: Existing studies have proposed various metrics to evaluate the performance of physical adversarial attacks against face recognition systems. Here, we provide detailed formulas and descriptions of commonly used metrics.\n\u2022 Attack Success Rate (ASR) [17]\u2013[22], [28], [30], [74]\u2013[76], [78]\u2013[88], [90], [91], [99]\u2013[102] is the most commonly used metric to evaluate the effectiveness of physical adversarial attacks, which measures the proportion of successful misclassifications induced by adversarial samples on the target model:\n$ASR = \\frac{N_{success}}{N}$, (3)\nwhere $N_{success}$ denotes the number of successful attacks, and N represents the total number of adversarial examples. Similarly, other studies [23], [25], [89], [98] also employ the Recognition Rate (1 - ASR) for evaluation.\n\u2022 Cosine Similarity [22], [24], [27], [29], [73], [76], [77], [83], [86], [98] measures the similarity between the feature vectors of x and the adversarial image $x^{adv}$. In impersonation attacks, x represents the target image $x_t$, where a larger value indicates a more effective attack. While in dodging attacks, x denotes the original image x, with a smaller value indicating a more successful attack:\n$CosineSimilarity = \\frac{f_0(x) \\cdot f_0(x^{adv})}{||f_0(x)|| ||f_0(x^{adv})||}$. (4)\n\u2022 Number of Queries [19], [78], [79] counts the queries made to the target model to generate an adversarial example. This metric is particularly important in black-box attacks, where minimizing the number of queries improves the imperceptibility and efficiency of the attack.\n\u2022 Mean Confidence Score (MCS) [83] quantifies the average confidence of the FR model when classifying adversarial examples, with higher values indicating that the model is overly confident in its incorrect predictions when misled by adversarial perturbations:\n$MCS = \\frac{1}{N} \\sum_{i=1}^N confidence(x^{adv})$. (5)"}, {"title": "III. OVERVIEW", "content": "To address RQ1, we first discuss the differences between physical and digital attacks, and then introduce the unique challenges of physical attacks as identified in the literature."}, {"title": "A. Differences Between Physical and Digital Attacks", "content": "Adversarial attacks are generally categorized into digital and physical types, primarily depending on the environment they target [35], [39], [41]. To fully understand the challenges posed by adversarial attacks against face recognition systems in the physical world, it is essential to understand the core differences between these two types of attacks. Digital attacks, though excel in digital environments, often struggle to maintain their effectiveness in the physical world. Although physical adversarial attacks are more complex to implement, they can be effective in both physical and digital domains.\nThis disparity arises primarily from the differing workflows of physical and digital adversarial attacks. In particular, while digital adversarial attacks follow a \"virtual-to-virtual\" process, physical adversarial attacks involve a \u201cvirtual-to-real-to-virtual\" process, introducing additional information loss.\nSpecifically, in the virtual-to-real phase, attackers transform digital adversarial perturbations into physical mediums through various methods, such as printing adversarial patches or projecting adversarial patterns onto human faces. The technique in which the adversarial physical medium is employed can significantly affect the attack's effectiveness. For instance, a high-resolution color printer can better preserve adversarial information compared to a low-resolution printer, resulting in a more successful attack. The material on which adversarial samples are printed, such as paper or silk, can also impact the attack's effectiveness, depending on the carrier's ability to retain ink. Additionally, the strobe characteristics of projectors can influence how well a camera captures adversarial information when a pattern is projected onto a face.\nIn the real-to-virtual phase, physical adversarial samples are captured by a real-world camera and then transformed into digital images through resampling. The quality of this transformation largely depends on the resolution and capabilities of the camera used; higher-resolution cameras can capture more detail, resulting in less information loss compared to lower-resolution cameras. Moreover, external environmental factors, such as lighting, weather, viewing angles, and the distance between the camera and the target, can introduce noise and artifacts, which may degrade the quality of the captured image and reduce the effectiveness of the adversarial attack.\nTo execute physical attacks on deployed FR systems in the real world, attackers design adversarial perturbations in the form of physical objects, such as masks, eyeglasses, or stickers. These physical adversarial samples are then captured by cameras and translated back into the digital domain for processing. In the following section, we discuss the challenges posed by such a \u201cvirtual-to-real-to-virtual\u201d process."}, {"title": "B. Challenges in Physical Attacks", "content": "Based on the aforementioned differences, physical adversarial attacks rely on physical mediums and exhibit unique characteristics. One basic attribute is imperceptibility. The imperceptibility of physical adversarial samples differs from that of digital samples. In digital attacks, adversarial noise is constrained by a small e-norm, making the adversarial samples nearly indistinguishable from benign examples. In contrast, physical adversarial examples face different constraints, allowing for slight but subtle visual discrepancies, often disguised as everyday items like hats or masks. Therefore, selecting an appropriate physical medium is an important yet challenging factor in enhancing the imperceptibility of such attacks.\nAnother crucial attribute is robustness. Robustness refers to how well physical adversarial samples maintain their attack effectiveness despite varying environmental noise. If physical samples require strict environmental conditions, their applicability and practical value are significantly reduced. Strong robustness ensures that these samples remain effective through virtual-to-real and real-to-virtual transformations, adapting to dynamic environmental changes. Thus, enhancing robustness is a key challenge in the development of physical attacks.\nA third challenge is the complexity and cost associated with producing physical adversarial samples. Complexity refers to the difficulty in generating the optimal physical adversarial perturbations, while cost relates to the financial feasibility of their development. Physical-world attackers utilize various techniques to design adversarial samples, but those that are difficult and expensive to produce tend to be less practical and harder to implement in real-world attack scenarios.\nThe fourth attribute is universality, which refers to the attack's ability to remain effective across diverse input samples. Designing a universal attack that consistently performs well despite variations in input data is a significant challenge, requiring broad applicability and high adaptability.\nThe final attribute is transferability-the ability of physical adversarial samples to succeed in black-box environments. Physical attacks take place in complex, real-world settings, where variations in face recognition system architectures and parameters hinder attack success. Enhancing the black-box capability of these attacks is therefore a critical focus within the exploration of physical adversarial examples."}, {"title": "C. Fundamental Approaches", "content": "To address these challenges, researchers have developed various mediums, such as hats and masks, to execute the attacks. Accordingly, their efforts have focused on optimizing physical adversarial examples designed for these mediums, considering the aforementioned attributes. Two primary strategies are employed to optimize p: the pixel space optimization strategy and the latent space optimization strategy. The pixel space optimization strategy perturbs pixel values to optimize the objective function during the generation of p. The latent space optimization strategy typically refers to the Generative Adversarial Network (GAN), which consists of a generator and a discriminator to learn target data distributions from latent space in a competitive way to generate high-quality p.\nPixel Space Strategy. The pixel space optimization strategy adjusts pixel values of p using gradient descent to optimize the objective function L(x,p) 1. In dodging attacks, the goal is to maximize the difference between the features of $x^{adv}$ and x extracted by f making faces undetectable by face recognition systems. Its objective function is defined as:\n$min\\limits_{p} 1 - L(x, p) = 1 \u2212 L_{adv} (f_0(x^{adv}), f_0(x))$, (6)\nwhere $L_{adv}$ is the classification loss of the FR model.\nIn contrast, the goal of impersonation attacks is to minimize the distance of features between $x^{adv}$ and the target image $x_t$, making the face recognition system identify $x^{adv}$ as the target identity. The objective function is defined as follows:\n$min\\limits_{p} L(x, p) = L_{adv} (f_0(x^{adv}), f_0(x_t))$. (7)\nLatent Space Strategy. Benefit from some advantages of GANs (such as their knowledge of facial structures or textures), some works use the generator of GAN to generate and achieve more realistic, coherent, or visually natural p. The GAN consists of two components: the generator (G), which maps input noise z to data samples, and the discriminator (D), which differentiates between real and generated samples.\nThe latent space strategy utilizes the generator (G) to generate realistic p from the input noise z, making them visually similar to real samples, using the following loss function:\n$L_G = -E_{z\u223cz}[log D(x^{adv})]$,\ns.t. $x^{adv} = (1 \u2013 M) \u00b7 x + M \u00b7 G(z)$. (8)\nAccordingly, the objective function for dodging attack is:\n$min L(x,p) = L_G \u2013 \u03bbL_{adv}(f_0(x^{adv}), f_0(x))$, (9)\nwhere \u03bb is a regularization parameter to balance the Generator Loss and the Adversarial Classifier Loss. The objective function for an impersonation attack is denoted as:\n$min L(x, p) = L_G + \u03bbL_{adv}(f_0(x^{adv}), f_0(x_t))$. (10)\nThe objectives of physical adversarial defense can be divided into two main aspects: enhancing the model's robustness and detecting adversarial attacks. Robustness ensures that the model maintains high classification or recognition accuracy even when subjected to physical attacks. The robustness objective aims to minimize the maximum loss under adversarial patches and can be mathematically expressed as follows:\n$min\\limits_\\theta max\\limits_p L(f_0(x + p), f_0(x))$. (11)\nFor brevity, in the following, we refer to L(x,p) as the Adversarial Total Loss and $L_{adv}$ as the Adversarial Classifier Loss."}, {"title": "IV. PHYSICAL ADVERSARIAL ATTACKS AGAINST FACE RECOGNITION SYSTEMS", "content": "To address RQ2, we first categorize physical adversarial attacks based on the medium and then analyze in detail how these methods overcome the aforementioned challenges."}, {"title": "A. Methodology", "content": "This section provides a detailed exploration of physical adversarial attacks on face recognition systems. We focus on the mediums used for physical adversarial attacks, attack attributes such as specificity and universality, and strategies including robustness, imperceptibility, and optimization. As shown in Table III, existing work can be classified into three categories based on the physical medium employed. The first category, disguise-based adversarial attacks, involves generating adversarial samples using items such as hats, masks, glasses, stickers, and makeup. These digitally-generated samples can be materialized through 3D printing for real-world application. The second category is infrared-based attacks, which utilize infrared or lasers. These attacks exploit the system's sensitivity to certain light wavelengths, employing infrared light or lasers to disrupt face recognition systems. Although imperceptible, their effectiveness is often limited by environmental conditions. The third category, illumination-based attacks, primarily involves projecting adversarial patterns onto the face or using point light sources to alter facial illumination. These methods allow for significant manipulation of lighting conditions and facial appearance. Fig. 3 provides a schematic of three types of attacks, further detailed in Table IV.\nBased on three types, we define seven evaluation criteria related to two categories: (1) effectiveness, which assesses the category's threat level, transferability, universality, robustness, and imperceptibility (criteria i-v); and (2) complexity, which evaluates the category's complexity of deployment and operation (criteria vi-vii). To evaluate the performance of the three category attacks, the criteria are classified into three levels: low (L), medium (M), and high (H). In some cases, a range of levels (e.g., low to medium (L-M) or medium to high (M-H) ) is used due to varying characteristics.\n(i) Threat Level evaluates the potential harm attacks can cause in real-world scenarios. Impersonation attacks generally have a higher threat level than dodging attacks due to their ability to cause more severe consequences.\n(ii) Transferability assesses the attack effectiveness when the internal structure and parameters of the target system are unknown, crucial in real-world scenarios to assess the adaptability and practicality of the attack.\n(iii) Universality measures the attack effectiveness across different input samples, indicating universal applicability and high adaptability of the attack method.\n(iv) Robustness evaluates the consistency of the attack's performance under varying environmental conditions.\n(v) Imperceptibility evaluates the imperceptibility of the attack, requiring a balance between it and effectiveness.\n(vi) Complexity assesses the difficulty of implementing and deploying adversarial attacks.\n(vii) Cost assesses the financial feasibility of developing an attack, as high costs can limit research accessibility."}, {"title": "B. Disguise-based Adversarial Attacks", "content": "As shown in Table IV", "implementation": "adversarial accessories", "Attacks": "nPixel Space Strategy. Eq. (6) and (7) fulfill the fundamental objective of generating adversarial patches capable of deceiving FR models. However", "24": "see Fig. 4a) incorporated Total Variation (TV) loss [124", "74": "integrated an optional black penalty loss $L_{blk"}, "to reduce an amount of black color on the patch enabling the mask to be less unusual (Eq. 15).\n$L_{blk} = \\sum_{i,j} -P_{i,j}$. (15)\nIn addition, Kaziakhmedov et al. [74"], "follows": "n$x^{adv} = (1\u2212M)\u00b7x+Marg min\\limits_P E_{t\u223cT}[L_{adv} (f_0(x+p), f_0(x))]$. (16)\nTo improve robustness, AdvMask [98] built upon TV Loss by incorporating random location- and color-based transformations (RLC-TA) and digitally applying masks to face images using an end-to-end UV location map for universal dodging attacks. Additionally, an ensemble training approach using multiple face recognition models was adopted for black-box attacks. The UV position maps record the 3D coordinates of a complete facial point cloud from a 2D image, providing dense correspondence for each point in the UV space. This allows for a near-realistic approximation of the mask, essential for creating practical adversarial patches.\nRSTAM [81] (see Fig. 4b) enhanced the transferability of the adversarial masks through their proposed random similarity transformation (RST) strategy with four degrees of freedom (4DoF), which consists of translational, rotational, and scaling transformations. Furthermore, they proposed a random meta-optimization strategy for ensembling several pre-trained FR models to generate more universal adversarial masks that are highly effective in black-box impersonation attacks.\nTo enhance the realism and adaptability of adversarial patches, particularly when viewed from different angles or under varying lighting conditions, researchers have turned to 3D models and rendering techniques. AT3D [100] utilized 3D Morphable Models (3DMM) [126] to manipulate the 3D geometry and texture of facial meshes. By incorporating neural rendering techniques [127], AT3D generated highly natural 2D adversarial images from 3D models, which were more effective across different viewing angles and environmental conditions. Moreover, AT3D utilized surrogate models to improve the transferability of patches in black-box models.\nAdvGlass [21], a foundational study in adversarial glasses (see Fig. 4d), introduced adversarial patches on eyeglass frames. It employed a loss function integrating TV loss for smooth color transitions and a non-printable score (NPS) loss to ensure printability. While it was not effective against state-of-the-art face recognition models, as demonstrated in AdvHat [24], its robustness strategy significantly influenced subsequent attack techniques. NPS, as shown in Eq. 17, measures the quality degradation during the digital-to-physical transition and its impact on the sample's overall effectiveness. Specifically, when an adversarial patch is printed on a physical medium (e.g., paper) and re-captured through a camera, the process introduces discrepancies due to factors like color deviation, brightness, and resolution limitations. These changes can affect the carefully controlled perturbations in the original digital patch, potentially weakening its adversarial effectiveness. Thus, NPS quantifies the degree of quality loss during the digital-to-physical transition and its impact on the overall effectiveness of the adversarial sample.\n$L_{nps}(p) = \\sum_{p \\in G(z)} \\prod_{p \\in P} (1 - p)$. (17)\nBuilding on AdvGlass, Singh et al. [85] introduced the patch-noise combo attack, which combines the patch with imperceptibly small noises applied to other areas of the face image. They utilized regularization techniques to optimize TV loss and employed the Input Diversity Method [128], Ensemble Diversity Method [129], or their combination to improve robustness and black-box transferability. Additionally, they incorporated the curriculum learning to enhance the resilience of adversarial samples to brightness variations, increasing their reliability in physical environments [87]. Further research explored adversarial glasses for specialized scenarios, including systems operating in near-infrared (NIR) wavelengths. Cohen et al. [86] adapted adversarial designs to this domain, incorporating strategies like TV loss, NPS, and EoT algorithm to maintain robustness, with optimization conducted through the PGD algorithm.\n$L(x, p) = E_{t_1,t_2\u223cT_1,T_2}L_{adv} + \u03bb_{tv}L_{tv} + \u03bb_{nps}L_{nps}$, (18)\nwhere $E_{t_1,t_2\u223cT_1,T_2}$ denotes the EoT transformation.\nLatent Space Strategy. When dealing with FR"}