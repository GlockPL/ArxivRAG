{"title": "A Blueprint for Auditing Generative Al", "authors": ["Jakob M\u00f6kander", "Justin Curl", "Mihir Kshirsagar"], "abstract": "The widespread use of generative Al systems is coupled with significant ethical and social challenges. As\na result, policymakers, academic researchers, and social advocacy groups have all called for such systems\nto be audited. However, existing auditing procedures fail to address the governance challenges posed by\ngenerative Al systems, which display emergent capabilities and are adaptable to a wide range of\ndownstream tasks. In this chapter, we address that gap by outlining a novel blueprint for how to audit\nsuch systems. Specifically, we propose a three-layered approach, whereby governance audits (of\ntechnology providers that design and disseminate generative Al systems), model audits (of generative Al\nsystems after pre-training but prior to their release), and application audits (of applications based on top\nof generative Al systems) complement and inform each other. We show how audits on these three levels,\nwhen conducted in a structured and coordinated manner, can be a feasible and effective mechanism for\nidentifying and managing some of the ethical and social risks posed by generative Al systems. That said, it\nis important to remain realistic about what auditing can reasonably be expected to achieve. For this\nreason, the chapter also discusses the limitations not only of our three-layered approach but also of the\nprospect of auditing generative Al systems at all. Ultimately, this chapter seeks to expand the\nmethodological toolkit available to technology providers and policymakers who wish to analyse and\nevaluate generative Al systems from technical, ethical, and legal perspectives.", "sections": [{"title": "1 Introduction", "content": "The promises and perils of artificial intelligence (AI) have been widely discussed, with generative Al\nsystems being the most recent technological advance to capture the world's attention. Each year,\nStanford University publishes an Al index report covering promising trends in research and applications\nrelated to Al. The 2023 report describes 2022 as the year that generative Al broke into the public\nconsciousness with popular applications like ChatGPT, Stable Diffusion and Make-A-Video.\u00b9 Amid growing\nprivate sector investment and adoption in industries as diverse as healthcare, financial services and\ncybersecurity, generative Al is already changing how we live and work. Stanford's report gives two\nexamples that help illustrate exactly how this change is occurring. First, DeepMind released a generative\nAl model that assists scientists researching de novo antibody discovery, accelerating medical research by\nenabling them to generate and assess more promising candidates more quickly. Second, GitHub released\na text-to-code generative tool called Copilot, which reportedly improved users' coding productivity and\nenjoyment. The model handles the more rote parts of software development, freeing up users to focus\non the interesting parts of projects.\nYet one need only read news of Al scams and misuse to see that the technology also creates real\nsocietal harms. QTCinderella is one of many popular female Twitch streamers experiencing sexual\nharassment as Al-generated, non-consensual pornographic images and videos of her are circulated online.\nThese images and videos were created with generative Al tools and sold on a deepfake pornography\nwebsite without her knowledge.2 Scammers have also been using generative Al tools to imitate the voices\nof distressed family members. These scammers have swindled thousands of people out of millions of\ndollars, leaving affected families with little recourse.3\nWith the aim of addressing these harms, experts from academia, industry and government have\ncalled for generative Al systems to be audited. Simplified, auditing in this context refers to structured and\nindependent assessments of how well an Al system's design, properties, or impact adheres to applicable\nstandards, regulations, and norms. In Section 2, we will explain and discuss what Al auditing is in greater\ndetail. For now, the key point to stress is that Al auditing has recently attracted much attention as a\npromising Al governance mechanism. For example, in their seminal paper on foundation models (which\nare Al systems adaptable to a wide range of downstream tasks\u2074), Bommasani et al. suggested that \u201csuch\nmodels should be subject to rigorous testing and auditing procedures.\u201d5 Similarly, OpenAl's CEO Sam"}, {"title": "2 Why audit generative Al systems?", "content": "Auditing is a systematic process of gathering and evaluating evidence about an entity and communicating\nthe results to relevant stakeholders. Just as audits can be used to create transparency and legal\naccountability in areas like financial accounting and worker safety, auditing can also be used as a\ngovernance mechanism for Al systems. Three ideas underpin the promise of auditing as an Al governance\nmechanism, namely, that:\ni. procedural regularity and transparency contribute to good governance;\nii. proactivity in the design of Al systems helps identify risks and prevent harm before it occurs; and,\niii. the operational independence between the auditor and the auditee contributes to the objectivity\nof the evaluation.\nThis field of Al auditing is still relatively new, dating back to Sandvig et al.'s 2014 article Auditing\nAlgorithms. Since then, Al auditing has been defined in many ways for many purposes. In some cases,\npolicymakers mandate audits to ensure Al systems meet specific legal standards.10 In other cases,\ntechnology firms commission audits to identify and mitigate risks in their Al systems. In still others,\norganisations conduct audits to inform citizens about the conduct of specific companies. While Al auditing"}, {"title": "3 How to audit generative Al systems?", "content": "Al audits can be structured in many ways, and a wide range of Al auditing procedures have already been\ndeveloped. 18 However, not all auditing procedures are equally effective in handling the risks posed by\ngenerative Al systems. Nor are they all equally likely to be implemented, due to factors including technical\nlimitations, institutional access and administrative costs. In this section, we first introduce our blueprint\nfor how to audit generative Al systems. The question thus remains: how can auditing procedures for\ngenerative Al systems be designed that are feasible and effective in practice?\nIn this section, we build on previous research in the fields of IT audits and systems engineering to\noutline a blueprint for how to audit generative Al systems. Specifically, we propose a three-layered\napproach. First, technology providers developing generative Al systems should undergo governance\naudits that assess their organisational procedures, accountability structures and quality management\nsystems. Second, generative Al models should undergo model audits, assessing their capabilities and\nlimitations after initial training but before adaptation and deployment in specific applications. Third,\ndownstream applications using generative Al models should undergo continuous application audits that\nassess the ethical alignment and legal compliance of their intended functions and their impact over time."}, {"title": "4 Governance audits", "content": "At the first layer of our blueprint, technology providers developing generative Al systems should undergo\ngovernance audits that assess their organisational procedures, incentive structures and management\nsystems. Governance audits have a long history in areas like IT governance and systems engineering, 27\nwhere they ensure appropriate risk management systems are in place and promote procedural regularity\nand transparency throughout the software development lifecycle. They have also been shown to improve\naccountability by requiring companies to publish their results, preventing coverups and incentivising\nbetter behaviour. Specifically, we argue that governance audits of generative Al system providers should\nfocus on at least the following three tasks:\n1) Reviewing the adequacy of organisational governance structures to ensure that model development\nprocesses follow best practices and that quality management systems can capture risks specific to\ngenerative Al. While technology providers have in-house quality management experts, confirmation\nbias may prevent them from recognising critical flaws; involving external auditors addresses that\nissue. Nevertheless, governance audits are often most effective when auditors and technology\nproviders collaborate to identify risks.28 Therefore, it is important to distinguish accountability from\nblame at this stage of an audit.\n2) Creating an audit trail of the generative Al model development process to provide documentation\nabout the development of a model's capabilities, including information about its intended purpose,\ndesign specifications and choices, as well as training and testing procedures. Companies can use\nmodel cards, 29 datasheets,30 and system cards31 to create these audit trails. Specifying the purpose\nand capabilities of models upfront facilitates auditing and enforcement downstream (e.g., enforcing\nlicensing agreements that only allow specific applications). Finally, requiring technology providers\nto document and justify their design choices should spark greater ethical deliberation by making\ntrade-offs explicit.\n3) Mapping roles and responsibilities within organisations that design generative Al models facilitates\nthe allocation of accountability for system failures. The adaptability of generative Al models\ndownstream does not absolve technology providers of all responsibility. Some risks are \u201creasonably\nforeseeable\". In the related field of machine learning (ML) facial recognition, a study found that\ncommercial gender classification systems were less accurate for darker-skinned females than\nlighter-skinned males.32 After these findings were published, the technology providers quickly\nimproved their models' accuracy, suggesting the main issue was inadequate risk management,\nrather than some inherent problem. Mapping the roles and responsibilities of different stakeholders\ncan improve accountability and increase the likelihood of impact assessments being structured\nrather than ad-hoc, thus helping identify and mitigate harms proactively.\nTo conduct these three tasks, auditors will require the highest level of access (what Koshiyama et al. term\nwhite-box auditing). This includes knowledge of how and why a model was developed, as well as privileged\naccess to facilities, documentation and personnel. It can even include access to the input data, learning\nprocedures and task objectives used to train the model. This level of access may add to the logistical\nburden of governance audits by requiring nondisclosure and data-sharing agreements, but it is necessary\nfor assessing generative Al models and the organisational safeguards around high-risk projects that\nproviders may prefer not to discuss publicly. Importantly, this type of privileged access is not unique. It is\nstandard practice in governance audits in other fields. IT auditors, for example, have full access to material\nand reports related to operational processes and performance metrics.33\nThe results of these governance audits should be tailored to the audience.34 To the technology\nprovider's management and directors, the auditors should provide a full report that directly and\ntransparently discusses the vulnerabilities of existing governance structures. Such reports may\nrecommend actions, but taking actions remains the provider's responsibility. Usually, such audit reports\nare not made public. Auditors should also create reports for two other audiences: regulators and\ndevelopers of downstream applications. In some jurisdictions, hard legislation may demand that\ntechnology providers follow specific requirements. The EU AI Act, for example, requires providers to\nregister high-risk Al systems with a centralised database.35 In such cases, reports from independent\ngovernance audits can help ensure adherence to legislation. These reports can also inform downstream\napplication developers' understanding of a model's intended purpose, testing, risks and limitations.\nFinally, how do governance audits help identify and mitigate the potential harms of generative Al\nsystems? Weidinger et al. listed six broad risk areas: discrimination, information hazards, misinformation\nhazards, malicious use, human-computer interaction harm, and automation and environmental harms.\nGovernance audits address some of these directly. By assessing the adequacy of governance structures\nlike structured access protocols for generative Al systems, these audits help reduce the risk of malicious\nuse. They also reduce the risk of information hazards \u2013 which primarily stem from adversarial attacks that\nextract sensitive information from generative Al models \u2013 by reviewing the process by which datasets are\nsourced and the techniques by which models are trained. That said, we acknowledge governance audits\nonly have an indirect impact (e.g., insofar as improved transparency reduces overall risks) on most of the\nrisk areas listed by Weidinger et al. We find that the risk areas of discrimination, misinformation hazards\nand human-computer interaction harms are better addressed by model and application audits.\""}, {"title": "5 Model audits", "content": "At the second layer, we propose that generative Al systems should undergo model audits that assess their\ntechnical characteristics before they are adapted for downstream applications. These audits can provide\ncritical insights about a model's capabilities and limitations, offering valuable information to both internal\nand external stakeholders.36 For internal stakeholders, they can provide valuable benchmarks that inform\nmodel redesign and API licensing agreements, helping improve existing systems and limit unintended\nuses. Externally, clearly communicating the capabilities and limitations of a model can assist downstream\ndevelopers in designing better applications that do not exceed a model's intended scope.\nAs mentioned in Section 1, evaluating a generative Al system independent of its applications is\nchallenging. However, auditors can do so by leveraging two different approaches. The first seeks to\nidentify generative Al systems' intrinsic characteristics. For example, the dataset on which a generative Al\nsystem has been trained can be assessed for completeness without reference to specific use cases.37\nHowever, for large datasets, this can be impractical or expensive.38 The second approach involves testing\na generative Al system's performance across multiple use cases and then aggregating the results. While\nthat approach may be better suited for assessing generative Al systems' technical characteristics, it\nremains challenging to decide what use cases to include in the test and what model characteristics to test\nfor. We recommend that model audits should test for characteristics that are\ni. socially and ethically relevant (i.e., can be linked to specific risks);\nii. predictably transferable (i.e., impacts translate to downstream applications); and\niii. meaningfully operationalisable (i.e., can be assessed with available tools and methods).\nWith those criteria in mind, we suggest model audits should focus on (at least) the performance,\ninformation security and robustness of generative Al models. Other characteristics may meet the three\ncriteria listed above, but we chose these three examples purely to illustrate how model audits can function\nin our framework.\n1) Performance, i.e., how well the generative Al model functions on various tasks. Standardised\nbenchmarks can help assess a model's performance by comparing it to a human baseline. For\nexample, GLUE is a benchmark that aggregates a large language model's performance across multiple\ntasks into a single reportable metric.39 The choice of tasks and benchmarks here is very important,\nwith more generally being better. Some benchmarks have been criticised because the tasks are too\nnarrow or easy and risk overestimating a model's capabilities. This likely happens because the model's\nperformance on the task rapidly approaches that of a non-expert human, leaving little room for\nmeaningful comparison.\n2) Information security, i.e., how difficult it is to extract training data from a generative Al model.\nWeidinger et al. identify \u201cinformation hazards\u201d as a category of risks in which Al models leak personal\nor sensitive information. Carlini et al. show that diffusion models like DALL-E 2 and StableDiffusion\nmemorise images from the training data, which can then be successfully recovered by an adversarial\nactor attempting to extract that information.40 Although differential privacy techniques may be able\nto reduce the amount of at-risk information, these methods are often too computationally expensive\nor severely degrade performance.41 As such, auditing the extent to which models might expose\ninformation is important, though we will add the caveat that assessing a model's information security\nduring model audits does not address all information hazards. In some cases, the risk of exposing\nsensitive information about users can only be determined (and audited) at the application level.\n3) Robustness, i.e., how well the model reacts to unexpected prompts or edge cases. In ML, robustness\nindicates how well an algorithm performs when faced with new, potentially unexpected (i.e., out-of-\ndomain) input data. A generative Al model that lacks robustness introduces the risks (among others)\nof critical system failures and adversarial attacks when it encounters data unlike what it has seen\nduring training. As a result, researchers have created tools and methods for evaluating a model's\nrobustness. Again, consider LLMs as an example. AdvGLUE is a benchmark that can be used to\nevaluate an LLM's susceptibility to adversarial attacks in different domains using a multi-task\nbenchmark.42 By quantifying robustness, AdvGLUE facilitates comparisons between LLMs. There are,\nhowever, many possible ways to operationalise robustness. Group robustness, for instance, measures\na model's performance across different sub-populations.43 To comprehensively assess different kinds\nof robustness, model audits should employ multiple tools and methods.\nThese three characteristics pertain to generative Al systems, but model audits should also review datasets\nas training models with biased or incomplete datasets can lead to poor robustness and discriminatory\noutcomes. Although curating \u201cunbiased\" datasets may be impossible, disclosing how a dataset was\nassembled can suggest its potential biases. Model auditors can then use existing tools and methods to\ninterrogate biases in a pre-trained model. While the availability of such tools is encouraging, it is important\nto remain realistic about what these audits can achieve. They do not ensure generative Al systems are\nethical in any global sense. Instead, they contribute to better precision in claims about a model's\ncapabilities and inform the design of downstream applications.\nModel audits require auditors to have privileged access to generative Al models and their training\ndatasets. In the typology provided by Koshiyama et al., this corresponds to medium-level access, whereby\nauditors have access equivalent to a model's developer, meaning they can manipulate model parameters\nand review learning procedures and task objectives. This is required to assess generative Al systems'\ncapabilities accurately during model audits. But in contrast to white-box audits, model auditors' access is\nlimited to the technical system and does not extend to technology providers' organisational processes.\nSome of the characteristics tested for during model audits correspond directly to the risks in\nWeidinger et al.'s taxonomy, such as information security and information hazards. However, it should be\nnoted that our proposed model audits only focus on a few characteristics of generative Al systems. That\nis because the criterion of meaningful operationalisability sets a high bar: not all risks associated with\ngenerative Al can be addressed at the model level. Consider discrimination as an example. Model audits\ncan expose how biases in models can be traced back to training datasets that reflect historic injustices.\nHowever, what constitutes unjust discrimination is often context-dependent and varies between\njurisdictions, which makes it difficult to meaningfully address risks like unjust discrimination on a model\nlevel. This observation, however, does not argue against model audits but rather for complementary\napproaches like application audits, as discussed in the next section.\""}, {"title": "6 Application audits", "content": "In the third layer, we recommend that products and services built on top of generative Al models undergo\napplication audits that assess the legality of their intended functions and how they will impact users and\nsocieties. Unlike governance and model audits, application audits focus on actors employing generative\nAl models in downstream applications.44 Such audits are well-suited to ensure compliance with national\nand regional legislation, sector-specific standards and organisational ethics principles. We recommend\napplication audits have two components: functionality audits, which evaluate applications using\ngenerative Al systems based on their intended and operational goals, and impact audits, which evaluate\napplications based on their impacts on different users, groups and the natural environment.\nDuring functionality audits, auditors check whether an application's intended purpose is (1) legal\nand ethical in and of itself and (2) aligned with the intended uses of the underlying generative Al model.\nThe first check assesses compliance with laws, regulations, ethics principles and codes of conduct relevant\nto the application. If an application is unlawful or unethical, its performance is irrelevant, and it should\nnot be permitted on the market. The second check addresses risks stemming from how developers build\non top of generative Al models, especially if they overstate or misrepresent what the application can do.\nTo do so, these audits rely on the outputs of audits at other layers. For example, functionality audits can\nonly assess whether developers' applications align with a model's intended use cases because governance"}, {"title": "7 Clarifications and limitations", "content": "In this section, we tie together loose ends from the previous three sections and discuss the limitations of\nour approach. On the first point, we illustrate how audits on the three different layers work together and\nhow one might select independent, third-party auditors. On the second, we discuss four limitations of\nauditing approaches to governing generative Al: one conceptual, one institutional, one technical, and one\npractical.47 First, all audits will struggle with operationalising normative concepts like robustness. Second,\nwe currently lack an institutional ecosystem that can support independent third-party audits of generative\nAl. Third, static auditing procedures will quickly become dated as technological innovation accelerate and\ngenerative Al systems operate with ever higher degrees of autonomy, complexity, and adaptability.\nFinally, not all risks from generative Al can be practically addressed on the technology level.\nTo adequately address the multiplicity and complexity of risks from generative Al, we have argued\nthat governance, model and application audits must work together in a complementary, structured\nprocess. In practice, this means that outputs from audits on one level become inputs for audits on other\nlevels. In our blueprint, for instance, model audits produce reports summarising a model's properties and\nlimitations, which should inform application audits that verify whether a model's known limitations have\nbeen considered when designing downstream applications. Similarly, ex-post application audits produce"}, {"title": "8 Conclusion", "content": "Existing governance mechanisms, including past efforts to audit Al, fall short with generative Al systems\nthat are adaptable to a wide range of downstream applications. In this chapter, we have attempted to\nbridge this governance gap by outlining a blueprint for how to audit generative Al systems. Specifically,\nwe have advocated for a three-layered approach, whereby governance, model and application audits\ninform and complement each other.\nDuring governance audits, technology providers developing generative Al systems undergo\nassessments of their organisational procedures, accountability structures and quality management\nsystems. During model audits, a model's capabilities and limitations are assessed along several\ndimensions, including performance, information security and robustness. During application audits,\nproducts and services built on top of generative Al models undergo functionality and impact audits that\nassess the legality of their intended functions and their potential impact on users.\nIt is important to remember that the effectiveness of this auditing approach derives from the way\nit combines and coordinates different types of audits. Governance audits should ensure that providers\nhave mechanisms to take the output logs generated during application audits into account when\nredesigning their models. Similarly, application audits should ensure that downstream developers take\nthe limitations identified during model audits into account when building on top of a specific model.\nHowever, even under ideal circumstances, audits will not protect against all the risks associated with\ngenerative Al. We remind readers of the main limitations that we discussed in Section 7. Namely, the\ndifficulty of operationalising normative concepts, the lack of an institutional auditing ecosystem and the\nfact that not all harms can be addressed at the technology level.\nThis leaves us with one last question: what are the implications for lawmakers and technology\nproviders? First, lawmakers can help develop an institutional ecosystem for conducting and enforcing\ngovernance, model, and application audits of generative Al systems. For example, they can encourage\nprivate sector auditing initiatives by developing standardising evaluation metrics or rewarding\nachievements through monetary incentives. 49 They can also draw on this three-layered blueprint to\ndevelop clearer, more operationalisable systems requirements. In doing so, they will encourage\ncompanies to invest in cost-effective, comprehensive auditing tools. Fortunately, no one needs to start\nfrom scratch. Technology providers and lawmakers are already experimenting with some of the auditing\nactivities we mentioned. Consequently, when thinking about mandating governance, model and\napplication audits, lawmakers and regulators can leverage a wide range of existing tools and methods,\nincluding impact assessments, benchmarking, model evaluation and red teaming.\nTechnology providers can also get ahead of the curve. To start with, they can subject themselves\nand their models to governance and model audits, simultaneously creating demand for independent\nauditing bodies and helping spark methodological innovation as these procedures are used in practice. In\nthe medium term, providers can demand that products and services built on top of their models undergo\napplication audits, enforcing this requirement through structured access procedures that make use\nconditional on these audits. Although this requirement may risk losing some downstream applications,\nrequiring application audits can help technology providers build a better brand and minimise future\nfinancial and legal risks. Finally, in the long term, technology providers should establish and fund an\nindependent industry body that conducts or commissions governance, model and application audits.\nIt is worth concluding this chapter with some words of caution. First, we recognise that future\ndevelopments may change the feasibility and effectiveness of this blueprint. For example, governance\naudits make sense when a limited number of actors have the resources and talent to train and disseminate\ngenerative Al models. If this process is democratised \u2013 either through reduced barriers-to-entry or a shift\nto open-source everything \u2013 then the form of, and potentially the need for, governance models will need\nto be reassessed. As a result, while maintaining the usefulness of our three-layered approach, we\nacknowledge that it will need to be continuously revised in response to the changing technological and\nregulatory landscape. Finally, our blueprint is intended to complement and interlink existing governance\nmodels by strengthening procedural transparency and regularity. Instead of being adopted wholesale by\nlawmakers and technology providers, we hope our three-layered approach will be adopted, adjusted and\nexpanded to meet the governance needs of different stakeholders and contexts."}]}