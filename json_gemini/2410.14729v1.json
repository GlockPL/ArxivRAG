{"title": "TOKENS ON DEMAND: TOKEN CONDENSATION AS TRAINING-FREE TEST-TIME ADAPTATION", "authors": ["Zixin Wang", "Dong Gong", "Sen Wang", "Zi Huang", "Yadan Luo"], "abstract": "In this work, we introduce Token Condensation as Adaptation (TCA), a training-free approach designed to mitigate distribution shifts encountered by vision-language models (VLMs) during test-time inference. TCA bridges distribution gaps at the patch level by condensing image tokens that exhibit low attentiveness to the <cls> token. Recognizing the <cls> token may correspond to universal concepts, TCA identifies and tracks the most reliable <cls> tokens that align specifically with target classes from historical data streams. To achieve this, we propose a context token reservoir (CTR), which retains tokens with the lowest uncertainty as \"anchors\" to guide the preservation of class-relevant tokens during inference. These anchors, in turn, act as token-level classifiers to correct VLM predictions and improve visual-text alignment. Utilizing anchors sampled from CTR, TCA condenses tokens through two operations: (1) pruning class-irrelevant tokens that consistently rank low across all attention heads to reach cross-head consensus on their irrelevance, and (2) merging the remaining class-ambiguous tokens into representative centers using coreset selection, maintaining linear computational complexity. As the first method to explore token efficiency in test-time adaptation, TCA consistently demonstrates superior performance across cross-dataset and out-of-distribution adaptation tasks, reducing GFLOPs by 12.2% to 48.9% while achieving accuracy improvements up to 21.4% against the strongest baseline without introducing additional parameters. Our code is available at here.", "sections": [{"title": "INTRODUCTION", "content": "Online test-time adaptation (TTA) (Wang et al., 2023c) has emerged as a promising strategy to handle distribution shifts encountered during inference (Liang et al., 2023). TTA dynamically fine-tunes pretrained models on unlabeled data batches, enhancing generalization by aligning intermediate-layer batch statistics (Niu et al., 2023), optimizing for first-order flatness in the loss landscape (Foret et al., 2021), promoting self-supervised consistency across augmentations (Zhang et al., 2022), or tracking model historical weights (Lee & Chang, 2024). Despite the success of traditional TTA methods, they often require computationally expensive tuning of the backbone's parameters. This challenge is further amplified in vision-language models (VLMs), which consist of vast parameter sets and require large batch sizes (e.g., 256) for stabilizing adaptation (D\u00f6bler et al., 2024).\nTest-time prompting (TPT) offers a more efficient alternative for TTA by shifting adaptation focus to the language side of VLMs, learning a small set of task-specific context prompts for downstream tasks while freezing the visual backbone. Nevertheless, TPT largely overlooks the impact of visual distribution shifts. Adapting to high-variance target images through prompts often relies on external source data (Samadh et al., 2023) or extensive data augmentation (Feng et al., 2023) (e.g., 60\u00d7 more AugMix or diffusion-based synthetic samples). In strict online TTA settings, where the batch size is constrained to one, this reliance on augmentation significantly inflates computational costs, leading to a 60\u00d7 rise in GFLOPs compared single-sample processing (i.e., 1108.61 vs. 17.59 GFLOPs). The need for gradient backpropagation during inference further increases the computation burden, making exiting TPT suboptimal for many resource-constrained applications.\nIn this paper, we attempt to tackle visual shifts at a patch level by introducing a novel approach named Token Condensation as Adaptation (TCA). TCA allows the model to adapt on the fly to"}, {"title": "RELATED WORK", "content": "Online Test-time Adaptation. To address performance degradation during test time, online test-time adaptation (TTA) has gained significant attention. Current TTA methods can be categorized"}, {"title": "OUR APPROACH", "content": "Problem Set-up. We begin by revisiting online test-time adaptation (TTA) of VLMs, focusing on contrastive language-image pre-training (CLIP) as a representative case. For a given downstream task \\(D_{tar}\\), the test data \\(x = \\{x_t\\}_{t=1}^{T}\\) arrives sequentially at each time step \\(t\\). The objective is to adapt CLIP on the fly to classify the incoming test samples into one of C classes, each represented by a textual prompt like \u201ca photo of a <classname>\". CLIP embeds both visual and textual\""}, {"title": "TOKEN CONDENSATION AS ADAPTATION", "content": "Building upon our empirical findings, we propose a novel strategy called Token Condensation as Adaptation (TCA), which selectively removes or merges tokens contributing to drift, enabling efficient adaptation in a training-free manner. Specifically, given an L-layer ViT, the forward pass"}, {"title": "CONTEXT-AWARE CROSS-HEAD TOKEN PRUNING fprune", "content": "Prior token pruning methods for ViTs such as (Liang et al., 2022) primarily discard patch tokens with lower averaged attention scores \\(S \\in \\mathbb{R}^{N}\\) relative to the <cls> token \\(v_{cls}^{1}\\) across all attention"}, {"title": "CONTEXT-AWARE TOKEN MERGING fmerge", "content": "As depicted in Figure 2a, a subset of tokens, although relevant to the target class, exhibit high uncertainty. These tokens are referred to as class-ambiguous tokens, identified from the ranked token list derived using Equation (9):\n\\(\\Phi = \\{i \\mid \\Theta_{merge}(R) \\leq S_{i}^{head} < \\Theta_{prune}(\\alpha, R), \\forall i\\}\\)\nwhere \\(\\Theta_{merge}(R)\\) denotes thresholds for token selection during merging. The selected tokens \\(V = \\{v_i\\}_{i \\in \\Phi}\\) can introduce variance or noise into latent representation \\(z_t\\) and negatively impact the final classification decision. To address this, we propose a context-aware token merging strategy to consolidate these tokens into more representative ones."}, {"title": "EXPERIMENTS", "content": "Experimental Setup.\nDatasets. Following prior works, we conducted two main benchmarks: the cross-dataset (CD) benchmark and the out-of-distribution (OOD) benchmark. The CD benchmark assesses the model's performance on unseen classes across 10 datasets: Aircraft (Maji et al., 2013), Caltech101 (Fei-Fei et al., 2007), Cars (Krause et al., 2013), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019), Flower102 (Nilsback & Zisserman, 2008), Food101 (Bossard et al., 2014), Pets (Parkhi et al., 2012), SUN397 (Xiao et al., 2010), and UCF101 (Soomro et al., 2012). In contrast, the OOD benchmark focuses on evaluating the model's effectiveness on shifted data using label sets previously seen by CLIP. This includes variants of ImageNet (Deng et al., 2009): ImageNet-A (Hendrycks et al., 2019), ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021), and ImageNet-S (Wang et al., 2019).\nBaselines. To provide a comprehensive evaluation, we compare TCA with existing approaches across four categories: (1) Prompt-tuning methods like CoOp (Zhou et al., 2022b) and CoCoOp (Zhou et al., 2022a), which require multi-epoch adaptation; (2) Conventional online test-time adaptation (TTA) methods such as Tent (Wang et al., 2021) and SAR (Niu et al., 2023). Tent updates batch normalization layers, while SAR further incorporates sharpness-aware minimization for reliable model updates. Following D\u00f6bler et al. (2024), we reran these experiments with adjusted batch sizes to align with our settings; (3) Test-time prompting methods, including TPT (Shu et al., 2022), C-TPT (Yoon et al., 2024), and Diff-TPT (Feng et al., 2023), as well as TTA methods for CLIP such as MTA (Zanella & Ayed, 2024) and TDA (Karmanov et al., 2024); and (4) Token pruning and merging methods for ViTs, such as EViT (Liang et al., 2022), ToMe (Bolya et al., 2023), and ATS (Fayyaz et al., 2022). As ATS is an adaptive token pruning method with no fixed budget, we constrain its computational cost by an upper bound to ensure fair comparison.\nImplementation Details. We utilize the official CLIP \u00b9 prompts as text inputs. The batch size is set to 1 without data augmentations to mimic realistic deployment scenarios. All experiments are conducted using the pre-trained CLIP models, specifically using ViT-B/16 and ViT-L/14 architectures as the visual backbone. For both CD and OOD benchmarks, we set K to 2. Notably, our method is training-free, which achieves rapid adaptation with no need for any hyperparameters for optimization. All experiments are performed on a single NVIDIA RTX A6000 GPU."}, {"title": "COMPARISON WITH STATE-OF-THE-ART", "content": "Cross-Dataset Benchmark.\nTable 1 presents the results for fine-grained cross-dataset benchmark using the ViT-B/16 architecture. As observed in Figure 2a, the core idea behind TCA is that condensing inattentive tokens can effectively mitigate distribution shifts caused by visual-text misalignment. This concept is first validated by the improved performance of token pruning baselines over CLIP inference, where a condensed token set yields a 0.9% increase in average accuracy when R = 0.9. TCA further enhances its performance by dealing with visual-text misalignment, moving visual features toward historical anchor tokens from CTR. As a result, TCA achieves an average accuracy of 68.69%, outperforming both train-required and training-free baselines without augmentation. Conventional TTA methods perform poorly on all datasets even with the requirement of fine-tuning a large amount of learnable parameters. In contrast, prompt-tuning methods, although requiring fewer learnable parameters, rely heavily on augmentation and struggle to effectively handle visual shifts. While TDA is a training-free method, it requires a large number of hyperparameters (a total of 10 for managing positive and negative caches) to achieve optimal performance. On the other hand, TCA uses significantly fewer hyperparameters and delivers a 1.72% improvement in average accuracy over TDA, with approximately 12.2% fewer GFLOPs. Further details on TDA combined with token condensation baselines can be found in Figure 4c. To verify the universality of the proposed TCA, we examine the impact of the visual backbone (ViT-L/14), where we reduce 48.9% GFLOPs without compromising adaptation.\nOut-of-Distribution Benchmark.\nA consistent observation can be seen in the out-of-distribution (OOD) benchmark, where TCA demonstrates significant improvements over the CLIP baseline under a constrained GFLOPs budget of R = 0.95, as shown in Table 3. TCA outperforms traditional test-time adaptation methods while maintaining efficiency. TCA also achieves superior results on ImageNet-R and ImageNet-S, outperforming TPT without augmentation. Additionally, when compared to other training-based approaches, even those with unlimited computational budgets, TCA delivers comparable performance. However, we observe that TCA does not perform as strongly on the OOD benchmark as it does on the CD benchmark even with a higher rate R. This may be due to the conceptual shifts in OOD datasets, as shown in Section A.4, which could present a challenge for training-free adaptation methods."}, {"title": "ABLATION STUDY", "content": "We conducted a comprehensive ablation study to evaluate TCA's effectiveness and efficiency. For further analysis on reservoir size, merging center, pruning to merging ratio, visual backbone, and more baseline comparisons, see Section A.1."}, {"title": "Impact of Logits Correction Temperature \u03b2", "content": "In Table 2, we examine how different logits correction temperatures \u03b2 affect the adaptation results. The intuition is that with a smaller \u03b2 value, the logits correction will emphasize the tokens in shallower layers (Equation (6)), while a larger \u03b2 value will shift the focus to deeper layers. We observe that a smaller value of \u03b2 is preferred for the Pets dataset as it contains animals as objects, requiring more high-level contextual information for accurate predictions (Raghu et al., 2021). In contrast, for EuroSAT, the best predictions are obtained with larger \u03b2 values, suggesting that low-level, local information is crucial. This aligns well with the nature of the dataset, where different types of land can be distinguished by features such as colors and edges. Nevertheless, our method consistently provides significant improvements across all \u03b2 values, with accuracy gains of up to 20%, highlighting the effectiveness of logits correction using the anchor tokens."}, {"title": "Impact of Correction Weight \u03bb", "content": "To investigate how different correction weights \u03bb affect performance, as described in Equation (6), we conducted experiments across a wide range of \u03bb values, from 2 to 8, as shown in Table 4. We observe that Pets exhibits stable results across different \u03bb values, indicating that less aggressive correction is sufficient. In contrast, datasets such as Flower102 and EuroSAT which initially do not perform well on CLIP, benefit from stronger corrections, achieving their best performance with larger correction weights of 7 and 8, respectively. This highlights the effectiveness of our logits correction module."}, {"title": "Impact of GFLOPs Budget", "content": "We evaluate TCA's performance under different GFLOPs budgets: R = {0.6,0.7, 0.8, 0.9}, resulting in GFLOPs of 9.91, 11.68, 13.27, and 15.45, respectively, compared to the baseline (R = 1, 17.58 GFLOPs). As shown in Figure 4b, condensing inattentive tokens can even enhance performance on certain datasets, notably Pets, and EuroSAT. Specifically for EuroSAT, when R = 0.9, the model's adaptation performance is significantly improved, aligning with our findings in Figure 2a. However, excessively aggressive pruning budgets (e.g., GFLOPs less than 13) lead to significant performance degradation across all datasets. This occurs since higher pruning rates may inadvertently remove informative tokens, causing irreversible harm in training-free scenarios where we lack supervision or the ability to update the model for extra correction."}, {"title": "Impact of Reservoir Saving Strategy", "content": "In Table 5, we examine the performance changes across different reservoir saving strategies. We compare several approaches: First-In, First-Out (FIFO); an uncertainty-based strategy, which discards the most uncertain sample when the reservoir reaches capacity; a similarity-enforced strategy, where samples with high certainty and high cosine similarity to the saved samples are preferred; and a diversity-enforced strategy, which prioritizes saving prototypes that contain distinct tokens compared to those already stored.\nOur results show that the FIFO strategy performs poorly on Flower102 and EuroSAT, likely because CLIP's low confidence leads to retaining misclassified samples. Conversely, Pets has high CLIP zero-shot accuracy (86.91% in Table 1), which makes FIFO acceptable. Among all strategies, the diversity-based approach consistently achieves the best performance. This is intuitive, as it maintains a representative set of features by capturing dataset diversity, whereas entropy-based methods may store homogenous features and overlook multiple class prototypes. By prioritizing diversity, our method ensures that a more representative set of features is maintained, leading to more robust performance across datasets."}, {"title": "Impact of Component", "content": "The impact of the historical anchor A and the head-wise sorting score Shead (Equation (8)) is presented in Table 6. We observe that each component individually contributes to performance improvements. On the Food101 and Pets datasets, incorporating either component yields measurable gains in accuracy. By leveraging historical anchors, the model acquires rich contextual information, enhancing the stability of token importance over time. Simultaneously, cross-head token sorting ensures that token pruning decisions are more robust by accounting for consensus across attention heads. An intriguing case arises with the EuroSAT dataset. Here, the baseline performance without any components is 68.14%. Applying either component alone results in a slight performance decrease. However, when both components are used together, performance significantly improves to 70.43%. This outcome emphasizes the necessity of combining historical anchors and cross-head token sorting to fully realize the model's potential."}, {"title": "Visualization of Proposed Token Condensation", "content": "We visualize the pruned and merged tokens of different ViT layers in Figure 5. Here, the black mask indicates pruned regions while different colors are set for different merging clusters. We observe that as token condensation progresses, non-discriminative tokens are gradually removed, leading to better alignment with the text semantics."}, {"title": "CONCLUSION", "content": "In this paper, we introduced Token Condensation as Adaptation (TCA), a novel training-free test-time adaptation method for CLIP models. Our comprehensive experiments demonstrated that token condensation significantly benefits the visual-text alignment in CLIP, which can further serve as an interpretation of visual semantics. Additionally, our method reduces GFLOPs as a beneficial byproduct, enhancing computational efficiency. For fair comparisons, we fixed the GFLOPs budget by pre-setting R in our experiments; however, the condensing rate can be adaptively estimated using the distances to merging center as an indicator, which can be a promising direction for future research. We also acknowledge the limitations of TCA as a training-free method, particularly its bottleneck in handling datasets with severe distribution shifts, as discussed in Section A.4."}, {"title": "APPENDIX", "content": "This appendix provides additional descriptions of the proposed TDA, including empirical results and algorithm. Visual aids for token condensation are also included to enhance understanding of the proposed method.\n\u2022 Section A.1: Additional Ablation Study.\n\u2022 Section A.2: Token Condensation Algorithm.\n\u2022 Section A.3: Quantitative study for token condensation (R = 0.7).\n\u2022 Section A.4: Potential limitation of TCA."}, {"title": "ADDITIONAL ABLATION STUDY", "content": "Impact of Reservoir Size M. We assess the effectiveness of TCA across various reservoir sizes M on Pets, Flower102, and EuroSAT datasets, as illustrated in Figure 4a. Remarkably, although the best performances are achieved at different reservoir sizes for different datasets, our TCA consistently maintains stable and high performance across a wide range of M values. This showcases the robustness and flexibility of TCA with respect to different reservoir budgets. Notably, even under extreme conditions with a minimal reservoir size (i.e., M = 1), our strategy significantly surpasses the strongest baseline method, TDA, by a large proportion on the EuroSAT dataset (14.2%).\nImpact of Merging Center Number K. We evaluate TCA performance by giving different numbers of merging centers K for Pets, EuroSAT, and Food101 datasets. As shown in Table 7, setting K = 2 consistently yields the best results. This choice balances preserving important information and reducing redundancy. A smaller K (i.e., K = 1) may over-simplify the merging process, leading to the loss of critical details, especially in diverse datasets like EuroSAT. Conversely, increasing K beyond 2 introduces unnecessary complexity and can over-segment the token space, retaining redundant tokens that contribute little to classification. Therefore, maintaining a very small K (where K \u226a N) is sufficient and advantageous.\nImpact of Pruning & Merging Ratio. We experiment with different token pruning and merging ratios under the same computational budget, as shown in Table 8. Incorporating token diversity through merging consistently enhances performance. Specifically, the 2:1 merging-to-pruning ratio outperforms other configurations, especially those favoring pruning. This is because merging preserves diverse token representations by K coresets that pure pruning might discard. When comparing pruning-only (0:1) with the 1:2 merging-pruning ratio on Pets, pruning-only performs better. This may be because the dataset features images with a single prominent object, meaning that pruning background tokens has minimal impact since essential object information remains intact. In contrast, for the EuroSAT dataset, which comprises diverse satellite imagery, simply pruning tokens leads to the loss of important contextual features necessary for accurate classification.\nImpact of Visual Backbone. Trends similar to ViT-B/16 are observed with the ViT-L/14 architecture, as shown in Table 9. TCA consistently surpasses TDA across multiple datasets, including Aircraft, Caltech101, EuroSAT, Flower102, Pets, and UCF101, while adhering to a limited GFLOPS budget (19.6% GFLOPs reduction). Even with a 48.9% reduction in GFLOPs, TCA continues delivering satisfactory results. This demonstrates the scalability and robustness of our method across different model sizes, reinforcing its effectiveness without additional training.\nComparison with State-of-the-Art TTA Using Token Condensation. We additionally evaluate the performance when combining TDA with token pruning and merging baselines and show the performance gain over TDA + ATS in Figure 4c. Although TDA achieves considerable performance gain, it heavily relies on the negative cache and a large set of hyperparameters. In contrast, TCA's accuracy gain significantly surpasses that of TDA + EViT and TDA + ToME across multiple datasets"}, {"title": "ALGORITHM", "content": "Algorithm 1 outlines a simple process for performing token pruning and merging at layer l in a ViT-based CLIP model. We first obtain the averaged anchor token A* from the <cls> tokens saved in the reservoir R. Token condensation is then conducted given the anchor token. Specifically, we conduct token pruning by relative ranking positions of token i across multiple attention heads. Then, coreset selection is used for token merging. Finally, we concatenate the <cls> token v* with the retained tokens as the input for the next layer, where the original N + 1 tokens are shrunk to (RN) + 1, thereby reducing the computational cost."}, {"title": "QUANTITATIVE STUDY", "content": "We visualize the token condensation masks at layer 3, layer 6, and layer 9, and compare them with the original image across multiple datasets, as shown in Figure 6. As the layers go deeper, we observe that class-irrelevant patches are gradually pruned, as indicated by the black mask. TCA also merges class-ambiguous patches, such as fur in cat images, and ground and sky in aircraft and car images. All similar tokens are merged into a single token using our proposed coreset selection strategy. After token condensation, the sample features retain only discriminative information, thereby bridging the gap between visual and text features, and mitigating the distribution shift between pre-trained data and unseen datasets."}, {"title": "DISCUSSION ON THE LIMITATION OF TCA", "content": "In this section, we discuss the potential limitations of our proposed TCA. Due to the training-free nature of the approach, it is challenging to mitigate the performance gap when the testing domain diverges significantly from the training domain. As observed in the out-of-distribution (OOD) samples shown in Figure 7, the ground truth object is not always centrally located, and larger class-irrelevant"}]}