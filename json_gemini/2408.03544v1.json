{"title": "Unlocking the Non-Native Language Context Limitation: Native Language Prompting Facilitates Knowledge Elicitation", "authors": ["Baixuan Li", "Yunlong Fan", "Zhiqiang Gao"], "abstract": "Multilingual large language models (MLLMs) struggle to answer questions posed in non-dominant languages, even though they have already acquired the relevant knowledge from their dominant language corpus. In contrast, human multilinguals can overcome this issue by invoking the relatively rich knowledge acquired from native language texts through Positive Native Language Transfer (PNLT). Inspired by this, we analogize the dominant language of MLLMs to the native language of human multilinguals, and propose Native Language Prompting (NatLan) to simulate the PNLT observed in human multilinguals. It explicitly creates native language contexts for MLLMs to facilitate the elicitation of the rich native language knowledge during question-answering, unlocking the limitations imposed by non-native language contexts on the effective application of knowledge. By employing multi-MLLM collaboration, NatLan reduces the workload on each MLLM in simulating PNLT and refines semantic transfer. On the C-Eval benchmark, NatLan provides up to a 10.1% average accuracy improvement and up to a 5.0% increase in the hard-level subset across five MLLMs, surpassing all top-notch related methods. Our code is available at https://github.com/AnonyNLP/NatLan.", "sections": [{"title": "Introduction", "content": "Multilingual large language models (MLLMs) (Brown et al., 2020; Achiam et al., 2023) have propelled the advancement of nearly all natural language processing tasks across various languages (Xu et al., 2024; Li et al., 2024; Thakur et al., 2024). However, it's observed that MLLMS perform poorly on questions articulated in non-dominant languages, as depicted in Figure 1 (Left), failing to answer some questions that they could address when presented in their dominant language (i.e., the language with the highest proportion during training, such as English for Llama (Touvron et al., 2023a), which accounts for over 70% of the tokens in the pretraining corpus, significantly more than any other languages). This indicates that MLLMs are unable to effectively apply the rich knowledge acquired from dominant language contexts when operating in non-dominant language contexts. Existing work attributes this to the differing volumes (Xue et al., 2021; ImaniGooghari et al., 2023) and quality (Sitaram et al., 2023) of training data in various languages, which leads to insufficient training in non-dominant languages.\nIn contrast, such issues rarely occur in human multilinguals. Although human multilinguals also possess a most proficient language, typically their"}, {"title": "", "content": "native language, they can still correctly answer a question posed in their less proficient non-native languages, provided they have already acquired relevant knowledge in their native language (Nsengiyumva et al., 2021). In cognitive science, the process of using the rich knowledge acquired in one's native language to benefit addressing questions in a less proficient non-native language is known as the Positive Native Language Transfer (PNLT) (Gass and Selinker, 1992). As depicted in Figure 1 (Right), for human multilinguals, the native language regions of their brain are non-selectively activated when addressing questions in a non-native language (Zeng et al., 2022), then they can autonomously perform pre-thinking in their native language before responding, thereby flexibly invoking knowledge acquired in their native language.\nGiven that Ren et al. (2024) have observed significant similarities between MLLMs and the human brain in language processing, we analogize the dominant language of MLLMs (hereafter referred to as the native language) to the native language of human multilinguals. Phenomena similar to PNLT have also been observed to occur autonomously in MLLMs: they tend to generate intermediate representations (Wendler et al., 2024) and output tokens (Marchisio et al., 2024) in their native language when addressing questions posed in a non-native one. However, since the cognitive capabilities of MLLMs fall considerably short of those of the human brain (Chemero, 2023), relying solely on a single MLLM for autonomous implicit processing cannot replicate the PNLT of human multilinguals. Considering that explicit prompts enhance the consistency of MLLMs with brain cognitive language processing (Ren et al., 2024), we attempt to design specific prompting processes that explicitly guide multiple MLLMs to collaboratively simulate the PNLT of human multilinguals when addressing questions in non-native languages. This aims to replicate a brain-like cognitive process, thereby addressing the issue of MLLMs' inability to effectively utilize the rich native language knowledge.\nIn this study, we propose Native Language Prompting (NatLan), which decomposes PNLT simulation into semantic-transferring and answer-generating, sequentially undertaken by two distinct MLLMs, referred to as the Transferor LLM and the Speaker LLM. Through the collaboration of two MLLMs, NatLan reduces the workload on each MLLM involved in simulating the PNLT of human multilinguals, and leverages the outstanding capa-"}, {"title": "", "content": "bilities of the Transferor LLM in the non-native target language (hereafter referred to as the target language) to achieve the semantic transfer from the target language to the native language. As depicted in Figure 1 (Right), NatLan simulates PNLT by first using the Transferor LLM to translate questions from the target language into the native language of the Speaker LLM before the Speaker LLM answers. This approach explicitly creates native language contexts for the Speaker LLM to elicit the rich native language knowledge, unlocking the limitations imposed by the non-native language contexts on the effective application of knowledge when answering questions in the target language.\nApplied to five MLLMs (Speaker LLMs) (Touvron et al., 2023b; Jiang et al., 2023; Team et al., 2024; Abdin et al., 2024), NatLan achieves up to a 10.1% average accuracy improvement in the C-Eval benchmark of question-answering (Huang et al., 2023), as well as up to a 5.0% increase in the hard-level subset, surpassing all top-notch related methods (Schulhoff et al., 2024). Furthermore, we explore how the semantic capabilities of three Transferor LLMs (Bai et al., 2023) impact the effectiveness of NatLan. This study contributes to advancing the understanding of MLLMs from the perspective of explicit PNLT simulation."}, {"title": "Related Work", "content": "Positive Native Language Transfer in Multilingualism. For human multilinguals, previous work (Wu et al., 2022; Gao et al., 2023) indicated that they tend to subconsciously process texts in the native language when using other languages, with the native language regions of the brain being non-selectively activated (Zeng et al., 2022). This facilitates the effective access of native language knowledge to address questions in non-native languages, without the need for the question to be presented specifically in the native language context.\nSimilarly, English-centric LLMs tend to generate intermediate representations (Wendler et al., 2024) and outputs in English (Marchisio et al., 2024). Ren et al. (2024) noted that explicit prompts contribute to the consistency of LLMs with human brain cognitive language processing. Our proposed NatLan explicitly simulates the Positive Native Language Transfer (PNLT) in prompting processes to facilitate the activation of regions similar to the native language areas in the human brain within MLLMs, thereby achieving brain-like knowledge elicitation."}, {"title": "Multi-MLLM Collaboration", "content": "Due to the varying capabilities of different LLMs, previous work has proposed using multiple LLMs to fulfill distinct roles within a collaborative framework (Talebirad and Nadiri, 2023; Dong et al., 2024). In this study, we decomposed the Positive Native Language Transfer (PNLT) simulation into two more straightforward sub-processes: (i) semantic-transferring and (ii) answer-generating. Since one single MLLM's capabilities are insufficient for simulating the PNLT of human multilinguals, we designed a Multi-MLLM Collaboration framework and defined distinct roles for different MLLMs, collaborating to simulate the PNLT progressively, with their respective targets and required characteristics outlined as follows:\n(i) Transferor requires MLLMs that are proficient in the target language and also possess strong capabilities in the native language of the subsequently mentioned Speaker LLMs. It undertakes semantic-transferring: translating questions from the target language into the native language of the Speaker LLMs, and translating the responses of Speaker LLMs back into the target language when required.\n(ii) Speaker requires MLLMs that excel in their native language (the dominant language during training) and are capable of understanding the target language, though not necessarily to an exceptional degree. It undertakes answer-generating: understanding questions translated by the Transferor and providing answers based on their acquired knowledge.\nThe Multi-MLLM Collaboration reduces the workload on each MLLM and alleviates the capability bottlenecks by assigning different MLLMs to each specific sub-process within PNLT."}, {"title": "Native Language Prompting", "content": "Utilizing our constructed Multi-MLLM Collaboration framework, we further proposed Native Language Prompting (NatLan) to simulate the PNLT of human multilinguals. The question-answering workflow is illustrated in Figure 2. As depicted in Figure 2, NatLan initially constructs domain-specific translation prompts (Pink) to provide domain-specific contexts, facilitating the Transferor LLMs' grasp of proper terms specific to the domain. This enables the accurate and coherent semantic transfer of the original questions from the target language to the native language. Subsequently, the proposed NatLan constructs domain-specific Q&A prompts (Blue), which also provide"}, {"title": "", "content": "domain-specific contexts, promoting knowledge recall by the Speaker LLMs for specific domain questions. It is important to note that the Q&A prompts at this stage exhibit the translated question examples, ensuring consistency with the process undertaken by the Speaker LLMs, namely answering the translated questions in their native language. By employing NatLan, we present questions semantically transferred into the native language to the Speaker LLMs before answering, which mimics the PNLT, facilitating the rich native language knowledge elicitation in the Speaker LLMs."}, {"title": "Experiments", "content": "To explore the improvements that NatLan brings to knowledge elicitation, we selected question-answering as the evaluation task because it clearly indicates whether the relevant knowledge in the MLLMs has been correctly elicited. Since the native language (dominant language) of nearly all mainstream MLLMs is English, we have selected English as the native language in this study. Subsequently, considering that the level of knowledge elicitation requires sufficient language resources for comprehensive, multidisciplinary capability evaluation, we chose another representative language, Chinese, as the target language."}, {"title": "", "content": "Dataset. Based on the target language (Chinese), we selected the C-Eval benchmark of question-answering (Huang et al., 2023) to assess the knowledge elicited from MLLMs. C-Eval comprises 13,948 multiple-choice questions across 52 different disciplines (subsets), providing a comprehensive knowledge evaluation in Chinese contexts."}, {"title": "", "content": "NatLan Setup. In the proposed NatLan, the Transferor must be capable of translating the content from the target language (Chinese) into the native language (English) as accurately and coherently as possible. Therefore, we selected the Qwen series MLLMs (Bai et al., 2023) as Transferors, for their leading capabilities in Chinese comprehension among all MLLMs. We chose Qwen models with 4B, 7B, and 14B parameters to analyze the effects of Transferors with varying capabilities on NatLan in \u00a75.5 and \u00a75.6. Additionally, we selected a five representative MLLMs with strong English comprehension skills and the capacity to understand Chinese to serve as Speakers. These include models from the Phi (Abdin et al., 2024), Gemma (Team et al., 2024), Mistral (Jiang et al., 2023),"}, {"title": "", "content": "and Llama (Touvron et al., 2023b) series. For ease of joint deployment with the Transferor LLMs, all these Speaker LLMs possess a moderate parameter scale, ranging from 3.8B to 7B."}, {"title": "", "content": "Baselines. Two top-notch related methods most relevant to the NatLan were selected as baselines: (i) Self-Translation (Etxaniz et al., 2024), which entails a single MLLM sequentially undertaking the semantic-transferring and answer-generating processes, serving both as the Transferor and the Speaker. (ii) Google-MT (Shi et al., 2022), which uses Google Neural Machine Translation (NMT) system (API) as the Transferor and MLLMs as the Speaker. It is important to note that the requirement for Speaker LLMs to possess Chinese comprehension abilities is crucial for conducting Self-Translation and direct evaluations on Chinese questions, ensuring fair performance comparisons."}, {"title": "Overall Performance Results", "content": "We conducted a comparative analysis of performance between the proposed NatLan method and top-notch related methods across the test sets of 52 different disciplines within the C-Eval benchmark.\nAs shown in Table 1, while Self-Translation can bring certain improvements for some Speaker LLMs, the performance enhancement is not stable. This instability arises because Self-Translation uses Speaker LLMs as their own Transferors, encountering Language Comprehension Bottlenecks in the target language. Specifically, it cannot ensure that Speaker LLMs fully comprehend the inherent semantics of the questions in the target language, thus failing to guarantee accurate and coherent semantic transfers. If semantic transfer errors occur during the translation, it can significantly impair the subsequent behavior of Speaker LLMs, potentially causing the performance of Speaker LLMs in their native language to decline below that of directly answering questions in the target language. Google-MT, by incorporating state-of-the-art Google Neural Machine Translation (NMT) systems as Transferors, ensuring relatively high-quality translations and stable performance improvements. Our proposed NatLan further refines this process by employing additional MLLMs with superior semantic understanding capabilities as Transferors. This addresses the shortcomings of NMT systems, which often produce overly literal translations due to a lack of rich semantic abilities,"}, {"title": "", "content": "thus achieving superior semantic transfer (see \u00a75.3 for details). The proposed NatLan achieves optimal performance across all five Speaker LLMs."}, {"title": "NatLan Produces More Relative Improvements", "content": "To explore in more depth, we conducted a detailed performance analysis of Google-MT and our proposed NatLan method on the validation sets of specific disciplines within the C-Eval benchmark.\nWe define our analysis process as follows: Considering each discipline individually, we calculate the relative performance improvements brought by NatLan/Google-MT compared to having Speaker LLMs directly answer questions in Chinese (Original). Specifically, this involves computing the relative increase in the number of correct answers provided by NatLan/Google-MT compared to the Original. Subsequently, we apply Min-Max Normalization to the relative improvements achieved by NatLan/Google-MT across various disciplines, resulting in normalized relative improvements.\nAs shown in Figure 3, NatLan provides more relative improvements than Google-MT in the ma-"}, {"title": "", "content": "jority of disciplines. It is important to note that we have excluded disciplines from this analysis where neither method provided more correct answers than the Original. Additionally, since the performance gains from Self-Translation are quite limited and often result in frequent performance declines, this method has not been included in the analysis."}, {"title": "NatLan Refines Semantic Transfer", "content": "To substantiate NatLan's superiority in refining semantic transfer, we sampled representative questions from the C-Eval test sets for a comparative analysis. Original indicates that the Speaker LLMs respond directly to questions in Chinese."}, {"title": "", "content": "Enhanced Semantic Coherence. Semantic coherence aims to emphasize the relationships between relevant entities in questions and answers. As shown in the first row of Table 2, NatLan uses \"is accessed\" to highlight the relationship between the \"operand\" in the question and the \"addressing method\" in the answers, reducing the difficulty for Speaker LLMs in recalling the relevant knowledge."}, {"title": "", "content": "Enhanced Semantic Accuracy. As shown in the second row of Table 2, NatLan uses \"Kingdom\" instead of \"Country\", which more accurately captures the folkloric connotation of the term. Additionally, it uses \"literacy\" instead of \"quality\","}, {"title": "NatLan Rectifies Knowledge Activation", "content": "In our question-answering task setup, since the Speaker LLMs only need to generate the answer options, the last hidden state for predicting the first token reflects the internal knowledge activation pattern used for answer generation, avoiding extraneous influences introduced when generating tokens in different languages. Therefore, we extract it for more in-depth analysis in knowledge activation.\nAs shown in Figure 4, areas of substantial overlap indicate better alignment of knowledge between the target language (Chinese) and the native language (English). Conversely, the divergences represent different knowledge activations in the Speaker LLMs. When addressing the same questions, significant differences in activation patterns are exhibited when answering directly in Chinese (Original) versus answering based explicitly on knowledge"}, {"title": "", "content": "learned in English through NatLan. Considering the potential correlation between knowledge activation differences and the correctness of responses, we sampled the activations of questions and observed intriguing phenomena, as shown in Figure 6 (Top). (i) When the knowledge to answer a question is correctly activated by directly using Chinese for prompting (Original), the resulting knowledge activation shows minimal differences compared to the English knowledge activation guided by NatLan (Yellow). This confirms"}, {"title": "", "content": "that NatLan effectively simulates PNLT in Speaker LLMs, producing highly similar knowledge activations, i.e. correct answers can be generated independent of different language contexts. (ii) When PNLT cannot occur autonomously and implicitly, and direct prompting in Chinese cannot correctly activate the relevant knowledge, NatLan can explicitly guide the Speaker LLMs to adjust the activation pattern onto the correct track, resulting in significant activation differences (Green).\nIt is important to note that, compared to Google-MT, NatLan provides a more significant corrective effect on knowledge activation, as shown in Figure 6 (Bottom). Google-MT is insufficient to correct"}, {"title": "Impact of Transferor's Semantic Capabilities on NatLan", "content": "Furthermore, we conducted a detailed analysis to evaluate how the semantic capabilities of the Transferor LLMs in the target language affect the overall effectiveness of the proposed NatLan method."}, {"title": "Analysis of NatLan with Different Transferors in Various Domains", "content": "More comprehensively, we conducted a fine-grained analysis of the impact of Transferor LLMs on NatLan across four subdomains and even at the level of individual disciplines within C-Eval.\nAs shown in Figure 5, NatLan consistently achieved stable performance improvements across four subdomains. Moreover, the trends in performance improvements across four subdomains, which correlate with shifts in the semantic capabilities of Transferor LLMs, align closely with the analyses presented in \u00a75.5. Additionally, it can be observed that the degree of improvements brought by NatLan is closely linked to the upper limits of performance of Speaker LLMs in their native language. This implies that when the semantic transfer challenges attributed to Transferors are alleviated, the primary determinant of NatLan's performance increasingly becomes the intrinsic knowledge level"}, {"title": "", "content": "of Speaker LLMs in their native language. Additionally, it should be noted that the degree of performance improvement NatLan delivers varies across more fine-grained disciplines. As shown in Figure 7, in the majority of disciplines, such as Veterinary Medicine (Vet. Med.) and Basic Medicine (Basic Med.), NatLan achieves substantial improvements. We believe that in such disciplines, Speaker LLMs have access to more relevant knowledge in their native language training data compared to the target language. However, in a few rare cases, such as Probability and Statistics (Prob. & Stat.) and Ideological and Moral Cultivation (Ideol.), using NatLan leads to a slight decline in performance. We believe such results are consistent with intuition, as in these disciplines, challenges arise from the complexity of translation, which can lead to semantic transfer errors, or from knowledge that is intimately associated with Chinese. These factors contribute to the diminished performance of Speaker LLMs in their native language (English)."}, {"title": "Conclusion", "content": "It has been observed that MLLMs fail to answer some questions articulated in non-dominant languages, which they could address when presented in their dominant language. To mitigate this, we propose NatLan to simulate PNLT in the cognitive processes of human multilinguals. It reinterprets the effectiveness of the existing translate-first prompting methods from the perspective of PNLT in human multilinguals and suggests employing multi-MLLM collaboration to alleviate the Language Comprehension Bottlenecks and refine semantic transfer, thereby more effectively eliciting relevant knowledge for question-answering. The proposed NatLan achieves up to a 10.1% average accuracy improvement in the C-Eval benchmark, as well as up to a 5.0% increase in the hard-level subset, surpassing all top-notch related methods."}, {"title": "Limitations", "content": "The Speaker LLMs selected for this study all use English as their dominant language (native language). Although we aimed to assess MLLMs with various native languages, the vast majority of existing MLLMs primarily utilize English as their native language. Even if some MLLMs demonstrate stronger capabilities in other languages, they still cannot significantly outperform the performance under English prompting. Therefore, we encourage future research to explore MLLMs with different native languages other than English, or investigate whether the phenomenon of PNLT can be transferred to other non-native languages through alternative methods. Such explorations could have a profound impact on the development of applications for low-resource languages.\nFurthermore, although NatLan significantly enhances the performance of MLLMs, the potential improvements attributable to NatLan are inherently limited by the capabilities of the Transferor LLMs and particularly the Speaker LLMs, where the primary bottlenecks tend to occur. Moreover, as observed in the analysis from \u00a75.6, for a minority of disciplines, NatLan fails to enhance performance. In addition to translation errors produced by Transferor LLMs, another significant factor is that some knowledge is closely tied to specific languages, such as in the Ideology and Moral Cultivation discipline. Employing the native language to address these types of issues may not yield benefits and could instead prevent the successful recall of relevant knowledge. Therefore, we encourage future work to explore the scope of knowledge covered by various languages in MLLMs, aiming to achieve an adaptive and dynamic language switching during question-answering, specifically switching to the language that best encompasses the required knowledge for optimal knowledge elicitation."}, {"title": "Ethical Considerations", "content": "LLMs are prone to generating incorrect and potentially biased information. This issue becomes especially significant when LLMs are tasked with responding to sensitive questions. While NatLan enhances the performance of LLMs, it does not eliminate the issue of producing biased or incorrect statements. In light of some potential issues, this study advocates for usage under research purposes. Cautious deployment is advisable when integrating such systems into user-facing applications."}, {"title": "A Implementation Details", "content": "In this study, to minimize randomness introduced during the sampling process, we standardized the decoding method across all MLLMs to greedy decoding, which includes both Transferor and Speaker LLMs. Furthermore, all MLLMs involved in the experiments are open-source models of the Instruct/Chat version: Phi-3-mini (3.8B) 7, Phi-3-small (7B) 8, Gemma-1.1 (7B) 9, Mistral-0.3 (7B) 10, Llama-2 (7B) 11, Qwen-1.5 (4B) 12, Qwen-2 (7B) 13, and Qwen-1.5 (14B) 14.\nAt the same time, as we deployed Transferor LLMs within NatLan that required designing translation prompts, we used GPT-40 15 to translate the dev sets of various disciplines in the C-Eval benchmark from Chinese to English. This ensures the quality of the translations in the prompts, with each discipline's dev set containing five examples, allowing us to construct five-shot translation prompts for each discipline. We also created five-shot Q&A prompts using the C-Eval dev sets. In practical applications, we provide the MLLMs with prompts corresponding to the discipline currently being tested, thus maximizing the elicitation of their domain-specific knowledge.\nSince the Transferor LLMs and Speaker LLMs used in the proposed NatLan method are required to undertake distinct processes, the former are required to translate questions from the target language to the native language, while the latter are required to provide answers based on the translated questions in the native language. Therefore, they use different sets of prompts. First, we report the details of the translation prompts used in our experiments as follows:"}, {"title": "A.2 Comparative Analysis of Chinese-to-English Translation Cases", "content": "As a supplement to Table 2, we report a more detailed comparative analysis of Chinese-to-English translation cases between Google-MT and the proposed NatLan in Table 4.\nAs shown in Table 4, in the examples from the first two rows, NatLan provides more semantically coherent translations. This coherent semantic description enables Speaker LLMs to more easily understand the relationship between the question and the answer. In the cases presented in the latter two rows, NatLan delivers translations with greater semantic accuracy. For these two questions pertaining to the High School Chemistry discipline, the enriched semantic comprehension of the Transferor LLMs enables NatLan to generate terminology that aligns more closely with domain-specific usage. For instance, it translates to \"combusted\", which is preferred in chemical contexts, rather than the general term \"burned\", and \"Reactivity\" instead of \"The intensity of reaction\".\nThis comparative study further confirms the superiority of NatLan over methods using external NMT systems like Google-MT in terms of semantic transfer during translation. The effective semantic conveyance provided by NatLan enhances the understanding of questions by Speaker LLMs and facilitates knowledge elicitation, thereby yielding superior practical performance."}, {"title": "A.3 Sampled Cases Used for Knowledge Activation", "content": "As a supplement to \u00a75.4, we report cases used to measure differences in knowledge activation in this experiment, which were sampled from the C-Eval val/test sets. Detailed content is shown in Table 5.\nIt should be noted that the reason for excluding the comparison of the Self-Translation method in the experiments for Figure 6 is due to its inability to guarantee basic accuracy in the semantic transfer process. This method may generate incomplete translated questions, preventing the Speaker LLMs from accessing complete question information. Such issues can greatly disrupt overall knowledge activation, making comparisons of activation"}, {"title": "", "content": "differences with this method meaningless. If complete question information cannot be conveyed to the Speaker LLMs, it is akin to the Speaker LLMs addressing an entirely different question, thereby rendering its knowledge activation incomparable.\nAdditionally, as the case shown in the second row of Table 5 is mathematical and lacks substantial textual content, and given that our goal is to demonstrate that the knowledge activation provided by NatLan can unlock the limitations posed by different language contexts on the effective application of knowledge in Speaker LLMs, this case may not effectively illustrate the differences between target language (Chinese) and native language (English) prompt contexts."}, {"title": "A.4 Analysis of NatLan with Different Transferors in Various Domains", "content": "As a supplement to Figure 7, we present a detailed performance analysis of NatLan, employing three different Transferor LLMs applied to various Speaker LLMs, across specific disciplines. These include Phi-3-mini (3.8B) in Figure 9, Gemma-1.1 (7B) in Figure 10, Mistral-0.3 (7B) in Figure 11, and Llama-2 (7B) in Figure 12.\nAs shown in these figures, NatLan has provided widespread and consistent performance improvements across all Speaker LLMs, with only minor performance declines in a very few disciplines. Furthermore, across each Speaker LLM, performance improvements and the disciplines where declines occur vary due to differences in performance preferences, the proportion of different language data in the training corpora, and variations in data sources and quality. This variation highlights that the knowledge elicitation facilitated by NatLan, aside from the influence of Transferor LLMs, is primarily dependent on the capabilities of the Speaker LLMs in their native languages.\nAdditionally, it is important to note that since NatLan relies heavily on the collaboration of MLLMs, it also demands a high level of compliance with instructions from the MLLMs. As shown in Figure 12, Llama-2 (7B), compared to other Speaker LLMs, has relatively weaker instruction-following capabilities. Consequently, it is more prone to producing answers that do not conform to the prescribed format during testing. We applied a strict evaluation criterion in these instances, considering any output that did not meet the established format as incorrect. Thus, the performance improvements brought about by NatLan using different Transferor LLMs on Llama-2 (7B) show relatively greater variability. However, from a holistic perspective, disregarding the variations between different Transferor LLMs, NatLan still manages to provide stable performance improvements for Llama-2 (7B). This further confirms the superiority of the proposed NatLan method."}]}