{"title": "Unlocking the Non-Native Language Context Limitation:\nNative Language Prompting Facilitates Knowledge Elicitation", "authors": ["Baixuan Li", "Yunlong Fan", "Zhiqiang Gao"], "abstract": "Multilingual large language models (MLLMs)\nstruggle to answer questions posed in non-\ndominant languages, even though they have\nalready acquired the relevant knowledge from\ntheir dominant language corpus. In contrast,\nhuman multilinguals can overcome this issue\nby invoking the relatively rich knowledge ac-\nquired from native language texts through Pos-\nitive Native Language Transfer (PNLT). In-\nspired by this, we analogize the dominant\nlanguage of MLLMs to the native language\nof human multilinguals, and propose Native\nLanguage Prompting (NatLan) to simulate the\nPNLT observed in human multilinguals. It ex-\nplicitly creates native language contexts for\nMLLMs to facilitate the elicitation of the rich\nnative language knowledge during question-\nanswering, unlocking the limitations imposed\nby non-native language contexts on the effec-\ntive application of knowledge. By employing\nmulti-MLLM collaboration, NatLan reduces\nthe workload on each MLLM in simulating\nPNLT and refines semantic transfer. On the\nC-Eval benchmark, NatLan provides up to a\n10.1% average accuracy improvement and up\nto a 5.0% increase in the hard-level subset\nacross five MLLMs, surpassing all top-notch\nrelated methods. Our code is available at\nhttps://github.com/AnonyNLP/NatLan.", "sections": [{"title": "1 Introduction", "content": "Multilingual large language models (MLLMs)\nhave\npropelled the advancement of nearly all natu-\nral language processing tasks across various lan-\nguages. However, it's observed that MLLMS\nperform poorly on questions articulated in non-\ndominant languages, as depicted in Figure 1 (Left),\nfailing to answer some questions that they could\naddress when presented in their dominant language\n(i.e., the language with the highest proportion dur-\ning training, such as English for Llama , which accounts for over 70% of\nthe tokens in the pretraining corpus, significantly\nmore than any other languages). This indicates\nthat MLLMs are unable to effectively apply the\nrich knowledge acquired from dominant language\ncontexts when operating in non-dominant language\ncontexts. Existing work attributes this to the dif-\nfering volumes and quality of\ntraining data in various languages, which leads to\ninsufficient training in non-dominant languages.\nIn contrast, such issues rarely occur in human\nmultilinguals. Although human multilinguals also\npossess a most proficient language, typically their"}, {"title": "", "content": "native language, they can still correctly answer a\nquestion posed in their less proficient non-native\nlanguages, provided they have already acquired\nrelevant knowledge in their native language. In cognitive science, the pro-\ncess of using the rich knowledge acquired in one's\nnative language to benefit addressing questions\nin a less proficient non-native language is known\nas the Positive Native Language Transfer (PNLT)\n. As depicted in Figure\n1 (Right), for human multilinguals, the native lan-\nguage regions of their brain are non-selectively ac-\ntivated when addressing questions in a non-native\nlanguage , then they can au-\ntonomously perform pre-thinking in their native\nlanguage before responding, thereby flexibly invok-\ning knowledge acquired in their native language.\nGiven that  have observed signif-\nicant similarities between MLLMs and the human\nbrain in language processing, we analogize the\ndominant language of MLLMs (hereafter referred\nto as the native language) to the native language of\nhuman multilinguals. Phenomena similar to PNLT\nhave also been observed to occur autonomously in\nMLLMs: they tend to generate intermediate repre-\nsentations and output tokens\nin their native language\nwhen addressing questions posed in a non-native\none. However, since the cognitive capabilities of\nMLLMs fall considerably short of those of the hu-\nman brain , relying solely on a\nsingle MLLM for autonomous implicit processing\ncannot replicate the PNLT of human multilinguals.\nConsidering that explicit prompts enhance the\nconsistency of MLLMs with brain cognitive lan-\nguage processing , we attempt to\ndesign specific prompting processes that explicitly\nguide multiple MLLMs to collaboratively simulate\nthe PNLT of human multilinguals when address-\ning questions in non-native languages. This aims\nto replicate a brain-like cognitive process, thereby\naddressing the issue of MLLMs' inability to effec-\ntively utilize the rich native language knowledge.\nIn this study, we propose Native Language\nPrompting (NatLan), which decomposes PNLT\nsimulation into semantic-transferring and answer-\ngenerating, sequentially undertaken by two distinct\nMLLMs, referred to as the Transferor LLM and the\nSpeaker LLM. Through the collaboration of two\nMLLMs, NatLan reduces the workload on each\nMLLM involved in simulating the PNLT of human\nmultilinguals, and leverages the outstanding capa-"}, {"title": "", "content": "bilities of the Transferor LLM in the non-native\ntarget language (hereafter referred to as the target\nlanguage) to achieve the semantic transfer from the\ntarget language to the native language. As depicted\nin Figure 1 (Right), NatLan simulates PNLT by\nfirst using the Transferor LLM to translate ques-\ntions from the target language into the native lan-\nguage of the Speaker LLM before the Speaker LLM\nanswers. This approach explicitly creates native\nlanguage contexts for the Speaker LLM to elicit\nthe rich native language knowledge, unlocking the\nlimitations imposed by the non-native language\ncontexts on the effective application of knowledge\nwhen answering questions in the target language.\nApplied to five MLLMs (Speaker LLMs) \n, NatLan achieves up\nto a 10.1% average accuracy improvement in the\nC-Eval benchmark of question-answering , as well as up to a 5.0% increase\nin the hard-level subset, surpassing all top-notch\nrelated methods . Further-\nmore, we explore how the semantic capabilities of\nthree Transferor LLMs  impact\nthe effectiveness of NatLan. This study contributes\nto advancing the understanding of MLLMs from\nthe perspective of explicit PNLT simulation."}, {"title": "2 Related Work", "content": "Positive Native Language Transfer in Multi-\nlingualism. For human multilinguals, previous\nwork  indicated\nthat they tend to subconsciously process texts in\nthe native language when using other languages,\nwith the native language regions of the brain being\nnon-selectively activated . This\nfacilitates the effective access of native language\nknowledge to address questions in non-native lan-\nguages, without the need for the question to be pre-\nsented specifically in the native language context.\nSimilarly, English-centric LLMs tend to generate\nintermediate representations and outputs in English . Ren\net al.  noted that explicit prompts contribute\nto the consistency of LLMs with human brain cog-\nnitive language processing. Our proposed NatLan\nexplicitly simulates the Positive Native Language\nTransfer (PNLT) in prompting processes to facili-\ntate the activation of regions similar to the native\nlanguage areas in the human brain within MLLMs,\nthereby achieving brain-like knowledge elicitation."}, {"title": "3 Multi-MLLM Collaboration", "content": "Translate First Prompting. Translate First\nPrompting aims to leverage\nthe strength of MLLMs in English. Etxaniz et al.\n introduced Self-Translation, which requires\nMLLMs themselves to perform translation tasks,\nbefore answering questions. However, this encoun-\nters Language Comprehension Bottlenecks: if the\nmodel has poor capabilities in the target language,\nit may not complete the translation task accurately,\nleading to performance instability. Shi et al. (2022)\nused external Neural Machine Translation (NMT)\nsystems to translate the questions. However, unlike\nMLLMS , NMT systems lack rich semantic un-\nderstanding capabilities, resulting in overly literal\ntranslations. Our proposed NatLan reinterprets the\neffectiveness of translate-first prompting from the\nperspective of PNLT in human multilinguals and\nsuggests employing multi-MLLM collaboration to\nachieve a more effective PNLT simulation.\nDue to the varying capabilities of different LLMs,\nprevious work has proposed using multiple LLMs\nto fulfill distinct roles within a collaborative frame-\nwork. In this study, we decomposed the Posi-\ntive Native Language Transfer (PNLT) simulation\ninto two more straightforward sub-processes: (i)\nsemantic-transferring and (ii) answer-generating.\nSince one single MLLM's capabilities are insuf-\nficient for simulating the PNLT of human multi-\nlilinguals, we designed a Multi-MLLM Collabora-\ntion framework and defined distinct roles for differ-\nent MLLMs, collaborating to simulate the PNLT\nprogressively, with their respective targets and re-\nquired characteristics outlined as follows:"}, {"title": "", "content": "(i) Transferor requires MLLMs that are profi-\ncient in the target language and also possess\nstrong capabilities in the native language of\nthe subsequently mentioned Speaker LLMs.\nIt undertakes semantic-transferring: translat-\ning questions from the target language into\nthe native language of the Speaker LLMs, and\ntranslating the responses of Speaker LLMs\nback into the target language when required.\n(ii) Speaker requires MLLMs that excel in their\nnative language (the dominant language dur-\ning training) and are capable of understand-\ning the target language, though not necessar-\nily to an exceptional degree. It undertakes\nanswer-generating: understanding questions\ntranslated by the Transferor and providing\nanswers based on their acquired knowledge.\nThe Multi-MLLM Collaboration reduces the\nworkload on each MLLM and alleviates the ca-\npability bottlenecks by assigning different MLLMs\nto each specific sub-process within PNLT."}, {"title": "4 Native Language Prompting", "content": "Utilizing our constructed Multi-MLLM Collab-\noration framework, we further proposed Native\nLanguage Prompting (NatLan) to simulate the\nPNLT of human multilinguals. The question-\nanswering workflow is illustrated in Figure 2.\nAs depicted in Figure 2, NatLan initially con-\nstructs domain-specific translation prompts (Pink)\nto provide domain-specific contexts, facilitating the\nTransferor LLMs' grasp of proper terms specific to\nthe domain. This enables the accurate and coherent\nsemantic transfer of the original questions from\nthe target language to the native language. Subse-\nquently, the proposed NatLan constructs domain-\nspecific Q&A prompts (Blue), which also provide"}, {"title": "", "content": "domain-specific contexts, promoting knowledge\nrecall by the Speaker LLMs for specific domain\nquestions. It is important to note that the Q&A\nprompts at this stage exhibit the translated question\nexamples, ensuring consistency with the process\nundertaken by the Speaker LLMs, namely answer-\ning the translated questions in their native language.\nBy employing NatLan, we present questions se-\nmantically transferred into the native language to\nthe Speaker LLMs before answering, which mim-\nics the PNLT, facilitating the rich native language\nknowledge elicitation in the Speaker LLMs."}, {"title": "5 Experiments", "content": "To explore the improvements that NatLan brings\nto knowledge elicitation, we selected question-\nanswering as the evaluation task because it clearly\nindicates whether the relevant knowledge in the\nMLLMs has been correctly elicited. Since the na-\ntive language (dominant language) of nearly all\nmainstream MLLMs is English, we have selected\nEnglish as the native language in this study. Sub-\nsequently, considering that the level of knowledge\nelicitation requires sufficient language resources\nfor comprehensive, multidisciplinary capability\nevaluation, we chose another representative lan-\nguage, Chinese, as the target language.\nDataset. Based on the target language (Chinese),\nwe selected the C-Eval benchmark of question-\nanswering  to assess the knowl-\nedge elicited from MLLMs. C-Eval comprises\n13,948 multiple-choice questions across 52 differ-\nent disciplines (subsets), providing a comprehen-\nsive knowledge evaluation in Chinese contexts.\nNatLan Setup. In the proposed NatLan, the\nTransferor must be capable of translating the con-\ntent from the target language (Chinese) into the\nnative language (English) as accurately and coher-\nently as possible. Therefore, we selected the Qwen\nseries MLLMs as Transferors, for\ntheir leading capabilities in Chinese comprehen-\nsion among all MLLMs. We chose Qwen models\nwith 4B, 7B, and 14B parameters to analyze the\neffects of Transferors with varying capabilities on\nNatLan in \u00a75.5 and \u00a75.6. Additionally, we selected\na five representative MLLMs with strong English\ncomprehension skills and the capacity to under-\nstand Chinese to serve as Speakers. These include\nmodels from the Phi , Gemma\n, Mistral ,which entails a single MLLM sequentially undertaking\nthe semantic-transferring and answer-generating\nprocesses, serving both as the Transferor and the\nSpeaker. Google-MT , which\nuses Google Neural Machine Translation (NMT)\nsystem (API) as the Transferor and MLLMs as\nthe Speaker. It is important to note that the re-\nquirement for Speaker LLMs to possess Chinese\ncomprehension abilities is crucial for conducting\nSelf-Translation and direct evaluations on Chinese\nquestions, ensuring fair performance comparisons.\nMore details are available in Appendix A.1."}, {"title": "5.1 Overall Performance Results", "content": "We conducted a comparative analysis of perfor-\nmance between the proposed NatLan method and\ntop-notch related methods across the test sets of 52\ndifferent disciplines within the C-Eval benchmark.\nAs shown in Table 1, while Self-Translation\ncan bring certain improvements for some Speaker\nLLMs, the performance enhancement is not sta-\nble. This instability arises because Self-Translation\nuses Speaker LLMs as their own Transferors, en-\ncountering Language Comprehension Bottlenecks\nin the target language. Specifically, it cannot ensure\nthat Speaker LLMs fully comprehend the inherent\nsemantics of the questions in the target language,\nthus failing to guarantee accurate and coherent se-\nmantic transfers. If semantic transfer errors occur\nduring the translation, it can significantly impair\nthe subsequent behavior of Speaker LLMs, poten-\ntially causing the performance of Speaker LLMs\nin their native language to decline below that of\ndirectly answering questions in the target language.\nGoogle-MT, by incorporating state-of-the-art\nGoogle Neural Machine Translation (NMT) sys-\ntems as Transferors, ensuring relatively high-\nquality translations and stable performance im-\nprovements. Our proposed NatLan further refines\nthis process by employing additional MLLMs with\nsuperior semantic understanding capabilities as\nTransferors. This addresses the shortcomings of\nNMT systems, which often produce overly literal\ntranslations due to a lack of rich semantic abilities,"}, {"title": "5.2 NatLan Produces More Relative\nImprovements", "content": "To explore in more depth, we conducted a detailed\nperformance analysis of Google-MT and our pro-\nposed NatLan method on the validation sets of spe-\ncific disciplines within the C-Eval benchmark.\nWe define our analysis process as follows: Con-\nsidering each discipline individually, we calculate\nthe relative performance improvements brought by\nNatLan/Google-MT compared to having Speaker\nLLMs directly answer questions in Chinese (Origi-\nnal). Specifically, this involves computing the rel-\native increase in the number of correct answers\nprovided by NatLan/Google-MT compared to the\nOriginal. Subsequently, we apply Min-Max Nor-\nmalization to the relative improvements achieved\nby NatLan/Google-MT across various disciplines,\nresulting in normalized relative improvements.\nAs shown in Figure 3, NatLan provides more\nrelative improvements than Google-MT in the ma-"}, {"title": "5.3 NatLan Refines Semantic Transfer", "content": "To substantiate NatLan's superiority in refining se-\nmantic transfer, we sampled representative ques-\ntions from the C-Eval test sets for a comparative\nanalysis. Original indicates that the Speaker LLMs\nrespond directly to questions in Chinese.\nEnhanced Semantic Coherence. Semantic co-\nherence aims to emphasize the relationships be-\ntween relevant entities in questions and answers.\nAs shown in the first row of Table 2, NatLan uses\n\"is accessed\" to highlight the relationship between\nthe \"operand\" in the question and the \"addressing\nmethod\" in the answers, reducing the difficulty for\nSpeaker LLMs in recalling the relevant knowledge.\nEnhanced Semantic Accuracy. As shown in the\nsecond row of Table 2, NatLan uses \"Kingdom\"\ninstead of \"Country\", which more accurately cap-"}, {"title": "5.4 NatLan Rectifies Knowledge Activation", "content": "In our question-answering task setup, since the\nSpeaker LLMs only need to generate the answer\noptions, the last hidden state for predicting the first\ntoken reflects the internal knowledge activation pat-\ntern used for answer generation, avoiding extrane-\nous influences introduced when generating tokens\nin different languages. Therefore, we extract it for\nmore in-depth analysis in knowledge activation.\nAs shown in Figure 4, areas of substantial over-\nlap indicate better alignment of knowledge between\nthe target language (Chinese) and the native lan-\nguage (English). Conversely, the divergences repre-\nsent different knowledge activations in the Speaker\nLLMs. When addressing the same questions, sig-\nnificant differences in activation patterns are exhib-\nited when answering directly in Chinese (Original)\nversus answering based explicitly on knowledge"}, {"title": "5.5 Impact of Transferor's Semantic\nCapabilities on NatLan", "content": "that NatLan effectively simulates PNLT in Speaker\nLLMs, producing highly similar knowledge activa-\ntions, i.e. correct answers can be generated inde-\npendent of different language contexts. (ii) When\nPNLT cannot occur autonomously and implicitly,\nand direct prompting in Chinese cannot correctly\nactivate the relevant knowledge, NatLan can explic-\nitly guide the Speaker LLMs to adjust the activation\npattern onto the correct track, resulting in signifi-\ncant activation differences (Green).\nIt is important to note that, compared to Google-\nMT, NatLan provides a more significant corrective\neffect on knowledge activation, as shown in Figure\n6 (Bottom). Google-MT is insufficient to correct"}, {"title": "5.6 Analysis of NatLan with Different\nTransferors in Various Domains", "content": "More comprehensively, we conducted a fine-\ngrained analysis of the impact of Transferor LLMs\non NatLan across four subdomains and even at the\nlevel of individual disciplines within C-Eval.\nAs shown in Figure 5, NatLan consistently\nachieved stable performance improvements across\nfour subdomains. Moreover, the trends in per-\nformance improvements across four subdomains,\nwhich correlate with shifts in the semantic capa-\nbilities of Transferor LLMs, align closely with the\nanalyses presented in \u00a75.5. Additionally, it can be\nobserved that the degree of improvements brought\nby NatLan is closely linked to the upper limits of\nperformance of Speaker LLMs in their native lan-\nguage. This implies that when the semantic transfer\nchallenges attributed to Transferors are alleviated,\nthe primary determinant of NatLan's performance\nincreasingly becomes the intrinsic knowledge level"}, {"title": "6 Conclusion", "content": "It has been observed that MLLMs fail to answer\nsome questions articulated in non-dominant lan-\nguages, which they could address when presented\nin their dominant language. To mitigate this, we\npropose NatLan to simulate PNLT in the cogni-\ntive processes of human multilinguals. It reinter-\nprets the effectiveness of the existing translate-first\nprompting methods from the perspective of PNLT\nin human multilinguals and suggests employing\nmulti-MLLM collaboration to alleviate the Lan-\nguage Comprehension Bottlenecks and refine se-\nmantic transfer, thereby more effectively eliciting\nrelevant knowledge for question-answering. The\nproposed NatLan achieves up to a 10.1% average\naccuracy improvement in the C-Eval benchmark,\nas well as up to a 5.0% increase in the hard-level\nsubset, surpassing all top-notch related methods."}, {"title": "Limitations", "content": "The Speaker LLMs selected for this study all use\nEnglish as their dominant language (native lan-\nguage). Although we aimed to assess MLLMs with\nvarious native languages, the vast majority of exist-\ning MLLMs primarily utilize English as their na-\ntive language. Even if some MLLMs demonstrate\nstronger capabilities in other languages, they still\ncannot significantly outperform the performance\nunder English prompting. Therefore, we encourage\nfuture research to explore MLLMs with different\nnative languages other than English, or investigate\nwhether the phenomenon of PNLT can be trans-\nferred to other non-native languages through alter-\nnative methods. Such explorations could have a\nprofound impact on the development of applica-\ntions for low-resource languages.\nFurthermore, although NatLan significantly en-\nhances the performance of MLLMs, the potential\nimprovements attributable to NatLan are inherently\nlimited by the capabilities of the Transferor LLMs\nand particularly the Speaker LLMs, where the pri-\nmary bottlenecks tend to occur. Moreover, as ob-\nserved in the analysis from \u00a75.6, for a minority of\ndisciplines, NatLan fails to enhance performance.\nIn addition to translation errors produced by Trans-\nferor LLMs, another significant factor is that some\nknowledge is closely tied to specific languages,\nsuch as in the Ideology and Moral Cultivation dis-\ncipline. Employing the native language to address\nthese types of issues may not yield benefits and\ncould instead prevent the successful recall of rele-\nvant knowledge. Therefore, we encourage future\nwork to explore the scope of knowledge covered by\nvarious languages in MLLMs, aiming to achieve\nan adaptive and dynamic language switching dur-\ning question-answering, specifically switching to\nthe language that best encompasses the required\nknowledge for optimal knowledge elicitation."}, {"title": "Ethical Considerations", "content": "LLMs are prone to generating incorrect and po-\ntentially biased information. This issue becomes\nespecially significant when LLMs are tasked with\nresponding to sensitive questions. While NatLan\nenhances the performance of LLMs, it does not\neliminate the issue of producing biased or incorrect\nstatements. In light of some potential issues, this\nstudy advocates for usage under research purposes.\nCautious deployment is advisable when integrating\nsuch systems into user-facing applications."}, {"title": "A Appendix", "content": "translator. Translation rules: Proper\nnouns in English or Chinese need to be\nretained without translation, retain the\noriginal meaning to the greatest extent,\nand follow the original format in the\ntranslation process."}, {"title": "A.1 Implementation Details", "content": "In this study, to minimize randomness introduced\nduring the sampling process, we standardized the\ndecoding method across all MLLMs to greedy\ndecoding, which includes both Transferor and\nSpeaker LLMs. Furthermore, all MLLMs involved\nin the experiments are open-source models of the\nInstruct/Chat version: Phi-3-mini (3.8B) 7, Phi-3-\nsmall (7B) 8, Gemma-1.1 (7B) 9, Mistral-0.3 (7B)\n10, Llama-2 (7B) 11, Qwen-1.5 (4B) 12, Qwen-2\n(7B) 13, and Qwen-1.5 (14B) 14.\nAt the same time, as we deployed Transferor\nLLMs within NatLan that required designing trans-\nlation prompts, we used GPT-40 15 to translate\nthe dev sets of various disciplines in the C-Eval\nbenchmark from Chinese to English. This en-\nsures the quality of the translations in the prompts,\nwith each discipline's dev set containing five exam-\nples, allowing us to construct five-shot translation\nprompts for each discipline. We also created five-\nshot Q&A prompts using the C-Eval dev sets. In\npractical applications, we provide the MLLMs with\nprompts corresponding to the discipline currently\nbeing tested, thus maximizing the elicitation of\ntheir domain-specific knowledge.\nSince the Transferor LLMs and Speaker LLMs\nused in the proposed NatLan method are required\nto undertake distinct processes, the former are\nrequired to translate questions from the target\nlanguage to the native language, while the latter are\nrequired to provide answers based on the translated\nquestions in the native language. Therefore, they\nuse different sets of prompts. First, we report\nthe details of the translation prompts used in our\nexperiments as follows:"}, {"title": "A.2 Comparative Analysis of\nChinese-to-English Translation Cases", "content": "As a supplement to Table 2, we report a more de-\ntailed comparative analysis of Chinese-to-English\ntranslation cases between Google-MT and the pro-\nposed NatLan in Table 4.\nAs shown in Table 4, in the examples from the\nfirst two rows, NatLan provides more semantically\ncoherent translations. This coherent semantic de-\nscription enables Speaker LLMs to more easily\nunderstand the relationship between the question\nand the answer. In the cases presented in the latter\ntwo rows, NatLan delivers translations with greater\nsemantic accuracy. For these two questions pertain-\ning to the High School Chemistry discipline, the\nenriched semantic comprehension of the Transferor\nLLMs enables NatLan to generate terminology that\naligns more closely with domain-specific usage.\nFor instance, it translates to \"combusted\", which\nis preferred in chemical contexts, rather than the\ngeneral term \"burned\", and \"Reactivity\" instead of\n\"The intensity of reaction\".\nThis comparative study further confirms the su-\nperiority of NatLan over methods using external\nNMT systems like Google-MT in terms of semantic\ntransfer during translation. The effective seman-\ntic conveyance provided by NatLan enhances the\nunderstanding of questions by Speaker LLMs and\nfacilitates knowledge elicitation, thereby yielding\nsuperior practical performance."}, {"title": "A.3 Sampled Cases Used for Knowledge\nActivation", "content": "As a supplement to \u00a75.4, we report cases used to\nmeasure differences in knowledge activation in this\nexperiment, which were sampled from the C-Eval\nval/test sets. Detailed content is shown in Table 5.\nIt should be noted that the reason for exclud-\ning the comparison of the Self-Translation method\nin the experiments for Figure 6 is due to its in-\nability to guarantee basic accuracy in the semantic\ntransfer process. This method may generate incom-\nplete translated questions, preventing the Speaker\nLLMs from accessing complete question informa-\ntion. Such issues can greatly disrupt overall knowl-\nedge activation, making comparisons of activation"}, {"title": "", "content": "differences with this method meaningless. If com-\nplete question information cannot be conveyed to\nthe Speaker LLMs, it is akin to the Speaker LLMs\naddressing an entirely different question, thereby\nrendering its knowledge activation incomparable.\nAdditionally, as the case shown in the second\nrow of Table 5 is mathematical and lacks substan-\ntial textual content, and given that our goal is to\ndemonstrate that the knowledge activation provided\nby NatLan can unlock the limitations posed by dif-\nferent language contexts on the effective applica-\ntion of knowledge in Speaker LLMs, this case may\nnot effectively illustrate the differences between\ntarget language (Chinese) and native language (En-\nglish) prompt contexts."}, {"title": "A.4 Analysis of NatLan with Different\nTransferors in Various Domains", "content": "As a supplement to Figure 7, we present a de-\ntailed performance analysis of NatLan, employing\nthree different Transferor LLMs applied to various\nSpeaker LLMs, across specific disciplines. These\ninclude Phi-3-mini (3.8B) in Figure 9, Gemma-1.1\n(7B) in Figure 10, Mistral-0.3 (7B) in Figure 11,\nand Llama-2 (7B) in Figure 12.\nAs shown in these figures, NatLan has provided\nwidespread and consistent performance improve-\nments across all Speaker LLMs, with only minor\nperformance declines in a very few disciplines.\nFurthermore, across each Speaker LLM, perfor-\nmance improvements and the disciplines where de-\nclines occur vary due to differences in performance\npreferences, the proportion of different language\ndata in the training corpora, and variations in data\nsources and quality. This variation highlights that\nthe knowledge elicitation facilitated by NatLan,\naside from the influence of Transferor LLMs, is pri-\nmarily dependent on the capabilities of the Speaker\nLLMs in their native languages.\nAdditionally, it is important to note that since\nNatLan relies heavily on the collaboration of\nMLLMs, it also demands a high level of compli-\nance with instructions from the MLLMs. As shown\nin Figure 12, Llama-2 (7B), compared to other\nSpeaker LLMs, has relatively weaker instruction-\nfollowing capabilities. Consequently, it is more\nprone to producing answers that do not conform\nto the prescribed format during testing. We ap-\nplied a strict evaluation criterion in these instances,\nconsidering any output that did not meet the estab-\nlished format as incorrect. Thus, the performance\nimprovements brought about by NatLan using dif-\nferent Transferor LLMs on Llama-2 (7B) show rel-\native greater variability. However, from a holistic\nperspective, disregarding the variations between\ndifferent Transferor LLMs, NatLan still manages\nto provide stable performance improvements for\nLlama-2 (7B). This further confirms the superiority\nof the proposed NatLan method.\nFurthermore, we have reported the detailed per-\nformance evaluation scores of NatLan and top-\nnotch related methods in Table 7 for all settings, as\na supplement to Table 1 and Table 3"}]}