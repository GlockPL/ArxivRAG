{"title": "A Collaborative Ensemble Construction Method for Federated Random Forest", "authors": ["Penjan Antonio Eng Lim", "Cheong Hee Park"], "abstract": "Random forests are considered a cornerstone in machine learning for their robustness and versatility. Despite these strengths, their conventional centralized training is ill-suited for the modern landscape of data that is often distributed, sensitive, and subject to privacy concerns. Federated learning (FL) provides a compelling solution to this problem, enabling models to be trained across a group of clients while maintaining the privacy of each client's data. However, adapting tree-based methods like random forests to federated settings introduces significant challenges, particularly when it comes to non-identically distributed (non-IID) data across clients, which is a common scenario in real-world applications. This paper presents a federated random forest approach that employs a novel ensemble construction method aimed at improving performance under non-IID data. Instead of growing trees independently in each client, our approach ensures each decision tree in the ensemble is iteratively and collectively grown across clients. To preserve the privacy of the client's data, we confine the information stored in the leaf nodes to the majority class label identified from the samples of the client's local data that reach each node. This limited disclosure preserves the confidentiality of the underlying data distribution of clients, thereby enhancing the privacy of the federated learning process. Furthermore, our collaborative ensemble construction strategy allows the ensemble to better reflect the data's heterogeneity across different clients, enhancing its performance on non-IID data, as our experimental results confirm.", "sections": [{"title": "1. Introduction", "content": "The Random Forest (RF) model, introduced by Breiman (2001), is an ensemble of decision trees renowned for its robustness against overfitting and its high efficacy in both classification and regression tasks. It comprises numerous individual decision trees, each contributing to a collective prediction. This aggregation of multiple trees allows random forests to gain a detailed and accurate understanding of the data, which leads to substantially improved generalization ability and enhances their accuracy and stability compared to single decision tree models.\nHowever, the traditional centralized training approach of RF models encounters significant limitations in the modern digital landscape, where data is frequently distributed across various locations, decentralized in nature, and often encompasses sensitive information. This paradigm shift in data handling has been driving"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Federated Learning", "content": "Federated learning (FL) (McMahan et al., 2017) facilitates collaborative learning by training a global model through the iterative aggregation of updates from decentralized clients. This approach is particularly crucial in data privacy and security scenarios. The conventional methodology for FL, known as FedAvg, involves a multi-step process. Each round of FedAvg commences with a central server sending a global model to all participating parties. These parties then locally update their models using gradient descent. Subsequently, these locally updated models are sent back to the central server, where they are averaged to refine the global model for the next training round.\nThe presence of non-independent and identically distributed (non-IID) data among clients poses significant challenges in federated learning environments (Kairouz et al., 2021). In the context of federated deep learning, various methods have been proposed in recent years to address the challenges of non-IID data. T. Li et al. (2020) introduced FedProx, a federated learning method designed for heterogeneous devices, which allows for variable amounts of work to be performed locally across devices and relies on a proximal term to restrict local updates to be closer to the global model. Y. Zhao et al. (2018) proposed a method where a small subset of globally shared data is used in conjunction with private data from each client to train local models. This approach aims to mitigate the effects of non-IID data by leveraging a shared dataset across all clients.\nPersonalized federated learning has emerged as another approach to address the challenges of non-IID data in deep learning. Instead of relying on a single global model shared across all clients, personalized federated learning aims to create customized models for each client, taking into account their heterogeneous data distributions. In this paradigm, each client maintains its own local model and periodically communicates with a cloud server to update a shared global model (Tan et al., 2023; Tu et al., 2024)."}, {"title": "2.2. Federated Learning Based on Random Forests", "content": "Random forests (RF) (Breiman, 2001) are a robust ensemble learning method used for classification and regression tasks. An RF model combines numerous decision trees, each contributing to the final prediction. This method utilizes the technique of 'bagging' or bootstrap aggregating, where multiple learners are trained independently, and their predictions are combined to enhance the model's accuracy and stability (Breiman, 1996). Each decision tree in an RF is constructed using a subset of the training data via bootstrap sampling, a process in which data points are chosen randomly with replacement. During the tree-building process, only a randomly selected subset of features is considered for splitting at each node. For predictions, the model aggregates the outputs of individual trees. In classification tasks, the final prediction is determined by selecting the class label that is most commonly predicted among all trees based on a principle of majority voting.\nUnlike models that rely on gradient-based optimization, the model parameters in decision trees, including the criteria for splitting nodes and the values in leaf nodes, are derived from statistical processes and cannot be updated using traditional gradient descent methods. This discrepancy necessitates the development of tailored algorithms and systems specifically for tree-based models within the federated learning framework.\nAdapting the random forest algorithm to a federated environment is a growing area of research. The prevalent method involves training decision trees or their ensembles independently on decentralized clients and later aggregating them server-side to construct a global ensemble model. This approach, which we refer to as Non-Collaborative Federated Forest (NCFF), lacks collaborative training of trees across clients."}, {"title": "2.3. Other Tree-based Federated Learning Models", "content": "Gradient Boosted Decision Trees (GBDT) (Friedman, 2001) are a popular machine learning algorithm that employ a \"boosting\u201d technique (Freund & Schapire, 1997), in which trees are trained sequentially, with each new tree aiming to correct the errors made by previously built trees, thereby incrementally improving the model's accuracy over iterations. XGBoost (Chen & Guestrin, 2016) is a highly optimized and widely used implementation of the GBDT algorithm, recognized for its efficiency, scalability, and robustness.\nIn recent years, there have been several efforts to adapt tree-based models like GBDTs and XGBoost to federated learning settings. For instance, Q. Li et al., (2023) introduced the FedTree system, a unified histogram-sharing scheme, where local histograms are computed by parties and aggregated by a server. This approach incorporates cryptographic methods and differential privacy to protect the communicated messages."}, {"title": "3. Proposed Federated Random Forest", "content": "Our method aims to construct a random forest model in an FL setting, where a small number k of entities or organizations, referred to as clients, collaborate in growing an ensemble of decision trees, alongside a global server. This ensemble, maintained by the server, is denoted as \\(E = \\{t_1, t_2, \u2026, t_m\\}\\). We specifically assume a scenario where \\(m \u2265 k\\). Each client contains a local training dataset, and all datasets share the same feature space. Our proposed method, illustrated in Figure 1, encompasses three steps: Initialization, Tree Growing, and Tree Adjustment. Each step is further elaborated in the subsequent subsections."}, {"title": "3.1. Initialization", "content": "Diverging from the traditional approach of growing the trees independently on each client, our approach employs an iterative process where each decision tree in the ensemble is grown collaboratively across the federation of clients. This strategy addresses the challenge of non-IID data in federated learning environments.\nThe server initiates the model training by generating an ensemble of empty decision trees, represented as \\(E = \\{t_1, t_2, \u2026, t_m\\}\\). The server then implements a random permutation of the client list for each tree, ensuring an equitable and unbiased participation from each client in the tree growth process. These permutations determine the order in which each tree is scheduled to be distributed to the clients for growth."}, {"title": "3.2. Tree Growing", "content": "In each iteration of growth, denoted as h, the trees in the ensemble are divided into subsets \\(\\{G_{h1}, G_{h2}, ..., G_{hk}\\}\\). Each subset \\(G_{hi}\\) is formed by selecting the trees whose client permutation contains client i at position h. Thus, each subset \\(G_{hi}\\) comprises the trees designated for growth by client i in the h-th iteration.\nConcurrently, each client receives and grows their assigned subset of trees, utilizing its local data. In the first iteration (h = 1), the initially empty trees in E are grouped and distributed to the first client in their respective permutation order for growth. In each subsequent iteration, the trees, now partially grown, are passed to the next clients in their permutation sequence for further growth. Once the clients complete the growth of their respective tree subsets, they are returned to the server and aggregated to update the global ensemble model. This collective learning approach enables the trees to encapsulate a diverse range of data distributions, thereby enhancing the robustness of the resulting model.\nThe client-side process of the tree growth is detailed in Table 1. In this phase, each client receives a subset of decision trees, \\(G\\), from the server's global ensemble E. The client then grows each tree in \\(G\\) using its local data. First, the client creates a bootstrap sample Z of its data, using random sampling with replacement, as originally proposed by Breiman (2001). If the tree is initially empty, a root node is established. Otherwise, the data Z is sent down from the existing root to the child nodes based on the partitioning condition of each"}, {"title": "3.3. Tree Adjustment", "content": "Once the iterative growing process is complete, the server redistributes the grown trees back to the clients for leaf node adjustment. Table 2 describes the leaf node adjustment process. This process plays a crucial role in refining the decision trees to align with the unique data characteristics of each client. Up to this point, the leaf nodes in the decision trees remain empty, as the ensemble construction process aims to simply set up a generalized tree structure by identifying informative split points across the overall federation of clients. The leaf node adjustment process focuses on updating the information at the leaf nodes to reflect the majority class label derived from the samples of each client's data that reach them.\nIn this process, each client receives an ensemble of decision trees, denoted as \\(E\\), from the server. Each client then adjusts the leaf nodes of each tree in \\(E\\), using its local data. The client's entire dataset is passed through the tree. This traversal uses the split conditions established during the tree growing phase, directing data instances from the root to the respective leaf nodes. For each leaf node, the algorithm calculates the frequency of each class present within the subset of data, D, that reaches this node. The class with the highest frequency in D is designated as the majority class label \\(l\\) and stored in the leaf node. Due to our model's design, where the collective growth of the tree structure uses data from all clients, but individual leaf nodes are formed from the data of specific clients, some deep leaf nodes may not receive data at a particular client and thus remain empty."}, {"title": "3.4. Prediction", "content": "In the prediction phase, each test sample is passed through the final ensemble of decision trees E, which comprises the global model. As the sample traverses a tree, it follows the decision paths that were established during the ensemble construction phase, leading it to a particular leaf node in each tree. At each leaf node, the sample is associated with a list L, representing the majority class labels determined by each client that reaches the node during the leaf node adjustment phase.\nThe lists of majority class labels from all trees are aggregated and the frequency of each class label is computed. The final class prediction for the test sample is determined through majority voting, where the class label with the highest frequency is selected. This method ensures a consensus prediction that reflects the heterogeneous data distributions across clients. In cases where multiple classes get the same number of votes, the tie is resolved by randomly selecting one of these classes for the final prediction."}, {"title": "3.5. Discussions on Privacy Preservation", "content": "Our proposed federated random forest model incorporates several techniques to protect the privacy of the participating clients' data during the collaborative tree construction process and the leaf node adjustment phase. Unlike the Collaborative Federated Forest with Class Probabilities (CFF-CP) model, which we introduced preliminarily in a conference paper (Eng Lim et al., 2023), where clients store class frequencies at the leaf nodes and the server aggregates them and normalizes them to calculate class probabilities, our method employs a more privacy-preserving approach by storing only the majority class label at each leaf node. This design choice reduces the granularity of information shared across clients, making it more difficult for an adversary to infer detailed information about the data of individual clients while still enabling effective collaborative learning."}, {"title": "4. Experimental Evaluation", "content": ""}, {"title": "4.1. Experimental Settings", "content": "We evaluated the performance of our model using seven benchmark datasets from the UCI repository (Dua & Graff, 2017): Pendigits, Dry Bean, Letter Recognition, Statlog (Landsat Satellite), Nursery, Hand Postures, and Covertype. All datasets were partitioned with an 80:20 train-test split for every class. Each experiment was run 10 times with different random partitions, and the average results are reported. We conducted experiments under simulated non-IID and IID conditions, distributing the training data across k = 10 clients, employing an ensemble of 100 trees.\nFor the non-IID setting, we used an alpha chunking strategy to distribute the data across clients, as proposed by McMahan et al. (2017). This approach divides each class in the training dataset into \u03b1 chunks, shuffles these chunks from all classes, and then allocates them to the clients in a round-robin fashion. The maximum number of classes each client receives is given by:\n\\(M = \\lfloor \\frac{c \\cdot \\alpha}{k} \\rfloor\\)  (1)\nwhere c is the total number of classes in the dataset. As \u03b1 increases, the data distribution among clients resembles IID conditions more closely. A higher \u03b1 implies a larger share of classes per client, reducing the non-IID nature of the data. We carefully selected the \u03b1 value for each dataset based on the total number of classes c. Our objective was to ensure that the maximum number of classes allocated to clients, M, did not exceed 3, while preventing clients from receiving data exclusively from a single class. This choice was designed to strike a balance between maintaining non-IID conditions and avoiding overly skewed distributions, as further discussed in Section 4.3.1. In contrast, the IID environment was structured to provide each client with an equal proportion of each class, ensuring a balanced and representative dataset for every client."}, {"title": "4.2. Experiment Results", "content": "We evaluated the performance of our federated random forest model and compared it with the following approaches:\n\u2022 Centralized RF: The centralized random forest model, where all the data resides in a central server for model training and evaluation, is used as a reference point for performance. It represents the conventional method of random forest training without the constraints of federated learning.\n\u2022 Markovic et al. (2022): This approach, proposed by Markovic et al. (2022), involves each client independently training a random forest model using its subset of data. The top-performing trees from each client are then selected based on their accuracy or weighted accuracy, calculated using a validation set.\n\u2022 Non-Collaborative Federated Forest (NCFF) model: As described in Section 2.3, this model symbolizes the foundational approach in federated learning for random forests. It involves training an ensemble of 10 trees on each client independently, without any collaborative learning across clients. After the local training, the ensembles from each of the 10 clients are transmitted back to a central server, where they are aggregated to construct a global model comprising 100 trees.\n\u2022 FedTree (Q. Li et al., 2023): FedTree is a federated learning framework designed for GBDTs. It utilizes a histogram-sharing scheme, in which, for the construction of each node, clients generate local histograms from their data and transmit them to the server. The server aggregates the received histograms and uses them to identify the optimal split for the node. This process is repeated iteratively to construct the tree ensemble.\n\u2022 Collaborative Federated Forest with Class Probabilities (CFF-CP) model: This model utilizes the same collaborative ensemble tree growing strategy as our proposed approach, but it adopts a different leaf node adjustment strategy. During the leaf adjustment phase, the entire local data from each client is sent down each tree in the ensemble to adjust the information at the leaf nodes. At each leaf node,"}, {"title": "4.3. Ablation Study", "content": "This ablation study investigates the impact of various parameters on our proposed federated random forest model. Key parameters include the \u03b1 value, the number of trees in the ensemble, and the maximum tree depth."}, {"title": "4.3.1. Impact of \u03b1", "content": "To assess the influence of the \u03b1 parameter on model performance, we conducted experiments on the Statlog dataset with \u03b1 ranging from 2 to 6. For each value of \u03b1, Table 6 presents the accuracy of the proposed model along with the maximum number of classes allocated to clients, denoted as M and calculated using Equation (1).\nFor \u03b1 = 2, the model encounters a highly non-IID scenario where M = 2, leading to lower accuracy. Increasing \u03b1 to 3, although maintaining the same value of M, resulted in a more balanced class distribution with more chunks of each class distributed to clients, leading to a noticeable improvement in accuracy. For both of these values of \u03b1, the limited number of class chunks leads to some clients receiving data from only one class, causing extremely skewed class distributions and adversely affecting the model's performance.\nFor \u03b1 = 4, M increases to 3, with no client receiving data from a single class. This more balanced distribution of classes significantly enhances model performance. As \u03b1 reaches 6, the scenario transitions to an IID setting (since the dataset has 6 classes), and model accuracy further improves. However, the gain in accuracy from \u03b1 = 4 to \u03b1 = 6 is relatively modest, indicating that, significant performance improvements can be achieved by avoiding extremely skewed distributions, but additional increases in \u03b1 beyond this level yield only slight improvements. The results of this ablation study on the Statlog dataset demonstrate a pattern of initial significant gains in accuracy with increases in \u03b1, followed by diminishing returns upon further increasing \u03b1, a trend that was consistent across other datasets."}, {"title": "4.3.2. Impact of the Maximum Depth", "content": "In this section, we examine the impact of setting a maximum tree depth on model performance under non-IID conditions. We focused these experiments on the Pendigits dataset. Table 7 summarizes the experiments performed on our proposed model, examining various metrics such as model accuracy, the total"}, {"title": "4.3.3. Impact of the Number of Trees", "content": "This section of the ablation study examines the effect of varying the number of trees in the ensemble on model performance. For this purpose, we conducted experiments on the Pendigits dataset, operating under non-IID conditions. Table 9 presents the accuracy of each model configured with different numbers of trees: 10, 30, 50, and 100 trees.\nAs indicated in Table 9, all models exhibit improved performance with an increase in the number of trees in the ensemble. Upon reaching 100 trees, most models seem to converge towards their peak performance. Notably, our proposed model shows a significant increase in accuracy from 10 to 30 trees, and achieves near-optimal performance with 50 trees. However, the increase in accuracy from 50 to 100 trees is marginal, indicating a point of diminishing returns. In contrast, the centralized model demonstrates relatively smaller increments in accuracy with the addition of more trees. The Markovic et al. (2022) and NCFF models, both of which involve trees grown independently by each client without collaboration, exhibit the most significant performance increases as the number of trees is increased, with significantly lower accuracies compared to"}, {"title": "4.3.4. Impact of the Minimum Sample Threshold", "content": "In this section, we investigate the impact on model performance when setting a minimum number of samples required to split an internal node as a stopping condition for tree growth. We conducted experiments on our proposed model under non-IID conditions for all seven datasets, with the minimum number of samples threshold ranging from 1 to 50. Figure 3 presents the results, where the horizontal axis indicates the minimum number of samples required to split a node. As the value on the horizontal axis increases, the degree of privacy protection becomes stronger, as the leaf nodes are constrained to represent a larger number of samples, reducing the granularity of the information that can be inferred about individual clients' data.\nAs shown in Figure 3, the model's accuracy decreases as the minimum sample threshold increases across all datasets. By imposing a minimum number of samples required to split a node, the trees are forced to terminate their growth earlier, resulting in less detailed and specialized nodes, and potentially leading to underfitting. However, for most datasets, including Pendigits, Dry Bean, Statlog, Nursery, and Hand Postures, the decrease in accuracy is relatively small when the minimum sample threshold is set to a moderate value (e.g., 10 or 15). This suggests that setting a moderate threshold can help preserve privacy without significantly sacrificing performance. For the Letter Recognition and Covertype datasets, the accuracy decline is more pronounced as the minimum number of samples increases. This indicates that the choice of the threshold value should be carefully considered based on the specific dataset and the desired balance between privacy and accuracy.\nIn conclusion, this ablation study demonstrates that setting a moderate minimum sample threshold can enhance data privacy while maintaining a relatively high level of model performance for most datasets. By requiring a larger number of samples to be represented in each leaf node, the amount of information that can be inferred about individual clients' data is reduced, providing a higher degree of privacy protection. The choice of the threshold value should be based on the specific requirements of the application and the characteristics of the dataset, considering the trade-off between privacy protection and predictive accuracy."}, {"title": "4.4. Computational Complexity and Communication Overhead", "content": "In this section, we analyze the computational complexity and communication overhead of our proposed method and compare it with the Non-Collaborative Federated Forest (NCFF) method. It is important to note that expressing the computational complexity of tree-based models, such as random forests, is not straightforward since the number of nodes induced in a decision tree can vary significantly depending on the characteristics of the data. To simplify the analysis, we make the following assumptions:\nAssumption 1: Each client holds an equal size of data, denoted as n.\nAssumption 2: When a client grows an initially empty tree (case h = 1 in Figure 1) using its local data of size n, the time complexity of building the tree is denoted as A, and the number of nodes in the resulting tree is denoted as B.\nAssumption 3: When a client continues to grow a tree that has been partially grown by previous clients (case h > 1 in Figure 1), using its local data of size n, the time complexity of growing this tree is also A, and the number of nodes added to the tree is also B.\nIn the NCFF method, an ensemble of m trees is divided equally among k clients, with each client independently growing its assigned subset of trees using its local data. The time complexity for growing each tree is O(A), resulting in a total time complexity of O(mA) for the entire ensemble. The communication overhead from clients to the server involves sending the grown trees, which is O(mB).\nIn our proposed method, the tree growing process for the ensemble of m trees is performed collaboratively across k clients in an iterative manner. The time complexity for growing a single tree across k clients is O(kA), resulting in a total construction time of O(mkA) for the tree growing step. The communication overhead in this step is O(m(B + 2B + ... + kB)) = \\(O(\\frac{mBk(k+1)}{2})\\) = O(mk\u00b2B), as each client receives the trees grown by the previous clients, grows them further by approximately B nodes, and sends them back to the server.\nIn the tree adjustment step, all the grown trees in the ensemble are sent to each client for leaf node adjustment. Each client adjusts the leaf nodes of the trees using its local data and sends the updated trees back to the server. The time complexity for this step is O(mkn \u00b7 tree depth), as each client needs to traverse its entire dataset through each tree. The communication overhead for this step involves the sharing of the fully grown trees from the server to each client for leaf node adjustment, which corresponds to\nO(m(kB + kB + \u2026 + kB)) = O(mk\u00b2B), as each tree has already been grown collaboratively by all k clients, reaching a size of approximately kB nodes.\nIn summary, our proposed method has a higher computational complexity of O(mkA) for the tree growing step and O(mkn \u00b7 tree depth) for the tree adjustment step, compared to the complexity of O(mA) in NCFF. The communication overhead of our proposed model is O(mk\u00b2B) for both the tree growing and tree adjustment steps, compared to O(mB) in NCFF. We also counted the average number of nodes in the decision trees constructed by both the proposed model and NCFF in the non-IID experimental setting described in Section 4.1, for all seven datasets. The results, shown in Table 10, indicate that the proposed model consistently induces a larger number of nodes compared to NCFF."}, {"title": "5. Conclusions", "content": "In this work, we introduced an innovative federated random forest approach aimed at improving performance in non-IID scenarios. We adopt a collaborative tree-growing method, where decision trees are developed collectively among clients, along with a privacy-preserving approach that restricts leaf node information. This strategy effectively addresses the challenges posed by skewed data distributions in federated learning scenarios.\nThe experimental results, using seven benchmark datasets, reveal that our model consistently outperforms both Markovic et al. (2022) and the Non-Collaborative Federated Forest (NCFF) model, which train decision trees independently across clients. This underlines the efficacy of our collaborative tree-growing strategy and its robustness to skewed data distributions across clients. Moreover, our proposed model demonstrates similar or superior performance compared to FedTree (Q. Li et al., 2023), a federated learning model for GBDTs. Additionally, in both non-IID and IID settings, our model achieves performance levels comparable to both centralized models and the Collaborative Federated Forest with Class Probabilities (CFF-CP) model, which maintains class frequencies at the leaf nodes and calculates probabilities by aggregating them. This proves"}]}