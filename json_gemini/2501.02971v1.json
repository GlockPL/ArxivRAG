{"title": "Proof-of-Data: A Consensus Protocol for Collaborative Intelligence", "authors": ["Huiwen Liu", "Feida Zhu", "Ling Cheng"], "abstract": "Existing research on federated learning has been focused on the setting where learning is coordinated by a centralized entity. Yet the greatest potential of future collaborative intelligence would be unleashed in a more open and democratized setting with no central entity in a dominant role, referred to as \"decentralized federated learning\". New challenges arise accordingly in achieving both correct model training and fair reward allocation with collective effort among all participating nodes, especially with the threat of the Byzantine node jeopardising both tasks.\nIn this paper, we propose a blockchain-based decentralized Byzantine fault-tolerant federated learning framework based on a novel Proof-of-Data (PoD) consensus protocol to resolve both the \"trust\" and \"incentive\" components. By decoupling model training and contribution accounting, PoD is able to enjoy not only the benefit of learning efficiency and system liveliness from asynchronous societal-scale PoW-style learning but also the finality of consensus and reward allocation from epoch-based BFT-style voting. To mitigate false reward claims by data forgery from Byzantine attacks, a privacy-aware data verification and contribution-based reward allocation mechanism is designed to complete the framework. Our evaluation results show that PoD demonstrates performance in model training close to that of the centralized counterpart while achieving trust in consensus and fairness for reward allocation with a fault tolerance ratio of 1/3.", "sections": [{"title": "1 INTRODUCTION", "content": "The data economy today is becoming increasingly collaborative in nature. Take business intelligence, for example. To unleash the full potential of big data, it is essential to integrate multi-source data depicting entities from a multi-faceted and multi-modal perspective, which, not surprisingly, is not achievable by any company alone. It is mutually beneficial for companies to leverage each other's data for collective model training. On the other hand, however, privacy and security concerns have long been major roadblocks in cross-entity data exchange.\nAmong all the approaches proposed to resolve these data silo issues, federated learning [17] has gained growing popularity due to the fact that participating training nodes can retain all their data on-premise, train models locally and exchange only model parameters to cooperatively obtain a common model better than what each can individually train, maximally protecting their data privacy and security. This is the learning environment of collaborative intelligence we will focus on in this paper.\nUnfortunately, existing federated learning models focus mostly on settings with a central entity to coordinate all other nodes in the training process, which we refer to as the \"centralized\" setting. While useful for some scenarios, we argue that the centralized setting will not be the most important and challenging collaborative intelligence mode in the future. Competing businesses will not participate if one is in a dominating position superior to the rest in the ecosystem. For all businesses to willingly contribute and collaborate sustainably, the system must be open to all and dominated by none, which we refer to as the \"decentralized\" setting.\nWhile there have been research efforts on decentralized federated learning [14, 16, 19, 21], the assumption is that all participating training nodes are cooperative and motivated for a common goal in good-will spirit, typical of a consortium with permissioned entry. The main task there is to maintain model consistency across various nodes in an amicable setting. Yet real-life application settings are never that rosy. A large-scale collaborative data intelligence ecosystem open to all must accommodate participants of all kinds, including those malicious nodes which are typically referred to as Byzantine nodes in distributed systems. Achieving correct consensus despite the existence of Byzantine nodes is called Byzantine fault tolerance.\nTwo main challenges arise in this decentralized setting with Byzantine fault tolerance: (I) How to collectively train a common correct model with comparable results as in centralized federated learning; and (II) How to design fair incentivization to properly reward participating training nodes for their contribution in terms of data. The most difficult part of both challenges is to handle the \"Byzantine\" nodes that are largely ignored in existing studies for centralized settings. It is worth noting that in our case, Byzantine nodes not only refer to malicious nodes in traditional consensus research - those attacking the system by compromising the consensus, which is the main task of challenge I \u2013 but also to nodes harmful to the system by scheming for unwarranted reward allocation from their data contribution, the main task of challenge II.\nTo overcome these challenges, we propose a novel consensus protocol called Proof-of-Data (i.e., PoD) to achieve a decentralized Byzantine fault-tolerant federated learning framework. The contributions of this paper can be summarized as follows:\n\u2022 We propose a novel consensus protocol called Proof-of-Data tailored for decentralized federated learning with Byzantine fault tolerance. PoD combines Proof-of-Work (i.e., PoW) style asynchronous consensus with epoch-based consensus locking by a PBFT-style component, integrating the best of both by endowing, on the one hand, the missing consensus finality to the practically robust yet theoretically flawed PoW consensus and lending, on the other hand, the scalability necessary for societal-scale application setting to the otherwise sound PBFT consensus.\n\u2022 We design a privacy-preserving data verification mechanism based on P4P[11], a zero-knowledge proof protocol, to prevent participating nodes from producing inconsistent data contributions in the training process without violating their data privacy.\n\u2022 We devise an incentive mechanism to assess and allocate rewards based on nodes' data contribution, mitigating the risk of Byzantine attacks in terms of data contribution without sacrificing model performance economically.\n\u2022 Finally, we comprehensively evaluate the performance of the framework as well as analyze the resilience, performance and governance of PoD. An analysis of the framework's anti-attacking capability is also provided. The framework is empirically validated through a wide range of experiments on both time-invariant and time-varying datasets. The results of these experiments show that our framework performs closely with Centralized Federated Learning.\nThe remainder of this paper is organized as follows. First, we formulate the decentralized Byzantine federated learning (i.e., DBFL) problem and present design ideas in Section 2.2 to better understand our PoD consensus protocol, which is detailed in Section 3. In section 4, the resilience, performance and governance of PoD are theoretically analyzed. Section 5 presents the experiments and evaluates the framework's performance. Section 6 discusses related work, and Section 7 concludes this paper."}, {"title": "2 PROBLEM FORMULATION AND DESIGN IDEAS", "content": "We follow standard notion to model the underlying environment of our problem with $n$ training devices among which several devices could be Byzantine [18] adversaries.\nDefinition 1. (Decentralized Byzantine Federated Learning): Given 1) a set $P$ of $n$ geo-distributed devices ${P_i}_{i \\in [n]}$ with private datasets ${D_i}_{i \\in [n]}$ connected by P2P network of which a set $A$ of $f$ devices are Byzantine nodes which conduct Byzantine attacks randomly, where $[n]$ is short for ${1, 2, ..., |P|}$ through the paper; 2) a model with the objective function $F(w)$, where $w$ is the model weights, the Decentralized Byzantine Federated Learning problem, denoted as DBFL, aims to make all devices ${P_i}_{i \\in [n]}$ collectively train a common model weights set $W = {w_i}_{i \\in [n]}$ with private datasets ${D_i}_{i \\in [n]}$ through the exchange of information over an asynchronous network. Formally, the objective function of DBFL is to minimize the following function:\n$\\min G(W) = \\sum P_iF(w_i; D_i).$\nHere, $w_i \\in R^d$ is the model weights of device $P_i$, $p_i \\ge 0$ specifies the relative impacts of each device to the whole network, and $\\sum_i p_i = 1$.\nTo explain our design ideas, we start with our goal: To achieve decentralized federated learning for societal-scale applications. This context entails three essential characteristics - (1) the large number of participating nodes, (2) the existence of Byzantine nodes and (3) the absence of a central coordinating entity. As a result, a consensus protocol is necessary to guarantee the consistency of model training and contribution accounting across different nodes with no \"trust\" assumption among them, lending the \"trust\" component to the solution. On the other hand, \"incentive\" component in terms of reward based on data contribution is equally indispensable to ensure the motivation of participation. The combination of both \"trust\" and \"incentive\" is the foundation of sustainable collaborative intelligence in a real-life setting.\nConstrained by the FLP impossibility result [13], which states that a correct consensus algorithm is impossible if three properties are to be achieved simultaneously: (I)Asynchrony, (II)Determinism and (III)Fault tolerance, the design of any consensus protocol is essentially balancing the trade-off among the three properties.\nIn our setting, first of all, \"fault tolerance\" is indispensable as an open-access societal-scale application with neither simple faults nor Byzantine faults is simply unimaginable. \"Determinism\" is also deemed important because the consistency and the assurance of both model training result and reward distribution are crucial for continual participation of data-contributing nodes and liveliness of the system. We are left with \"asynchrony\" as the only option to let go.\nSuppose we relax the \"asynchrony\" property, can we use existing consensus protocol designed for synchronous or partially synchronous setting such as PBFT [3, 4, 24]? The answer seems to be negative because a quadratic time complexity in the number of nodes is infeasible for large-scale applications as we aim for in our case. Meanwhile, the nodes responsible for carrying out the protocol in PBFT are fixed, but in our setting, the nodes can join or quit at any time. More fundamentally, an asynchronous mode is much more desirable in our federated learning context as nodes do not need to wait for all others to complete training to benefit from the already partially-trained result.\nIn order to still enjoy the efficiency from asynchrony while keeping both fault tolerance and determinism, we draw inspiration from the design of PoW (i.e., Proof-of-Work [26]) as used in Bitcoin. Asynchrony (i.e., a node does not need to wait for any other node to proceed to mine independently) has played a critical role in the success of Bitcoin as the first application of consensus protocol in a societal-scale setting (18,000 public nodes as of February 2024). Unfortunately, however, PoW does not achieve the classic definition of consensus, as the finality is never secured. Specifically, it does not achieve the classic round-by-round consensus in terms of resilience, and the probability of eventually achieving global consensus increases over time, approaching infinitely close to but never reaching one.\nTo remedy the situation, we propose the idea of decoupling of model training and contribution accounting based on the following observation:\n\u2022 Model training is the task performed by all the nodes most of the time with each data update. Contribution accounting, on the other hand, can be executed periodically at model training milestones when actual reward distribution is conducted.\n\u2022 The task to benefit the most from asynchronous processing is the model training part a globally consistent model"}, {"title": "3 PROOF-OF-DATA CONSENSUS PROTOCOL", "content": "The two-layer consensus protocol as introduced in Section 2.2 is termed Proof-of-Data (i.e., PoD). In a DBFL system, all nodes ${P_i}_{i\\in [n]}$ join the network at random and start training the model $F(w; D_i) \\rightarrow w_i^{t+1}$ with their private datasets ${D_i}_{i\\in [n]}$, and can only exchange information employing two-party messages with P2P network with no central server to integrate information or manage distributed nodes. During the process of model training, nodes in the system generate a different sequence of model weights states ${w_i}_{i\\in [n],j\\ge1}$, and some of these states may be spurious due to Byzantine nodes. Therefore, PoD takes trained model weights sequences ${w_i}_{i\\in [n],j\\ge1}$ from nodes ${P_i}_{i\\in [n]}$ as inputs and aims to output a common model weights set $W = w^{j+1}$. PoD guarantees the properties below except with negligible probability under the influence of any Byzantine attacks:\n\u2022 Safety: 1) Agreement: if any two honest training nodes output $w$ and $w'$ for ids respectively, then $w = w'$; 2) Validity: if a training node outputs a model weight $w$ for id, then it is an honest node; 3) Finality: if a model weight $w$ is locked by voting nodes, it can not be revised any more.\n\u2022 Liveness: 1) Termination: if every honest node $P_i \\in (P \\setminus A)$ is activated on identification id, with taking as input a dataset $D_i$ s.t. $F(w; D_i) \\rightarrow w_i^{t+1}$, then every honest node output a model weights $w$ for id; 2) Liveliness: if a training node outputs a model weight $w_i^j$ for id, then it is available to output $w_i^{j+1}$ continuously.\nNote that the algorithm need not reveal which nodes are faulty and that the outputs of faulty nodes may be arbitrary; it matters only that the non-faulty devices compute the same valid model weights vector for any given faulty node. Eventually, the non-faulty nodes come to a consistent view of the values of the model weight vector held by all the nodes, including the faulty ones. Fig. 1 shows the overview of the PoD consensus, and we will introduce three important components: block structure, sharing layer and voting layer in subsections 3.1, 3.2, and 3.3 respectively with two key proposed mechanisms: data verification in subsection 3.4 and measurement of data contribution in subsection 3.5."}, {"title": "3.1 Block Structure", "content": "The block structure in POD would contain most of the information in a typical blockchain structure (e.g., a block header and a block body). We would focus on the information unique to PoD.\nDefinition 3. (Update): We define the training result of a device $P_i$ with private dataset $D_i$ as an update $U_h$, where $h$ is the block height. An update mainly consists of the following fields:\n\u2022 $w$: the latest model weights;\n\u2022 $ID_Sender$: unique identity of the sender;\n\u2022 $data_Summary$: a summary that summarizes the characteristics of the data; In our work, we use the Gaussian Mixture Model (i.e., GMM [2]) to fit the private datasets of users and $data_Summary$ mainly includes mean $\\mu$, covariance $\\Sigma$ and coefficient $a$.\n\u2022 $data_Signature$: a data falsification verification proving the reliability of the training results.\nIn PoD, nodes will pass local updates to other nodes, who will then decide whether to merge or not. We define the merging result of local updates of sharing device $p_i$, $i \\in [n]$ as a block $B_i^h$, where $h$ is the block height. The block body mainly stores the specific data of all the updates that have been merged in the block, and the block header consists of some specific fields related to the storage management and system settlement.\nMost noticeably, an important notion epoch is introduced to finalize, exactly to address the finality issue in the proof of our consensus, we use epoch to lock consensus on the blockchain permanently. We denote an epoch as $EH$, where $H$ is the epoch height. Each epoch contains a certain number of blocks (e.g., 100) with a fixed block size, and the number of blocks in an epoch depends mainly on the number of participating nodes in the system. At the end of an epoch, the voting layer must finalize the consensus by locking the current epoch. Once an epoch is locked, the information stored on the blockchain cannot be changed anymore."}, {"title": "3.2 Sharing layer", "content": "Nodes in the sharing layer are responsible for handling three tasks:\n\u2022 Obtain the intermediate parameters with the latest private dataset, merge updates from other devices in the network to generate a new block, and then broadcast the new block to other devices. To do that, the node must first deposit for epoch-sharing authorization.\n\u2022 Listen for new blocks and epoch locking to trigger blockchain replacement and block merging events.\n\u2022 Merge the updates generated by themselves and the listened blocks from the network"}, {"title": "3.2.1 Block generation", "content": "Essentially, in centralized federated learning, the central server is mainly responsible for aggregating the parameters of all training nodes. Similarly, in the decentralized setting, we aim to lock the updates with the largest aggregative parameters for the settlement, and this idea is conceptually similar to the \"longest chain.\"\nIn order to give priority to messages with the largest parameters, from a training node perspective, we need to do three things. First of all, we must train the model with the private dataset to generate updates. Specifically, we first deposit for epoch training authorization, then train a new update with the latest private dataset, merge updates from other nodes in the network to generate a new block, and then broadcast the new block to other nodes. At the same time, we need to constantly listen for new blocks and new epoch locking to trigger events of blockchain replacement and block merging. We need to merge the updates we generated and the listened blocks from the network. Moreover, if we receive the locking message in the middle of the training recurrence, we have to stop and merge."}, {"title": "3.2.2 Block merging", "content": "It is particularly important to explain why proof of work can successfully progress in an asynchronous manner. It does so because it uses the idea of the 'longest chain' to achieve the possibility of a global consensus in the long run. Here, we want to borrow the idea of the 'longest chain', but in our setting, the idea of the longest chain is the peer training node update, which contains the maximum aggregative result of the whole system's training updates.\nFor each epoch, each device, including newly entered devices, starts training the model from the same initial state $w_0$, which is finalized in the last block of the previous epoch. After generating a new update $U_i^h$ for device $P_i(i \\in [n])$ with the private dataset $D_i$, $P_i$ packages new block and broadcasts the block $B$ to the whole network and also listens to Blocks $B'$ from other devices in the network. Once a block is received from another device, the device $P_i$ integrates the received update according to the block merging protocol. Each block in the network is the merging result of the device and has the merging list or update list. If the update list contains the received list, the block will not need to be merged. Otherwise, we will continue to merge the new block and broadcast it to the network. Meanwhile, if the block is the history block and we do not merge, we need to roll back the training process and retrain from the height of the history block."}, {"title": "3.3 Voting layer", "content": "The finality of the consensus can not be resolved within the sharing layer. To do this, we have to overlay the voting layer on top of it, which will be a small set of nodes. The voting layer is mainly responsible for Epoch locking, data verification and value allocation."}, {"title": "3.3.1 Epoch locking", "content": "In traditional Proof-of-X (e.g., PoW [25], PoS [23]), there is no deterministic finality for all transactions stored in the blockchain ledger. It's just that over time, the chance of the ledger being tampered with decreases, going to zero indefinitely, which doesn't mean tampering doesn't happen. However, for the PoD, tampering with the consensus result stored in the distributed ledger is easy. Therefore, we propose a BFT voting (i.e., PBFT [4]) layer to solve the problem.\nWe divide the block height into several epochs, and at the end of the epoch, we will lock the epoch through the BFT voting. Once the epoch is locked, all the updates stored in the previous blocks can no longer be tampered with. Specifically, we maintain an active training list of the whole system. Once the primary node of the voting layer receives the signal of the epoch locking from the sharing layer, it will call for a vote according to the active training list. If more than fault tolerance threshold per cent of nodes, the voting devices pass the validation and sign for the new block containing the locking signal, then the epoch will be locked, and all other training devices will synchronize the locking epoch and start new training based on the new deterministic global model and all the updates in the previous blocks cannot be tampered any more.\nIn addition, before broadcasting the signed epoch, the voting devices will make an epoch settlement to distribute rewards according to the data contribution (see section 3.5). Moreover, we also listen for the data verification to prevent data forgery with privacy protection based on the P4P method (see section 3.4)."}, {"title": "3.4 Data Verification", "content": "One important task of the voting level is data verification. If a training device violates a rule, the voting layer should detect the violation and know which device violated the rule. Accountability allows us to penalize malfeasant devices. For example, a training device has private data $D$={$d_1, d_2,..., d_n$}, and claims the Gaussian distributions $G$={$g_1, g_2,..., g_q$} for all features $F$={$f_1, f_2,\\cdots, f_q$} in its data. Our data verification objective is to check whether the training devices have data that match the distributions they claim. Meanwhile, we can access devices' private data.\nConcretely, we need to verify the matching for all features. Then, the system splits $f_j$'s value range([Min($f_j$), Max($f_j$)]) into some intervals, and the server calculates each interval's average value according to the corresponding Gaussian distribution $g_j$. If the user has data that matches the Gaussian distribution, then the same interval's average sampled by the user should be similar to the average calculated by the server. To do this, our data verification consists of two parts, namely Consistency Check and Distribution Matching Check"}, {"title": "3.4.1 Consistency Check", "content": "To validate the 'data-distribution matching' without accessing data, we need three independent entities: the training device, privacy peer, and server. The training device has to send data-related information to both privacy peer and server. To validate the existence of private data and verify its consistency with server and privacy peer, we launch the P4P [35] for each training device.\nThe system requires the user to fetch some data points in each interval and transmit their $u$ and $v$ to the system. The system verifies the consistency of the data transmitted by the user, as shown in Algo. 1. Under the ZK-Proof scheme, the system will broadcast several challenge vectors for corresponding independent checks. We need the verifications for all challenge vectors to be successful. For each independent check, the training device calculates the corresponding $u_i$ and $v_i$ for datum $d_i$ where $i \\in$ {1, n} and send them to sever and peer respectively. Only if all Pair-Consist-Checks are successful is this independent check verified."}, {"title": "3.4.2 Distribution Matching Check", "content": "To verify whether $D$ matches $G$ without data breaching, take Feature $f_j$ as an example. First, the user gives the Gaussian distribution $g_j$ of Feature $f_j$ in her data. To prevent the user from tampering with the information provided later, the user needs to put all $v_i$ into a recording matrix (under the shape of $\\sqrt{n} * \\sqrt{n}$), calculate the hash value of each row and column and transmit them to the system. Therefore, the system only needs $2 * \\sqrt{n}$ hash values, significantly reducing the information required for verification.\nThe system first verifies the consistency of the data transmitted by the user, as mentioned before. After passing the Consistency Check, the server calculates its average value through Algo. 2. Both Algo. 1 and Algo. 2 are same to the processes in P4P [35] scheme. $F$ is a specific nonlinear function.\nAt the same time, for each $v$ in a specific interval, the system also requires the user to give all values of the same row and the same column in the recording matrix where $v$ is located. This is to verify whether $v$ is consistent with the original data. For $f_j$, if the verification of all intervals passes, then the verification of feature $f_j$ is successful. For a user, if the verification of all features passes, the user's data verification is successful."}, {"title": "3.5 Measurement of data contribution", "content": "To facilitate our proposed framework, we need first to design an algorithm for the voting nodes to calculate the contribution of each user's dataset ${D_i}_{i\\in [n]}$ to the model update under the premise of privacy protection at each epoch. For this purpose, we design an algorithm to indirectly calculate the data contribution of each training node based on data summary (e.g., Gaussian fitting). For each training node, Gaussian fitting is performed locally first, and then the fitting result $G_i$, $i \\en$ is sent to the voting layer. At each epoch settlement, the voting layer will calculate the contribution $r_i$, $i \\en$ of each training node according to the Gaussian fitting results of all nodes participating in the settlement at this epoch."}, {"title": "3.5.1 Sharing layer: Gaussian fitting", "content": "The Gaussian fitting $G_i$ for each user's private dataset is defined as a set of Gaussian functions ${G_{ik}}_{k\\in [1,q]}$, where $q$ is the number of features after the feature extraction from raw dataset $D_i$. Specifically, after feature extraction, features are independent, so we carry out Gaussian fitting $G_{ik}$, $k \\in [1, q]$ for each feature, respectively. Therefore, the result of Gaussian fitting of original data $G_i$ is the combination of a series of feature Gaussian fitting result ${G_{ik}}_{k=[1,q]}$."}, {"title": "3.5.2 Voting layer: contribution calculation", "content": "In the settlement stage, the voting layer needs to update the global model and allocate benefits according to the proportion of user data contribution. Since the voting layer only has the result of each user's Gaussian fitting result $G_i$ = {$G_{i1}, G_{i2}, ..., G_{iq}$} and the number of user volumes $count_i$, $i \\in [n]$, we adopt random sampling method to approximate the real data distribution according to the all Gaussian fitting results $G^k$ = {$G_{mk}$}, $m \\in [n]$ on specific feature k, k$\\in [1, q]$ to indirectly calculate the proportion of each user's data contribution {$r_{1k}, r_{2k}, ..., r_{nk}$} on the feature k. Therefore, the proportion of user data contribution $r$ = {$r_1, r_2, ..., r_n$}, $\\sum_{i\\in [1,n]} r_i = 1$ and $r_i = \\Sigma_{k\\in [1,q]} r_{ik}/q$.The data contribution calculation method is shown in algorithm 3."}, {"title": "3.6 An Example for flow of PoD Algorithm", "content": "Supposing we have four training nodes, A, B, C, and D and one primary voting node, V, as shown in Fig. 3. In the process, they first train parameters locally and independently. Suppose A is the first one to finish the training with the private dataset and gets the updated parameter vector $w_a$; then, A will broadcast the $w_a$ to training nodes B, C, and D and apply it to the voting layer for the epoch settlement. Meanwhile, A listens to the epoch-locking flag from the voting layer and new updates from B, C and D. Because the merging list only contains A's update less than the voting threshold $\\tau$, the voting layer passes the request directly. After that, if B is the second one to finish and gets the parameters update $w_b$, B will merge the $w_a$ from A with his own update, then broadcast the merged result $w_a + w_b$ to A, C and D and request to voting layer for the epoch settlement too. Similarly, this request is passed until C finishes training and submits the request with merged $w_a + w_b + w_c$. The epoch settlement will be agreed on because the merging list is more extensive than the voting threshold $\\tau$. Then, the voting layer will broadcast the locking flag to all training nodes with the latest merged result $w_a + w_b + w_c$ and value settlement result. Once each training node receives the locking signal, it first updates the merging result with the latest one and then stores the result in the blockchain, and then continues to train the next w continuously.\nMoreover, because training node D is slow and will be left out in this epoch, the value settlement will only happen in A, B, and C. But if D catch up in the next epoch, D will also benefit from the value allocation. Moreover, suppose D is now dead or malicious in purposely holding the result; the whole system will not be affected because A, B and C will be able to proceed."}, {"title": "4 ANALYSIS", "content": "In this section, we analyze the PoD consensus on liveness, safety, performance and fault tolerance."}, {"title": "4.1 Liveness and safety", "content": "Firstly, in the training layer, all nodes train the model with a private dataset and then broadcast the blocks to the whole network, so the timing model is asynchronous for the training layer. Moreover, the PoD uses the PBFT to realize the verification work for the voting layer, so the timing model is partially synchrony. Last but not least, PoD dictates that training nodes must wait for the verification and epoch-locking flag from the voting layer. Thus, we consider the protocol partially synchronous because of timeouts for data verification and epoch locking. So, the PoD can ensure the system's liveness.\nBFT vote only happens every epoch, so it can not prevent the temporary forks of the asynchronous training layer. So the agreement is probabilistic, and the finality is temporary, but the probability increases with epoch length between two consecutive epochs due to the \"longest chain\". However, after epoch locking, the agreement is deterministic, and the finality is permanent. The validity is deterministic due to data verification and signature. Specifically, data verification can guarantee the correctness and consistency of raw data, and a signature from the sender's private key can block the possibility of block tampering during propagation. The termination of the PoD is deterministic because of the repeated submission of each node during the merging process and voting threshold. Repeated submission guarantees that if the node doesn't receive the epoch-locking flag from the voting network, it will keep submitting the locking requests to the voting network. Moreover, because of the partially synchronous assumption of communication between training and voting layers, the number of nodes is fixed at a certain time. Once the merging list reaches the threshold, the locking works. All of these key designs can guarantee the safety of the PoD consensus algorithm."}, {"title": "4.2 Fault-tolerant threshold analysis", "content": "In the PoD, nodes in both layers participate in the consensus-reaching process. The voting layer is a classic PBFT model that tolerates no more than $[\\frac{m}{3}]$ faulty nodes based on the conclusion $Z \\ge 3f + 1$ [5].\nIn the sharing layer, if it is a Proof-based consensus model, we need to analyze the threshold of consensus-reached sharing nodes required to ensure the security and liveness of the whole system. During the consensus-reaching process, as there is a voting threshold $\\tau$ for the voting layer to validate the epoch-locking application, the $\\tau \\le 1-f$."}, {"title": "4.3 Communication complexity", "content": "The PoD model is proposed in Fig. 1, where the sharing layer has $n$ sharing nodes, and the voting layer has $m$ voting nodes. In the voting layer, for peer-to-peer communication, the communication complexity is the square of the node number. Thus, the required complexity $C_0$ to reach consensus is $C_0 = m^2$. Moreover, we investigate the merging and locking process with minimum communication complexity for a proof-based system $C_t = n$. But based on the design of PoD, we can get that $m$ is much less than $n$, so the communication complexity of the PoD is $n$."}, {"title": "4.4 Proof of Correctness", "content": "To prove the correctness of PoD, we must first prove that we have successfully addressed the two challenges which we have analyzed in the section 2.2. For challenge one, we need to prove that our model training is correct. That means our decentralized version can achieve what a centralized version can do. Then, we are going to examine possible ways that a malicious node can tamper with the system.\nSimilarly, with the training nodes A, B, C and D as examples, as shown in Fig. 3, suppose the D is the malicious node. What are the possible ways D can ruin the system?\nFirstly, D can train fast to rush into the epoch locking and leave out all others in the system to achieve the most benefits. Our solution is that we have the Voting threshold $\\tau \\le 1 - f$ in the voting layer to make sure that less than fault-tolerant ratio malicious nodes will not be able to dominate the system. Secondly, D can run slow intentionally to delay epoch locking, which ruins the system's liveliness. Similarly, we have the voting threshold in the voting layer to ensure that malicious nodes with a lower fault-tolerant ratio cannot affect the settlement of the epoch. Next, D can construct multiple transactions to submit epoch settlement requests redundantly to block the primary node in the voting layer (i.e., DoS attack). D may continuously broadcast his demand to the voting layer and not perform any subsequent operations. Our solution sets a request priority, which is inversely proportional to the number of locking requests. Also, D can collude other training nodes to launch Eclipse attack, our voting threshold can protect the system from this attack. At last, depending on the epoch locking at epoch settlement, the finality can not be overturned.\nFor challenge two, we need to prove that our value settlement is correct. That means our value allocation is fair and cannot be overturned. Similarly, D can launch data falsification attacks, and our solution is to have data verification to ensure data consistency during model training. In order to encourage training nodes to attend the system, especially those with small datasets, our data contribution calculation can tolerate overlap to protect fairness from data domination or data shadowing. Moreover, our solution uses asynchronous encryption to protect the block data consistency from tampering. At last, depending on the epoch locking at epoch settlement, the value settlement can not be overturned."}, {"title": "5 EXPERIMENTS AND EVALUATION", "content": "In this section, we first declare the experimental setup and then evaluate the performance and fairness of PoD on dataset ImageNet [9] with varied data allocation for all training nodes."}, {"title": "5.1 Experimental setup", "content": "The publicly accessible dataset ImageNet [9], widely used in the image classification field, is used for our performance and fairness validation in this chapter. We extracted a subset from ImageNet, which comprises about 1,500,000 images and a total of 1,000 categories, with a training set of 1,400,000 examples and a test set of 100,000 examples, respectively."}, {"title": "5.1.2 Dataset allocation", "content": "In our experiments, we aim to verify system performance and system fairness in different data distribution scenarios. We mainly simulate the practical application scenarios from"}]}