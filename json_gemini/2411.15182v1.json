{"title": "Forecasting Application Counts in Talent Acquisition Platforms: Harnessing Multimodal Signals using LMs", "authors": ["Md Ahsanul Kabir", "Kareem Abdelfatah", "Shushan He", "Mohammed Korayem", "Mohammad Al Hasan"], "abstract": "As recruitment and talent acquisition have become more and more competitive, recruitment firms have become more sophisticated in using machine learning (ML) methodologies for optimizing their day to day activities. But, most of published ML based methodologies in this area have been limited to the tasks like candidate matching, job to skill matching, job classification and normalization. In this work, we discuss a novel task in the recruitment domain, namely, application count forecasting, motivation of which comes from designing of effective outreach activities to attract qualified applicants. We show that existing auto-regressive based time series forecasting methods perform poorly for this task. Henceforth, we propose a multimodal LM-based model which fuses job-posting metadata of various modalities through a simple encoder. Experiments from large real-life datasets from CareerBuilder LLC show the effectiveness of the proposed method over existing state-of-the-art methods.", "sections": [{"title": "INTRODUCTION", "content": "Amidst the competitive talent acquisition landscape, a variety of research efforts have focused on developing job recommendation systems [1], job-candidate matching [2], and creating shared latent spaces for job and skill representation [3]. However, relatively few studies address user engagement with job postings-a metric strongly linked to recruiting agencies' bottom line and valuable for crafting promotional strategies that attract qualified candidates. To bridge this gap, CareerBuilder LLC is advancing research on forecasting job application counts (JACs), a crucial measure of user engagement. This paper introduces the task of JACs forecasting to the knowledge discovery community, detailing our models and experimental results.\nForecasting JACs is central to recruitment analytics, offering insights to tailor recruitment strategies by predicting the number of applications a job may receive. This enables systems to respond proactively; for example, low application counts may trigger targeted marketing efforts or prompt a review of job descriptions. Predictive analytics also supports diversity and inclusion by highlighting demographic trends and providing tailored suggestions to diversify applicant pools.\nAlthough forecasting is a well-established task in fields like retail and manufacturing, developing an accurate model for JACs forecasting is challenging. Job attributes comprise multimodal data-textual, categorical, and numerical-requiring robust representation learning to integrate these elements. Key challenges include effective entity extraction and schema standardization across typical job attributes like location [4], qualifications, skills, and salary. This complexity often leads to an extensive production pipeline. Auto-regressive time-series models, while an option, are limited as they use only historical counts, not external job features, as we demonstrate in our results.\nOur approach to JACs forecasting is streamlined yet effective: we treat each data field (textual, categorical, or numerical) as a sentence, concatenate them into a paragraph, and input it into a pre-trained BERT model, leveraging BERT's ability to process complex text structures. We show that this method outperforms traditional approaches requiring individual feature representations and complex feature fusion. By simplifying the pipeline and bypassing explicit entity extraction, this approach is efficient and highly manageable.\nIn summary, we claim three key contributions in this research work: First, we present a novel task, namely job applicant counts (JACs) forecasting. Second, we show that language models, like BERT can fuse multi-modal feature type seamlessly, thereby saves efforts of representation learning of features of different data types: numerical, categorical, or textual. Third, our extensive experiments demonstrate that our proposed job applicant counts method outperforms a large number of existing baselines by a significant margin."}, {"title": "PROBLEM FORMULATION", "content": "Say, a dataset $D = \\{(x_i, y_i)\\}_{i=1}^N$ is given, where $y_i$ is number of applicants for a job by time $t_i$ and $x_i$ comprises a blend of text (T), categorical (c), graph (g), location (l) and numerical (n) features associated with that job. For instance, textual features, like job title, company name, and job"}, {"title": "Methodology", "content": "At CareerBuilder (CB), the existing pipeline predominantly focused on the conventional practice of feature extraction, treating each column separately, as illustrated in Figure 2. CareerBuilder's current approach for application count forecasting task is built on top of this pipeline. Recently, we also developed a simple, yet effective method for application forecasting task on top of different language models, utilizing their superior textual comprehension ability. Our current approach is named as Multimodal Feature Fusion (FF) and the recently developed method is called Multimodal Language Model (LM).\n1) Multimodal-FF: Multimodal-FF utilizes our proprietary embedding techniques to convert multimodal data of different types: categorical, graph, location, and textual to high-dimensional real vector representation. These vectors are then fused to create a representation for a given job, and the vector is fed into a multilayer perceptron for JAC forecasting. Figure 2 illustrates the process for handling data from each modality. In this method, data from each modality is treated independently to represent it as a vector. For example, when dealing with a location feature (city, state), the mapper identifies an appropriate embedding method for the location, and based on that calculates the associated location vector.\nLet the embedding of the textual features is $E_t$, embedding of the categorical features is $E_c$, embedding of the graph nodes is $E_g$, and the embedding of the job location is $E_l$. N is the remaining numerical feature values. The concatenated features, $F_e$ is then expressed as:\n$F_e = E_t||E_c||E_g||E_l||N$\nThe right side of the Figure 1 depicts the feature fusion state. The vectors coming from each modality is simply concatenated to form the embedding of a particular job. In the following paragraphs, we illustrate how we embed features in each modality.\nFor text data, we employ a convolutional neural network (CNN)-based approach for text embedding [5], stemming from a multi-stream architecture. We leverage three text columns: company name, job title, and job description. Initially, we focus on the company name to construct a character-level embedding layer. Employing a stack of CNN layers, we extract features from the company name. Subsequently, a word-level embedding layer is applied to the job title, followed by feature extraction from each layer of a CNN stack. Similarly, features are extracted at the word level from job descriptions. The resulting concatenated vectors from each representation form the desired text embedding, $E_t$.\nWithin CareerBuilder, job and skill embedding involves the creation of a bipartite graph comprising jobs and skills, reflecting their direct associations. Additionally, two supplementary graphs, job transition, and skill transition, are established by observing user behavior, such as simultaneous application to two jobs or learning two skills. A method is then devised to embed jobs and skills, taking into account these interconnected graphs [6]. The aim is to represent each job and skill so that similar and related skills or jobs demonstrate higher similarity scores compared to unrelated ones. This embedding process involves placing the jobs and skills into a d-dimensional latent space. To determine the correlation between a specific job and skill, all the skills associated with a job instance are embedded, and the mean pooling of all skill embeddings for that job is computed, resulting in the representation of the skill, denoted as $E_g$, for that particular job.\nJob location is an important factor for candidates seeking a job, so the geographical location of a job holds significant importance for predicting application counts. However, traditional latitude and longitude coordinates pose a challenge due to their angular nature. To address this, we transform latitude and longitude into three-dimensional vectors using Cartesian coordinates, utilizing the equations [4]: $x = cos\\theta * cos\\phi$, $y = cos\\theta * sin\\phi$, and $z = sin\\phi$. Here, $\\theta$ and $\\phi$ denote the latitude and longitude of a location, respectively. This three-dimensional feature vector serves to embed the job's location, $E_l$."}, {"title": "", "content": "Additionally, we include category features such as job type, state, channel, and job level. These features undergo one-hot encoding and are subsequently concatenated to form the category embedding, denoted as $E_c$. Furthermore, numerical features like salary, represented as N are concatenated to form the final feature representation, $F_e$. Once the representation learning phase is completed, $F_e$ is stored as a database column for job instance embedding. To tackle the JAC forecasting task, we use a Multilayer Perceptron (MLP) by treating the forecasting as a regression problem.\nEmbedding a job instance with various modalities separately is a common strategy, although it involves a complex procedure. As shown on the right side of Figure 1, the highlighted block signifies additional intricate tasks for feature extraction. Multiple models must be trained to acquire embeddings for each modality, and a mapper is required to discern the exact modality. Addressing unknown locations by relying on zero embedding introduces noise into the feature. Additionally, each modality is unaware of the embedding space of others; for instance, the text embedding method is not cognizant of job-skill representation, impacting the overall performance of a model.\n2) Multimodal-LM: The prior feature extraction method is intricate, demanding entity extraction followed by the training of multiple models to address diverse modalities. To overcome this issue, we propose a different approach leveraging language models, which streamline the complex process by treating each modality as text-based sentence in a paragraph, often without employing an entity extraction. For example, numerical features like salary (or salary range) is generally available in job description; in this approach, we leave them as text in the job description paragraph expecting that a pretrained language model (LM) trained on vast corpora would automatically grasp the semantics of this information. This made job feature representation straightforward. As illustrated on the left side of Figure 1, the process involves the integration of converted text features into a paragraph, thereby eliminating the need for the explicit multimodal representation learning on different modality (the boxed part of right side of Figure 1). We call this process Multimodal-LM.\nThus, for Multimodal-LM methods, the feature extraction technique involves converting all textual, categorical, graph node, location, and numerical features into textual repre-"}, {"title": "", "content": "sentations. Categorical values simply becomes the text of category value instead of one-hot encoding, numerical values becomes the ascii string of the digits in a number, etc. These representations are then concatenated into a single unified text. Formally, textual features $T$, categorical features (c), graph based features g, location features l, and numerical features (n) are all concatenated to F as below:\n$F = t + TEXT(c) + TEXT(g) + TEXT(l) + TEXT(n)$\nwhere TEXT (c), and + denote casting a value into text and string-level concatenation operation respectively. For the skills, we intentionally concatenate all the significant skills into a text sentence, which is the output of TEXT (g). When we combine all the features together, F becomes a text paragraph representing a job instance.\nWith one paragraph and a JAC value, we can readily rely on Language Models (LMs) to tackle the JAC forecasting task. We use both 'bert-base-uncased' pretrained model of BERT [7], and the 'roberta-base' model of ROBERTa [8] to fine-tune for the JAC prediction regression task. We refer them as Multimodal-BERT and Multimodal-RoBERTa, respectively."}, {"title": "EXPERIMENTS AND RESULTS", "content": "We first discuss our dataset and the competing methods. Then we present the comparison between our proposed methods and the competing methods."}, {"title": "Dataset Description", "content": "CareerBuilder in-house dataset is used for this experiment. As we analyze the data, it is evident that job applicant count (JAC) follows a long-tail distribution. Majority of the jobs has 1-2 applicants, and very few jobs have large applicant counts; all JAC values are less than 76. This count increases based on the number of days elapsed since the job has been posted, as can be seen in Table I. Shelf-life of many jobs are about 1 week, so in this table t = 7 days has the highest number of data instances. If we combine dataset of all days, there are approximately 1M instances in the train dataset, 174K in the validation dataset, and 243K in the test dataset, maintaining an approximate 8:2:2 ratio."}, {"title": "Description of the Competing Methods", "content": "1) bi-LSTM and GRU-TSF: We employ bi-directional LSTM (bi-LSTM) and GRU Text Sequence Forecasting (TSF) approaches for JAC prediction. Both methods use Spherical Embedding for Text [9] with d = 100 in the embedding layer and L1 loss for regression."}, {"title": "", "content": "applicant counts for different days. To address this, we devised a consolidated dataset by integrating t as an external feature, employing established techniques for feature extraction. While various methods exist for concatenating day features, our approach varies: for Attention Fusion BERT, DeepTLF, and Multimodal-FF, the day acts as an external feature encoded with one-hot encoding, while for other methods, including t, we adopt the Heterogeneous Feature Concatenation approach, incorporating all features in a single paragraph. Opting for a single model to streamline operations, we introduce GRU TSF for experimentation. Despite using a unified trained model, performance evaluation is conducted based on individual days. In the joint training results presented in Table IV, Multimodal-BERT outperforms other methods for all days except when t = 1, for which Multimodal-FF shows a slightly higher performance. However, as t increases, Multimodal-BERT consistently outshines other baselines in terms of both MAE and MALE. The overall MALE of Multimodal-BERT is 23% better than the second-best performing method, GRU TSF. The superior performance of Multimodal-BERT is surprising, as this model does not perform any explicit representation learning for data of different modality, rather treat all data as textual feature in a paragraph.\nWe also assess the performance of the jointly trained methods by grouping the instances based on the application count value. Within the test dataset, when JAC equals 1, Multimodal-BERT demonstrates the most superior performance. Surprisingly, for JAC equal to 3, the LSTM with Spherical Embedding model outperforms all other methods. Nevertheless, Multimodal-BERT consistently exhibits better performance than other methods across various JACs. One notable observation is that, as the data follows a long-tail distribution, predicting for larger JACs becomes more challenging. Despite this, the increase in JAC does not lead to a proportional increase in the Mean Absolute Label Error (MALE) of Multimodal-BERT compared to competing methods.\nGiven that the dataset we are analyzing exhibits characteristics of time-series data concerning the variable t, we conducted experiments utilizing various time-series methodologies as previously outlined. The left side of Figure 3 presents selected instances of the time series data from the test dataset, revealing a notable upward trend in JACs with the progression of t. The right side of Figure 3 shows the predictions of the example job instances using Multimodal-BERT.\nConventional time series forecasting (TSF) models only consider past time stamps of the test dataset to predict for t = 30. The overall performance of the time-series based methods is shown in Table VI, with results exclusively provided for t = 30 due to the necessity of observing a sufficient number of instances for effective prediction by time-series methods. Notably, CES, AutoETS, and the AutoRegressive methods collectively exhibit the best performance, all sharing the same MALE value of 3.972. It's essential to highlight that the overall performance of time-series methods are much poorer than the methods outlined in Table IV. This discrepancy could be attributed to the feature-agnostic nature of the time-series methods, which rely solely on the explored JACs of observed days. Additionally, the distinct day selection throughout the research paper results in a noticeable gap between t = 14 and t = 30. Depending solely on JACs for TSF does not seem promising based on our experimental findings."}, {"title": "CONCLUSION", "content": "In conclusion, in this paper, we introduce a novel task in the recruitment domain: job application count (JAC) forecasting. The significance of this work lies in its ability to address the challenges posed by the multifaceted nature of job postings, which contain textual, categorical, graph, location and numerical data. The proposed approach simplifies the feature extraction process and achieves superior performance compared to existing research works."}]}