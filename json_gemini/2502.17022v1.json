{"title": "Class-Dependent Perturbation Effects in Evaluating Time Series Attributions", "authors": ["Gregor Baer", "Isel Grau", "Chao Zhang", "Pieter Van Gorp"], "abstract": "As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contributed the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics. Through empirical analysis across multiple datasets, model architectures, and perturbation strategies, we identify important class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences. Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common.", "sections": [{"title": "1 Introduction", "content": "Explainable Artificial Intelligence (XAI) has emerged as a critical paradigm for understanding complex machine learning models, particularly in domains where trust and explainability are essential, such as finance or healthcare. Within XAI, feature attribution methods quantify how input features contribute to model predictions, with their often model-agnostic nature enabling application across different architectures and data types. These methods are increasingly being applied to structured data domains, such as time series, where temporal dependencies pose unique challenges. In such contexts, ensuring reliable evaluation of attribution quality becomes crucial [22].\nThe evaluation of feature attribution methods faces a fundamental method-ological challenge: the absence of a ground truth for explanations. Although human-centered evaluation offers a direct assessment of the utility of explana-tions [11], it suffers from scalability limitations and potential domain-specific biases. Consequently, functional evaluation approaches have emerged as primary validation frameworks, with the aim of computationally verifying whether attri-bution methods satisfy certain desirable properties [2,9]. Perturbation analysis represents one such framework that evaluates attribution correctness by measur-ing how modifying features impacts model predictions. This approach rests on a key assumption: perturbing important features should yield proportional changes in model output.\nAlthough perturbation analysis has gained traction for evaluating attribu-tion methods in structured data domains like time series, previous work has mainly focused on aggregate performance metrics. Studies note that perturbation effectiveness can vary substantially with data characteristics, leading to recom-mendations to evaluate multiple ways of perturbing features [14,17]. However, how this effectiveness varies with specific data characteristics remains largely unexplored. A closer examination of reported results reveals an intriguing pattern: substantial portions of datasets can remain unaffected by perturbation when using a single strategy uniformly across all instances [14,18]. This observation sug-gests underlying methodological challenges that have not yet been systematically investigated.\nOur analysis of these empirical patterns points to an important methodological limitation: the effectiveness of perturbation-based evaluation can vary substan-tially across different predicted classes, which we refer to as class-dependent perturbation effects. These effects manifest when perturbation strategies effec-tively validate feature attributions for some classes while showing limited or no sensitivity for others. We hypothesize that such behavior emerges from classifier biases, where models learn to associate certain perturbation values with specific classes, potentially compromising the reliability of current evaluation practices.\nOur research examines how class-dependent effects influence perturbation-based evaluation of attributions. Through extensive empirical analysis, we show that these effects appear more pronounced with perturbation strategies that show strong aggregate performance, and persist across different perturbation strategies, model architectures, and attribution methods. This asymmetry in perturbation effectiveness has important implications: data set imbalance may influence evaluation results, and evaluation metrics might reflect specific model behaviors rather than attribution quality. Although our evidence stems from time series classification, similar considerations may extend to other structured data domains such as computer vision."}, {"title": "2 Related Work", "content": "The evaluation of explanations remains a critical challenge in XAI research. To address this, functional evaluation techniques have emerged as key com-putational methods for assessing the quality of explanations without human intervention [2,9].\nWe focus on perturbation analysis as a computational method to measure the correctness and sparsity of explanations. This approach was first introduced by Samek et al. [12] to evaluate feature attribution methods in the image domain. It involves sequentially perturbing pixels in order of the most relevant features first by replacing them with noninformative values and observing the impact on model predictions. This process generates a perturbation curve that tracks these prediction changes, allowing the calculation of metrics such as the area under or over the curve to jointly measure the correctness and sparsity of explanations. The fundamental assumption underlying this approach is that perturbing important features should degrade model predictions proportionally to their attributed importance, while perturbing irrelevant features should have minimal effects on the model output.\nWithin time series classification, there are various explanation methods, categorized into approaches based on time points, subsequences, instances, and others [22]. Our work focuses on feature attribution at the level of time points, examining how each point within a time series contributes to model predictions. As illustrated in Figure 1, these explanations identify the most influential parts of a time series for a prediction, visualized as a heatmap where darker regions indicate minimal contribution and lighter regions indicate stronger contribution to the prediction. The figure also demonstrates how different attribution methods can yield varying explanations for the same instance, highlighting the need for robust evaluation methods that answer the question of which explanation is correct."}, {"title": "3 Class-adjusted Perturbation Analysis", "content": "Feature attribution methods for time series classification identify the time points that influence a model's predictions. Since there is usually no ground truth for evaluating attributions, perturbation analysis is commonly used to assess attribution quality by modifying input features and observing the impact on model predictions. The assumption is that destroying information at important time points should cause the predictions to change, while perturbing irrelevant time points should have minimal impact.\nLet $x = [x_1,...,x_N"}]}