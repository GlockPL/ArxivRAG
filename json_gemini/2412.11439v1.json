{"title": "Bayesian Flow Is All You Need to Sample\nOut-of-Distribution Chemical Spaces", "authors": ["Nianze TAO"], "abstract": "Generating novel molecules with higher properties than the training space, namely the out-of-\ndistribution generation, is important for de novo drug design. However, it is not easy for distribution\nlearning-based models, for example diffu-\nsion models, to solve this challenge as these\nmethods are designed to fit the distribu-\ntion of training data as close as possible.\nIn this paper, we show that Bayesian flow\nnetwork is capable of effortlessly generating\nhigh quality out-of-distribution samples that\nmeet several scenarios. We introduce a semi-\nautoregressive training/sampling method that\nhelps to enhance the model performance\nand surpass the state-of-the-art models.", "sections": [{"title": "Introduction", "content": "The chemical space is large, and even the sub-space is too large to be explored completely. For\ninstance, the number of small drug-like molecules is believed to be over 1060,1\u20133 amongst which,\nhowever, only a much smaller sub-region, e.g., macrocycles, 4-6 were tested in the laboratories\nand applied into real world challenges; when we start considering larger systems like proteins,\nthe space grows exponentially. 7\nRecent emerging deep generative models that virtually search the chemical space are at-\ntractive alternatives to trial-and-error processes conducted by human scientists. 8-12 Despite the\nsuccess of generative models, especially the distributional learning models, in molecule genera-\ntion, some researchers have pointed out the limitations of these methods: because the models\nand benchmarks were designed to optimise and test the in-distribution performance, i.e., how\nclose the generated molecules are to the training data, (1) the models were pool at generating\nhighly novel samples with desired properties; 9,13 (2) multi-objective optimisation was difficult;9\n(3) the sampling space could change to the false-positive region volatilely when an overconfident\nguidance appeared. 14 To overcome these problems, it is important to improve the performance\nof out-of-distribution (OOD) generation, i.e., generating compounds with higher properties than\nthat of molecules in the training dataset.\nPioneering works have introduced a few methods to enhance the diffusion models' OOD per-\nformance including introducing a dedicatedly designed control method to steer towards the OOD"}, {"title": "Methods", "content": ""}, {"title": "Bayesian Flow Networks", "content": "Similar to denoising diffusion models 15-17 (DMs), Bayesian flow networks 18 (BFNs) splits the\ngenerative process to a sequential steps. Some research has pointed out that the generative pro-\ncess of BFN is similar to thus can be approximated as a reversed stochastic differential equation\n(SDE) though, 19,20 BFN method does not require to define a diffusion process nor learns the noise\ndistribution: instead, BFN directly optimises the parameters of a distribution towards a more\ninformative direction, which makes it applicable to continuous, discretised, and discrete data as\nthe parameters of any real-world distribution is continuous. 18 The previous studies have already\napplied the idea of BFN to 3-dimensional molecular conformation generation 21,22 (discretised\nand continuous case), text-based molecule generation 23 (discrete case), and protein sequence\ngeneration 20 (discrete case), which proved its capability of understanding the chemical space. In\nthis research, we employ ChemBFN, 23 a BFN model handling the 1-dimensional molecular rep-\nresentation originally designed to generate SMILES 24 and SELFIES 25 strings, to study BFN's\npotentials of out-of-distribution sampling, i.e., generating samples with high properties out of a\nlow-property training space. We show that a small change in the training or sampling process of\nChemBFN can significantly enhance the out-of-distribution generative performance."}, {"title": "Semi-autoregressive Training and Sampling", "content": "In the original ChemBFN 23 model and any BERT 26-like model, tokens are updated bidirec-\ntionally as illustrated in Figure 1 (left), where for an arbitrary token in a finite fixed-length\nsequence sj \u2208 (80, 81, 82, ..., sn) a function \u0192BI maps s; from ith layer to (i + 1)th layer based on\nitself and tokens from both left side (previous tokens) and right side (subsequent tokens), i.e.,\nsi+1\nSj\n= FBI(8; 80:-1, 8+1:N). In the case of autoregressive models, however, since the sequence\nis extended step by step (as shown in Figure 1 (middle)), the next token sj+1 only comes from the\nprevious tokens via a function far, i.e., s+1 = fAR(So:j). For models employing self-attention\nmechanism, e.g., the decoder of autoencoder transformer 27 model and GPT28 model, a causal\nmask that maps entities of the attention matrix above the main diagonal to zero is applied to\nimplement this autoregressive behaviour. We found that in a trained ChemBFN model, the\nentities that were far away from the main diagonal in the attention matrices were extremely\nclose to zero, which enlightened the possibility of applying causal masks to ChemBFN models\nwithout breaking models' generative capability. By employing the causal mask, we introduce\na semi-autoregressive (SAR) behaviour to ChemBFN models, in which all tokens are updated\ntogether as a block but subsequent tokens are not used to update the current token (Figure 1\n(right)), i.e., s+1 = SAR(8;; 80:j-1).\nTo be simple, we denote applying causal masks to models as 'SAR' while 'normal' stands\nfor disenabling causal masks. Depending on whether the causal masks are used in training or\nsampling processes, four strategies (strategy 1 - 4) are proposed in Table 1. We show how these\nsettings affect the generative behaviours of our model in later texts."}, {"title": "Datasets and Benchmarks", "content": "MOSES 29 is a widely-used benchmark in the studies of small molecule generation. Apart from\ntesting the validity, uniqueness, diversity, and novelty of generated molecules, this benchmark\nfocuses more on the distribution learning performances of tested models, including Tanimoto\nsimilarity (SNN), fragment similarity (Frag), scaffold similarity (Scaf), and Fr\u00e9chet ChemNet\nDistance 30 (FCD). Although the purpose of MOSES is to estimate how close the learnt dis-\ntribution is to the training space, in the study it was utilised to showcase how far away the\ngenerated space of our method could be from the training space while retaining the chemical\nmeaningfulness.\nLee et al,\u00ba on the other hand, proposed ZINC250k dataset, which is consisted of 249,455\nmolecules collected from ZINC 31 database and their paired quantitative estimate of drug-likeness 32\n(QED), synthetic accessibility 33 (SA), and docking scores (DS) unit in kcal/mol to five different\nproteins (PARP1, FA7, 5HT1B, BRAF, and JAK2) calculated via QuickVina 2,34 and the cor-\nresponding metrics to test the OOD multi-objective guided molecular generating performance.\nIn this benchmark, a group of filters that should be applied to generated molecules are defined\nas\nQED > 0.5\n5\nSA < DS < DS(molecules in training data)\nSNN < 0.4,\n(1)\nwhere DS strands for median value of docking score and SNN is the Margon fingerprint Tanimoto\nsimilarity to the nearest neighbour in the training set. The two metrics, novel hit ratio and novel"}, {"title": "Experiments and Results", "content": "We first demonstrated how different training and sampling strategies defined in Section 2.2\naffected generated spaces of unconditional cases, then quantified the performance of our model\nin OOD multi-object optimisations."}, {"title": "Unconditional Generation of Small Molecules", "content": "The MOSES testing metrics of ChemBFN using different strategies were visualised in Figure 2\nand the 2-dimensional UMAP 36 plots of unconditional sample spaces of models trained on\nZINC250k dataset against the training space were shown in Figure 3. It is clear that diver-\nsity and structure related metrics did not significantly response to the change of training and/or\nsampling strategies, which indicated that SAR process did not worsen the model's capability\nof learning molecular structures. The key observation was that the OOD-ness indicated by the\nmagnitude of FCD was strongly affected by different strategies. Figure 3 and Figure 5 (a) further\nshowed that the sample spaces were far away from the training space while changing training\nand/or sampling strategies led the model to explore different OOD spaces."}, {"title": "Conditional Generation of Small Molecules", "content": "When a guidance vector y = (QED, SA, DS) pointing to a higher-property space than training\nspace, i.e., high drug-likeness, low synthetic difficulty and more negative docking affinity, was\napplied to the sampling process via the classifier-free guidance method, 37 we observed that\nthe sample spaces had a tendency to be close to each other regardless the change of strategy\n(Figure 4). The OOD-ness was significantly larger than unconditional cases (Figure 5)."}, {"title": "Conditional Generation of Protein Sequences", "content": "An amino acid tokenizer was added to the original ChemBFN model to enable protein sequence\ngenerating. We trained one model to optimise the percentage of beta sheets and one model\nto optimise SASA. Each model, after training, generated 32 \u00d7 2 protein sequences (half was\ngenerated via strategy 3 and half via strategy 4) guided by an objective value pointing to the\nhigh property regions. As shown in Figure 8, the generated samples all had higher objective\nvalues than the training space. When estimated the naturalness (ProtGPT243 log likelihood as\nsuggested by N. Gruver et al35) of the generated proteins, we found that models utilising either\nstrategy 3 or strategy 4 gave reasonably acceptable results compared with natural proteins (see\nFigure 8). As SASA and the percentage of beta sheets of proteins are highly correlated their\nstructures, we conclude that our model is capable of determining the relationship between an\nobjective (scalar) value to its corresponding chemical structures, unsupervised, and extrapolating\nto unseen spaces."}, {"title": "What If You Care More About In-Distribution Sampling", "content": "In this section, we demonstrate how pre-training affects the sampling distributions of MOSES\nas an example. Finetuned models based on models, provided by N. Tao et al,23 pretrained on\n40M and 190M molecules selected from ZINC15 database 44 were denoted as 'finetuned 1' and\n'finetuned 2', respectively. As shown in Figure 9, the increasing of pre-training data surprisingly"}, {"title": "Computational Details", "content": "All the models were trained on single Nvidia A100 GPU with a batch-size of 120 for MOSES\ndataset and 128 for ZINC250k dataset for 100 epochs. The learning rate was 5 \u00d7 10-5 which\nwas linearly increased from 10-8 during the first 1,000 steps. AdamW46 method with default\nparameters was employed to optimise the model weights. A unconditional rate of 0.2 was chosen\nwhen training conditional models. During sampling process, the batch-size was 3,000 for small\nmolecules and 32 for protein sequences; the guidance strength was 0.5 for small molecules and\n1.0 for protein sequences when a guidance vector was applied. The number of sampling steps\nwas 100 for MOSES and 1,000 for ZINC250k and protein sequences. The models trained for\ngenerating small molecules had 12 layers, 8 attention heads per layer and 512 hidden feature\nsizes (54.5M of total parameters); the models for protein sequences had 12 layers, 16 attention\nheads per layer and 1,024 hidden feature sizes (216M of total parameters).\nRDKit 47 was used to generate 3-dimensional conformations and Margon fingerprint (radius\n= 2, dimension = 1024) from SMILES strings and calculate the Tanimoto similarity, QED, and\nSA quantities. The docking scores were computed via QuickVina 2.34 The properties of proteins\nwere calculate via Biopython 48 package version 1.84; to obtain the solvent accessible surface\narea, the 3-dimensional structures were predicted by ESMFold 49 model first, which was followed\nby Shrake-and-Rupley algorithm 50 calculations.\nIn order to generate UMAP plots, the vector molecular representations were extracted from\nthe last activation layer of ChemNet, 30 which were later projected to 2-dimensional vectors by\nUMAP package. 36 5,000 random molecules were selected from the dataset as the representatives."}, {"title": "Conclusion", "content": "In this research, we showed that BFN, especially ChemBFN model, is naturally a controllable\nout-of-distribution sampler, which is versatile to generate both small molecules and large chemical\nsystems such as proteins. We found that for unconditional generation, the normal training-SAR\nsampling strategy promoted the OOD-ness of the model most; when a guidance was switched\non, the OOD behaviour was pushed to a higher level, in which case the SAR trained models out-"}, {"title": "Data and Software Availability", "content": "The code of ChemBFN and instructions necessary to reproduce the results of this study are\navailable for download at https://github.com/Augus1999/bayesian-flow-network-for-chemistry."}, {"title": "Acknowledgements", "content": "The computational source of GPU was provided by Research Center for Computational Science,\nOkazaki, Japan (Project: 24-IMS-C043)."}, {"title": "Conflict of Interest", "content": "The author claim no conflicts of interest."}, {"title": "Funding Sources", "content": "The authors claim that there is no funding related to this research."}]}