{"title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity", "authors": ["Tong Xie", "Hanzhi Zhang", "Shaozhou Wang", "Yuwei Wan", "Imran Razzak", "Chunyu Kit", "Wenjie Zhang", "Bram Hoex"], "abstract": "Natural Language Processing (NLP) is widely used to supply summarization ability from long context to structured information. However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information. To address this, we introduce ByteScience, a non-profit cloud-based auto fine-tuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora. The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science. The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction. The platform achieves remarkable accuracy with only a small amount of well-annotated articles. This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics.", "sections": [{"title": "I. INTRODUCTION", "content": "AI has the potential to revolutionize scientific discovery (AI4Science [1]), but challenges remain. Scientific knowledge is scattered across documents, making it hard to fully leverage past research. LLMs offer a promising solution but require structured texts, including converting PDFs and generating fine-tuning examples for NLP tasks. While machine learning models are used in fields like drug discovery [2], protein design [3], and crystal structure generation [4], limited structured data hinders their effectiveness. Databases like Materials Project [5] and NOMAD [6] cover only a fraction of data, leaving much unstructured information untapped. This gap presents an opportunity for AI to accelerate discovery. Although converting documents to markup is well-studied, extracting complex relationships remains challenging but essential for building knowledge graphs and fine-tuning datasets.\nContextual Dependency: Relationships in scientific texts often depend heavily on context that may span multiple sentences or sections. For instance, a material's properties might be discussed concerning its synthesis method, as described in paragraphs several pages before.\n\u2022 Implicit Connections: Many relationships in scientific writing are implied rather than explicitly stated, requiring deep domain knowledge to infer correctly.\n\u2022 Hierarchical Structures: Scientific documents frequently contain nested relationships, such as experiment subsets or multi-step processes, which are challenging to represent in flat data structures.\n\u2022 Cross-Reference Complexity: Relationships often span different document parts, such as tables, figures, and citations, requiring holistic understanding.\n\u2022 Domain-Specific Semantics: Each scientific field has unique terminology and conventions, complicating universal extraction methods. For example, pseudocode and flowcharts may be unfamiliar in other domains.\nTraditional methods like MatKG [7], which define relationships by entity co-occurrence, often miss the nuances of scientific knowledge. While useful, they risk oversimplifying complex relationships. Advanced techniques are needed to better capture this complexity for improved knowledge extraction in AI-driven scientific discovery. Therefore, we introduce ByteScience, a cloud-based platform featuring an auto-fine-tuned LLM to extract structured scientific data and synthesize new scientific knowledge from extensive scientific corpora. We conclude as follows:\n1) Tailored with DARWIN [8], an open-source state-of-the-art nature-science LLM, to provide research focus utilization;\n2) Zero-code user-friendly semi-automated annotation and processing for uploaded science documents;\n3) A personalized and domain-specific auto fine-tuning LLM that requires only a single fully annotated piece of literature;\n4) Time efficiency high-quality science data extraction from millions of papers for less than a second per article."}, {"title": "II. PLATFORM DESIGN", "content": "ByteScience is a robust, scalable cloud-based solution leveraging AWS Sagemaker. This architecture ensures high availability, scalability, and performance for processing large scientific documents. Figure 1 illustrates the extraction pipeline for custom model development and data extraction. The pipeline consists of two primary phases:\n\u2022 Initial Setup (First-time use for a specific field):\nDataset Construction (Green Pipeline): This phase builds a domain-specific corpus of structured scientific data.\nLLM Fine-tuning (Blue Pipeline): The system fine-tunes a large language model on the constructed dataset to optimize performance for the target scientific domain.\n\u2022 Operational Phase: Once the initial setup is complete, users can directly utilize the fine-tuned LLM stored in AWS to efficiently generate structured datasets from new scientific documents in the same field.\nThis two-phase approach allows ByteScience to quickly adapt to various scientific domains while maintaining high extraction accuracy. The cloud-based architecture enables seamless scaling and ensures users always have access to the latest fine-tuned models, streamlining the conversion of unstructured scientific literature into structured data. Key steps include:\n1) Create Database: Users upload scientific documents in JSON, PDF, HTML, or XML formats. Non-JSON text is extracted and saved as JSON, with HTML/XML markup stripped, and PDF conversion done using PDFMiner [9].\n2) Define Structure: Users define annotation structures, including entity labels and relationships, using pre-built or custom templates.\n3) Random Selection: A small text subset is randomly selected for initial annotation on first use.\n4) Auto Labelling: The LLM applies automatic pre-labeling to the selected texts.\n5) Correction: Users review and correct the auto-labeled annotations, ensuring accuracy and consistency.\n6) Training: Corrected annotations are used to train or fine-tune an LLM, with training done via Amazon SageMaker.\n7) Fine-Tuned LLM: The training process results in a fine-tuned LLM customized for the specific annotation task.\n8) Structured Data Generation: The fine-tuned LLM processes new documents into structured data stored in MongoDB as JSON, allowing flexible use and efficient querying.\nAfter uploading the training dataset, it is transformed into the LLM's instruction format for fine-tuning. Users should first test a small text subset and assess accuracy and recall. If accuracy is low, add more annotated data and retrain. For low recall, generate more corpus data and use sequential learning to train a new model."}, {"title": "III. ARCHITECTURE OF AWS CLOUD-BASED SERVICES", "content": "ByteScience utilizes the robust, scalable infrastructure of Amazon Web Services (AWS) to efficiently handle user requests and data processing. Figure 2 shows the detailed architecture of our platform.\n\nA. General Service(Green Pipeline) Infrastructure\n\nThe user interaction layer is built on a series of AWS services that ensure high availability, security, and performance:\n\u2022 DNS Management: AWS Route 53 routes incoming user requests to the appropriate services within the architecture.\n\u2022 Load Balancing: An Application Load Balancer (ALB) distributes traffic evenly across multiple backend servers, ensuring fault tolerance and optimal performance.\n\u2022 Compute Resources: Backend servers, organized in an Auto Scaling group within a Virtual Private Cloud (VPC), provide a secure, scalable environment to traffic demands."}, {"title": "B. LLM Service (Blue Pipeline) Architecture", "content": "The LLM fine-tuning capability is a core function, implemented through a sophisticated pipeline of AWS services:\n\u2022 Model Development: SageMaker Notebook is used for developing foundation models. We reproduced the DARWIN model by fine-tuning LLaMA 7B, leveraging SageMaker's computational tools.\n\u2022 a DARWIN LLM processes Training Dataset Preparation: User annotations hosted on a SageMaker Endpoint, preparing data for model training.\n\u2022 Data Storage: Training datasets are securely stored on Amazon S3, ensuring durability and accessibility.\n\u2022 Model Fine-tuning: A SageMaker Training Job handles fine-tuning the model at scale, utilizing AWS's distributed computing capabilities.\n\u2022 Model Deployment: The fine-tuned model is deployed to a SageMaker Endpoint, providing a managed environment for querying the LLM for data extraction tasks."}, {"title": "C. Workflow Integration", "content": "ByteScience workflow seamlessly integrates these components:\n1) Users interact with the system through Route 53 and the ALB for initial annotation tasks.\n2) The DARWIN LLM processes annotated data on a Sage-Maker Endpoint.\n3) The SageMaker Notebook is used for model development and improvement.\n4) Training datasets on S3 are used to fine-tune the model via SageMaker Training Jobs.\n5) The resulting model is deployed to a SageMaker Endpoint.\n6) Users can then perform data extraction tasks, processing requests by the fine-tuned LLM.\nThis architecture enables ByteScience to offer customized, high-performance language models tailored to specific scientific domains, facilitating accurate and efficient structured data extraction from unstructured scientific literature."}, {"title": "IV. STRUCTURED DATA EXTRACTION PERFORMANCE", "content": "LLMs significantly improve human-in-the-loop annotation. Using 300 training samples reduced annotation time by 57% compared to a single sample [10]. In the GPT-3/Doping-English model, 10-20 samples were enough to learn the correct structure, with precision, recall, and F1 scores reaching 0.8-0.9 with around 300 samples.\nIn our experiment, we compared non-LLM and LLM methods for structured data extraction on 90 samples covering batteries, catalysis, and photovoltaics, alongside ByteScience's results. As shown in Table I, we evaluated Named Entity Recognition (NER), Relation Extraction (RE), and Entity Resolution (ER). While models like MatBERT performed well, they often produced irrelevant entities, lowering precision. In contrast, LLMs handled unstructured information more reliably, and our system outperformed traditional methods across all tasks with fewer samples."}, {"title": "V. BYTESCIENCE IN ACTION: A USER CASE STUDY", "content": "To showcase ByteScience's application, we present Thomas, a materials scientist automating alloy synthesis by analyzing literature to establish \"Composition-Processing-Structure-Performance\" (CPSP) relationships. He designs alloy compositions, develops processing methods, and predicts microstructures using data on casting, solution treatment, and aging."}, {"title": "A. Initial Setup: Schema, Semi-Annotation, Model Fine-Tune", "content": "Thomas configures ByteScience to meet his research needs by designing a custom annotation schema for alloy synthesis, annotating key details like compositions, casting parameters, solution treatment, and aging variables. ByteScience then initiates semi-automatic annotation, where the DARWIN LLM auto-labels papers from his corpus based on this schema. Thomas reviews and corrects the annotations to refine the model's understanding. Afterward, ByteScience fine-tunes the LLM using AWS SageMaker, optimizing it for alloy synthesis data extraction. The fine-tuned model is deployed to a SageMaker Endpoint for efficient, large-scale processing of complex scientific papers."}, {"title": "B. Data Generation: Document Upload, Endpoint Utilization, and Dataset Creation", "content": "With the fine-tuned model, Thomas uploads his entire corpus of scientific papers to ByteScience, which processes various formats for comprehensive coverage. He initiates large-scale data extraction via the SageMaker Endpoint, where the model extracts detailed information on alloy compositions, casting processes, solution treatments, and aging procedures. This automation accelerates his research, completing in days what would have taken months manually. The extracted data is structured and stored in MongoDB, allowing Thomas to easily query, analyze, and identify trends in alloy synthesis, uncovering insights that manual review might have missed."}, {"title": "C. Further Dataset Updates and Refinement", "content": "As Thomas advances in his research, he updates his dataset with ByteScience, uploading new papers and processing them through the fine-tuned model to continually enrich his dataset. When discrepancies or improvements are needed, he initiates a re-training cycle, reviewing and correcting a subset of the newly processed papers to further fine-tune the model. This iterative process ensures the model stays accurate and adapts to evolving terminologies or methods in alloy synthesis. Through this dynamic interaction, Thomas maintains an up-to-date, accurate dataset, enhancing his research and keeping him at the forefront of alloy synthesis advancements."}, {"title": "VI. SIGNIFICANCE TO SCIENCE", "content": "Constructing databases from scholarly literature is crucial for modern research, but traditional methods are time-consuming and resource-intensive. ByteScience transforms this process by enabling users to create a customized data extraction tool in hours, achieving 80%-90% human accuracy. It can process a 10-page scientific document in one second, compared to the 20-30 minutes it takes a researcher. With an extraction cost of just $0.023 per paper for 10,000 articles, ByteScience makes large-scale data extraction affordable and accessible. Its versatility across scientific fields democratizes access to advanced data extraction, providing computational power equivalent to hundreds of annotators. This accelerates discovery, enhances research decision-making, and fosters innovation across disciplines."}, {"title": "VII. CONCLUSION", "content": "ByteScience is leveraging a powerful approach to handle unstructured text by fine-tuning DARWIN, a pre-trained natural science LLM, using a minimal set of annotated articles. Hosted on the AWS cloud, this platform automates the process of extracting structured data from scientific texts, presenting a zero-code solution that could significantly enhance efficiency in natural science research. The key advantage of ByteScience lies in its ability to train the DARWIN model with few annotations, making it exceptionally adaptive and efficient. This capability ensures that the extracted material data is high-quality and highly accurate. ByteScience exemplifies how cutting-edge technology can be harnessed to propel advancements in science, engineering, and research by integrating advanced NLP techniques with cloud computing. This initiative represents a substantial step forward in making vast scientific corpora more accessible and usable, highlighting the transformative potential of AI in scientific data processing. To optimize resource efficiency, we are developing a slicing version that fine-tunes a low-resource inference model using only partial data from extensive content."}]}