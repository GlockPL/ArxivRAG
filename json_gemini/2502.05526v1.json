{"title": "Towards Learning Scalable Agile Dynamic Motion Planning for Robosoccer Teams with Policy Optimization", "authors": ["Brandon Ho", "Batuhan Altundas", "Matthew Gombolay"], "abstract": "In fast-paced, ever-changing environments, dynamic Motion Planning for Multi-Agent Systems in the presence of obstacles is a universal and unsolved problem. Be it from path planning around obstacles to the movement of robotic arms, or in planning navigation of robot teams in settings such as Robosoccer, dynamic motion planning is needed to avoid collisions while reaching the targeted destination when multiple agents occupy the same area. In continuous domains where the world changes quickly, existing classical Motion Planning algorithms such as RRT* and A* become computationally expensive to rerun at every time step. Many variations of classical and well-formulated non-learning path-planning methods have been proposed to solve this universal problem but fall short due to their limitations of speed, smoothness, optimally, etc. Deep Learning models overcome their challenges due to their ability to adapt to varying environments based on past experience. However, current learning motion planning models use discretized environments, do not account for heterogeneous agents or replanning, and building up to improve the classical motion planners' efficiency, leading to issues with scalability. To prevent collisions between heterogenous team members and collision to obstacles while trying to reach the target location, we present a learning-based dynamic navigation model and show our model working on a simple environment in the concept of a simple Robosoccer Game.", "sections": [{"title": "I. INTRODUCTION", "content": "A fast dynamic Motion Planning for Multi-Agent Systems in the presence of obstacles is a universal and unsolved problem [1], [2] applicable to robotics, planning, and optimization. In many industrial applications, users are interested in trajectories with minimum time to achieve the highest productivity [3]. Especially in athletics, Agile Motion Planning becomes an important factor for fast reactions for both gameplay quality and safety [4], especially when collaborating with humans. Our work can be used to provide an agile motion planning policy for Robosoccer, an environment that aims to promote AI and robotic research, specifically in a soccer domain [5].\nIn continuous domains where the world changes quickly, existing classical Motion Planning algorithms such as RRT* [6] and A* [7] become computationally expensive to rerun at every time step. The classical algorithms cannot capture the real-world dynamics to produce a non-collision path at once for all future time steps. Many variations of classical and well-formulated non-learning path-planning methods have been proposed to solve this universal problem [6] [8] [9]. Those methods are generalized in the following categories: sampling-based, graph-based, geometric-based, and optimization-based. However, these methods have their disadvantages. Sampling-based methods lack in producing consistent trajectories, and the produced trajectories are not smooth [1], [2].\nDeep learning models' ability to adapt based on past experiences to varying environments is more appealing to use than classical models with fixed parameters [10]. Currently, the application of deep learning in motion planning is un-matured and will continue to grow, especially in Multi-Agent Reinforcement Learning (MARL) [2], [11], [10]. Many current learning motion planners have similar characteristics: discretized environment, single- or multi-agent, non-generative, and no replanning. A discretized environment limits the learning algorithm's efficiency and cost-effectiveness based on the granularity of the discretization. Many learning models do not account for heterogeneous agents. Learning-based approaches such as Neural-RRT [12] or Neural-A* [13] utilize a Deep Learning method to provide candidate locations or probability distributions for classical algorithms to leverage instead of generating a path themselves. Thus, they still run the baseline algorithms to generate the paths, making them inefficient. These methods, while shown to be faster than simple RRT* and A*, respectively, are still slow compared to pure learning-based approaches.\nUsing a Learning-based Motion Planner with symbolic knowledge of the agents provides a new path-finding method that can be trained to maximize the number of targets reached while avoiding collisions. In addition, our method minimizes the computational cost of recalculating a new non-colliding path in a dynamic environment.\nWe present the following in this paper:\n\u2022 Present a Dynamic Motion Planning Environment for Heterogenous Teams that can be used for Robots in Sports.\n\u2022 Provide an End-to-End Trainable Method that can be run in a Decentralized Setting for Agile-Motion Planning.\n\u2022 Discuss future work to make the models scalable through the use of Graph Neural Networks to allow for the use of the model in different team compositions."}, {"title": "II. BACKGROUND", "content": "Table I represents recent work on motion planning. The existing work indicate that there is a need for heterogenous multi-agent motion planner for dynamic environments in both discrete and continuous space. While this paper focuses on the learning of a dynamic motion planner, we plan to address scalability, which accounts for either multiple obstacles or multiple agents, in our future work."}, {"title": "A. Motion Planning", "content": "The simplest motion planning goes by in a straight line from a start location to the end location with no consideration of obstacles. Non-learning motion planning algorithms, including RRT, RRT*, and RRG, account for obstacle avoidance but have poor sample efficiency. Learning motion planning algorithms, such as a learning RRT, can improve the sample efficiency of its non-learning counterpart. However, learning motion planner algorithms still require non-learning motion planners to generate their path, so they have the same time complexity as their non-learning counterparts."}, {"title": "III. PROBLEM STATEMENT AND SETUP", "content": ""}, {"title": "A. Environment", "content": "We present a simple motion planning environment that allows for the representation of team members, opponents, and goal locations. The environment can be translated into the context of Robosoccer Environment[5] as shown in Fig. 1. The agent is tasked with moving to the target location with the ball, intercepting it, and moving it to another series of locations while avoiding getting close to the opponent players who aim to intercept the player or avoid colliding with the other teammates. Both other teammates and opponent players are represented as static or moving obstacles, making the model decentralized. The training was done on a single agent, and therefore each agent can also be trained decentralized.\n\u2022 Agents: Heterogenous Agents with different speeds and different collision radius that are assigned a different sequence of target locations\n\u2022 Targets: Target locations with a circular keep-out radius that the agents are assigned to. The model takes in the sequence of target locations for each agent. This approach allows for the integration of a higher-level centralized controller that handles the Task Allocation and Scheduling[22] while allowing the Motion Planner to be run decentralized.\n\u2022 Obstacles: Obstacles or opponents that the agents need to avoid with a circular keep-out radius. These opponents can be moving or static. At each time step, the model takes in their current location and generates a new action for the agents."}, {"title": "B. Problem Formulation", "content": "We formulate the motion planning problem as a fully observable Markov Decision Process (MDP) using the five-tuple (S, A, T, R, \u03b3) shown below:\n\u2022 States: The problem state S in Motion Planning Problems is a joint state consisting of all the agent, target, and obstacle information.\n\u2022 Actions: Actions at time-step t within the Motion Planning Domain refers to a complete set of single-step motion plans for each agent, denoted as At = [{\\langle \u2206x_{i1t}, \u2206y_{i1t}\\rangle, \\langle \u2206x_{i+1+t}, \u2206y_{i+1+t}\\rangle,...], to be executed at the same time in time-step t for all of agents i.\n\u2022 Transitions: T corresponds to executing the action in Motion Planner and proceeds to the next time step.\n\u2022 Rewards: Rt is based on the scheduling objective the user wants to optimize and the number of collisions that the agents have collided. In Section III-D we show how to compute Rt when optimizing motion plan to avoid collision with obstacles.\n\u2022 Discount factor: \u03b3."}, {"title": "C. Model", "content": "We utilize a simple Neural Network Model that takes in the following information to generate an output of (dx, dy).\n\u2022 Agent Information: Current location (xu,t, yu,t) at time-step t for agent u.\n\u2022 Target Information: Current location, (xv,t, yv,t) at time-step t for target v."}, {"title": "D. Policy Optimization", "content": "Our reward policy is trained using a Reward based on the distance to target, and the distance to the obstacles [23] and is formulated in Equation 1.\n$R = -\\alpha D(u, v) + \\sum_{i=1}^{N}g(D(u, o_i), r_{o_i})$\n$g(D,r) = \\begin{cases} \\beta_1(D - r), \\text{ if } D-r\\geq0\\\\ \\beta_2(D - r) \\text{ otherwise } \\end{cases}$\nwhere \u03b1, \u03b2\u2081, and \u03b22 are constants set to 10, 1, 100, respectively, and D is the distance function to the obstacle's center and r is the radius of the keep out range from the center where the unit is 'threatened' and collides with the obstacle.\nWe train our model in using Policy Gradient methods that seek to directly optimize the model parameters based on the present reward received from the environment [24].\n$\\nabla_{J}(\\theta)= E_{\\pi} [\\sum_t A^{\\pi}(s_t,a_t) \\nabla_{\\theta}log \\pi_{\\theta} (a_t|s_t)]$\nIn Equation 3, the advantage term, At, is estimated by subtracting the mean reward from a batch of policies from the reward of each policy. Each decision is sampled from a policy generated from a single observation, and an action is chosen randomly across them to perpetuate the environment in the next time step. We use the gradients calculated from Equation 3 to update the model weights."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We train our model in a static environment for a single agent, 3 targets, and 10 obstacles with knowledge of N = 10 closest obstacles for 4000 training steps. Our observation for the model is limited to observing the closest 10 obstacles. This is the limitation of the model being unable to scale as the number of obstacles increases.\nWe set \u03b3 = 0.99, batch size = 8 and used Adam optimizer [25] with a learning rate of 8 \u00d7 10-3, and a weight decay of 1 \u00d7 10-4. Specifically, we compute the gradient of the model using the log-likelihood at each stage for each agent, as shown in Equation 3: We train our model using static obstacles for 1 agent and 3 targets. All models are trained and evaluated on a Mac Studio 2022 with an Apple M1 Ultra Chip.\nWe test our model on different seeds (10, 11, 12) and compare against the Baseline on 3 domains for 3 independent sets of 100 test problems as follows:\n\u2022 Static Simple Domain: 1 Agent assigned to 3 Targets in random order with static 10 Obstacles.\n\u2022 Multi-Agent Dynamic Domain: 3 Agents assigned to 3 Targets in random order, with mobile 10 Obstacles moving to one of the 3 Agents at random in a straight line, at the same speed as Agents.\n\u2022 Multi-Agent Dynamic Domain (Large): 3 Agents assigned to 10 Targets in random order, with 10 mobile Obstacles that are moving to one of the 3 Agents at random in a straight line, at the same speed as Agents.\nWe evaluate the performance of a Motion Planning Policy using 3 metrics:\n\u2022 Number of Collisions over the entire timeline of each problem instance, with the maximum performance based on the number of time steps.\n\u2022 Number of Targets Reached over the entire runtime of the problem.\n\u2022 Weighted Score based on:\n$S=\\alpha \\times n_t - n_c$\nwhere \u03b1 is a constant set to 10, nt is the number of targets reached, and nc is the number of collisions over the entire runtime instance.\nWe show the performance of the trained obstacle avoidance policy compared to the baseline solution of going directly to the target, without any knowledge of the obstacles in Figures 2, 3 and 4.\nThe baseline is always able to reach the provided targets due to taking the shortest path to the location in the continuous domain. The performance of our model in Figs. 2(b), 3(b) and 4(b) show that our model learns to reach the target locations.\nIn the Simple Domain in Figs. 2, the performance of the trained model is less than the performance of the baseline, as there are more collisions and the weighted score shows a similar pattern to the number of targets reached. The initial low number of collisions is due to the untrained policy, where the agent is not moving to the target and therefore not moving within the keep-out area of the obstacles.\nIn the Multi-Agent Dynamic Domain in Figs. 3, the number of collisions decreases over time as the mobile obstacles move to the location of the agents. This forces the agents to learn to move away. The learned performance shows that training learned on simple path planning in static environments is transferable to dynamic domains.\nThe main challenges of Multi-Agent Dynamic Domain (Large) are the number of Targets being too large for even the fastest policy (baseline) to complete, and the lack of full-observability for our model. The trained model takes in the 10 closest obstacles, treating Targets that it is not currently assigned to as obstacles as well. The limited observability of a larger number of obstacles leads to a decrease in the performance of the trained model as seen in Figs. 4."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "We present a Multi-Agent Agile Motion Planning Model for navigation across an environment with static and moving obstacles. We show that our model is end-to-end trainable in a custom environment that is based on Robosoccer, with multiple agents required to avoid moving obstacles. We further show that our model is capable of handling navigation in continuous space dynamically and avoiding moving targets.\nA key limitations of the current model is its scalability. Scalability in multi-agent teams is a challenge that is an open research problem [26]. We plan to integrate the use of Graph Neural Networks for the Motion Planner to address this need for scalability. While our model scales with the number of agents, it is unable to accurately represent the observation space leading to performance drop as seen in Figure 4. The scalability of graph-based models would allow us to address the limitations presented within our results [27], [28].\nThe environment that we have presented utilizes a fully observable MDP. We show that our model is able to perform under partial observability or stochastic behavior of team members or opponents. We plan to expand on our current work to account for different observability conditions.\nWith the presence of an adversary, the environment we have presented can be further developed into accounting for Adversarial games, using the Game Theoretic approaches for MARL [29]. We plan to further develop our environment to more accurately represent different adversarial gameplay scenarios."}]}