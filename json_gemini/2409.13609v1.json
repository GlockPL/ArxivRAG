{"title": "MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension", "authors": ["Ting Liu", "Zunnan Xu", "Yue Hu", "Liangtao Shi", "Zhiqiang Wang", "Quanjun Yin"], "abstract": "Referring Expression Comprehension (REC), which aims to ground a local visual region via natural language, is a task that heavily relies on multimodal alignment. Most existing methods utilize powerful pre-trained models to transfer visual/linguistic knowledge by full fine-tuning. However, full fine-tuning the entire backbone not only breaks the rich prior knowledge embedded in the pre-training, but also incurs significant computational costs. Motivated by the recent emergence of Parameter-Efficient Transfer Learning (PETL) methods, we aim to solve the REC task in an effective and efficient manner. Directly applying these PETL methods to the REC task is inappropriate, as they lack the specific-domain abilities for precise local visual perception and visual-language alignment. Therefore, we propose a novel framework of Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER. Specifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned prior, and Local Convolution Adapters to extract precise local semantics for better visual perception. Moreover, the Prior-Guided Text module is proposed to further utilize the prior for facilitating the cross-modal alignment. Experimental results on three widely-used benchmarks demonstrate that MaPPER achieves the best accuracy compared to the full fine-tuning and other PETL methods with only 1.41% tunable backbone parameters.", "sections": [{"title": "1 Introduction", "content": "Referring Expression Comprehension (REC) (Kamath et al., 2021; Liu et al., 2023; Wu et al., 2023; Bu et al., 2023) is a crucial and challenging task within the multimodal fields, which needs to localize the local image region according to the language expression semantics. REC is fundamental for visual language understanding, with broad applications in fields such as visual-language navigation (Liu et al., 2024a) and human-machine interaction (Chen et al., 2023). Different from vanilla object detection task, REC needs to extract not only global and local spatial information from images, but also relies on the alignment of multimodal features.\nExisting approaches (Deng et al., 2021; Kamath et al., 2021; Deng et al., 2023; Shi et al., 2022) transfer the language and vision knowledge from pre-trained models by fully fine-tuning. However, such a fine-tuning strategy is sub-optimal for REC, as reflected in the following aspects: 1) Fine-tuning the entire backbone might suffer catastrophic forgetting and undermine the extensive prior knowledge learned from pre-training. 2) The computational cost requirements surge dramatically, particularly for larger foundational models, leading to a significant increase in GPU memory usage. This limits the accessibility of large models for researchers with limited hardware resources.\nTo address these issues, we shift our focus to Parameter-Efficient Transfer Learning (PETL) (Lester et al., 2021; Chowdhury et al.,"}, {"title": "2 Related Work", "content": "2.1 Referring Expression Comprehension\nReferring expression comprehension (REC) (Yu et al., 2018; Yang et al., 2019; Deng et al., 2021; Xiao et al., 2023; Liu et al., 2024e; Xiao et al., 2024) aims to locate a local visual region in images by textual descriptions. Early propose-and-rank methods (Liu et al., 2019b; Hong et al., 2019; Chen et al., 2019) follow a two-stage pipeline which first utilizes pre-trained object detectors to obtain a set of region proposals, which are then ranked based on their similarity scores with the given textual description. However, these two-stage methods face challenges in terms of the performance of the proposal generators and the additional ranking mechanisms. After the introduction of ViT, the Transformer-based methods (Deng et al., 2021; Du et al., 2022; Yang et al., 2022; Zhu et al., 2022; Su et al., 2023; Liu et al., 2024c; Zhu et al., 2023)"}, {"title": "2.2 Parameter-efficient Transfer Learning", "content": "The continuous expansion of pre-trained models demands significant computational resources and consumes considerable storage during fine-tuning (Liu et al., 2024d). To address these challenges, researchers in the NLP and CV domain have explored PETL methods (Lester et al., 2021; Hu et al., 2022; Chen et al., 2022; Yuan et al., 2023; Zhou et al., 2024; Xu et al., 2024; Liu et al., 2024b). By focusing on updating only a small subset of parameters, PETL achieves a balance between maintaining high performance and ensuring computational efficiency. This method is particularly advantageous for deploying large-scale models, addressing the challenges posed by increasing model sizes while streamlining the adaptation process to new tasks. The main PETL methods can be classified into three categories: (i) selectively updating a tiny number of existing model parameters (Guo et al., 2020; Zaken et al., 2021); (ii) adjusting newly added parameters to the model or its input (Li and Liang, 2021; Zhou et al., 2022; Xin et al., 2024b); (iii) applying low-rank factorization techniques to the parameters that require updates (Hu et al., 2022; Karimi Mahabadi et al., 2021; Hao et al., 2023; Liu et al., 2024f; Xin et al., 2024a). Some pioneering works like ETRIS (Xu et al., 2023) and DARA (Liu et al., 2024c) sought to utilize adapters to adapt pre-trained models to referring image segmentation and referring expression comprehension, respectively. However, their proposed modules like Bridger (Xu et al., 2023) and RA (Liu et al., 2024c) are insufficient for capturing the complexity of multi-scale local visual features."}, {"title": "3 Methodology", "content": "3.1 Framework Overview\nThe overall framework of the proposed MaPPER is illustrated in Figure 2. Our approach freezes the pre-trained backbone, ensuring parameter ef-"}, {"title": "3.2 Text & Image Feature Extraction", "content": "Text Encoder. The REC task relies heavily on word-level understanding due to its concise linguistic expression format, such as \"front middle yellow guy\", to convey referring information. Owing to its bi-directional encoder representations and the masked language modeling, BERT (Devlin et al., 2018) excels in word-level understanding, making it suitable for text encoding in REC domain. Given the input referring expression T, the text expression is firstly converted into a one-hot vector. Subsequently, each one-hot vector is tokenized into a series of linguistic tokens. A special [CLS] token is prefixed to the sequence, and the sequence of tokens is then fed into a stack of 12 transformer encoder layers to progressively capture and model the intricate language tokens.\nVisual Encoder. Our work adopts the transformer-based DINOv2-B/14 (Oquab et al., 2023) as the visual backbone. The model involves training the Vision Transformer (ViT) model (Dosovitskiy et al., 2020) on the extensive LVD-142M dataset, utilizing a self-supervised learning strategy. This approach equips the model with the ability to extract powerful visual features, which in turn delivers impressive performance across various downstream tasks. Given an input image $I_0 \\in \\mathbb{R}^{H_0 \\times W \\times 3}$, the image is initially divided into N non-overlapping patches, which are then linearly projected into D-dim patch embeddings $I_p \\in \\mathbb{R}^{N \\times D}$. Meanwhile, a learnable [CLS] token is prepended to $I_p$, producing $I \\in \\mathbb{R}^{(N+1) \\times D}$.\nConsidering the substantial number of parameters, we opt to freeze visual and text encoders during the fine-tuning process. This strategy allows for a more efficient allocation of computational resources and focuses the learning on the adjustments of other modules."}, {"title": "3.3 Prior-guided Text Understanding", "content": "As detailed in section 3.2, the pre-training mechanism of BERT makes it ideal for the REC task, which has a relatively high word-level understanding. However, BERT lacks alignment with vision in the pre-training process, and we introduce a Vision-aligned Prior Module to generate a vision-aligned prior. The prior serves for better adjusting BERT encoder, and promoting the interaction of text and vision features.\nVision-aligned Prior Module (VAP). The core of VAP to a produce vision-aligned prior for the REC domain. Considering that CLIP (Radford et al., 2021) model inherently has the ability to align visual with text feature, we used the frozen CLIP followed by a mapping layer M as the VAP module. Given the text input t, the vision-aligned prior p can be formulated as follows:\n$p = M(CLIP_f(t)).$ (1)\nwhere the $CLIP_f$ denotes the frozen CLIP backbone.\nDynamic Prior Adapter (DyPA). To dynamically bridge the gap between the pre-trained BERT model and the complex REC task, we introduce the Dynamic Prior Adapter, which operates in parallel with the text encoder, as shown in Figure 3. DyPA comprising four module: a dynamic scale module (DS), a downward projection with parameters $W_{down} \\in \\mathbb{R}^{r \\times d}$, a ReLU activation layer, and an upward projection with parameters $W_{up} \\in \\mathbb{R}^{d \\times r}$. Specifically, we adopt the DS module for integrating the vision-aligned prior p to different layers in the BERT encoder. The module generates scale factors $S_f$ using a scoring weight matrix $W_s \\in \\mathbb{R}^{1 \\times d}$, eliminating manual hyper-parameter tuning. Given the prior p, the dynamic scaling factor can be formulated as follows:\n$S_f = ReLU(pW_s).$ (2)\nThe downward projection and the upward projection are connected by a ReLU function. In one text encoder layer, the downward projection layer receives processed language tokens $x_t$ from the Multi-head Attention (MHA) layer as input and produces adapted. In general, the output of DyPA $x_t^'$ can be described as\n$x_t^' = S_f \\times [(ReLU(x_t W_{down})) W_{up}].$ (3)\nDyPA utilizes the vision-aligned prior p to dynamically regularize the feed-forward during adapter tuning. To mitigate the influence of Adapter outputs during the initial stages of model training, we initialize $W_{up}$ to zero.\nPrior-guided Text Module (PGT). Through the design of the DyPA module, we efficiently fine-tune the BERT model to produce fine-grained aligned text features for the REC tasks. In order to promote the interaction of text and vision features for the Multimodal Interactive Module in"}, {"title": "3.4 Global & Local Visual Perception", "content": "For visual perception in the REC task, local features and global representations are important counterparts. Although pre-trained DINOv2 can provide powerful and robust visual features to achieve promising performance, the task-specific visual attention in the REC task often focuses on localized areas of uncertain size in images, which have been visualized in Figure 4.\nLocal Convolution Adapter (LoCA). To further facilitate the visual perception ability of DINOv2 for the REC task, we propose a Local Convolution Adapter (LoCA) module to adjust the visual foundation models. LOCA introduces the multi-scale local information to further enhance visual perception. The local convolution adapter consists of a down-projection layer $W_{down}$, a multi-scale convolution module, a ReLU activation layer, and the up-projection layer $W_{up}$.\nSpecifically, in one visual encoder layer, the downward projection layer receives processed visual tokens $x$ from the Multi-head Attention (MHA) layer as input and produces adapted. The multi-scale convolution module consists of two parallel convolutional paths of multi-scale (1\u00d71, 3\u00d73). The 1x1 convolution is strategically placed before the 3x3 convolutions to reduce channel dimension. This design and the bottleneck structure make the local convolution adapter still lightweight. The outputs of the multi-scale convolutional paths are concatenated to form the local feature $f_{loc}$.\n$f_v = ReLU(x W_{down}),$\n$f_{v1} = Conv_{1 \\times 1}(f_v),$\n$f_{v2} = Conv_{3 \\times 3}(Conv_{1 \\times 1}(f_v)),$\n$f_{loc} = Concat[f_{v1}, f_{v2}].$ (5)\nbefore the up-projection, a skip connection operates in parallel with the multi-scale convolution module.\n$f_{loc}' = f_{loc} + f_v,$\n$f_{loc} = (f_{loc}' W_{up}).$ (6)\nGlobal and Local Visual Integration. To augment the DINOv2 backbone with multi-scale local visual perception on the REC task, we integrate the Local Convolution Adapter (LoCA) in parallel with the MLP layer within the transformer block. By the concise design, LoCA module adds multi-scale local prior into the DINOv2 model for the REC task. The output of each adapted transformer block can be described as:\n$v^{mha}_{l} = MHA(LN(v_{l-1})) + v_{l-1},$\n$v_l = MLP(LN(v^{mha}_{l})) + s \\cdot f_{loc} + v^{mha}_l$ (7)\nwhere s is the scaling factor, and the $v_{l-1}$ represents the previous layer output."}, {"title": "3.5 Multimodal Interactive Module", "content": "We have implemented a transformer (Vaswani et al., 2017) architecture that seamlessly integrates multimodal embeddings to forecast the bounding box of the referenced object. Specifically, the adapted vision embeddings $f_v \\in \\mathbb{R}^{N_v \\times C_v}$ and language embeddings $f_l \\in \\mathbb{R}^{N_l \\times C_l}$ are first projected into a common space of joint embeddings $f_v' \\in \\mathbb{R}^{N \\times C_p}$ and $f_l' \\in \\mathbb{R}^{N_l \\times C_p}$, both with a unified channel size. Followed by TransVG (Deng et al., 2021) and"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets and Evaluation Metrics. We validate our method on three widely-used REC benchmarks: RefCOCO (Yu et al., 2016), RefCOCO+ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016; Nagaraja et al., 2016). We follow the previous research that employs top-1 accuracy (%) as the evaluation metric. Specifically, a prediction is deemed accurate only when its IoU exceeds or equals 0.5. In addition to Precision@0.5, we also report the number of tunable parameters in the pre-trained encoders to compare the fine-tuning efficiency with traditional full fine-tuning and other PETL methods.\nImplementation Details. The vision encoder is initialized with DINOv2-B/14 (Oquab et al., 2023), while the language encoder uses BERT-base (Devlin et al., 2018). The resolution of the input image is 518x518. Both the DINOv2-B/14 model and the BERT-base model process tokens with a feature dimension of 768. The Multimodal Interactive Module uses Xavier initialization. DyPA are initialized with Kaiming normal initialization and inserted into the transformer layers for the language encoder. The bottleneck dimension $C_d$ for DyPA is 32. For LoCA, the 1x1 convolution before the 3\u00d73 convolution reduces the channel to 24. The output dimensions of the two convolutional paths are 192 and 96, so the input dimension of the these convolutional paths is 288. For fair comparisons, PETL methods in Table 2 use the same base architecture, and keeping the vision and language encoder fixed."}, {"title": "4.2 Main Results", "content": "We conducted a comprehensive comparison between our proposed MaPPER model and a series of previous referring expression comprehension (REC) methods. The main experimental results are presented in Table 1, from which we can observe that: MaPPER achieves the best accuracy while ensuring parameter efficiency among all methods, thus validating its effectiveness and efficiency.\nEffectiveness. As Table 1 shown, on the three commonly challenging benchmarks, MaPPER outperforms all traditional full fine-tuning methods. Compared to DARA (Liu et al., 2024c), a parameter-efficient transfer learning method, we achieves best results on the three benchmarks. Notably, even"}, {"title": "4.3 Comparison with Other PETL Methods", "content": "We conduct experiments comparing our MaPPER with other parameter-efficient tuning methods using DINOv2-Base as the backbone. To ensure fairness, we retain the original parameter settings from previous methods and adjust the bottleneck to achieve comparable parameter counts. Table 2 illustrates that MaPPER outperforms other PETL methods on all three benchmarks, and even performs better than fully fine-tuning. This highlights the effectiveness of MaPPER in adapting pre-trained knowledge for the REC domain. Through introducing vision-aligned prior, MaPPER enhance the modeling of the vision-text alignment capability."}, {"title": "4.4 Ablation Study", "content": "Effectiveness of Local Convolution Adapter. We assess the impact of the Local Convolution Adapter (LOCA) by performing an ablation study and reporting the results on RefCOCO validation and test datasets. From Table 3, it is evident that introducing the LoCA yields a great improvement, increasing the average performance to 1.87%. This indicates that the LoCA enhances the visual perception of DINOv2 with local visual feature.\nEffect of Multi-scale Size for Visual Branch. To further verify the effect of local visual information, we perform the attempts of using only a single-size convolution kernel (1\u00d71), and three scales (1\u00d71 3x3 5x5). Table 4 indicates that it is difficult for an adapter with a single-size convolution kernel (a) to perform well for the REC. Local Features are too fine-grained (c) are also not optimal. In contrast, appropriate multi-scale (b) provide proper local information, thus achieving the best performance.\nEffect of the Vision-aligned Prior for Text Branch. From Table 5, we can see that: (1) Freezing the text encoder while only tuning local"}, {"title": "4.5 Qualitative Results", "content": "To investigate the impact of vision-aligned prior, we visualize the attention maps from the Multimodal Interactive Module under two strategies: with and without the vision-aligned prior. In the absence of the prior represents the text adapter without dynamic scale, and the prior-guided text module is not introduced. As shown in Fig. 4, referring expressions contain object appearance attributions, human actions, and spatial relationships. It is observable that the model can focus well on the local target region of the whole image with the visionaligned prior. This indicates that vision-aligned prior enhancing the alignment ability of MaPPER."}, {"title": "5 Conclusion", "content": "In this study, we present an innovative ParameterEfficient Transfer Learning (PETL) approach designed for multi-modal language grounding tasks, especially in referring expression comprehension. MaPPER enhances the adapters with multi-modal prior through the implementation of a simple yet effective fine-tuning strategy. We aims at improving both the effectiveness and efficiency of visualtext alignment, as well as enhancing visual perception by incorporating local visual semantics. The Dynamic Prior Adapter (DyPA) employs aligned prior to dynamically adjust the language encoder, while the Local Convolution Adapter (LoCA) introduces local visual features for enhancing the visual encoder. MaPPER not only surpasses the performance of fully fine-tuned models but also"}, {"title": "6 Limitation", "content": "While our proposed method has shown enhanced efficiency, scalability, and parameter optimization in the realm of REC tasks, surpassing conventional fully fine-tuned models, our empirical inquiries have been confined to this specific domain. It is imperative for future research to broaden the validation scope encompass include variety range of other multi-modal tasks. Moreover, while our approach can effectively decrease the quantity of parameters necessitating training, thus conserving computational and storage resources, it still mandates a training process. As the frontier of multi-modal large-scale models progresses, there is a significant opportunity for future exploration into openvocabulary zero-shot referring expression comprehension. This area of research could unveil innovative pathways and contribute to the evolution of models capable of comprehending and generating expressions without the constraint of prior training."}]}