{"title": "Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks", "authors": ["Yun Qu", "Boyuan Wang", "Jianzhun Shao", "Yuhang Jiang", "Chen Chen", "Zhenbin Ye", "Lin Liu", "Junfeng Yang", "Lin Lai", "Hongyang Qin", "Minwen Deng", "Juchao Zhuo", "Deheng Ye", "Qiang Fu", "Wei Yang", "Guang Yang", "Lanxiao Huang", "Xiangyang Ji"], "abstract": "The advancement of Offline Reinforcement Learning (RL) and Offline Multi-Agent Reinforcement Learning (MARL) critically depends on the availability of high-quality, pre-collected offline datasets that represent real-world complexities and practical applications. However, existing datasets often fall short in their simplicity and lack of realism. To address this gap, we propose Hokoff, a comprehensive set of pre-collected datasets that covers both offline RL and offline MARL, accompanied by a robust framework, to facilitate further research. This data is derived from Honor of Kings, a recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate nature, closely resembling real-life situations. Utilizing this framework, we benchmark a variety of offline RL and offline MARL algorithms. We also introduce a novel baseline algorithm tailored for the inherent hierarchical action space of the game. We reveal the incompetency of current offline RL approaches in handling task complexity, generalization and multi-task learning.", "sections": [{"title": "Introduction", "content": "Online Reinforcement Learning (Online RL) relies on the interaction between the training policy and the environment for data collection and policy optimization [33, 18, 7]. However, this paradigm makes online RL unsuitable for certain real-world scenarios, such as robotics and autonomous driving [23, 18], as deploying untested policies to the environment can be costly and dangerous [19]. In contrast, Offline Reinforcement Learning (Offline RL) can learn satisfactory policies using a fixed dataset without the need for further interaction with the environment [23, 18, 7, 19]. This characteristic alleviates the aforementioned issue, making offline RL potentially more suitable for certain real-world scenarios compared to online RL [23].\nThe research on offline RL has attracted significant attention in recent years and has made substantial progress in both theoretical analysis and practical performance. The core challenge of offline RL is the value overestimation issue induced by distributional shift [9, 21]. Existing studies mitigate this problem by constraining the learning policy to closely resemble the behavior policy induced by the dataset [39, 18, 7], or adopting conservative value iteration [19, 16]. The success of offline RL can be largely attributed to the availability of widely-adopted open-access datasets, such as D4RL [6] and RL Unplugged [12]. These datasets offer standardized and diverse pre-collected data for the development of new algorithms, while also offering proper evaluation protocols that facilitate fair comparisons between different algorithms. However, despite their benefits, tasks contained in these datasets (such"}, {"title": "Related Works", "content": "Offline RL gains significant attention in recent years, primarily due to the inherent difficulties of directly applying online RL algorithms to offline environments. The main hurdles encountered is the issue of erroneous value overestimation, which arises from the distributional shift between the dataset and the learning policy [9]. Theoretical studies have demonstrated that the overestimation issue can be alleviated by pessimism, which results in satisfactory performance even with imperfect data coverage [4, 5, 14, 20, 25, 30, 41, 48]. In practice, certain studies [1, 2, 40, 1, 2, 40] employ uncertainty-based methods to estimate Q-values pessimistically or to perform learning on pessimistic dynamic models by estimating the epistemic uncertainty of Q-values or dynamics. Some studies [18, 9, 39, 16, 19, 7] adopt behavior regularization-based approaches by imposing constraints on the"}, {"title": "Background", "content": "Honor of Kings (HoK) is one of the most popular MOBA games worldwide, boasting over 100 million daily active players [38]. The game involves two teams, each consisting of several players who have the option to select from a wide range of heroes with diverse roles and abilities. In the game, heroes are expected to eliminate enemy units, such as heroes, creeps, and turrets, to gain gold and experience. The primary objective is to destroy the enemies' turrets and crystal while defending their own. To succeed in MOBA games, players must learn how to choose the appropriate hero combination, master complex information processing and action control, plan for long-term decision-making, cooperate with allies, and balance multiple interests. The complex rules and properties of HoK make it be more in line with the complex decision-making behavior of human society. Thus, HoK has attracted numerous researchers interest [45, 44, 38, 37, 10].\nThe underlying system dynamic of HoK can be characterized by a Partially Observable Markov Decision Process (POMDP [33]), denoted by $\\mathcal{M} = (S, O, A, P, r, \\gamma, d)$. Due to the fog of war and private features, each agent has access to only local observations $o$ rather than the global state $s$. Specifically, the agents are limited to perceiving information about game units within their field of view, as well as certain global features. Due to the intricate nature of control, the action space $A$ is organized in a hierarchically structured manner, rather than being flattened, which avoids the representation of millions of discretized actions. Randomness is added into the transition distribution $P$ in the form of critical hit rate. The reward $r$ is decomposed into multi-head form and each hero's reward is a weighted sum of different reward items and is designed to be zero-sum. Details of observation space, action space and reward are presented in Appendix D."}, {"title": "Hokoff", "content": "This study is based on the HoK gaming environment, which encompasses both 1v1 and 3v3 maps. Our research proposes a comprehensive offline RL framework applicable to this gaming environment and utilizes it to generate diverse datasets. This section provides an introduction to the framework, game modes, datasets, and evaluation protocol employed in this study."}, {"title": "Framework", "content": "To enhance the usability of our Hokoff, we propose a reliable and comprehensive Offline RL framework that consists of three modules: sampling, training, and evaluation. This framework streamlines the process of sampling new datasets, developing and training baselines, and evaluating their performance. The sampling module provides a simple and unified program for sampling diverse datasets using any pre-trained checkpoints. There are several reasons why our framework excels in sampling. Firstly, diverse datasets at different levels of expertise can be sampled by leveraging Multi-Level Models as described in Sec. 4.1.1. Secondly, our framework employs parallel sampling techniques, ensuring efficient sampling of large and diverse datasets. Based on the training module, we have implemented various offline RL and offline MARL algorithms as baselines. Additionally, we consolidate crucial components and provide user-friendly APIs, facilitating researchers to effortlessly develop novel algorithms or innovative network architectures. The evaluation module enables the assessment of trained models from different algorithms, ensuring fair comparisons. Fig. 2 demonstrates the architecture of our framework and Appendix E provides an example of the APIs."}, {"title": "Multi-Level Models", "content": "To ensure a valid and unbiased comparison of the performance of distinct algorithms, it is crucial to establish appropriate evaluation protocols [6, 12]. One such effective evaluation protocol is the normalized score [6]. However, HoK is a zero-sum adjustable rewards MOBA game. The episode return in the game is heavily influenced by the opponents and game settings, and the objective is to win, which renders the use of return as a performance metric biased. Therefore, normalized score may not fully capture our requirements. Furthermore, similar to our situation, the evaluation protocol for SMAC [31], a competitive game, is based on win rate against a pre-programmed AI. Nonetheless, it is exceedingly challenging to create a built-in AI with human-like performance due to the complexity of MOBA games.\nInspired by prior works of HoK [38], we present Multi-Level Models for sampling and evaluating which contains multiple checkpoints with different level. Specifically, we have extracted several checkpoints from pre-trained dual-clip PPO [45, 44] models with varying levels determined by the outcome of the battle separately for HoK1v1 and HoK3v3. We adopt the win rate against different checkpoints as our evaluation protocols to assess the ability of models. Additionally, these models, with varying levels, can be utilized on both sides to sample diverse battle data. The capabilities of these models surpass those of rule-based AI and match the levels of different human players, thus making these evaluation protocols more suitable for comparing algorithmic performance with human-level performance and facilitating diverse and effortless sampling. The details of these Multi-Level Models are provided in the Appendix F."}, {"title": "Game Modes", "content": "We have incorporated two game modes from HoK into our study, namely HoK1v1 [38] and HoK3v3. The environment code of HoK3v3 is integrated into the open-source HoK1v1 code following Apache License V2.0. These game modes differ in the number of agents involved and the underlying map used. Detailed information on each game mode is presented below."}, {"title": "Honor of Kings Arena", "content": "Honor of Kings Arena (HoK Arena or HoK1v1) is a 1v1 game mode where each player attempts to beat the other and destroy its opponent's crystal. Specifically, each player chooses a hero before the game starts and controls it during the whole game. There are a total of 20 heroes available for players to select, each possessing distinct skills that exert diverse effects on the game environment. The observation space is a continuous space consisting of 725 dimensions that contain partial observable information about the hero, opponent, and other game units. The action space is hierarchically structured and discretized, covering all possible actions of the hero in a hierarchical triplet form: (1) which action button to take; (2) who to target; and (3) how to act. Furthermore, the reward is a weighted sum of five categories: farming, kill-death-assist (KDA), damage, pushing, and win-lose. For a full description of this game mode, please refer to the Appendix D.1."}, {"title": "Honor of Kings 3v3 Arena", "content": "To further cater to the demand for Offline MARL, we adopt Honor of Kings 3v3 Arena (HoK3v3) as our experimental platform. HoK3v3 is a MOBA game, where each team comprises three heroes who collaborate to defeat their opponents. The basic rules and win conditions of HoK3v3 are similar to HoK1v1. However, the HoK3v3 map contains additional turrets on the middle road and features a new area called the \"wilderness\", inhabited by diverse monsters. Besides, collaboration is essential in HoK3v3, where players must select different heroes and fulfill distinct roles to work together more efficiently. For instance, one hero might focus on slaying monsters in the wilderness to earn gold and experience, while the other heroes engage in offensive tactics against the enemy heroes and game units. The design philosophies for observation space, action space, and reward are comparable to those used in HoK1v1. However, the level of complexity in HoK3v3 is significantly elevated. We provide a detailed description of the game mode in the Appendix D.2 for reference."}, {"title": "Subtasks", "content": "Both HoK1v1 and HoK3v3 are full MOBA games, featuring multi-camp competitions, which inherently pose challenges and limitations. Consequently, training on these game modes demands extensive training time and computational resources. However, HoK game comprises various sub-objectives, allowing us to decompose the overall game into manageable subtasks. These subtasks can represent diverse scenarios and are suitable for evaluating various algorithms. In this study, we propose two specific noncompetitive subtasks as outlined below. It is worth noting that researchers can readily expand upon our framework to develop additional subtasks."}, {"title": "Datasets", "content": "To enhance the practical implications of our datasets, we have incorporated design factors that align with the real-world applications of both HoK and other relevant scenarios."}, {"title": "Sub-Task", "content": "As introduced in Sec 4.2.3, we designed several practical and meaningful sub-tasks to provide diverse scenarios based on HoK. Based on these sub-tasks, we proposed diverse datasets to support Offline RL research similar to the design of previous studies [6, 12]."}, {"title": "Benchmarking", "content": "Based on our framework, we reproduce various Offline RL and Offline MARL algorithms. Besides, we fully validate and compare these baselines on our datasets. The results are presented in the form of test winning rate. Each algorithm is run for three random seeds, and we report the mean performance with standard deviation. The performance of behaviour policies is presented in Appendix C. Details of the implementations and experimental results can be referenced in Appendix G."}, {"title": "Baselines", "content": "The Offline RL baseline algorithms we implement are briefly introduced below: BC: Behavior cloning. TD3+BC [7]: One of the state-of-the-art single agent offline algorithm, simply adding the BC term to TD3 [8]. CQL [19]: Conservative Q-Learning conducts conservative value iteration by adding a regularizer to the critic loss. IQL [16]: Implicit Q-Learning leverages upper expectile value function to learn Q-function and extracts policy via advantage-weighted behavioral cloning.\nThe structured action space in HoK is similar to the joint action space in multi-agent settings, which inspires us to resort to the design in MARL methods. We propose a novel baseline algorithm, named QMIX+CQL. Specifically, we import QMIX algorithm from the MARL literature [29] to tackle the structured action space by regarding each head of the action space as a single agent and incorporate CQL regularizer term into local Q-funtion in QMIX for offline learning."}, {"title": "HoK3v3", "content": "The Offline MARL baseline algorithms are briefly introduced below: IND+BC: Behavior cloning with independent learning paradigm. IND+CQL: Adopts an independent learning paradigm for multi-agent settings, using conservative Q-learning [19]. COMM+CQL: Incorporate inter-agent communication based on IND+CQL. IND+ICQ [43]: Implicit Constraint Q-learning with independent learning paradigm, which only uses insample data for value estimation to alleviate the extrapolation error. MAICQ [43]: Multi-agent version of implicit constraint Q-learning by decomposed multi-agent joint-policy under implicit constraint with CTDE paradigm. OMAR [28]: Using zeroth-order optimization for better coordination among agents' policies, based on independent CQL."}, {"title": "Benchmark Results", "content": "We have validated the offline RL and offline MARL baselines on our datasets and aggregated the results in Table 4 and Table 5."}, {"title": "Additional Discussion", "content": "Task difficulty: Intuitively, the level of difficulty in the environment significantly impacts the performance of algorithms. However, previous researches only utilized one set of datasets with a uniform level of difficulty in the environment. Providing datasets with diverse difficulty can not only more comprehensively evaluate the ability of offline algorithms but also be more suited for real-world tasks like HoK, which are characterized by diverse levels of difficulty."}, {"title": "Author Statement", "content": "The authors of this work would like to state that we bear full responsibility for any potential violation of rights, including copyright infringement or unauthorized use of data. We affirm our commitment to conducting this research in accordance with ethical guidelines and legal requirements.\nWe further guarantee that we will ensure access to the data and the framework codes used in this study, making them available to interested researchers for verification and replication purposes. Additionally, we are committed to providing the necessary maintenance and support to ensure the longevity and accessibility of the data. For datasets, we have plans to consistently offer more datasets in the future. These datasets will include larger sizes for larger models, higher levels for expert agents, and novel design factors for other research directions. Updating our datasets is an ongoing and long-term effort, and we welcome contributions from the community. Regarding benchmarks, we will actively monitor the latest state-of-the-art (SOTA) algorithms in the offline RL domain and integrate them into our benchmarks. Additionally, we will develop new algorithms within the benchmarks based on existing datasets and baselines. This ensures that our benchmarks remain up-to-date and reflect the advancements in offline RL research.\nShould any concerns or inquiries arise regarding the contents of this work or the associated data, we encourage readers and fellow researchers to contact us directly. We are dedicated to addressing any issues promptly and transparently to uphold the integrity of our research."}, {"title": "Limitations and Future Works", "content": "In our future endeavors, we plan to integrate our framework with a large-scale deep reinforcement learning platform namely KaiwuDRL, specifically designed to support Honor of Kings. By doing so, we will gain access to greater computational resources, enabling us to delve deeper into our current research endeavors and expand our investigations."}, {"title": "Additional Datasets Details", "content": "In the Generalization category, \"norm_general\" and \"hard_general,\" have their corresponding datasets.\n For example, to sample the \"norm_general\" dataset, we let the level-1 model fight with level-0, level- 2, and level-4 models. However, during the test stage, we assess the generalization capabilities of the trained model by letting it fight against the level-1 model.\nDetails about how we sample the generalization datasets can be referred to Table 6. The latter four experiments do not require extra datasets. For example, in the \"norm_hero_general\" experiment, we directly use the model trained on the \"norm_medium\" dataset and let the model control different heroes. This is possible because the \"norm_medium\" dataset only contains the fixed default hero \"luban.\""}]}