{"title": "Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks", "authors": ["Yun Qu", "Boyuan Wang", "Jianzhun Shao", "Yuhang Jiang", "Chen Chen", "Zhenbin Ye", "Lin Liu", "Junfeng Yang", "Lin Lai", "Hongyang Qin", "Minwen Deng", "Juchao Zhuo", "Deheng Ye", "Qiang Fu", "Wei Yang", "Guang Yang", "Lanxiao Huang", "Xiangyang Ji"], "abstract": "The advancement of Offline Reinforcement Learning (RL) and Offline Multi-Agent Reinforcement Learning (MARL) critically depends on the availability of high-quality, pre-collected offline datasets that represent real-world complexities and practical applications. However, existing datasets often fall short in their simplicity and lack of realism. To address this gap, we propose Hokoff, a comprehensive set of pre-collected datasets that covers both offline RL and offline MARL, accompanied by a robust framework, to facilitate further research. This data is derived from Honor of Kings, a recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate nature, closely resembling real-life situations. Utilizing this framework, we benchmark a variety of offline RL and offline MARL algorithms. We also introduce a novel baseline algorithm tailored for the inherent hierarchical action space of the game. We reveal the incompetency of current offline RL approaches in handling task complexity, generalization and multi-task learning.", "sections": [{"title": "1 Introduction", "content": "Online Reinforcement Learning (Online RL) relies on the interaction between the training policy and the environment for data collection and policy optimization [33, 18, 7]. However, this paradigm makes online RL unsuitable for certain real-world scenarios, such as robotics and autonomous driving [23, 18], as deploying untested policies to the environment can be costly and dangerous [19]. In contrast, Offline Reinforcement Learning (Offline RL) can learn satisfactory policies using a fixed dataset without the need for further interaction with the environment [23, 18, 7, 19]. This characteristic alleviates the aforementioned issue, making offline RL potentially more suitable for certain real-world scenarios compared to online RL [23].\nThe research on offline RL has attracted significant attention in recent years and has made substantial progress in both theoretical analysis and practical performance. The core challenge of offline RL is the value overestimation issue induced by distributional shift [9, 21]. Existing studies mitigate this problem by constraining the learning policy to closely resemble the behavior policy induced by the dataset [39, 18, 7], or adopting conservative value iteration [19, 16]. The success of offline RL can be largely attributed to the availability of widely-adopted open-access datasets, such as D4RL [6] and RL Unplugged [12]. These datasets offer standardized and diverse pre-collected data for the development of new algorithms, while also offering proper evaluation protocols that facilitate fair comparisons between different algorithms. However, despite their benefits, tasks contained in these datasets (such"}, {"title": "2 Related Works", "content": "2.1 Offline RL and Offline MARL\nOffline RL gains significant attention in recent years, primarily due to the inherent difficulties of directly applying online RL algorithms to offline environments. The main hurdles encountered is the issue of erroneous value overestimation, which arises from the distributional shift between the dataset and the learning policy [9]. Theoretical studies have demonstrated that the overestimation issue can be alleviated by pessimism, which results in satisfactory performance even with imperfect data coverage [4, 5, 14, 20, 25, 30, 41, 48]. In practice, certain studies [1, 2, 40, 1, 2, 40] employ uncertainty-based methods to estimate Q-values pessimistically or to perform learning on pessimistic dynamic models by estimating the epistemic uncertainty of Q-values or dynamics. Some studies [18, 9, 39, 16, 19, 7] adopt behavior regularization-based approaches by imposing constraints on the"}, {"title": "2.2 Offline Datasets", "content": "The availability of large-scale pre-collected datasets has greatly facilitated the progress of deep supervised learning [11]. Offline RL, which is regarded as a bridge between RL and supervised learning, also requires learning policies from pre-collected datasets [6]. Therefore, high-quality pre-collected offline datasets play a significant role in the development of offline RL. To meet this demand, some datasets have been published and widely adopted. D4RL [6] is designed to address key challenges often faced in practical applications where datasets may have limited and biased distributions, incomplete observations, and suboptimal data. To tackle these issues, D4RL offers a range of datasets that enjoy these characteristics. Similarly, RL Unplugged [12] introduces a benchmark to evaluate and compare offline RL methods with various settings, such as partially or fully observable and continuous or discrete actions. These offline datasets play a significant role in offline RL research, and many previous works train and evaluate their methods based on these datasets [16, 2, 15, 28, 43].\nHowever, both D4RL and RL Unplugged primarily focus on relatively simple tasks and lack high-dimensional, practical and multi-agent tasks that closely resemble real-world scenarios. StarCraft II Unplugged [27] introduces a benchmark for StarCraft II, a complex simulated environment with several practical properties. However, they only utilize a dataset derived from human replays, which lacks diversity in design for offline RL, and they did not evaluate existing offline RL methods. To address this research gap, we propose Hokoff, a benchmark based on HoK, which aims to provide diverse offline datasets for high-dimensional, practical tasks, and present a comprehensive evaluation of previous offline RL and offline MARL methods with a general, easy-to-use framework."}, {"title": "3 Background", "content": "Honor of Kings (HoK) is one of the most popular MOBA games worldwide, boasting over 100 million daily active players [38]. The game involves two teams, each consisting of several players who have the option to select from a wide range of heroes with diverse roles and abilities. In the game, heroes are expected to eliminate enemy units, such as heroes, creeps, and turrets, to gain gold and experience. The primary objective is to destroy the enemies' turrets and crystal while defending their own. To succeed in MOBA games, players must learn how to choose the appropriate hero combination, master complex information processing and action control, plan for long-term decision-making, cooperate with allies, and balance multiple interests. The complex rules and properties of HoK make it be more in line with the complex decision-making behavior of human society. Thus, HoK has attracted numerous researchers interest [45, 44, 38, 37, 10].\nThe underlying system dynamic of HoK can be characterized by a Partially Observable Markov Decision Process (POMDP [33]), denoted by M = (S, O, A, P, r, \\gamma, d). Due to the fog of war and private features, each agent has access to only local observations o rather than the global state s. Specifically, the agents are limited to perceiving information about game units within their field of view, as well as certain global features. Due to the intricate nature of control, the action space A is organized in a hierarchically structured manner, rather than being flattened, which avoids the representation of millions of discretized actions. Randomness is added into the transition distribution P in the form of critical hit rate. The reward r is decomposed into multi-head form and each hero's reward is a weighted sum of different reward items and is designed to be zero-sum. Details of observation space, action space and reward are presented in Appendix D."}, {"title": "4 Hokoff", "content": "This study is based on the HoK gaming environment, which encompasses both 1v1 and 3v3 maps. Our research proposes a comprehensive offline RL framework applicable to this gaming environment and utilizes it to generate diverse datasets. This section provides an introduction to the framework, game modes, datasets, and evaluation protocol employed in this study."}, {"title": "4.1 Framework", "content": "To enhance the usability of our Hokoff, we propose a reliable and comprehensive Offline RL framework that consists of three modules: sampling, training, and evaluation. This framework streamlines the process of sampling new datasets, developing and training baselines, and evaluating their performance. The sampling module provides a simple and unified program for sampling diverse datasets using any pre-trained checkpoints. There are several reasons why our framework excels in sampling. Firstly, diverse datasets at different levels of expertise can be sampled by leveraging Multi-Level Models as described in Sec. 4.1.1. Secondly, our framework employs parallel sampling techniques, ensuring efficient sampling of large and diverse datasets. Based on the training module, we have implemented various offline RL and offline MARL algorithms as baselines. Additionally, we consolidate crucial components and provide user-friendly APIs, facilitating researchers to effortlessly develop novel algorithms or innovative network architectures. The evaluation module enables the assessment of trained models from different algorithms, ensuring fair comparisons. Fig. 2 demonstrates the architecture of our framework and Appendix E provides an example of the APIs."}, {"title": "4.1.1 Multi-Level Models", "content": "To ensure a valid and unbiased comparison of the performance of distinct algorithms, it is crucial to establish appropriate evaluation protocols [6, 12]. One such effective evaluation protocol is the normalized score [6]. However, HoK is a zero-sum adjustable rewards MOBA game. The episode return in the game is heavily influenced by the opponents and game settings, and the objective is to win, which renders the use of return as a performance metric biased. Therefore, normalized score may not fully capture our requirements. Furthermore, similar to our situation, the evaluation protocol for SMAC [31], a competitive game, is based on win rate against a pre-programmed AI. Nonetheless, it is exceedingly challenging to create a built-in AI with human-like performance due to the complexity of MOBA games.\nInspired by prior works of HoK [38], we present Multi-Level Models for sampling and evaluating which contains multiple checkpoints with different level. Specifically, we have extracted several checkpoints from pre-trained dual-clip PPO [45, 44] models with varying levels determined by the outcome of the battle separately for HoK1v1 and HoK3v3. We adopt the win rate against different checkpoints as our evaluation protocols to assess the ability of models. Additionally, these models, with varying levels, can be utilized on both sides to sample diverse battle data. The capabilities of these models surpass those of rule-based AI and match the levels of different human players, thus making these evaluation protocols more suitable for comparing algorithmic performance with human-level performance and facilitating diverse and effortless sampling. The details of these Multi-Level Models are provided in the Appendix F."}, {"title": "4.2 Game Modes", "content": "We have incorporated two game modes from HoK into our study, namely HoK1v1 [38] and HoK3v3. The environment code of HoK3v3 is integrated into the open-source HoK1v1 code, following Apache License V2.0. These game modes differ in the number of agents involved and the underlying map used. Detailed information on each game mode is presented below."}, {"title": "4.2.1 Honor of Kings Arena", "content": "Honor of Kings Arena (HoK Arena or HoK1v1) is a 1v1 game mode where each player attempts to beat the other and destroy its opponent's crystal. Specifically, each player chooses a hero before the game starts and controls it during the whole game. There are a total of 20 heroes available for players to select, each possessing distinct skills that exert diverse effects on the game environment. The observation space is a continuous space consisting of 725 dimensions that contain partial observable information about the hero, opponent, and other game units. The action space is hierarchically structured and discretized, covering all possible actions of the hero in a hierarchical triplet form: (1) which action button to take; (2) who to target; and (3) how to act. Furthermore, the reward is a weighted sum of five categories: farming, kill-death-assist (KDA), damage, pushing, and win-lose. For a full description of this game mode, please refer to the Appendix D.1."}, {"title": "4.2.2 Honor of Kings 3v3 Arena", "content": "To further cater to the demand for Offline MARL, we adopt Honor of Kings 3v3 Arena (HoK3v3) as our experimental platform. HoK3v3 is a MOBA game, where each team comprises three heroes who collaborate to defeat their opponents. The basic rules and win conditions of HoK3v3 are similar to HoK1v1. However, the HoK3v3 map contains additional turrets on the middle road and features a new area called the \"wilderness\", inhabited by diverse monsters. Besides, collaboration is essential in HoK3v3, where players must select different heroes and fulfill distinct roles to work together more efficiently. For instance, one hero might focus on slaying monsters in the wilderness to earn gold and experience, while the other heroes engage in offensive tactics against the enemy heroes and game units. The design philosophies for observation space, action space, and reward are comparable to those used in HoK1v1. However, the level of complexity in HoK3v3 is significantly elevated. We provide a detailed description of the game mode in the Appendix D.2 for reference."}, {"title": "4.2.3 Subtasks", "content": "Both HoK1v1 and HoK3v3 are full MOBA games, featuring multi-camp competitions, which inherently pose challenges and limitations. Consequently, training on these game modes demands extensive training time and computational resources. However, HoK game comprises various sub-objectives, allowing us to decompose the overall game into manageable subtasks. These subtasks can represent diverse scenarios and are suitable for evaluating various algorithms. In this study, we propose two specific noncompetitive subtasks as outlined below. It is worth noting that researchers can readily expand upon our framework to develop additional subtasks.\nDestroy Turret: One of the key sub-objectives in HoK is to destroy the enemy's turrets as quickly as possible, to gain access to the enemy crystal. To train this specific skill, we have devised a subtask called Destroy Turret, which is based on HoK1v1. In this subtask, the focus is solely on destroying the enemy's turret and crystal as quickly as possible, and the enemy hero is removed.\nGain Gold: Gold is a critical resource in HoK that can be used to purchase equipment, which enhances the abilities of the heroes. Inspired by resource collection tasks from previous studies [22], we have designed a subtask called Gain Gold, which is based on HoK3v3, where the new objective is to collect golds in restricted time steps, and the enemy heroes are removed. As a multi-agent setting, it focuses on the cooperation or intra-team competition while avoiding inter-team competition."}, {"title": "4.3 Datasets", "content": "To enhance the practical implications of our datasets, we have incorporated design factors that align with the real-world applications of both HoK and other relevant scenarios.\nMulti-Difficulty\nIntuitively, the level of difficulty in the environment significantly impacts the performance of al-gorithms. However, previous researches only utilized one set of datasets with a uniform level of difficulty in the environment, which is not appropriate for HoK, where the difficulty of the task can be substantially affected by the level of opponents. Therefore, to examine the effects of varying levels of difficulty in the environment, we propose several multi-difficulty datasets with different difficulty levels. Specifically, we develop two sets of datasets: norm and hard, which are categorized based on the opponent's level. Within each level, we propose four datasets according to diverse win rates against the opponent: poor, medium, expert and mixed. To elaborate, the poor/medium/expert dataset is generated by recording the battle trajectories of a relative lower/equal/higher level model compared to the opponent, and the mixed dataset is an equal mixture of the three datasets mentioned above.\nMulti-Task\nAs a MOBA game, HoK features a diverse cast of heroes with distinct roles and skillsets. While the overall objective remains consistent throughout matches, the selection of heroes can significantly alter the nature of the task at hand. Consequently, HoK presents multi-task challenge which requires a single model to handle multiple tasks [47, 24, 46]. However, none of the current works provide uniform datasets for multi-task offline RL. To address this research gap, we propose a series of multi-task datasets based on the multi-task nature of HoK and evaluate the multi-task learning ability of current offline RL and offline MARL algorithms. Specifically, we define a hero pool with several heroes and randomly select heroes from it to sample data. Depending on whether the selected heroes are on the controlled side or the opponent side, we sample either the multi_hero or the multi_oppo dataset. In cases where both sides choose random heroes, we sample the multi_hero_oppo dataset.\nFurthermore, as mentioned in the previous section, different levels of opponents naturally form multiple tasks with varying environmental difficulties. Thus, we propose several level-based multi-task datasets by sampling data with randomly selected opponent levels. According to different difficulty levels, we have proposed two datasets, named norm_multi_level and hard_multi_level.\nGeneralization\nThe unique gameplay mechanics of HoK, characterized by a diverse cast of heroes with distinct roles and skillsets, lend themselves well to multi-task and serve as an ideal testbed for evaluating the generality of models across a range of tasks. Building on the previous work [38] and taking into account the realities of human combat in HoK, we have identified three key challenges for generalization: hero generalization, opponent generalization, and level generalization."}, {"title": "4.3.1 Datasets Details", "content": "Table 1, Table 2 and Table 3presents the details of our proposed datasets. All the datasets are sampled using checkpoints with different levels as introduced in Sec. 4.1.1. Typically, each dataset consists of 1000 trajectories, except for the sub-task datasets, which contain 100 trajectories. The default heroes chosen for both camps are luban with Summoner Spells set to frenzy in HoK1v1 and {zhaoyun},\n{diaochan}, {liyuanfang} with Summoner Spells assigned as {smite}, {purify}, {purify} based on their respective roles in HoK3v3. However, in specific scenarios such as Generalization or Multi-Task settings, we employ a random selection of heroes from a predefined set, multi_hero. For the HoK1v1 mode, the set comprises five heroes, {luban, direnjie, houyi, makeboluo, gongsunli}. In HoK3v3, the set consists six heroes, with two heroes assigned to each role, namely {zhaoyun, zhongwuyan}, {diaochan, zhugeliang}, {liyuanfang, sunshangxiang}. The win rate of the behavior policy is recorded in the column labeled Win_rate for reference. The column labeled Levels denotes the levels of opponents used for evaluation. More details of the datasets are presented in Appendix C."}, {"title": "5 Benchmarking", "content": "Based on our framework, we reproduce various Offline RL and Offline MARL algorithms. Besides, we fully validate and compare these baselines on our datasets. The results are presented in the form of test winning rate. Each algorithm is run for three random seeds, and we report the mean performance with standard deviation. The performance of behaviour policies is presented in Appendix C. Details of the implementations and experimental results can be referenced in Appendix G."}, {"title": "5.1 Baselines", "content": "5.1.1 HoK1v1\nThe Offline RL baseline algorithms we implement are briefly introduced below: BC: Behavior cloning. TD3+BC [7]: One of the state-of-the-art single agent offline algorithm, simply adding the BC term to TD3 [8]. CQL [19]: Conservative Q-Learning conducts conservative value iteration by adding a regularizer to the critic loss. IQL [16]: Implicit Q-Learning leverages upper expectile value function to learn Q-function and extracts policy via advantage-weighted behavioral cloning.\nThe structured action space in HoK is similar to the joint action space in multi-agent settings, which inspires us to resort to the design in MARL methods. We propose a novel baseline algorithm, named QMIX+CQL. Specifically, we import QMIX algorithm from the MARL literature [29] to tackle the structured action space by regarding each head of the action space as a single agent and incorporate CQL regularizer term into local Q-funtion in QMIX for offline learning."}, {"title": "5.1.2 HoK3v3", "content": "The Offline MARL baseline algorithms are briefly introduced below: IND+BC: Behavior cloning with independent learning paradigm. IND+CQL: Adopts an independent learning paradigm for multi-agent settings, using conservative Q-learning [19]. COMM+CQL: Incorporate inter-agent communication based on IND+CQL. IND+ICQ [43]: Implicit Constraint Q-learning with inde-pendent learning paradigm, which only uses insample data for value estimation to alleviate the extrapolation error. MAICQ [43]: Multi-agent version of implicit constraint Q-learning by decom-posed multi-agent joint-policy under implicit constraint with CTDE paradigm. OMAR [28]: Using zeroth-order optimization for better coordination among agents' policies, based on independent CQL."}, {"title": "5.2 Benchmark Results", "content": "We have validated the offline RL and offline MARL baselines on our datasets and aggregated the results in Table 4 and Table 5."}, {"title": "6 Conclusion", "content": "In this paper, taking into account the limitations of existing offline RL datasets about practical applications, we introduce Hokoff, based on Honor of Kings, a well-known MOBA game that offers a high level of complexity for simulating real-world scenarios. We present a comprehensive framework for conducting research in offline RL and release a diverse and extensive collection of datasets, incorporating various levels of difficulty and a range of research factors. Moreover, the chosen tasks for dataset collection not only cater to Offline RL but also serve the purpose of offline MARL. We replicate multiple offline RL and offline MARL algorithms and thoroughly validate these baselines on our datasets. The obtained results highlight the shortcomings of existing Offline RL methods, underscoring the necessity for further research in areas such as challenging task settings, generalization capabilities, and multi-task learning. All components, including the framework, datasets, and baseline implementations, discussed in this paper are fully open-source."}, {"title": "A Author Statement", "content": "The authors of this work would like to state that we bear full responsibility for any potential violation of rights, including copyright infringement or unauthorized use of data. We affirm our commitment to conducting this research in accordance with ethical guidelines and legal requirements.\nWe further guarantee that we will ensure access to the data\u2074 and the framework codes used in this study, making them available to interested researchers for verification and replication purposes. Additionally, we are committed to providing the necessary maintenance and support to ensure the longevity and accessibility of the data. For datasets, we have plans to consistently offer more datasets in the future. These datasets will include larger sizes for larger models, higher levels for expert agents, and novel design factors for other research directions. Updating our datasets is an ongoing and long-term effort, and we welcome contributions from the community. Regarding benchmarks, we will actively monitor the latest state-of-the-art (SOTA) algorithms in the offline RL domain and integrate them into our benchmarks. Additionally, we will develop new algorithms within the benchmarks based on existing datasets and baselines. This ensures that our benchmarks remain up-to-date and reflect the advancements in offline RL research.\nShould any concerns or inquiries arise regarding the contents of this work or the associated data, we encourage readers and fellow researchers to contact us directly. We are dedicated to addressing any issues promptly and transparently to uphold the integrity of our research."}, {"title": "B Limitations and Future Works", "content": "In our future endeavors, we plan to integrate our framework with a large-scale deep reinforcement learning platform namely KaiwuDRL, specifically designed to support Honor of Kings. By doing so, we will gain access to greater computational resources, enabling us to delve deeper into our current research endeavors and expand our investigations."}, {"title": "C Additional Datasets Details", "content": "C.1 HoK1v1\nIn the Generalization category, \"norm_general\" and \"hard_general,\" have their corresponding datasets. For example, to sample the \"norm_general\" dataset, we let the level-1 model fight with level-0, level-2, and level-4 models. However, during the test stage, we assess the generalization capabilities of the trained model by letting it fight against the level-1 model. Details about how we sample the generalization datasets can be referred to Table. 6. The latter four experiments do not require extra datasets. For example, in the \"norm_hero_general\" experiment, we directly use the model trained on the \"norm_medium\" dataset and let the model control different heroes. This is possible because the \"norm_medium\" dataset only contains the fixed default hero \"luban.\" Therefore, we use the model trained on this dataset to test its generalization ability at controlling different heroes.\nIn the Sub-Task: Destroy Turret category presented in Table 3, there are three datasets sampled, each consisting of 100 trajectories. Notably, these datasets lack an opponent hero, making them simpler in nature. This design choice allows for broad applicability, diversity, and cost-effectiveness in research endeavors.\nThe primary objective in the Sub-Task: Destroy Turret scenarios is to efficiently dismantle the enemy's turret and crystal, with the enemy hero removed. Consequently, we adopt the number of game frames elapsed from the start of the game until the crystal's destruction as our evaluation protocol. Equation 1 outlines the scoring methodology employed, following a similar approach as presented in [6]. The score is normalized by two factors: random_frame_length, set to 2880, and expert_frame_length, set to 1812. A higher score is achieved by minimizing the time required to destroy the crystal.\nIn addition, we have generated violin charts to represent the distribution of episode returns in each dataset as shown in Fig. 3. We calculate episode returns using the formula $R = \\sum_{t=1}^{T} \\gamma^{t}r_{t}$, where"}, {"title": "C.2 HoK3v3", "content": "The design of datasets and experiments pertaining to the concept of Generalization closely resembles that of the HoK1v1."}, {"title": "D Environment Details", "content": "D.1 Honor of Kings Arena\nFor a more detailed account of the game settings, please refer to the original paper [38] and its documentation of Honor of Kings Arena. In this context, we will only summarize the critical information that is relevant to the RL research.\n\u2022 Observation Space\nWe have utilized the fundamental set of observations presented in the aforementioned paper [38]. Specifically, the observation space of Honor of Kings Arena consists of a normalized vector with 725 dimensions, which includes five main components: hero_state_common_feature,\nhero_private_feature, creep_feature, turret_feature, and global_feature. The details of the ob-servation vector are demonstrated in Table 7. In the table, Main_camp and Enemy_camp refer to the information of the controlled side and enemy side, respectively. Moreover, the information of invisible units is set to the default value.\n\u2022 Action Space To tackle the complicated control, the Honor of Kings adopt a structured action space. Specifically, illustrated in Fig. 5 the action space is 6 dimensions, consisting of a triplet form, i.e. the action button, the movement or skill offset and the target game unit, which covers all the possible actions of the hero hierarchically: 1) what action button to take, e.g. skill or move.; 2) who to target, e.g., a turret, an enemy hero, or a creep in the troop; 3) how to act, e.g., the discretized direction to move and release skills [38]. Please refer to Table 8 for details of action space in HoK1V1.\n\u2022 Action Mask There are two action masks designed to reduce the complexity of the action space, namely the legal_action_mask and the sub_action_mask. The former is constructed based on the rules of the game in order to exclude illegal actions, while the latter is determined by the selected button to eliminate actions that cannot be executed simultaneously with the chosen button, such as 'skill offset' and 'target unit' are not needed for 'move'."}, {"title": "E Framework APIs", "content": "We provide an example of the APIs in our framework, Listing 1. A comprehensive account of our framework can be found in our readily accessible open-access code repository."}, {"title": "F Evalution Protocols: Multi-Level Models", "content": "Based on the parallel training system named SAIL proposed by previous work [45], we have extracted and published several checkpoints from pre-trained dual-clip PPO [45, 44] models with varying levels determined by the outcome of the battle separately for HoK1v1 and HoK3v3.\nHere, we present tables displaying the win rate of each level model against the model listed di-rectly below it. The win rate is calculated with fixed hero selection, i.e. luban for HoK1v1 and"}, {"title": "G Additional Experimental Details", "content": "G.1 Additional Algorithm Details\n\u2022 Encoder: Due to the complexity of the observation space, it is necessary to utilize a well-designed encoder for effective feature extraction. Taking inspiration from the \"divide and conquer\" approach employed in previous works [45, 44], in each algorithm, we implement a shared encoder network to process features, instead of directly feeding raw observations into the policy or critic network. For further details on the design of the encoder network, please refer to the mentioned papers [45, 44] as well as our code.\n\u2022 BC: Behavior clone with maximum likelihood estimation loss. While, in multi-agent setting, HoK3v3, we adopt shared parameter and independent learning paradigm [34]."}, {"title": "H Additional Discussion", "content": "H.1 The significance of our design factors in the context of offline reinforcement learning\nTask difficulty: Intuitively, the level of difficulty in the environment significantly impacts the performance of algorithms. However, previous researches only utilized one set of datasets with a uniform level of difficulty in the environment. Providing datasets with diverse difficulty can not only more comprehensively evaluate the ability of offline algorithms but also be more suited for real-world tasks like HoK, which are characterized by diverse levels of difficulty.\nMulti-task: Combining offline reinforcement learning with multi-task learning enables efficient use of limited data. Sharing knowledge[1] and representations across tasks enhances data efficiency, leading to more general and robust feature learning. Besides, multi-task learning facilitates knowledge transfer between tasks. Leveraging shared parameters and representations accelerates learning for the target task in offline reinforcement learning, benefiting from related tasks' knowledge.\nGeneralization: Firstly, in offline RL, learning is based on a fixed dataset collected from previous experiences. This dataset might not cover all possible scenarios, so the learned policy needs to generalize well to new, unseen situations to perform effectively. Secondly, real-world environments are often complex and diverse. A policy only limited to the dataset without generalizing would likely fail when facing even slightly different conditions. Generalization ensures the policy's adaptability to various situations in the real-world scenarios.\nHeterogeneous Teammate: Heterogeneous teammates are a crucial research direction in Multi-Agent Reinforcement Learning (MARL). In practical scenarios such as HoK or other multi-agent systems, players typically possess varying capacities. Consequently, the datasets collected from real-world scenarios consist of heterogeneous teammate data, necessitating the need for corresponding research in the offline MARL domain.\nH.2 From the perspective of the Honor of Kings game, why offline reinforcement learning is necessary and what potential limitations exist when compared to online reinforcement learning?\nTraining agents for the Honor of Kings game using offline reinforcement learning (RL) offers several advantages, including reduced training time, lower computation resource requirements, and better utilization of existing data resources. We compare the computational and time costs of online RL and offline RL in Table 18. It is evident that training an online agent from scratch to reach specific levels (level_5 for HoK1v1 and level_7 for HoK3v3) requires thousands of CPU cores and dozens of hours. On the other hand, training offline agents to reach same levels only requires a few CPUs and a shorter training time using pre-collected datasets. Additionally, there is a wealth of previously collected battle data that can be used for training offline RL agents. However, compared to online RL, it is important to note that offline RL in the HoK game heavily relies on large amounts of high-level battle data to train expert-level agents, which may be a potential limitation."}]}