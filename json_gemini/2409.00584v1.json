{"title": "FastBO: Fast HPO and NAS with Adaptive Fidelity Identification", "authors": ["Jiantong Jiang", "Ajmal Mian"], "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are powerful in attaining state-of-the-art machine learning models, with Bayesian optimization (BO) standing out as a mainstream method. Extending BO into the multi-fidelity setting has been an emerging research topic, but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge, we propose a multi-fidelity BO method named FastBO, which adaptively decides the fidelity for each configuration and efficiently offers strong performance. The advantages are achieved based on the novel concepts of efficient point and saturation point for each configuration. We also show that our adaptive fidelity identification strategy provides a way to extend any single-fidelity method to the multi-fidelity setting, highlighting its generality and applicability.", "sections": [{"title": "1 Introduction", "content": "HPO [9] and NAS [7] aim to find the hyperparameter configuration or architecture A* that minimizes f(x), the performance obtained by configuration \u03bb. BO [2,10,30] is an effective model-based method for HPO and NAS. It maintains a surrogate model of the performance based on past evaluations of configurations, which guides the choice of promising configurations to evaluate. Recent studies on BO have explored expert priors [11,20,26,29], derivative information [1,27,35], and enhancing the interpretability [5, 36-39] of HPO and NAS [3, 24, 25].\nHowever, standard BO requires full evaluations of configurations, which incurs significant costs, especially considering the escalating model evaluation overhead. Despite efforts to accelerate model evaluation [13,15-17], smart strategies are required to widely adopt HPO and NAS. Thus, multi-fidelity methods have been proposed [4, 12, 21, 22], where the fidelities mean the performance levels obtained under various resource levels. They follow the idea of successive halving (SHA) [12]: initially, they evaluate many random configurations using few resources; then, based on the low-fidelity performances, only the well-performing ones successively continue to be evaluated with increasing resources.\nFollow-up studies [8, 19, 23, 28, 33] propose model-based multi-fidelity methods, replacing random sampling with more informed models to improve sample"}, {"title": "2 Key Idea of FastBO", "content": "We propose a multi-fidelity extension of BO, namely FastBO, which tackles the challenge of deciding the appropriate fidelity for each configuration to fit the surrogate model. Here, we first propose the key concepts of efficient point and saturation point, which are crucial in the optimization process. Then, we briefly describe the process of FastBO and highlight its generality."}, {"title": "2.1 Efficient Point and Saturation Point", "content": "We first formally define the efficient point as follows.\nDefinition 1 (Efficient point). For a given learning curve Ci(r) of hyper- parameter configuration or architecture Ai, where r represents the resource level (also referred to as fidelity), the efficient point ei of Xi is defined as: e\u2081 = min{r | Ci(r) - Ci (2r) < d\u2081}, where d\u2081 is a predefined small threshold.\nThe semantic of Definition 1 is that starting from the efficient point onwards, when the resources are doubled, the performance improvement falls below a small threshold. Consequently, this point signifies a fidelity of performance achieved with comparably efficient resource usage. Thus, we make the following remark.\nRemark 1. The efficient points of the configurations can serve as their ap- propriate fidelities used for fitting the surrogate model. This is due to their (i) optimal resource-to-performance balance, (ii) ability to capture valuable learning curve trends, and (iii) customization for different hyperparameter configurations.\nWe elaborate on the reasons as follows. Firstly, efficient points balance the trade- off between computational cost and result quality. Beyond the efficient point, allocating additional resources becomes less efficient. Secondly, efficient points capture valuable behaviors within the learning curves, enabling more informed decision-making. Thirdly, the ability to customize the fidelity for each specific configuration is an advantage. This adaptive approach is more reasonable than previous studies that use a fixed fidelity for all the configurations."}, {"title": "2.2 FastBO Process and Generalization", "content": "With the two crucial points, we show the main process of FastBO in Fig. 1. Each configuration Ai first enters a warm-up stage to get its early observation set. Some configurations are terminated here if they are detected consecutive performance deterioration. Then, FastBO estimates the learning curve of A\u00bf from its observation set. Thus, the efficient point and saturation points are adaptively extracted. After that, i continues to be evaluated to its efficient point; the result is used to update the surrogate model. Finally, the post-processing stage let a small set of promising configurations resume evaluating to their saturation points, and the optimal configurations can be obtained.\nGeneralizing FastBO to single-fidelity methods. The inefficiency of single- fidelity methods like BO stems from their reliance on expensive final fidelity evaluations. Notably, low-fidelity evaluations provide informative insights but are computationally cheaper. Therefore, we can extend single-fidelity methods to the multi-fidelity setting by acquiring the low-fidelity performance for each configuration to fit the surrogate model. To do this, we need to determine the fidelity used to fit the surrogate model. FastBO adaptively determines this fidelity for each configuration by identifying its efficient point. While this adaptive identification strategy is described in the context of model-based methods, it can"}, {"title": "4 Conclusion and Discussion", "content": "We propose FastBO, a model-based multi-fidelity HPO method, which excels in adaptively identifying the fidelity for each configuration to fit the surrogate model and efficiently providing high-quality performance. The proposed adaptive fidelity identification strategy also provides a simple way to extend any single- fidelity method to the multi-fidelity setting. While this paper provides a strong foundation on HPO and NAS, we see challenges that demand future improve- ments. Future work could refine and expand Fast-BO to larger search spaces and distributed computing systems to improve its applicability and scalablity."}]}