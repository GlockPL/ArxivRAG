{"title": "CLINICAL INSPIRED MRI LESION SEGMENTATION", "authors": ["Lijun Yan", "Churan Wang", "Fangwei Zhong", "Yizhou Wang"], "abstract": "Magnetic resonance imaging (MRI) is a potent diagnostic\ntool for detecting pathological tissues in various diseases.\nDifferent MRI sequences have different contrast mechanisms\nand sensitivities for different types of lesions, which pose\nchallenges to accurate and consistent lesion segmentation.\nIn clinical practice, radiologists commonly use the sub-\nsequence feature, i.e. the difference between post contrast-\nenhanced T1-weighted (post) and pre-contrast-enhanced (pre)\nsequences, to locate lesions. Inspired by this, we propose a\nresidual fusion method to learn subsequence representation\nfor MRI lesion segmentation. Specifically, we iteratively\nand adaptively fuse features from pre- and post-contrast se-\nquences at multiple resolutions, using dynamic weights to\nachieve optimal fusion and address diverse lesion enhance-\nment patterns. Our method achieves state-of-the-art perfor-\nmances on BraTS2023 dataset for brain tumor segmentation\nand our in-house breast MRI dataset for breast lesion segmen-\ntation. Our method is clinically inspired and has the potential\nto facilitate lesion segmentation in various applications.", "sections": [{"title": "1. INTRODUCTION", "content": "Magnetic Resonance Imaging (MRI), a non-invasive and\nradiation-free 3D medical imaging technique, plays a cru-\ncial role in screening [1], diagnosing, and treating various\ndiseases due to its high sensitivity and detailed anatomical vi-\nsualization. The initial MRI scanning involves T1-weighted\nsequences, known as pre-contrast sequences. Then, a contrast\nagent, typically Gadolinium (Gd), is administered, followed\nby multiple scans of T1 sequences, creating post-contrast\nsequences [2]. Post-contrast sequences highlight structures\nlike blood vessels, inflammation, and lesions with elevated\nsignal intensity. Comparing pre-contrast and post-contrast\nsequences enables doctors to identify potential lesions.\nDistinct enhancement patterns are observed in different\ntissues [3, 4]. Tissues with abundant blood supply, such as\ntumors and inflammation, often exhibit high signal inten-\nsity in enhanced images. Conversely, structures like cysts\nand calcification show no enhancement. Furthermore, cer-\ntain healthy tissues may undergo enhancement; for instance,\nnormal breast fibroglandular tissue may appear highly en-\nhanced in post-contrast sequence due to its rich blood supply\n(Fig. 1 illustrates these scenarios). Given the diversity in\nenhancement patterns, clinical diagnosis requires radiologists\nto analyze both pre-contrast and post-contrast sequences,\nbecause depending solely on post-contrast sequences for di-\nagnosis is one-sided and error-prone and a comprehensive\ndiagnosis must integrating pre-contrast information.\nAn intuitive idea to achieve the fusion of pre-contrast\nand post-contrast MRI sequences is to directly utilize pre-\ncontrast and post-contrast images and design attention-based\nmechanisms[5, 6, 7, 8, 9, 10, 11]. Meanwhile, some methods\nfocus on learning the enhancement process and extracting\ncorresponding lesion information [12, 13]. However, di-\nrect fusion approaches may not fully utilize the distinct and\ncomplementary information each sequence offers. More-\nover, they may not adequately address the variations in image\ncharacteristics, such as noise levels and contrast differences,\npotentially impacting the accuracy of lesion characterization.\nTo solve this problem, we focus on how to naturally ex-\nplore the relationship between pre-contrast and post-contrast\nimages. The residual concept aligns more closely with the\ndiagnostic process, where radiologists often look for changes\nbetween sequences to make informed decisions. Inspired by\nresidual learning [14] and counterfactual generation[15, 16],\nwe propose a residual learning framework. Our approach fo-\ncuses on learning the differences or 'residuals' between pre-\ncontrast and post-contrast images. We involve integrating\npre-contrast images into the segmentation encoder at each en-\ncoding step, enabling the network to focus on learning en-\nhancement patterns. By doing so, our network can more ef-\nfectively highlight the changes due to contrast enhancement,\nwhich is critical for accurate lesion detection and segmenta-\ntion. Hence, our method not only improves the integration of\nMRI sequences but also enhances the clinical relevance of the\nsegmentation results. In general, our work contributes in the\nfollowing ways:\n\u2022 We introduce a novel neural network architecture that\nseamlessly incorporates pre-contrast information into\npost-contrast images, contributing to enhanced lesion\ndiagnosis and segmentation, with only a minimal increase\nin the parameters of the backbone network.\n\u2022 Aligning with the diagnostic practices of radiologists, our\nmethod leverages more comprehensive information, en-\nhancing lesion diagnosis and segmentation efficacy.\n\u2022 Our model achieves state-of-the-art performance in brain\ntumor and breast lesion segmentation tasks."}, {"title": "2. METHOD", "content": "Our study proposes a model integrating pre- and post-contrast\nT1-weighted images to enhance tumor analysis, with the net-\nwork architecture shown in Fig. 2. We extract features from\nboth images using shared neural networks in encoding blocks,\ncapturing local and global features via convolutional architec-\ntures. Post-T1 images, with enhanced brightness for lesions,\nform the main branch, while pre-T1 images serve as an auxil-\niary branch. Additionally, a multi-scale feature fusion mech-\nanism is implemented to learn residuals of image features at\nvarious scales, enabling the model to grasp intricate details\nacross scales and improve lesion exploration performance."}, {"title": "2.1. Residual Module & Multi-scale Feature Aggregation", "content": "Derived from the ResNet architecture [14], we incorporate\nskip connections into the encoding process, departing from\nthe traditional Res-UNet [17] approach where input features\ndirectly link to post-convolutional features. Our innovation\ninvolves residual learning of the main branch and the auxiliary\nbranch, fostering a feature fusion step between the main and\nauxiliary branches.\nInspired by residual learning in ResNet, we propose a\nmethod to learn the residual between two branches. Fig. 2\nillustrates the residual module in ResNet and the modified\nresidual module we derived. Fig. 2(a) shows the original\nresidual module from the ResNet paper, which improves\ntraining efficiency and stability by learning the difference\n(i.e. residual F(x)) between the input x and the desired output\nH(x) Considering that the MRI images before and after en-\nhancement theoretically only have significant differences in\nthe lesion areas and have very similar structures, we modify\nthe residual module, which is shown in Fig. 2(b). We set the\ndesired output as H(x) = E(Xmain) + xaux, which enables\nthe network to more efficiently learn lesion-related features\nthrough the residuals between the two branches.\nThroughout the multiple down-sampling stages of the\nencoder-decoder architecture, we repeatedly use the modified\nresidual module mentioned above, ensuring the integration\nof both the main and auxiliary branches across different\nresolutions. This approach aims to enhance the network's ca-\npacity to capture intricate details and contextual information\nat diverse levels. By linking feature maps from the auxil-\niary branch to the main branch, our model can leverage both\nthe detailed information learned by the main branch and the\ncontrasting information provided by the auxiliary branch.\nThis feature fusion strategy not only maintains the net-\nwork's high-resolution information capture capability but also\nenriches feature representations with complementary details\nfrom both branches. The resulting network architecture is\nwell-suited for medical image analysis tasks, particularly in\nscenarios where accurate diagnosis and segmentation require\nthe incorporation of intricate details and subtle contrasts."}, {"title": "2.2. Dynamic Learning of Auxiliary Branch Weights", "content": "The fusion of features from both branches involves a direct\naddition operation. However, recognizing the need for dif-\nferent weights for features of the two branches, we intro-\nduce a block designed to dynamically learn the weights of the\nauxiliary branch before the addition operation. Specifically,\nwe employ a 1*1*1 convolutional layer to dynamically learn\nthese weights. The schematic of the final residual module is\nshown in Fig. 2(c). This approach introduces a minimal num-\nber of parameters, ensuring efficiency, while also allowing for\nadaptable weight adjustments for the auxiliary branch."}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Dataset", "content": "In-house Breast Dataset: The breast MRI dataset encom-\npasses a total of 515 cases. The dataset comprises images\nacquired from Siemens, GE, and Philips MRI machines,\nfeaturing both pre-contrast and post-contrast T1-weighted se-"}, {"title": "3.2. Experimental Results", "content": "Our proposed module can adapt to various encoders and de-\ncoders. To maintain comparability with ResUNet, we used\nencoders from ResUnet architecture in our experiments, con-\nsistent with that in [17].\nTo assess the effectiveness of our proposed method, we\nconduct comparative experiments with three recent papers:\nA2FNet [6], TSF-Seq2Seq [7] and LiTS [8]. We also con-\nduct experiments solely on post-contrast images, denoted as\nRes-UNet-Single. We utilize Dice Coefficient (DSC) and re-\ncall (at pixel level) as evaluation metrics\nIn-house Breast Dataset: Experimental results on our in-\nhouse breast MRI dataset are presented in Tab. 1. Our pro-\nposed approach achieves DSC of 82.55%, surpassing the\nperformance of all compared methodologies. Additionally,\nthe Recall for our method stands at 82.05%, demonstrating\nits effectiveness in accurately capturing relevant information\nwithin the breast MRI dataset. The results underscore the\nsuperior performance of our proposed method in comparison\nto state-of-the-art techniques, highlighting its potential for\nrobust and accurate segmentation in breast MRI applications.\nFig. 3 shows the segmentation results of different models\non breast MRI, each row represents a patient with a lesion\nsegmented out by radiologists. The lesion in the first row is a\nvery small, spot-like enhancement that some models tend to\noverlook, our model maintains a high detection rate for small\nlesions. The lesion in the second patient has an irregular shape\nwith additional spot-like local enhancements. Only our model\ncan fully detect the irregular contour of the entire lesion.\nWe also select two visualization results to further demon-\nstrate the function of the aux-branch, which is shown in Fig. 4.\nThe lesion from patient (a) exhibits significant heterogeneous\nenhancement after contrast, with the central area being a low-\nenhancement zone. The second column shows the segmenta-\ntion results of ResUNet and our proposed model, respectively,\nwhich shows that incorporating the pre-contrast phase signif-"}, {"title": "BraTS2023 Glioma Dataset:", "content": "Tab. 2 shows DSC values\nfor Whole Tumor (WT), Tumor Core (TC), and Enhancing\n(EN) regions on the BraTS2023 Glioma dataset. Our method\nachieves the highest DSCs: 83.87% for WT, 89.17% for TC,\nand 86.28% for EN, with an overall mean of 86.44%. This\nconsistent superiority shows the effectiveness of our approach\nand its potential for accurate glioma segmentation in clinical\nsettings. Fig. 5 shows brain MRI segmentation results from\ndifferent models. Accurate boundary delineation between\nbrain tissue and lesion areas, particularly the challenging tu-\nmor edema region (blue in the figure), is crucial. Our model\noutperforms others by providing clearer edema boundaries\nwhile maintaining high core tumor segmentation accuracy."}, {"title": "4. CONCLUSION", "content": "Inspired by the clinical diagnostic practices of radiologists,\nwe introduce a new method that combines data from both\npre- and post-contrast MRI scans. This innovative approach\nis characterized by its multi-scale fusion of images, which is\nadeptly modulated by dynamically adjusted fusion weights.\nThis method is pivotal as it enriches the model's ability to\ndelineate lesion boundaries with heightened precision and to\ndiscern subtle abnormalities with greater clarity. Despite not\ncausing a substantial rise in the parameter count, experiments\nconducted on both our in-house breast and the BraTS2023\nGlioma dataset show the superior performance of this net-\nwork in segmentation tasks. Moreover, our method stands out\nas a clinically inspired solution, promising to elevate the stan-\ndards of MRI lesion segmentation in line with the real-world\ndemands of medical diagnostics."}]}