{"title": "On the Statistical Capacity of Deep Generative Models", "authors": ["Edric Tam", "David B. Dunson"], "abstract": "Deep generative models are routinely used in generating samples from complex, high-dimensional distributions. Despite their apparent successes, their statistical properties are not well understood. A common assumption is that with enough training data and sufficiently large neural networks, deep generative model samples will have arbitrarily small errors in sampling from any continuous target distribution. We set up a unifying framework that debunks this belief. We demonstrate that broad classes of deep generative models, including variational autoencoders and generative adversarial networks, are not universal generators. Under the predominant case of Gaussian latent variables, these models can only generate concentrated samples that exhibit light tails. Using tools from concentration of measure and convex geometry, we give analogous results for more general log-concave and strongly log-concave latent variable distributions. We extend our results to diffusion models via a reduction argument. We use the Gromov-Levy inequality to give similar guarantees when the latent variables lie on manifolds with positive Ricci curvature. These results shed light on the limited capacity of common deep generative models to handle heavy tails. We illustrate the empirical relevance of our work with simulations and financial data.", "sections": [{"title": "INTRODUCTION", "content": "A fundamental task in statistics is to generate samples x from a target probability distribution \u03c0. When \u03c0 has an explicitly specified density up to normalization, often the case in Bayesian modeling, Markov chain Monte Carlo samplers are the gold standard. However, in modern applications involving complex data such as images and natural language, \u03c0 is often too complicated and high-dimensional to be explicitly stated. Instead, the target distribution \u03c0 is implicitly specified via a collection of independent training samples. Learning to sample from these implicit targets is known as \u201cgenerative modeling\u201d in the machine learning literature.\nDeep generative models are related to latent variable models in the probabilistic and Bayesian modeling literature, with deep neural networks used in defining mappings from latent variables to observed data. The core idea is to transform latent variables z with a function f so that the law of f(z) approximates the target \u03c0. Deep neural networks f, given their immense flexibility, are natural candidates for modeling f. A variety of loss functions have been proposed for fitting f, with motivations ranging from adversarial considerations (Goodfellow et al., 2020) to variational inference (Kingma & Welling, 2014). To generate approximate samples from \u03c0, one simply applies the fitted f to realizations of z. One can further consider sequentially transforming z using multiple neural networks, as in diffusion models (Ho et al., 2020).\nThe vast majority of existing work in the deep generative modeling literature impose Gaussian distributions on the latent variables z (Rezende et al., 2014; Kingma & Welling, 2014). Owing to the status of neural networks as universal function approximators (Cybenko, 1989; Barron, 1993; Hornik, 1991), there is a folklore that deep generative models enjoy similarly rich expressivity (Doersch, 2016; Kingma et al., 2019). It is widely assumed that, given enough training data and sufficiently large neural networks, such transformation-based deep generative models will have arbitrarily small approximation error for any continuous target distribution, even when the latent variable distributions are chosen to be simple (Hu et al., 2018).\nOur work here debunks this belief. We start by showing that for Gaussian latent variables z, the law of $f(z) \u2013 E{f(z)}$ is light-tailed. This demonstrates that deep generative models such as generative adversarial networks and variational autoencoders are not universal generators in practice. This also shows that the common practice of defaulting to Gaussian latent variables is not always appropriate. We generalize in several directions. First, we show analogous results for log-concave and strongly log-concave latent variables z. Second, we give similar guarantees when the latent variables z lie on a manifold with positive Ricci curvature. Third, we extend our results to denoising diffusion models by using a reduction argument. Many of our results are dimension-free, in the sense that the bounds obtained do not explicitly depend on the dimension of the latent variables. None of our results resort to asymptotic approximations.\nOur work shows that a broad class of common deep generative models are not universal generators. Since the center of the learned distribution of f(z) remains completely flexible, it is unsurprising that a typical sample from such deep generative models empirically resembles typical samples from the target distribution. However, due to the light-tailedness of the law of $f(z) \u2013 E{f(z)}$, when the target distribution is heavy-tailed, samples from such deep generative models will tend to underestimate the uncertainty and diversity of the true distribution. This has substantial implications for practitioners. For one, deep generative models are commonly adopted in anomaly detection (Schlegl et al., 2017) and finance (Eckerli & Osterrieder, 2021), applications where tails play a crucial role. For another, there is an emerging interest in the Bayesian literature in leveraging various generative models for posterior sampling (Polson & Sokolov, 2023; Winter et al., 2024), a setting in which underestimating uncertainty can lead to incorrect downstream inference."}, {"title": "1.1. Related work", "content": "There is a broad literature on deep generative models. See Bond-Taylor et al. (2021) for a review. There is a common impression that such models are extremely expressive (Kingma et al., 2019; Doersch, 2016; Hu et al., 2018). There is a literature (Lu & Lu, 2020; Yang et al., 2022) that offers universal approximation theorems for deep generative models under moment conditions using metrics such as the Wasserstein distance. Research on the theoretical limitations of deep generative models is relatively scarce. It has been observed that variational autoencoders and generative adversarial networks have difficulty modeling multi-modal distributions (Salmona et al., 2022). Wiese et al. (2019) studies the limitations of certain deep generative models from a tail asymptotics perspective. Oriol & Miot (2021) gives limitations of Gaussian generative adversarial networks when the output is one-dimensional."}, {"title": "2. PRELIMINARIES", "content": ""}, {"title": "2.1. Deep neural networks", "content": "We consider feed-forward neural networks of depth L. Given input $z \\in \\mathbb{R}^d$, define the network via the composition $f(z) = h_L[h_{L-1}\\{ . . . h_1(z) . . . \\}]$, where $h_l(z) = \\sigma_l(W_lz + b_l)$, $\\sigma_l$ is a non-linear activation function operating elementwise on the lth layer, and $W_l$ and $b_l$ are respectively the weight matrix and bias vector corresponding to the lth layer. This setup allows the dimensions of $W_l$, as well as the choice of activation functions, to vary between layers. Let width($W_l$) denote the maximum of the number of rows and columns of $W_l$, and $\\max_l width(W_l)$ denote the width of the neural network. For additional information, see the excellent review by Fan et al. (2021).\nWe use d to denote latent variable dimension and p to denote output dimension. Let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$ denote the trained neural network function used for sample generation. The function f is Lipschitz if $\\sup_{z,y\\in\\mathbb{R}^d} ||f(x) - f(y)||_2/||x - y||_2 \\leq L$ for some L > 0, where $|| . ||_2$ denotes the Euclidean norm. Letting S denote the set of all Lipschitz activation functions, S includes common choices in practice (Virmaux & Scaman, 2018), including the rectified linear unit function ReLU(x) = max(0, x), the logistic function $\\sigma_{logistic}(x) = \\{1 + exp(-x)\\} ^{-1}$, the hyperbolic tangent function tanh(x), and beyond. We define finite feed forward neural networks below.\nDEFINITION 1 (FINITE FEED-FORWARD NEURAL NETWORKS). A feed-forward neural network is finite if (1) the depth L is finite, (2) the width $\\max_{l=1}^L width(W_l)$ is finite, (3) all entries in the matrices $W_l$ and vectors $b_l$ are finite, and (4) all activation functions $\\sigma_l$ are members of S. We denote the set of all finite feed-forward neural networks as F.\nThis notion of finity encompasses most feed-forward neural networks used in practice.\nPROPOSITION 1. Finite feed-forward neural networks are Lipschitz with respect to the Euclidean norm.\nRemark 1. Many popular neural network operations, such as dropout, pooling and batch normalization, have finite Lipschitz constants. Our results can be extended to a generalized function class that incorporates a finite number of these Lipschitz operations."}, {"title": "2.2. Deep generative modeling", "content": "Consider the following latent variable model.\n$X_i \\sim f(z_i) + \\epsilon_i, z_i \\sim P,  \\epsilon_i \\sim Q$ where xi is the observed data for sample i, which is equal to a function f of a latent variable zi plus an additive noise \\epsiloni. The latent variable distribution P and noise distribution Q are often chosen to be multivariate Gaussian with diagonal covariance. Linear f leads to classical Gaussian factor models, while using a deep neural net for f provides the foundation of broad classes of deep generative models. In the next section, we give general theoretical results on the law of f(z) that hold for any $f \\in F$."}, {"title": "3. ISOPERIMETRY AND CONCENTRATION OF DEEP GENERATIVE MODELS", "content": "The notion of concentration of measure is central to the development below. Related definitions of sub-Gaussian and sub-exponential random vectors are reviewed in the Supplementary Materials. To ease notation, throughout the paper we follow the convention where we use C, c > 0 to denote absolute constants whose values are unspecified. We use subscripts like $C_p$ to highlight any dependencies. After training, the fitted neural network $f \\in F$ at the generation phase is a fixed function with a finite Lipschitz constant. The output dimension p and latent dimension d are fixed constants here. We do not make any attempts to optimize constant factors in any inequalities below. We use the notation $S^{p-1}$ to denote the unit (p \u2013 1)-sphere in $\\mathbb{R}^p$.\nWe start with a result on deep generative models with Gaussian latent variables, the predominant case in the literature.\nTHEOREM 1 (DEEP GENERATIVE MODELS WITH GAUSSIAN LATENT VARIABLES). Let z be a Gaussian random vector with mean \u00b5 and covariance \u03a3. Let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$ be any finite neural network function with Lipschitz constant L. Then for any unit vector $u \\in S^{p\u22121}$, $Pr(|u^T[f(z) \u2013 E\\{f(z)\\}]| \\geq t) \\leq 2 exp(-t^2/C_u^2)$ where $C_u = C\\sqrt{p}L||\\Sigma^{1/2}||$ and C > 0.\nThe above theorem, which relies on the well-known Gaussian isoperimetric inequality, implies that $f(z) \u2013 E\\{f(z)\\}$ is sub-Gaussian. Since sub-Gaussian distributions have light tails, they are inappropriate for modeling heavy-tailed distributions. Since this result can be applied to any member of F, this limitation cannot be overcome by increasing training data or enlarging the neural network. Since we are chiefly interested in the tail behavior of the generated samples, rather than the location of the mean, this centred quantity is appropriate for our context. The mean $E\\{f(z)\\}$ in the above result can be replaced by the median with only changes to universal constants (Wainwright, 2019).\nWhile the Gaussian latent variables case is the most prevalent, a variety of alternative easy-to-sample latent variable distributions have been considered. We give analogous theoretical results on log-concave latent variables. Log-concave distributions are a broad family that include the important case of uniform distributions on any convex body, such as the hypercube and hyperball.\nTHEOREM 2 (DEEP GENERATIVE MODELS WITH LOG-CONCAVE LATENT VARIABLES). Let $z \\in \\mathbb{R}^d$ be a log-concave random vector with covariance \u03a3. Let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$ be any finite neural network with Lipschitz constant L. Then for any $u \\in S^{p\u22121}$ and t > 0, we have\n$Pr(|u^T[f(z) \u2013 E\\{f(z)\\}]| \\geq t) \\leq 2 exp(-t/C_p)$ for $C_p = C\\sqrt{p}L||\\Sigma^{1/2}||/\\Psi_z$, where $\\Psi_z$ is the Cheeger's constant of the density of z and C > 0.\nThe above theorem implies that $|f(z) \u2013 E\\{f(z)\\}|$ is a sub-exponential random vector, which means it is also light-tailed, albeit less so than a sub-Gaussian. Theorem 2 leverages tools from high-dimensional geometry (Lee & Vempala, 2018; Gromov & Milman, 1983). Notably, recent progress in the area (Chen, 2021; Jambulapati et al., 2022; Klartag & Lehec, 2022) demonstrates that the Cheeger's constant involved in the above upper bound can be replaced by a poly-logarithmic factor of the input dimension.\nFurther variations, such as exponential-tilted Gaussian latent variables, have been proposed in the literature for applications such as out-of-distribution detection (Floto et al., 2023). These kinds of latent variables are strongly log-concave, for which sub-Gaussian bounds are available.\nTHEOREM 3 (STRONGLY LOG-CONCAVE LIPSCHITZ CONCENTRATION). Let $z_0$ be a \u03b3-strongly log-concave random vector with covariance \u03a3. Let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$ be any finite neural network with Lipschitz constant L. Then for any unit vector $u \\in S^{p\u22121}$ we have $Pr(|u^T [f(z) \u2013 E\\{f(z)\\}]| \\geq t] \\leq 2 exp(-t^2/C^2_{\\gamma})$ where $C^2_{\\gamma} = C^2\\sqrt{p}L^2||\\Sigma||/\\gamma$ and C > 0.\nThis result again shows that $f(z) \u2013 E\\{f(z)\\}$ is a sub-Gaussian random vector. The above bounds for Gaussian and strongly log-concave latent variables do not explicitly depend on latent variable dimension d. This phenomenon is known as dimension-free concentration in the probability literature. In the case of log-concave latent variables, the bound's dependence on d is poly-logarithmic. If a mathematical conjecture known as the Kannan-Lov\u00e1sz-Simonovits conjecture is true (Lee & Vempala, 2018), even this small poly-logarithmic dependence on d can be removed."}, {"title": "3.1. Manifold Setting", "content": "Thus far, we have considered latent random variables that lie in Euclidean space. There are multiple other approaches that place latent variables on non-Euclidean manifolds, such as hyperspheres (Davidson et al., 2018). Hence, we consider related results for deep generative models under the manifold setting. A particular property on manifolds that yields strong concentration behavior is positive Ricci curvature, with the canonical example being the hypersphere. We use the Gromov-Levy inequality from geometry to study the behavior of deep generative models when the latent variables come from such manifolds.\nWe detail the main setting here. Let (M, g) be a compact, connected $d_{int}$-dimensional Riemannian manifold with $d_{int} \\geq 2$. Let \u03bb denote the infimum of the Ricci curvature tensor evaluated over any pair of unit tangent vectors associated with any point on the manifold and assume \u03bb > 0. Letting v be the corresponding normalized volume element, assume $z \\sim v$. We consider the setting where M is embedded in an ambient Euclidean space $\\mathbb{R}^{d_{ext}}$. We assume that the embedding map $ \\phi : M \\rightarrow \\mathbb{R}^{d_{ext}}$ is Lipschitz with respect to the geodesic distance $D_{geo}$, so that $sup_{a,b \\in M} ||\\phi(a) \u2013 \\phi(b)||_2 \\leq L D_{geo}(a, b)$. This can be interpreted as a condition that controls the distortion of the geodesic distance structure when performing the embedding.\nTo concretely illustrate the above setting, consider the (d \u2013 1)-hypersphere $rS^{d\u22121}$ of radius r naturally embedded in $\\mathbb{R}^d$. It is a compact and connected manifold with $d_{int} = d \u2212 1, d_{ext} = d$ and constant positive Ricci scalar curvature. $z \\sim v$ implies z is uniformly distributed on the hypersphere. The Lipschitz property is verified by observing that for any $x, y \\in rS^{d\u22121} \\subset \\mathbb{R}^d$, the geodesic distance $D_{geo}(x, y) = r \\arccos(x^T y/r^2)$ upper bounds the Euclidean distance $||x - y||_2$. We now state our result.\nTHEOREM 4 (CONCENTRATION OF LATENT VARIABLES ON MANIFOLD). Let (M, g), v, z, \u03bb be defined as above. Let the embedding $ \\phi : M \\rightarrow \\mathbb{R}^{d_{ext}}$ be a $L_{\\phi}$-Lipschitz function with respect to the geodesic distance, and let $f : \\mathbb{R}^{d_{ext}} \\rightarrow \\mathbb{R}^p$ be any finite neural network function with Lipschitz constant L. Then for any $u \\in S^{p\u22121}$,\n$Pr(|u^T [f \\circ \\phi (z) \u2013 E\\{f \\circ \\phi (z)\\}]| \\geq t) \\leq 2 exp(-t^2/C_u^2)$ where $C_u^2 = C^2\\sqrt{p}L^2L_{\\phi}^2/\\lambda$ and C > 0 is an absolute constant.\nThe above result shows that the random vector $f \\circ \\phi (z) \u2013 E\\{f \\circ \\phi (z)\\}$ is sub-Gaussian."}, {"title": "4. DIFFUSION MODELS", "content": "Diffusion models are important classes of deep generative models, with the denoising diffusion probabilistic model (Ho et al., 2020) being one prominent example. Such models operate by modeling data generation via a diffusion process $(X_\\tau)^{\\mathcal{T}}_{\\tau=0}$, with $X_\\mathcal{T} \\in \\mathbb{R}^p$. One infers a reverse sampling process $X_\\mathcal{T}, X_{\\mathcal{T}-1}, . . ., X_0$ starting with a Gaussian latent variable $z = X_\\mathcal{T} \\sim N_p(0, I)$ and performing a sequence of neural network transformations to generate the sample $X_0$ by iterating the following update step (Ho et al., 2020) from $\\tau = \\mathcal{T}, . . ., 1$:\n$X_{\\tau-1} = (1/\\sqrt{\\alpha_\\tau})\\{X_{\\tau} \u2013 (1 \u2212 \\alpha_\\tau)/\\sqrt{1 \u2013 \\bar{\\alpha}_\\tau}f(X_\\tau, \\tau)\\} + \\epsilon_\\tau \\sigma_\\tau \\mathbb{1}_{\\tau>1},$(1)\nwhere $(\\alpha_\\tau)$, $(\\bar{\\alpha}_\\tau)$ and $(\\sigma_\\tau)$ are fixed sequences, $\\epsilon_1,..., \\epsilon_{\\mathcal{T}}$ are independent $N_p(0, I)$ random vectors, $\\mathbb{1}_{\\tau>1}$ is an indicator function that prevents the sampler from adding noise on the last step, and f is a finite neural network that takes in $X_\\tau$ and the time step \u03c4 as input.\nWe develop a reduction argument, detailed in section C in the supplementary materials, that allows us to treat the iterative transformations performed above equivalently as a single Lipschitz transformation on an augmented Gaussian random vector. This yields the following result for diffusion models with Gaussian latent variables.\nTHEOREM 5 (DIFFUSION MODELS WITH GAUSSIAN LATENT VARIABLES). Let $X_0 \\in \\mathbb{R}^p$ be a sample generated from a denoising diffusion probabilistic model using the iterative procedure in (1). Then for any unit vector $u \\in S^{p\u22121}$, there exists $L_1,..., L_{\\mathcal{T}} > 0$ such that $Pr[|u^T \\{X_0 \u2013 E(X_0)\\}| > t] \\leq 2 exp(-t^2/C^2)$ where $C^2 = C^2\\sqrt{p}(\\prod^{\\mathcal{T}}_{\u03c4=1} L_\u03c4)^2$ and C > 0.\nHere, $X_0 - E(X_0)$ is a sub-Gaussian random vector. We thus demonstrate that qualitatively, Gaussian diffusion models also suffer from light-tails, despite utilizing multiple transformations. The quantities $L_1,\u2026\u2026\u2026, L_\\mathcal{T}$ can intuitively be thought of as Lipschitz constants characterizing each iterative step of the sampling process."}, {"title": "5. SIMULATIONS AND DATA ILLUSTRATION", "content": "We assess the practical relevance of our theoretical results through simulations and data illustrations. We sampled 10000 values from a bivariate Cauchy distribution with mode 0 and scale matrix I. We then trained multiple generative adversarial networks with different depths and latent variable dimensions, as well as a denoising diffusion model on these data. We show the Cauchy training data and 10000 samples from a four-layer generative adversarial network fitted with 64 standard Gaussian latent variables in Figure 1. Although the generated samples matched the center of the Cauchy samples well, in sharp contrast to the observed data, there were no outlying values. We observe the same pattern when inspecting samples generated from the other fitted generative adversarial networks (Figure 3) and the diffusion model (Figure 4). Our simulation results thus agree well with our theory that these deep generative models are unable to capture heavy tails in the target distribution. We also attempted to fit a Gaussian variational autoencoder to the Cauchy data but were unable to get the training to converge. Even when the learning rate was set to extremely small values (such as $1e - 8$), the training loss often fluctuates by orders of magnitude over epochs. Practitioners often report numerical instability when training variational autoencoders (Child, 2021; Dehaene & Brossard, 2021; Rybkin et al., 2021).\nNext, we analyzed data on the Standard and Poor's 500 and the Dow Jones Industrial Average indices from Yahoo Finance. We computed daily returns in basis points for both indices from January 2008 to April 2024, totaling 4096 data points. We then trained a generative adversarial network using these data. The generator has four layers and 64-dimensional standard Gaussian latent variables. We overlay 4096 samples of the generated returns with the actual returns in Figure 2a. The actual daily returns from the Standard and Poor's and Dow Jones indices are positively correlated with each other. The generated returns were able to capture this correlation well. Financial returns are well known to be heavy-tailed. We take the magnitudes of actual and generated returns and inspect them on a log-log plot. Observe that the generated returns are much more concentrated than the actual returns in Figure 2b.\nIn both the simulated and financial data setting, despite the samples being only 2 dimensional, samples from the fitted generative networks with 64 dimensional latent variables were unable to capture tail values and generally underestimated uncertainty."}, {"title": "6. DISCUSSION", "content": "The literature on deep generative models is vast and rapidly evolving. The general framework outlined in this article can be used to analyze other generative models that push forward Gaussian and log-concave latent variables, for example, flow-based models, as long as one can show that the overall push-forward mapping is Lipschitz."}, {"title": "SUPPLEMENTARY MATERIAL", "content": ""}, {"title": "A. PRELIMINARIES ON CONCENTRATION", "content": ""}, {"title": "A.1. Sub-Gaussian and sub-exponential random vectors", "content": "In the section, we review the definitions of sub-Gaussian and sub-exponential random variables and random vectors, as well as their various equivalent characterizations.\nDEFINITION 2 (SUB-GAUSSIAN RANDOM VARIABLE). A real-valued random variable z with mean E(z) = \u00b5 is sub-Gaussian if there exists a constant $C_1$ > 0 such that\n$Pr(|z \u2013 \u03bc| \u2265 t) \u2264 2 exp(-t^2/C_u^2)$for all t\u2265 0.\nThe following equivalent characterization will be useful:\nPROPOSITION 2 (SUB-GAUSSIANITY VIA ORLICZ NORM). A real-valued random variable z with mean E(z) = \u00b5 is sub-Gaussian if and only if there exist constant $C_2$ > 0 such that\n$E[exp\\{(z \u2013 \u03bc)^2/C_2\\}] \u2264 2$The smallest such constant $inf_{C_2>0} E[exp\\{(z \u2212 \u03bc)^2/C_2\\}] \u2264 2$ is an Orlicz norm of z, denoted $||z \u2212 \u03bc||_{\\psi_2}$. In other words, z is sub-Gaussian if and only if $||z \u2013 \u03bc||_{\\psi_2}$ is finite.\nIt is known that the constants $C_1$ and $C_2$ above are equivalent to each other up to universal constants (Vershynin, 2018). The definition of sub-Gaussianity can be extended to random vectors.\nDEFINITION 3 (SUB-GAUSSIAN RANDOM VECTORS). A random vector $z \\in \\mathbb{R}^d$ with mean E(z) = \u00b5 is sub-Gaussian if $sup_{u\\in S^{d\u22121}} ||u^T (z \u2212 \u03bc)||_{\\psi_2} < \u221e$, where $S^{d\u22121}$ is the unit sphere.\nThere is a related weaker notion of sub-exponentiality that we review below.\nDEFINITION 4 (SUB-EXPONENTIAL RANDOM VARIABLE). A real-valued random variable z with mean E(z) = \u00b5 is sub-exponential if there exists a constant $C_3$ > 0 such that\n$Pr(|z - \u03bc| \u2265 t) \u2264 2 exp(-t/C_u)$for all t \u2265 0.\nPROPOSITION 3 (SUB-EXPONENTIALITY VIA ORLICZ NORM). A real-valued random variable z with mean E(z) = \u00b5 is sub-exponential if and only if there exist constant $C_4$ > 0 such that\n$E\\{exp(|z \u2013 \u03bc|/C_4)\\} \u2264 2$The smallest such constant $inf_{C_4>0} E\\{exp(|z - \u03bc|/C_4)\\} < 2$ is an Orlicz norm of z, denoted $||z \u2013 \u03bc||_{\\psi_1}$. In other words, z is sub-exponential if and only if $||z \u2013 \u03bc||_{\\psi_1}$ is finite."}, {"title": "A.2. Lipschitz concentration of random vectors", "content": "We are interested in studying the distributional properties of f(z), where f is a Lipschitz function and z is sub-Gaussian or log-concave. We start with the Gaussian isoperimetric inequality.\nTHEOREM 6 (GAUSSIAN ISOPERIMETRIC INEQUALITY). Let z be a standard multivariate Gaussian random vector in $\\mathbb{R}^d$. Let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be a real-valued, L-Lipschitz function with respect to the Euclidean norm. Then the following concentration inequality holds:\n$Pr[|f(z) \u2013 E\\{f(z)\\}| \u2265 t] \u2264 2 exp\\{-t^2/(2L^2)\\}$for all t \u2265 0.\nWhile the above result is for standard multivariate Gaussian z, as a simple corollary, we can obtain similar results for z' that follow a general multivariate Gaussian distribution with mean \u00b5 and covariance \u03a3. Observe that z' can be written as $\\Sigma^{1/2}z + \u00b5$, a linear transformation with Lipschitz constant $||\\Sigma^{1/2}||$, where ||\u00b7 || denotes the spectral norm. Composing Lipschitz functions, and observing that $||\\Sigma^{1/2}||^2 = ||\\Sigma||$, we obtain\n$Pr[|f(z') \u2013 E\\{f(z')\\}| \u2265 t] \u2264 2 exp\\{-t^2/(2L^2||\\Sigma||)\\}$for all t > 0.\nSimilar Lipschitz transformation results for log-concave and strongly log-concave distributions are available from the high dimensional geometry literature.\nTHEOREM 7 (LOG-CONCAVE LIPSCHITZ CONCENTRATION). Let $z \\in \\mathbb{R}^d$ be a random vector with isotropic log-concave probability density, so that it is centred and has identity covariance. Let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be a L-Lipschitz function. Then\n$Pr[|f(z) \u2013 E\\{f(z)\\}| > tL] < exp(-\\mathcal{C}_5I_zt)$for some absolute constant $\\mathcal{C}_5 > 0$, where $I_z$ is the Cheeger's constant of the density of z.\nBy reparametrizing t' = tL and rewriting $\\mathcal{C}_5$ as $1/\\mathcal{C}_6$, we can obtain an equivalent inequality, which we use repeatedly,\n$Pr[|f(z) \u2013 E\\{f(z)\\}| > t'] < exp\\{-\\Psi_zt'/(\\mathcal{C}_6L)\\}$.\nThe isotropic and centered conditions can be dropped by considering $\\Sigma^{1/2}z + \u00b5$ for general covariance \u03a3 and mean \u00b5, with the Lipschitz constant in the upper bound then increasing by a factor of $||\\Sigma^{1/2}||$.\nWe also consider Lipschitz transformations of strongly log-concave random vectors. The definition of \u03b3-strongly log-concave distributions can be found in chapter 3 of Wainwright (2019).\nTHEOREM 8 (STRONGLY LOG-CONCAVE LIPSCHITZ CONCENTRATION). Let z be a \u03b3-strongly log-concave random vector in $\\mathbb{R}^d$. Let $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be a real-valued, L-Lipschitz function with respect to the Euclidean norm. Then the following concentration inequality holds.\n$Pr[|f(z) \u2013 E\\{f(z)\\}| \u2265 t] \u2264 2 exp\\{-\\gamma t^2/(4C^2)\\}$for all t \u2265 0."}, {"title": "B. PROOFS", "content": ""}, {"title": "B.1. Proof of Proposition 1", "content": "A finite neural network consists of finitely many compositions of affine transformations by Wi and bi and non-linear activations by $\\sigma_l$. Since the Lipschitz property is preserved under finite composition, and since $\\sigma_l \\in S$, it suffices to show that the affine transformations are Lipschitz. The Lipschitz constant of the affine function g(x) = $W_lx + b_l$ is upper bounded by the Frobenius norm $||W_l||_F$, which is finite since matrix entries and dimensions are finite."}, {"title": "B-2. Proof of Theorem 1", "content": "We break down function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$ into its p component functions $f_1, ..., f_p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, which are also L-Lipschitz by Lemma 1. Focus on $f_1$ without loss of generality. Apply the Gaussian isoperimetric inequality to get that $f_1(z) \u2013 E\\{f_1(z)\\}$ is a sub-Gaussian random variable with Orlicz norm $||f_1(z) \u2013 E\\{f_1(z)\\}||_{\\psi_2} \u2264 cL||\\Sigma||^{1/2}$ for some c > 0. This holds for all $f_1,..., f_p$. Apply Lemma 2 to see that $f(z) \u2013 E\\{f(z)\\}$ is a sub-Gaussian random vector with $||u^T [f(z) \u2013 E\\{f(z)\\}]||_{\\psi_2} \u2264 c\\sqrt{p}L||\\Sigma||^{1/2}$ for any $u \\in S^{p\u22121}$. This implies the concentration inequality $Pr[u^T[f(z) \u2013 E\\{f(z)\\}| \u2265 t] \u2264 2 exp\\{-t^2/(C^2pL^2||\u03a3||)\\}$ for any unit vector $u \\in S^{p-1}$ and some C > 0."}, {"title": "B.3. Proof of Theorem 2", "content": "We again break down $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$ into p component functions $f_1, ..., f_p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, which are also L-Lipschitz by Lemma 1. Focus on $f_1$ without loss of generality. Apply the log-concave concentration inequality in Lemma 7 to get that $f_1(z) \u2013 E\\{f_1(z)\\}$ is a sub-exponential random variable with Orlicz norm $||f_1(z) \u2013 E\\{f_1(z)\\}||_{\\psi_1} \u2264 cL||\u03a3||^{1/2}/\\Psi_z$ for some c > 0. Apply Lemma 3 to see that $f(z) \u2013 E\\{f(z)\\}$ is a sub-exponential random vector with $sup_{u\\in S^{p-1}} ||u^T [f(z) \u2013 E\\{f(z)\\}]||_{\\psi_1} \u2264 c\\sqrt{p}L||\u03a3||^{1/2}/\\Psi_z$. This implies the concentration inequality $Pr[u^T|f(z) \u2013 E\\{f(z)\\}| \u2265 t] \u2264 2 exp\\{-\\Psi_zt/(C\\sqrt{p}L||\u03a3||^{1/2})\\}$ for any unit vector $u \\in S^{p\u22121}$ and some C > 0."}, {"title": "B.4. Proof of Theorem 3", "content": "We yet again break $f : \\mathbb{R"}, "d \\rightarrow \\mathbb{R}^p$ into component functions $f_1,..., f_p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$. Each of which is also L-Lipschitz by lemma 1. Focus on $f_1$ without loss of generality. The strong log-concave concentration inequality implies $f_1(z"]}