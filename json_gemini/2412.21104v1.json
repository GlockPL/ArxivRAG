{"title": "On Parallel External-Memory Bidirectional Search", "authors": ["Lior Siag", "Shahaf S. Shperberg", "Ariel Felner", "Nathan R. Sturtevant"], "abstract": "Parallelization and External Memory (PEM) techniques have significantly enhanced the capabilities of search algorithms when solving large-scale problems. Previous research on PEM has primarily centered on unidirectional algorithms, with only one publication on bidirectional PEM that focuses on the meet-in-the-middle (MM) algorithm. Building upon this foundation, this paper presents a framework that integrates both uni- and bi-directional best-first search algorithms into this framework. We then develop a PEM variant of the state-of-the-art bidirectional heuristic search (BiHS) algorithm BAE* (PEM-BAE*). As previous work on BiHS did not focus on scaling problem sizes, this work enables us to evaluate bidirectional algorithms on hard problems. Empirical evaluation shows that PEM-BAE* outperforms the PEM variants of A* and the MM algorithm, as well as a parallel variant of IDA*. These findings mark a significant milestone, revealing that bidirectional search algorithms clearly outperform unidirectional search algorithms across several domains, even when equipped with state-of-the-art heuristics.", "sections": [{"title": "Introduction", "content": "A* [11] and its many variants are commonly used to optimally solve combinatorial and pathfinding problems. However, as these problems often involve state spaces of exponential size, practical limitations in terms of time and memory resources hinder the ability of these algorithms to tackle large instances, particularly when the scale of the problem increases (e.g., more variables, larger graphs, etc.). Thus, an important line of research in heuristic search is harnessing hardware capabilities to overcome these limitations. Parallelizing various components of the search process across multiple threads can lead to a significant reduction in running time. Similarly, exploiting external memory, such as large disks, allows algorithms to scale the size of OPEN and CLOSED lists, substantially increasing the size of problems that can be solved [6, 8, 9, 14, 23].\nOrthogonally, recent research has focused on bidirectional heuristic search (BiHS) algorithms, which have been shown to outperform unidirectional search (UniHS) methods [1, 5, 31, 33], on some problems, but the majority of these results are on relatively small problems or with weakened heuristics. It has been conjectured [3] that bidirectional search does not perform well with strong heuristics, and it is unclear whether these results will scale to the largest problems. Thus, our aim is to scale bidirectional search algorithms to significantly larger problems and stronger heuristics.\nThe intersection of parallel and external memory (PEM) and BiHS has only been explored in the context of the meet-in-the-middle"}, {"title": "Background and Definitions", "content": "Research on bidirectional heuristic search (BiHS) has spanned several decades, dating back to the work of Pohl [25]. Recently, a new theoretical understanding emerged regarding the necessary nodes for expansion during the search [7, 29], sparking a series of algorithms that find optimal [1, 4, 5, 31], near-optimal [2], and memory-efficient solutions [32]. Other research has delved into algorithm comparisons [1, 30, 33] and explored the potential advantages of BiHS over Unidirectional search (UniHS) [37].\nThe focus of this paper is on two algorithms: MM, the first algorithm to be integrated with parallel external-memory search, and BAE*, chosen due to its superior performance. BAE* and other BiHS algorithms have demonstrated strong performance, surpassing their UniHS counterparts, on small problems with relatively weak heuristics. Some have suggested that, as the scale of problems increase, BiHS algorithms can maintain their dominance over UniHS algorithms, even when stronger heuristics are employed [1, 34, 37]. Others have conjectured that bidirectional search will not perform well on problems with strong heuristics [3]. This paper evaluates these conjectures by constructing a PEM variant of BAE* capable of solv-"}, {"title": "BiHS: Definitions and Algorithms", "content": "In BiHS, the aim is to find a least-cost path, of cost C*, between start and goal in a given graph G. dist(x, y) denotes the shortest distance between x and y, so dist(start, goal) = C*. BiHS executes a forward search (F) from start and a backward search (B) from goal until the two searches meet. BiHS algorithms typically maintain two open lists OPENF and OPENB for the forward and backward searches, respectively. Each node is associated with a g-value, an h-value, and an f-value (gF, hF, fF and gB, hB, fB for the forward and backward searches). Given a direction D (either F or B), we use fD, gD and hD to indicate f-, g-, and h-values in direction D.\nThe g-value of a state s is cost of the best path discovered to s, and f-value of a state is the sum of its g- and h- values. Most BiHS algorithms consider the two front-to-end heuristic functions [15] hF(s) and hB(s) which respectively estimate dist(s, goal) and dist(start, s) for all s \u2208 G. hF is forward admissible iff hF(s) \u2264 dist(s, goal) for all s in G and is forward consistent iff hF(s) \u2264 dist(s, s') +hF(s') for all s and s' in G. Backward admissibility and consistency are defined analogously.\nBiHS algorithms mainly differ in their node- and direction-selection strategies and other termination criteria. We next describe MM and BAE*, both implemented in this paper.\nMeet in the middle (MM). MM [13] is a BiHS algorithm ensuring that the search frontiers meet in the middle. In MM, nodes n in OPEND are prioritized by:\n\nprD(n) = max(fD(n), 2gD(n))\n\nMM expands the node with minimal priority, PrMin on both OPENF and OPENB. MM halts the search once the following two conditions are met: (1) the same node n is found on both OPEN lists; (2) the cost of the path from start to goal through n is < LBMM where LBMM is a lower bound on C* that is computed as follows:\n\nLBMM = max(PrMin, fMinF, fMinB, gMinF+gMinB)\n\nwhere fMinF (fMinB) and gMinF (gMinB), are the minimal f- and g-values in OPENF (OPENB), respectively. Sturtevant and Chen [35] provided a PEM variant of MM (PEMM). Our framework below is an extension of PEMM.\nBAE*. Most algorithms (e.g., MM) only assume that the heuristics used are admissible. BAE* [27, 1] (and the identical algorithm DIBBS [28]) are BiHS algorithms that specifically assume that both hF and hB are consistent and thus exploit this fact. Let dF(n) = gF(n) - hB(n), the difference between the actual forward cost n (from start) and its heuristic estimation to start. This indicates the heuristic error for node n (as hB(n) is a possibly inaccurate estimation of gB(n)). Likewise, dB(m) = gB(m) -hF(m). BAE* orders nodes in OPENF according to\n\nbF(n) = fF(n) +dF(n)\n\nbF(n) adds the heuristic error dF(n) to fF(n) to indicate that the opposite search using hB(n) will underestimate by dF(n). Likewise, bB(m) = fB(m) + dB(m) is used to order nodes in OPENB. At every expansion cycle, BAE* chooses a search direction D and expands a node with minimal bD-value. Additionally, BAE* terminates once the same state n is found on both OPEN lists and the cost"}, {"title": "Parallel External-Memory Search", "content": "External Memory Search structures a search such that the maximum size of a problem solved scales according to the size of the disk, instead of available RAM [24]. These algorithms are often paired with parallel search methods, as techniques to minimize random I/O often group states together, allowing parallel processing.\nTwo classes of external memory search appear in the literature. One class aims to perform a complete breadth-first search of a state space, employed for verifying state space properties [18] or building large heuristics [14]. These approaches maintain information about every state on disk, loading portions of the data into memory for expansion and duplicate detection. Another class of algorithms, including External A* [9] and search with structured duplicate detection [40, 41], is used to solve large problem instances. In these algorithms, OPEN is stored explicitly on disk, and CLOSED may or may not be stored, depending on the properties of the state space [22].\nBoth classes aim to reduce I/O operations to disk, e.g., by delaying operations like duplicate detection until many states can be processed in parallel [17], and dividing the state space up into smaller buckets of states [9, 12, 19, 36, 38] which are stored together on disk, and then loaded and expanded together. When duplicate states all hash into the same bucket, it reduces the complexity of checking for duplicates. Hash-based duplicate detection [18] and sorting-based duplicate detection [20] take buckets of states where duplicate detection has been delayed, load them into memory, and remove duplicates using hash tables or sorting, respectively. Structured duplicate detection [40] structures the state space so that all successor buckets can be loaded into memory for immediate duplicate detection.\nExternal memory search has often relied on exponential-growing state-spaces with unit costs to ensure sufficient states are available to efficiently process in parallel. Notably, algorithms like PEDAL [12] have extended these approaches to non-unit-cost problems."}, {"title": "The PEM-BiHS Framework", "content": "We next introduce a high-level framework called Parallel External Memory Bidirectional Heuristic Search (PEM-BiHS), into which both BiHS and UniHS can be seamlessly integrated. PEM-BiHS is designed to efficiently solve very large problem instances. Leveraging parallelization capabilities along with using external memory, PEM-BiHS utilizes the foundations laid by algorithms such as"}, {"title": "State-space Representation", "content": "Typically, states are represented using high-level structures for convenient programming. In the context of combinatorial puzzles (e.g., the 15- and 24-puzzles), states are commonly stored as an array, with each cell's value corresponding to the label of the object it holds. To pack these states into files, an additional encoding/decoding mechanism is required for converting states into bits and decoding them. These decodings aim to minimize memory consumption and reduce I/O time. For example, in the 24-puzzle practical implementations often use 8 bits (byte) to represent the identity of each tile demanding 200 bits (8 \u00d7 25). Leveraging bit manipulation allows compression to 125 bits (5 bits are enough to store the identity of a tile). Furthermore, recognizing that a state is a permutation of 0-24 enables to use only [log2(25!)] = 84 bits, e.g., using the Lehmer encoding, which maps each permutation to a unique integer in the range {1... 25!}."}, {"title": "Bucket Structure", "content": "Nodes are grouped into buckets based on pre-determined attributes or identifiers. E.g., in the context of External A*, a bucket groups nodes with identical g- and h-values (e.g., all nodes n with g(n) = 3 and h(n) = 4 belong to the 3-4-bucket). In PEMM, a bucket groups nodes with identical priority (pr(n), as defined for MM) as well as identical g-value. Alternatively, a hash value of a state, obtained by applying a hashing function to divide the set of states into a fixed number of values, can also serve as a property for defining a bucket. Importantly, PEM-BiHS assumes that an entire bucket can fit into memory (in addition to a fixed-sized cache used to store successors before flushing them to disk, as detailed in Section 3.7). Therefore, bucket identifiers may not allow too many nodes to be mapped into a single bucket, although adaptive methods have been used to dynamically adapt bucket sizes [39]. Buckets are then written to files and loaded into memory as needed.\nIn PEM-BiHS, OPEN and CLOSED are maintained inside main memory. They store bucket records, which include the bucket identifiers and a link to the file containing the bucket. During an expansion cycle, a bucket record from OPEN is chosen for expansion. That bucket's file is then loaded into memory. Different algorithms within PEM-BiHS will choose different buckets as described next."}, {"title": "Direction, Prioritization, and Lower-bound", "content": "Selecting direction (line 7 in Algorithm 1). Numerous strategies can be employed for direction selection in BiHS. Three prevalent strategies appear in the BiHS literature: i) choosing the direction with minimal priority (as employed by MM), ii) alternating between search directions, and iii) Pohl's cardinality criterion, which chooses the direction with the smaller open-list. Naturally, UniHS algorithms consistently choose the same direction.\nPrioritizing buckets (line 8). Selecting the next node to expand is the essence of any search algorithm. PEM-BiHS allows the use of any priority function, under the restriction that the priority function must induce a total order among buckets based on the bucket identifiers. Thus, a bucket is chosen by comparing bucket identifiers within OPEN. For example, in the implementation of a PEM variant of A* using this framework (denoted as PEM-A*), the bucket identifier includes the g- and h-values of nodes.\nIt is well known that the priority function influences the number of nodes expanded. For example, A* prioritizes nodes based on lower f-values, and often adds a second prioritization criterion (\"tie-breaking\" between nodes that share the same f-value), which prefers nodes with higher g-value (and thus lower h-value). This \u201chigher-g-first\u201d tie-breaking typically reduces the number of expanded nodes compared to \"lower-g-first\", particularly in domains with unit edge costs. Nevertheless, in standard A*, different tie-breaking policies"}, {"title": "Reading Buckets", "content": "In prior PEM search studies, the process of reading a bucket (file) was carried out sequentially, influenced by the constraints of Hard Disk Drives (HDDs). Concurrent threads accessing the same file on HDDs could lead to performance degradation. However, the recent, popular Solid-State Drives (SSDs) not only provide faster memory access but also benefit from parallelized reading. Therefore, PEM-BiHS further optimizes performance by parallelizing the reading process. To parallelize the reading of a bucket, PEM-BiHS distributes the file containing the bucket equally among multiple threads. In our experiments, this resulted in a twofold speed-up compared to sequential reading. Subsequently, each thread reads states from the disk and decodes them into their in-memory state representation (See Section 3.1)."}, {"title": "In-bucket Duplicate Detection", "content": "To minimize I/O operations, the elimination of duplicates is delayed until a bucket is chosen for expansion. Duplicate nodes may arise within the same bucket when a state is discovered via different paths. Additionally, duplicates can occur within closed buckets if the state has already been expanded with a lower g-value, or within other open buckets if states have been discovered with the same g-value. Notably, if the bucket identifiers include the g-value, other open buckets cannot contain duplicates of the same state with the same g-value, and can be disregarded for duplicate detection. During the reading phase, we manage in-bucket duplicate detection. However, to minimize I/O, we postpone duplicate detection within the closed list (as discussed in Section 3.6) until after all nodes have been read.\nSimilar to hash-based delayed duplicate detection (DDD) [17, 18], when loading a bucket into memory, its nodes are placed into a hash table based on their states using a perfect hash function, and duplicates are ignored if their cell is already filled. Unlike the work of Korf [18], PEM-BiHS allows multiple threads to read from the same bucket concurrently. To support this, each unique hash value is paired with a mutex. Once the reading and in-bucket duplicate detection is done, all threads are synchronized."}, {"title": "Duplicate Detection against CLOSED", "content": "To identify and eliminate duplicates of in-memory nodes with respect to nodes stored in closed buckets (line 10), a scanning process is initiated which reads relevant buckets (in small increments) and compares them against the in-memory nodes. With Delayed Duplicate Detection [17], each bucket is associated with a hash value and exclusively contains states assigned that particular value by a hash function. As a result, the scanning process is confined to closed buckets sharing the same hash value as the state of the node under consideration. For instance, if a node's state has a hash value of 5, only buckets associated with the hash value 5 are relevant and considered for duplicate detection.\nIn PEM-BiHS, the determination of the relevant buckets for scanning relies on the identifiers that define them. If the bucket records include hash values of states, a similar approach to Korf [17] can be employed. Alternatively, if the bucket record contains the hD-value of states, for direction D, as an identifier, only buckets with the same hD-value need to be scanned. Therefore, if a bucket record contains both the hF-value and the hB-value, the scanning process is limited to buckets that possess identical values for both hF and hB.\nIn addition, with unit edge-cost undirected graphs, (where edges can be followed in both ways) there are three cases for finding duplicates of a node generated at level x. (1) A parent p at level x - 2 generates a child node n at level x - 1. The child node n generates its parent again at level x. (2) Consider a cycle of even length k which was first explored by the search at the ancestor node a at level 0. The farthest node of this cycle will be seen twice at level x = k/2 from two parents which are at level x - 1. (3) Consider a cycle with odd length k. Here, the two nodes farthest from a will be generated at level y = [k/2] and each will generate the other at level x = y + 1. Note that duplicates only occur as a result of a cycle, and a cycle can be either even or odd. Thus, when generating node n with g(n) = x we only need to check buckets with g-values of x - 2, x, and x - 1, for these three cases, respectively. Consequently, using the g-value as an identifier could significantly reduce the number of buckets that need to be scanned. This approach optimizes the duplicate detection process based on the available information in the bucket records."}, {"title": "Parallel Node Expansion", "content": "The process of parallel bucket expansion (line 12) is outlined in Algorithm 2. To enable parallel expansion, each thread is allocated an equal portion of the nodes. Newly generated nodes are inserted into a dedicated successor cache for each thread. Each cache is divided into smaller arrays, where each array corresponds to a specific bucket record. This avoids the necessity of immediately writing each new successor to the file, thereby minimizing I/O operations.\nThe framework supports reopenings by adding a new bucket with the same identifier to OPEN and merging it with the corresponding CLOSED bucket after its expansion. In our evaluation, we used consistent heuristics, so we did not encounter any reopenings. Once an array reaches its capacity, all its nodes are flushed into the disk, generating new buckets on disks as well as new bucket records in OPEN if necessary. To prevent simultaneous writes to the same file and maintain data integrity, each bucket record is linked to a mutex, which is locked when a thread writes to a bucket. This design maintains data integrity and allows concurrent writing.\nNote that bucket expansion and delayed solution detection in this framework (see Section 3.8) can occur concurrently with separate threads, as delayed solution detection is only concerned with the nodes that are about to be expanded, which are already loaded into memory and will not change due to the expansion."}, {"title": "Solution Detection", "content": "There are two approaches for solution detection (Algorithm 1, line 11): immediate solution detection (ISD) and delayed solution detection (DSD). In ISD, solutions are identified upon generation of a node n. This involves a query to the open list of the opposite frontier to check for the existence of a node with the same state as the newly generated node n. This is trivially implemented in UniHS, as it only entails checking if the state of n is the goal. Similarly, in standard BiHS, when the open lists are stored in memory, ISD can be efficiently performed using a hash table or a direct-access table where each item can be accessed in constant time.\nHowever, when buckets are stored on disk, ISD can lead to frequent I/O calls every time a node is generated or when the cache of generated nodes is filled. Therefore, PEM-BiHS employs DSD as suggested by Sturtevant and Chen [35]. In DSD, solutions are identified during the expansion phase once all states of the bucket that was chosen to be expanded are already loaded into memory and stored in a hash table. Following that, closed buckets from the opposite direction are loaded in segments and compared against the hash table of expanded nodes in an effort to identify a solution. It's important to highlight that only relevant buckets need to be loaded; for instance, if the identifiers of the bucket of expanded nodes include hF and hB values, only closed buckets corresponding to the same h-values need to be considered. We note again that when the expansion and solution detection are done, all threads are synchronized.\nIt is important to acknowledge that the benefits of DSD come with certain trade-offs. First, since solutions are detected at a later stage, some nodes might be expanded which could have been avoided with ISD. Furthermore, search bounds leveraging information across the minimal edge cost (often denoted as e) cannot be employed with DSD. This limitation arises because these bounds rely on ISD to improve the lower bound (LB) on the solution cost (we refer the reader to Sturtevant and Chen [35] for more details)."}, {"title": "Parallel External-Memory BAE*", "content": "We next describe the implementation details needed for obtaining a PEM variant of the BAE* algorithm (PEM-BAE*).\nDirection, Prioritization, and Lower-bound. The prioritization of buckets relies on the BAE* priority function (Eq.3). The lower bound"}, {"title": "Experimental Results", "content": "We performed experiments with PEM-BiHS on the 15- and 24-sliding-tile puzzles (STP) and 4-peg Towers of Hanoi (ToH4). All experiments were executed on 2 Intel Xeon Gold 6248R Processor 24-Core 3.0GHz, 192 GB of 3200MHz DDR4 RAM, and 100TB SSD for the external memory. By default, all parallel algorithms were assessed utilizing all 96 available virtual threads (with 48 physical cores). Nevertheless, we have also conducted an ablation study to investigate algorithm performance while varying the number of threads.\nWe tested the following algorithms besides PEM-BAE*. First, we instantiated both A* and MM within the PEM-BiHS framework, resulting in PEM-A* and PEMM, respectively. Similarly to A*, PEM-A* consistently expands nodes in the forward direction, employs the minimal f-value in OPEN as a lower bound for the solution cost (LB), and expands a g-h bucket with the minimal f-value during each expansion cycle. PEM-A* uses the low-g-first tie-breaking as detailed in Section 3.3. PEM-A* incorporates ISD, checking for solution upon node generation, where the PEM-BAE* and PEMM employ DSD, checking for solutions before node expansions. As BiHS search algorithms explore from both the goal and the start states, it is crucial to ensure that any potential advantage is not merely a consequence of asymmetries, which causes the search tree from one side to be much smaller. Such asymmetries could also be leveraged in a unidirectional search from goal to start. To address this concern, we executed a reverse variant of PEM-A*, denoted as PEM-rA*, where the search is conducted from goal to start.\nPEMM uses the direction-selection, lower-bound, and prioritizations of MM while using the lower-g-first tie-breaking. PEMM uses the g-value and priority value (Eq. 1) as bucket identifiers. As a BiHS, PEMM adopts DSD as part of its operational strategy.\nFor comparison, we have also implemented Asynchronous Parallel IDA* (AIDA*), [26]). AIDA* is a parallelized adaptation of IDA* which conducts a breadth-first search to a predetermined depth. The resulting frontier is subsequently distributed among all threads. Additionally, we've evaluated the reverse version of AIDA*, referred to as rAIDA*. Lastly, we evaluated the standard versions of A* and"}, {"title": "15-Puzzle", "content": "We first experimented on Korf's 100 random instances of the 15-puzzle [16]. This domain is relatively compact (1013 states) and could be solved without external memory. However, its size enables a comprehensive comparison of all algorithms across different heuristics before moving to larger domains. For heuristics, we used Manhattan Distance (labeled MD) and a 3-4-4-4 additive pattern database [10]) (labeled PDB). To construct the PDB, we divided the puzzle into four squares, one for each corner, with each pattern also including the blank. The average runtime (in seconds) and the number of node expansions are presented in Table 1 (top).\nNaturally, using the PDB heuristic substantially reduced both the time and node expansions for all algorithms when compared to using the MD heuristic. The AIDA* variants expanded the largest number of nodes but exhibited the fastest overall runtime. Due to their DFS nature, the AIDA* variants do not store nodes in memory, let alone external memory, nor do they involve sorting nodes, as typically done by best-first search algorithms. Thus, they have the smallest time overhead per node in comparison with all other algorithms. Among the PEM algorithms, both BiHS algorithms outperformed the UniHS algorithms in terms of node expansions and runtime, when using the MD heuristic. Notably, PEM-BAE* significantly outperformed all PEM or iterative deepening algorithms in terms of node expansions by an order of magnitude. This finding is consistent with prior studies that compared BAE* to A* [1, 33], underscoring the advantages of harnessing the consistency of heuristics, as demonstrated by the performance of both A* and BAE* as presented in the table. Even when employing the PDB heuristic, PEM-BAE* maintained a significant edge over all other PEM or iterative deepening algorithms in terms of node expansions. However, it exhibited slower runtime than"}, {"title": "24-Puzzle", "content": "We experimented with the 50 24-puzzle problems of Korf and Felner [21], using a 6+6+6+6 additive PDB heuristic coupled with its reflection about the main diagonal [10]. Given the immense size of these 24-puzzle problems and the extensive computation time they demand, assessing all algorithms becomes impractical. Moreover, the memory demands for tackling these problems are substantial, making it impractical to execute standard (in-memory) A* and BAE* algorithms. Thus, we compared PEM-BiHS with the AIDA* variants.\nFigure 1 illustrates the runtime (left) and the number of expanded nodes (right) for each instance. Instances are sorted solution length in ascending order, which roughly indicates the problem's difficulty. For instances with the same solution length, we averaged the results. The plot legends also display the average runtime and expansions for each algorithm across all instances, shown in parentheses.\nIn general, PEM-BAE* performs the best in both node expansions and runtime. On average, PEM-BAE* expands only 4.4% of the nodes expanded by AIDA* and runs 4.5 times faster. These findings align with the observed trend in the 15-puzzle, indicating that on challenging problems, PEM-BAE* outperforms UniHS algorithms even when equipped with state-of-the-art (or near state-of-the-art) heuristics. The average disk space consumption of PEM-BAE* for the 5x5 STP instances was 653GB, with a peak of 4TB. This underscores the impracticality of employing in-memory algorithms (such as A* and BAE*) for tackling these challenging problems."}, {"title": "4-peg Towers of Hanoi", "content": "In TOH4, we examined 20 random start and goal pairs with 20 disks, utilizing a 16+4 additive PDB heuristic [10]. In this domain, numerous cycles exist, posing a challenge for algorithms that lack duplicate detection, as already noted by Felner et al. [10]. This issue is"}, {"title": "Analyzing Previous Conjectures", "content": "In a previous analysis of bidirectional search [3], it was suggested that in a unidirectional search \"if the majority [of nodes] are expanded at shallower depth than the solution midpoint [C*/2] then [...] a bidirectional heuristic search would expand more nodes than a unidirectional heuristic search.\". The analysis for this claim predates our current understanding of BiHS. But, since we have the data it is worthwhile to evaluate the validity of this claim.\nIn this context, in our experiments we found that in 19 of 20 Towers of Hanoi instances and all 15-puzzle instances when using the PDB, the unidirectional algorithm (PEM-A*) expanded the majority of states prior to the solution midpoint. Yet, in all of these problems PEM-BAE* expanded fewer node than PEM-A*. Thus, the previous analysis does not hold for PEM-BAE* on these problems. This analysis for a representative ToH4 instance is presented in Figure 3. It is a matter for future work to analyze these claims in more depth and to consider whether or how to revise them to be more accurate."}, {"title": "Analyzing Expansion-per-second Ratios", "content": "The Table in Figure 5 presents the number of nodes expanded per second (NPS) by different PEM algorithms on the domains in which all PEM algorithms were evaluated, namely 15-STP and TOH4. While these algorithms differ in how they choose their search direction, decide on which bucket to expand, and terminate the search, these differences are not expected to significantly impact NPS. Two factors, however, have the potential to affect NPS. First, BiHS algorithms perform DSD, while UniHS algorithms perform ISD. DSD generally requires more computational effort, though a profiling analysis revealed that this additional time was negligible. Second, differences in bucket structure can affect the number of buckets and their sizes, affecting the I/O time of the algorithms. The results show significant variations in NPS among the algorithms. Despite its relative strength in nodes and in time, PEM-BAE* exhibited the worst (smallest) NPS overall, suggesting that an alternative bucket structure might further improve its advantages over the other algorithms."}, {"title": "Ablation Study on the Number of Threads", "content": "To assess the thread utilization of PEM-BAE*, we compare its performance against AIDA* on the 24-STP while varying number of threads. Specifically, we conducted experiments using 1, 16, 32, 48, 64, 80, and 96 (virtual) threads on a subset of problems (problems 4, 36, 45, 48) with an intermediate solution cost, C* = 100 .\nFigure 4 shows average runtimes for various thread configurations, with a logarithmic y-axis. As observed previously, PEM-BAE* surpasses AIDA* performance with 96 threads and consistently outperforms it across varying thread counts. As anticipated, adding more threads yields diminishing returns. PEM-BAE* reduced its runtime by a factor of 11 when transitioning from 1 thread to 16 threads, whereas AIDA* improved by a factor of 7. Beyond 16 threads, runtime reduction becomes smaller, decreasing only by a factor of 2 for both algorithms when transitioning from 16 to 96 threads. This reduced gain can be attributed to memory access required for obtaining heuristic values, imbalanced subtrees and last-layer expansions for AIDA*, constrained I/O parallelization (relative to the thread count), imbalanced bucket sizes, and locking overhead for PEM-BiHS."}, {"title": "Conclusions and Future Work", "content": "We presented PEM-BiHS, a parallel external-memory (PEM) BiHS framework for single-target search in undirected, uniformly weighted search graphs, which was used to create a PEM variant of BAE* (PEM-BAE*). Our empirical evaluations show that PEM-BAE* outperforms UniHS algorithms both in runtime and node expansions, even with well-informed heuristics. These findings challenge the conjecture put forth by Barker and Korf [3], suggesting that BiHS algorithms would not significantly surpass UniHS or bidirectional brute-force search. Further theoretical study is necessary to analyze our results in relation to this conjecture, as well as to other theoretical comparisons between BiHS and UniHS algorithms [13, 37].\nFuture research could also address NPS differences among algorithms by exploring dynamic bucket sizes and other approaches that relax the best-first assumption, aiming to achieve more balanced buckets [12]."}]}