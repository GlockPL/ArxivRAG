{"title": "Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks", "authors": ["Hongjun Wang", "Sagar Vaze", "Kai Han"], "abstract": "Detecting test-time distribution shift has emerged as a key capability for safely deployed machine learning models, with the question being tackled under various guises in recent years. In this paper, we aim to provide a consolidated view of the two largest sub-fields within the community: out-of-distribution (OOD) detection and open-set recognition (OSR). In particular, we aim to provide rigorous empirical analysis of different methods across settings and provide actionable takeaways for practitioners and researchers. Concretely, we make the following contributions: (i) We perform rigorous cross-evaluation between state-of-the-art methods in the OOD detection and OSR settings and identify a strong correlation between the performances of methods for them; (ii) We propose a new, large-scale benchmark setting which we suggest better disentangles the problem tackled by OOD detection and OSR, re-evaluating state-of-the-art OOD detection and OSR methods in this setting; (iii) We surprisingly find that the best performing method on standard benchmarks (Outlier Exposure) struggles when tested at scale, while scoring rules which are sensitive to the deep feature magnitude consistently show promise; and (iv) We conduct empirical analysis to explain these phenomena and highlight directions for future research.", "sections": [{"title": "Introduction", "content": "Any practical machine learning model is likely to encounter test-time samples which differ substantially from its training set; i.e., models are likely to encounter test-time distribution shift. As such, detecting distribution shift has emerged as a key research problem in the community [1-3]. Specifically, out-of-distribution (OOD) detection [4, 5] and open-set recognition (OSR) [6, 7] have emerged as two rich sub-fields to tackle this task. In fact, both tasks explicitly tackle the setting in which multi-way classifiers must detect if test samples are 'unseen' with respect to their training set, with a variety of methods and benchmarks proposed within each field. OOD detection methods test on images which come from different datasets to the training set, while OSR methods are evaluated on the ability to detect test images which come from different semantic categories to the training set. Research efforts in both of these fields largely occur independently, with little cross-pollination of ideas. Though many prior works have recognized the similarity of the two sub-fields [8-11], there has been little benchmarking to understand the underlying similarities and differences between them.\nIn this study, we investigate the detection of distribution shifts, with a focus on exploring and analyzing OOD detection and OSR methods and benchmarks. Our aim is to gain a comprehensive understanding of the underlying similarities"}, {"title": "Related work", "content": "Open-set recognition. Previous work [20] coins 'open-set recognition', the objective of which is to identify unknown classes while classifying the known ones. OpenMax resorts to Activation Vector (AV) and models the distribution of AVs based on the Extreme Value Theorem (EVT). Recent works [21-23] show that the generated data from synthetic distribution would be helpful to improve OSR. OSRCI [22] generates images belonging to the unknown classes but similar to the training data to train an open-set classifier. [23] adversarially trains discriminator to distinguish closed\nOut of Distribution Detection. The goal of OOD detection is generally specified as identifying test-time samples coming from a 'different distribution' from the training data. [2] formalizes the task of out-of-distribution detection and provides a paradigm to evaluate deep learning out-of-distribution detectors using the maximum softmax probability (MSP). A test sample with a large MSP score is detected as an in-distribution (ID) example rather than out-of-distribution (OOD) example. ODIN [24] and its learnable variant G-ODIN [25] add adversarial perturbations to both ID and OOD samples and employ temperature scaling strategy on the softmax output to separate them. [3] proposes the energy score derived from the logit outputs for OOD uncertainty estimation. [5] rectifies the distribution of per-unit activations in the penultimate layer for ID and OOD data. GradNorm [26] calculates gradients by backpropagating the KL divergence between the softmax output and a uniform distribution, assuming that the magnitude of gradients is higher for ID data than that for OOD data. ASH [27] removes a large portion of the activations based on the pth-percentile of the entire representation at a late layer. The remaining activations are utilized to calculate an energy score for OOD detection. SHE [28] quantifies the dissimilarity between the ID training samples from each category and the testing samples based on the features extracted from the penultimate layer of the model. This dissimilarity is then used as the score to judge whether a testing sample is OOD or not. Outlier Exposure (OE) [4] and GradNorm [26] both design a loss based on the KL divergence between the softmax output and a uniform probability distribution to encourage models to output a uniform softmax distribution on outliers. The former leverages real OOD data for training while the latter directly employs the vector norm of gradients to perform uncertainty estimation.\nRelations between OOD detection and OSR. Prior works discuss the separation between covariate and semantic distributional shift [13, 29-31]. [13] discusses separately detecting covariate and concept distributional shift on small-scale datasets (i.e., CIFAR-10/100). However, similarly to [8], we suggest that small-scale datasets with no explicit taxonomies (like CIFAR) are not well suited for defining semantic shift. As such, we aim to build a large-scale benchmark with a clear underlying taxonomy. [12] introduces ImageNet-A (i.e., collections of natural adversarial examples) and ImageNet-O (i.e., samples of held-out classes from ImageNet-21K) for robustness evaluation and unseen classes recognition, while [29] curates a set of artificial datasets to disentangle the evaluation of non-semantic distributional shift and semantic-shift. However, they focus more on achieving robustness to non-semantic distributional shift and do not develop cross-evaluation between state-of-the-art methods in the OOD detection and OSR settings. [30] treats both semantic and non-semantic tasks in an anomaly detection (AD) paradigm and applies popular AD methods to them on CIFAR10. Our work explicitly explores the relation between OSR and OOD detection tasks, and verify the effectiveness of respective popular methods in each field. Two surveys [10, 11] summarize a number of approaches within the OOD detection and OSR settings, along with anomaly detection and Novelty Detection. [32] constructs a unified benchmark to verify existing OOD detection methods, delineating 'far-OOD' and 'near-OOD'. Meanwhile, [33] rethinks the importance of ID misclassifications in the OOD context and examines different approaches on selective classification in the presence of OOD datasets. In our work, we not only discuss the link between robustness and OOD detection, but also propose a new metric to reconcile the tasks. Concurrent work [34] provides a codebase for representative methods within OSR and OOD detection. In this paper, we categorize shift detection methods into two types: scoring rules (e.g., MSP, MLS, etc), which operate post-hoc on pre-trained networks, and specialized training, which modifies the networks' optimization procedures (e.g., ARPL/ARPL+CS, OE, etc).\nAuxiliary data in OOD detection. Inspired by OE [4], recent work [37-40] leverage auxiliary data in some form to enhance the model's ability to detect OOD data. This could be through posterior sampling [37], adversarial training [38], augmenting distributions [39] or model perturbation [40]. POEM [37] focuses on posterior sampling to learn a decision boundary between ID and OOD data. ATOM [38] introduces an adversarial training method with informative outlier mining, which is specifically designed to improve the robustness against adversarial attacks, in which the adversarial data is considered as a special type of OOD data. DAL [39] addresses the distribution discrepancy between auxiliary and unseen real OOD data, by training predictors over the worst OOD data"}, {"title": "Cross-benchmarking of OOD detection and OSR methods", "content": "Despite the growing popularity of OOD detection and OSR studies, these two tasks have largely evolved independently and in isolation from each other, as shown in Table 1. Indeed, methods designed for OSR can be seamlessly adopted to address the OOD detection problem, and vice versa. Recent generalized OOD detection frameworks [30, 34] unify tasks relevant to OOD detection and OSR. However, there is still a lack of cross-evaluation between methods developed for OOD detection and OSR on current standard benchmarks. Given the strong inherent connections between OOD detection and OSR, a comprehensive cross-benchmarking comparison is crucial to shed light on the future development of the broader distribution shift detection problem.\nAs a starting point to reconcile OOD detection and OSR, in this section we perform cross-evaluation of methods from both sub-fields."}, {"title": "Experimental setup", "content": "Problem setting. Let \\(X \\in \\mathbb{R}^D\\) denote an input sample and \\(C \\in \\mathbb{R}\\) denote the label of interest. Test-time distribution shift occurs when the testing joint distribution is not equal to the training joint distribution, i.e., \\(P_{\\text{test}}(X,C) \\neq P_{\\text{train}} (X, C)\\). This shift can be further divided into two types: covariate shift and semantic shift. Covariate shift occurs when \\(P_{\\text{test}} (C|X) = P_{\\text{train}}(C|X)\\) but \\(P_{\\text{test}}(X) \\neq P_{\\text{train}}(X)\\). Semantic shift occurs when \\(P_{\\text{test}} (C|X) \\neq P_{\\text{train}}(C|X)\\) but \\(P_{\\text{test}}(X) = P_{\\text{train}}(X)\\). In OOD detection and OSR for multi-class classification, the label space contains multiple semantic categories \\(\\{c_1,\\ldots,c_L\\}\\), where L is the total number of categories in the testing data. The model needs to identify the distribution from which test-time samples originate and conduct classification based on the posterior probability, represented as \\(p(C = c_i | X)\\).\nMethods. We distinguish two categories of shift detection methods: scoring rules (which operate post-hoc on top of pre-trained networks); and specialized training (which change the optimization procedure of the networks).\nFor scoring rules, we compare the maximum softmax probability (MSP) [2], the Maximum Logit Score (MLS) [8], ODIN [24], GODIN [25], Energy scoring [3], GradNorm [26] and SEM [36]. We"}, {"title": "Quantitative results", "content": "In Table 2, we benchmark OOD detection and OSR tasks across nine common datasets, with different training strategies and scoring rules. The results are averaged from five independent runs. Although there is not always one clear winner regarding methodology, we have three main observations.\nFirstly, MLS [8] and Energy [3] tend to perform best across OOD and OSR datasets. We hypothesize that this is because both are sensitive to the magnitude of the feature vector before the networks' classification layer. To verify our conjecture, we investigate the magnitude of features by projecting the features of both ID and OOD/open-set samples into a two-dimensional space in Figure 2. We experiment on generic and fine-grained datasets, namely, CIFAR-10 and CUB. This projection is achieved by training a linear layer with an output dimension of two, after the penultimate layer of the model. The feature magnitude of ID data is larger than that of open-set/OOD data. This is consistent with the finding in [8] that 'unfamiliar' examples tend to have lower feature magnitude than ID samples, providing a strong signal for distribution shift detection.\nSecondly, we observe that Outlier Exposure [4] provides excellent performance on the OOD detection benchmarks, often nearly saturating performance. More results can be found in Section A in Appendix. It also often boosts OSR performance, though to a lesser degree, a phenomenon which we explore next in Section 4.\nThirdly, for small-scale datasets, OOD detection accuracy is positively related to ID accuracy, while an inverse correlation is observed for large-scale datasets. In Figure 3, we further include the results using the recent"}, {"title": "Qualitative analysis", "content": "In this section, we qualitatively interrogate the learned representations of Cross-Entropy and Outlier Exposure networks in order to explain the stark performance boost of OE on existing OOD detection benchmarks. Specifically, we use the value of the maximally activated neuron at various layers to analyze how the networks respond to distribution shifts. We pass every sample through the network, and plot the histogram of maximum activations at every layer in Figure 5 (see Figure A2 for the analogous results by training with the ARPL+CS method).\nThis is inspired by [8], who show the 'maximum logit score' (MLS, the maximum activation at a network's output layer) can achieve SOTA for OSR. Furthermore, [52] propose that networks respond to a 'lack of familiarity' under distribution"}, {"title": "Disentangling distribution shifts", "content": "Having analysed methodologies for detecting distribution shift across the OOD detection and OSR settings, we turn our attention to the benchmarks. While it is clear that OSR specifically aims to"}, {"title": "Datasets", "content": "As a starting point, we note that [8] introduced the Semantic Shift Benchmark (SSB), a distribution shift benchmark with isolates semantic shift. We mainly focus on ImageNet-SSB [56] and CUB-SSB [57] datasets. 'Seen' classes in ImageNet-SSB are the original ImageNet-1K classes, while 'unseen' classes selected from the disjoint set of ImageNet-21K-P [15]. Meanwhile, CUB-SSB splits the 200"}, {"title": "Quantitative analysis", "content": "In Tables 4 and 5, we evaluate a selection of previously discussed methods on our large-scale benchmark for both OOD detection and OSR. Through this large-scale evaluation, we find that in terms of training methods, among CE, ARPL (+CS), and OE, there are no clear winners across the board. It is surprising that the best performer on the previous small scale benchmarks (see Table 2), OE, appears to struggle when scaled up (last two rows in Table 5). We analyse this contradiction in the next section. In terms of scoring rules, we again find that the magnitude-aware scoring rules (MLS and Energy), consistently produce the best performance regardless of the methods and benchmarks (both standard small-scale ones and our large-scale ones)."}, {"title": "OE on large-scale datasets", "content": "Here, we investigate why OE performs worse than other methods on a large-scale benchmark. One"}, {"title": "Dataset proximity vs. OOD detection performance", "content": "To verify that the dataset proximity between auxiliary data and OOD data correlates to the OOD detection performance, we measure the correlation between OOD detection performance and dataset proximity. We quantify this proximity by calculating the distance between OOD data and auxiliary data. For a specific OOD dataset, we compute the distance via Top-K nearest neighbors and a deep kernel method [59], respectively. For Top-K nearest neighbors, we compute the average of all the distances between the normalized feature of each OOD sample and its Top-K nearest neighbors in"}, {"title": "Outlier-aware accuracy", "content": "Finally, we introduce a new metric to reconcile the problems of detecting covariate shift and being robust to it. Although AUROC is commonly used to compare different techniques for distinguishing out-of-distribution samples, it does not capture the model's ability to reliably classify testing samples in the presence of distribution shifts. To analyze the relationship of performance between covariate shift and robustness, we introduce a novel measure, which we term Outlier-Aware Accuracy (OAA). At a given threshold, and a given set of predictions (both ID vs. OOD predictions, and predictions within the closed-set categories), we compute the aggregate frequency of 'correct' predictions. The definition of 'correct' varies depending on the prediction. Specifically, as shown in Figure 10, all instances predicted as ID samples should have accurate class predictions, and all OOD samples that are not already categorized should be detected.\nThis is because we expect a good model to correctly classify all samples with any covariate shift and identify any remaining OOD samples.\nThe number of the counted instances is then divided by the total number of testing instances to produce the OAA, which is robust to non-semantic shift. This measure is computed under different thresholds based on the scoring rules and aggregated:\nMOAA metric. The OAA values across all thresholds can also be aggregated into a single value within [0, 1] as an overall measure, mean OOA (mOAA). To compute the mOAA, we consider testing images including both ID data from \\(D_{\\text{ID}}\\) and OOD data from \\(D_{\\text{OOD}}\\). The mOAA is defined as follows:\n\\[\\text{mOAA} = \\frac{1}{N} \\sum_{i=1}^N \\text{OAA}_i = \\frac{1}{N} \\sum_{i=1}^N \\frac{p_i + p_o}{|D_{\\text{ID}}| + |D_{\\text{OOD}}|}\\]\nwhere \\(p_i\\) denotes the correct predictions among the testing samples predicted as ID, \\(p_o\\) denotes the correct predictions among the testing samples predicted as OOD (see Figure 10), \\(|\\cdot|\\) represents the size of the testing set, and N is the number of different thresholds. The mOAA score ranges from 0 to 1. A higher value indicates better performance, with a score of 1 representing perfect detection and recognition, while a score of 0 represents the worst performance in terms of separation and recognition.\nBased on the numerical results in Figure 11 and Table 6, we have observed a turning point for the threshold that achieves the optimal balance between model robustness and OOD detection. It is worth noting that this metric has a connection to AURC [62]. While both AURC and OAA consider classifier performance, our proposed OAA specifically measures the 'correct prediction rate', providing an interpretable value between 0 and 1. Therefore, we believe that this metric can be effectively used to study the tradeoff between OOD detection and generalization with greater precision."}, {"title": "Summary of empirical phenomena", "content": "In the previous sections, we thoroughly evaluated methods for OOD detection and OSR in terms of scoring rules, training methods, and auxiliary data. To summarize these phenomena, we briefly highlight the key observations as follows: (i) Magnitude-aware scoring rules (i.e., MLS and energy) offer obvious advantages for both OOD detection and"}, {"title": "Conclusion", "content": "In this study, we explore Out-of-Distribution (OOD) detection and Open-set Recognition (OSR). We conducted a thorough cross-evaluation of methods for OOD detection and OSR. Additionally, we introduced a new benchmark setting that separates the distribution shift problem into covariate shift and semantic shift, proposing large-scale evaluation protocols for both settings. Our study revealed that the best performing method current OSR and OOD datasets (Outlier Exposure) does not generalize well to our challenging large-scale benchmark. We also discovered that magnitude-aware scoring rules are generally more reliable than others. Overall, our new benchmark can serve as an improved testbed for measuring progress in OSR and OOD detection while providing insights into these two problems. We hope that our thorough empirical investigation on the OOD detection and OSR methods and benchmarks can shed light for future study and applications on the broader data distribution shift detection problem."}, {"title": "Appendix A", "content": "More experimental results on benchmarks\nWe additionally evaluate different methods on Scars-SSB and FGCV-Aircraft-SSB datasets from the Semantic-Shift Benchmark [8] to further investigate the performance of scoring rules against semantic shifts. From Table Al to Table A4, we can observe that magnitude-aware scoring rules perform well among different training methods.\nWe also investigate the effect of OE using different auxiliary data (e.g., Places and YFCC-15M). As shown in Table A3 and Table A4, we can see that OE performance heavily depends on the auxiliary training data.\nThe degeneration of OE when applied to large-scale datasets drives us to think about the core contribution behind the OE method. To find out the reason, we apply OE to the small-scale datasets with different auxiliary data (i.e., YFCC-15M) in Table A5. Compared with results using 300K random images, the one using YFCC-15M cannot exceed the performance of the OE baseline. This indicates that the selection of auxiliary data is essential to the OE method and the success of OE may come from the similarity of auxiliary data distribution and test-time outliers."}, {"title": "Appendix B", "content": "Activations of OOD and open-set data at different layers\nWe provide the maximum activations for intermediate layers of ResNet-18 trained on CIFAR10 when evaluated on in-distribution data and data with different shifts. Activations in later layers are more discriminative between ID and OOD/open-set samples. After using the OE loss, we can easily notice that the OOD samples are more separable than the model trained with CE loss in Figure 5. We also show the maximum activation of the remaining datasets in Figure A1.\nWe also visualize the histogram of maximum activations of the model trained using ARPL+CS at every layer in Figure A2. Our findings align with the observations in Section 3.3 in the main paper, indicating that the early layer activations closely resemble those of the ID test data while the activation patterns begin to differ in the deeper layers. Notably, the ARPL+CS method demonstrates superior separation compared to CE, but it lags behind OE, as illustrated in Figure 5 and Figure A1. These findings align with results in Table 2."}, {"title": "Appendix C", "content": "Correlation of OOD and auxiliary training data\nIn Figure A3, we also visualize t-SNE projections of representations for various datasets: ID data (CIFAR10), auxiliary training OOD data (300K [4] vs. YFCC15M), and different test-time OOD datasets. As seen in Figure A3 and Figure A4, using 300K images generally leads to better overlap with test-time OOD data. Consequently, OE trained with 300K as auxiliary ODD achieves superior performance compared to its counterpart trained with YFCC15M (Table 2 vs. Table A5 in Section A) because it shows a better overlap with the test-time OOD data."}, {"title": "Appendix D", "content": "Different architectures and training setups\nApart from ResNet, we also conduct experiments using DenseNet121 on small-scale datasets and using DinoViT-S/8 on large-scale datasets. As shown in Table A6 and Table A7, magnitude-aware approaches still perform others on several datasets."}]}