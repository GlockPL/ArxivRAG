{"title": "TOWARDS QUANTIFYING AND REDUCING LANGUAGE MISMATCH EFFECTS\nIN CROSS-LINGUAL SPEECH ANTI-SPOOFING", "authors": ["Tianchi Liu", "Ivan Kukanov", "Zihan Pan", "Qiongqiong Wang", "Hardik B. Sailor", "Kong Aik Lee"], "abstract": "The effects of language mismatch impact speech anti-spoofing\nsystems, while investigations and quantification of these ef-\nfects remain limited. Existing anti-spoofing datasets are\nmainly in English, and the high cost of acquiring multilin-\ngual datasets hinders training language-independent models.\nWe initiate this work by evaluating top-performing speech\nanti-spoofing systems that are trained on English data but\ntested on other languages, observing notable performance\ndeclines. We propose an innovative approach - Accent-based\ndata expansion via TTS (ACCENT), which introduces di-\nverse linguistic knowledge to monolingual-trained models,\nimproving their cross-lingual capabilities. We conduct exper-\niments on a large-scale dataset consisting of over 3 million\nsamples, including 1.8 million training samples and nearly 1.2\nmillion testing samples across 12 languages. The language\nmismatch effects are preliminarily quantified and remarkably\nreduced over 15% by applying the proposed ACCENT. This\neasily implementable method shows promise for multilingual\nand low-resource language scenarios.", "sections": [{"title": "1. INTRODUCTION", "content": "With the rapid advancement of generative models in speech\nsynthesis, synthetic speech is increasingly integrated into\ndaily life and is used in various applications. Speech synthe-\nsis is capable of replicating an individual's voice, emotions,\nand speaking style, and even generate speech in multiple\nlanguages for a given person [1-3]. However, this progress\nalso presents significant risks to voice biometrics. Advanced\nspeech generation technologies can be exploited to attack au-\ntomatic speaker verification (ASV) systems and compromise\nusers [4-7]. Finding effective ways to detect spoofing attacks\nand protect users from spoofed speech is important. This is\nespecially important for the ethical use of generative models.\nAs illustrated in Fig. 1, the language mismatch effects\nrefer to the decrease in performance resulting from discrep-\nancies between the languages used during training and in-\nference. These effects have been extensively studied across"}, {"title": "2. METHODOLOGY", "content": "Studies on automatic speech recognition systems show that\nmodels trained on multiple languages are more robust against\nunseen languages compared to those trained only on a single\nlanguage [18], highlighting the significance of addressing lan-\nguage mismatch effects. While the language effect has been\nvalidated in many speech-related tasks [8,9,18], it hasn't been\nexplored in the field of speech anti-spoofing.\nTo fill this gap, we design experiments using an En-\nglish dataset for training and evaluate the models on test\nsets both in English and in other languages. To ensure that\nthe performance differences are primarily due to language\nmismatch effects rather than a single model's incidental\nbetter adaptation to a specific domain, we re-implemented\nfour state-of-the-art (SOTA) models, including their vari-\nants, for a total of seven models: SENet [19] of 34 and 50\nlayers, SE-Res2Net [20], SCG- and MLCG-Res2Net [21],\nand AASIST/AASIST-L [22]. We also carefully select the\nEnglish and other language samples from the same dataset\nto avoid any biases from different vocoders or data pre-\nprocessing differences. Our findings not only confirm the\neffect of language mismatch on system performance but also\noffer an initial quantification. Detailed results and analysis\nare provided in Section 4."}, {"title": "2.2. Motivations and Hypothesis for ACCENT", "content": "Obtaining a comprehensive multilingual training set, par-\nticularly for low-resource languages, presents a challenge.\nMoreover, language mismatch has a significant negative im-\npact on performance. To address this, we aim to enhance the\ncross-lingual capabilities of systems trained on monolingual\ndatasets.\nThe accent refers to distinct pronunciation patterns that\nmark an individual's speech as belonging to a specific lan-\nguage group [23]. When learning a second language, indi-\nviduals often retain stylistic features from their native lan-\nguage, blending linguistic characteristics in their speech [23]."}, {"title": "2.3. Methodology of Creating Dataset using ACCENT", "content": "The pipeline to expand the dataset using the proposed AC-\nCENT method is shown in Fig. 2. Considering that data gen-\nerated by TTS models are inherently suitable as spoofed data\nfor training anti-spoofing models, and Google TTS (gTTS)\u00b9\nsupports a wide range of languages and accents, we utilize\nthe proposed ACCENT method to create two datasets using\ngTTS: gTTS-Eng and gTTS-Mix. The gTTS-Eng includes\n14 English accents and the gTTS-Mix is generated by 78 en-\ngines of other languages. The creation of these datasets aims\nto enhance the models' robustness to various English accents\nand, more crucially, to leverage diverse linguistic knowledge\nfor training language-independent anti-spoofing models."}, {"title": "3. EXPERIMENTAL SETUP", "content": ""}, {"title": "3.1. Dataset", "content": ""}, {"title": "3.1.1. Training Set", "content": "As presented in Table 1, we divide the training set into\nthree portions: Portion I primarily consists of public datasets;\nPortion II contains augmented data created using the proposed\nACCENT method; and Portion III comprises private datasets.\nBoth Portions I and II exclusively use English, while in Por-\ntion III, 99.93% of the utterances are in English. The total\ncombined data amounts to over 1.8 million."}, {"title": "3.1.2. Test Sets", "content": "The test sets in Table 2 include both monolingual and cross-\nlingual testing scenarios. For monolingual testing, we in-\nclude the widely-used ASVspoof series test sets [13, 14] and\nFakeAVCeleb [16]. The WaveFake dataset [26], which en-\ncompasses both English and Japanese, is utilized for cross-"}, {"title": "3.2. Training Strategy", "content": "We use Adam optimizer with a weight decay of $1 \\times 10^{-4}$ and\na learning rate scheduler with 1000 warm-up steps. The learn-\ning rate decrease proportionally to the inverse square root of\nthe step number. The negative log-likelihood loss is used.\nTraining ceases if there is no improvement in validation per-\nformance for 12 epochs. The best model on the validation\nset is used for testing. Three type of data augmentations of\nadding Gaussian noise, pitch shifting, and time stretching are\napplied during training. We train all the models using the\nsame setup, with the exception of two specific cases: (1). The\nmodels in Table 4 are trained with a patience of 15 epochs.\n(2). The self-supervised learning (SSL)-based systems are\ntrained using WavLM large [32] coupled with a Long Short-\nTerm Memory (LSTM) with hidden dimension of 192. The\ntraining starts with 5-epoch warm up, then exponentially de-\ncaying. The SSL model is fixed for the first 10 epochs."}, {"title": "4. RESULTS AND DISCUSSION", "content": ""}, {"title": "4.1. Effects of Language Mismatch on SOTA Models", "content": "As detailed in Section 2.1, we train seven SOTA models using\nthe ASVspoof2019 LA [13] dataset in an English-only condi-"}, {"title": "4.2. Evaluation for the proposed ACCENT method", "content": "We compare the performance of the SCG-Res2Net model on\ntwo cross-lingual test sets, both with and without the imple-\nmentation of the proposed ACCENT method. The results are\npresented in Table 5. In this context, Portion I is the public\ndataset, while Portion II consists of data augmented with\nour proposed ACCENT method; both are English datasets.\nA comparison between System 1 and System 5 reveals that\nthe proposed ACCENT method significantly enhances the\nmodel's performance on cross-lingual test sets, achieving\nan average relative improvement of 19.6%, even when the\ntraining data is still exclusively monolingual in English.\nExcluding the Impact of Data Volume Changes. By\nmerging Portions I and II, the total number of training"}, {"title": "Evaluation for Different Languages.", "content": "In Figure 3, we\nconduct a detailed investigation of the performance across\nvarious languages. The radar charts show that the proposed\nACCENT method significantly enhances performance in\n9 out of 10 different languages, with the performance in\nthe remaining language, Chinese, also being very close.\nThis demonstrates the robustness of the proposed ACCENT\nmethod, which encompasses 78 language accents and 14\nEnglish accents, in improving performance across diverse\nlanguages. Comparing the languages supported by gTTS,\nmost languages in the VC-CL3 and TTS-CL datasets are cov-"}, {"title": "Evaluation for Different Models.", "content": "We perform experi-\nments to evaluate the compatibility of the proposed ACCENT\nmethod with two other models.\nThe Gemini model, as presented in [33, 34], is designed\nto investigate the effects of temporal and frequency resolu-\ntions on speech tasks in the context of 2D-CNN models. The\nRes2Net structure, known for its ability to extract multi-scale\nfeatures, has had its performance validated in related speech\ntasks [35-38]. Therefore, we include Gemini Res2Net in our\nexperiments. According to the results presented in Table 5,\nour proposed ACCENT method shows great compatibility\nwith the Gemini Res2Net model, achieving an average of\n28.5% relative improvement."}, {"title": "Evaluation on English Test Sets.", "content": "Our previous exper-\niments clearly show the benefits of the proposed ACCENT\nmethod in cross-lingual testing scenarios. We now turn our\nattention to its effects on the training language, English. The"}, {"title": "Ablation Study.", "content": "When various language accents are ap-\nplied, system 3 demonstrates obvious improvements in mul-\ntilingual performance over the baseline system 1. This high-\nlights the effectiveness of the proposed ACCENT method.\nWith augmentation limited to English accents, system 2\nshows a slight advantage over all other systems in English\ntest sets, which is expected given its English-centric design.\nMoreover, the combination of both achieves a better balance."}, {"title": "Compatibility with Massive Types of Synthetic Data.", "content": "Considering that the proposed method introduces additional\nTTS models into the data, the observed performance improve-\nment might be attributed to the extra model rather than the\nlinguistic information related to the accent, which enhances\nthe model's robustness. To minimize this possibility, we in-\ncluded data from Portion III. Portion III contains data gen-\nerated by over 70 different synthetic models. By introduc-\ning a large number of different models, we can minimize the\nbenefit gained from additional TTS models. Additionally, we\ninvestigate whether the proposed ACCENT method method\nremains effective for data generated by a wide variety of syn-\nthetic models. The experimental results from Table 5 indicate\nthat by enriching the types of synthetic data, there is a signif-\nicant improvement in cross-lingual performance. Moreover,\nthe concurrent use of the proposed ACCENT method can fur-\nther improve the performance by 16.7% (system 10 vs. 11)\nand 12.1% (system 12 vs. 13)."}, {"title": "4.3. Evaluation on Synthetic Singing Test Set", "content": "With the advancements in research on synthetic singing,\nthe detection of synthetic singing has recently garnered more\nattention in the research community [27, 43-45]. As intro-\nduced in Section 3.1.2, CtrSVDD is a dataset comprising both\nJapaneses and Chinese, meeting our requirements for cross-\nlingual performance evaluation. Therefore, we also evaluate\nthe proposed method to determine if it remains effective in the"}, {"title": "4.4. Quantifying Language Mismatch Effects", "content": "Currently, the extent to which language mismatch affects\nthe performance of anti-spoofing systems remains unknown.\nBased on observations from all of our experiments, the over-\nall performance improvement with the proposed method is\napproximately 15%. Given that not all linguistic informa-\ntion can be preserved during the vocoding process and that\nour method cannot perfectly solve the language mismatch\neffect, we estimate that the language mismatch effects could\nrelatively decrease performance by over 15%. However,\nthis can vary depending on factors such as the diversity of\ntraining data, model architectures, training strategies, and the\nlanguages used for testing."}, {"title": "5. CONCLUSION", "content": "We initiate our work by investigating the language mismatch\neffects on several top-performing anti-spoofing systems. We\nobserve that their effectiveness decreases by an average of\n20% when tested on a mix of Japanese and English samples,\nversus only English samples. This confirms the existence of\nlanguage mismatch effects between English and Japanese. To\nbroaden this analysis to more languages and enhance cross-\nlingual capabilities in anti-spoofing models, we introduce a\ndata expansion method named ACCENT. This method inte-\ngrates diverse linguistic information into monolingual-trained\nsystems through a vocoding process. The results show that\nour approach remarkably improves cross-lingual performance\nby 15%, without compromising the system's performance in\nits original training language. This method is promising for\nmultilingual and low-resource language scenarios. Moreover,\nit holds potential for application in other speech tasks to im-\nprove language generalization ability and robustness. Addi-\ntionally, we are the first to quantify the effects of language\nmismatch on performance in speech anti-spoofing, offering a\npreliminary estimate of over 15% relative reduction."}]}