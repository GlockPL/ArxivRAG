{"title": "Explaining Explaining", "authors": ["Sergei Nirenburg", "Marjorie McShane", "Kenneth W. Goodman", "Sanjay Oruganti"], "abstract": "Explanation is key to people having confidence in high-stakes AI systems. However, machine-learning-based systems - which account for almost all current AI \u2013 can't explain because they are usually black boxes. The explainable AI (XAI) movement hedges this problem by redefining \"explanation\". The human-centered explainable AI (HCXAI) movement identifies the explanation-oriented needs of users but can't fulfill them because of its commitment to machine learning. In order to achieve the kinds of explanations needed by real people operating in critical domains, we must rethink how to approach AI. We describe a hybrid approach to developing cognitive agents that uses a knowledge-based infrastructure supplemented by data obtained through machine learning when applicable. These agents will serve as assistants to humans who will bear ultimate responsibility for the decisions and actions of the human-robot team. We illustrate the explanatory potential of such agents using the under-the-hood panels of a demonstration system in which a team of simulated robots collaborates on search task assigned by a human.", "sections": [{"title": "1 INTRODUCTION", "content": "Explanation is clearly one of Marvin Minsky's \"suitcase\" words \"that we use to conceal the complexity of very large ranges of different things whose relationships we don't yet comprehend (Minsky 2006, p. 17). The Stanford Encyclopedia of Philosophy includes detailed entries for mathematical, metaphysical and scientific explanation and a separate one on causal approaches to the latter. Specialist philosophical literature is devoted to discussions of Carl Hempel's (1965) deductive-nomological model of explanation and the rival inductive-statistical approaches. Explanation is also discussed in other disciplines, e.g., psychology (e.g., Lombrozo 2010). Special attention is also paid to the differences between explainability and interpretability, transparency, explicitness and faithfulness. (e.g., Rosenfeld and Richardson 2019). Recent years have also seen a pronounced interest in developing novel explanation theories (Yang et al. 2022, Rizzo et al. 2023).\nExplanation in Al has a long history as well. Arguably the first AI-related contribution was Craik (1943). Kenneth Craik was a psychologist and an early cyberneticist whose work influenced AI and cognitive science (Boden 2006, pp. 210-218). His book was titled \"The Nature of Explanation\" discusses a variety of the senses of this suitcase word and, among other things, stresses the distinction between causal explanation in terms of a formal world model (what would be later termed ontology) and statistical explanation seeking to explain by pointing out uninterpreted relations among observable entities.\nThe above distinction still remains in the spotlight today. Most current generative Al systems are black boxes whose functioning cannot be explained in normal human terms. For certain applications, this is not a problem:\n1. Non-critical Al capabilities \u2013 such as GPS systems, machine translation systems, and search engines are widely and happily employed by end users who don't require explanations.\n2. Al capabilities that emulate physical rather than cognitive capabilities \u2013 such as robotic movement and speech recognition are incompatible with the kinds of user-elucidating explanations we address here. That is, we all understand that it would be pointless to ask a robot exactly how it extends its arm or keeps its balance when walking on uneven surfaces."}, {"title": null, "content": "3. \u0391\u0399 systems that emulate cognitive capabilities (e.g., X-ray analysis systems) can be useful, despite their lack of explainability, as long as they are leveraged as orthotic assistants rather than autonomous systems. This requires that domain experts understand the reliability and deficits of the technology well enough to judiciously incorporate it into their workflow (Chan & Siegel, 2019; Nirenburg, 2017).\nBy contrast, lack of explainability is a problem for ML-based Al systems that are designed to operate autonomously in critical domains. For example, as of June 2021, the FDA cleared 343 AI/ML-based medical devices, with over 80% of clearances occurring after 2018 (Matzkin, 2021). This supply of new Al systems has continued unabated even though their adoption has been less than enthusiastic. Fully 70% of these devices offer radiological diagnostics and typically claim to exceed the precision and efficiency of humans. But, according to Gary Marcus (2022), as of March 2022, \"not a single radiologist has been replaced.\" So, regulators keep approving systems whose operation cannot be explained, and developers keep hoping that their systems, though unexplainable, will be adopted. (For further discussion, see McShane, Nirenburg, and English, 2024 Section 2.7.1.)\nThe unexplainability problem has been addressed in earnest by the Explainable-AI (XAI) movement (Finzel et al. 2023, Bodria et al. 2021, Cambria et al. 2022, Nagahisarchoghaei et al. 2023, Schwalbe and Finzel 2023), but the results are, in general, less than satisfying (Barredo et al. 2020). XAI investigators hedge the explainability problem by redefining \"explanation\" (Gunning, 2017; Mueller et al., 2019). XAI research does not seek to explain how systems arrived at their output. Instead, it concentrates on \"post hoc algorithmically generated rationales of black-box predictions, which are not necessarily the actual reasons behind those predictions or related causally to them... [and which] are unlikely to contribute to our understanding of [a system's] inner workings\" (Babic et al., 2021).\nThe related human-centered explainable Al (HCXAI) movement, for its part, identifies the explanation-oriented needs of users but is hampered in fulfilling them because of its commitment to machine learning (Babic et al., 2021; Ehsan et al., 2022; Liao & Varshney, 2022).\nThe solution to the problem of unexplainable ML-based Al is not to keep trying to square that circle: generative Al techniques are, and will remain, unexplainable. The solution is to step back and reconsider how to develop Al systems so that they are"}, {"title": null, "content": "explainable to the degree, and in the ways, that are necessary for different types of applications.\nIntelligent behavior by humans and Al agents involves a variety of different capabilities of perception, reasoning, decision-making and action. Some of them are arguably better fit to be implemented using generative Al approaches, some others, by symbolic AI approaches. Therefore, hybrid Al systems are better suited for comprehensive (non-silo) applications than either of the above approaches alone. This observation was first made over 60 years ago (Minsky 1961) and has finally received due attention in the field. Indeed, hybrid \u201cneurosymbolic\" AI architectures are at present one of the foci of work in AI (Hitzler et al. 2023).\nOur team is developing a family of hybrid cognitive systems that we call LEIAs: Language-Endowed Intelligent Agents. Our work is a part of the movement toward integrating empirical (deep learning-based) and deductive/rational (knowledge-based) approaches to building intelligent agent systems. Explanation is an important component of such systems (de Graaf et al. 2021).\nWe believe that explanations are needed only for actions, plans, attitudes and expectations that are of interest to users and collaborators of Al agents, not their developers. It follows that generative Al methods are most useful for implementing system modules for whose behavior explanations are not typically required, such as motor control and uninterpreted perception. For any process that requires explainable reasoning (perception interpretation, decision-making, action specification, etc.) and any application where confidence in the system output is important, the black-box LLMs are not a good fit. Accordingly, LEIAs use LLMs whenever this simplifies or speeds up work on tasks that do not require explanation and where hallucinations are either benign or expected to be detected by human or system inspectors.\nIn what follows we briefly illustrate some of the explanation capabilities of LEIAS."}, {"title": "2 LEIAS", "content": "The LEIA program of R&D is a theoretically grounded, long-term, effort that has two main emphases: developing cognitive robotic systems whose capabilities extend beyond what machine learning alone can offer, and earning people's trust in those systems through explainability (McShane, Nirenburg, & English, 2024).\nLEIAS are implemented in a dual-control cognitive-robotic architecture that integrates strategic, cognitive-level decision-making with tactical, skill-level robot control (Oruganti et al., 2024). The strategic (cognitive) layer relies primarily on knowledge-based computational cognitive modeling for interpreting perceptive inputs, reasoning, decision-making, learning, etc. The tactical (robotic, skill-level) module relies on data-driven tools for recognizing perceptive inputs and rendering actions.\nLEIAs can explain their operation because they are configured using human-inspired computational cognitive modeling. Their explanations make clear the relative contributions of symbolic and data-driven methods, which is similar to a human doctor explaining a recommended procedure using both causal chains, such as how the procedure works, and population-level statistics, such as the percentage of patients for whom it is curative."}, {"title": "3 EXPLANATION VIA UNDER-THE-HOOD PANELS", "content": "As detailed in McShane, Nirenburg, and English (2024, chapter 8, \"Explaining\"), there are many things that a LEIA can explain (what it knows, how it interpreted an input, why it made a given decision, etc.) and there are many ways to present explanations to people. Although the most obvious way is through language, other expressive means can be even more useful in some contexts. One such way is by dynamically showing traces of system operation using what we call under-the-hood panels.\nWe first introduced under-the-hood panels in the Maryland Virtual Patient (MVP) proof-of-concept clinician training application (McShane et al., 2008; McShane & Nirenburg, 2021). There, the under-the-hood panels showed traces of the physiological simulation of the virtual patient, the patient's interoception, its thoughts, the knowledge it learned, and how it interpreted text inputs from the user, who was playing the role of attending physician. These insights into system functioning were geared at earning the trust of medical educators, who would ultimately need to choose to incorporate such a system into their pedagogical toolbox."}, {"title": "3.1 A search-and-retrieve request", "content": "We will illustrate the explanatory power of under-the-hood panels using a new system in which two simulated robots, a drone and a ground vehicle, work as a team to fulfill a search-and-retrieve request by a person (Fig. 1). A human named Danny, who is located remotely, asks the team of robots a drone and ground vehicle (UGV) \u2013 to find keys that he lost in his apartment. Danny communicates with the UGV since it is serving as the robotic team leader, with the drone as its subordinate. The full dialog, which we'll walk through, is shown in Fig. 2.\nWhen Danny issues his request, both robots semantically interpret the input, resulting in text-meaning representations (TMRs) that are written in the ontologically-grounded metalanguage used for all agent knowledge, memory, and reasoning. This metalanguage uses unambiguous ontological concepts (not words of English) and their instances, described by ontologically-grounded properties and values. Ontological concepts are written in small caps to distinguish them from words of English, and their instances are indicated by numerical suffixes. The process of language understanding is complicated, as detailed in McShane and Nirenburg (2021).\nSearching each zone is a subtask of the plan FIND-LOST-OBJECT. After completing each subtask \u2013 i.e., searching each zone each robot reports to the other one about whether it was successful, which is driven by the COLLABORATIVE-ACTIVITY plan.\nWhen the team leader finds the keys, it ceases searching and first reports this to its subordinate and then to Danny. The trace of this reasoning is shown in Fig. 9. It uses different formulations for each of them because its language generation system (whose traces are not shown in this demo system) is designed to mindread its interlocutors and present information in the most useful way for them. Whereas these robots operate in terms of cardinal directions, making north of the couch a good descriptor, most humans prefer relative spatial terms like behind the couch."}, {"title": "4 CONCLUSIONS", "content": "Explainability is essential to critical applications, and in order for systems to be truly explanatory, they must first of all understand what they are doing. This requires that they be grounded in high-quality knowledge bases that optimally integrate causal and correlational reasoning.\nThis paper focused on explanation via traces of system operation using under-the-hood panels. The panels selected for this demo displayed the agents' interpretation of language inputs and visual stimuli, their reasoning, and their agenda. Much more could be shown if target users would find that helpful: the agents' ontologies, episodic memories, lexicons, decision-making about language generation, and so on. The under-the-hood panels do not attempt to capture unexplainables that are implemented using machine learning, such as what drives robotic movement or the robots' approach to searching a space.\nIn the current benchmark-driven climate, under-the-hood panels offer an alternative as a standard of evaluation.\nUnder-the-hood panels are just one mode of evaluation for LEIAs. The other primary one is language. The many things that a LEIA can explain using language are detailed in Chapter 8 of McShane, Nirenburg, and English.\nAlthough the theoretical, methodological, and knowledge prerequisites for explanation by LEIAs are quite mature, this doesn't mean that all problems associated with explanation are solved.\nConsider the example of physicians explaining relevant aspects of clinical medicine to patients, a capability that was relevant for the MVP clinician-training system mentioned above. The task has two parts: deciding what to say and how to say it. Both of these depend not only medical and clinical knowledge, but also on the salient features of patients, such as their health literacy (as hypothesized by the physician), their interest in medical details, their ability to process information based on their physical, mental, and emotional states, and so on. Identifying these salient features involves mindreading (Spaulding, 2020) \u2013 also known as mental model ascription. For example, an explanation may be presented in many different ways:\n\u2022 as a causal chain: \"You feel tired because of an iron deficiency.\"\n\u2022 as a counterfactual argument: \"If you hadn't stopped taking your iron supplement you wouldn't be feeling so tired.\"\n\u2022 as an analogy: \"Most people find it easier to remember to take their medicine first thing in the morning; you should try that.\"\n\u2022 using a future-oriented mode of explanation: \"If you take your iron supplement regularly, you should feel much more energetic.\"\nMoreover, explanations are not limited to speech; they can include images, videos, body language, live demonstration, and more. Overall, generating explanations tailored to particular humans is a difficult task. However, as with all other aspects of cognitive modeling, simplified solutions hold promise to be useful, particularly given the well-"}]}