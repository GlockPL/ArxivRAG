{"title": "Can Large Language Models Adapt to Other Agents In-Context?", "authors": ["Matthew Riemer", "Zahra Ashktorab", "Djallel Bouneffouf", "Payel Das", "Miao Liu", "Justin Weisz", "Murray Campbell"], "abstract": "As the research community aims to build better AI assistants that are more dynamic and person-alized to the diversity of humans that they interact with, there is increased interest in evaluating the theory of mind capabilities of large language models (LLMs). Indeed, several recent studies suggest that LLM theory of mind capabilities are quite impressive, approximating human-level performance. Our paper aims to rebuke this narrative and argues instead that past studies were not directly measuring agent performance, potentially leading to findings that are illusory in nature as a result. We draw a strong distinction between what we call literal theory of mind i.e. measuring the agent's ability to predict the behavior of others and functional theory of mind i.e. adapting to agents in-context based on a rational response to predictions of their behavior. We find that top performing open source LLMs may display strong capabilities in literal theory of mind, depending on how they are prompted, but seem to struggle with functional theory of mind even when partner policies are exceedingly simple. Our work serves to highlight the double sided nature of inductive bias in LLMs when adapting to new situations. While this bias can lead to strong performance over limited horizons, it often hinders convergence to optimal long-term behavior.", "sections": [{"title": "1. Introduction", "content": "Recently, generative AI, particularly in the form of LLM assistants, has been deployed as a tool for a growing variety of real-world use cases where the LLM must interact with a diverse set of people performing a diverse set of tasks. As typically deployed today, these LLMs are only interacting with users at inference time due to the significant compu-\n. Our work is inspired by the work of Akata et al. [2] to leverage canonical repeated games from behavioral game theory as a way to asses the adaptation ability of LLMs across the full spectrum of incentive structures. Unfortunately, we find significant deficiencies in the ability of top open source LLMs to adapt to new scenarios in-context, providing a sobering analysis of the ability of LLMs to reliably adapt without continual training of the model weights themselves.\nMany recent papers have been inspired by how humans are evaluated for theory of mind capabilities when evaluating theory of mind in LLMs [17; 46; 55; 95; 94]. While this does on the surface seem to like a logical course of action, it is important to remind ourselves that AI has a tendency to over optimize for its training objectives in a manner that is quite alien to the way the human brain works. For example, in Bubeck et al. [17] they tout performance of LLMs on human theory of mind tests, but then in the very next section note a noticeable lack of \"process consistency\" in LLM explanations. What this means is that LLMs can come up with compelling explanations for what they do that have very little to do with their actual reasoning process. We must therefor proceed with caution about our conclusions when we evaluate LLMs in a way that does not directly align with what we really care about. When we evaluate humans, we typically focus on what we term in this paper to be literal theory of mind, which is their ability to predict the behavior of other agents. As noted by Ma et al. [55] this can take the form of various abstractions of this behavior including actions, intentions, beliefs, percepts, desires, knowledge, and emotions. However, with humans we take for granted that this prediction of the behavior of other agents will be consistently applied into their own reasoning process when determining their own behavior. The main insight of our paper is that this process consistency cannot be taken for granted when evaluating LLMs.\nThe direction of our paper was inspired by an unexpected result in our early experiments testing behavioral game theory"}, {"title": "2. Related Work", "content": "Frameworks for Machine Theory of Mind. In the field of multi-agent reinforcement learning, theory of mind is generally interpreted as the ability to directly predict the behavior (i.e. the actions) that another agent will take [54; 72]. However, there can be a number of different ways to abstractly represent this behavior [55]. For example, abstraction can be applied in the temporal dimension representing behavior as a composition of hierarchical skills [98; 8; 79; 81; 4; 1] or goals [90; 6; 112; 38; 39]. Abstraction can also be applied at the agent level in order to represent the abstract actions of groups [61; 103]. Moreover, as showcased by the games Hanabi [9; 63; 58; 64] and Poker [27; 16], theory of mind can be directly related to inferring recursive [62] beliefs about unobserved information or knowledge. We consider the efficacy of predictions of all of these representations of behavior as examples of measuring literal theory of mind (Definition 3.1). However, it has been recently recognized in the human-computer interaction (HCI) literature that we must go even further to achieve mutual theory of mind for better collaboration with humans [104; 113]. In this framework, the mutual shaping of mental representations as well as the functional goal of improved performance on tasks are emphasized. We are inspired by this aspirational goal in our paper, leading us to define a functional measurement of theory of mind performance (Definition 3.2).\nMeasuring LLM Theory of Mind. While a number of recent studies have touted superior human-level LLM theory of mind capabilities, to the best of our knowledge these tasks have always center around passive question answering [17; 94; 46; 95] as typified by the classic Sally-Anne false-belief test [10]. LLMs have also been successfully employed to simulate a diversity of personas [70; 28], trust behaviors [109], or even for macro-economic simulations [50]. However, these tasks all lack interactivity and only reflect strong performance at literal theory of mind (Definition 3.1). On the other hand, Kim et al. [43] found that LLMs perform poorly when subjected to multiple question types that demand the same consistent underlying reasoning. This finding is in the same spirit and complimentary to the finding of our paper as well as the finding that LLMs lack process consistency [17]. Moreover, LLMs have been found to fail at important interactive applications such as adaptive eduction for users of diverse age or education levels [86] and providing coding assistance for beginner programmers [65]. Our aim in this paper is to formulate functional theory of mind (Definition 3.2) to describe the set of tasks where current LLM theory of mind capabilities tend to fall short.\nBehavioral Game Theory with LLMs. Our paper builds off the work of Akata et al. [2] who considered LLMs playing the Prisoner's Dilemma and Battle of Sexes as we do. We closely mimic the prompting style and experimental"}, {"title": "3. Building and Evaluating Multi-Agent RL Policies with In-Context Learning", "content": "The Environment. The interaction process between multiple agents is often formalized as a stochastic game [92] or rather a Markov game in the fully observable setting [52]. For generality in this work, we rather consider the setting of Decentralized partially observable Markov decisions processes (Dec-POMDPs) [13] that generalize POMDPs [33] and MDPs [71] to the multi-agent and decentralized setting. A Dec-POMDP is defined by the tuple $\\langle I, S, A, T, R,X,O,T \\rangle$. Here I is a finite set of agents, S is a finite set of global states, and $A = \\times_{i \\in I}A^i$ is the set of joint across agents $i \\in I$. $T : S\\times \\bar{A} \\rightarrow S$ is the state transition function based on the joint actions across agents, and $R = \\times_{i \\in I}R^i$ is the joint reward function with reward function $R^i : S\\times A \\leftrightarrow \\mathbb{R}$ for each agent $i \\in I$. $X = x_{i\\in I}X^i$ is the joint set of observations with $X^i$ denoting a finite set of observations for each agent. $O : S\\leftrightarrow \\mathcal{E}$ is the function that produces observations for each agent based on the state. Finally, T is the horizon of interactions before termination.\nAgent Interaction. We can now consider environment interaction at each step from the perspective of a given focal agent $i \\in I$ where we will use $-i := 1 \\backslash i$ to denote set of all other agents. At step t agent i takes action $a^i_t$ and the other agents take a joint action $a^{-i}_t$ yielding a transition from $s_t$ to $s_{t+1}$ with probability $T (s_{t+1}|s_t, a_t)$ where $a_t = \\{a^i_t, a^{-i}_t \\}$. The agents then receives its own observation of this state $x^i_{t+1}$ and reward $r^i_t$. In the environments we consider in our paper it is assumed that $x^i_{t+1} \\in x$ also contains information about the actions of the other agents at the previous step $a^{-i}_{t-1}$. However, this need not always be the case. In such a POMDP, each agent's policy $a^i_t \\sim \\pi^i(h_t)$ generates its action stochastically based on its own interaction history $h_t$ where $h_t:= \\{x^i_1,a^i_1,r^i_1,..., x^i_t \\}$. Note that policies being defined in this way subsumes the common case where part of the history is discarded for computational or memory efficiency.\nLLM History Representations. As we are interested in evaluating LLMs for this problem it is additionally assumed that this history representation h must be encoded in text (i.e. LLM tokens). This implies that there must be a function that we have direct access to converting the observations x, actions a, and rewards r to the form of LLM token representations. Moreover, it is important to note that we are interested in learning generalist policies across tasks with LLMs. As a result, following the protocol of Akata et al. [2], it can also be useful to include any information about the environment tuple $\\langle I, S, A, T,R,X,O,T \\rangle$ to the agent to promote rapid adaptation to new tasks."}, {"title": "3.1. Disentangling Literal Theory of Mind from Functional Theory of Mind Evaluation", "content": "The goal of any individual agent i interacting in an environment for T steps is to learn a policy $\\pi^{i*}$ that maximize its expected reward given the policies of the other agents in the environment $\\pi^{-i}$ starting from state $s_1$:\n$\\pi^{i*}= arg max_{\\pi^i} \\mathbb{E}_{T \\\\ s_1, \\\\ pi^{-i}} \\[ \\sum^T_{t=1} \\pi \\]$\nBecause the best course of action $\\pi^{i*}$ is critically dependent on the policies of the other agents $\\pi^{-i}$, it is common for agents to directly learn a model of the other agent's policies $\\pi^{-i}$ in decentralized settings. This is analogous to model-based RL [97; 15; 91] and is useful for stabilizing learning [54]. However, it is worth noting that it is not a strict requirement for representing $\\pi^{i*}$ that is optimal in functionality [96]. That said, it is tempting to consider when using LLMs as it can be generated by simply prompting the model with a token representation of the interaction history $h_t$ for $j \\in -i$. We can then model performance of any particular approximate literal theory of mind model using Definition 3.1.\nDefinition 3.1 (T-Step Literal Theory of Mind Loss). The loss from start state $s_1$ with respect to a joint policy $\\pi^{-i}$ that generates T actions $a^{-i}_1,...,a^{-i}_T$ of its approximation $\\hat{\\pi}^{-i}$ that generates T actions $\\hat{a}^{-i}_1,...,\\hat{a}^{-i}_T$ is:\n$\\mathcal{L}_{Literal}(s_1,\\hat{\\pi}^{-i}, \\pi^{-i}, T)=\\mathbb{E}\\big[ D(\\phi(a^{-i}_1,..., a^{-i}_T), \\phi(\\hat{a}^{-i}_1,..., \\hat{a}^{-i}_T)) \\big]$\nwhere D is some distance function and $\\phi$ is some abstraction mapping function over actions.\nWe phrase Definition 3.1 in terms of an abstract distance function D and abstraction mapping function $\\phi$ to keep the definition as broad as possible and encompass as much as we can of the existing literature on theory of mind. Primitive actions can definitely be directly considered (as we do in this paper). In this case, $\\phi$ is the identity mapping and D is set to a percent error metric. $\\phi$ can also be straightforwardly set to various temporal abstractions of actions given sufficient history length T. Moreover, latent features that impact the behavior of agents such as intentions, beliefs, percepts, desires, knowledge, and emotions can also be considered as alternative for $\\phi$ - as an inverse mapping must exist. However, the agent's actions alone may not provide enough information, for example, to fully determine an agent's emotional state, we can only judge the theory of mind performance of an agent in terms of the information provided. As such, Bayesian reasoning may be the desired outcome in the presence of lack of information, so this does not serve as meaningful limitation of this definition.\nOne obvious issue with Definition 3.1 is that it is a function of a theory of mind model $\\hat{\\pi}^{-i}$, which is not even necessarily needed to express the policy of an agent $\\pi^i$. For cases where the value of $\\hat{\\pi}^{-i}$ is decoupled from the reasoning involved in $\\pi^i$, it is of particular importance to consider a functional metric of the degree to which the policy $\\pi^i$ is catered to the particular other agent policies $\\pi^{-i}$. We can define this metric in terms of the T-step regret incurred by $\\pi^i$.\nDefinition 3.2 (T-Step Functional Theory of Mind Regret). The T step regret from start state $s_1$ of policy $\\pi^i$ that receives individual rewards $r^i_1,...,r^i_T$ when playing with policy $\\pi^{-i}$ in comparison to the optimal policy $\\pi^{i*}$ that plays the expected optimal T-step response to policy $\\pi^{-i}$ to receive rewards $r^{i*}_1,...,r^{i*}_T$:\n$\\bigtriangleup_{Functional}(s_1, \\pi^i, \\pi^{-i}, T)= \\sum^T_{t=1}(r^{i*}_t - r^i_t)$\nDefinition 3.2 provides us with a functional metric for measuring theory of mind in the presence of other agents parameterized by $\\pi^{-i}$. However, it is still worth considering when the conclusions from this metric will be much different than conclusions from a literal theory of mind metric following Definition 3.1. To do this we must define a new policy $\\pi_{ToM}$ that is directly based on the literal theory of mind model prediction $\\hat{\\pi}^{-i}$ - such that behavior is chosen to be a rational maximization of the action-value function. In our experiments we simulate such a policy utilizing ground truth knowledge of the payoff structure and use $\\bigtriangleup_{ToM}(s_1, \\hat{\\pi}^{-i}, \\pi^{-i}, T)$ to denote the regret of such a policy. $\\bigtriangleup_{ToM}$ thus represents the best regret that can be possibly achieved by faithfully following the predictions of our theory of mind model under the assumption that the predictions are correct.\nMetrics for our Experiments. In our experiments, we aim to get a wholistic view both the literal theory of mind and functional theory of mind performance of each LLM and prompting strategy. We report the accuracy of the literal theory of mind predictions with respect to individual actions as ToM %. We also report the regret per step functionally achieved by each policy as $\\bigtriangleup Functional/T$. Finally, to get a clearer picture of the difference between the literal theory of mind performance and functional theory of mind performance, we report $\\bigtriangleup ToM/T$ the regret per step of the rational policy based on the literal theory of mind model."}, {"title": "3.2. Initial Results", "content": "We begin by conducting experiments on the Rock, Paper, Scissor domain as discussed in the introduction. LLAMA-2 had previously been found to achieve strong performance on repeated matrix games [53], so we considered initial experiments with the full LLAMA-2 family [102] as well as the competitive Falcon 40B [5] and Mixtral models [32]."}, {"title": "4. Further Analysis: LLMs Playing Matrix Games with Simple Partner Strategies", "content": "In this section, we attempt to gain a further understanding of this surprising result. Rocks, Paper, Scissors (RPS) is a canonical competitive game as can be seen by the payoff table in Figure 5. We also wanted to evaluate our models on a canonical cooperative game, for which we chose the Iterated Battle of Sexes (IBS) following Akata et al. [2] with a payoff table detailed in Figure 6. Once again following Akata et al. [2], we also evaluate our models on the famous Iterated Prisoner's Dilemma (IPD) mixed incentive game with a payoff table detailed in Figure 7. We opted to test our each model on the full spectrum of incentive structures to rule out explanations for performance lacking in competitive games such as an intrinsic altruism bias in LLMs (as has been previously suggested by Leng & Yuan [49])."}, {"title": "4.1. Different Prompting Strategies", "content": "We wanted to first take a deeper look at the impact of prompting strategy on performance. We will henceforth call the prompting strategy used in the previous section LM Prompting see Figure 2) where the probability of each action in the action space is explicitly drawn from the model to generate the next action via next token prediction. See Figure 3 for the literal theory of mind version of the prompt. In QA Prompting (Figures 5 and 6), decision making is posed as a question answering problem where the LLM keeps generating stochastic outputs until they are in the desired format and action space vocabulary. This is implemented as a step along the way to Chain of Thought (CoT) reasoning [106] that we refer to as CoT Prompting (Figures 7 and 8) in which the LLM must generate a reasoning process and its own answer based on that reasoning process in the correct format. Finally, we consider Social Prompting as proposed for IBS by Akata et al. [2] where the LLM first generates a prediction of the other agent's action and then conditions its reasoning based on that action. This approach is detailed in Figure 14"}, {"title": "4.2. Reasoning Over Long Contexts", "content": "Due to the surprising result of Social Prompting still experiencing a significant functional theory of mind gap, we wanted to understand more about the difficulty these LLMs may experience when reasoning over a long context. We now consider the LLAMA-3 70 Instruct model [23] for its superior performance over long context tasks. We now also include Oracle Prompting where the actual action the partner will take (and not just a prediction) is directly provided as input (Figure 11). As well as Oracle + Max Prompting where maximizing the reward in response to their action is further emphasized. We additionally consider a variant where the interaction history or the payoff table are removed from the prompt. Finally, we added a variant of CoT Prompting where three in-context examples of ideal thought processes are provided (Figure 9) that we call CoT 3-Shot.\nIn Table 3 we provide the regret per step of each prompting strategy across the three games. LLAMA-3 seems to consistently outperform LLAMA-2, but even still Social Prompting does not close the gap with the tabular model. Meanwhile, CoT leads to big improvements sometimes, but is inconsistent, making performance worse for same cases."}, {"title": "4.3. Coordinating with Simple Adaptive Policies", "content": "Our main aim is to build towards LLMs that display mutual theory of mind capabilities [104] in which theory of mind is used to foster coordination behavior between agents. As such, the single action agents may not be realistic as they do not shift their actions in response to the LLM agent's actions. In Table 4 we test the prompting strategies we have considered previously, but now playing with tit for tat style strategies [7] made famous for the efficacy in IPD. Analogously, in RPS we always play the best response to the other agent's action at the last step and in IBS we always play the same action that the other agent did at the last step. We also add the very strong Mistral Large 2 model to match LLAMA-3 models but within the Mistral family. In most cases the LLMs perform quite poorly. All models are again much worse than the tabular model with the exception of"}, {"title": "4.4. The Good and Bad of Inductive Bias", "content": "So far in this section we have used the neutral action representations J, F, and B to avoid contamination following prior work [14; 2]. However, it is interesting given our results so far to get a better sense of the degree that prior knowledge in the LLMs impacts performance and the choice of action representation is great way for us to control the extent that this knowledge is evoked. For the best performing models LLAMA-3 70B Instruct and Mistral Large 2, we conduct experiments for all three games playing with both single action and tit for tat style partners. We provide comprehensive results for functional theory of mind performance in Figures 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, and 37, and for literal theory of mind performance in Figures 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, and 38. Here we compare neutral actions with the actual canonical action names for the game, the neutral actions with 20 repetitions to take up a longer portion of the context, and nonsense word actions (see Appendix B). We generally find that both LLM models experience systematic bias that prevents them from converging to optimal performance as the number of interactions grow. Indeed, only LLAMA-3 playing IBS with tit for tat partners (Figure 23) seems on the road to slow convergence. Meanwhile, literal theory of mind seems to be converging in a greater number of settings (Figures 24, 28, 30, and 32). We find that real actions often help with functional theory of mind performance early in the interaction stream while"}, {"title": "5. Conclusion", "content": "In this paper, we have tried to make sense of the ongoing discourse about theory of mind with LLMs. We defined two types of theory of mind that we call literal theory of mind (Definition 3.1) and functional theory of mind (Definition 3.2), concluding that functional theory of mind presents the most pressing problem for modern LLMs. We demonstrate that modern LLMs cannot display function theory of mind in simple matrix games with simple partner policies and there is a big gap with what would be expected based on rational reasoning based on its prediction of what the other agents will do. We hope that our analysis will inspire researchers evaluating the theory of mind capabilities of LLMs to consider more interactive evaluation procedures. Current procedures developed for evaluating humans have presented a misleading picture about the status of LLM capabilities for tasks where multi-agent coordination is paramount."}, {"title": "A. Extended Related Work: Continual Learning and Applications", "content": "In-Context Learning vs. Continual Learning. In a multi-agent environment, the environment is nonstationary from the perspective of each agent if the policies of the other agents it interacts with are changing or learning over time [52]. As a result, the setting of rapid adaptation to new agents in our paper can be considered a special case of continual RL (see Proposition 3 of Khetarpal et al. [37]). Proposition 2 of Khetarpal et al. [37] established that all continual RL problems can be modeled as partial observable problems with agents conditioned on the full interaction history. So, in principle, in-context learning over sufficiently long context representations with sufficiently expressive neural networks can be considered a general solution to continual learning problems. However, it remains to be seen if it is possible to see in-context behaviors resembling continual learning strategies like reservoir sampling replay buffers [78] or scalable memory efficient approximate buffers [76; 80; 12]. It would be interesting to also consider policy changes from step to step and their degree of correspondence with older models similar to work leveraging knowledge distillation for continual learning [51; 77; 44]. Intuitively, chain of thought reasoning may be beneficial because of its similarity with continual learning approaches that can select which layers to process at inference time [87; 18; 19; 88], see [89] for a survey of approaches. More generally, this adaptive computation problem can be formalized within the coagent networks framework [101; 47; 114]. That said, the compositional generalization needed for utilizing these models in practice makes achieving real-world success very challenging [45], potentially making LLM style training a more viable strategy for learning this composition. That said, a word of caution comes from recent theoretical insights about the difficulty of efficiently evaluating models with large context lengths [83] acting in complex environments [82]. This could be a leading reason for the poor performance we are currently seeing with LLMs. Ultimately, there are many potentially interesting continual learning settings that should be considered for a comprehensive evaluation [66], which we leave to future work.\nAlternatives to Continual Training. While our work highlights the limitations of using in-context learning for continual learning with current LLMs, full fine-tuning or continual training can be quite expensive in the context of LLMs \u2013 especially when it must be done for each end user. However, more computationally efficient alternatives like parameter efficient tuning [30; 21; 99] or model merging [31; 111; 3; 100] may constitute a more economical middle ground. We leave exploration of the comparative efficacy of these strategies to future work. It is important to note that neural scaling laws only demonstrate generalization improvements with bigger model sizes when there is access to as much data as needed [34]. In fact, capacity limits have been found to enhance generalization for RL [57; 60] and multi-agent RL [56; 59; 58] in the limited data regime. Smaller models also may be necessary to maintain performance in realtime environments that are sufficiently stochastic [85; 84]. As such, it may make more sense to consider smaller LLMs if we plan on doing user specific finetuning or auxiliary objectives we would like to maintain like moral values [68; 22].\nApplicability Beyond Multi-agent RL. In this paper, we focused on multi-agent RL environments because of our use of this formalism in Definitions 3.1 and 3.2. However, the principles discussed in this paper should apply to many practical domains that are not typically modeled using RL such as biomedical applications [20], making decisions based on conversation topics across the internet [74; 29; 73; 36], learning what information to teach other models [67], and making decisions based on incoming internet data [75]. These problems can generally be considered a special case of the RL formalism [11]."}, {"title": "B. Experiment Details", "content": "For our experiments we ran each LLM model using the Watsonx API playing a sequence of 100 step episodes with selected policies for the other agents over the course of 24 hours or a maximum of 100 episodes. Every model was evaluated for at least 30 episodes and confidence intervals are based on the actual sample size considered for each LLM model. The payoff tables used for each game are provided in Tables 5, 6, and 7. We also considered the following representations for each action as indicated in the main text:\n\u2022 RPS\n\u2022 IBS\naction: R, J, JJJJJJJJJJJJJJJJJJJJJJ, Rock, Pasta\naction\u2081: P, F, FFFFFFFFFFFFFFFFFFFFF, Paper, Rice\naction\u2082: S, B, BBBBBBBBBBBBBBBBBBBBB, Scissors, Bread\naction: J, JJJJJJJJJJJJJJJJJJJJJJ, Fight, Pasta"}, {"title": "B.1. Example Prompts", "content": "In this section, we provide example prompts for each prompting type. In all cases, we tailor our provided example to the Iterated Battle of Sexes (IBS) with actions J and F in which the agent happens to be playing round 5 of 100. There is a slight change in terminology for Rock, Paper, Scissors (RPS) where it is said that the agents \"receive a score of +1/0/-1\" rather than that they \"win 10/8/7/5/0 points\". This is just to accommodate for a negative scale of rewards in this game as we did not feel having the reward as always positive would properly reflect the incentive structure of the game to the LLMs."}, {"title": "B.2. Additional Experiments on Action Space Inductive Bias", "content": "Partner\nactiono action1\nYou\nactiono (8,8) (0,10)\naction1 (10, 0) (5,5)"}]}