{"title": "The Interpretability of Codebooks in Model-Based\nReinforcement Learning is Limited", "authors": ["Kenneth Eaton", "Jonathan Balloch", "Julia Kim", "Mark Riedl"], "abstract": "Interpretability of deep reinforcement learning systems could assist operators with\nunderstanding how they interact with their environment. Vector quantization\nmethods-also called codebook methods-discretize a neural network's latent space\nthat is often suggested to yield emergent interpretability. We investigate whether\nvector quantization in fact provides interpretability in model-based reinforcement\nlearning. Our experiments, conducted in the reinforcement learning environment\nCrafter, show that the codes of vector quantization models are inconsistent, have no\nguarantee of uniqueness, and have a limited impact on concept disentanglement, all\nof which are necessary traits for interpretability. We share insights on why vector\nquantization may be fundamentally insufficient for model interpretability.", "sections": [{"title": "Introduction", "content": "Although deep neural networks have advanced the performance of reinforcement learning (RL)\nagents, these \"black box\" models give little insight as to the agent's decision making and learned\ntransition models. Interpretability of RL agents is important in high-stakes domains that demand\nhuman trust, such as autonomous vehicles and infrastructure. Interpretability is also crucial to\nanalyze behavior and develop adaptations when trained agents make mistakes or experience novel\nchanges to their environment.\nThere has been significant recent interest in vector quantization (VQ) in neural networks, largely\ndriven by the work of Van Den Oord et al. (2017); the authors show that VQ latent space dis-\ncretization in generative models helps regularize the latent space, thereby improving performance.\nAlthough not claimed by Van Den Oord et al. (2017), several subsequent works suggest that the\nlatent disentanglement caused by VQ enables greater semantic interpretability of neural models (Hsu\net al., 2017; Tamkin et al., 2023; Aloufi et al., 2020).\nWe examine whether VQ within model-based reinforcement learning (MBRL) captures semantic\ninformation about entities in the environment. Specifically, we use the IRIS (Micheli et al., 2022)\nMBRL agent, which quantizes the latent vector encoding of the world state and uses a transformer\nto model transition dynamics. Grad-CAM (Selvaraju et al., 2017) is used to perform qualitative\nand quantitative analysis to determine whether codes are consistent and represent semantically\ngroundable entities, such as objects, across diverse inputs.\nWe find VQ lacks the necessary constraints to enforce that disentangled codes correspond to semantic\nentities; codes are not consistent enough to be grounded to a semantic concept. Based on our findings,\nwe disprove the intuition that VQ alone is sufficient for interpretability and hypothesize that latent\nsemantic alignment is needed alongside discretization to make latent spaces generally interpretable."}, {"title": "Background and Related Work", "content": "In MBRL, the agent learns a model to approximate the transition dynamics of the environment (Sut-\nton & Barto, 1998). An agent can benefit from having a good predictive model of the future"}, {"title": "Experiments in VQ Interpretability", "content": "Experimental Approach. In our work, we use the state-of-the-art MBRL model IRIS (Micheli\net al., 2022) featuring a codebook with 512 codes. We make no modifications to the IRIS architecture.\nTo evaluate interpretability, we apply Grad-CAM to the code inputs of the decoder. Grad-CAM\nscores importance of input image pixels based on the activations and gradients of a selected weight\nvector or layer. We apply this process individually for each code in the model by masking out the\nother codes' values. The end result is a heatmap for each code that overlays the input image with\nthe regions that were most influential in that code being selected.\nTo measure the quantitative properties of the learned codes, we evaluate the code consistency over\nagent observations and episodes by looking at the areas of high Grad-CAM activation for the same\ncode. We split images into connected components of high activation and filter out those with an\narea smaller than a threshold. The remaining connected components are used to crop the original\nimages to the bounding box region they encapsulate. We use a frozen pre-trained model to compare\nthe cropped inputs and gauge their semantic similarity.\nWe conducted our experiments in Crafter (Hafner, 2021), an open-world survival game designed for\nRL with similar dynamics to Minecraft. The model and corresponding policy was first trained to\nconvergence. Then we collected a dataset of 127,434 (ot, St, At, St+1, Ot+1) transitions of the agent\nacting in the environment over 714 episodes, where o is the input observation, a is the selected\naction, and s are the latent states. We applied our interpretation approach to investigate different\ncodes correspondance to the inputs on newly collected runs, and saved the Grad-CAM activation\nheatmaps for every code the model selected in each observation.\nWe found that 90% of the heatmaps contained all zero values. Since all the values being zero is\nnot useful for our interpretability evaluation, those samples were dropped. 477 of the 512 codes had\nnon-zero heatmaps, equating to 205,231 heatmaps for the final dataset.\nResults. Table 1 displays eight heatmaps overlaid on their observation image, randomly sampled\nfor each code, from the most frequently occurring code in our dataset, the tenth most frequently\noccurring, and the median occurring. We chose these codes based on their frequency to avoid biasing\nthe results and capture diverse regions of the frequency distribution, as discussed further in Appendix\nA. In general, the heatmaps from a code focus on the same regions of the image, especially for the\nmore frequently occurring codes. To examine the content the codes focus on, in Table 2 we show the\ncrops resulting from the heatmaps. To display results for more codes while still focusing on diverse\nfrequencies, we show crops for the second most frequent code, eleventh most frequent, and the next\nmost frequent code after the median code used earlier. In the crops, code 46 is consistently focused\non numbers indicating agent inventory, while the other two have several similar crops but no clear,\nsingular focus. Therefore, we conclude codes did not consistently associate with objects or concepts.\nAdditional examples of crops are presented in Appendix B.\nTo quantify code interpretability, we compared the embeddings for each crop from the last hidden\nlayer of a pre-trained, frozen ResNet50 (He et al., 2016) model trained on ImageNet-1k (Russakovsky\net al., 2015). Given these crop embeddings, we calculated the mean embedding of each code and"}, {"title": "Discussion", "content": "As reported above, the model learned a limited number of codes that capture entities such as\ngrass, water, and resources in the inventory. In Appendix C, we examine co-occurring codes to\nfind additional interpretable examples occurring in superposition Elhage et al. (2022), however,\nthey occur very sparingly. Outside of these instances, the codes lacked a consistent semantic theme.\nThus, VQ alone cannot provide interpretability of transition models in MBRL. One of the reasons we\nbelieve this happened is that VQ offers insufficient constraints to enforce semantic disentanglement of\ncodes. Specifically, the model is always trained to reconstruct the entire image using a combination\nof codes, and due to the depth of the CNN in our model, the latent vectors have all seen the entire\nimage. This creates a condition where there is no need for the model to cleanly isolate entities in\nthe image to a single code. Thus, indicating there are characteristics of discretization that matter\nto ensure semantic associations, a deeper study of which is left for future work."}, {"title": "Code Frequency", "content": "In order to decide which codes would be the most beneficial to examine for consistency, we organized\nthem by how frequently they occur in the dataset. As seen in Figure 3, the model is not overly reliant\non any codes because the most frequent one occurs in barely over 1%. However, there is a significant\ndropoff between the top ten codes and the rest, which is the reason we presented heatmaps and\ncrops with three distinct frequencies. The top two codes occur significantly more than the tenth and\neleventh codes. The median codes then occur at a much lower rate. We chose to focus on the higher\nfrequency end of the distribution because we believe even if the low frequency codes are consistent,\nthey will not occur enough to provide useful interpretability, since in many observations, none of\nthose codes will appear."}, {"title": "Consistent and Inconsistent Code Crops", "content": "In Table 3, we show additional cropped images randomly sampled from six codes. The top three\nshown are examples of the more consistent codes we observed. The first one focuses on stone, while\nthe second captures grass, and the third focuses on water. Throughout our entire dataset of 477"}, {"title": "Code Co-occurrence", "content": "One of the potential reasons for the lack of interpretability of codes is that the model learned a\ncombination of codes to capture entities. In order to investigate this, we found the codes that most\nfrequently appear together and calculated their co-occurrence rate. We define co-occurrence rate as\nthe number of appearances together divided by the average number of times the two codes are used.\nThis is related to the mechanistic interpretability concept of superposition Elhage et al. (2022),\nwhere given a latent feature space represented by a set of d-dimensional vectors v, optimization\ntries to use these vectors to represent more features than they have the dimensional capacity. This\nleads to phenomena such as representing concepts with groups of features, or individual features\nalternately representing more than one concept depending on the input, both of which we observe\nin our work Elhage et al. (2022); Nanda (2022). Figure 4 shows a heatmap of the rates for the ten"}]}