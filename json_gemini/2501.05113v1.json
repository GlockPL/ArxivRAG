{"title": "Constrained Optimization of Charged Particle Tracking with Multi-Agent Reinforcement Learning", "authors": ["Tobias Kortus", "Ralf Keidel", "Nicolas R. Gauger", "Jan Kieseler"], "abstract": "Reinforcement learning demonstrated immense success in modelling complex physics-driven systems, providing end-to-end trainable solutions by interacting with a simulated or real environment, maximizing a scalar reward signal. In this work, we propose, building upon previous work, a multi-agent reinforcement learning approach with assignment constraints for reconstructing particle tracks in pixelated particle detectors. Our approach optimizes collaboratively a parametrized policy, functioning as a heuristic to a multidimensional assignment problem, by jointly minimizing the total amount of particle scattering over the reconstructed tracks in a readout frame. To satisfy constraints, guaranteeing a unique assignment of particle hits, we propose a safety layer solving a linear assignment problem for every joint action. Further, to enforce cost margins, increasing the distance of the local policies predictions to the decision boundaries of the optimizer mappings, we recommend the use of an additional component in the blackbox gradient estimation, forcing the policy to solutions with lower total assignment costs. We empirically show on simulated data, generated for a particle detector developed for proton imaging, the effectiveness of our approach, compared to multiple single- and multi-agent baselines. We further demonstrate the effectiveness of constraints with cost margins for both optimization and generalization, introduced by wider regions with high reconstruction performance as well as reduced predictive instabilities. Our results form the basis for further developments in RL-based tracking, offering both enhanced performance with constrained policies and greater flexibility in optimizing tracking algorithms through the option for individual and team rewards.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) and multi-agent reinforcement learning (MARL) are promising paradigms for constructing and optimizing autonomous agents that can compete in a wide variety of complex sequential decision problems such as games [1], [2], robotics [3], [4] or autonomous driving [5] by discovering complex interaction mechanisms in the underlying environment. Coupled with the tremendous success in the aforementioned fields, RL has recently demonstrated great potential in optimizing and controlling physics processes [6], [7], [8], [9], by maximizing a scalar reward signal using trial and error [10], [11]. Especially for problems of combinatorial nature, RL demonstrated to be able to learn generalizable policies that are even able to outperform supervised learning approaches, despite the lack of ground truth information [12]. Kortus et al. [9] and V\u00e5ge [8] have shown for charged particle tracking used in high-energy physics reconstruction, the potential of deep reinforcement learning for optimizing over discrete assignment operations, aiming to construct discrete sets of particle tracks over subsequent layers under the influence of particle interaction mechanisms. Extending previous work, we further investigate the concept of RL-based charged particle tracking as a combinatorial optimization problem. We therefore propose a collaborative MARL approach with assignment constraints, iteratively optimizing a joint policy of multiple track follower. We represent the stepwise agent constraints as a centralized safety layer, ensuring unique hit assignment across all agents, both during training and inference, by solving a linear sum assignment problem (LSAP) projecting the unsafe local agent policies to a global safe policy. All source code together with hyperparameters, data, and models are available on GitHub\u00b9 and Zenodo\u00b2. Our main contributions and findings in this paper summarize as follows:\n\u2022 Building upon previous work in [9], we propose multiple multi-agent extensions of RL-based particle tracking, using decentralized agents with optional safety layer, satisfying assignment constraints, trained in a centralized manner using centralized critic architectures.\n\u2022 Increasing the cost margins between predictions and decision boundaries efficiently, we extend the blackbox differentiation technique by [13] by an additional simple gradient component, resulting in significantly improved training and generalization abilities.\n\u2022 We demonstrate excellent empirical performance of our method, compared to a conventional track follower [14] as well as single-agent [9] and multi-agent baselines.\n\u2022 Finally, we validate the benefit of the architecture and adapted gradient through the safety layer by examining reconstruction performance, reward surfaces [15], prediction instabilities [16], and policy entropy."}, {"title": "II. THEORY AND BACKGROUND", "content": "Throughout this work, we focus on particle data generated by the digital tracking calorimeter (DTC) prototype developed by the Bergen pCT Collaboration [17], [18] for proton computed tomography. In the following section, we describe both the detector and the basic particle interaction mechanisms expected at relevant particle energies of O(230 MeV).\na) Bergen pCT detector prototype: The Bergen pCT DTC is a multi-layer pixelated tracking calorimeter, consisting in total of two tracking layers and 41 detector-absorber sandwich calorimeter layers. It uses multiple strips of ALPIDE pixel sensors [19], [20] with additional 3.5 mm aluminum absorbers in each calorimeter layer, for measuring and reconstructing particles stopping in the detector. Further details and a fine-grained decomposition of the detector material is described in [17]. While the exact composition of the detector is not essential for our work, we want to point out the different material budgets of the tracking and calorimeter layer. Both components are used in combination for accurate estimation of the incoming particle direction and the stopping of the particles for energy estimation respectively, resulting in different particle interaction behavior.\nb) Particle interactions and tracking: Accelerated charged particles undergo numerous complex interactions with the matter traversed [21]. In proton imaging, charged particles are mainly influenced by Coulomb interactions with atomic electrons, decelerating the particle, as well as nuclei, randomly deflecting the particle from its straight path [22], [21]. Additionally, on some occasions, particles undergo complex inelastic interactions with the atomic nucleus in a destructive process where the original primary particle is absorbed, and new particles are created. Due to its highly stochastic nature, secondary tracks cause additional complexities during reconstruction and are unusable for imaging. To recover usable characteristic properties of the particles, tracking algorithms aim to model or learn the pattern of the particle in the detector readouts under the influence of the inherent interaction mechanisms, aiming to reconstruct full particle trajectories."}, {"title": "III. RELATED WORK", "content": "a) Particle tracking: While early particle tracking algorithms heavily relied on conventional algorithms such as iterative [23], [14], evolutionary [24] or combinatorial [25] approaches, modern tracking solutions heavily utilize machine learning to tackle the increasing combinatorial explosion due to increasing particle counts. Especially geometric deep learning, operating either on node [26], [27] or edge level [28], [29] of graph representations, demonstrated to be highly effective. Aiming to combine advantages from conventional tracking and deep learning, recent work on RL-based tracking demonstrated both on discrete- [9] and continuous action spaces [8], the ability to learn reconstruction policies by interacting with an environment. Our work extends the mechanisms in [9] to a multi-agent setting.\nb) Safe/Constrained Reinforcement Learning: Learning safe policies, operating under safety or functional constraints, is an emerging research field, both in single and multi-agent reinforcement learning. For this work, we focus on state-wise safety by constraining the set of feasible policies. Our work is closely related to the idea of safety layers and shielding. [30], [31] and [32], proposed the usage of an implicit layer that performs action correction of the policy using a linearized version of the constraint function. Similarly, [33] and [34] proposed the usage of safety editors, restricting the agent to safe actions, by either reducing the safe action space or correcting unsafe actions of the policy."}, {"title": "IV. METHODOLOGY", "content": "In the following, we outline a general notion of constrained and unconstrained collaborative charged particle tracking, extending existing work in [9], and propose multiple agent architectures for the centralized training for decentralized execution (CTDE) paradigm [35]. Finally, we describe training schemes for both unconstrained and constrained MARL, highlighting the task-specific modifications and challenges.\nA. Problem Statement\nWe formulate multi-agent particle tracking over multiple layers of discrete particle readout data as a decentralized partially observable Markov decision process (Dec-POMDP) [36], operating on a directed acyclic graph. Here, S is a set of global (unobservable) environment states describing the current local trajectories of all agents. Instead of perceiving the global environment state, each agent can only draw individual local observations $o^{(i)}_t \\in O$, defined by the last reconstructed partial track segment, and all possible next segments $O^{(i)}_t = \\{v_t, e_{t-1,t}\\} \\cup \\{v_{t+1,j}^{(i)}\\}\\cup \\{U_t, e_{t,t+1}\\}$. Each agent can select, based on its perceived observation, from a set of actions defined by the set of next hit candidates, which we treat later on either as unconstrained or constrained (by unique assignments). For each interaction, all agents receive a scalar reward signal $r_t$, accumulated until a terminal state triggered by the absence of a valid action as the end of the detector, is reached.\na) Graph construction: Following the parametrization of particle readouts described in [9], we model the particle data, as a directed acyclic graph (hit graph), where each hit represents a vertex in the graph. Edges are generated between hits of adjacent layers, opposite to the direction of the particle. Both, vertices and edges are parametrized by a set of features $v_i = [\\Delta E, x, y, z]$ and $e_{ij} = [r_{ij}, \\theta_{ij}, \\phi_{ij}]$, defining the energy deposition and position of the hit with one-hot encoded layer index as well as the spherical coordinates of the edge connections. Finally, we employ the feature normalization scheme of [9], compensating for the beam position in the detector, providing translation invariant features.\nb) Sampling of track candidates: Track candidates are constructed for a hit graph, starting from all initial unoccupied graph vertices in the last detector layer, by iteratively adding new vertices in subsequent layers, until a terminal state is reached. Unassigned vertices in subsequent layers are incrementally added to the list of track candidates. To provide a starting track segment, functioning as an initial local observation, we rely on ground-truth seeding [9], avoiding unwanted dependencies of seeding algorithms on the performance of the proposed algorithms and providing a performance upper bound of RL-based tracking.\nc) Objective: We attempt to find, by repeatedly interacting in the described environment, generating sampled track candidates, a joint policy, that collaboratively maximizes the gathered expected discounted return under a shared team reward. Similar to [9], we aim to optimize the reconstruction policy by minimizing the average amount of particle scattering in a readout frame over all agents. We thus define the reward signal as the negative average scatter angle obtained for each transition in the graph. In the multi-agent case, we rely on this naive description over the more detailed modelling of the energy dependent scattering behavior [37], described in [9], to remove the dependence of the reward signal on full track candidates, making it more suitable for off-policy algorithms."}, {"title": "B. Architecture and Implementation", "content": "In this section, we describe extensions to the existing attention-based agent parametrization [9], providing both a permutation invariant and action size independent processing. Our main focus lies on centralized critic components, that can be seamlessly integrated into the existing framework for particle tracking [9]. To improve over the existing architecture, we simplify the policy by moving computationally intensive layers from the policy to the centralized critic. Finally, we propose the use of a differentiable safety layer, similar to [31], [32] for constrained particle tracking, guaranteeing unique assignments of particle hits. We further provide useful gradient information, building upon existing work in decision-focused learning by [13], [38].\na) Feature preparation: Following the description of local observations in Section IV-A, we extract edge- and node-level features, for both last reconstructed $(v_{t-1} \\rightarrow v_t)$ and possible next track segments $(v_t \\rightarrow v_{t+1,j})$ from the hit graph according to\n$h_t^{obs,(i)} = \\Psi_1 ([v_t, e_{t-1,t}])$ and\n$h_{t+1}^{act,j} = \\Psi_2 ([v_{t+1}, e_{t,t+1,j}])$,\nwhich are projected by separate multi-layer perceptrons into an equally sized higher dimensional feature space. For performance reasons, we omit the additional feature vector generated by a graph neural network as proposed in [9], as we found the simple feature description to be sufficient in combination with the use of a safety layer. The positional encoding with adaptive receptive field (PE-ARF) mechanism, proposed in [9], is used to provide additional positional information in the form of cosine similarities restricted to a learnable area of interest.\nb) Local agent policies: We parameterize the local policy $\\mu_{\\theta}^{(i)}$ of each agent using a pointer mechanism [39] (Ptr-Net), predicting the conditional probability of the local action $a_t^{(i)}$ conditioned on observation- and action features. This mechanism is defined by additive attention [40] according to\n$a_{j,t} = v^T tanh(W_1 h_{act,j}^{emb} + W_2 h_{obs}^{emb})$,\nwhere $W_1$, $W_2$ and $v$ are learnable parameter matrices/vectors. The output scorings are normalized over all possible segments using a softmax activation.\nc) Communication: We focus in this work on decentralized actor architectures, requiring no or minimal global communication during inference, thus minimizing the computational overhead of communication protocols. While [9] uses multi-head attention (MHA) to learn an agreement between segment candidates, we consider this mechanism as a form of centralization and thus reallocate it from the actor to the centralized critic for all multi-agent architectures, reducing the computational cost of evaluating the policy.\nd) Safety Policy Layer: To correct the predicted local policies for duplicate assignments, we propose, similar to [29], the usage of a centralized safety layer [31], [32], performing for every reconstruction step an action correction for the learned joint policy by solving a linear sum assignment problem (LSAP). The safety layer ensures during both training and inference a full or partial unique matching defined by\n$\\min \\sum_{(i,j)\\in E} \\hat{\\mu}_{ij} C_{ij}$\ns.t. $\\sum_{i \\in V_s} \\mu_{ij} = 1,  \\forall j \\in V_T$,\n$\\sum_{j \\in V_T} \\mu_{ij} \\leq 1,  \\forall i \\in V_S$,\nthat minimizes the required cost of deviating from the proposed local policies. Here $C_{ij} \\in \\hat{C}$ are the individual elements of a $n\\times m$ cost matrix, defined, either by infinite cost for assignments already occupied by another track due to its initial seeding mechanism, or by the L2-norm of the local policy to the one-hot encoding of the corresponding target vertex, according to\n$C_{ij} = \\begin{cases} ||\\mu_{\\theta}^{(i)}(a_t|o_t) - 1||_2 \\text{ if not used for seeding} \\\\ \\infty \\\\ \\text{otherwise.} \\end{cases}$\nBy projecting the unsafe action, the action-corrected policy becomes inherently deterministic, requiring off-policy optimization and an exploratory policy for generating training samples. We sample track candidates with random exploration using parameter noise [41], [42]. We therefore replace the linear layers of the pointer mechanism with noisy linear layers [41]. While [31] and [32] propose a safety layer, that performs action correction without being able to differentiate through the layer itself, we use blackbox gradient information to reduce the complexity of the learning task, especially for the high dimensionality of the assignment problem.\ne) Blackbox differentiation: To provide gradient information for a combinatorial solver of the general form $y(C) = arg \\min_{y \\in Y} c(C, y)$, [13] proposed, substituting the piecewise constant solvers mapping of combinatorial solvers at the point $\\hat{C}$ by a linear interpolation between the points $\\hat{C}$ and $C'$ according to\n$\\nabla_{BB}^\\lambda f_{\\theta}(\\hat{C}) := \\frac{1}{\\lambda} [y(\\hat{C}) - y(\\hat{C}')]$, where\n$\\hat{C}' = clip(\\hat{C} + \\lambda \\frac{\\partial y}{\\partial C}(\\hat{C}), 0, \\infty)$.\nHere, $y(\\hat{C})$ and $y(\\hat{C}')$ are solutions generated by predicted and perturbed cost. Further, $\\lambda \\in \\mathbb{R}^+$ functions as a tunable hyperparameter, interpolating between truthfulness and informativeness of the gradients [13]. The usefulness of the gradient information for particle tracking has been already demonstrated in [29].\nf) Cost margins: With increasing number of solution sets, the policy becomes prone to settle changes in the cost matrix, limiting generalization. [38] proposed, adding random noise to the predicted cost, increasing the margin to the decision boundaries of the predictive output. As we found this mechanism to be highly instable for our use case, we instead add an additional component $\\nabla_W$ to the BB-scheme, with\n$\\nabla_{BB}^W(\\hat{C}) + \\nu \\nabla f(\\hat{C})$, where $\\nabla f(\\hat{C}) = y(\\hat{C})$,\nforcing the assignments of the joint policy $\\mu$ in the direction of lower assignment costs. The influence of $\\nabla_W$ can be controlled using the hyperparameter $\\nu \\in \\mathbb{R}^+$.\ng) Centralized critic: To mitigate instationarity, introduced by the otherwise independent learners [43], [44], we propose centralized factored critic functions for state- $V_{\\theta}(o_t)$ and action-value function $Q_{\\theta}(a_t|o_t)$, decomposing the global value function into agent-wise values [44] according to\n$Q(o_t, a_t) = \\sum_{i=1}^N q_{\\phi}^{(i)}(a_t^{(i)}, o_t^{(i)})$\n$V(o_t) = \\frac{1}{N} \\sum_{i=1}^N v_{\\phi}^{(i)}(o_t, \\phi(o_t))$.\nEach agent-wise value is composed, using local and global information, utilizing a mixture of additive [40] and self-attention [45]. To provide for each agent a single feature, we compress the set of agent observations $(h_{obs}^{(i)}, h_{act}^{(N_{act})})$ for both $V_{\\theta}$ and $Q_{\\theta}$. For the action dependent Q-function, we model the compressed representation $h_j^{(i)}$ by a joint policy-weighted function of observation- action features according to\n$h^{(i)}_j = \\sum_{j=1}^M \\mu_{\\theta}(a_{t,j}, o_t) (h^{emb, (i)}_{obs} + h^{emb,(i)}_{act,j})$\nHere, $h$ is an assembled feature over true and uncorrelated reference action features aggregated as a weighted sum over multiple random samples from a replay buffer D following\n$\\hat{h}_{act,j}^{emb,(i)} = \\frac{\\sum_{t',i',j' \\tilde{} D} \\mu_{\\phi}(o_{t'}, a_{j'})}{\\sum_{t',i',j' \\tilde{} D} } h_{act,j'}^{emb,(i')}$ ,\nwhere $\\gamma$ is a hyperparameter. This expression functions as a smoothing and regularization term with contextual information, allowing for reduced variance during training, improving convergence. For the action-independent state-value function $V_{\\theta}$, the weighting of the action features is replaced by a learnable weighting, modelled using an additive attention mechanism [40] according to\n$h^{(i)} = h^{emb, (i)}_{obs} + \\sum_{j=1}^M \\alpha_{t,j} h^{emb,}_{act,j}$ with\n$\\alpha_{t,j} = v \\text{ tanh } (W_1 h^{emb,}_{act,j} + W_2 h^{emb,(i)}_{obs}).$\nThe soft weighting makes the cross-state regularization for variance reduction obsolete. Further, we encourage global communication between agents in form of two stacked self-attention blocks with layer normalization [46] and skip connections [47], each defined as\n$h^{(i,l)} = LN (h^{(i,l-1)}_{Q/V} + ReLU (MHA (h^{(i,l-1)}_{Q/V})))$.\nFinally, factored values are obtained as the average agent-wise estimate conditioned on $h_{obs}^{oh}$ using an MLP. The value range for Q and V is restricted for either raw- (sigmoid) or normalized rewards (tanh) accordingly (additional details in Section IV-C) and scaled by the learnable parameter $s$.\n$Q(s, a) = \\frac{1}{N} \\sum_{i=1}^N s \\cdot \\sigma (\\Phi_Q (h^{(i)}))$.\n$V(s) = \\frac{1}{N} \\sum_{i=1}^N s \\cdot \\text{tanh } (\\Phi_V (h^{(i)}))$.\nFor completed particle tracks without valid assignments (early termination), we employ a value masking, where the relevant local agent-wise value estimates are excluded from the global value estimate. This representation prevents the observation of rewards obtained after early termination, posing additional complexity to the credit assignments [48], however, we choose the masking mechanism in favor of simplicity of the overall architecture\u00b3."}, {"title": "C. Optimization of Agents", "content": "The following section outlines the different optimization schemes for optimizing both unconstrained and constrained agents. Here we put specific focus on the details and modifications required for particle tracking.\na) Unconstrained on-policy baseline: We optimize an unconstrained joint policy using multi-agent proximal policy optimization algorithm (MAPPO) [49], [50], providing an extrapolation of the learning abilities of [9] to a collaborative multi-agent setting. We use the architecture described in Section IV-B, replacing the deterministic joint policy $\u00b5_{\\theta}$ by an unconstrained stochastic policy $\\pi_{\\theta}$ and a centralized state-value estimator $V_\\theta$. We estimate team advantages using the generalized advantage estimator [51] and employ independent reward normalization for calorimeter and tracker layer, following the normalization scheme in [9].\nb) Off-policy optimization: To cope with the deterministic safety-layer corrected policies, we optimize it similarly to [32], using a multi-agent variant of the Deep Deterministic Policy Gradient (DDPG) algorithm [52]. However, while [32] uses the multi-agent DDPG algorithm [53], we found the MATD3 [54] algorithm with two critic networks, mitigating overestimation bias, together with periodical hard critic updates worked superior for our use case. We found for the independent reward normalization mechanism to have a negative impact on optimization. Finally, we use a replay buffer with a small buffer size, owed to the quickly changing distribution of samples of the large joint action space [55]."}, {"title": "V. EXPERIMENTS", "content": "For the studies reported in this work, we rely on Monte-Carlo (MC) simulations of detector readout data [57], generated using the GATE toolkit [58], [59] based on the Geant4 simulation framework [60], [61], [62]. The dataset consists of multiple simulations with and without water phantom (100 mm, 150 mm and 200 mm), positioned between the particle beam and detector. The data is further diversified by manually splitting the data into readout frames of different particle densities $(p^+/F)$ of 50, 100, 150 and 200. Each simulation consists of 10,000 simulated primary particles. All data is publicly available on Zenodo [57].\na) Configurations: To explore the performance of single- and multi-agent systems of various degrees of complexity, we construct variations of the agent described in the previous sections, summarized in Table I. Each variant is constructed based on the selected optimization algorithm, the usage of a safety layer (during training SL(T) and execution SL(E)) as well as the differentiation scheme. We couldn't find a stable MATD3 configuration without a safety layer that consistently converged to low-reward solutions, and thus excluded it from the results. The single agent results for PPO and PPO+LSA are based on the trained models in [9].\nb) Training procedure: We use particle simulations without any absorber material between beam source and detector for optimization, providing a worst-case scenario in terms of secondary production and track length. We then train, for each configuration in Table I, five independent policies on sampled track candidates with a particle density of 50 primary particles per readout frame, to obtain robust results with confidence intervals.\nc) Baselines: In addition to the multi-agent schemes, listed in Table I, we compare the reconstruction performance, with both two single-agent variants of particle tracking described in [9] (with an additional centralized version using the proposed safety layer during inference) and a sequential track follower searching for solutions that minimize the total amount of scattering [14]. To obtain comparable results, all techniques construct the initial seed used for tracking using ground-truth information.\nd) Performance metrics: We assess and compare the performance of the proposed tracking algorithms using track purity (p) and efficiency (\u20ac), estimated after prior rejecting partial or implausible tracks using simple cuts for scattering angle and energy deposition according to [63]. For assessing the correctness of a track, we rely on a perfect matching criterion, where all hits in a track need to be correctly assigned."}, {"title": "A. Optimization and Tracking Performance", "content": "We examine and compare the performance for all configurations in Table I to identity and quantify the necessary factors for multi-agent based particle tracking using MARL. Figure 3 shows the average reward obtained as a function of network updates and sampled track segments. Here, we find similar training performance for the on-policy MAPPO and off-policy MATD3 approaches for equal number of training iterations. However, due to the on-policy nature of MAPPO, requiring data generated from the current policy, this approach requires significantly more transitions to converge and is thus significantly more sample inefficient than the off-policy MATD3 algorithm, utilizing a replay buffer. Further, while all multi-agent variants except for the unconstrained MATD3 approach, which we excluded from the experiments, converge to high average team rewards, MAPPO converges consistently to the highest average reward, suggesting the best optimization behavior of all. Finally, we find that both constrained agents with cost margins show significantly faster convergence to high rewards, requiring approximately 300 training iterations less than the other agents.\nTable II summarizes the reconstruction performance (purity p and efficiency \u20ac) of all MARL and baseline algorithms. We find that, while achieving lower average rewards compared to MAPPO, MATD3+LSA (BB) outperforms all baseline and MARL variants in both configurations of \u03bd by a significant margin. Especially for higher particle densities, the constrained policy with cost margins can benefit from the increased assignment complexity, outperforming the single-agent and unconstrained algorithms. We find the safety layer to be a critical component in multi-agent tracking, allowing for efficient sampling during training and inference, simplifying spacial credit assignment across agents, while avoiding duplicate assignment of particle hits. Further, we find the performance of MATD3+LSA(BB) to be robust to exact choice of \u03bd, producing similar results for both selected configurations.\nTo quantify the impact of the multi-agent optimization, we compare the performance of MATD3+LSA(BB) with a post-training centralized version of the single-agent PPO algorithm (PPO+LSA). Table III shows that PPO+LSA achieves similar performance, with only slight improvements in performance for the multi-agent approach. We find that the overall difference in performance is statistically not or only marginally significant (avg. p-values obtained by one-sided ttest [64]: p: 0.19, \u20ac: 0.12), demonstrating the strong ability of single-agent RL to efficiently learn reasonable conditional probabilities usable to resolve assignment conflicts during inference. Similar results are presented in [29] for supervised learning. However, for large particle multiplicities (e.g. 200 p+/F) we find the constrained multi-agent approach to outperform the single-agent approach by 0.75 percentage points (pp) (p-value: 0.03) in purity and 1.12 pp (p-value: 0.02) in efficiency, while only using limited information of the single-agent reward, indicating the usefulness of constrained multi-agent optimization."}, {"title": "B. Effectiveness of Cost Margins", "content": "We verify the effectiveness of the enforced cost margins, described in Section IV-B, by analyzing the predictive entropy of the learned policies. Figure 5 shows the distribution of the agents' local policies estimated over all decisions generated over a subset of the first five environments in the dataset for multiple particle density and phantom configurations. We find that local agent policies trained without enforced cost margins show the highest predictive uncertainties (Avg. entropy H(\u03bc) = 4.099 \u00b1 0.221), indicating only minimal separation from the decision boundaries. For both parameter values of \u03bd, weighing the cost-margin gradient, the long tail of the distribution is reduced significantly, lowering the average entropy by multiple orders of magnitude (H(\u03bc) = 0.241 \u00b1 0.002 and H (\u03bc) = 0.022 \u00b1 0.003). We find, similar to the results in Table II, that the reduction in uncertainty is robust to the exact choice of \u03bd, showing only marginal different values that are likely due to random mechanisms during training."}, {"title": "C. Analysis of Policy Constraints and Cost Margins", "content": "The following section presents analyses of reward surfaces for different agents, together with their corresponding surfaces of reconstruction performance. By comparing the reward surfaces with the track reconstruction performance, we aim to compare and highlight discrepancies in optimization and generalization. Understanding these differences allows us to explain why certain agents, despite achieving similar rewards during training, exhibit vastly different outcomes in terms of reconstruction quality, highlighting the importance of policy constraints as well as cost margins. We generate all surfaces, based on the technique described in [65], [15], as two-dimensional slices through the high dimensional landscapes along two directions defined by v and \u03b7 according to\nf(\u03b1, \u03b2) = L(0* + \u03b1\u03bd + \u03b2\u03b7).\nWe parameterize v and \u03b7 as the first two principal components over the entirety of saved training checkpoints (updated every three training iterations). All figures are generated for the 100 p+/F, 100mm phantom dataset with a resolution of, 25\u00d725 uniformly sampled parameter configurations in a region of [-1,1] \u00d7 [-1,1] for cost margins and [-3,3] \u00d7 [-3,3] for constrained and unconstrained policies. In the latter we experienced multiple configurations where the policy showed numerical issues, resulting in the prediction of nan values, marked in black.\na) Cost Margins: Analyzing the characteristic structure of reward and performance surfaces in Figure 6, we confirm the initial finding in Section V-A, that enforcing cost margins with the additional gradient term in Section IV-B significantly improves both optimization and generalization. Although the reward surfaces for policies with and without cost margins exhibit a similar shape, we observe a substantial difference in the surfaces for purity and efficiency. We find that the agents with cost margins converge to regions, characterized by wider and stable maxima, suggesting a better generalization performance and a reduced complexity during training.\nb) Policy Constraints: Figure 7 visualizes the differences in learning abilities for the unconstrained MAPPO and constrained MATD3+LSA architecture with cost margins. Here, we find similarly to Figure 6 good agreement of the reward surfaces, while the unconstrained policy shows wider regions of high reward. However, the received reward correlates only moderately with the reconstruction performance, demonstrating a strong degeneracy of the reward surface introduced by the larger combinatorial space caused by unconstrained assignments. Due to misaligned reward signals, the unconstrained agents demonstrate a significant decline in performance, governed by random effects during training (see Table II), indicating the necessity of policy constraints."}, {"title": "D. Functional Similarities and Prediction Instabilities", "content": "While both, post-training centralized single-agent (PPO+LSA) and per design centralized multi-agent policies (MATD3+LSA), achieve comparable reconstruction performances, a remaining key question is, whether the two approaches learn similar reconstruction policies and how stable the optimization and final learned policies are, e.g., across random initializations. To quantify potential prediction instabilities [16], [66], we closely follow the techniques in [16] and [67], where the amount of disagreement between two predictors $f_1$ and $f_2$ is quantified as the average fractions of classification errors, defined as\nd = \\mathbb{E} [\\mathbb{1}\\{\\text{arg max }f_1(x) \\neq \\text{arg max }f_2(x)\\}].\n[67] proposes an additional extension (min-max normalized disagreement), mapping the raw disagreement rates to a value range of [0, 1] providing better interpretability over the initial approach in [16]. Following this definition, $d_{norm}(f_1, f_2)$ is calculated according to\nd_{norm}(f_1, f_2) = \\frac{d(f_1, f_2) - \\text{min } d(f_1, f_2)}{\\text{max } d(f_1, f_2) - \\text{min } d(f_1, f_2)},\nwith $\\text{min } d(f_1, f_2) = \\text{min} \\{q_{Err}(f_1), q_{Err}(f_2)\\}$ and $\\text{max } d(f_1, f_2) = \\text{min} (q_{Err}(f_1) + q_{Err}(f_2), 1)$, where $q_{Err}$ is the error rate of a model. However, due to the sequential nature of reinforcement learning, the presented concept of quantifying prediction instabilities not directly applicable, as different predictions lead to changing track candidates. We thus calculate the prediction instability for all manually constructed correctly assigned states, avoiding the propagation of errors throughout the whole detector.\nFigure 8 shows both the full correlation-like instability matrix for all combinations of trained agents across agent type and random initializations, as well as the grouped distribution of values. We find that PPO and MATD3+LSA show pronounced differences in training behavior, resulting in substantial prediction instabilities, with a median of approximately 16.5%. Across different random initializations of the same agent type, we find that the instabilities are reduced. Here, the by design centralized agent demonstrates lower instabilities with an average difference of 0.98 pp (p-value: 0.01). While this difference is minor at the presented state, we argue that by the flexibility introduced by team rewards, this effect can be further enhanced. Further we find that while the average prediction instability is considerably low, outliers on a frame-by-frame level, in the form of a long tail of the otherwise Gaussian distribution (see Figure 9), demonstrate more pronounced instabilities for complex readout frames, posing additional risk for the reconstruction of complex readout frames. Here, we find that our multi-agent approach is able to reduce the number of outliers more effectively compared to the single-agent approach."}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduce multiple extensions to an existing single-agent reinforcement learning scheme for charged particle tracking, enabling the joint reconstruction of particle tracks in a multi-agent setting with additional (optional) assignment constraints. We realize the assignment constraints by an implicit, centralized safety layer, projecting the local unsafe actions onto global safe actions. Demonstrating the strong empirical performance of our approach on simulated data for a detector prototype designed for proton, computed tomography, we show that constrained optimization provides an immense advantage over its unconstrained MARL counterpart, as unconstrained approaches fail to converge consistently to good solutions, due to (1) the high degeneracy of solutions that maximize the team reward signal, while producing a significant amount of incorrect tracks and (2) the increased complexity of spacial credit assignment, most likely introduced by the significantly larger action space of the unconstrained problem. While we were able to achieve similar performance for a post-hoc centralized agent that was trained in a single agent manner, we find that learning particle tracking with constraints reduces the predictive instability, across random initializations. Additionally, using MARL during training provides more flexibility than RL and enables the design of more sophisticated reward functions utilizing information that can be only obtained collaboratively for an aggregate over multiple"}]}