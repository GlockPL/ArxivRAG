{"title": "Stochastic Variance-Reduced Iterative Hard Thresholding in Graph Sparsity Optimization", "authors": ["Derek Fox", "Samuel Hernandez", "Qianqian Tong"], "abstract": "Stochastic optimization algorithms are widely used for large-scale data analysis due to their low per-iteration costs, but they often suffer from slow asymptotic convergence caused by inherent variance. Variance-reduced techniques have been therefore used to address this issue in structured sparse models utilizing sparsity-inducing norms or lo-norms. However, these techniques are not directly applicable to complex (non-convex) graph sparsity models, which are essential in applications like disease outbreak monitoring and social network analysis. In this paper, we introduce two stochastic variance-reduced gradient-based methods to solve graph sparsity optimization: GRAPHSVRG-IHT and GRAPHSCSG-IHT. We provide a general framework for theoretical analysis, demonstrating that our methods enjoy a linear convergence speed. Extensive experiments validate the efficiency and effectiveness of our proposed algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph structures enable the imposition of intricate sparsity constraints on the model, allowing them to better reflect relationships present in the data. For instance, graph-structured sparsity models are well-suited for predicting the spread of diseases or identifying social groups in networks. The search of connected subgraphs or clusters has a significant impact on identifying disease-related genes [1], [2], [22], [23]. Graph sparsification, which aims to reduce the complexity of large-scale graphs while preserving their essential structural properties, has garnered increasing attention as a crucial technique in modern data analysis and machine learning [6], [11]\u2013[13], [30].\nGraph sparsification can be formulated as the following greatly decreases the computational costs associated with the gradient calculations. However, if the selected batch does not represent the whole dataset, the gradient may not accurately point towards a local minimum of the function, introducing variance into the gradient descent process.\nTo reduce the randomness inherent in SGD, variance re- duction techniques may be used such as SVRG [17], SAGA [9], or SCSG [18]. During each iteration, the history of the stochastic process is considered to regulate the newly calculated gradient and minimize large changes in direction. This improvement on SGD is our main interest; both proposed algorithms utilize this technique to leverage fast gradient calculations while still enjoying quick convergence. In this paper we leverage the recent success of stochastic variance- reduced algorithms for non-convex problems and propose"}, {"title": "II. PRELIMINARIES", "content": "Notations. We use lowercase letters, e.g. x, to denote a vector and use || . || to denote the l2-norm of a vector. The operator E represents taking expectation over all random variables, [n] denotes the integer set {1, ..., n}. The notation supp(x) means the support of x or the index set of non-zero elements in x. Other important parameters are listed in Appendix A.\nDefinition 1. (Subspace model) [14] Given the space Rp, a subspace model M is defined as a family of linear subspaces of Rp:\n$\\mathcal{M}={S_1, S_2,..., S_k,...}$\nwhere each Sk is a subspace of Rp. The set of corresponding vectors in these subspaces is denoted as\n$\\mathcal{M}(\\mathcal{M})={x: x \\in V for some V \\in \\mathcal{M}}$.\nDefinition 2. (Weighted graph model) [13] Given an under- lying graph $G = (V, E)$ defined on the coefficients of the unknown vector x, where $V = [p]$ and $E \\subseteq V \\times V$, then the weighted graph model $(G, s, g, C')$- WGM can be defined as the following set of supports\n$\\mathcal{M}={S : |S|<s$, there is an $F \\subseteq V$ with"}, {"title": "III. METHODS", "content": "In this section, we introduce two proposed algorithms: GRAPHSVRG-IHT and GRAPHSCSG-IHT. Both algorithms employ the variance-reduced techniques derived from SVRG [17] and SCSG [18] respectively, while also utilizing the graph projection operators found in GRAPHSTO-IHT [30]. This results in methods that are applicable to graph-structured sparsity problems and effectively reduce the variance inher- ent to stochastic gradient descent. Therefore, our algorithms converge faster and more accurately than their predecessors."}, {"title": "V. EXPERIMENTS", "content": "A. Experimental setup\nWe perform multiple experiments to compare our proposed algorithms with baseline methods. For our experiments, we consider the residual norm of the loss function, $||Ax_{t+1} \u2013 y||$, as the number of epochs increases. Due to the non-convex nature of the problem, there are several local minima and the algorithm may not approach the global minimum, $x^*$. Additionally, in real-world applications, $x^*$ is often unknown. Therefore, we use the residual norm as a measurement of convergence as opposed to the distance from the final iterate to the target vector, $||x_{t+1} \u2212 x^*||$. All experiments are tested on a Ubuntu 22.04.4 server with 256 AMD EPYC 9554 64-core processors and with 1.6 TB RAM. All codes are written in Python\u00b9."}, {"title": "VI. CONCLUSION", "content": "We have proposed two algorithms to utilize variance reduction techniques in the setting of graph sparse optimization. The proposed algorithms can significantly improve the stability and efficiency in machine learning and other applications. Theoretically, we provide a general proof framework showing linear convergence. Empirically, our algorithms are compet- itive in minimizing the objective loss function compared to their predecessors in various experimental settings with a synthetic dataset. Additionally, testing on a large-scale medi- cal dataset demonstrated superior performance in identifying cancer-related genes. Future work should include testing our algorithms on more larger real-world datasets. By employing graph-structured hard thresholding, we can uncover more underlying subgraphs and related patterns, with significant implications in fields such as medical research and social net- work analysis. This approach can enhance our understanding of complex data structures and lead to more effective solutions."}]}