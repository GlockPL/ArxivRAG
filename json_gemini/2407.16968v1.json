{"title": "Stochastic Variance-Reduced Iterative Hard Thresholding in Graph Sparsity Optimization", "authors": ["Derek Fox", "Samuel Hernandez", "Qianqian Tong"], "abstract": "Stochastic optimization algorithms are widely used for large-scale data analysis due to their low per-iteration costs, but they often suffer from slow asymptotic convergence caused by inherent variance. Variance-reduced techniques have been therefore used to address this issue in structured sparse models utilizing sparsity-inducing norms or lo-norms. However, these techniques are not directly applicable to complex (non-convex) graph sparsity models, which are essential in applications like disease outbreak monitoring and social network analysis. In this paper, we introduce two stochastic variance-reduced gradient-based methods to solve graph sparsity optimization: GRAPHSVRG-IHT and GRAPHSCSG-IHT. We provide a general framework for theoretical analysis, demonstrating that our methods enjoy a linear convergence speed. Extensive experiments validate the efficiency and effectiveness of our proposed algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph structures enable the imposition of intricate sparsity constraints on the model, allowing them to better reflect relationships present in the data. For instance, graph-structured sparsity models are well-suited for predicting the spread of diseases or identifying social groups in networks. The search of connected subgraphs or clusters has a significant impact on identifying disease-related genes [1], [2], [22], [23]. Graph sparsification, which aims to reduce the complexity of large-scale graphs while preserving their essential structural properties, has garnered increasing attention as a crucial technique in modern data analysis and machine learning [6], [11]\u2013[13], [30].\nGraph sparsification can be formulated as the following optimization problem:\n$\\min_{x \\in R^p} F(x), \\quad F(x) := \\frac{1}{n} \\sum_{i=1}^{n} f_i(x),$ (1)\nwhich is known as the empirical risk minimization problem. Each $f_i(x)$ ($i \\in [n]$) is convex and differentiable, and the graph structured sparsity is reflected by the constraint set $R^p$ on $x$. The input vector $x$ denotes the parameter of the model, and the output $f_i(x)$ is defined as the loss associated with sample $i$. By minimizing the loss $F(x)$, we guide the model towards the optimal solution. Typically, sparsity can be encoded by adding sparsity-inducing norms or penalties such as $l_0$ norm, $l_1$ norm and mixed norms [7], [8], [10], [16], [24]\u2013[26], [28], [29]. These models often involve convex penalties and can be solved using convex optimization algorithms [3]\u2013[5]. However, dealing with more complex sparse settings, such as graph-structured models, is more challenging.\nIn stochastic optimization, iterative hard thresholding (IHT) methods include gradient descent IHT (GD-IHT) [16], stochastic gradient descent IHT (SGD-IHT) [21], hybrid stochastic gradient IHT (HSG-IHT) [31], and variance-reduced methods such as stochastic variance reduced gradient IHT (SVRG-IHT) [19], stochastically controlled stochastic gradient IHT (SCSG-IHT) [20]. These methods update the parameter iterate $x$ via gradient descent or its variants, and then apply a hard thresholding (HT) operator to enforce sparsity of $x$, preserving the top $s$ elements in $x$ while setting other elements to zero. In the context of graph-structured sparse optimization, the stochastic gradient decent based IHT method, named GRAPHSTO-IHT, achieves HT through the use of Head and Tail Projections, first described by [30]. Head and Tail Projections map arbitrary vectors from the data onto the graph while simultaneously enforcing model sparsity [12]\u2013[14]. Specifically, Head Projection identifies and preserves the largest entries in $x$, while Tail Projection identifies the smallest entries and sets them to zero. By ignoring the small magnitude entries in the vector, these projections help prevent overfitting and ensure sparse solutions. Meanwhile, stochastic sampling of the data is used to speed up gradient calculations. A single data point or small batch of the data is selected and a gradient is calculated only with respect to that batch. This greatly decreases the computational costs associated with the gradient calculations. However, if the selected batch does not represent the whole dataset, the gradient may not accurately point towards a local minimum of the function, introducing variance into the gradient descent process.\nTo reduce the randomness inherent in SGD, variance re-duction techniques may be used such as SVRG [17], SAGA [9], or SCSG [18]. During each iteration, the history of the stochastic process is considered to regulate the newly calculated gradient and minimize large changes in direction. This improvement on SGD is our main interest; both proposed algorithms utilize this technique to leverage fast gradient calculations while still enjoying quick convergence. In this paper we leverage the recent success of stochastic variance-reduced algorithms for non-convex problems and propose a series of efficient stochastic optimization algorithms for"}, {"title": "II. PRELIMINARIES", "content": "Notations. We use lowercase letters, e.g. $x$, to denote a vector and use $|| . ||$ to denote the $l_2$-norm of a vector. The operator $E$ represents taking expectation over all random variables, $[n]$ denotes the integer set ${1, ..., n}$. The notation $supp(x)$ means the support of $x$ or the index set of non-zero elements in $x$. Other important parameters are listed in Appendix A.\nDefinition 1. (Subspace model) [14] Given the space $R^p$, a subspace model $M$ is defined as a family of linear subspaces of $R^p$:\n$M = \\{S_1, S_2,..., S_k,...\\}$\nwhere each $S_k$ is a subspace of $R^p$. The set of corresponding vectors in these subspaces is denoted as\n$M(M) = \\{x: x \\in V \\text{ for some } V \\in M\\}.$\nDefinition 2. (Weighted graph model) [13] Given an under-lying graph $G = (V, E)$ defined on the coefficients of the unknown vector $x$, where $V = [p]$ and $E \\subseteq V \\times V$, then the weighted graph model (G,s,g,C')- WGM can be defined as the following set of supports\n$M = \\{S : |S| < s, \\text{ there is an } F \\subseteq V \\text{ with }$\n$V_F = S, \\gamma(F) = g, \\text{ and } w(F) \\leq C'\\},$\nwhere $C$ is the budget on weight of edges $w$, $g$ is the number of connected components of $F$, and $s$ is the sparsity.\nDefinition 3. (Projection Operator) [14] We define a pro-jection operator onto $M(M)$, i.e, $P(\\cdot, M(M)) : R^p \\rightarrow R^p$ defined as\n$P(x, M(M)) = \\arg \\min_{Y \\in M(M)} ||x -y||_2.$\nWith this projection definition, we borrow two important projections: Head Projection and Tail Projection from previous literature to help us with theoretical analysis.\nAssumption 1. (Head Projection) [14] Let $M$ and $M_H$ be the predefined subspace models. Given any vector $x$, there exists a $(c_H, M, M_H)$-Head-Projection which is to find a subspace $H \\in M_H$ such that\n$||P(x, H)||_2 \\geq c_H \\max_{S' \\in M} || P(x, S') ||_2,$ (1)\nwhere $0 < c_H \\leq 1$. We denote $P(x, H)$ as $P(x, M, M_H)$.\nAssumption 2. (Tail Projection) [14] Let $M$ and $M_T$ be the predefined subspace models. Given any vector $x$, there exists a $(c_T, M, M_T)$-Tail-Projection which is to find a subspace $T \\in M_T$ such that\n$||P(x,T)-x||_2 <c_T \\min_{S \\in M} ||x - P(x, S) ||_2, $ (2)\nwhere $c_T > 1$. We denote $P(x,T)$ as $P(x, M, M_T)$.\nWe can see that head projection keeps large magnitudes whereas tail projection sets small magnitudes to zero.\nDefinition 4. (($\\alpha$, $\\beta$, $M(M)$)-RSC/RSS Properties) [14] We say a differentiable function $f(\\cdot)$ satisfies the ($\\alpha$, $\\beta$, $M(M)$)-\nRestricted Strong Convexity (RSC)/Smoothness (RSS) property if there exist positive constants $\\alpha$ and $\\beta$ such that\n$\\frac{\\alpha}{2} ||x - y||^2 \\leq B_f(x,y) \\leq \\frac{\\beta}{2} ||x-y||^2,$ (3)\nfor all $x,y \\in M(M)$, where $B_f(x,y)$ is the Bregman divergence of $f$, i.e.,\n$B_f(x,y) = f(x) - f(y) - \\langle \\nabla f(y), x - y\\rangle.$\n$\\alpha$ and $\\beta$ are the strong convexity parameter and strong smoothness parameter, respectively.\nThe RSC/RSS definition is widely used in sparsity optimiza-tion. Together with the following assumption, we characterize the properties of the objective function.\nAssumption 3. [14] Given the objective function $F(x)$ in (1), we assume that $F(x)$ satisfies $\\alpha$-RSC in subspace model $M(M \\oplus M_H \\oplus M_T)$. Each function $f_i(x)$ satisfies $\\beta$-RSS in $M(M \\oplus M_H \\oplus M_T)$, where $\\oplus$ of two models $M_1$ and $M_2$ is defined as $M_1 \\oplus M_2 := \\{S_1 \\cup S_2 : S_1 \\in M_1, S_2 \\in M_2\\}.$"}, {"title": "III. METHODS", "content": "In this section, we introduce two proposed algorithms: GRAPHSVRG-IHT and GRAPHSCSG-IHT. Both algorithms employ the variance-reduced techniques derived from SVRG [17] and SCSG [18] respectively, while also utilizing the graph projection operators found in GRAPHSTO-IHT [30]. This results in methods that are applicable to graph-structured sparsity problems and effectively reduce the variance inher-ent to stochastic gradient descent. Therefore, our algorithms converge faster and more accurately than their predecessors.\nA. GRAPHSVRG-IHT\nOur proposed GRAPHSVRG-IHT algorithm (Algorithm 1) utilizes variance reduction by periodically computing the full gradient, significantly reducing the inherent variance in stochastic gradient methods. By incorporating graph projection operators, our GRAPHSVRG-IHT adapts to non-convex graph sparsity constraints, enhancing its applicability and efficiency. The key steps are outlined below:\n1) Calculate the full gradient, $v$, with the position at the start of each outer loop, $x^i$ (Line 4).\n2) In the inner loop, compute two gradients from a single sampled data point: one at the copied position $x^i$ and the other at $x_k$. Then calculate the stochastic variance reduced gradient, $v_k$ (Line 7-8).\n3) Pass $v_k$ through the Head Projection operator (Line 9); and use the resulting gradient to update the next iterate $x_k$, through the Tail Projection operator (Line 10).\nB. GRAPHSCSG-IHT\nTo better understand the calculation of variance-reduced gradi-ents and stochastically control the outer batch size, we propose the GRAPHSCSG-IHT algorithm (Algorithm 2). While similar to Algorithm 1 in its use of variance-reduced gradients, GRAPHSCSG-IHT has the following key characteristics:\n1) In the outer loop, the gradient is calculated using a batch of data of size $B$, whereas Algorithm 1 calculates a full gradient at this step (Line 4-5).\n2) In the inner loop, when calculating the stochastic vari-ance reduced gradient, a mini-batch is used instead of a single data point (Line 10-11).\n3) The number of inner loops, $K_i$, is not fixed. Instead, $K_i$ is chosen from a geometric distribution (Line 7) or can be set as $\\frac{n}{B}$ (Line 8).\n4) After a fixed number (K) of inner loop iterations, update the outer loop position $x^{i+1}$ and re-calculate the new full gradient."}, {"title": "IV. THEORETICAL ANALYSIS", "content": "In this section, we present our main theoretical results char-acterizing the estimation error of parameters $x$. The proof provides a general framework based on the gradients from GRAPHSCSG-IHT. Consequently, the main theorem is appli-cable to our two proposed algorithms, GRAPHSVRG-IHTand GRAPHSCSG-IHT. We demonstrate the convergence of these algorithms by bounding the final error using the $l^2$ norm of the initial and the optimal distance. Additionally, we consider the history of the stochastic process up to iteration $j * K$ with the notation $I_k$.\nBefore delving into the main theorem, we present a key lemma that is crucial for the proof.\nLemma 1. [30] If each $f_{\\varepsilon_t}(\\cdot)$ and $F(x)$ satisfy Assumption 3, and given head projection model $(c_h, M \\oplus M_T, M_H)$ and tail projection model $(c_T, M, M_T)$, then we have the following inequality\n$E_{I_k} || (x^k - x^*)_H || \\leq \\sqrt{1 - \\alpha_0} E_{I_k} ||x^i - x^*|| + \\sigma_1,$\nwhere\n$\\sigma_1 = \\left(\\frac{1}{\\alpha_0} + \\frac{\\eta\\alpha_0\\beta_0}{1 - \\alpha_0}\\right) E_{\\xi} ||\\nabla f_{\\varepsilon_t}(x^*)||$,\n$H = supp(P(\\nabla f_{\\varepsilon_t}(x^*), M \\oplus M_T, M_H)),$\n$\\alpha_0 = c_H \\sqrt{\\alpha\\beta}\\tau^2 - 2\\alpha\\tau + 1$, $\\beta_1 = (1 + c_H)\\tau,$\n$I = \\arg \\max_{S \\in M \\oplus M_T \\oplus M_H} E_{\\xi} ||\\nabla f_{\\varepsilon_t}(x^*)||$,\nand $\\tau \\in (0,2/\\beta].$\n$\\gamma = (1 + c_T)(\\frac{\\beta_0}{\\alpha_0} + \\frac{\\alpha_0\\beta_0}{\\alpha_0\\sqrt{1 - \\alpha_0^2}} + \\eta).$\nTheorem 2 demonstrates that our new algorithm achieves linear convergence with stochastic variance-reduced gradients, even with more stochastic settings in batch and mini-batch. Each variable defined in the theorem is strictly less than 1, ensuring that the error decreases as the number of iterations increases. This result aligns with our experimental findings, where more iterations consistently lead to smaller errors.\nFrom Theorem 2, we derive the following corollary, which further justifies the convergence of our algorithm and specifies the appropriate range for the learning rate $\\eta$.\nCorollary 2.1. To ensure convergence of our algorithm, the learning rate $\\eta$, which is a constant, should be chosen within the range $(\\frac{2\\alpha - \\sqrt{4\\alpha^2 - 3.75\\alpha\\beta}}{\\Sigma\\alpha\\beta}, \\frac{2\\alpha + \\sqrt{4\\alpha^2 - 3.75\\alpha\\beta}}{\\Sigma\\alpha\\beta})$. For this range to be valid, the following inequality must hold:\n$\\frac{\\delta}{1-\\lambda} < 1.$\nCorollary 2.1 is the cornerstone of Theorem 2. It ensures that the upper bound for the estimation error does not blow up to infinity, and provides a constant value for the finite series. Similarly, it also ensures that the upper bound will decay more after performing more iterations. Corollary 2.1 also provides a range of $\\eta$, which is smaller than the one given by GRAPHSTO-IHT. This way we can find $\\eta$ such that the algorithm will always converge. All the proofs are provided in the appendix of the paper."}, {"title": "V. EXPERIMENTS", "content": "A. Experimental setup\nWe perform multiple experiments to compare our proposed algorithms with baseline methods. For our experiments, we consider the residual norm of the loss function, $||Ax_{t+1} \u2013 y||$ as the number of epochs increases. Due to the non-convex nature of the problem, there are several local minima and the algorithm may not approach the global minimum, $x^*$. Additionally, in real-world applications, $x^*$ is often unknown. Therefore, we use the residual norm as a measurement of convergence as opposed to the distance from the final iterate to the target vector, $||x_{t+1} \u2212 x^*||$. All experiments are tested on a Ubuntu 22.04.4 server with 256 AMD EPYC 9554 64-core processors and with 1.6 TB RAM. All codes are written in Python.\nB. Synthetic Dataset\nWe first tested our methods on synthetic datasets to determine the optimal parameters. For a fair comparison, we followed the exact settings used in GRAPHSTO-IHT, conducting multiple experiments using a grid graph with a dimension of 256 and unit-weight edges.\nChoice of $\\eta$. To study the effect of the learning rate on the performance of our algorithms, we varied $\\eta$ across {0.1, 0.01, 0.001} and tested these rates in various sparsity cases, with\nChoice of sparsity. Studying the setting of sparsity, $s$, is crucial for understanding the performance of our algorithms in graph sparsification optimization. To examine the effect of sparsity, we compared our methods, GRAPHSVRG-IHT and GRAPHSCSG-IHT against the baseline algorithm GRAPH-STO-IHT. Using the experimental settings from GRAPHSTO-IHT we employed a grid graph of dimension 256, fixed the learning rate $\\eta = 0.01$ and set the batch size equal to the sparsity parameter $B = s$. We varied the sparsity parameter $s$ to observe the behavior of all algorithms, as shown in Figure 2. Figure 2 demonstrates that as the sparsity parameter $s$ decreases, GRAPHSVRG-IHT outperforms the other algorithms in minimizing the residual norm $|| Ax_{t+1} \u2013 y||$ over epochs. Another interesting finding is that we observed that GRAPHSTO-IHT and GRAPHSCSG-IHT display almost identical behavior in this parameter setting.\nChoice of batch size. After exploring the choice of $\\eta$ and $s$, we varied the batch size $B$ on a grid graph with a dimension of 256 to demonstrate the advantages of GRAPHSCSG-IHT. Here we fixed $\\eta = 0.01$, $s = 32$. For fair comparison, we considered the number of data points instead of the number of epochs to estimate the run time of the algorithms, which is a common practice in optimization.\nSince gradient calculations are computationally expensive, using fewer data points would result in faster run times. Figure 3 shows three different scenarios with varying batch sizes $B$. When $B$ equals to the dimension, GRAPHSCSG-IHT degrades"}, {"title": "VI. CONCLUSION", "content": "We have proposed two algorithms to utilize variance reduction techniques in the setting of graph sparse optimization. The proposed algorithms can significantly improve the stability and efficiency in machine learning and other applications. Theoretically, we provide a general proof framework showing linear convergence. Empirically, our algorithms are compet-itive in minimizing the objective loss function compared to their predecessors in various experimental settings with a synthetic dataset. Additionally, testing on a large-scale medi-cal dataset demonstrated superior performance in identifying cancer-related genes. Future work should include testing our algorithms on more larger real-world datasets. By employing graph-structured hard thresholding, we can uncover more underlying subgraphs and related patterns, with significant implications in fields such as medical research and social net-work analysis. This approach can enhance our understanding of complex data structures and lead to more effective solutions."}]}