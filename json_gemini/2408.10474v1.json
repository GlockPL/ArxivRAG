{"title": "LeCov: Multi-level Testing Criteria for Large Language Models", "authors": ["Xuan Xie", "Jiayang Song", "Yuheng Huang", "Da Song", "Fuyuan Zhang", "Felix Juefei-Xu", "Lei Ma"], "abstract": "Large Language Models (LLMs) are widely used in many different domains, but\nbecause of their limited interpretability, there are questions about how trustworthy\nthey are in various perspectives, e.g., truthfulness and toxicity. Recent research has\nstarted developing testing methods for LLMs, aiming to uncover untrustworthy is-\nsues, i.e., defects, before deployment. However, systematic and formalized testing\ncriteria are lacking, which hinders a comprehensive assessment of the extent and\nadequacy of testing exploration. To mitigate this threat, we propose a set of multi-\nlevel testing criteria, LECOV, for LLMs. The criteria consider three crucial LLM\ninternal components, i.e., the attention mechanism, feed-forward neurons, and un-\ncertainty, and contain nine types of testing criteria in total. We apply the criteria\nin two scenarios: test prioritization and coverage-guided testing. The experiment\nevaluation, on three models and four datasets, demonstrates the usefulness and\neffectiveness of LECOV.", "sections": [{"title": "1 Introduction", "content": "Recent research highlights the phenomenal accomplishments of Large Language Models (LLMs)\nacross various fields, such as natural language processing [1], code generation [2], and robotic sys-\ntem control [3, 4]. After being trained on large and varied datasets [5\u20137], LLMs can generate\nanswers that mimic human intellect and common sense understanding. They have proven essential\nin many applications due to their adaptability, efficiency, and scalability, greatly contributing to the\nadvancement of artificial general intelligence [8].\nThe rapid deployment of LLM also raises the concern about the trustworthiness of LLM [9\u201311],\ne.g., hallucination [12] and toxicity [13]. Hence, recent work focuses on developing trustworthiness\nanalysis techniques for LLMs, and a recent trend is uncovering untrustworthy responses\u00b9 through\nLLM testing [14\u201317]. For example, Hong et al. propose to train a red teaming testing LLM (as\na tester) by maximizing novel reward and entropy reward [14]. AutoDAN [15] can automatically\nproduce testing prompts (i.e., test cases) by hierarchical genetic algorithms.\nAlthough various testing techniques have been developed to assess the trustworthiness of LLMs,\na systematic approach for measuring their testing sufficiency and coverage is still missing. Test\ncoverage criteria are crucial for ensuring that the model is evaluated across a broad spectrum of\nscenarios, thereby increasing its reliability in real-world applications. Therefore, there is an urgent\nneed for formalized and systematic criteria to evaluate the sufficiency of LLM testing, which would\nenhance our understanding of test set quality. Moreover, existing techniques primarily focus on\nanalyzing the input/output behavior of LLMs, which provides a limited perspective. Incorporating\nan analysis of other components, such as the internal structure of LLMs, could offer deeper insights\ninto their behavior."}, {"title": "2 Background", "content": ""}, {"title": "2.1 LLM Defects", "content": "Software defects are generally defined as flaws or errors that affect a system's reliability, quality,\nand availability [18]. We extend this concept as LLM defects considering the human interaction\ncharacteristics of LLM-driven systems (e.g., chatbots). These defects refer to scenarios where the\nresponses of LLMs fail to meet the expectations of various stakeholders. From an objective stand-\npoint, LLM defects include issues related to correctness and truthfulness. Subjectively, they also\nencompass concerns such as safety, privacy, and machine ethics. All these defects impact the trust-\nworthiness of LLM-driven systems [9]. In this study, we focus on two types of defects: an objective\ndefect, where LLM responses deviate from real-world truth (i.e., hallucination) [19, 20], and a sub-\njective defect, where LLM responses are toxic and potentially violate regulations [13]."}, {"title": "2.2 Deep Learning System Testing", "content": "Testing has long been a critical method for understanding system performance and identifying po-\ntential issues to ensure higher quality [21]. In related work on DNN systems, the focus often lies on\nclassification-only models, exploring how to perform effective testing by automatically generating\nnew test cases [22\u201325] and accelerating the testing process by prioritizing test cases where DNNs\nare more likely to fail [26\u201329]. Both methods rely on indicators of system states, often referred to\nas testing criteria [30\u201332]. For automated test generation, a criterion similar to code coverage is\nnecessary to guide the process and uncover valuable test cases within the vast input space. For test\ncase prioritization, a criterion that reflects the system's capabilities for handling different inputs is\nessential to identify potential errors among the large volume of test cases. In this study, we extend\nthe exploration of testing criteria from classification-based models to auto-regressive foundation\nmodels, paving the way for future advancements in LLM testing frameworks."}, {"title": "3 Testing Criteria for LLM", "content": "In this section, we introduce the proposed testing criteria, LECOV, for LLMs. LECOV includes three\ntypes of criteria: attention-wise, neuron-wise, and uncertainty-wise. Attention-wise coverage is\nmotivated by the unique attention mechanism of LLMs; neuron-wise coverage follows the principles\nof DNN testing [32\u201334]; and uncertainty-wise coverage is inspired by recent studies showing that\nthe quality of LLM responses is closely linked to uncertainty [35, 36]."}, {"title": "3.1 Attention-wise Coverage Criteria", "content": "With the goal of artificial general intelligence, the training dataset of LLMs is universal, encompass-\ning a wide range of data (e.g., LLaMA 3.1 is trained on more than 15 trillion tokens [37]). Nev-\nertheless, testing on a similarly large scale of data is intractable due to the computational expense\nof LLMs. Intrinsically, the attention value is an approximation to the training distribution, w.r.t.\nthe extent of input relevance [38, 39]. At the attention level, we utilize the output of the attention\nhead, i.e., the attention value, to characterize its behavior. We gauge the coverage of the attention\nvalue using a statistical measurement, which will be introduced later. Specifically, this measurement\ncharacterizes the attention value and maps it to a scalar, allowing us to compute criteria. At a high\nlevel, let's denote the upper bound of the measurement as UB, and the lower bound as LB. The\ncoverage space of an attention head a lies within [LB, UB]. We divide [LB, UB] into k sections,\nand each section is considered covered if the measurement falls within that section.\nLet $A = \\{a_1, a_2, . . ., a_n \\}$ be the set of attention heads, and $\u03ba_a(x)$ be the output of the head a, given\nan input x at time t. We define the k-multisection coverage of an attention head a as:\n$\\frac{| \\{S_i^a | \\exists x \\in X, \\exists t \\in [0, T_x], \u03ba_a(f_a(x)) \\in S_i^a\\} |}{k}$ (1)\nwhere $S_i^a = [lb_i, ub_i]$ is the i-th section, x is a test prompt in the test set X, $T_x$ is the total generated\ntoken length of LLM given x as the input, and $\u03ba$ is a statistical measurement for describing the\ncharacteristics of a distribution.\nThe k-multisection coverage of an LLM is defined as:\n$\\frac{\\sum_{a \\in A} { \u03b4_a | \\exists x \\in X, \u03ba_a(f_a(x)) \\in S_i^a\\} }{k \\times |A|}$ (2)\nPrevious methods of gauging the coverage of DNN by the neuron activation [31, 40, 33] (which is\na scalar), are not suitable in attention coverage computation, since, the attention values are vectors,\ne.g., the output shape of an attention head of LLaMA-7B is 4,096. Hence, we use statistical mea-\nsurements to describe and identify patterns and trends of the attention value. In particular, for $\u03ba$, we\nutilize four statistical measurements to describe the attention values: mean, variance, kurtosis, and\nskewness. These measurements describe the characteristics of a set of data, i.e., distribution, quanti-\ntatively and they provide different insights into the distribution, such as the central tendency (mean),\nvariability (variance), and the distribution shape (kurtosis and skewness). In total, we have four\nattention coverage: k-multisection Mean Attention Coverage (KMAC), k-multisection Vari-\nance Attention Coverage (KVAC), k-multisection Kurtosis Attention Coverage (KKAC), and\nk-multisection Skewness Attention Coverage (KSAC). Intuitively, KMAC measures indicate the\ncentral or typical value within a dataset, KVAC describes the spread or variability of the distribution,\nwhile KKAC and KSAC characterize the asymmetry or the concentration of data points in the tails\nof the distribution, respectively."}, {"title": "3.2 Neuron-wise Coverage Criteria", "content": "Unlike classic feed-forward neural networks, LLMs exhibit temporal semantics, meaning the model\ngenerates a sequence of different outputs over time in response to a given input prompt. As a result,\nexisting neuron coverage criteria, such as neuron coverage [30] and neuron boundary coverage [31],\nare inadequate for quantitatively measuring the testing adequacy of LLMs because they do not ac-\ncount for temporal behavior. To address this, we propose three novel neuron-wise coverage criteria"}, {"title": "3.3 Uncertainty-wise Coverage Criteria", "content": "Uncertainty of an LLM refers to the degree of confidence the model has in its predictions [36, 41\u2013\n43], essentially quantifying the expected variability or reliability of those predictions. Recent studies\nindicate that the quality of LLM outputs is closely tied to various forms of uncertainty [36, 41].\nConsequently, we select the exploration of the uncertainty space as a testing criterion. Specifically,\nthe uncertainty-wise coverage criteria include k-multisection entropy coverage and k-multisection\nlikelihood coverage.\nk-Multisection Entropy Coverage (KMEC). The KMEC is computed as:\n$\\text{KMEC}(X) = \\frac{| \\{S_i^H | \\exists x \\in X, \\exists t \\in [0, T_x], H(x, t) \\in S_i^H\\} |}{k}$ , (7)\nwhere H(x, t) is the entropy at time step t of the generation given a test case x, k is the section\nnumber, and $S_i^H = [lb_i, ub_i]$ is the i-th section.\nk-Multisection Likelihood Coverage (KMLC). The KMLC is computed as:"}, {"title": "4 Application", "content": "To demonstrate the usefulness of the newly proposed criteria, we apply LECOV to two practical\napplication scenarios: (1) test prioritization, which prioritizes test cases likely to expose untrustwor-\nthy behavior, i.e., defect, in the LLM under test; and (2) coverage-guided testing, which generates\ndefect-inducing test cases to evaluate the model.\nTest Prioritization. Test prioritization for LLMs involves choosing a subset of test cases that are\nlikely to trigger errors during the model's operations [44, 26, 45]. Given the extensive capabilities of\nLLMs, exhaustive testing can be computationally expensive and resource-intensive. By prioritizing\na subset of high-risk test cases, developers can focus on addressing the most critical risks, such as\nminimizing biases [46] and preventing harmful outputs [47, 41]. Specifically, given a test set X and\na model M, test selection aims to choose a subset {x1,...,xk} (k < |X|) that is most likely to\ntrigger errors. In our approach, we rank the test cases based on the selected coverage criteria and\nprioritize those with the highest coverage value.\nCoverage-Guided Testing. Coverage-Guided Testing (CGT) of LLMs aims to systematically ex-\nplore the model's input space to ensure a comprehensive evaluation [48, 22, 49]. CGT leverages the\nproposed criteria to guide the test generation process and assess the quality of the generated samples.\nThe process begins with selecting a test case from a queue that keeps seed samples with the potential\nto trigger model errors. The mutation is then applied to generate mutants for testing the model. If a\nmutant increases coverage, it is pushed back into the queue as a sample likely to trigger errors.\nAlgorithm 1 summarizes the detailed procedure of the coverage-guided testing of LLM. The inputs\nto the algorithms include the initial seeds of test case I, the testing budget b, and the LLM under test\nM. At first, the test case queue Q is enqueued with I, defect queue U is empty, and the counter i is\ninitialized as 0 (Line 1). The algorithm enters the testing loop until the test budget limit is reached\n(Line 2). At each test iteration, a test case to is first dequeued from Q (Line 4). To obtain new test\ncases that could incur faults of LLM, we perform mutation (which will be introduced later) on to to\nproduce a new prompt tn (Line 5). The new test case tn is given to M, and we get the output of the\nLLM r and the internal states of LLM state (Line 6). If the test case triggers a defect of M, tn is\nenqueued to U (Line 8). If tn covers a new space of M, it is enqueued to Q as a seed for the testing\nprocedure later (Line 10). At the end of the coverage-guided testing procedure, U, which contains\nthe set of the failed test cases, returned (Line 11).\nHere, we leverage five types of mutation operators to generate new test cases [44, 50]: synonym\nreplacement, random deletion, random insertion, random swap, and punctuation insertion. Synonym\nreplacement randomly selects words in the text and replaces them with synonyms. Random deletion\nremoves words from random positions in the text, while random insertion adds words at random\npositions. Random swap randomly exchanges the positions of two words, and punctuation insertion\nrandomly adds punctuation marks in the text."}, {"title": "5 Experiment", "content": "In this section, we perform an evaluation of LECOV to demonstrate its usefulness and effectiveness.\nIn particular, we mainly investigate the following research questions:\n\u2022 RQ1: Can the proposed testing criteria approximate the functional feature of LLMs?\n\u2022 RQ2: How effective are the criteria in conducting test prioritization?\n\u2022 RQ3: Are the proposed criteria effective in guiding the testing procedure to find LLM defects?"}, {"title": "5.1 Experimental Setting", "content": "Experimental Models and Datasets.\nWe choose three open-source models for the experiment: LLaMA2-7B, LLaMA2-13B [51], and\nVicuna [52]. We select four popular benchmark datasets: TruthfulQA [19], TriviaQA [53], Natural\nQuestions Open (NQ-OPEN) [54], and RealToxicityPrompt [13]. The first three datasets focus on\nquestion-answering, with the primary goal to evaluate if the model can understand the question\nand provide correct answers, where the untruthful response is a defect and we use GPT-judge as\nthe judgment model [19]; while RealToxicityPrompt is a text continuation dataset on generating\ncoherent and contextually relevant contents, where generating toxic contents is a typical defect and\nwe utilize LLaMaGuard [47] to estimate the toxicity.\nBaseline and Metrics. In RQ2, we set the budgets (i.e., the number of test cases to prioritize)\nat 5 We use four baselines as the comparison methods: Random, DeepGini [26], MaxP [55], and\nMargin [56]. We use the mean absolute error and mean squared error, which are common metrics\nfor evaluating the prioritization performance, as the evaluation metrics. In RQ3, we compare with\nthree baselines: Random, AutoDAN-GA, and AutoDAN-HGA [15]. The Random method decides\nwhether a new test case is enqueued at random (as in Line 9 of Algorithm 1), which aims to present\nthe usefulness of our guidance and play as an ablation study. AutoDAN is an optimization-based\nalgorithm that generates test cases from seed prompts using genetic algorithms (GA) and hierarchical\ngenetic algorithms (HGA). We evaluate the performance of these test generation methods using the\nTest Success Rate (TSR), which measures the proportion of test cases that successfully trigger LLM\ndefects.\nHardware Dependencies. All of our experiments were conducted on a server with a 4.5GHz AMD\n5955WX 16-Core CPU, 256GB RAM, and two NVIDIA A6000 GPUs with 48GB VRAM each."}, {"title": "5.2 Experimental Evaluation", "content": ""}, {"title": "5.2.1 RQ1: Can the proposed testing criteria approximate the functional feature of LLMs?", "content": "In this RQ, we want to examine whether the testing criteria can reflect and approximate the func-\ntional features of LLMs. We follow the assumption that a label category reflects a functional feature,\nwhich is an attribute that the model learns to recognize and use for predictions or decisions [57].\nSpecifically, we randomly generate 1,000 test cases using seed queue mutation, starting with 100\ninitial seeds that contain one, two, three, or all label categories, and then observe the coverage"}, {"title": "5.2.2 RQ2: How effective are the criteria in conducting test prioritization?", "content": "In this RQ, we compare the performance of test prioritization using the proposed criteria against\nbaseline methods. Test prioritization was conducted with budgets of 5\nOur proposed metrics outperform the baseline method in test prioritization. For example, in\nLLaMA-7B with RealToxicityPrompt, the MAE and MSE of the attention-wise metrics KKAC is\nthe lowest (0.78 and 0.78 respectively); in LLaMA-7B with TriviaQA, the average MAE and MSE\nof the neuron-wise metrics ITNC is 0.49 and 0.29, respectively; in Vicuna with NQ-OPEN, the\naverage MAE and MSE of FHNC is 0.48 and 0.27.\nAttention-wise criteria can provide effective prioritization, with KMAC, KKAC, and KSAC emerg-\ning as effective metrics across multiple datasets and models, e.g., they achieve the lowest error rate\nseven times in dataset-model pairs. They consistently provide lower MAE/MSE, suggesting that\nleveraging attention mechanisms for test prioritization is beneficial. Neuron-wise criteria, also fre-\nquently achieve the best performance, e.g., FHNC delivers the best performance for Vicuna with\nTruthfulQA, TriviaQA, and NQ-OPEN (0.47/0.26, 0.44/0.24, and 0.48/0.27).\nDifferent models exhibit varying strengths depending on the criteria used, and the performance of\nthese criteria also varies across different model-dataset pairs. For instance, KSAC is good with\nLLaMA2-7B-NQ-OPEN, while Vicuna benefits from KMEC. Certain criteria, e.g., KSAC, consis-\ntently perform well across diverse datasets and models, indicating their robustness and adaptability.\nAdditionally, the criteria typically offer consistent performance under different budgets."}, {"title": "5.2.3 RQ3: Are the proposed criteria effective in guiding the testing procedure to find LLM\ndefects?", "content": "Fig. 2 shows the test success rate of the baseline methods compared to the proposed coverage-\nguided testing. The coverage-guided testing outperforms the baseline methods across all models\nand datasets. The average TSR of the proposed criteria on LLaMA2-7B and Vicuna over all datasets\nare 76 and 49, respectively, while the ones of baseline methods are 57 and 43, respectively. IHNC\ngenerally shows high scores across most models and datasets, i.e., it achieves the best of six trials\nover eight model-dataset pairs. For example, it gets a TSR of 90 for LLaMA2-7B\u2013TriviaQA, and\n86 for LLaMA2-7B-NQ-OPEN. This demonstrates the effectiveness of using the proposed criteria\nin coverage-guided testing. For the baseline, the AutoDAN-HGA technique generally shows higher\nperformance than the AutoDAN-GA and Random baselines across most models and datasets. For\nexample, in the LLaMA2-7B model, the TSR of AutoDAN-HGA is 70 for NQ-OPEN, compared to\n66 for AutoDAN-GA and 60 for Random."}, {"title": "6 Related Work", "content": "Testing Criteria of DNNs. Researchers have been actively exploring indicators to better represent\nDNN behavior patterns as criteria for guiding diverse quality assurance methods in DNN-centric\nsystems, which can be categorized into two types: those relying on internal white-box analysis and\nthose focusing on external black-box analysis. For the former, structural test coverage criteria have\ngarnered significant attention [30, 58, 22, 32]. They typically analyze neuron activation patterns,\ncompute their distribution on both training and test data, and perform comparisons. This analysis\ncan be extended to specific structures, such as RNNs, using internal-neuron-based stateful abstrac-\ntion [48, 59]. For black-box analysis, both uncertainty-wise metrics (model-specific) [26] and test\ndata diversity metrics (model-agnostic) [55, 25] have proven effective in DNN testing. Different\nfrom most previous studies that focus on specific-purpose models (e.g., CNNs, RNNs) and classifi-\ncation tasks, we aim to explore LLM-specific criteria for general foundation models.\nTrustworthiness of LLMs. While LLMs demonstrate record-breaking performance on various\ndownstream tasks, recent studies have highlighted concerns about their trustworthiness in practical\napplications. Benchmarks [10, 60, 9] focusing on issues such as hallucination, safety, fairness, ro-\nbustness, privacy, and machine ethics reveal that current LLMs are still far from perfect. In response\nto these challenges, efforts to improve LLM trustworthiness include designing better prompts for\nmore appropriate responses [61, 62], leveraging fine-tuning techniques for safety alignment [63\u201365],\nand building safeguard systems to mitigate potential risks [66, 47]. Unlike these direct improvement\napproaches, our paper's criteria focus on offline testing stages to identify potential trustworthiness\nissues. Additionally, the criteria studied in this paper can potentially enhance the effectiveness of\nthe aforementioned improvement methods."}, {"title": "7 Conclusion", "content": "In this work, we propose LECOV, a set of multi-level testing coverage criteria for LLMs. LECOV\nincludes three types of criteria: attention-wise coverage, neuron-wise coverage, and uncertainty-\nwise coverage. We apply these criteria to two application scenarios, i.e., test prioritization and\ncoverage-guided testing, and the experimental results demonstrate their effectiveness and usefulness.\nFuture work will explore how to utilize these criteria in the fine-tuning or retraining process to further\nenhance the trustworthiness of LLMs."}]}