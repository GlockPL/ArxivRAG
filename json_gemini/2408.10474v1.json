{"title": "LeCov: Multi-level Testing Criteria for Large Language Models", "authors": ["Xuan Xie", "Jiayang Song", "Yuheng Huang", "Da Song", "Fuyuan Zhang", "Felix Juefei-Xu", "Lei Ma"], "abstract": "Large Language Models (LLMs) are widely used in many different domains, but because of their limited interpretability, there are questions about how trustworthy they are in various perspectives, e.g., truthfulness and toxicity. Recent research has started developing testing methods for LLMs, aiming to uncover untrustworthy is-sues, i.e., defects, before deployment. However, systematic and formalized testing criteria are lacking, which hinders a comprehensive assessment of the extent and adequacy of testing exploration. To mitigate this threat, we propose a set of multi-level testing criteria, LECOV, for LLMs. The criteria consider three crucial LLM internal components, i.e., the attention mechanism, feed-forward neurons, and un-certainty, and contain nine types of testing criteria in total. We apply the criteria in two scenarios: test prioritization and coverage-guided testing. The experiment evaluation, on three models and four datasets, demonstrates the usefulness and effectiveness of LECOV.", "sections": [{"title": "1 Introduction", "content": "Recent research highlights the phenomenal accomplishments of Large Language Models (LLMs) across various fields, such as natural language processing [1], code generation [2], and robotic sys-tem control [3, 4]. After being trained on large and varied datasets [5-7], LLMs can generate answers that mimic human intellect and common sense understanding. They have proven essential in many applications due to their adaptability, efficiency, and scalability, greatly contributing to the advancement of artificial general intelligence [8].\nThe rapid deployment of LLM also raises the concern about the trustworthiness of LLM [9\u201311], e.g., hallucination [12] and toxicity [13]. Hence, recent work focuses on developing trustworthiness analysis techniques for LLMs, and a recent trend is uncovering untrustworthy responses\u00b9 through LLM testing [14-17]. For example, Hong et al. propose to train a red teaming testing LLM (as a tester) by maximizing novel reward and entropy reward [14]. AutoDAN [15] can automatically produce testing prompts (i.e., test cases) by hierarchical genetic algorithms.\nAlthough various testing techniques have been developed to assess the trustworthiness of LLMs, a systematic approach for measuring their testing sufficiency and coverage is still missing. Test coverage criteria are crucial for ensuring that the model is evaluated across a broad spectrum of scenarios, thereby increasing its reliability in real-world applications. Therefore, there is an urgent need for formalized and systematic criteria to evaluate the sufficiency of LLM testing, which would enhance our understanding of test set quality. Moreover, existing techniques primarily focus on analyzing the input/output behavior of LLMs, which provides a limited perspective. Incorporating an analysis of other components, such as the internal structure of LLMs, could offer deeper insights into their behavior.\nWe also called it as defects of LLMs in this work."}, {"title": "2 Background", "content": ""}, {"title": "2.1 LLM Defects", "content": "Software defects are generally defined as flaws or errors that affect a system's reliability, quality, and availability [18]. We extend this concept as LLM defects considering the human interaction characteristics of LLM-driven systems (e.g., chatbots). These defects refer to scenarios where the responses of LLMs fail to meet the expectations of various stakeholders. From an objective stand-point, LLM defects include issues related to correctness and truthfulness. Subjectively, they also encompass concerns such as safety, privacy, and machine ethics. All these defects impact the trust-worthiness of LLM-driven systems [9]. In this study, we focus on two types of defects: an objective defect, where LLM responses deviate from real-world truth (i.e., hallucination) [19, 20], and a sub-jective defect, where LLM responses are toxic and potentially violate regulations [13]."}, {"title": "2.2 Deep Learning System Testing", "content": "Testing has long been a critical method for understanding system performance and identifying po-tential issues to ensure higher quality [21]. In related work on DNN systems, the focus often lies on classification-only models, exploring how to perform effective testing by automatically generating new test cases [22-25] and accelerating the testing process by prioritizing test cases where DNNs are more likely to fail [26-29]. Both methods rely on indicators of system states, often referred to as testing criteria [30\u201332]. For automated test generation, a criterion similar to code coverage is necessary to guide the process and uncover valuable test cases within the vast input space. For test case prioritization, a criterion that reflects the system's capabilities for handling different inputs is essential to identify potential errors among the large volume of test cases. In this study, we extend the exploration of testing criteria from classification-based models to auto-regressive foundation models, paving the way for future advancements in LLM testing frameworks."}, {"title": "3 Testing Criteria for LLM", "content": "In this section, we introduce the proposed testing criteria, LECOV, for LLMs. LECOV includes three types of criteria: attention-wise, neuron-wise, and uncertainty-wise. Attention-wise coverage is motivated by the unique attention mechanism of LLMs; neuron-wise coverage follows the principles of DNN testing [32-34]; and uncertainty-wise coverage is inspired by recent studies showing that the quality of LLM responses is closely linked to uncertainty [35, 36]."}, {"title": "3.1 Attention-wise Coverage Criteria", "content": "With the goal of artificial general intelligence, the training dataset of LLMs is universal, encompass-ing a wide range of data (e.g., LLaMA 3.1 is trained on more than 15 trillion tokens [37]). Nev-ertheless, testing on a similarly large scale of data is intractable due to the computational expense of LLMs. Intrinsically, the attention value is an approximation to the training distribution, w.r.t. the extent of input relevance [38, 39]. At the attention level, we utilize the output of the attention head, i.e., the attention value, to characterize its behavior. We gauge the coverage of the attention value using a statistical measurement, which will be introduced later. Specifically, this measurement characterizes the attention value and maps it to a scalar, allowing us to compute criteria. At a high level, let's denote the upper bound of the measurement as UB, and the lower bound as LB. The coverage space of an attention head a lies within [LB, UB]. We divide [LB, UB] into k sections, and each section is considered covered if the measurement falls within that section.\nLet \\(A = \\{a_1, a_2, . . . , a_n \\}\\) be the set of attention heads, and \\(\\kappa_a(x)\\) be the output of the head a, given an input x at time t. We define the k-multisection coverage of an attention head a as:\n\\(\\frac{| \\{S_i^a | \\exists x \\in X, \\exists t \\in [0, T_x], \\kappa(\\alpha_t(x)) \\in S_i^a\\} |}{k}\\)\nwhere \\(S_i^a = [lb_i, ub_i]\\) is the i-th section, x is a test prompt in the test set X, \\(T_x\\) is the total generated token length of LLM given x as the input, and \\(\\kappa\\) is a statistical measurement for describing the characteristics of a distribution.\nThe k-multisection coverage of an LLM is defined as:\n\\(\\frac{\\Sigma_{a \\in A} {\\delta_a | \\exists x \\in X, \\kappa(\\alpha_t(x)) \\in S_i^a\\}}{k \\times |A|}\\)\nPrevious methods of gauging the coverage of DNN by the neuron activation [31, 40, 33] (which is a scalar), are not suitable in attention coverage computation, since, the attention values are vectors, e.g., the output shape of an attention head of LLaMA-7B is 4,096. Hence, we use statistical mea-surements to describe and identify patterns and trends of the attention value. In particular, for \\(\\kappa\\), we utilize four statistical measurements to describe the attention values: mean, variance, kurtosis, and skewness. These measurements describe the characteristics of a set of data, i.e., distribution, quanti-tatively and they provide different insights into the distribution, such as the central tendency (mean), variability (variance), and the distribution shape (kurtosis and skewness). In total, we have four attention coverage: k-multisection Mean Attention Coverage (KMAC), k-multisection Vari-ance Attention Coverage (KVAC), k-multisection Kurtosis Attention Coverage (KKAC), and k-multisection Skewness Attention Coverage (KSAC). Intuitively, KMAC measures indicate the central or typical value within a dataset, KVAC describes the spread or variability of the distribution, while KKAC and KSAC characterize the asymmetry or the concentration of data points in the tails of the distribution, respectively."}, {"title": "3.2 Neuron-wise Coverage Criteria", "content": "Unlike classic feed-forward neural networks, LLMs exhibit temporal semantics, meaning the model generates a sequence of different outputs over time in response to a given input prompt. As a result, existing neuron coverage criteria, such as neuron coverage [30] and neuron boundary coverage [31], are inadequate for quantitatively measuring the testing adequacy of LLMs because they do not ac-count for temporal behavior. To address this, we propose three novel neuron-wise coverage criteria tailored to the time-varying characteristics of LLMs. Specifically, the neuron-wise criteria are di-vided into two types: instant level, which considers neuron activation at a single timestamp, and frequent level, which considers neuron activation across multiple timestamps.\nInstant Hyperactive Neuron Coverage (IHNC). We define instant hyperactive neuron coverage INHC as:\n\\(IHNC(X, h) = \\frac{|\\{n | \\exists x \\in X, \\exists t \\in [0,T_x], \\mathbb{I}[f_n(x) > h]\\}|}{|N|}\\)\nwhere \\(T_x\\) is the total output time steps given input x, \\(f_n(x)\\) is the output of a neuron n, N is the set of neurons, x is an input, t is a time step, and h is the threshold. The neuron that has been activated during the generation process is considered covered.\nInstant Top-K Neuron Coverage (ITNC). We first define an auxiliary function top(x,l, t, k), which takes a test case x, a selected layer l, a time step t, and a rank k as input, and returns the neurons in l-th layer that are ranked as the top k according to the activation values at time step t of the generation procedure. Then, ITNC is defined as:\n\\(ITNC(X, k, r) = \\frac{|\\bigcup_{x \\in X} \\bigcup_{l \\in L} \\bigcup_{t \\in [0,T_x]} top(x, l, t, k)|}{|N|}\\)\nThis criterion gauges the proportion of the neurons that are the k-th most active within the layer, for at least one time step during the generation. The neurons in the same layer are considered to have similar functionality, and a relatively higher activation means it plays a more pivotal role [31]. ITNC compares the activation values over different neurons in the same layer and counts the crucial ones.\nFrequent Hyperactive Neuron Coverage (FHNC). We first define a predicate PFHNC for a neuron n on whether it has been activated more than r times during the generation:\n\\(P_{FHNC}(x, n, h, r) = |\\{t | f_n(x) > h, t \\in [0,T_x]\\}| > r\\).\nIntuitively, if neuron n has been activated multiple times, it indicates that it is highly impacting the output content during that period of generation.\nFHNC is then defined as:\n\\(FHNC(X, h, r) = \\frac{|\\{n | \\exists x \\in X, P_{FHNC}(x, n, h, r)\\}|}{|N|},\\)\nwhere the neurons that have been activated for r times during the generation process, i.e., highly affect the generation, are considered as covered."}, {"title": "3.3 Uncertainty-wise Coverage Criteria", "content": "Uncertainty of an LLM refers to the degree of confidence the model has in its predictions [36, 41\u201343], essentially quantifying the expected variability or reliability of those predictions. Recent studies indicate that the quality of LLM outputs is closely tied to various forms of uncertainty [36, 41]. Consequently, we select the exploration of the uncertainty space as a testing criterion. Specifically, the uncertainty-wise coverage criteria include k-multisection entropy coverage and k-multisection likelihood coverage.\nk-Multisection Entropy Coverage (KMEC). The KMEC is computed as:\n\\(KMEC(X) = \\frac{|\\{S_i^H | \\exists x \\in X, \\exists t \\in [0,T_x], H(x, t) \\in S_i^H\\}|}{k},\\)\nwhere H(x, t) is the entropy at time step t of the generation given a test case x, k is the section number, and \\(S_i^H = [lb_i, ub_i]\\) is the i-th section.\nk-Multisection Likelihood Coverage (KMLC). The KMLC is computed as:\n\\(KMEC(X) = \\frac{|\\{S_i^H | \\exists x \\in X, \\exists t \\in [0,T_x], L(x, t) \\in S_i^H\\}|}{k},\\)\nL(x, t) is the average output likelihood at time step t of the generation given input x."}, {"title": "4 Application", "content": "To demonstrate the usefulness of the newly proposed criteria, we apply LECOV to two practical application scenarios: (1) test prioritization, which prioritizes test cases likely to expose untrustwor-thy behavior, i.e., defect, in the LLM under test; and (2) coverage-guided testing, which generates defect-inducing test cases to evaluate the model.\nTest Prioritization. Test prioritization for LLMs involves choosing a subset of test cases that are likely to trigger errors during the model's operations [44, 26, 45]. Given the extensive capabilities of LLMs, exhaustive testing can be computationally expensive and resource-intensive. By prioritizing a subset of high-risk test cases, developers can focus on addressing the most critical risks, such as minimizing biases [46] and preventing harmful outputs [47, 41]. Specifically, given a test set X and a model M, test selection aims to choose a subset \\(\\{x_1,...,x_k\\}\\) (k < |X|) that is most likely to trigger errors. In our approach, we rank the test cases based on the selected coverage criteria and prioritize those with the highest coverage value.\nCoverage-Guided Testing. Coverage-Guided Testing (CGT) of LLMs aims to systematically ex-plore the model's input space to ensure a comprehensive evaluation [48, 22, 49]. CGT leverages the proposed criteria to guide the test generation process and assess the quality of the generated samples. The process begins with selecting a test case from a queue that keeps seed samples with the potential to trigger model errors. The mutation is then applied to generate mutants for testing the model. If a mutant increases coverage, it is pushed back into the queue as a sample likely to trigger errors.\nAlgorithm 1 summarizes the detailed procedure of the coverage-guided testing of LLM. The inputs to the algorithms include the initial seeds of test case I, the testing budget b, and the LLM under test M. At first, the test case queue Q is enqueued with I, defect queue U is empty, and the counter i is initialized as 0 (Line 1). The algorithm enters the testing loop until the test budget limit is reached (Line 2). At each test iteration, a test case \\(t_0\\) is first dequeued from Q (Line 4). To obtain new test cases that could incur faults of LLM, we perform mutation (which will be introduced later) on \\(t_0\\) to produce a new prompt \\(t_n\\) (Line 5). The new test case \\(t_n\\) is given to M, and we get the output of the LLM r and the internal states of LLM state (Line 6). If the test case triggers a defect of M, \\(t_n\\) is enqueued to U (Line 8). If \\(t_n\\) covers a new space of M, it is enqueued to Q as a seed for the testing procedure later (Line 10). At the end of the coverage-guided testing procedure, U, which contains the set of the failed test cases, returned (Line 11).\nHere, we leverage five types of mutation operators to generate new test cases [44, 50]: synonym replacement, random deletion, random insertion, random swap, and punctuation insertion. Synonym replacement randomly selects words in the text and replaces them with synonyms. Random deletion removes words from random positions in the text, while random insertion adds words at random positions. Random swap randomly exchanges the positions of two words, and punctuation insertion randomly adds punctuation marks to the text."}, {"title": "5 Experiment", "content": "In this section, we perform an evaluation of LECOV to demonstrate its usefulness and effectiveness. In particular, we mainly investigate the following research questions:\n\u2022 RQ1: Can the proposed testing criteria approximate the functional feature of LLMs?\n\u2022 RQ2: How effective are the criteria in conducting test prioritization?\n\u2022 RQ3: Are the proposed criteria effective in guiding the testing procedure to find LLM defects?"}, {"title": "5.1 Experimental Setting", "content": "Experimental Models and Datasets.\nWe choose three open-source models for the experiment: LLaMA2-7B, LLaMA2-13B [51], and Vicuna [52]. We select four popular benchmark datasets: TruthfulQA [19], TriviaQA [53], Natural Questions Open (NQ-OPEN) [54], and RealToxicityPrompt [13]. The first three datasets focus on question-answering, with the primary goal to evaluate if the model can understand the question and provide correct answers, where the untruthful response is a defect and we use GPT-judge as the judgment model [19]; while RealToxicityPrompt is a text continuation dataset on generating coherent and contextually relevant contents, where generating toxic contents is a typical defect and we utilize LLaMaGuard [47] to estimate the toxicity.\nBaseline and Metrics. In RQ2, we set the budgets (i.e., the number of test cases to prioritize) at 5 We use four baselines as the comparison methods: Random, DeepGini [26], MaxP [55], and Margin [56]. We use the mean absolute error and mean squared error, which are common metrics for evaluating the prioritization performance, as the evaluation metrics. In RQ3, we compare with three baselines: Random, AutoDAN-GA, and AutoDAN-HGA [15]. The Random method decides whether a new test case is enqueued at random (as in Line 9 of Algorithm 1), which aims to present the usefulness of our guidance and play as an ablation study. AutoDAN is an optimization-based algorithm that generates test cases from seed prompts using genetic algorithms (GA) and hierarchical genetic algorithms (HGA). We evaluate the performance of these test generation methods using the Test Success Rate (TSR), which measures the proportion of test cases that successfully trigger LLM defects.\nHardware Dependencies. All of our experiments were conducted on a server with a 4.5GHz AMD 5955WX 16-Core CPU, 256GB RAM, and two NVIDIA A6000 GPUs with 48GB VRAM each."}, {"title": "5.2 Experimental Evaluation", "content": ""}, {"title": "5.2.1 RQ1: Can the proposed testing criteria approximate the functional feature of LLMs?", "content": "In this RQ, we want to examine whether the testing criteria can reflect and approximate the func-tional features of LLMs. We follow the assumption that a label category reflects a functional feature, which is an attribute that the model learns to recognize and use for predictions or decisions [57]. Specifically, we randomly generate 1,000 test cases using seed queue mutation, starting with 100 initial seeds that contain one, two, three, or all label categories, and then observe the coverage changes. Tab. 1 shows the coverage results. We observe that coverage increases with the number of label categories used as initial seeds, indicating that more functional features are being exploited. For instance, in LLaMA-7B, ITNC starts at 0.1881 with only one category, and rises to 0.2016, 0.2166, and 0.3211 when two, three, and all categories are included, respectively. Moreover, dif-ferent criteria exhibit varying sensitivity degrees, which reflects different dimensions of the feature space.\nFor example, INHC achieves over 80 In contrast, KSAC increases more than 200\nAnswer to RQ1: The proposed coverage criteria, which are based on LLM internal informa-tion, can approximate and reflect the functional feature space of LLMs."}, {"title": "5.2.2 RQ2: How effective are the criteria in conducting test prioritization?", "content": "In this RQ, we compare the performance of test prioritization using the proposed criteria against baseline methods. Tab. 2 presents the results of test prioritization for LLMs. We report the mean absolute error (MAE) and mean squared error (MSE), which are common metrics for evaluating prioritization performance. Test prioritization was conducted with budgets of 5\nOur proposed metrics outperform the baseline method in test prioritization. For example, in LLaMA-7B with RealToxicityPrompt, the MAE and MSE of the attention-wise metrics KKAC is the lowest (0.78 and 0.78 respectively); in LLaMA-7B with TriviaQA, the average MAE and MSE of the neuron-wise metrics ITNC is 0.49 and 0.29, respectively; in Vicuna with NQ-OPEN, the average MAE and MSE of FHNC is 0.48 and 0.27.\nAttention-wise criteria can provide effective prioritization, with KMAC, KKAC, and KSAC emerg-ing as effective metrics across multiple datasets and models, e.g., they achieve the lowest error rate seven times in dataset-model pairs. They consistently provide lower MAE/MSE, suggesting that leveraging attention mechanisms for test prioritization is beneficial. Neuron-wise criteria, also fre-quently achieve the best performance, e.g., FHNC delivers the best performance for Vicuna with TruthfulQA, TriviaQA, and NQ-OPEN (0.47/0.26, 0.44/0.24, and 0.48/0.27).\nDifferent models exhibit varying strengths depending on the criteria used, and the performance of these criteria also varies across different model-dataset pairs. For instance, KSAC is good with LLaMA2-7B-NQ-OPEN, while Vicuna benefits from KMEC. Certain criteria, e.g., KSAC, consis-tently perform well across diverse datasets and models, indicating their robustness and adaptability. Additionally, the criteria typically offer consistent performance under different budgets.\nAnswer to RQ2: The experimental result demonstrates that the proposed metrics are effective in providing test prioritization guidance and they typically have a better performance than the existing baseline methods. In practice, we recommend the user should first utilize the attention-wise and neuron-wise metrics and try other criteria for better performance."}, {"title": "5.2.3 RQ3: Are the proposed criteria effective in guiding the testing procedure to find LLM defects?", "content": "Fig. 2 shows the test success rate of the baseline methods compared to the proposed coverage-guided testing. The coverage-guided testing outperforms the baseline methods across all models and datasets. The average TSR of the proposed criteria on LLaMA2-7B and Vicuna over all datasets are 76 and 49, respectively, while the ones of baseline methods are 57 and 43, respectively. IHNC generally shows high scores across most models and datasets, i.e., it achieves the best of six trials over eight model-dataset pairs. For example, it gets a TSR of 90 for LLaMA2-7B\u2013TriviaQA, and 86 for LLaMA2-7B-NQ-OPEN. This demonstrates the effectiveness of using the proposed criteria in coverage-guided testing. For the baseline, the AutoDAN-HGA technique generally shows higher performance than the AutoDAN-GA and Random baselines across most models and datasets. For example, in the LLaMA2-7B model, the TSR of AutoDAN-HGA is 70 for NQ-OPEN, compared to 66 for AutoDAN-GA and 60 for Random.\nAnswer to RQ3: Our coverage-guided testing is effective in guiding the testing procedure to find faults for diverse models and datasets. Criteria like IHNC generally yield higher test success rates and demonstrate their usefulness in practice."}, {"title": "6 Related Work", "content": "Testing Criteria of DNNs. Researchers have been actively exploring indicators to better represent DNN behavior patterns as criteria for guiding diverse quality assurance methods in DNN-centric systems, which can be categorized into two types: those relying on internal white-box analysis and those focusing on external black-box analysis. For the former, structural test coverage criteria have garnered significant attention [30, 58, 22, 32]. They typically analyze neuron activation patterns, compute their distribution on both training and test data, and perform comparisons. This analysis can be extended to specific structures, such as RNNs, using internal-neuron-based stateful abstrac-tion [48, 59]. For black-box analysis, both uncertainty-wise metrics (model-specific) [26] and test data diversity metrics (model-agnostic) [55, 25] have proven effective in DNN testing. Different from most previous studies that focus on specific-purpose models (e.g., CNNs, RNNs) and classifi-cation tasks, we aim to explore LLM-specific criteria for general foundation models.\nTrustworthiness of LLMs. While LLMs demonstrate record-breaking performance on various downstream tasks, recent studies have highlighted concerns about their trustworthiness in practical applications. Benchmarks [10, 60, 9] focusing on issues such as hallucination, safety, fairness, ro-bustness, privacy, and machine ethics reveal that current LLMs are still far from perfect. In response to these challenges, efforts to improve LLM trustworthiness include designing better prompts for more appropriate responses [61, 62], leveraging fine-tuning techniques for safety alignment [63-65], and building safeguard systems to mitigate potential risks [66, 47]. Unlike these direct improvement approaches, our paper's criteria focus on offline testing stages to identify potential trustworthiness issues. Additionally, the criteria studied in this paper can potentially enhance the effectiveness of the aforementioned improvement methods."}, {"title": "7 Conclusion", "content": "In this work, we propose LECOV, a set of multi-level testing coverage criteria for LLMs. LECOV includes three types of criteria: attention-wise coverage, neuron-wise coverage, and uncertainty-wise coverage. We apply these criteria to two application scenarios, i.e., test prioritization and coverage-guided testing, and the experimental results demonstrate their effectiveness and usefulness. Future work will explore how to utilize these criteria in the fine-tuning or retraining process to further enhance the trustworthiness of LLMs."}]}