{"title": "EXION: Exploiting Inter- and Intra-Iteration Output Sparsity for Diffusion Models", "authors": ["Jaehoon Heo", "Adiwena Putra", "Jieon Yoon", "Sungwoong Yune", "Hangyeol Lee", "Ji-Hoon Kim", "Joo-Young Kim"], "abstract": "Over the past few years, diffusion models have emerged as novel solutions in Al industries, offering the capability to generate diverse, multi-modal outputs such as images, videos, and motions from input prompts (i.e., text). Despite their impressive capabilities, diffusion models face significant challenges in computing, including excessive latency and energy consumption due to the numerous iterations in model architecture. Although prior works specialized in transformer acceleration can be applied to diffusion models, given that transformers are key components, the problem of the iterative nature of diffusion models still needs to be addressed.\nIn this paper, we present EXION, the first software-hardware co-designed diffusion accelerator that solves the computation challenges of excessive iterations by exploiting the unique inter- and intra-iteration output sparsity in diffusion models. To this end, we propose two software-level optimizations in EXION. First, we introduce the FFN-Reuse algorithm that identifies and skips redundant computations in FFN layers across different iterations (i.e., inter-iteration sparsity). Second, we use a modified eager prediction method that employs two-step leading-one detection to accurately predict the attention score in diffusion models, skipping unnecessary computations within an iteration (i.e., intra-iteration sparsity). We also introduce a novel data compaction mechanism named ConMerge, which can enhance hardware utilization by condensing and merging large and sparse matrices into small and compact forms. Finally, EXION has a dedicated hardware architecture that supports the above sparsity-inducing algorithms, translating high output sparsity into improved energy efficiency and performance. To verify the feasibility of the EXION accelerator, we first demonstrate that it has no impact on accuracy in various types of multi-modal diffusion models, including text-to-motion, -audio, -image, and -video. We then instantiate EXION in both server- and edge-level settings and compare its performance against GPUs with similar specifications. Our evaluation shows that EXION achieves dramatic improvements in performance and energy efficiency by 3.2-379.3x and 45.1-3067.6\u00d7 compared to a server GPU and by 42.6-1090.9\u00d7 and 196.9-4668.2\u00d7 compared to an edge GPU.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, the demand for artificial intelligence (AI) that can imagine and create innovative information like humans has been a driving force in machine learning (ML) research. As a result, diffusion models [12] have emerged as promising solutions in various generative AI industries. For instance, with a simple text prompt as input, Stable Diffusion [35] can generate high-quality images, and Sora [1] can generate vivid and complex videos. Some models can easily generate human motions from text [43] or audio [44] inputs. This versatility highlights their applicability across multiple applications,\nmaking diffusion models a crucial aspect of generative Al technology.\nAlthough diffusion models can create remarkably accurate and detailed outputs, they come with high energy consumption and long latency. This is due to their fundamental characteristic of starting with random noise and progressively removing it to obtain the desired outputs over numerous iterations, which can reach up to 1000 [12]. Furthermore, as diffusion models are primarily based on transformer blocks [45], each iteration involves a significant number of operations. Consequently, their energy consumption and latency are significantly higher than those of other generative models. For example, Stable Diffusion generates images from text prompts with better quality and diversity compared to StyleGAN-XL [37], a popular generative adversarial network. However, our experimental results show that on an identical GPU device, NVIDIA's RTX 6000 Ada [26], the energy consumption and latency of Stable Diffusion are 1546.7J and 11.8s, respectively, which are 23.6\u00d7 and 9.8\u00d7 higher than those of StyleGAN-XL.\nTo solve these problems derived from repeated iterations,\na few software (SW)-based approaches [6], [19], [36], [39]\nthat optimize the scheduler of the diffusion model have been\nproposed to reduce the large number of iterations, i.e., infer-\nence steps. However, these fast sampling methods come at the\ncost of accuracy [53]. Additionally, some of them even require\nretraining of the pre-trained diffusion model [21].\nAnother approach to accelerating the diffusion model is\nusing specialized hardware (HW) accelerators to speed up a\nlarge number of operations in each iteration without modifying\nthe scheduler aggressively. Existing transformer accelerators\ncan be utilized [9], [20], [32] to speed up computations in each\niteration. Most previous transformer accelerators focus on op-\ntimizing the query, key, and value (QKV) projection and atten-\ntion computation using methods such as eager prediction [32]\nand attention pruning [20]. However, these optimizations do\nnot significantly reduce the energy consumption and latency\nof the overall diffusion process since they have not exploited\nthe iterative nature of it. To achieve optimal performance, it\nis necessary to exploit the unique sparsity that exists only in\ndiffusion models.\nIn this work, we present EXION, the first diffusion accel-\nerator developed to solve the energy consumption and latency\nproblems in diffusion models by exploiting inter- and intra-\niteration output sparsity. To this end, we adopt a SW-HW co-"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "Figure 2 shows an overview of the diffusion process, which comprises forward diffusion and reverse denoising processes, using a text-to-image generation example [23]. During the forward diffusion process, the model progressively adds noise to the original input data over numerous iterations, converting the given input to noise. In contrast, the reverse denoising process starts from noise and revives the original input data.\nDuring the training phase, the diffusion network learns to predict noise from input data at the corresponding iteration while repeating the forward diffusion and reverse denoising processes. By subtracting the predicted noise from the input data, the network produces output data with a reduced noise level, which becomes the input for the next iteration. For"}, {"title": "B. Eager Prediction Algorithm", "content": "While the number of operations in FFN layers is dominant, this does not mean that QKV projection and attention computation are trivial in diffusion models. To address these efficiently, we adopt an eager prediction (EP) algorithm from the state-of-the-art (SOTA) transformer accelerator [32]. This algorithm first predicts the attention score using simple log-domain arithmetic. Then, using the predicted attention score, it identifies unnecessary output results that can be skipped in more complex real-domain computations.\nFigure 5 (a) explains the log-domain arithmetic in detail. It begins with preprocessing by separating the input and weight data of QKV projection into sign bits and absolute values. Then, it identifies the position of the leading-one bit, i.e., the\nfirst bit that is set to one from the most significant bit (MSB),"}, {"title": "III. EXION'S SOFTWARE-HARDWARE CO-DESIGN STRATEGY", "content": "To reduce the overhead of numerous operations resulting from excessive iterations, EXION first provides SW-level optimizations consisting of the FFN-Reuse and EP algorithms, which lead to inter- and intra-iteration output sparsity patterns, respectively. Notably, the main novelty of this work and the resulting efficiency gains come from the FFN-Reuse algorithm. This algorithm exploits the iterative nature of the diffusion model, achieving unique inter-iteration output sparsity in FFN layers, which are the main overhead of the transformer block. In this section, we will focus on the major aspects that EXION"}, {"title": "A. FFN-Reuse Algorithm for Inter-Iteration Output Sparsity", "content": "Focusing on the fundamental characteristic of diffusion models that progressively removes noise over numerous iterations, we examined data patterns across these iterations. As a result, we discovered that temporal data redundancy exists across different iterations in FFN layers. Consequently, we propose a FFN-Reuse algorithm that can significantly reduce the number of operations in FFN layers by skipping redundant computations.\nFigure 6 describes the algorithm in detail. We first conduct the normal diffusion, i.e., computing all operations required in the FFN layers for a single iteration, which we call a dense iteration. In dense iteration, we also generate the bitmask according to the output of the non-linear layer (e.g., GELU [11] or GEGLU [38]) by comparing the output values with a predetermined threshold. Values larger than this threshold are considered important and need to be recomputed at every iteration, while values smaller than the threshold will be reused for the next N iterations, which we refer to as sparse iterations.\nThe table in Figure 6 shows that the output sparsity of\nthe 1st FFN layer achieves 70% to 97% on average. The\nproposed FFN-Reuse leverages this high output sparsity, di-\nrectly skipping the 1st FFN layer's unnecessary computations.\nThe 2nd FFN layer also skips unnecessary computations by\nonly accumulating the updated values to a partial sum derived\nfrom dense iteration. As a result, by continuously executing\none dense iteration and N sparse iterations during the entire\ndiffusion process, we can skip 52.47% to 85.41% of operations\nin FFN layers within our benchmarks."}, {"title": "B. ConMerge Mechanism for Data Compaction", "content": "While the number of operations in transformer blocks has been significantly reduced, conventional GPUs cannot efficiently leverage the output sparsity. Since GPUs can utilize structured sparsity well, they cannot translate the unstructured inter- and intra-iteration sparsity into high performance and energy efficiency. Additionally, prior works [31], [49] have addressed fine-grained sparsity in either input or weight data by placing additional networks between the memory and\ncomputation engines. Although these methods are efficient,\nthe sparsity in diffusion models is different since it only\nexists in the output data, while input and weight data remain\ndense. Therefore, many existing mechanisms and dedicated\naccelerators [29], [31], [49], [51] designed for input or weight\nsparsity are not suitable for efficiently handling output sparsity.\nTo solve this problem, we introduce the data compaction\nmechanism, ConMerge, which consists of condensing and\nmerging.\nCondensing. Firstly, we propose a simple but efficient\nstrategy named condensing. As shown in Figure 8, when\nall elements in a column are sparse, the condensing process\nremoves the corresponding column. This reduces the number\nof required operations in the MMUL proportionally to the\nreduction in columns. Moreover, it decreases the required\nexternal memory accesses for fetching weight data. The graph\nin the figure shows the percentage of remaining columns after\ncondensing in MLD [5] and Stable Diffusion models. In the\ncase of MLD, condensing is highly efficient, significantly\nreducing the number of columns and leaving only 13.8% of\nthe columns.\nHowever, condensing alone can be less effective. For in-\nstance, compared to MLD, Stable Diffusion has a relatively\nlarge number of rows in the output matrix, making it less likely\nfor all the elements in a column to be zero, even though the\noverall output sparsity is high. Consequently, the remaining\ncolumn percentage after condensing is still 77.4%, indicating\nthe need for an additional method.\nMerging. To efficiently handle the remaining output sparsity\nafter condensing, we propose a method called merging. Before\ndelving into merging, it is essential to consider the MMUL\nfrom an HW perspective. Typically, an HW accelerator run-\nning MMUL operations cannot generate all the output data\nsimultaneously and, therefore, employs a tiling strategy. It\ninvolves partitioning the output matrix into multiple tiled\nblocks, which are executed sequentially.\nConsidering this tiling concept, as illustrated in Figure 9,\nthe first step of the merging is to partition the output matrix"}, {"title": "IV. EXION'S HARDWARE ARCHITECTURE", "content": "Figure 10 illustrates the overall architecture that comprises a top controller with instruction memory (INSTMEM), a diffusion-sparsity aware core (DSC), a global scratchpad (GSC) connected to the DSC via a network-on-chip, and a direct memory access (DMA) that stores/loads data to/from external DRAM.\nThe heart of the architecture is the DSC, which can run the diffusion model's dense and sparse iterations with an identical core. It is equipped with a sparse-dense unified engine (SDUE) comprising a dot-product unit (DPU) array."}, {"title": "B. Sparse-Dense Unified Engine", "content": "Figure 11 illustrates the design of SDUE, which efficiently computes the most compute-intensive MMUL operations in\nthe transformer block, regardless of the presence of output\nsparsity. It consists of an array of DPUs, and each DPU con-\ntains integer multipliers, followed by Wallace tree adders [46]\nand accumulation registers. For normal (dense) MMUL oper-\nations without any output sparsity, each DPU receives input\nand weight data from the corresponding memory banks (e.g.,\nDPU(0,0) receives input from IMEM bank #0 and WMEM\nbank #0), executes MAC operations repeatedly and generates\nthe final output of the dot product.\nAs for the MMUL computation with output sparsity, SDUE\nmaps perfectly to the result of the ConMerge mechanism.\nIn order to achieve this, each row of the DPU array, named\nDPU lane, is equipped with a conflict vector switch (cv_sw).\nConsequently, each DPU has two input lines broadcasted from\ntwo different IMEM banks. We refer to these lines as the\noriginal line and the conflict line, based on the originating\nIMEM bank being broadcasted. For example, all DPUs within\nDPU lane #0 directly receive data from IMEM bank #0 via\nthe original line, while the conflict line data can come from\nany IMEM bank ranging from #0 to #15, as determined by\nthe CV that controls the cv_sw.\nAdditionally, each DPU is equipped with two switches: a\nweight switch (w_sw) and an input switch (i_sw). The w_sw\nallows the DPU to select among three weight lines broadcasted\nvertically, each corresponding to three WMEMs, WMEM #0 to\nWMEM #2. Since our ConMerge mechanism allows merging\ntwice, all the WMEMs, which are triple-buffered, are utilized.\nWeight data in WMEM's bank corresponds to the column of\nthe weight matrix. Therefore, three columns of weight data,\neach from the bank with the same index in three WMEMS, are\nbroadcasted to each column of the DPU array (e.g., DPU(0,0)\nreceives weight data from bank #0 of the three WMEMs). On\nthe other hand, the i_sw enables the DPU to choose between\nthe two input lines being broadcasted (original or conflict line).\nBoth switches are controlled by control maps (CMs) generated\nby the CAU, and with a combination of them, each DPU can\nflexibly compute any combination of dot product operations\nwith respect to the input and weight being broadcasted.\nConsequently, the SDUE enables the realization of the Con-\nMerge mechanism without using large and complex HW re-\nsources. This design allows EXION to achieve high utilization\nand reduce latency. Additionally, clock gating is applied to all\nthe registers in the SDUE's datapath. This strategy addresses\nany remaining output sparsity after merging, contributing\nto increased energy efficiency. It is important to note that\nvarious accelerators designed to handle fine-grained sparsity\nin input and weight data often include specialized but complex\nnetworks to distribute data to processing engines, which incurs\nadditional latency or area overheads. In contrast, the SDUE\nbroadcasts input and weight data to the DPU array, efficiently\nsupporting the ConMerge mechanism for fine-grained output\nsparsity handling with relatively small overhead."}, {"title": "C. ConMerge Assistant Unit", "content": "In the proposed architecture, the CAU generates the Con-Merge vectors that enable the SDUE to address merged blocks"}, {"title": "D. Eager Prediction Engine", "content": "As aforementioned in Section II, the SOTA transformer accelerator has proposed the EP method that predicts the attention score with a simple log-domain algorithm. However, directly applying this scheme to diffusion models incurs critical accuracy loss. Figure 15 illustrates image generation with the DiT, showing that EP's leading-one detection (LOD) produces a poor-quality image and incurs a PSNR drop compared to applying the FFN-Reuse only."}, {"title": "V. EVALUATION", "content": "To demonstrate the flexibility and accuracy of EXION, we selected seven different diffusion models as our workloads, as they encompass all three types of diffusion models (see Figure 3). These models include various applications such as text-to-motion (MLD [5] and MDM [43]), music-to-motion (EDGE [44]), text-to-image (Stable Diffusion [35]), class-to-image (DiT [30]), text-to-audio (Make-an-Audio [13]), and text-to-video (VideoCrafter2 [2]) generations.\nAccuracy. Since the rationale for achieving output sparsity in diffusion models is fundamentally based on an approximation strategy, we focused on verifying that the accuracy loss incurred by the proposed EXION is minimal. To this end, we tested the model accuracy in various diffusion models with the given dataset. Additionally, to demonstrate that the proposed solution in EXION can be generally adapted to any diffusion model during inference, we did not conduct any retraining. We also determined values such as the threshold in FFN-Reuse and the top-k selection ratio through empirical experiments.\nTable I shows the results of the accuracy evaluation experi-ments. They indicate that the proposed FFN-Reuse algorithm,"}, {"title": "C. Performance Evaluation", "content": "Figure 17 shows the efficiency of the proposed ConMerge mechanism, comprising condensing and merging. To verify their efficiency in each step, the figure displays the remaining column percentage in the output matrix of the first FFN layer and the attention score after applying condensing and then merging. For the condensing case, the average percentage of remaining columns in the first FFN layer and attention score is 60.3% and 80.0%, respectively. Although small models such as MLD and MDM can significantly reduce columns with condensing alone, the average value appears high. This is because, in models where the row size of the output matrix is large, there is less chance that all elements in a column are sparse. For instance, although the output sparsity is high at 97% and 70% in Stable Diffusion and Videocrafter2, respectively, 77.4% and 98.6% of output columns in the FFN layer still remain.\nMerging solves this issue by significantly reducing these remaining columns. The average percentage of remaining columns in the first FFN layer and attention score achieve 16.2% and 50.0%, respectively, after merging. Notably, for the problematic models, only 8.4% and 35.2% of columns in the FFN layer are left for Stable Diffusion and VideoCrafter2, respectively. These results verify that the proposed ConMerge successfully enables the EXION to handle output matrices with high sparsity."}, {"title": "D. Power and Area Breakdown", "content": "Table III breaks down the power consumption and area of the EXION equipped with a single DSC. Comparison with the server GPU verifies that the architecture of EXION is area-efficient. Specifically, the area of EXION24, which contains 24 DSCs and 64MB GSC, occupies 152.28mm\u00b2. This is relatively smaller compared to the server GPU with a die area of 609mm\u00b2 [42]. In terms of power consumption, EXION with a single DSC consumes 1511.43mW in total. Considering the peak power consumption of the server GPU and edge GPU are 300W and 15W, respectively, the power consumption of EX-ION is significantly lower even when scaled out. Furthermore, given the dramatic reduction in energy consumption thanks to utilizing output sparsity, it is acceptable that additional HW components such as EPRE and CAU for sparsity handling consume up to 18.6% of the total power."}, {"title": "VI. RELATED WORKS", "content": "A few SW-based approaches [6], [19], [36], [39] propose fast sampling methods that reduce the number of iterations. However, without retraining, the reduction is limited in achieving acceptable sampling quality [53]. A-DiT [4] suggests skipping computations of certain DiT blocks by reusing cached output data across iterations. Despite these advancements, as these methods rely on GPU-based optimizations, they have not leveraged the opportunities presented by the significant yet unstructured inter- and intra-iteration sparsity. EXION successfully exploits them by proposing the ConMerge mechanism along with a dedicated HW architecture.\nTransformer Accelerators. Recently, a large body of work [8], [9], [20], [33], [48] has been proposed to accelerate attention computation in transformer blocks, primarily targeting language models. However, FFN layers, which have the largest number of operations in recent diffusion models, have not yet been adequately addressed. Although methods handling FFN layers like FACT [32], which reduces energy consumption via mixed-precision, and SpAtten [47], which reduces computations via token pruning, can be applied to diffusion models, the high sparsity present in diffusion models is not yet employed. EXION achieves inter-iteration sparsity by reusing output data across iterations and directly translates this high sparsity into improvements in energy efficiency and performance.\nVarious generalized sparse matrix-matrix multiplication (SpGEMM) accelerators [10], [18], [29], [40], [50], [51] have been proposed to exploit sparsity present in MMUL operations. With the optimization of datapaths and hardware for sparsity handling, they can accelerate SpGEMM operations. SIGMA [31] and Trapezoid [49] propose methods to handle both sparse and dense operations by placing additional networks between memory and computational engines. However, the unique output sparsity present in diffusion models has yet to be addressed. EXION solves this problem by introducing the ConMerge mechanism and a dedicated hardware architecture that efficiently broadcasts both input and weight data."}, {"title": "VII. CONCLUSION", "content": "In this work, we present EXION, the first software-hardware co-designed diffusion accelerator that solves the computation challenges of excessive iterations by exploiting the unique inter- and intra-iteration output sparsity in diffusion models. To this end, we propose two software-level optimizations. First, we propose the FFN-Reuse algorithm that identifies and skips redundant computations in FFN layers across different iterations. Second, we use a modified eager prediction method that employs two-step leading-one detection to predict the attention score accurately. To handle the resulting unstructured sparsity, we introduce the data compaction mechanism, ConMerge, to condense and merge sparse matrices into compact forms. EXION's dedicated architecture supports this, translating high output sparsity into improved energy efficiency and performance. Our evaluation shows that EXION achieves significant improvements in performance and energy efficiency, with gains of up to 379.3\u00d7 and 3067.6\u00d7 over a server GPU and up to 1090.9\u00d7 and 4668.2\u00d7 over an edge GPU."}]}