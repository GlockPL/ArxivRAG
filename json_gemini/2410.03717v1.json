{"title": "Revisiting the Superficial Alignment Hypothesis", "authors": ["Mohit Raghavendra", "Vaskar Nath", "Sean Hendryx"], "abstract": "The Superficial Alignment Hypothesis posits that almost all of a language model's abilities and knowledge are learned during pre-training, while post-training is about giving a model the right style and format. We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering. Taken together, these results shed new light on the Superficial Alignment Hypothesis, suggesting that it is, at best, an over-simplification.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) based on the Transformer architecture have achieved state-of-the-art performance on tasks that involve instruction following, problem-solving, and reasoning (Achiam et al., 2023; Dubey et al., 2024; Vaswani et al., 2017). The standard pipeline for building LLMs powered applications involves unsupervised training of a model on a giant corpus of data to gain general language understanding capability, referred to as pre-training (Brown et al., 2020; Radford et al., 2019). The model is further improved using post-training, which involves finetuning it to excel at a particular domain or behave like a helpful chatbot. This process is also referred to as alignment. The predominant way to do this is through Supervised Finetuning (SFT) where the language model is provided with a prompt, and the model is finetuned to respond to the task (Wei et al., 2022). An additional step is Reinforcement Learning through Human Feedback (RLHF) where a model is trained using reinforcement learning to generate human-preferred responses, by being rewarded for good responses and penalized for bad responses (Ouyang et al., 2022). To achieve the post-training goal of responding appropriately to various user queries, LLMs need to develop several task-specific capabilities, like mathematics, reasoning, utilizing knowledge, and tool use. To teach a model these capabilities, model builders collect human-annotated or synthetically generated"}, {"title": "2. Key Takeaways", "content": "* Post-training performance on a task has a power law relationship of the form $P x D^{1/b}$ with the number of post-training samples, similar to scaling laws established for pertaining and inference (Brown et al., 2024; Kaplan et al., 2020), across models of multiple families and sizes. (Section 3)\n* Evaluating alignment models using win-rates as shown in Zhou et al. (2024) could be misleading for reasoning-based tasks. For instance, LLM-based judges can prefer model generations that exhibit a chatbot-style answer for mathematical questions, even though the model might be poor at mathematical abilities as observed on math benchmarks. (Section 3.3)\n* Through extensive error analysis on tasks like math and multihop reasoning, we see that when a model is finetuned for a task, the improvements in task-specific style and formatting saturate in just 100 examples, as hypothesized by the Superficial Alignment Hypothesis. However, the model's performance on the task is directly correlated with its improvements in reasoning ability, which improves notably during post-training with more finetuning examples. (Section 4)\n* Post-training a model for reasoning can also help a model integrate knowledge beyond its pre-training knowledge cutoff. Compared to pre-trained models, post-trained models learn and use new knowledge on downstream tasks effectively. (Section 5)"}, {"title": "3. Post-training Data Scaling", "content": "How does post-training model performance scale with dataset size?\nThe primary implication of the Superficial Alignment Hypothesis is that pre-training is all that matters, and with a rather small set of examples, we can align a model during post-training. However, this is a broad claim that is supported by a limited set of chatbot-style experiments. Post-training a model involves instruction following, problem-solving, and coding, and unlike chatbot-style dialogue whose evaluation is subjective and comparative, these capabilities can be judged using standardized benchmarks. For researchers and model builders who aim to improve performance on such tasks, it is important to understand performance scaling on such benchmarks with increasing fine-tuning data."}, {"title": "3.1 Experiment Design", "content": "We look at four tasks (training datasets used are in parenthesis) - mathematical problem solving (GSM8k (Cobbe et al., 2021)), instruction following ((Conover et al., 2023; Sun et al., 2024)), coding (StarCoder Self-Align \u00b9) and multihop question answering (SubQA (Tang et al., 2021)). Starting from the base model and finetuning with increasing dataset size, we observe how performance scales during post-training. For each task, we ran evaluations using a standard framework where available (Gao et al., 2024; Liu et al., 2023; Zhou et al., 2023). More details about the training data are available in Table 1 and additional dataset construction details can be found in Appendix A.3.\nFor our experiments, we finetune Llama-3, Llama-2 and Mistral model families on these tasks (Dubey et al., 2024; Jiang et al., 2023; Touvron et al., 2023). We chose base models because it is likely that instruct models are already extensively finetuned for these tasks.\nWe first finetuned the smallest (sub-10 Billion parameter) models from these model families, with the dataset splits of 0, 100, 500, 1000, 5000, and 10000 examples until the training dataset was exhausted."}, {"title": "3.2 Results", "content": "Model performance for a task follows a power-law relationship with fine-tuning data. Figure 1 shows the performance scaling with increasing post-training data for the smallest, sub-10 Billion parameter models, with the power-law fit line. Task accuracy $P$ closely follows a power-law of the form $P x D^{1/b}$ with the number of finetuning examples $D$, for all the models on all the tasks. This power-law relationship is in line with several other empirical scaling laws of LLMs with data size during pre-training, quantization, and inference (Brown et al., 2024; Kaplan et al., 2020; Michaud et al., 2024). The coefficients for the power law curves are in Appendix A.6.\nIn addition, model improvement curves do not cross each other. Better base models for a task are consistently better during post-training as well. This is in line with other works that relate pertaining performance with downstream task performance (Zhang et al., 2024a). Performance scaling with data is also more consistent and predictable on reasoning-centric tasks like Math, Multihop QnA, and Coding, as opposed to subjective tasks like Instruction Following. We also perform additional ablations on scaling curves with different dataset quality for Instruction Following in Appendix A.3.3, and evaluations on the GSM1k benchmark (Zhang et al., 2024b) for math to check for dataset contamination, in Appendix A.1.\nLarger models in a model family learn better than smaller models with more data. Figure 2 shows results for model scaling. All models of the family follow the same power law distribution. As expected, larger models consistently outperform smaller models for the same training data. However, the improvement is not always just an upward parallel shift. The performance of larger models curves upwards with more training datasets, indicating an increasing rate of improvements with additional data. The coefficients for the power law curves are in Appendix A.6.\nPutting these results together, we see that larger and better base models scale even better during post-training. This highlights the role of pre-training in preparing a model to learn better during post-training."}, {"title": "3.3 General Purpose Alignment vs Task Specific Finetuning", "content": "The LIMA paper also introduced the LIMA dataset, a collection of 1,000 carefully curated prompts that was intended to align a pre-trained model to be on par with state-of-the-art post-trained models. It is based on the hypothesis that the model has inherently learned most of its capabilities during pre-training and thus, the model just needs to adopt a stylistic format to answer questions.\nIn this section, we finetune the Llama-3 8b model with the LIMA dataset, using the same training setup as the rest of the datasets. We call this model LIMA-1k. We also finetune a pre-trained model with 1,000 examples specific to a task - GSM8k for math and SubQA for multihop reasoning. This ensures that the LIMA and task-specific models are trained with a similar data \u201cbudget\u201d. We call models finetuned with task-specific datasets Task-1k. We then evaluate the performance of the chat-bot style aligned LIMA-1k against task-specific fine-tuned Task-1k model on the task-specific benchmarks."}, {"title": "4. Learning Reasoning and Style", "content": "Is the model significantly improving on task-related capabilities or just learning the response style?\nIn this section, we investigate what is driving the improvements in these tasks with more data, specifically aiming to delineate improvements in style/formatting versus improvements in reasoning. We do this by evaluating the generations of models with different finetuning levels."}, {"title": "4.1 Experiment Design", "content": "We evaluate finetuned model generations over math (GSM8k dataset) and multihop QnA (SubQA dataset). We took the Llama-3 8b base model as well as fine-tuned models using 100, 1000, and full training splits of the two datasets.\nBoth GSM8k and SubQA responses use the Socratic Method of generating subquestions to arrive at the final answer. So, the model is finetuned to follow this specific style of generating a subquestion-answer reasoning chain, a delimiter, followed by the final answer. Examples of expected model response styles and formats are in Appendix A.3.1 and A.3.2.\nWe then collect all the incorrect responses from these models on the test split and annotated them using GPT-40 (OpenAI, 2024). If the responses fail to stick to the previously specified format, it is annotated as Incorrect Formatting. If the responses contain an error in their subquestion-answer reasoning steps, we annotate it as Incorrect Reasoning. For math, we also check for Incorrect Arithmetic Calculations, since they are a major source of model errors. Each error category was evaluated independently for a response, using a tailored prompt. So, an error response can belong to multiple categories. More details about the prompt used for this categorization are in Appendix A.4."}, {"title": "4.2 Results", "content": "Style and formatting improvements saturate quickly. From Figure 3 we see that the models get better at style and format errors with just 100 examples. If one just takes a passing look at the responses from this model, they could incorrectly conclude that the model is \u201caligned\u201d to answer math or multi-hop questions. However, all of these responses are still incorrect for the task for which we fine-tune the model.\nReasoning performance continues to improve with more data. Models continue to get better at reasoning and question understanding with more examples. The total number of mistakes a model makes highly correlates with reasoning errors (r\u00b2 value of 0.98 for math and 0.99 for multihop QnA on Llama-3 8B) as opposed to total mistakes and formatting errors (r\u00b2 value of 0.93 for math and 0.83 for multihop QnA). It also signifies that a model's capabilities are not entirely learned during pre-training, because models can significantly improve their reasoning, or learn to apply it effectively, during post-training. This leads us to the idea that the superficial alignment hypothesis could be limited in"}, {"title": "5. Learning New Knowledge", "content": "Can a model integrate new knowledge from beyond the pertaining knowledge cutoff?\nIn this section, we examine how post-training can help LLMs learn new knowledge after the pre-taining knowledge cutoff, and more importantly, use it correctly on downstream tasks."}, {"title": "5.1 Experiment Design", "content": "We first created Facts100, a hand-curated dataset of 100 news events that occurred after March 2023, the knowledge cutoff of the Llama-3-8b base model. The news events are from across the world and cover domains such as entertainment, sports, business, politics and science. We then created two questions for each event, as shown in example 4:\n* Direct Question: A single-hop direct question related to the news event and its entity.\n* Multihop Question: A multistep reasoning style question that first requires recalling what happened in the event in the first step and using it to answer in the second step. This checks if the model learns how to use the learned knowledge in the right way"}, {"title": "5.2 Introducing new knowledge", "content": "There are two primary ways to introduce new knowledge to a model - finetuning on the new events or during inference as part of the prompt. The latter is a simplified Retrieval Augmented Generation (RAG) setup Lewis et al. (2020). We investigate the role of post-training in both of these cases.\nEvent SFT: To train the model to learn this new information, we finetune a model on the Direct Question as the prompt and the Event + Answer as the response. The format is the same as the SubQARecall to keep the data in-distribution.\n\\newlineWe didn't finetune the Llama-3 8B instruct model on the new facts because it performed poorly. This is because it is strongly aligned to refuse to answer questions beyond its finetuning cutoff data. Attempts to get the model to overcome this behavior through finetuning (ex: increasing learning rate) led to behavior degradation. Although unlearning methods can help undo this behavior, it is out of the scope of this study."}, {"title": "5.2.1 Results", "content": "Post-training a model for reasoning helps models learn and integrate new knowledge better. As seen in Table 4, models post-trained for reasoning are significantly better at learning new knowledge (Direct Question) as well as integrating the new knowledge (Multihop Reasoning Question). However, SFT or RAG on the pre-trained model fails to show the same improvement on the harder multi-hop questions. Note that this answer is just one hop from the answer to the direct question but requires it to reason through the steps. This shows that the model can't correctly use this new information in the right way on reasoning tasks.\nHowever, if the model is first post-trained to do reasoning, it gets better at absorbing new information and using it in multihop reasoning tasks. This post-training was done on data from before the knowledge cutoff. Such post-training led to a marked difference in both SFT and RAG-based methods for introducing new knowledge.\nModels hallucination is mitigated by post-training for reasoning, but is not eliminated. Several studies have shown that LLMs hallucinate when introduced to new knowledge. Since all of the models in our experiments are finetuned to recall the event first and generate the answer based on it, we can easily check for hallucination in the recalled event. We also analyze the subsequent reasoning chain for reasoning errors. Both of these are done using GPT-40 and the prompt used is shown in the Appendix A.3.4.\nFrom Figure 5, we see that hallucinations about the event are a major factor in erroneous model responses. Even after introducing new knowledge, base models hallucinate significantly. This is in line with other works that demonstrate that finetuning models with new knowledge can lead to hallucinations (Gekhman et al., 2024).\nHowever, models post-trained for reasoning make a marked improvement in both hallucination and reasoning errors. Although hallucination is not completely mitigated, the true value of finetuning is in preparing the model to reason with the new knowledge it receives. In addition, RAG-based methods are better than SFT for introducing new knowledge because the relevant information being in the context is a lot more useful. We also do a variety of ablation studies to compare against continual pre-training and LIMA-based finetuning, which are detailed in the Appendix A.2."}, {"title": "6. Related Work", "content": "Language Model Alignment: LLMs are first pretrained to gain general language understanding and world modeling, which is then followed by an alignment phase that involves SFT and RLHF to appropriately respond to user's prompts (Dubey et al., 2024; Ouyang et al., 2022; Wei et al., 2021). Several recent works laid out their hypothesis on whether alignment is more about learning style and might not even be necessary because of In-Context Learning (Lin et al., 2023; Zhou et al., 2024). More recent works by Zhao et al. (2024) show that instruction tuning on better base models can outperform In Context Learning.\nScaling Laws: Several works like Bahri et al. (2024); Hoffmann et al. (2022); Kaplan et al. (2020); Michaud et al. (2024) have studied and developed scaling laws for LLM Pretraining in terms of the dataset token count and the model parameter count against the cross-entropy loss. However, Schaeffer et al. (2024) show that it is hard to predict scaling laws for downstream task performance because of constraints in the action space. Hernandez et al. (2021); Isik et al. (2024) derive scaling laws for finetuning as a function of pretraining data and transfer learning respectively, and not post-training or instruction following. However, researchers and model builders improve model performance by collecting individual prompt-response examples rather than collective dataset tokens (Dubey et al., 2024)."}, {"title": "7. Conclusions and Future Work", "content": "Conclusions: LLM post-training is a complex endeavor that involves improvements to instruction following, stylistic formatting, reasoning abilities, and general alignment to human preferences. LLMs can imitate the required style with \u201csuperficial\" finetuning using a handful of examples, leading to the Superficial Alignment Hypothesis. However, a solely stylistic evaluation fails to characterize the many aspects of reasoning and task-specific capabilities that are key goals of finetuning. In fact, task-specific skills & reasoning significantly improve after post-training with more examples compared to the pre-trained model. These improvements closely follow a power law in our experiments with the number of finetuning examples across multiple model families and sizes. We also see that these improvements are driven by the model's reasoning ability during generation, and are not limited to the model's alignment to formatting or style. In addition, we see that the win rate against other models can be a misleading metric to measure tasks that require complex reasoning, signaling the need for holistic evaluation programs leveraging standardized, objective benchmarks, in addition to measurement of alignment to human preferences.\nWe also observe that good post-training can help LLMs overcome problems associated with knowledge cutoff, by enabling them to better utilize knowledge from beyond the pre-training corpus either via further finetuning or RAG. These results put together highlight the qualitative and quantitative characteristics of post-training, and the role of data scaling in this.\nLimitations and future work: In this work we showed the performance improvement of a model on a task when it is finetuned with increasing task-specific data. However, frontier LLMs are trained to excel at multiple tasks, and we don't thoroughly understand how finetuning for one task or domain affects the performance on others. A big open question would be investigating how to take advantage of this scaling behavior while preventing model degradation on existing capabilities. Similarly, we showed how models can learn new knowledge beyond their pre-training data cutoff, but the issue of hallucination isn't solved. Further research in effectively introducing new knowledge, like continual learning methods during post-training can shed light on this.\nIn this work, we also limited the scope to supervised finetuning, on tasks that involve text generation. However, the implications from this opens up several interesting directions to explore further. Most notably, LLM post-training involves RLHF after supervised finetuning, and it would be interesting to see how RLHF can contribute to these improvements and how it scales with more data."}, {"title": "A.3.1 GSM8k Dataset", "content": "We chose the Socratic split of the GSM8k dataset since it contains well-developed subquestion-answer steps. This makes it easy to evaluate finetuned model responses for following this format, as well as evaluating the reasoning itself.\nAn example question-answer pair is given below:\nSystem Message: You are an expert in mathematics. Solve the following math problem\nPrompt:\nQuestion: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\nAnswer:\nResponse:\nHow many clips did Natalia sell in May? ** Natalia sold 48/2 = \u00ab48/2=24\u00bb24 clips in May.\nHow many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24 = \u00ab48+24=72\u00bb72 clips altogether in April and May.\n#### 72"}, {"title": "A.3.2 SubQA Dataset", "content": "We modify the SubQA dataset to build a multi-hop reasoning dataset. The original SubQA dataset contains a select 1,000 subset of HotpotQA bridge-style questions, and two human-verified subquestions for it. For each of the thousand questions, we construct a multi-hop reasoning chain by chaining together the two subquestions, as shown below.\nQuestion: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\nAnswer: Chief of Protocol\nSubquestion 1:\nQuestion: Which woman portrayed Corliss Archer in the film Kiss and Tell?\nAnswer: Shirley Temple\nSubquestion 2:\nQuestion: What government position was held by Shirley Temple?\nAnswer: Chief of Protocol\nGenerated multi-hop question and reasoning-based answer that the model is fine-tuned on along with the system message:"}, {"title": "System Message: You are a helpful assistant. If you are asked a question, answer it carefully and if you are given an instruction, follow it thoroughly. If the question contains some context, use it to answer the question or follow the instruction", "content": "Prompt: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\nResponse: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\nLet's answer this step-by-step\nWhich woman portrayed Corliss Archer in the film Kiss and Tell? \u2192 Shirley Temple\nWhat government position was held by Shirley Temple? \u2192 Chief of Staff\nThe final answer is Chief of Staff"}, {"title": "The SubQARecall version of the dataset was a simple modification of the response format, in which we added an event recall step first, which can be used to evaluate hallucinations. The event annotation was done using GPT-40 using the steps from the response. An example response format for this modified dataset is given below", "content": "System Message: You are a helpful assistant. If you are asked a question, answer it carefully and if you are given an instruction, follow it thoroughly. If the question contains some context, use it to answer the question or follow the instruction\nPrompt:\nWhat government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\nResponse:\nEvent Recall: Shirley Temple played the role of Corliss Archer in the film Kiss and Tell and also held the position of Chief of Staff.\nWhat government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\nLet's answer this step-by-step\nWhich woman portrayed Corliss Archer in the film Kiss and Tell? \u2192 Shirley Temple\nWhat government position was held by Shirley Temple? \u2192 Chief of Staff\nThe final answer is Chief of Staff\nFor evaluation, simple string matching ignores answers that are correct but differently worded. It also introduces false positives if the model just contains the final answer as part of its response instead of actually giving it as the answer. So, we use GPT-40 to evaluate the model response and provide the question, the correct answer, and the model-generated answer."}, {"title": "A.3.3 Conifer and Dolly Dataset", "content": "The Conifer dataset has multiturn instruction following prompt-response dialogue examples and at each turn, the prompts gets harder, with more constraints and better responses.\nFor the Instruction Following Tasks, Dolly is of lower quality for the IFEval benchmark because the prompts are very different from the benchmark prompts and do not include constraints and directions. Conifer is a better dataset as it is targeted for constrained instruction following, similar to the IFEval prompts.\nAn example prompt-response pair from Conifer is given below:"}, {"title": "System Message: You are a helpful assistant. If you are asked a question, answer it carefully and if you are given an instruction, follow it thoroughly. If the question contains some context, use it to answer the question or follow the instruction", "content": "Prompt:\nCan you provide examples of effective strategies for engaging senior partners in the investment sector through cold-calling? Ensure to cover:\n1. Across industries such as Private Equity and Venture Capital;\n2. When communicating with C-suite executives and Managing Directors;\n3. A concise and to-the-point, data-driven approach;\n4. Objectives include pitching a new investment opportunity and building a relationship;\n5. Adherence to financial promotion rules;\n6. Consideration of time zone differences and avoidance of holidays;\n7. Demonstrating a thorough understanding of the partner's portfolio and recent industry trends;\n8. Provide a bulleted list.\nResponse:\nEffective strategies for engaging senior partners in the investment sector through cold-calling include:\n* Tailoring the pitch to the specific industry, whether it's Private Equity or Venture Capital, highlighting relevant trends and opportunities.\n* Addressing C-suite executives and Managing Directors with respect and understanding of their time constraints, ensuring brevity and relevance in the conversation.\nUtilizing a data-driven approach to substantiate the investment opportunity, presenting clear metrics and potential ROI.\nClearly stating the purpose of the call, whether it's to introduce a new investment opportunity or to initiate a relationship for future collaboration.\nEnsuring compliance with financial promotion rules by providing accurate information and not making misleading claims.\nScheduling calls considering the recipient's time zone and avoiding holidays to respect their personal time.\nDemonstrating knowledge of the partner's current portfolio and aligning the pitch with their investment strategy and recent industry trends."}]}