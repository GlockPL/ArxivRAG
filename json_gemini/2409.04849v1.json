{"title": "FedModule: A Modular Federated Learning Framework", "authors": ["Chuyi Chen", "Zhe Zhang", "Yanchao Zhao"], "abstract": "Federated learning (FL) has been widely adopted across various applications, such as healthcare, finance, and smart cities. However, as experimental scenarios become more complex, existing FL frameworks and benchmarks have struggled to keep pace. This paper introduces FedModule\u00b9, a flexible and extensible FL experimental framework that has been open-sourced to support diverse FL paradigms and provide comprehensive benchmarks for complex experimental scenarios. FedModule adheres to the \"one code, all scenarios\" principle and employs a modular design that breaks the FL process into individual components, allowing for the seamless integration of different FL paradigms. The framework supports synchronous, asynchronous, and personalized federated learning, with over 20 implemented algorithms. Experiments conducted on public datasets demonstrate the flexibility and extensibility of FedModule. The framework offers multiple execution modes\u2014including linear, threaded, process-based, and distributed-enabling users to tailor their setups to various experimental needs. Additionally, FedModule provides extensive logging and testing capabilities, which facilitate detailed performance analysis of FL algorithms. Comparative evaluations against existing FL toolkits, such as TensorFlow Federated, PySyft, Flower, and FLGo, highlight FedModule's superior scalability, flexibility, and comprehensive benchmark support. By addressing the limitations of current FL frameworks, FedModule marks a significant advancement in FL experimentation, providing researchers and practitioners with a robust tool for developing and evaluating FL algorithms across a wide range of scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Nowadays, Federated Learning (FL) [1], [9] has been widely used in various applications, such as healthcare, finance, and smart cities [2]-[4]. In FL, the data are distributed among multiple clients, and the model is trained on the data of the clients without uploading the data to the server. The server aggregates the model updates from the clients and updates the global model. Massive research has been conducted to improve the performance of FL, such as communication-efficient algorithms [5], [6], secure aggregation [7], model personalization [8], and client heterogeneity [9]. However, as the depth and width of FL research methods evolve, experimental scenarios become increasingly complex, yet the associated experimental frameworks and benchmarks have not kept pace. The lack of a unified experimental framework and benchmark makes it difficult to compare the performance of different FL algorithms and reproduce the results of existing algorithms [10], [11]. This has become a bottleneck in the development of FL research.\nRecently, several FL frameworks have been proposed to address this issue [10]-[13]. For example, TensorFlow Federated(TFF) [12] provides a simulation environment for FL algorithms, and PySyft and Flower provide a distributed computing environment for FL. However, these frameworks are designed for specific scenarios and lack flexibility. For example, TFF is designed for FL algorithms based on synchronous federated learning, and PySyft is designed for FL algorithms with differential privacy. These frameworks are not suitable for comparing the performance of FL algorithms in different scenarios. For instance, when comparing synchronous algorithms with asynchronous algorithms, or synchronous algorithms with personalized algorithms, the existing frameworks face significant challenges in implementing these algorithms. Furthermore, they lack the necessary benchmarks for conducting experiments. Therefore, it is necessary to design a flexible and extensible FL experimental framework that supports various federated learning paradigms and provides a rich set of benchmarks to address complex and varied experimental scenarios.\nThus, we propose our framework\u2014FedModule. FedModule decomposes Federated Learning into individual modules, enabling it to seamlessly expand to support new paradigms and benchmarks. As illustrated in Fig. 1, FedModule assembles various roles according to user requirements, with each module"}, {"title": "II. RELATED WORK", "content": "Since Google introduced Federated Learning in 2017 [1], numerous Federated Learning algorithms have been proposed. However, the development of frameworks suitable for related experiments has not progressed as quickly, with only a limited number of frameworks being introduced. TensorFlow Federated(TFF) [12] is a widely used FL framework that provides a simulation environment for FL algorithms. It is principally intended to simulate the training process of a limited number of homogeneous clients. However, its interface is highly coupled, which constrains its extensibility and flexibility. PySyft [13] and Flower [11] are two other FL frameworks that provide a distributed computing environment for FL. PySyft is a research platform primarily designed for data science applications based on differential privacy. Due to its rapid development cycles, certain versions lack support for FL, not to mention more complex variants of FL paradigms. Flower is a specialized experimental platform for FL that primarily offers users the capability to conduct large-scale FL experiments and explore diverse scenarios involving heterogeneous devices. However, it should be noted that Flower only supports synchronous FL and does not extend to variant paradigms such as asynchronous FL and personalized FL. In addition, these frameworks two lack benchmarks for evaluating the performance of FL algorithms. Additionally, the lightweight framework FLGO [10] has recently garnered considerable attention due to its extensive baseline and benchmark support. However, its reliance on a linear execution method to simulate the FL training process limits its applicability, particularly in time-sensitive scenarios where FLGO fails to offer adequate support.\nAlthough these frameworks have contributed significantly to the field of FL experimentation, they each suffer from certain limitations. Specifically, their lack of scalability and flexibility highlights the urgent need for a more advanced framework. Thus, we propose FedModule, a framework designed with a modular structure and adhering to the \"one code, all scenarios\" principle."}, {"title": "III. FRAMEWORK DESIGN", "content": "FedModule consists of two kernels: the Framework Core and the Module Repository. The Framework Core is responsible for creating essential components, such as server, FL benchmark, and message queue, and managing the whole running process. The Module Repository contains various modules, such as updater, scheduler, and mode. Each module can be loaded dynamically to support different federated learning paradigms.\nAs shown in Fig. 2, the workflow of FedModule is as follows. First, the Framework Core generates the essential components, including server, FL benchmark, message queue, client manager. Then, each component can be assembled and extended according to the user's configuration through the Module Repository to meet different experimental scenarios. After the assembly, the client manager organizes clients to participate in training according to the configuration. It is important to note that, to improve the adaptability of the framework, a variety of methods are available to organize clients, which will be discussed in more detail in III-C. During the FL process, the server and the clients communicate through the message queue. This approach decouples the server and the clients, facilitating better extensibility of the framework. In the end, the Framework Core collects the results of the training process and gives the final results to the user."}, {"title": "B. Framework Core & Module Repository", "content": "In our framework, the Framework Core and Module Repository are essential elements that significantly enhance its flexibility and extensibility. The Framework Core deconstructs the entire federated learning (FL) process into discrete modules, allowing them to be assembled into various FL paradigms much like assembling Lego bricks. Specifically, as shown in Fig. 2, the Framework Core segments the FL process into four primary components: the FL benchmark, message queue, server, and client manager, along with other functional modules stored in the Module Repository. The FL benchmark is responsible for setting up experimental scenarios, including training models, configuring data distribution, and managing client heterogeneity, among other settings. The server and clients correspond to the server and client entities in FL. The Client Manager is a management class designed to control clients, allowing users to set the client's execution mode and providing interfaces to start and stop clients. This functionality enables the simulation of real-world scenarios where clients may join or leave the network at any time. The message queue"}, {"title": "C. Custiomize Execution Mode", "content": "To facilitate the slogan of \"one code, all scenarios\", we make clients to be organized in various ways. Specifically, we utilize the dynamic language feature of Python to implement the execution mode of the clients. Clients can choose their execution mode based on the configuration file. The execution mode can be linear (which means that the clients are running in a linear order, like for loop), thread, process, or even distributed. Despite the variety of execution modes, the client only needs to be implemented once to run in all modes, which is what we advocate as \"one code, all scenarios\". This design not only simplifies the implementation of the client, but also enhances the flexibility of the framework, making it much easier for researchers to develop.\nThe implementation of the client follows parallel design principles, inheriting from the thread/process class, with users only needing to implement the run method. Most of the operating modes in our framework are based on processes or threads, which can naturally run such client implementations. However, the linear execution mode cannot be directly supported by the thread/process class, as it requires the clients to run sequentially. Moreover, the distributed mode is more complex than the other modes, as it requires the clients to run on different machines. To address these challenges, we have designed separate solutions specifically for these two modes."}, {"title": "1) Timeslice Mode:", "content": "Transforming a group of parallel clients into sequential execution is extremely challenging, as it is not possible to inform the clients of each other's execution times in order to enforce a linear order. However, thanks to the dynamic nature of Python, we designed an innovative timeline-based solution, which we called the timeslice mechanism. In order to reconstruct the entire training process to achieve sequential operation, we split the entire FL process into multiple tasks and discretize time into time slices, where a time slice is considered the smallest time unit in the mechanism, and each task corresponds to a different number of time slices. As illustrated in Fig. 3(a), after reaching a specific time slice, the timeslice mechanism will then proceed to sequentially activate"}, {"title": "2) Distributed Mode:", "content": "The distributed execution mode permits the execution of users' code on distributed machines, thereby addressing the requirements of large-scale experiments. We run a sub-client manager class on each device, which manages the number of clients running on each device. Furthermore, a main client manager class operates on the server to facilitate the coordination of the sub-client managers and oversee the management of clients on each device. Compared to other execution modes, the communication issues faced by the distributed mode are more complex, so we designed a suitable distributed communication framework for this purpose. As shown in Fig. 3(b), the distributed communication framework is comprised of two main components: the intra-device communication and the inter-device communication. The intra-device communication employs the adapter pattern to wrap the existing communication method, rendering data transmission transparent to the client, whereas the inter-device communication is responsible for the actual communication. The inter-device communication is also modular, with the user able to select the desired communication method, such as socket, MQTT, or HTTP, based on the specific requirements of the experiment."}, {"title": "D. Other Features", "content": "Config File: In contrast to other platforms that employ command-line arguments, FedModule utilizes configuration files for parameter configuration. This approach is advantageous in the context of an evolving FL experimental environment, where the number of required hyperparameters is increasing. Configuration files offer a convenient management and review solution, as well as a more efficient means of reuse and extension. The configuration file is divided into several sections, including the client, server, clientmanager and so on. Each section contains the corresponding hyperparameters, which can be easily modified by the user. The configuration file is loaded by the Framework Core and passed to the corresponding components, which then utilize the hyperparameters to configure the components."}, {"title": "IV. EVALUATION", "content": "In this section, we conduct experiments to show the ability of FedModule on different federated learning paradigms and benchmarks. We also open-source the config files and code of the experiments to facilitate the reproduction of the results\u00b2."}, {"title": "A. Experimental Setup", "content": "1) Datasets: In the experiments, we used a total of 4 datasets: CIFAR10 [14], FashionMNIST [15], SVHN [16], and UCIHAR [17]. The CIFAR10 dataset contains 60,000 32x32 color images divided into 10 classes, with 6,000 images per class. It is split into 50,000 training images and 10,000 test images. FashionMNIST includes 60,000 28x28 grayscale images organized into 10 classes, with 6,000 images per class, and is divided into 50,000 training images and 10,000 test images. The SVHN dataset includes 73,257 32x32 color images across 10 classes, with 65,000 images for training and 13,257 for testing. Lastly, the UCIHAR dataset consists of 10,299 instances, split into 7,352 training instances and 2,947 test instances, with each instance having 561 features across 6 classes.\n2) Training Settings: Most experiments were conducted on a server equipped with an NVIDIA RTX4090 GPU and an NVIDIA RTX2090 GPU, running Ubuntu 21.04.\nConvolutional Neural Networks (CNNs) [?] were trained on the FashionMNIST and UCIHAR datasets, while the ResNet-18 architecture [19] was used for the CIFAR10 and SVHN datasets. Stochastic Gradient Descent (SGD) was employed as the optimizer, with the learning rate set to 0.01. The CNN model utilized in the experiments includes two convolutional layers, two pooling layers, and two fully connected layers. Each selected client undergoes local training for 2 epochs. The batch sizes for the datasets are set to 64 for FashionMNIST, 64 for UCIHAR, and 128 for the remaining dataset. For a comprehensive overview of the hyperparameter settings, please refer to Table II. Further details on the hyperparameter configurations for each baseline can be found in our open-source repository."}, {"title": "B. Performance of Different Execution Modes", "content": "FedModule provides users with the option of selecting different runtime modes, which are designed to accommodate the specific experimental requirements of the user, taking into account the characteristics of the experimental hardware and the available memory. In this section, we evaluate the performance of different execution modes in FedModule. We present the performance of FedAvg using five different execution modes: linear, thread, process, MQMT, and distributed. The dataset employed is CIFAR10. The results are shown in Fig. 4(a).\nAs shown in the Fig4(a), we used the same random seed for all experiments, resulting in nearly identical accuracy across all excution modes. The execution times for each mode, in ascending order, are as follows: process, mpmt, distributed, timeslice, and thread. The process mode has the shortest execution time because each process operates independently without communication delays between the client and server. The mpmt (multi-process multi-threading) mode, which employs multiple parallel processes to handle client operations, is faster than distributed, timeslice and thread modes. In the distributed mode, additional communication time between devices is introduced compared to the process mode. The thread mode experiences the longest execution time due to the overhead from frequent thread context switching.\nRegarding memory consumption, as shown in Fig. 4(b), the distributed mode has the highest memory usage, followed by process, mpmt, thread, and timeslice. The process mode consumes less memory than the distributed mode because the latter requires additional space to handle inter-device communication. The timeslice mode, due to its non-native sequential execution, requires extra space compared to the thread mode to store the stack information for each client.\nThe results indicate that FedModule allows users to choose the most appropriate execution mode based on their specific needs, showcasing its flexibility and extensibility, which make it adaptable to various experimental scenarios."}, {"title": "C. Experimental Validation on Diverse Datasets", "content": "Our framework enables seamless switching between different datasets, requiring users to simply configure the appropriate model and parameters without needing to write additional code. We support a wide range of datasets, including not only commonly used datasets but also mixed datasets, streaming datasets, and others. We conducted experiments using several datasets, and the results are presented in Fig. 5.\nMoreover, FedModule supports loading datasets into memory to enhance experimental speed. We compared the performance with and without dataset preloading, as shown in Fig. 6. Experiments on FedAvg in mpmt mode revealed that when clients run concurrently, I/O operations consume a substantial portion of the runtime, thus slowing down the experiments and increasing the divergence between simulation and real-world scenarios. The results show that preloading datasets accelerated the process by 3.6x times compared to loading data during training, validating the effectiveness of this feature and demonstrating its potential to significantly reduce simulation distortions."}, {"title": "D. Support for Client Heterogeneity", "content": "Moreover, FedModule allows the configuration of client heterogeneity to support custom benchmarks. In this section, we will adjust the data heterogeneity and system heterogeneity of the clients to showcase the framework's capabilities.\nIn previous experiments, we used the Dirichlet distribution to configure the data heterogeneity of the clients. In this subsection, we implemented finer-grained configurations by"}, {"title": "E. Different FL Paradigms", "content": "FedModule supports various FL paradigms, including asynchronous and personalized FL. Our experiments demonstrate the framework's versatility in these paradigms, as shown in Fig. 8. For the asynchronous FL paradigm (Fig. 8(a)), we evaluated four different acceleration algorithms: FedAsync, a fully asynchronous algorithm where the server aggregates updates immediately upon receipt; FedVC, a semi-asynchronous algorithm where the server aggregates updates only after collecting a sufficient number; EAFL, another semi-asynchronous algorithm that groups clients and performs asynchronous aggregation within groups and semi-asynchronous aggregation between groups; and TWAFL, a synchronous acceleration algorithm where clients upload only specific model parameters at designated rounds. For the personalized FL paradigm (Fig. 8(b)), we tested two algorithms: PFedMe, which involves a global model, and FedDL, which aggregates parameters by grouping based on parameter similarity and operates without a global model. These experiments demonstrate the extensive applicability of our framework, accommodating a wide range of federated learning variants. Furthermore, we are actively developing security-related FL paradigms to support experiments focusing on security aspects."}, {"title": "F. Abundant Log and Test", "content": "In the previous experimental section, we demonstrated some of the comprehensive data recording capabilities of FedModule, such as tracking test accuracy over time and by logical criteria (Figs. 8(a) and 5(b)), as well as recording the average accuracy across clients (Fig. 8(b)). In this section, we present additional experimental records. As shown in the figure, Fig. 9(a) shows the data distribution of the experiment, while Figs 9(b) to 9(d) depict various device performance metrics during the experiment, such as GPU usage and memory consumption. Additionally, FedModule saves the configuration details of each experiment upon completion and provides an outline summarizing the experiment."}, {"title": "V. CONCLUSION", "content": "In this work, we introduce FedModule, a modular FL framework that adheres to the \"one code, all scenarios\" principle. FedModule decouples the FL training process into multiple independent components, allowing each component to select functional modules based on user requirements to construct specific FL experiments. This modular design enables seamless switching between different FL paradigms and benchmarks. We conducted extensive experiments demonstrating FedModule's capability to support existing algorithms and validating the effectiveness of its features. In the future, we plan to implement more algorithms within FedModule to further enhance its applicability."}]}