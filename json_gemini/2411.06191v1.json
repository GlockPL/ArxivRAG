{"title": "Generalizing Hyperedge Expansion for Hyper-relational Knowledge Graph Modeling", "authors": ["Yu Liu", "Shu Yang", "Jingtao Ding", "Quanming Yao", "Yong Li"], "abstract": "By representing knowledge in a primary triple associated with additional attribute-value qualifiers, hyper-relational knowledge graph (HKG) that generalizes triple-based knowledge graph (KG) has been attracting research attention recently. Compared with KG, HKG is enriched with the semantic qualifiers as well as the hyper-relational graph structure. However, to model HKG, existing studies mainly focus on either semantic information or structural information therein, which however fail to capture both simultaneously. To tackle this issue, in this paper, we generalize the hyperedge expansion in hypergraph learning and propose an equivalent transformation for HKG modeling, referred to as TransEQ. Specifically, the equivalent transformation transforms a HKG to a KG, which considers both semantic and structural characteristics. Then an encoder-decoder framework is developed to bridge the modeling research between KG and HKG. In the encoder part, KG-based graph neural networks are leveraged for structural modeling; while in the decoder part, various HKG-based scoring functions are exploited for semantic modeling. Especially, we design the sharing embedding mechanism in the encoder-decoder framework with semantic relatedness captured. We further theoretically prove that TransEQ preserves complete information in the equivalent transformation, and also achieves full expressivity. Finally, extensive experiments on three benchmarks demonstrate the superior performance of TransEQ in terms of both effectiveness and efficiency. On the largest benchmark WikiPeople, TransEQ significantly improves the state-of-the-art models by 15% on MRR.\nKeywords: Hyper-relational knowledge graph, knowledge graph completion, hyperedge expansion, graph neural network, representation learning", "sections": [{"title": "1 Introduction", "content": "In the past decade, knowledge graph (KG) has been widely studied in artificial intelligence area (Wang et al., 2017; Ji et al., 2021). By representing facts into a triple of (s, r, o) with subject entity s, object entity o and relation r, KG stores real-world knowledge in a graph structure. However, recent studies find that KG with simple triples provides incomplete information (Galkin et al., 2020; Yu and Yang, 2021; Xiong et al., 2023). For example, both (Alan Turing, educated at, Cambridge) and (Alan Turing, educated at, Princeton) are true facts in KG, which might be ambiguous when the degree matters.\nHence, the hyper-relational KG (HKG) (Galkin et al., 2020), a.k.a., knowledge hypergraph (Fatemi et al., 2020) and n-ary knowledge base (Guan et al., 2019; Liu et al., 2021), is proposed for more generalized knowledge representation. Formally, in HKG, a primary triple is augmented with additional attribute-value qualifiers for rich semantics, called the hyper-relational fact\u00b9 (Guan et al., 2020). Taking Figure 1 as an example, both (Alan Turing, educated at, Cambridge, (degree, Bachelor)) and (Alan Turing, educated at, Princeton, (degree, PhD)) are hyper-relational facts, where (degree, Bachelor) and (degree, PhD) are qualifiers with the degree attribute considered. Such hyper-relational facts are ubiquitous that over 1/3 of the entities in Freebase (Bollacker et al., 2008) involve in them (Wen et al., 2016)."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Knowledge Graph (KG) Modeling", "content": "Learning representations for entities and relations in KGs has been investigated thoroughly (Wang et al., 2017; Ji et al., 2021), which designs various SFs to model the semantics in triple knowledge (s,r,o). Based on translational thought, TransE (Bordes et al., 2013), TransH (Wang et al., 2014) and RotatE (Sun et al., 2019) measure the distance between subject and object entities in a relation-specific latent space. Besides, ConvE (Dettmers et al., 2018) adopts the convolutional neural networks for SF design. TuckER (Balazevic et al., 2019) employs Tucker decomposition for SF design. Furthermore, several models combine the bilinear product with various types of embeddings (Trouillon et al., 2016; Cao et al., 2021; Yang et al., 2015). For example, ComplEx (Trouillon et al., 2016) and DulE (Cao et al., 2021) employ complex-valued embeddings and dual quaternion embeddings, respectively. However, models above ignore the multi-relational graph structure of KGs.\nUntil the emergence of message passing mechanism with GNN, structural information capture becomes an important topic in KG modeling. An encoder-decoder framework is developed in recent KG-based GNN studies, where GNNs encode structural information of KG and various SFs are combined for semantic information. Specifically, both R-GCN (Schlichtkrull et al., 2018) treats the multi-relational KG as multiple single-relational graphs, and applies relational graph convolutional network (GCN) for entity representations. Moreover, VR-GCN (Ye et al., 2019) combines the translational idea with GNN to learn both entity and relation representations. CompGCN (Vashishth et al., 2019) and InGram Lee et al. (2023) develop three entity-relation composition operators to update entity representations in GCN, and KE-GCN (Yu et al., 2021) further incorporates the composition with relation update. Overall, GNN-based models achieve promising results in KG modeling, which demonstrates the importance of capturing structural information."}, {"title": "2.2 Hyper-relational Knowledge Graph (HKG) Modeling", "content": "As described before, related studies mainly exploit two aspects of semantic information and structural information for HKG modeling, considering HKG-based SF design and hyper-relational graph structure, respectively.\nSemantic Modeling Studies. For the semantic information, given a hyper-relational fact (Alan Turing, educated at, Cambridge, (degree, Bachelor)), some studies (Wen et al., 2016; Zhang et al., 2018; Abboud et al., 2020; Fatemi et al., 2020, 2021; Liu et al., 2020; Wang et al., 2023) treat all involved relations as an n-ary composed relation educated at_degree (heren is 3) with the fact (educated at_degree, Alan Turing, Cambridge, Bachelor). These studies are directly extended from KG modeling methods without multiple relational semantics considered. Especially, both m-TransH (Wen et al., 2016) and RAE (Zhang et al., 2018) extend the SF of TransH (Wang et al., 2014) to the hyper-relational case, while BoxE (Abboud et al., 2020) combines translational idea with box embeddings. Moreover, GETD (Liu et al., 2020) and S2S (Di et al., 2021) are both generalized from TuckER (Balazevic et al., 2019), where GETD further introduces tensor ring decomposition while S2S applies neural architecture search techniques. The bilinear product is also extended to multilinear product with symmetric embeddings in m-DistMult (Yang et al., 2015), convolutional filters in HypE (Fatemi et al., 2020), and relational algebra operations in ReAlE (Fatemi et al., 2021). On the other hand, NaLP (Guan et al., 2019) and RAM (Liu et al., 2021) decompose all involved relations into semantically equal attributes, and treat the example fact into a collection of attribute-value qualifiers, (educated at_head, Alan Turing, educated at_tail, Cambridge, degree, Bachelor). Nevertheless, models above largely ignore the semantic difference in hyper-relational facts. To capture the semantic difference between the primary triple and attribute-value qualifiers, NeuInfer (Guan et al., 2020) and HINGE (Rosso et al., 2020) design two sub-modules for HKG modeling, i.e., one for triple modeling and the other one for qualifier modeling, where NeuInfer mainly adopts fully connected layers while HINGE resorts to convolutional neural networks. Besides, GRAN (Wang et al., 2021), Hy-Transformer (Yu and Yang, 2021) and HyNT (Chung et al., 2023) leverage transformer and embedding processing techniques for HKG modeling. However, these neural network based models rely on tremendous parameters for expressivity and are prone to overfitting.\nStructural Modeling Studies. As for the structural information, G-MPNN (Yadati, 2020) ignores attribute information and treats HKG as a multi-relational ordered hypergraph with n-ary composed relations, and further proposes multi-relational HGNN for modeling. The rough design makes G-MPNN less competitive in practice. StarE (Galkin et al., 2020) firstly introduces GNN for HKG modeling with a relation-specific message passing mechanism developed. However, StarE aggregates hyper-relational fact messages for a specific entity only when the entity involves with the primary triple, but ignores the ones when the entity is in attribute-value qualifiers, i.e., StarE only captures connections among primary triples (Yu and Yang, 2021). Thus, capturing structural information for HKG modeling is still immature and needs further investigation.\nOverall, existing HKG modeling studies are affected by various limitations from semantics and structure, while our proposed TransEQ elegantly models both aspects with full expressivity achieved, which is a quite important property for learning capacity in both KG modeling (Balazevic et al., 2019; Sun et al., 2019) and HKG modeling (Fatemi et al., 2020; Liu et al., 2020; Abboud et al., 2020). Besides, the inductive link prediction and logical query for HKG are investigated in recent studies (Ali et al., 2021; Alivanistos et al., 2022; Chen et al., 2022; Lee et al., 2023), which are beyond the scope of this paper."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Hypergraph & Hyperedge Expansion", "content": "A hypergraph is a generalization of graph, where a hyperedge can join any number of nodes (Ouvrard, 2020; Antelmi et al., 2023). Especially, hyperedge expansion (Agarwal et al., 2006; Zhou et al., 2006; Dong et al., 2020) is introduced to transform a hypergraph to a homogeneous graph, such that graph learning methods can work on hypergraphs (Yadati et al., 2019; Feng et al., 2019). Since HKG is viewed as a multi-relational hypergraph (Yadati, 2020), here we investigate the representative expansion strategy of star expansion for additional insights.\nOn the other hand, the structural information loss has always been a concerned issue with hyperedge expansion strategy (Zhou et al., 2006; Dong et al., 2020; Antelmi et al., 2023). To be specific, an expansion strategy on hypergraph suffers from structural information loss, if there can be two distinct hypergraphs on the same node set reduced to the same graph by the expansion (Dong et al., 2020). According to (Dong et al., 2020), the star expansion preserves the complete structural information. However, existing hyperedge expansion strategies are focus on hypergraph, which cannot handle the HKG with hyper-relational semantics considered. Therefore, a generalization of hyperedge expansion to HKG is necessary to preserve both structural and semantic information therein."}, {"title": "3.2 Hyper-relational Knowledge Graph", "content": "Here we introduce the mathematical definition of HKG as well as the investigated problem.\nDefinition 1 Hyper-relational Knowledge Graph. A HKG is defined as GH = (E,R,FH), where E and R are the sets of entities and relations, respectively. A hyper-relational fact can be expressed as (s,r,o, {(ai, vi)}=1), where (s,r,o) is the primary triple and {(ai, vi)| ai \u2208 R, vi \u2208 E}=1 is the attribute-value qualifier set. Moreover, FH \u2286 E\u00d7R\u00d7 E \u00d7 P denotes the fact set and P denotes all possible combinations of attribute-value qualifiers.\nNote that the number of qualifiers can be zero for a hyper-relational fact, i.e., HKG reduces to KG with an empty set P. In practice, attributes and values are also described by relations and entities, respectively (Galkin et al., 2020; Yu and Yang, 2021). Then we state our research problem.\nProblem 1 HKG Modeling Problem. Given a HKG GH = (E,R, FH), the HKG modeling problem aims to learn representations for entities and relations in E and R.\nEspecially, the HKG is always incomplete, which specifies the research problem as HKG completion problem in practice, i.e., given an incomplete hyper-relational fact with an entity missing at triple or qualifiers, inferring the missing entity from E with observable facts FH. According to the definition, HKG involves semantic information of primary triple and attribute-value qualifiers as well as structural information of hyper-relational graph structure, which should be elegantly considered in modeling."}, {"title": "4 Method", "content": "As described before, the encoder-decoder framework has shown superior performance to capture both structural information and semantic information in KG (Schlichtkrull et al., 2018; Vashishth et al., 2019; Lee et al., 2023), and thus a natural idea is to explore it for HKG modeling. Moreover, the hyperedge expansion studies provide a motivation to our work transforming a HKG to a KG with the encoder-decoder framework combined. Hence, we build TransEQ with such points in mind, which is presented in the following."}, {"title": "4.1 The TransEQ Model", "content": "We now come to the details of our proposed TransEQ model, the architecture of which is illustrated in Figure 3. TransEQ first introduces the equivalent transformation with a HKG transformed to a KG, and then develops an encoder-decoder framework to capture both structural information and semantic information in HKG. The model training process as well as other variants of transformations are also provided in this part."}, {"title": "4.1.1 ONE STONE: EQUIVALENT TRANSFORMATION", "content": "To identify the importance of transformation between HKG and KG, here we first introduce the definition of equivalent transformation.\nDefinition 2 Equivalent Transformation. A transformation between HKG and KG is equivalent, if the transformation preserves the complete information, i.e., given any HKG and its transformed KG via the transformation, they can be retrieved from each other.\nMoreover, a hyper-relational fact (s, r, o, {(ai, vi)}=1) can be viewed as a hyper-relational edge, which connects entities of s, 0, {vi}=1 with heterogeneous semantics of primary relation r and attributes {a}=1, as shown in Figure 4(a) with the k-th fact in a HKG. Thus, by generalizing star expansion as well as standard RDF reification in semantic web (Bollacker et al., 2008; Hern\u00e1ndez et al., 2015; Frey et al., 2019), we propose an equivalent transformation for hyper-relational edges such that entities and relations in the original HKG are reorganized to the transformed KG with both structural information and semantic information preserved."}, {"title": "4.1.2 TwO BIRDS: ENCODER-DECODER FRAMEWORK", "content": "To model both structural information and semantic information in the original HKG, TransEQ further introduces an encoder-decoder framework on the transformed KG.\nAs for the encoder part, powerful GNN is developed to capture structural information, where the semantic relatedness in HKG and the mediator entities in equivalent transformation are also incorporated therein. Especially, the semantic relatedness explicitly lies in the shared primary relations across hyper-relational facts. For example, the hyper-relational facts of (Alan Turing, educated at, Cambridge, (degree, Bachelor)) and (Alan Turing, educated at, Princeton, (degree, PhD)) share the same primary relation, which indicates a strong semantic relatedness. On the other hand, in our proposed equivalent transformation, each mediator entity plays an important role of relaying connections among the entities in an original hyper-relational fact, and thus mediator entities aggregate the semantics of corresponding facts.\nHence, we introduce the sharing embedding for mediator entities to capture the semantic relatedness, and further combine it with three steps of unified multi-relational message passing mechanism in KG-based GNN (Schlichtkrull et al., 2018; Vashishth et al., 2019).\n\u2022 Initialized embedding. Given the embedding dimension d, for the mediator entity b, we denote \u03c8(b) the mapping from b to its involved primary relation, and initialize its representation as hb = [e\u03c8(b); eb], where e\u03c8(b) \u2208 R[\u03b1d] and eb \u2208 R(1\u2212\u03b1)[d] are sharing and independent embeddings, respectively. \u03b1 is the hyperparameter to tune the sharing embedding ratio. Thus, mediator entities involved with the same primary relation \u03c8(b) share part of embedding e\u03c8(b)\u00b7\n\u2022 Message calculation. Considering the stacking layers of GNN, we denote mutl+1,ent and mutl+1,rel the messages from a triple (u, r, t) for target entity t and relation r at the (l+1)-th layer, respectively, which are calculated as follows,\n\nmultl+1,ent = MSGent(hu, h, h), mutl+1,rel = MSGrel(hu, h, h),\nwhere hu, h, h \u2208 Rd are the embeddings of entities and relation at the l-th layer, while MSGent and MSGrel can be composition function in CompGCN (Vashishth et al., 2019), relation-specific projection in R-GCN (Schlichtkrull et al., 2018) and etc. Besides, the entity representations at the input layer are expressed as,\n\nh0x = \n \t [e\u03c8(x); ex] if x is mediator entity\n\te\u2032x \u2208 Rd if x is original entity\n\n, for x \u2208 {u, t},\n\u2022 Message aggregation. Then neighborhood messages of Ml+1 and Ml+1 are aggregated as follows,\n\nMl+1t = AGGent(mul+1,entrt | r \u2208 R, u \u2208 Nrt),\nMl+1r = AGGrel(mul+1,relut | (u, t) \u2208Nr),\nwhere Nrt denotes the entities linked to t via relation r and Nr denotes the entity pair linked by relation r. AGGent and AGGrel are aggregation functions such as mean and sum pooling functions.\n\u2022 Representation update. Finally, the representations at the (l + 1)-th layer are updated with aggregated messages and former layer representations:\n\nhl+1t = UPDent(Ml+1t , hl), hl+1r = UPDrel(Ml+1r, h),\nwhere UPDent and UPDrel can be nonlinear activation functions.\nOwing to above encoding process, TransEQ fully exploits the topological connections between entities for structural information.\nFurthermore, the decoder part exploits various SFs to model semantic information. For each hyper-relational fact, the encoder part feeds the representations of corresponding entities and relations into the SF-based decoder to model the interaction between entities and relations therein. Especially, the choice of SF is orthogonal to the encoder (Schlichtkrull et al., 2018; Vashishth et al., 2019), and most existing SFs on HKG modeling can be modified in the decoder. For example of the hyper-relational fact x := (s, r, o, {(ai, vi)}=1) \u2208 FH, we rewrite m-DistMult's SF (Fatemi et al., 2020) as:\n\u03c8(x) = \u3008\u03c6(hs), \u03c6(hr), \u03c6(ho), \u03c6(ha1), \u00b7 \u00b7 \u00b7 , \u03c6(haL), \u03c6(hv1), \u00b7 \u00b7 \u00b7 , \u03c6(hvL)\u3009,\nwhere \u03c6(x) is plausibility score measured by TransEQ, \u3008\u00b7\u3009 denotes the multilinear product4, and L denotes the number of GNN layers in encoder part. Since m-DistMult adopts a composed relation for SF, we introduce the function \u03c6 to aggregate the embeddings of involved primary relation and attributes for the composed relation embedding, such as mean/sum pooling function. Note that the semantic difference in HKG is also modeled by the SF in decoder. Various SFs are further investigated in experiments later."}, {"title": "4.1.3 MODEL TRAINING", "content": "Algorithm 2: TransEQ training algorithm.\nInput: HKG GH = (E,R, FH);\nInit: E for e \u2208 E, R for r \u2208 R, \u03b8Enc for GNN-based encoder, \u03b8Dec for SF-based decoder;\n1 Build encoder module Enc(\u00b7) with \u03b8Enc;\n2 Build decoder module Dec(\u00b7) with \u03b8Dec;\n3 Transform HKG GH to KG G with Algorithm 1;\n4 for t = 1, . . . , Niter do\n5 Sample a mini-batch Fbatch \u2208 FH of size mb, L \u2190 0;\n6 E, R = Enc(G, E, R, \u03b8Enc);\n7 for x:= (s,r,o, {(ai, vi)}=1) \u2208 Fbatch do\n8 Construct negative samples Nx;\n9 \u03c6(x) = Dec(x, E, R, \u03b8Dec);\n10 \u03c6(x\u2032) = Dec(x\u2032, E, R, \u03b8Dec), \u2200x\u2032 \u2208 Nx;\n11 Update loss L \u2190 L + Lx(\u03c6) with Lx in (1);\n12 end\n13 Update learnable parameters w.r.t. the gradients \u2207L;\n14 end\nOutput: Embeddings E, R and parameters \u03b8Enc, \u03b8Dec.\nTo learn the model parameters, we adopt the cross-entropy loss for training (Fatemi et al., 2020; Yadati, 2020; Liu et al., 2021). For the hyper-relational fact x \u2208 FH with \u03c6(x), the practical loss can be written as:\nL = \u2211Lx(\u03c6) = \u2212 log\n\nexpx \u03c6(x)\nLx \u2211x\u2032\u2208Nx exp \u03c6(x\u2032)\n(1)"}, {"title": "4.1.4 OTHER VARIANTS OF GENERALIZED TRANSFORMATIONS", "content": "To demonstrate the effectiveness of our generalized equivalent transformation in Section 4.1.1, here we further show other variants of transformations in Figure 5. Especially, the plain transformation in Figure 5(a) follows star expansion without attributes considered."}, {"title": "4.2 Theoretical Understanding", "content": ""}, {"title": "4.2.1 COMPLEXITY ANALYSIS", "content": "To distinguish our proposed TransEQ model design, in Table 1, we present a comparison of HKG modeling studies with structural modeling, semantic modeling, full expressivity as well as time and space complexity.\nAccording to the table, structural information is rarely explored in existing studies, while HGNN in G-MPNN is at its early stage and thus fails to model attribute semantics. StarE only captures triple-based connections (Yu and Yang, 2021), while TransEQ combines the equivalent transformation with GNN for structural information. As for modeling semantic information, existing studies follow three views of relations in HKG, and adopt various SFs to model the interaction between entities and relations. Especially, TransEQ can utilize arbitrary SF with semantic difference considered. Compared with the weak expressive power of most studies, the flexible choice of SF guarantees the full expressivity of TransEQ to model various HKGs, and brings performance improvement.\nBesides, the message passing mechanism in modeling structural information leads to the time complexity of O(Nd\u00b2), and Transformer module in StarE brings an additional complexity of O(n\u03b1d\u00b2). Since the equivalent transformation introduces a mediator entity for each hyper-relational fact, TransEQ builds the space complexity of O(ned + nrd + Nd)"}, {"title": "4.2.2 INFORMATION PRESERVING TRANSFORMATION", "content": "Following the structural information loss concern in hyperedge expansion (Zhou et al., 2006; Dong et al., 2020; Arya et al., 2021), here we investigate the information loss problem for our generalized transformation on HKG, which emphasizes on preserving both structural information and semantic information.\nBased on the equivalent transformation in Definition 2 and the generalized transformation in TransEQ, we identify the property with the following theorem, and provide the proof in Appendix A.2.\nTheorem 1 In the conversion from a HKG to a KG, the generalized transformation in TransEQ is an equivalent transformation and preserves the complete information, while other variants of transformations like plain transformation and clique-based transformations can lead to partial information loss."}, {"title": "4.2.3 FULL EXPRESSIVITY", "content": "To demonstrate the expressivity of TransEQ, here we introduce the full expressivity property (Fatemi et al., 2020; Liu et al., 2020; Abboud et al., 2020; Fatemi et al., 2021; Di et al., 2021). A HKG modeling model is fully expressive if, for any given HKG, the model can separate valid hyper-relational facts from invalid ones by appropriate parameter configuration.\nConsidering the encoder-decoder framework in TransEQ, such property is mainly determined by the SF in decoder part, thus we establish the expressivity of TransEQ with the following theorem.\nTheorem 2 With encoder parameters configured appropriately, the expressivity of TransEQ is in accord with that of the scoring function it uses in decoder, i.e., TransEQ is fully expressive if the scoring function used in decoder is fully expressive.\nThe proof is provided in Appendix A.3. Thus, with appropriate choice of SF like HypE (Fatemi et al., 2020) as well as model parameters, a fully expressive TransEQ model has the potential to represent all types of relations in HKG including symmetric relations, inverse relations, etc. (Sun et al., 2019; Liu et al., 2021), which generally outperforms the weak ones in practice, as validated in Section 5.4."}, {"title": "5 Experiments and Results", "content": ""}, {"title": "5.1 Experimental Setup", "content": ""}, {"title": "5.1.1 DATASETS", "content": "The experiments are conducted on three benchmark HKG datasets, i.e., WikiPeople (Guan et al., 2019), JF17K (Zhang et al., 2018) and FB-AUTO (Fatemi et al., 2020). We follow the original splits of WikiPeople and FB-AUTO, while JF17K follows the split in RAM (Liu et al., 2021) with validation set further split. The detailed statistics are summarized in Table 2. Details are provided in Appendix B."}, {"title": "5.1.2 BASELINES", "content": "As for performance comparison, we compare with several state-of-the-art HKG modeling approaches, including semantic modeling ones of m-TransH (Wen et al., 2016), HypE (Fatemi et al., 2020), RAM (Liu et al., 2021), S2S (Di et al., 2021), HINGE (Rosso et al., 2020), NeuInfer (Guan et al., 2020), BoxE (Abboud et al., 2020), HyConvE (Wang et al., 2023) as well as structural modeling ones of StarE (Galkin et al., 2020), G-MPNN (Yadati, 2020)."}, {"title": "5.1.3 TASK AND EVALUATION METRICS", "content": "Following typical settings (Liu et al., 2020; Abboud et al., 2020; Fatemi et al., 2020; Guan et al., 2019; Wang et al., 2021), we evaluate HKG modeling approaches on HKG completion task in transductive setting, and predict the missing entity at each position. i.e., for a testing sample (s, r, o, {(ai, vi)}=1), we assume that either entity in {s, 0, v1, \u00b7 \u00b7 \u00b7 , vn} may be missing, and apply the model to predict missing entity. Note that this task is more generalized than previous study StarE (Galkin et al., 2020) only predicting missing entity in {s,p} of triple part. As for evaluation metrics, the standard mean reciprocal ranking (MRR) and Hit@{1,3,10} are utilized in filtered setting (Bordes et al., 2013)."}, {"title": "5.1.4 IMPLEMENTATION", "content": "We implement TransEQ in PyTorch (Paszke et al., 2019) with Adam optimizer. The embedding dimension d is set to the typical size 200 (Wang et al., 2021; Abboud et al., 2020; Fatemi et al., 2020; Yu and Yang, 2021; Galkin et al., 2020). The batch size, learning rate and dropout are chosen from {64, 128}, {0.0001, 0.0005, 0.001, 0.005} and [0.1, 0.5] with step 0.1, respectively. Besides, we mainly adopt CompGCN (Vashishth et al., 2019) as encoder and m-DistMult (Fatemi et al., 2020) as decoder. For the encoder part, the number of GNN layers and sharing ratio \u03b1 are chosen from {1, 2, 3, 4} and [0.0, 1.0] with step 0.2, respectively. The composition operation in encoder is set to rotate function (Sun et al., 2019). We tune hyperparameters over the validation set with early stopping strategy employed. All experiments are run on a RTX 2080 Ti GPU."}, {"title": "5.2 HKG Completion Results", "content": "We present the benchmark comparison of HKG completion in Table 3. According to the results, our proposed TransEQ model achieves the state-of-the-art performance on all benchmarks. On the hardest dataset WikiPeople with the most entities and relations, TransEQ significantly improves the best baseline (BoxE) by 27% and 15% on Hit@1 and MRR, respectively. Considering hyper-relational connections provided in WikiPeople, this improvement demonstrates that our proposed equivalent transformation preserves complete HKG information. Besides, TransEQ significantly outperforms m-DistMult, its original decoder model without GNN-based encoder, which indicates the effectiveness and necessity to consider structural information in HKG modeling. Such results also imply that with powerful SFs like BoxE, TransEQ can obtain even better performance. Moreover, compared with structural modeling approaches of G-MPNN and StarE, the substantial improvement of TransEQ owes to subtle design of the equivalent transformation as well as the semantic information captured in the decoder part.\nFurthermore, we compare the learning processes of TransEQ with structural modeling approaches on three datasets in Figure 6. The learning curve of HypE with linear time complexity is also plotted for comparison. It can be observed that TransEQ achieves similar convergence speed with HypE in practice, which owes to the multilinear product based SF (Liu et al., 2021) and efficient implementation. With a similar form of SF adopted, G-MPNN achieves a close convergence rate but inferior performance, which demonstrates the strength of GNN-based encoder compared with HGNN. As for StarE with Transformer-based SF, tremendous parameters lead to time-consuming training on all datasets."}, {"title": "5.3 Transformation Comparison", "content": "To analyze the effects of various transformations, we present the performance comparison in Table 4. As described in Section 4.1.1, our proposed equivalent transformation connects subject and object entities via a relational edge r to form the motif for semantic difference. Thus, we investigate the effectiveness of such operation by removing the edge in the transformation, referred to as w/o distinction transformation.\nFrom the table, we can observe that the equivalent transformation outperforms other variants of transformations, which is in accord with the information loss analysis in Section 4.2.2, i.e., only equivalent transformation preserves complete information. Moreover, removing the relational edge in the equivalent transformation leads to a Hit@1 performance drop of 7% on JF17K, which demonstrates the effectiveness and necessity of considering semantic difference in the transformation. Besides, since any two entities are connected in two clique-based transformations, the relatedness between entities is largely captured and thus they obtain close performance, i.e., the clique structure makes these transformations insensitive to semantic information. In comparison with equivalent transformation, the star structure in plain transformation is quite simple without hyper-relational semantics incorporated, which also accounts for the obvious gap between these two transformations. Such results also validate the effectiveness of our generalization from hyperedge expansion to equivalent transformation. Considering the zero information loss and experimental performance, the equivalent transformation becomes the best choice for TransEQ."}, {"title": "5.4 Encoder-Decoder Choice Comparison", "content": "To further investigate the effects of different GNN-based encoders along with HKG-based SFs as decoders, we compare the performance of different encoder-decoder choices on JF17K and FB-AUTO in Table 5. In the table, each result corresponds to the TransEQ model with X as encoder and Y as decoder.\nAccording to the results of each row in Table 5, compared with original models (X=No Encoder), TransEQ models with various GNN-based encoders bring substantial improvement, which again demonstrates the effectiveness of structural information encoding. Since neural network models with tremendous parameters easily overfit, the performance improvement of GNN-based encoder for Transformer is much lower than that for other models. As for the encoder in each column, the decoder choices of HypE achieve the best performance, mainly attributed to the linear complexity and full expressivity property. Benefited from the proposed encoder-decoder framework, TransEQ can flexibly adapt to various GNNs and SFs for both superior performance and full expressivity."}, {"title": "5.5 Information Sharing Study", "content": "To validate whether the semantic relatedness in HKG is captured by sharing embedding on mediator entities, we obtain hyper-relational facts of top ten primary relations and visualize their mediator entity embeddings via t-SNE (Maaten and Hinton, 2008), as shown in Figure 7, which compare the cases of utilizing sharing embedding (the best setting with \u03b1 = 0.8) and independent embedding (\u03b1 = 0.0). We select WikiPeople for"}]}