{"title": "On the Complexity of Learning to Cooperate with Populations of Socially Rational Agents", "authors": ["Robert Loftin", "Saptarashmi Bandyopadhyay", "Mustafa Mert \u00c7elikok"], "abstract": "Artificially intelligent agents deployed in the real-world will require the ability to reliably cooperate with humans (as well as other, heterogeneous AI agents). To provide formal guarantees of successful cooperation, we must make some as-sumptions about how partner agents could plausibly behave. Any realistic set of assumptions must account for the fact that other agents may be just as adaptable as our agent is. In this work, we consider the problem of cooperating with a population of agents in a finitely-repeated, two player general-sum matrix game with private utilities. Two natural assumptions in such settings are that: 1) all agents in the population are individually rational learners, and 2) when any two members of the population are paired together, with high-probability they will achieve at least the same utility as they would under some Pareto efficient equilibrium strategy. Our results first show that these assumptions alone are insufficient to ensure zero-shot cooperation with members of the target population. We therefore consider the problem of learning a strategy for cooperating with such a population using prior observations its members interacting with one another. We provide upper and lower bounds on the number of samples needed to learn an effective cooperation strategy. Most importantly, we show that these bounds can be much stronger than those arising from a \"naive\" reduction of the problem to one of imitation learning.", "sections": [{"title": "1 Introduction", "content": "In this work, we address the problem of learning to cooperate with a socially intelligent population of agents from observations interactions between members of this population. We study cooperation in finitely-repeated, two-player, general-sum matrix games with private payoffs. W say that a population of adaptive agents is socially intelligent if its members are (1) individually Hannan-consistent and (2) compatible in the sense that any pair of agents will perform nearly as well as some Pareto-optimal Nash equilibrium of the matrix game. We argue that this model of cooperation is more realistic than those that assume identical payoffs or public utilities. In real-world applications it is unlikely that independent agents will have identical utilities, or that they will provide complete information about their preferences or future behaviour to others. In the case of AI-AI cooperation,"}, {"title": "2 Preliminaries", "content": "Repeated bi-matrix games with private types. Let $i \\in \\{1,2\\}$ denote the agent index. We assume both agents have $N$ pure strategies (henceforth \"actions\"). Let $\\Theta$ denote the finite type space, where $\\theta_1, \\theta_2 \\in \\Theta$ denote the private types of the two agents, and $\\theta = (\\theta_1, \\theta_2)$ denotes the joint type. We denote agent $i$'s payoff matrix as $G(\\theta_i) \\in \\mathbb{R}^{N \\times N}$, and let $G(\\theta) = [G(\\theta_1), G(\\theta_2)^T]$ denote the bi-matrix game parameterized by $\\theta$ (with agent 1 as the row player). In a single episode, the agents play $G(\\theta)$ for a fixed number of stages $0 < T < \\infty$. We let $a_t^1$ and $a_t^2$ denote the actions chosen by agents 1 and 2 in stage $0 < t < T$. For mixed strategies $\\sigma, \\sigma' \\in \\Delta(N)$, we let $G(\\sigma, \\sigma'; \\theta) = \\sigma^T G(\\theta) \\sigma'$.\nWe overload $a_t^1$ and $a_t^2$ to also denote the mixed strategies that assign all probability mass to actions $a_t^1$ and $a_t^2$, such that $G(a_t^1, a_t^2; \\theta_1)$ and $G(a_t^1, a_t^2; \\theta_2)$ are agent 1 and 2's respective payoffs at stage $t$. We also assume that for all $\\theta \\in \\Theta$, $G_{ij} (\\theta) \\in [0, 1], \\forall i, j \\in [N]$.\nLet $H_t = (N \\times N)^t$ be the set of histories of length $t$ (with $H_0 = \\{\\emptyset\\}$), and let $H_{<t} = \\bigcup_{s=0}^t H_s$ be the set of all histories of length at most $t$. The strategy space $\\Pi$ for an agent is then the space of mappings $\\pi: \\Theta \\times H_{<T-1} \\rightarrow \\Delta(N)$, where $\\Delta(N)$ is the set of probability distributions over the action set $[N]$. As a functional, a strategy $\\pi$ maps each type $\\theta$ to a behavioral strategy [2, Chapter 5.2.2] that maps histories of play to action distributions, such that $a_t^i \\sim \\pi_i(\\theta_i, h_{t-1})$. We denote agent $i$'s expected total payoff for following strategy $\\pi$ against $\\pi'$ as\n$M_i(\\pi, \\pi'; \\theta, \\theta') = \\mathbb{E}_{\\theta, \\theta', \\atop [\\cdot] \\sim \\pi, \\pi'} [\\sum_{t=1}^T G(a_t^i, a_t^{-i}; \\theta_i) | \\pi_i = \\pi, \\pi_{-i} = \\pi', \\theta_i = \\theta, \\theta_{-i} = \\theta']$ (1)\nwhere the expectation is taken over the actions $a_t^1$ and $a_t^2$ sampled from the agents' strategies."}, {"title": "2.1 Consistency", "content": "A natural criterion for rationality is that an agent should attempt to to achieve a payoff nearly as large as the best response to its partner's average strategy, which we refer to as consistency. To account for the non-stationary behavior of other agents', we specifically consider Hannan consistency [3], which in our finite-time setting simply requires that an agent have bounded external regret over $T$ stages. The external regret for agent $i$ is defined as\n$R_t^{ext}(h_t; \\theta) = \\max_{\\alpha \\in [N]} \\sum_{t=1}^h \\{G(a_t^i, a_t^{-i}(h); \\theta_i) - G(a_t(h), a_t^{-i}; \\theta_i)\\}$ (2)\nwhere $a_t^i(h)$ denotes the action $i$ played at stage $t$ within the history $h \\in H_{<T}$.\nDefinition 2.1 (Consistency). For $\\delta, \\epsilon, T > 0$, an agent $i \\in \\{1,2\\}$ is $(\\delta, \\epsilon, T)$-consistent if, for all types $\\theta \\in \\Theta$, and any partner strategy, we have that $P[- R^{ext}(h_T; \\theta) < \\epsilon] > 1 - \\delta$.\nWe also define the expected external regret $\\mathbb{R}_t^{ext}(h_t; \\theta)$ by replacing the $a_t^i(h)$ (the action $i$ played at stage $t$) with their full strategy $\\pi_i (\\theta_i, h_t)$. $R_t^{ext}(h_t; \\theta)$ and $\\mathbb{R}_t^{ext}(h_t; \\theta)$ are related by the inequality\n$R_t^{ext}(h_t; \\theta) \\leq \\mathbb{R}_t^{ext}(h_t; \\theta) + \\sqrt{\\frac{T}{\\delta}}$ (3)\nwhich holds w.p. at least $1-\\delta$ for all $t < T$ simultaneously (this follows directly from [4, Lemma 4.1]). We therefore only need to bound $\\mathbb{R}_t^{ext}(h_t; \\theta)$ to provide high-probability regret bounds."}, {"title": "2.2 Cooperative compatibility", "content": "Even in a fully cooperative game, the fact that both agents are consistent does not guarantee that they will achieve an optimal outcome. In the $2 \\times 2$ game in Table 1a for example, both $(A, A)$ and $(B, B)$ are Nash equilibria to which consistent agents could converge, but only $(A, A)$ is optimal. In general-sum games, consistency may preclude Pareto-optimal outcomes, as in the classic prisoner's dilemma game (Table 1b), where the only outcome in which neither player incurs positive regret is $(D, D)$, which is Pareto-dominated by $(C, C)$.Therefore, similar to [1], we define successful cooperation in terms of the Pareto-optimal Nash equilibria (PONE) [5] of a game $G$.\nLet $\\mathcal{N}(G) \\subseteq \\Delta(N) \\times \\Delta(N)$ be the set of Nash equilibria (NE) of $G$. For a fully-cooperative game, $\\mathcal{N}(G)$ will contain all globally optimal strategy profiles for $G$. It may, however, also contain joint strategies that are highly sub-optimal. Let $\\mathcal{P}(G) \\subseteq \\mathcal{N}(G)$ denote the set of Pareto optimal Nash equilibria. In this work, we say that a strategy profile $(\\sigma_1, \\sigma_2) \\in \\mathcal{P}(G)$ if and only if $(\\sigma_1, \\sigma_2) \\in \\mathcal{N}(G)$, and there does not exist $(\\sigma_1', \\sigma_2') \\in \\mathcal{N}(G)$ such that $G(\\sigma_1', \\sigma_2'; \\theta_1) > G(\\sigma_1, \\sigma_2; \\theta_1)$ and $G(\\sigma_2', \\sigma_1'; \\theta_2) > G(\\sigma_2, \\sigma_1; \\theta_2)$. This means that $\\langle \\sigma_1, \\sigma_2 \\rangle$ is a PONE if it is a Nash equilibrium of $G$, and it is not strongly Pareto-dominated by any other Nash equilibrium of $G$. Intuitively, if two agents are individually consistent, and willing to cooperate with each other, their joint payoff profile should not be dominated by any PONE. We formalize this intuition as follows:\nDefinition 2.2 (Compatibility). For $\\delta, \\epsilon, T > 0$, two agents $\\pi^1$ and $\\pi^2$ are $(\\delta, \\epsilon, T)$-compatible if, when played together, for any joint type $\\theta \\in \\Theta \\times \\Theta$, w.p. at least $1 - \\delta$, $\\exists (\\sigma_1^*, \\sigma_2^*) \\in \\mathcal{P}(G(\\theta))$ s.t.\n$\\frac{1}{T} \\sum_{t=1}^T (G_i(\\sigma_i^*, \\sigma_{-i}^*; \\theta_i) - G(a_t^i, a_t^{-i}; \\theta_i)) \\leq \\epsilon,$ (4)\nfor both $i = 1$ and $i = 2$.\nA pair of agents is compatible if, when paired together, with high-probability over their path of play $h_t$ there will exist some PONE that does not $\\epsilon$-dominate their realized payoffs. Note that this definition is the approximate and finite-horizon version of the one provided in [1]."}, {"title": "2.3 Socially intelligent agents", "content": "We argue that it is natural to model an existing population of cooperating agents as a set of approx-imately compatible, but otherwise heterogeneous agents. We therefore introduce the more general idea of a socially intelligent class of agents that are compatible with any other member of their class:\nDefinition 2.3 (Social Intelligence). A set $C$ of agents forms a socially intelligent class w.r.t. $\\Theta$ if, for some $\\delta, \\epsilon, T > 0$, each agent $\\pi \\in C$ is $(\\delta, \\epsilon, T)$-consistent for all $\\theta \\in \\Theta$, and any two agents $\\pi, \\pi' \\in C$ are $(\\delta, \\epsilon, T)$-compatible over all joint types $\\Theta$. An individual agent $\\pi$ is called socially intelligent if it forms a socially intelligent class $\\{\\pi\\}$ with itself.\nThe Hannan consistency requirement ensures that any agent in the population always has bounded average regret, whereas the approximate compatibility means if both agents are from $C$, with high probability there will exist some PONE that does not $\\epsilon$-dominate their path of play. Below we describe a socially intelligent class based on a pre-agreed coordination protocol.\nCoordination protocols For a type space $\\Theta$, we first define a function $s(\\theta) \\in \\mathcal{P}(G(\\theta))$ that maps from each joint type $\\theta$ to a strategy profile in $\\mathcal{P}(G(\\theta))$. We can think of $s(\\theta)$ as a common \"convention\" the agents in $C$ have settled upon. Since we assume private types, members of $C$ do not know each other's type at the beginning of their interaction. If any type $\\theta \\in \\Theta$ can be communicated to others in a sequence of $k < T$ actions, then agents in $C$ can agree on a coordination protocol similar to a handshake. Let the protocol be a map $\\kappa(\\theta)$ from types to a history-dependent policy. Then, at the beginning of each interaction, both agents will play $\\kappa$ for $k$-steps in order to communicate their types. After coordinating with each other, the agents play $s(( \\theta_i, \\theta_{-i}))$ for the remaining $T - k$ steps. The agents must still ensure their partner does not deviate from $s(( \\theta_i, \\theta_{-i}))$ for safety against adversarial \u201cimposters\u201d. Since playing a PONE jointly will lead to low regret for both, if $i$'s regret exceeds a certain threshold, this would indicate $-i$ is deviating from $s$ significantly. The threshold can be chosen by the aid of the following lemma,\nLemma 2.4. For any $\\delta, T > 0$, if both players follow strategy $s(\\theta)$ at each stage, then with probability at least $1 - \\delta$ we have\n$R_t^{ext}(h_t; \\theta_i) \\leq \\sqrt{\\frac{2}{T}} \\ln \\frac{4}{\\delta} \text{ and } \\mathbb{R}_t^{ext}(h_t; \\theta_i) \\leq \\sqrt{\\frac{2}{T}} \\ln \\frac{4}{\\delta},$ (5)\nwhich follows from an application of the Azuma-Hoeffding inequality (shown in Appendix A.1). Then the question is what safe strategy should the $i$ fall back into, if the rule is triggered. We base the fallback strategy on the multiplicative weights [6] update rule, defined as:\n$s_{mw, k}(h_t; \\theta_i) \\propto s_{mw, k}(h_{t-1}; \\theta_i) \\exp \\left( - \\eta G(k, a_{t-1}^{-i}(h); \\theta_i) \\right)$ (6)\nfor $k \\in N$, where $s_{mw}(h_0; \\theta_i)$ is the uniform strategy. Define $\\pi^{mw, T}$ as the agent that plays $s_{mw}(h_t; \\theta_i)$ with learning rate $\\eta = \\sqrt{\\frac{8 \\ln (N)}{T}}$. The expected external regret of $\\pi^{mw, T}$ is bounded\nas\n$R^{ext}(h_T; \\theta_i) \\leq 2 \\sqrt{\\frac{T}{\\delta}} \\sqrt{\\frac{\\ln N}{2}}$ (7)\nsurely [4, Theorem 2.2]. We then define the agent's overall strategy $\\pi^{T, \\epsilon}$ as follows:\n1. In first $k$ steps, play $\\kappa(\\theta_i)$.\n2. If $-i$'s behaviour in $h_k$ not compatible with $\\kappa(\\theta)$ for any $\\theta \\in \\Theta$, switch to $\\pi^{mw, T}$ for all subsequent stages.\n3. While $\\mathbb{R}_t^{ext}(h_t; \\theta_i) \\leq k + \\epsilon (T - k) - \\sqrt{\\frac{T - k}{2}} \\sqrt{\\ln N} - 1$, play $s_i(\\theta)$.\n4. Otherwise, switch to $\\pi^{mw, T}$ for all subsequent stages.\nThe theorem below shows that agents that follow the social authentication strategy above form a socially intelligent class among themselves. All proofs have been deferred to appendix A.2.\nTheorem 2.5. For any $\\delta, T > k$, let $\\epsilon_0 \\geq \\sqrt{\\frac{2}{T - k}} \\ln \\frac{4}{\\delta}, and let $\\epsilon_1 = \\epsilon_0 + \\sqrt{\\frac{2}{T - k}} \\sqrt{\\ln N} + \\frac{1}{T - k}$. Then for $\\epsilon = \\epsilon_1 + \\sqrt{\\frac{2}{T - k}} \\ln \\frac{1}{\\delta}$, the $\\pi^{T, \\epsilon_1}$ is $(\\delta, \\epsilon, T)$-socially intelligent."}, {"title": "3 Learning to Cooperate", "content": "Going forward, we will assume that our agent (henceforth referred to as the \u201cAI\u201d) will take the role of agent 1, while the other agent (referred to as the \"partner\") will be agent 2. Our goal is to choose a strategy for the AI that can cooperate with a partner drawn from some target population nearly as effectively as agents from this population cooperate with one another. For parametric game $G$, with type space $\\Theta$, we will let the target population be a set $C$ of strategies forming an $(\\delta, \\epsilon, T)$-SI class w.r.t. $\\Theta$. Ideally, we would hope to choose an AI strategy $\\pi$ that can cooperate with $C$ without any additional information the strategies in $C$. Looking at the coordination protocol example in Section 2.3, we can see that in many cases a population is likely to use arbitrary conventions to coordinate their behavior, and intuitively we would imagine cooperation to be impossible without prior knowledge of these conventions. (We make this intuition formal in Theorem 3.5).\nWe therefore consider the problem of learning an cooperative AI strategy using prior observations of members of the target population interacting with one another. We define a social learning problem by a tuple $\\{G, \\Theta, C, \\rho, \\mu\\}$, where $C$ is the target population (SI w.r.t. $\\Theta$), $\\rho$ is a distribution over $C$, while $\\mu$ is a distribution over the joint type space $\\Theta \\times \\Theta$. We can think of $C$ as the set of possible strategies that any member of the target population might follow, while $\\rho$ is the frequency of those strategies within the population. To choose an AI strategy, we leverage a dataset $D = \\{(\\theta_i, \\theta_j, h^j)|j \\in [n]\\}$ covering $n$ episodes of length $T$. In each episode $j$, two agents $\\pi^i$ and $\\pi^j$ are sampled independently from $\\rho$, and played together under the joint type $\\theta^j \\sim \\mu$. The AI observes the full history $h^j$, along with the agents' types $\\theta_i$ and $\\theta_j$. We denote a specific learning algorithm as a data conditioned strategy $\\pi(D)$."}, {"title": "3.1 Altruistic Regret", "content": "We seek an AI strategy that minimizes the regret relative to some Pareto optimal solution to $G(\\theta)$. Rather than minimizing regret in terms of the Al's own payoffs, however, we seek to minimize partner's relative to their (worst case) PONE in $G(\\theta)$. We formalize this regret with the following definition:\nDefinition 3.1 (Altruistic Regret). Let the $(\\sigma_i^*, \\sigma_{-i}^*) \\in \\mathcal{P}(G_{-i}(\\theta_{-i}))$ denote the PONE with the lowest payoff for the agent $-i$ where $i \\in \\{1, 2\\}$. The altruistic regret of agent $i$ is defined as\n$R^{alt}(h_T; \\theta_{-i}) = \\sum_{t=1}^T G(\\sigma_{-i}^*, \\sigma_i^*; \\theta_i) - G(a_t^i(h_t), a_t^{-i}(h_t); \\theta_{-i}).$ (8)\nIn practical cooperation tasks, we would expect outcomes that have low regret for the partner will have low regret for the AI as well.\nThe cooperation objective for the AI agent can then be formalized as minimising the altruistic regret. Unlike the definition suggests, the AI agent must know its own type as well. This is due to the fact that as seen in the coordination protocols example, if the AI fails to imitate a human of its type or fail to communicate its type correctly, the partner might switch to a safe strategy.\nThe goal for the AI is to minimize its expected altruistic regret over partners sampled from $\\rho$ and types sampled from $\\mu$. The following lemma shows that we can treat the problem of minimizing regret with respect to a heterogeneous population $C$ as that of minimizing regret w.r.t. a single stochastic strategy.\nLemma 3.2. Let $C$ be a finite set of agents that are $(\\delta, \\epsilon, T)$-socially intelligent w.r.t. type space $\\Theta$, and let $\\rho$ be a distribution over $C$. There exists a mixed strategy $\\bar \\rho$ that forms an $(\\delta, \\epsilon, T)$-socially intelligent class, and which is equivalent to playing against partners sampled from $\\rho$ in expectation.\nProof. In a perfect recall game, every behavioural strategy has an equivalent mixed strategy and vice-versa [7]. Thus $\\rho$ can equivalently be defined as a distribution over mixed strategies so that $\\rho \\in \\Delta(\\Delta(N))$. Then defining $\\bar \\rho(a) = \\int_{\\Delta(N)} \\sigma(a) d\\rho(\\sigma)$ where $a \\in [N]$ denotes a pure strategy (i.e. action) completes the proof.\nIn order to show the joint impact of consistency and compatibility on the learning problem, we discuss the cases where the population is either consistent or compatible, but not both."}, {"title": "3.2 Consistency without Compatibility", "content": "Assume that $C$ consists of agents that are consistent but not necessarily compatible. The most general class in this case is the class of all no-external-regret learners (no-regret henceforth). It is a well-established result that the long-run average of no-regret learning converges to the set of coarse correlated equilibria. The question is whether the AI agent can learn to do better than a coarse correlated equilibrium when paired with a member of $C$, using only a dataset $D$ that consists of histories of play for different CCEs.\nTheorem 3.3. There exists a consistent yet incompatible class of agents $C$ such that even with an infinite amount of data, the AI cannot learn strategies that minimise altruistic regret.\nProof. The proof follows from the theorem 3 of Monnot and Piliouras [8] which shows that given any coarse correlated equilibrium of a two-player normal-form game, there exists a pair of no-regret learners that would converge to it. Since $C$ can be any subset of no-regret learners, we cannot exclude those who converge to inefficient CCE. If the class $C$ contains only the agents that converge to Pareto-inefficient CCE, we cannot hope to learn optimal strategies from any dataset. Given an observed CCE $z$ in the dataset, assume that the AI knows it is facing one of the two agents that generated $z$, but does not know their type explicitly. Using a Stackelberg argument similar to Brown et al. [9], we prove in appendix B.2 that the AI can compute and commit to a leader strategy such that the payoffs are never strongly Pareto-dominated by $z$. However even in this case, we cannot eliminate the possibility of it being weakly dominated.\nRegardless of the dataset, in the online phase, the AI faces a new agent from $C$ each time and does not know their type. We may hope to learn a classifier to quickly infer our partner's type online from their behaviour, assuming there exists a mapping from initial behaviour to types. However, since $C$ consists only of no-regret learners guaranteed to converge to a CCE in self-play, they have no reason to initially communicate their types to each other."}, {"title": "3.3 Compatibility without consistency", "content": "Assume that the members of $C$ are compatible but not consistent. We can construct such a class by using the coordination protocols example from section 2.3. Now, when agents from $C$ successfully identify each other after the authentication phase, they proceed with playing the agreed-upon PONE. However, if at any moment they play the wrong action, there is no constraint on what strategy they will switch to. This setting is equivalent to the case considered by Loftin and Oliehoek [10] in their impossibility result. The members of $C$ can employ grim-trigger strategies that forever punish the other agent, triggered by a mistake at any point. Even if we eliminate grim-trigger strategies, the impossibility result has proven that there still exists strategies the members of $C$ can play once triggered, and make the other agent suffer regret arbitrarily close to $\\frac{1}{2}$ with payoffs in $[0, 1]$. Since a single mistake during the online interaction can lead to partner playing strategies that yield linear regret, the outsider must learn to imitate at least one member of $C$ perfectly from the dataset. Therefore the offline problem in this setting reduces to imitation learning, in particular the no-interaction case from Rajaraman et al. [11].\nFor each agent, the authentication protocol k is equivalent to a history-dependent policy that they commit to playing in the first k time-steps. The lower-bound on the expected sub-optimality of the imitation learning from Rajaraman et al. [11] is based on the fact that the imitator cannot do better than uniformly random in unseen states. In the case of k, states correspond to histories up to length k. Since every k-step history can be uniquely embedding a type, an unseen history means a high probability of making a mistake if paired with the corresponding type. Therefore, to avoid linear altruistic regret, the AI must observe at least $|H_k|$ samples, where $H_k$ is the set of all possible k-step histories.\nTheorem 3.4. Let $M$ be the number of unique samples of k-step histories in the dataset. There exists a class of agents $C$ with a k-step social authentication protocol such that to bound the probability of failing to authenticate, we need $M > \\frac{N^{3k}-8N^{3k}-N^{2k}}{N-1}$ samples. Then for growing k, the sample complexity lower bound is $M = \\Omega(N^{2k})$.\nProof. Consider the coordination protocol example mentioned above. Let $h_k \\in H_k$ be missing from the dataset. When the AI is paired with the corresponding partner type, the probability of correctly authenticating is $\\frac{1}{N^k}$, and thus authentication fails with probability $1 - \\frac{1}{N^k}$. Assuming we face each"}, {"title": "3.4 Lower bound for socially intelligent populations", "content": "Theorem 3.5. Let $M$ denote the number of histories with unique first k-steps in dataset $D$ generated by the members of a socially intelligent class $C$. There exists a $C$ where $R^{alt}(h_T; \\theta_{-i}) = T$ with probability $\\frac{N^{2k}-M}{N^{2k}} > \\frac{1}{N} + \\frac{1}{M} - \\frac{1}{N^{3k}}$.\nProof: Let C be a socially intelligent class of agents following a coordination protocol akin to the one described in section 2.3. The probability follows from the proof of theorem 3.4 as the probability of failing to authenticate. If the authentication fails, the partner switches to an arbitrary Hannan-consistent strategy. As stated in section 3.2, a consistent partner strategy may never communicate the partner's type. Without knowing the partner's type, the agent's worst-case average altruistic regret can be 1, since it cannot compute its true regret without the partner's type (see definition 3.1). Let there be two partner types $\\theta_{-i} = \\theta_2$ or $\\theta_3$. If the agent i mistakenly assumes $\\theta_{-i} = \\theta_2$, its behaviour attempts to minimize $R^{alt}(h_T; \\theta_2) = \\sum_{t=1}^T G(\\sigma^*, \\sigma_i; \\theta_2) - G(a_t^i(h_t), a_t^{-i}(h_t); \\theta_2)$. Meanwhile, the play of the partner will be a no-regret algorithm with respect to the external regret $R^{ext}(h; \\theta_3)$. Having no other constraints in the type space, there is nothing stopping us from constructing a $\\Theta$ such that a strategy minimizing $R^{ext}(h; \\theta_3)$ ends up maximizing $R^{alt}(h_T; \\theta_2)$. Imagine the ideal case of $R^{ext}(h; \\theta_3) = 0$ where -i plays the fixed best action in hindsight $a^*$ throughout $h_T$. Then the altruistic regret observed by i is $R^{alt}(h_T; \\theta_2) = \\sum_{t=1}^T G(\\sigma^*, \\sigma_i; \\theta_2) - G(a_t^i(h_t), a^{-i} = a^*; \\theta_2)$. Let $G(a^i, a^*; \\theta_2) = 0$ for all $a^i$. Then the altruistic regret is $\\sum_{t=1}^T G(\\sigma^*, \\sigma_i; \\theta_2)$ which is T in the worst-case."}, {"title": "4 Upper bound for socially intelligent populations", "content": "A key idea behind this work is that against a socially intelligent target population, rather than trying to imitate a member of the population perfectly throughout the entire episode, the AI only needs to imitate them long enough to learn about its partner's private type. Once it has this information, the AI can leverage the fact that the partner's strategy is consistent against any strategy, and try to \"coerce\" the human partner into playing a strategy that minimizes the altruistic regret. We will refer to such strategies as imitate-then-commit (IC) strategies, which use the previous observations $D$ to learn an imitation strategy to follow over the first $\\tilde T < T$ steps of the interaction. In this section we provide an upper bound on the altruistic regret of a specific (IC) strategy, as a function of the number of episodes in $D$, subject to the following assumptions:\nAssumption 4.1. For $\\delta, \\epsilon > 0$, and some $\\tilde T < T$, we have that\n1. $\\rho$ is $(\\delta, \\epsilon, T)$-consistent.\n2. $\\rho$ is $(\\delta, \\epsilon, \\tilde T)$-compatible.\nImitation learning. Under an imitate-then-commit strategy, the sample complexity is defined en-tirely by the number of episodes the AI needs to observe to learn a good $\\tilde T$-step imitation policy. Fortunately, imitation learning is a well-studied problem, and we can largely leverage existing com-plexity bounds. The one caveat is that in this setting we need bounds on the total variation distance between the distribution over the partial history $h_{\\tilde T}$ under the population strategy $\\rho$, and that un-der the learned strategy. Given the dataset $D$, we define the imitation strategy $\\hat \\pi_{\\rho}(D)$ such that"}, {"title": "5 Related Work", "content": "Our work is closely related to the previous targeted learning model [1, 13, 14],which defines similar compatibility and consistency criteria. The notion of targeted optimality [15] include convergence to learning an approximately best response in a multi-agent model with high probability in a tractable number of steps against a population of memory-bounded adaptive agents. The main difference with our work is that targeted learning only requires consistency against a specific target class of partners, which generally would not include the agent itself, or other adaptive agents. We require"}, {"title": "6 Conclusion", "content": "We provide formal guarantees for successful and reliable cooperation of AI agents with populations of socially intelligent rational agents. This is based on the assumptions that 1) agents in the population are individually rational, and 2) agents in the population when cooperating with another agent in the same group can achieve, at least the same utility that they would with respect to some Pareto efficient equilibrium strategy. We formalize the notion of consistency and cooperative compatibil-ity of agents in two-player general-sum finitely-repeated bi-matrix games between the agents and the population with private type. Our theoretical guarantees are in the offline cooperation setting where the agent has to cooperate with unseen partners in the population to strategize against and manipulate no-regret policies for which we formalize the idea of altruistic regret. We prove that the assumptions on its own are insufficient to learn zero-shot cooperation with partners of the socially intelligent target population. We provide upper bounds on the sample complexity needed to learn a successful cooperation strategy along with lower bounds on when the multi-agent cooperation setting is needed with respect to the populations' trajectories, the state space and the length of the learning episodes. The bounds in these settings of the agent actively querying the MDP without knowing the transition dynamics of the population or the agent observing the populations' transition dynamics are much stronger than the bounds that can be derived by naively reducing the cooperation problem to one of reinforcement learning. These complexity analysis and formally proven bounds can be helpful to sustainably model the alignment problem of AI agents."}]}