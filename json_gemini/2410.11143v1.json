{"title": "LLM UNLEARNING VIA LOSS ADJUSTMENT\nWITH ONLY FORGET DATA", "authors": ["Yaxuan Wang", "Jiaheng Wei", "Chris Yuhao Liu", "Jinlong Pang", "Quan Liu", "Ankit Parag Shah", "Yujia Bao", "Yang Liu", "Wei Wei"], "abstract": "Unlearning in Large Language Models (LLMs) is essential for ensuring ethical and responsible AI\nuse, especially in addressing privacy leak, bias, safety, and evolving regulations. Existing approaches\nto LLM unlearning often rely on retain data or a reference LLM, yet they struggle to adequately\nbalance unlearning performance with overall model utility. This challenge arises because leveraging\nexplicit retain data or implicit knowledge of retain data from a reference LLM to fine-tune the model\ntends to blur the boundaries between the forgotten and retain data, as different queries often elicit\nsimilar responses. In this work, we propose eliminating the need to retain data or the reference LLM\nfor response calibration in LLM unlearning. Recognizing that directly applying gradient ascent\non the forget data often leads to optimization instability and poor performance, our method guides\nthe LLM on what not to respond to, and importantly, how to respond, based on the forget data.\nHence, we introduce Forget data only Loss AjustmenT (FLAT), a \"flat\" loss adjustment approach\nwhich addresses these issues by maximizing f-divergence between the available template answer\nand the forget answer only w.r.t. the forget data. The variational form of the defined f-divergence\ntheoretically provides a way of loss adjustment by assigning different importance weights for the\nlearning w.r.t. template responses and the forgetting of responses subject to unlearning. Empirical\nresults demonstrate that our approach not only achieves superior unlearning performance compared\nto existing methods but also minimizes the impact on the model's retained capabilities, ensuring\nhigh utility across diverse tasks, including copyrighted content unlearning on Harry Potter dataset\nand MUSE Benchmark, and entity unlearning on the TOFU dataset.", "sections": [{"title": "1 Introduction", "content": "The widespread integration of Large Language Models (LLMs) into daily applications has raised significant concerns\nregarding the trustworthiness of such models. Their outputs may contain sensitive, private, or illegal content [1, 2],\nreflect societal biases [3, 4], or provide harmful instructions [5, 6, 7]. In particular, for privacy concerns, regulations [8]\nhave been introduced, requiring applications to support the deletion of information contained in training samples upon\nuser request. This has motivated research into machine unlearning (MU) [9, 10, 11, 12, 13], a critical process aimed\nat removing the influence of specific data points, data classes, or even higher-level data concepts from trained models.\nLLM unlearning [14, 5, 13] is part of a broader set of MU techniques aiming to make the unlearned model forget\nthe knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the un-\nlearning target [13, 15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning\napproaches: input-based, data-based, and model-based methods. Input-based methods [16, 17] design input instruc-\ntions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based\nmethods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data"}, {"title": "2 Preliminaries", "content": "In this section, we introduce the preliminary formulation of LLM unlearning and the existing LLM unlearning frame-\nwork."}, {"title": "2.1 Formulation", "content": "Given a forget dataset $D_f$, a retain dataset $D_r$, and an LLM $\\theta_0$, the task of LLM unlearning is to fine-tune the original\nmodel such that the updated LLM $\\theta$ resembles a model trained without $D_f$."}, {"title": "2.2 Existing LLM Unlearning Paradigm", "content": "The mainstream class of existing LLM unlearning methods involves fine-tuning the original LLM against an unlearning\nobjective function. Although the exact designs vary, the general type of loss adjustment in LLM unlearning can be\ncharacterized as follows:\n$L = L_{FG} + L_{RT} + L_{Custom}.$\n(1)\nThe modified loss function comprises three main components:\n$\\bullet$ $L_{FG}$ (Forget Loss): Encourages the model to \"forget\" the undesired data or patterns. This typically involves increas-\ning the loss on the data to be forgotten, effectively making the model perform worse on those specific examples. The\ngoal is to reduce the model's reliance on these data points, thereby minimizing their influence on future predictions.\n$\\bullet$ $L_{RT}$ (Retain Loss): Ensures that the model maintains its overall performance and general knowledge on unaffected\ndata. It typically involves using the original loss function from training or a modified version that focuses on the\ndata the model is meant to retain. This term prevents the unlearning process from degrading the model's overall\ncapabilities beyond the scope of the specific unlearning objective.\n$\\bullet$ $L_{Custom}$ (Custom Loss): Allows for additional flexibility and customization in the unlearning process. It may\ninclude regularization terms to control the magnitude of parameter updates or specific constraints to enforce certain\nunlearning behaviors. This component enables researchers to tailor the unlearning process to specific requirements\nor incorporate domain-specific knowledge.\nIn summary, common loss adjustment methods employ one [24], two [23, 20, 22], or all three [5] of these components\nto guide the model towards forgetting specific data while minimizing the impact on its overall performance and utility.\nThe interplay between these terms allows for controlled and targeted unlearning, ensuring the model retains its valuable\ncapabilities while selectively forgetting undesired information. More detailed formulations of these loss adjustment-\nbased methods, along with related work, are deferred to Appendix C.1 and Appendix E."}, {"title": "An Example: Large Language Model Unlearning (LLMU)", "content": "We adopt a popular approach in LLM unlearning,\nLLMU [5], to interpret a special case of Eqn. (1). Specifically, the objective of LLMU contains three components:\nthe Unlearn Harm $L_{FG}$, the Maintain Performance $L_{RT}$, and the Random Mismatch $L_{Random}$ (the custom loss). The\ntraining objective is as follows:\n$L_{LLMU} = L_{FG} + L_{RT} + L_{Random},$\nThe forget loss $L_{FG} = -\\sum_{(x_f,y_f)\\in D_f} L(x_f, y_f; \\theta)$, where $(x_f, y_f)$ indicates the forget data pairs from the forget\ndataset $D_f$, $\\theta$ is the updated unlearned model. It is actually the Gradient Ascent loss to forget the samples subject\nto unlearning. The retain loss $L_{RT} = \\sum_{(x_r,y_r)\\in D_r} \\bigcup KL(h_{\\theta_0}(x_r, y_{r < i})||h_{\\theta}(x_r, y_{r < i}))$, where $KL(\\cdot)$ is the KL\ndivergence term, $(x_r, y_r)$ indicates the retain data pairs from the retain dataset $D_r$, $\\theta_0$ is the original model, and $\\theta$ is\nthe updated model. The random loss $L_{Random} = \\sum_{(x_f, y_f)\\in D_f} \\sum_{yrdn \\in Y_{rdn}} L(x_f, Y_{rdn}; \\theta)$. Here, $Y_{rdn}$ is a set of\nrandom responses that do not have a connection to the forget prompts $x_f$."}, {"title": "3 Method", "content": "In this section, we introduce Forget data only Loss AjustmenT (FLAT), a \"flat\" loss adjustment approach which\nadjusts the loss function using only the forget data, by leveraging f-divergence maximization towards the distance\nbetween the preferred template and original forget responses. We first derive the formulation of our method via f-\ndivergence maximization ($\\S$ 3.1), followed by the presentation of the empirical alternative to our approach ($\\S$ 3.2). In\nsection $\\S$ 3.3, we explore the estimation gap between the theoretical and empirical f-divergence. Finally, we discuss\nthe connection between our method and DPO ($\\S$ 3.4)."}, {"title": "3.1 Loss-Adjustments via f-divergence Maximization", "content": "For each learning batch, we assume that we only have access to a set of forget samples $(x_f, y_f) \\in D_f$. Instead of\ndirectly adopting gradient ascent over these forget samples, we propose to maximize the divergence between exemplary\nand bad generations of forget data. Key steps are summarized as below.\n$\\bullet$ Step 1: Equip example/template responses $y_e$ for each forget sample $x_f$. Together we denote the paired samples as\n$D_e = \\{(x, y)\\}_{j\\in[N]}$.\nThis could be done by leveraging open-source LLMs such as Llama 3.1 [25] or self-defining the responses according\nto our wish, etc. The designated unlearning response could be a reject-based answer such as \"I don't know\" (denoted\nas \"IDK\") or an irrelevant answer devoid of the unlearning target-related information.\nMotivation: Step 1 generates example responses for LLM fine-tuning and provides better instructions on what\nLLM should respond given the forget data. Besides, certain existing methods make LLM generate hallucinated\nresponses after unlearning, which further illustrates the importance of example responses for LLM unlearning.\n$\\bullet$ Step 2: Loss adjustmens w.r.t. the sample pairs $(x_f, y_e, y_f)$ through:\n$L(x_f, y_e, y_f; \\theta) = \\lambda_e \\cdot L_e(x_f, y_e; \\theta) - \\lambda_f \\cdot L_f(x_f, y_f; \\theta),$\n(2)\nwhere $L_e$, $L_f$ are losses designed for the data sample $(x_f, y_e)$ and $(x_f, y_f)$, respectively. The corresponding closed\nform will be introduced in Section $\\S$ 3.2.\nMotivation: Step 2 encourages the LLM to forget the forget data with bad responses, meanwhile, learn to generate\ngood responses on relevant forget data. [such as template answers]\n$\\bullet$ Step 3: How to decide on the values of $\\lambda_e$ and $\\lambda_f$?\nWe leverage f-divergence to illustrate the appropriate balancing between $L_e(x_f, y_e; \\theta)$ and $L_f(x_f, y_f; \\theta)$. Assume\n$X_f, Y_e$ is generated by the random variable $X_f, Y_e$ jointly following the distribution $D_e$. Similarly, $x_f, y_f$ is given\nby $X_f, Y_f$ and $(X_f,Y_f) \\sim D_f$. Step 2 shares similar insights as if we are maximizing the divergence between\n$D_e$ and $D_f$. Our theoretical purpose is to obtain the model that maximizes the f-divergence between $D_e$ and $D_f$,\ndefined as $f_{div}(D_e||D_f)$.\nThe variational form f-divergence Instead of optimizing the $f_{div}$ term directly, we resolve to the variational\nform of it. Due to the Fenchel duality, we would have:\n$f_{div}(D_e||D_f) = \\sup_{g} [E_{z_e \\sim D_e} [g(Z_e)] - E_{z_f \\sim D_f} [f^*(g(Z_f))]] := \\sup_{g} VA(\\theta, g),$\n(3)\nwe define $f^*$ as the conjugate function of the f-divergence function. For simplicity, we define $VA(\\theta, g^*) :=\n\\sup_{g} VA(\\theta, g)$, where $g^*$ is the optimal variational function.\nHence, the objective of FLAT is to obtain: $\\theta^* := \\arg \\max_{\\theta} VA(\\theta, g^*)$.\nMotivation: Note that existing solutions fail to keep a good balance between model performance on forget data and\nretain data, step 3 provides a formal theoretical framework of our loss revision in Step 2, under the f-divergence\nmaximization between $D_e$ and $D_f$, the method assigns the appropriate weights $f^*(\\cdot)$, $g^*(f^*(\\cdot))$ w.r.t. the joint data\ndistributions $D_e$, $D_f$."}, {"title": "3.2 Empirical Alternative of Loss Adjustment", "content": "Note that Eqn. (3) could be viewed as a data distribution level loss adjustment, in practice, when given access to a set\nof forget data as well as example and bad answers, $x_f, y_e, y_f$, the per-sample loss function (closed form of Eqn. (2))"}, {"title": "3.3 The upper bound of the estimation gap", "content": "To connect the empirical alternative of FLAT with the corresponding theoretical format, in this section, we aim to\nexplore the estimation gap between the theoretical f-divergence $f_{div}(D_e||D_f)$ and the empirical optimal estimated\nf-divergence $\\hat{f_{div}}(D_e||D_f)$, here we define:\n$\\hat{f_{div}}(D_e||D_f) := E_{z_e \\sim \\hat{D_e}}[\\hat{g}(Z_e)] - E_{z_f \\sim \\hat{D_f}}[f^*(\\hat{g}(Z_f))]$,\nwhere $\\hat{g} := \\sup_{g\\in \\Phi} E_{z_e \\sim \\hat{D_e}} [g(Z_e)] - E_{z_f \\sim \\hat{D_f}}[f^*(g(Z_f))]$ and $\\Phi$ is the function space.\nAssumption 3.1 (Bounded Density Ratio). The density ratio $Z_e/Z_f$ is lower and upper bounded by positive constants\na and b, respectively.\nAssumption 3.1 is wildely adopted by the literature [28, 29], which necessitates that the probability density functions\n$Z_e$, $Z_f$ share the same support.\nAssumption 3.2 (Regularity of Divergence Function). f(\u00b7) is smooth on [a, b], and f(1) = 0. f is $\\mu_0$-strongly convex,\nand has $L_0$-Lipschitz continuous gradient on [a, b], for positive constants $\\mu_0$, $L_0$.\nAssumption 3.2 is a mild condition since it only requires the condition to hold for the interval [a, b], which works for\nmany commonly used f-divergence functions, i.e., KL divergence.\nLet (V, $|| \\cdot ||_{L_2}$) be a normed space, and $\\Phi \\subset V$. $V_1, ..., V_c$ is a $\\delta$-covering over $\\Phi$ of size C if $\\Phi \\subset \\cup_{i=1}^C B(v_i, \\delta)$\nwhere $B(v_i, \\delta)$ is the $\\delta$-ball centered at $v_i$. The covering number is then defined as $C_2 (\\delta, \\Phi) = min\\{C:\n$\\delta$-covering over $\\Phi$ of size $C\\}$. The following assumption would characterize the representation power of the func-\ntion space $\\Phi$.\nAssumption 3.3 (Order of Covering Number). $C_2(\\delta, \\Phi) = O(exp\\{\\delta^{-r_\\Phi}\\})$ , and $r_\\Phi \\in (0,2)$.\nTheorem 3.4. Given Assumptions 3.1-3.3, suppose $\\hat{g} \\in \\Phi$, with probability $> 1 - e^{-Nr_\\Phi/(2+r_\\Phi)/c_1}$, we have:\n$|f_{div}(D_e||D_f) - \\hat{f_{div}}(D_e||D_f)| \\leq N^{\\frac{-1}{r_\\Phi+2}}$, where we have defined N as the number of samples in the forget data.\nTheorem 3.4 illustrates that the empirical alternative of FLAT, $\\hat{f_{div}}(D_e||D_f)$, achieves the optimal non-parametric\nrate of convergence towards $f_{div}(D_e||D_f)$."}, {"title": "3.4 Connection with DPO", "content": "In this section, we discuss the connection and key differences between our approach and the celebrated Direct Prefer-\nence Optimization (DPO) [21] approach for aligning LLMs.\nGiven a dataset $D = \\{(x^j_f, y^j_e, y^j_f)\\}_{j \\in [N]}$, where $y_e$ and $y_f$ are preferred template and original forget responses to\nthe forget prompt $x_f$, DPO [21] fine-tunes original model $\\theta_0$ using $D$ to better align it with good answer preferences,\nwhich minimizes:\n$\\mathcal{L}_{DPO,\\beta}(\\theta) = - \\frac{1}{2\\beta} \\mathbb{E}_D \\Big[ log\\sigma \\Big( \\beta \\Big(log \\frac{\\pi_{\\theta}(y_e | x_f)}{\\pi_{ref}(y_e | x_f)} - log \\frac{\\pi_{\\theta}(y_f | x_f)}{\\pi_{ref}(y_f | x_f)} \\Big) \\Big) \\Big]$\n$= - \\frac{1}{2\\beta} \\mathbb{E}_D \\Big[ log\\sigma \\Big( \\beta \\Big( \\sum_{i=1}^{|y_e|} log \\pi_{\\theta}(x_f, y_{e,<i}) - \\sum_{i=1}^{|y_f|} log \\pi_{\\theta}(x_f, y_{f,<i}) \\Big) - M_{ref} \\Big) \\Big]$\nwhere, $\\sigma(t) = \\frac{e^t}{1 + e^{-t}}$ is the sigmoid function, $\\beta > 0$ is the inverse temperature, $\\pi_{\\theta} := \\Pi_{i=1} h_{\\theta}(x,y_{<i})$ is the\npredicted probability of the response y to prompt x given by LLM $\\theta$, $\\pi_{ref}$ is the predicted probability given by\nreference model, and $M_{ref} := \\beta \\Big( \\sum_{i=1}^{|y_e|} log h_{\\theta_0}(x_f, y_{e,i}) - \\sum_{i=1}^{|y_f|} h_{\\theta_0}(x_f, y_{f,i}) \\Big)$.\nAs for FLAT, we calculate the average probability of all correctly generated tokens and employ a novel re-weighting\nmechanism that assigns different importance to each term using distinct activate functions for both the example and\nforget loss terms, which minimizes:\n$\\mathcal{L}_{FLAT} (\\theta) = - \\mathbb{E}_D \\Big[ g^* \\Big(\\frac{1}{|y_e|} \\sum_{i=1}^{|y_e|} h_{\\theta} (x_f, y_{e,<i}) \\Big) - f^* \\Big(g^* \\Big(\\frac{1}{|y_f|} \\sum_{i=1}^{|y_f|} h_{\\theta} (x_f, y_{f,<i}) \\Big) \\Big]$\nHere, $f^*(\\cdot)$, $g^*(f^*(\\cdot))$ are the activate functions that assign appropriate weights to each loss term. The detailed\nderivation is in Appendix B.2. The key differences are highlighted in red. Specifically, DPO relies on a reference\nmodel to guide the unlearning process, whereas FLAT only uses a sample pair dataset containing both exemplar and\nforget responses. Besides, our solution differs from DPO in three critical aspects: the re-weighting activation function,\nwhether to sum or average the token losses, and whether to apply the logarithm to the output probability. We conduct\nan ablation study with DPO to evaluate the effectiveness of the proposed re-weighting mechanism in Section $\\S$ 4.5."}, {"title": "4 Experiment", "content": "In this section, we compare the proposed method with baseline unlearning methods on three widely used LLM unlearn-\ning tasks: copyrighted content unlearning on Harry Potter (HP) Series Book [5] ($\\S$ 4.2), entity unlearning on TOFU\ndataset [20] ($\\S$ 4.3), and unlearning on MUSE-News benchmark [30] ($\\S$ 4.4). We conduct additional ablation studies\nto assess the effectiveness of our methods in Section $\\S$ 4.5."}, {"title": "4.1 Baseline Methods", "content": "We evaluate the effectiveness of our proposed method FLAT by comparing it to a series of strong LLM unlearning\nbaselines, particularly those based on loss adjustment. We consider Gradient Ascent (GA) [24, 5], KL minimization\n(KL) [20], GradDiff (GD) [23], and NPO [22] across all three tasks. For copyrighted content and entity unlearning,\nwe also include Preference Optimization (PO) [20], Large Language Model Unlearning (LLMU) [5], and DPO [21].\nFor the Harry Potter dataset, we add Mismatch [16] as a simple and effective baseline method. For the MUSE-News\nbenchmark, we additionally consider Task Vectors [31], Who's Harry Potter (WHP) [14] and an extended version\nof NPO (NPO-RT) as a comparable method, which incorporates a fine-tuning term on the retain dataset. Further\nexperiment details are provided in Appendix C.1."}, {"title": "4.2 Copyrighted Content Unlearning", "content": "Experiment Setup. We select Harry Potter and the Sorcerer's Stone [32] as the copyrighted content for unlearning.\nThe objective is to ensure that the unlearned model does not generate passages with high similarity to the original text.\nFollowing prior works [16, 5], we first fine-tune LLMs on the corresponding corpus, treating it as the model subject\nto unlearning, while using the original pre-trained checkpoint as the retained model. Following [5, 33], We extract"}, {"title": "4.3 Entity Unlearning", "content": "Experiment Setup. The TOFU dataset [20] is a synthetic question-answering dataset focused on author biographies,\naiming to enable a LLM to unlearn a portion of fictitious authors while retaining knowledge about the rest and real-\nworld facts. The dataset includes 200 fake authors, each with 20 QA pairs, and experiments are conducted with 1% of\nthese authors marked for unlearning. We use Llama2-7B [36], Phi-1.5B [40], and OPT-2.7B [35] as base LLM."}, {"title": "4.4 MUSE-News Unlearning", "content": "Experiment Setup. We focus on the task of unlearning on News corpus presented in [30]. News consists of BBC\nnews articles [41] collected after August 2023. All articles are randomly divided into forget, retain, and holdout sets.\nWe perform unlearning directly on the pre-trained models provided by the benchmark, following the corresponding\nexperimental setup.\nEvaluation Metrics. We report the proposed four metrics, VerbMem on forget dataset, KnowMem on forget and\nretain dataset, and Privacy leakage (PrivLeak). We quantify the verbatim memorization VerbMem by prompting the\nmodel with the first 1 tokens from a sequence and comparing the continuation outputted by the model $\\theta$ to the true\ncontinuation using the ROUGE-L F1 score [38]. We gather the model's answers to questions and then average the\nROUGE scores for all question-answer pairs in forget dataset or retain dataset to compute the knowledge memorization"}, {"title": "4.5 Ablation Studies", "content": "The Effectiveness of Re-weighting Mechanism. As FLAT (KL) demonstrates strong overall performance, we base\nour ablation study on KL divergence to explore the effectiveness of the implicit re-weighting mechanism within our\nloss adjustment. This study is conducted on the TOFU dataset using Llama2-7B. For the ablation study on the HP\ndataset, please refer to Table 13 in Appendix D.3.\nWhen using preferred template data for unlearning, we compare our method with DPO (without the term $M_{ref}$) and\nSimPO, as outlined in Appendix C.1. All methods use the same data and have similar formulations, with two terms in\nthe loss function; the only difference lies in the intrinsic re-weighting mechanism. As shown in Table 6, our method\nachieves the highest number of best results across 12 metrics. When replacing the IDK data with retain data, the results\nshow that the retain version performs better on the Retain Set but worse on Real Authors and Real World compared to\nFLAT (KL).\nSince GD shares the same data usage and formulation as our method, except for the re-weighting mechanism and\nutilization of retain data, we compare the retain version to GD. The results show that our method achieves better\nperformance on both the Retain Set and Forget Set, with the decline in Real Authors and Real World performance\ncaused by the use of retain data.\nThe Imapct of Good Answer Type. In the first step of our approach, we intend to generate good example responses\nfor each forget sample. We primarily use the reject-based response \"I don't know\" (denoted as IDK) as the default\nchoice. In this section, we conduct an ablation study on data usage for FLAT to analyze how these good responses\nimpact unlearning performance. Table 7 presents the ablation study of good answer type using three LLMs on TOFU."}, {"title": "5 Conclusion", "content": "In this paper, we address the limitations of existing LLM unlearning methods, which often rely on the retain data\nor a reference LLM for response calibration. To overcome these challenges, we propose FLAT (Forget data only\nLoss AdjustmenT), a \"flat\" loss adjustment approach that eliminates the need for retain data or a reference model.\nBy optimizing the f-divergence between the template and forget responses, FLAT offers a clear and theoretically\ngrounded solution for balancing forget quality with model utility in LLM unlearning. Through extensive experiments\non three key unlearning tasks: copyrighted content unlearning on the Harry Potter dataset, the MUSE benchmark, and\nentity unlearning on the TOFU dataset, we demonstrate the superior performance of FLAT. Our method consistently\nachieves high unlearning efficiency while preserving overall model utility, showcasing its effectiveness in addressing\nboth practical and theoretical challenges in LLM unlearning."}, {"title": "A Limitations and Broader Impacts", "content": null}, {"title": "A.1 Broad Impacts", "content": "The proposed FLAT method for LLM unlearning has the potential to significantly advance ethical and responsible AI\ndeployment, particularly in addressing key challenges such as privacy concerns, bias, and regulatory compliance. By\nenabling models to effectively forget specific data without compromising overall model utility, this approach directly\naddresses issues related to data privacy, including compliance with regulations like GDPR, which mandates data\ndeletion upon user request. The ability to unlearn sensitive or copyrighted information, as demonstrated on datasets\nlike Harry Potter and TOFU, ensures that AI models can be continually refined without propagating harmful or biased\ncontent.\nFurthermore, the reduced reliance on the retain data or a reference LLM makes FLAT more resource-efficient, low-\nwering the computational and financial costs associated with large-scale unlearning. This opens up opportunities for\nwider adoption across industries and research institutions where access to retain data or additional model resources\nmay be limited or not accessible. The implications of this work span multiple domains, including healthcare, finance,\nand education, where ethical considerations are paramount."}, {"title": "A.2 Limitations", "content": "One key limitation of our approach is the unsatisfactory performance in the privacy leakage evaluation on the MUSE\ndataset. While FLAT demonstrates strong unlearning efficiency and retains model utility across several benchmarks,\nit struggles to prevent privacy leakage. Note that all other tested methods suffer from the same issue, this suggests\nthat further refinement is needed to strengthen the privacy-preserving aspects of existing LLM unlearning approaches.\nFuture work could explore more robust strategies to address privacy leakage while maintaining the balance between\nunlearning performance and model utility."}, {"title": "B Theoretical Illustration and Proofs", "content": null}, {"title": "B.1 Additional Examples for Loss Adjustments under More f-divergence", "content": "Example 2: Jenson-Shannon (JS) For Jenson-Shannon f-divergence, we have $f^*(u) = - log (2 - e^u)$, $g^*(v) =\\frac{2}{log \\frac{2}{1 + e^{-v}}}$ hence:\n$g^*((P(x_f, y_e; \\theta)) - f^* (g^* ((P(x_f, y_f; \\theta)) = log \\frac{2}{log \\frac{2}{1 + e^{-P(x_f, y_f; \\theta)}}} - log \\frac{2}{log \\frac{2}{1 + e^{-P(x_f, y_f; \\theta)}}} \\\\= log \\frac{2}{1 + e^{-P(x_f, y_e; \\theta)}} - log \\frac{1}{2} log(2 - e^u) =  log \\frac{2}{1 + e^{-P(x_f, y_f; \\theta)}}\\\\\\\\= log \\frac{2}{1 + e^{-P(x_f, y_e; \\theta)}} - log \\frac{1}{2} log(2 - e^u) = log \\frac{2}{1 + e^{-P(x_f, y_f; \\theta)}} - log 2+  log e^u = \\\\\\log (2- e^{-u}))$\nExample 3: Pearson For Pearson f-divergence, we have $f^*(u) = \\frac{u + u}{4}$ , $g^*(v) = v$ , hence:\n$g^* (P(x_f, y_e; \\theta)) - f^* (g^* ((P(x_f, y_f; \\theta)) = P(x_f, y_e; \\theta) -  \\frac{P(x_f, y_f; \\theta)^2 + P(x_f, y_f; \\theta)}{4}$ \\\\\\(P(x_f, y_f;\\theta))\\\\= \\frac{P(x_f, y_f;\\theta)) - P(x_f, y_f; \\theta) + P(x_f, y_e; \\theta)}{4}.$\nExample 4: KL For KL f-divergence, we have $f^*(u) = e^{u -1}$ , $g^*(v) = v$, hence:\n$g^*((P(x_f, y_e; \\theta)) - f^* (g^* ((P(x_f, y_f; \\theta)) = P(x_f, y_e; \\theta) - e^{P(x_f, y_f; \\theta) -1}$"}, {"title": "B.2 The derivation of empirical loss function", "content": "According to Eqn. (4)", "have": "n$L(x_f", "theta))": "f^*(g^*(P(x_f", "Big": "frac{L_e(x_f", "N": ""}, "where $y_e$ and $y_f$ are preferred template and original forget responses to the\nforget prompt $x_f$, we estimate $P(x_f, y_e; \\theta)$, $P(x_f, y_f; \\theta)$ via the following two quantities:\n$\\hat{P(x_f, y_e; \\theta)} := \\frac{\\sum_{i=1}^{|y_e|} P(h_{\\theta}(x_f, y_{e,<i}) = y_{e,i})}{|y_e|},$\n$\\hat{P(x_f, y_f; \\theta)} := \\frac{\\sum_{i=1}^{|y_f|}  P(h_{\\theta}(x_f, y_{f,<i}) = y_{f,i})}{|y_f|}.$\nEmpirically, we can obtain the loss function for the dataset D:\n$\\mathcal{L}_{FLAT} (\\theta) = - \\mathbb{E}_D [g^* (\\hat{P(x_f, y_e; \\theta)})) - f^*(g^*(\\hat{P(x_f, y_f; \\theta)}))"]}