{"title": "Flatten: Video Action Recognition is an Image Classification task", "authors": ["Junlin Chen", "Chengcheng Xu", "Yangfan Xu", "Jian Yang", "Jun Li", "Zhiping Shi"], "abstract": "In recent years, video action recognition, as a fundamental task in the field of video understanding, has been deeply explored by numerous researchers. Most traditional video action recognition methods typically involve converting videos into three-dimensional data that encapsulates both spatial and temporal information, subsequently leveraging prevalent image understanding models to model and analyze these data. However, these methods have significant drawbacks. Firstly, when delving into video action recognition tasks, image understanding models often need to be adapted accordingly in terms of model architecture and preprocessing for these spatiotemporal tasks; Secondly, dealing with high-dimensional data often poses greater challenges and incurs higher time costs compared to its lower-dimensional counterparts. To bridge the gap between image-understanding and video-understanding tasks while simplifying the complexity of video comprehension, we introduce a novel video representation architecture, Flatten, which serves as a plug-and-play module that can be seamlessly integrated into any image-understanding network for efficient and effective 3D temporal data modeling. Specifically, by applying specific flattening operations (e.g., row-major transform), 3D spatiotemporal data is transformed into 2D spatial information, and then ordinary image understanding models are used to capture temporal dynamic and spatial semantic information, which in turn accomplishes effective and efficient video action recognition. Extensive experiments on commonly used datasets (Kinetics-400, Something-Something v2, and HMDB-51) and three classical image classification models (Uniformer, SwinV2, and ResNet), have demonstrated that embedding Flatten provides a significant performance improvements over original model.", "sections": [{"title": "Introduction", "content": "In computer vision, for a long period, video modeling methods based on three-dimensional data have dominated the field of video action recognition. These methods can be broadly classified into three categories: a). Methods based on 3D convolutional backbone networks, such as C3D (Tran et al. 2015), I3D (Carreira and Zisserman 2017), R(2+1)D (Tran et al. 2018), X3D (Feichtenhofer 2020), etc. b)Methods based on temporal fusion with 2D convolutions, such as Two-Stream Networks (Simonyan and Zisserman 2014), TSN (Wang et al. 2016), TDN (Wang et al. 2021), SlowFast (Feichtenhofer et al. 2019), etc. c). Methods based on transformer backbone networks, such as TimeS-former (Bertasius, Wang, and Torresani 2021), Vivit (Arnab et al. 2021), Video swin transformer (Liu et al. 2022b), etc.\nMethods based on 3D convolutional backbone networks utilize 3D convolutional kernels to model the dependencies between adjacent spatial and temporal regions. By stacking a sufficient number of network layers, they achieve global dependency modeling of video information. Methods based on temporal fusion with 2D convolutions decompose video tasks into short-term and long-term temporal modeling, and then fuse the short-term and long-term information to achieve comprehensive modeling of video information. Thanks to the transformer (Vaswani et al. 2017) architecture's superior long-sequence modeling capabilities, sparking significant interest among researchers in exploring the use of transformers for spatiotemporal modeling in videos. They have investigated various approaches (Tong et al. 2022; Wang et al. 2024b) to balance the effectiveness and efficiency of video spatiotemporal modeling, utilized additional data for joint training to enhance modeling performance, and constructed large models for video-understanding.\nAlthough these three types of methods employ different concepts for video sequence modeling, they also share some common aspects. Firstly, these methods treat video action recognition as a three-dimensional data classification problem, attempting to infer video actions from the spatiotemporal relationships conveyed by the video. Secondly, the original Neural network model and pre-trained models for these methods are derived from image classification tasks. They adapt data preprocessing methods and model structures to fit the characteristics of video action recognition tasks, enabling image models to be used for video-understanding tasks. These characteristics also reflect certain drawbacks of the aforementioned methods. Since the original Neural network models are designed for two-dimensional image modeling, they typically require corresponding modifications for three-dimensional video modeling. This adds to the workload and, to some extent, separates image tasks from video tasks. Additionally, modeling relationships between high-dimensional data is more challenging compared to modeling relationships between low-dimensional data.\nA better understanding of the intrinsic logic of action recognition is the premise for effectively solving the problem of video action recognition, i.e., how to distinguish one action from another through video. From an empirical perspective, two factors typically influence action recognition. One factor is the static scene information in the frame. For instance, different behaviors of individuals in a car can be distinguished based on whether a person is holding the steering wheel. The behavior of a person holding the steering wheel is defined as driving, while the behavior of a person not holding the steering wheel is defined as riding. Another factor is the relative motion or position change in the frames, essentially a dynamic relative change. For example, determining whether a person on a playground is walking or running can be done by observing the amplitude and speed of their body movements, reflecting relative motion and position changes. For the first scenario, to achieve video action recognition, modeling the static images of the video is sufficient. For example, in UCF101 (Soomro, Zamir, and Shah 2012), action recognition accuracy can exceed 80% by solely recognizing images. For the second scenario, past practices in tasks such as object detection (Carion et al. 2020; Wang et al. 2024a) and semantic segmentation (Cao et al. 2022) have proven that deep learning models can recognize different objects in different spatial locations through various features. This also suggests a possibility that deep learning models can differentiate between the actions of the same object in images and establish relationships between different actions and between the subject and the environment. This phenomenon is akin to how humans understand actions in a comic; the model can comprehend action information through images, as shown in figure 1.\nBased on the aforementioned review of past research and the analysis of the video action recognition task, we propose Flatten. Flatten transforms the video action recognition task into an image classification task, which to some extent, blurs the boundary between image-understanding tasks and video-understanding tasks, thereby reducing the complexity of video action recognition. In summary, our contributions are as follows:\n\u2022 To the best of our knowledge, we are the first to propose the Flatten method, a plug-and-play parameter-free transform that transforms spatiotemporal modeling into spatial modeling, blurs the boundaries between image and video understanding tasks, and provides a novel and efficient solution to address video action recognition.\n\u2022 We construct three different instances of Flatten and demonstrate through comparative experiments on these three instances that image-understanding models can learn data mapping relationships to achieve the function of modeling temporal information in the spatial dimension.\n\u2022 Extensive experiments conducted on classical CNN, Transformer, and hybrid CNN-Transformer models for image classification have consistently demonstrated that incorporating the Flatten transformation leads to significant performance gains. In particular, \u201cUniformer(2D)-S + Flatten\" achieves the best recognition performance of 81.1 on the Kinetics400 dataset when compared to other state-of-the-art methods with the same amount of computation."}, {"title": "Related Work", "content": "Temporal action information is particularly important in the field of video-understanding. Numerous works revolve around capturing spatiotemporal information in videos, with researchers attempting to model this information to establish robust video-understanding capabilities. The Two-Stream Network (Simonyan and Zisserman 2014) and its variants (Feichtenhofer, Pinz, and Zisserman 2016) enhance the network's video-understanding ability by utilizing optical flow information between adjacent frames to enrich spatiotemporal information. However, a consequent issue is that obtaining optical flow information is very costly. Some works use 3D convolutions to model video information, capturing spatiotemporal information by stacking network depth. This approach started with C3D (Tran et al. 2015), and subsequent researchers proposed methods like I3D (Carreira and Zisserman 2017) and R2+1D (Tran et al. 2018) to achieve faster convergence and lower computational complexity for 3D convolution-based methods. With the excellent capabilities of Transformers (Vaswani et al. 2017) in temporal modeling, multimodal fusion, and large-scale models, numerous works have begun exploring the application of Transformers in the video domain. Methods such as TimeSformer (Bertasius, Wang, and Torresani 2021), Vivit (Arnab et al. 2021), Video swin transformer (Liu et al. 2022b), Video MAE (Tong et al. 2022), and Intern-Video (Wang et al. 2024b) have emerged. Despite the differences among the aforementioned methods, they also share"}, {"title": "Method", "content": "In this section, we first present the definition of the Flatten method, followed by providing several examples of different types of Flatten.\nAs described above, the starting point of Flatten is to transform the video action recognition task from a three-dimensional data modeling task into a two-dimensional data modeling task, thereby blurring the boundary between image-understanding and video-understanding. Following the starting point of Flatten, we define a general Flatten operation in deep neural networks as follows:\nFormulation\n$Y = g(f(x, y, z)).$ (1)\nIn the equation 1, $g(x, y)$ represents a general image-understanding neural network, and $f(x,y,z)$ denotes a transformation rule. Where x, y, and z represent the width, height, and number of frames of a sequence of images, respectively. Through this transformation, the video data represented by a set of images $I \\in R^{x \\times y \\times z}$ can be converted into video data represented by a single image $I' \\in R^{x' \\times y'}$. In this description, we temporarily disregard the channel information C of the images and focus more on the spatial representation of temporal information Z in the video data, i.e., $(x, y, z) \\rightarrow (x', y')$. We aim to construct a mapping between three-dimensional and two-dimensional data through $f(x, y, z)$, enabling the deep learning model to learn this mapping relationship based on image-understanding, thereby achieving temporal modeling in the spatial dimension.\nThe work most similar to the Flatten method is the \"Uniform frame sampling\" method in Vivit (Arnab et al. 2021). This method encodes each frame in a set of images containing video information into tokens using the VIT (Dosovitskiy et al. 2020) method and then concatenates all the tokens from the images to represent the entire video segment. After this, an image-understanding model like VIT can be used for video-understanding. However, this method has two problems: first, its applicability is limited and can only be used with specific neural networks, such as Transformer-like networks. Second, due to the large amount of redundant information present in videos and the limited range of models this method can be applied to, it is very costly to use this approach.\nSince the Flatten method does not alter the model structure, it is very flexible and can be easily applied to various image-understanding models, including but not limited to ResNet, Swin-Transformer, and Uniformer. Relying on a significant amount of image-understanding work(Swin, Uniformer), it does not face the exponential growth in computational complexity that the \"Uniform frame sampling\" method in Vivit encounters. Therefore, we can use the Flatten operation to easily construct an image-understanding model to comprehend temporal information in videos, thereby achieving accurate video action recognition.\nNext, we describe several types of $f(x, y, z)$. Interestingly, our experimental results show that the model is not sensitive to several fixed transformation rules of $f(x, y, z)$, but it is sensitive to unfixed transformation rules of $f(x, y, z)$. This further indicates that the model learns this mapping relationship $f(x, y, z)$, thereby achieving temporal information modeling in the spatial dimension.\nInstantiations\nRow-major transformation. The most empirically intuitive data transformation method is row-major ordering. As shown in Figure 2 (a). This transformation method is akin to converting a video into a comic strip, i.e., unfolding the data along the row direction through a transformation function f. In this paper, we define f as follows:\n$f(x_i, y_j, z_k) \\begin{cases} x'_i = x_i + l_k \\times w \\\\ y'_j = y_j + c_k \\times h \\end{cases}$ (2)\n$l_k = \\lfloor z_k/m \\rfloor,$ (3)\n$c_k = z_k\\% m.$ (4)\nHere, $(x_i, y_j, z_k)$ represents the video information represented by a set of images before the transformation, and $(x', y')$ represents the video information represented by a single image after the transformation. In Equation 3 and Equation 4, m denotes that the width and height of the transformed image $I' \\in R^{x' \\times y'}$ are composed of m frames of the original image $i \\in R^{x \\times y}$, and $m \\times m$ equals the total number of frames in this set of images $I \\in R^{x \\times y \\times z}$. This transformation is illustrated in Figure 2 (a).\nNested transformation. Inspired by methods such as SlowFast (Fan et al. 2020) and TDN (Wang et al. 2021), we attempt to consider the balance between short-term and long-term temporal relationships during the data transformation process, leading to the development of the second data transformation method. In this paper, it is represented by Equation 9:\n$I = \\{I_1, I_2, I_3, ...I_k\\},$ (5)\n$q = K/N,$ (6)\n$i'_n = f (I_n),$ (7)\n$I' = \\{i'_1, i'_2, i'_3, ...i'_n\\},$ (8)\n$I'' = f(I'),$ (9)\n$I_n = \\{i_{(n-1)\\times q+1},i_{(n-1)\\times q+2},i_{(n-1)\\times q+3},...i_{n \\times q}\\}.$ (10)\nAmong them, I represents an ordered image sequence containing K images that encapsulate the video information. By partitioning, I is divided into N sub-image sequences, with the n-th sub-sequence represented as $I_n$, and each sub-sequence contains q images. Each sub-sequence $I_n$ undergoes a row-major transformation f, resulting in an ordered image sequence I' represented by N images, with each image in transformed from a sub-image sequence $I_n$. Finally, by applying another row-major transformation to I', we obtain the nested transformation image I\". An example of this transformation is illustrated in Figure 2 (b).\nRandom transformation. In real life, sometimes the sequence of ordered objects can be disturbed without affecting the understanding of the original meaning. For example, \"AAAI is a renowned academic conference\" can be rearranged as \"a renowned AAAI is a conference academic\" without significantly affecting people's understanding of the sentence. With a certain background knowledge, we can correct the errors in the sentence. Inspired by this, we perform a random transformation on the ordered image sequence. On the one hand, we aim to compare the random transformation with the two aforementioned fixed rule transformations to investigate whether the model can learn the transformation rules to model temporal relationships in the spatial dimension. On the other hand, we attempt to understand how the model's ability to understand video is affected when the image sequence is disturbed, and whether it possesses a certain \"error correction capability.\" The transformation rule is shown in the following equation:\n$I' = random(I),$ (11)\n$I' = \\{I_3, I_k, I_8, ...I_2\\},$ (12)\n$I'' = f (I') .$ (13)\nHere, we first shuffle the temporal sequence of the ordered image sequence I, obtaining a disordered set of images I'. Finally, applying a row-major transformation results in the randomly transformed output I\". We provide an illustration of this transformation, as shown in Figure 2 (c)."}, {"title": "Experiments", "content": "In this section, we introduce the validation experiments and details and analyze them. The experimental section will be presented in the following order: Experimental Setup, Ablation Experiments on the Flatten, Comparison to state-of-the-art and Visualization.\nExperimental Setup\nOur experiments involve three mainstream network models, three types of Flatten operations, and three mainstream video action recognition datasets. The three network models are ResNet (He et al. 2016), Swinv2 (Liu et al. 2022a), and Uniformer (Li et al. 2023), representing convolutional neural networks, Transformers, and a fusion of CNN and Transformer networks, respectively. We apply the Flatten operation to different neural networks to demonstrate the method's generality. The three types of Flatten operations are the three transformations introduced in the methodology section, which can be classified into fixed rule transformations and random transformations. Ablation experiments with the three Flatten operations demonstrate the network models' ability to learn spatiotemporal mapping relationships and show that the models can model temporal relationships through spatial information. The three mainstream video action recognition datasets are Kinetics400 (Kay et al. 2017), Something-Something V2 (Goyal et al. 2017), and HMDB51 (Kuehne et al. 2011). By showcasing performance on these three datasets, we prove the effectiveness of the Flatten method.\nNetwork Models: First, we briefly introduce the network models used in the experiments and their training parameters.\nResNet (He et al. 2016) is a classic convolutional neural network widely used in deep learning. It utilizes residual connections and blocks to mitigate degradation in deep neu-"}, {"title": "Ablation Study", "content": "As introduced in Section Method, our experiments design three types of Flatten operations. These can be categorized into two groups. The first group consists of rule-based Flatten transformations, such as row-major transformation and nested transformation. Given a set of images $I \\in R^{x \\times y \\times z}$, each transformation will consistently yield the same $I' \\in R^{x\u00b4 \\times y\u00b4}$. The second group consists of non-rule-based Flatten transformations, such as random transformation, where the transformed data is random each time, with only a small probability that two transformations will be identical.\nFlatten Variants:\nWe chose to compare the effects of three Flatten operations on the model's ability to model temporal relationships in the spatial dimension using the Uniformer-XXS model on the Kinetics400 dataset. The objectives of this set of experiments are threefold: 1. To explore the impact of different Flatten transformations on the model's ability to model temporal relationships in the spatial dimension, especially the effects of different rule-based Flatten transformations on the model. 2. To investigate the impact of unordered image sequences on the model's video-understanding capability, as described in Section Method \"Random Transformation\". We aim to understand the model's \"error-correction ability\" for image sequences. 3. By comparing rule-based Flatten transformations with non-rule-based Flatten transformations, we attempt to demonstrate that the network can learn rule-based Flatten transformations f, thereby achieving temporal relationship modeling in the spatial dimension.\nAs shown in Table 2, both the row-major transformation and the nested transformation outperform the original Uniformer(3D)-XXS in terms of Top-1 accuracy, demonstrating the effectiveness of the Flatten method. Additionally, the accuracy difference between the row-major transformation(75.87%) and the nested transformation(75.83%) in rule-based Flatten transformations is minimal, with only a Top-1 accuracy difference of 0.04%. In contrast, the non-rule-based random transformation(70.9%) has a Top-1 accuracy difference of 4.8% compared to the row-major transformation. This phenomenon indicates that deep learning models can achieve temporal relationship modeling in the spatial dimension by learning fixed Flatten transformation rules and are not sensitive to different rule-based Flatten transformations. Additionally, the model using random transformation is still able to converge, demonstrating that it possesses a certain degree of \"error-correction ability\".\nBased on the results of the ablation experiments in the previous section, we selected the row-major transformation as the Flatten operation for subsequent experiments. As described in Subsection Experimental Setup, to demonstrate the effectiveness and generality of the Flatten method, we will test its performance on three mainstream video action datasets (Kinetics400, SSV2, HMDB51) and three mainstream network models (ResNet, SwinV2, Uniformer).\nComparison with State-of-the-Art methods\nTable 1 shows the performance of three different image-understanding network models using the Flatten method for video action recognition on the Kinetics400 dataset. In the XXS and S model sizes of Uniformer, the accuracy 81.1% of the Uniformer models using the Flatten method exceeds that of the 3D version of Uniformer(3D) used for video-"}, {"title": "Conclusion", "content": "We propose a data transformation method called Flatten, which for the first time treats video-understanding tasks as image-understanding tasks, thereby blurring the boundary between these two domains. Experiments on multiple mainstream video-understanding datasets and image-understanding neural networks have demonstrated the effectiveness and versatility of the Flatten method. In the future, we plan to further investigate the relationship between video and image data. Building on Flatten, we aim to explore joint training methods for images and videos to mitigate the issue of limited video data. Additionally, we intend to delve deeper into the \"error correction ability\" of models. As shown in Subsection \"Ablation Study\", the model can still accurately recognize some sequences even after the images are shuffled, which could lead to new methods for video data augmentation. Lastly, we hope to extend the Flatten method to other types of sequential data, broadening its range of applications."}, {"title": "Appendix", "content": "In the appendix, we will provide more detailed implementation specifics and supporting materials. These include the experimental platform, hyperparameter settings, visualization results, and key portions of the code.\nIn our experiments, we built all training code using the PyTorch deep learning framework, with the training framework sourced from Facebook (Meta)'s open-source SlowFast (Fan et al. 2020) project on GitHub. We modified this codebase to incorporate the Flatten method and conducted experiments across multiple datasets (K400 (Kay et al. 2017), SSV2 (Goyal et al. 2017), HMDB51 (Kuehne et al. 2011)) and deep learning models(ResNet (He et al. 2016), Swinv2 (Liu et al. 2022a), Uniformer). The training environment included an NVIDIA GTX 3090 GPU, with Python version 3.12.4.\nOur experiments can be divided into three parts: (1) evaluating Uniformer with different model sizes on Kinetics-400, (2) testing Uniformer across various datasets(K400, SSV2, HMDB51), and (3) assessing different network models on the same dataset, Kinetics-400. The experimental parameters are detailed in Tables 5, 6, and 7.\nImplementation Details\nFor all models trained on the K400 dataset, we use the pretrained weights from the original models trained on the ImageNet1k dataset as initialization. For instance, we initialize the ResNet+Flatten model with the pre-trained weights obtained from the ImageNet1k dataset and then train it on the K400 dataset. For models trained on SSV2 and HMDB51, their pre-trained weights come from the training results of those models on the K400 dataset. For example, the initialization weights for Uniformer-S+Flatten come from the training results of Uniformer-S on the K400 dataset.\nIn this section, we will present the key code for the Flatten method. The code is relatively short and does not modify the network model in any way. Therefore, it can be considered a simple, effective, and versatile method. By converting a sequence of images that represent both temporal and spatial information of a video into a single 2D image, the Flatten method reduces the boundary between video understanding tasks and image understanding tasks.\nCode\n```python\ndef flatten (frames, flatten_type):\n    assert frames.ndim == 4,\n        \"Input tensor should have 4\\n        dimensions.\"\n    t = frames.shape[0]\n    c = frames.shape[1]\n\n    size = frames.shape[2]\n\n    for n in range(1, length // 2 + 1):\n        m = length // n\n        if n*m == t and n==m:\n            break\n    #Rearrange the order of the image\n    #sequence based on the Flatten\n    #type.\n    if flatten_type is \"row_major\":\n        pass\n    else:\n        frames = transformation(frames,\n                               flatten_type)\n\n    img = rearrange (frames,\n                       \"(nm) c h w -> c (nh) (mw)\",\n                       n=n, m=m)\n\n    return img\n```\nImplementation Details\nThe code above is the specific implementation of the Flatten method. 'Frames' represents a sequence of images that convey video information. Depending on the Flatten type, the sequence of 'frames' will be transformed, and then the transformed sequence of 'frames' will be flattened to achieve the conversion from an image sequence to a single image.\nVisualization\nIn Figure 5, we compare the attention maps of the Uniformer-S+Flatten and Uniformer (3D)-S models. It can be observed that Uniformer-S+Flatten has a greater focus on motion information and exhibits stronger attention. The visualization further demonstrates that the Flatten method aids the image understanding model in modeling temporal relationships through spatial positional information, thereby achieving a reduction in the boundary between image understanding and video understanding.\nIn Figure X, we show the changes in feature maps during the model's inference process. Compared to the original Uniformer (3D)-S, the Uniformer-S+Flatten model increasingly focuses on overall feature information as the network depth increases, thereby achieving global video temporal modeling.\nVisualization"}]}