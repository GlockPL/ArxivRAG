{"title": "MonoSparse-CAM: Harnessing Monotonicity and Sparsity for Enhanced Tree Model Processing on CAMS", "authors": ["Tergel Molom-Ochir", "Hai (Helen) Li", "Brady Taylor", "Yiran Chen"], "abstract": "Despite significant advancements in AI driven by neural networks, tree-based machine learning (TBML) models excel on tabular data. These models exhibit promising energy efficiency, and high perfor- mance, particularly when accelerated on analog content-addressable memory (aCAM) arrays. However, optimizing their hardware de- ployment, especially in leveraging TBML model structure and aCAM circuitry, remains challenging.\nIn this paper, we introduce MonoSparse-CAM, a novel content- addressable memory (CAM) based computing optimization tech- nique. MonoSparse-CAM efficiently leverages TBML model sparsity and CAM array circuits, enhancing processing performance. Our experiments show that MonoSparse-CAM reduces energy consump- tion by up to 28.56\u00d7 compared to raw processing and 18.51\u00d7 com- pared to existing deployment optimization techniques. Additionally, it consistently achieves at least 1.68\u00d7 computational efficiency over current methods.\nBy enabling energy-efficient CAM-based computing while pre- serving performance regardless of the array sparsity, MonoSparse- CAM addresses the high energy consumption problem of CAM which hinders processing of large arrays. Our contributions are twofold: we propose MonoSparse-CAM as an effective deployment optimization solution for CAM-based computing, and we investi- gate the impact of TBML model structure on array sparsity. This work provides crucial insights for energy-efficient TBML on hard- ware, highlighting a significant advancement in sustainable AI technologies.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence (AI) has revolutionized numerous fields, in- cluding computer vision, robotics and health care [1, 12, 19]. These accomplishments are fueled by deep neural network models trained on large datasets. However, tree-based models have shown superior performance on tabular data compared to their deep learning coun- terparts [7, 9, 22]. Additionally, tree-based models are also more interpretable and easier to debug. For this reason, implementing tree-based models in hardware is a growing topic of research. One implementation of a tree-based model was realized by mapping a tree structure onto an analog content-addressable memory (aCAM) array [18]."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Tree-Based Machine Learning (TBML) Models", "content": "A tree-based model is a machine learning approach that trains a decision tree to classify data based features. A feature vector is provided to a parent node and one feature is compared to upper and lower bounds. Depending on the outcome, the algorithm descends to either the right or left child node. This process continues until the final child node, where a logical True value classifies the data into the associated class.\nIn real-world applications, tabular data, which consists of sam- ples with the same number of features, is one of the most common data types [15, 17]. Top performing TBML models, such as Random Forest and XGBoost, consist of ensembles of decision trees. These models are highly interpretable and transparent, which is crucial in fields requiring explainability. Due to their interpretability, cost ef- fectiveness, and high performance, TBML algorithms remain SOTA for tabular data [2].\nWith the success of deep learning on data types such as images, text, and audio, there has been a recent interest in using deep neu- ral network (DNN) models for tabular data [11, 20, 24]. However,"}, {"title": "2.2 Sparsity in Tree-Based Models", "content": "Tree-based models' structures, which can vary in balance and spar- sity based on their training, significantly influence decision-making processes. Balance refers to the difference in the number of nodes between the left and right sub-trees splitting from the root node; a balanced tree has nearly equal nodes in both sub-trees. Tree-based models, depending on their training, can suffer from sparsity. In- vestigating the correlation between the architecture of decision trees and their performance is essential, including the impact of tree balance, sparsity, and their representations in data process- ing. For instance, a tree's structure, particularly its sparsity and balance, can affect the performance of different CAM-based com- puting optimization algorithms. Sparse tree structures may favor specific algorithms, leading to higher computational efficiency. Un- derstanding these nuances is crucial for optimizing decision tree performance in various applications. While an earlier technique has sped up processing, it comes with notable limitations. Specifically, FR excels with sparse trees but falls short with less sparse trees."}, {"title": "2.3 Analog CAM Arrays", "content": "CAM cells were originally designed to quickly find the location of data in memory based on the desired input contents. In digital CAM, cells are connected horizontally by match-lines (MLs) and vertically by data-lines (DLs). Before a search, MLs are pre-charged to a high logic state, and the desired bit sequence is provided to the DLs. A mismatch between the bit stored in the CAM cell and the bit on the DL discharges the matchline. At the end of the search, any ML that is not discharged corresponds to a match in that row. Analog CAM cells work similarly, but check if the provided analog"}, {"title": "2.4 High Power Consumption in CAMs", "content": "Content Addressable Memory (CAM) is renowned for its rapid data retrieval but struggles with high power consumption due to simultaneous activation of large chip portions [5]. This limits scalability in power-constrained environments. Addressing this at both the inter-array circuitry and architectural levels, recent advances include novel TCAM cell circuits with memristor devices [8] and hybrid-type CAM designs that balance NOR-type CAM's speed with NAND-type CAM's energy efficiency [3]. NOR-type CAMs use precharge/discharge for fast searches, while NAND- type CAMs use pass transistors for low-power, sequential searches. Additionally, architectural innovations like Feature Reordering have been developed [18]. In this work, we focus on a method that merges characteristics from both levels, aiming for energy-efficient and high-speed processing."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 MonoSparse-CAM Algorithm", "content": "The MonoSparse-CAM algorithm builds upon FR by disabling pro- cessing for any row that has already received a mismatch. This method skips two kinds of cells: rows of empty cells and cells on an already discharged matchline. An empty, or \"don't care\" cell, has a range of 0 to 1 and will match with any input value. If a row of all empty cells exists, we skip writing this row onto the aCAM array, as it will always get a row match.\nPre-charging the matchline before the computation cycle en- sures monotonicity: once discharged, a matchline will not be pulled up again until the next pre-charge cycle. Any cell attempting to"}, {"title": "3.2 Sparsity Considerations and Tradeoffs", "content": "To explore the relationship between sparsity and structural bal- ance in TBML models, we conducted an experimental analysis. Structurally-balanced trees reduce the computational complexity, allowing for faster computation and lower memory usage [16]. We created binary classification datasets (1000 samples, 5 features), split them 67:33 for training and testing, and trained XGBoost and RandomForest models with a single decision tree (max depth 10). Structural balance was evaluated by calculating the difference in node counts between the left and right sub-trees. The trees were converted into arrays, and empty cells were counted. Repeating this 5000 times, we found an inverse relationship between tree balance and sparsity. XGBoost models has a Pearson Correlation coefficient of -0.35 (p-value 1.433E-148), and RandomForest models had a cor- relation of -0.327 (p-value 2.809E-125). These results indicate that"}, {"title": "3.3 Energy Consumption and Gain", "content": "Results for varying CAM widths differ from varying heights. A 320x240 array was processed using CAM arrays of varying heights (H=40 to 64, W=16) and varying widths (H=48, W=24 to 60). Energy gain using the proposed algorithm over raw and FR processing is computed across different sparsity levels (green for raw; blue for FR). Gain over FR, for instance, is com- puted as \u2211 EnergyFR/\u03a3EnergyMonoSparse-CAM.\nWith a constant size of 24x48, total energy and delay for a values from 0.1 to 0.5 are the same as at \u03bb=0.5."}, {"title": "3.4 Circuit Simulation for analog CAM Cell and Arrays", "content": "The 6T2M analog CAM cells were designed in Cadence Virtuoso, with energy measurements simulated in HSPICE using the TSMC 65 nm PDK. A custom Python script varied CAM dimensions, config- uring cell conductances and input voltages, with an 11 pF capacitor for each match line. Simulations used CMOS device models (TT, SS, FF). A specialized function linked voltage ranges to conductances, ensuring precise conductance-voltage relationships."}, {"title": "4 RESULTS AND DISCUSSION", "content": "We evaluate MonoSparse-CAM performance against existing meth- ods by examining energy consumption, delay, and computation efficiency. We also assess how CAM size and process variations impact energy gain. Lastly, we address the high energy consump- tion of CAMs, which bottlenecks large array processing, and how MonoSparse-CAM mitigates this issue."}, {"title": "4.1 Energy Consumption and Delay Analysis", "content": "A 240x320 array is processed using a 24x48 CAM. MonoSparse-CAM consistently achieves the lowest total energy consumption across all sparsity levels , with energy gains peaking at 22.16\u00d7 over raw processing and 10.96x over FR."}, {"title": "4.2 Size Variability Analysis", "content": "The width and height of a CAM array significantly impact total en- ergy consumption. that smaller heights often yield the highest gains, with high sparsity arrays reaching up to 28.56\u00d7 gain over raw and 2.00\u00d7 over FR, and low sparsity arrays achieving up to 18.51\u00d7 over both. Varying CAM widths show similar patterns, with high sparsity arrays reaching up to 22.17\u00d7 over raw and 3.45\u00d7 over FR, and low sparsity arrays achieving up to 11.0x over both.\nGain varies more with changes in width than changes in height. Reduced width improves energy efficiency because the system's register can quickly identify discharged MLs, skipping them in the subsequent feature group cycle and streamlining computation. For instance, in a 160-cell wide array, if a mismatch occurs at cell 80 and is processed by CAM arrays with a width of 8, the algorithm stops after 10 operations, checking 80 cells. Processed by CAM arrays with a width of 32, the algorithm stops after 3 operations, checking 96 cells, 16 more than necessary.\n\n$GOPS/W = \\frac{H_{LargeArray} W_{LargeArray}/Delay_{Technique}}{AvgPower_{Technique}}$"}, {"title": "4.3 Computational Efficiency and Throughput Analysis", "content": "presents the computational efficiency of different methods at varying sparsity levels, measured in Giga Operations Per Second per Watt (GOPS/W). MonoSparse-CAM consistently shows the highest computational efficiency, peaking at 418 GOPS/W (computed as in Eq. 1) for high sparsity (\u03bb = 0.9), while Feature Reordering shows moderate efficiency, and raw processing and monotonicity only processing exhibit lower efficiencies. These results highlight the significant performance gains of MonoSparse-CAM in optimizing computational efficiency across varying degrees of sparsity."}, {"title": "4.4 Corner Analysis", "content": "We explored the impact of corner analysis on different designs by examining three corners: TT (typical conditions), FF (fast NMOS, fast PMOS), and SS (slow NMOS, slow PMOS). Our simulations revealed significant variations in total energy consumption across these corners, as illustrated. MonoSparse-CAM consistently achieved the lowest total energy consumption across all corners, with at least 3.7x energy savings compared to existing methods."}, {"title": "4.5 Scalability Analysis", "content": "In Fig. 4i, an array of increasing sizes is processed by a 40x24 analog CAM using different tech- niques at \u03bb = 0.7 sparsity level. Prediction curves (with R\u00b2 > 0.97) are generated. As array size increases, energy usage grows expo- nentially with raw and FR, while MonoSparse-CAM shows nearly linear growth."}, {"title": "5 CONCLUSION", "content": "In this study, we introduced MonoSparse-CAM, a monotonicity and sparsity-aware technique for processing tree-based machine learning models with Content-Addressable Memory (CAM) arrays. This hardware-software co-designed approach successfully exploits the sparsity in tree-based models and the monotonicity of CAM cir- cuits, leading to significant improvements in processing efficiency and energy consumption. Our method demonstrated a remarkable 28.56\u00d7 and 18.51\u00d7 reduction in energy consumption over raw pro- cessing and a state of the art optimization technique, respectively."}]}