{"title": "MonoSparse-CAM: Harnessing Monotonicity and Sparsity for Enhanced Tree Model Processing on CAMs", "authors": ["Tergel Molom-Ochir", "Hai (Helen) Li", "Brady Taylor", "Yiran Chen"], "abstract": "Despite significant advancements in AI driven by neural networks, tree-based machine learning (TBML) models excel on tabular data. These models exhibit promising energy efficiency, and high performance, particularly when accelerated on analog content-addressable memory (aCAM) arrays. However, optimizing their hardware deployment, especially in leveraging TBML model structure and aCAM circuitry, remains challenging.\nIn this paper, we introduce MonoSparse-CAM, a novel content-addressable memory (CAM) based computing optimization technique. MonoSparse-CAM efficiently leverages TBML model sparsity and CAM array circuits, enhancing processing performance. Our experiments show that MonoSparse-CAM reduces energy consumption by up to 28.56\u00d7 compared to raw processing and 18.51\u00d7 compared to existing deployment optimization techniques. Additionally, it consistently achieves at least 1.68\u00d7 computational efficiency over current methods.\nBy enabling energy-efficient CAM-based computing while preserving performance regardless of the array sparsity, MonoSparse-CAM addresses the high energy consumption problem of CAM which hinders processing of large arrays. Our contributions are twofold: we propose MonoSparse-CAM as an effective deployment optimization solution for CAM-based computing, and we investigate the impact of TBML model structure on array sparsity. This work provides crucial insights for energy-efficient TBML on hardware, highlighting a significant advancement in sustainable AI technologies.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence (AI) has revolutionized numerous fields, including computer vision, robotics and health care [1, 12, 19]. These accomplishments are fueled by deep neural network models trained on large datasets. However, tree-based models have shown superior performance on tabular data compared to their deep learning counterparts [7, 9, 22]. Additionally, tree-based models are also more interpretable and easier to debug. For this reason, implementing tree-based models in hardware is a growing topic of research. One implementation of a tree-based model was realized by mapping a tree structure onto an analog content-addressable memory (aCAM) array [18].\nA comparison of TBML accelerators has been made in the literature, highlighting memristive analog CAMs as the least energy hungry option [13, 18]. Architecture optimization of TBML accelerators is crucial for processing large arrays, which are difficult to fabricate. Prior works have explored different deployment management techniques to optimize the processing performance. Feature reordering (FR) is a novel approach in architecture optimization, prioritizing the reordering of rows and columns based on activity levels, effectively concentrating the most active cells towards the bottom left corner of the array [18]. By prioritizing active cells and skipping empty ones, FR saves time and energy consumption, with a slight improvement in throughput as shown in Fig. 1a. While the exact-matching technique has been widely used, other potentially efficient matching criteria, such as best match, threshold match, and partial match, have yet to be implemented in hardware [10].\nThis paper introduces and evaluates a software-hardware codesign for accelerating TBML models on CAMs, focusing on how model sparsity influences algorithm choice for optimizing CAMbased computation. This paper makes the following contributions:\n\u2022 We present MonoSparse-CAM, a novel CAM-based computing optimization algorithm that leverages decision-tree sparsity and CAM array monotonicity.\n\u2022 We explore how tree-based model shapes affect array sparsity, highlighting the need for a tree-aware technique.\n\u2022 We demonstrate that MonoSparse-CAM effectively addresses scalability issues in large array processing and significantly reduces the high energy consumption inherent in CAM circuitry.\nOur results show that the structural balance of a binary TBML model is an indicator of sparsity in the array, presenting opportunities for further optimization. Regardless of sparsity levels, by acknowledging the circuit's monotonocity as shown in Fig. 1b, a large array can be processed with minimal energy consumption whilst maintaining high accuracy and throughput. Compared to state-of-the-art (SOTA) technique, MonoSparse-CAM reduces energy consumption by 3.45\u00d7 for high sparsity arrays and 18.51\u00d7 for low sparsity arrays."}, {"title": "2 BACKGROUND", "content": "2.1 Tree-Based Machine Learning (TBML)\nModels\nA tree-based model is a machine learning approach that trains a decision tree to classify data based features. A feature vector is provided to a parent node and one feature is compared to upper and lower bounds. Depending on the outcome, the algorithm descends to either the right or left child node. This process continues until the final child node, where a logical True value classifies the data into the associated class. Fig. 2a shows an example of a decision tree for a dataset with four features and six terminating child nodes, each representing a final classification.\nIn real-world applications, tabular data, which consists of samples with the same number of features, is one of the most common data types [15, 17]. Top performing TBML models, such as Random Forest and XGBoost, consist of ensembles of decision trees. These models are highly interpretable and transparent, which is crucial in fields requiring explainability. Due to their interpretability, cost effectiveness, and high performance, TBML algorithms remain SOTA for tabular data [2].\nWith the success of deep learning on data types such as images, text, and audio, there has been a recent interest in using deep neural network (DNN) models for tabular data [11, 20, 24]. However, SOTA TBML models, such as XGBoost and RandomForest, consistently outperform deep learning models on diverse tabular datasets with the same tuning protocols [7, 9, 22]. Additionally, in federated learning settings, TBML models still outperform federated DNNs on tabular data [14]. Despite the high accuracy of DNNs, the computational workload required by training DNNs has a surprisingly large carbon footprint [21]. In a recent study, TBML models were trained on real-world data to predict effluent pH and concentration in the capacitive deionization process. These ensemble models incurred 100x less computational cost than a DNN model whilst achieving the same level of accuracy [23].\n2.2 Sparsity in Tree-Based Models\nTree-based models' structures, which can vary in balance and sparsity based on their training, significantly influence decision-making processes. Balance refers to the difference in the number of nodes between the left and right sub-trees splitting from the root node; a balanced tree has nearly equal nodes in both sub-trees. Tree-based models, depending on their training, can suffer from sparsity. Investigating the correlation between the architecture of decision trees and their performance is essential, including the impact of tree balance, sparsity, and their representations in data processing. For instance, a tree's structure, particularly its sparsity and balance, can affect the performance of different CAM-based computing optimization algorithms. Sparse tree structures may favor specific algorithms, leading to higher computational efficiency. Understanding these nuances is crucial for optimizing decision tree performance in various applications. While an earlier technique has sped up processing, it comes with notable limitations. Specifically, FR excels with sparse trees but falls short with less sparse trees.\n2.3 Analog CAM Arrays\nCAM cells were originally designed to quickly find the location of data in memory based on the desired input contents. In digital CAM, cells are connected horizontally by match-lines (MLs) and vertically by data-lines (DLs). Before a search, MLs are pre-charged to a high logic state, and the desired bit sequence is provided to the DLs. A mismatch between the bit stored in the CAM cell and the bit on the DL discharges the matchline. At the end of the search, any ML that is not discharged corresponds to a match in that row. Analog CAM cells work similarly, but check if the provided analog voltage on the DL falls within a range between lower bound and upper bound voltages. One implementation of an analog CAM cell [13] uses memristors in common-source amplifiers to compare the input voltage to a threshold and activate an NMOS to discharge the ML. Memristors are two-terminal devices with tunable, nonvolatile resistances [4], allowing the threshold of the cell to be changed.\nAn analog CAM cell stores ranges from 0 to 1 using its lower-bound and upper-bound subcircuits. An empty, or \"don't care\", cell is stored by setting the low bound to 0 and the high bound to 1, outputting a \"match\" to any input.\nPrevious works have used analog CAMs to implement TBML models efficiently [18]. When mapping a model to a CAM array, each row represents a complete path through the tree, from the root node to a final terminating child node that represents a classification. Paths through the tree can be rearranged such that each column includes all nodes for a given feature of the data. The feature is converted into a voltage and applied to the DL, and any matching MLs indicate the data belongs to the associated class. As the order of the features is arbitrary in each root-to-leaf path, FR reorders the features (columns) based on occurrence, moving all active or important features to the left side of the array. Similarly, rows are reordered based on the number of non \"empty\" cells, moving the most populated rows to the bottom of the array. This leaves the top right region of the array largely empty, offering compressibility and efficient tiling of a large data array onto reasonably-sized analog CAM arrays [18].\n2.4 High Power Consumption in CAMs\nContent Addressable Memory (CAM) is renowned for its rapid data retrieval but struggles with high power consumption due to simultaneous activation of large chip portions [5]. This limits scalability in power-constrained environments. Addressing this at both the inter-array circuitry and architectural levels, recent advances include novel TCAM cell circuits with memristor devices [8] and hybrid-type CAM designs that balance NOR-type CAM's speed with NAND-type CAM's energy efficiency [3]. NOR-type CAMs use precharge/discharge for fast searches, while NANDtype CAMs use pass transistors for low-power, sequential searches. Additionally, architectural innovations like Feature Reordering have been developed [18]. In this work, we focus on a method that merges characteristics from both levels, aiming for energy-efficient and high-speed processing."}, {"title": "3 METHODOLOGY", "content": "3.1 MonoSparse-CAM Algorithm\nThe MonoSparse-CAM algorithm builds upon FR by disabling processing for any row that has already received a mismatch. This method skips two kinds of cells: rows of empty cells and cells on an already discharged matchline. An empty, or \"don't care\" cell, has a range of 0 to 1 and will match with any input value. If a row of all empty cells exists, we skip writing this row onto the aCAM array, as it will always get a row match.\nPre-charging the matchline before the computation cycle ensures monotonicity: once discharged, a matchline will not be pulled up again until the next pre-charge cycle. Any cell attempting to discharge an already discharged ML has no impact on the result of the circuit, as it is already a mismatch. This observation can be used to improve system efficiency. By leveraging the monotonicity of the CAM array, we propose a highly efficient CAM-based computing optimization algorithm, whose performance is unaffected by array sparsity.\nFig. 3e illustrates the use of a 2x2 analog CAM array to process a 10x6 model array, divided into three feature groups (fg0, fg1, fg2) to optimize the CAM's width. This figure emphasizes a strategic data processing methodology where cells are selectively loaded onto the CAM based on criteria such as emptiness, discharge status, and monotonicity. By employing a tiling technique, a large decision-tree model's array is divided into manageable segments that fit into the smaller physical CAM arrays [18]. In the initial cycle (fg0), shown in purple outlining, selective loading is used to omit non-essential rows, indicated by light purple boxes with Xs. For example, rows ML8 and ML10 in fg0, ML10 in fg1, and ML4 in fg2 are skipped, and not loaded onto and procssed on CAMs, as these rows will automatically match the input. As demonstrated in Fig. 3e, after the first cycle, rows ML2, ML5, ML6, and ML9 are deactivated, preventing any cells on these rows from being loaded onto the CAM in subsequent cycles. Following the second cycle, rows ML1, ML3, ML7, and ML8 are discharged, ensuring that cells on these rows are not loaded into the CAM in future cycles.\nTo keep track of the discharged matchlines, a register is needed. This register is updated after a feature group (i.e. FG0, FG1, FG2, ...) is tiled, processed on aCAM, and ML states are retrieved. The register is accessed when the algorithm tiles the feature group and determines which rows to write onto the CAM.\nTo simulate large arrays with different levels of sparsity, we sample cells from a random Gaussian distribution, concentrating active cells near the center; according to previous studies, when an image is saved to an array, the most importance features are near the center while the least significant features are located around the edges [18]. We use a values from 0.0 (low sparsity) to 1.0 (highly sparse), where $\\lambda = 1.0 - \\sigma$, with o being standard deviation from the Gaussian distribution. Inputs to the tree or the large array are randomly generated and broken into arrays with length of WVDL/WCAM each, with each fed into its corresponding feature group.\n3.2 Sparsity Considerations and Tradeoffs\nTo explore the relationship between sparsity and structural balance in TBML models, we conducted an experimental analysis. Structurally-balanced trees reduce the computational complexity, allowing for faster computation and lower memory usage [16]. We created binary classification datasets (1000 samples, 5 features), split them 67:33 for training and testing, and trained XGBoost and RandomForest models with a single decision tree (max depth 10). Structural balance was evaluated by calculating the difference in node counts between the left and right sub-trees. The trees were converted into arrays, and empty cells were counted. Repeating this 5000 times, we found an inverse relationship between tree balance and sparsity. XGBoost models has a Pearson Correlation coefficient of -0.35 (p-value 1.433E-148), and RandomForest models had a correlation of -0.327 (p-value 2.809E-125). These results indicate that well-balanced trees exhibit a high sparsity when mapped to arrays (Fig. 2b). Given the structural and sparsity differences among trees, an optimization technique capable of handling all types of TBML models is essential for efficient aCAM processing.\nFig. 4a shows that computation cost remains low for MonoSparse-CAM even in low sparsity scenarios (i.e., $\\lambda$=0.5) where FR performs poorly. This is due to the increased number of non-empty cells, allowing the algorithm to leverage monotonicity more effectively. A higher number of non-empty cells increases the chances of encountering cells that predictably discharge the ML, enabling the algorithm to skip these computations and reduce overall computational cost. At lower sparsity levels, computational savings in MonoSparseCAM are driven by monotonicity-based skipping, while at higher sparsity levels, savings come from Feature Reordering; as shown in Fig.4a and Fig.4b, monotonicity-only techniques excel at low sparsity and Feature Reordering at high sparsity.\n3.3 Energy Consumption and Gain\nResults for varying CAM widths differ from varying heights. A 320x240 array was processed using CAM arrays of varying heights (H=40 to 64, W=16) and varying widths (H=48, W=24 to 60) as shown in Fig. 4d and 4e and Fig. 4f and 4g. Energy gain using the proposed algorithm over raw and FR processing is computed across different sparsity levels (green for raw in Fig. 4d and 4f; blue for FR in Fig. 4e and 4g). Gain over FR, for instance, is computed as $\\sum Energy_{FR}/\\sum Energy_{MonoSparse-CAM}$. Results in Fig. 4 are averaged over at least three trials and Energy consumption measurements from Sense Amplifier were excluded. With a constant size of 24x48, total energy and delay for a values from 0.1 to 0.5 are the same as at $\\lambda$=0.5. Therefore, Fig. 4a-c focus on a values from 0.5 to 0.9.\n3.4 Circuit Simulation for analog CAM Cell and\nArrays\nThe 6T2M analog CAM cells were designed in Cadence Virtuoso, with energy measurements simulated in HSPICE using the TSMC 65 nm PDK. A custom Python script varied CAM dimensions, configuring cell conductances and input voltages, with an 11 pF capacitor for each match line. Simulations used CMOS device models (TT, SS, FF). A specialized function linked voltage ranges to conductances, ensuring precise conductance-voltage relationships."}, {"title": "4 RESULTS AND DISCUSSION", "content": "We evaluate MonoSparse-CAM performance against existing methods by examining energy consumption, delay, and computation efficiency. We also assess how CAM size and process variations impact energy gain. Lastly, we address the high energy consumption of CAMs, which bottlenecks large array processing, and how MonoSparse-CAM mitigates this issue.\n4.1 Energy Consumption and Delay Analysis\nA 240x320 array is processed using a 24x48 CAM. Fig. 4a shows that, for raw processing, the total energy consumption stays stable across different sparsity levels (mean = 3.77\u00b5J, stdev = 0.15\u03bcJ). FR achieves 15\u00d7 energy gain at high sparsity (\u03bb = 0.9), but its energy usage increases at lower sparsity, matching raw processing levels. MonoSparse-CAM consistently achieves the lowest total energy consumption across all sparsity levels (mean = 0.40\u00b5J, stdev = 0.48\u00b5J), with energy gains peaking at 22.16\u00d7 over raw processing and 10.96x over FR.\n4.2 Size Variability Analysis\nThe width and height of a CAM array significantly impact total energy consumption. Fig. 4e and 4d demonstrate that smaller heights often yield the highest gains, with high sparsity arrays reaching up to 28.56\u00d7 gain over raw and 2.00\u00d7 over FR, and low sparsity arrays achieving up to 18.51\u00d7 over both. Varying CAM widths show similar patterns, with high sparsity arrays reaching up to 22.17\u00d7 over raw and 3.45\u00d7 over FR, and low sparsity arrays achieving up to 11.0x over both.\nGain varies more with changes in width than changes in height. Reduced width improves energy efficiency because the system's register can quickly identify discharged MLs, skipping them in the subsequent feature group cycle and streamlining computation. For instance, in a 160-cell wide array, if a mismatch occurs at cell 80 and is processed by CAM arrays with a width of 8, the algorithm stops after 10 operations, checking 80 cells. Processed by CAM arrays with a width of 32, the algorithm stops after 3 operations, checking 96 cells, 16 more than necessary.\n$GOPS/W = \\frac{H_{LargeArray} \\cdot W_{LargeArray}/Delay_{Technique}}{AvgPower_{Technique}}$\n4.3 Computational Efficiency and Throughput\nAnalysis\nFig. 4c presents the computational efficiency of different methods at varying sparsity levels, measured in Giga Operations Per Second per Watt (GOPS/W). MonoSparse-CAM consistently shows the highest computational efficiency, peaking at 418 GOPS/W (computed as in Eq. 1) for high sparsity (\u03bb = 0.9), while Feature Reordering shows moderate efficiency, and raw processing and monotonicity only processing exhibit lower efficiencies. These results highlight the significant performance gains of MonoSparse-CAM in optimizing computational efficiency across varying degrees of sparsity.\n4.4 Corner Analysis\nWe explored the impact of corner analysis on different designs by examining three corners: TT (typical conditions), FF (fast NMOS, fast PMOS), and SS (slow NMOS, slow PMOS). Our simulations revealed significant variations in total energy consumption (\u00b5J) across these corners, as illustrated in Fig. 4h. MonoSparse-CAM consistently achieved the lowest total energy consumption across all corners, with at least 3.7x energy savings compared to existing methods.\n4.5 Scalability Analysis\nIn Fig. 4i, an array of increasing sizes (i.e., 160x120, 320x240, 480x360, 640x480) is processed by a 40x24 analog CAM using different techniques at \u03bb = 0.7 sparsity level. Prediction curves (with R\u00b2 > 0.97) are generated. As array size increases, energy usage grows exponentially with raw and FR, while MonoSparse-CAM shows nearly linear growth."}, {"title": "5 CONCLUSION", "content": "In this study, we introduced MonoSparse-CAM, a monotonicity and sparsity-aware technique for processing tree-based machine learning models with Content-Addressable Memory (CAM) arrays. This hardware-software co-designed approach successfully exploits the sparsity in tree-based models and the monotonicity of CAM circuits, leading to significant improvements in processing efficiency and energy consumption. Our method demonstrated a remarkable 28.56\u00d7 and 18.51\u00d7 reduction in energy consumption over raw processing and a state of the art optimization technique, respectively."}]}