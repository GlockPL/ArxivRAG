{"title": "A Survey of Quantized Graph Representation Learning: Connecting\nGraph Structures with Large Language Models", "authors": ["Qika Lin", "Zhen Peng", "Kaize Shi", "Kai He", "Yiming Xu", "Erik Cambria", "Mengling Feng"], "abstract": "Recent years have witnessed rapid advances in\ngraph representation learning, with the continu-\nous embedding approach emerging as the dominant\nparadigm. However, such methods encounter is-\nsues regarding parameter efficiency, interpretabil-\nity, and robustness. Thus, Quantized Graph Rep-\nresentation (QGR) learning has recently gained in-\ncreasing interest, which represents the graph struc-\nture with discrete codes instead of conventional\ncontinuous embeddings. Given its analogous repre-\nsentation form to natural language, QGR also pos-\nsesses the capability to seamlessly integrate graph\nstructures with large language models (LLMs). As\nthis emerging paradigm is still in its infancy yet\nholds significant promise, we undertake this thor-\nough survey to promote its rapid future prosper-\nity. We first present the background of the gen-\neral quantization methods and their merits. More-\nover, we provide an in-depth demonstration of cur-\nrent QGR studies from the perspectives of quan-\ntized strategies, training objectives, distinctive de-\nsigns, knowledge graph quantization, and applica-\ntions. We further explore the strategies for code de-\npendence learning and integration with LLMs. At\nlast, we give discussions and conclude future direc-\ntions, aiming to provide a comprehensive picture of\nQGR and inspire future research.", "sections": [{"title": "1 Introduction", "content": "Graph representation usually involves transforming nodes,\nedges, or structures within graph data to low-dimensional\ndense embeddings, which is called continuous graph repre-\nsentation as shown in Figure 1 (a). The learned representa-\ntion is obligated to preserve both node attributes and topo-\nlogical structure [Zhu et al., 2020]. This field has seen sub-\nstantial development and expansion in recent years. The\nemergence of numerous advanced technologies has invari-\nably drawn significant attention within the graph commu-\nnity, becoming focal points of extensive studies, e.g., the\nwell-known node2vec [Grover and Leskovec, 2016] model,\ngraph neural networks (GNNs) [Kipf and Welling, 2017],\nand self-supervised graph learning [Liu et al., 2022]. They\nhave achieved great empirical success in graph-related do-\nmains, including bioinformatics, social networks, recommen-\ndation systems, and anomaly detection [Xia et al., 2021;\nLiu et al., 2023c].\nAlthough continuous graph representation successfully ex-\ntracts rich semantic information from the input graph to adapt\nto a wide range of machine learning and deep learning tasks,\nconcerns may arise regarding the embedding efficiency, in-\nterpretability, and robustness [Wang et al., 2024]. Specifi-\ncally, with the great growth of graph scales, there will be a\nlinear increase in embedding parameters, becoming a con-\nsiderable challenge in graph learning contexts involving mil-\nlions or even more nodes. Besides, continuous embeddings\nare generated in a black-box fashion and the meaning of the\noverall representation or any one dimension is unknowable,\nleading to a lack of interpretability. Moreover, these con-\ntinuous representations are typically task-specific and show\nno significant abstraction at the representation level, making\nthem not robust enough to generalize to a variety of practical\ntasks as effectively as LLMs.\nInterestingly, in real life, discrete features are often pre-\nferred for representation. For example, as shown in Fig-\nure 1 (b), descriptions affiliated to an individual are often\na combination of discrete and meaningful features, such as\nage (e.g., 18), gender (male or female), and place of res-\nidence (e.g., Beijing). This scheme is simple and easy to"}, {"title": "2 Background", "content": null}, {"title": "2.1 Merits of Quantized Graph Representation", "content": "Compared to continuous representation methods, QGR offers\na series of distinct advantages that fertilize its applications.\nEmbedding Parameter Efficiency. As large-scale graphs\nbecome more prevalent, there is a corresponding substantial\nincrease in the representation parameters, which necessitates\nsignificant memory usage for storage and substantial compu-\ntational expenses [Galkin et al., 2022]. Supposing n as the\nnumber of nodes and d as the dimensions, the required em-\nbedding parameters would be n \u00d7 d. In the quantized settings,\nthe total count of parameters would reduce to n\u00d7m+M\u00d7d,\nwhere M and m denote the length of the codeword set and\nthe number of codes assigned to each node. For example, if\nthere are 106 nodes with 1024 feature dimensions, 1024 code-\nwords, and 8 assigned codes, it would be a 113-fold reduction\nin required embedding parameters."}, {"title": "Explainability and Interpretability", "content": "The discrete nature\nof QGR brings the explainability and interpretability for em-\nbeddings and reasoning, as shown in Figure1 (b). Each code\nwould be assigned to the practical item by specific designs\nfor direct interpretability, e.g., Dr. E [Liu et al., 2024b] intro-\nduces a language vocabulary as the code set and each node in\ngraphs can be represented as the permutation of explainable\nlanguage tokens."}, {"title": "Robustness and Generalization", "content": "The discrete tokens of\nthe QGR are more robust and generalizable, which usually\nutilizes the self-supervised strategies to learn both local-level\nand high-level graph structures (e.g., edge reconstruction and\nDGI [Velickovic et al., 2019]) as well as semantics (e.g., fea-\nture reconstruction). Moreover, the acquired tokens can read-\nily adapt to downstream applications, either directly or indi-\nrectly [Yang et al., 2024]."}, {"title": "Seamless Integration with NLP Models", "content": "The swift ad-\nvancement in natural language processing (NLP) techniques,\nparticularly LLMs, has sparked an increased interest in em-\nploying NLP models to resolve graph tasks. However, the\ninherent representation gap between the typical graph struc-\nture and natural language poses a significant challenge to their\nseamless and effective integration. Graph quantization, by\nlearning discrete codes that bear resemblance to natural lan-\nguage forms, can facilitate this integration directly and seam-\nlessly [Lin et al., 2025a; Liu et al., 2024b]."}, {"title": "2.2 General Quantization Methods", "content": "The mainstream quantization methods can be roughly cate-\ngorized into product quantization, vector quantization, finite\nscalar quantization, and anchor selection and assignment.\nProduct quantization (PQ). The core idea of PQ [Jegou\net al., 2011] is to divide a high-dimensional space into mul-\ntiple low-dimensional subspaces and then quantize them in-\ndependently within each subspace. Specifically, it splits a\nhigh-dimensional vector into multiple smaller subvectors and\nquantizes each subvector separately. In the quantization pro-\ncess, each subvector is mapped to a set of a finite number of\ncenter points by minimizing the quantization error."}, {"title": "Vector Quantization (VQ)", "content": "To effectively learn quantized\nrepresentations in a differentiable manner, the VQ strat-\nergy [van den Oord et al., 2017; Esser et al., 2021] is pro-\nposed and has emerged as the predominant method in the\nfield. The core idea involves assigning the learned continu-\nous representations to the codebook's index and employing\nthe Straight-Through Estimator [Bengio et al., 2013] for ef-\nfective optimization. The formulation is as follows. fe, fd,\nfq denote the encoder, decoder, and quantization process, re-\nspectively. C\u2208 RM\u00d7d is the codebook representation with\ndimension d, which corresponds to M discrete codewords\nC = {1,2,\u2026\u2026, M}. G = {V,E, X, Y} is the graph, where\nV is the node set with size n and E is the edge set. The pres-\nence of X or Y is not guaranteed, where X \u2208 Rnxd is the\ninput feature of a graph and Y = {Y1, Y2,} is the label\nset of each node. X \u2208 Rn\u00d7d is the latent representation af-\nter encoder and X \u2208 Rn\u00d7d is the reconstruction feature after\nquantization and decoder, i.e., X = fe(X), X = fa(fq(X))."}, {"title": "Algorithm 1: Residual Vector Quantization (RVQ).", "content": "Specifically, the VQ strategy yields a nearest-neighbor\nlookup between latent representation and prototype vectors\nin the codebook for quantized representations:\n$f_{q}(X_{i}) = C_{m}, m = \\arg \\min_{j \\epsilon C}||X_{i} - C_{j}||_{2}$,\nindicating that node i is assigned codeword cm. Further, the\nmodel can be optimized by the Straight-Through Estimator:\n$L_{vq} = \\frac{1}{n} \\sum_{i=1}^{n} ||sg[X_{i}] - C_{m} || + \\beta ||sg[C_{m}] - X_{i}||$,\nwhere sg represents the stop-gradient operator. \u03b2 is a hyper-\nparameter to balance and is usually set to 0.25. In this loss,\nthe first codebook loss encourages the codeword vectors to\nalign closely with the encoder's output. The second term of\nthe commitment loss aids in stabilizing the training process\nby encouraging the encoder's output to adhere closely to the\ncodebook vectors, thereby preventing excessive deviation.\nUsing VQ techniques, a continuous vector can be quan-\ntized to the counterpart of a discrete code. However, there\nwould be distinctions and differences in the vector pre and\npost-quantization. To address it, Residual Vector Quantiza-\ntion (RVQ) [Lee et al., 2022] is proposed to constantly fit\nresiduals caused by the quantization process. The whole pro-\ncess can be seen in Algorithm 1. RVQ is actually a multi-\nstage quantization method and usually has multiple code-\nbooks. The codebook stores the residual values of the quan-\ntized vector and the original vector at each step, and the orig-\ninal vector can be approximated infinitely by quantizing the\nresidual values at multiple steps."}, {"title": "Finite Scalar Quantization (FSQ)", "content": "Because of the intro-\nduction of the codebook, there would be complex technique\ndesigns and codebook collapse in VQ techniques. Thus,\nFSQ [Mentzer et al., 2024] introduces a very simple quan-\ntized strategy, i.e., directly rounding to integers rather than\nexplicitly introducing parameters of the codebook. FSQ\nprojects the hidden representations into a few dimensions\n(usually fewer than 10) and each dimension is quantized into\na limited selection of predetermined values, which inherently\nformulates a codebook as a result of these set combinations.\nIt is formulated as:\n$FSQ(X_{i}) = R[(L \u2212 1)\\sigma(X_{i})] \\epsilon \\{0,1,\u2026\u2026\u2026, L \u2212 1\\}$,\nwhere R is the rounding operation and L\u2208 N is the number\nof unique values. The process of rounding simply adjusts a\nscalar to its nearest integer, thereby executing quantization in\nevery dimension. o is a sigmoid or tanh function. Its loss is\ncalculated by also using the Straight-Through Estimator:\n$L_{fsq} = \\frac{1}{n} \\sum_{i=1}^{n} (L-1)\\sigma(X_{i}) + sg[R[(L\u22121)\\sigma(X_{i})]\u2212(L\u22121)\\sigma(X_{i})]$.\nFSQ has the advantages of better performance, faster con-\nvergence and more stable training, particularly when dealing\nwith large codebook sizes."}, {"title": "Anchor Selection and Assignment (ASA)", "content": "It is typically\nunsupervised and often employs prior strategies to identify\ninformative nodes within the graph, e.g., the Personalized\nPageRank [Page, 1999] and node degree. Consequently, each\nnode can be attributed to a combination of anchors that ex-\nhibit structural or semantic relationships, like using the short-\nest path. This form of quantization designates the code as a\nreal node, unlike the aforementioned three methods."}, {"title": "3 Quantized Graph Learning Framework", "content": "We summarize the main studies of QGR in Table 1 from the\nprimary perspectives of graph types, quantization methods,\ntraining strategies, and applications. Additionally, we also\nhighlight if there are in pipeline and self-supervised manners,\nas well as whether they learn code dependence and integrate\nwith language models.\nThe general process of QGR is illustrated in Figure 2,\nwhich comprises the encoder fe, decoder fd, and quantiza-\ntion process fq. The encoder fe is to model the original graph\nstructure into latent space, where MLPs [He et al., 2020] and\nGNNs [Xia et al., 2023] are usually utilized. The decoder fa\nalso commonly utilizes GNNs for structure embedding and\nthe quantization process fa implements the strategies outlined\nin Section 2.2. In particular, Bio2Token [Liu et al., 2024a]\nutilizes Mamba [Gu and Dao, 2023] as both encoder and de-\ncoder. UniMoT [Zhang et al., 2024] leverages the pre-trained\nMoleculeSTM molecule encoder [Liu et al., 2023a] to con-\nnect the molecule graph encoder and causal Q-Former [Li et\nal., 2023b]. In the following sections, we will delve into the\ndetails of the model design."}, {"title": "3.1 Quantized Strategies", "content": "Except for SNEQ [He et al., 2020] and d-SNEQ [He et al.,\n2023], the majority of research tends to employ VQ-related\nstrategies. Beyond the general techniques in VQ, Dr.E [Liu et\nal., 2024b] utilizes RVQ in one specific layer, which is called\nintra-layer residue. For more effective in encoding the struc-\ntural information of the central node, it involves preserving\nthe multi-view of a graph. Specifically, between GCN layers,\nthe inter-layer residue is carried out to enhance the represen-\ntations:\n$h_{v}^{l+1} = \\sigma (W \\cdot Con(h_{v}^{l} + Pool(\\{c_{v}^{l,k}\\}_{k=1}^{K}), h_{v}^{l}))$,\nwhere h and c denote the latent representations and quantized\nones. K is the number of learned codes for each layer. Using\nthis process, the discrete codes of each node are generated\nin a gradual (auto-regressive) manner, following two specific\norders: from previous to subsequent, and from the bottom\nlayer to the top. This approach guarantees comprehensive"}, {"title": "3.2 Training Objectives", "content": "In this section, we will delve deeper into training details from\nthe perspectives of node, edge, and graph levels.\nNode Level. Feature reconstruction usually serves as a sim-\nple self-supervised method for QGR learning, which is to\ncompare the original node features with reconstructed fea-\ntures based on the learned codes. It mainly has the following\ntwo implementation types:\n$L_{fr} = \\frac{1}{n} \\sum_{i=1}^{n} (1 - \\frac{X_{i} \\cdot \\hat{X}_{i}}{||X_{i}|| \\cdot ||\\hat{X}_{i}||}), L_{fr} = \\frac{1}{n} \\sum_{i=1}^{n} ||X_{i} - \\hat{X}_{i}||^{2}$,\nThe first involves employing cosine similarity with scaled\nparameter \u03b3 > 1, as implemented in Mole-BERT and VQ-\nGraph. The second is the mean square error as in Dr.E.\nNID [Luo et al., 2024] utilizes GraphMAE [Hou et al.,\n2022] for self-supervised learning, which can be viewed as a\nmasked feature reconstruction. It involves selecting a subset\nof nodes, masking the node features, encoding by a message-\npassing network, and subsequently reconstructing the masked\nfeatures with a decoder.\nEdge Level. It is a primary strategy to acquire the graph\nstructures in QGR codes using a self-supervised paradigm.\nThe direct link prediction, i.e., edge reconstruction, is a"}, {"title": null, "content": "widely adopted technique to assess the presence or absence\nof each edge before and after the reconstruction process as:\n$L_{lp} = ||A \u2013 \\sigma(\\hat{X} \\cdot \\hat{X}^{T}) ||^{2}$,\n$L_{lp} = - \\frac{1}{\\varepsilon} \\sum_{j=1}^{\\varepsilon} [y_{j} log(\\hat{y}_{j}) + (1 \u2212 y_{j}) log(1 \u2013 \\hat{y}_{j})]$,\nwhere the mean square error (e.g., VQGraph) and the binary\ncross-entropy are utilized to optimize (e.g., Dr.E), respec-\ntively. \u0177 is the prediction of probability for the existence\nof the edge based on the learned representations. yj \u2208 {0,1}\nis the ground label.\nBeyond direct edge prediction, He et al. [2020] proposes\nhigh-level connection prediction between two nodes, i.e.,\nstructural connection, and semantic connection. The former\naims to accurately maintain the shortest distances & within the\nrepresentations:\n$L_{stc} = \\frac{1}{n} \\sum_{(i,j,k)} max(D_{i,j} \u2013 D_{i,k} + d_{i,j} \u2013 d_{i,k}, 0)$,\nwhere D is to calculate the distance of two nodes based on\nthe learned codes. Considering that nodes sharing identical\nlabels ought to be situated closer together within the embed-\nding space, semantic connection is proposed:\n$L_{sec} = \\frac{1}{n \\cdot T} \\sum_{i,j=1}^{n} \\sum_{i=1}^{T} (D_{i,j} - S_{i})^{2}$,\nwhere T is the sample size. S presents the constant semantic\nmargin, which is set to 0 if the labels of two nodes are totally\ndistinct, otherwise, it is set to a constant.\nGraph Level. For high-level representations, the graph\nmodeling targets are proposed. For instance, graph con-\ntrastive learning is a widely utilized method for modeling re-\nlationships between graph pairs:\n$L_{gcl} = \\frac{1}{GED} log \\frac{e^{sim(h_{g1},h_{g2})/\\tau}}{\\sum_{g^{\\prime} \\epsilon B}e^{sim(h_{g1},h_{g^{\\prime}})/\\tau}}$,\nwhere D is the dataset that contains all graphs and hg is\nthe graph representation based on fq(X). g1 and g2 can be"}, {"title": "3.3 Distinctive Designs", "content": "Codebook Design. Beyond randomly setting the codebook\nfor updating, there are several specifically designed strategies\nfor domain knowledge adaption and generalization. For ex-\nample, Dr.E [Liu et al., 2024b] sets the vocabulary of LLaMA\nas the codebook, realizing token-level alignment between"}, {"title": null, "content": "GNNs and LLM. In this way, each node of graphs can be\nquantized to a permutation of language tokens and then can\nbe directly input to LLMs to make predictions. When mod-\neling molecular graphs, Mole-BERT [Xia et al., 2023] cate-\ngorizes the codebook embeddings into various groups, each\nrepresenting a distinct type of atom. For instance, the quan-\ntized codes of carbon, nitrogen and oxygen are confined to\ndifferent groups.\nTraining Pipelines. Aside from the single-stage learning\nprocess applied to QGR and particular tasks, some meth-\nods incorporate multiple stages into the training framework.\nGQT [Wang et al., 2024] modulate the learned codes through\nhierarchical encoding and structural gating, which are subse-\nquently fed into the Transformer network and aggregate the\nlearned representations through an attention module. Based\non the learned quantized representation, NID [Luo et al.,\n2024] trains an MLP network for downstream tasks, such as\nnode classification and link prediction."}, {"title": "3.4 KG Quantization", "content": "Recently, several KG quantization methods have been pro-\nposed for effective embedding, responding to the increasing\ndemand for larger KGs. Unlike the techniques employed in\nthe HOG or HEG setting, these methods tend to leverage an\nunsupervised paradigm for quantization. NodePiece [Galkin\net al., 2022], EARL [Chen et al., 2023], and random entity\nquantization (RandomEQ for short) [Li et al., 2023a] are all\nin such a framework. They first select a number of entities\nas anchors and then utilize the structural statistical strategy\nto match them for each entity. Specifically, NodePiece em-\nploys metrics such as Personalized PageRank [Page, 1999]"}, {"title": "3.5 Application Scenarios", "content": "General Graph Tasks. Based on the learned quantized\ncodes, many general graph tasks can be addressed, for ex-\nample, node classification, link prediction, graph classifica-\ntion, graph regression, and graph generation. In addition,\nSNEQ [He et al., 2020] can be employed for node recom-\nmendation that ranks all nodes based on a specific distance\nmetric and suggests the nearest node. d-SNEQ [He et al.,\n2023] is further utilized for path prediction that predicts the\npath (e.g., the shortest path) between two nodes.\nBased on the learned structure-aware codes for nodes, VQ-\nGraph [Yang et al., 2024] enhances the GNN-to-MLP distil-\nlation by proposing a new distillation target, namely soft code\nassignments, which utilizes the Kullback\u2013Leibler divergence\nto make two distributions (codes distributions by GNN and\nMLP) be close together. This can directly transfer the struc-\ntural knowledge of each node from GNN to MLP. The re-\nsults show it can improve the expressiveness of existing graph\nrepresentation space and facilitate structure-aware GNN-to-\nMLP distillation. In the KG scenarios, NodePiece [Galkin\net al., 2022], EARL [Chen et al., 2023], RandomEQ [Li et\nal., 2023a], SSQR [Lin et al., 2025a] all can be used for KG\nlink prediction, which is a ranking task that to predict the ob-\nject entity based the given subject entity and the relation, i.e.,\n(s, r,?). Using the learned codes as features, SSQR can also\nfix the triple classification problem, predicting the validity of\nthe given triple (s, r, t).\nMolecular Tasks & AI for Science. Recently, QGR meth-\nods have been widely used for molecular tasks and AI for\nscience scenarios, including molecular property prediction\n(classification and regression), molecule-text prediction, and\nprotein & RNA reconstruction. Specifically, DGAE [Boget\net al., 2024a] conduct the graph generation. It first itera-\ntively samples discrete codes and then generates the graph\nstructures by the pre-trained decoder, which can be used in\nthe generation for molecular graphs and has the potential for\ndrug design, material design and protein design. LLPS [Gau-\njac et al., 2024] shows the potential for the reconstruction of\nprotein sequences by training a de novo generative model for\nprotein structures using a vanilla decoder-only Transformer\nmodel. Bio2Token [Liu et al., 2024a] conducts efficient rep-\nresentation of large 3D molecular structures with high fi-\ndelity for molecules, proteins, and RNA, holding the potential\nfor the design of biomolecules and biomolecular complexes.\nUniMoT [Zhang et al., 2024] unifies the molecule-text ap-\nplications, including molecular classification & regression,\nmolecule captioning, molecule-text retrieval, and caption-\nguided molecule generation. It demonstrates the impressive"}, {"title": null, "content": "and comprehensive potentials that arise from the combination\nof QGR and LLMs."}, {"title": "4 Code Dependence Learning", "content": "Inspired by the distribution learning of the discrete codes in\ncomputer vision field [Esser et al., 2021] that predicts the\nnext code by the auto-regressive Transformer, QGR methods\nalso implement this technique, facilitating comprehensive se-\nmantic modeling and supporting the generation tasks. Simi-\nlar to BERT [Devlin et al., 2019] pre-training style, MOLE-\nBERT adopts a strategy similar to Masked Language Mod-\neling (MLM). It employs this method to pre-train the GNN\nencoder by randomly masking certain discrete codes. Subse-\nquently, it pre-trains GNNs to predict these masked codes, a\nprocess known as Masked Code Modeling (MCM):\n$L_{mcm} = - \\sum_{GED} \\sum_{j \\epsilon M}logp(c_{j}|G_{M})$,\nwhere M is the set of masked nodes in the graph and cj is the\nquantized codes. Auto-regressive code dependence learning\nis another mainstream strategy (usually using Transformer)\nbased on next token prediction:\n$L_{ntp} = \\frac{1}{N} \\prod_{j=1}^{N} P_{\\theta} (c_{j}/c_{1}, c_{2}, ..., c_{j\u22121})$,\nwhere network Pe is with parameter 0. N is the length of\nthe code sequence c. DGAE [Boget et al., 2024a] splits\nthe latent representation of each node into C parts. So af-\nter the quantization, each graph can be represented as n \u00d7 C\ncode permutation. To learn the dependence of it, the 2D\nTransformer [Vaswani et al., 2017] is introduced to auto-\nregressively generate the codes, i.e., the joint probability can\nbe $ \\prod_{i=1} ^{n} \\prod_{j=1} ^{C} P_{\\theta}(c_{i,j}/c_{1},***, c_{C<i,1,***, Ci,<j})$.\nMoreover, GLAD [Boget et al., 2024b] implements diffu-\nsion bridges [Liu et al., 2023b] for codes' dependence, where\nBrownian motion is utilized as a non-conditional diffusion\nprocess defined by a stochastic differential equation (SDE).\nBased on the learned model bridge, discrete practical codes\ncan be acquired by the iterative denoising process from the\nrandom Gaussian noise. Followed by the pre-trained decoder,\nthese codes can be used for the graph generation. Although\nDr.E does not directly or explicitly learn the dependence of\nthe codes, it introduces intra-layer and inter-layer residuals to\ngradually generate the codes, which enhances the represen-\ntation of sequential information as the newly generated code\ndepends on the previously obtained codes. It can be viewed\nas an implicit auto-regressive manner."}, {"title": "5 Integration with LLMs", "content": "The quantized codes, being discrete, share a similar structure\nwith natural language. As such, QGR methods can be seam-\nlessly incorporated with LLMs to facilitate robust modeling\nand generalization as shown in Figure 3. Table 2 provides\nexamples of tuning instructions for both Dr.E and SSQR, ar-\nranged for better understanding and intuitive interpretation.\nDr.E implements token-level alignment between GNNs\nand LLMs. Based on the quantized codes through intra-\nlayer and inter-layer residuals, the preservation of multiple"}, {"title": "6 Discussions and Future Directions", "content": "Beyond the above methods, there are also some methods that\ndoes not use the quantized code for the final representation,\nsuch as VQ-GNN [Ding et al., 2021], VQSynery [Wu et al.,\n2024], and MSPmol [Lu et al., 2024]. For example, instead\nof employing discrete motif features as the ultimate represen-\ntation, MSPmol [Lu et al., 2024] utilizes the Vector Quan-\ntization (VQ) method on motif nodes within specific lay-\ners of the molecular graph. There are also benefits for the\nrepresentation, which can be viewed as a paradigm of con-\ntiguous+quantized with the network, rather than the outputs.\nThey differ from the studies in Table 1, so we do not include\nthem in this survey.\nDespite some achievements of the QGR methods, there are\nalso some shortcomings from both methodology and applica-\ntion perspectives. First, the quantization process may result in\nthe loss of some details of the original data, which could pose\nchallenges in areas that require accurate modeling. Second,\nthe quantization process can require complex techniques to\nensure effective and efficient optimization, while pinepine's\nschema and integration with LLMs add additional complex-\nity to the overall framework. Third, compared with the wide\napplication and success of general LLMs, the research on the\nintegration of QGR and LLMs is still lacking. Thus, the direc-\ntion of future development lies in the following four aspects.\nChoice of Codeword. For QGR learning, one can incorpo-\nrate certain domain knowledge into the construction of the\ncodebook. This knowledge might encompass semantic con-\ncepts, structural formations, and textual language. Such an\napproach can augment the interpretability of the whole frame-\nwork, thereby facilitating a better human understanding.\nAdvanced Quantized Methods. GQR can adopt the latest\nadvanced quantized methods to enhance the efficiency and\nefficacy of learning graph codes, which may have already\ndemonstrated success within the domain of computer vision.\nFor instance, the rotation trick [Fifty et al., 2024] is a proven\nmethod to mitigate the issues of codebook collapse and un-\nderutilization in VQ. SimVQ [Zhu et al., 2024] simply incor-\nporates a linear transformation into the codebook, yielding\nsubstantial improvement.\nUnified Graph Foundation Model. While the GQR meth-\nods have indeed made some notable strides, the current re-\nsearch primarily follows a similar approach that tackles var-\nious tasks using distinct training targets, resulting in mul-\ntiple distinct models. This could restrict its applicability,\nparticularly in the age of LLMs. Drawing inspiration from\nthe remarkable success of unified LLMs and multimodal\nLLMs [Zhao et al., 2023; Lin et al., 2025b], the unified graph\nfoundation model could be realized to execute various tasks\nacross diverse graphs through QGR, by learning unified codes\nfor different graphs and then tuning with LLM techniques.\nGraph RAG with QGR. Retrieval-Augmented Generation\n(RAG) has emerged as a significant focal point for improv-\ning the capabilities of LLMs within specific fields. Graph\nRAG [Edge et al., 2024] would be beneficial for downstream"}]}