{"title": "Reinforcing Competitive Multi-Agents for Playing 'So Long Sucker'", "authors": ["Medant Sharan", "Chandranath Adak"], "abstract": "This paper explores the application of classical deep reinforcement learning (DRL) algorithms\u2014DQN, DDQN, and Dueling DQN-to the strategy game So Long Sucker (SLS), a complex, diplomacy-driven game characterized by coalition formation and strategic betrayal. Unlike other strategy games, SLS presents unique challenges, as players must navigate both adversarial and cooperative dynamics, making it a compelling testbed for multi-agent learning and game theory research.\nOur primary objective is to teach autonomous agents the fundamental rules and strategies of SLS using classical DRL methods. To facilitate this, we developed a novel, publicly available implementation of SLS, which includes a graphical user interface (GUI) and benchmarking capabilities for DRL algorithms. Experimental results show that, despite being considered rudimentary in comparison to contemporary DRL techniques, DQN, DDQN, and Dueling DQN agents attained approximately half of the maximum possible game reward, which indicates a significant, albeit suboptimal, understanding of SLS. These results demonstrate that classical DRL algorithms enable agents to make more legal moves than illegal ones on average, suggesting a baseline proficiency in the game's mechanics.\nHowever, a key limitation observed was the extended training duration required for agents to achieve near-optimal rewards. Unlike human players, who can understand the game within a few rounds, the DRL agents required approximately 2000 games to reach peak performance, and even then, they occasionally made illegal moves. This highlights both the potential and constraints of classical DRL methods when applied to semi-complex games with social and adversarial elements.\nIn conclusion, our study provides a foundation for training agents in SLS and similar negotiation-based environments, emphasizing the need for more advanced or hybrid DRL techniques to improve learning efficiency and adaptability. Future work could explore the integration of game-theoretic strategies to enhance agents' decision-making processes in dynamic, multi-agent contexts.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) is a critical area of machine learning that equips artificial agents with the capability to make optimal decisions in dynamic environments by maximizing cumulative rewards. Unlike supervised and unsupervised learning methods, which rely on extensive labeled datasets to achieve high performance, RL allows agents to learn autonomously through iterative interactions with the environment. This characteristic makes RL particularly well-suited for complex decision-making tasks that involve sequential and strategic planning.\nMany studies in agent training focus on RL techniques that provide policies for achieving cooperative or competitive goals. At each step in the learning process, an agent observes the state of the environment and selects an action, causing a transition to a new state. This mechanism becomes more intricate in multi-agent environments, especially in zero-sum games or competitive settings where agents must consider adversarial strategies. In such contexts, principles like the minimax approach are employed, where each agent aims to maximize its own payoff while assuming that opponents are trying to minimize it. Algorithms like minimax-Q leverage this principle, combining strategy computation with temporal-difference learning to navigate state transitions.\nThe field of multi-agent reinforcement learning (MARL) extends RL concepts to scenarios involving multiple interacting agents, each with its own objectives. These agents can exhibit cooperative or adversarial behavior, and modeling such complex interactions often requires incorporating game-theoretic principles. The integration of deep reinforcement learning (DRL) has further advanced MARL, enabling efficient handling of high-dimensional state and action spaces through the use of neural networks. This combination has led to significant breakthroughs, exemplified by state-of-the-art systems like AlphaGo and AlphaStar [1], which exhibit strategic mastery in games that demand deep foresight and complex reasoning.\nSo Long Sucker (SLS) [2] is a strategy game centered on diplomacy, coalition formation, and betrayal, making it a compelling testbed for multi-agent deep reinforcement learning (DRL) research. Drawing on classic game-theoretic constructs, such as the prisoner's dilemma, SLS emphasizes trust, negotiation, and strategic deception. Despite the game's rich strategic landscape, no previous research has systematically tackled the challenge of teaching autonomous agents to play SLS using computational techniques or machine learning models.\nOur work aims to address this gap by training autonomous agents to learn the rules and develop effective strategies in SLS through classical DRL algorithms, including DQN, DDQN, and DuelingDQN. To support this research, we have developed a publicly available version of SLS, complete with a graphical user interface (GUI), to facilitate benchmarking of these classical DRL algorithms. This study represents a foundational effort toward creating negotiation-capable agents, demonstrating how multi-agent reinforcement learning (MARL) techniques can be applied to address the exploration-exploitation dilemma in a multi-agent, game-theoretic environment by using SLS as a case study.\nOur contributions are threefold:\n\u2022 We develop a computational framework for training autonomous agents in the game of So Long Sucker.\n\u2022 We introduce novel self-playing agents for SLS that benchmark the performance of classical DRL algorithms within this environment, analyzing their effectiveness in strategic and adversarial gameplay.\n\u2022 We make our novel version of the game (which is streamlined for machine learning endeavors) publicly available, providing a platform for future research on negotiation and coalition formation in MARL settings.\nThe remainder of this paper is organized as follows. In Section II, we review the existing literature on multi-agent reinforcement learning (MARL) and game-theoretic approaches to strategic gameplay, providing a contextual background for our work. Section III outlines the computational framework and reinforcement learning methodologies applied to train autonomous agents in the So Long Sucker environment. Section IV presents a detailed analysis of experimental results, including a comparative performance evaluation of DQN, DDQN, and Dueling DQN agents, highlighting their learning trajectories and strategies. Finally, Section V concludes the paper, summarizing our contributions and discussing potential avenues for future research in this area."}, {"title": "II. BACKGROUND", "content": "So Long Sucker (SLS) is a strategic board game developed within the framework of game theory, emphasizing concepts such as coalition formation and strategic betrayal. The original version of SLS incorporates deal-making and negotiations, making the gameplay non-sequential and significantly increasing the complexity of modeling strategic interactions. For our study, we implement a simplified version of the game that enforces sequential gameplay and omits negotiations, which better aligns with the requirements of multi-agent reinforcement learning frameworks. Notably, this simplification does not diminish the core adversarial and strategic dynamics, as it is entirely feasible to engage in the game without concurrent negotiations or chip transfers, thus preserving the original strategic intent."}, {"title": "A. Game Variant", "content": "The analysis in this paper is centered on a variation of SLS referred to as the Generalized Hofstra's version [3], [4]. In this formulation, four players are designated by distinct colors (typically blue, green, red, and yellow) and each begins the game with a fixed number of chips, denoted as c\u2208 N. The game is played on a board with a predetermined number k \u2208N of empty piles, where a pile is defined as an ordered stack of chips. The primary objective is to be the last player remaining, though the number of chips held by a player is not necessarily indicative of their likelihood of winning. Interestingly, it is possible for a player to achieve victory even when they have no chips left in their possession. Also, this new version introduces a slight variation in rules II-B3 to handle cases where the backtracking occurs until failure to find an existing player who can play. This nuance was surprisingly overlooked in all previous studies done on SLS.\nThe Generalized Hofstra's version employs a progressive scoring system, where players earn increasingly higher scores based on their order of elimination, specifically {1,2,3,4}, with the last player receiving the highest score. However, for the purposes of this research, we propose and investigate a novel variation called the Zero-Sum Generalized Hofstra's version. In this modified version, the scoring structure is altered to {0,0,0, s}, where s \u2208 N and s > 0.\nThis zero-sum approach is specifically designed to simplify reward structures, thereby facilitating faster convergence of deep reinforcement learning agents. The standard progressive scoring mechanism can lead to ambiguities and extend convergence times, which is suboptimal for the performance of simple DRL algorithms."}, {"title": "B. Rules of Zero-Sum Generalized Hofstra's SLS", "content": "1) Starting the Game:\n\u2022 Each player is assigned a unique color and starts with 5 chips of that color. The colors are mutually exclusive among the players.\n\u2022 A randomly selected player begins the game by placing a chip onto the playing area and designating another player to make the subsequent move.\n2) Gameplay Mechanics:\n\u2022 During a turn, a player places a chip of any color on the board, either initiating a new pile or adding to an existing stack. The game can have a maximum of 6 active piles at any given time.\n\u2022 If no chips are captured, the player must choose the next participant, ensuring that the selected player's starting chip color is not present in the current pile. If all colors are represented, the next player is determined by the chip of their color that is deepest within the stack.\n\u2022 A pile is captured when two consecutive chips of the same color are played. The player associated with that color must remove (or \"kill\") one chip from the pile, and the remaining chips are taken by the capturing player, who then makes the next move.\n\u2022 Chips held by a player other than the original owner are considered prisoners.\n\u2022 Since negotiations are not a part of our framework, a player is defeated if it is their turn to move and they have no available chips.\n\u2022 If a defeated player's chips capture a pile, the entire pile is eliminated, and the move rebounds to the player who made the capture.\n3) 3. Order of Play:\n\u2022 If a pile is captured, the player whose chip triggered the capture takes the next turn.\n\u2022 If a player is eliminated, the move reverts to the player who passed the turn to them. This backtracking continues until a valid player is found. If necessary, a random assignment is made to ensure continuity."}, {"title": "B. State, Action, and Reward Definition", "content": "We define the game environment mathematically with the state, action, and reward as follows:\na) State (St): The state $s_t$ at time step t captures all relevant information about the current game environment. It is structured as a vector with specific components representing the game board, player chips, eliminated chips, the current player, game phase, and step count:\n$S_t = \\{Board State, Player Chips, Dead Chips, \\\\\n Current Player, Game State, Steps\\}$\n(2)\nThe overall observation space size, denoted as total_obs_size, is computed as follows:\ntotal_obs_size = num_rows \u00d7 num_players \u00d7 max_pile_size+\nnum_players\u00b2 + 2 \u00d7 num_players + 4 + 1\n(3)\nEach term in this equation corresponds to a specific component of the state:\n\u2022 Board State: num_rows \u00d7 num_players \u00d7 max_pile_size.\nThis term represents the current state of the game board.\n\u2022 Player Chips: num_players\u00b2. This term keeps track of the chips each player currently holds.\n\u2022 Dead Chips: num_players. This term represents the chips belonging to players who have been eliminated.\n\u2022 Current Player: num_players. This term indicates which player is currently active."}, {"title": "B. State, Action, and Reward Definition", "content": "b) Action (at): The action $a_t$ taken by an agent at time step t is chosen from the action space A, defined as:\n$A = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}$\n(4)\nThe action space includes 10 discrete actions, categorized as follows:\n\u2022 Actions {0,1,2,3,4,5} correspond to the selection of rows and are valid only in the choose_pile state. Each action in this range selects one of the six rows available.\n\u2022 Actions {6,7,8,9} correspond to the selection of players and are valid in the choose_chip, choose_next_player, and eliminate_chip states. Each of these four actions corresponds to a specific player, allowing for targeted choices during these phases.\nc) Reward (rt): The reward function rt is designed to encourage agents to make beneficial moves while penalizing illegal actions. Specifically:\n\u2022 A reward of -5 is given for every illegal move, discouraging agents from violating game rules.\n\u2022 For each positive (legal) move, the agent receives an initial reward of 5. However, this reward is dynamically adjusted to account for the number of chips and steps taken by the agent.\nSince the number of chips is set to a constant of 5 in our experiments, the reward adjustment is primarily influenced by the steps taken. The adjusted reward function is defined as:\n$r_t = min (5, \\frac{\u03b1}{steps})$\n(5)\nwhere the adjustment parameter \u03b1 is given by:\n$\\alpha = \\frac{15}{chips \u00d7 \u03b2}$\n(6)\nIn this setup:\n\u2022 chips = 5, which is constant in our experiments.\n\u2022 \u03b2 = 50, a fixed scaling parameter to regulate the rate of reward decay.\nThe purpose of this dynamic reward adjustment is to incentivize the agents to complete their tasks with fewer steps. As the number of steps increases, the adjusted reward decreases, encouraging efficiency in gameplay and discouraging unnecessary actions."}, {"title": "C. Cumulative Learning Framework", "content": "We employ a cumulative learning [5] approach, where all four agents share a common learning network, such as a DQN. The centralized learning scheme allows the agents to:\n\u2022 Share experiences through a collective experience buffer.\n\u2022 Update the shared policy network, benefiting from more diverse and comprehensive data."}, {"title": "D. Pseudocode for Agent-Based Gameplay", "content": "The pseudocode is presented in Algorithm 1."}, {"title": "IV. EXPERIMENTS AND DISCUSSION", "content": "In our experiments, we utilized three foundational deep reinforcement learning (DRL) algorithms\u2014DQN, DDQN, and Dueling DQN-alongside a baseline random agent. The agents were implemented using TensorFlow, employing a simple feed-forward neural network architecture to approximate the Q-values. The primary hyperparameters used for training each agent are as follows:\n\u2022 Discount Factor (\u03b3): 0.95\n\u2022 Initial Exploration Rate (\u20ac): 1.0\n\u2022 Exploration Decay Rate ($E_{decay}$): 0.995\n\u2022 Minimum Exploration Rate ($E_{min}$): 0.01\n\u2022 Learning Rate: 0.001\n\u2022 Batch Size: 64"}, {"title": "B. Base Model Architecture", "content": "Each agent's neural network follows a simple structure, described as follows:\n\u2022 Input Layer: Accepts the state vector of size state_size\n\u2022 Hidden Layers: Two fully connected layers, each with 64 neurons and ReLU activations\n\u2022 Output Layer: A fully connected layer with linear activation, producing action_size outputs representing the Q-values for each action\nThe model is compiled with Mean Squared Error (MSE) loss and the Adam optimizer with a learning rate of 0.001."}, {"title": "C. Reward Structure and Performance Analysis", "content": "The maximum attainable reward in a game of So Long Sucker (SLS) is approximately 200, which aligns with the reward function graph for our trained agents. This suggests that classical DRL algorithms like DQN, DDQN, and Dueling DQN effectively learn the rules and structure of SLS, despite its strategic complexity."}, {"title": "V. CONCLUSION", "content": "In this study, we explored the application of classical deep reinforcement learning (DRL) algorithms-namely DQN, DDQN, and Dueling DQN-to teach autonomous agents the rules and strategies of the diplomacy-driven strategy game So Long Sucker (SLS). Our results indicate that, despite the inherent complexity of SLS, these classical algorithms successfully learned to navigate the game's rules, achieving approximately half of the maximum possible reward. This performance implies that agents executed more legal moves than illegal ones on average, demonstrating a foundational understanding of the game.\nWhile our DRL agents were able to grasp SLS's rules and achieve satisfactory rewards after some training, their learning speed and adaptability still lag significantly behind human players, who can understand the game's rules within a few rounds [6]. The agents required around 2000 games to reach peak performance, and they continued to make occasional illegal moves even at their highest performance levels, highlighting limitations in their nuanced understanding of game context and adaptability.\nThe results demonstrate that although classical DRL algorithms can be effective in teaching agents the basics of SLS, these models may struggle with the game's more intricate strategic and context-specific decisions. This study serves as a foundational step in training autonomous agents for complex, negotiation-based environments. Future research may consider more advanced algorithms or hybrid approaches that combine DRL with game-theoretic principles, aiming to bridge the gap between the learning efficiency of humans and autonomous agents."}]}