{"title": "Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling", "authors": ["Shenghong He", "Chao Yu", "Qian Lin", "Yile Liang", "Donghui Li", "Xuetao Ding"], "abstract": "Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset. Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step t). However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution. In this paper, we propose a new MORL algorithm Reliability-guaranteed Transformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data). Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data. We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods on several benchmark tasks.", "sections": [{"title": "1 Introduction", "content": "Model-based offline reinforcement learning (MORL) [Kidambi et al., 2020] aims to learn a policy by leveraging a dynamics model derived from an existing dataset, without requiring interaction with the environment. Recent approaches often leverage uncertainty quantification [Yu et al., 2020; Wagenmaker et al., 2023; Bhardwaj et al., 2024] and value function penalization [Yu et al., 2021; Sun et al., 2023; Luis et al., 2023] to learn the dynamics model, ensuring that the data generated based on current information (e.g., state and action) align with the real data distribution.\nHowever, these methods implicitly assume that the current information fully provides all relevant information for data generation at the next time step. Thus, unreliable trajectories that may not exist in the real environment can be generated due to neglect in considering the impact of historical information on the environmental dynamics. For instance, considering a robot performing cleaning tasks in a complex room, at a historical time step t - k, the robot takes an action of moving a chair, causing a partial obstruction in the passage. At the current time stept, employing a generative model to generate trajectories could result in the robot passing through the chair due to the model's inability to remember the historical action of moving a chair. This generated trajectory is unreliable because the robot cannot pass through the chair in the real environment. Furthermore, by simply focusing on imitating the observed data transitions, existing methods generally fail to identify states and actions that lead to high cumulative returns, thus significantly impairing the learning performance.\nTo address the above issues, in this paper, we propose a novel method called Reliability-guaranteed Transformer (RT). Specifically, taking advantage of the sequence modeling capabilities of transformer, RT captures historical information between different positions in the input sequence to generate data (e.g., states, actions, and rewards). Subsequently, RT computes the variation distance between the generated data and the real data to derive a reliability value of the generated data for each time step, and then incorporates a truncation metric based on the cumulative reliability along the trajectory to adaptively determine the trajectory length, so as to mitigate the accumulation of errors and consequently enhances the overall reliability of the generated sequences. Moreover, RT estimates the likelihood of future high returns for candidate actions and uses these estimations as generation conditions to generate high-return trajectories. Our contributions are as follows:\n\u2022 We explore the role of leveraging historical information to generate reliable trajectories with high future returns to facilitate and stabilize policy learning in MORL.\n\u2022 We propose RT that enhances trajectory reliability by incorporating cumulative reliability into sequence modeling and continuously selects actions with high rewards to generate high-return trajectories.\n\u2022 Empirical results on various continuous control tasks demonstrate that our method achieves superior learning performance compared to existing baselines."}, {"title": "2 Preliminaries", "content": "MDPs and offline RL. A Markov Decision Process (MDP) can be defined as M = (S, A, R, P, \u03c1_0, \u03b3), where S is the state space, A is the action space, R(s,a) is the reward function, P(s'|s, a) is the transition function, \u03c1_0 is the the initial state distribution, and \u03b3\u2208 (0,1) is the discount factor. A policy \u03c0 : S \u00d7 A \u2192 [0,1] takes action a at state s with probability \u03c0(a|s). The goal of RL is to find the optimal policy \u03c0* that maximizes the expected return \u03c0* = argmax_\u03c0 E_\u03c0[\u03a3_{t=0}^T \u03b3^t R(s_t, a_t)]. In the offline RL setting, the agent only has access to a static dataset D_{env} = {(s,a,r,s')}. The agent's objective is to learn a policy without interaction with the environment for any additional online exploration.\nModel-based offline RL. Model-based offline RL methods use a dataset to learn the dynamics model P, which is usually trained by maximum likelihood estimation: min_P E_{(s,a,s')~D_{env}} [- log P(s'|s, a)]. Additionally, when the reward function is unknown, a model of the reward function R(s, a) can be trained. Once the dynamics model is trained, samples generated by P are then placed into the model buffer D_{model}, which is merged with the offline data D_{env} \u222a D_{model} to learn a policy. In this work, we assume that the transition function of the environment is determined and propose RT to augment the offline data."}, {"title": "3 Method", "content": "The overall process of RT is described in Algorithm 1. RT first learns a dynamics model from the offline dataset and establishes a threshold for the reliability of the offline data (lines 1 to 11). Next, RT randomly selects a trajectory from the offline dataset and generates a reverse trajectory from any state along that trajectory. This generated trajectory is then combined with the original data to create a new dataset, which is used to train the model-free algorithm (lines 12 to 21)."}, {"title": "3.1 Reliability-Guaranteed Sequence Generation", "content": "Inspired by previous sequence representation learning [Chen et al., 2021], we treat each input trajectory \u03c4 as a sequence and add reward-to-go R_t = \u03a3_{h=t}^T \u03b3^{h-t}r_h as an auxiliary information at each time step t, which acts as future heuristics for further generating data. Specifically, each trajectory \u03c4 = (s_1,a_1,r_1, R_1,\u22ef,s_T,a_T, r_T, R_T) is used as an input sequence for RT, which is trained using the standard teacher-forcing method [He et al., 2022], as expressed by:\nlog P_\u03b8(\u03c4_t| \u03c4_{<t}) = log P_\u03b8(s_t| \u03c4_{<t}) + log P_\u03b8(a_t|s_t, \u03c4_{<t}) + log P_\u03b8(r_t|a_t, s_t, \u03c4_{<t}) +log P_\u03b8(R_t| r_t, a_t, s_t, \u03c4_{<t}),\nwhere \u03c4_{<t} represents the trajectory from the initial state to time step t - 1, \u03b8 is the parameter of RT, and P_\u03b8 is the induced conditional probability. The training objective is to maximize:\nL_\u03b8 = \u03a3_{t=1}^T log P_\u03b8(\u03c4_t| \u03c4_{<t}).\nHowever, during the sequence generation, the model generation error at moment t is continuously accumulated by the subsequent generation process, which may cause the generated data to deviate from the distribution of the original data. In order to solve this issue, we propose a reliability estimation mechanism, which automatically determines the generated trajectory lengths based on the cumulative reliability along the trajectory and incorporates these reliability values into the pessimistic MDP [Blanchet et al., 2023] to provide performance bounds for the learned policy. We first define the cumulative reliability of trajectories and the truncation metric:\nDefinition 1 (Cumulative Reliability). Given a trajectory \u03c4, the cumulative reliability along the trajectory at step t is defined as:\n\u0393(s_t, \u03c4_{<t}) = D(P(\u00b7|s_i, \u03c4_{<i}), P(\u00b7|s_i, \u03c4_{<i}))\n= \u03a3_{i=1}^t (exp(e_i)/\u03a3_j exp(e_j)) \u00b7 DIST(P(\u00b7|s_i, a_i), P(\u00b7|s_i, a_i)),\nwhere P is the estimated transition probability of the environment after training, P is the true dynamics, e is an attention value, and DIST represents the total variation distance [Ho and Yeung, 2010] between two distributions P and P.\nDefinition 2 (Truncation Metric). Given a cumulative reli-"}, {"title": "ability I, the truncation metric at t as:", "content": "U_t = {0 (i.e., reliable) if \u0393(s_t, \u03c4_{<t}) < \u03b1, 1 (i.e., unreliable) otherwise,}\nwhere \u03b1 represents the cumulative threshold for generating reliable trajectories.\n\u0393 provides a metric of the cumulative reliability of the trajectories generated by the model, while U defines the truncation point of the generated trajectory. To directly quantify the impact of generation errors on policy learning, inspired by previous works [Kidambi et al., 2020; Blanchet et al., 2023; Zhang et al., 2023] that used pessimistic reward-based training policies, we integrate the reliability values into the pessimistic MDP framework. Specifically, the \u03b1-pessimistic MDP is defined by the tuple M_p = {S, A, R, P_p, \u03c1_p,\u03b3}, where S,A, \u03c1_p and \u03b3 retain the same definitions as in the original MDP (see Sec. 2). The transition function and reward function are dynamically updated and evaluated using the truncation metric, defined as follows:\nP(s'|s, \u03c4_{<t}) = {0 if U_t = 1, P(s'|s, a) otherwise,}\nR(s_t, a_t) = {0 if U_t = 1, r(s_t, a_t) - \u03b2\u00b7\u0393(s_t, \u03c4_{<t}) otherwise,}\nwhere \u03b2 is the reliability penalty hyperparameters for each state-action pair (s, a).\nBy introducing the \u03b1-pessimistic MDP, we incorporate trajectory generation errors into the reward function, ensuring that trajectories with larger errors receive smaller reward values. In this way, the policy learns a lower Q-value when encountering such trajectories, thereby becoming more conservative and robust [Kidambi et al., 2020; Kumar et al., 2020]. Moreover, If the truncation metric U_t = 1, indicating that the cumulative reliability \u0393at step t exceeds the threshold \u03b1, the trajectory generation process will be terminated.\nIn order to analyze the effect of integrating truncation metric into pessimistic MDP, we assume that r^ is the maximum reward value in \u03b1-pessimistic MDP, and then derive the performance bound between the \u03b1-pessimistic MDP policy and the real MDP policy.\nTheorem 1. Given \u03c0* as the optimal policy in the real MDP (M) and \u03b5_\u03c0 as the performance gap between the optimal policy and any policy in the \u03b1-pessimistic MDP (M_p), the performance lower bound of the policy \u03c0 learned by Algorithm 1 is:\nJ_{\u03c1_0} (\u03c0, M) \u2265 J_{\u03c1_0} (\u03c0^*, M) - E_\u03c0 [ (4\u03b3r^)/(1 - \u03b3)^2 \u00b7 \u03b1 - (4r^)/(1 - \u03b3) \u00b7 DIST(\u03c1_0, \u03c1_0) ]\n(7)"}, {"title": "performance lower bound for a policy in the real MDP. Simply put, if a policy learned is close to the optimal policy (i.e., \u03b5_\u03c0 is small) in the pessimistic MDP, then the policy is also close to the true optimal policy in the real MDP (see Appendix for detailed analysis).", "content": "However, the cumulative reliability and threshold \u03b1 cannot be directly derived through the calculation of DIST because P(\u00b7|s, \u03c4_{<t}) are typically unknown. To solve this issue, we employ a Variational Autoencoder (VAE) [Li and Yao, 2024; Zhao et al., 2024] to calculate the distribution error of (s, a, s') as a replacement for the total variation distance DIST(P(\u00b7|s, a), P(\u00b7|s, a)). Specifically, we use a neural network to learn an encoder and a decoder. The encoder maps x = (s, a, s') to a latent vector z, while the decoder reconstructs x from z. The conditional probability of the encoder is q(z|x), and the constrained probability distribution of the decoder is p(x|z). Then, the loss function of the VAE is expressed as:\nL(\u03c8, \u03c6) = -E_{z~q(z|x)} [log(p(x|z))] + D_{KL}(q(z|x)||p(z)),\nwhere \u03c8 is the encoder parameter, \u03c6 is the decoder parameter and D_{KL} is KL divergence. By learning the VAE, we identify the maximum reconstruction error in the offline data and set it as the threshold \u03b1. Next, the generated states and actions use the VAE to calculate the DIST, which is then used as a reliable value for the generated data to compute the cumulative reliability of the trajectory."}, {"title": "3.2 High-return Trajectory Generation", "content": "While the above method can generate more reliable trajectories, it does not ensure the generation of high-return trajectories. To address this issue, we use the reward at time t as a condition to guide the model to generate the action at t + 1, which is similar to the discriminator-guided generation method used in language models [Chen et al., 2024a]. Specifically, RT applies a binary classifier P(H+|\u06f0\u06f0\u06f0) to identify whether an action brings a high reward before taking an action at time t, and applies the Bayes' rule to approximate the high reward distribution P(R_t,...|H+) \u221d P(H+|R_t,...)P(R_t,...), where H_t denotes the high reward at time t. This probabilistic modeling [Lee et al., 2022] forms a simple autoregressive process in which R_t is first generated based on a high and reasonable log-probability, and actions are then generated according to P_\u03b8(a_t | R_t,...), which can be considered a variant of the class-conditional model [Keskar et al., 2019] that automatically takes actions with high rewards at each time step to generate high-return trajectories.\nTo further enhance the generation of high-return trajectories, RT employs a backward generation mechanism to generate trajectories by randomly selecting a state from the trajectory as the starting point and modeling the states, actions, and cumulative rewards in a backward sequence, thus ensures that the generated trajectories include the actual goal state\u00b9 typically associated with high rewards. Specifically, each trajectory \u03c4_{back} = (s_t,a_t,r_\u03c4, R_\u03c4,\u22ef,s_1,a_1,r_1, R_1) is used"}, {"title": "as an input sequence for RT, which is expressed as follows:", "content": "log P_\u03b8(\u03c4_t|\u03c4_{>t}) = log P_\u03b8(s_t|\u03c4_{>t}) + log P_\u03b8(a_t|s_t, \u03c4_{>t}) +log P_\u03b8(r_t|a_t, s_t, \u03c4_{>t}) +log P_\u03b8(R_t| r_t, a_t, s_t, \u03c4_{>t}),\nwhere \u03c4 denotes \u03c4_{back}, and \u03c4_{>t} represents the trajectory from time step t to the terminal state. The training objective is to maximize:\nL_\u03b8 = \u03a3_{t=1}^T log P_\u03b8 (\u03c4_{T-t+1}|\u03c4_{>T-t+1}).\nAfter the training process, the learned RT generates trajectories to augment the offline dataset D. For the original trajectories \u03c4\u2208 D in the original dataset, RT begins generating trajectories backward from time step T':\n\u03c4 = (\u2026\u2026, \u015d_{T-T'-1}, \u00e2_{T-T'-1}, r_{T-T'-1}, R_{T-T'-1} s_{T'},a_{T'},r_{T'},R_{T'},\u2026) ~ P_\u03b8(\u03c4_{<T-T'}|\u03c4_{>T-T'}),\nwhere \u03c4_{>T-T'} denotes the original trajectory truncated at time step T - T'. Then, the generated sequence is concatenated with the truncated original trajectory, resulting in the formation of a new trajectory \u03c4' = \u03c4_{<T-T'} + \u03c4_{>T-T'}, where + is the concatenation operation. By adopting this approach, we are able to augment a solitary high-return trajectory into the trajectories, consequently enriching the offline dataset."}, {"title": "4 Experiments", "content": "In this section, we first compare RT with existing mainstream offline RL methods across various domains in the D4RL benchmark [Fu et al., 2020], including Maze2D, MuJoCo and AntMaze, and then analyze the impact of different mechanisms or components of RT on its performance. Refer to the Appendix for more details about the environments."}, {"title": "4.1 Comparison Methods", "content": "Since our goal is to augment offline datasets to improve the performance of offline RL algorithms, we compare RT with recent augmentation algorithms, including ROMI [Wang et al., 2021], CABI [Lyu et al., 2022]), TATU [Zhang et al., 2023] and DStitch [Li et al., 2024]. In addition, in order to intuitively understand the performance difference between RT and notable offline RL algorithms, RT is compared to the following notable methods, including offline model-free algorithms (i.e., BCQ [Fujimoto et al., 2019], CQL [Kumar et al., 2020], IQL [Kostrikov et al., 2022], and DT [Chen et al., 2021]), as well as offline model-based algorithms (i.e., MOPO [Yu et al., 2020], MOREL [Kidambi et al., 2020] and MOPP [Zhan et al., 2022]. More description of the above methods is provided in the Appendix."}, {"title": "4.2 Results", "content": "Comparison to augment methods. ROMI, CABI, TATU, DStitch and RT are combined with model-free methods (e.g., BCQ, CQL and IQL) and evaluated on Hopper-medium, Walker-medium, sparse Maze2D-medium and diverse Antmaze-medium. The experimental results shown in Fig. 1 demonstrate that RT consistently outperforms ROMI, CABI and TATU. Moreover, both ROMI, CABI and TATU perform worse than IQL alone in the Antmaze-medium environment because their inability to generate high-return trajectories introduces a large amount of unrewarded data, leading to inaccurate policy constraints and then a suboptimal policy. Although DStitch can merge high-reward and low-reward trajectories, the presence of a high proportion of suboptimal data poses a significant challenge to its ability to find suitable trajectories for merging, as in the Maze2D environment. In contrast, RT employs sequential modeling to generate high-return trajectories starting from the goal state and utilizes reliability estimation to automatically determine the length of generated trajectories, resulting in more robust performance across different environments.\nNext, to evaluate the return of the trajectories generated by RT, we employ Behavioral Cloning (BC) [Torabi et al., 2018] to learn a policy from the generated data, as the effectiveness of BC is highly dependent on the quality and quantity of the high-return data. We train ROMI, CABI, TATU, DStitch and RT on the Walker-medium, Halfcheetah-medium, Antmaze-umaze, and Maze2D-umaze environments, and then select 50 trajectories from these environments to generate new trajectories by sampling each selected trajectory 10 times for learning BC. The results in Fig. 2 show that RT shows strong learning performance, while ROMI, CABI, TATU and DStitch exhibit large performance fluctuations in different environments, which can be attributed to the fact that ROMI, CABI, TATU and DStitch prioritize generating trajectories that adhere to the original data distribution without considering the returns of the generated trajectories, preventing BC from learning an effective policy from the generated trajectories. In contrast, RT takes into account both the reliability and high-return of the data when generating trajectories, which allows to achieve better learning performance across different environments.\nFurthermore, we employe T-SNE [Van der Maaten and Hinton, 2008; Chen et al., 2024b] visualization to compare the distribution of states generated by RT with the original states, and evaluate the rewards of each state using a value function. As shown in Fig. 3, the distribution of samples generated by RT is essentially consistent with that of the original data. However, when looking at the non-overlapping regions of the generated and original states, we find that the generated data has much higher values than the original state. Those out-of-distribution and high-value regions reflect the ability of RT to generate high-return trajectories (refer to the Appendix for results of other environments).\nComparison to notable offline methods. Table 1 shows the results of RT+BCQ compared with recent offline RL methods (refer to the Appendix for the full results). The experiments demonstrate that RT can significantly enhance the performance of the original algorithm and outperform all the baselines. It can be seen that the model-based methods have large performance fluctuations in different environments due to the failure in generating high-return trajectories from offline data; for instance, MOREL shows performance comparable to model-free methods in the MuJoCo environment but"}, {"title": "4.3 Further Analysis", "content": "Generated trajectory analysis. To better understand the effectiveness of RT in addressing unreliable trajectory problems, we construct a game called BoxBall, where the ball needs to move from the starting point to the goal state using up, down, left, and right actions while navigating around a wall in front of the goal state.\nIn this environment, we use DQN to collect a dataset and use it to train forward, backward, and RT models. Then, we use the trained forward model, backward model and RT to generate and visualize 50 trajectories. As shown in Fig. 4, the trajectory of the random strategy does not include the action of walking through the wall, while the trajectories generated by the forward model contain many unreliable trajectory (i.e., trajectories cross the wall) and fail to reach the goal state. The values of unreliable trajectories may be arbitrarily misestimated, and then the erroneous values may be propagated throughout the state-action space leading to overestimation of Q-values during training. Despite employing the same backward trajectory generation method, the backward model generates some unreliable trajectories due to cumulative errors and model lacks of ability to capture long-term dependencies. RT utilizes sequence modeling techniques to effectively learn the distribution of trajectories and employs reliability estimation to adaptively determine the generated trajectories, which ensures that actions involving passing through walls do not occur. A more detailed analysis of the generating reliable trajectory is discussed in the Appendix.\nForward and backward analysis. In order to explore whether backward generation is more advantageous than forward generation, we conduct forward and backward comparison experiments on Maze2D and Antmaze, where forward generation is donoted as FT. Table 2 shows the experimental results for BCQ, FT+BCQ, and RT+BCQ with 10 random seeds. As can be seen, FT+BCQ provides superior performance in most environments, but not as"}, {"title": "significant as RT+BCQ. For example, in Antmaze-large, where the maze area is extensive and the paths are complex, FT requires longer generation lengths to generate high-return trajectories, which in turn increases trajectory distribution error. In contrast, RT is the backward generation, which enables safe generalization across all layouts (i.e., RT can ensure that the generated trajectories contain goal states), thus ensuring RT+BCQ significantly outperforms FT+BCQ and BCQ.", "content": "Reliability estimation analysis. In order to explore the impact of reliability estimation on RT performance, we conduct experiments on Halfcheetah-medium-replay, Walker-medium-replay, dense Maze2D-larger and diverse Antmaze-large. RT is divided into two categories: fixed-length generation, with lengths set to 3 (RT-F3) and 5 (RT-F5), and reliability estimation. Fig. 5 illustrates the impact of different generation length mechanisms on the convergence of the IQL learning process. Both RT-F3 and RT-F5 can improve the performance of IQL in the Halfcheetah-medium-replay and Walker-medium-replay because of the sequence modeling, which ensures that the data generated by the models are consistent with the original offline data. RT uses reliability estimation, which makes it more flexible and diverse when generating trajectories, thereby improving the performance of IQL.\nIn the dense Maze2D-large and diverse Antmaze-large environments, where maze layouts and paths become increasingly complex, shorter trajectories may fail to yield higher cumulative rewards, or may even decrease cumulative rewards. RT-F3 and RT-F5 generate shorter trajectories by using fixed-length generation to prevent the generated data from exceeding the distribution of offline data, which also limits the generation of high-return trajectories. In contrast, RT employs a reliability estimation, which can automatically determine the length of the generated trajectories while ensuring that these state-action pairs remain within the distribution range of the original offline data. Hence, RT can generate more high-return trajectories to facilitate IQL learning."}, {"title": "4.4 Related Work", "content": "Offline model-based methods [Kidambi et al., 2020; Swazinna et al., 2021] learn an approximate dynamics model of the environment and utilize planning algorithms to search for optimal trajectories within that model. However, research indicates that even small generation errors can significantly degrade the performance of multi-step rollouts, as generation errors compound, causing the model to deviate from the high-precision region after a few steps [Talvitie, 2017; Asadi et al., 2018]. To mitigate generation errors, some studies [Yu et al., 2020; Diehl et al., 2021; Zheng et al., 2023] employ multiple models to predict states and actions, using uncertainty quantification to eliminate samples with excessive errors. In addition, some studies [Chen et al., 2022; Yang et al., 2022; Ma et al., 2023] restrict the learning policy from accessing areas with significant differences between the learned and the real dynamics to prevent policy learning failures due to generation errors.\nRecent studies [Wang et al., 2021; Lyu et al., 2022; Lu et al., 2023; Shao et al., 2024] suggest the use of fixed truncation techniques [Janner et al., 2019] to generate shorter trajectories and thus reduce the impact of generation errors on generated data. This generated data is then combined with the original data to create a new dataset, which can be utilized in model-free offline algorithms. However, these studies neglect the influence of historical information on environmental dynamics, potentially resulting in the generation of data that fails to align with the environment. Unlike the above methods, RT extends the offline dataset by capturing historical information between different locations in the input sequence and using reliability assessment to generate reliable trajectories. Moreover, RT employs a cumulative reliability metric to dynamically adjust the trajectory length, enabling a more flexible mitigation of the adverse effects of"}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel model-based offline RL method RT to facilitate policy learning by generating reliable and high-return trajectories. RT leverages Reliability-guaranteed sequence generation techniques and conditions the generation process on high rewards to generate high-return trajectories that are legitimate in the environment. Extensive experiments demonstrate that RT can be effectively integrated with existing model-free algorithms and achieve better performance against existing offline RL benchmarks.\nHowever, RT presupposes a fully observable state space in its empirical evaluations. For partially observable environments, RT may require additional techniques (e.g., Kalman Filters) to address the issue of incomplete information in order to accurately predict states and rewards. Future work includes addressing this above issue and extending RT to multi-agent offline algorithms."}]}