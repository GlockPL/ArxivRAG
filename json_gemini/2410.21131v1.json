{"title": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments", "authors": ["Marharyta Domnich", "Julius V\u00e4lja", "Rasmus Moorits Veski", "Giacomo Magnifico", "Kadi Tulver", "Eduard Barbu", "Raul Vicente"], "abstract": "As machine learning models evolve, maintaining transparency demands more human-centric explainable AI techniques. Counterfactual explanations, with roots in human reasoning, identify the minimal input changes needed to obtain a given output and, hence, are crucial for supporting decision-making. Despite their importance, the evaluation of these explanations often lacks grounding in user studies and remains fragmented, with existing metrics not fully capturing human perspectives. To address this challenge, we developed a diverse set of 30 counterfactual scenarios and collected ratings across 8 evaluation metrics from 206 respondents. Subsequently, we fine-tuned different Large Language Models (LLMs) to predict average or individual human judgment across these metrics. Our methodology allowed LLMS to achieve an accuracy of up to 63% in zero-shot evaluations and 85% (over a 3-classes prediction) with fine-tuning across all metrics. The fine-tuned models predicting human ratings offer better comparability and scalability in evaluating different counterfactual explanation frameworks.", "sections": [{"title": "Introduction", "content": "The rapid adoption of AI across various domains has significantly increased the urgency for explainable AI models. Counterfactual explanations, which address the question \"How should the input be different in order to change the model's decision outcome?\" (Wachter, Mittelstadt, and Russell 2017), not only clarify the machine's reasoning but also suggest potential changes that users might implement to achieve different results. These explanations enhance user trust and understanding by providing a richer mental representation compared to causal explanations (Warren, Byrne, and Keane 2023). Additionally, counterfactual explanations align closely with human cognitive processes (Miller 2019), as they provide alternative hypothetical realities that are pervasive in our natural reasoning (Byrne 2002).\nEvaluating counterfactual explanations poses a significant challenge in the field. While various quantitative metrics, such as validity, proximity, sparsity, coherence, robustness, and diversity (Guidotti 2022; Karimi et al. 2022; Rasouli and Chieh Yu 2024) are currently used, they often fall short in capturing the human perspective, missing key explanatory virtues and leading to inconsistent findings that complicate the development of a standardized evaluation framework. It is commonly recommended that user studies should be conducted to assess the efficacy of counterfactual explanations as \u201cexcellent computational explanations may not be good psychological explanations\u201d (Keane et al. 2021). Despite this, such studies are rarely utilized for benchmarking counterfactual explanations (Longo et al. 2024). One of the reasons for this is the difficulty and expense of recruiting a sufficient number of experts capable of performing these evaluations. Even when executed, user studies do not guarantee consistent and reproducible results as perceptions of what constitutes a reasonable explanation can vary widely between individuals and user groups (Kenny et al. 2021). Furthermore, most studies only employ a few qualitative measures, such as satisfaction and trust, which fail to address the nuanced features influencing human preferences (Warren, Byrne, and Keane 2023). While human assessments of counterfactual explanations are invaluable, these issues of cost and scalability make it very challenging to make meaningful comparisons and generalizations between multiple frameworks or domains.\nRecognizing the limitations of existing methodologies, this paper explores the potential of Large Language Models (LLMs) to serve as a benchmark for automating the evaluation of counterfactual explanations. Current LLMs have demonstrated remarkable capabilities in interacting with natural language data, from extensive data summarisation (Liu et al. 2024) and pattern deduction (Jin et al. 2024) to idea generation (Girotra et al. 2023) and problem-solving through branching solutions (Yang et al. 2024), and many more (Wang et al. 2024a). Based on these premises, LLMs are hypothesized to mimic human evaluative judgments effectively, offering a more accessible and cost-efficient alternative to traditional methods.\nIn light of these considerations, this paper addresses the following question: Can the evaluation process of counterfactual explanations be effectively automated using LLMs? To answer this question we created a diverse set of 30 counterfactual scenarios that were varied across different dimensions of explanatory qualities. The scenarios were evaluated by 206 human respondents in overall satisfaction and metrics of feasibility, consistency, completeness, trust, fairness, complexity and understandability. Next, we"}, {"title": "Related Works", "content": "In the following section, we review user studies that focus on evaluating counterfactual explanations and the potential of LLMs in simulating human responses."}, {"title": "User studies in evaluating counterfactual explanations", "content": "In addition to quantitative explanatory metrics like proximity, validity, or sparsity, most researchers agree that it is crucial to also capture the subjective preferences of human users in aiming for more human-centric AI explanations (Kirsch 2017; Keane et al. 2021; Longo et al. 2024). Yet, a survey found only 21% of 100 studies on counterfactual methods included user evaluations (Keane et al. 2021). Furthermore, many of those studies test the use of counterfactual explanations vs no-explanations rather than comparing different methods, leaving only 7% of papers that report user evaluations for benchmarking different counterfactual algorithms.\nIn recent years, some user studies have been conducted with tabular counterfactual data. For instance, (Warren, Keane, and Byrne 2022) conducted a study with 127 participants to compare the effects of counterfactual and causal explanations on both objective prediction accuracy and subjective judgments of satisfaction and trust. (Bove et al. 2023) explored the impact of plural counterfactual examples on objective understanding and a modified version of the Explanation Satisfaction Scale (Hoffman et al. 2018) in a lab study with 112 participants. (F\u00f6rster et al. 2021) conducted a study with 46 participants assessing the realism and typicality of an explanation. Two user studies have benchmarked counterfactual methods for perceived practicality of users in a study with 135 participants (Ghazimatin et al. 2020), and an online study with 500 responders (Spreitzer, Haned, and van der Linden 2022). Additionally, (Akula et al. 2022) tested their approach on image data, evaluating justified trust as quantitative metric and explanation satisfaction as qualitative metric across different algorithms.\nOverall, user studies on explanation satisfaction often focus on a limited range of aspects (Mueller et al. 2019), typically measuring satisfaction and trust, while neglecting other essential qualities of the explanations themselves. These studies may not adequately capture human preferences, which are influenced by context, presentation, and cognitive biases, especially when preferences are not clearly defined (Covell 2019; Kliegr, Bahn\u00edk, and F\u00fcrnkranz 2021; Tversky and Simonson 1993). As a result, studies that fail to capture the full spectrum of explanatory qualities contribute to a narrow and inconsistent perspective of human judgements, leaving a significant gap in our understanding of which features are central to good explanations."}, {"title": "Potential of LLMs in Simulating Human Responses", "content": "Predicting human evaluation using Machine Learning has garnered widespread acceptance in various domains, such as human-computer interaction (Kiseleva et al. 2016; Yang, Levow, and Meng 2012), recommendation systems (Siro, Aliannejadi, and De Rijke 2023), speech quality assessment (Reddy, Gopal, and Cutler 2022), etc. The progressive advancement of LLMs' causal reasoning abilities (Bhattacharjee et al. 2024) suggests their usage in the context of explainability, since the explanations in natural language generated with these processes present qualities akin to those of human output (Castelnovo et al. 2024) and the explanatory process can be further enhanced through a post-output chat pipeline (Slack et al. 2023). LLMs have also been used to evaluate and model user satisfaction to provide insight regarding choices and preferences (Kim et al. 2024), to directly simulate user feedback for model tuning (Ebrat and Rueda 2024), and as artificial user / model-in-tuning pairs (Gao et al. 2024). However, to the best of our knowledge there is currently no work related to simulating human assessment in evaluating of counterfactual explanations with LLMs."}, {"title": "Development and human evaluation of a Counterfactual explanation dataset", "content": "Training LLMs to evaluate the quality of counterfactual explanations as humans do requires human-labeled data. As of the writing of this article, there exists no widely-used dataset of human-evaluated counterfactual explanations. To fill this gap, we created a varied dataset of 30 counterfactual explanation instances, which were graded on 8 different criteria by 206 people through an online survey."}, {"title": "Dimensions of explanatory qualities", "content": "For selecting the dimensions to include in our study, we reviewed literature on qualitative metrics influential to human judgements. Among the most frequently cited explanatory virtues are coherence and simplicity (Mackonis 2013), aligning with the understanding of human mental models and a preference for consistent and parsimonious information (Johnson-Laird 2010). Coherence as a qualitative metric can be measured internally, representing consistency within the explanation, or externally, taking into account the prior knowledge of the rater (Zemla et al. 2017). Our work focuses on internal coherence to measure consistency between different parts of the explanation, independent of an individual's prior experiences.\nThe virtue of simplicity is also discussed under the terms (Desired) Complexity (Zemla et al. 2017) and Selection (Vilone and Longo 2021), assuming people prefer simple explanations (Lombrozo 2007). However, evidence suggests humans sometimes favor complex explanations involving more causal links (Zemla et al. 2017), or that moderate complexity and sufficient detail are preferred (Ramon et al. 2021; Hoffman et al. 2018). For this study, we chose to include the metric of Complexity, with desired values falling in the middle, as explanations can be perceived as either too simple or too complex.\nA commonly assessed quality in user studies is Trust. Various definitions focus on trust in the method generating explanations (Perrig, Scharowski, and Br\u00fchlmann 2023; Scharowski et al. 2024). Trust in explanations is considered in terms of trustworthiness, evaluating the perceived credibility of suggested changes (Stepin et al. 2022). We define Trust as belief that following the explanation would lead to the desired outcome.\nFeasibility is one of the most agreed-upon metrics when discussing counterfactual explanations, although discussed under different names: Controllability (Byrne 2019), Actionability (Rasouli and Chieh Yu 2024) and even split into Actionability and Mutability (Karimi et al. 2022). While actionability has also been employed as a quantitive measure (Guidotti 2022), feasibility refers to whether the proposed changes are perceived as achievable and realistic. Research indicates that explanations failing this criterion are rated poorly (McCloy and Byrne 2000; Butz et al. 2024).\nUnderstandability, also known as Readability (Stepin et al. 2022) or Comprehensibility (Ali et al. 2023; Vilone and Longo 2021), relates to how effectively an explanation conveys the model's decision process to the user or how easily the user grasps it. Generally, higher understandability is linked to greater user satisfaction, with clear and comprehensible explanations generally preferred, though complex answers may be favored in some contexts.\nCompleteness has previously been discussed as Incompleteness (Zemla et al. 2017) or Informativeness, the latter of which also includes the notion of extraneous information (Stepin et al. 2022), is tied to understanding causal relations and partially depends on domain knowledge (Keil 2006). Evaluating completeness is challenging as people tend to fill logical gaps in explanations (Strickland and Keil 2011).\nFinally, the dimension of Fairness in counterfactual explanations has also been highlighted in recent work (Wang et al. 2024b). Due to the concern of models unintentionally encoding or even amplifying biases present in training data (Corbett-Davies et al. 2023), it is crucial to address potential unfairness and discrimination. Fairness has mostly been viewed as a quantitative metric (Ge et al. 2022) with little understanding of how it influences the perceived quality of explanation."}, {"title": "Generating counterfactual explanations scenarios", "content": "Relying on previous work on human preferences and explanatory virtues, we selected 8 different criteria capturing a range of relevant dimensions (see previous section for an overview) guiding the creation of diverse counterfactual scenarios. The Adult dataset (Becker and Kohavi 1996) and the Pima Indians Diabetes dataset (Bennett, Burch, and Miller 1971) were chosen as a basis when formulating the counterfactual explanations, as they encompass a varying level of domain knowledge and include both categorical and continuous data. To ensure the dataset consists of diverse counterfactual explanations, we included explanations constructed fully from the features of the datasets as well as explanations that were constructed from artificial data in the final set. All the counterfactual scenarios were designed from the perspective of improving the factual situation, as direction-"}, {"title": "Questionnaire results", "content": "To assess the overall suitability and comprehensibility of the compiled scenarios and evaluation metrics, a pilot study was conducted with 15 volunteers recruited among university students and colleagues. Feedback gathered during the pilot led to revisions in the wording of some metric descriptions. Additionally, the Coherency metric was renamed to Consistency and Bias was changed to Fairness to aid comprehension for the participants.\nThe final version containing 30 counterfactual scenarios"}, {"title": "Modelling human assessment with LLMs", "content": "With the questionnaire data as the input dataset, we aimed to test and fine-tune Large Language Models for automated evaluation of counterfactual explanations. The models selected for this were Llama 3.1 Instruct, Llama 3 Instruct (Dubey et al. 2024) and GPT-4 (OpenAI 2023). GPT-4 was accessed via OpenAI API and the Llama models were fine-tuned on HPC clusters with NVIDIA Tesla A100 GPUs using the transformers library by Huggingface (Wolf et al. 2020). QLORA, which relies on rank decomposition matrices and quantization, was used for reducing memory requirements during fine-tuning (Dettmers et al. 2023)."}, {"title": "Dataset preparation", "content": "After gathering and filtering questionnaire responses, further data processing was needed to create a useful dataset. For each question-metric pair, we used the average response from 196 participants as the final value. Complexity, originally rated on a -2 to 2 scale, was linearly scaled to align with the 1 to 6 scale used for other metrics. To minimize scale effects and enhance generalizability, we consolidated all metric values into three distinct categories. Data analysis suggested that the differences between scores of 1 and 2, 3 and 4, and 5 and 6 could be effectively compressed. Subsequent analyses confirmed that three classes adequately predicted outcomes. Thus, we classified values below 3 as \u201clow,\u201d values between 3 and 4 as \u201cmedium,\u201d and values above 4 as \u201chigh.\u201d These categories were deliberately balanced to ensure an equal distribution across the classes. With 30 questions and 8 metrics per question, this resulted in 240 instances of metric evaluation in total."}, {"title": "Prompt engineering", "content": "To achieve the best possible performance from an LLM, three prompt structures were tested and compared.\nImportantly, the instruction part of the prompt was taken from the questionnaire directly to ensure that the task reflects the gathered data, and all changes were made in what is known as a \u201csystem prompt\u201d. For this task, the following system prompts were developed:\n\u2022 A baseline prompt which contains an introduction to counterfactual explanations, the expected output format, and the definition of the metric being evaluated.\n\u2022 A prompt that contains all the information present in the baseline prompt, but additionally provides definitions for all the metrics, not just the metric being evaluated.\n\u2022 A prompt that additionally contains two examples of input and expected output, one with Consistency rated as \u201chigh\u201d and the other with Feasibility rated as \u201clow\u201d. These examples were crafted based on the examples provided for metrics in the questionnaire. The specific examples were chosen to contain different metrics and different output values. All the additional information present in previous prompts is contained in this prompt as well.\nThe instruction or \u201cuser prompt\u201d was adapted from the questionnaire, meaning it contained a factual-counterfactual pair from the questionnaire, alongside a modified metric evaluation question, such as \u201cPlease rate as 'low' (very unfeasible), 'medium' or 'high' (completely feasible), how feasible is this explanation:\u201d. Consequently, each counterfactual explanation resulted in 8 instances, one for every metric under evaluation. The specific phrasing of all three system prompts can be found in Appendix B."}, {"title": "Modelling individual preferences", "content": "Different people's preferences for explanations exhibit significant variability. To explore the effects of this, an experiment was carried out with a dataset based on specific participants' answers, instead of the sample averages. To ensure that these participants represent different subgroups of participants, t-SNE was used to reduce the dimensionality of the data and DBSCAN clustering to cluster the results. The goal of clustering was to discern the largest clusters present in the data, after which a random participant was chosen from each of the four largest clusters. The results of clustering and participant selection can be viewed in Appendix D, Figure D.1.\nThe selected participants, each from different European countries with educational levels from high school to Master's degree, ensured a diverse range of viewpoints. One participant's experience in machine learning further enriched the variety of responses, detailed in Appendix D, Table D.1.\nFor each of these participants, zero-shot evaluation and fine-tuning was carried out using the same procedure as in the previous experiments, but using only the model Llama 3 70B Instruct, as it proved to be the most capable (for hyperparameters see Appendix C, Table C.2). The testing set contained the same question-metric pairs as in the first experiment, but with answers from the specific participant.\nThe results of this process were varied with accuracies ranging from 58% to 69% for zero-shot. Table 5 shows that the LLM's predictions improved significantly after fine-tuning, reaching accuracies over all metrics of ~90% for 3 participants. One participant appeared to be less consistent, as the model managed to simulate their answers with an accuracy of only 66%. This leads to two conclusions: while LLM's biases and preferences can be tuned to match specific participants to a great extent, some participants' preferences prove significantly more difficult to mimic. However, since this comparison only contained 4 participants and 30 explanations, these conclusions should be considered tentative."}, {"title": "Discussion", "content": "The traditional assessment of counterfactual explanations often overlooks human aspects, relying either on inconsistent quantitative metrics (frequently used both within objective function optimization and for evaluation (Cheng, Ming, and Qu 2021)) or on user studies that focus on a specific subset of individuals, lacking comparability over time and methods. To address this, we developed a novel dataset of counterfactual explanations, evaluated by human participants, which demonstrated a diverse spread of evaluations across all metrics, highlighting its applicability in different contexts. Utilizing this dataset to fine-tune LLMs demonstrated promising results, achieving an 85% accuracy, suggesting they can be used to approximate human judgment across various metrics. Furthermore, the zero-shot LLM performance was already notable, achieving up to 63% accuracy. Our experiments also indicate the potential to fine-tune models to individual experts, targeting specific knowledge or preferences.\nHowever, employing LLMs for evaluating counterfactual explanations introduces ethical considerations. There is a risk of reinforcing or introducing biases if the models are not continuously monitored and updated with diverse training data. Furthermore, optimizing explanations to align with model preferences might lead to \u201cgaming\u201d the system, skewing results towards what the model favours rather than enhancing the relevance of the explanations to human users.\nA considerable limitation of our study is the dataset size, consisting of only 30 unique counterfactual explanations. A larger dataset would likely enhance model training capabilities. Future work should aim to generate larger datasets using recent counterfactual algorithms (Rasouli and Chieh Yu 2024; Domnich and Vicente 2024; Dandl et al. 2024). These should be presented in smaller subsets to participants for evaluation, given that a single participant can only assess a limited number of explanations thoroughly.\nIn the future, the main implication of this work is that a fine-tuned LLM should be applied to evaluate various counterfactual algorithms. Additionally, the model can be iteratively retrained with newer and larger architectures and datasets. With the continuously improving size and capabilities of LLMs, this is likely to lead to further improvements in mimicking human evaluation patterns.\nDespite the potential, it is crucial to acknowledge that LLMs do not replace the nuanced insights provided by human evaluations. Instead, they can serve as a complementary tool, enhancing scalability and reducing the resources required for broad assessments across multiple frameworks. Moreover, we propose exploring the idea of integrating this model within a human-in-the-loop approach to produce a hybrid model that could refine the quality of counterfactual explanations during the generation process (i.e. creating an LLM-in-the-loop instead of a human) (Abrate et al. 2024), leveraging the strengths of both automated and human evaluations."}, {"title": "Conclusion", "content": "This study aims to advance towards more standardized and human-centric evaluations of counterfactual explanations in AI systems. The development and application of our novel dataset, which captures a broad spectrum of human evaluations, reveals the significant potential of LLMs to mirror human judgment with a high degree of accuracy."}, {"title": "Ethical Statement", "content": "All data were collected without any personal identifiers. The study was approved by The University of Tartu Research"}]}