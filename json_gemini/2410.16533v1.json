{"title": "Large Body Language Models", "authors": ["Saif Punjwani", "Larry Heck"], "abstract": "As virtual agents become increasingly prevalent in human-computer interaction, generating realistic and contextually appropriate gestures in real-time remains a significant challenge. While neural rendering techniques have made substantial progress with static scripts, their applicability to human-computer interactions remains limited. To address this, we introduce Large Body Language Models (LBLMs) and present LBLM-AVA, a novel LBLM architecture that combines a Transformer-XL large language model with a parallelized diffusion model to generate human-like gestures from multimodal inputs (text, audio, and video). LBLM-AVA incorporates several key components enhancing its gesture generation capabilities, such as multimodal-to-pose embeddings, enhanced sequence-to-sequence mapping with redefined attention mechanisms, a temporal smoothing module for gesture sequence coherence, and an attention-based refinement module for enhanced realism. The model is trained on our large-scale proprietary open-source dataset Allo-AVA. LBLM-AVA achieves state-of-the-art performance in generating lifelike and contextually appropriate gestures with a 30% reduction in Fr\u00e9chet Gesture Distance (FGD), and a 25% improvement in Fr\u00e9chet Inception Distance compared to existing approaches.", "sections": [{"title": "1 Introduction", "content": "Generating realistic and contextually appropriate gestures in real-time is a critical challenge to produce engaging virtual agents, and while neural rendering techniques have made strides in our ability to generate them as such, their realism is often hindered from their static movements and limited ability to have multimodal conversational interactions (Li et al., 2023; Windle et al., 2023). To address this problem, we introduce Large Body Language Models (LBLMs), a new type of architecture specifically designed for generating gestures in the context of real-time, multimodal communication.\nIn the context of conversational AI and gesture generation, we define the LBLM inference problem as generating an optimal multimodal (body movement and facial expression) gesture sequence $G^*$ given a multimodal input sequence $X$ and conversational context $C$:\n$G^* = \\arg \\max_{G} p_{\\theta}(G | X, C)$ (1)\nwhere $X = \\{(T_t, A_t, V_t)\\}_{t=1}^{f}$, with $T_t$ representing text, $A_t$ audio, and $V_t$ video at time $t$, and $p_{\\theta}$ denotes the model parameterized by $\\theta$.\nPrior approaches to gesture generation have relied on rule-based systems (Nyatsanga et al., 2023), motion capture databases (Rueux et al., 2014), or learning-based methods (Yoon et al., 2020; Zhou et al., 2022). While these techniques have made notable improvements, they are limited in their ability to capture the complex relationships between speech, facial expressions, and body language in dynamic conversational settings. Rule-based systems can appear mechanical, motion capture databases are constrained by the specific gestures they contain, and conventional learning-based methods struggle to generate gestures that are coherent across long time horizons and adapt to evolving conversational contexts.\nRecent works such as (Korzun et al., 2022; Neff et al., 2007; Bhattacharya et al., 2021) represent early examples of LBLMs, leveraging the success of transformer-based language models (Vaswani et al., 2017) to capture the intricacies of human communication. These models demonstrate the potential for generating human-like gestures by learning the complex relationships between language, audio, and visual cues. However, they have limitations in terms of their ability to handle real-time, multimodal inputs and generate diverse, contextually appropriate gestures.\nWe present LBLM-AVA, a novel LBLM architecture that combines a Transformer-XL (Dai et al., 2019) language model with a parallelized diffusion model (Shih et al., 2023) to generate human-like gestures from multimodal inputs, including text, audio, and video. LBLM-AVA incorporates several key components to enhance its gesture generation capabilities, such as multimodal-to-pose embeddings, sequence-to-sequence mapping with redefined attention mechanisms, a temporal smoothing module for gesture sequence coherence, and an attention-based refinement module for enhanced realism. The model is trained on our proprietary open-source dataset Allo-AVA, a large-scale, multimodal corpus derived from diverse sources like TEDx Talks and podcasts.\nExperimental evaluations demonstrate that LBLM-AVA achieves state-of-the-art performance in generating lifelike and contextually appropriate gestures. Gestures generated by LLBM-AVA significantly boost the perceived naturalness and engagement of virtual agents. Our results highlight the potential of LLBMs to advance the field of gesture generation and create more compelling human-agent interactions."}, {"title": "2 Approach", "content": "2.1 LBLM-AVA Architecture\nOur proposed LBLM-AVA model builds upon the Transformer-XL architecture and incorporates several novel components to generate realistic and contextually appropriate gestures from multimodal inputs. The overall architecture is illustrated in Figure 1.\n2.1.1 Multimodal Input Representation\nThe input to the model consists of text, audio,\nand video features, denoted as $T\\in \\mathbb{R}^{L_T\\times d_T}$,\n$A\\in \\mathbb{R}^{L_A\\times d_A}$, and $V \\in \\mathbb{R}^{L_V\\times d_V}$ respectively,\nwhere $L_*$ and $d_*$ represent the sequence length and feature dimensionality for each modality. These features are projected to a common dimension $d$ using learned linear transformations:\n$T' = W_TT, A' = W_AA, V' = W_VV$ (2)\nwhere $W_T \\in \\mathbb{R}^{d \\times d_T}$, $W_A\\in \\mathbb{R}^{d \\times d_A}$, and $W_V \\in \\mathbb{R}^{d \\times d_V}$ are learnable projection matrices.\n2.1.2 Transformer-XL Encoder\nThe projected multimodal features are concatenated along the sequence dimension and passed through a Transformer-XL encoder. The Transformer-XL architecture introduces the notion of recurrence by reusing hidden states from previous segments, enabling the model to capture longer-term dependencies.\nGiven the input sequence $X = [T'; A'; V'] \\in \\mathbb{R}^{L\\times d}$, the hidden state for the n-th segment $s_n \\in \\mathbb{R}^{L_s\\times d}$ is computed as:\n$h_n = \\text{TransformerXL}([s_{n-1};x_n])$ (3)\nwhere $s_{n-1}$ is the cached hidden state from the previous segment, $x_n \\in \\mathbb{R}^{L_s\\times d}$ is the input sequence for the current segment, and $L_s$ is the segment length. The TransformerXL function applies multi-head attention with relative positional encodings followed by position-wise feed-forward layers.\n2.1.3 Multimodal-to-Pose Embedding\nTo facilitate the mapping from the different features (language, audio, video) to gesture poses, we introduce a multimodal-to-pose embedding module. This module learns a transformation from the encoded modality features to a latent pose space:\n$E = W_E H_T$ (4)\nwhere $H_T \\in \\mathbb{R}^{L_T\\times d}$ is the encoded sequence dependent on input from the Transformer-XL encoder, $W_E \\in \\mathbb{R}^{d_p\\times d}$ is a learnable weight matrix, and $E \\in \\mathbb{R}^{L_T \\times d_p}$ is the embedded pose sequence.\n2.1.4 Parallelized Diffusion Model\nTo generate realistic and diverse gesture sequences, we employ a parallelized diffusion model (Ho et al., 2020). Diffusion models learn to denoise a Gaussian noise signal into a target data distribution through a series of iterative refinement steps. In our approach, we parallelize the diffusion process to generate multiple gesture sequences simultaneously.\nGiven the language-to-pose embeddings E, we sample a set of N initial noise sequences $\\{z_0^{(i)}\\}_{i=1}^N$ where $z_0^{(i)} \\in \\mathbb{R}^{L_T \\times d_p}$. The diffusion process then iteratively refines these sequences over K steps:\n$z_k^{(i)} = z_{k-1}^{(i)} - \\frac{\\beta_k}{2} (z_{k-1}^{(i)} - f_{\\theta} (z_{k-1}^{(i)}, E, k))$ (5)"}, {"title": "2.1.5 Attention-based Temporal Refinement", "content": "To improve the temporal coherence of the generated gestures, we introduce an attention-based refinement module. This module applies multi-head self-attention to the generated pose sequences, allowing the model to capture long-range dependencies and ensure smooth transitions between gestures.\nGiven the generated pose sequences $\\{P^{(i)}\\}_{i=1}^N$,\nthe refined sequences $\\{\\tilde{P}^{(i)}\\}_{i=1}^N$ are computed as:\n$\\tilde{P}^{(i)} = \\text{MultiHeadAttention}(P^{(i)}, P^{(i)}, P^{(i)})$ (6)\nwhere the MultiHeadAttention function applies self-attention to the input sequence, using the same sequence for the query, key, and value matrices.\n2.1.6 Adversarial Training\nTo further enhance the realism and diversity of the generated gestures, we employ adversarial training (Goodfellow et al., 2014). We introduce a discriminator network D that learns to distinguish between real and generated gesture sequences. The generator (i.e., the diffusion model) is then trained to maximize the discriminator's confusion:\n$L_G = -E_{P \\sim p_G} [\\log D(P)]$ (7)\nwhere $p_G$ is the distribution of generated gesture sequences. The discriminator is trained to minimize the adversarial loss:\n$L_D = -E_{P \\sim p_{\\text{data}}} [\\log D(P)] - E_{P \\sim p_G} [\\log(1 - D(P))]$ (8)\nwhere $p_{\\text{data}}$ is the distribution of real gesture sequences. The generator and discriminator are trained in an alternating fashion, promoting the generation of realistic and diverse gestures.\nThe integration of these novel components, including the Transformer-XL architecture, language-to-pose embeddings, parallelized diffusion model, attention-based refinement, and adversarial training, enables LLBM-AVA to generate highly expressive and contextually appropriate gestures from multimodal inputs. The model architecture is designed to capture the complex relationships between language, audio, and video features while ensuring the temporal coherence and realism of the generated gestures (Chen et al., 2023)."}, {"title": "2.2 Gesture Parameterization and Representation", "content": "To efficiently represent and manipulate gestures, we introduce a compact parameterization scheme (Zhou et al., 2020). Each gesture is represented as a sequence of pose vectors $p_t \\in \\mathbb{R}^D$, where D is the dimensionality of the pose space. The pose vectors encode the positions and orientations of key body joints, such as the hands, elbows, and shoulders, relative to a root joint (e.g., the pelvis).\nWe further decompose each pose vector into a set of sub-vectors corresponding to different body parts:\n$p_t = [p_t^{(1)}, p_t^{(2)}, ..., p_t^{(B)}]$ (9)\nwhere B is the number of body parts and $p_t^{(b)} \\in \\mathbb{R}^{D_b}$ is the sub-vector for body part b at time step t. This hierarchical representation allows for more fine-grained control over the generated gestures and facilitates the modeling of inter-part dependencies.\nTo ensure smooth and realistic motion, we apply a series of kinematic constraints and postprocessing steps to the generated pose sequences. These include enforcing joint angle limits, maintaining bone lengths, and applying Gaussian smoothing to eliminate high-frequency jitter (Savitzky and Golay, 1964)."}, {"title": "2.2.1 Pose Embedding", "content": "To feed the pose vectors into our Transformer-based model, we first embed them into a higher-dimensional feature space using a learnable embedding matrix $E \\in \\mathbb{R}^{D \\times d}$, where d is the embedding dimensionality:\n$e_t = E \\cdot p_t$ (10)\nThe embedded pose vectors $e_t \\in \\mathbb{R}^d$ serve as input to the Transformer encoder, allowing the model to learn rich, context-dependent representations of the gesture sequences.\n2.3 Attention-based Gesture Refinement\nGiven our pose sequence above, our attention-based refinement module that operates on the output of the Transformer decoder is adjusted to utilize the pose embedding.\nGiven the decoded pose sequence $\\tilde{P} = [\\tilde{P}_1, \\tilde{P}_2, ..., \\tilde{P}_T]$, the refinement module computes a set of attention weights $a_{t, t'}$ that measure the relevance of each time step t' to the current time step t:\n$a_{t, t'} = \\frac{\\exp(\\beta \\cdot q_t^T k_{t'})}{\\sum_{t'=1}^T \\exp(\\beta q_t^T k_{t'})}$ (11)\nwhere $q_t$ and $k_{t'}$ are learnable query and key vectors, respectively, and $\\beta$ is a temperature parameter controlling the sharpness of the attention distribution.\nThe refined pose vector $p_t$ is then computed as a weighted sum of the decoded pose vectors, using the attention weights:\n$p_t = \\sum_{t'=1}^T a_{t, t'} \\tilde{p}_{t'}$ (12)\nThis refinement process helps to smooth out irregularities and ensure that the generated gestures are temporally coherent and well-coordinated."}, {"title": "3 Dataset", "content": "Our research builds upon the work on listener motion generation, but we have substantially expanded and enriched our dataset to address the limitations of previous studies and to encompass a broader range of communicative contexts. While datasets like the TED-Gesture dataset (Yoon et al., 2017) and the AMT Gesture dataset (Nyatsanga et al., 2023) have made valuable contributions, they lack the required diversity and scale. The TED-Gesture dataset, for instance, consists of 1,766 video clips from 15 TED talks, with a total duration of approximately 5 hours. In contrast, our Allo-AVA dataset comprises 1,250 hours of high-quality video, audio, and text data, curated from a wide array of sources such as talk shows, podcasts, TED talks, and other public speaking forums (Ruffieux et al., 2014). This represents a 240-fold increase in data volume compared to the TED-Gesture dataset, enabling our model to learn from a vastly more diverse and comprehensive set of human gestures and expressions.\n3.1 Dataset Composition and Improvements\nThe Allo-AVA dataset is carefully balanced across multiple dimensions to ensure diversity and representativeness. Table 1 presents a detailed breakdown of the dataset composition, highlighting the diversity of the speakers and the communicative contexts represented."}, {"title": "4 Experiments and Results", "content": "We present a comprehensive evaluation of our text-driven gesture generation model, comparing its performance against state-of-the-art baselines on multiple metrics and datasets.\n4.1 Evaluation Metrics\nWe evaluate the quality and diversity of the generated gestures using several widely adopted metrics:\n\u2022 Fr\u00e9chet Gesture Distance (FGD) (Yoon et al., 2020): Measures the differences between the distributions of real and generated gesture sequences in the pose embedding space. The FGD is computed as:\n$FGD = ||\\mu_r - \\mu_g ||^2 + Tr(\\Sigma_r + \\Sigma_g - 2\\sqrt{\\Sigma_r \\Sigma_g})$ (13)\nwhere $\\mu_r$ and $\\mu_g$ are the means, and $\\Sigma_r$ and $\\Sigma_g$ are the covariance matrices of the real and generated gesture sequences, respectively, in the pose embedding space.\n\u2022 Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017): Measures the quality and diversity of the generated gestures by comparing their feature distributions in a pretrained gesture recognition network (Rautaray and Agrawal, 2012). The FID is similar to the FGD, but the domain is the feature space of the model:\n$FID = ||\\mu_r - \\mu_g ||^2 + Tr(\\Sigma_r + \\Sigma_g - 2\\sqrt{\\Sigma_r \\Sigma_g})$ (14)\nwhere $\\mu_r$ and $\\mu_g$ are the means, and $\\Sigma_r$ and $\\Sigma_g$ are the covariance matrices of the real and generated gesture sequences, respectively, in the feature space of the model.\n\u2022 Average Pairwise Distance (APD): Measures the diversity of the generated gestures by computing the average pairwise Euclidean distance between pose vectors in a batch:\n$APD = \\frac{1}{\\binom{N}{2}} \\sum_{i=1}^{N-1} \\sum_{j=i+1}^N ||p_i - p_j||$ (15)\nwhere N is the batch size and $p_i$ and $p_j$ are the pose vectors of the i-th and j-th generated gestures in the batch."}, {"title": "4.2 Results and Discussion", "content": "Table 3 presents a comprehensive comparison of our model's performance against state-of-the-art baselines on the evaluation metrics described above. Our full model, incorporating attention-based refinement and adversarial training, achieves the best performance across all metrics, surpassing the baselines by a significant margin. The low FGD and FID scores indicate that our model generates gestures that are highly similar to real ones in terms of both quality and diversity. The high APD score suggests that our model produces a wide range of diverse gestures, avoiding repetitiveness and monotony.\nTo better understand the contribution of each component in our model, we perform an ablation study by evaluating the performance of our model with and without attention-based refinement and adversarial training. As shown in Table ??, both attention-based refinement and adversarial training contribute significantly to the overall performance of our model. Removing either component leads to a noticeable drop in performance across all evaluation metrics, with adversarial training having a slightly larger impact than attention-based refinement.\nTo further analyze the impact of our model components, particularly adversarial training, on the generated gestures, we introduce two key metrics:"}, {"title": "the Gesture Realism Score (GRS) and the Gesture Diversity Index (GDI).", "content": "The Gesture Realism Score (GRS) measures the perceptual realism of the generated gestures based on a learned discriminator network. The GRS is computed as:\n$GRS = \\frac{1}{N} \\sum_{i=1}^N D(G_i)$ (16)\nwhere N is the number of generated gesture sequences, $D(\\cdot)$ is the discriminator network, and $G_i$ is the i-th generated gesture sequence.\nTo assess the variety in gesture sequences, we employ the Gesture Diversity Index (GDI), calculated as follows:\n$GDI = \\frac{1}{\\binom{N}{2}} \\sum_{i=1}^{N-1} \\sum_{j=i+1}^N d(E_i, E_j)$ (17)\nHere, $E_i$ and $E_j$ represent embedded pose sequences, and d(, ) is a measure like Euclidean distance. A GDI value close to 1 indicates good diversity in the generated gestures.\nAs shown in Table 5, employing adversarial training significantly improves both the perceptual realism and diversity of the generated gestures, as indicated by the higher GRS and GDI scores. The full LBLM-AVA model achieves the highest scores for both metrics (GRS: 0.85, GDI: 0.75), while removing adversarial training causes the most substantial drops (GRS: 0.68, GDI: 0.62).\nAdversarial training not only enhances the realism and diversity of generated gestures but also promotes stability during training and mitigates mode collapse (Goodfellow et al., 2014). The results show a significant increase in both GRS and GDI after implementing adversarial training, indicating improved realism and diversity in gesture outputs (Saleh, 2022).\n4.2.1 Incremental Analysis of LBLM-AVA Components\nLooking at the model from a holistic perspective, instead of completing a traditional ablation study, we can examine the effect of iteratively adding each component until we build the complete LBLM-AVA model.\nTable 6 demonstrates the incremental performance gains achieved by each component of the LBLM-AVA architecture. The results highlight several key observations:\n1.  The baseline model, utilizing only multimodal embedding, shows the poorest performance across all metrics.\n2.  The addition of the Transformer-XL architecture brings significant improvements to all metrics, particularly FGD and FID, indicating enhanced gesture quality and coherence.\n3.  Incorporating Multi-Head Attention further boosts the model's performance, especially in terms of gesture diversity (APD) and realism (GRS).\n4.  The Parallelized Diffusion component introduces another substantial improvement, particularly in FGD and FID, suggesting better overall gesture quality.\n5.  Temporal Smoothing primarily enhances the smoothness and coherence of the gestures, as reflected in improved FGD, FID, and GRS scores.\n6.  The Attention-Based Refinement module further improves the FGD, FID, and APD metrics. However, there's a slight decrease in GRS and GDI, possibly due to the refinement process reducing some variability in the gestures.\n7.  Finally, the addition of Adversarial Training, completing the full LBLM-AVA model, yields the best performance across all metrics. This is particularly evident in the significant improvements in gesture realism (GRS) and diversity (GDI)."}, {"title": "5 Conclusion", "content": "In this work, we introduced Large Body Language Models (LBLMs) and presented LBLM-AVA, a novel architecture that combines Transformer-XL and diffusion models to generate realistic and contextually appropriate gestures from multimodal inputs in real-time conversational settings. Extensive evaluations show that LBLM-AVA achieves state-of-the-art performance, outperforming existing approaches (Yoon et al., 2020; Zhou et al., 2022).\nThe development of Allo-AVA, a large-scale, diverse dataset of multimodal human communication, was crucial for training robust and expressive LBLMs. With a 240-fold increase in data volume compared to existing datasets and a broad range of communicative contexts, Allo-AVA serves as an great resource for pushing research in gesture generation and multimodal communication modeling.\nOur work opens up new possibilities for creating virtual agents that communicate with the nuance and expressiveness of humans in real-time, multimodal interactions. LBLMs have the potential to transform various fields, including virtual assistants, social robotics, telepresence systems, and educational technologies, by enabling more natural, engaging, and immersive human-computer interactions. However, while they have a lot of benefits, they also have a lot of drawbacks. Gesture generation can be used in deepfakes with unrecognizable movement coming from powerful people and sources. Our dataset sought to address in terms of making the distribution diverse, but this problem is still prevalent in the model architecture itself.\nFuture research directions exploring techniques for controlling the style and personality of the generated gestures, and investigating the potential of LBLMS for other aspects of nonverbal communication (Kucherenko et al., 2022). These are some areas where our model falls behind, so the development of more efficient and scalable LBLM architectures and training techniques could further extend the applicability of these models to resource-constrained environments and real-time applications.\nOur work represents an advancement in the field of gesture generation for real-time, multimodal communication. The introduction of Large Body Language Models, the development of LBLM-AVA, and the creation of the Allo-AVA dataset provide a solid foundation for future research and development in this area. By demonstrating the potential of these models to generate realistic, diverse, and contextually appropriate gestures, we have taken a crucial step towards creating virtual agents that can communicate with the richness and nuance of human nonverbal behavior."}, {"title": "6 Limitations", "content": "LBLM-AVA, while advancing gesture generation, faces several limitations. The model's computational complexity may restrict real-time applications on resource-constrained devices. Despite efforts to create a diverse dataset, Allo-AVA may contain inherent biases, potentially under-representing certain demographic groups or gesture styles. The model's training data, primarily from Western speakers, may limit its ability to generate culturally diverse gestures. While improving long-term coherence, LBLM-AVA may still struggle with extended conversations, potentially leading to gesture repetition or inconsistency. The integration of multimodal inputs, though advanced, may not fully capture the complex interplay between speech and gesture in human communication. Lastly, while our evaluation metrics provide valuable insights, they may not fully capture the nuanced aspects of gesture quality and appropriateness as perceived by humans, suggesting the need for comprehensive user studies in future work."}, {"title": "7 Ethical Considerations", "content": "The development of LBLM-AVA raises important ethical considerations. The potential misuse of this technology for creating convincing deepfakes necessitates robust detection methods and careful release strategies. The use of large-scale video datasets raises privacy and consent concerns for individuals in the training data. Despite efforts to minimize bias, any existing biases in Allo-AVA could be amplified, potentially leading to misrepresentation of certain groups. The increasing realism of virtual agents may have unforeseen psychological effects on users, particularly in long-term interactions. As these technologies advance, it's crucial to consider transparency in AI interactions and ensure accessibility for individuals with diverse needs. Additionally, the potential adaptation of this technology for surveillance or behavior analysis raises privacy and civil liberty concerns. Addressing these ethical considerations requires ongoing collaboration between researchers, ethicists, policymakers, and the broader community to ensure the responsible development and deployment of gesture generation technologies."}, {"title": "A.1 Gesture Smoothing and Aligning", "content": "As shown in 1, part of the post-processing section included a Gesture Smoothing and Temporal Alignment module. Both of those were crucial in optimizing the gesture look and accuracy, and while they are not essential to the model's outputs, their development should be noted.\nA.1.1 Gesture Smoothing\nTo generate realistic and smooth gesture sequences, we propose a novel gesture smoothing algorithm that operates on the raw output of the LLBM-AVA model. This algorithm is designed to reduce jitter and sudden movements while preserving the overall dynamics and expressiveness of the generated gestures."}, {"title": "A.1.1 Gesture Smoothing", "content": "Algorithm 1 Gesture Smoothing Algorithm\nRequire: Gesture sequence $G = [g_1, g_2,...,g_T]$, where each $g_t \\in \\mathbb{R}^D$\nEnsure: Smoothed gesture sequence $\\tilde{G} = [\\tilde{1}, \\tilde{2},..., \\tilde{T}]$\n1: Compute the velocity sequence $V = [V_1, V_2, . . ., V_{T-1}]$, where $v_t = g_{t+1} - g_t$\n2: Apply a Gaussian smoothing filter to V:\n3: for t = 1 to T - 1 do\n4: $\\tilde{v}_t = \\sum_{k=-K}^{K} v_{t+k} \\exp(-\\frac{k^2}{2\\sigma^2})$\\\nAssume $v_t = 0$ for $t \\notin [1,T - 1]$\n5: end for\n6: Initialize $\\tilde{g}_1 = g_1$\n7: for t = 2 to T do\n8: $\\tilde{g}_t = \\tilde{g}_{t-1} + \\tilde{v}_{t-1}$\n9: end for\n10: return G\nThe Gaussian smoothing filter acts as a low-pass filter, attenuating high-frequency components in the velocity domain while preserving the overall shape and dynamics of the gesture sequence. The filter size K and standard deviation $\\sigma$ control the smoothing. K, the filter size, determines the breadth of the smoothing window. A larger K incorporates more neighboring points in the smoothing process, leading to greater smoothing and less sensitivity to short-term fluctuations. $\\sigma$, the standard deviation, controls the weighting of neighboring points. A larger $\\sigma$ results in a wider spread of the Gaussian kernel, providing a smoother transition between points at the expense of potentially oversmoothing dynamic gestures.\nA.1.2 Temporal Alignment of Gestures\nPart of our approach is aligning the multimodal input to the frame-by-frame gestures to ensure accurate mapping. To do this, we introduce a temporal alignment post-processing approach that synchronizes the generated gestures with the input modality (Kucherenko et al., 2022). The temporal alignment module takes as input the encoded features $S \\in \\mathbb{R}^{L\\times d_S}$, where L is the sequence length and $d_S$ is the feature dimensionality, and the generated gesture sequence G. The module learns a mapping between the modality features and the gesture sequence, aligning them in time. We employ a multi-head attention mechanism to compute the alignment scores between the modality features and the gesture sequence:\n$A = \\text{softmax} (\\frac{QK^T}{\\sqrt{d_k}})$ (18)\nwhere $Q = WS$ and $K = W_k G$ are the query and key matrices, respectively, $W_q \\in \\mathbb{R}^{d_k\\times d_S}$ and $W_k \\in \\mathbb{R}^{d_k\\times D}$ are learnable projection matrices, and $d_k$ is the dimensionality of the query and key vectors. The aligned gesture sequence $\\tilde{G} \\in \\mathbb{R}^{L\\times D}$ is then computed as:\n$\\hat{G} = A G$ (19)\nThis alignment process ensures that the generated gestures are synchronized with the corresponding segments. The temporally aligned gesture sequence $\\hat{G}$ is then used as the final output of the LBLM-AVA model, replacing the original generated gesture sequence G.\nA.2 Gesture Generation Approach\nIn the Dataset section, the approach for gathering data and actually outputting gestures was discussed. The approach is as follows 3:\nA.3 Real Mesh Example\nThe figure below is an example of how a mesh would look like. Our generated gestures are mapped onto a mesh such as this and those meshes become animated."}]}