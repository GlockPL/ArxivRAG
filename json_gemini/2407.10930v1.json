{"title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together", "authors": ["Dilara Soylu", "Christopher Potts", "Omar Khattab"], "abstract": "Natural Language Processing (NLP) systems are increasingly taking the form of multi-stage pipelines involving multiple distinct language models (LMs) and prompting strategies. Here we address the question of how to fine-tune such systems to improve their performance. We cast this as a problem of optimizing the under-lying LM weights and the prompting strategies together, and consider a challenging but highly realistic scenario in which we have no gold la-bels for any intermediate stages in the pipeline. To address this challenge, we evaluate approxi-mate optimization strategies in which we boot-strap training labels for all pipeline stages and use these to optimize the pipeline's prompts and fine-tune its weights alternatingly. In ex-periments with multi-hop QA, mathematical reasoning, and feature-based classification, we find that simple approaches for optimizing the prompts and weights together outperform di-rectly optimizing weights alone and prompts alone by up to 65% and 5%, respectively, on av-erage across LMs and tasks. We will release our new optimizers in DSPy at http://dspy.ai.", "sections": [{"title": "1 Introduction", "content": "While the capabilities of language models (LMs) continue to grow, recent work has shown the poten-tial of building more powerful Natural Language Processing (NLP) systems by composing multi-ple skills of LMs into pipelines. Examples of this include systems for retrieval-augmented genera-tion (Guu et al., 2020; Lewis et al., 2020), multi-hop reasoning (Qi et al., 2021; Khattab et al., 2021), information extraction (Pourreza and Rafiei, 2023; D'Oosterlinck et al., 2024), and other sophisticated pipelines (Dohan et al., 2022; Khattab et al., 2022; Beurer-Kellner et al., 2023; Schlag et al., 2023).\nSuch LM Programs offer much more control for designing NLP systems, as they break down problems into modular, more manageable sub-tasks that can be assigned to LMs. If we could teach these LMs to accurately conduct their easier sub-tasks and to communicate effectively within multi-stage pipelines, this could greatly expand the scope of reliable NLP systems we can build.\nTo this end, Khattab et al. (2023) recently in-troduced the DSPy framework for defining and automatically optimizing LM Programs. In it, a program is defined as a function \u03a6 that composes a set of stages, which we will refer to as language modules M = (M1,..., M|M|), into a pipeline. Each module Mi specifies a fuzzy natural-language transformation (e.g., generating a summary of a supplied document) that needs to be learned. To do so, each module learns a particular prompt (tem-plate) \u03c0 to make a call to a particular LM with weights \u03b8. The optimization problem is then de-fined as maximizing the expected performance (per a downstream metric \u00b5) of the program \u03a6 over a set of inputs by updating each module's \u03c0 and \u03b8. Existing work (Khattab et al., 2023; Opsahl-Ong et al., 2024) has studied optimizing the discrete string prompt of each module and has considered simple approaches for fine-tuning each module's LM weights. In this empirical study, we investigate updating each module's prompt and LM weights together to maximize a downstream metric on the final output of the program. Doing this is chal-lenging as \u03a6 is not generally differentiable and its modules Mi generally lack labeled outputs and exhibit sophisticated dependencies. Moreover, in realistic settings, the training set is usually very small and only a small number of LM calls are possible for training and inference.\nTo address this challenge, we propose to alter-nate between optimizing prompts and fine-tuning LM weights and evaluate approximate optimiza-tion strategies in which we bootstrap training la-bels for all pipeline modules. In experiments with multi-hop QA (HotPotQA), mathematical rea-soning (GSM8K), and feature-based classification (Iris), we show that these tandem strategies"}, {"title": "2 Problem Statement", "content": "We are given an LM program \u03a6, which operates like a blackbox function \u03a6 : X \u2192 Y, in which X and Y are typically in natural language (e.g., ques-tions and their program generated answers, respec-tively). For example, we may have a program \u03a6 for answering complex questions with short factoid answers. In the course of its execution, \u03a6 makes one or more calls to each of |M| \u2265 1 language modules, M = (M1,..., M|M|).\nFor example, the program may implement a multi-hop, retrieval-augmented pipeline for ques-tion answering. This common pipeline (Qi et al., 2021; Khattab et al., 2021; Press et al., 2023; Khat-tab et al., 2022) breaks down the input into sub-questions that are used to iteratively find relevant passages (e.g., from a corpus like Wikipedia) until the question can be faithfully answered. In general terms, each module Mi : Xi \u2192 Vi is a declara-tive LM invocation that defines, in inherently fuzzy natural-language terms, an input Xi domain (like a user-supplied question and a set of retrieved pas-sages) and an output Vi co-domain (like a search query to find additional relevant passages).\nWe seek to implement each language module as some specific, well-tuned strategy for invoking an underlying language model LM. Concretely, we assume that a module Mi will be fully implemented by specifying (1) the string prompt \u03c0i in which the module inputs Xi are plugged in to decode the module outputs Vi and (2) the floating-point weights \u03b8i assigned to the parameters of LM in the course of this module. We refer to the version of \u03a6 in which the prompts and LM weights are assigned explicitly to \u03a0 and \u0398, respectively, as \u03a6(\u0398,\u03a0).\nGiven nothing but a small training set X = {(x1, m1),..., (x|X|,m|X|))} of inputs xi \u2208 X and optional metadata like output labels or other hints mi \u2208 M that can be used for determining the correctness of a given program run, and a metric \u03bc: Y \u00d7 M \u2192 R, our goal is to optimize \u03a6, that is, configure its modules' prompts and LM weights to maximize the following objective:\n$\\arg \\max_{\\Theta,\\Pi} \\frac{1}{|X|} \\sum_{(x,m) \\in X} \\mu(\\Phi_{(\\Theta,\\Pi)}(x), m)$ \nResearchers tuning LM pipelines are in effect seek-ing to achieve this objective. It is also a very large subspace of the optimization problem in the DSPy framework for LM programs. Unfortunately, this problem is intractable: we don't have gradients or intermediate output labels to optimize each mod-ule, so we seek approximate strategies for such optimization."}, {"title": "3 Alternating Prompt and Weight Optimization Steps for LM Programs", "content": "We now introduce the BetterTogether algorithm, which simply alternates prompt and weight opti-mization steps for LM programs. We hypothesize that, when an LM is used to teach itself how to tackle the task defined by an LM program, optimiz-ing prompts and fine-tuning LM weights are both essential to achieve the highest quality. In particu-lar, we expect that (1) prompt optimization before fine-tuning can lead to more successful datapoints for fine-tuning and (2) prompt optimization after fine-tuning can make adjustments to the behavior of the LM program that lead to higher quality. Consid-ering that fine-tuning is often perceived as a more powerful tool, this can be surprising, especially when both forms of optimization are ultimately applied over the same set of training inputs X.\nAccordingly, the general optimization frame-work for our algorithm is defined in Algorithm 1. Given a program \u03a6, the algorithm begins by opti-mizing \u03a6's prompts, then fine-tuning its set of LM weights, and finally optimizing its prompts again. In principle, each of these steps could be treated"}, {"title": "4 Experimental Evaluation", "content": "We now seek to evaluate our hypothesis on the importance of optimizing both prompts and LM weights of LM programs. We conduct our eval-uation across three datasets that span different tasks (and thus LM programs) each. In partic-"}, {"title": "5 Results & Discussion", "content": "Table 1 reports how each of the strategies described in Section 3 perform on the held-out test sets of our datasets. Reported values are averaged across three runs with unique random seeds. Appendix D separately reports the results from each run.\nIn 7 out of the 9 dataset and LM pairs, we ob-serve that the best-performing strategies are always strategies that utilize prompt (\u03a0) and weight (\u0398) optimization steps together, although there is no clear winner among the three methods that optimize both. Overall, optimizing prompts is essential on all the tasks, but optimizing prompts and weights together leads to strong gains over the best setting that only optimizes one of the two.\nIn summary, we have proposed to alternate be-tween prompt optimization and fine-tuning LM weights. In experiments with multi-hop QA (HotPotQA), mathematical reasoning (GSM8K), and feature-based classification (Iris), we show that our strategies are highly effective for getting an LM to teach itself to perform an LM program via boot-strapping, leading to 5-78% gains for HotPotQA, 2.5-10% gains for GSM8K, and -5.9\u2013136% gains for Iris."}, {"title": "6 Limitations", "content": "While this short paper presents strong evidence from nine case studies in total, spanning three tasks (and their corresponding LM programs) and three LMs, it is possible that other tasks, programs, or LMs will change the pattern in unforeseen ways. In particular, we have only experimented with weight optimization in the form of LoRA fine-tuning of pre-trained models. It is in principle possible that some other fine-tuning strategy would be so pow-erful and cost-effective as to remove the need for prompt optimization.\nIn addition, though we expect our findings to in-form many researchers and practitioners interested in optimizing LM programs, and encourage them to explore optimizing prompts and fine-tuning LM weights together, we do not yet understand why both are important. The role of prompt optimiza-tion and the role of fine-tuning multi-stage LM pro-grams are both new, and the relative lack of deep understanding of these roles in the emerging litera-ture could pose risks in unanticipated interactions between these components, compared with stan-dard gradient descent for neural networks, which has been studied for decades."}, {"title": "Appendices", "content": "The DSPy programs for HotPotQA, GSM8K, and Iris are shared in Snippets 1, 2, 3, respectively."}, {"title": "B Asset Information", "content": "We share the associated licenses for the models and datasets we used below. For models, we list the specific HuggingFace model id we used to retrieve the respective weights.\n1. mistralai/Mistral-7b-Instruct-v0.2: Apache License 2.0\n2. meta-llama/Llama-2-7b-chat-hf: Meta Llama 2 Community License at https://ai.meta. com/llama/license/"}, {"title": "C Implementation Details", "content": "In this section, we share the implementation details as it pertains to sizes of the splits, LM sampling, fine-tuning, and compute requirements. We also share the details for how we compute the gains reported throughout the paper.\nSplit Sizes For optimizing prompt templates with BootstrapFewshotRandomSearch (BFRS), we sub-sample 100 examples from the training set for BFRS training set and 250 examples for its validation set. We allow BFRS to use up to 3 boostrapped as well as 3 labeled in-context-examples to search over 6 candidate few-shot prompts.\nThe original Iris dataset has a total of 150 examples across all the splits. We re-split all the data-points into train, development, and test sets, each with 50 examples. We use this test set to report our final numbers. From the training split, we use a 15 / 35 sub-split for internal prompt-optimization training and validation, respectively.\nSampling For sampling, we host our models in Docker (Merkel, 2014) instances through HuggingFace's text-generation-inference (HuggingFace, 2023) toolkit. We keep the sampling parameters the same across all experiments, using TopK sampling with a temperature of 0.1, and top_k of 0.97, until the model either generates a stopping string or a total of 1024 tokens (including the tokens in the prompt, if supplied).\nFine-tuning For fine-tuning, we use Low Rank Adaptation (LoRA) (Hu et al., 2021) to train the query and key self-attention layers of our models, using a LoRA rank of 32, alpha of 64, with no dropout. We fine-tune all of our models for 5 epochs using bfloat16 precision, with a learning rate of le-5 and an effective batch size of 8. We use gradient accumulation steps larger than 1 in order to effectively use a large batch size, without having to fit all the batch in memory at once.\nCompute Requirement We use A100 GPUs to run our experiments. The total time it takes to run the experiments varies based on the strategy, LM and dataset. Total approximate GPU hours to produce Table 1 was \u224875 hours."}, {"title": "D Extended Results", "content": "The results shared in 1 are the average of three runs. Tables 2, 3, and 4 show the breakdown of the individual runs for HotPotQA GSM8K and Iris respectively."}]}