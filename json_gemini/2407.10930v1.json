{"title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together", "authors": ["Dilara Soylu", "Christopher Potts", "Omar Khattab"], "abstract": "Natural Language Processing (NLP) systems are increasingly taking the form of multi-stage pipelines involving multiple distinct language models (LMs) and prompting strategies. Here we address the question of how to fine-tune such systems to improve their performance. We cast this as a problem of optimizing the under-lying LM weights and the prompting strategies together, and consider a challenging but highly realistic scenario in which we have no gold labels for any intermediate stages in the pipeline. To address this challenge, we evaluate approximate optimization strategies in which we bootstrap training labels for all pipeline stages and use these to optimize the pipeline's prompts and fine-tune its weights alternatingly. In experiments with multi-hop QA, mathematical reasoning, and feature-based classification, we find that simple approaches for optimizing the prompts and weights together outperform directly optimizing weights alone and prompts alone by up to 65% and 5%, respectively, on average across LMs and tasks. We will release our new optimizers in DSPy at http://dspy.ai.", "sections": [{"title": "1 Introduction", "content": "While the capabilities of language models (LMs) continue to grow, recent work has shown the potential of building more powerful Natural Language Processing (NLP) systems by composing multiple skills of LMs into pipelines. Examples of this include systems for retrieval-augmented generation (Guu et al., 2020; Lewis et al., 2020), multi-hop reasoning (Qi et al., 2021; Khattab et al., 2021), information extraction (Pourreza and Rafiei, 2023; D'Oosterlinck et al., 2024), and other sophisticated pipelines (Dohan et al., 2022; Khattab et al., 2022; Beurer-Kellner et al., 2023; Schlag et al., 2023).\nSuch LM Programs offer much more control for designing NLP systems, as they break down problems into modular, more manageable sub-tasks that can be assigned to LMs. If we could teach these LMs to accurately conduct their easier sub-tasks and to communicate effectively within multi-stage pipelines, this could greatly expand the scope of reliable NLP systems we can build.\nTo this end, Khattab et al. (2023) recently in-troduced the DSPy framework for defining and automatically optimizing LM Programs. In it, a program is defined as a function \u03a6 that composes a set of stages, which we will refer to as language modules M = (M1,..., M|M|), into a pipeline. Each module Mi specifies a fuzzy natural-language transformation (e.g., generating a summary of a supplied document) that needs to be learned. To do so, each module learns a particular prompt (template) \u03c0 to make a call to a particular LM with weights \u03b8. The optimization problem is then defined as maximizing the expected performance (per a downstream metric \u00b5) of the program \u03a6 over a set of inputs by updating each module's \u03c0 and \u03b8. Existing work (Khattab et al., 2023; Opsahl-Ong et al., 2024) has studied optimizing the discrete string prompt of each module and has considered simple approaches for fine-tuning each module's LM weights. In this empirical study, we investigate updating each module's prompt and LM weights together to maximize a downstream metric on the final output of the program. Doing this is challenging as \u0398 is not generally differentiable and its modules Mi generally lack labeled outputs and exhibit sophisticated dependencies. Moreover, in realistic settings, the training set is usually very small and only a small number of LM calls are possible for training and inference.\nTo address this challenge, we propose to alternate between optimizing prompts and fine-tuning LM weights and evaluate approximate optimization strategies in which we bootstrap training labels for all pipeline modules. In experiments with multi-hop QA (HotPotQA), mathematical reasoning (GSM8K), and feature-based classification (Iris), we show that these tandem strategies"}, {"title": "2 Problem Statement", "content": "We are given an LM program \u03a6, which operates like a blackbox function \u03a6 : X \u2192 Y, in which X and Y are typically in natural language (e.g., questions and their program generated answers, respectively). For example, we may have a program \u03a6 for answering complex questions with short factoid answers. In the course of its execution, \u03a6 makes one or more calls to each of |M| \u2265 1 language modules, M = (M1,..., M|M|).\nFor example, the program may implement a multi-hop, retrieval-augmented pipeline for question answering. This common pipeline (Qi et al., 2021; Khattab et al., 2021; Press et al., 2023; Khattab et al., 2022) breaks down the input into sub-questions that are used to iteratively find relevant passages (e.g., from a corpus like Wikipedia) until the question can be faithfully answered. In general terms, each module Mi : Xi \u2192 Vi is a declarative LM invocation that defines, in inherently fuzzy natural-language terms, an input Xi domain (like a user-supplied question and a set of retrieved passages) and an output Vi co-domain (like a search query to find additional relevant passages).\nWe seek to implement each language module as some specific, well-tuned strategy for invoking an underlying language model LM. Concretely, we assume that a module Mi will be fully implemented by specifying (1) the string prompt \u03c0i in which the module inputs Xi are plugged in to decode the module outputs Vi and (2) the floating-point weights \u03b8i assigned to the parameters of LM in the course of this module. We refer to the version of \u03a6 in which the prompts and LM weights are assigned explicitly to \u03a0 and \u0398, respectively, as \u03a6(\u0398,\u03a0).\nGiven nothing but a small training set X = {(x1, m1),..., (x|X|,m|X|))} of inputs xi \u2208 X and optional metadata like output labels or other hints mi \u2208 M that can be used for determining the correctness of a given program run, and a metric \u03bc: Y \u00d7 M \u2192 R, our goal is to optimize \u03a6, that is, configure its modules' prompts and LM weights to maximize the following objective:\n$\\arg \\max_{\\Theta,\\Pi} \\frac{1}{|X|} \\sum_{(x,m)\\in X} \\mu(\\Phi_{(\\Theta,\\pi)}(x), m)$\nResearchers tuning LM pipelines are in effect seeking to achieve this objective. It is also a very large subspace of the optimization problem in the DSPy framework for LM programs. Unfortunately, this problem is intractable: we don't have gradients or intermediate output labels to optimize each module, so we seek approximate strategies for such optimization."}, {"title": "3 Alternating Prompt and Weight Optimization Steps for LM Programs", "content": "We now introduce the BetterTogether algorithm, which simply alternates prompt and weight optimization steps for LM programs. We hypothesize that, when an LM is used to teach itself how to tackle the task defined by an LM program, optimizing prompts and fine-tuning LM weights are both essential to achieve the highest quality. In particular, we expect that (1) prompt optimization before fine-tuning can lead to more successful datapoints for fine-tuning and (2) prompt optimization after fine-tuning can make adjustments to the behavior of the LM program that lead to higher quality. Considering that fine-tuning is often perceived as a more powerful tool, this can be surprising, especially when both forms of optimization are ultimately applied over the same set of training inputs X.\nAccordingly, the general optimization framework for our algorithm is defined in Algorithm 1. Given a program \u03a6, the algorithm begins by optimizing \u03a6's prompts, then fine-tuning its set of LM weights, and finally optimizing its prompts again. In principle, each of these steps could be treated"}, {"title": "4 Experimental Evaluation", "content": "We now seek to evaluate our hypothesis on the importance of optimizing both prompts and LM weights of LM programs. We conduct our evaluation across three datasets that span different tasks (and thus LM programs) each. In particular, we use HotPotQA (Yang et al., 2018) for multi-hop reasoning, GSM8K (Cobbe et al., 2021) for arithmetic reasoning, and Iris (Fisher, 1988) for classification. Unless otherwise specified, we use 1000 training set and 500 development set examples for each dataset. We conduct our main experiments using the same model for prompt optimization, bootstrapping training traces, and fine-tuning. We experiment with three models: mistral-7b-instruct-v0.2 (Jiang et al., 2023), 1lama-2-7b-chat (Touvron et al., 2023), 1lama-3-8b-instruct (MetaAI, 2024).\nWe implement all of our programs and optimizers as extensions to the DSPy framework. All evaluation results are the average of three random seeds, which are used to shuffle our training sets before optimization. Full text for programs is shared in Appendix A. Appendices B and C report the license information for all LMs and datasets used as well as our implementation details (e.g., hyperparameters and software), respectively.\nMulti-hop Reasoning HotPotQA (in the \"full-wiki\" setting) is a question answering task in which systems must find two Wikipedia pages via search and use them to answer a factoid question. Therefore it can be implemented as a program that has three LM modules: the first two for generating search queries (i.e., hops) and the last one for generating an answer. Each module uses Chain-of-Thought (CoT; Wei et al. 2022) to generate its outputs, producing a reasoning string before the search query or the answer. Search queries are passed to a frozen ColBERTv2 (Santhanam et al.,"}, {"title": "5 Results & Discussion", "content": "Table 1 reports how each of the strategies described in Section 3 perform on the held-out test sets of our datasets. Reported values are averaged across three runs with unique random seeds. Appendix D separately reports the results from each run.\nIn 7 out of the 9 dataset and LM pairs, we observe that the best-performing strategies are always strategies that utilize prompt (\u03a0) and weight (\u0398) optimization steps together, although there is no clear winner among the three methods that optimize both. Overall, optimizing prompts is essential on all the tasks, but optimizing prompts and weights together leads to strong gains over the best setting that only optimizes one of the two.\nIn summary, we have proposed to alternate between prompt optimization and fine-tuning LM weights. In experiments with multi-hop QA (HotPotQA), mathematical reasoning (GSM8K), and feature-based classification (Iris), we show that our strategies are highly effective for getting an LM to teach itself to perform an LM program via bootstrapping, leading to 5-78% gains for HotPotQA, 2.5-10% gains for GSM8K, and -5.9\u2013136% gains for Iris."}, {"title": "6 Limitations", "content": "While this short paper presents strong evidence from nine case studies in total, spanning three tasks (and their corresponding LM programs) and three LMs, it is possible that other tasks, programs, or LMs will change the pattern in unforeseen ways. In particular, we have only experimented with weight optimization in the form of LoRA fine-tuning of pre-trained models. It is in principle possible that some other fine-tuning strategy would be so powerful and cost-effective as to remove the need for prompt optimization.\nIn addition, though we expect our findings to inform many researchers and practitioners interested in optimizing LM programs, and encourage them to explore optimizing prompts and fine-tuning LM weights together, we do not yet understand why both are important. The role of prompt optimization and the role of fine-tuning multi-stage LM programs are both new, and the relative lack of deep understanding of these roles in the emerging literature could pose risks in unanticipated interactions between these components, compared with standard gradient descent for neural networks, which has been studied for decades."}, {"title": "Appendices", "content": "A Programs\nThe DSPy programs for HotPotQA, GSM8K, and Iris are shared in Snippets 1, 2, 3, respectively.\nB Asset Information\nWe share the associated licenses for the models and datasets we used below. For models, we list the specific HuggingFace model id we used to retrieve the respective weights."}, {"title": "C Implementation Details", "content": "In this section, we share the implementation details as it pertains to sizes of the splits, LM sampling, fine-tuning, and compute requirements. We also share the details for how we compute the gains reported throughout the paper.\nSplit Sizes For optimizing prompt templates with BootstrapFewshotRandomSearch (BFRS), we sub-sample 100 examples from the training set for BFRS training set and 250 examples for its validation set. We allow BFRS to use up to 3 boostrapped as well as 3 labeled in-context-examples to search over 6 candidate few-shot prompts.\nThe original Iris dataset has a total of 150 examples across all the splits. We re-split all the data-points into train, development, and test sets, each with 50 examples. We use this test set to report our final numbers. From the training split, we use a 15 / 35 sub-split for internal prompt-optimization training and validation, respectively.\nSampling For sampling, we host our models in Docker (Merkel, 2014) instances through HuggingFace's text-generation-inference (HuggingFace, 2023) toolkit. We keep the sampling parameters the same across all experiments, using TopK sampling with a temperature of 0.1, and top_k of 0.97, until the model either generates a stopping string or a total of 1024 tokens (including the tokens in the prompt, if supplied).\nFine-tuning For fine-tuning, we use Low Rank Adaptation (LoRA) (Hu et al., 2021) to train the query and key self-attention layers of our models, using a LoRA rank of 32, alpha of 64, with no dropout. We fine-tune all of our models for 5 epochs using bfloat16 precision, with a learning rate of le-5 and an effective batch size of 8. We use gradient accumulation steps larger than 1 in order to effectively use a large batch size, without having to fit all the batch in memory at once.\nCompute Requirement We use A100 GPUs to run our experiments. The total time it takes to run the experiments varies based on the strategy, LM and dataset. Total approximate GPU hours to produce Table 1 was \u224875 hours."}, {"title": "D Extended Results", "content": "The results shared in 1 are the average of three runs. Tables 2, 3, and 4 show the breakdown of the individual runs for HotPotQA GSM8K and Iris respectively."}]}