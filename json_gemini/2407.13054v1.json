{"title": "Comprehensive Review and Empirical Evaluation of Causal Discovery Algorithms for Numerical Data", "authors": ["Wenjin Niu", "Zijun Gao", "Liyan Song", "Lingbo Li"], "abstract": "Causal analysis has become an essential component in understanding the underlying causes of phenomena across various fields. Despite its significance, the existing literature on causal discovery algorithms is fragmented, with inconsistent methodologies and a lack of comprehensive evaluations. This study addresses these gaps by conducting an exhaustive review and empirical evaluation of causal discovery methods for numerical data, aiming to provide a clearer and more structured understanding of the field. Our research began with a comprehensive literature review spanning over a decade, revealing that existing surveys fall short in covering the vast array of causal discovery advancements. We meticulously analyzed over 200 scholarly articles to identify 24 distinct algorithms. This extensive analysis led to the development of a novel taxonomy tailored to the complexities of causal discovery, categorizing methods into six main types. Addressing the lack of comprehensive evaluations, our study conducts an extensive empirical assessment of more than 20 causal discovery algorithms on synthetic and real-world datasets. We categorize synthetic datasets based on size, linearity, and noise distribution, employing 5 evaluation metrics, and summarized the top-3 algorithm recommendations for different data scenarios. The recommendations have been validated on 2 real-world datasets. Our results highlight the significant impact of dataset characteristics on algorithm performance. Moreover, a metadata extraction strategy was developed to assist users in algorithm selection on unknown datasets.", "sections": [{"title": "1 Introduction", "content": "Causality Research, as a dynamically evolving interdisciplinary field, has garnered increasing scholarly attention. This section elaborates on the fundamental concepts, evolution, and classifications of causal analysis, addresses the limitations of previous studies, and introduces the innovative contributions and specific objectives of the current work.\nCausal analysis employs a systematic approach to uncovering the underlying causes of phenomena, primarily addressing the question of \"Why.\" Since Granger's seminal work in 1969 (Granger, 1969), which introduced a mathematical concept of causality, the field has expanded from philosophy into economics (Imbens, 2004) and other domains such as medicine (Mani and Cooper, 2000), environmental science (Li et al., 2014), and dynamics (Hu et al., 2015). Recently, the rapid advancement of Artificial Intelligence (AI) has opened new avenues for causal analysis. The integration of machine learning has enhanced the precision and efficiency of data processing for causal inference, while causal learning has established a more reliable and trustworthy framework for machine learning (Scholkopf, 2019; Makhlouf et al., 2020). These two disciplines mutually reinforce each other, driving significant advancements in scientific research.\nGiven that correlation does not imply causation, causal research necessitates a thorough investigation beyond simple association analysis. Pearl's work has provided a widely accepted framework (Pearl et al., 2000), known as the \"Ladder of Causation,\" which delineates three stages: Association, Intervention, and Counterfactual. The initial stage, Association, involves observing relationships between variables, yet it is insufficient for identifying confounders or selection bias that may lead to spurious causation (Cheng et al., 2019). The second stage, Intervention, involves controlled experiments to quantify the causal impact of one variable on another. The final stage, Counterfactual analysis, requires a deep understanding of the causal mechanisms underlying the phenomena.\nResearch on causal analysis is primarily categorized into two areas (Nogueira et al., 2022): Causal Inference and Causal Discovery. Causal Inference typically progresses from cause to effect, focusing on the quantitative problems of \"Intervention Stage\" within the causation ladder framework (Peters et al., 2017). The central concept of inference involves controlled trials (Yao et al., 2021), where both experimental and control groups are observed to determine the effects of interventions. In contrast, Causal Discovery seeks to identify causal relationships from observed outcomes, emphasizing the qualitative problems to learn causal structures (Gelman, 2011). Upon uncovering causal mechanisms, it becomes possible to infer outcomes based on hypothetical scenarios that have not occurred. Despite their inverse logical relationship, causal discovery and causal inference differ significantly in their research methodologies, algorithms, and applications."}, {"title": "2 Background", "content": "Here we will analyze the preliminaries, related survey literature, and research gaps in causal discovery. The section aims to structure the knowledge body of this academic domain systematically."}, {"title": "2.1 Preliminaries", "content": "This section presents the fundamental definitions and corresponding notations associated with causal discovery. One needs to state that matrices are denoted by uppercase bold letters, whereas vectors are indicated by lowercase bold letters. Consider the dataset denoted by X, which manifests as a m\u00d7n matrix. Here, x_n designates the nth variable, while x embodies m observations. This endeavour categorizes observational data into cross-sectional and time-series data, as defined below.\nDefinition 1 (Cross-sectional Data): Cross-sectional data is a set of observations collected from subjects at one time point.\nNote that we mainly focus on a common type of cross-sectional data, namely independent and identically distributed (i.i.d.) data.\nDefinition 2 (Time Series): Time series is a sequence of data points arranged in temporal order. Given a time series x_m, an observation at a specific temporal point t is represented as x_t.\nThe concept of time lag is introduced to discern the demarcation between time-delay and instantaneous causality."}, {"title": "2.2 Assumptions for Causality", "content": "When conducting causal discovery tasks, it is crucial to consider certain foundational assumptions. All the methods analyzed in this study are based on at least one of these assumptions. These assumptions help in relating causality to probability densities (Spirtes and Zhang, 2016)."}, {"title": "2.2.1 THE CAUSAL MARKOV ASSUMPTION", "content": "Spirtes and Zhang (2016) argue that in a dataset, all features are independent of their non-effects (nondescendants in the causal graph) conditional on their direct causes (parents in the causal graph). However, it is important to note that the Causal Markov Assumption can be an oversimplification. It assumes that causality is the sole reason for associations between all features, which is not always true, as other kinds of associations exist as well.\nFor instance, consider the case of ice cream sales and drowning incidents. According to the Causal Markov Assumption, if these two features were present in a dataset and shared an association, it should imply a causal link. However, we know this isn't true. During summers, both ice cream sales and swimming activities increase. While there might be some conditional dependencies between the two due to common factors, confusing correlation with causality would lead to spurious results."}, {"title": "2.2.2 THE CAUSAL FAITHFULNESS ASSUMPTION", "content": "The Causal Faithfulness Assumption asserts that all and only the true causal links between features are represented in the observed data. This means there are no latent or unobserved confounding variables impacting the observed features and their underlying causal relationships (Spirtes and Zhang, 2016).\nFor example, consider a dataset of patients in a lung cancer ward, with smoking habits as one of the features and lung cancer as the target variable. To determine whether smoking causes lung cancer, the causal faithfulness assumption holds if there are no unmeasured factors (like genetic vulnerability) linked to both smoking habits and lung cancer but are absent from the observed dataset. If the assumption does not hold, it indicates the presence of confounding factors. For instance, if the dataset lacks information about the patients' medical history or genetic predisposition, the observed causal link between smoking and lung cancer may be spurious. In such a scenario, smoking habits may appear to cause lung cancer in the data, but the true causal relationship is distorted by latent confounders."}, {"title": "2.2.3 MARKOV EQUIVALENCE CLASSES (MEC)", "content": "Markov Equivalence Classes (MEC) are another important concept in causal discovery. MECs group statistically indistinguishable causal models that make the same predictions about the probability distributions of observed variables (Spirtes and Zhang, 2016). In a given set of variables V, a Markov Equivalence Class represents an aggregation of DAGs that exhibit the same pattern of conditional independence associations among variables in V.\nFor example, consider two DAGs, G1 and G2. G1 and G2 lie in the same Markov Equivalence Class if, for each pair of variables v1 and v2 in V, v1 and v2 are conditionally independent given all other variables v3 in V in G1 if and only if they are conditionally independent given v3 in G2. Simply put, for G1 and G2 to lie in the same Markov Equivalence Class, they must imply the same set of conditional independence statements among the variables.\nTo illustrate this further, consider two plausible causal models, M1 and M2, and three variables P, Q, and R:\nM1: P\u2192Q\u2192R\nM2: P\u2190Q\u2190R\nIn M1, P causally influences Q, and Q causally influences R. In M2, R causally influences Q, and Q causally influences P. Despite the different causal directions, M1 and M2 lie in the same Markov Equivalence Class because the observed data generated from these models will exhibit the same trends and conditional independence associations (Spirtes and Zhang, 2016)."}, {"title": "2.2.4 THE CAUSAL SUFFICIENCY ASSUMPTION", "content": "The Causal Sufficiency Assumption (Spirtes et al., 2001) states that the common causes between any two variables of the variable set v are entirely contained within v itself, thereby excluding the presence of latent confounders. This condition is considered a prerequisite for the efficacy of most causal discovery algorithms.\nFor example, consider one causal model M and three variables, P, L, and R:\nM:P\u2190L\u2192R\nIf L is an unobserved variable, indicating that L is a hidden confounder of R and Q, then the model M does not satisfy the causal sufficiency assumption. Under this assumption, a directed edge in DAGs represents a causation from a cause to its effect."}, {"title": "2.3 Related Works", "content": "In recent years, the topic of causal discovery has garnered significant interest as researchers endeavor to uncover causal relationships from observational and experimental data. To establish a solid foundation for understanding causal discovery, it is crucial to delve into the field's core concepts and theories. An exhaustive search was conducted within the Web of Science repository to comprehensively encompass the pivotal papers in this domain, curating 301 relevant articles. In order to facilitate a more in-depth examination and exploration of these papers, a network diagram depicting co-cited literature (Figure 3) was generated.\nThe term \"co-citation\" means the phenomenon wherein two papers are simultaneously referenced by one or more subsequent papers, thereby establishing interlinked co-citation relationships between the two papers. In Figure 3, the threshold for the minimum number of co-cited articles was set at 15, affording a lucid means to observe literature of close thematic association, thereby furnishing a framework and cluster of causal discovery. We can see from the figure that foundational works by Judea Pearl, Peter Spirtes, and Clark Glymour have laid the groundwork for causal discovery.\nThe most prominent node, depicted in red, represents the seminal work of Peter Spirtes (2001). Pearl's work on causal Bayesian networks and the introduction of causal graphical models significantly advanced the field's theoretical foundation. These seminal contributions serve as the basis for further research and methodologies. Peter Spirtes introduced nonparametric structural causal models (SCM) (Pearl, 2009) as a formal and intelligible language for articulating causal knowledge and explaining causal notions used in scientific discourse. These include concepts like randomization, intervention, direct and indirect effects, confounding, counterfactuals, and attribution. The structural language's algebraic component corresponds to the potential-outcome framework, while its graphical component incorporates Wright's method of path diagrams. When combined, these components provide a robust approach for causal inference, addressing long-standing issues in empirical sciences, such as confounding control, policy evaluation, mediation analysis, and the algorithmization of counterfactuals.\nSince the 1990s, conditional independence connections in data have been used to recover the underlying causal structure. One such constraint-based algorithm, developed by Spirtes and Glymour, is the PC algorithm. This algorithm starts with an undirected graph and recursively deletes edges based on conditional independence judgments. The PC algorithm assumes no confounders (unobserved direct common causes of two measured variables) and that the causal information retrieved is asymptotically true. However, the PC algorithm has limitations, especially with high-dimensional biological datasets, where its runtime grows exponentially with the number of variables, making it inefficient for large datasets. Additionally, the order of factors in the input data influences the output, reducing its reliability. Notably, Runge et al. (2019) proposed PCMCI, represented in a blue node in Figure 3, which extends the PC algorithm to time series data.\nAnother constraint-based algorithm is the FCI algorithm, also developed by Spirtes et al. The FCI algorithm starts with a fully connected undirected graph and removes edges connecting conditionally independent variables to create a causal graph. In the second phase, it orients edges by recognizing the \"V\" and \"Y\" structures and then orients the remaining edges using a set of criteria. FCI can produce asymptotically correct results even in the presence of confounders, making it more robust than PC in dealing with latent confounders. These methods can handle a wide range of data distributions and causal links and they have efficient means of conditional independence verification. It should be noted, however, that they may not always convey entire causal information because they yield equivalence classes that represent multiple causal structures satisfying the same conditional independencies (Spirtes and Zhang, 2016).\nScore-based algorithms, which facilitate the identification of causal structures by optimizing a well-defined score function, do not entail any confounders. A well-known score-based method is Greedy Equivalence Search (GES) (Chickering, 2002), which directly searches the space of equivalence classes. GES iteratively adds or removes edges to improve the model's score until no further modifications are possible. Another recent score-based technique is the Greedy Relaxation of the Sparsest Permutation (GRaSP) (Lam et al., 2022) algorithm developed by Lam et al. This algorithm aims to discover the most sparse permutation of variables that fulfills a specific set of causal constraints by greedily adding variables to the permutation while minimizing the impact on the sparsity measure.\nHistorically, deliberations on the causal relationship amid two factors have concentrated on the standard situation, where the factors included are Gaussian and have a Linear causal relationship. On the other hand, the discrete case has been considered. The Linear Gaussian circumstance, then again, is an exemption in which the causal asymmetry does not uncover itself in the observed data or their joint dispersion. In the event that the causal asymmetry exists, we should conclude that it must have been concealed somehow, either through latent factors or factual predisposition.\nFor causal discovery from continuous data, numerous techniques based on Functional Causal Models (FCMs) have recently been presented. The effect Y is represented by an FCM as a function of the direct causes X and certain unmeasurable components or noise.\nSeveral variations of the FCM have been demonstrated to be capable of producing distinct causal directions and have found practical uses. The green node attributed to Shimizu (2006), introduces the linear, non-Gaussian, and acyclic model (LiNGAM). f is linear in the LiNGAM and at most one of the noise terms and cause X is Gaussian. The impact Y is further generated by a post nonlinear transformation on the nonlinear effect of the cause X+ noise term in the post nonlinear (PNL) causal model.\nRCD (Repetitive Causal Discovery) is another causal functional model-based method recently developed by Maeda et. al. (2020) to uncover variable pairs influenced by latent confounders and determine the causal direction between variables unaffected by confounders. Experiments with simulated and real-world data are used to validate RCD's effectiveness. RCD consists of three major phases. It begins by extracting a collection of ancestors for each variable, which indicate direct or indirect causes. The back-door criterion (Pearl et al., 2000) is used to infer causal directions between observable variables by taking into account the identified shared causes. The procedure is repeated until no additional changes occur. Second, RCD distinguishes direct causes from indirect causes via conditional independence inference. Finally, RCD finds correlated variable pairs with undetermined causal directions to identify pairs of variables impacted by the same latent confounders. As noted by Maeda et al., the experimental results show that RCD is effective at detecting latent confounders, determining causal directions, and detecting variable pairings that are affected by common confounders.\nSpirtes and Zhang's review (2016) comprehensively covers concepts and methodologies for causal inference, including manipulations, causal models, sample predictive modeling, causal predictive modeling, structural equation models, the causal Markov assumption, and the faithfulness assumption. Their review focuses on constraint-based causal structures, structural equation models, and mechanisms for distinguishing cause and effect.\nBenozzo et al. (2016) in their recent studies propose a novel approach for causal discovery in time series. The developed technique is a classification- based causal discovery technique by defining a feature space fundamentally based on Granger Causality and by using the Multivariate Auto Regressive (MAR) model as the data generator. Cross Valida- tion was used to assess the performance of the model and the results were quite promising. This makes it evident that this novel approach is a good substitute to the signal processing method for causality detection between two time series. However, as noted by Benozzo et. al., this approach is only limited to three time series at the moment.\nZheng et al. (2018) introduced DAG-NOTEARS in 2018, a method for causal discovery using continuous optimization. This method offers efficient global updates and avoids as- sumptions about the local graph structure, although it still faces challenges with nonconvex optimization problems. Another approach by Zheng et al., DAG-GNN, uses variational autoencoders with graph neural network architectures to address NP-hardness (Chickering, 1996; Chickering et al., 2004) in causal discovery.\nTraditional approaches face NP-hardness due to the combinatorial nature of the task. NP-hardness is a common issue faced by traditional approaches for causal discovery. Zheng et al. proposes another method called DAG Structure Learning with Graph Neural Net- works (DAG-GNN) to cater to this issue with the help of a continuous constraint that allows implementation of renowned continuous optimisation techniques by exploring the effective- ness of neural networks as functional approximators to develop a deep generative model."}, {"title": "2.4 Relevant Survey on Causal Analysis", "content": "In addition to the basic works and concepts mentioned in the previous section, we also need to investigate causal discovery from a more holistic perspective. Consequently, an exhaustive analysis must be conducted, combined with authoritative surveys over the past five years, to gain a comprehensive understanding of this field.\nIt is imperative to differentiate the objectives and conceptual frameworks associated with causal discovery and causal inference to further delve into these two notions, as elucidated by Guo et al. (2020). Guo et al. emphasised that causal inference involves tracing the causal path from cause to effect, aiming to understand the impact of manipulating specific variables on others. Within the realm of causal inference, Yao et al. (2021) have conducted an in-depth investigation into the concepts, methods, and applications of causal inference, contributing significantly to the field. Acknowledging the distinct nature of time series data, which differs from i.i.d. data, is essential. This distinction presents unique challenges and considerations in analysing causality.\nAdditionally, to supplement the understanding of these concepts, Nogueira et al. (2022) have analysed and compared causal discovery and inference using software tools, providing practical examples for testing. Their work contributed to the existing body of knowledge by exploring the practical application and evaluation of different approaches.\nFocusing on time series, Moraffah et al. (2021) provided a comprehensive examina- tion of causality for such data, offering insights into the generation of time series data, methodologies employed in the causal analysis, and evaluation metrics used to assess causal relationships. Notably, the study enumerated these evaluation metrics' specific attributes and characteristics, providing valuable information for researchers in selecting appropri- ate metrics for their analyses. Regrettably, their research remained confined to theoretical realms and has yet to be realized through practical trial.\nAnother article on evaluation metrics was proposed by Cheng et al. (2022). The re- search conducted thoroughly examines the evaluation methods employed in causal analysis. Their analysis referred to a wide range of considerations, including the availability and suitability of software packages, algorithms' effectiveness, and datasets' appropriateness for evaluating causal learning algorithms. By investigating these aspects, Cheng et al. provided researchers with valuable guidance for selecting appropriate evaluation methods in causal analysis studies.\nCharles K. Assaad et al. (2022) have made a notable contribution to the field of time series causal discovery, and their work serves as a pivotal reference for the research project at hand. In the theory field, they presented a comprehensive framework comprising seven distinct categories for analyzing causal relationships. In the empirical field, they employed ten algorithms to assess these methods' performance across different causal structures.\nMore recently, Runge, J. et al. (2023a) comprehensively summarized methods of causal discovery and proposed a Question-Assumptions-Data (QAD) template, embedding causal discovery into Pearl's causal ladder. They also designed a method selector to match the optimal algorithm to different graph assumptions. However, they did not further discuss parametric assumptions about datasets through experimentation.\nHasan, U. et al. (2023) summarized causal discovery methods for i.i.d. data and time series, and collected source code of relevant algorithms. They also tested and compared the performance of 9 algorithms of i.i.d. data and 7 algorithms of time series on benchmark datasets. However, they did not further test and analyze the influence of data assumptions on algorithms performance, such as dependency functions and noise distributions."}, {"title": "3 Survey Methodology", "content": "This section will introduce how to collect relevant research resources, including literature, codes, metrics, and datasets. Moreover, the last section briefly explains the analytical technologies we used."}, {"title": "3.1 Method Design", "content": "A quantitative research approach is employed to gather and analyze papers from databases systematically. This approach facilitates measuring and exploring trends, methods, datasets, and evaluation in the research domain. Google Scholar was chosen as the literature database due to its extensive coverage of scholarly articles. Information acquisition was initiated by employing a keyword search approach. Primarily, Figure 5 demonstrates the research trend of causal discovery, underpinned by the number of articles published during the preceding two decades.\nThe graph illustrates an escalating annual trend in research endeavours about causal discovery. This discernible growth can be attributed partly to the robust advancement of AI technologies in contemporary times, which has laid the groundwork for algorithmic developments facilitated by enhanced data processing capabilities.\nFurthermore, for preparing the programming underpinnings, our endeavour extends more than literature collection to encompass acquiring requisite algorithmic source code. To this end, we searched for the source code associated with each algorithm on the GitHub platform, with the integration and revision of original algorithm codes as deemed necessary.\nTo facilitate a comprehensive and impartial investigative analysis, this study employs both Comparative Analysis and Case Study methodologies. Specifically, a series of comparative experiments involving diverse algorithms on distinct artificial datasets are con- ducted. By analyzing the magnitudes of evaluation metrics, a quantitative assessment of algorithmic performance is executed, yielding overarching insights. Ultimately, to affirm the robustness and effectiveness of our findings, the conclusions are corroborated through case studies such as real datasets."}, {"title": "3.2 Data Collection", "content": "Here we executed a keyword-based search focusing on aspects within the tree graph above. The subsequent table illustrates the collection result derived from the published paper from 2004 to 2023, as sourced from Google Scholar.\nTo prevent repetitive retrieval and collection of papers, \"Causal Discovery\" is used as the basic keyword, with previously retrieved terms excluded when searching for new supplementary terms. It should be noted that we also searched for keywords related to \"causal inference\" to ensure comprehensive collection of papers within cross fields. We have identified 220 articles with the highest correlation from over 3000 related articles and have summarized the datasets, algorithms, and evaluation metrics for causal discovery. The terms \"Hits,\" \"Title,\" and \"Body\" in Table 1 refer to the number of papers returned by the search, the relevance of the titles to the desired content, and the number of papers that remain after title and body framing, respectively.\nA selection of widely employed datasets for causal discovery has been identified by synthesising diverse review articles. These datasets encompass both real-world instances and artificial constructs. To streamline ensuing experimental processes, a classification frame- work has been devised to categorise these datasets into four types of causality relationships. Additionally, the sources of these datasets are also categorized into real-world and synthetic datasets. Specifically, CausalWorld (Ahmed et al., 2020), SynTReN (Van den Bulcke et al., 2006), LUCAS (Guyon et al., 2011), ALARM (Lauritzen and Spiegelhalter, 1988) and ASIA (Lauritzen and Spiegelhalter, 1988) are synthetic datasets, while Tubingen (Mooij et al., 2016), ADNI (Petersen et al., 2010), AntiCD3/CD28 (Sachs et al., 2005), Abalone (Asuncion and Newman, 2007), fMRI (Smith et al., 2011), Causality 4 Climate (Runge et al., 2020), Traffic Prediction (Pan et al., 2018), OHDNOAA (Jangyodsuk et al., 2014), Temperature Ozone (Gong et al., 2017), Sachs (Sachs et al., 2005) and CHILD (Spiegelhalter et al., 1993) are real-world datasets. This categorisation is illustrated in Table 2.\nLikewise, we have identified performance metrics for assessing causal discovery techniques. These metrics are divided into two overarching families: graph-based metrics (Peters and B\u00fchlmann, 2015) and classification-based metrics, as expounded in Table 3. It is pertinent to highlight that nearly all metric computations necessitate the availability of both estimated DAGs and ground truth DAGs. Therefore, the prudent selection of datasets with well-established ground truth becomes imperative."}, {"title": "3.3 Survey Analysis", "content": "Utilizing a comparative analysis framework, we systematically process the performance metrics. We selected a reference algorithm exhibiting superior performance to enhance result lucidity and then calculated residuals for other algorithms compared with the reference. The overall distribution of these residuals is visually represented through the violin plots.\nNevertheless, two specific algorithms may manifest insignificant disparities between their metric values. Hence, we introduce a significance assessment mechanism to pursue a more methodical treatment of data relationships. Given the constraint that each data size consists of merely five datasets, we opt for the non-parametric Mann-Whitney U test (Mann and Whitney, 1947). This approach circumvents the necessity to assume normality in data distribution. Significance is determined based on a p-value threshold of 0.05; when under this threshold, it signals a notable divergence in the performance of the two algorithms.\nIn addition, a ranking table is formulated to delineate the evaluation outcomes for en- hancing the discernibility of inter-algorithm performance disparities. This table is ordered in a descending manner upon the average values. Meanwhile, we calculate the standard devia- tion for each algorithm's metric values; reduced standard deviation signifies less dependence of algorithmic performance on data size, which means enhanced stability of algorithms."}, {"title": "4 Causal Discovery Algorithms", "content": "Various research articles present diverse taxonomy for causal discovery, yet a universally accepted classification structure does not currently exist. Specifically, in recent years, the rapid advancements in this domain have resulted in the incompleteness and obsolescence of numerous surveys. It is essential to collect and analyse the taxonomy methodologies proposed in papers critically to establish a systematic categorisation of existing methods. Simultaneously, the devised structure should encompass as many algorithms as possible. Thus, this project diligently compiles and summarises the prevailing methods, effectively partitioning causal discovery into six fundamental categories, as shown in Figure 6: Granger- Based, Conditional Independence-Based, State Space Dynamics-Based, Structural Equation Modelling-Based, Deep Learning-Based, and Hybrid Method."}, {"title": "4.1 Granger Based Method", "content": "Granger causality (GC) is one of the pioneering measurement methods for analysing time series data. Over several decades, undergoing refinement and evolution, it still maintains an irreplaceable position in the contemporary landscape of causal discovery. The core premise of Granger causality postulates that future events do not affect the present or past, while past events potentially impact both the present and the future. When the historical information of variables x and y are included, leading to better predictions for variable y than predictions based solely on the information of y, it signifies that variable x is considered the Granger cause of variable y. In mathematical notation, the given statement can be expressed as follows (McCracken, 2016):\n$P(Y_{n+1} \\in A | \\Omega_n) \\neq P(y_{n+1} \\in A | \\Omega_n - x)$\nEquation 1 illustrates a Granger causes y, wherein the variable x and y represents two discrete time series, and the subscript n corresponds to the time point t. The all- encompassing set of information available at all points t \u2264 n is symbolically denoted as \u03a9_n. To ascertain the Granger relationship based on the aforementioned formula, the Vector Autoregressive (VAR) model (L\u00fctkepohl, 2005) stands as the prevailing technique, built upon the premise of data stationarity and equipped to forecast variable values.\nOur attention is directed towards several primary algorithms based on GC. Arize et al. (1993) put forth the Multivariate Granger (MVGC) analysis method to overcome the limitations of Pairwise Granger (PWGC), which can only deal with bivariate data. However, ensuring linearity in real-world datasets can present a significant challenge. To address this issue, Chen et al. (2004) introduced an approach known as Extended Granger Causality (EGC), specifically designed to handle nonlinear data. An alternative model catering to nonlinear time series is the Kernel Granger (KGC) method (Marinazzo et al., 2008; Liao et al., 2009; Marinazzo et al., 2011), showcasing notable attributes such as high accuracy and flexibility. When confronted with continuous time series data, Hu et al. (2014) proposed the Copula Granger method, which is capable of uncovering nonlinear and higher-order causal relationships (Kim et al., 2020; Jang et al., 2022)."}, {"title": "4.2 Condition Independence Based Method", "content": "The conditional independence-based method exhibits a close association with probabil- ity. By quantifying the mutual information (Runge, 2018) between variables, this approach enables the determination of causal relations and causal strength. A fundamental concept in this context is the transfer entropy (Schreiber, 2000), which is defined as follows:\n$T_{x \\to y} = \\sum P(Y_{n+1} ,y_n^{k(y)}, x_n^{l(x)})) log \\frac{P(Y_{n+1}|y_n^{k(y)}, x_n^{l(x)}))}{P(Y_{n+1} | y_n^{k(y)}))}$\nIn contrast to Shannon entropy, the transfer entropy is computed by the Kullback en- tropy (Kullback, 1997). In this equation, x_n represents the value of variable x at the nth time point, and likewise for the variable y, with the superscript indicating the time delay length. When T_{x\u2192y} - T_{y\u2192x} > 0, it can be inferred that variable x is the cause of variable y; conversely, variable y is the cause of variable x.\nCausal discovery algorithms based on conditional independence can be categorized into two distinct groups. The first category is the information-theoretic-based approach, with Optimal Causation Entropy (oCSE) (Sun et al., 2014, 2015) being a representative algo- rithm. OCSE is a two-step discovery algorithm explicitly designed for short time series data. The second approach is causal network-based, where the optimal causal graph is determined through statistical testing. The Peter-Clark (PC) algorithm (Kalisch and B\u00fchlman, 2007) has gained widespread adoption and has proven effective in analyzing high-dimensional time series using causal graphs. Recognizing the potential interference of latent confounders in causal detection, corresponding approaches have been developed. A notable example is the Fast Causal Inference (FCI) algorithm (Zhang, 2008; Entner and Hoyer, 2010b; Spirtes et al., 2013), a classical method that explicitly accounts for unobserved confounders. To further enhance control over false positive rates, Runge et al. introduced the PCMCI method and its variants (Runge et al., 2019; Runge, 2020; Gerhardus and Runge, 2020) by incorporating the MCI test into the PC algorithm. PCMCI is an improvement of PC in time series, which can detect contemporaneous and time-delay effects.\nCDS (Fonollosa, 2019) was proposed to detect the causal direction of bivariates. This method measures the statistical characteristics of the joint distribution of marginal variance data after conditioning the bins. This algorithm has been proven to be robust as it has a high AUC in ChaLearn causal pair challenges.\nCD-NOD (Constraint-based causal Discovery from heterogeneous/NOnstationary Data) (Huang et al., 2020) is another framework designed to discover causal relationships in data where generating processes change over time or across domains. It detects changing local mechanisms, recovers causal structures, and estimates the driving force behind nonstation- arity. This nonparametric method leverages data heterogeneity and connects nonstationar- ity with soft interventions, demonstrating efficacy on synthetic and real-world datasets like task-fMRI and stock market data."}, {"title": "4.3 State Space Dynamic Based Method", "content": "The state space dynamics-based method can be regarded as a complementary approach to address a category of data not encompassed by GC. This method investigates the causal- ity of variables within weakly coupled dynamic systems, significantly enhancing the causal discovery capability in ecological, dynamics, and other relevant domains. This method draws inspiration from the Takens theorem (Takens, 1981) and computes the bidirectional cross-correlation between two variables to establish a cross-mapping. To be specific, variable x causes y when C_{xy} > C_{yx} is satisfied, wherein C_{xy} and C_{yx} represent the Convergent Cross-Mapping (CCM) correlations from x to y and from y to x, respectively. The cal- culation formula for CCM correlation is as follows, where \u03c1 means the Pearson correlation coefficient.\n$C_{xy} = [\\rho(y, y|x)]^2$\nWe assume a classic example to explain cross-mapping further, using variable x to construct shadow manifold Mx, and y to construct My. If x leads to y, then using the neighbouring points of a certain point in My should be able to identify better the neigh- bouring points of the corresponding point in Mx. Supposing a delay of 1 and the shadow manifold graphs from two directions are shown in Figure 7, where x causes y.\nSugihara et al. (2012) introduced the concept of CCM to infer causality. This method proves advantageous for non-separable and weakly connected dynamic systems. An essen- tial feature of CCM is convergence, which means that the longer the time series used (with a larger sample size), the smaller the estimated error of the obtained cross-mapping. To overcome this limitation that necessitates long time series data, Ma et al. (2014) developed Cross Map Smoothness (CMS), specifically designed to handle varying data sizes, particu- larly short time series. Additionally, Inner Composition Alignment (IOTA) (Hempel et al., 2011; Wang et al., 2014) offers an alternative technique for short-time series analysis. Mc- Cracken et al. (2014) proposed Pairwise Asymmetric Inference (PAI) as an exploratory tool for analysing high-dimensional dynamic systems. PAI aids in uncovering causal relation- ships within complex systems by evaluating asymmetries in pairwise interactions."}, {"title": "44 Structural Equation Modelling Based Method", "content": "The three methods above are well-suited for identifying time delay causality but may not necessarily be adept at detecting instantaneous causality. On the other hand, the Structural Equation Modeling (SEM) based method represents a significant advancement and revolution in the realm of instantaneous causal discovery. This method ascertains the edges of DAG by establishing a structural equation to solve the coefficient matrix. The most basic form of the structural equation (Shimizu et al., 2006) is as follows:\nX = BX + E\nThe matrix B is referred to as the coefficient matrix, with its row and column represent- ing the two dimensions of cause and effect. Upon determining B, the causal relation can be discerned. Additionally, the matrix E represents the noise matrix in the model, usually non-Gaussian noise.\nThe Linear Non-Gaussian Acyclic Model (LiNGAM) provides a directed acyclic graph that reveals instantaneous causal relations between variables. LiNGAM assumes linear and non-Gaussian independent noise about the data generation method of the system, solving these using Independent Component Analysis (ICA) (Lee and Lee, 1998; Naik and Kumar, 2011). Since ICA algorithms typically employ FastICA and gradient-based algorithms, they may converge to local rather than global optima. To address these issues, Shimizu et al. (2011) proposed the Direct LiNGAM algorithm. Compared to ICA-LiNGAM, Direct LiNGAM produces more stable and reliable results, though it has some drawbacks. One drawback is its slower computational efficiency compared to ICA-LiNGAM. Additionally, its assumptions are relatively strict; in real-world scenarios, data generation mechanisms are often nonlinear and do not conform to its assumptions.\nTo address the gap in the LiNGAM algorithm family concerning delayed causality, Hyv\u00e4rinen et al. (2010) designed the VARLINGAM algorithm. VARLINGAM operates in two steps: first, it predicts time lag effects using a Vector Autoregression (VAR) model; second, it estimates instantaneous causality by applying LiNGAM.\nBuilding upon the insights from LiNGAM, Hoyer et al. (2008) introduced Additive Noise Models (ANM) to detect nonlinear time series, emphasizing that nonlinearities offer valuable identification power. One shortcoming of this algorithm is its high computational cost, as it involves determining the direction between pairs of variables one-on-one, making the algorithm pairwise causality.\nThe Post-Nonlinear (PNL) causal model (Zhang and Hyvarinen, 2012; Zhang et al., 2015; Uemura et al., 2022) addresses the complexities of nonlinear effects, inner noise, and measurement distortions in observed variables for causal discovery. Representing each vari- able as a function of its direct causes, an independent disturbance, and a post-nonlinear distortion, PNL can distinguish between causes and effects, especially in non-Gaussian scenarios. The model's identifiability has been extensively studied, revealing that it can generally identify causal directions except in specific conditions. Empirical results demon- strate its efficacy in various real-world data sets, making it a robust tool for causal inference in complex systems.\nPeters et al. (2013) proposed Time Series Models with Independent Noise (TiMINo) to capture both lagged and instantaneous effects. This model is based on nonlinear in- dependence tests and can perform well even when the dataset does not satisfy the causal sufficiency assumption.\nDAGs with NO TEARS (Zheng et al., 2018) is a causal discovery method that uses continuous optimization schemes to learn the structure of Directed Acyclic Graphs (DAGs) from observational data. The name \"NO TEARS\" stands for \u201cNonlinear Optimal Transfor- mations for Efficient and Accurate Recovery of Structure,\u201d emphasizing its focus on tackling issues related to learning nonlinear causal relationships. The algorithm transforms the data to make it linear or Gaussian and learns a Structural Equation Model (SEM) linking features in a causal graph. It optimizes the SEM with a focus on fit and sparsity using methods like gradient descent or ADMM. Soft thresholding promotes sparsity by zeroing out edges. The process iterates until convergence criteria are met, then returns a DAG with NO TEARS, representing causal relationships.\nGOLEM, introduced by Ng et al. (2020), is a continuous likelihood-based method for causal discovery. It uses a score-based approach with soft sparsity and DAG constraints to maximize the data probability of a linear Gaussian model. GOLEM employs two objective functions to account for noise variances and uses an \u21131 penalty for complexity. It formu- lates an unconstrained optimization problem, ensuring the graph remains a DAG under reasonable assumptions. The algorithm utilizes gradient-based optimization methods, with Adam optimizer and GPU acceleration. A post-processing step removes low-weight edges to enhance performance. GOLEM effectively restores DAG structures while managing soft constraints.\nRepetitive Causal Discovery (RCD) (Maeda and Shimizu, 2020; Maeda, 2022) is a method for identifying causal structures in data affected by latent confounders. It repeatedly"}, {"title": "4.5 Deep Learning Based Method", "content": "Deep learning-based methods have emerged as powerful tools in causal discovery, closely connected with machine learning. These methods offer significant technical advantages, par- ticularly in processing vast amounts of data. Notably, deep learning-based causal algorithms can better infer hidden variables using network information.\nFor instance, Goudet et al. (2018) designed Causal Generative Neural Networks (CGNN) to address the challenges posed by latent variables in causal analysis. CGNN is an algorithm that infers the optimal causal direction on a causal skeleton diagram, which belongs to pairwise causality. Through testing on both artificial and real-world datasets, CGNN has demonstrated advanced performance in handling potential confounders.\nDAG-GNN, developed by Yu et al. (2019), combines Graph Neural Networks (GNNs) with a score-based approach to learn DAGs from data. It uses GNNs for node embed- dings to model feature dependencies and detect causal relationships. The method starts by embedding features using GNNs, then defines a score to evaluate the causal structure. It formulates an optimization problem to maximize this score using gradient-based tech- niques like stochastic gradient descent or Adam. Causal constraints ensure acyclicity. The data is split into training and validation sets to optimize the score and train the model. After achieving optimal performance on the validation set, the algorithm returns a DAG representing causal relationships.\nAnother noteworthy algorithm is the Temporal Causal Discovery Framework (TCDF) (Nauta et al., 2019a), which effectively handles both latent and instantaneous causal effects. TCDF adopts an attention mechanism in convolutional neural networks. The attention coefficients of different variables, learned by the network, can be interpreted as the degree of correlation between variables. If the attention coefficient is below a certain threshold, it indicates no causal relationship between the two variables.\nGraNDAG (Lachapelle et al., 2019) is a novel score-based approach for learning DAGs from observational data. It adapts a recent continuous constrained optimization formulation to accommodate nonlinear relationships between variables using neural networks. This method effectively models complex interactions and avoids the combinatorial nature of the problem. By comparing GraNDAG to existing continuous optimization methods and nonlinear greedy search methods, it has been demonstrated that GraNDAG outperforms current continuous methods on most tasks and remains competitive with existing greedy search methods on important causal inference metrics.\nOrdering-Based Causal Discovery with Reinforcement Learning (CORL), developed by Wang et al. (2021), combines ordering-based causal discovery with reinforcement learning techniques to learn causal relationships by generating and refining an ordering of variables. The algorithm treats the problem as a sequential decision-making task, where a reinforce- ment learning agent arranges variables to approximate true causal relationships. A reward function provides feedback, incentivizing correct orderings and penalizing incorrect ones. The task is formulated as a Markov Decision Process (MDP), with states representing the current ordering and actions selecting the next variable. The agent is trained using rein- forcement learning algorithms like Q-learning or Proximal Policy Optimization to optimize long-term rewards. Balancing exploration and exploitation is crucial. A post-processing step, such as local search, refines the ordering to improve accuracy. The algorithm ulti- mately returns an optimal causal graph representing the relationships between features.\nMore recently, L\u00f6we et al. (2022) introduced the Amortized Causal Discovery (ACD) algorithm for time series, which is effective with small data sample sizes and demonstrates efficacy in dynamic systems. This model utilizes shared information between dynamic system variables to identify confounders in additive noise datasets."}, {"title": "4.6 Hybrid Method", "content": "Hybrid methods combine two or more algorithms to complement and optimize each other, enhancing the ability to discover causality. These methods leverage the strengths of different approaches to address their individual limitations and improve overall performance.\nOne illustrative hybrid approach is the Autoregressive Moving Average - Linear Non- Gaussian Acyclic Model (ARMA-LINGAM) (Kawahara et al., 2011), which combines Granger Causality (GC) and Structural Equation Modeling (SEM). This composite method resolves both instantaneous and delayed causality by leveraging the attributes of LiNGAM and ARMA models. ARMA-LiNGAM's integration allows for a more comprehensive analysis of time series data, accommodating both immediate and lagged effects. Incorporating deep learning techniques with GC models, Neural Granger Causality (NGC) (Tank et al., 2021b; Wang et al., 2023) stands out. NGC utilizes the Causal Multilayer Perceptron (CMLP) model to train data, thereby enhancing the accuracy of causal inference tasks. By combin- ing the predictive power of neural networks with the interpretability of Granger causality, NGC offers a robust framework for identifying causal relationships in complex datasets.\nJanzing et al. (2012) introduced Information Geometric Causal Inference (IGCI) to address the nonlinear challenges encountered by the Additive Noise Model (ANM) algo- rithm. IGCI enhances causal inference by incorporating information entropy, providing a more effective method for dealing with nonlinear data structures. This approach allows for better differentiation between cause and effect in scenarios where traditional linear models fall short.\nMao et al. (2017) extended the application of the Convergent Cross Mapping (CCM) algorithm from bivariate to multivariate analysis by incorporating transfer entropy. This integration, named Phase State Delay Reconstruction - Transfer Entropy (PSDR-TE), ef- fectively addresses the limitation of the CCM algorithm, which was originally designed for detecting bivariate relationships. PSDR-TE expands the capability of causal discovery to more complex multivariate systems, improving the detection of causal links across multiple variables.\nRaghu et al. (2018) proposed the Scalable Causation Discovery Algorithm (SCDA), which combines structural equation modeling-based and conditional independence-based methods. SCDA provides a solution for mixed data containing both continuous and discrete sequences. By integrating these two methodologies, SCDA can handle a broader range of data types and improve the robustness of causal inference in heterogeneous datasets.\nIn summary, hybrid methods in causal discovery leverage the strengths of multiple al- gorithms to address their respective weaknesses. By combining techniques such as Granger causality, structural equation modeling, neural networks, and information entropy, these hybrid approaches offer powerful tools for uncovering causal relationships in diverse and complex datasets."}, {"title": "5 Empirical Study Design", "content": "The experimental framework is structured across four distinct phases. The inaugural phase involves conducting a comparative assessment of algorithms applied to synthesised datasets with specific features while concurrently evaluating a range of performance metrics. Subsequently, the second phase encompasses presenting and analysing outcomes derived from the initial stage to extract meaningful insights. Transitioning to the third phase, real-world datasets are engaged for testing, utilising the insights garnered in the preceding phase to ascertain the optimal algorithm. This stage aims to verify whether the test results are consistent with the predicted optimal algorithm. The fourth and final phase entails deploying diverse data processing and testing methodologies to ascertain the metadata of the time series datasets. This, in turn, facilitates the extrapolation of insights from the second phase to previously unexplored datasets."}, {"title": "5.1 Datasets", "content": "This experimental inquiry necessitates two dataset categories: synthetic datasets designed to explore underlying patterns and real-world datasets serving the purpose of vali- dation. The data generation structure of the artificial dataset is shown in Figure 8.\nGiven the categorization of testing algorithms into three classes, a concomitant prepa- ration of diverse composite datasets becomes imperative. The first category entails a data generator tasked with producing causal pairs, whereby the data generation tool within the causal discovery toolbox (Kalainathan and Goudet, 2019b) was employed. The second and third categories encompass instantaneous and time-delay causality, for which the data generator in Tigramite framework, posited by Runge et al. (2023a), was harnessed for data synthesis. In the fourth category, characterized by i.i.d. data, we adopted the data generation model established within the gCastle package.\nSubsequently, we considered the dataset sizes for both time series and i.i.d. data. For time series data, the experimental framework established distinct time series lengths ranging from 50 to 300 time points for small-scale datasets, 300 to 1000 for medium-scale datasets, and 1000 to 3000 for large-scale datasets. For i.i.d. data, the sizes were similarly categorized into small (50, 100, 150, 200, 250), medium (300, 440, 580, 720, 860), and large (1000, 1400, 1800, 2200, 2600).\nThe design of dataset attributes necessitates attention to causal relationships and noise distribution types. Causal relationships within this context are bifurcated into linear and nonlinear relationships. Linear relationships are generated through polynomial operations on dataset variables, while nonlinear relationships involve trigonometric operations. The noise distribution types encompass Gaussian noise with parameters defined by a mean of 0 and a standard deviation of 1, and uniform noise spanning the interval (0,1).\nBuilding upon the framework above, five datasets are generated for each data size, each subjected ten times to mitigate runtime-induced biases, resulting in a total of 180 distinct datasets for comprehensive algorithm evaluation. All generated datasets adhere to the causal sufficiency assumption and possess stability, prerequisites fundamental to the operation of the algorithmic processes.\nRegarding authentic datasets, scarcity in datasets featuring established ground truth, particularly within the domain of time series, is evident. Consequently, this study incorpo- rates two verifiable datasets. The first is the \"Tuebingen\" dataset, which comprises 100 real cause-effect pairs. Additionally, the \"fMRI\" dataset, which aims to investigate the Blood Oxygen Level Dependent (BOLD) signal across 28 distinct intrinsic brain networks, is also integrated into the study."}, {"title": "5.2 Evaluation Metrics", "content": "In devising this project's evaluation criteria, we tried to encompass multiple aspects as comprehensively as possible. Recognizing that a singular indicator might introduce bias, we select five distinct indicators for our assessment."}, {"title": "5.3 Algorithms", "content": "By collecting resources on GitHub, we introduce in this section the algorithms selected for the experiment, as well as their source code and packages. We mainly use four packages, Causal Discovery Toolbox (CDT) (2019a), gCastle (2021b), causal_discovery_for_time_series (CD_TS)(2022), causal-learn (2024) to implement the testing algorithms.\nAs shown in Table 4, we select MVGC and PWGC, implemented in CD_TS(2022), from the repertoire of GC methods, as the subject of experimentation. It is worth noting that MVGC exclusively addresses delayed causality while PWGC addresses pairwise causality, so their applicability has certain limitations.\nIn the context of the conditional independence-based method, Runge et al. (2023b) de- veloped PCMCI and its variant algorithms for time series. The OCSE, tsFCI and DYNOTEARS algorithms are also included in the experiment for time series. Considering i.i.d. data, we chooose PC, FCI, GES, GRaSP, ES and CDS. These methods are chosen for their robust performance in different scenarios, providing a comprehensive evaluation of algorithmic capabilities.\nMost algorithms rooted in the state space dynamic-based approach primarily concentrate on resolving the directionality of causal pairs. Among these, we examine the classical CCM and PAI algorithms (Javier, 2021), categorizing them as instances of pairwise causality. Additionally, IGCI and ANM are incorporated as pairwise causality algorithms.\nAs for the structural equation modeling-based approach, our emphasis lies on exploring a variant of the LiNGAM algorithm (Ikeuchi et al., 2023) tailored for i.i.d. data and time series data, referred to as ICALINGAM, DirectLiNGAM, RCD, CAM-UV and VARLINGAM. The TiMINO algorithm is excluded from consideration due to its documented inferiority compared to PCMCI and TCDF (Nauta et al., 2019a).\nTCDF algorithm(2019b) frequently emerges in diverse surveys and holds a pivotal posi- tion within the field, thus making it a suitable choice as a representative algorithm for deep learning-based methods. For i.i.d. data, we include three algorithms from gCastle package: DAG-GNN, CORL and GraNDAG. Besides, we do not include CGNN in this experiment due to its prolonged running time, which could impede the efficiency of the overall analysis.\nWithin the hybrid method, we opt to employ both NeuralGC (Tank et al., 2021a) and IGCI. NeuralGC possesses the ability to handle both delay and instantaneous causal relationships, while IGCI is specialized in resolving causal pairs.\nIn order to present a lucid exposition of the algorithms employed in this experiment, we marked the testing algorithms with * in Table 4. It is essential to highlight that, in pursuit of optimal performance for each algorithm, a series of individual tests were conducted. According to these tests, specific default values of several algorithms were modified to ensure the validation of the results."}, {"title": "5.4 Environment Settings", "content": "Here we will expound upon the environment's configuration of the entire code architec- ture, encompassing domains such as software provisioning, hardware parameters, database integration, and related facets.\nThe instantiation of this project is grounded in the Python programming language, with compilation facilitated through the PyCharm (professional edition) software. Upon successful compilation, the resultant code is subsequently uploaded onto the designated server for operational deployment.\nThe particular details about the server infrastructure are outlined herewith: The server infrastructure is established on the Ubuntu operating system, boasting four Graphics Pro- cessing Units (GPUs) that operate in tandem. These GPUs are compatible with the 11.4 version of the Compute Unified Device Architecture (CUDA). Notably, each GPU has a computational prowess of 350 Watts and 24,268 Megabytes of memory.\nThe source code's comprehensive architecture encompasses five submodules, each ful- filling designated functions. The initial submodule, termed the \"dataset\", functionality encompasses the storage of datasets and ground truth sets in CSV format alongside the ca- pacity to generate synthetic datasets. Moving forward, the second submodule, designated the \"examples\", serves the dual purpose of harmonizing testing algorithms and facilitating the visual representation of obtained outcomes. The third submodule, \"save\", is dedicated to the archival of causal discovery results derived from algorithmic testing endeavours. Within the fourth submodule, designated the \"src\", the central objective pertains to inte- grating external configuration files, thus ensuring seamless operational functionality of the codebase. The final submodule, aptly labelled the \"doc\", assumes the role of elucidating practical usage instances of this code library while presenting information regarding the version of the installation package."}, {"title": "5.5 Research Questions", "content": "With a focus on the analysis of algorithm performance and result effectiveness, our experiment addresses the following research questions:\nRQ 1 (Comparison of Algorithm Performance): Among the assessed algorithms, which one demonstrates superior effectiveness or efficiency under specific data characteris- tics?\nRQ 1 is the baseline question in the experiment and a prerequisite for answering other questions. By evaluating the impact of data features on algorithm performance, we establish a foundation that informs subsequent steps in our experimental analysis.\nRQ 2 (Real-World Applicability): Are the insights derived from the synthetic datasets consistent with those acquired from the real datasets?\nRQ 2 is crucial for determining the effectiveness of insights gained from RQ 1, as it connects the findings from synthetic data to real-world scenarios. This question ensures that our conclusions are not limited to controlled experimental conditions but are also valid in practical applications.\nRQ 3 (Generalization to Unknown Datasets):\n\u2022 RQ 3.1 (Metadata Recognition for Algorithm Selection): Is it feasible to pre- cisely capture the representative attributes of unknown datasets using their metadata to ascertain the optimal algorithm based on our previous conclusions?\n\u2022 RQ 3.2 (Practical Recommendations for Users): Upon successfully identifying the optimal algorithm for an unknown dataset in RQ 3.1, what practical recommen- dations can we provide to users for selecting appropriate methods for their specific datasets?\nRQ 3 is a challenging aspect of the experiment as it involves extending the results of RQ 1 and RQ 2 to a broader range of applications. RQ 3.1 focuses on the feasibility of applying our findings to new datasets by analyzing their metadata, ensuring that our methods are robust and versatile. RQ 3.2 aims to translate these validated approaches into practical, user-friendly guidelines that assist practitioners in choosing the best algorithms for their unique datasets, thus bridging the gap between theoretical research and practical implementation."}, {"title": "6 Results Analysis", "content": "Within this section, the research questions posited in the preceding section will be addressed systematically, resulting in generalizable findings according to experimental plots and tables."}, {"title": "6.1 Answer to RQ1: Comparison of Algorithm Performance", "content": "Through meticulous experimentation, we have derived comparative graphs and ranking tables for algorithms across four distinct causality types. Considering that the ranking algorithm table only displays the average of metrics across all data sizes, we still need violin plots in Appendix A to supplement the changes in recommendation algorithms under specific sample lengths. This section will subsequently present an in-depth analysis of each category.\nThe causal relationships, whether linear or nonlinear, in conjunction with noise distribu- tions (Gaussian or non-Gaussian), yield four distinct subgraphs, each representing different data modalities. Dataset time lengths are categorized as follows: small (50, 300), medium (300, 1000), and large (1000, 5000). Each dataset is processed ten times to compute mean values."}, {"title": "6.1.1 PAIRWISE CAUSALITY", "content": "Figure A illustrates the experimental outcomes for the pairwise causal discovery algorithms. We identify an algorithm that consistently performs well as the reference algorithm. For instance, within linear relationships, the CCM algorithm serves as the benchmark, whereas the PAI algorithm is selected for nonlinear relationships. The devia- tions between the algorithms and their respective reference counterparts are depicted using violin plots in Appendix A.\nAcknowledging the limitations of violin plots in conveying precise numerical results, we supplement our analysis with a ranking table. Table 5 quantifies and compares the performance disparities among the algorithms under consideration. The algorithms are ordered in descending sequence based on metric values in this table. Higher F1 scores and AUROC values indicate superior model performance, while lower FPR and SHD values reflect better algorithmic behavior."}, {"title": "6.1.2 INSTANTANEOUS CAUSALITY", "content": "Following this exposition, we elucidate the performance evaluation of instantaneous causal discovery algorithms, as visually depicted in Figure A and quantitatively delin- eated in Table 6. Figure A records the performance of seven algorithms, with OCSE serving as the reference algorithm for Gaussian noise datasets and VARLINGAM as the reference for non-Gaussian noise datasets.\nSimilar to the analysis of pairwise causality, we consider five scenarios when evaluating instantaneous causality algorithms. The top 3 best-performing algorithms under each metric can be summarized from Table 6. We will present them in Table 10 and not elaborate here. It is necessary to supplement some insights based on data sample size in Figure A.\nFirstly, when prioritizing F1 scores, for non-linear Gaussian datasets, tsFCI performs in the top three in small data sizes, while PCMCI performs better in large data sizes. For Nonlinear non-Gaussian datasets, VARLINGAM will only be recommended when the sample length is small (L < 1000). The average ranking of algorithm performance under AUROC metric is basically consistent with the ranking under each data size.\nIn the context of prioritizing FPR metrics, PCMCI belongs to the recommendation algorithms in all four data types, indicating its superior performance under the FPR metric. For Linear Gaussian datasets, tsFCI is only recommended when the sample size is small."}, {"title": "6.1.3 TIME-DELAY CAUSALITY", "content": "Below, we will analyze the performance comparison of time-delay causal algorithms in detail. The top 3 best-performing algorithms under each metric can be summarized from Table 7. We will present them in Table 10 and not elaborate here.\nIt is necessary to supplement some insights based on data sample size in Figure A. Firstly, we analyze the algorithms under the F1 metric. For Linear Gaussian data, VAR- LINGAM performs better in larger sample sizes. On the contrary, MVGC is more suitable for small sample size data. The top three ranking algorithms perform relatively stably on Linear non-Gaussian and Nonlinear Gaussian data, and their rankings do not change signif- icantly based on changes in sample size. For Nonlinear non-Gaussian data, DYNOTEARS is recommended only when the sample size is small, since it does not ranked in the top three in large sample size.\nThe ordering of algorithms according to the AUROC metric exhibits a relatively sta- ble pattern. Specifically, the top three algorithms in average ranking table reflects the recommended algorithms for AUROC across all data sizes.\nIn evaluating performance utilizing the FPR metric, for linear data, TCDF ranks in the top three only when the sample length > 1000, so it is not a recommended algorithm when the sample size is small. For non-linear data, VARLINGAM is not outstanding when the sample length < 300 and is only recommended for use in large data size. Note that although the oCSE algorithm does not always in the top three of the mean ranking, it is always the best algorithm in small sample sizes (L < 300).\nConsidering SHD, for linear type, VARLINGAM is more suitable for large sample data, while PCMCI is more suitable for small sample data. For nonlinear type, MVGC is more suitable for small sample data. In Nonlinear non-Gaussian data, although NeuralGC is not among the top three algorithms, it is the best algorithm when sample length > 1000 and can be recommended as a supplementary algorithm.\nAfter comparing the performance of time-delay causal discovery algorithms, it can be concluded that tsFCI are not recommended in any scenario, as they are not competitive across any metric."}, {"title": "6.1.4 I.I.D. CAUSALITY", "content": "Below, we will analyze the performance comparison of i.i.d. data causal algorithms in detail.\nBased on Figure A and Table 8, we can derive insights into the performance of recom- mendation algorithms across different i.i.d. data types. Analyzing the algorithms using the F1 metric, Table 8 reveals that the optimal algorithms vary according to specific data char- acteristics, with the CORL algorithm consistently ranking among the top three performers.\nWhen considering the AUROC metric, the CORL algorithm shows superior performance on nonlinear datasets, although the improvement over the second-best algorithm is less than 10%. In contrast, GOLEM or DirectLiNGAM exhibit a slight advantage over CORL when applied to linear datasets.\nAssessing performance using the FPR metric, NOTEARS emerges as the most proficient algorithm for nonlinear datasets. The GraNDAG algorithm performs best when the sample size is large, so we also include it in Table 8. For linear datasets, GOLEM performs best under Gaussian noise distribution, while DirectLiNGAM excels with non-Gaussian noise. Note that we need to supplement the DAG-GNN algorithm on large datasets, although it performs poorly on small-sized datasets.\nEvaluated using the SHD metric, CORL demonstrates optimal performance on nonlinear datasets. Conversely, GOLEM achieves the lowest SHD values on linear datasets with Gaussian noise, while DirectLiNGAM performs best with non-Gaussian noise."}, {"title": "6.1.5 DISCUSSION ON ALGORITHM EFFICIENCY", "content": "It is known that effectiveness does not equal efficiency. Even if some algorithms have good causal discovery performance, they may not be suitable for users because of long running time. So here we will specifically discuss the running time of the algorithms we tested in the first four sections, as shown in Table 9, to help users make a trade-off between effectiveness and efficiency.\nConsidering time cost of pairwise algorithms, RECI, PWGC, and IGCI stand out as the exemplars of efficiency across all datasets, showcasing a runtime nearly one order of magnitude lower than that of other algorithms.\nFor instantaneous and time-delay causality, DYNOTEARS, PCMCI, VARLINGAM al- gorithm are preferred. In contrast, the NeuralGC algorithm exhibits the most prolonged computational execution time, exceeding that of the best algorithm by nearly three orders of magnitude.\nWhen considering i.i.d. data, FCI, ICALINGAM, and GRaSP are the preferred choices. In contrast, the least efficient algorithms, GraNDAG and CORL, have runtime that are thousands of times longer than the most efficient ones."}, {"title": "6.1.6 RECOMMENDATION ALGORITHMS", "content": "It is important to note that the ranking table for each data category is obtained by calculating the average value of the results run on 15 datasets. To answer RQ 1, by sorting out the experimental findings of these causal discovery algorithms, we select the top three with the best average value under each evaluation metric as our recommended algorithms, as shown in Table 10.\nNote that if the optimal algorithm for a specific data size is not among the top three in average ranking, we will supplement it with gray font to ensure that Table 10 covers all possible scenarios as much as possible. \"S\" indicates that the algorithm is more suitable on small sample length (L < 1000), while \u201cL\u201d indicates that the algorithm is more suitable on large sample length (L > 1000)."}, {"title": "6.2 Answer to RQ2: Real-World Applicability", "content": "We first tested the real-world dataset, Tuebingen, which comprises 100 pairs of causal relationships within a stationary time series framework characterized by nonlinearity and non-Gaussian attributes. The time length spans from 94 to 16,382 time points. Leveraging the insights posited in Section 6.1, it is deduced that the PAI, CDS, CCM algorithms are the preeminent choices under metrics such as F1 score, AUROC, and SHD; PAI, CDS, ANM are recommended for FPR metric. Additionally, RECI, PWGC, IGCI algorithm are identified as the most efficient. The dataset consists of one hundred instances of causal pairs, which we divided based on their temporal extent: those exceeding 1000 time points were classified as \"large datasets\" and those below 1000 time points were categorized as \"small datasets\". Subsequently, an exhaustive execution of all algorithms on this real dataset was conducted, resulting in Figure 9.\nObservation of the graph reveals a clear pattern: using the PAI algorithm as the bench- mark, except for CDS, the average F1 and AUROC metrics of the other algorithms consis- tently reside beneath the horizontal baseline, while the metrics of FPR and SHD exhibit values surpassing those of PAI. This collective trend signifies that PAI demonstrates supe- riority as the optimal algorithm across these four evaluative metrics on \"small datasets\", while CDS performs better on \"large datasets\u201d.\nMoreover, when temporal considerations are factored in, the violin plot corresponding to PECI, IGCI, PWGC, CCM algorithms are prominently positioned beneath the horizontal reference line. This distinctive placement underscores that RECI holds the lowest time complexity.\nThese findings align with the algorithmic recommendations derived from experiments on the authentic dataset and corroborate the deductions drawn based on the outcomes expounded in Section 6.3. This congruence augments our confidence in extending the the- oretical framework to real-world datasets, thereby validating our theoretical assertions and demonstrating their practical applicability.\nThe second real dataset pertains to functional Magnetic Resonance Imaging (fMRI), comprising 28 sets of multivariate time series. A subset of data that failed to meet criteria associated with causal sufficiency was omitted, resulting in the examination of 27 datasets. This dataset is characterized by nonlinearity and Gaussian attributes, emblematic of time- delay causal causality with lag = 1. Among the dataset constituents, 21 sets comprise fewer than 1000 data points, while the remaining six sets exceed this threshold. Each dataset includes 5, 10, or 15 time series variables. Guided by these salient attributes, we predict that one of PCMCI, DYNOTEARS, VARLINGAM algorithms will exhibit optimal performance under F1 score, whereas PCMCI, MVGC, DYNOTEARS algorithms will attain primacy in terms of AUROC. For FPR, MVGC is the most recommended algorithm since it performs well on datasets of all sizes, while VARLINGAM is only recommended on large sample sizes and OCSE is only recommended on small sample sizes. Considering SHD, PCMCI, DYNOTEARS, and MVGC are recommended algorithms, with MVGC being more suitable for small sample sizes. When taking into account the time cost, we recommend PCMCI, VARLINGAM, and DYNOTEARS algorithms."}, {"title": "6.3 Answer to RQ3: Generalization to Unknown Datasets", "content": "In Section 6.3.1, a metadata detection program was designed to extract data features. Subsequently, in Section 6.3.2, our recommendation program was tested on various datasets to verify its consistency with the algorithm test results."}, {"title": "6.3.1 ANSWER TO RQ 3.1: METADATA RECOGNITION FOR ALGORITHM SELECTION", "content": "Given that our prior analyses focused on causality types, linearity among series, and noise distribution, capturing these pivotal attributes within an unknown dataset is crucial for the project's universality and practical applicability.\nThe first task of metadata detection is identifying temporal lags within variables. We employ the Time Lag Cross Correlation (TLCC) technique to accomplish this. TLCC is measured by gradually shifting a time series vector and repeatedly calculating the correlation between two signals. Identifying correlation maxima facilitates the ascertainment of inter- variable temporal lag. Specifically, a zero lag denotes an instantaneous causal association, whereas a non-zero lag signifies a time-delay causality. If no lag is detected, the dataset is classified as i.i.d. data.\nSubsequently, identifying the noise distribution is requisite. We employ concurrent eval- uative methodologies encompassing the Shapiro-Wilk, Kolmogorov-Smirnov, and Anderson- Darling tests. These tests collectively serve to discern the presence of Gaussian noise in the data. The following criteria serve as benchmarks:\n1. The Shapiro-Wilk test's computed p-value surpasses the significance threshold of 0.05.\n2. The p-value resulting from the Kolmogorov-Smirnov test exceeds 0.05.\n3. The p-value derived from the Anderson-Darling test remains below its critical thresh- old.\nFulfillment of these conditions collectively allows for the inference of Gaussian noise as the prevailing noise type characterizing the dataset.\nLastly, a crucial inquiry involves ascertaining potential linear interdependence among variables. To address this, a linear regression framework is applied to every pair of variables. Subsequently, the coefficient of determination (R-squared) is derived to gauge the efficacy of the model fit. A predetermined threshold of 0.5 is set for assessment. If the computed R-squared value surpasses this threshold, it signifies the presence of a discernible linear relationship between the variables. Conversely, an R-squared value below the threshold implies suboptimal alignment with the linear regression framework, indicating the absence of a linear relationship between the implicated variables.\nTo comprehensively appraise the previously delineated feature extraction procedures, we conducted metadata detection experiments on 100 datasets, with causality types, linear relations, noise distributions, and time lengths randomly generated. This evaluation was accomplished by computing the accuracy of judgments for the three metadata categories. We conducted ten trials, yielding comprehensive average and standard deviation metrics, as detailed in Table 11."}, {"title": "6.3.2 ANSWER TO RQ 3.2: PRACTICAL RECOMMENDATIONS FOR USERS", "content": "LUCAS and Sachs was selected as the test dataset. The extracted metadata, based on the program described in the previous section, along with the corresponding recommenda- tion algorithms provided by Table"}]}