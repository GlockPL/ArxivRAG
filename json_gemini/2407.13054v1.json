{"title": "Comprehensive Review and Empirical Evaluation of Causal Discovery Algorithms for Numerical Data", "authors": ["Wenjin Niu", "Zijun Gao", "Liyan Song", "Lingbo Li"], "abstract": "Causal analysis has become an essential component in understanding the underlying causes of phenomena across various fields. Despite its significance, the existing literature on causal discovery algorithms is fragmented, with inconsistent methodologies and a lack of comprehensive evaluations. This study addresses these gaps by conducting an exhaustive review and empirical evaluation of causal discovery methods for numerical data, aiming to provide a clearer and more structured understanding of the field. Our research began with a comprehensive literature review spanning over a decade, revealing that existing surveys fall short in covering the vast array of causal discovery advancements. We meticulously analyzed over 200 scholarly articles to identify 24 distinct algorithms. This extensive analysis led to the development of a novel taxonomy tailored to the complexities of causal discovery, categorizing methods into six main types. Addressing the lack of comprehensive evaluations, our study conducts an extensive empirical assessment of more than 20 causal discovery algorithms on synthetic and real-world datasets. We categorize synthetic datasets based on size, linearity, and noise distribution, employing 5 evaluation metrics, and summarized the top-3 algorithm recommendations for different data scenarios. The recommendations have been validated on 2 real-world datasets. Our results highlight the significant impact of dataset characteristics on algorithm performance. Moreover, a meta-data extraction strategy was developed to assist users in algorithm selection on unknown datasets.", "sections": [{"title": "1 Introduction", "content": "Causality Research, as a dynamically evolving interdisciplinary field, has garnered increasing scholarly attention. This section elaborates on the fundamental concepts, evolution, and classifications of causal analysis, addresses the limitations of previous studies, and introduces the innovative contributions and specific objectives of the current work.\nCausal analysis employs a systematic approach to uncovering the underlying causes of phenomena, primarily addressing the question of \"Why.\" Since Granger's seminal work in 1969 (Granger, 1969), which introduced a mathematical concept of causality, the field has expanded from philosophy into economics (Imbens, 2004) and other domains such as medicine (Mani and Cooper, 2000), environmental science (Li et al., 2014), and dynamics (Hu et al., 2015). Recently, the rapid advancement of Artificial Intelligence (AI) has opened new avenues for causal analysis. The integration of machine learning has enhanced the precision and efficiency of data processing for causal inference, while causal learning has established a more reliable and trustworthy framework for machine learning (Scholkopf, 2019; Makhlouf et al., 2020). These two disciplines mutually reinforce each other, driving significant advancements in scientific research.\nGiven that correlation does not imply causation, causal research necessitates a thorough investigation beyond simple association analysis. Pearl's work has provided a widely accepted framework (Pearl et al., 2000), known as the \"Ladder of Causation,\" which delineates three stages: Association, Intervention, and Counterfactual. The initial stage, Association, involves observing relationships between variables, yet it is insufficient for identifying confounders or selection bias that may lead to spurious causation (Cheng et al., 2019). The second stage, Intervention, involves controlled experiments to quantify the causal impact of one variable on another. The final stage, Counterfactual analysis, requires a deep understanding of the causal mechanisms underlying the phenomena.\nResearch on causal analysis is primarily categorized into two areas (Nogueira et al., 2022): Causal Inference and Causal Discovery. Causal Inference typically progresses from cause to effect, focusing on the quantitative problems of \"Intervention Stage\" within the causation ladder framework (Peters et al., 2017). The central concept of inference involves controlled trials (Yao et al., 2021), where both experimental and control groups are observed to determine the effects of interventions. In contrast, Causal Discovery seeks to identify causal relationships from observed outcomes, emphasizing the qualitative problems to learn causal structures (Gelman, 2011). Upon uncovering causal mechanisms, it becomes possible to infer outcomes based on hypothetical scenarios that have not occurred. Despite their inverse logical relationship, causal discovery and causal inference differ significantly in their research methodologies, algorithms, and applications.\nIn practical scenarios, a significant challenge in causal analysis is managing diverse and complex data types. Standard data formats include time series (Eichler, 2012), cross-sectional, and panel data. Panel data, which has two dimensions (time and samples), combines elements of cross-sectional data, where all sample observations at a specific time point are included, and time series data, where observations of the same sample are recorded at different time points. Since time series data does not adhere to the assumptions of independent and identically distributed (i.i.d.) random variables, it necessitates specialized research on data processing techniques and causal analysis algorithms. Furthermore, due to the importance and widespread use of time series in real-world applications, many researchers have focused extensively on this data type (Assaad et al., 2021; Biswas and Mukherjee, 2024). If a time series can be viewed as a \"list\", then the i.i.d. variable is a \"set\" without self-causes. Therefore, causal discovery for i.i.d. data, which differs from time-series causality, has also attracted a lot of research attention (Xie et al., 2019).\nThis study concentrates on two pivotal elements of causal analysis: (1) Causal Discovery and (2) Time Series and i.i.d. data analysis. An exhaustive investigation was conducted to delve into the methodologies of causal discovery, encompassing principles, algorithmic strategies, and recent advancements in the field.\nCurrently, the literature lacks a universally applicable algorithm for causal analysis (Edinburgh et al., 2021). Unlike causal inference, the precision of causal discovery heavily relies on the selection of an appropriate causality model. Hence, it is crucial to systematically organize and consolidate existing algorithms to effectively address various scenarios.\nExtensive efforts have been made to review and evaluate causal discovery algorithms. To address the challenge of limited benchmark datasets for causal discovery, many researchers have focused on developing data simulators in fields such as industrial systems (Menegozzo et al., 2022), neurology (Tu et al., 2019), and biology (Ma et al., 2023). Some works evaluated causal discovery methods through experiments, but they are not comprehensive as they only consider one type of methods. For example, Sogawa et al. (2010) evaluated the identification accuracy and robustness of linear non-Gaussian methods and its variants. Raghu et al. (2018) compared the performance of four conditional independence-based algorithms on mixed data with latent variables. Ko et al. (2018) summarized estimation of distribution algorithms (EDAs) and compared their performance on four datasets to infer the best ones.\nIn addition, there are articles that survey various types of methods. Song et al. (2016) and K\u00e4ding et al. (2021) compared causal discovery methods for bivariates on real-world bench datasets. However, their study did not include analysis of multivariate algorithms. Ombadi et al. (2020) evaluated four causal discovery algorithms on hydrometeorological data, aiming to guide researchers in determining which causal method is most appropriate based on the characteristics of hydrological system. Assaad et al. (2022) not only systematically organized time-series methodologies but also performed thorough evaluations of representative algorithms based on distinct causal structures.\nDespite these contributions, most existing literature primarily focuses on theoretical summaries. Even when experiments were conducted, they mainly assessed the impact of causal structures on model performance. Given the often ambiguous causal structure of observational data, it is vital to provide practical and reliable insights from the user's perspective.\nUnlike previous surveys, this project adopts a data-oriented approach, categorizing causal relationships into four types: i.i.d. causality, time-delay causality, instantaneous causality, and causal pairs. To address the research gap that often overlooks the user's perspective, this study performs a comprehensive analysis starting from the intrinsic characteristics of the data (data assumptions). Motivated by this approach, comparative experiments were conducted, treating data assumptions as experimental factors and employing various algorithms as experimental subjects. These comparisons aim to establish the relationship between algorithms and specific data features within each causality category. Specifically, our pursuit is dedicated to identifying the optimal algorithm, considering various factors, including data size, linearity, stationarity, and noise attributes. By leveraging the extracted data features, users can choose the most appropriate algorithm for their specific needs.\nThe specific objectives have been enumerated to provide coherent guidance and tasks for this study:\n1. Systematic Investigation and Taxonomy of Causal Discovery Methods: Comprehensively collect and categorize methods and algorithms for causal discovery, summarizing the characteristics and applications of these algorithms.\n2. Empirical Assessment of Algorithmic Performance through Comparative Experiments: Conduct horizontal comparisons of algorithms on datasets with different features and select appropriate evaluation metrics to assess their performance from multiple perspectives.\n3. Recommendation Mechanism for the Optimal Algorithm for Users: Provide meaningful insights and recommendations on the optimal algorithm for specific datasets, offering decision-making suggestions in various application fields.\nThe primary aim of this research is to develop recommendation algorithms tailored to specific data features through an exhaustive survey of existing methods and rigorous empirical analysis. Key contributions of this study include the creation of a method library featuring 24 algorithms for future experimental use and the execution of 16 comparative analyses to elucidate the relationship between algorithm performance and specific data"}, {"title": "2 Background", "content": "Here we will analyze the preliminaries, related survey literature, and research gaps in causal discovery. The section aims to structure the knowledge body of this academic domain systematically."}, {"title": "2.1 Preliminaries", "content": "This section presents the fundamental definitions and corresponding notations associated with causal discovery. One needs to state that matrices are denoted by uppercase bold letters, whereas vectors are indicated by lowercase bold letters. Consider the dataset denoted by $X$, which manifests as a $m \\times n$ matrix. Here, $x_n$ designates the $n^{th}$ variable, while $x$ embodies $m$ observations. This endeavour categorizes observational data into cross-sectional and time-series data, as defined below.\nDefinition 1 (Cross-sectional Data): Cross-sectional data is a set of observations collected from subjects at one time point.\nNote that we mainly focus on a common type of cross-sectional data, namely independent and identically distributed (i.i.d.) data.\nDefinition 2 (Time Series): Time series is a sequence of data points arranged in temporal order. Given a time series $x^m$, an observation at a specific temporal point $t$ is represented as $x_t$.\nThe concept of time lag is introduced to discern the demarcation between time-delay and instantaneous causality."}, {"title": "Definition 3 (Time Lag)", "content": "Time Lag $\\tau$ refers to the temporal interval between a cause and its effect.\nIn cases where $\\tau > 0$, it signifies the occurrence of the cause $\\tau$ units of time prior to its effect. This phenomenon is referred to as \"time-delay causality\". Nonetheless, circumstances arising from sampling techniques or other factors might occasion an instance wherein $\\tau = 0$. In such scenarios, the causal latency is deemed insignificant for observation, and this relationship is classified as \"instantaneous causality\".\nTo further depict the causal interdependencies among variables within the dataset, it becomes imperative to introduce the notion of causal graphs.\nDefinition 4 (Causal Graph): The causal graph $G$ is composed of two subsets: a set of nodes $v$ and a set of edges $\\epsilon$. If variable $x_i$ is cause of variable $x_j$, denoted as $x_i \\rightarrow x_j$, this relationship manifests as an edge from node $i$ to node $j$ in Directed Acyclic Graphs (DAGs) (Pearl, 1985).\nHowever, when there are hidden variables in the dataset, Maximal Ancestral Graphs (MAGs) (Richardson and Spirtes, 2002) can represent causal relationships. The types of edges in MAGs are as follows:\n$x_i \\rightarrow x_j$: $x_i$ causes $x_j$;\n$x_i \\leftrightarrow x_j$: there is a hidden confounder between $x_i$ and $x_j$;\n$x_i - x_j$: there is a hidden effect variable from both $x_i$ and $x_j$.\nIf we do not consider causal directions, a skeleton graph can signify the causal relations between variables. There are only undirected edges in the skeleton graph that represent causal links. For time series, a window graph is common for causal discovery, referring to the causal graph within the maximum time lag window (Assaad et al., 2023). \nBuilding upon the precedent definitions, we can elucidate the tasks of causal discovery. Given a dataset $X$, the objectives of causal discovery are the deduction of the causal"}, {"title": "Definition 5 (Adjacency Matrix)", "content": "The Adjacency Matrix of causal discovery constitutes a square matrix of dimensions $n \\times n$. Specifically, the row vector is set to signify the cause, while the column vector signifies the effect. If the value located at position $(i, j)$ in the matrix is 1, it denotes the presence of a causal link from variable $x_i$ to $x_j$.\n2.2 Assumptions for Causality\nWhen conducting causal discovery tasks, it is crucial to consider certain foundational assumptions. All the methods analyzed in this study are based on at least one of these assumptions. These assumptions help in relating causality to probability densities (Spirtes and Zhang, 2016)."}, {"title": "2.2.1 THE CAUSAL MARKOV ASSUMPTION", "content": "Spirtes and Zhang (2016) argue that in a dataset, all features are independent of their non-effects (nondescendants in the causal graph) conditional on their direct causes (parents in the causal graph). However, it is important to note that the Causal Markov Assumption can be an oversimplification. It assumes that causality is the sole reason for associations between all features, which is not always true, as other kinds of associations exist as well.\nFor instance, consider the case of ice cream sales and drowning incidents. According to the Causal Markov Assumption, if these two features were present in a dataset and shared an association, it should imply a causal link. However, we know this isn't true. During summers, both ice cream sales and swimming activities increase. While there might be some conditional dependencies between the two due to common factors, confusing correlation with causality would lead to spurious results."}, {"title": "2.2.2 THE CAUSAL FAITHFULNESS ASSUMPTION", "content": "The Causal Faithfulness Assumption asserts that all and only the true causal links between features are represented in the observed data. This means there are no latent or unobserved confounding variables impacting the observed features and their underlying causal relationships (Spirtes and Zhang, 2016).\nFor example, consider a dataset of patients in a lung cancer ward, with smoking habits as one of the features and lung cancer as the target variable. To determine whether smoking causes lung cancer, the causal faithfulness assumption holds if there are no unmeasured factors (like genetic vulnerability) linked to both smoking habits and lung cancer but are absent from the observed dataset. If the assumption does not hold, it indicates the presence of confounding factors. For instance, if the dataset lacks information about the patients' medical history or genetic predisposition, the observed causal link between smoking and lung cancer may be spurious. In such a scenario, smoking habits may appear to cause lung cancer in the data, but the true causal relationship is distorted by latent confounders."}, {"title": "2.2.3 MARKOV EQUIVALENCE CLASSES (MEC)", "content": "Markov Equivalence Classes (MEC) are another important concept in causal discovery. MECs group statistically indistinguishable causal models that make the same predictions about the probability distributions of observed variables (Spirtes and Zhang, 2016). In a given set of variables V, a Markov Equivalence Class represents an aggregation of DAGs that exhibit the same pattern of conditional independence associations among variables in V.\nFor example, consider two DAGs, G1 and G2. G1 and G2 lie in the same Markov Equivalence Class if, for each pair of variables v1 and v2 in V, v1 and v2 are conditionally independent given all other variables v3 in V in G1 if and only if they are conditionally independent given v3 in G2. Simply put, for G1 and G2 to lie in the same Markov Equivalence Class, they must imply the same set of conditional independence statements among the variables.\nTo illustrate this further, consider two plausible causal models, M1 and M2, and three variables P, Q, and R:\nM1: P\u2192Q\u2192R\nM2: P\u2190Q\u2190R\nIn M1, P causally influences Q, and Q causally influences R. In M2, R causally influences Q, and Q causally influences P. Despite the different causal directions, M1 and M2 lie in the same Markov Equivalence Class because the observed data generated from these models will exhibit the same trends and conditional independence associations (Spirtes and Zhang, 2016)."}, {"title": "2.2.4 THE CAUSAL SUFFICIENCY ASSUMPTION", "content": "The Causal Sufficiency Assumption (Spirtes et al., 2001) states that the common causes between any two variables of the variable set v are entirely contained within v itself, thereby excluding the presence of latent confounders. This condition is considered a prerequisite for the efficacy of most causal discovery algorithms.\nFor example, consider one causal model M and three variables, P, L, and R:\nM:P\u2190L\u2192R\nIf L is an unobserved variable, indicating that L is a hidden confounder of R and Q, then the model M does not satisfy the causal sufficiency assumption. Under this assumption, a directed edge in DAGs represents a causation from a cause to its effect."}, {"title": "2.3 Related Works", "content": "In recent years, the topic of causal discovery has garnered significant interest as researchers endeavor to uncover causal relationships from observational and experimental data. To establish a solid foundation for understanding causal discovery, it is crucial to delve into the field's core concepts and theories. An exhaustive search was conducted within the Web of Science repository to comprehensively encompass the pivotal papers in this domain, curating 301 relevant articles. In order to facilitate a more in-depth examination and exploration of these papers, a network diagram depicting co-cited literature was generated.\nThe term \"co-citation\" means the phenomenon wherein two papers are simultaneously referenced by one or more subsequent papers, thereby establishing interlinked co-citation relationships between the two papers. In Figure 3, the threshold for the minimum number of co-cited articles was set at 15, affording a lucid means to observe literature of close thematic association, thereby furnishing a framework and cluster of causal discovery. We can see from the figure that foundational works by Judea Pearl, Peter Spirtes, and Clark Glymour have laid the groundwork for causal discovery.\nThe most prominent node, depicted in red, represents the seminal work of Peter Spirtes (2001). Pearl's work on causal Bayesian networks and the introduction of causal graphical models significantly advanced the field's theoretical foundation. These seminal contributions serve as the basis for further research and methodologies. Peter Spirtes introduced nonparametric structural causal models (SCM) (Pearl, 2009) as a formal and intelligible language for articulating causal knowledge and explaining causal notions used in scientific discourse. These include concepts like randomization, intervention, direct and indirect effects, confounding, counterfactuals, and attribution. The structural language's algebraic component corresponds to the potential-outcome framework, while its graphical component incorporates Wright's method of path diagrams. When combined, these components provide a robust approach for causal inference, addressing long-standing issues in empirical sciences, such as confounding control, policy evaluation, mediation analysis, and the algorithmization of counterfactuals.\nSince the 1990s, conditional independence connections in data have been used to recover the underlying causal structure. One such constraint-based algorithm, developed by Spirtes and Glymour, is the PC algorithm. This algorithm starts with an undirected graph and recursively deletes edges based on conditional independence judgments. The PC algorithm assumes no confounders (unobserved direct common causes of two measured variables) and that the causal information retrieved is asymptotically true. However, the PC algorithm has limitations, especially with high-dimensional biological datasets, where its runtime grows exponentially with the number of variables, making it inefficient for large datasets. Additionally, the order of factors in the input data influences the output, reducing its reliability. Notably, Runge et al. (2019) proposed PCMCI, which extends the PC algorithm to time series data.\nAnother constraint-based algorithm is the FCI algorithm, also developed by Spirtes et al. The FCI algorithm starts with a fully connected undirected graph and removes edges connecting conditionally independent variables to create a causal graph. In the second phase, it orients edges by recognizing the \"V\" and \"Y\" structures and then orients the remaining edges using a set of criteria. FCI can produce asymptotically correct results even in the presence of confounders, making it more robust than PC in dealing with latent confounders. These methods can handle a wide range of data distributions and causal links and they have efficient means of conditional independence verification. It should be noted, however, that they may not always convey entire causal information because they yield equivalence classes that represent multiple causal structures satisfying the same conditional independencies (Spirtes and Zhang, 2016).\nScore-based algorithms, which facilitate the identification of causal structures by optimizing a well-defined score function, do not entail any confounders. A well-known score-based method is Greedy Equivalence Search (GES) (Chickering, 2002), which directly searches the space of equivalence classes. GES iteratively adds or removes edges to improve the model's score until no further modifications are possible. Another recent score-based technique is the Greedy Relaxation of the Sparsest Permutation (GRaSP) (Lam et al., 2022) algorithm developed by Lam et al. This algorithm aims to discover the most sparse permutation of variables that fulfills a specific set of causal constraints by greedily adding variables to the permutation while minimizing the impact on the sparsity measure.\nHistorically, deliberations on the causal relationship amid two factors have concentrated on the standard situation, where the factors included are Gaussian and have a Linear causal relationship. On the other hand, the discrete case has been considered. The Linear Gaussian circumstance, then again, is an exemption in which the causal asymmetry does not uncover itself in the observed data or their joint dispersion. In the event that the causal asymmetry exists, we should conclude that it must have been concealed somehow, either through latent factors or factual predisposition.\nFor causal discovery from continuous data, numerous techniques based on Functional Causal Models (FCMs) have recently been presented. The effect Y is represented by an FCM as a function of the direct causes X and certain unmeasurable components or noise.\nSeveral variations of the FCM have been demonstrated to be capable of producing distinct causal directions and have found practical uses. The green node attributed to Shimizu (2006), introduces the linear, non-Gaussian, and acyclic model (LiNGAM). f is linear in the LiNGAM and at most one of the noise terms and cause X is Gaussian. The impact Y is further generated by a post nonlinear transformation on the nonlinear effect of the cause X+ noise term in the post nonlinear (PNL) causal model.\nRCD (Repetitive Causal Discovery) is another causal functional model-based method recently developed by Maeda et. al. (2020) to uncover variable pairs influenced by latent confounders and determine the causal direction between variables unaffected by confounders. Experiments with simulated and real-world data are used to validate RCD's effectiveness. RCD consists of three major phases. It begins by extracting a collection of ancestors for each variable, which indicate direct or indirect causes. The back-door criterion (Pearl et al., 2000) is used to infer causal directions between observable variables by taking into account the identified shared causes. The procedure is repeated until no additional changes occur. Second, RCD distinguishes direct causes from indirect causes via conditional independence inference. Finally, RCD finds correlated variable pairs with undetermined causal directions to identify pairs of variables impacted by the same latent confounders. As noted by Maeda et al., the experimental results show that RCD is effective at detecting latent confounders, determining causal directions, and detecting variable pairings that are affected by common confounders.\nSpirtes and Zhang's review (2016) comprehensively covers concepts and methodologies for causal inference, including manipulations, causal models, sample predictive modeling, causal predictive modeling, structural equation models, the causal Markov assumption, and the faithfulness assumption. Their review focuses on constraint-based causal structures, structural equation models, and mechanisms for distinguishing cause and effect.\nBenozzo et al. (2016) in their recent studies propose a novel approach for causal discovery in time series. The developed technique is a classification- based causal discovery technique by defining a feature space fundamentally based on Granger Causality and by using the Multivariate Auto Regressive (MAR) model as the data generator. Cross Validation was used to assess the performance of the model and the results were quite promising. This makes it evident that this novel approach is a good substitute to the signal processing method for causality detection between two time series. However, as noted by Benozzo et. al., this approach is only limited to three time series at the moment.\nZheng et al. (2018) introduced DAG-NOTEARS in 2018, a method for causal discovery using continuous optimization. This method offers efficient global updates and avoids assumptions about the local graph structure, although it still faces challenges with nonconvex optimization problems. Another approach by Zheng et al., DAG-GNN, uses variational autoencoders with graph neural network architectures to address NP-hardness (Chickering, 1996; Chickering et al., 2004) in causal discovery.\nTraditional approaches face NP-hardness due to the combinatorial nature of the task. NP-hardness is a common issue faced by traditional approaches for causal discovery. Zheng et al. proposes another method called DAG Structure Learning with Graph Neural Networks (DAG-GNN) to cater to this issue with the help of a continuous constraint that allows implementation of renowned continuous optimisation techniques by exploring the effectiveness of neural networks as functional approximators to develop a deep generative model.\nThe method uses variational autoencoders parameterized with novel graph neural network architectures. The performance is tested over synthetic and real world datasets and the results achieved are promising.\nAnother recent advancement in this field was by Zhu et al. (2020) who introduced a novel approach to use Reinforcement Learning to search for the DAG with optimal score by incorporating a predefined score function and penalties for acyclicity. Zhu et al. use the actor-critic algorithm, implementing encoder-decoder models for building the actor. Post experimenting on synthetic and real datasets, the method gives promising results with 30-node graphs. However, dealing with larger graphs is still a challenge, as noted by Zhu et al. Moreover, there's still scope for development of a more efficient core function and/or implementing other RL algorithms with appropriate early stoppage.\nRecently, Wang et al. (2021) proposed a novel approach based on ordering based causal discovery with reinforcement learning. The ordering-based method for causal discovery used in this study is combined with reinforcement learning (RL), where an ordering is created and pruned using variable selection to engender a DAG. The two mechanisms are created using a Markov Decision Process (MDP) formulation along with an encoder-decoder architecture. The proposed approach has been tested on both artificial and real world datasets. Results show positive improvement as compared to other RL-based causal discovery approaches.\nIn this analysis, we compare and evaluate these novel approaches with conventional ones, considering recent advancements in the domain to provide a comprehensive evaluation of methods for causal discovery."}, {"title": "2.4 Relevant Survey on Causal Analysis", "content": "In addition to the basic works and concepts mentioned in the previous section, we also need to investigate causal discovery from a more holistic perspective. Consequently, an exhaustive analysis must be conducted, combined with authoritative surveys over the past five years, to gain a comprehensive understanding of this field.\nIt is imperative to differentiate the objectives and conceptual frameworks associated with causal discovery and causal inference to further delve into these two notions, as elucidated by Guo et al. (2020). Guo et al. emphasised that causal inference involves tracing the causal path from cause to effect, aiming to understand the impact of manipulating specific variables on others. Within the realm of causal inference, Yao et al. (2021) have conducted an in-depth investigation into the concepts, methods, and applications of causal inference, contributing significantly to the field. Acknowledging the distinct nature of time series data, which differs from i.i.d. data, is essential. This distinction presents unique challenges and considerations in analysing causality.\nAdditionally, to supplement the understanding of these concepts, Nogueira et al. (2022) have analysed and compared causal discovery and inference using software tools, providing practical examples for testing. Their work contributed to the existing body of knowledge by exploring the practical application and evaluation of different approaches.\nFocusing on time series, Moraffah et al. (2021) provided a comprehensive examination of causality for such data, offering insights into the generation of time series data, methodologies employed in the causal analysis, and evaluation metrics used to assess causal relationships. Notably, the study enumerated these evaluation metrics' specific attributes"}, {"title": "3 Survey Methodology", "content": "This section will introduce how to collect relevant research resources, including literature, codes, metrics, and datasets. Moreover, the last section briefly explains the analytical technologies we used."}, {"title": "3.1 Method Design", "content": "A quantitative research approach is employed to gather and analyze papers from databases systematically. This approach facilitates measuring and exploring trends, methods, datasets, and evaluation in the research domain. Google Scholar was chosen as the literature database due to its extensive coverage of scholarly articles. Information acquisition was initiated by employing a keyword search approach. Primarily, Figure 5 demonstrates the research trend of causal discovery, underpinned by the number of articles published during the preceding two decades.\nThe graph illustrates an escalating annual trend in research endeavours about causal discovery. This discernible growth can be attributed partly to the robust advancement of AI technologies in contemporary times, which has laid the groundwork for algorithmic developments facilitated by enhanced data processing capabilities.\nFurthermore, for preparing the programming underpinnings, our endeavour extends more than literature collection to encompass acquiring requisite algorithmic source code. To this end, we searched for the source code associated with each algorithm on the GitHub platform, with the integration and revision of original algorithm codes as deemed necessary.\nTo facilitate a comprehensive and impartial investigative analysis, this study employs both Comparative Analysis and Case Study methodologies. Specifically, a series of comparative experiments involving diverse algorithms on distinct artificial datasets are conducted. By analyzing the magnitudes of evaluation metrics, a quantitative assessment of algorithmic performance is executed, yielding overarching insights. Ultimately, to affirm the robustness and effectiveness of our findings, the conclusions are corroborated through case studies such as real datasets."}, {"title": "3.2 Data Collection", "content": "Here we executed a keyword-based search focusing on aspects within the tree graph above. The subsequent table illustrates the collection result derived from the published paper from 2004 to 2023, as sourced from Google Scholar.\nTo prevent repetitive retrieval and collection of papers, \"Causal Discovery\" is used as the basic keyword, with previously retrieved terms excluded when searching for new supplementary terms. It should be noted that we also searched for keywords related to \"causal inference\" to ensure comprehensive collection of papers within cross fields. We have identified 220 articles with the highest correlation from over 3000 related articles and have summarized the datasets, algorithms, and evaluation metrics for causal discovery.\nA selection of widely employed datasets for causal discovery has been identified by synthesising diverse review articles. These datasets encompass both real-world instances and artificial constructs. To streamline ensuing experimental processes, a classification framework has been devised to categorise these datasets into four types of causality relationships. Additionally, the sources of these datasets are also categorized into real-world and synthetic datasets.\nLikewise, we have identified performance metrics for assessing causal discovery techniques. These metrics are divided into two overarching families: graph-based metrics (Peters and B\u00fchlmann, 2015) and classification-based metrics. It is pertinent to highlight that nearly all metric computations necessitate the availability of"}, {"title": "3.3 Survey Analysis", "content": "Utilizing a comparative analysis framework, we systematically process the performance metrics. We selected a reference algorithm exhibiting superior performance to enhance result lucidity and then calculated residuals for other algorithms compared with the reference. The overall distribution of these residuals is visually represented through the violin plots.\nNevertheless, two specific algorithms may manifest insignificant disparities between their metric values. Hence, we introduce a significance assessment mechanism to pursue a more methodical treatment of data relationships. Given the constraint that each data size consists"}, {"title": "4 Causal Discovery Algorithms", "content": "Various research articles present diverse taxonomy for causal discovery, yet a universally accepted classification structure does not currently exist. Specifically, in recent years, the rapid advancements in this domain have resulted in the incompleteness and obsolescence of numerous surveys. It is essential to collect and analyse the taxonomy methodologies proposed in papers critically to establish a systematic categorisation of existing methods. Simultaneously, the devised structure should encompass as many algorithms as possible. Thus, this project diligently compiles and summarises the prevailing methods, effectively partitioning causal discovery into six fundamental categories:\n4.1 Granger Based Method\nGranger causality (GC) is one of the pioneering measurement methods for analysing time series data. Over several decades, undergoing refinement and evolution, it still maintains an irreplaceable position in the contemporary landscape of causal discovery. The core premise of Granger causality postulates that future events do not affect the present or past, while past events potentially impact both the present and the future. When the historical information of variables x and y are included, leading to better predictions for variable y than predictions based solely on the information of y, it signifies that variable x is considered the Granger cause of variable y. In mathematical notation, the given statement can be expressed as follows (McCracken, 2016):\n$P(Y_{n+1} \\in A | \\Omega_n) \\neq P(y_{n+1} \\in A | \\Omega_n - x)$ (1)\nEquation 1 illustrates a Granger causes y, wherein the variable x and y represents two discrete time series, and the subscript n corresponds to the time point t. The all-encompassing set of information available at all points $t \\leq n$ is symbolically denoted as $\\Omega_n$. To ascertain the Granger relationship based on the aforementioned formula, the Vector Autoregressive (VAR) model (L\u00fctkepohl, 2005) stands as the prevailing technique, built upon the premise of data stationarity and equipped to forecast variable values.\nOur attention is directed towards several primary algorithms based on GC. Arize et al. (1993) put forth the Multivariate Granger (MVGC) analysis method to overcome the limitations of Pairwise Granger (PWGC), which can only deal with bivariate data. However,"}, {"title": "4.2 Condition Independence Based Method", "content": "The conditional independence-based method exhibits a close association with probability. By quantifying the mutual information (Runge", "follows": "n$T_{x \\rightarrow y} = \\sum P(Y_{n+1}, y_n^{(k)}, x_n) log \\frac{P(Y_{n+1} | y_n^{(k)}, x_n))}{P(Y_{n+1} | Y_n^{(k)})}$ (2)\nIn contrast to Shannon entropy, the transfer entropy is computed by the Kullback entropy (Kullback, 1997). In this equation, $x_n$ represents the value of variable x at the nth time point, and likewise for the variable y, with the superscript indicating the time delay length. When $T_{x \\rightarrow y} - T_{y \\rightarrow x} > 0$, it can be inferred that variable x is the cause of variable y; conversely, variable y is the cause of variable x.\nCausal discovery algorithms based on conditional independence can be categorized into two distinct groups. The first category is the information-theoretic-based approach, with Optimal Causation Entropy (oCSE) (Sun et al., 2014, 2015) being a representative algorithm. OCSE is a two-step discovery algorithm explicitly designed for short time series data. The second approach is causal network-based, where the optimal causal graph is determined through statistical testing. The Peter-Clark (PC) algorithm (Kalisch and B\u00fchlman, 2007) has gained widespread adoption and has proven"}]}