{"title": "Synthesize, Partition, then Adapt:\nEliciting Diverse Samples from Foundation Models", "authors": ["Yeming Wen", "Swarat Chaudhuri"], "abstract": "Presenting users with diverse responses from foundation models is crucial for\nenhancing user experience and accommodating varying preferences. However,\ngenerating multiple high-quality and diverse responses without sacrificing accu-\nracy remains a challenge, especially when using greedy sampling. In this work,\nwe propose a novel framework, Synthesize-Partition-Adapt (SPA), that leverages\nthe abundant synthetic data available in many domains to elicit diverse responses\nfrom foundation models. By leveraging signal provided by data attribution meth-\nods such as influence function, SPA partitions data into subsets, each targeting\nunique aspects of the data, and trains multiple model adaptations optimized for\nthese subsets. Experimental results demonstrate the effectiveness of our approach\nin diversifying foundation model responses while maintaining high quality, show-\ncased through the HumanEval and MBPP tasks in the code generation domain\nand several tasks in the natural language understanding domain, highlighting its\npotential to enrich user experience across various applications.", "sections": [{"title": "Introduction", "content": "Transformer-based foundation models have revolu-\ntioned the fields of natural language processing\n(NLP) and code generation with their remarkable abil-\nities a wide range of understanding and generation\ntasks (Vaswani et al., 2017; Devlin et al., 2019; Brown\net al., 2020; Chen et al., 2021). These models are\ntypically pre-trained on vast amounts of text data and\nthen undergo instruction fine-tuning a post-training\nprocess to improve alignment with user expecta-\ntions and enhance the overall user experience (Ouyang\net al., 2022). Due to the high cost of human-annotated\ndata, synthetically generated datasets (Wang et al.,\n2022b) such as OSS-Instruct (Wei et al., 2023) and Al-\npaca (Taori et al., 2023) have become an important component of instruction tuning, demonstrating\nstrong effectiveness in improving foundation model performance.\nTo date, these synthetic datasets have been primarily used to align foundation models with instruc-\ntions or to induce certain preferable behaviors. In this paper, we focus on a different use of synthetic\ndata: in improving the diversity of foundation models' outputs. Diversifying the generated responses\nis crucial for accommodating diverse user preferences and enhancing user satisfaction. Consider the\nscenario illustrated in Fig. 1, where a user prompts a foundation model with \"Give me a personal\nwebsite template\". In this case, we would prefer the model to generate two diverse templates while\nmaintaining good quality, providing users with a variety of styles and layouts. Conventional meth-\nods for improving diversity, such as temperature sampling (Ackley et al., 1985; Hinton et al., 2015;\nWang et al., 2019, 2023), rely on sampling techniques that anneal the probabilistic distribution of\noutputs. These methods often trade off diversity for quality, as the generated responses may deviate\nfrom the learned distribution and produce hallucination or less coherent outputs (Lee, 2023). More-\nover, these techniques are not applicable when using greedy sampling, which is often preferred for\nits simplicity and precision. This highlights the need for approaches that not only align foundation\nmodel outputs with user expectations but also elicit diverse responses without sacrificing quality.\nIn this paper, we present a framework, Synthesize-Partition-Adapt (SPA), that achieves these objec-\ntives. The framework partitions the synthetic data and adapts foundation models to these partitions\nin the post-training stage. By leveraging the inherent diversity in the training data, this approach can\ngenerate diverse responses without compromising accuracy. The potential of partition-and-adapt\napproach is further amplified by the increasing availability of large-scale synthetic datasets because\nthe utility of instruction-tuning a single model on the entire dataset diminishes. In particular, we\nshow that influence function (Koh & Liang, 2017) can be an effective signal to partition synthetic\ndatasets into subsets, each targeting unique aspects that elicit distinct model behaviors. However,\nSPA is not limited to influence function and can be extended to other partitioning strategies. By\ntraining multiple adaptations on these subsets using parameter-efficient fine-tuning techniques, such\nas LoRA (Hu et al., 2021), we enable the generation of diverse and accurate responses.\nTo demonstrate the effectiveness of our approach, we conduct experiments on a range of tasks in\nboth the code generation and natural language understanding domains. We evaluate our method on\nthe HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) datasets for code generation,\nas well as several natural language understanding tasks. The results showcase the ability of our\napproach to diversify model responses while maintaining high accuracy, highlighting its potential to\nenrich user experience across various applications.\nTo summarize, the main contributions of this paper are as follows:\n\u2022 We propose SPA, a novel framework that leverages synthetic data, data partitioning, and model\nadaptation to elicit diverse responses from foundation models.\n\u2022 We demonstrate the effectiveness of SPA in diversifying foundation model responses while\nmaintaining sampling quality through extensive experiments on code generation and natural\nlanguage understanding tasks.\n\u2022 We highlight the potential of SPA to leverage the increasing availability of large-scale synthetic\ndatasets for improving the diversity of foundation model responses."}, {"title": "Background", "content": "By fine-tuning foundation models on human-annotated data that demonstrates desired behaviors,\ninstruction tuning aims to improve the alignment between the model's outputs and the user's inten-\ntions (Ouyang et al., 2022; Wei et al., 2021; Sanh et al., 2022). Let $D = \\{(x_i, y_i)\\}_{i=1}^N$ denote a dataset\nof input-output pairs, where $x_i$ represents the input instruction and $y_i$ represents the correspond-\ning desired output. The objective of instruction tuning is to minimize the following loss function:\n$L(\\theta) = -\\frac{1}{N} \\sum_{i=1}^N log P_\\theta(y_i | x_i)$ where $\\theta$ represents the parameters of the foundation model, and\n$P_\\theta(y_i | x_i)$ is the probability of generating the target response $y_i$ given the input $x_i$.\nClassical approaches for instruction tuning typically require a substantial amount of parallel labeled\ndata of NL intents and gold model responses. Collecting large-scale, high-quality annotated datasets\nis often time-consuming and expensive. To mitigate this issue, researchers have explored the use of\nsynthetic data for instruction tuning. By leveraging techniques such as data augmentation (Wei &\nZou, 2019; Sennrich et al., 2016) and back-translation (Edunov et al., 2018), synthetic data can be\ngenerated at scale, providing a cost-effective alternative to human-annotated datasets. Furthermore,\nsynthetic instruction-following data can also be generated from the foundation model itself (Wang\net al., 2022a; Honovich et al., 2022; Taori et al., 2023; Peng et al., 2023; Wen et al., 2024, inter alia)."}, {"title": "Data Attribution and influence function", "content": "Data attribution methods aim to quantify the importance or influence of individual training points on\na model's predictions. One such method is the influence function (Koh & Liang, 2017). Formally, let\n$L(\\theta)$ denote the loss function of the model, where $\\theta$ represents the model parameters. The influence\nof a training point $z$ on the model's parameters $\\theta$ is given by $I(z) = -H^{-1}\\nabla_\\theta L(z, \\theta)$, where $H$ is\nthe Hessian matrix of the loss function with respect to the model parameters, and $\\nabla_\\theta L(z, \\theta)$ is the\ngradient of the loss function with respect to the model parameters, evaluated at the training point $z$.\nNext, the influence of elevating the weight of $z$ on the loss associated with a test point $z_{test}$ is:\n$I(z, z_{test}) = -\\nabla_\\theta L(z_{test}, \\theta) H^{-1} \\nabla_\\theta L(z, \\theta)$                                                         (1)\nIt is impossible to calculate the full Hessian $H^{-1}$ matrix in deep neural networks. Koh & Liang\n(2017) developed a simple and efficient implementation that requires only oracle access to gradients\nand Hessian-vector products. This implementation makes it feasible to apply influence function\nto large-scale models. However, the vast parameter space of foundation models presents an even\ngreater challenge, rendering the direct application of influence function impractical. In response to\nthis, recent advancements in Grosse et al. (2023) have further refined the methodology, enabling the\napplication of influence function to large language models."}, {"title": "Problem Formulation", "content": "Given a user input x, our goal is to generate a di-\nverse set of high-quality responses $y_1, y_2, \u2026, y_K$ from\na foundation model M. One approach to generating\ndiverse responses is to sample from the model multi-\nple times using techniques like temperature sampling:\n$y_k = M(x;\\theta,\\tau)$, where $k = 1, 2, ..., K$ and $\\theta$ rep-\nresents the model parameters and $\\tau$ is the temperature\nhyperparameter. However, this approach often trades\noff diversity for quality as studied in Chung et al.\n(2023). An alternative approach is to train multiple\nmodel adaptations $M_1, M_2, ..., M_K$ and sample one\nresponse from each adaptation:\n$y_k = M_k(x; \\theta_k), k = 1, 2, ..., K$,                                          (2)\nwhere $\\theta_k$ represents the parameters of the k-th model\nadaptation. By training each adaptation on a differ-\nent subset of the data that captures unique aspects and\nyields distinct model behaviors, we can generate di-\nverse responses while maintaining their quality. Moreover, this approach allows us to elicit diverse\nsamples even with greedy sampling, which is often preferred for maximum precision.\nTraditionally, training multiple model adaptations has been considered unfavorable due to the re-\npeated training process, which can be computationally expensive and time-consuming. However,\nwith the increasing popularity of instruction tuning, it has become common practice to go through\na post-training stage using instruction data before deploying the model to users. This post-training\nstage presents an opportunity to train multiple model adaptations without incurring significant addi-\ntional costs, making the approach more feasible and practical in real-world scenarios.\nAs the volume of synthetic data grows, the utility of fine-tuning a single model on the entire dataset\ndiminishes due to the diminishing returns in the post-training stage, as demonstrated in Fig. 2. The\npass@1 accuracy after fine-tuning on the entire synthetic dataset using LORA is roughly the same\nas only consuming 15% of the data\u00b2. This creates an opportunity to leverage the abundant synthetic\ndata to train multiple model adaptations, each specializing in a specific subset of the data. In this\nwork, we propose the Synthesize, Partition, then Adapt (SPA) framework to address the diverse re-\nsponse generation problem. SPA leverages existing synthetic datasets, data partitioning techniques,\nand parameter-efficient fine-tuning methods to train multiple model adaptations. By sampling from\nthe collection of these adaptations, SPA generates diverse and high-quality responses, enhancing the\noverall user experience."}, {"title": "Partitioning Synthetic Data and Training Adaptations", "content": "We present the technical details of our proposed SPA framework for training multiple adaptations.\nWe leverage an existing synthetic dataset $D = \\{x_i, y_i\\}_{i=1}^N$ for the purpose of this study. The use\nof an existing synthetic dataset allows us to focus on the effectiveness of the Partition then Adapt\nsteps in eliciting diverse samples, while demonstrating the flexibility of our framework to work\nwith various synthetic datasets. Fig. 3 provides an overview of the framework. After obtaining the\nsynthetic data, our approach consists of three main steps: (1) computing data attribution scores for\nsynthetic data points, (2) partitioning the synthetic dataset based on these scores, and (3) training\nmultiple foundation model adaptations using parameter-efficient fine-tuning techniques like LORA.\nConsider a pre-trained foundation model M with parameters $\\theta$. Our goal is to leverage the synthetic\ndataset D to train a set of K foundation model adaptations $\\{M_k\\}_{k=1}^K$. Each adaptation focuses on a\nspecific subset of the data that yields similar model behaviors. To partition the synthetic dataset, we\nemploy data attribution methods that measure the importance of each training point to the model's\npredictions. Although we use influence function as an example to label the data, the SPA framework\nis not limited to influence function and can be extended to other data attribution methods, such as\nlexical overlap or TRAK (Park et al., 2023). To calculate the influence function, we first fine-tune the\npre-trained foundation model M on the synthetic dataset D. The fine-tuning process optimizes the\nmodel parameters $\\theta$ to minimize the loss function $L(\\theta)$ on the synthetic dataset using LORA (Hu\net al., 2021): $L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N l(y_i, M(x_i; \\theta))$, where $l(\u00b7, \u00b7)$ is a suitable loss function, such as\ncross-entropy loss for language modeling tasks. This fine-tuning process yields the optimized model\nparameters $\\theta$.\nNext, we select a set of M test queries $\\{(x_t^{(m)}, y_t^{(m)}\\}_{m=1}^M$, which can be a collection of questions\nrequiring various expertise knowledge to solve. For each test query $(x_t^{(m)}, y_t^{(m)})$, we compute the\ninfluence score of each synthetic data point $(x_i, y_i) \\in D$ using Eq. (1):\n$I((x_i, y_i), (x_t^{(m)}, y_t^{(m)})) = -\\nabla_\\theta l(y_t^{(m)}, M(x_t^{(m)}; \\theta)) H_\\theta^{-1} \\nabla_\\theta l(y_i, M(x_i; \\theta))$.                                    (3)\nTo efficiently compute the influence scores, we employ the stochastic estimation method proposed\nby Koh & Liang (2017), which approximates the inverse Hessian-vector product using conjugate"}, {"title": "Partitioning Synthetic Dataset", "content": "After computing the data attribution scores for each synthetic data point with respect to the M test\npoints, we obtain an influence matrix $I \\in R^{N \\times M}$, where $I_{i,m}$ represents the attribution score of the\ni-th synthetic data point for the m-th test point. To partition the synthetic dataset D into K subsets\n$\\{D_k\\}_{k=1}^K$, a clustering algorithm can be applied to solve the following objective:\n$\\min_{\\{D_k\\}_{k=1}^K} \\sum_{k=1}^K \\sum_{(x,y) \\in D_k} \\sum_{(x',y') \\in D_k} ||I_{i,:} - I_{i',:}:||_2^2$,                                                 (4)\nwhere $I_{i,:}$ denotes the i-th row of the influence matrix I, subject to $\\cup_{k=1}^K D_k = D$ and $D_k \\cap D_{k'} = \\emptyset$\nfor all $k \\neq k'$. In this work, we assume partitions are disjoint for the simplicity of the study.\nThe clustering algorithm assigns each synthetic data point $(x_i, y_i)$ to one of the K subsets based\non the similarity of its influence scores across the M test points. This partitioning ensures that\ndata points within each subset have similar impacts on the model's predictions. The choice of the\nclustering algorithm may depend on the specific characteristics of the dataset. For simplicity and\nease of implementation, in this study, we use a ranking heuristic to partition the synthetic dataset.\nThe details of this heuristic will be explained in the experiment section \u00a75.1. However, it is important\nto note that our SPA framework is not limited to any specific clustering algorithm."}, {"title": "Training Multiple Adaptations with LORA", "content": "Once the synthetic dataset is partitioned into K subsets, we train a foundation model $M_k$ for each\nsubset $D_k$ using parameter-efficient fine-tuning techniques like LORA (Hu et al., 2021). LORA\nadapts the pre-trained foundation model parameters $\\theta$ by learning low-rank matrices $A_k \\in R^{r \\times d}$\nand $B_k \\in R^{d \\times r}$ for each weight matrix $W \\in R^{d \\times d}$ in the pre-trained foundation model, where\n$r < d$ is the rank of the adaptation matrices.\nThe adapted weight matrix $W_k$ for the foundation model adaptation $M_k$ is computed as: $W_k =$\nW + $B_k A_k$. During the fine-tuning process, only the adaptation matrices $A_k$ and $B_k$ are learned,\nwhile the pre-trained weights W remain frozen. This significantly reduces the number of trainable\nparameters, making it feasible to train multiple foundation model adaptations with limited com-\nputational resources. The training objective for each foundation model adaptation $M_k$ is given\nby $\\min_{\\theta_k} \\sum_{(x_i, y_i) \\in D_k} l (y_i, M_k (x_i; \\theta_k))$ where $\\theta_k$ represents the parameters of $M_k$, which\ninclude the pre-trained weights $\\theta$ and the LoRA adaptation matrices $A_k, B_k$. By training multi-\nple foundation model adaptations using LORA, we can efficiently adapt the pre-trained foundation\nmodel to different subsets of the synthetic data, each focusing on a specific aspect of the data that\nyields similar model behaviors. This approach enables the creation of a diverse set of specialized\nmodels that capture different knowledge or expertise present in the synthetic data, while leveraging\nthe knowledge acquired during the pre-training phase.\nInference with Multiple Adaptations During inference, given a user input x, our goal is to gen-\nerate a diverse set of responses by leveraging the multiple foundation model adaptations trained on\ndifferent subsets of the synthetic data. To achieve this, we randomly sample a foundation model\nadaptation $M_k$ from the set of K adaptations $\\{M_k\\}_{k=1}^K$ and generate the output y using the se-\nlected adaptation. By randomly sampling from the set of adaptations, we can generate a diverse\nset of responses for the user input x. This approach ensures that the generated responses are not\nonly diverse but also maintain reasonable quality. It is worth noting that this approach is compatible\nwith various sampling techniques, such as temperature scaling, top-k and top-p sampling, which can\nfurther enhance the diversity of the generated responses."}, {"title": "Experiments", "content": "In this section, we present the experimental setup and results for evaluating the effectiveness of our\nproposed SPA framework in improving the diversity of foundation model outputs. We conduct ex-\nperiments on both code generation tasks such as HumanEval (Chen et al., 2021) and MBPP (Austin\net al., 2021) and several natural language understanding tasks."}, {"title": "Experimental Setup", "content": "Base Model and Synthetic Dataset For the code generation experiments, we use CodeLLaMA\n7B (Rozi\u00e8re et al., 2023) as the base foundation model. CodeLLaMA is a state-of-the-art language\nmodel specifically designed for code-related tasks, pre-trained on a large corpus of code and natu-\nral language data. For the synthetic dataset, we utilize the OSS-Instruct dataset (Wei et al., 2023),\nwhich consists of 75,000 code-related question-answering pairs generated by GPT-3.5 Turbo (Ope-\nnAI, 2023). In the natural language understanding domain, we employ Llama-2 13B (Touvron\net al., 2023) as the base foundation model. Llama-2 is a powerful language model trained on a di-\nverse range of web-scale data, demonstrating strong performance across various natural language\nunderstanding tasks. For the synthetic dataset, we use Platypus (Lee et al., 2023), which focuses\non improving LLMs' STEM and logic knowledge. Platypus consists of a curated sub-selection of\npublic text datasets, comprising approximately 25,000 question-answer pairs.\nData Attribution Scores We compare two methods for computing data attribution scores: influ-\nence function and lexical overlap.\nFor the influence-based method, we hand-write 12 examples that cover a wide range of knowledge\nfor each domain. For each of these examples, we calculate the influence score with respect to\neach training example in the corresponding synthetic dataset using Equation 3. We then select the\ntop 8 test queries whose distribution of influence scores over the dataset has the highest variance.\nThis ensures that the selected test queries have diverse impacts on the synthetic dataset, capturing\ndifferent aspects of the domain knowledge. The resulting influence matrices $I_{code} \\in R^{8 \\times 75,000}$ and\n$I_{nlu} \\in R^{8 \\times 25,000}$ are used for partitioning the OSS-Instruct and Platypus datasets, respectively.\nFor the lexical overlap method, we compute the BM25 score (Robertson et al., 1994) between each\ntraining example and the hand-written test queries. The BM25 score is calculated as follows:\n$I(z, Z_{query}) = \\sum_{t \\in Z_{query}} log \\frac{N+1}{N_t} \\frac{(k_1+1)f(z,t)}{k_1((1-b) + b \\frac{L(z)}{L_{avg}}) + f(z,t)}$,                                              (5)\nwhere $f(z, t)$ is the overlap count, N is the number of training examples, $L(z)$ is the length of the\nexample, and $L_{avg}$ is the average example length. We adopted the framework and the hyperpa-\nrameters in Lv & Zhai (2011). While we focus on influence function in this work, exploring the\neffectiveness of alternative data attribution methods like BM25 could be an interesting direction for\nfuture research. More details are provided in Appendix A.\nPartitioning the Synthetic Datasets To train multiple foundation model adaptations, we first set\nthe hyperparameter K, which represents the total number of adaptations. We use K = 8 for both\ncode generation and natural langauge understanding domain. For each data point in the synthetic\ndataset, we aim to find the test queries that provides the most influence. Formally, for each synthetic\ndata point $(x_i, y_i)$, we assign it to the subset $D_{k^*}$ corresponding to the test point with the highest\ninfluence score or the BM25 score: $k^* = \\arg \\max_{k \\in \\{1,...,K\\}} I_{k,i}$. where $I_{k,i}$ represents either\nthe influence matrix or the BM25 score matrix. This process partitions the OSS-Instruct dataset"}, {"title": "Natural Language Understanding Results", "content": "To demonstrate the effectiveness of SPA in the natural language understanding domain, we evaluate\nits performance on several diverse tasks, including Big-Bench Hard (BBH) (Suzgun et al., 2022),\nGPQA (Rein et al., 2023), MMLU (Hendrycks et al., 2020), and WinoGrande (Sakaguchi et al.,\n2019). For tasks that involve multiple-choice questions, we asked the model to continue generating\ntext even after producing an answer choice for the purpose of measuring sample diversity. As shown\nin Fig. 5, SPA with influence function consistently achieves higher diversity scores and average KL\ndivergence compared to the lexical overlap and random adaptation across all tasks. Interestingly,"}, {"title": "Related Work", "content": "Sampling-based methods have been widely explored to generate diverse text from language models.\nOne of the most common approaches is temperature sampling (Ackley et al., 1985; Hinton et al.,\n2015). Several studies have investigated the impact of temperature on model sampling and its effect\non the diversity-quality trade-off (Caccia et al., 2018; Renze & Guven, 2024; Wang et al., 2023).\nHigher temperatures lead to more diverse but potentially less coherent samples, while lower temper-\natures produce more conservative and deterministic outputs. When using high temperatures, human\ninterventions can help to correct errors during the sampling process (Chung et al., 2023). Dynamic\ntemperature strategies have also been explored during the model training and inference stages (Lin\net al., 2018; Zhang et al., 2018; Wang et al., 2019; Chang et al., 2023).\nBesides adjusting temperature, top-k, top-p (nucleus) sampling (Holtzman et al., 2019) and their\nvariants are common sampling methods (Fan et al., 2018; Meister et al., 2022; Hewitt et al., 2022;\nRavfogel et al., 2023), which restrict the sampling space or dynamically adjust the number of tokens\nconsidered at each step. Another line of works studied how to formulate quality-diversity trade-off\nas a search or RL problem (Naik et al., 2023; Lim et al., 2024; Mudgal et al., 2023; Bradley et al.,\n2023; Ji et al., 2023)."}, {"title": "Conclusion", "content": "In summary, we proposed SPA, which that leverages synthetic data, data partitioning, and model\nadaptation to elicit diverse responses from foundation models. By partitioning synthetic datasets into\nsubsets that capture unique aspects of the data and training multiple model adaptations optimized\nfor these subsets, SPA enables the generation of diverse and high-quality responses.\nLimitation One main challenges is the computational cost associated with influence function,\nwhich require several extra epochs of backward passes to estimate. Future work could explore more\nefficient data attribution methods, such as TRAK (Park et al., 2023) and K-FAC (Grosse et al., 2023)."}, {"title": "Experimental Setup Details", "content": "This section provides additional details on the experimental setup that were not included in the main\ncontent due to space constraints.\nComputing Data Attribution Scores For the lexical overlap method, we use a publicly available\nBM25 (Lv & Zhai, 2011; Trotman et al., 2014) implementation written in Python and released under\nhttps://pypi.org/project/rank-bm25/. We used the default hyperparameters.\nWhen calculating the influence function, we employ the conjugate gradient method with LiSSA\napproximation (Martens, 2010; Agarwal et al., 2016). We leverage a publicly available implementa-\ntion from https://github.com/alstonlo/torch-influence/. For the OSS-Instruct dataset,\nwe use a damping factor of 0.001, a depth of 120, and 500 repeats, following the guideline that the\nproduct of depth and repeats should be roughly equal to the dataset size. For the Platypus dataset,\nwe use a depth of 120 and 200 repeats. It is worth noting that computing the influence function is\nalso intensive with LORA. Each column of the I matrix in Eq. (4) requires approximately one epoch\nof backward passes over the entire synthetic dataset. On the OSS-Instruct dataset, this takes roughly\n5 hours using a single A100 80GB GPU. However, this is offline computation which is consumed\nbefore deploying the model to users."}, {"title": "Impact of Number of Adaptations", "content": "In this section, we investigate the impact of the number of model adaptations on the diversity of the\ngenerated responses. We focus on the HumanEval benchmark in the code generation domain and\nvary the number of adaptations from 8 to 12. The results are presented in Fig. 6.\nAs shown in Fig. 6, the diversity score remains relatively stable as the number of adaptations in-\ncreases from 8 to 12, regardless of the partitioning method used. These results suggest that increas-\ning the number of adaptations beyond a certain point may not necessarily lead to an improvement in\nthe diversity of the generated responses."}, {"title": "Test Queries", "content": "In this appendix, we provide the hand-written test queries used in our experiments for both the code\ngeneration and text generation domains. These examples were utilized to compute data attribution\nscores. Most of the examples are generated by GPT-4 (OpenAI, 2023)."}, {"title": "Code Generation Domain", "content": "1. \"\"\"Title: Longest Palindromic Subsequence\n2 Query: Write a function to find the longest palindromic subsequence\nin a given string.\n3 Solution:\n\"\"\"\n4\n5 def longest_palindromic_subsequence(s):\n6 n = len(s)\n7 dp = [[0] * n for in range(n)]\nfor i in range(n):\ndp[i][i] = 1\nfor length in range(2, n+1):\nfor i in range(n-length+1):\nj = i + length - 1\nif s[i] == s[j] and length == 2:\ndp[i][j] = 2\nelif s[i] == s[j]:\ndp[i][j] = dp[i+1][j-1] + 2\nelse:\ndp[i][j] = max(dp[i+1][j], dp[i][j-1])\nreturn dp[0] [n-1]\n 2. \"\"\"Title: Nth Fibonacci Number\n25 Query: Implement a function to calculate the nth Fibonacci number\nusing dynamic programming.\n26 Solution:\n\"\"\"\n def fibonacci(n):\nif n <= 0:\nreturn 0\nelif n == 1:\nreturn 1\nfib = [0] * (n + 1)\nfib [1] = 1\nfor i in range(2, n + 1):\nfib [i] = fib [i - 1] + fib [i 2]\nreturn fib [n]\n 3. \"\"\"Title: Sum of Two Largest Elements\n43 Query: Create a function that takes a list of integers and returns\nthe sum of the two largest elements in the list.\n44 Solution:\n\"\"\"\ndef sum_of_two_largest(nums):\nif len (nums) < 2:\nreturn sum (nums)\nlargest = second_largest = float('-inf')\nfor num in nums:\nif num > largest:\nsecond_largest = largest\nlargest = num\nelif num > second_largest:\nsecond_largest = num\nreturn largest + second_largest\n 4. \"\"\"Title: Maximum Subarray Sum\n62 Query: Implement a function to find the maximum subarray sum in a\ngiven array of integers.\n63 Solution:\n\"\"\"\n def max_subarray_sum(nums):\nmax_sum = float('-inf')\ncurrent_sum = 0\nfor num in nums:"}, {"title": "Text Generation Domain", "content": "Title: Economic Impacts of the Black Death\n2 Query: Explain the economic impacts of the Great Mortality in\nmedieval Europe.\n3 Response: The Great Mortality drastically reduced the population\nof Europe, leading to severe labor shortages, higher wages,\nlower prices for land, and a shift in economic power from the\nfeudal lords to the working class and merchants.\n4\n 2. Title: Photosynthesis Process\n6 Query: Describe the process of photosynthesis and its importance\nto the Earth's ecosystem.\n7\n8 Response: Photosynthesis is the process by which green plants and\nsome other organisms use sunlight to synthesize nutrients from\ncarbon dioxide and water. It generates oxygen as a byproduct,\nwhich is vital for most life forms on Earth.\n9\n 3. Title: Calculating Travel Distance\n10 Query: If a car travels at 60 miles per hour for 3 hours, how far\nhas it gone? Explain your calculation.\n11\n12 Response: The car has traveled 180 miles, calculated as 60\nmiles/hour * 3 hours.\n13\n 4. Title: Utilitarianism vs Deontological Ethics"}]}