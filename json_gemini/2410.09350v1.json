{"title": "Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation", "authors": ["Jinyoung Park", "Minseok Joo", "Joo-Kyung Kim", "Hyunwoo J. Kim"], "abstract": "Knowledge graph-grounded dialog generation requires retrieving a dialog-relevant subgraph from the given knowledge base graph and integrating it with the dialog history. Previous works typically represent the graph using an external encoder, such as graph neural networks, and retrieve relevant triplets based on the similarity between single-vector representations of triplets and the dialog history. However, these external encoders fail to leverage the rich knowledge of pretrained language models, and the retrieval process is also suboptimal due to the information bottleneck caused by the single-vector abstraction of the dialog history. In this work, we propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant knowledge subgraphs by directly generating their token sequences on top of language models. For effective generative subgraph retrieval, we introduce two key methods: (i) structure-aware knowledge graph linearization with self-supervised graph-specific tokens and (ii) graph-constrained decoding utilizing graph structural proximity-based entity informativeness scores for valid and relevant generative retrieval. DialogGSR achieves state-of-the-art performance in knowledge graph-grounded dialog generation, as demonstrated on OpenDialKG and KOMODIS datasets.", "sections": [{"title": "1 Introduction", "content": "The goal of dialog generation is to generate an informative and appropriate response given an input dialog. Pretrained Language Models (PLMs) have demonstrated promising performance on the dialog generation (Roberts et al., 2020; Touvron et al., 2023; Achiam et al., 2023). However, they often generate irrelevant, factually incorrect, or hallucinatory responses since the generation process heavily depends on the internal parameters of the language models (Lewis et al., 2020; Shuster et al., 2021). \u03a4\u03bf mitigate these issues, several studies (Wang et al., 2020; Zhao et al., 2020) have explored knowledge-grounded dialog generation models, which incorporate external knowledge to generate more factually accurate responses. Some approaches utilize unstructured texts such as Wikipedia articles (Dinan et al., 2019) and internet web pages (Ghazvininejad et al., 2018) while others (Moon et al., 2019; Galetzka et al., 2021; Tuan et al., 2022; Kang et al., 2023) leverage structured knowledge graphs (KGs) to capture both the relational and semantic information for grounding dialog responses.\nMany existing knowledge graph-grounded dialog generation models (Tuan et al., 2022; Kang et al., 2023) employ encoder-based retrieval methods. They encode the dialog history into a single vector and then use it on another encoder (e.g., bi-encoder) to retrieve relevant triplets from the KG. However, this approach can lead to an information bottleneck due to the limited capacity of a single vector to represent long and complex multi-turn dialogs (Humeau et al., 2020; Cao et al., 2021; Lee et al., 2022). Moreover, these methods (Galetzka et al., 2021; Tuan et al., 2022; Kang et al., 2023) often rely on separate models, such as graph neural networks (GNNs), to encode the knowledge graphs, which limits the integration of natural language comprehension capabilities of PLMs.\nRecent studies (Lee et al., 2022; Sun et al., 2023) have addressed the information bottleneck issue by applying generative retrieval methods, which cast retrieval as an autoregressive generation process to facilitate direct interactions between query context and knowledge paragraphs. Despite this progress, most generative retrieval works focus solely on natural language-based knowledge, employing conventional token representations and decoding strategies, which do not fully capture the structure and properties of knowledge graphs.\nTo address the aforementioned issues, we propose Dialog Generation model with Generative Subgraph Retrieval (DialogGSR), which integrates generative subgraph retrieval with response generation. Our proposed method adopts two key graph-specialized techniques: (1) a structure-aware knowledge graph linearization for effective graph representation and (2) graph-constrained decoding for valid subgraph retrieval. Our knowledge graph linearization approach introduces a small set of special token embeddings to account for both the structural positioning of knowledge entities and the reverse relationships between them. By self-supervising these special tokens using a knowledge graph reconstruction loss, the method effectively represents the knowledge graph. The graph-constrained decoding facilitates autoregressively retrieving the knowledge considering the graph structural information, thus generating valid and relevant knowledge subgraphs. Since DialogGSR utilizes pretrained language models for both subgraph retrieval and dialog generation, it leverages the pretrained language models' internal knowledge in both tasks.\nWe evaluate DialogGSR on two KG-grounded dialog generation datasets: OpenDialKG (Moon et al., 2019) and KOMODIS (Galetzka et al., 2020). Our proposed method shows the best performance on both benchmark datasets.\nOur contributions are three-fold as follows:\n\u2022 We propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves the relevant knowledge subgraphs by generating their token sequences.\n\u2022 We design knowledge graph linearization for effective graph representations and graph-constrained decoding for retrieving valid and relevant subgraphs.\n\u2022 We show the state-of-the-art response generation performance on two benchmark datasets, OpenDialKG and KOMODIS."}, {"title": "2 Related Works", "content": "Retrieving relevant information from a large corpus such as a text corpus or a knowledge base is crucial in many tasks (Chen et al., 2017; Thorne et al., 2018; Lewis et al., 2020; Izacard and Grave, 2021). Recent studies (Cao et al., 2021; Bevilacqua et al., 2022; Wang et al., 2022; Lee et al., 2022, 2023) have demonstrated that generative retrieval models can be more effective than conventional encoder-based retrieval models. They cast retrieval tasks as generation tasks, where relevant sequences are generated rather than retrieved given input queries. Several studies (Chen et al., 2022a; Thorne, 2022; Lee et al., 2022; Yu et al., 2023; Xu et al., 2023; Luo et al., 2024) have shown the effectiveness of generative retrieval in various knowledge-intensive natural language processing tasks. Motivated by these works, we propose a generative subgraph retrieval model with knowledge graph linearization and graph-constrained decoding for effective graph representation and generation."}, {"title": "2.1 Generative Retrieval", "content": "Retrieving relevant information from a large corpus such as a text corpus or a knowledge base is crucial in many tasks (Chen et al., 2017; Thorne et al., 2018; Lewis et al., 2020; Izacard and Grave, 2021). Recent studies (Cao et al., 2021; Bevilacqua et al., 2022; Wang et al., 2022; Lee et al., 2022, 2023) have demonstrated that generative retrieval models can be more effective than conventional encoder-based retrieval models. They cast retrieval tasks as generation tasks, where relevant sequences are generated rather than retrieved given input queries. Several studies (Chen et al., 2022a; Thorne, 2022; Lee et al., 2022; Yu et al., 2023; Xu et al., 2023; Luo et al., 2024) have shown the effectiveness of generative retrieval in various knowledge-intensive natural language processing tasks. Motivated by these works, we propose a generative subgraph retrieval model with knowledge graph linearization and graph-constrained decoding for effective graph representation and generation."}, {"title": "2.2 Knowledge-Grounded Dialog Generation", "content": "Many language generation approaches leverage pretrained language models (PLMs) (Radford et al., 2019; Devlin et al., 2019; Roberts et al., 2020; Thoppilan et al., 2022; Touvron et al., 2023; Achiam et al., 2023), showing strong performance. However, they often suffer from the hallucination issue (Du\u0161ek et al., 2018; Balakrishnan et al., 2019; Du\u0161ek et al., 2020), which generates plausible but factually wrong responses since they rely on the models' internal parameters. To address this problem, recent works (Moon et al., 2019; Dinan et al., 2019; Lian et al., 2019; Park et al., 2023) have proposed to augment the models with external knowledge sources. This approach is effective in generating factually accurate responses in various language generation tasks (Fernandes et al., 2019; Huang et al., 2020; Yasunaga et al., 2021; Yu et al., 2022; Zhang et al., 2022b). Regarding dialog generation, various works incorporate external knowledge graph into the generation (Moon et al., 2019; Zhou et al., 2018; Tuan et al., 2019; Zhang et al., 2020; Zhou et al., 2021). For instance, Space Efficient (Galetzka et al., 2021) proposes an efficient method to encode knowledge triplets. RHO (Ji et al., 2023) generates responses with the dialog history and knowledge graph represented by graph embedding methods (e.g., TransE (Bordes et al., 2013)). DiffKG (Tuan et al., 2022) uses a graph reasoning encoder on top of sparse matrices for graph representations. SURGE (Kang et al., 2023) applies GNNs to retrieve context-relevant subgraphs. Different from these works, our work autoregressively retrieves the context-relevant sub-graphs and then generates knowledge-grounded dialogs without separate knowledge graph modules."}, {"title": "3 Methods", "content": "We propose a retrieval-augmented dialog generation approach that retrieves contextually relevant subgraphs from knowledge graphs to generate better responses. Our model, Dialog Generation model with Generative Subgraph Retrieval (DialogGSR) consists of a generative subgraph retriever and a response generator. We first define the task of knowledge graph-grounded dialog generation (Sec. 3.1). Next, we propose Generative Subgraph Retrieval (GSR), which autoregressively retrieves subgraph by applying structure-aware knowledge graph linearization and graph-constrained decdoing (Sec. 3.2). We then present a response generator, which performs subgraph-grounded dialog generation (Sec 3.3). Finally, we provide the training details of DialogGSR including our self-supervised knowledge graph reconstruction loss (Sec 3.4). The inference process of DialogGSR is illustrated in Figure 1."}, {"title": "3.1 KG-Grounded Dialog Generation", "content": "The goal of knowledge graph-grounded dialog generation is to generate a dialog response by jointly reasoning over a dialog history and a knowledge graph. We represent a dialog history as a token sequence, x = [x\u2081, x\u2082,..., x\u2099], where x\u1d62 \u2208 V is the i-th token of the dialog history and V denotes the vocabulary set. A knowledge graph is defined as G = (E,R,T), where & is the set of entities and R is the set of relations. T denotes the set of triplets, (e\u2095,r,e\u209c) \u2208 T, each of which are composed of a head entity e\u2095 \u2208 E, a tail entity e\u209c \u2208 E, and a relation r\u2208 R between the two entities. We use k-hop subgraph linked to the entities mentioned in the input dialog as retrieval candidates following previous works (Kang et al., 2023). The example of a extracted candidate subgraph is in Figure 3. We formulate knowledge graph-grounded dialog generation as follows:\n$P_\\theta(Y|x, G) = \\prod_{j=1}^{t} P_\\theta(y_j|x, y_{<j}, G),$   (1)\nwhere y = [y\u2081, y\u2082,..., y\u209c] is the output response, t is the length of the response, and y_{<j} = [y\u2081,\u2026 y_{j-1}] denotes the generated sequence at the previous time steps. Since a KG can include a huge number of irrelevant entities and relations, KG-grounded dialog generation works generally retrieve subgraphs related to the dialog context for the efficiency and effectiveness."}, {"title": "3.2 Generative Subgraph Retrieval", "content": "We introduce Generative Subgraph Retrieval (GSR), which autoregressively retrieves a knowledge subgraph G. Since a knowledge subgraph can be represented as a set of triplets, retrieving sequences of knowledge triplets is equivalent to subgraph retrieval. Many subgraph retrieval methods in dialog generation (Zhang et al., 2022a; Kang et al., 2023) compute the relevance score between the dialog history and each knowledge triplet and retrieve the triplets with the highest scores.\nHowever, these methods often suffer from the information bottleneck problem (Izacard et al., 2020; Luan et al., 2021), as they encode long, multi-turn dialog histories into a single fixed-length vector, which has a limited capacity to accurately represent complex multi-turn dialogs. Moreover, these approaches require independent knowledge graph encoders to represent knowledge graphs, which cannot fully leverage the pre-trained knowledge embedded in the pretrained language models.\nTo address these limitations, generative subgraph retrieval casts graph retrieval as a graph generation, enabling more direct interaction between the dialog context and the knowledge graph by representing the graph with a token sequence. For effective generative retrieval, our GSR model incorporates two novel techniques: (1) Structure-aware knowledge graph linearization, which converts the knowledge graph into token sequences enriched with learnable special tokens that capture the connectivity and reverse relations between entities, and (2) Graph-constrained decoding, which ensures the language model to generate valid knowledge subgraphs by predicting the next tokens based not only on the language model's scores but also on the relational proximities of entities within the graph.\nStructure-aware knowledge graph linearization. The goal of knowledge graph linearization is to convert a knowledge graph into a token sequence comprehensible to language models. Our structure-aware knowledge graph linearization augments a sequence of knowledge graph tokens with graph-specific learnable special tokens to help the language model understand the graph's structural information without separate graph encoders. Different from prior graph linearization methods such as Xu et al. (2023), which do not take into account multi-hop graph connections and reverse relations, our structure-aware knowledge graph linearization better captures and effectively represents the underlying structures of knowledge graphs.\nSpecifically, if there are connected triplets (e.g., (e\u2081, r\u2081, e\u2082) and (e\u2082, r\u2082, e\u2083)), we efficiently represent the path as [Head] e\u2081 [Int\u2081] r\u2081 [Int\u2082] e\u2082 [Int\u2083] r\u2082...e\u2081\u208a\u2081 [Tail]. To represent multiple disconnected triplets or paths, we insert [SEP] between them. For more expressive representations of the special tokens, we use multiple consecutive tokens to represent each of [Int], [Rev], which improves the performance as in Section A.1.\nAdditionally, since a knowledge graph can contain reverse relations, representing them is crucial in knowledge graph processing (Feng et al., 2020; Qi et al., 2023; Zhu et al., 2024). Therefore, we introduce another special token [Rev] for reverse relations when (1) there is a mentioned entity that is the tail of a triplet because the decoding always starts with one of the mentioned entities, or (2) two triplets are connected with opposite directions (e.g., (e\u2081, r\u2081, e\u2082) and (e\u2083, r\u2082, e\u2082)). We effectively represent reverse relations by adding special tokens [Rev\u2081] and [Rev\u2082] without modifying the relation tokens. For example, given a triplet (e\u2083, r\u2082, e\u2082), the corresponding triplet with the reverse relation (e\u2082,\u0159\u2082, e\u2083) is represented as [Head] e\u2082 [Rev\u2081] r\u2082 [Rev\u2082] e\u2083 [Tail].\nIn sum, we represent the subgraph \u0177 as the concatenation of the knowledge paths converted with the special tokens as follows:\nz = [Head] e\u2081 [Int\u2081] r\u2081...\ne_{l+1} [Tail] [SEP] [Head] e\u2096\u2026    (2)\nAll the special tokens are learnable with soft prompting. They are learned with both downstream task loss and knowledge graph reconstruction loss, which will be introduced in Section 3.4. Our structure-aware knowledge graph linearization with the special tokens helps the language model capture knowledge graph information without any separate knowledge graph encoders, which leads to the full utilization of the power of PLMs.\nGraph-constrained decoding. The language model is prone to generating invalid or irrelevant subgraphs due to its bias, often disregarding the knowledge graph structures (Cao et al., 2021; Chen et al., 2022b). To address this issue, we introduce a graph-constrained decoding method that ensures the generation of valid and relevant subgraphs. Formally, given a dialog \u00e6 and the previously generated segments of linearized knowledge path \u03c0<t, the log probability of the next token w is computed with log pvocab (w|x, \u043f<t, \u0421\u043c). Here, \u0421\u043c represents a prefix tree derived from the ego-graph (Zhu et al., 2021) of a set of mentioned entities em \u2208 M as depicted in Figure 1 (right). The mentioned entities are the entities that appear in the input dialog history and correspond to entities in the knowledge"}, {"title": "3.3 Response Generation", "content": "After retrieving the subgraphs, we generate a response based on both the dialog history and the retrieved subgraphs. To incorporate the retrieved knowledge subgraph \u011c, we first apply the knowledge graph linearization to convert \u0177 into a sequence of tokens, z\u011d. This linearized subgraph is then concatenated with the dialog history \u00e6, forming the input sequence for the dialog generation model as\nx = [z\u0302; x],    (7)\nwhere [;] denotes concatenation operation. The combined sequence is fed into the response generation model to get the final response y. By augmenting the dialog input with the knowledge graph, this method ensures that the generated response is both contextually relevant and knowledge-grounded."}, {"title": "3.4 Training DialogGSR", "content": "Our DialogGSR is trained in a multi-stage process. The training process consists of: (1) self-supervision through knowledge graph reconstruction, (2) training the generative subgraph retriever, and (3) optimizing the response generation model. These stages work in synergy to ensure the model effectively retrieves knowledge from graphs and generates coherent, knowledge-grounded responses. We also train the response generator by minimizing response generation loss.\nKnowledge graph reconstruction. Inspired by masked language modeling techniques (Roberts et al., 2020; Devlin et al., 2019), we propose a self-supervised learning approach to learn the special tokens by masking either an entity token or a relation token in the token sequence of each knowledge path and reconstructing it. Specifically, we first sample k-hop path G' from the knowledge source graph G and convert it into token sequence zG\u0131. During training, we randomly mask out either an entity token or a relation token from the sequence. The loss is formulated as\n$L_{GraphRecon} = -log p(z_{G^\\prime}|\\hat{z_{G^\\prime}}),$    (8)\nwhere zG\u0131 is the token sequence of a sampled path and \u017cG\u0131 is its randomly masked sequence. For example, a knowledge triplet zP = <\u2018Scarlet Letter\u2019, \u2018written by\u2019, \u2018N.Hawthorne\u2019 \u3009 can be randomly masked as\n<<M>, \u2018written by\u2019, \u2018N.Hawthorne\u2019>\n('Scarlet Letter\u2019, <M>, \u2018N.Hawthorne\u2019)\n('Scarlet Letter\u2019, \u2018written by\u2019, <M>\u3009.\nNote that masking is done at the entity or relation level as done in Roberts et al. (2020). By minimizing the graph reconstruction loss, our framework self-supervise the special tokens [Head], [Int],[Rev],[Tail] in (2), resulting in better knowledge graph representations. All the other parameters are frozen during this stage.\nKnowledge subgraph retrieval. We train our generative subgraph retriever (GSR) to identify relevant subgraphs for dialog generation. Unlike conventional retrieval methods, our approach frames retrieval as a generation task, enabling a more seamless integration with the dialogue context. The loss is defined as follows:\n$L_{Ret} = E_x [-log p (G^*|x)]$   (9)\nwhere G* is the gold subgraph and \u00e6 is the dialog context We use cross-entropy loss to train the retriever, ensuring it generates subgraphs that are both relevant and informative.\nResponse generation. The final stage of training DialogGSR is response generation. We generate dialog responses with dialog history \u00e6 and context-relevant knowledge subgraphs G retrieved from GSR. The response generation loss is defined as follows:\n$L_{Gen} = E_x [-log p (y^*|x, G)]$    (10)\nwhere y* is the golden response."}, {"title": "4 Experiments", "content": "In this section, we evaluate the effectiveness of the proposed DialogGSR on knowledge graph-grounded dialog generation. We first introduce the two datasets (OpenDialKG (Moon et al., 2019) and KOMODIS (Galetzka et al., 2020)), and the experimental setup and metrics. Then, we demonstrate the effectiveness of DialogGSR on the two benchmark datasets. Lastly, we provide ablation studies, and analyses of our DialogGSR."}, {"title": "4.1 Datasets", "content": "OpenDialKG is an open-domain dialog dataset, which consists of 15K dialogs with 91K turns and 1.12M triplets from Freebase knowledge graph (Bast et al., 2014). The knowledge graph has 1,190,658 triplets, 100,813 entities, and 1,358 relations. There are 49% of the turns having gold knowledge triplets. Following (Galetzka et al., 2020), we randomly split the samples into train (70%), validation (15%), and test (15%) sets. We evaluate the response generation and retrieval performance of our DialogGSR with other baselines using OpenDialKG dataset.\nKOMODIS is a closed-domain dialog dataset that consists of 7.5k dialogs with 103k turns and the corresponding KG, which contains 88K triplets. Following (Moon et al., 2019; Kang et al., 2023; Galetzka et al., 2020), we randomly split the dialogs into train (70%), validation (15%), and test (15%) sets for KOMODIS dataset, too. With KOMODIS dataset, we evaluate the response generation performance of our DialogGSR with other baselines following (Kang et al., 2023; Galetzka et al., 2021)."}, {"title": "4.2 Experimental Setup", "content": "For fair comparisons with previous works, we use T5-small (Roberts et al., 2020) as the base PLM. We select the best model on the validation set to evaluate the performance of all experiments. More details are in Appendix B.\nEvaluation metrics. We evaluate the dialog generation performance of different models with BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and unigram F1 score, by comparing the generated responses with the gold responses. In addition, we use the KQA metric (Kang et al., 2023), which measures whether the factually correct and necessary knowledge is contained in the generated response given the dialog history. We also evaluate the performance of the retriever with path@k metrics, which are the recall@k of ground-truth paths following (Moon et al., 2019; Jung et al., 2020)."}, {"title": "4.3 Experimental Results", "content": "We compare our DialogGSR with existing knowledge-grounded dialog generation models on Open-DialKG dataset. Table 1 shows that DialogGSR achieves the best performance in all metrics (BLEU, ROUGE, KQA, and F1 score). In particular, DialogGSR outperforms other baselines on KQA metrics by a large margin (4.61 on EM metric), which indicates that the proposed method generates more factually correct responses with relevant knowledge. In addition, our method achieves a 1.53 performance gain on BLEU-1 metric compared to the best baseline method, which is an 8.61% improvement. The performance gain of DialogGSR compared to SURGE, which retrieves the subgraph with a bi-encoder and uses graph neural networks for graph representations, indicates that our generative retrieval is effective in retrieving relevant knowledge and generating more accurate responses based on the retrieved knowledge.\nWe also conduct experiments on KOMODIS (Galetzka et al., 2020) dataset. Similar to the OpenDialKG result, Table 2 demonstrates that our DialogGSR achieves the best performance compared to all the previous approaches. To further validate the effectiveness of our generative subgraph retrieval, we compare the retrieval performance by path@k metrics. Table 3 shows that DialogGSR achieves the best performance compared to the other baselines. This result indicates that our generative subgraph retrieval successfully retrieves context-relevant subgraphs from the knowledge graph by fully utilizing the power of pretrained language models."}, {"title": "4.4 Human Evaluation", "content": "We conduct human evaluation to assess the generated responses of our dialog generation model. The detailed process of human evaluation is in Appendix C. Table 4 shows the experimental results of the human evaluation, where DialogGSR outperforms SURGE in all the metrics (Consistency, Informativeness, Fluency). In particular, on the Consistency and Informativeness metrics, DialogGSR achieves statistically significant performance gains of 0.16 and 0.47 over SURGE (based on t-test with p-value < 0.05), which indicates that our generative subgraph retrieval performs significantly better in retrieving informative knowledge compared to existing retrieval methods. Our DialogGSR provides a relatively small performance gain of 0.11 on the Fluency metric. Since the Fluency metric is more influenced by the language model's performance than the knowledge retrieval performance, it is reasonable to expect similar fluency scores when using the same base language model (T5-small) for fair comparisons."}, {"title": "4.5 Analysis", "content": "We analyze DialogGSR to answer the following research questions: [Q1] Does each component of DialogGSR contribute to a performance improvement? [Q2] Are graph-constrained decoding and the entity informativeness score helpful for retrieving context-relevant subgraphs? [Q3] Is GSR robust to the information bottleneck issue? [Q4] Is DialogGSR effective with large language models (LLMs)?\nAblation studies. We provide the ablation studies to answer [Q1], [Q2] by empirically showing the contribution of each component of DialogGSR in Table 5. w/o Const. is generative retrieval without graph-constrained decoding. Hard const. is the retrieval with graph-constrained decoding but not considering entity informativeness score. Connection and Katz use entity informativeness scores based on Connection (IScon) and Katz metrics (ISKatz) referred in Section 3.4, respectively. with Special tokens (w/o Recon.) uses special tokens to linearize the knowledge graph without graph reconstruction learning while with Special tokens (w/ Recon.) uses prompts learned with graph reconstruction. Table 5 shows that each component contributes to the performance improvement of the model. In particular, graph-constrained decoding is crucial in our generative approach.\nIn addition, the models with graph constraints show improvements compared to the model without the constraints, which indicates that the graph constraint is important for the generative retrieval of knowledge subgraphs. Also, using entity informative score (Connection, Katz) performs better than graph constraints without it since the entity informativeness score reflects graph structural proximity in the decoding process.\nEffectiveness of DialogGSR with LLMs. To assess the effectiveness of our DialogGSR with Large Language Models (LLMs) ([Q4]), we apply it to LLaMA-3 (Meta, 2024). The experimental result is shown in Table 6. From the table, the performance gain of DialogGSR compared to the base model is 2.42 in BLEU-1 score. In addition, the experimental result demonstrates that our proposed graph-constrained decoding is still important in LLMs. This indicates that DialogGSR is also effective in large language models."}, {"title": "Information bottleneck issue", "content": "Information bottleneck issue (Humeau et al., 2020; Lee et al., 2022) usually occurs when a long text sequence, such as a dialog history, is encoded into a single fixed length of vector. To explore the robustness of DialogGSR to the information bottleneck issue ([Q3]), we compare the retrieval performance of DialogGSR with the baselines such as DiffKG and SURGE with respect to the number of turns in dialog histories in Figure 2. The result shows that DialogGSR is robust for long dialogs whereas the other methods often deteriorate as the number of turns increases."}, {"title": "Qualitative analysis", "content": "We perform qualitative analysis by comparing responses generated from SURGE and DialogGSR. Table 7 shows a sampled Gold response and the responses generated by SURGE (Baseline response) and DialogGSR (DialogGSR response) given a multi-turn dialog. From the table, DialogGSR retrieves more informative knowledge to generate responses compared to the baseline. Given the last turn \u201cDo you by any chance remember who Mila Kunis is married too, I totally forgot\u201d, DialogGSR successfully retrieves the knowledge information related to 'Mila Kunis' to help provide the appropriate response from the question while the baseline fails to retrieve information related to answer the question. In contrast, the baseline incorrectly retrieves knowledge information related to \u201cJustin Timberlake\u201d, who is mentioned in the past turn (4th turn), which results in a factually incorrect response. This demonstrates that generative retrieval is effective in retrieving informative knowledge and generating knowledge-grounded multi-turn dialogs. More qualitative results are included in Appendix A.2."}, {"title": "5 Conclusion", "content": "We have presented DialogGSR, a dialog generation model with generative subgraph retrieval. DialogGSR retrieves context-relevant subgraphs, by generating the subgraph token sequences considering both the dialog context and the graph information. We have proposed novel knowledge graph linearization to convert knowledge triplets into token sequences with self-supervised graph-specific tokens to represent knowledge graphs without separate knowledge graph modules. In addition, we have formulated a graph-constrained decoding for valid and relevant generative retrieval. Our experiments demonstrate the effectiveness of our proposed method in knowledge-graph grounded dialog generation. Our codes are publicly available at https://github.com/mlvlab/DialogGSR."}, {"title": "Limitations", "content": "The proposed DialogGSR generatively retrieves token sequences of the subgraph from a knowledge graph and then generates a response with the retrieved subgraph. However, similar to works using graph retrieval on knowledge-grounded dialog generation, our generative subgraph retrieval can retrieve only the knowledge information contained in the knowledge graph. Second, the benchmark datasets for knowledge graph-grounded dialog generation are limited. Therefore, new benchmark datasets on dialog generation with knowledge graphs warrants greater attention."}, {"title": "Ethics Statement", "content": "Our DialogGSR does not have any direct negative social impacts, but it can potentially be used maliciously, similar to other dialog generation models. These models may produce factually incorrect or biased responses, particularly in sensitive areas such as politics, religion, and diplomacy. To address these risks, we advocate for the release of benchmark datasets without private information and emphasize the need for research into the methods that detect the source of texts. These measures are essential for the responsible development and use of dialog generation technologies."}, {"title": "A Additional experiments", "content": null}, {"title": "A.1 Additional Quantitative Analysis", "content": "We also conduct experiments to verify the contribution of using [Rev] to represent reverse relations and multiple consecutive tokens to represent each [Rev] or [Int] in Table 8. By adding reverse tokens to the knowledge, which allows mentioned entities that are tail entities in the provided triplets to be the starting points for the decoding, the performance is improved by 0.33 on BLEU-1 metric. Also, using multiple consecutive tokens to represent each [Rev] or [Int] (e.g., [Head] e\u2081 [Int\u2081\u2081] [Int\u2081\u2082] r\u2081 [Int\u2082\u2081] [Int\u2082\u2082] e\u2082 [Tail]) gives the performance gain on all the metrics since using the multiple tokens improve the capacity of representing the entities and the relations on top of language models. By adding all the components, performance significantly improves by 0.56 on BLEU-1 metric compared to the linearized knowledge graph without any special tokens, which demonstrates the effectiveness of our proposed knowledge graph linearization approaches with special tokens. Interestingly, adding reverse tokens with using multiple consecutive tokens improves the overall performance compared to adding reverse tokens without using multiple consecutive tokens, which indicates that representing reverse relations is more effective when the capacity of the knowledge representation is increased."}, {"title": "A.2 Additional Qualitative Analysis", "content": "In Table 9, we provide additional qualitative examples for what we have shown in Table 7 of the main paper. Our DialogGSR often generates high-quality responses similar to the main paper. For example, in the first example, our DialogGSR generates a factually correct response \u201cIt was written by Frank Beddor\u201d based on the retrieved triplet (\u2018The Looking Glass Wars\u2019, \u2018written_by\u2019, \u2018Frank Beddor\u2019) while SURGE generates a factually incorrect response \u201cTerry Pratchett\u201d with the same triplet (\u2018The Looking Glass Wars\u2019, \u2018written_by\u2019, \u2018Frank Beddor\u2019). It demonstrates that our DialogGSR is more effective in generating responses even with the same knowledge information given. In the second example, DialogGSR successfully generates a factually correct response by retrieving context-relevant knowledge triplets whereas the factually incorrect response is generated by the baseline due to the retrieval of irrelevant knowledge. These results demonstrate that our generative retrieval is effective in retrieving informative knowledge and generating knowledge-grounded dialogs."}, {"title": "B Experimental details", "content": null}, {"title": "B.1 Implementation details", "content": "In this section, we describe the implementation details not included in our main paper. For all the experiments, we use PyTorch\u00b9 (Paszke et al., 2019) and Transformer module of Huggingface\u00b2 (Wolf et al., 2019) as our code base. All experiments are conducted with 48GB NVIDIA RTX A6000 GPU. We select the best model on the validation set to evaluate the performance of all experiments. The epoch for training is set to 50 and the weight decay is 0.1. We use AdamW optimizer (Loshchilov and Hutter, 2019) to train our model and adopt learning rate decay.\nKnowledge graph-constrained decoding. Without the graph constraints, the language model is prone to generate invalid or irrelevant subgraphs due to the language model's bias (Chen et al., 2022b; Cao et al., 2021). To inject the knowledge graph information into the language model in the decoding step, we present a knowledge graph-constrained decoding method. We use \u03b1 = 0.8 and k = 2 for calculating Katz (Katz, 1953) index-based entity informativeness score. Pgraph is defined in Eq. (6) of the main paper, and b is 5."}, {"title": "C Details of Human Evaluation", "content": "We first randomly selected 30 dialogs from Open-DialKG test dataset (Moon et"}]}