{"title": "Understanding the Emergence of Multimodal Representation Alignment", "authors": ["Megan Tjandrasuwita", "Chanakya Ekbote", "Liu Ziyin", "Paul Pu Liang"], "abstract": "Multimodal representation learning is fundamentally about transforming incomparable modalities into comparable representations. While prior research primarily focused on explicitly aligning these representations through targeted learning objectives and model architectures, a recent line of work has found that independently trained unimodal models of increasing scale and performance can become implicitly aligned with each other. These findings raise fundamental questions regarding the emergence of aligned representations in multimodal learning. Specifically: (1) when and why does alignment emerge implicitly? and (2) is alignment a reliable indicator of performance? Through a comprehensive empirical investigation, we demonstrate that both the emergence of alignment and its relationship with task performance depend on several critical data characteristics. These include, but are not necessarily limited to, the degree of similarity between the modalities and the balance between redundant and unique information they provide for the task. Our findings suggest that alignment may not be universally beneficial; rather, its impact on performance varies depending on the dataset and task. These insights can help practitioners determine whether increasing alignment between modalities is advantageous or, in some cases, detrimental to achieving optimal performance. Code is released at: https://github.com/MeganTj/multimodal_alignment.", "sections": [{"title": "1. Introduction", "content": "Multimodal AI represents a cutting-edge paradigm in machine learning that enables integrating and learning from many heterogeneous and interacting data modalities. These AI systems are revolutionizing predictive analytics across many applications, including in multimedia (Alayrac et al., 2022; Sun et al., 2019; Ramesh et al., 2021; Singer et al., 2022), healthcare (Cai et al., 2019; Muhammad et al., 2021), and physical sensing (Kirchner et al., 2019; Lee et al., 2019; Xiao et al., 2020). A large body of research in designing and training multimodal models has focused on aligning the representations from different modalities such that they are comparable in some semantic representation space (Baltru\u0161aitis et al., 2018; Liang et al., 2024). Conventional wisdom posits that aligned representations are a crucial precursor to multimodal fusion and representation learning (Li et al., 2021). As a result, many learning methods, such as contrastive learning and its variants (Frome et al., 2013; Jia et al., 2021; Radford et al., 2021a), and model architectures (Bertinetto et al., 2016; Lenc & Vedaldi, 2019; Bansal et al., 2021; Csisz\u00e1rik et al., 2021) have been proposed to explicitly align incomparable modalities into comparable representation spaces for further processing.\nHowever, recent work on the \"Platonic Representation Hypothesis\" showed that, surprisingly, alignment could even emerge across independently pre-trained vision and language models without explicitly aligning them together (Huh et al., 2024). Crucially, alignment increases with model size and performance, and it has been hypothesized that unimodal models will become increasingly aligned. These findings raise fundamental questions regarding the emergence of aligned representations and their implications on multimodal learning: (1) when and why does alignment emerge implicitly, and (2) is alignment a reliable indicator of performance? We illustrate these open questions in Figure 1.\nIn this paper, we study these questions comprehensively across two principal dimensions that taxonomize multimodal data: interactions and heterogeneity (Baltru\u0161aitis et al., 2018; Liang et al., 2024; Tian et al., 2020), visualized in Figure 2. Interactions measure the information shared between two modalities for a task, from more redundant (e.g., images and corresponding captions) to more unique (e.g., sensor placement). We expect alignment to emerge more easily between redundant modalities. Heterogeneity measures the degree of similarity across two modalities independent of the task, from more similar (e.g., two languages) to more different (e.g., text and video). We expect alignment to emerge more easily between similar modalities.\nThrough extensive experiments on controlled and real-world datasets with varying degrees of interactions and heterogeneity, we discover several key insights. First, the maximum alignment achievable depends on the degree of heterogeneity and uniqueness in the modalities, which inherently limits alignment. Second, while alignment correlates with performance in datasets with high redundancy, this relationship breaks down when uniqueness dominates redundancy. These findings highlight that performance often does not directly correspond to alignment, and the connection between them is a nuanced property of the data that varies across modalities and tasks. Therefore, our work provides important considerations for practitioners designing and training multimodal models, emphasizing that scale alone does not guarantee modality alignment and that careful assessment is necessary to determine when alignment is beneficial."}, {"title": "2. Representation Alignment", "content": "In this section, we review the concept of representation alignment through prior work on measuring alignment, methods to explicitly align representations, and observations regarding the emergence of alignment.\nMeasuring Alignment. Measuring alignment between neural network representations is a widely used approach in the research community to analyze and improve training dynamics (Huh et al., 2024; Klabunde et al., 2024; Kornblith et al., 2019). A prominent class of alignment metrics is based on canonical correlation analysis (CCA), a statistical technique for comparing two subspaces (Thompson, 2005; Golub & Zha, 1995), along with its nonlinear extensions using kernels (Lai & Fyfe, 2000; Raghu et al., 2017) and neural networks (Andrew et al., 2013; Wang et al., 2015; Morcos et al., 2018). Among these methods, Kornblith et al. (2019) highlight the advantages of Centered Kernel Alignment (CKA), particularly its invariance to orthogonal transformations and isotropic scaling. Given mean-centered feature sets of n samples, $Z_1, Z_2 \\in \\mathbb{R}^{n\\times d}$, from two modalities $X_1$ and $X_2$, the CKA metric with a linear kernel is:\n$CKA(Z_1, Z_2) = \\frac{ALIGN (Z_1, Z_2)}{\\sqrt{ALIGN(Z_1, Z_1)\\cdot ALIGN(Z_2, Z_2)}}$ where $ALIGN(Z_1, Z_2)$ denotes $HSIC(Z_1Z_1^T, Z_2Z_2^T)$ with HSIC denoting an empirical estimator of the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005). Intuitively, CKA quantifies alignment by comparing the covariance structures of two feature sets, capturing whether their representations encode similar relationships. This property ensures that even if $Z_1$ and $Z_2$ undergo arbitrary rotations (e.g., due to different initialization schemes), the CKA metric remains consistent, making it a robust choice for assessing representation alignment. For large vision-language models, given the high dimensionality and large embedding sizes of learned representations, we employ a computationally efficient variation of CKA the mutual k-nearest neighbors (mutual KNN) method (Huh et al., 2024). Instead of directly comparing full covariance structures, this approach measures similarity by analyzing the overlap between the k-nearest neighbor sets of embeddings, improving scalability. Details are provided in Appendix B.\nExplicit Alignment. In addition to research on measuring alignment in neural networks, another line of work focuses on explicitly aligning representations, a widely used technique for handling heterogeneous modalities (Liang et al., 2024). A popular approach in this domain is multimodal contrastive learning, where representations of the same concept across different modalities (i.e., positive pairs) are brought closer together, while representations of different concepts (i.e., negative pairs) are pushed apart (Frome et al., 2013; Jia et al., 2021; Radford et al., 2021a). The coordination distance in contrastive learning is typically measured using cosine distance (Mekhaldi, 2007) or max-margin losses (Hu et al., 2019). Theoretical results demonstrate that contrastive learning effectively captures redundant information shared between modalities (Tian et al., 2020; Tosh et al., 2021). More recent extensions have been proposed to also capture unique and synergistic information, further refining multimodal representation learning (Dufumier et al., 2024; Liang et al., 2023b).\nEmergence of Implicit Alignment. In contrast to explicit alignment methods, recent findings suggest that alignment can emerge implicitly, even when neural networks differ in training objectives, datasets, and architectures (Li et al., 2015; Raghu et al., 2017; Lenc & Vedaldi, 2019; Barannikov et al., 2022; Bonheme & Grzes, 2022). Notably, this similarity becomes more pronounced in larger and wider networks (Raghu et al., 2017; Morcos et al., 2018; Kornblith et al., 2019). Building on the observation that latent spaces are inherently comparable, a line of research explores composing components of different models with minimal or no additional training. Lenc & Vedaldi (2019) demonstrate that latent spaces can be stitched together using trainable stitching layers, while subsequent studies (Bansal et al., 2021; Csisz\u00e1rik et al., 2021) show that better-performing models tend to learn more similar representations when stitched. More recently, the Platonic Representation Hypothesis (Huh et al., 2024) suggests that as vision and language models scale in capacity and performance, independently trained models exhibit increasing alignment. This finding implies that models are converging toward modality-agnostic representations, reinforcing the idea that alignment may emerge naturally as a byproduct of model scaling. However, if alignment continues to emerge, there would be no need for any of the explicit alignment methods described above. That explicit alignment has consistently been helpful implies either emergent alignment is not sufficient or emergent alignment does not always lead to improved performance. This discrepancy between the possibility of emergent alignment and the need for explicit alignment methods calls for a systematic exploration of the role of alignment and its downstream relationship to performance in multimodal learning."}, {"title": "3. Research Questions and Experimental Setup", "content": "The recent line of work on the emergence of alignment across independently pre-trained unimodal models raises fundamental questions regarding the emergence of aligned representations and their implications on multimodal learning. Our research seeks to understand (1) when and why alignment emerges implicitly, and (2) whether alignment is a reliable indicator of performance. To reliably and comprehensively study these questions across all types of multimodal data, we use two principle dimensions to taxonomize multimodal data: interactions and heterogeneity (Baltru\u0161aitis et al., 2018; Liang et al., 2024; Tian et al., 2020). Interactions measure the information shared between two modalities for a task, from more redundant (e.g., images and corresponding captions) to more unique (e.g., sensor placement). We expect alignment to emerge more easily between redundant modalities. Heterogeneity measures the degree of similarity across two data modalities independent of the task, from more similar (e.g., two languages) to more different (e.g., text and video). We expect alignment to emerge more easily between similar modalities. Our experiments aim to study the emergence of alignment and its relationship to downstream task performance by systematically varying the interactions and heterogeneity in multimodal data. To summarize, our fundamental guiding questions are:\n1. Does alignment emerge when uniqueness and heterogeneity increase?\n2. Does higher alignment always predict better performance when uniqueness is present?\n3. How can we characterize datasets through the correlation between performance and alignment?\nBased on these questions, we define our problem setting.\n3.1. Problem Setting\nWe focus on a simplified setting with two modalities and an associated label, the generalization is straightforward. Concretely, we consider a scenario where we sample multimodal data and labels $X_1,X_2, y \\sim P(X_1, X_2, Y)$ from a data distribution $P(X_1, X_2, Y)$. $X_i$ represents the random variable for the i-th modality and Y for the task. Based on the relationships between $X_1, X_2$, and Y, these modalities can exhibit different degrees of interactions and heterogeneity.\nInteractions measure the information shared between two modalities for a task, from more redundant to more unique. Redundancy R represents the shared information between the two modalities and the task (Y), such as between images and captions that describe the image (Radford et al., 2021a). Uniqueness in modality 1 ($U_1$) quantifies the amount of information present in the first modality absent in the second but critical for the downstream task (and likewise for $U_2$). For example, feature selection is often optimized to provide new unique information and minimize redundancy to previous ones (Peng et al., 2005).\nTo investigate how alignment and performance change with respect to different interactions, we need synthetic controllable datasets and real-world multimodal benchmarks with different interactions. For constructing synthetic data, we assume that the task-relevant information (for a particular label y) can be decomposed into $x_r, x_{u_1}, x_{u_2}$, where $x_r$ denotes the common or redundant information, $x_{u_1}$ represents information unique to the first modality, and $x_{u_2}$ captures information unique to the second modality.\nWe construct the input data as $X_1 = [x_r, x_{u_1}]$ and $x_2 = [x_r, x_{u_2}]$. An overview of the data generation process is shown in Figure 3. By selecting specific features to compute the label, we control the levels of redundancy and uniqueness. Specifically, Y is a nonlinear function of a subset of features, $S\\subseteq [x_r, x_{u_1}, x_{u_2}]$. This enables us to control R as the number of features in S that come from $x_r$, and $U_i$ as the number of features that come from $x_{u_i}$ for $i\\in {1, 2}$. We denote the total uniqueness $U = |S|-R$. By keeping $|S|$ fixed while varying U, we generate datasets with different proportions of redundant versus non-redundant information.\nHeterogeneity. Different modalities often exhibit distinct structures, qualities, and representations (Liang et al., 2024). For example, when one modality is a time series and another is a static image, differences in their vocabulary tokens, and different noise or distribution shifts in each modality. We aim to investigate how alignment and performance change with different degrees of heterogeneity, from more similar (e.g., two languages) to more different (e.g., text and video).\nTo generate synthetic datasets with varying heterogeneity, we start with the case where both modalities are redundant, meaning Y (the labels) is a nonlinear function of $x_r$. Specifically, let $X_1 = x_r$ and $X_2 = \\phi(x_r)$, where $\\phi(\\cdot)$ is a nonlinear bijective function, as shown in Figure 3. In this setting, heterogeneity is defined as the number of nonlinear transformations involved in $\\phi(\\cdot)$. Concretely, if $\\phi(\\cdot)$ is modeled as a multilayer perceptron (MLP), the number of layers $D_{\\phi}$ quantifies the level of heterogeneity between the two modalities. We extend this definition to cases where the modalities contain unique information. Let $x_r$ represent the shared information between the two modalities, and $x_{u_1}$ and $x_{u_2}$ denote the unique information in each modality. In this scenario, $X_1 = [x_r, x_{u_1}]$, and a modality that is heterogeneous with respect to $X_1$ is defined as $X_2 = \\phi([x_r, x_{u_2}])$. Here, we assume $\\phi$ is a bijection, ensuring that the information content of $X_2$ and $[x_r, x_{u_2}]$ remains unchanged.\nExperimental setup for synthetic datasets. We evaluate how uniqueness, redundancy, and heterogeneity influence the emergence of alignment by training encoders independently on each modality and measuring the alignment between their learned representations. Specifically, we train a single-layer encoder on the first modality, denoted as $E_1$. For the second modality, which is transformed by the nonlinear function $\\phi(\\cdot)$ with varying depths ($D_{\\phi}$), we train a series of encoders denoted as $E_{2,D_{Enc}}$, where $D_{Enc}$ represents the depth of the encoder trained on the second modality and varies as $D_{Enc} \\in {1, ..., 10}$.\nExperimental setup for real benchmarks. In addition to experiments on synthetic data, we conduct analogous experiments on vision-language models. We use the same dataset and models as Huh et al. (2024), which evaluates alignment on the Wikipedia caption dataset (Srinivasan et al., 2021) with naturally co-occurring text and images. This dataset is inherently heterogeneous (text and images are different) with high redundancy due to overlapping semantic information. To vary the amount of unique information, we introduce a preprocessing procedure that perturbs the data in each modality. Specifically, for text, we randomly delete a percentage of characters, reducing the amount of information available in the textual representation. For image data, we replace a certain percentage of pixels with values drawn from a Gaussian distribution that matches the mean and variance of the original image.\nFinally, we experiment with MultiBench (Liang et al., 2021b) which collects a diverse range of real-world multimodal datasets: MOSEI (Bagher Zadeh et al., 2018), a"}, {"title": "4. RQ1: When does Alignment Emerge?", "content": "We empirically evaluate whether alignment emerges naturally by systematically varying redundancy, uniqueness, and heterogeneity. In the synthetic setting, the level of uniqueness U denotes the number of unique features used in computing the label. In Figure 4, we observe that as U increases, the maximum alignment decreases across different model depths and transformation depths. A similar trend is evident in Figure 5, which examines the alignment between large-scale language models and DINOv2 (Oquab et al., 2023) vision models over different levels of U is the percentage of perturbation. See Appendix D.2 for experiments with more vision models. An additional experiment, detailed in Appendix D.1, demonstrates that data heterogeneity is negatively correlated with the level of achievable alignment. Collectively, these experiments provide strong empirical evidence supporting the hypothesis that the level of alignment is indeed constrained by the degrees of heterogeneity and interactions between the modalities.\nWe now investigate whether increasing model capacity can improve alignment between representations of increasingly heterogeneous and unique modalities. In Figure 6, we plot alignment scores as a function of $(D_{Enc}, D_{\\phi})$, where $D_{Enc}$ represents the encoder depth and $D_{\\phi}$ represents the transformation depth of the second modality. When uniqueness is low, we observe that alignment improves significantly when the model capacity (relative to the transformation depth) is greater. This suggests that increased model capacity is effective in handling heterogeneity between modalities. Concretely, in these scenarios, alignment appears to follow the trend $(D_{Enc} - D_{\\phi}) \\propto Alignment$, meaning that the relative capacity of the encoder compensates for the complexity introduced by the transformation depth. However, as uniqueness increases, the relationship between alignment and relative model capacity becomes much weaker. In these cases, $(D_{Enc} - D_{\\phi})$ no longer predicts higher alignment scores. This indicates that when modalities have a high level of unique information, simply increasing model capacity is insufficient to achieve higher alignment. Instead, other factors such as the degree of shared information-may become the limiting factor in determining alignment.\nIn summary, while model size and capacity are correlated with alignment, there exists an upper limit to the level of achievable alignment, which is fundamentally determined by the intrinsic properties of the data. This finding implies that perfect alignment cannot be simultaneously achieved with optimal performance when the data modalities inherently differ in their information content. Moreover, increasing model depth only effectively aligns heterogeneous modalities when they contain highly redundant information and can fail for high uniqueness."}, {"title": "5. RQ2: Is Alignment Correlated with Performance?", "content": "In this section, we systematically investigate the relationship between alignment and performance, identifying scenarios where alignment enhances performance and others where it may introduce unintended trade-offs.\n5.1. Alignment-performance vs interactions/uniqueness\nFor each synthetic dataset, we analyze the relationship between alignment, performance, and model capacity. We include model capacity in our analysis as increased capacity generally leads to better performance and is assumed to correlate with better alignment. Our findings are summarized in Figure 7, where we plot the correlations between alignment and performance across different dataset dimensions, where U is defined in Section 4. In highly redundant settings, the correlation between alignment and performance is strong, with relatively little variation across different levels of heterogeneity and random seeds. However, as uniqueness increases, the median correlation decreases toward zero, and the range of correlations expands significantly. Notably, for U > 3, the correlation even becomes negative in some cases, suggesting that higher uniqueness can disrupt the relationship between alignment and performance. A similar trend is observed when examining the correlation between alignment and model depth in Figure 7 (right). As uniqueness increases, both the median correlation decreases and the variance in correlation increases substantially, with instances of anticorrelated alignment-depth relationships. While deeper models do not necessarily lead to better alignment when uniqueness is high, we see in Figure 7 (center) that performance and depth remain positively correlated across different levels of uniqueness, with much lower variance in correlation at higher uniqueness levels. This suggests that while alignment may not always be a reliable predictor of performance, increasing model capacity can still improve task performance.\nWe next verify whether these findings extend to large vision-language models. In Figure 8, we observe a strong correlation between alignment to DINOv2 and language model performance when uniqueness is low. However, this relationship weakens as uniqueness increases. By U = 25, models with lower alignment outperform those with higher alignment. As uniqueness increases further, the correlation between alignment and performance largely disappears. Additionally, we find that higher-performing language models are not necessarily those with greater capacity. This suggests that alignment and model size alone may not guarantee better performance in high-uniqueness settings. We include more analysis in Appendix D.3 involving different vision model training schemes. Overall, our experiments on both synthetic data and large vision-language models indicate that as uniqueness increases, higher-performing models with greater capacity do not necessarily exhibit stronger alignment. This reinforces the conclusion that alignment does not always predict model effectiveness, particularly when the modalities contain significant amounts of unique information.\n5.2. Alignment-performance vs heterogeneity\nAdditionally, we analyze whether alignment correlates with performance across varying levels of heterogeneity in Figure 9. Intuitively, we expect higher levels of heterogeneity to result in lower performance, but it is unclear whether this trend is reflected in alignment scores. Our findings show that while alignment and performance exhibit a strong linear relationship at low levels of uniqueness, this relationship weakens as uniqueness increases. Specifically, with higher uniqueness, models trained on similar modalities do not consistently achieve better alignment than those trained on heterogeneous modalities. This suggests that alignment does not uniformly degrade with increasing heterogeneity and that the interaction between uniqueness, heterogeneity, and alignment is more complex than a simple linear relationship. These results further reinforce the idea that alignment alone is not a sufficient predictor of model performance, especially in multimodal settings where modalities contain varying levels of interactions and heterogeneity."}, {"title": "6. RQ3: Alignment-Performance Correlation is an Inherent Property of Datasets", "content": "Finally, we investigate how the alignment-performance correlation varies across real-world multimodal datasets. Quantifying this relation is important to practitioners, as a positive alignment-performance correlation suggests that a practitioner can improve performance by explicitly aligning modalities. As shown in our experiments in Section 5, we expect that on tasks involving redundant information, alignment positively correlates with performance whereas for tasks that require unique information in modalities, the correlation may be weaker and not necessarily positive. We evaluate these hypotheses on a subset of datasets from MultiBench (Liang et al., 2021a) with varying degrees of task-relevant redundant and unique information content, including MOSEI (Bagher Zadeh et al., 2018), a dataset for predicting emotions from videos (vision, audio, text); MOSI (Zadeh et al., 2016), a dataset for predicting sentiment from videos (vision, audio, text), URFUNNY (Hasan et al., 2019), a humor detection dataset from videos (vision, audio, text); MUSTARD (Castro et al., 2019), a sarcasm detection dataset from TV shows (vision, audio, text); and AVMNIST (P\u00e9rez-R\u00faa et al., 2019), a dataset for digit classification from paired images and spoken digits (vision, audio). See Appendix C.2 for details about the datasets. For each modality, we train transformers with varying depths and compute the cross-modal alignment. See Appendix A for details on our experiment setup.\nWe show these results in Table 1. On sentiment analysis tasks that typically require unique information from language, alignment and performance are weakly correlated or even negatively correlated. For a given dataset, the alignment-performance relationship can even vary between different modalities. For example, on MUSTARD, alignment is more highly correlated with vision performance, whereas audio and text performance do not seem as correlated. On AVMNIST, alignment strongly correlates with performance for both modalities, as the information content is largely redundant information about the digit identity. These results corroborate our findings that the alignment-performance relationship heavily depends on dataset characteristics."}, {"title": "7. Conclusion", "content": "This paper provides a comprehensive analysis of the relationship between multimodal alignment, performance, and multimodal data characteristics. We offer a nuanced perspective on how alignment emerges across different modalities and how its effectiveness is influenced by the interactions and heterogeneity within the data. Specifically, our findings show that as uniqueness and heterogeneity increase, the emergence of alignment weakens, and that alignment often fails to track performance in datasets with higher uniqueness. In the case of perfect redundancy, our result supports the Platonic Representation Hypothesis, but as the amount of unique information and data heterogeneity increases, our results provide a generalization of this phenomenon.\nOur work opens up the possibility of characterizing and quantifying multimodal datasets via alignment-performance relationships. This can help advance our understanding of multimodal data and inspire the design of better methods that appropriately align (or perhaps even unalign) modality representations when necessary. Our work also inspires new theoretical questions regarding why different models sometimes converge to similar representations, even though they are often overparametrized and theoretically capable of learning arbitrary representations. Answering these questions can advance our understanding of today's large-scale multimodal AI systems."}, {"title": "Impact Statement", "content": "This paper presents an empirical analysis whose goal is to advance the field of multimodal representation learning. Multimodal models are broadly impactful for many real-world applications including understanding human verbal and nonverbal communication, fusing multiple physical sensors, and analyzing multiple sources of medical data. There are many potential long-term societal consequences of our work, but since our work is primarily an empirical analysis, we feel that none of these impacts must be specifically highlighted here."}, {"title": "A. Experimental Details", "content": "A.1. Synthetic Data Experiments\nOn the synthetic dataset, we train MLPs with the AdamW optimizer with the number of hidden dimensions kept the same as the number of input features, 12. For a given level of uniqueuness, we choose suitable hyperparameters across different model depths and transformation depths. Specifically, we tune the learning rate in the range {1e - 1, 1e - 2, 1e \u2013 3, 1e - 4} and weight decay in the range {0, 1e \u2013 1, 1e \u2013 2, 1e \u2013 3, 1e \u2013 4} for each modality. The depth 1 MLP for the untransformed modality were trained for 50 epochs and the models for the transformed modality were trained for 300 epochs. To ensure robustness, we report results with five different random seeds for each dataset.\nA.2. Vision-Language Alignment\nWe evaluate alignment using the same set of language and vision models as Huh et al. (2024). The language model families considered are BLOOM (Workshop et al., 2023), OpenLLaMA (Geng & Liu, 2023), and LLaMA (Touvron et al., 2023) downloaded from HuggingFace (Wolf et al., 2020). The vision models are vision transformer models of various sizes trained on various data and objectives. These include classification on ImageNet-21K (Russakovsky et al., 2015), MAE (He et al., 2022), DINOv2 (Oquab et al., 2023), CLIP (Radford et al., 2021b), and CLIP finetuned on ImageNet-12K. These models were downloaded from PyTorch Image Models (Wightman, 2019).\nA.3. MultiBench Experiments\nWe train transformers on the pre-extracted video, audio, and text features for the affective computing datasets and the audio modality of AVMNIST, and vision transformers on the AVMNIST digit images. For each modality, we vary the depth of the transformers in the range {1, ..., 10}. We use a single head for self-attention and set the embedding size to the input dimension. For classification tasks, we append a [cls] token to the sequence with a learnable embedding. The embedding of this token is used to compute alignment between layers; otherwise, we do average pooling over the input sequence. We use the AdamW optimizer. For each dataset, we choose suitable hyperparameters across different model depths and tune the learning rate in the range {1e \u2013 3, 5e \u2013 4, 1e \u2013 4, 5e \u2013 5, 1\u0435 \u2014 5} and and weight decay in the range {0, 11, 1e \u2013 2, 1\u0435 \u2013 3, 1\u0435 \u2013 4}.\nTo ensure robustness, we train each architecture across 3 different seeds, providing 30 alignment-performance data points. To get the alignment-performance correlation given modalities 1 and 2, the alignment of every modality 2 model is computed with respect to the modality 1 model with the highest validation score across the different seeds, and we report the correlation of these alignment scores with the performance of the modality 2 models."}, {"title": "B. Alignment Computation", "content": "Given mean-centered feature sets of n samples, $Z_1, Z_2 \\in \\mathbb{R}^{n\\times d}$, from two modalities X1 and X2, we first compute the covariances of these two different feature sets, and then compute the empirical estimator of the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005) using a linear kernel. Hence,\n$HSIC(Z_1Z_1^T, Z_2Z_2^T) = \\frac{1}{(n - 1)^2}Tr(Z_1Z_1^TZ_2Z_2^T) = \\frac{1}{(n - 1)^2}||Z_1^TZ_2||_F^2$ (1)\nThe Centered Kernel Alignment (CKA) (Kornblith et al., 2019) is then obtained by normalizing HSIC to ensure scale invariance and comparability across different feature sets:\n$CKA(Z_1, Z_2) = \\frac{HSIC(Z_1Z_1^T, Z_2Z_2^T)}{\\sqrt{HSIC(Z_1Z_1^T, Z_1Z_1^T)HSIC(Z_2Z_2^T, Z_2Z_2^T)}}$ (2)\nAs demonstrated in (Huh et al., 2024), the definition of alignment can be adjusted to limit the cross-covariance measurement to only those samples identified as nearest neighbors of the current sample i. This modification prioritizes similarity over dissimilarity, thereby emphasizing local alignment:"}, {"title": "C. Dataset Details", "content": "C.1. Synthetic Data\nWe discuss in detail how we construct a synthetic dataset with two modalities to analyze how uniqueness", "x_{u_1}": "and $X_2 = [x_r", "x_{u_2}": ".", "vector": "n$M_{R_i"}, "begin{cases}1 \\text{ if } 0 \\leq i < n_R,\\\\ 0 \\text{ otherwise}.\\end{cases}$\n$M_{U_{1_i}} =\\begin{cases}1 \\text{ if } 0 \\leq i < \\lfloor\\frac{n_v}{2}\\rfloor ,\\\\ 0 \\text{ otherwise}.\\end{cases}$\n$M_{U_{2_i}} =\\begin{cases}1 \\text{ if } 0 \\leq i < \\lfloor\\frac{n_v}{2}\\rfloor ,\\\\ 0 \\text{ otherwise}.\\end{cases}$\nWe define the task label y as a function $\\psi_y (\\cdot)$ of the masked components $x_r\\odot M_R, x_{u_1}\\odot M_{U_1}$, and $x_{u_2}\\odot M_{U_2}$, such that:\n$y = \\psi_y (x_r \\odot M_R, x_{u_1}\\odot M_{U_1}, x_{u_2}\\odot M_{U_2}),$ (9)\nwhere $x_r\\odot M_R$ captures the task-relevant redundant information, $x_{u_1}\\odot M_{U_1}$ captures the task-relevant unique information from modality 1 and $x_{u_2}\\odot M_{U_2}$ captures the task-relevant unique information from modality 2.\nHere, $\\odot$ denotes the element-wise (Hadamard) product. Intuitively, the task masks $M_R, M_{U_1}$, and $M_{U_2}$ are essential for controlling the relative contributions of the redundant ($x_r$) and unique ($x_{u_1}, x_{u_2}$) components to the label generation process.\nIn our synthetic experiments, we assume the joint distribution of the components as follows:\n$P(X_C, X_{U_1}, X_{U_2}) = P(X_C)P(X_{U_1})P(X_{U_2})$\nWhere, $P(X_C) = Uniform({0,1}^{n_R})$\n$P(X_{U_1}) = Uniform({0,1}^{n_U})$\n$P(X_{U_2}) = Uniform({0,1}^{n_v})$\nThis formulation assumes that the redundant information ($x_r$) and the unique components ($x_{u_1}, x_{u_2}$) are all independently distributed.\nThe task masks play a critical role in modulating which features are used to compute the labels, thereby allowing precise control over the relative importance of shared and unique information in the synthetic dataset. This design facilitates the study of how these components influence alignment and downstream task performance.\nIn our experiments, we fix $n_y$"]}