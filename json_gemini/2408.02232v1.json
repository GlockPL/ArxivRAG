{"title": "SpecRover: Code Intent Extraction via LLMs", "authors": ["Haifeng Ruan", "Yuntong Zhang", "Abhik Roychoudhury"], "abstract": "Abstract\u2014Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover. Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite. The production of explanation by SpecRover allows for a better \"signal\" to be given to the developer, on when the suggested patches can be accepted with confidence. SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.\nIndex Terms-Large Language Models, Autonomous Software Engineering, Program Repair, Specification Inference, Explainable AI in coding.", "sections": [{"title": "I. INTRODUCTION", "content": "Automatic programming has long been an aspiration of software engineering research. It has inspired research in topics like program synthesis and repair. In recent times, automatic programming from natural language specifications has become somewhat more realistic due to the emergence of tools like GitHub Copilot. At the same time, the automatically generated code from Large Language Models (LLMs) suffers from errors and vulnerabilities [1], [2] and needs to be improved. For this reason, there has been a recent research focus on autonomous program improvement. The problem setting for autonomous program improvement involves solving of GitHub issues which would typically involve bug fixes or feature additions. Though these tools are employed on manually written software projects such as the recently proposed SWE-bench [3], they hold the promise of high quality trustworthy code construction from LLMs. Starting with the AI software engineer Devin [4] from a stealth startup, recently several autonomous program improvement tools such as AutoCodeRover [5] have been proposed for automatically solving GitHub issues (such as bug fixes or feature additions). By combining these technologies with code generation via GitHub Copilot, one can envision trustworthy code construction from LLMs.\nProgram improvement or program repair, typically requires capturing developer intent to guide the process. However, there is no formal specification of developer intent. The natural-language description of the developer intent is usually only available at a \"higher level\" - it captures the intended behavior of the entire software system. However to improve or repair specific components of a software system (where the error might have been localized) one needs to infer specifications of the different components. A successful approach to program repair may thus involve specification inference - where by carefully analyzing the artifacts of the buggy program (such as program executions), we can infer snippets of the intended program behavior. The works on semantic program repair [6], [7] extract specifications via symbolic analysis of the given tests. Indeed, the existing literature on program repair [8] uses a given test-suite as developer intent, and hence is focused on avoiding test-data over-fitting. The works on semantic repair alleviate the over-fitting concern by inferring symbolic specifications from tests. Nevertheless, for the general problem of program improvement, the buggy program may or may not be accompanied by tests. Moreover, symbolic analysis based program repair has a high entry barrier for developers. For these reasons, recently autonomous program improvement using Large Language Models (LLMs) [5], [9], [10] has been studied.\nIn this work, we explore the role of program specifications thoroughly in LLM-guided autonomous software engineering workflows. To understand the intent of the developer and perform program improvement based on inferred specifications, we build our work on the publicly available AutoCodeRover [5] tool. The reason for this choice is strategic. In essence, AutoCodeRover takes the position that the structure of the program also captures a coarse encoding of the developer intent, and it tries to glean intent by analyzing (and searching over) the program structure; it performs code search on the project structure for fix localization. Thus, to build a workflow where we conduct high quality program improvement via iterative specification inference, we choose to build our work on AutoCodeRover. Our work looks into various sources of specifications such as function-level code summaries and testcases, apart from program structure. The core contribution thus lies in distilling the various specifications coming from"}, {"title": "II. MOTIVATING EXAMPLE", "content": "We now present the SpecRover approach via an example. The GitHub issue involved in this example is scikit-learn-155351, shown in Figure 1a. In the issue, two relevant code snippets are provided. According to the issue report, both snippets had worked without problem on an older version of scikit-learn, and it is expected that they continue to work on the current version. However, on the current version, the first snippet now crashes. The associated error information indicates that the crash occurred when scikit-learn mistakenly tries to convert a non-numeric array element into a float."}, {"title": "III. METHODOLOGY", "content": "Problem setup: Given a software codebase $C$ and a natural language problem description $D$, the goal is to automatically derive a patch $p$ (i.e. a set of code modifications) to $C$, such that the patched codebase $C'$ satisfies the requirements in $D$. One example setup for $D$ is GitHub issues, in which the issue description contains requirements for fixing a bug or adding a new feature.\nIn this paper, we drive autonomous program improvement with the help of program specifications. We try to acquire an understanding of the intended program behavior (the specification), which then allows us to produce high-quality patches that successfully resolve GitHub issues. Beyond producing high-quality patches, an additional benefit of understanding the specification is that it also serves as evidence as to why the patch is correct. The evidence holds promise in terms of easing software maintenance and engendering trust in the code. The key novelty of our approach lies in how we infer and utilize various forms of specifications. For an overview of all the specifications involved, we depict the general workflow of our approach SpecRover in Figure 2. In this figure, the inferred specifications are highlighted in yellow. We also highlight in blue all the LLM agents present in the workflow. As shown in Figure 2, the specifications are inferred in an iterative fashion: the agents take in specifications, possibly produced by other agents, and in turn infer new forms of specifications. This iterative process generates a variety of specifications, until a patch is generated and deemed correct by one of our agents that vets generated patches.\nSpecifically, as shown in Figure 2, the following specifications are inferred in sequence in SpecRover, which is given as input an issue statement and a software codebase.\n1) The input issue statement is passed to a reproducer agent, which writes a reproducer test that reproduces the program fault reported in the issue.\n2) The reproducer test, its execution results, along with the issue statement and the codebase, are passed to a context retrieval agent. The context retrieval agent explores the program codebase and identifies the relevant code to the issue. It eventually decides on a set of buggy locations that need patching.\n3) The context retrieval agent also produces a function summary for every function encountered while exploring the program code. A function summary describes the intended behavior of a function in natural language, with respect to the current issue being solved.\n4) The buggy locations, together with their corresponding function summaries, are passed to a patching agent, which tries to write a patch to resolve the issue.\n5) The patch and the reproducer test are passed to a reviewer agent for scrutiny. The reviewer agent will produce a reviewer feedback if the patch is deemed incorrect; the patching agent will take in the reviewer feedback and try writing another patch. The reviewer feedback is a natural-language explanation of why the patch is incorrect and how it can be rectified. Likewise, a reviewer feedback for the reproducer test will be produced at the same time if the test is deemed incorrect.\n6) If a patch is deemed correct by the reviewer agent, and there is an existing regression test suite available for the program, the patch will be checked via the regression test suite. If there is no regression, the patch will be accepted as the final patch. Otherwise, if some of the regression tests fail, we will retry the workflow up to a predefined number of times.\n7) Finally, after multiple retries, there can be multiple patch candidates. A selection agent is invoked to select one final patch among the patch candidates, and give the reason why this patch is selected. The final patch, the reason for selection, and optionally the rest of the candidate patches will be sent to the user.\nAmong the specifications, the 3) function summary and 5) reviewer feedback are unique to SpecRover and unexplored by other LLM agents. These specifications have boosted the effectiveness of SpecRover in resolving software issues, because they fully exploit different kinds of software artifacts: the function summary exploits the program code behavior, and the reviewer feedback exploits both the code and the test."}, {"title": "B. Function Summary: Specification from Program", "content": "In this section, we first describe how the context retrieval agent gathers code context for the software issue to be resolved. We then discuss how SpecRover transforms the user intent in the issue description into program specifications for shorter code elements such as functions.\nExisting LLM programming agents typically employ a context retrieval step to collect necessary code context related to the given issue from a large codebase. SpecRover follows the general architectural design of programming agents in its context retrieval stage, as shown in Figure 3. SpecRover conducts context retrieval by providing a set of APIs to the LLM for exploring the codebase. The LLM agent invokes the retrieval APIs to investigate the relevant code snippets in the program. The retrieved code forms the code context for the current to-be-resolved issue, which can contain definitions of the relevant classes and methods. After each round of retrieval API invocations, the LLM agent takes the code context collected so far and decides whether the context is sufficient for understanding and resolving the problem. If the context is deemed sufficient, the retrieval process will end, and the agent will decide on a set of buggy locations, which are sent to the patching agent for repairing. Otherwise, the retrieval process continues until a predefined threshold count is reached.\nOne key novelty in SpecRover is the explicit extraction of function summaries while collecting code snippets during context retrieval. In SpecRover, whenever a new code snippet is retrieved with an API and sent to the context retrieval agent, we explicitly prompt the agent to analyze the \"intended behavior\" of this code snippet in the current problem context.\nThe intended behavior (or specification) is a concise natural-language summary of how a function should behave to meet the requirements specified in the high-level problem description. This function-level summary of intended behavior serves as a local specification to guide the patch construction. The system-level intended behavior specification given by the user (i.e. the issue description) is often on how the program should behave rather than how a unit function should behave. So we usually do not have the intended behavior of a function. Although the issue description may provide some \"direction\" on the intended behavior of a function it is usually not sufficient to guide the patching agent. On the other hand, the extracted function-level specification (capturing the intended behavior of the function) serves as a more direct guide to the patching agent. Instead of giving a set of bug locations ${L_1, L_2, ..., L_n}$ to the patching agent to modify, SpecRover gives the pairs of bug locations and their corresponding local specification ${(L_1, Spec_1), (L_2, Spec_2), ..., (L_n, Spec_n)}$. The patching agent can then refer to the specifications of intended behavior and modify code at the function level (so as to achieve this intended behavior). Intuitively, our approach decomposes the repository-level issue solving task to several function-level code modification tasks, in which each function-level task has a natural language specification. LLMs have been extensively studied for function-level coding tasks and have shown promising results in function-level benchmarks such as HumanEval [11], [12] and MBPP [13]. Therefore, this task decomposition helps the patching agent of SpecRover which then has to solve smaller and more manageable tasks."}, {"title": "C. Reviewer Feedback: Reconciling Specifications", "content": "Another kind of specification inferred by SpecRover is the reviewer feedback. To be more precise, the reviewer feedback can be called a meta-specification: it is a reflection on the specifications inferred in previous steps. Concretely, given a patch and a reproducer test, the reviewer agent in SpecRover will provide feedback, which includes 1) a binary decision of whether the patch and the reproducer test are correct respectively; and 2) an explanation for the decisions."}, {"title": "D. Patch Selection", "content": "As shown in Figure 2, a patch approved by the reviewer agent is checked through a regression test suite, which serves as an oracle for whether the patch breaks existing functionality of the program. However, in the setting of resolving GitHub issues, the regression test suite can be an inaccurate oracle, meaning that they can reject correct patches which resolve the issue. This is because the correct patch may inevitably modify existing functionalities of the program while resolving the issue, thus causing some of the existing regression tests to fail. For example, if the patch needs to modify the signature of an existing function $f$ in order to resolve an issue, regression tests that invoke $f$ will now fail. Since the correct patch can be rejected by the regression tests, we employ a patch selection process at the end of the workflow to select the most promising patches among the rejected candidate patches.\nDuring the final patch selection phase, SpecRover goes beyond the test cases and employs a selection agent to choose a patch based on the natural language issue description. All candidate patches that failed some tests are presented to the selection agent, together with the issue description. The selection agent is instructed to analyze the root cause of the issue, think about how the issue can be possibly resolved, and select a patch that best addresses the issue. This natural language-guided patch selection can recover correct patches that are mistakenly filtered out by an inaccurate test suite. It exploits the natural language issue report as that captures the most up-to-date intents from users/developers. While making a choice among the candidate patches, the selection agent also explicitly states a reason why it chooses a particular patch among the candidate patches. This \u201creason for selection\" can be given as evidence together with the final patch."}, {"title": "E. Evidence", "content": "SpecRover is designed to not only generate a patch to resolve the issues in software repositories, but also to provide the inferred specifications as evidence for why a patch was selected. Specifically, along with the final patch, the following artifacts can be the outputs of SpecRover as well:\n\u2022\n\u2022\n\u2022\n\u2022 Buggy locations and their intended behaviors.\nThe reproducer test written by the reproducer agent.\nThe reason why the final patch was approved by the reviewer agent (if the patch is approved by the reviewer and the regression test suite).\nThe reason why the final patch is selected by the selection agent (if there are multiple candidate patches which cannot be differentiated by the tests)."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We address the following research questions:\nRQ1: What is the efficacy of SpecRover in resolving issues?\nRQ2: What level of confidence can developers get from the patch and specifications produced by SpecRover?\nRQ3: What is the quality of the specification produced by SpecRover as evidence?\na) Benchmark: We evaluate the efficacy of SpecRover on SWE-bench [3], a widely-used benchmark for autonomous program improvement consisting of 2294 real-world GitHub issues. For each issue, the only input for SpecRover is the issue statement and the buggy codebase. Note that the regression test suite used by SpecRover is part of the buggy program; we do not access any code or test that is added by the developer in the fixed version of program.\nb) Baselines and Evaluation Metrics: For RQ1, we compare with the state-of-the-art systems that target the repository-level issue solving task. In our comparison, we include all the open-source software engineering agents which have reported results on SWE-Bench. The baseline tools include:\n\u2022\nAutoCodeRover [5]. AutoCodeRover employs a set of program structure-aware APIs to gather relevant code context. It optionally integrates debugging techniques such as Spectrum-based Fault Localization to sharpen the context.\n\u2022\nSWE-agent [9]. SWE-agent designs an agent-computer interface, which defines the possible actions taken by an agent to edit code, navigate the codebase, and execute tests.\n\u2022\n\u2022 AppMap Navie [14]. Navie uses a retrieval-augmented generation (RAG) based approach to construct the code context, and performs an explicit planning step before generating code changes [15].\n\u2022\n\u2022 OpenDevin [16]. OpenDevin's CodeActAgent tackles the tasks by having a general action space, where the agent is allowed to execute arbitrary Python and bash commands inside a sandbox environment.\nAider [17]. Aider constructs a repository map which helps the LLM to understand the repository context. It also uses the regression test suite as a harness to retry the task.\nMoatless Tools [18]. Moatless Tools builds an agentic loop that functions as a finite state machine and transitions between states. It focuses on building a set of good tools for the agent instead of relying on the agent for reasoning.\nWe report pass@1 efficacy on SWE-Bench for all tools. For each issue, SWE-Bench has a set of acceptance tests written by the developers to evaluate the patch correctness. These acceptance tests are not used by the tools when generating patches. We follow the official SWE-Bench evaluation criteria - if the single patch generated by a tool passes the SWE-Bench acceptance tests for the issue, the issue is considered as resolved.\nc) Implementation and Parameters: We implemented SpecRover on top of the AutoCodeRover codebase and reuse its context retrieval APIs. We implemented new features unique to SpecRover such as function summary extraction as part of the context retrieval process. Other unique features such as patch reviewing and selection are implemented as new LLM agents. SpecRover supports multiple LLMs as backend. In our experiments, we used the Claude 3.5 Sonnet as the main foundation model, and only switch to OpenAI GPT-40 for a task if that task encounters an API error when invoking the Claude remote APIs. We set maximum retries on regression test suite failures to be 3.\nd) Randomness of LLMs: LLMs are inherently random in its output generation, which may threaten the validity of LLM-based coding agents including SpecRover. We address this by setting the model temperature to 0, so that the model output is more deterministic."}, {"title": "V. EVALUATION", "content": "We first evaluate the overall efficacy of SpecRover in resolving repository-level tasks. We report the efficacy of SpecRover on both SWE-Bench (consisting of 2294 real-world GitHub issues), and SWE-Bench lite (which is a subset of SWE-Bench consisting of 300 issues). For the baseline tools, we compare with their corresponding reported efficacy. If a tool supports different configurations (e.g. different LLMs as the backend), we compare with the configuration with the highest efficacy."}, {"title": "B. RQ2: Utility of autonomous SE, confidence in results", "content": "Although the efficacy in resolving issues is an important aspect of autonomous program improvement, it is not the sole purpose of such a technique. Rather, the efficacy is a means to an end to reduce human effort in software maintenance. To this end, a program improvement technique must not only have high efficacy, but also minimize the effort required of an end user to use the technique. The effort is related to two metrics: 1) signal-to-noise ratio, i.e., the ratio of correct to incorrect patches presented to a user; and 2) the difficulty of examining each auto-generated patch that is suggested.\nWe have designed SpecRover to reduce both of these efforts. First, to reduce the number of incorrect patches that a user may examine, we use the reviewer agent to decide the correctness of the generated patch and the reproducer test. The user can choose to examine the generated patch only when both the patch and the reproducer test are deemed correct by the reviewer agent. The accuracy of the reviewer decisions are measured in RQ2. Second, to make it easy for a user to examine each patch, SpecRover provides a variety of evidence to help understand the patch, as discussed in Section III-E. The quality of the evidence will be discussed in RQ3.\nThere can be four different scenarios when the reviewer decision is viewed in relation to the actual correctness of the patch. For convenience, we say a patch is accepted when the reviewer agent decides that both the generated patch and the reproducer test are correct. With this, we discuss the following four scenarios:\n\u2022\n\u2022\n\u2022\n\u2022 TP (true positive): Patch is accepted and correct;\nTN (true negative): Patch is not accepted and incorrect;\nFP (false positive): Patch is accepted but incorrect;\nFN (false negative): Patch is not accepted but correct."}, {"title": "C. RQ3: Quality of Specifications produced", "content": "In this section, we illustrate the quality of evidence generated by SpecRover with two examples. The high-quality evidence allows a developer to easily integrate auto-generated patch into an existing codebase.\na) Reviewer Feedback as Summary: In the first example, we show that the reviewer feedback can serve as a concise summary of a generated patch. The summary describes the behavior of the patch at a high level. Therefore, a developer can understand the generated patch faster by reading the summary before examining the details of the patch. Besides, after the developer accepts the patch and decides to merge it into existing code, the summary constitutes a good commit message, so that the developer does not need to write one. From a developer's perspective, the whole process is very much like reviewing a pull request, which is already part of a developer's day-to-day workflow. The specific issue involved in this example is pytest-74322. The issue statement is shown in Figure 7a, which reports that pytest (a python testing framework) would miscalculate a line number in its output when an irrelevant option (runxfail) is enabled. The bug is caused by the code shown in Figure 7b. As can be seen, the calculation of the line number is wrongly placed in a branch that is mutaully exclusive with the runxfail branch. Therefore, the calculation is wrongly skipped when the option is enabled.\nTo resolve the issue, SpecRover was able to locate the relevant code and produce the correct patch. An abridged version of the patch is shown in Figure 7c. It correctly addresses the issue by moving the line number calculation to a branch unaffected by the runxfail option. However, the patch might not be immediately understandable to a developer, because it changes as many as 51 lines in the original program (though most of the changes just involve the indentation level). Fortunately, the understanding of the patch can be eased by the reviewer feedback. The reviewer agent was able to identify the patch as correct and produced the feedback in Figure 7d. It properly summarized that the patch just moved the calculation to another branch. Using this summary, the developer would easily understand the patch and accept it.\nb) Tolerance of Incorrect Tests: Another advantage of the reviewer is enhanced tolerance of incorrect automatically generated tests. We illustrate this advantage with the example of SymPy-216143, where SpecRover rejects an incorrectly written test while approving a correct patch. The issue statement and the buggy location identified by SpecRover are shown in Figure 8a and 8b. The issue mentioned an unexpected behavior of the kind attribute. After its context retrieval stage, SpecRover correctly identifies that the buggy location is in the Derivative class, and that its intended behavior is to have an additional kind property. Figure 8c and 8d show the automatically generated reproducer test and patch for this issue. In this case, the reproducer test is incorrect the assertions compare an object with a string, which always evaluate to False. If this reproducer test is used solely to determine the correctness of the generated patches, any patch, even a correct one, will be rejected. However, since the reviewer agent in SpecRover simultaneously examines both the reproducer test and the patch without assuming either is correct, it is capable of rejecting the reproducer test while approving the patch. Figure 8e shows the reviewer agent's decision and comments towards the test and patch. The reviewer identifies that the assertions in the reproducer are written incorrectly, thereby rejecting the reproducer. On the other hand, the reviewer correctly approves the patch despite the presence of an unreliable test. The correct patch, along with the reasons for rejecting the reproducer, can be sent to the developer. The developer can then integrate the patch into the codebase. Additionally, the developer can revise the \u201calmost correct\" reproducer test based on the feedback provided by the reviewer agent.\""}, {"title": "VI. CASE STUDY: SECURITY VULNERABILITY REPAIR", "content": "Although SpecRover is initially designed to resolve GitHub issues in Python repositories, it can be easily adapted for program improvements in other application domains, and for programs written in other programming language. We demonstrate how SpecRover fixes security vulnerabilities in C programs, through an example challenge problem from the DARPA AI Cyber Challenge (AIxCC) in 2024 [20]. The AIxCC is a two-year competition organized by DARPA and ARPA-H to encourage the development of novel cyber-reasoning systems to safeguard critical software. The AIxCC has publicly released exemplar challenges, where each challenge consists of a software project with a vulnerability in it. The task is to have an autonomous system to find and fix the vulnerability. Each exemplar challenge also contains a Proof-of-Concept (PoC) input file that triggers the vulnerability, so we use this PoC to show how SpecRover can be used to fix the vulnerability after it is detected. Figure 9 shows one exemplar challenge, which is a buffer overflow vulnerability in the Linux kernel4. This buffer overflow happens in the Linux networking module for the Transparent Inter-Process Communication (TIPC) protocol, and allows remote attackers to cause denial-of-service or disclosure of sensitive information. Specifically, when the user-supplied sizes in the message body are invalid for the received messages, a buffer overflow happens with the memcpy call, as shown in Figure 9a. This vulnerability has been triggered by a PoC, which results in a vulnerability report as shown in Figure 9b.\nSpecRover fixes this vulnerability by first analyzing the vulnerability report, similar to how it resolves GitHub issues by initially examining the issue descriptions. It conducts context retrieval, and decides on the buggy locations and intended behaviors as shown in Figure 9c. Even though the vulnerability report only contains the call trace and minimal description of the bug (e.g., \u201cslab-out-of-bounds\u201d), SpecRover can infer the intended local behavior at the function level. Based on the intended behavior, SpecRover generated the patch in Figure 9d, which correctly fixes the vulnerability inserting additional checks before the dangerous memory operation. The reviewer agent approved the patch with the comments shown in Figure 9e, with which the developers can gain an initial understanding of the patch before closely examining the changed code."}, {"title": "VII. RELATED WORK", "content": "Automated program repair (APR) [8], [21] is a well studied research area in software engineering. Given a buggy program $P$, and a test-suite $T$, automated program repair attempts to (minimally) modify $P$ to a program $P'$ which passes the given test-suite $T$. APR techniques involve metaheuristic search [22], semantic analysis [6], machine learning [23], or a combination of different techniques. APR can also be used to rectify automatically generated code from LLMs, e.g. see the work of [1] among others.\nThe recent interest in prompt engineering as well as agent based solutions has somewhat evolved the research in program repair. LLM agents try to combine the power of LLM with program analysis and test execution reasoning. Thus LLM agents can combine LLMs with test generation, static and dynamic analysis as well as specification inference. In the recent past, lot of LLM based approaches have been proposed for solving software \"issues\" described in natural language, including [4], [5], [9], [10]. Among these our work is thematically closest to the work of AutoCodeRover [5]. Like AutoCodeRover. we take the position that program modifications like bug fixing are best aided by inference of the developer intent. AutoCodeRover infers the developer intent only from the software project structure. In contrast, SpecRover is more general and is capable of inferring specifications from different sources including program structure, program behavior, tests and so on. Furthermore, SpecRover focuses on giving an explanation of the produced patches."}, {"title": "VIII. PERSPECTIVES", "content": "Owing to the growth of large language based automatic programming (see [24] for a recent summary), there exists interest in autonomous program improvement technologies. We propose SpecRover with the perspective of autonomously producing patches which are suggested with confidence (thus developers can confidently accept them) and come with explanations. The technical innovations supporting SpecRover are the specification inference to guide patching, and the rigorous vetting of patches via our reviewer agent. Our work on SpecRover seeks to put the matter of quality of patches produced by LLM agents into the research community's attention, whereas other works are mostly focusing on the agent efficacy."}, {"title": "APPENDIX", "content": "An issue that can bias the evaluation of LLM agent-generated patches is data memorization. Data memorization occurs when an LLM deals with a program that exists in its training set. This issue is rather common, since LLMs have been trained on a vast amount of publicly available code. In the presence of possible data memorization, when a correct patch is generated by an LLM agent, it may be not because of the capability of the LLM agent, but because the LLM has seen the repaired version of the program. In this way, the capability of the LLM agent is overestimated.\nOne way of gauging the threat of data memorization is to check whether the generated patch is syntactically equivalent to the ground-truth correct patch. Intuitively, the LLM is likely to generate a syntactically equivalent patch if it has seen the correct program. Therefore, the threat of data memorization is lower if the generated patch turns out not syntactically equivalent.\nTo gauge the data memorization threat and have a more accurate evaluation of the LLM agents, we perform this syntactic check of patches generated by agent systems on SWE-Bench lite. For each issue resolved by a tool, we compare the patch generated by the tool against the ground-truth developer-written patch, and check whether they are syntactically equivalent. The results are shown in Table III. The column \"# Resolved\" shows the number of resolved issues by a tool; \u201c# Syntactically Equiv.\" shows the number of resolved issues with the generated patch being syntactically equivalent to the ground-truth patch; \"% Equiv. / Resolved\u201d shows the ratio between these two numbers."}]}