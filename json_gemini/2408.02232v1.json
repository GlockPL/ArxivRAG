{"title": "SpecRover: Code Intent Extraction via LLMs", "authors": ["Haifeng Ruan", "Yuntong Zhang", "Abhik Roychoudhury"], "abstract": "Abstract\u2014Autonomous program improvement typically in-\nvolves automatically producing bug fixes and feature additions.\nSuch program improvement can be accomplished by a combi-\nnation of large language model (LLM) and program analysis\ncapabilities, in the form of an LLM agent. Since program repair\nor program improvement typically requires a specification of\nintended behavior - specification inference can be useful for pro-\nducing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification infer-\nence within an LLM agent. Given a GitHub issue to be resolved\nin a software project, our goal is to conduct iterative code search\naccompanied by specification inference - thereby inferring intent\nfrom both the project structure and behavior. The intent thus\ncaptured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the\nvetted patches. Our approach SpecRover (AutoCodeRover-v2)\nis built on the open-source LLM agent AutoCodeRover. In an\nevaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over\nAutoCodeRover. Compared to the open-source agents available,\nour work shows modest cost ($0.65 per issue) in resolving an\naverage GitHub issue in SWE-Bench lite. The production of\nexplanation by SpecRover allows for a better \"signal\" to be\ngiven to the developer, on when the suggested patches can be\naccepted with confidence. SpecRover also seeks to demonstrate\nthe continued importance of specification inference in automated\nprogram repair, even as program repair technologies enter the\nLLM era.\nIndex Terms-Large Language Models, Autonomous Software\nEngineering, Program Repair, Specification Inference, Explain-\nable AI in coding.", "sections": [{"title": "I. INTRODUCTION", "content": "Automatic programming has long been an aspiration of soft-\nware engineering research. It has inspired research in topics\nlike program synthesis and repair. In recent times, automatic\nprogramming from natural language specifications has become\nsomewhat more realistic due to the emergence of tools like\nGitHub Copilot. At the same time, the automatically generated\ncode from Large Language Models (LLMs) suffers from errors\nand vulnerabilities [1], [2] and needs to be improved. For this\nreason, there has been a recent research focus on autonomous\nprogram improvement. The problem setting for autonomous\nprogram improvement involves solving of GitHub issues\nwhich would typically involve bug fixes or feature additions.\nThough these tools are employed on manually written software\nprojects such as the recently proposed SWE-bench [3], they\nhold the promise of high quality trustworthy code construction\nfrom LLMs. Starting with the AI software engineer Devin [4]\nfrom a stealth startup, recently several autonomous program\nimprovement tools such as AutoCodeRover [5] have been\nproposed for automatically solving GitHub issues (such as bug\nfixes or feature additions). By combining these technologies\nwith code generation via GitHub Copilot, one can envision\ntrustworthy code construction from LLMs.\nProgram improvement or program repair, typically requires\ncapturing developer intent to guide the process. However, there\nis no formal specification of developer intent. The natural-\nlanguage description of the developer intent is usually only\navailable at a \"higher level\" - it captures the intended behavior\nof the entire software system. However to improve or repair\nspecific components of a software system (where the error\nmight have been localized) one needs to infer specifications\nof the different components. A successful approach to program\nrepair may thus involve specification inference - where by\ncarefully analyzing the artifacts of the buggy program (such\nas program executions), we can infer snippets of the intended\nprogram behavior. The works on semantic program repair [6],\n[7] extract specifications via symbolic analysis of the given\ntests. Indeed, the existing literature on program repair [8] uses\na given test-suite as developer intent, and hence is focused\non avoiding test-data over-fitting. The works on semantic\nrepair alleviate the over-fitting concern by inferring symbolic\nspecifications from tests. Nevertheless, for the general problem\nof program improvement, the buggy program may or may\nnot be accompanied by tests. Moreover, symbolic analysis\nbased program repair has a high entry barrier for developers.\nFor these reasons, recently autonomous program improvement\nusing Large Language Models (LLMs) [5], [9], [10] has been\nstudied.\nIn this work, we explore the role of program specifications\nthoroughly in LLM-guided autonomous software engineering\nworkflows. To understand the intent of the developer and per-\nform program improvement based on inferred specifications,\nwe build our work on the publicly available AutoCodeRover\n[5] tool. The reason for this choice is strategic. In essence,\nAutoCodeRover takes the position that the structure of the\nprogram also captures a coarse encoding of the developer\nintent, and it tries to glean intent by analyzing (and searching\nover) the program structure; it performs code search on the\nproject structure for fix localization. Thus, to build a workflow\nwhere we conduct high quality program improvement via\niterative specification inference, we choose to build our work\non AutoCodeRover. Our work looks into various sources\nof specifications such as function-level code summaries and\ntestcases, apart from program structure. The core contribution\nthus lies in distilling the various specifications coming from"}, {"title": "II. MOTIVATING EXAMPLE", "content": "We now present the SpecRover approach via an example.\nThe GitHub issue involved in this example is scikit-learn-\n155351, shown in Figure 1a. In the issue, two relevant code\nsnippets are provided. According to the issue report, both\nsnippets had worked without problem on an older version\nof scikit-learn, and it is expected that they continue to work\non the current version. However, on the current version, the\nfirst snippet now crashes. The associated error information\nindicates that the crash occurred when scikit-learn mistakenly\ntries to convert a non-numeric array element into a float."}, {"title": "III. METHODOLOGY", "content": "Problem setup: Given a software codebase C and a\nnatural language problem description D, the goal is to auto-\nmatically derive a patch p (i.e. a set of code modifications) to\nC, such that the patched codebase C' satisfies the requirements\nin D. One example setup for D is GitHub issues, in which\nthe issue description contains requirements for fixing a bug or\nadding a new feature.\nIn this paper, we drive autonomous program improvement\nwith the help of program specifications. We try to acquire an\nunderstanding of the intended program behavior (the specifi-\ncation), which then allows us to produce high-quality patches\nthat successfully resolve GitHub issues. Beyond producing\nhigh-quality patches, an additional benefit of understanding\nthe specification is that it also serves as evidence as to why the\npatch is correct. The evidence holds promise in terms of easing\nsoftware maintenance and engendering trust in the code. The\nkey novelty of our approach lies in how we infer and utilize\nvarious forms of specifications. For an overview of all the\nspecifications involved, we depict the general workflow of our\napproach SpecRover in Figure 2. In this figure, the inferred\nspecifications are highlighted in yellow. We also highlight in\nblue all the LLM agents present in the workflow. As shown in\nFigure 2, the specifications are inferred in an iterative fashion:\nthe agents take in specifications, possibly produced by other\nagents, and in turn infer new forms of specifications. This\niterative process generates a variety of specifications, until a\npatch is generated and deemed correct by one of our agents\nthat vets generated patches.\nSpecifically, as shown in Figure 2, the following specifica-\ntions are inferred in sequence in SpecRover, which is given\nas input an issue statement and a software codebase."}, {"title": "A. Overview", "content": "1) The input issue statement is passed to a reproducer\nagent, which writes a reproducer test that reproduces the\nprogram fault reported in the issue.\n2) The reproducer test, its execution results, along with the\nissue statement and the codebase, are passed to a context\nretrieval agent. The context retrieval agent explores the\nprogram codebase and identifies the relevant code to the\nissue. It eventually decides on a set of buggy locations\nthat need patching.\n3) The context retrieval agent also produces a function\nsummary for every function encountered while exploring\nthe program code. A function summary describes the\nintended behavior of a function in natural language, with\nrespect to the current issue being solved.\n4) The buggy locations, together with their corresponding\nfunction summaries, are passed to a patching agent, which\ntries to write a patch to resolve the issue.\n5) The patch and the reproducer test are passed to a reviewer\nagent for scrutiny. The reviewer agent will produce a\nreviewer feedback if the patch is deemed incorrect; the\npatching agent will take in the reviewer feedback and try\nwriting another patch. The reviewer feedback is a natural-\nlanguage explanation of why the patch is incorrect and\nhow it can be rectified. Likewise, a reviewer feedback for\nthe reproducer test will be produced at the same time if\nthe test is deemed incorrect.\n6) If a patch is deemed correct by the reviewer agent, and\nthere is an existing regression test suite available for the\nprogram, the patch will be checked via the regression test\nsuite. If there is no regression, the patch will be accepted\nas the final patch. Otherwise, if some of the regression\ntests fail, we will retry the workflow up to a predefined\nnumber of times.\n7) Finally, after multiple retries, there can be multiple patch\ncandidates. A selection agent is invoked to select one final\npatch among the patch candidates, and give the reason\nwhy this patch is selected. The final patch, the reason for\nselection, and optionally the rest of the candidate patches\nwill be sent to the user.\nAmong the specifications, the 3) function summary and 5)\nreviewer feedback are unique to SpecRover and unexplored\nby other LLM agents. These specifications have boosted\nthe effectiveness of SpecRover in resolving software issues,\nbecause they fully exploit different kinds of software artifacts:\nthe function summary exploits the program code behavior, and\nthe reviewer feedback exploits both the code and the test."}, {"title": "B. Function Summary: Specification from Program", "content": "In this section, we first describe how the context retrieval\nagent gathers code context for the software issue to be re-\nsolved. We then discuss how SpecRover transforms the user\nintent in the issue description into program specifications for\nshorter code elements such as functions.\nExisting LLM programming agents typically employ a\ncontext retrieval step to collect necessary code context related\nto the given issue from a large codebase. SpecRover follows\nthe general architectural design of programming agents in\nits context retrieval stage, as shown in Figure 3. SpecRover\nconducts context retrieval by providing a set of APIs to the\nLLM for exploring the codebase. The LLM agent invokes\nthe retrieval APIs to investigate the relevant code snippets in\nthe program. The retrieved code forms the code context for\nthe current to-be-resolved issue, which can contain definitions\nof the relevant classes and methods. After each round of\nretrieval API invocations, the LLM agent takes the code\ncontext collected so far and decides whether the context is\nsufficient for understanding and resolving the problem. If the\ncontext is deemed sufficient, the retrieval process will end,\nand the agent will decide on a set of buggy locations, which\nare sent to the patching agent for repairing. Otherwise, the\nretrieval process continues until a predefined threshold count\nis reached.\nOne key novelty in SpecRover is the explicit extraction\nof function summaries while collecting code snippets during\ncontext retrieval. In SpecRover, whenever a new code snippet\nis retrieved with an API and sent to the context retrieval\nagent, we explicitly prompt the agent to analyze the \"intended\nbehavior\" of this code snippet in the current problem context.\nThe intended behavior (or specification) is a concise natural-\nlanguage summary of how a function should behave to meet\nthe requirements specified in the high-level problem descrip-\ntion. This function-level summary of intended behavior serves\nas a local specification to guide the patch construction. The\nsystem-level intended behavior specification given by the user\n(i.e. the issue description) is often on how the program should\nbehave rather than how a unit function should behave. So\nwe usually do not have the intended behavior of a function.\nAlthough the issue description may provide some \"direction\"\non the intended behavior of a function it is usually not\nsufficient to guide the patching agent. On the other hand, the\nextracted function-level specification (capturing the intended\nbehavior of the function) serves as a more direct guide to\nthe patching agent. Instead of giving a set of bug locations\n{L1, L2, ..., Ln} to the patching agent to modify, SpecRover\ngives the pairs of bug locations and their corresponding lo-\ncal specification {(L1, Spec\u2081), (L2, Spec2), ..., (Ln, Specn)}. The patching agent can then refer to the specifications of\nintended behavior and modify code at the function level (so\nas to achieve this intended behavior). Intuitively, our approach\ndecomposes the repository-level issue solving task to several\nfunction-level code modification tasks, in which each function-\nlevel task has a natural language specification. LLMs have\nbeen extensively studied for function-level coding tasks and\nhave shown promising results in function-level benchmarks\nsuch as HumanEval [11], [12] and MBPP [13]. Therefore,\nthis task decomposition helps the patching agent of SpecRover\nwhich then has to solve smaller and more manageable tasks."}, {"title": "C. Reviewer Feedback: Reconciling Specifications", "content": "Another kind of specification inferred by SpecRover is the\nreviewer feedback. To be more precise, the reviewer feedback\ncan be called a meta-specification: it is a reflection on the\nspecifications inferred in previous steps. Concretely, given a\npatch and a reproducer test, the reviewer agent in SpecRover\nwill provide feedback, which includes 1) a binary decision\nof whether the patch and the reproducer test are correct\nrespectively; and 2) an explanation for the decisions."}, {"title": "D. Patch Selection", "content": "As shown in Figure 2, a patch approved by the reviewer\nagent is checked through a regression test suite, which serves\nas an oracle for whether the patch breaks existing functionality\nof the program. However, in the setting of resolving GitHub\nissues, the regression test suite can be an inaccurate oracle,\nmeaning that they can reject correct patches which resolve\nthe issue. This is because the correct patch may inevitably\nmodify existing functionalities of the program while resolving\nthe issue, thus causing some of the existing regression tests to\nfail. For example, if the patch needs to modify the signature of\nan existing function f in order to resolve an issue, regression\ntests that invoke f will now fail. Since the correct patch can be\nrejected by the regression tests, we employ a patch selection\nprocess at the end of the workflow to select the most promising\npatches among the rejected candidate patches.\nDuring the final patch selection phase, SpecRover goes\nbeyond the test cases and employs a selection agent to choose\na patch based on the natural language issue description. All\ncandidate patches that failed some tests are presented to\nthe selection agent, together with the issue description. The\nselection agent is instructed to analyze the root cause of the\nissue, think about how the issue can be possibly resolved,\nand select a patch that best addresses the issue. This natural\nlanguage-guided patch selection can recover correct patches\nthat are mistakenly filtered out by an inaccurate test suite. It\nexploits the natural language issue report as that captures the\nmost up-to-date intents from users/developers. While making a\nchoice among the candidate patches, the selection agent also\nexplicitly states a reason why it chooses a particular patch\namong the candidate patches. This \u201creason for selection\" can\nbe given as evidence together with the final patch."}, {"title": "E. Evidence", "content": "SpecRover is designed to not only generate a patch to\nresolve the issues in software repositories, but also to provide\nthe inferred specifications as evidence for why a patch was\nselected. Specifically, along with the final patch, the following\nartifacts can be the outputs of SpecRover as well:\n\u2022 Buggy locations and their intended behaviors.\n\u2022 The reproducer test written by the reproducer agent.\n\u2022 The reason why the final patch was approved by the\nreviewer agent (if the patch is approved by the reviewer\nand the regression test suite).\n\u2022 The reason why the final patch is selected by the selection\nagent (if there are multiple candidate patches which\ncannot be differentiated by the tests).\nThe benefits of generating evidence are threefold. First,\nthese artifacts can guide the LLM agents in constructing\nhigher quality patches, as discussed in Section III-B and III-C.\nSecond, the natural language artifacts can assist the developers\nin understanding the auto-generated patches more quickly.\nBefore examining the actual patch, developers can gain a"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We address the following research questions:\nRQ1: What is the efficacy of SpecRover in resolving issues?\nRQ2: What level of confidence can developers get from the\npatch and specifications produced by SpecRover?\nRQ3: What is the quality of the specification produced by\nSpecRover as evidence?\na) Benchmark: We evaluate the efficacy of SpecRover\non SWE-bench [3], a widely-used benchmark for autonomous\nprogram improvement consisting of 2294 real-world GitHub\nissues. For each issue, the only input for SpecRover is the issue\nstatement and the buggy codebase. Note that the regression test\nsuite used by SpecRover is part of the buggy program; we do\nnot access any code or test that is added by the developer in\nthe fixed version of program.\nb) Baselines and Evaluation Metrics: For RQ1, we com-\npare with the state-of-the-art systems that target the repository-\nlevel issue solving task. In our comparison, we include all the\nopen-source software engineering agents which have reported\nresults on SWE-Bench. The baseline tools include:\n\u2022 AutoCodeRover [5]. AutoCodeRover employs a set of pro-\ngram structure-aware APIs to gather relevant code con-\ntext. It optionally integrates debugging techniques such as\nSpectrum-based Fault Localization to sharpen the context.\n\u2022 SWE-agent [9]. SWE-agent designs an agent-computer in-\nterface, which defines the possible actions taken by an agent\nto edit code, navigate the codebase, and execute tests.\n\u2022 AppMap Navie [14]. Navie uses a retrieval-augmented gen-\neration (RAG) based approach to construct the code context,\nand performs an explicit planning step before generating\ncode changes [15].\n\u2022 OpenDevin [16]. OpenDevin's CodeActAgent tackles the\ntasks by having a general action space, where the agent\nis allowed to execute arbitrary Python and bash commands\ninside a sandbox environment.\n\u2022 Aider [17]. Aider constructs a repository map which helps\nthe LLM to understand the repository context. It also uses\nthe regression test suite as a harness to retry the task.\n\u2022 Moatless Tools [18]. Moatless Tools builds an agentic loop\nthat functions as a finite state machine and transitions\nbetween states. It focuses on building a set of good tools\nfor the agent instead of relying on the agent for reasoning."}, {"title": "V. EVALUATION", "content": "We first evaluate the overall efficacy of SpecRover in resolv-\ning repository-level tasks. We report the efficacy of SpecRover\non both SWE-Bench (consisting of 2294 real-world GitHub\nissues), and SWE-Bench lite (which is a subset of SWE-Bench\nconsisting of 300 issues). For the baseline tools, we compare\nwith their corresponding reported efficacy. If a tool supports\ndifferent configurations (e.g. different LLMs as the backend),\nwe compare with the configuration with the highest efficacy."}, {"title": "A. RQ1: Overall Efficacy of Task Resolving", "content": "Result: Table I shows the efficacy of issue resolving in\nboth SWE-Bench and SWE-Bench lite. Overall, SpecRover\nachieves the highest efficacy among all the open-source tools\nin both SWE-Bench and SWE-Bench lite. In SWE-Bench lite,\ncompared to the previously top-performing group of tools\nwhich resolved approximately 26% to 27% of the issues,\nSpecRover improved the efficacy to 31%. This efficacy im-\nprovement is also evident in the full SWE-Bench, where\nSpecRover improved the efficacy from 14.60% to 19.31%.\nFigure 5 illustrates the number of uniquely resolved issues by\nSpecRover and other tools in SWE-Bench lite. For clarity, this\nfigure includes only the top five performing tools from Table I.\nSpecRover uniquely resolved 12 issues, the highest number\nof uniquely resolved issues among all the tools. Among the\n12 uniquely resolved issues, SpecRover resolved six of them\nby generating only one patch, demonstrating that the inferred\nfunction summary can effectively guide the LLM to generate\ncorrect patches. For the other six issues, SpecRover deemed\nthe first generated patch as incorrect from the reviewer agent\nand the regression test suite. In this case, the patches are\niteratively refined based on the reviewer feedback and the test\nresults, and eventually the correct patch is selected at the end\nof the workflow.\nTime and Cost: We also report the average time taken and\naverage costs for each issue in Table I. For each tool, we in-\nclude the time and cost statistics in Table I if these information\nwas publicly reported or can be calculated from their publicly\navailable execution traces. On average, SpecRover costs $0.65\nUSD to generate patches for each issue in SWE-Bench lite,\nachieving the highest efficacy with a relatively low cost. We\nfurther investigate the 93/300 issues resolved by SpecRover\nin SWE-bench lite. For the resolved issues, SpecRover only\ncosts $0.36 USD per issue to generate the correct patch,\nsuggesting that the resolved issues requires less retries and\nless API calls to the LLM in general. Time-wise, SpecRover\nspends an average of 309 seconds (i.e. 5.15 minutes) on each\nissue, which includes the time for executing the reproducer\nand the regressions tests in the project. According to a recent\nstudy, most developers accept automated program repair tools\nwhich takes less than 30 minutes [19]. SpecRover requires\napproximately 5 minutes, which we deem acceptable."}, {"title": "B. RQ2: Utility of autonomous SE, confidence in results", "content": "Although the efficacy in resolving issues is an important\naspect of autonomous program improvement, it is not the sole\npurpose of such a technique. Rather, the efficacy is a means\nto an end to reduce human effort in software maintenance.\nTo this end, a program improvement technique must not only\nhave high efficacy, but also minimize the effort required of\nan end user to use the technique. The effort is related to two\nmetrics: 1) signal-to-noise ratio, i.e., the ratio of correct to\nincorrect patches presented to a user; and 2) the difficulty of\nexamining each auto-generated patch that is suggested.\nWe have designed SpecRover to reduce both of these efforts.\nFirst, to reduce the number of incorrect patches that a user may\nexamine, we use the reviewer agent to decide the correctness\nof the generated patch and the reproducer test. The user\ncan choose to examine the generated patch only when both\nthe patch and the reproducer test are deemed correct by the\nreviewer agent. The accuracy of the reviewer decisions are\nmeasured in RQ2. Second, to make it easy for a user to\nexamine each patch, SpecRover provides a variety of evidence\nto help understand the patch, as discussed in Section III-E. The\nquality of the evidence will be discussed in RQ3.\nThere can be four different scenarios when the reviewer\ndecision is viewed in relation to the actual correctness of the\npatch. For convenience, we say a patch is accepted when the\nreviewer agent decides that both the generated patch and the\nreproducer test are correct. With this, we discuss the following\nfour scenarios:\n\u2022 TP (true positive): Patch is accepted and correct;\n\u2022 TN (true negative): Patch is not accepted and incorrect;\n\u2022 FP (false positive): Patch is accepted but incorrect;\n\u2022 FN (false negative): Patch is not accepted but correct."}, {"title": "C. RQ3: Quality of Specifications produced", "content": "In this section, we illustrate the quality of evidence gen-\nerated by SpecRover with two examples. The high-quality\nevidence allows a developer to easily integrate auto-generated\npatch into an existing codebase.\na) Reviewer Feedback as Summary: In the first example,\nwe show that the reviewer feedback can serve as a concise\nsummary of a generated patch. The summary describes the\nbehavior of the patch at a high level. Therefore, a developer\ncan understand the generated patch faster by reading the\nsummary before examining the details of the patch. Besides,\nafter the developer accepts the patch and decides to merge it\ninto existing code, the summary constitutes a good commit\nmessage, so that the developer does not need to write one.\nFrom a developer's perspective, the whole process is very\nmuch like reviewing a pull request, which is already part of a\ndeveloper's day-to-day workflow. The specific issue involved\nin this example is pytest-74322. The issue statement is shown\nin Figure 7a, which reports that pytest (a python testing\nframework) would miscalculate a line number in its output"}, {"title": "VI. CASE STUDY: SECURITY VULNERABILITY REPAIR", "content": "Although SpecRover is initially designed to resolve GitHub\nissues in Python repositories, it can be easily adapted for\nprogram improvements in other application domains, and\nfor programs written in other programming language. We\ndemonstrate how SpecRover fixes security vulnerabilities in\nC programs, through an example challenge problem from\nthe DARPA AI Cyber Challenge (AIxCC) in 2024 [20].\nThe AIxCC is a two-year competition organized by DARPA\nand ARPA-H to encourage the development of novel cyber-\nreasoning systems to safeguard critical software. The AIxCC\nhas publicly released exemplar challenges, where each chal-\nlenge consists of a software project with a vulnerability in it.\nThe task is to have an autonomous system to find and fix the\nvulnerability. Each exemplar challenge also contains a Proof-of-Concept (PoC) input file that triggers the vulnerability, so\nwe use this PoC to show how SpecRover can be used to fix the\nvulnerability after it is detected. Figure 9 shows one exemplar\nchallenge, which is a buffer overflow vulnerability in the Linux\nkernel. This buffer overflow happens in the Linux network-\ning module for the Transparent Inter-Process Communication\n(TIPC) protocol, and allows remote attackers to cause denial-\nof-service or disclosure of sensitive information. Specifically,\nwhen the user-supplied sizes in the message body are invalid\nfor the received messages, a buffer overflow happens with the\nmemcpy call, as shown in Figure 9a. This vulnerability has\nbeen triggered by a PoC, which results in a vulnerability report\nas shown in Figure 9b.\nSpecRover fixes this vulnerability by first analyzing the vul-\nnerability report, similar to how it resolves GitHub issues by\ninitially examining the issue descriptions. It conducts context\nretrieval, and decides on the buggy locations and intended"}, {"title": "VII. RELATED WORK", "content": "Automated program repair (APR) [8], [21] is a well studied\nresearch area in software engineering. Given a buggy program\nP, and a test-suite T, automated program repair attempts to\n(minimally) modify P to a program P' which passes the given\ntest-suite T. APR techniques involve metaheuristic search [22],\nsemantic analysis [6], machine learning [23], or a combination\nof different techniques. APR can also be used to rectify\nautomatically generated code from LLMs, e.g. see the work\nof [1] among others.\nThe recent interest in prompt engineering as well as agent\nbased solutions has somewhat evolved the research in program\nrepair. LLM agents try to combine the power of LLM with pro-\ngram analysis and test execution reasoning. Thus LLM agents\ncan combine LLMs with test generation, static and dynamic\nanalysis as well as specification inference. In the recent past,\nlot of LLM based approaches have been proposed for solving\nsoftware \"issues\" described in natural language, including [4],\n[5], [9], [10]. Among these our work is thematically closest\nto the work of AutoCodeRover [5]. Like AutoCodeRover,\nwe take the position that program modifications like bug\nfixing are best aided by inference of the developer intent.\nAutoCodeRover infers the developer intent only from the\nsoftware project structure. In contrast, SpecRover is more\ngeneral and is capable of inferring specifications from different\nsources including program structure, program behavior, tests\nand so on. Furthermore, SpecRover focuses on giving an\nexplanation of the produced patches."}, {"title": "VIII. PERSPECTIVES", "content": "Owing to the growth of large language based automatic\nprogramming (see [24] for a recent summary), there exists\ninterest in autonomous program improvement technologies.\nWe propose SpecRover with the perspective of autonomously\nproducing patches which are suggested with confidence (thus\ndevelopers can confidently accept them) and come with ex-\nplanations. The technical innovations supporting SpecRover\nare the specification inference to guide patching, and the\nrigorous vetting of patches via our reviewer agent. Our work\non SpecRover seeks to put the matter of quality of patches\nproduced by LLM agents into the research community's\nattention, whereas other works are mostly focusing on the\nagent efficacy."}, {"title": "TOOL RELEASE", "content": "We will share access to the tool within 45 days of the paper\nfirst appearing in public domain via arXiv."}, {"title": "APPENDIX", "content": "An issue that can bias the evaluation of LLM agent-\ngenerated patches is data memorization. Data memorization\noccurs when an LLM deals with a program that exists in its\ntraining set. This issue is rather common, since LLMs have\nbeen trained on a vast amount of publicly available code. In\nthe presence of possible data memorization, when a correct\npatch is generated by an LLM agent, it may be not because\nof the capability of the LLM agent, but because the LLM\nhas seen the repaired version of the program. In this way, the\ncapability of the LLM agent is overestimated.\nOne way of gauging the threat of data memorization is to\ncheck whether the generated patch is syntactically equivalent\nto the ground-truth correct patch. Intuitively, the LLM is likely\nto generate a syntactically equivalent patch if it has seen the\ncorrect program. Therefore, the threat of data memorization\nis lower if the generated patch turns out not syntactically\nequivalent.\nTo gauge the data memorization threat and have a more\naccurate evaluation of the LLM agents, we perform this\nsyntactic check of patches generated by agent systems on\nSWE-Bench lite. For each issue resolved by a tool, we\ncompare the patch generated by the tool against the ground-\ntruth developer-written patch, and check whether they are\nsyntactically equivalent. The results are shown in Table III.\nThe column \"# Resolved\" shows the number of resolved\nissues by a tool; \u201c# Syntactically Equiv.\u201d shows the number\nof resolved issues with the generated patch being syntactically\nequivalent to the ground-truth patch; \"% Equiv. / Resolved\u201d\nshows the ratio between these two numbers."}]}