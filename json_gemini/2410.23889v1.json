{"title": "GEPS: Boosting Generalization in Parametric PDE Neural Solvers through Adaptive Conditioning", "authors": ["Armand Kassa\u00ef Koupa\u00ef", "Jorge Mifsut Benet", "Yuan Yin", "Jean-No\u00ebl Vittaut", "Patrick Gallinari"], "abstract": "Solving parametric partial differential equations (PDEs) presents significant challenges for data-driven methods due to the sensitivity of spatio-temporal dynamics to variations in PDE parameters. Machine learning approaches often struggle to capture this variability. To address this, data-driven approaches learn parametric PDEs by sampling a very large variety of trajectories with varying PDE parameters. We first show that incorporating conditioning mechanisms for learning parametric PDEs is essential and that among them, adaptive conditioning, allows stronger generalization. As existing adaptive conditioning methods do not scale well with respect to the number of parameters to adapt in the neural solver, we propose GEPS, a simple adaptation mechanism to boost GEneralization in Pde Solvers via a first-order optimization and low-rank rapid adaptation of a small set of context parameters. We demonstrate the versatility of our approach for both fully data-driven and for physics-aware neural solvers. Validation performed on a whole range of spatio-temporal forecasting problems demonstrates excellent performance for generalizing to unseen conditions including initial conditions, PDE coefficients, forcing terms and solution domain.", "sections": [{"title": "Introduction", "content": "Solving parametric partial differential equations, i.e. PDE in which certain parameters\u2014such as initial and boundary conditions, coefficients, or forcing terms-can vary, is a crucial task in scientific and engineering disciplines, as it plays a central role in enhancing our ability to model and control systems, quantify uncertainties, and predict future events. Neural networks (NNs) are increasingly employed as surrogate models for solving PDEs by approximating their solutions. A primary challenge with these data-driven solvers is their ability to generalize across varying contexts of the physical phenomena. This is especially pronounced for dynamical systems, which can exhibit significantly different behaviors when subject to small changes in PDE parameters.\n\nThe usual approach for solving a parametric PDE with neural networks involves sampling instances from the PDE parameter distribution (e.g., the PDE coefficients), and then sampling trajectories for each instance of the underlying PDE. The training set thus consists of multiple trajectories per parameter instance, with the objective of generalizing to new instances. This approach aligns with the classical empirical risk minimization (ERM) framework: it assumes that the training dataset is large enough to represent the distribution of the dynamical system behaviors and that this distribution is i.i.d.. However, given the complexity of dynamical"}, {"title": "Problem Description", "content": "We consider parametric time-dependent PDEs and aim to train models that generalize across a wide range of the PDE parameters, including initial conditions, boundary conditions, coefficient parameters, and forcing terms. For a given PDE, an environment e is an instance of the PDE characterized by specific parameter values. We assume that all environments share common global features, such as the general form of the dynamics, while each environment e exhibits some unique and distinct behaviors. A solution u(x, t) of the PDE in environment e satisfies the PDE:\n\n$\\frac{\\partial u(x, t)}{\\partial t} = F^e\\Big(u, \\frac{\\partial u^e}{\\partial x^1}, \\frac{\\partial u^e}{\\partial x^2} \\dots, \\mu, f\\Big) \\quad \\forall x \\in \\Omega, \\forall t \\in (0, T]$\n\n$\\quad \\qquad \\qquad(1)$\n\n$B(u)(x,t) = 0,  \\forall x \\in \\Omega, \\forall t \\in (0, T]$\n\n$\\quad \\qquad \\qquad(2)$\n\n$u(x, 0) = u_0, \\qquad \\forall x \\in \\Omega$\n\n$\\qquad \\qquad \\qquad(3)$"}, {"title": "Motivations: ERM versus adaptive approaches for parametric PDES", "content": "The classical ERM approach learns a single model on the data distribution, assuming that the training dataset is large enough to approximate the true data distribution. In practice, data acquisition is often costly and even simple physical systems can demonstrate a large variety of behaviors due to changes in the parameters. This may lead to poor generalization especially with scarce data.\n\nTo illustrate this, we will compare the performance and behavior of classical ERM approaches and of our adaptive conditioning method for solving parametric PDEs, on two example datasets: the 2D Gray-Scott and the 1D Burgers equations. In these experiments, we generate for each PDE a series of environments by sampling only the physical coefficients of the PDE $\\xi = {\\mu}$ (more details in Appendix B). We consider in-distribution generalization and out-of-distribution generalization (respectively in sections 3.1 and 3.2, both for an initial value problem (IVP), a common setting where the initial condition u0 corresponds to the system state at one time to only. We then consider an alternative setting, where the neural solver is conditioned over a sequence of past states instead of"}, {"title": "In-distribution generalization for IVP: classical vs. adaptive conditioning approaches", "content": "Let us first compare adaptive conditioning and ERM approaches, for in-distribution evaluation, when scaling the number of training environments and trajectories. The models are trained on a range of environments - corresponding to different coefficients of the underlying PDE - and evaluated on the same environments with different initial conditions. Here GEPS is implemented with a classical CNN, while the tests for ERM are performed with four different backbones: the same CNN as used for GEPS but without adaptive conditioning, FNO, MP-PDE and Transolver. Additionally, we also compared with a reference foundation model \"Poseidon\". This model has been pre-trained on a variety of IVP PDE equations and is fine-tuned on our data. Poseidon being trained on 2D data is thus evaluated only for the Gray-Scott equation. For all the baselines, we consider the classical IVP setting where only one initial state is given as input.\n\nScaling w.r.t. the number of environments. We first examine how the number of training environments affects generalization to unseen trajectories within the same range of environments. The models are trained on 4 and 1024 environments with 4 trajectories per environments corresponding to different initial conditions. The evaluation is performed on 32 new trajectories from the same set of environments. As shown in Figure 2, Transolver, FNO, MP-PDE, and CNN fail to capture the diversity of behaviors and their performance stagnate when increasing the number of training environments. Non-conditioned methods are not able to capture the diversity of behaviors for several environments, regardless of the backbone used, when using only an initial state as input. Poseidon on its side behaves much better on Gray-Scott and is able to capture this diversity of dynamics. Our adaptive conditioning approach (GEPS on the figures) performs significantly better than all the baselines, outperforming also the large Poseidon foundation model. We can also observe that GEPS benefits from being trained on a large amounts of environments, as its generalization performance improves with the number of training environments."}, {"title": "Out-of-distribution generalization to new environments for IVP: classical vs. adaptive conditioning approaches", "content": "Let us now consider the out-of-distribution behavior of the two approaches. The models are trained on a sample of the environments from $E_{tr}$ and their associated trajectories, in the same condition as for section 3.1. They are then evaluated on the trajectories of new environments. We report in figure 4 the out-of-distribution generalization performance of ERM methods and GEPS, for the Gray-Scott and Burgers equations, when pretrained on 4 (left) and 1024 (right) environments, with 4 trajectories per environment. For the test, one considers 4 new environments and evaluate on 32 trajectories per environment. Adaptation (GEPS) or fine tuning (baselines) is performed on one trajectory of a new environment. As for the baselines, we consider CNN and Transolver, plus the Poseidon foundation model for the 2-D Gray-Scott equation only. As above, one may observe a large performance gap between the non adaptive approaches and the adaptive GEPS. This supports our claim on the limitations of pure ERM based approaches to generalize to unseen dynamics configurations and new environments."}, {"title": "In and out-of-distribution generalization performance with temporal conditioning", "content": "So far we have considered the classical IVP setting with only one initial state provided at time t0. We consider now the situation where the model has access at t0 to an history of past states and not to a"}, {"title": "GEPS method", "content": "We introduce our framework for learning to adapt neural PDE solvers to unseen environments. It leverages a 1st order adaptation rule and low-rank adaptation to a new PDE instance. We consider two settings commonly used for learning the PDE solvers. The first one leverages pure agnostic data-driven approaches, as already considered in section 3. The second one leverages incomplete physics priors and considers hybrid approaches that complement differentiable numerical solvers with deep learning components. The general framework is illustrated in Figure 5."}, {"title": "Adaptation rule", "content": "We aim to train a model $G_\\theta$ to forecast dynamical systems coming from multiple environments. We perform adaptation in the parameter-space: some parameters are shared across all the environments while others are environment-specific. Training consists in estimating the shared parameters and learning to condition the model on environment specific parameters. At test time, the shared"}, {"title": "Formulation", "content": "We adapt the parameters of our model $G_\\theta$ using a low-rank formulation. Most deep-learning architectures can be decomposed into modules or layers - in our experiments we use MLPs, CNNs and FNOs. For simplification let us then consider a layer $L_i$ from $G_\\theta$ parameterized by a weight matrix $W_{L_i} \\in \\mathbb{R}^{d_{in} \\times d_{out}}$. Adaptation to an environment is performed through a low-rank matrix $\\Delta W_{L_i} = A_{L_i} diag(c^e)B_{L_i}$, where $A_{L_i} \\in \\mathbb{R}^{d_{in}\\times r}, B_{L_i} \\in \\mathbb{R}^{r\\times d_{out}}, c^e \\in \\mathbb{R}^r$. The weights of layer $L_i$ with the adaptation mechanism are then:\n\n$W_{L_i}^e = W_{L_i} + A_{L_i} diag(c^e)B_{L_i} \\qquad \\qquad (4)$\n\nwhere $diag(c^e)$ is a diagonal matrix capturing environment specific information and $A_{L_i}, B_{L_i}, W_{L_i}$ are shared parameter matrices across all environments. Ideally, we want $c^e$ to capture the number of degrees of variations for our environments. If for example our model $G_\\theta$ is an MLP, the adaptation mechanism for a linear layer $L_i$ corresponds to:\n\n$z_i^e = (W_{L_i} + A_{L_i} diag(c^e)B_{L_i})z_{i-1} + b_i + b_i^c \\qquad \\qquad (5)$\n\nwhere $W_{L_i}, A_{L_i}, B_{L_i}, b_1, b_2$ are the parameters of layer $L_i$, shared across all environments. Only $c^e$ is specific to each environment, but shared across all the layers of the network (cf Fig. 5).\n\nConsidering a low-rank adaption rule is popular in NLP, where large pre-trained models are adapted to new tasks by learning a low-rank matrix $\\Delta W = AB$ added to the frozen weights W of the pre-trained model. Our approach differs in two ways from this setting: (i) the model is learned from scratch without pretraining, i.e., we learn parameters {$W, A, B, c^e$} jointly, (ii) during adaptation, we can adapt to new environments $e \\in E_{ev}$ by optimizing only the context vector $c^e$, where it is initialized as $c^e = \\bar{c}_{tr}$, with $\\bar{c}_{tr}$ the averaged value of contexts learned during training. We experimentally show in Appendix D.3 that the classical Gaussian parameter initialization proposed in LoRA is inefficient in our context."}, {"title": "Two-step training procedure", "content": "This meta-learning framework operates in two steps: Training and Adaptation at inference. The goal is to learn an initial starting point during the training stage using a sample of environments, allowing adaptation to a new environment by adjusting a small subset of parameters based on a limited data sample from the new environment. Unlike many meta-learning gradient based approaches, GEPS does not involve an inner loop and is a 1st order method. During the adaptation phase, all the parameters except the context parameters ce are frozen and ce is learned from the new environment sample. This approach ensures rapid adaptation by keeping ce low-dimensional. For simplicity, we refer to parameters shared across environments as $\u03b8^s$ and parameters specific to each environment as $\\delta\u03b8^c$, and denote $\u03b8_e = {\u03b8^s, \\delta\u03b8^c }$. The optimization problem can thus be formulated as follows:\n\n$\\underset{\\theta^s, \\delta\\theta^c}{min} \\sum_{D \\in \\mathbb{E}_{tr}} L(\\lbrace \\theta^s, \\delta\\theta^c \\rbrace, D)  \\qquad (6)$\n\nsubject to $\\delta\\theta^c = arg \\underset{\\delta\\theta^c}{min} \\sum_{D_{ev} \\in \\mathbb{E}_{ev}} L(\\lbrace \\theta^s,\\delta\\theta^c \\rbrace, D_{ev})$\n\nWe separate training and adaptation steps into a training and an adaptation loop as described in the pseudo-code Algorithm 1."}, {"title": "Hybrid formulation for learning dynamics", "content": "We consider here an alternative problem to the above agnostic formulation. We assume that part of the physics is modeled through a PDE equation and shall be complemented with a statistical module. This is a common situation in many domains where prior physical knowledge is available, but only in an incomplete form. We follow the formulation in Yin et al. (2021) were starting from a complete PDE, we assume that part of the equation is known and will be modeled with a differentiable solver,"}, {"title": "Dynamical systems", "content": "We performed experiments on four dynamical systems, including one ODE and 3 PDEs. The ODE models the motion of a pendulum, which can be subject to a driving or damping term. We consider a Large Eddy Simulation (LES) version of the Burgers equation, a common equation used in CFD where discontinuities corresponding to shock waves appear. We additionally study two PDEs on a 2D spatial domain: Gray-Scott, a reaction-diffusion system with complex spatio-temporal patterns and a LES version of the Kolmogorov flow, a 2D turbulence"}, {"title": "Evaluation Setting", "content": "While in section 3 we compared GEPS with baselines ERM approaches and with a foundation model, our objective here is to assess the performance of GEPS w.r.t. alternative adaptation based approaches. We evaluate the model performance on two key aspects. In-distribution generalization: the model capability to predict trajectories defined by unseen ICs on all training environments e \u2208 Etr, referred as In-d. Out-of-distribution generalization: the model ability to adapt to a new environment e \u2208 Eev by predicting trajectories defined by unseen ICs, referred as Out-d. Each environment e \u2208 E is defined by changes in system parameters, forcing terms or domain definition. dp represents the degrees of variations used to define an environment for each PDE equation; dp = 4 for the pendulum equation, dp = 3 for the 1D Burgers, dp = 2 and dp = 3 for the Gray-Scott and the Kolmogorov flow equation respectively (more details in Table 4 in Appendix B). For each dataset, we collect Ntr trajectories per training environment. For adaptation, we consider Nad = 1 trajectory per new environment in Eev to infer the context vector ce. Evaluation is performed on 32 new test trajectories per environment. We report in Table 2 the relative MSE: $\\frac{1}{N}\\sum_{i=1}^{N} \\frac{||Y_i - \\hat{Y}_i||_2}{||Y_i||_2}$."}, {"title": "Generalization results", "content": "Implementation We used a standard MLP for the Pendulum equation, a ConvNet for GS and Burgers equations and FNO for the vorticity equation. All activation functions are Swish functions. We use an Adam optimizer over all datasets. Contrary to Section 3, we perform time-integration with a NeuralODE with a RK4 solver, as it was done for other multi-environments frameworks for physical systems . Architectures and training details are provided in Appendix E.\n\nBaselines As baselines, for the pure data-driven problem, we consider four families of multi-environment approaches. The first one consists in gradient-based meta-learning methods with CAVIA and FOCA. The second one is a multi-task learning method for dynamical systems: LEADS . The third one is a hyper-network-based meta-learning method which is currently SOTA among the adaptation methods, CoDA. As for the hybrid physics-aware problem, we implemented a meta-learning formulation of the hybrid method APHYNITY, where the adaptation is performed on the physical PDE coefficients only while the NN component is shared across all the environments. GEPS-Phy is our physics-aware model (section 4.3) were all the parameters, PDE coefficients and context vector ce are adapted at inference. We also implemented \"Phys-Ad\", where we use the same formulation as for the hybrid GEPS-Phy, but adaptation is performed on the coefficients of the physical component only while the neural network component is shared across all environments. All the baselines share the same network architectures than the ones used for GEPS, indicated in the implementation paragraph above. More details on baselines implementation are provided in Appendix E.6.\n\nIn-distribution and out-of-distribution results We report results for in and out-of-distribution generalization in table 2 for both the data-driven and the hybrid settings. Across all datasets, our framework performs competitively or better than the baselines for the two settings. Our method is able to correctly adapt to new environments in an efficient manner, updating only context parameters ce. For the agnostic data-driven experiments, GEPS obtains the best results, although being on the same range of performance as other methods.\n\nThe main differentiator of GEPS w.r.t. the baselines lies in its lower complexity. In terms of training time, GEPS is way less expensive than gradient-based approaches like CAVIA that involves an outer and inner loop and LEADS, which learns a model specific to each environment. In terms of number of parameters, CoDA needs more training parameters because of its adaptation mechanism relying on a hyper-network. A comparison with the baselines in terms of parameter complexity is provided in table 10. Additional results in Appendix D, show that our data-driven framework adapts faster"}, {"title": "Scaling to a larger dataset", "content": "To further illustrate the benefits of GEPS, we compare it to CODA, the SOTA adaptation model, on a large dataset with a larger model than the ones used for the previous experiments. We use a dataset generated from a PDE with multiple differentiable terms that encompasses several generic PDEs, inspired from Brandstetter et al. (2022). The PDE writes as (more details in Appendix B):\n\n$[\u2202_t u + \u2202_x(\u03b1u^2 - \u03b2\u2202_x u + \u03b4\u2202_{xx} u + \u03b3\u2202_{xxx} u)](t, x) = 0, \\qquad (10)$\n\nWe report the results in table 3. All the methods are trained using a ResNet architecture, using a context size ce of size 8. For In-d, we trained our model on 1200 environments with 16 training trajectories per environment and evaluated it on 16 new trajectories. For Out-d, we further adapt each model to 10 new environments given 1 context trajectory per environment and then evaluate it on 16 new trajectories per environment."}, {"title": "Discussion and limitations", "content": "Limitations We have seen that adaptation is essential for learning to generalize neural PDE solvers, and that within this setting, integrating physical knowledge may help. Adaptation still requires sufficient training samples - both environments and trajectories. These findings are still to be confirmed for more complex dynamics and real world conditions for which the variety of behaviors should be much larger than for the simple dynamics we experimented with.\n\nConclusion We empirically demonstrated the importance of adaptation for generalizing to new environments and its superiority with respect to ERM strategies. We proposed a new 1st order and low-rank scalable meta-learning framework for learning to generalize time-continuous neural PDE solvers in a few-shot setting. We also highlighted the benefits of directly embedding PDE solvers as hard constraints in data-driven models when faced with scarce environments."}, {"title": "Related Work", "content": "We review data-driven methods for learning parametric PDEs and existing works for incorporating physical priors in neural networks, in the context of dynamical systems."}, {"title": "Learning parametric PDEs", "content": "Traditional ML paradigm The majority of current approaches for learning parametric PDEs considers a traditional ERM approach by sampling from the PDE parameter distribution. In MP-PDE, PDE parameters are directly embedded in the graph, allowing generalization to PDE parameters sampled from the same data distribution as those used for training. Neural operators do similarly but require large training sets and often perform poorly for out-of-distribution (OOD) data. Alternatives to ERM such as invariant risk minimization (IRM) have shown stronger generalization by learning domain invariants . However, even for simple classification problems, IRM methods can fail to capture natural invariances and are extremely sensitive to sampling .\n\nMulti-environment paradigm Several works have explored multi-environment/meta-learning settings for dynamical systems. \u2022 Data-driven methods like DyAd adapt dynamics models using a time-invariant context from observed state histories, which are often not accessible. Yin et al. developed LEADS, a multi-task method for dynamical systems that adapts in the functional space but requires training a new model for each environment. Park et al. proposed FOCA to address the limitations of second-order optimization of gradient-based meta-learning approaches. (Kirchmeyer et al., 2022) make use of a hyper-network for fast adaptation to new dynamics by updating an environment dependent context encoding. While effective, gradient-based and hyper-network-based approaches respectively involve inner-loop updates or parameter increases with respect to context dimension. \u2022 Model-based approaches like physics-informed neural networks (PINNs) have been adapted for parametric PDEs ; Huang et al. used meta-learning with PINNs through context vectors c learned via auto-decoding. One drawback of PINN is their inability to handle scenarios where some physics are unknown."}, {"title": "Hybrid learning", "content": "Data-driven methods with soft constraints Physics losses as proposed by have been used as prior knowledge under the form of soft constraints for neural operators, with the objective to alleviate data scarcity issues.\n\nData-driven methods with hard constraints Alternatively, physical priors can be incorporated directly as hard constraints, leveraging the training of neural networks with differentiable physics. It has been particularly successful for accelerating computational fluid dynamics (CFD) by addressing numerical errors inherent in PDE discretization and for augmenting incomplete physics. Most of these approaches do not consider the parametric PDE setting."}, {"title": "Dataset details", "content": "We present the equations and the data generation settings used for all dynamical systems considered in this work. In table 4, we report the different PDE parameters changed to generate training and adaptation environments."}, {"title": "Damped and driven pendulum equation", "content": "We propose to study the damped and driven pendulum equation. The ODE represents the motion of a pendulum which can be subject to a damping and a forcing term. Even simple dynamical systems such as the pendulum equation can present a variety of complicated behaviors (e.g., under-damped, critically damped, resonance, super/sub-harmonic resonance, etc.), presented in figure 6."}, {"title": "The form of the ODE is:", "content": "$\\frac{d^2 \\theta}{dt^2} + \\omega_0^2 sin \\theta + \\alpha \\frac{d \\theta}{dt} = f(t) \\qquad (11)$\n\nwhere $\\theta(t)$ is the angle, $\\omega_0$ the natural frequency, $\\alpha$ the damping coefficient, $f(t)$ is a forcing function of the form $f(t) = F cos(\\omega_f t)$, where $\\omega_f$ is the forcing frequency and $F$ the amplitude of the forcing. In our case where we suppose we only have incomplete knowledge of the phenomenon, we consider we do not know the damping term. For the initial condition, the pendulum is dropped from a distribution $\\theta(t_0) \\sim U(0, \\pi/12)$ and $\\frac{d \\theta}{dt}(t_0) \\sim U(0, 1)$.\n\nData generation Each environment is defined by changes in the system parameters resulting in different behaviors. In training environments, the pendulum is either subject to damping or forcing, e.g., environment 1 is a damped pendulum (F = 0) while environment 2 is a driven pendulum ($\\alpha$ = 0). We generate trajectories using a Runge-Kutta 8 solver. For training, we generated 4 distinct environments, each composed of 8 trajectories on the time horizon [0, 25] with a time step $\\Delta t$ = 0.5. We also generate 32 trajectories on a longer time horizon [0, 50] per training environment to evaluate in-distribution generalization. For adaptation, we evaluate our method on 4 distinct environments defined by parameters unseen during training. Only one trajectory is generated per environment with a time horizon is [0, 25] to adapt the model to new dynamics; we evaluate the model's performance on 32 trajectories per environment on a time horizon [0, 50]. During adaptation, environments are defined such that trajectories are subject both to the forcing and damping term simultaneously."}, {"title": "Gray-Scott equation", "content": "The PDE describes reaction-diffusion system with complex spatiotemporal pattern through the following 2D PDE:\n\n$\\frac{du}{dt} = D_u \\Delta u - uv^2 + F(1 - u) \\qquad (12)$\n\n$\\frac{dv}{dt} = D_v \\Delta v - uv^2 - (F + k)v \\qquad (13)$\n\nwhere u, v represent the concentrations of two chemical components in the spatial domain S with periodic boundary conditions. Du, Dv denote the diffusion coefficients respectively for u, v and F, k are the reactions parameters. We consider complete knowledge of the physical model, but instead uses a RK4 solver with $\\Delta t$ = 1 instead of using an adaptive RK45 solver. We generated environments by changing F, k values, while Du, Dv are kept constant across all environments. For experiments in Section 3, we sampled uniformly values for F and k in ranges [0.03, 0.04] and [0.058, 0.062] respectively. For experiments in Section 5.3, details about the environments are given in table 4.\n\nData generation We generated trajectories on a temporal grid using an adaptive RK-45 solver. S is a 2D space of dimension 32 \u00d7 32 with a spatial resolution of $\\Delta s$ 2. For training, we generate 4 environments with one trajectory per environment defined on a temporal horizon [0, 200]. We evaluate in-distribution generalization on 32 trajectories per environment defined on a temporal horizon [0, 400]. For adaptation, we adapt to 4 new environments in one-shot learning manner. We evaluate the model's performance on 32 new trajectories per environments, defined on a temporal horizon [0, 400]."}, {"title": "Burgers equation", "content": "Burgers' equation is a nonlinear equation which models fluid dynamics in 1D and features shock formation:\n\n$\\frac{du}{dt} + u\\frac{du}{dx} = \\nu \\frac{d^2 u}{dx^2} + R_{closure}(\\bar{u}, u) = f(x, t) \\qquad (14)$\n\nwhere u is the velocity field and $\\nu$ is the diffusion coefficient and $f(x,t)$ is a forcing term. The unresolved scales $R_{closure}(\\bar{u}, u)$ and the forcing function is typically unknown, and needs to be directly learned from data. For the experiments done in Section 3, environments have been generated by sampling uniformly $\\nu \\in [1e \u2013 4, 0.5]$. For experiments in Section 5.3, we generated environments by changing the diffusion coefficient or changing the forcing function, as detailed in table 4.\n\nData generation For the DNS, we generate complex trajectories using a 5th order central difference scheme using Runge-Kutta 45 solver with a time-step $\\Delta t = 1e-5$ and $\\Delta x = \\frac{2\\pi}{16384}$ Such trajectories are particularly costly to generate, therefore, we rather use LES. To obtain the ground truth LES trajectories, we apply a spatial filtering operator on the DNS trajectories. We also down-sample the temporal and spatial grid. Therefore, we obtain LES trajectories with a timestep $\\Delta t = 1e-3$ and $\\Delta x = \\frac{2\\pi}{256}$\n\nFor training, we generate 6 environments with 4 trajectories per environment. Trajectories have been generated on a temporal horizon [0, 0.05] for training and for in-distribution evaluation. For adaptation, we adapt our model on 4 new unseen environments using only trajectory per environment. We evaluate out-distribution performance on 32 new ICs on a time horizon [0, 0.05]. For the generation of the data, we define a [0, 2\u03c0] periodic domain and consider the following initial condition:\n\n$E(k)=\\frac{2}{5}(\\frac{k}{k_0})^{\\frac{5}{2}}exp(-\\frac{1}{2}(\\frac{k}{k_0})^2)\\qquad(15)$\n\nVelocity is linked to energy by the following equation :\n\n$\\hat{u}(k) = \\sqrt{2E(k)}\\qquad(16)$"}, {"title": "Kolmogorov flow", "content": "We propose a 2D turbulence equation. We focus on analyzing the dynamics of the vorticity variable. The vorticity, denoted by w, is a vector field that characterizes the local rotation of fluid elements,"}, {"title": "The vorticity, defined as w = \u2207 \u00d7 u. Like Burgers, we generate LES, leading to an unknown unclosed term appearing in the vorticity equation, expressed as:", "content": "$\\frac{\\partial \\omega}{\\partial t} + (u \\cdot \\nabla)\\omega - \\nu \\nabla^2 \\omega + R_{closure}(\\bar{u}, u) = f(x,y,t) \\qquad (17)$\n\nHere, u represents the fluid velocity field, $\\nu$ is the kinematic viscosity with $\\nu = 1/Re$ and $f(x, y, t)$ is a forcing term. The unresolved scales $R_{closure} (\\bar{u}, u)$ and the forcing function is typically unknown, and needs to be directly learned from data. For the vorticity equation, environments can be defined either by changes in the viscosity term, the domain size or the forcing function term.\n\nData generation For the data generation of DNS, we use a 5 point stencil for the classical central difference scheme of the Laplacian operator. For the Jacobian, we use a second order accurate scheme proposed by Arakawa that preserves the energy, enstrophy and skew symmetry . Finally for solving the Poisson equation, we use a Fast Fourier Transform based solver. We discretize a periodic domain into 512 \u00d7 512 points for the DNS and uses a RK4 solver with $\\Delta t = 5e - 3$. We obtain the ground truth LES by applying a spatial filtering operator on the DNS trajectories. We also down-sample temporal and spatial grid, thus obtaining LES trajectories with a timestep $\\Delta t = 5e - 2$ on a 64 \u00d7 64 grid.\n\nWe generate 8 environments with 16 trajectories per environments for training. Trajectories have been generated on a temporal horizon [0, 1] for training and [0, 2] for in-distribution evaluation. During adaptation, we adapt our model on 4 unseen environments in a one-shot manner, using only one trajectory per environment. We evaluate out-distribution performance on 32 new ICs on a time horizon [0, 2]. We consider the following initial conditions:\n\n$E(k) = \\frac{4}{3} \\sqrt{\\frac{4}{\\pi}} k_0 (\\frac{k}{k_0})^4 exp(-\\frac{1}{2} (\\frac{k}{k_0})^2) \\qquad (18)$\n\nVorticity is linked to energy by the following equation :\n\n$\\omega(k) = \\sqrt{\\frac{E(k)}{\\pi k}} \\qquad (19)$"}, {"title": "Combined equation", "content": "We used the setting introduced in Brandstetter et al. (2023), but with the exception that we do not include a forcing term and add the 4thorder spatial derivative. The combined equation is thus described by the following PDE:\n\n$[\u2202_t u + \u2202_x (\u03b1u^2 - \u03b2\u2202_x u + \u03b4\u2202_{xx} u + \u03b3\u2202_{xxx} u)](t, x) = 0, \\qquad (20)$\n\n$u_0 (x) = \\sum_{j=1}^J A_j sin(2 \\pi l_j x/L + \\phi_j). \\qquad (21)$\n\n\u03b1, \u03b2, \u03b4 and \u03b3 are the parameters that are varied.\n\nData generation We used the spectral solver proposed in Brandstetter et al. to generate the solution. We sampled 1200 environments for training, by sampling uniformly within the ranges $\u03b1\u2208 [0.5, 1]$, $\u03b2\u2208 [0,0.5]$, $\u03b4\u2208 [0,1]$ and $\u03b3\u2208 [0,1]$. For each parameter instance, we sampled 16 trajectories, resulting in 19200 trajectories. We then evaluate it on 19200 new trajectories. The trajectories were generated with a spatial resolution of 256 on a temporal horizon [0, 30]. We only keep 10 time-steps. For out-distribution evaluation, we sample 10 new environments from the same ranges of parameters, but for parameters not seen during training. We have access to one trajectory from each evaluation environment for adapting the model, and evaluate it on 16 new trajectories."}, {"title": "Trajectory based formulation", "content": "In practice, Fe is unavailable and we can only approximate it from discretized trajectories. As done in (Kirchmeyer et al., 2022), we use a trajectory-based formulation of Eq. (9). We consider a set of"}]}