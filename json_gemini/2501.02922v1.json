{"title": "Label-free Concept Based Multiple Instance Learning for Gigapixel Histopathology", "authors": ["Susu Sun", "Leslie Tessier", "Fr\u00e9d\u00e9rique Meeuwsen", "Cl\u00e9ment Grisi", "Dominique van Midden", "Geert Litjens", "Christian F. Baumgartner"], "abstract": "Multiple Instance Learning (MIL) methods allow for gigapixel Whole-Slide Image (WSI) analysis with only slide-level annotations. Interpretability is crucial for safely deploying such algorithms in high-stakes medical domains. Traditional MIL methods offer explanations by highlighting salient regions. However, such spatial heatmaps provide limited insights for end users. To address this, we propose a novel inherently interpretable WSI-classification approach that uses human-understandable pathology concepts to generate explanations. Our proposed Concept MIL model leverages recent advances in vision-language models to directly predict pathology concepts based on image features. The model's predictions are obtained through a linear combination of the concepts identified on the top-K patches of a WSI, enabling inherent explanations by tracing each concept's influence on the prediction. In contrast to traditional concept-based interpretable models, our approach eliminates the need for costly human annotations by leveraging the vision-language model. We validate our method on two widely used pathology datasets: Camelyon16 and PANDA. On both datasets, Concept MIL achieves AUC and accuracy scores over 0.9, putting it on par with state-of-the-art models. We further find that 87.1% (Camelyon16) and 85.3% (PANDA) of the top 20 patches fall within the tumor region. A user study shows that the concepts identified by our model align with the concepts used by pathologists, making it a promising strategy for human-interpretable WSI classification.", "sections": [{"title": "I. INTRODUCTION", "content": "ADVANCEMENTS in artificial intelligence, along with the growth of pathology datasets, have driven significant progress in histopathology [2], [4], [5], [37]. However, there are several unique challenges in applying deep learning techniques directly to gigapixel whole-slide image (WSI) analysis. For instance, WSIs are high-resolution and thus large, making the end-to-end training of deep neural networks challenging [11], [21]. Moreover, pixel-level annotations are often unavailable, with only slide-level or patient-level weak annotations provided [36], [37]. To overcome these challenges, algorithms based on Multiple Instance Learning (MIL), such as Attention-based MIL [20], CLAM [27], and TransMIL [36] have been developed, and have been used with great success in computational pathology. This type of model treats the patches from WSI as a bag of instances and trains models using only slide-level annotations in a weakly-supervised manner.\nWhile these MIL approaches offer effective strategies for WSI analysis, there are concerns regarding their interpretability, which is essential for algorithms in safety-critical applications in medicine [35]. MIL-based models typically learn an attention score for each patch to indicate its importance and provide an attention map as a spatial explanation for the prediction [21], [25], [27], [43]. Although attention maps have been shown to correlate with disease-relevant regions, there are several issues with using them as explanations [21]. The non-linear activations within MIL networks introduce a non-linear relationship between the attention scores and the final prediction, making this kind of explanation inaccurate and incapable of faithfully representing the model's real decision-making process [21], [22].\nInherently interpretable models, such as those proposed in [3], [6], [24], have recently gained attention in natural image analysis for their \"white box\" property. This type of model enhances computational transparency and can provide explanations that faithfully reflect the model's underlying mechanisms. In the broader medical domain, several such models have been proposed for chest X-ray images [38], fundus images [10], and biological sequences [9]. However, there is relatively little work in computational pathology, particularly for WSI analysis. To our knowledge, only two inherently interpretable models for WSI analysis exist: Additive MIL [21] and Self-Interpretable MIL (SI-MIL) [22]. Notably, SI-MIL [22] achieves inherent interpretability by providing explanations using pathology-related features, such as the statistical properties of nuclei. While these explanations deliver more information than attention maps, the pathology-related features are hand-crafted and derived from the outputs of a nuclei segmentation model pre-trained on other datasets. This approach limits the model's flexibility to adapt to different diseases and can inherit errors from the segmentation model. Furthermore, explanations from SI-MIL are based on 205-dimensional pathology-related features, which remain challenging for humans to interpret.\nConcept-based interpretable models such as Concept Bottleneck Models (CBMs) [24], [28], [44] provide explanations using human-understandable concepts, making them an increasingly important category in explainable AI. However, these models typically rely on concept annotations, restricting the concepts to those already known by the annotator and requiring expert input. Recently, researchers started to build CBMs in a label-free way [28], [44] by leveraging vision-language models such as CLIP [32], which are pretrained on large-scale image-caption datasets. This approach removes the need for manual labeling and obtains promising results on the natural images. In the field of computational pathology, vision-language foundation models such as PLIP [19] and CONCH [26], which are specifically trained on histopathology data, have been proposed recently and have significantly enhanced the performance of downstream tasks. These foundation models offer a tool for building concept-based models in a label-free manner. However, to our knowledge, there is no concept-based interpretable model for WSI analysis.\nHere, we introduce Concept MIL, a concept-based, inherently interpretable MIL model for WSI classification. Our model is influenced by SI-MIL [22] and incorporates recent advancements in concept-based interpretable models. We address the limitations of SI-MIL by replacing the self-interpretable MIL branch with one that employs flexible, pathologist-defined concepts. This approach removes the potential mistakes from upstream models, reduces the tedious task of designing and extracting pathological features, and offers greater flexibility for deployment across various diseases. Our Concept MIL delivers faithful local and global explanations through pathological concepts, ensuring high interpretability while maintaining robust performance. Importantly, our model eliminates the need for manual concept labeling by leveraging the pathology vision-language foundation model CONCH [26], offering flexibility in model design and easy adaptions across various diseases. We conduct both quantitative and qualitative evaluations of the model's local and global explanations, including a user study with pathologists."}, {"title": "III. METHODS", "content": "In the following, we discuss the main components of our approach. Our model consists of two branches: the image MIL branch, which identifies the top K most significant patches, and the concept MIL branch, which generates final predictions and explanations using the concept activation vectors of the selected patches. First, we explain the image"}, {"title": "A. Generating Image Features", "content": "In the image feature generation step, we extract patch-wised image features from the WSI. As shown in Fig. 1a, the tissue regions are segmented from the WSI and then cropped into N non-overlapping patches. Afterward, we employ the image encoder from the histopathology-specific vision-language model CONCH [26] to extract image features for these patches. For simplicity, we extract patches of the same size 256 \u00d7 256 at the resolution level of 0.5\u00b5m/pixel.\nAfter the image feature extraction step, the N patches from WSI are converted into N image embedding vectors $\\mathbf{i}_1, \\mathbf{i}_2...\\mathbf{i}_N$, each with a dimensionality of D = 512. These image features are subsequently used for generating concept-based features, as depicted in Fig. 1b, and for training a conventional image MIL branch, as shown in Fig. 1c."}, {"title": "B. Generating Concepts Features", "content": "As illustrated in Fig. 1b, the concept-based features $\\mathbf{f}_n$ are computed based on the image features $\\mathbf{i}_n$ from Fig. 1a and text embeddings to extracted from the target concept set. We use the text encoder from the vision-language model CONCH [26] to generate text embeddings. Since CONCH is trained on large-scale histopathology image-text pairs, it learns a shared embedding space for images and text, enabling the direct prediction of text-based concepts from image features. This approach eliminates the need for concept annotations in the concept feature generation step.\nFollowing the way of Label-free CBMs [28], we initialize a concept set by prompting GPT4o [29] for features of tumor tissue. For instance, to collect pathology concepts for detecting breast cancer tumor tissues, we ask GPT4o the following question: \"List the most important features for recognizing breast cancer metastases in hematoxylin and eosin (H & E) stained lymph node whole-slide images\". We additionally ask a pathologist to refine the initialized concept set to ensure the quality. In routine practice, pathologists examine whole slide images for morphological features across different resolutions and scales to make diagnoses. However, our model currently focuses on features at a specific scale and resolution. To make sure that the defined concepts are actually visible at this scale, we provide both GPT4o and pathologists with a few image patches as references during the initialization and adjustment of the concepts. This allows us to obtain a set of concepts that are relevant and visible at the given resolution and scale.\nTo generate the text embeddings for the target concepts, we construct the prompt as \"an H & E image of CONCEPT\" by replacing the CONCEPT placeholder with a specific concept in the target concept set. These prompts are projected into text embeddings using the CONCH [26] text encoder. The image features $\\mathbf{i}_n$ are then mapped to the concept space by calculating the cosine similarity between the normalized image embeddings and text embeddings, producing the concept activation vectors $\\mathbf{f}_n$ for the N patches.\nIn our preliminary experiment, we observed misalignments between the image and text spaces in the CONCH model. For instance, as shown in Fig. 2, patches containing fat cells get high cosine similarity scores with the concept \"fibrous tissue\", indicating that CONCH may not fully understand the concept \"fibrous tissue\u201d in the context of histopathology. Such misalignments can affect user's trust in the model and explanations. To address this, we conduct an additional validation step to filter out misaligned concepts. We select 50K patches from tumor regions based on ground truth masks from the training set. For each concept, we retrieve five patches with high cosine similarity scores and ask a pathologist to verify the relevance of the target concept in the selected patches. Since the target concept sets are relatively small, this verification process is easy to perform. Finally, we use the refined concepts listed in Table I for concept activation vector extraction."}, {"title": "C. Training", "content": "Similar to the SI-MIL [22] model, we train the image MIL branch and the self-interpretable concept MIL branch jointly through a Patch Attention-Guided (PAG) Top-K module (see Fig. 1c).\nWe implement the image MIL branch in Fig. 1c as a conventional attention-based MIL model. It treats N image features from a WSI as a bag of instances, transforms them with a projector H(\u00b7), and weights the projected features with patch-wise attention scores a from attention module AP(.). The transformed image features and the attention scores are denoted as:\n$V = H(I); \\alpha = AP(V)$.\nHere, I represents the input image feature matrix for the WSI, where each individual image feature is $\\mathbf{i}_n$. V represents the transformed image features, with each feature vector represented as $\\mathbf{u}_n$. $\\alpha$ refers to the attention scores, with $\\alpha_n$ being the attention score for patch n.\nThe prediction, denoted as $\\hat{Y}_{img}$, is obtained by applying logistic regression to the scaled image features, as follows:\n$\\hat{Y}_{img} = \\sigma(\\sum_{n=1}^{N} \\sum_{d=1}^{D} \\alpha_n u_{nd} w_d + b_{img})$\nwhere $\\sigma$ represents the logistic function, $u_{nd}$ denotes the value of the d-th dimension of the transformed image feature $\\mathbf{u}_n$, and $w_d$ and $b_{img}$ correspond to the weights and bias of the linear classifier, respectively.\nThe PAG Top-K module identifies the top K patches based on the highest attention scores in $\\alpha$ and passes the indices of the selected patches to the concept MIL branch. Instead of using non-differentiable standard Top-K selection, we adopt the differentiable PAG Top-K module used by [22], [39]. This module implements a differentiable Top-K operator based on the perturbed maximum method [7], [39]. During the forward pass, uniform Gaussian noise is added to each attention score to create perturbed attention values, and the corresponding linear program for maximization is solved. In the backward pass, the Jacobian for the forward operation is computed. This differentiable Top-K selection allows learning the parameters of the attention module in the image MIL branch, thus enabling the joint training of two branches.\nThe concept MIL branch in Fig. 1c receives the concept activation vectors $\\mathbf{f}_j$ of the top K salient patches selected by the PAG Top-K module and makes predictions based on these concept-based features. The concept attention scores $\\beta$ are first calculated using the attention module $A^{\\mathcal{C}}(.)$, scaled by its $\\gamma^{th}$ percentile $P_{r_\\gamma}$, and then transformed using a sigmoid function with a temperature hyperparameter t. This operation turns the attention scores that are less than the $\\gamma^{th}$ percentile towards zeros, enforcing sparsity in feature selection. The resulting concept attention scores $\\beta$ are given by:\n$\\mathbf{\\beta}_{scaled} = A^{\\mathcal{C}}(F^T)$\\n$\\beta = \\frac{1}{std(\\mathbf{\\beta})} ; \\qquad \\beta = \\frac{1}{1 + e^{-\\mathbf{\\beta}_{scaled}/t}}$\nwhere $F^T$ is the transposed concept-based feature matrix of top K patches, $\\beta$, $\\mathbf{\\beta}_{scaled}$, and $\\mathbf{\\beta}$ denote the raw concept attention scores from the attention module, the scaled scores, and the transformed scores, respectively. std(\u00b7) is the standard deviation, and t is a scaling hyperparameter.\nThe transformed concept attention scores $\\mathbf{\\beta}$ are applied to scale the corresponding concept activation vectors, and the final prediction of the concept MIL branch is a linear combination of C concepts from the top K patches:\n$\\hat{Y}_{concept} = \\sigma(\\sum_{j=1}^{K} \\sum_{c=1}^{C} w_c. (f_{jc} \\beta_c) + b_{concept})$\nwhere $\\sigma$ is a sigmoid function, $f_{jc}$ represents the value of the c-th concept in the concept activation vector $\\mathbf{f}_j$, $\\beta_c$ represents the attention score for concept c, and $w_c$ and $b_{concept}$ are the weights and bias terms of a linear classifier.\nDuring training, both the image MIL branch and the self-interpretable concept MIL branch perform classification, generating predictions $\\hat{Y}_{img}$ and $\\hat{Y}_{concept}$. We use the binary cross-entropy (BCE) loss to calculate the classification loss. In addition, we apply an $L_2$ regularization to further constrain the attention scores $\\alpha$ from the image MIL branch. We optimize this dual branch MIL model with the following loss function:\n$L = L_{BCE}(Y, \\hat{Y}_{img}) + L_{BCE}(Y, \\hat{Y}_{concept}) + \\lambda L_2(\\alpha)$,\nwhere Y is the ground truth of the WSI, and $\\lambda$ is a hyper-parameter for the $L_2$ regularization term."}, {"title": "D. Obtaining Prediction and Local Explanation", "content": "During inference, we discard the prediction from the image MIL branch and only use the prediction $\\hat{Y}_{concept}$ from the concept MIL branch as the final output, ensuring the prediction is inherently interpretable.\nAs shown in (5), $\\hat{Y}_{concept}$ is a linear combination of concept activation vectors from the top K patches, the contribution of a specific concept to the WSI prediction can be written as:\n$K_c = \\sum_{j=1}^{K} w_c f_{jc} \\beta_c$\nThus, the prediction $\\hat{Y}_{concept}$ can be reformulated as the sum of the contributions from each concept, along with a bias:\n$\\hat{Y}_{concept} = \\sigma(\\sum_{c=1}^{C} K_c + b_{concept})$\nFor a WSI prediction $\\hat{Y}_{concept}$, we provide a local explanation consisting of four components, as shown in Fig. 3. The first component is the attention map $\\alpha$ from the image MIL branch, highlighting the top K patches that indicate the key regions influencing the prediction. The second component is a visualization of the top K patches (Fig. 3b), allowing the end user to examine the patches and inspect the image details. The third component includes the concept activation vectors for each patch (Fig. 3c), enabling a detailed exploration of individual concept-based features. Finally, the fourth component is the WSI-level concept contributions (Fig. 3d), where the contribution $K_c$ of each concept to the final prediction can be assessed."}, {"title": "E. Obtaining Global Explanations", "content": "In addition to local explanations for individual samples, our model offers a global explanation of its overall prediction mechanism. Figure 4a and b show the mean WSI-level concept contributions for tumor and normal samples in the entire train set. These mean concept contribution vectors provide a dataset-level perspective on how each concept, on average, influences the tumor or normal prediction. Beyond the WSI-level mean vectors, we can project the concept vectors into 2D space with t-SNE plot [40] to explore how well in the feature space the tumor and normal cases are separated, as shown in Fig. 4c and d. Additionally, we can look into the distributions of a specific concept across tumor and normal cases at both the patch and WSI levels, as illustrated in Fig. 4e. These global explanations provide a systematic approach to understanding the model, allowing for an evaluation of the model's quality and identifying potential directions for further improvement."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "We present results on two widely-used computational pathology datasets: Camelyon16 [2] and PANDA [4]. Camelyon16 is a breast cancer metastasis detection dataset consisting of 270 WSIs in the training set and 129 samples in the test set labeled as tumor or normal. We split the official training set into training and validation subsets with ratios of 0.8 and 0.2, while the official test set was used for evaluation. PANDA is a prostate cancer grading dataset comprising 10,616 digitized prostate biopsies in the official training set. Each WSI is annotated with Gleason scores [12] and International Society of Urological Pathology (ISUP) grades to reflect cancer severity, which can be further mapped to binary class labels of tumor and normal. Since the official PANDA test set is not publicly available, we divided the training set into training, validation, and test subsets with ratios of 0.8, 0.1, and 0.1, respectively. Further, ground-truth tumor masks are available for all WSIs in Camelyon16 and most in PANDA. We used those pixel-level labels to quantitatively evaluate the attention map from the local explanations."}, {"title": "B. Implementation details", "content": "For all WSIs, we generated image and concept features and trained our Concept MIL model as described in Sec. III. We implemented the image MIL branch in our model as attention-based MIL [20]. The image feature projector H(.) was implemented as a Fully Connected Layer (FCL) followed by the activation function ReLU. The attention module AP (.) was a gated attention module following [27]. We used an FCL followed by a sigmoid function for the classification in the image branch. The PAG Top K patch selection module was adapted from [22], [39], and we set K to 20 for models on both datasets. In the concept MIL branch of our model, the attention module $A^{\\mathcal{C}}(.)$ was also implemented as a gated attention mechanism, and the classification layer consisted of a fully connected layer with a sigmoid function. We set percentile $P_{r_\\gamma}$ to 0.75 and temperature t to 3 for scaling the attention scores $\\beta$. And set the $\\lambda$ in loss function (6) to 0.05.\nDuring training, we used a batch size of 1 to fit the WSIs with different bag sizes. The learning rate was set to 0.001 for the Camelyon16 dataset and 0.0001 for the PANDA dataset, with a weight decay of 0.001 applied to both datasets. The model was trained for 300 epochs, and the final model from the last epoch was used for evaluation."}, {"title": "C. Baselines", "content": "We compared our model with Attention MIL [20], CLAM [27], TransMIL [36] and Additive MIL [21]. We used the same image features extracted with CONCH [26] as input for all models. The implementation of the baseline models was based on their official code releases. Unfortunately, a direct comparison to SI-MIL was not possible. As discussed in Sec. II-A, the pathological features used by SI-MIL [22] rely on the output from HoVerNet [16] which cannot be directly applied to the datasets used in this paper. A comparison to the values reported in the SI-MIL publication is also not feasible, as no quantitative evaluations of the local explanations were conducted. Additionally, the reported Jensen-Shannon divergence values in [22] for the global explanations are in a different range and cannot be directly compared to ours.\nWe trained Attention MIL, TransMIL, and Additive MIL for 300 epochs with a learning rate of 0.0002 and weight decay of 0.001. We trained the CLAM multi-branch model for 200 epochs with a learning rate of 0.0001 following the official code release. For all compared models, we used the models from the last epoch for evaluation."}, {"title": "D. Metrics", "content": "We evaluated our model and the baseline models on the classification performance and explanation quality. We used"}, {"title": "E. Classification Performance", "content": "The classification results of all compared models are presented in Table II. Our model achieved classification performance on par with state-of-the-art baselines, demonstrating that it maintains classification performance while offering interpretability. We conducted an ablation study on the two MIL branches by training them separately, using only image features or concept-based features. The ablation results showed that using only a concept MIL branch did not yield strong classification performance, indicating that guidance from the image MIL branch is necessary for effective classification."}, {"title": "F. Evaluations of Local Explanations", "content": "Several MIL models offer local explanations through attention scores-based heatmaps. However, these explanations are often assessed qualitatively. In this work, we conduct both quantitative and qualitative evaluations of local explanations.\n1) Quantative Evaluation: We evaluated the local explanations from our model and baselines using the disease localization score defined in Sec. IV-D. We excluded TransMIL [36] from this evaluation because the official paper lacks details on generating explanations for it. The disease localization scores in Table III show that the top 20 patches selected by the MIL models effectively identified the tumor regions. Our model achieved the highest localization score on the Camelyon16 dataset and the second-best on the PANDA dataset, with an average of 17 out of 20 top patches correctly localized within the tumor region for both.\n2) Qualitative Evaluation: To assess whether the concept-based local explanations align with clinical knowledge, we conducted a user study involving three pathologists.\nWe selected 6 positive, 2 negative, and 2 misclassified samples from both the Camelyon16 and PANDA datasets. For each sample, we provided the pathologists with our model's WSI classification results, the top 20 patches selected by the model (see Fig. 3b), and the concept list (see Table I). The pathologists were asked two questions for each sample. First, they were asked to select the five most significant concepts based on the provided patches. This question aimed to evaluate the alignment between the concepts identified by the pathologists and those predicted by the model. Second, we asked whether the pathologists agreed with the model's classification results, given the top 20 patches, to evaluate if the explanations were useful in identifying the failure cases. This study was conducted independently with three pathologists. The agreement on concepts was quantified by the proportion of common concepts selected by both our model and the pathologists out of the top five concepts.\nTable IV shows that on the Camelyon16 dataset, our model achieved agreements of 0.38, 0.55, and 0.53 with pathologists 1, 2, and 3, respectively. This means that two to three concepts among the top five predicted by our model overlapped with those selected by the pathologists. By looking into the concepts related to tumor cases, we found that \u201cnuclear pleomorphism\u201d, \u201cdisorganized cells", "high nuclear-to-cytoplasmic ratio": "loose chromatin", "hyperchromatic nuclei": "ere the five most frequently used concepts by our model. Meanwhile, pathologists primarily referenced \u201ctumor cells\u201d, \u201cdisorganized cells\u201d \u201cenlarged nuclei\u201d, \u201chigh nuclear-to-cytoplasmic ratio\u201d and \u201cnuclear pleomorphism", "disruption of basal cell layer": "glands infiltrate stroma", "nuclear pleomorphism": "apoptotic cells", "neoplastic cells": "Meanwhile, the most frequently referenced concepts by pathologists included \u201cdisruption of glandular architecture\u201d, \u201cglands infiltrate stroma\u201d, \u201cdisruption of basal cell layer\u201d, \u201cprostate glands with poorly defined borders"}, {"neoplastic cells": "again with three concepts shared between our model and the pathologists.\nAs shown in Table IV, agreement scores among pathologists ranged from 0.48 to 0.65, indicating an average of two to three shared concepts between individual pathologists. This notable variance suggests that pathologists often rely on different sets of concepts for their diagnosis.\nBy reviewing the responses to the second question, in which we asked whether the pathologists agreed with the model's prediction, we found that for each of the four misclassified cases, at least two pathologists expressed uncertainty and suggested further examination was needed. In a false negative case from the Camelyon16 dataset, pathologists noted that the 20 patches selected by the model contained both tumor and normal tissue, therefore disagreeing with the model's \u201cNormal\u201d prediction. In a PANDA false positive case, pathologists pointed out that the selected patches were normal stroma, which did not support a \u201cTumor\u201d prediction. The pathologists' responses to this question indicate that our model's explanations can assist users in identifying failure cases, which is desirable for human-AI collaboration."}, {"title": "G. Evaluations of Global Explanations", "content": "Global explanations offer insights into the model's behavior at the dataset level. After the model's training, we collected the whole slide-level concept contributions of tumor and normal predictions, calculated the mean concept vectors (Fig. 4a, b), and visualized the distributions of both the concept vectors and individual concepts (Fig. 4c, d, e) to assess the overall quality of the model.\n1) Quantative Evaluation: As described in Sec. IV-D, we used the JS divergence and Silhouette score [34] to evaluate the global explanations on class separability. Table V presents the evaluation results for these two metrics at patch and WSI levels, with the JS Divergence score averaged across all concepts. Notably, the Silhouette scores at the WSI level are 0.518 on Camelyon16 and 0.579 on PANDA, which are significantly higher than the patch level scores of 0.216 on Camelyon16 and 0.261 on PANDA. This indicates that the concept contribution vectors at the WSI level are more tightly clustered within the same class and better separated from the opposite class. This conclusion is supported by the t-SNE [40] plots in Fig. 4c and d. In Fig. 4e, we visualize the WSI-level distributions of three concepts across tumor and normal cases, with the corresponding JS divergence noted. These plots demonstrate that the individual concept has distinct distributions between tumor and normal cases."}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "We introduced Concept MIL, an inherently interpretable model for WSI classification that offers pathology concepts as explanations. Our key idea is to use an image MIL branch to support the patch selection for an interpretable concept MIL branch, allowing predictions to be made based on a linear combination of concepts from the 20 most salient patches. The quantitative evaluations show that inherently interpretable models like ours can be performant while offering highly interpretable explanations. Feedback from pathologists indicates that faithful local and global explanations could be helpful for building trust with the end users. Local explanations such as ours, which combine image information from WSI with high-level pathology concepts, allow pathologists to understand how a prediction is made and to investigate suspicious cases further. Global explanations that reflect the entire model's decision mechanism provide a way to assess the model's overall quality. In the future, we plan to expand the user study to a larger group of clinicians to gain deeper insights into the clinical utility of our concept-based explanations.\nSince our model uses a vision language foundation model for both image feature extraction and concept projection, a strong foundation model that effectively aligns the vision and text spaces could enhance its performance. In future work, we plan to explore the integration of pathology concepts at multiple scales, develop automatic methods for concept definition, and extend our model to support multi-class classification."}]}