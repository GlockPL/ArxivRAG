{"title": "Avoiding exp(Rmax) scaling in RLHF through Preference-based Exploration", "authors": ["Mingyu Chen", "Yiding Chen", "Wen Sun", "Xuezhou Zhang"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for large language model (LLM) alignment. This paper studies the setting of online RLHF and focus on improving sample efficiency. All existing algorithms in online RLHF, whether doing passive exploration or active exploration, suffer from a sample complexity that scales exponentially with the scale of the reward function. This fundamental limitation hinders their effectiveness in scenarios with heavily skewed preferences, e.g. questions with a unique correct solution. To address this, we introduce Self-Exploring Preference-Incentive Online Preference Optimization (SE-POPO), an online RLHF algorithm that for the first time achieves a sample complexity that scales polynomially with the reward scale, answering an open problem raised by Xie et al. (2024). Theoretically, we demonstrate that the sample complexity of SE-POPO dominates that of existing exploration algorithms. Empirically, our systematic evaluation confirms that SE-POPO is more sample-efficient than both exploratory and non-exploratory baselines, in two primary application scenarios of RLHF as well as on public benchmarks, marking a significant step forward in RLHF algorithm design. The code is available at https://github.com/MYC000801/SE-POPO.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique in the post-training of Large Language Models (LLMs) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022). Earlier works on RLHF focus primarily on the offline setting (Ouyang et al., 2022; Rafailov et al., 2024), where the preference data are pre-collected and fixed prior to the fine-tuning phase. However, in this setting, the quality of alignment is fundamentally limited by the quality of response in the pre-collected preference dataset (Xiong et al., 2024a). To overcome this limitation, recent works attempt to perform RLHF in an online framework. By continually generating and subsequently labeling new samples during training, online RLHF allow the agents to receive feedbacks on out-of-distribution (OOD) responses, and thus achieving great empirical performance (Dong et al., 2024).\nSimilar to online reinforcement learning, the most critical challenge in online RLHF is how to balance the exploration-exploitation trade-off. In naive online RLHF algorithms (Guo et al., 2024), the exploration is carried out passively, relying solely on the inherent randomness of the LLM policy. Such a passive approach will still fail to sufficiently explore the prompt-response space even with many samples. More recently, a number of active exploration algorithms have been proposed (Dwaracherla et al., 2024; Xiong et al., 2024a; Xie et al., 2024; Cen et al., 2024; Zhang et al., 2024). By leveraging optimism-based approaches to encourage the policy to target OOD regions, active exploration has demonstrated superior performance over passive exploration in both theoretical analysis and empirical evaluations.\nHowever, all existing online RLHF algorithms, whether with passive or active exploration, share a fundamental limitation: They remain effective only in settings with a small reward scale. In particular, under the Bradley-Terry (BT) model assumption, all known sample complexity bounds scales exponentially with the reward range (Xie et al., 2024). Intuitively, this issue arises because human feedback in RLHF is given in the form of preferences rather than explicit rewards. Under the BT model, even if there is a significant gap in rewards between two responses, they may behave very similar in their chance of being preferred preference when pairing with another response. As a result, exponentially many samples are required to distinguish the quality of responses based on preference signals. This leads to the open question raised by Xie et al. (2024):\nDoes there exist a sample-efficient online RLHF algorithm that remains effective under large reward scale?"}, {"title": "2. Related Works", "content": "Theoretical Study on RLHF Theoretical analysis of RLHF has recently emerged as one of the main interests in the community. The earliest study trace back to the dueling bandits literature (Yue et al., 2012; Saha & Gopalan, 2018; Bengs et al., 2021), along with studies considering tabular RL with finite state space (Xu et al., 2020; Novoseller et al., 2020) and linear RL or general function approximation RL with infinite state space (Pacchiano et al., 2021; Chen et al., 2022; Wu & Sun, 2023; Zhan et al.; Das et al., 2024a; Wang et al., 2023). Apart from the online setting, a substantial body of research focuses on offline RLHF (Zhu et al., 2023; Zhan et al., 2023; Ji et al., 2024; Liu et al., 2024), which leverages predetermined offline datasets with appropriate coverage conditions over the state-action space and can be considered complementary to our work. Although these studies offer sample complexity guarantees for RLHF, most algorithms are not scalable enough to be applicable to modern LLMs with large transformer architectures. For instance, (Pacchiano et al., 2021; Das et al., 2024a) incorporate exploration bonuses tailored for linear models in the reward estimation. (Chen et al., 2022; Zhan et al., 2023; Wang et al., 2023) rely on model-based function approximation and explicitly estimate the policy confidence set. These approaches fail to yield efficient or practical algorithms when applied to LLMs.\nExploration for online LLM alignment Exploration in online RLHF has seen rapid development recently. Earlier attempts, such as online DPO (Guo et al., 2024) and iterative DPO (Xu et al., 2023; Dong et al., 2024; Xiong et al., 2024b), primarily rely on passive exploration, i.e. the inherent randomness of LLM policy, and lack explicit mechanisms to encourage diverse and exploratory responses. The importance of active exploration in RLHF has been highlighted by (Dwaracherla et al., 2024). Subsequent works, such as (Ye et al., 2024; Xiong et al., 2024a), propose algorithms with an active exploration mechanism and provide a sample complexity guarantees for online RLHF. However, these exploration strategies involve solving an intractable optimization problem, making them impractical to implement in LLM alignment. Notably, in these works, experiments are often conducted based on heuristic variants of the proposed algorithms, resulting in a significant gap between theory and practice.\nRecent studies (Cen et al., 2024; Xie et al., 2024; Zhang et al., 2024) introduce implementation-friendly and provably sample-efficient exploration algorithms for RLHF, which are most relevant to our work. All three papers are based on the common idea of augmenting the DPO loss with a reward-based optimistic bonus to encourage exploration. Among them, (Zhang et al., 2024; Cen et al., 2024) mainly focus on the exploration under the contextual bandit formulation of RLHF, whereas (Xie et al., 2024) provides analysis for the token-level MDP formulation. However, a significant limitation of these algorithms is that their sample complexity scales exponentially with Rmax, the scale of the reward function (see Asm. 3.2), which is highly inefficient in both theory and practice. Our algorithm becomes the first that remove such exp(Rmax) dependency."}, {"title": "3. RLHF Preliminaries", "content": "In RLHF, we denote a policy by \u3160, which generates an answer y \u2208 Y given a prompt x \u2208 X according to the conditional probability distribution \u03c0(\u00b7|x). Given two responses y and y' with respect to prompt x, we assume a preference oracle, i.e. a human evaluator, will evaluate the quality of two responses and indicate the preferred one. Following prior works, we consider Bradley-Terry model as the preference oracle. The mathematical definition is below.\nDefinition 3.1. (Bradley-Terry (BT) Model) There exists an underlying reward function r* : X \u00d7 Y \u2192 R such that"}, {"title": "4. Preference-based Exploration", "content": "Although existing algorithms based on (5) obtain theoretical sample efficiency guarantees, there is a significant gap between their bounds and what could be achieved under the standard MDP framework. In particular, a key weakness in all existing sample complexity bounds of these algorithms is an exponential dependency on the scale of reward Rmax. This makes existing guarantees quite subtle, as the bound quickly becomes vacuous as soon as Rmax is moderately large. In practical LLM applications, it is common that one response can strictly dominate another, i.e., P*(y > y'|x) \u2192 1. Under the BT model (Definition 3.1), this implies a very large Rmax and thus existing bounds will fail. Authors of prior works have admitted that this is a significant drawback of these results and in fact conjectured that the exponential dependency might be unavoidable (Xie et al., 2024). In this paper, we resolve this conjecture in the negative by presenting the first algorithm that avoids such exponential dependency on the reward scale. In what follows, we start by discussing the cause of exponential dependency on Rmax and why it's a real limitation of existing algorithms rather than merely a result of weak analysis. We then introduce our algorithm SE-POPO and present its theoretical properties."}, {"title": "4.1. The cause of exp(Rmax) scaling", "content": "Using online-to-batch technique, the sample complexity of an online algorithm can be derived from its regret, which is defined by $\\frac{1}{T} \\sum_{t=1}^T SubOpt(\\pi_t)$. In the standard analysis of optimism online RLHF, the regret can be bounded by the sum of reward uncertainty, i.e., $\\sum_{t=1}^T E_{x \\sim \\rho, y \\sim \\pi_t(\\cdot|x)} [[r_t(x, y) - r^*(x, y)]]$, where rt is the induced reward function from \u03c0t as in DPO. To bound the reward uncertainty, prior works reduce it to the preference uncertainty, i.e., $\\sum_{t=1}^T E_{x \\sim \\rho, y \\sim \\pi_t(\\cdot|x), y' \\sim \\pi_t(\\cdot|x)} [|P_t(y > y'|x) - P^*(y > y'|x)|]$, as the preference uncertainty can be effectively bounded using concentration inequalities. Unfortunately, this reduction is not a free lunch: due to the presence of sigmoid function in Bradley-Terry Model, for some x, y, y', there is"}, {"title": "4.2. Algorithm Design", "content": "Given the above intuition, we propose Preference-Incentive Online Preference Optimization with Self-updated Sampler (SE-POPO), which for the first time enjoys a sample complexity bound that scales polynomially with Rmax. Conceptually, SE-POPO differs from prior algorithms in two main aspects: 1) it uses a preference-based exploration bonus instead of a reward-based bonus to explore more efficiently, and 2) it updates the second sampler at intervals instead of fixing it as ref or updating per step. The pseudocode of the algorithms is presented in Algorithm 1 and 2.\nSE-POPO operates over K intervals. In each interval, SE-POPO selects a fixed sampler sam to generate the second response and runs the subroutine POPO for T iterations. The output of POPO is used as the sampler for the next interval, and the output from the final interval serves as the output of SE-POPO. Let us start by the subroutine POPO. As illustrated in Algorithm 2, POPO shares a similar structure with existing optimism RLHF algorithms (Xie et al., 2024; Zhang et al., 2024; Cen et al., 2024). However, unlike prior designs that are tailored towards bounding the reward-based regret, i.e. $\\sum_{t=1}^T SubOpt(\\pi_t)$, POPO is designed to optimize the preference-based regret over sampler sam, i.e., Regpref(sam, T) ="}, {"title": "4.3. Theoretical Guarantees", "content": "Let the regularization parameter \u03b2 > 0 be fixed. We start by a reward realizability assumption, which states that the reward class used in SE-POPO is sufficiently expressive.\nAssumption 4.2. (Reward realizability) There exists a set of reward functions R satisfying r* \u2208 R.\nGiven Assumption 4.2, we define P as the set of preference model induced by R, and define II as the optimal policies induced by R under KL-regularized reward objective (2). Notice that |P| = |R| = |II| by definition. We focus on the linear preference model settings for our theoretical result.\nAssumption 4.3. (Linear preference oracle) Every prefer-"}, {"title": "4.4. A Lightweight Implementation of SE-POPO", "content": "One practical challenge we encounter when implementing SE-POPO is that calculating the gradient of the objective"}, {"title": "5. Experiments", "content": "In this section, we provide a comprehensive empirical evaluation of SE-POPO in LLM alignment tasks. There are two primary use cases for LLM alignments in real practices:\n1. Domain-specific alignment: This is where the goal is to fine-tune LLMs for a specific type of tasks, e.g. fashion design.\n2. Generalist algnment: This is where the goal is to train a general-purpose question answering AI that could answer a wide variety of questions. This is for instance what GPTs are designed for.\nImportantly, in both use cases, the preference feedback during both training and evaluation would have been provided by the same oracle, e.g. human evaluators. In other words, there should not be any distribution shift in the underlying preference model between training and testing. What distinguishes the two use cases is the prompt distribution during training and deployment. For use case 1, the prompts should come from the same domain during both training and deployment, i.e. no distribution shift in the prompt distribution. For use case 2, the prompt distribution between training and testing could be different.\nMotivated by the real use cases discussed above, we present three sets of experiments. For all experiments, our implementation build upon the iterative DPO codebase from (Dong et al., 2024), and we use the 3-iteration online RLHF framework following the setting in (Xie et al., 2024). Across all three experiments, we use Llama-3-8B-SFT as the base model, RLHFlow-ultrafeedback dataset as the training prompt sets, and GRM-Llama3-8B-rewardmodel-ft as the training preference model. More details about the experiment setup are deferred to Appendix H. The results from the three sets of experiments are shown as three columns in Table 1:\n\u2022 \"IID data\" refers to the setting where the models are evaluated on a held-out test prompt set that are drawn from the same distribution as the training prompt set, and the responses are evaluated by the same preference model used during training. This is to simulate use case 1.\n\u2022 \"Alpaca data\" refers to the setting where the models are evaluated on the AlpacaEval 2.0 dataset, but the responses are still evaluated by the same preference model used"}, {"title": "6. Conclusion", "content": "In this work, we propose SE-POPO, the first practical and provably sample-efficient online exploration algorithm for RLHF with a polynomial dependence on the reward scale. In theory, SE-POPO offers a strictly superior sample complexity guarantee compared to existing online RLHF methods, while in practice, SE-POPO is able to match and sometimes outperform existing baselines with either passive or active exploration.\nThere are several open questions raised by our work. Future directions include investigating online exploration algorithms with minimal length exploitation (Singhal et al., 2023; Meng et al., 2024), extending our algorithms to token-level MDP (Xie et al., 2024; Zhong et al., 2024) or multi-turn RLHF settings (Shani et al., 2024; Gao et al., 2024; Xiong et al., 2024b), and supporting general preference models beyond Bradley-Terry model (Munos et al., 2023; Swamy et al., 2024; Ye et al., 2024)."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. We study both of the theoretical formulation and the implied practical algorithmic designs. The proposed algorithms can help better align the strong LLMs with human value and preference, thus making the LLMs more helpful, controllable and contributing to the welfare of society."}, {"title": "A. More Related Works", "content": "RLHF and RLHF algorithms The current RLHF framework was first popularized by (Christiano et al., 2017), which served to direct the attention of the deep RL community to the preference-based feedback. Due to its significant success in LLM alignment (OpenAI, 2022; Touvron et al., 2023), RLHF has gained substantial interest and become one of the prominent research topics in recent years. The most widely adopted and standard RLHF framework, as described in (Ouyang et al., 2022; Touvron et al., 2023), consists of two primary stages: 1) optimizing a reward model using the preference dataset, and 2) refining the LLM policy using PPO (Schulman et al., 2017) based on the optimized reward model. While this RLHF framework has achieved tremendous success in the industry, its adoption by academic and open-source communities is challenging due to the essential limitations of PPO, such as issues with reproducibility (Choshen et al., 2019), hyperparameters sensitivity (Engstrom et al., 2020), and its significant computational resource requirements. Inspired by the limitations of this two-stage approach, a new line of research focuses on single-stage algorithms, including to SLiC (Zhao et al., 2023), DPO (Rafailov et al., 2024), and its variants, such as IPO (Azar et al., 2024), SPPO (Wu et al., 2024), VPO (Cen et al., 2024), XPO (Xie et al., 2024), and SELM (Zhang et al., 2024). These algorithms bypass the reward modeling step and learn a policy by optimizing a designed loss function on the preference dataset directly. It is observed that such algorithms are much more stable than PPO and achieve impressive performance on public benchmarks (Tunstall et al., 2023; Dubois et al., 2024; Zheng et al., 2023)."}, {"title": "B. Proof of Theorem 4.4", "content": "This proof is adapted from the proof of Theorem 1 in (Cen et al., 2024). By the definition of G, there is"}, {"title": "Bounding TERM 1", "content": "Notice that in objective (8), \u03c0t is completely dependent on rt. In this regard, the function G can be considered as a function that depends only on the reward. By the choice of rt, we have"}, {"title": "Bounding TERM 2", "content": "The proof completely follows that in (Cen et al., 2024). For completeness, we provide a rewritten version here. By Assumption 4.3, we can rewrite TERM 2 into"}, {"title": "F. Proof of Auxiliary Lemmas", "content": "The proof refers to the proof of Lemma 2 in (Cen et al., 2024). To begin with, there is"}, {"title": "G. Generalization beyond linear preference oracle", "content": "In this section, we extend Theorem 4.4 from the linear preference oracle setting to a more general preference oracle. To do this, we introduce a general complexity measure\u2014 preference-based generalized eluder coefficient (PGEC)\u2014which aligns with the complexity measures definitions in prior works (Xie et al., 2024; Zhang et al., 2024).\nDefinition G.1. (Preference-based GEC) Given the reward class R, we define the preference-based Generalized Eluder Coefficient (PGEC) as the smallest dPGEC such that for any sequence of policies \u03c0\u2081,..., \u03c0\u03c4 \u2208 II and rewards r1,...,ry \u2208 R"}, {"title": "H. Experiments Details", "content": "The experiments were conducted on 4 x Nvidia A100 80G GPUs. The pseudocode of our algorithm's implementation is illustrated in Algorithm 3. In the implementation, we set sam = t and use the chosen responses to simulate the on-policy responses. To accelerate training, following (Dong et al., 2024), we do not restart from the initial model at each iteration but use the last-iteration model as the initial checkpoint. Moreover, following Zhang et al. (2024), we update ref ="}]}