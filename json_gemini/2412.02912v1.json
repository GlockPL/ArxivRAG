{"title": "ShapeWords: Guiding Text-to-Image Synthesis with 3D Shape-Aware Prompts", "authors": ["Dmitry Petrov", "Pradyumn Goyal", "Divyansh Shivashok", "Yuanming Tao", "Melinos Averkiou", "Evangelos Kalogerakis"], "abstract": "We introduce ShapeWords, an approach for synthesizing images based on 3D shape guidance and text prompts. ShapeWords incorporates target 3D shape information within specialized tokens embedded together with the input text, effectively blending 3D shape awareness with textual context to guide the image synthesis process. Unlike conventional shape guidance methods that rely on depth maps restricted to fixed viewpoints and often overlook full 3D structure or textual context, ShapeWords generates diverse yet consistent images that reflect both the target shape's geometry and the textual description. Experimental results show that ShapeWords produces images that are more text-compliant, aesthetically plausible, while also maintaining 3D shape awareness.", "sections": [{"title": "1. Introduction", "content": "Recent advances in generative image models based on diffusion [2, 17, 36-38, 41, 44] have made it possible to generate impressive imagery from input text prompts. A challenge in text-to-image models has been to provide users with fine-grained control over shapes or forms in the synthesized images, which can be difficult to convey through text descriptions alone. To address this, conditioning methods have been proposed, such as ControlNet [51] and IP-adapter [48], that aim to capture the desired shape or form more explicitly through the use of edge or depth maps as input conditions.\nDespite these advancements, current text- and image-conditioned synthesis approaches still face a number of challenges. First, they often struggle to balance both textual and visual conditions, when text describes a particular context that should be combined with the target shape to guide an image synthesis (Figure 1, top row). Second, commonly used visual conditions such as edge or depth maps are limited to a single viewpoint, resulting in a loss of valuable 3D shape information when users seek image variations of an underlying shape from different poses. Third, even when these models accurately reflect the target shape in specific views, users may want to explore shape variations \u2013 yet current models often lack flexible controls for such exploration.\nTo overcome these challenges, we propose ShapeWords, a method designed to generate images that faithfully adhere to both the text prompt and a target 3D shape geometry,"}, {"title": "2. Prior work", "content": "Conditional Diffusion. Denoising diffusion models [17, 44] have revolutionized image synthesis by generating high-quality, diverse, and plausible image content. To control the generation process, the most common condition is text. Popular diffusion approaches, such as Stable Diffusion [38], DALLE [2, 36, 37], Imagen [41] have used large-scale text-image datasets, including pretrained LLMs [6, 11, 35] for text-to-image synthesis. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by diffusion models, especially when flexible and accurate control is needed in terms of form or layout. To this end, ControlNet [51] pioneered explicit conditioning through visual signals like depth or edge maps, enabling spatial control but limited to single viewpoints. T2I-Adapter [27] proposed a lighter-weight alternative through specialized adapters of visual control signals. UniControl [33] consolidated a wide array of controllable condition-to-image tasks within a single framework. IP-Adapter [48] proposed a decoupled cross-attention strategy for text features and image features from input conditions for more accurate controllable generation. In general, these methods rely on view-dependent control signals (e.g., depth, edge maps), lack explicit 3D geometry awareness, and may fail to generate images adhering to both the text prompt and target 3D shapes. In contrast, ShapeWords enables view-independent shape control, while adhering well both to target shape geometry and text prompts.\nStructure guidance. Several approaches have explored novel view synthesis aimed at maintaining some level of 3D awareness for view consistency [15, 23-25, 46]. However, these methods do not disentangle 3D structure from appearance and do not provide geometric control to users, meaning they generate images of a shape with consistent appearance across different views without user-driven control over geometry. A number of approaches have been proposed to incorporate structure guidance from depth maps or coarse 3D primitives disentangled from appearance. FreeControl [26] explores structure guidance in diffusion feature subspaces extracted from depth maps and other visual conditions. Ctrl-X [22] investigated more efficient, disentangled and zero-shot control of structure and appearance. LooseControl [4] introduced more flexible conditioning schemes through 3D scene boundary control, 3D box control and attribute editing. Diffusion Handles [29] enabled localized control of 3D object parts in diffusion models by introducing deformation handles to edit images while maintaining consistent perspective and structure across views. In our approach, we take a different route by embedding 3D shapes into tokens within text prompts, providing more explicit geometry guidance in image synthesis and combining it with additional specification of separate context, style, and appearance constraints through text.\nGuidance from learnable tokens and concept learning. Several methods have explored embedding concepts into textual tokens for personalized image synthesis using textual inversion [13]. The required optimization for textual inversion is often very slow, thus various methods have explored more efficient fine-tuning strategies [19], feed-forward architectures to predict textual tokens [21, 43], or using hypernets [39]. Recently, more efficient methods of visual concept learning were introduced [1, 12, 14, 40] allowing for more efficient learning and transfer of visual concepts. A more spiritually similar approach to ours is that of \"continuous 3D words\" [9], which embeds 3D-aware attributes, such as time-of-day lighting, bird wing orientation, dolly zoom effects, and object pose, into learnable tokens. Viewpoint textual inversion[7] learns 3D view tokens that can be used to control the viewpoint for image synthesis. However, unlike these methods, our approach learns to embed 3D shapes directly into tokens, enabling image generation that is guided by both target 3D shape geometry and text. To our knowledge, text-to-image synthesis through"}, {"title": "3. Method", "content": "Overview. Given a text prompt p and a target 3D shape S, ShapeWords generates an image that reflects both the textual description and the desired shape. Users indicate the desired shape directly within the prompt using a special token, such as \"a red [SHAPE-ID] on a beach\", where \"SHAPE-ID\" corresponds to a shape, e.g., one imported from a 3D shape database.\nThe pipeline (illustrated in Figure 2) proceeds as follows at test time. First, a shape representation is extracted from the chosen 3D shape using a pre-trained transformer, Point-BERT [49]. The text prompt is mapped into CLIP space through an OpenCLIP encoder [10], where the token \u201cSHAPE-ID\" is replaced by a category name for the shape (e.g., \"chair\"). Our method then applies a new module, named Shape2CLIP, trained to modify the prompt's word embeddings, including the shape identifier token, so that the resulting prompt embedding integrates the desired 3D shape geometry in the prompt while preserving its original textual context. This shape-enhanced embedding is passed to a Stable Diffusion model [38], along with a user-defined parameter that controls the degree of 3D shape influence, to synthesize images consistent with both the textual and 3D shape cues.\nIn the following sections, we discuss preprocessing of the input shapes and text prompts to extract Point-Bert [49] and OpenCLIP [10] representations respectively (Section 3.1), then we discuss the Shape2CLIP module (Section 3.2), our inference pipeline (Section 3.3), and finally its training procedure (Section 3.4)."}, {"title": "3.1. Preprocessing", "content": "Point-BERT. To represent the input 3D shape, we use a pre-trained Point-BERT [49], a transformer-based model for point clouds. The input shape is first converted into a point cloud of 1024 points using farthest point sampling, then passed through the Point-BERT architecture, which partitions the cloud into 64 patches. These patches are encoded by PointNet [32] and a transformer based encoder as a set of tokens representing the geometry of the shape. The resulting shape embedding, $B \\in \\mathbb{R}^{65 \\times 384}$, consists of 65 tokens representing both patches and a class token in a 384-dimensional space. Trained via masked modeling for self-supervised learning, Point-BERT effectively captures structure-aware features demonstrated in various shape processing applications, such as part segmentation.\nOpenCLIP space. As our base generative model, we use Stable Diffusion 2.1 whose text encoder is OpenCLIP ViT-H/14 [10]. Thus, we process the input textual prompt p, through this OpenCLIP encoder to generate a sequence of 77 token embeddings capturing the words in the prompt and their context in the prompt:\n$T = [t_0, t_1, ..., t_{EOS}, ..., t_{PAD}]$ (1)\nwhere $t_i$ represents the encoded embedding of the j-th token in the prompt; $t_{EOS}$ represents end-of-sequence (EOS) token which captures the context of the whole prompt; and $t_{PAD}$ represent padding token embeddings that pad sequence after EOS tokens to some predefined length (e.g. 77)."}, {"title": "3.2. Shape2CLIP module", "content": "Given a text prompt embedding T and shape representation B, our Shape2CLIP module generates a modified"}, {"title": "3.3. Guided Diffusion", "content": "The modified embedding T' can be directly used as input to a diffusion model to generate images, formulated as $I = D(z, T')$, where z in the latent space and D represents Stable Diffusion 2.1 [38] in our implementation. Notably, our method does not require any additional shape conditions (e.g., depth, normal, or edge maps).\nShape Guidance. Our method enables users to control the influence of the 3D shape, allowing for image variations that deviate from the target shape. This flexibility is valuable when the exact shape a user has in mind does not exist in any available 3D shape databases, deeming more necessary to explore variations of existing shapes. The shape influence is modulated by a parameter $\\lambda \\in [0, 1]$ using a linear interpolation scheme to adjust the embedding as follows:\n$T'[s, e] = T[s, e] + \\lambda \\cdot \\delta T(B, T; \\theta)$ (5)\nAs shown in our results, varying $\\lambda$ from 0 to 1 gradually shifts from disregarding the shape influence to incorporating it in the generated images. Intermediate values yield plausible images, while higher $\\lambda$ values produce shapes in the generated images that increasingly match the desired target shape."}, {"title": "3.4. Training", "content": "Our training procedure aims to learn the parameters $\\theta$ of the Shape2CLIP module, keeping the rest of the pipeline components fixed (i.e., PointBERT encoder, text encoder, image encoder/decoder, and denoising network).\nTraining dataset. To train the Shape2CLIP module, we constructed a dataset of shape-prompt-image triplets based on ShapeNet [8] reference shapes. Images were generated using ControlNet [51] conditioned on ShapeNet's depth maps, as provided by the ULIP authors [47], where each shape is rendered from 30 viewpoints, rotated in 12-degree increments around the vertical axis with fixed elevation (see [47] for details).\nFor each depth image, we applied a randomly selected prompt from a set of 13716 prompts for additional Control-Net conditioning, created from a base set of 100 prompts, then augmented with variants produced by ChatGPT [28]. To achieve diversity and structural agnosticism in prompts, this set was created by combining 127 artistic mediums (e.g., \"painting,\u201d \u201cwatercolor,\u201d \u201csketch\u201d) with 108 style adjectives (e.g., \u201ccolorful,\u201d \u201cpixelated,\u201d \u201cfantasy\u201d), deliberately avoiding references to specific 3D structures to help the model learn generalizable mappings instead of overfitting to particular geometries or appearance combinations. To reduce bias and enhance background diversity, we used the Stable Diffusion's inpainting model [38] to modify the backgrounds while preserving the foreground objects. The above procedure resulted in a diverse dataset of 1.58M prompt-image pairs (30 per each ShapeNet shape), generated from all depth images in the training split from 3DILG [50] in ShapeNet. As demonstrated in Section 4, although our method is trained on data generated by ControlNet, ShapeWords exhibits strong generalization capabilities. Notably, ShapeWords achieves significantly better performance on compositional prompts, a setting that ControlNet struggles to handle effectively. The dataset will be publicly released along with our source code upon acceptance.\nSDS-based training. We train our Shape2CLIP model using the Score Distillation Sampling (SDS) loss [31]. Specifically, for sampled noise $\\epsilon_{t,i} \\sim \\mathcal{N}(0, I)$ for a training image i at step t, we use the pretrained Stable Diffusion"}, {"title": "4. Evaluation", "content": "We now discuss our evaluation and experiments to test the effectiveness of ShapeWords compared to alternatives.\nEvaluation goals & metrics. The evaluation aims to assess three main aspects:\n(a) Prompt Adherence: We evaluate how well the images generated by each competing method align with the given prompt. We use the standard score of CLIP similarity [34].\n(b) Shape Adherence: We assess how well the generated images from each method adhere to a reference 3D shape. To this end, we compare the shapes in the generated images with the reference 3D shapes based on their silhouette. To extract silhouttes from reference 3D shapes, we render them from a target pose, extract the silhouette, then we compare it with the silhouette of the shape in the generated images conditioned on the target pose. The silhouette from generated images are extracted through the foreground detection model [52]. The similarity between silhouettes is measured using standard geometric similarity metrics, specifically Intersection over Union (S-IOU) and Chamfer Distance (S-CD), averaged per shape across six uniform views sampled with 60 degree increments, starting at 0.\n(c) Image Plausibility: We evaluate the aesthetic quality of the generated images using the aesthetics score (Aes.) [42]. We also include the commonly used image quality generation metrics of FID [16] and KID [5]. Given reported issues with Inception-based features [20, 30], we compute FID and KID on CLIP features as recommended in [3].\nTest Datasets. We have designed two datasets for evaluation to test different properties of our model:\n(a) Simple Prompts Dataset: This dataset contains \"simple prompts\" with text structured as \"a photo of a [SHAPE-ID],\" where the [SHAPE-ID] token corresponds to a 3D shape from the ShapeNet test splits from all 55 categories (2592 test shapes). This dataset is particularly useful for evaluating shape adherence through geometric similarity of extracted silhouettes (using S-CD and S-IOU metrics), as the generated images are photo realistic and feature a single object, so the foreground detection methods perform reliably here.\n(b) Compositional Prompts Dataset: This dataset serves as our main evaluation set since it is more challenging, containing \"compositional prompts\" that involve a target 3D"}, {"title": "5. Conclusion", "content": "We have presented ShapeWords, the first to our knowledge method that allows for geometric manipulation of text-to-image models via mapping of 3D shapes into the space of"}, {"title": "6. Implementation details", "content": "6.1. Data generation details\nFor depth images, we used the inverted ShapeNet data provided by the ULIP authors [47]. As stated in the main paper, for each depth image, we applied a randomly selected prompt from a set of 13, 716 prompts for ControlNet conditioning. The ControlNet-based generation was done for 50 steps with control strength of 2. To promote adherence to shape geometry and reduce appearance biases during training, we additionally used the Stable Diffusion 2.1 inpainting model [38] for 50 steps to modify the backgrounds while preserving the foreground objects. The inpainting strength was set to 0.5.\n6.2. SDS weighting function\nFor the SDS optimization loss of Eq. 6, we use the weighting function W (t), proposed by DreamTime [18], that enhances training stability:\n$W(t) = \\frac{1}{Z} \\frac{1-\\hat{\\alpha}_t}{\\hat{\\alpha}_t} exp\\left(-\\frac{(t-m)^2}{2 s^2}\\right)$,\nwhere m and s are hyperparameters controlling the weight distribution at each time step; $\\hat{\\alpha}_t$ is the noise scale for step t; and Z is a normalization constant ensuring that the weights sum to one over all timesteps. We set m = 500 and s = 250, which provide a good balance between high-frequency details (fine geometry) and low-frequency details (coarse geometry).\n6.3. Training\nWe trained the model for 55 epochs on four NVIDIA A5000 GPUs with batch size 24 per GPU. The learning rate was set to 0.0005 with 1,000 warm-up steps to help stabilizing the training process. Similarly to textual inversion pipelines, we randomly crop and resize the training images to prevent overfitting of the model to spatial positions. The maximum scale of the crop was set to 0.8.\nDuring training, the guidance prompt delta $\\delta T$ is applied to all 77 word embeddings (padding was set to max sequence length). We empirically found that this strategy during training helps the model to better generalize compared to adding the guidance delta to the object and EOS tokens only. We suspect that the usage of deltas on all token embeddings during training helps the model to diffuse training appearance biases across all tokens, which in turn reduces the overall appearance biases distilled in the object and EOS tokens."}, {"title": "7. Running times", "content": "We note that ShapeWords and ControlNet-based baselines rely on the same Stable Diffusion model (Stable Diffusion 2.1 base) and have similar computation requirements at test time: given a text prompt or/and depth image, it takes a few seconds to generate an image with 100 diffusion steps on a single GPU: 6.79s for ControlNet; and 5.00s for Stable Diffusion 2.1 with ShapeWords. The forward pass of the Shape2CLIP module takes 0.003s with pre-computed"}, {"title": "8. Token replacement strategies", "content": "We qualitatively compare token replacement strategies in Figure 10 at test time. Adding the guidance prompt delta $\\delta T$ to all tokens in the prompt yields overly smooth images that do not adher well to stylistic cues provided in the text or the target geometry. Adding $\\delta T$ only to object token without addition to the EOS token results in good geometry but still poor adherence to the stylistic cues in the prompt. Conversely, modifying only the EOS token results in good stylistic adherence but poor geometry. The strategy described in the main text, which is to add $\\delta T$ to both the object and EOS tokens, yields the best balance of textual and target shape adherence."}, {"title": "9. Additional quantitative results", "content": "We provide additional quantitative results in Table 2. Our model consistently outperforms ControlNet-Stop@K variants in terms of aesthetic score. In terms of CLIP score, we outperform all ControlNet-Stop@K variants, except for ControlNet-Stop@30 that matches the CLIP score of our method. Yet, as we discussed in our experiments in the \"simple prompts dataset\" as well as our perceptual user study in the \"compositional prompts dataset\", this variant severely underperforms in terms of shape adherence compared to our method. According to our user study, it also underperforms with respect to textual cue matching, when this is evaluated perceptually."}, {"title": "10. Additional qualitative results", "content": "We provide additional qualitative results for shape and prompt adherence in Figures 13 and 14, respectively."}, {"title": "11. Failure cases", "content": "We observed that the failure cases for our model fall in two modes. First, it struggles with capturing details of challenging fine-grained geometry (Figure 11). In such cases, ShapeWords correctly captures coarse shape structure but struggles to reproduce fine geometric details. Our hypothesis is that the geometric precision of ShapeWords is likely to be bound by the image resolution of OpenCLIP model (ViT-H/14, 224px) which we used to train ShapeWords, and the ability of PointBert to capture such fine-scale geometric details. Training ShapeWords with variants of CLIP of higher resolution might yield better geometric precision \u2013 we consider that this is a promising direction for future work.\nSecond, our model struggles to generalize to largely out-of-distribution text prompts. We illustrate this issue in Figure 12. For example, the prompt 'an origami of a chair' requires both adjustment of texture and local geometry. Our model struggles to do both, especially for high values of guidance strength. We think this issue arises from a combination of two factors: a) our set of prompts is biased towards smoother appearances (e.g. 'photo', 'sketch', 'illustration'), b) our supervisory images come from ControlNet that also tends to produce smooth surfaces following depth maps. However, results for intermediate guidance strength suggest that our model can still generalize to such prompts to some extent. We suspect that this issue could potentially be alleviated by using more diverse training data."}]}