{"title": "Average-Over-Time Spiking Neural Networks for Uncertainty Estimation in Regression", "authors": ["Tao Sun", "Sander Boht\u00e9"], "abstract": "Uncertainty estimation is a standard tool to quantify the reliability of modern deep learning models, and crucial for many real-world applications. However, efficient uncertainty estimation methods for spiking neural networks, particularly for regression models, have been lacking. Here, we introduce two methods that adapt the Average-Over-Time Spiking Neural Network (AOT-SNN) framework to regression tasks, enhancing uncertainty estimation in event-driven models. The first method uses the heteroscedastic Gaussian approach, where SNNs predict both the mean and variance at each time step, thereby generating a conditional probability distribution of the target variable. The second method leverages the Regression-as-Classification (RAC) approach, reformulating regression as a classification problem to facilitate uncertainty estimation. We evaluate our approaches on both a toy dataset and several benchmark datasets, demonstrating that the proposed AOT-SNN models achieve performance comparable to or better than state-of-the-art deep neural network methods, particularly in uncertainty estimation. Our findings highlight the potential of SNNs for uncertainty estimation in regression tasks, providing an efficient and biologically inspired alternative for applications requiring both accuracy and energy efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Uncertainty estimation plays a pivotal role in machine learning, especially in high-stakes applications such as autonomous systems, healthcare, and financial forecasting, where the confidence in predictions is as vital as the predictions themselves [1]. In these high-stake models, uncertainty is often represented through probabilities, with high-quality uncertainty estimation ensuring that predicted probabilities accurately reflect the true likelihood of a prediction being correct [2]. While classification models typically provide a probability distribution over discrete classes, which facilitates uncertainty estimation, regression models produce continuous outputs, making uncertainty estimation significantly more challenging [3]. In deep neural networks (DNNs), uncertainty estimation has been extensively studied for both classification and regression tasks, thereby enhancing model reliability and robustness [4], [5]. However, the methods developed for uncertainty estimation in DNNs are generally not directly applicable to event-driven neural networks, such as spiking neural networks (SNNs) [6].\nInspire by biological computation, SNNs are increasingly gaining attention for real-time, resource-constrained applications [7]. The sparse and asynchronous event-driven nature of SNNs makes them highly suitable for tasks requiring low-latency decision-making and energy-efficient processing [8]. However, these characteristics also pose considerable challenges for integrating uncertainty estimation techniques originally developed for DNNs. For DNNs, Monte Carlo dropout (MC-dropout) [9] and deep ensembles [3] are state-of-the-art uncertainty estimation methods that have demonstrated high-quality uncertainty estimation. Both MC-dropout (Fig. 1a) and deep ensembles (Fig. 1b) rely on multiple forward passes through their network(s), making inference inefficient when directly applied to SNNs [6]. This inefficiency stems from the need for SNNs to propagate through all their layers multiple times to achieve accurate predictions for each input. Moreover, uncertainty estimation for regression tasks faces additional challenges due to the absence of existing solutions for SNN-based regression and the inherent difficulty of event-based SNNs to create continuous outputs, further complicating the process.\nPrevious research has examined uncertainty estimation for SNNs using MC-dropout in classification tasks [6]. As illustrated in Fig 1c, the Average-Over-Time SNN (AOT-SNN) framework [6] exploits the temporal processing capabilities of SNNs to reduce the computational cost of multiple forward passes, while still maintaining precise uncertainty estimation. Yet, unlike DNNs, SNNs inherently generate event-based discrete outputs rather than continuous ones, which adds a layer of complexity to adapt the AOT-SNN framework and other DNN-based uncertainty estimation techniques to regression tasks.\nFor classification tasks, DNNs typically compute a value for each class, which is subsequently converted into a normalized probability using the softmax function. In regression, estimating uncertainty is more challenging because a regular regression model only predicts a point estimate of the target variable as the outcome, rather than a probability distribution conditioned on an input. Deep ensembles [3], often regarded as 'the gold standard for accurate and well-calibrated predictive distributions' [10], address this challenge by estimating both the mean and variance for a given input and subsequently generating a conditional probability distribution (CPD) of the target variable, assuming a heteroscedastic Gaussian distribu-"}, {"title": "II. BACKGROUND", "content": "In this section, we first formalize the regression problem from a probabilistic perspective. Then, we briefly introduce two approaches for probabilistic regression: the heteroscedastic Gaussian approach [3], [11], and the RAC approach [13].\n\nA. Problem Setup\nA regression task aims to predict a continuous target variable y given an input vector x. Formally, let $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ denote a dataset of N independent observations, where $x_i \\in \\mathbb{R}^d$ represents the d-dimensional vector for the i-th observation, and $y_i \\in \\mathbb{R}$ is the corresponding target value. A regression model learns a mapping function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ that minimizes a loss function, typically the mean squared error (MSE):\n$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2$.", "latex": ["\\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2$"]}, {"title": "B. Regression with the heteroscedastic Gaussian assumption", "content": "The constant variance assumption in the homoscedastic Gaussian distribution of the MSE loss may not be satisfied in many real regression problems. To tackle this issue, as depicted in Fig. 2a, a neural network is commonly employed to compute two values: the mean $\\mu(x)$ and the variance $\\sigma^2(x)$ of the target variable y [11]. In this framework, $\\mu(x)$ and $\\sigma^2(x)$ are assumed to follow heteroscedastic Gaussian distributions (Fig. 2b). The CPD of the corresponding target value y can be written as:\n$p(y|x) = \\mathcal{N}(\\mu(x), \\sigma^2(x))$\nThe NLL loss used to train such a regression model is\n$\\text{NLL} = \\frac{1}{2} \\left(\\log(2\\pi\\sigma^2(x)) + \\frac{(y - \\mu(x))^2}{2\\sigma^2(x)}\\right)$", "latex": ["p(y|x) = \\mathcal{N}(\\mu(x), \\sigma^2(x))", "\\frac{1}{2} \\left(\\log(2\\pi\\sigma^2(x)) + \\frac{(y - \\mu(x))^2}{2\\sigma^2(x)}\\right)"]}, {"title": "C. Regression-as-Classification (RAC)", "content": "As shown in Fig. 2c, the RAC approach [13] discretizes the range space $[Y_{min}, Y_{max}]$ of the target variable in a regression task into K equal-sized intervals, called bins. The boundaries and midpoints of bins are written as $b_1, b_2, \\dots, b_{k+1}$ and $m_1, m_2, \\dots, m_k$, respectively, with\n$m_k = \\frac{b_k + b_{k+1}}{2}$\nTreating each bin as a class $C_k$, where $k = 1, 2, \\dots, K$, a regression problem can be reformulated as a classification problem. The continuous target variables can be converted into discrete class labels $\\tilde{y}_i = \\text{argmin}_j |y_i - m_j|$, where the smallest j is taken in case of ties. By minimizing the cross-entropy loss, a neural network can be trained for the reformulated classification problem with the discretized dataset $\\tilde{\\mathcal{D}} = \\{(x_i, \\tilde{y}_i)\\}_{i=1}^N$. The RAC approach facilitates uncertainty estimation, as RAC models yield a normalized CPD for each input, written as $p_1, p_2, \\dots, p_K$, via the Softmax function. The predicted target value can be computed as the expected value over those probabilities $y^*(x) = \\sum_k p_k m_k$.", "latex": ["m_k = \\frac{b_k + b_{k+1}}{2}", "\\sum_k p_k m_k"]}, {"title": "D. Quality of Uncertainty Estimation", "content": "Calibration measures statistical compatibility of predictive probability distributions and real frequencies. Usually, calibration is taken as the indication of uncertainty quality [3]. A class of metrics to measure calibration is referred to as proper scoring rules [20], which include the Brier score (BS) and negative log-likelihood (NLL). When evaluating the quality of probabilities, an optimal score output by a proper scoring rule indicates a perfect prediction, which means that it is not generated by trivial solutions. Note that proper scoring rules are usually taken as the loss function to train models. We evaluate model performance in terms of the Root Mean Squared Error (RMSE) and NLL. RMSE measures the accuracy of the predicted values, while NLL assesses the quality of the uncertainty estimates."}, {"title": "III. METHOD", "content": "In this section, we introduce the AOT-SNN method and explain how we apply it to the heteroscedastic Gaussian regression model and the RAC model.\n\nA. AOT-SNN Method and Quality of Uncertainty Estimation\nSNNs typically use similar types of network topologies as DNNs, but their computation is distinct. SNNs employ stateful and binary-valued spiking neurons, as opposed to the stateless, analog-valued neurons of DNNs. Consequently, unlike the synchronous computation in DNNs, inference takes the form of an iterative process through multiple time steps $t = 0, 1, ..., T$ in typical SNN models. During each time step t, the membrane potential of a spiking neuron $U_t$ is influenced by the impinging spikes from connected neurons emitted at time step t - 1 and the previous membrane potential $U_{t-1}$. When the membrane potential $U_t$ reaches a threshold $\\theta$, the neuron emits a spike. This sparse, asynchronous communication between connected"}, {"title": "B. AOT-SNN over the heteroscedastic Gaussian model", "content": "As illustrated in the Fig. 1d, each time step of an AOT-SNN regression model with the heteroscedastic Gaussian assumption has two outputs, representing the the mean $\\mu_t(x)$ and the variance $\\sigma_t^2(x)$ of the target variable, respectively. In implementation, a readout integrator layer that has two non-spiking neurons is used as the output layer, as in [21]. As such, the membrane potentials of these two neurons are taken as $\\mu_t(x)$ and $\\sigma_t^2(x)$. The predicted mean and variance for an sample x are given by $\\mu^*(x) = \\frac{1}{T}\\sum_{t=1}^T\\mu_t(x)$ and $\\sigma^2(x) = \\frac{1}{T} \\sum_{t=1}^T (\\sigma_t^2(x) + \\mu_t^2(x)) - \\mu^*(x)$.", "latex": ["\\frac{1}{T}\\sum_{t=1}^T\\mu_t(x)", "\\frac{1}{T} \\sum_{t=1}^T (\\sigma_t^2(x) + \\mu_t^2(x)) - \\mu^*(x)"]}, {"title": "C. AOT-SNN over RAC", "content": "1) Predictive value of the target variable: As discussed previously, the RAC approach assigns a probability $p_i$ to each discretized bin, where $i = 1, 2, \\dots, K$. For uncertainty estimation, however, we need a probability distribution $p(y|x)$ that spans the entire range of the continuous target variable y."}, {"title": "D. Network configuration", "content": "Similar to AOT-SNN over the heteroscedastic Gaussian model, the output layer consists of a readout integrator layer with K non-spiking neurons, corresponding to K classes. At each time step, the membrane potentials of these non-spiking neurons are treated as logits, which are then converted to probabilities using the Softmax function. The final prediction is obtained by averaging the probabilities across all time steps, as in AOT-SNNs for classification [6]."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we describe the experimental evaluation of the proposed AOT-SNN methods for regression. We validate our approaches on both a toy dataset and the standard benchmark regression datasets to assess their uncertainty estimation capabilities in comparison with DNN methods.\n\nA. Toy Dataset\nThe toy dataset consists of 100 training examples randomly drawn from $y = x^3 + \\epsilon$, where $x \\in [-4, 4]$ and $\\epsilon \\sim \\mathcal{N}(0, 3^2)$. We have 100 evenly spaced numbers over [-4,4] for the testing examples. Both the regression with a heteroscedastic Gaussian assumption (referred to as the Gaussian approach) and the RAC approach were applied to the MC-dropout-based AOT-SNNs. Regular DNNs, used as a baseline, and the SNNs each had a single hidden layer of 100 neurons. The DNNs employed the ReLU activation function. While Gaussian models were trained using the NLL loss, RAC models were trained using the distance loss with $\\tau = 1$, optimized via the grid search. Both SNN models set their dropout rate as 0.05. The neuron model used was PLIF [22], where the time constant is learned and shared within the same layer. The number of time steps for the SNN models was set"}, {"title": "B. Benchmark Datasets", "content": "To compare the predictive performance of our AOT-SNN models with other state-of-the-art methods, we conducted experiments on the standard benchmark datasets from the UCI Machine Learning Repository. The dataset size (N) and input dimensionality (Q) are provided in Tables I and II. Each dataset was split into 20 train-test folds, except for the protein dataset, which was divided into 5 folds. For each fold, we selected the dropout rate from the set [0.005,0.01, 0.05, 0.1] that performed best in terms of NLL on the validation set, consistent with the approach in [1]. In AOT-SNNs, we used a network with a hidden layer of 200 PLIF neurons. The Gaussian models were trained for 600 epochs with the NLL loss, while the RAC models were trained for 200 epochs using the distance loss with $\\tau = 1$. For the RAC models, we set K = 50 for the output layer, following the choice made in [14] for the Concrete Strength and Energy Efficiency datasets. We found that this value provided satisfactory performance on these benchmark datasets. Further refinement of this hyperparameter could be explored to potentially enhance model performance for each dataset.\nThe RMSE and NLL results for the benchmark datasets are summarized in Tables I and II, respectively. The RAC-based AOT-SNN models consistently demonstrate strong performance in terms of RMSE, with significant improvements observed for the Concrete Strength, Energy Efficiency, and Protein Structure datasets. Meanwhile, the AOT-SNN Gaussian model performs comparably to MC-dropout and Deep"}, {"title": "V. CONCLUSION", "content": "In this paper, we presented two approaches for adapting the AOT-SNN framework to regression tasks: one based on the assumption of heteroscedastic Gaussian distributions and the other leveraging the RAC approach. Our results demonstrate that both approaches achieve strong performance in terms of uncertainty estimation, often surpassing traditional DNN-based methods in RMSE and NLL metrics, particularly in the case of RAC-based models. By exploiting the temporal dynamics of spiking neurons and incorporating uncertainty estimation through the AOT-SNN framework, our proposed methods provide a novel, energy-efficient solutions for uncertainty estimation in regression tasks. Future work will focus on further optimizing these models for hardware implementations, thereby enabling practical deployment in real-time, resource-constrained environments."}]}