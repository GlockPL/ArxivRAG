{"title": "Reconstructing 3D Flow from 2D Data with Diffusion Transformer", "authors": ["Fan Lei"], "abstract": "Fluid flow is a widely applied physical problem, crucial in various fields. Due to the highly nonlinear and chaotic nature of fluids, analyzing fluid-related problems is exceptionally challenging. Computational fluid dynamics (CFD) is the best tool for this analysis but involves significant computational resources, especially for 3D simulations, which are slow and resource-intensive. In experimental fluid dynamics, PIV cost increases with dimensionality. Reconstructing 3D flow fields from 2D PIV data could reduce costs and expand application scenarios. Here, We propose a Diffusion Transformer-based method for reconstructing 3D flow fields from 2D flow data. By embedding the positional information of 2D planes into the model, we enable the reconstruction of 3D flow fields from any combination of 2D slices, enhancing flexibility. We replace global attention with window and plane attention to reduce computational costs associated with higher dimensions without compromising performance. Our experiments demonstrate that our model can efficiently and accurately reconstruct 3D flow fields from 2D data, producing realistic results.", "sections": [{"title": "Introduction", "content": "Fluid flow is perhaps the most widely applied physical problem to date, from the design of aerospace vehicle wings [54], to optimizing the output rate of chemical reactors, to analyzing pollutant dispersion [25] and wastewater treatment in environmental issues. Due to the highly nonlinear and chaotic nature of fluids, analyzing fluid-related problems becomes exceptionally challenging. Analytically solving the Navier-Stokes equations, which determine the essence of fluid behavior, remains an impossible task [52].\nComputational fluid dynamics (CFD) is the best tool for analyzing fluid flow. It uses numerical methods and computer simulations to solve fluid problems. A typical CFD simulation process involves generating a mesh, i.e., dividing a geometric model into small discrete elements, setting boundary and initial conditions, and solving the Navier-Stokes equations to obtain flow field data."}, {"title": "Related Work", "content": "Flow Prediction and Reconstruction. Flow prediction focuses on forecasting flow behavior at future time steps based on domain geometry, boundary, and initial conditions. Although numerical solvers offer high accuracy, they are computationally expensive, with resource demands escalating rapidly in higher dimensions, driving the search for more efficient alternatives. Hybrid methods leverage data-driven models to replace costly components of numerical solvers, while purely machine learning approaches, such as neural networks, directly predict flow fields. Neural operators [2, 67, 30, 34, 33, 22] exemplify this by learning functional mappings for flow prediction. Other works applying deep learning to flow field prediction include [27, 35, 12]. Additionally, incorporating physical constraints into models [53, 74] has proven to enhance prediction accuracy. Flow field reconstruction, in contrast, seeks to recover the full 3D flow field from partial observations, leveraging the model's prior knowledge of the underlying flow dynamics. Matsuo et al. [41] and Yousif et al. [76] are the most relevant to our work. Matsuo et al. [41] utilized CNNs to reconstruct 3D flow fields from multiple parallel 2D sections, while Yousif et al. [76] employed a GAN-based approach to recover 3D flow fields from two orthogonal 2D flow fields. Unlike their approaches, we employ a powerful generative model, the diffusion model, combined with a Transformer architecture. Additionally, unlike their methods, which recover 3D flow fields from fixed-position 2D planes, we introduce a novel plane position embedding, allowing the model to reconstruct 3D flow fields from different combinations of 2D planes.\n3D Reconstruction. Recent advancements in computer vision have significantly enhanced 3D reconstruction techniques [16, 43, 44, 38, 60, 26, 46, 49, 79], with promising applications for 3D flow field reconstruction. Neural Radiance Fields (NeRFs) [44, 24, 9] generate 3D scenes by training neural networks on multiple 2D images to produce color and density outputs. Generative models further advance this field by directly creating 3D representations from 2D images [37] or synthesizing novel views [38, 60, 46] to support 3D scene reconstruction [73]. Additionally, significant progress has been made in Gaussian splatting [49, 26, 77] and implicit representations [65, 68, 78, 79]. Among these methods, the most relevant to our 3D flow field reconstruction is the generative model-based approach. However, instead of generating novel object views from images, we reconstruct the corresponding 3D flow field from provided 2D flow field slices. In this work, we represent the 3D flow field as voxels and reconstruct it from 2D slices. Common 3D representations include point clouds [69, 48, 42, 6], meshes [39, 36, 16], and voxels [15, 57, 45]. Of these, voxel-based techniques are particularly relevant to this approach. However, other 3D representations may also provide viable solutions for flow field reconstruction.\nDiffusion Models. Generative models, particularly diffusion models [61, 23, 62], have advanced rapidly, emerging as state-of-the-art in generation quality and diversity. They now outperform other generative frameworks, such as variational autoencoders (VAEs) [29], autoregressive models (ARMs) [8], flow-based models [11], and generative adversarial networks (GANs) [17]. Diffusion models have shown great potential across fields, with recent applications to 3D data [6, 45, 73, 48, 60, 46]. While U-Nets [56] have traditionally served as the backbone for diffusion models, Transformer-based architectures are gaining traction [70]. DiT [51] employs a pure Transformer backbone, demonstrating impressive scalability. Extensions like DiT-3D [45], U-ViT [3], and UniDiffuser [4] further explore Transformers in diffusion models. Given their success, we selected Transformers for reconstructing 3D flows. To manage the computational complexity of 3D spatial data, we replaced global attention with plane and window attention. These linear-complexity attention mechanisms introduce inductive biases by leveraging the locality of flow fields, enhancing efficiency without sacrificing expressive power."}, {"title": "Method", "content": "3.1 Preliminaries\nIn this subsection, we first introduce the problem setting of reconstructing 3D flow fields from 2D flow fields. Then, we briefly provide some background knowledge on denoising diffusion probabilistic models (DDPMs). Finally, we describe two-stage approach and CFG, explaining our reasons for not using them.\nProblem Setup. For a given set of 2D slices $P = \\{p_i\\}_{i=1}^n$ within a 3D flow field, each 2D slice $p_i \\in \\mathbb{R}^{d_1 \\times d_2 \\times c}$, where c represents flow field attributes such as pressure or velocity, our goal is to reconstruct the 3D flow field $S \\in \\mathbb{R}^{d_x \\times d_y \\times d_z \\times c}$. We aim to learn $f_\\theta$, a neural network parameterized by $\\theta$,\n$\\hat{S} = f_\\theta(P, E_P),$\t\t(1)\nas a function of the 2D plane set P and the plane position embedding $E_P$ (section 3.2), and outputs the reconstructed 3D flow field $S$. We want $\\hat{S}$ to be as similar as possible to $S$.\nTo create $f_\\theta$ for reconstructing the flow field with high quality, we need to address two key issues. First, reconstructing a 3D flow field from 2D slices is very challenging, as it is an ill-posed problem that involves inferring global information from local data and requires strong prior knowledge. The second question is how do we address the potentially enormous computational costs associated with 3D data? For the first problem, we chose a generative model with sufficient generalization capabilities\u2014a diffusion transformer. It learns the prior knowledge about flow fields from the training data, as introduced in section 3.3. For the second question, we addressed it by replacing the traditional attention mechanism with more computationally efficient window attention and plane attention mechanisms. Furthermore, to enhance the flexibility of flow field reconstruction, we propose the plane position embedding, discussed in section 3.2, which allows us to fully utilize the 3D data.\nBackground on Diffusion Models. Diffusion Models define the forward process as a Markov chain, which incrementally injects noise into the original data. The reverse process is also a Markov chain, continuously removing noise from a certain prior distribution to recover the original data. The essence of Diffusion Models lies in learning this reverse process.\nFor the original 3D flow field data $S_0$, the forward process continuously adds noise, generating $S_1, S_2,..., S_T$. The typical form of the transition kernel is usually $q(S_t | S_{t-1}) = N(S_t; \\sqrt{1 - \\beta_t} S_{t-1}, \\beta_t I)$, where $\\beta_t \\in (0,1)$ is a Gaussian noise value.\nFor the backward process, The reverse Markov chain is characterized by a transition kernel $p_\\theta(S_{t-1}|S_t) = N(S_{t-1}; \\mu_\\theta(S_t, t), \\Sigma_\\theta(S_t,t))$, where $\\theta$ denotes model parameters. For training, we can form variational upper bound [29] which reduces to $L(\\theta) = -p(S_0 | S_1) + \\Sigma_t D_{KL}(q(S_{t-1}|S_t, S_0) || p_\\theta(S_{t-1}|S_t))$. Given that both $q(S_t | S_0)$ and $p_\\theta(S_{t-1}|S_t)$ are Gaussian, and considering that the backward process operates conditioned on the 2D flow field and its corresponding positional embedding, we can simplify the training objective to\n$L_{simple} (\\theta) = \\mathbb{E}_{t, S_0, \\epsilon} || \\epsilon - \\epsilon_\\theta(S_t, c(t, P, E_P)) ||^2$\t\t(2)\nwhere $c(t, P, E_P)$ is the condition of timestep, 2D flow field and plane position embedding. Once $p_\\theta(S_{t-1}|S_t)$ is trained, the model $f_\\theta$ can reconstruct the 3D flow field $S_0$ by applying iterative denoising conditioned on $c(t, P, E_P)$.\nTwo-stage Approach and CFG. Due to the substantial computational resources required to train diffusion models in high-resolution pixel/voxel space,"}, {"title": "Plane Position Embedding", "content": "To enable the model to reconstruct 3D flow fields from any combination of 2D flow fields, we input both the features of the two-dimensional flow fields and their position embedding into the model. We use a Cartesian coordinate system to describe the spatial relationships within the flow field. For convenience, we normalize the coordinates of each point in 3D flow field to be contained inside the XYZ unit cube [0,1]^3. Consider the equation of planes Ax + By + Cz + D = 0, we normalize it by dividing by $n = \\sqrt{A^2 + B^2 + C^2}$, resulting in A'x + B'y + C'z + D' = 0. We do not consider planes where D' > 1, thereby ignoring planes that intersect the cube [0, 1]^3 with a small area. Instead of directly inputting the four values A', B', C', and D' into the model, we feed them into the model via Fourier feature embeddings. This approach enriches the features and provides a continuous and smooth representation of plane equation.\nSpecifically, for the given dimension d, and plane equation parameter A', the embedding can be represented as\n$PE(A', 2i) = sin (A'/10000^{(2i/d)-1}),$\t\t(3)\n$PE(A', 2i + 1) = cos (A'/10000^{(2i/d)-1}).$\t\t(4)\nwhere i is the dimension index. Note that since A', B', C', and D' are less than 1, we multiply them by 10,000. Consequently, in Equation (3) and (4), the exponent of the numerator becomes $(2i/d) - 1$ instead of the usual 2i/d. We concatenate the embeddings of the four plane equation parameters to obtain the positional embeddings $E_{p_i} = [PE(A'), PE(B'), PE(C'), PE(D')]$ for each plane Pi. We then concatenate these to form the overall plane positional embedding $E_P = [E_{p_1},..., E_{p_n}]$, project it to the model dimension D using a multi-layer perceptron (MLP), add it to the timestep embedding temb, and feed the result into the model."}, {"title": "Diffusion Transformer for 3D Flow Field Reconstruction", "content": "Inspired by recent works [45, 51, 7], we use a diffusion transformer to reconstruct 3D flow from 2D. The 3D flow field is converted into input tokens through a 3D patchify operation, and 3D positional embeddings are added before processing the tokens through N transformer layers, which utilize window and plane attention. The 2D flow features are extracted using CLIP which are input to the model through adaptive layer norm. Finally, a linear layer and an unpatchify operation output the predicted noise in the 3D flow. The entire pipeline and architecture are shown in Figure 1.\n3D Patchify. The typical Transformer architecture processes data of shape $B \\times L \\times D$, where B denotes the batch size, L is the sequence length, and D is the dimension of the model. However, we treat the 3D flow field as voxels, $S \\in \\mathbb{R}^{d_x \\times d_y \\times d_z \\times c}$. Therefore, the first layer of our model is designed to patchify S. For a patch size of $p_x \\times p_y \\times p_z$, we use a 3D convolution layer with both kernel size and stride set to the patch size. This layer converts the input flow field voxels into a sequence of 3D patchified tokens $T_s \\in \\mathbb{R}^{D}$ with sequence length $L = X \\cdot Y \\cdot Z = \\frac{d_x}{p_x} \\cdot \\frac{d_y}{p_y} \\cdot \\frac{d_z}{p_z}$. The patch size does not impact the number of parameters in the model but affects the size of L, which is the attention window size in the transformer. Changes in patch size influence L and model's flops and, consequently, the quality of flow reconstruction [51]. Next, we add 3D positional embeddings [45] to the input tokens based on the spatial position of the corresponding voxels.\nTransformer Layers. Each transformer layer contains a self-attention layer, a cross-attention layer, and a feed-forward layer. The self-attention is defined as attention($x_i$) = softmax$\\left(\\frac{QK^T}{\\sqrt{D}}\\right)V$, where $Q = x_i W^Q, K = x_i W^K$ and $V = x_i W^V$. $W^Q, W^K, W^V \\in \\mathbb{R}^{D \\times D}$ are parametric projection matrices, and D denotes the input token dimension. Finally, the output is passed through a linear layer and added to the residual connection\n$SelfAttn(x_i) = Linear (attention(x_i)) + x_i.$\t\t\t(5)"}, {"title": "Experiments", "content": "To alleviate the computational cost increase brought by 3D processing, we replace vanilla global attention with window attention or plane attention. Figure 2 provides a visual explanation of these operations.\nFor window attention, we use a window size of w x w x w, reducing the input token sequence length L to $\\frac{L}{w^3}$\n$x_i = Reshape(B \\cdot \\frac{L}{w^3}, w^3, D)(x_i),$\t\t(6)\nwhere $x_i$ is the input tokens, Reshape$(B \\cdot \\frac{L}{w^3}, w^3, D)$ denotes dividing the original L input tokens into $\\frac{L}{w^3}$ groups, each of length w^3, where attention operations are performed only within each group of w^3 tokens. Since the time complexity of the attention operation is quadratic in the input token length, the Reshape operation reduces the time complexity from $O(L^2)$ to $O(Lw^3)$. In our experiments, w = 4, thus reducing the time complexity by a factor of 8 (with L = 512).\nFor plane attention, similar to window attention, we group tokens based on their spatial positions\n$x_i = Reshape(B \\cdot \\frac{L}{Y \\cdot Z}, Y \\cdot Z, D)(x_i),$\t\t(7)\nwhere Reshape$(B \\cdot \\frac{L}{Y \\cdot Z}, Y \\cdot Z, D)$ ensures that tokens only in the yOz plane can attend to each other. Note that tokens in the yOz plane may correspond to multiple flow fields because of the 3D patchify operation at the beginning of the model. In one self-attention sub-layer, we repeat plane attention three times, corresponding to the yOz, xOz, and xOy planes, as shown in Figure 2. These simple modifications to vanilla attention operation introduce inductive biases into the model, leveraging the locality of flow fields to enhance computational efficiency while maintaining performance.\nFollowing the self-attention, cross-attention is applied to incorporate the 2D flow field information. Additionally, AdaLN and Scale, as shown in Figure 1, also serve as methods for injecting flow field information, which we will describe in detail later. The final feed-forward network (FFN) consists of two linear layers with an activation function in between, similar to [70].\nConditioning. We adopted a triple-stream conditioning mechanism. In the first stream, we pad the 2D flow field to 3D to align with the model's input, then channel-concatenate it with the 3D flow field being denoised. This approach directly feeds the detailed information from the 2D flow field into the model, ensuring that the local details of the reconstructed flow field align with the corresponding 2D slices.\nFor the second conditioning stream, we use CLIP's image encoder to extract features Fp for all 2D planes pi in the set P. These features are concatenated to form the plane feature embedding Fp, which is then projected to the dimension of the input tokens using an MLP. Next, the timestep embedding temb and plane position embedding Ep are added to obtain the conditional information embedding\nc(t, P, Ep) = temb + \\lambda_1 F_P + \\lambda_2 E_P,\t\t(8)"}, {"title": "Experimental Setup", "content": "where $\\lambda_1$ and $\\lambda_2$ are two learnable weight parameters, both initialized to 1. Note that we use the global semantic token from the 2D flow field, specifically CLIP's class token $f_{cls}$, to capture its global features. Following DiT [51], we input the conditional information embedding c into the model through adaptive layer norm.\nAssuming $x_i$ is a sequence of vectors in the i-th sub-layer of the Transformer, adaLN with embedding c is defined as\nadaLN_i(x_i) = (1 + \\gamma_i) \\cdot LN(x_i) + \\beta_i,\t\t\t\t(9)\n$\\beta_i, \\gamma_i = MLP^{Scale, Shift}(c),$\t\t(10)\nwhere $\\gamma_i$ and $\\beta_i$ are the scale and shift parameters, and LN is the layer normalization. adaLN is applied to each sub-layers of the Transformer layer. Additionally, a scale $\\alpha_i$ output by another MLP is applied to xi prior to the residual connections in each sub-layer\nScalei(xi) = \\alpha_i \\cdot x_i,\t\t\t\t\t\t(11)\n$\\alpha_i = MLP^{Scale}(c).$\t\t\t\t\t\t\t(12)\nWe zero-initialize these MLPs to accelerate training [18].\nAlthough the global token effectively represents the information of the flow field, it cannot capture the full extent of the flow. To provide more comprehensive flow information to the model, we use the CLIP image encoder's final layer output $F_{last} = \\{f_i\\}_{i=1}^n$ as input for the third conditioning stream via cross-attention. This output has been proven to offer high-fidelity representations in numerous studies.\n4.1 Experimental Setup\nDatasets. In our experiments, we primarily focus on two different datasets, corresponding to the incompressible Navier-Stokes (INS) equations and the compressible Navier-Stokes (CNS) equations.\nThe incompressible Navier-Stokes equations are a simplified version of the original equations, assuming that the fluid density is independent of pressure. The equations are given by\n$\\nabla \\cdot u = 0, \\rho (\\frac{\\partial u}{\\partial t} + u \\cdot \\nabla u) = - \\nabla p + \\mu \\nabla^2 u + f$\t\t\t\t(13)\nwhere u is the velocity, $\\rho$ is the density, p is the internal pressure, and f represents external forces. We use a dataset from [76], consisting of a turbulent channel flow at friction Reynolds numbers $Re_\\tau = 180$.\nThe compressible Navier-Stokes equations are expressed as\n$\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho u) = 0,$\t\t(14)"}, {"title": "Results", "content": "$\\rho (\\frac{\\partial u}{\\partial t} + u \\cdot \\nabla u) = - \\nabla p + \\eta \\nabla^2 u + (\\zeta + \\eta/3)\\nabla(\\nabla \\cdot u),$\t\t(15)\n$\\frac{\\partial (\\rho e)}{\\partial t} + \\nabla \\cdot [(\\epsilon + p + \\frac{\\rho u^2}{2} )u - u \\cdot \\sigma'] = 0,$\t\t\t(16)\nwhere $\\epsilon = \\frac{p}{(\\Gamma - 1)}$ is the internal energy density, $\\Gamma = 5/3$, $\\sigma'$ is the stress tensor, and $\\eta, \\zeta$ are the shear and bulk viscosity, respectively. Our dataset is from [64], with shear viscosity $\\eta$ of 1 x 10^-8 and bulk viscosity $\\zeta$ of 1 x 10^-8, Mach number as 1.0, and turbulence initial conditions.\nBoth datasets are generated using direct numerical simulation (DNS). For the INS dataset, we employ two different data partitioning methods to test the model's interpolation and extrapolation capabilities, denoted as INS(INT) and INS(EXT), respectively. Specifically, each flow field in the INS dataset has a corresponding time step. For INS(INT), we randomly split data from different time steps into training and test sets, whereas for INS(EXT), we use data from the first 80% of time steps for training and the remaining time steps for testing. Since $\\Delta t = 0.07$, the flow fields at consecutive time steps are similar, allowing INS(INT) to evaluate the model's ability to reconstruct 3D flow fields when ample data is available. In contrast, INS(EXT) assesses the model's generalization capability across different time steps. For the CNS dataset, we only test the model's extrapolation capability. In addition, we also test our model's capability to reconstruct flow fields around geometric objects (Section 4.4).\nMetrics. We use normalized root-mean-square error (nRMSE) measure the accuracy of the 3D flow field reconstruction\nnRMSE = $\\frac{||U_{pred} - U_{true}||_2}{||U_{true}||_2},$\t\t(17)\nand use the peak signal-to-noise ratio (PSNR) to assess the quality of the reconstructed 2D slices of the flow field\nPSNR = 10 \\cdot log_{10} \\frac{MAX^2(U_{true})}{MSE(U_{true}, U_{pred})}$,\t\t\t(18)\nWe also adapt the structural similarity index measure (SSIM) to better assess the detailed aspects of the reconstructed flow field\nSSIM = $\\frac{(2\\mu_{U_{true}} \\mu_{U_{pred}} + C_1) (2 \\sigma_{U_{true}U_{pred}} + C_2)}{(\\mu^2_{U_{true}} + \\mu^2_{U_{pred}} + C_1) (\\sigma^2_{U_{true}} + \\sigma^2_{U_{pred}} + C_2)},$\t\t(19)\nwhere C1 and C2 are constants, which we set to 0.01 and 0.03, respectively. We calculate SSIM with a local window of 11 x 11 x 11.\nImplementations. We implemented three different model sizes: Small, Base, and Large. Table 1 gives details of our models. Note that only some of the global attention in the model are replaced with window attention or plane attention.\nWe reconstruct the 3D velocity field u, v, w. The model uses the AdamW [28, 40] optimizer. The learning rate is set to 1 x 10^-4 and we use a Cosine-Annealing-LR scheduler. The batch size is 32, and we train for 10,000 epochs."}, {"title": "Ablation Study", "content": "4.5 Ablation Study\nIn this section, we present the results of the ablation experiments.\nNumber of 2D Planes. Table 4 shows the impact of the number of 2D planes used for reconstructing the 3D flow field on reconstruction quality. Adding additional 2D planes does not significantly affect reconstruction quality. The Small and Large models showed no improvement with the increase in the number of planes, while the Base model showed a slight improvement but a decrease in the SSIM. It is possible that adding just one more 2D flow field does not provide enough additional information and instead puts pressure on the model's training optimization.\nAttention. The test results for models using different attention mechanisms are shown in Table 5. Replacing some of the global attention in the model with window attention and plane attention resulted in at least a 25% improvement in training speed. For the Small and Base models, this led to a slight performance impact, while the performance of the Large model actually improved. These results validate that the attention mechanisms we adopted can significantly reduce computational costs with little to no impact on model performance.\nPlane Position Embedding. We use three 2D planes located at a relative position of -5 from the center of the flow field for reconstruction, with plane position embedding providing the positional information of the 2D planes to the model. Figure 7 shows the metrics for the reconstructed flow. It can be observed that the reconstruction quality is high for flow fields near the relative position of -5, demonstrating that our plane position embedding successfully enables the model to reconstruct 3D flow fields from 2D planes at different locations.\nPacth Size. Table 6 presents the ablation study results on patch size. It can be observed that a smaller patch size does not significantly improve the overall quality of the flow field but is effective in reconstructing the details of the flow field. We also found that the model fails to converge during training when the patch size is too large."}, {"title": "Conclusion", "content": "5 Conclusion\nIn this work, we propose a Diffusion Transformer-based method for reconstructing 3D flow from 2D data. Additionally, we use plane position embeddings to provide the model with the positional information of the 2D flow fields, enabling the reconstruction of 3D flow from 2D flows at arbitrary locations, thereby enhancing reconstruction flexibility. By replacing conventional global attention with window attention and plane attention, we significantly reduce the computational costs associated with the increased token length from the additional dimension, without substantially compromising model performance. Our experiments demonstrate that our model can efficiently and accurately reconstruct 3D flow fields from 2D flow data, producing realistic 3D flows. However, our approach treats flows reconstruction as a generative task, ignoring the underlying physical significance. A potential future improvement could involve embedding the corresponding physical information of the flow into the model."}]}