{"title": "EDORA: Efficient Weight-Decomposed Low-Rank Adaptation via Singular Value Decomposition", "authors": ["Hamid Nasiri", "Peter Garraghan"], "abstract": "Parameter-efficient fine-tuning methods, such as LORA, reduces the number of trainable parameters. However, they often suffer from scalability issues and differences between their learning pattern and full fine-tuning. To overcome these limitations, we propose Efficient Weight-Decomposed Low-Rank Adaptation (EDORA): a novel PEFT method that decomposes pre-trained weights into magnitude and directional components. By freezing low-rank matrices, initializing them by singular value decomposition, and introducing a small trainable matrix between them, EDORA achieves substantial reduction in trainable parameters while maintaining learning capacity. Experimental results on the GLUE benchmark demonstrate that EDORA achieves competitive or superior performance compared to state-of-the-art methods, such as LoRA and DORA, with up to 30x fewer trainable parameters. This makes EDORA a highly efficient solution for adapting LLMs to diverse tasks under memory-constrained settings. Code is available at https://github.com/Hamid-Nasiri/EDORA.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have shown remarkable success in a variety of applications, such as Natural Language Processing (Thirunavukarasu et al., 2023; Min et al., 2023) and multi-modal tasks (Liu et al., 2023; Cui et al., 2024; Jin et al., 2024). However, fine-tuning these models for specific downstream tasks can be computationally expensive. Parameter-efficient fine-tuning (PEFT) methods address this challenge by only updating a small subset of the model's parameters. Low-Rank Adaptation (LoRA) (Hu et al., 2022) is one of the most popular PEFT methods due to its good generalization, simplicity, and efficiency. Moreover, it does not add additional latency during the inference since it can merge with pre-trained weights before inference.\nLORA efficiently reduces the number of trainable parameters. Inspired by LoRA, researchers (Edalati et al., 2022; Fawi, 2024; Zhang et al., 2023; Ba\u0142azy et al., 2024; Kopiczko et al., 2023) have developed adaptation methods to decrease the number of parameters further while enhancing performance. Several studies (Zhang et al., 2023; Ba\u0142azy et al., 2024; Gu et al., 2024) applied various matrix decomposition methods such as CUR matrix decomposition (Fawi, 2024) and Singular Value Decomposition (SVD) (Zhang et al., 2023; Ba\u0142azy et al., 2024; Gu et al., 2024) to reduce LoRA's trainable parameters. While most methods focused on decreasing the number of trainable parameters, others (Liu et al., 2024; Kalajdzievski, 2023) have concentrated on improving LoRA's learning pattern and stability so that it more closely resembles full fine-tuning.\nAlthough these methods effectively reduce the number of trainable parameters in LoRA, they still need substantial storage and computational resources, especially in scenarios requiring large-scale personalized or task-specific adaptation (Ba\u0142azy et al., 2024). Existing methods also suffer from a scaling problem: their parameter count increases with model dimensionality. This dependency can substantially increase trainable parameters, particularly for LLMs with higher hidden dimensions. Increasing the number of parameters can exacerbate risks in overfitting, especially when adapting to smaller downstream datasets (Qin et al., 2024).\nBeyond these scaling issues, LoRA faces another key challenge with its learning pattern differs significantly from that of full fine-tuning. This difference potentially limits LoRA's learning capacity. LoRA tends to update both the magnitude and direction of weights proportionally, exhibiting a strong positive correlation between the changes in these two components. This contrasts with the learning pattern observed in full fine-tuning (Liu et al., 2024). LoRA's tendency to change the magnitude and direction proportionally, can lead to unnecessary adjustments to the pre-trained weights, potentially affecting the model's performance. To address these issues, we require a technique that can replicate full fine-tuning learning pattern and mitigate the scalability problem by reducing the number of trainable parameters."}, {"title": "2. Related Work", "content": "Fine-tuning LLMs is often extremely costly due to their size. PEFT methods tackle this challenge by adapting large models for downstream tasks by training a small number of parameters. These parameters can be a subset of the model's existing parameters or entirely new ones added to the model (Lialin et al., 2023). This significantly reduces both computational and storage costs. PEFT techniques fall into three main categories: 1. Additive Methods, 2. Selective Methods and 3. Reparametrization-based Methods."}, {"title": "2.1. Additive Methods", "content": "The core concept of additive methods is to enhance the existing pre-trained models by introducing additional parameters or layers to the original frozen backbone. Additive methods include two main subcategories: Adapter-based methods and Prompt-based methods. Adapters, as presented by Houlsby et al. (2019), add small fully-connected networks after transformer sub-layers (Lialin et al., 2023). They produce a compact and flexible model by introducing only a small number of trainable parameters for each task, allowing new tasks to be added without the need to revisit earlier ones. AdapterHub (Pfeiffer et al., 2020), a framework for adapting transformers, was developed by Pfeiffer et al. It allows dynamic \u201cstitching-in\u201d of pre-trained adapters for various tasks.\nThe second subcategory of additive methods is Prompt-based approaches. The main idea behind these methods is to introduce additional soft tokens to the initial input and concentrate exclusively on fine-tuning these trainable vectors. Vu et al. (2022) proposed SPOT (Soft Prompt Transfer). SPOT initially learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. The author showed that SPOT substantially improves the performance of prompt-tuning across various tasks; however, prompt-based methods are often hindered by their sensitivity to initialization, impacting their performance. The additive methods, whether modifying the model's input or architecture, lead to higher inference latency compared to the baseline model (Liu et al., 2024)."}, {"title": "2.2. Selective Methods", "content": "Selective methods fine-tune only a selection of layers or parameters of the model. One subcategory of selective methods is sparse update techniques, which can disregard the model's overall structure and selectively update individual parameters. Sung et al. (2021) developed the FISH (Fisher Induced Sparse Unchanging) method. FISH applies a fixed sparse mask to the model's parameters, selecting a subset for updating over multiple iterations. It creates the mask using the top n parameters with the highest Fisher information as a simple approximation of which parameters are most important for the given task. Although sparse update methods are efficient regarding memory consumption and communication overhead, their unrestricted unstructured sparsity makes them impractical on modern hardware."}, {"title": "2.3. Reparametrization-based Methods", "content": "Reparametrization-based techniques use low-rank representation to reduce the number of trainable parameters (Lialin et al., 2023). Unlike additive methods, these methods do not add additional computational overhead or latency during inference. Aghajanyan et al. (2020) showed that fine-tuning is efficient in low-rank subspaces. Moreover, the authors demonstrated that the subspace size that needs adaptation decreases as model size or pre-training duration increases. Low-Rank Adaptation (LoRA) (Hu et al., 2022) is the most widely known reparametrization-based method. It uses a simple low-rank matrix decomposition to parametrize the weight update and can merge with pre-trained weights before inference. The authors in (Hu et al., 2022) showed that its low-rank weight updates are highly correlated with the pre-trained weights, indicating that LoRA amplifies specific directions already present in the model's weight space.\nInspired by this finding, many researchers (Edalati et al., 2022; Fawi, 2024; Zhang et al., 2023; Ba\u0142azy et al., 2024;"}, {"title": "3. Proposed Method", "content": "In this paper we present an Efficient Weight-Decomposed Low-Rank Adaptation (EDORA). EDORA employs a decomposition strategy to decompose pre-trained weights into magnitude and directional components, freezes low-rank matrices, and introduces a small trainable matrix between them. This design replicates the learning dynamics of full fine-tuning, mitigating scalability issues and reducing the risk of overfitting. EDORA also uses SVD for its initialization phase, which helps ensure that the adaptation process starts from a subspace aligned with the most important features of the pre-trained model.\nThe major contributions of this paper are as follows:\n\u2022 We introduce EDORA, a highly parameter-efficient PEFT approach that leverages a decomposition strategy and SVD-based initialization to overcome the scalability and learning pattern limitations of existing methods.\n\u2022 We evaluate EDORA's performance on the GLUE benchmark. In terms of average performance over six different tasks, EDORA outperforms LoRA and other state-of-the-art methods, such as DoRA.\n\u2022 We present a parameter efficiency analysis of EDORA compared to LoRA and DORA. The results highlight that EDORA, on average, reduces the trainable parameters by over 45x compared to LoRA and DORA when applied to the GPT-3 model.\nThe main idea of EDORA is to decompose pre-trained weight into its magnitude and directional components. EDORA keeps the magnitude vector trainable and, due to the substantial size of the directional component in terms of parameters, fixes low-rank matrices $A\\in R^{r\\times n}$ and $B\\in R^{m\\times r}$, and introduces a small trainable matrix $R\\in R^{r\\times r}$ between them (Figure 1).\nThe fine-tuned weight W' can be updated incrementally by a low-rank decomposition:\n$W' = W_o + \\Delta W = W_o + BA$   (1)\nwhere $W_o \\in R^{m\\times n}$ represents the pre-trained weight"}, {"title": null, "content": "matrix, $\\Delta W \\in R^{m\\times n}$ is the low-rank weight update, $B\\in R^{m\\times r}$, $A\\in R^{r\\times n}$, and $r < min(m, n)$. During the fine-tuning process, $W_o$ is kept frozen and does not receive gradient updates, while $A$ and $B$ contain trainable parameters. $A$ is initialized with a random Gaussian distribution and $B$ is initialized with zeros, so $\\Delta W = BA$ is zero at the start of training to ensure that $W'$ equals $W_o$ before the fine-tuning.\nThe weight decomposition of $W \\in R^{m\\times n}$ can be represented as:\n$W=\\frac{m D}{||D||_c}$  (2)\nwhere $m \\in R^n$ denotes the magnitude vector, $D \\in R^{m\\times n}$ represents the directional matrix, and $||\\cdot||$ is the vector-wise norm of a matrix across each column. Notably, each column of $D/||D||$ is a unit vector whose magnitude is specified by the corresponding entry in $m$.\nDerived from Eqs. (1) and (2), DoRA can be formulated as:\n$W' = \\frac{m}{||D + \\Delta D||_c} = \\frac{m W_o + BA}{||W_o + BA||_c}$ (3)\nwhere A and B denote trainable parameters and $\\Delta D$ is the direction update computed by the product of A and B. The DORA approach applies LoRA to the directional matrix. Moreover, the magnitude vector also receives gradient updates.\nEDORA reduces the number of trainable parameters drastically by making matrices A and B frozen and adding a small trainable matrix $R \\in R^{r\\times r}$ to Eq. (3) as follows:\n$W' = \\frac{m}{||D + \\Delta D||_c} = \\frac{m W_o + BRA}{||W_o + BRA||_c}$ (4)\nDuring the training, matrix R is initialized with a Gaussian distribution, $N(0, \\sigma^2)$, where $\\sigma$ is set to a small value. The matrices A and B are initialized using the truncated SVD of the directional matrix D. Mathematically speaking, the SVD of the directional matrix can be computed as follows:\n$D = UE V^T$ (5)\nwhere $U \\in R^{m\\times m}$,$\\Sigma \\in R^{m\\times n}$, and $V \\in R^{n\\times n}$. Then, by considering the top r singular values of the directional matrix, we set A and B as follows:\n$A = V^T_r$\n$B = U_r\\Sigma_r$  (6)\nwhere $U_r \\in R^{m\\times r}$ and $V_r\\in R^{n\\times r}$ denote left and right singular vectors corresponding to the top r singular values, respectively, and $\\Sigma_r$ represents a diagonal matrix that contains the top r singular values of $\\Sigma$. Initializing A, B, and R as mentioned, ensures that the learning process starts with a model nearly identical to the pre-trained model.\nEDORA does not change the model architecture. It can be merged with the pre-trained weight before inference, so it does not add additional computational overhead or latency during inference. Limiting the algorithm to focus exclusively on directional adaptation while keeping the magnitude vector trainable, simplifies the task compared to the original approach. Furthermore, adding the trainable matrix R guarantees that the model functions within a subspace, capturing the pre-trained weights' most important components or directions."}, {"title": "3.1. Parameter Efficiency Analysis", "content": "In this section, we evaluate the parameter efficiency of EDORA in comparison to LoRA and DORA. To simplify the analysis, consider a transformer model with the weight matrix $W \\in R^{n\\times n}$. The ratio of trainable parameters of LoRA to EDORA can be computed as follows:\n$\\frac{P_{LORA}}{P_{EDORA}} = \\frac{2nr}{n + r^2}$ (7)\nwhere r represents the rank of low-rank decomposition. Similarly, the ratio of trainable parameters of DORA to EDORA can be computed as follows:\n$\\frac{P_{DORA}}{P_{EDORA}} = \\frac{n + 2nr}{n + r^2}$  (8)\nSince r < n and the model dimension n grows significantly larger than the rank r, the advantages of EDORA over LORA and DORA become increasingly evident. To illustrate this, we calculated the ratios for different LLMs and different values of r, as given in Table 1. The LLMs we considered were BERT (Devlin et al., 2018), ROBERTa (Liu et al., 2019), ALBERT (Lan, 2019), OPT 6.7B (Zhang et al., 2022), GPT-3 (Brown et al., 2020) and PaLM 540B (Chowdhery et al., 2023)."}, {"title": "4. Experiments", "content": "This section evaluates the performance of EDORA on the GLUE benchmark (Wang et al., 2018). We compared EDORA with three PEFT methods including DoRA (Liu et al., 2024), LoRA (Hu et al., 2022) and LoRA-XS (Ba\u0142azy et al., 2024) using various tasks, including inference tasks (QNLI and RTE), similarity and paraphrase tasks (MRPC and STS-B), and single-sentence tasks (CoLA and SST-2)."}, {"title": "4.3. Ablation Study", "content": "In this section, we conduct an ablation study to demonstrate the impact of SVD initialization on EDORA's performance. We repeated the experiments using a random initialization instead of SVD initialization. The results, summarized in Table 3, indicate that SVD initialization consistently outperforms random initialization across all tasks and ranks. For instance, at rank 32, the average performance with SVD initialization is 84.48%, compared to 82.68% with random initialization. SVD initialization achieved, on average, 1.46% better performance than the random initialization across different ranks. These results highlight the importance of initializing the low-rank matrices in a subspace aligned with the pre-trained weights' most important components, which SVD achieves effectively. In contrast, random initialization may start the adaptation process from a less optimal subspace, leading to slower convergence and reduced performance."}, {"title": "5. Conclusion", "content": "This paper proposed EDORA, a novel PEFT method that uses a decomposition strategy and SVD-based initialization to address the scalability and learning pattern challenges of existing approaches such as LoRA and DoRA. By decomposing pre-trained weights into magnitude and directional components, freezing low-rank matrices, and introducing a compact trainable matrix between them, EDORA achieves substantial reductions in trainable parameters while maintaining high performance across diverse tasks. This reduction in parameters not only lowers computational and storage demands but also mitigates the risk of overfitting, especially in scenarios with limited training data. EDORA's decomposition strategy aligns closely with the learning dynamics of full fine-tuning, further enhancing its adaptability and efficiency. Experimental results on the GLUE benchmark demonstrated EDORA's effectiveness, achieving competitive or superior performance compared to state-of-the-art methods with up to 30x fewer trainable parameters. The results highlighted EDORA's potential as a scalable and resource-efficient solution for adapting large language models to downstream tasks, making it particularly suitable for memory-constrained environments. Future work could explore extending EDORA to multi-modal tasks, making rank of trainable matrices adaptive based on the importance of weights in each layer, and further optimizing its initialization strategies to enhance generalizability across even more complex domains."}]}