{"title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective", "authors": ["Chengyin Xu", "Kaiyuan Chen", "Xiao Li", "Ke Shen", "Chenggang Li"], "abstract": "The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) the \u201cemergence phenomenon\", wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction; (2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability. Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities. To address these challenges, we propose a Clustering-On-Difficulty (COD) downstream performance prediction framework. COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance. The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process. Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36% across eight important LLM evaluation benchmarks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have emerged as transformative technologies in natural language understanding, generation, and reasoning [1, 14, 5]. Their impressive success heavily relies on scaling model parameters and pre-training data, with training loss empirically following a power-law relationship with compute [18, 21]. However, this reduction in training loss primarily reflects an in-domain compression effect and does not necessarily indicate improved out-of-domain generalization or downstream performance-the factor of primary concern in practice. Specifically, performance scaling of downstream tasks aims to predict the accuracy of the target LLM on downstream tasks using metrics from smaller models. Our objective is to develop a prediction method that works reliably across a diverse range of downstream tasks, minimizing the worst-case prediction error.\nDespite extensive efforts, a reliable scaling law for downstream tasks remains elusive. One line of work attempts to extrapolate large-model performance by modeling the performance-loss relationship [6, 13, 8, 38, 26], but this often fails to capture the emergent behaviors of LLMs and the"}, {"title": "2 Related Work", "content": "2.1 Loss Scaling Laws\nLoss scaling laws provide a systematic framework for understanding the relationship between computational resources, data, model size, and the final performance of LLMs. Early work by Kaplan et al. [21] demonstrates that the pre-training loss of LLMs follows a power-law relationship with the compute (the number of floating-point operations) used in training. Subsequent studies extend these findings to other domains, such as computer vision [41], graph learning [24] and vision-language models [2, 16]. Recent research has also explored scaling laws in specific contexts, such as fine-tuning [17, 34], vocabulary size optimization [33], retrieval-augmented models [30], and hyperparameter tuning [23, 40]. These studies highlight the broad applicability of scaling laws and their potential to guide the efficient allocation of computational resources.\n2.2 Downstream Task Performance Scaling\nPredicting downstream task performance remains a critical challenge due to emergent abilities in LLMs that manifest only after exceeding task-specific thresholds [37, 28]. Recent works, such as using loss as a proxy [6] or increasing metric resolution [19], have demonstrated potential but encounter challenges in aligning surrogate metrics with original task objectives. Here, we briefly review the two main types of methods for predicting downstream performance:\n1. Loss-intermediate prediction. These methods predict the final training loss (or in-domain validation loss) of LLMs with loss scaling laws first, and then predict downstream performance through loss-performance relationships [6, 13, 8]. While these methods leverage established scaling laws for loss predictions, they encounter a fundamental limitation: the inconsistent mapping between loss and performance metrics. In addition, Xiao et al. [38] employ the evaluation set answer loss as an intermediate variable for estimation. Although answer loss correlates with the final performance metrics, its predictability remains low as predicting answer loss shares the challenges with predicting performance, including emergence phenomenon and high variance in task difficulty."}, {"title": "3 Pilot Study", "content": "In this section, we present the pilot experiments to illustrate the shortcomings of existing approaches.\nTraining loss may mismatch downstream tasks performance. Predicting downstream performance based on training loss relies on the assumption that LLMs achieve identical downstream performance at the same loss value-an assumption that often does not hold. In practice, training loss primarily serves as an indicator of in-domain fitting, whereas downstream tasks typically represent out-of-domain evaluations. Moreover, training configurations, such as model size and learning rate, can significantly affect not only the final loss but also the model's generalization capabilities.\nDiverse scaling patterns within the evaluation set. Scaling patterns capture the performance-compute relationship for a single task sample. However, different task samples exhibit unique computational thresholds, learning slopes, and upper bounds, making it challenging to find a single fitting function (or set of fitting functions) that generalizes well across diverse task samples."}, {"title": "4 Method", "content": "In this section, we introduce the COD method in four parts, illustrated in Fig. 2: 1) We show the advantages of COD and present an improved mean-shift clustering algorithm (Section 4.1); 2) We derive a performance scaling law corresponding to task difficulty variance, which enhances the benefit of extrapolating the performance-compute relationship for task clusters with similar difficulty features (Section 4.2). We fit cluster-wise performance-compute curves on small models and filter extrapolatable clusters; 3) We extrapolate the performance on extrapolatable clusters and predict the accuracy of the target large model on the predictable subset(Section 4.3); 4) We show how to map accuracy on the predictable subset to full evaluations (Section 4.4).\n4.1 Clustering on Difficulty\nDespite sharing common themes, tasks within evaluation sets demonstrate substantial difficulty differences. These differences result in diverse performance scaling patterns across tasks, making it challenging to apply a universal fitting function for predictions. Instead, we propose clustering tasks with comparable performance scaling behaviors to enable more accurate predictions. This approach minimizes the heterogeneity of difficulty features within clusters while ensuring that each cluster contains a sufficient number of samples for robust evaluation.\nWe adopt the passrate metric to quantify the capabilities of small-scale models [19]. For each model, we conduct multiple evaluation runs (e.g., 100 trials) and calculate the mean accuracy as the expected probability of correct responses. For each task, we characterize its difficulty through the passrates of models of increasing size. These passrates are arranged in ascending order of model scale, forming feature vectors that ideally exhibit monotonic growth within the [0, 1] range, as model capability typically increases with size. However, we observe that some tasks deviate from the expected scaling pattern, showing non-monotonic difficulty features. This phenomenon may be attributed to metric instability or fluctuations in model performance during training.\nImproved clustering methods. We hope to adopt clustering algorithms with the following features: 1. Minimizing intra-class variance to ensure similar extrapolation properties within each cluster, 2. Automatic determination of cluster numbers, as the optimal number varies across evaluation sets and is difficult to pre-specify."}, {"title": "4.2 Fitting", "content": "Following cluster analysis, we compute evaluation metrics of small models within each cluster and conduct separate extrapolation curve fitting procedures. Small models are trained with the same ratio of training tokens to Compute Per Token (CPT). We propose a scaling law for downstream task performance, supported by theoretical analysis, which allows us to derive prediction formulas for performance scaling within clusters of tasks that share similar difficulty features. The fitting process initially excludes outlier samples, focusing only on the clustered sample set. For each cluster identified in the previous step, we compute accuracy metrics across small models, yielding an expected accuracy array for each cluster. By fitting these accuracy values against the computational costs of small models, we derive the expected accuracy-to-compute curve for each cluster.\nWe derive the fitting formula for the downstream task scaling law based on the following three assumptions:\n1. The relationship between the answer loss and the compute follows a power law, which generalizes the power law in loss prediction into (Question, Answer) format data.\n2. For task samples with a finite set of answers, the model gives a random guess choice if it cannot accurately solve it.\n3. The task passrate is defined as the product of the predicted probabilities for each token, implying that each task sample has a unique answer, and the model outputs the answer only without any intermediate reasoning progress.\nNote that these assumptions may not perfectly hold in practice, we provide additional discussions on Assumption 3 in Section 6. Under the above assumptions we can derive the scaling law for downstream task performance."}, {"title": "Proposition 1 (Scaling Law for Downstream Task Performance)", "content": "Given a language model trained with computational budget C, and a set of downstream tasks P, under the following assumptions:\nThe expected accuracy on tasks P can be modeled as:\n$E_P[Acc(C)] = g + (1 - g) * (e^{-aC^{-b}}-c+\\frac{\\sigma^2}{2\\mu}) + o(\\mu)$\nwhere:\n\u2022 g represents the random guess performance floor;\n\u2022 1 g represents the maximum achievable performance improvement;\n\u2022 a, b, c are positive constants;\n$\\mu = \\frac{1}{\\#P} \\sum_{(q,ans) \\in p} loss_{ans}$;\n$\\sigma^2 = \\frac{1}{\\#P} \\sum (loss_{ans_t} - \\mu)^2$.\nWe outline the key proof intuition here, with detailed proofs provided in Appendix B. Like existing approaches [19], we aim to establish the relationship between $loss_{ans}$ and model passrate, leveraging loss power-law scaling to derive a scaling formula for downstream task passrate metrics.\nProof intuition\nThe assumption 3 ensures unique task answers and neglecting the impact of model thinking before answers, the probability of correct task completion equals the product of token probabilities in the model output. This implies a negative logarithmic relationship between $loss_{ans}$ and $passrate$ for individual tasks.\nPrevious works overlook that computing the passrate metric for an evaluation set requires averaging $exp(-loss_{ans})$ across tasks, whereas applying the loss scaling law necessitates averaging loss before exponentiation. Mathematically, the performance scaling law computes the arithmetic mean of $exp(-loss_{ans})$, while the loss scaling law after exponentiation yields the geometric mean.\nWe show that the difference between arithmetic and geometric means can be estimated by $\\frac{\\sigma^2}{2\\mu}$, where $\\sigma^2$ and $\\mu$ denote the variance and mean of task passrates, respectively. Consequently, the downstream tasks performance scaling law derived from the loss scaling law is valid only for evaluation sets with limited difficulty variance. Our proposed clustering-based COD method constrains the variance of difficulty features within clusters, enabling better alignment with the performance scaling law.\nFinally, we constrain the model output space to a finite answer set, random guessing yields an expected score g for unsuccessful attempts.\nProposition 1 demonstrates that a metric of an evaluation set with similar difficulty features can be effectively modeled using the following formula:\n$y(C) = g + (1 \u2212 g) * e^{-aC^{-b} - c}$,\nwhere a and b jointly influence how accuracy varies with C, c controls the upper bound of the fitting curve, and g represents the expected random guess metric for the model on this cluster. Parameters a, b, c, and g are to be fitted."}, {"title": "4.3 Extrapolation", "content": "We aim to identify clusters exhibiting robust scaling patterns for reliable performance extrapolation since some clusters have saturated or non-emergent performance on small models and are not expected to give reasonable predictions. We will show that performance prediction on these scalable clusters contributes to the prediction on the full evaluation set. We give the following definition to check whether a cluster is extrapolatable."}, {"title": "4.4 Mapping from Predictable Subset to Target Evaluation Set", "content": "We extend our predictions from the predictable subset to the complete evaluation set through a principled mapping approach. Our method rests on the observation that extrapolatable and non-extrapolatable samples share question types but differ primarily in difficulty features, suggesting a preserved partial order of metrics across these subsets. We formalize this relationship through a mapping function $f : f(T') \u2192 T$ from predictable subset metrics $T'$ to total evaluation set metrics $T$. This function exhibits key properties: 1. continuity and smoothness over [0, 1], 2. monotonic increase, and 3. passage through points (0,0) and (1, 1). Empirical validation reveals that a quartic function optimally captures this relationship:\n$f(x) = a_1x^4 + a_2x^3 + a_3x^2 + (1 \u2212 a_1 \u2212 a_2 \u2212 a_3)x$\nTo ensure reliable extrapolation, we calibrate the mapping curve using evaluation results of existing models as anchors. Our results show that the subset-to-full mapping generally maintains robustness across model architectures and training data, enabling the use of external models (e.g., Qwen2-72B [39]) as anchors for most tasks. We conduct corresponding experiments in Section 5.5. For data-sensitive tasks, models with similar training distributions provide more reliable anchors, indicating that data consistency takes precedence over architectural variation ensuring mapping accuracy. This calibration strategy enables accurate metric predictions for the complete evaluation set while maintaining computational efficiency.\nFinally, combining Eq. (1) and Eq. (2), we get our final metric prediction $p = f \u2022 y(C_0)$, where $C_0$ is the estimated computation of training the target LLM."}, {"title": "5 Experiments", "content": "5.1 Experimental Setups\nIn our experimental setup, we train several smaller versions of the target model architecture for prediction. These models vary in size but share similar training procedures, with the training data scaled proportionally to their sizes.\nDownstream evaluation sets. We adopt the following widely-used benchmarks as our target downstream tasks: For evaluation, we adopt the following widely-used benchmarks, shown in Table 1. Evaluation sets cover popular downstream tasks of the language model, including math, logic, coding, reading comprehension, professional knowledge, etc.\nAll models are evaluated in a few-shot in-context learning manner, where they need to generate final answer labels based on given demonstrations and test inputs. We aligned our evaluation setups with LLaMa3 [10].\nModel training. To establish performance predictions for large language models, we conduct systematic experiments with a suite of smaller-scale models across different parameter counts and training data volumes, while controlling for other training configurations such as learning rate, batch size, and additional hyperparameters. All models are trained on a constant learning rate scheduler and data with the same distribution. We list model configurations in Table 2."}, {"title": "5.2 Prediction Experiments", "content": "Baselines. We evaluate our proposed COD performance scaling for LLMs against existing approaches. The evaluation is conducted on multiple public benchmarks mentioned above, where we utilize a series of smaller models with identical data distribution and architecture but different configurations to estimate the downstream tasks performance of the target 70B large language model.\nWe compare against three existing prediction methods:\n1. End-to-end performance-compute prediction: Extrapolate larger model metrics directly from smaller model evaluation set metrics using performance scaling laws.\n2. Passrate-compute prediction: Estimate large model passrates from smaller model pass-rates [1, 19]. We repeat and evaluate 100 trials for each evaluation set to enhance the performance reliability on smaller models. For a fair comparison, we report the absolution prediction error on the passrate metric instead of greedy decoding accuracy.\n3. Loss-intermediate performance prediction: First predict the final training loss of large language model, then estimate downstream task metrics based on the relationship between smaller model evaluation metrics and their corresponding losses [6].\nWe design two experimental groups to validate the benefits of clustering and the complete pipeline, respectively:\n1. COD w/o. mapping: Performing difficulty-based clustering using K-Means, extrapolating within each cluster independently, and then aggregating metrics across clusters without requiring subset-to-full mappings.\n2. COD complete: Complete multi-stage proposed approach consisting of clustering, pre- dictable cluster filtering, subset extrapolation, and subset-to-full mapping.\nThe comparative results across different benchmarks and estimation approaches are presented in Table 3. We evaluate prediction accuracy with the absolute error between predicted and actual performance. We report the prediction error on each single evaluation set and list the mean and the max prediction error.\nResults. Predictions with an absolute error of less than 2 percentage points (pp) are considered accurate estimations, and when the predicted values fall within the training metric fluctuation range, they are marked in green; predictions with an absolute error greater than 5 indicate invalid estimations, which reduce the overall reliability of the prediction method and are marked in red. These results show our approach significantly outperforms existing methods in both mean and maximum prediction errors, maintaining mean prediction error within 2 pp, thus offering practical guidance for large model"}, {"title": "5.3 Comparison of Clustering Methods", "content": "We evaluate the impact of clustering methods on the estimation approach. Our goal is to control the average distance between samples and cluster centers within clusters, making difficulty features more similar within clusters. We also ensure that the minimum number of questions in any cluster is not less than 10, considering that too small clusters may lead to instability in metric values. We compared our proposed Improved-MeanShift algorithm with clustering methods including DBScan, MeanShift, and K-Means. Since standard K-Means lacks the ability to filter outliers and directly control intra-cluster distances, we made the following adjustments: (1) Search for the number of clusters such that the minimum cluster size is close to but not less than 10 samples; (2) Draw spheres around cluster centers with a given threshold radius, and treat samples not covered by any sphere as outliers. If a cluster drops to less than 10 samples, we treat its samples as outliers."}, {"title": "5.4 Extrapolation Formula", "content": "To evaluate the effectiveness of different fitting formulas, we conducted an ablation study comparing various formulations of the accuracy-compute relationship. Our baseline formula incorporates random guess probability, exponential decay, and a constant offset term:\n$f(C) = g + (1 \u2212 g) * e^{-aC^{-b} - c}$\nTo understand the contribution of each component, we perform ablation experiments by removing or modifying different terms. 1) Without random guess component: $f_1(C) = e^{-aC^{-b} - c}$; 2) Without constant term c: $f_2(C) = g + (1 \u2212 g) * e^{-aC^{-b}}$; 3) Direct power law relationship [19]: $f_3(C) = e^{-aC^{-b}}$"}, {"title": "5.5 Anchor Point in Interpolation Mapping", "content": "During the mapping phase from estimable subset metrics to full evaluation set metrics, we discovered that models with different training data and architectures exhibit similar mapping relationships. This allows us to leverage metrics from pre-trained models to refine the mapping relationship, thereby improving the accuracy of our final metric estimation.\nWe use both Qwen2-72B [39] and an in-house model M with a Mixture-of-Experts(MoE) [22] structure trained on the same data distribution as anchor points in the mapping phase. We first obtain the interpolation curve using only small model metrics with points (0,0) and (1,1), then verify the compatibility of anchor points with the existing interpolation curve. When the score of the full set is 0 (1), the score of the subset must also be 0 (1). Results show that despite differences in computational requirements, model architectures, and training data, these models share similar mapping relationships.\nThis finding also indicates that estimable subset metrics are highly correlated with full-set metrics. Compared to loss-intermediate estimation, estimable subset metrics maintain predictability while reducing interference from other model parameters. Based on these observations, we incorporated the mapping relationships from pre-trained models into the interpolation process, thereby improving both the accuracy and confidence of our large model metric estimations.\nWe establish three experimental configurations of our method:\n\u2022 COD w/o. anchor: The complete estimation process is employed except for not using anchor point interpolation in the Mapping phase.\n\u2022 COD w. out-of-distribution (OOD) anchor: The complete proposed methodology incor- porates both difficulty-based clustering and predictable subset identification. Using the 72B Qwen2 pretraining model as the anchor model [39].\n\u2022 COD w. in-domain(ID) anchor: Using an in-house MoE model with consistent training distribution but different model architecture as the anchor point."}, {"title": "6 Conclusion and Discussion", "content": "In this work, we introduce a novel downstream performance scaling framework including (1) a difficulty-based clustering approach that effectively models the underlying distribution of each evaluation set; (2) a theoretically grounded scaling law for downstream task performance that provides a fitting formula for performance-compute extrapolation; and (3) a systematic methodology for identifying and leveraging predictable subset that provides a robust intermediate metric for accurate full-set performance predictions.\nOur framework, while effective for dense transformers, has not been fully explored for cost-efficient MoE models and does not account for the annealing phase in training, where high-quality data can rapidly enhance performance. The COD method requires sufficient test cases and is not suited for multiple-choice tasks, where performance metrics may diverge from true passrates. Additionally, the framework's theoretical foundation is insufficient for chain-of-thought reasoning, necessitating future adaptations to address these challenges.\nLooking forward, our approach can be further expanded across model architectures, training methods, and evaluation set types, while extending this framework to address chain-of-thought reasoning patterns offer promising avenues for future research."}, {"title": "A Improvements of Clustering Algorithm", "content": "A.1 Improved MeanShift Algorithm\nWe iteratively apply the MeanShift algorithm with a predefined cluster radius R and a minimum cluster size K. In each iteration, for the clustered samples, we examine whether the distance between each sample and its cluster center exceeds R, and relabel those samples that exceed this threshold as unclustered. For clusters containing fewer than K samples, we mark all samples in these clusters as unclustered. At the end of each iteration, we incorporate both the outliers from MeanShift and our marked unclustered samples into the next round of clustering, continuing this process until no further changes occur in sample labels. We present the pseudocode in Algorithm 1.\nFiltering Zero-performance Samples In the evaluation set, there may exist a few extremely difficult problems that require sufficient model parameters to emerge. All small models may fail to solve these problems even after 100 evaluation attempts, resulting in difficulty feature vectors of all zeros. We refer to these as zero-performance samples. Their presence leads to two issues:\n1. Zero performance on small models does not necessarily indicate zero accuracy on large models. For these samples, we cannot estimate when emergence will occur or predict large model metrics.\n2. During clustering, they may be confused with other low-performing but non-zero samples. Including them in the same cluster would lower the expected accuracy of that cluster, leading to inaccurate fitting and extrapolation later.\nTherefore, we pre-filter these zero-performance samples before clustering, treating them as outliers that do not participate in the clustering process. This approach eliminates the need to consider their metrics under large models during subsequent extrapolation and prevents disruption to the clustering of normal difficult samples.\nA.2 Smoothing Techniques\nHorizontal smoothing: adjacent checkpoint smoothing. Metric fluctuations of individual samples in downstream tasks are not solely due to limited sampling. Another potential factor is noise from uneven data distribution in recent training batches. Therefore, in addition to performing 100 evaluations to mitigate sampling variance, we evaluated 100 times on each of the adjacent checkpoints before and after the selected model. We then averaged these accuracy expectation values across three checkpoints, further reducing sampling variance while offsetting noise from uneven training data distribution. This approach also reduces the number of zero-performance samples, further improving clustering and prediction effectiveness."}, {"title": "Vertical smoothing", "content": "Each sample's features represent the expected correct response rate across models of increasing size, forming a partially ordered sequence. However, the Euclidean distance used for measurement does not consider this sequential information. For example, if a cluster center has a feature sequence of [0, 0, 0, 0.5], sample A with [0, 0, 0.2, 0.5] and sample B with [0.2, 0, 0, 0.5], sample A clearly fits the cluster better than sample B, yet their Euclidean distances are identical.\nNote that this smoothing method may not be effective for all downstream tasks. Our current observations suggest that for freeform tasks with limited solution spaces (such as multiple choice, ordering, or judgment questions in freeform format), once models learn to answer within the solution space, their random guess metrics on the evaluation set will be non-zero, more significantly affected by recent training batch data and few-shot cases in prompts. In such cases, vertical smoothing is more likely to bring positive benefits.\nIn our experiment, we only adopt horizontal smoothing, and leave vertical smoothing as an optional selection."}, {"title": "B Proof of Proposition", "content": "We use Proposition B.1 to derive scaling law for downstream task performance (Proposition B.2).\nProposition B.1 (Arithmetic-geometric mean difference). For any sequence of positive real numbers {$x_i$}$_{i=1}^{n}$, let:\n\u2022 $\u00b5_a = \\frac{1}{n} \\sum_{i=1}^{n} x_i$ be the arithmetic mean;\n\u2022 $\u00b5_g = (\\prod_{i=1}^{n} x_i)^{\\frac{1}{n}}$ be the geometric mean;\n\u2022 $\u03c3^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i \u2212 \u00b5_a)^2$ be the variance.\nThen the difference between the arithmetic mean and geometric mean can be estimated as:\n$\u00b5_a - \u00b5_g = \u00b5_a \\frac{1}{n} \\sum_{i=1}^{n} x_i - (\\prod_{i=1}^{n} x_i)^{\\frac{1}{n}} \\frac{\u03c3^2}{2\u00b5_a} + o(\u00b5_a)$    (4)\nProof. Taking the logarithm of the geometric mean $\u00b5_g$:\n$log(\u00b5_g) = \\frac{1}{n} \\sum_{i=1}^{n} log x_i$  (5)\nUsing Taylor expansion of log x around $\u00b5$:\n$log x = log \u00b5 + \\frac{x-\u00b5}{\u00b5} - \\frac{(x-\u00b5)^2}{2\u00b5^2} + o ((x \u2212 \u00b5)^2)$  (6)\nWe can simplify:\n$log(GM) = \\frac{1}{n} \\sum_{i=1}^{n} log x_i$\n$= log \u00b5 + \\frac{1}{n} \\sum_{i=1}^{n} \\frac{(x_i - \u00b5_a)}{\u00b5_a} - \\frac{(x_i - \u00b5_a)^2}{2\u00b5_a^2} + o(\u00b5_a)$\n$= log \u00b5 + \\frac{1}{n \u00b5_a} \\sum_{i=1}^{n}  x_i  - \\frac{1}{2\u00b5_a^2} \\sum_{i=1}^{n}(x_i - \u00b5_a)^2 + o(\u00b5_a)$\n$\\frac{equal to 0}{}= log \u00b5 -  \\frac{\u03c3^2}{2\u00b5^2} + o(\u00b5_a)$"}, {"title": "Therefore:", "content": "$\u00b5_a - \u00b5_g = \u00b5_a (1 - exp  (-\\frac{\u03c3^2}{2\u00b5_a^2})) + o(\u00b5_a)$    (7)\nWhen $\\frac{\u03c3^2}{2\u00b5_a^2}$ is small, this can be approximated as:\n$\u0394 \u2248 \\frac{\u03c3^2}{2\u00b5_a}$  (8)\nProposition B.2 (Scaling law for downstream task performance). Given a language model trained with computational budget C, and a set of downstream tasks P, under the following assumptions:\n1. The relationship between the answer loss and compute follows a power law, $\\frac{1}{n.} \\sum_{(q,ans)} loss_{ans} (C) \u223c aC^{\u2212b} + c$;\n2. For tasks with a finite answer set, the model gives a random guess choice if it cannot truly solve it;\n3. Task passrate equals the product of the predicted probability of each token $P_{ans} = \\prod_{t \\in ans} p(t)$, which means that each task has unique answer and model output answer only without thinking progress.\nThe expected accuracy on tasks P can be modeled as:\n$E_p[Acc(C)] = g + (1 \u2212 g) (e^{\u2212aC^{\u2212b} \u2212 c + \\frac{\u03c3^2}{2\u00b5}} + o(\u00b5))$ (9)\nwhere:\n\u2022 g represents the random guess performance floor;\n\u2022 a, b, c are positive constants;\n$\\mu = \\frac{1}{\\#P} \\sum_{(q,ans) \\in p} loss_{ans}$;\n$\\sigma^2 = \\frac{1}{\\#P} \\sum (loss_{ans_t} \u2212 \\mu)^2$.\nProof. We first use assumption 3 to establish the relationship between model passrate and loss on a task.\n$- log(p_{ans})  = -log( \\prod_{t \\in ans} p(t)) = \\sum_{t \\in ans} log(p_t) = loss_{ans}$ (10)\nThen take the exponential of both sides, and then take the expectation with respect to different tasks in the evaluation set p = (q, ans) \u2208 P. We note that both $p_{ans}$ and $loss_{ans}$ are functions of C.\n$E_p[p_{ans} (C)] = E_p [exp(\u2212loss_{ans} (C))]$\n$= \\frac{1}{n} \\sum_{(q,ans) \u2208P} exp(\u2212loss_{ans} (C)).$ (11) (12)\nWe can adopt Proposition B.1 to switch from arithmetic mean to geometric mean of loss, and apply the power law assumption 1.\n$\\frac{1}{n} \\sum_{(q,ans_t) \u2208 P} exp(\u2212loss_{ans} (C)) = exp \\frac{1}{n} \\sum loss_{ans} (C) +  \\frac{\u03c3^2}{2\u00b5} + o(\u00b5)$ (13)\nuse loss scaling law\n$= exp (\u2212aC^{\u2212b} \u2212 c) + \\frac{\u03c3^2}{2\u00b5} + o(\u00b5)$ (14)"}, {"title": "where n = #P, and \u03bc", "content": "\u03c3\u00b2 follow definitions in the proposition.\nFinally, we use assumption 2 to align the answer passrate and the accuracy metric. We can adopt the law of total expectation:\n$E_p[Acc(C)] = P_p(correct) E[Acc|correct] + P_p (incorrect)E[Acc|incorrect]$ (15)\nNote that $P_p(correct) = E_p[p_{ans} (C)]$, $E[Acc|correct] = 1$ and $P_p(incorrect) = 1 \u2212 P_p(correct)$.\nWe also define g as the random guess accuracy performance, thus we have $E[Acc|incorrect] = g$.\nTake these results into Eq. (15), and we have:\n$E_p[Acc(C)] = E_p[p_{ans} (C)] + (1 \u2212 E_p[p_{ans} (C)]) * g$  (16)\n$= g + (1 \u2212 g)E_p [p_{ans} (C)]$ (17)\n$= g + (1 \u2212 g) (e^{\u2212aC^{\u2212b} \u2212 c + \\frac{\u03c3^2}{2\u00b5}} + o(\u00b5))$  (18)\nProposition B.2 demonstrates that a metric of an evaluation set with similar difficulty features can be effectively modeled using the following formula:\n$f(C) = g + (1 \u2212 g) exp (\u2212aC^{\u2212b} \u2212 c)$  (19)"}, {"title": "C Additional Ablation Studies", "content": "C.1 Comparison of Clustering Methods on Extra Evaluation Sets.\nWe provide additional clustering evaluation results across more evaluation sets in Table C1 and Table C2, which maintain consistency with the conclusions presented in the main text.\nC.2 Interpolation Method\nTo evaluate different interpolation methods for prediction accuracy, we compared various mathematical approaches. Our baseline method uses quartic polynomial interpolation, which we compare against several alternative approaches, including Cubic spline interpolation, Cubic polynomial interpolation, and Quintic polynomial interpolation"}, {"title": "D Difficulty Distribution of Predictable Subset", "content": "We analyze the proportion of predictable subset tasks across different difficulty levels. The difficulty distributions of predictable subset versus complete sets for different evaluation benchmarks are illustrated in Fig. D2. We use the scores from the 12B model as the basis for difficulty classification. The results show that MMLU-pro and GSM8k evaluation sets have larger proportions of predictable subset, indicating that most questions in these datasets exhibit good performance scaling properties. In contrast, many difficult questions with near-zero scores in the Math evaluation set fall outside the predictable subset, requiring adjustment during the mapping phase. Meanwhile, BBH shows consistent proportions of predictable subset across difficulty levels, as some of its questions demonstrate oscillatory patterns with limited improvement despite increased computing.\nThe proportion of predictable subset can serve as a metric for assessing evaluation set quality. Evaluation sets with larger predictable subset yield more reliable experimental conclusions from smaller models. When constructing evaluation sets, we recommend screening or supplementing unpredictable clusters and ensuring a minimum number of questions for each difficulty feature to reduce metric volatility."}, {"title": "E Limitations", "content": "Influence of model structure and training configurations. Mixture-of-Experts (MoE) models excel in training and inference cost, and are widely used in production. In this work, we reveal the"}]}