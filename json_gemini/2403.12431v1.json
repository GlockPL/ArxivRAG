{"title": "GEOMETRIC CONSTRAINTS IN DEEP LEARNING FRAMEWORKS: A SURVEY", "authors": ["Vibhas K Vats", "David J Crandall"], "abstract": "Stereophotogrammetry is an emerging technique of scene understanding. Its origins go back to at least the 1800s when people first started to investigate using photographs to measure the physical properties of the world. Since then, thousands of approaches have been explored. The classic geometric techniques of Shape from Stereo is built on using geometry to define constraints on scene and camera geometry and then solving the non-linear systems of equations. More recent work has taken an entirely different approach, using end-to-end deep learning without any attempt to explicitly model the geometry. In this survey, we explore the overlap for geometric-based and deep learning-based frameworks. We compare and contrast geometry enforcing constraints integrated into a deep learning framework for depth estimation or other closely related problems. We present a new taxonomy for prevalent geometry enforcing constraints used in modern deep learning frameworks. We also present insightful observations and potential future research directions.", "sections": [{"title": "1 Introduction", "content": "Throughout this paper, we keep our focus on geometry-enforcing concepts used across different problems that either do depth estimation or are closely related to depth estimation problems. We only discuss the specific concepts used and their relevance to stereo or MVS depth estimation frameworks. This survey is organized in 8 sections. Starting from Sec. 2, we discuss the broad classification of geometric constraints presented in our taxonomy show in Fig. 2. For most of the sections, we first describe the a most common mathematical formulation of the concept that covers the majority of the methods and then, we describe different modifications applied to it by specific methods. Sec. 2 describes the traditional plane sweep algorithm and its variants. Sec. 3 focuses on all such geometric constraints that use alternate view(s) for enforcing consistency (cross-view consistency). Sec. 4 delves into geometric constraints that enforce structural consistency between a reference image and a target image to preserve the structural integrity of the scenes. Sec. 5 focuses on the orthogonal relation between depth and surface normal to guide geometric consistency. Sec. 6 discusses the integration of geometric constraints in attention mechanism and Sec. 7 presents the methods to enforce geometry-based representation learning in deep neural networks. We present our conclusion in Sec. 8."}, {"title": "2 Plane Sweep Algorithm", "content": "Stereophotogrammetry is the process of estimating the 3D coordinates of points on an object by utilizing measurements from two or more images of the object taken from different positions [33]. This involves stereo matching where two or more images are used to find matching pixels in the images and convert their 2D positions into 3D depths [8]. The process of finding matching pixels is based on the geometry of stereo matching (epipolar geometry), i.e the process of"}, {"title": "3 Cross-View Constraints", "content": "Cross-view constraints are applied to a scene with more than one view. It can be applied to stereo (two views per scene) and MVS (N > 2 view per scene) frameworks by projecting one view, either reference or source view, to the other view or vice-versa. Once projected to the other view, various constraints, like, photometric consistency, depth flow consistency, and view synthesis consistency can be utilized. An alternate way of utilizing cross-view constraints is to use forward-backward reprojection, where one view is projected to the other view and then it is back-projected to the first view to check the geometrical consistency of the scene. In this section, we discuss all such approaches that use cross-view consistency constraints in end-to-end deep learning-based frameworks."}, {"title": "3.1 Photometric Consistency", "content": "Photometric consistency minimizes the difference between a real image and a synthesized image from other views. The real and the synthesized images are denoted as reference (Iref) and source (Isrc) views in MVS, left (IL) and right (IR) or vice-versa in stereo problems. For video depth estimation, next frame (I+) is compared with the current frame (I0). The synthesized images are warped into real image views using intrinsic (K) and extrinsic (E) parameters. The warping process brings both real and synthesized images in the same camera view. The photometric loss is calculated as per Eqs. 1-3. Here, we use notations related to MVS problem formulation.\nTwo main variations of photometric loss are, pixel photometric loss and gradient photometric loss. As the name suggests, pixel photometric loss is the comparison between pixel values of these images, and gradient photometric loss is the comparison of the gradients of these images. Sometimes, pixel photometric loss and gradient photometric loss are combined for a more robust form of photometric loss, called robust photometric loss. Eqs. 1, 2 and 3 shows the pixel, gradient, and robust formulation of photometric loss for MVS methods.", "equations": ["\\mathcal{L}_{photopixel} = \\frac{||(I_{ref} - \\hat{I}_{src\\rightarrow ref}) \\odot M||_{l_1}}{||M||_1} \\tag{1}", "\\mathcal{L}_{photograd} = \\frac{||(\\nabla I_{ref} - \\nabla \\hat{I}_{src\\rightarrow ref}) \\odot M||_{l_1}}{||M||_1} \\tag{2}", "\\mathcal{L}_{photorobust} = \\lambda_1 \\cdot \\mathcal{L}_{photopixel} + \\lambda_2 \\cdot \\mathcal{L}_{photograd} \\tag{3}"]}, {"title": "3.2 Geometric Consistency", "content": "Just like photometric consistency, geometric consistency also involves cross-view consistency checks with projections. For photometric consistency, one view, either reference or source, is warped to another view to calculate the consistency error. Geometric consistency employ forward-backward reprojection (FBR) to estimate the error. FBR involves a series of projections of the reference view to estimate the geometric error, see Alg. 1. First, we project the reference image (I) to the source view (Ir\u2192s), then, we remap the projected reference view Ir\u2192s to generate Isremap and finally, we back-project Isremap to the reference view to obtain Irzs. Irzs is then compared with original I to estimate the photometric error.\nDong and Yao [50] use cross-view geometric consistency by applying FBR in the pixel domain in an unsupervised MVS pipeline. Once the FBR steps are done, the actual pixel values between the original reference image Ir and back-projected reference images Irs are used to check the geometrical consistency of the depth estimates. It diminishes the matching ambiguity between reference and source views. The following equation shows its mathematical formulation", "equations": ["\\mathcal{L}_{geometric} = \\frac{1}{N} \\sum_{i=1}^{N} |I_{ref} - \\hat{I}_{ref_{src}} | \\odot M_{ref} \\tag{7}"]}, {"title": "3.3 Cross-View Depth-Flow Consistency", "content": "Depth-flow estimations are typically used in optical flow problems [56, 57]. But it can easily be adapted in MVS problems by estimating flow from estimated depth maps as well as from input RGB images and comparing them. Xu et al. [46] propose a novel flow-depth consistency loss to regularize the ambiguous supervision in the foreground of depth maps. Estimation of flow-depth consistency loss requires two modules for an MVS method, RGB2Flow and Depth2Flow. As the name suggests, the Depth2Flow module transforms the estimated depth maps to virtual optical flow between the reference and arbitrary source view and the RGB2Flow module uses [56] to predict the optical flow from the corresponding reference-source pairs. The two flows predicted should be consistent with each other.\nDepth2Flow module In an MVS system, cameras move around the object to collect multi-view images. If we consider the relative motion between the object and the camera, the camera can be assumed to be fixed and the object can be assumed to be in motion towards the virtually still camera, see Fig. 4. This virtual motion can be represented as a dense 3D optical flow and it should be consistent with the 3D correspondence in real MVS systems. The virtual flow for a pixel pi can be defined as", "equations": ["F_{r\\rightarrow s} = Norm[K_s \\cdot E_s (K_r \\cdot T_r)^{-1} I_r(p_i)] - p_i \\tag{8}"]}, {"title": "3.4 View Synthesis Consistency", "content": "Most of the monocular, stereo, and MVS depth estimation frameworks use ground truth as a supervision signal. While these frameworks may utilize the additional source view images in the pipeline, they always estimate only one depth map, the reference view depth map. Estimation and use of only one depth map may not provide enough geometrical information for consistent estimation. To address this gap, many methods [31, 20, 49, 54] synthesis additional view (see Fig. 5), either depth map or RGB image (commonly referred to as target view), using camera parameters and reference view information. This additional view, when included in the training framework, provides additional geometric consistency information during the learning process. In this section, we discuss methods that utilize view synthesis in a depth estimation framework.\nBauer et al. [31] use view synthesis in a monocular depth estimation framework. They apply two networks, depth network (DepNet) and synthesis network (SynNet) in a series of operations to enforce geometric constraints with view synthesis. First, the RGB input (source view) is used in DepNet to generate a corresponding depth estimate. The estimated depth map is projected to a target view and using SynNet the holes in the target view are filled. Finally, the synthesized RGB target view is used as input to DepNet to estimate its depth map. This ensures that the DepNet learns to estimate geometrically consistent depth estimates of both the source and the synthesized target view. They use L1 loss to enforce consistency.\nYang et al. [20] also synthesize RGB target view to improve geometric consistency in video depth estimation. With estimated pixel matching pairs between source (I5) and target views (It), they synthesis a target view \\hat{I}_t using the source view, camera parameters, and bilinear interpolation [19]. To handle occlusion and movement of objects, an explainability mask (Ms) is applied during the loss calculation given as", "equations": ["\\mathcal{L}_{view Synthesis} = \\sum_{s=1}^{S} ||I_t - \\hat{I}_t \\odot M_s|| \\tag{11}"]}, {"title": "4 Geometry Preserving Constraints", "content": "Apart from utilizing cross-view consistency constraints, there are different ways of enforcing structural consistency in a depth estimation pipeline. In this section, we discuss all such approaches that utilize alternative methods of enforcing geometric constraints. We have classified these methods into four broader categories, i.e. structural similarity index measurement (SSIM), edge-aware smoothness constraints, consistency regularization, and planar consistency. We discuss each of these approaches in detail and highlight the research works that utilize these methods in their pipeline."}, {"title": "4.1 Structural Similarity Index Measurement", "content": "Objective image quality metrics are roughly classified into three categories based on the availability of distortion-free (original or reference) images. The metric is called full-reference when the complete reference image is known, it is called no-reference when the reference image is not available and it is called reduced-reference when the reference image is partially available. Eskicioglu and Fisher [61] discuss several such image quality metrics and their performance.\nStructural similarity index measurement (SSIM) is a full-reference image quality assessment technique [60]. Its assessment is based on the degradation of structural information between the reference and the noisy image. Specifically, it compares local patterns of pixel intensities that have been normalized for luminance and contrast. Luminance of a surface is the product of its illumination and reflectance, but the structure of the object is independent of illumination. SSIM separates the influence of illumination to analyze the structural information in an image.\nWang et al. [60] define structural information as the attributes that represent the structural information of objects in an image, independent of average luminance and contrast. As shown in Fig. 6, given two aligned non-negative signals"}, {"title": "4.2 Edge-Aware Smoothness Constraint", "content": "The smoothness constraint finds its origin in the optical flow estimation problem. It was first applied by Uras et al. [66] to estimate consistent optical flow from two images. Brox et al. [67] further explained the concept under three assumptions for the optical flow framework, i.e. the gray value constancy assumption, the gradient constancy assumption, and the smoothness assumption.\nSince the beginning of the optical flow estimation problem, it has been assumed that the gray value of a pixel does not change on displacement under a given lighting condition, Eq. 25 [67]. But brightness changes in a natural scene all the time. Therefore, it is considered useful to allow small variations in gray values but finds a different criterion that remains relatively invariant under gray value changes, i.e. the gradient constancy was assumed under displacement, Eq. 26. This brought about the third assumption, the smoothness of the flow field. While discontinuities are assumed to be present at the boundaries of the object in the scene, a piecewise smoothness can be assumed in the flow field [67]. To achieve this smoothness in flow estimation a penalty on the flow field was applied as shown in Eq. 27.\nIn the optical flow framework, objects are assumed to be moving with a fixed camera, in an MVS framework, the objects are assumed to be fixed and the camera moves concerning a fixed point. The relative motion of an object can be viewed as a moving camera to pose it as an MVS problem, see Fig. 4. With this assumption, the smoothness constraint can be applied to the depth estimation problem. Initially, only the first-order smoothness constraints were used in the depth estimation framework [27, 19, 26, 64, 25, 52]. After Yang et al. [20] used second-order smoothness constraint for regularization was combined with the first-order smoothness constraint in subsequent works [54, 22, 50, 68].", "equations": ["I(x,y,t) = I(x + u, y + v, t + 1) \\tag{25}", "\\nabla I(x, y, t) = \\nabla I(x + u, y + v, t + 1) \\tag{26}", "E_{smooth} (u, v) = \\int \\psi(\\sqrt{|\\nabla u|^2 + |\\nabla v|^2})dx \\tag{27}"]}, {"title": "4.3 Consistency Regularization", "content": "Deep learning-based frameworks inherently suffer from over-parameterization problems. One of the most efficient methods to counter it is to regularize the loss function. Photometric consistency, Sec. 3.1, which enforces geometrical consistency at the pixel level is highly susceptible to change in lighting conditions. Many MVS methods employ different consistency regularization techniques to effectively handle this problem [20, 19, 51].\nAs discussed in Sec 4.2, first-order and second-order gradients are often used for this task [20, 19]. Garg et al. [19] argue that the photometric loss is non-informative in homogeneous regions of a scene, which leads to multiple warps generating similar disparity outcomes. It uses L2 regularization (Eq. 31) on the disparity discontinuities as a prior. It also recommends the use of other robust penalty functions used in [67, 70] as an alternative regularization term. Yang et al. [20] use a spatial smoothness penalty with L1 norm of second-order gradient of depth, Eq. 32. It encourages depth values to align in the planar surface when no image gradient appears.\nXu et al. [51] apply consistency regularization in the semi-supervised MVS method. The proposed regularization minimizes the Kullback-Leibler (KL) divergence between the predicted distributions of augmented (PV) and non-augmented (PV) samples. With the K depth hypothesis, the probability volume PV, of size H \u00d7 W \u00d7 K, is separated into K categories of HW logits. Eq. 33 shows the formulation of the regularizer, where pi represents a pixel coordinate.", "equations": ["\\mathcal{L}_v = ||\\nabla D||^2 \\tag{31}", "\\mathcal{L}_{v2} = \\sum D \\cdot e^{-\\alpha |\\nabla I|}; \\alpha > 0 \\tag{32}", "\\mathcal{L}_{KL} = \\frac{1}{HW} \\sum_{i=1}^{HW} KL(PV_{p_i}^{NA}, PV_{p_i}^{A}) \\tag{33}"]}, {"title": "4.4 Structural Consistency in 3D Space", "content": "Structural consistency is not limited to the 2D image plane, it can easily be extended to camera 3D space or 3D point clouds. In this section, we discuss two such methods [52, 25] that uses structural consistency in 3D space alongside other geometric constraints in an end-to-end framework."}, {"title": "4.4.1 Planar Consistency", "content": "Planar consistency [52] is based on the assumption that most of the homogeneous-color regions in an indoor scene are planar regions and a continuous depth can be assumed for such regions. Extraction of such piece-wise planar regions is"}, {"title": "4.4.2 Point Cloud Alignment", "content": "Mahjourian et al. [25] use another approach to align the 3D point clouds of two consecutive frame (Qt\u22121, Qt) in video depth estimation pipeline. It directly compares the estimated point cloud associated with respective frames (Qt\u22121 and Qt), i.e. compare Qt\u22121 to Qt\u22121 or Qt to Qt using well know rigid registration methods, Iterative Closest Point (ICP) [72, 73, 74], that computes a transformation to minimize the point-to-point distance between two point clouds. It alternates between computing correspondences between 3D points and best-fit transformation between the two point clouds. For each iteration, it recomputes the correspondence with the previous iteration's transformation applied.\nICP is not differentiable, but its gradients can be approximated using the products it computes as part of the algorithm, allowing back-propagation. It takes two point clouds A and B as input and produces two outputs. First is the best-fit transformation T' which minimizes the distance between the transformed points in A and B, and second is the resudual rij, Eq. 39, which reflects the residual distances between corresponding points after ICP's minimization. The loss, L3DAlignment, is given as in Eq. 40.", "equations": ["r_{ij} = A_{ij} - T'^{-1} \\cdot B_{c(ij)} \\tag{39}", "L_{3DAlignment} = ||T' - I||_1 + ||r||_1; I = 1 \\tag{40}"]}, {"title": "5 Normal-Depth Orthogonal Constraint", "content": "Surface normal is an important 'local' feature of 3D point-cloud of a scene, which can provide promising 3D geometric cues to estimate geometrically consistent depth maps. In the 3D world coordinate system, the vector connecting two pixels in the same plane should be orthogonal to their direction of normal. Enforcing normal-depth orthogonal constraint tends to improve depth estimates in 3D space [75, 27, 20].\nDepth to normal: Given depth Di, to estimate the normal of each central pixel pi, Fig. 8 (left), we need to consider the neighboring pixels, Pneighbors. The Fig. 8 (left) shows 8 neighbor convention to compute the normal of the central pixel. Two neighbors, Pix and piy are selected from Pneighbors for each central pixel pi with depth value as Di and camera intrinsics K to estimate normal N\u2081. First, we project the depth in 3D space and then, use cross product between vector PiPix and PiPiy to estimate the normal, Eq. 42. With 8 such estimates of normal \u00d1\u00bf for each central pixel, we estimate the final normal N\u2081 as the mean value of all the estimates using Eq. 43.\nNormal to depth: Many methods [22, 20, 76] use normal to depth estimation to refine the depth values Di using the orthogonal relationship. For each pixel pi(xi, Yi), the depth of each neighbor Pneighbor should be refined. The corresponding 3D points are, Pi and Pneighbor and central pixel Pi's normal Ni(na, ny, n\u2082). The depth of Pi is Di and Pneighbor is Dneighbor. Using the orthogonal relations N 1 PiPneighbor, we can write the Eq. 44. With depth estimates coming from eight neighbors, we need a method for weighting these values to incorporate discontinuity of normal in some edges or irregular surfaces. Generally, the weight w\u2081 is inferred from the reference image I\u2081, making depth more conforming to geometric consistency. The value of wi depends on the gradient between pi and Pneighbor, Eq. 45. The bigger values of gradient represent the less reliability of the refined depth. Given the eight neighbors, the final refined depth \u010eneighbor is a weighted sum of eight different directions as shown in Eq. 46 [22]. This is the outcome of the regularization in 3D space, improving the accuracy and continuity of the estimated depth maps.", "equations": ["p_i = K^{-1}D_i p_i \\tag{41}", "N_i = \\overrightarrow{p_i p_{ix}} \\overrightarrow{p_i p_{iy}} \\tag{42}", "N_i = \\frac{1}{8} \\sum_{i=1}^8 (\\tilde{N_i}) \\tag{43}"]}, {"title": "6 Attention Meets Geometry", "content": "A transformer with a self-attention mechanism is introduced by Dosovitski et al. [12] in vision domain. It can learn long-range global-contextual representation by dynamically shifting its attention within the image. The inputs to the attention module are usually named query (Q), key (K), and value (V). Q retrieves information from V based on the attention weights, Eq. 53, where A(.) is a function that produces similarity score as attention weight between feature embeddings for aggregation.\nThe performance of stereo or MVS depth estimation methods depends on finding dense correspondence between reference images and source images. Recently, Sun et al. [81] showed that features extracted using a transformer model with self- and cross-attention can produce significantly improved correspondences as compared to the features extracted using convolutional neural networks. These attention mechanisms are designed to pay attention to the contextual information and not to geometry-based information. Recently, a few methods have modified these attention mechanisms to consider geometric information while calculating the attention weight [82, 30, 83, 84]. In this section, we discuss such methods and their approach to include geometric information in attention.", "equations": ["Attention(Q, K, V) = A(Q, K)V \\tag{53}", "p_i = K^{-1}(d_i . C_i); p_j = K^{-1}(d_j.C_j) \\tag{54}", "A^{spatial}_{i,j} = exp(-\\frac{||p_i - p_j||^2}{\\sigma^2}) \\tag{55}", "A^{temporal} = Softmax; (F_i^T.F_j) \\tag{56}"]}, {"title": "7 Learning Geometric Representations", "content": "Apart from utilizing direct methods of enforcing geometric constraints or providing geometric guidance or exploiting orthogonal relations between depth and normal, there are some indirect ways of learning geometrically and structurally consistent representations. For example, features with high-level of semantic information is more likely to retain structural consistency of objects compared to features with low-level of semantic information, pseudo-label generation purely on the basis of geometric consistency can be utilized for self-supervision, more robust feature representation can be learned using suitable data-augmentation techniques, attaching semantic segmentation information of objects or using co-segmentation can also provide suitable clues for the structural consistency of objects, contrastive learning with positive and negative pairs can guide a model to learn better representation which are geometrically sharp and consistent. We discuss all such methods in this section."}, {"title": "7.1 High-Level Feature Alignment Constraints", "content": "In deep learning-based frameworks for depth estimation, the quality of the extracted features directly impacts the quality of depth estimates. The poor quality of extracted features can greatly impact the local as well as global structural pattern. One way to handle this problem is by guiding the extracted features with better representation from an auxiliary pre-trained network. While the integrated feature extraction network in the depth estimation pipeline can learn useful features, it still lags in learning higher-level representations compared to a network explicitly designed to learn deep high-level representations like, VGG [85], Inception [86], ResNet [87] etc. To enforce the feature alignment constraint, Johnson et al. [88] propose two loss functions, feature reconstruction loss and style reconstruction loss.\nFeature reconstruction loss, Eq. 66, encourages the model to generate source features similar to the target features at various stages of the network [88]. Minimizing feature reconstruction loss for early layers improves local visual as well as structural patterns, while minimizing it for higher layers improves overall structural patterns [88]. Feature reconstruction loss fails to preserve color and texture, which is handled by style reconstruction loss [88].", "equations": ["\\mathcal{L}_{feature} = ||F_{target} - F_{source}||_{L_i} \\tag{66}", "\\mathcal{L}_{feature} = \\frac{1}{N} \\sum (F_{ref} - F_{src}) . M_{ref} \\tag{67}"]}, {"title": "7.2 Pseudo-Label Generation with Cross-View Consistency", "content": "In self-supervised MVS frameworks, one of the effective methods of applying geometric constraints is by generating pseudo-labels. Generating pseudo-labels requires application of cross-view consistency constraints, which encourages the MVS framework to be geometrically consistent during training and evaluation [21, 47]. Since the model learns with self-supervision, it also helps with the challenging task of collecting multi-view ground truth data. In this section, we discuss three methods of generating pseudo-labels for self-supervision, labels from high-resolution training images [21], sparse pseudo-label generation [47] and semi-dense pseudo-label generation [47].\nYang et al. [21] apply pseudo-label learning in four steps. First, they estimate the depth map based on photometric consistency, Sec. 3.1, using a coarse network (low-resolution network). With the initial pseudo-label in hand, they apply a two-step iterative self-training to refine these pseudo-labels, see Fig. 11. They utilize fine-network (high-resolution network) to refine the initial coarse pseudo-labels utilizing more discriminative features from high-resolution images. The fine-network estimates high-resolution labels which are then filtered with a cross-view depth consistency check, Sec. 3, utilizing depth re-projection error to measure pseudo-label consistency. Finally, the high-resolution filtered pseudo-labels from N different views are fused using a multi-view fusion method. It generates a more complete point cloud of the scene. The point cloud is then rendered to generate cross-view geometrically consistent new pseudo-labels to guide the coarse network depth estimation pipeline.\nLiu et al. [47] use two geometric prior-guided pseudo-label generation methods, sparse and semi-dense pseudo-label. For sparse label generation, they use a pre-trained Structure from Motion framework (SfM) [90] to generate a sparse point cloud. This sparse point cloud is then projected to generate sparse depth pseudo-labels. Since, the sparse point cloud can provide very limited supervision, they use a traditional MVS framework that utilizes geometric and photometric consistency to estimate preliminary depth maps, like COLMAP [91]. The preliminary depth map then undergoes cross-view geometric consistency checking process to eliminate any outliers. This filtered depth map is then used as a final pseudo-label for learning."}, {"title": "7.3 Data-Augmentation for Geometric Robustness", "content": "Deep-learning frameworks can always do better with more data [92], but collecting data for stereo or MVS setup is a difficult task. Applying data augmentation to these frameworks naturally makes sense, but it is not as easy to implement. The natural color fluctuation, occlusion, and geometric distortions in augmented images disturbs the color constancy of images, affecting the effective feature matching and hence, the performance of the whole depth estimation pipeline [18]. Because of these limitations, it has seldom been applied in either supervised [15, 13, 14] or unsupervised [48, 54, 22] MVS methods.\nKeeping these limitations in mind, Garg et al. [19] use three data-augmentation techniques in an unsupervised stereo depth estimation problem. They use color change \u2013 scalar multiplication of color channels by a factor c \u2208 [0.9, 1.1],"}, {"title": "7.4 Semantic Information for Structural Consistency", "content": "Humans can perform stereophotogrammetry well in ambiguous areas by exploiting more cues such as global perception of foreground and background, relative scaling, and semantic consistency of individual objects. Deep learning-based frameworks, operating on color constancy hypothesis [18], generally provide a superior performance as compared to traditional MVS algorithms, but both methods fail at featureless regions or at any such regions with different lighting conditions, reflections, or noises, color constancy ambiguity. Direct application of geometric and photometric constraints in such regions is not helpful, but high-level semantic segmentation clues can help these models in such regions. Semantic segmentation clues for a given scene can provide abstract matching clues and also act as structural priors for depth estimation [18]. In this section, we explore such depth estimation methods that include semantic clues in their pipeline.\nInspired by Cheng et al. [24], which incorporate semantic segmentation information to learn optical flow from video, Yang et al. [29] incorporate semantic feature embedding and regularize semantic cues as the loss term to improve disparity learning in stereo problem. Semantic feature embedding is a concatenation of three types of features, left-image features, left-right correlation features, and left-image semantic features. In addition to image and correlation features, semantic features provide more consistent representations of featureless regions, which helps solve the disparity problem. They also regularize the semantic cues loss term by warping the right image segmentation map to the left view and comparing it with the left image segmentation ground truth. Minimizing the semantic cues loss term improves its consistency in the end-to-end learning process. Dovesi et al. [28] also employ semantic segmentation networks in coarse-to-fine design and utilize additional information in stereo-matching problems.\nAnother way of utilizing semantic information for geometric and structural consistency is through co-segmentation. Co-segmentation method aims to predict foreground pixels of the common object to give an image collection [93]. Inspired by Casser et al. [94], which applied co-segmentation to learn semantic information in unsupervised monocular depth ego-motion learning problem, Xu et al. [18] apply co-segmentation on multi-view pairs to exploit the common semantics. They adopt non-negative matrix factorization (NMF) [95] to excavate the common semantic clusters among multi-view images during the learning process. NMF is applied to the activations of a pre-trained layer [96] to find semantic correspondences across images. We refer to Ding et al. [95] for more details on NMF. The consistency of the co-segmentation map can be expanded across multiple views by warping it to other views. The semantic consistency loss is calculated as per pixel cross-entropy loss between the warped segmentation map (\u015c\u2081 and the ground truth labels converted from the reference segmentation map S as follows", "equations": ["\\mathcal{L}_{semantic} = - \\frac{1}{HW} \\sum_{i=2}^N ||M_i||  \\sum_{j=1}^{HW} f(S_j)log(\\hat{S}_{i,j}) M_{ij} \\tag{71}"]}, {"title": "7.5 Geometric Representation Learning with Contrastive Loss", "content": "Contrastive learning [97] learns object representations by enforcing the attractive force to positive pair and the repulsive pair to negative pair [98]. This form of representation learning has not been explored much in a depth estimation problem. There is only a handful of research works that use contrastive learning for depth estimation [99, 100, 98]. Fan et al. [100] use contrastive learning to pay more attention to depth distribution and improve the overall depth estimation process by adopting a non-overlapping window-based contrastive learning approach. Lee et al. [99] use contrastive learning to disentangle the camera and object motion. While these methods use contrastive learning for estimating depth maps none of them use contrastive learning to promote the geometric representation of objects."}, {"title": "8 Conclusion", "content": "The instrumental progress in deep learning technologies has immensely benefited the depth estimation frameworks. It has enabled the extraction of high-level representations from input images for enhanced stereo matching. However, it has also limited the use of modeling explicit photometric and geometric constraints in the learning process. Most supervised stereo and MVS methods focus on better feature extraction, and enhanced feature matching through attention mechanism but apply a plane-sweep algorithm as the only geometric constraint. They largely depend on the quality of ground truth to learn about geometric and structural consistency in the learning process. In this review, we have comprehensively reviewed geometric concepts in depth estimation and its closely related domains that can be coupled with deep learning frameworks to enforce geometric and structural consistency in the learning process. explicitly modeling geometric constraints, along with the supervision signal, can enforce structural reasoning, occlusion reasoning,"}]}