{"title": "Bridging Swarm Intelligence and Reinforcement Learning", "authors": ["Karthik Soma", "Yann Bouteiller", "Heiko Hamann", "Giovanni Beltrame"], "abstract": "Swarm intelligence (SI) explores how large groups of simple individuals (e.g., insects, fish, birds) collaborate to produce complex behaviors, exemplifying that the whole is greater than the sum of its parts. A fundamental task in SI is Collective Decision-Making (CDM), where a group selects the best option among several alternatives, such as choosing an optimal foraging site. In this work, we demonstrate a theoretical and empirical equivalence between CDM and single-agent reinforcement learning (RL) in multi-armed bandit problems, utilizing concepts from opinion dynamics, evolutionary game theory, and RL. This equivalence bridges the gap between SI and RL and leads us to introduce a novel abstract RL update rule called Maynard-Cross Learning. Additionally, it provides a new population-based perspective on common RL practices like learning rate adjustment and batching. Our findings enable cross-disciplinary fertilization between RL and SI, allowing techniques from one field to enhance the understanding and methodologies of the other.", "sections": [{"title": "1 INTRODUCTION", "content": "Swarm Intelligence (SI) takes inspiration from how a collective of natural entities, following simple, local, and decentralized rules, can produce emergent and complex behaviors [3]. Researchers have extracted core principles such as coordination, cooperation, and local communication from these natural systems, and applied them to artificial systems, (e.g., swarm robotics [7, 8] and optimization algorithms [6]).\nIn this paper, we focus the specific SI problem of Collective Decision Making (CDM). In CDM, individuals work together to reach an agreement on the best option from a set of alternatives, a problem commonly called the best-of-n decision problem. To solve this problem, researchers have turned to opinion dynamics [26], a field that studies how opinions spread in a population. In particular, in the voter rule [4, 12], an individual copies the opinion of a randomly chosen neighbor. Similarly, researchers have taken inspiration from"}, {"title": "2 PRELIMINARIES", "content": "Multi-armed bandits are one of the simplest types of environments encountered in RL literature. They consist of a discrete set of available actions, called \"arms\", amongst which an agent has to find the most rewarding. In an n-armed bandit, pulling arm $a \\in \\{1, ..., n\\}$"}, {"title": "2.1 Multi-armed bandits and Cross Learning", "content": "returns a real-valued reward $r_a \\in [0, 1]$ sampled from a hidden distribution $r(a)$. The objective for an RL agent playing a multi-armed bandit is to learn a policy, denoted by the probability vector $\\pi = (\\pi_1,....\\pi_n)$, that maximizes the rewards obtained upon pulling the arms. Different exploration strategies exist to find such policies, one of them being Cross Learning [5]:\nCross Learning (CL). Let $k$ be an action and $r_k$ a corresponding reward sample ($r_k \\sim r(k)$). CL updates the policy $\\pi$ as:\n$\\pi_a \\leftarrow\\begin{cases}\n1 - \\alpha \\pi_a \\text{ if } a = k \\\\\n-\\alpha \\pi_a \\text{ otherwise }\n\\end{cases}$\nFor convenience, we denote the expected policy update on action $a$'s probability $\\pi_a$ when sampling reward $r_k$ from action $k$ as:\n$d\\pi_a(k) = E_{r_k\\sim r(k)} [\\begin{cases}\n1 - \\pi_a \\text{ if } a = k \\\\\n-\\pi_a \\text{ otherwise }\n\\end{cases}]$\nIn CL, every reward $r_k$ sampled when applying the associated action $k$ directly affects the probabilities accorded by policy $\\pi$ to all available actions. As noted earlier, CL is closely related to the Gradient Bandit algorithm, which performs a similar update at the parameter level (called \"preferences\u201d) of a parametric policy rather than directly updating the probability vector."}, {"title": "2.2 Evolutionary game theory", "content": "Evolutionary game theory (EGT) studies population games [15]. In a single-population game, a population P is made of a large number of individuals, where any individual i is associated with a type, denoted by $T_i \\in \\{1, ..., n\\}$. The population vector $\\pi = (\\pi_1, ..., \\pi_n)$ represents the fraction of individuals in each type ($\\Sigma_i \\pi_i = 1$). Individuals are repeatedly paired at random to play a game, each receiving a separate payoff defined by the game bi-matrix\u00b2 A. Individuals adapt their type based on these payoffs according to an update rule. One notable such rule is imitation of success [15]:\nImitation Dynamics: Any individual $i \\in P$ of type $T_i = a$ follows the voter rule $R_{voter}$:\n(1) i samples a random individual $j \\sim U(P)$ to imitate. Let $T_j$ be $b$.\n(2) Both individuals i and j play the game defined by A to receive payoffs $r_a$ and $r_b$ respectively (0 \u2264 $r_{a,b}$ \u2264 1). In general, each payoff may depend on the types of both individuals.\n(3) i switches to type b with probability $r_b$.\nOne can easily see why this rule is called \"imitation of success\": i imitates j based on j's payoff. When aggregated to the entire population, imitation of success yields a famous equation in EGT, called the Taylor Replicator Dynamic [16, 21] (TRD) (see Lemma 2):\n$\\dot{\\pi_a} = \\pi_a(q_a - v^{\\pi}),$\nwhere $\\dot{\\pi_a}$ is the derivative of the a-th component of the population vector $\\pi$, $q_a := E[r_a]$ is the expected payoff of the type a against the current population, and $v^{\\pi} := \\Sigma_i \\pi_iE[r_i]$ is the current average payoff of the entire population. Further, we describe another useful"}, {"title": "2.3 Collective-decision making in swarms", "content": "Consider a population P of N individuals trying to reach a consensus on which amongst n available options is the optimal. Similar to population games, each individual i has an opinion, denoted by $Op_i \\in \\{1, ..., n\\}$, about which option they prefer. We again call the population vector $\\pi = (\\pi_1,..., \\pi_n)$, which in this context represents the fraction of individuals sharing each opinion. The weighted voter rule models the dance of honey bees during nest-hunting [14]:\nWeighted voter rule: Any individual $i \\in P$ of opinion $Op_i = a$ follows the weighted voter rule $R_{wvoter}$:\n(1) i estimates the quality of its current opinion $r_a \\sim r(a)$, where 0 \u2264 $r_a$ \u2264 1.\n(2) After obtaining $r_a$, i locally broadcasts its opinion at a frequency proportional to $r_a$.\n(3) i switches its opinion to the first opinion b that it perceives from its neighborhood. Assuming all individuals are well mixed in the population [11], the corresponding expected probability of i switching to opinion b is the proportion of votes cast for b:\n$P(b \\leftarrow a) = \\frac{N_bE[r_b]}{\\Sigma_l \\pi_lE[r_i]}$ (where $N_k$ is the number of individuals\nof opinion k). This probability can further be written $\\frac{E[r_b]}{\\Sigma_l \\pi_lE[r_i]}$\nby dividing both the numerator and the denominator by N.\nNote that in this model, bees do not directly observe the quality estimate of other individuals, but only their opinion. This makes the weighted voter rule well-adapted to swarms of communication-limited organisms."}, {"title": "3 THEORY", "content": "REMARK 1. Population-policy equivalence. As noted by [2], a population vector $\\pi = (\\pi_1, ..., \\pi_n)$ can be abstracted as a multi-armed bandit RL policy (and vice-versa). In this view, uniformly sampling an individual of type a from the population is equivalent to sampling action a from the policy."}, {"title": "3.1 Voters and Cross Learning", "content": "PROPOSITION 1. An infinite population of individuals following $R_{voter}$ can be seen as an RL agent following Exact Cross Learning,\ni.e.,\n$d^{voter}\\pi_a = E_{k\\sim \\pi, r_k \\sim r(k)} [d^{CL}\\pi_a(k, r_k)],$\nwhere \u03c0 can be seen as both the population vector and the vector of action-probabilities under the population-policy equivalence, $d^{voter} \\pi$\nis the single-step change of population \u03c0 under the voter rule (i.e., the change in type proportions after all individuals simultaneously perform $R_{voter}$ once), and $d^{CL} \\pi(k, r_k)$ is the update performed by CL on the policy \u03c0 for a given action-reward sample (k, rk).\nTo prove Proposition 1, we use two intermediate results (Lem-mas 1 and 2). These results are already known from the literature (although to the best of our knowledge we are the first to integrate them and apply them in this context). We provide proofs using our"}, {"title": "3.2 Learning rate and batch-size", "content": "Instead of studying the mean-field effect of aggregated individual voters, we can look at the individual effect of each voter on the entire population. This effect yields interesting insights regarding two common practices in RL: adjusting the learning rate, i.e., scaling down RL updates by a small factor, and batching, i.e., averaging RL updates over several samples.\nInstead of an infinite population, let us consider a near-infinite population P of N > 1 individuals. Again, we describe P by the population vector \u03c0. A single individual i of type k sampling payoff $r_k$ and following $R_{voter}$ has the following influence (outflow from a to k) on the population vector for types $a \\neq k$:\n$\\forall a \\neq k, P^{(i)}(k \\leftarrow a) =  \\frac{\\pi_a}{N}$\nwhile its influence on type k is the sum of inflows (to k):\n$ \\frac{1}{N} \\Sigma_{a\\neq k} p^{(i)} (k\\leftarrow a)= (1 - \\pi_k) \\frac{1}{N} \\pi_k.$"}, {"title": "3.3 Swarms and Maynard-Cross Learning", "content": "Arguably, the meaning of Eq. 14 is non-intuitive from the population perspective: it describes the effect of a single individual i on the"}, {"title": "4 METHODS", "content": "We present two types of experiments to validate the findings from the previous section. First, we implement the two RL update rules, CL and MCL in two variants: batched and non-batched. Second, we conduct population-based experiments using Rvoter and Rwvoter (VR, WVR) for different population sizes. Moreover, we also numerically simulate the TRD and MRD to show how the above experiments compare with the analytical solutions."}, {"title": "4.1 Environment", "content": "We consider the standard multi-armed stateless bandit setting described in preliminaries (see Sec. 2.1). As it is clear from Remark 1, we can use the same environment for RL and population experiments. The environment in consideration returns rewards sampled from the hidden distribution r(a) when a is pulled. A normal distribution N(Q, \u03c3\u00b2) is used to generate these reward samples, where Q\u2208 (-\u221e, +\u221e) is the mean of N, and \u03c3\u00b2 the variance. These rewards need to be bounded between [0, 1], for which sigmoid function s(r) = 17 is used on r ~ N(Qa, \u03c3\u00b2), making this the hidden distribution r(a). Moreover, this transformation squeezes N and shifts the mean away from s(Q) to a new mean denoted by qa\u2208 [0, 1]. This qa can be estimated as Er~r(a) [r], by sampling a large number of samples (10\u2077 samples) from r(a) and averaging them. Further, three different kinds of environments are used, where Va: qa's are near 0, spread between 0 and 1, or near 1."}, {"title": "4.2 RL experiments", "content": "Non-batched: In these experiments, an RL agent starts with an initial random policy \u03c0. The agent then samples only one action k from in an iterative fashion. Further, pulling action k in the environment, the agent receives a noisy reward signal rk. For CL, the agent utilizes Eq. 14 to make an update. Whereas for MCL, Eq. 22 cannot be used directly, since we do not have access to vt. We therefore, approximate vt by employing a moving average over rewards, where y is a weighting factor for recent rewards:\n$ \\tilde r \\leftarrow \\gamma r + (1 - \\gamma )\\tilde r.$\nMoreover, since this update rule can make \u03c0 invalid, i.e., components could become negative or above one (see footnote 11), we clamp \u03c0 between 0 and 1:"}, {"title": "4.3 Population Experiments", "content": "In this section, we focus on the population update rules, VR, and WVR (see preliminaries Secs. 2.3 and 2.2). Since we cannot simulate P for infinite sizes, we choose two finite population sizes of 10 and 1000. For both VR and WVR, we start with an equal proportion of individuals associated with any type/opinion. Further, each individual receives a stochastic payoff/quality estimate for its type/opinion. Thereafter, with VR, everyone is paired with another random individual. All individuals then generate a random number between 0 and 1, and if the random number is greater than the"}, {"title": "4.4 TRD and MRD", "content": "To empirically validate Propositions 1 and 2, we numerically simulate both the differential equations 3 (TRD) and 4 (MRD). As these equations are continuous, we discretize them by a step \u03b4 (discretizing step). Further, we start from an initial random population/policy (\u03c0) and simulate its evolution according to TRD and MRD between time intervals [0, tf], using the privileged information qa not available to RL and population experiments."}, {"title": "5 RESULTS", "content": "For all experiments, we use the hyperparameters described in supplementary Sec. A.3.\nNon-batched RL update rules CL and MCL follow TRD and MRD respectively when the learning rate (a) is small. These results are presented in Figures 1 and 2. For all environments, CL and MCL follow TRD and MRD respectively, which can be explicitly seen with the dotted line of the analytical solutions (TRD, MRD) exactly at the center of the average reward curves of the CL and MCL update rules. This empirically validates that, with a small a, Eqs. 14 and 22 follow the TRD and MRD respectively, even when iteratively applied with single samples. However, as soon as a is increased, CL and MCL start deviating from their respective analytical solutions (see Figure 2). This is a well-known effect in optimization literature but from a population perspective (see Sec. 3.2) we see that a larger a corresponds to a smaller population, hence leading to a poor approximation of the expected update. Further, we also observe that MCL performs poorly compared to TRD with larger a.\nBatched RL update rules B-CL and B-MCL follow TRD and MRD respectively when the batch size (B) is large enough. As seen in Figure 4, it is clear that B-CL and B-MCL follow TRD and MRD respectively for large batch sizes (this can be seen from how the analytical solution is exactly at the center of the average reward curves of B-CL and B-MCL). However, as soon we make the batch sizes smaller, the batched updates deviate from their analytical solutions (see sub-section 3.2). See supplementary Sec. A.1 for other environments. We also observe that B-MCL performs poorly compared to B-CL with smaller batch sizes, similar to observations made with non-batched RL experiments."}, {"title": "6 CONCLUSIONS", "content": "With Propositions 1 and 2, we have demonstrated how RD is the underlying connection between Reinforcement Learning and Collective Decision-Making. Further, we have empirically validated this correspondence. This correspondence opens a bridge between these two fields, enabling the flow of ideas, new perspectives, and analogies for both. For example, it can be seen that Cross Learning, Maynard-Cross Learning, and, more generally, Reinforcement Learning take a single-agent perspective, where information from consecutive action samples/batches is accumulated into one centralized agent's policy. On the other hand, Rvoters and Rwvoters take a multi-agent perspective, where every individual implements simple local and decentralized rules, making independent decisions, which leads to an emergent collective policy.\nSignificance for RL. Similar to how we discovered a new RL update rule (i.e., Maynard-Cross Learning) from Swarm Intelligence, other ideas such as majority rule [22], and cross-inhibition [13], can be used to create new update rules for Reinforcement Learning. Moreover, Swarm Intelligence algorithms are often inspired by nature, and thus require individuals to follow physics. This mandates practical constraints such as congestion [19], communication, and finite size effects, which are generally ignored in Reinforcement Learning and population games. Comparing the performance of Reinforcement Learning agents with their equivalent swarm counterparts on such constraints is a direction for future work.\nSignificance for SI. The population-policy equivalence highlights how certain Swarm Intelligence methods are equivalent to single-agent Reinforcement Learning, demonstrating agency of the entire swarm as a single learning entity. Therefore, one could imagine that Multi-Agent Reinforcement Learning would similarly yield"}, {"title": "A SUPPLEMENTARY MATERIAL", "content": ""}, {"title": "A.1 Batched RL experiments", "content": ""}, {"title": "A.2 Population experiments", "content": ""}, {"title": "A.3 Hyperparametrs", "content": "In this section, we list out various hyperparameters used by RL and population experiments."}]}