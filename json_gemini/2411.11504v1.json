{"title": "Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering", "authors": ["Xinyan Guan", "Yanjiang Liu", "Xinyu Lu", "Boxi Cao", "Ben He", "Xianpei Han", "Le Sun", "Jie Lou", "Bowen Yu", "Yaojie Lu", "Hongyu Lin"], "abstract": "The evolution of machine learning has increasingly prioritized the development of powerful models and more scalable supervision signals. However, the emergence of foundation models presents significant challenges in providing effective supervision signals necessary for further enhancing their capabilities. Consequently, there is an urgent need to explore novel supervision signals and technical approaches. In this paper, we propose verifier engineering, a novel post-training paradigm specifically designed for the era of foundation models. The core of verifier engineering involves leveraging a suite of automated verifiers to perform verification tasks and deliver meaningful feedback to foundation models. We systematically categorize the verifier engineering process into three essential stages: search, verify, and feedback, and provide a comprehensive review of state-of-the-art research developments within each stage. We believe that verifier engineering constitutes a fundamental pathway toward achieving Artificial General Intelligence.", "sections": [{"title": "1. Introduction", "content": "The evolution of machine learning (Jordan & Mitchell, 2015) has undergone a progressive journey characterized by the pursuit of increasingly powerful models and the scaling of supervision signals (Friedland & Krell, 2018; Cao et al., 2024; Sevilla et al., 2022). Over the past decades, both model capacity and the scale of supervision signals have escalated in a synergistic manner. Powerful models necessitate more scalable and effective supervision signals to fully utilize their parameters. Conversely, the expansion of supervision signals requires models with enhanced capacities to effectively exploit these signals, thereby achieving more generalized capabilities.\nIn the early stages of machine learning, models were constrained by their limited capacity. This period was characterized as feature engineering, during which domain experts manually designed and extracted relevant features. Classical algorithms, such as Support Vector Machines (Hearst et al., 1998) and Decision Trees (Quinlan, 1986), relied heavily on meticulously crafted features due to their architectural limitations. These algorithms achieved optimal performance through carefully designed feature extraction techniques, with Term Frequency-Inverse Document Frequency (Robertson et al., 2009) serving as a prominent example.\nHowever, as addressed increasingly complex problems, the limitations of manual feature engineering became more pronounced, underscoring the need for more scalable approaches to construct features. The emergence of deep learning (Schmidhuber, 2015) approximately two decades ago marked a transformative shift, inaugurating the data engineering era. This new paradigm represented a fundamental departure from handcrafted features, emphasizing instead the curation of high-quality datasets and annotations to facilitate automated knowledge acquisition and pattern recognition across diverse domains and tasks. The remarkable success of landmark projects such as ImageNet (Deng et al., 2009) and BERT (Brown et al., 2020a) validated the effectiveness of this data-centric approach.\nUnfortunately, the emergence of foundation models in recent years (Ouyang et al., 2022a; Touvron et al., 2023a; Betker et al.; Dosovitskiy et al., 2021) has made it increasingly difficult to enhance model capabilities solely through data engineering. Specifically, foundation models, particularly large language models (LLMs) (Ouyang et al., 2022a; Touvron et al., 2023a), have demonstrated extraordinary abilities, increasingly matching or surpassing human performance across various domains. Nevertheless, the traditional data engineering approach of augmenting these models through large-scale data construction has reached its practical limitations. This limitation is evident in two primary challenges: the difficulty and unsustainable costs associated with high-quality human annotations for post-training (Anthropic, 2024; Burns et al., 2023), and the complexity involved in providing meaningful guidance that can further enhance model performance (Wen et al., 2024a). Consequently, the central challenge in the current era is determining how to supply more effective supervision signals to foundation models to achieve general artificial intelligence capabilities.\nIn this paper, we propose verifier engineering, a novel post-training paradigm designed for the foundation model era. The essence of verifier engineering lies in extending the construction of supervision signals beyond traditional manual feature extraction and data annotation. Instead, it utilizes a suite of effective automated verifiers to perform verification tasks and provide meaningful feedback to foundation models. Table 1 delineates the key distinctions among feature engineering, data engineering, and verifier engineering. This progression from annotate and learn to search and verify signifies a fundamental advancement in enhancing the capabilities of foundation models. Compared to preceding paradigms, verifier engineering streamlines the creation of verifiers and facilitates efficient feedback to foundation models through automated verification processes. Specifically, given an instruction, verifier engineering initiates by generating candidate responses, subsequently verifying these candidates using appropriate combinations of verifiers, and ultimately optimizes the model's output distribution. Unlike existing methodologies such as Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022a), which depend on a limited set of verifier sources and feedback mechanisms, verifier engineering integrates multiple diverse verifiers to deliver more accurate and generalizable feedback signals. By shifting the improvement of foundation models from a data-centric endeavor to a systematic engineering challenge, verifier engineering emphasizes the design and orchestration of complex verification systems to ensure effective and efficient feedback. Analogous to how feature engineering and data engineering achieved scalability in their respective eras, we posit that verifier engineering represents a crucial step toward the advancement of general artificial intelligence.\nTo this end, this paper provides a comprehensive exploration"}, {"title": "2. Verifier Engineering", "content": "In this section, we formalize verifier engineering as a Goal-Conditioned Markov Decision Process (GC-MDP) (Schaul et al., 2015; Plappert et al., 2018; Liu et al., 2022), enabling a unified and systematic perspective on the field of verifier engineering. Then, we introduce how the concepts of search, verify, and feedback correspond within this modeling framework, and analyze them through examples. Furthermore, we provide a comprehensive overview of the verifier engineering landscape by categorizing existing post-training approaches into three core stages, as summarized in Table 3.\n2.1. Preliminary\nWhile LLMs are typically trained to maximize generation likelihood given input, this objective alone cannot guarantee desired post-training capabilities. To bridge this gap, we formalize verifier engineering as a GC-MDP, denoted as tuple (S, A, T, G, Rg, pg), where:\nState Space S represents the model's state during interaction, including input context, internal states, and intermediate outputs.\nAction Space A represents the possible token selections at each generation step:\nA = {a1, a2,...,\u03b1\u03bd}\nTransition Function T typically defines the probability distribution over the next states, given the current state s \u2208 S and action a \u2208 A. Specifically, in large language models, the state transition is a deterministic function. That is, once the current state and selected action (the generated token) are given, the next state is fully determined. The search stage of the verifier engineering thus can be regarded as exploration in the action-state space in the condition of T.\nGoal Space G represents the various goals related to model capabilities. Each goal g\u2208 G corresponds to a specific model capability, such as code, math, writing, and so on. The goal space is multi-dimensional and can encompass various aspects of model capabilities.\nGoal Distribution pg is the probability distribution over goals from the goal space G . It represents the likelihood of a specific goal g\u2208 G being selected at any given time. This distribution can be learned from human feedback or other external signals.\nReward Function R\u338f(s, a) or Rg(s') represents the reward the model receives given the goal g, when taking action a from state s or at the transformed state s'. This reward thus reflects the verification result of verifier engineering for the specific goal g. For example, if the goal g is \"fairness,\" the reward might be based on whether the generated text avoids bias.\nThe objective for improving model capabilities can be defined as a goal-conditioned policy \u03c0 : S \u00d7 G \u00d7 A \u2192 [0, 1] that maximizes the expectation of the cumulative return over the goal distribution.\nJ(\u03c0) = \u0395\u03b1\u03b9~\u03c0(18,9),9~pg \u03a3Rg (St, at) (1)\nSt+1~T(St,at)\nThe reward function can be decomposed into sub-functions for different capability dimensions:\nRg(s, a) = F (Rg,i (s, a) | i \u2208 Sg) (2)\nwhere Sg represents selected sub-functions for goal g, and F combines their evaluations. Following Probably Approximately Correct Theory (PAC) (Vapnik, 2000), even with imperfect sub-functions, we can achieve reliable overall evaluation by combining multiple weak verifiers.\n2.2. Verifier Engineering Overview\nBuilding upon the GC-MDP framework, we demonstrate how the three stages of verifier engineering\u2014Search, Verify, and Feedback-naturally align with specific components of this formalism. This mapping provides both theoretical grounding and insights into how each stage contributes to optimizing the policy distribution \u3160 toward desired goals.\nAs illustrated in Figure 1, the search stage can be divided into linear search and tree search methods. The verify stage"}, {"title": "3. Search", "content": "Search aims to identify high-quality generated sequences that align with the intended goals, forming the foundation of verifier engineering, which is critical for evaluating and refining foundation models. However, it is impractical to exhaustively search the entire state-action space due to its exponential growth\u2014driven by the large vocabulary size N and the maximum generation length T. To address this challenge, efficient searching, which seeks to navigate this vast space by prioritizing diverse and goal-oriented exploration, plays a critical role in improving model's performance.\nIn this section, we first introduce how to implement diverse search from the perspective of search structure and discuss additional methods for further enhancing search diversity after the search structure is determined.\n3.1. Search Structure\nSearch structure denotes the framework or strategy used to navigate the state-action space, which significantly influences the effectiveness and efficiency of the search process. Currently, there are two widely adopted structures for implementing search: linear search and tree search. Linear search progresses sequentially, making it effective for tasks involving step-by-step actions, while tree search examines multiple paths at each decision point, making it well-suited for tasks requiring complex reasoning.\n\u2022 Linear Search is a widely used search method where the model starts from an initial state and proceeds step by step, selecting one token at a time until reaching the terminal state (Brown et al., 2020b; Wang & Zhou, 2024).\nThe key advantage of linear search is its low computational cost, which makes it efficient in scenarios where actions can be selected sequentially to progress toward a goal. However, a notable limitation is that if a suboptimal action is chosen early in the process, it may be challenging to rectify the subsequent sequence. Thus, during the progress of verifier engineering, careful verification at each step is crucial to ensure the overall effectiveness of the generation path.\n\u2022 Tree Search involves exploring multiple potential actions at each step of generation, allowing for a broader exploration of the state-action space. For instance, techniques like Beam Search and ToT (Yao et al., 2024) incorporate tree structures to enhance exploration, improving model performance in reasoning tasks. TouT (Mo & Xin, 2023) introduces uncertainty measurement based on Monte Carlo dropout, providing a more accurate evaluation of intermediate reasoning processes.\nThis approach significantly improves the likelihood of discovering global optimal solution, particularly in environments with complex state spaces. By simultaneously considering multiple paths, tree search mitigates the risk of being locked into sub-optimal decisions made early on, making it more robust in guiding the model toward an optimal outcome. However, this increased exploration comes at a higher computational cost, making tree search more suitable for scenarios when the optimal path is challenging to identify. To make tree search effective, the model must be continuously verified and prioritize paths that align better with the goal conditions.\n3.2. Additional Enhancement\nWhile search structures provide the foundational framework for navigating the state-action space, further enhancement is also crucial to enhance search performance. These enhancements address challenges such as balancing exploration and exploitation, escaping local optima, and improving the diversity of generated results. The enhancement strategies can be broadly categorized into two approaches: adjusting exploration parameters and intervening in the original state.\nAdjusting Exploration Parameters Techniques such as Monte Carlo Tree Search (MCTS), Beam Search, and Reject Sampling focus on refining the exploration process by adjusting parameters like temperature, Top-k (Fan et al., 2018), or Top-p (Holtzman et al., 2020). The challenge lies in bal-"}, {"title": "4. Verify", "content": "Due to the long delay and high cost associated with human feedback, we cannot directly employ human efforts to evaluate each candidate response sampled by the model during training (Leike et al., 2018a). Therefore, we employ verifiers as proxies for human supervision in the training of foundation models. The verifiers play a crucial role in search-verify-feedback pipeline, and the quality and robustness of the verifiers directly impact the performance of the downstream policy (Wen et al., 2024c).\nIn the context of GC-MDP, verify is typically defined as using a verifier that provides verification results based on the current state and a predefined goal:\nFRg(St-1,at) (3)\nwhere F represents the verification results provided by the verifier. g denotes the predefined goal we aim to achieve (e.g., helpfulness, honesty). The state s is typically composed of two concatenated components: the user's query or input and the model's output content {a1, ..., at}.\nIn this section, we classify individual verifiers across several key dimensions and summarize the representative type of verifiers in Table 2.\n4.1. A Comprehensive Taxonomy of Verifiers\nPerspective of Verification Form The verification result form of verifiers can be divided into four categories: binary feedback (Gao et al., 2023), score feedback (Bai et al., 2022a), ranking feedback (Jiang et al., 2023a), and text feedback (Saunders et al., 2022). These categories represent an increasing gradient of information richness, providing more information to the optimization algorithm. For instance, classic Bradley-Terry reward models (Bradley & Terry, 1952) can provide continuous score feedback which simply indicates correctness, while text-based feedback from generative reward models (Zhang et al., 2024d) and critique models (Sun et al., 2024b) offer more detailed information, potentially including rationals for scores or critiques.\nPerspective of Verify Granularity The verify granularity of verifiers can be divided into three levels: token-level (Chen et al., 2024b), thought-level (Lightman et al., 2023a), and trajectory-level (Cobbe et al., 2021). These levels correspond to the scope at which the verifier engages with the model's generation. Token-level verifiers focus on individual next-token predictions, thought-level verifiers assess entire thought steps or sentences, and trajectory-level verifiers evaluate the overall sequence of actions. Although most RLHF practices (Ouyang et al., 2022b; Bai et al., 2022a) currently rely on full-trajectory scoring, coarse-grained ratings are challenging to obtain accurately (Wen et al., 2024b), as they involve aggregating finer-grained verification. Generally, from the perspective of human annotators, assigning finer-grained scores is easier when the full trajectory is visible. From a machine learning perspective, fine-grained verification is preferable (Lightman et al., 2023a), as it mitigates the risks of shortcut learning and bias associated with coarse-grained verification, thereby enhancing generalization. A credit-assignment mechanism (Leike et al., 2018b) can bridge the gap between coarse-grained ratings and fine-grained verification.\nPerspective of Verifier Source Verifiers can be divided into program-based and model-based from the perspective of verifier source. Program-based verifiers provide deterministic verification, typically generated by predefined rules or logic embedded in fixed programs. These program-based verifiers offer consistent and interpretable evaluations but may lack flexibility when dealing with complex, dynamic environments. On the other hand, model-based verifiers rely on probabilistic models to generate verification results. These verifiers adapt to varying contexts and tasks through learning, allowing for more nuanced and context-sensitive evaluations. However, model-based verifiers introduce an element of uncertainty and can require significant training and computational resources to ensure accuracy and robustness.\nPerspective of Extra Training Verifiers can also be divided into two categories based on whether they require additional specialized training. Verifiers requiring additional training are typically fine-tuned on specific task-related data, allowing them to achieve higher accuracy in particular problem domains (Markov et al., 2023). However, their performance can be heavily influenced by the distribution of the"}, {"title": "5. Feedback", "content": "After obtaining the verification result, we aim to enhance the capabilities of the foundation model, we define this process as the feedback stage. In this paper, feedback specifically refers to enhancing the foundation model's capabilities based on the verification results. The feedback stage is critical, as the effectiveness of the feedback method directly determines whether the foundation model's capabilities can be appropriately enhanced in response to the verification results.\nIn this section, we explore how verifier engineering utilizes search algorithms and verifiers to feedback verification results on foundation models. To maximize the objective function J(\u03c0), the distribution of the policy \u03c0 can be optimized by adjusting st or the parameters of \u03c0. This leads to two distinct feedback approaches: training-based feedback and inference-based feedback. Training-based feedback involves updating model parameters using data efficiently obtained through searching and verifying. In contrast, inference-based feedback modifies the output distribution by incorporating search and verification results as auxiliary information during inference, without altering the model parameters.\n5.1. Training-based Feedback\nWe categorize the common training strategies for training-based feedback into three types, based on the nature and organization of the data utilized:\nImitation Learning Imitation Learning typically uses high-quality data selected by verifiers, employing supervised fine-tuning objectives like cross-entropy or knowledge distillation training objectives like Kullback-Leibler divergence (Hinton, 2015) to optimize the model.\nVarious approaches are employed to enhance specific capabilities of the foundation model through imitation learning. To enhance mathematical reasoning capabilities in LLMs, approaches like STaR (Zelikman et al., 2022b) and RFT (Yuan et al., 2023b) use rule-based verifiers to compare solution outcomes, while WizardMath (Luo et al., 2023a), MARIO (Liao et al., 2024), and MetaMath (Yu et al., 2023a) leverage verifiers to select responses from advanced LLMs or human inputs. Other methods such as MAmmoTH (Yue et al., 2023) and MathCoder (Wang et al., 2023a) utilize programmatic tools to verify comprehensive and detailed solutions. For coding capability improvement, various methods including Code Alpaca (Chaudhary, 2023), WizardCoder (Luo et al., 2023b), WaveCoder (Yu et al., 2023b), and OpenCodeInterpreter (Zheng et al., 2024b) construct code instruction-following datasets by distilling knowledge from advanced LLMs. To enhance instruction-following abilities, approaches like LLaMA-GPT4 (Peng et al., 2023), Baize (Xu et al., 2023), and Ultrachat (Ding et al., 2023) employ verifiers to distill responses from advanced LLMs for supervised fine-tuning. Other methods such as Deita (Liu et al., 2024) and MoDS (Du et al., 2023) implement a pipeline of verifiers to check complexity, quality, and diversity before selecting suitable data for SFT.\nPreference Learning Preference Learning leverages verification results to construct pairwise comparison data and employs optimization methods like DPO (Rafailov et al., 2024), KTO (Ethayarajh et al., 2024), IPO (Azar et al., 2024), and PRO (Song et al., 2024). Through this approach, models learn to align their outputs with verifier-provided preferences.\nVarious techniques are adopted to boost the foundation model's capabilities in specific areas through preference learning. For mathematical reasoning enhancement, MCTS-DPO (Xie et al., 2024) combines Monte Carlo Tree Search (Coulom, 2006; Kocsis & Szepesv\u00e1ri, 2006) with preference learning to generate and learn from step-level pairwise comparisons in an iterative online manner. For coding capability improvement, CodeUltraFeedback (Weyssow et al., 2024) constructs pairwise training data by using LLM verifiers to rank code outputs, then applies preference learning algorithms to optimize the model's performance. For instruction-following enhancement, Self-Rewarding (Yuan et al., 2024) enables models to generate their own verification results for creating pairwise comparison data, followed by iterative self-improvement using the DPO method.\nReinforcement Learning Reinforcement Learning optimizes models using reward signals from verifiers. Through environmental interaction and policy updates using algorithms like PPO (Schulman et al., 2017), PPO-max (Zheng et al., 2023), models iteratively improve their generation quality.\nMultiple approaches are used to enhance the foundation model's capabilities in specific domains using reinforcement learning. For mathematical reasoning enhancement, Math-Shepherd (Wang et al., 2023c) implements step-wise reward mechanisms to guide progressive improvement in mathematical problem-solving capabilities. For coding capability improvement, methods like RLTF (Liu et al., 2023a) and PPOCoder (Shojaee et al., 2023a) leverage code execution results as reward signals to guide models toward more effective coding solutions. For instruction-following enhancement, approaches like InstructGPT (Ouyang et al., 2022a) and Llama (Touvron et al., 2023a;b; Dubey et al., 2024) employ reward models trained to evaluate response helpfulness, optimizing models for better instruction adherence.\n5.2. Inference-based Feedback\nIn inference-based feedback, we modify inputs or inference strategies to obtain better outputs without changing model parameters. This approach is divided into two categories"}, {"title": "6. Discussion and Insights", "content": "In this section, we provide a detailed examination of the insights derived from our framework. We begin by revisiting SFT, DPO, and RLHF through the lens of verifier engineering. Subsequently, we conduct an independent analysis of each stage within the framework. Finally, we offer a systematic evaluation of potential challenges inherent to the framework as a whole.\n6.1. Revisiting SFT, DPO and RLHF from Verifier Engineering\nOur proposed approach to verifier engineering provides a unified perspective on commonly used post-training algorithms, offering valuable insights into their mechanisms.\nSFT generates candidate responses by employing a linear search strategy that adheres to a singular search path defined by its training data. Throughout this process, the verifier classifies each token along the search path as a positive signal, while treating all other tokens as negatives. Subsequently, the foundational model is optimized through imitation learning using a cross-entropy loss function.\nSimilarly, DPO employs a linear search strategy, maintaining only two distinct search paths: one corresponding to the \"chosen\" data derived from preference pairs and the other to the \"rejected\" data. The verifier treats the path associated with the chosen data as positive signals and the path associated with the rejected data as negative signals. Subsequently, the foundational model is optimized through the application of pairwise loss functions.\nRLHF employs a linear search strategy during the search stage, which is further enhanced by adjusting parameters such as Top-p and temperature to promote exploration within the candidate response generation process. As depicted in Figure 3(c), after search, a value function is utilized to assign scores to each state within the generation trajectory. This scoring mechanism estimates the expected return, thereby informing and guiding the optimization process through PPO algorithm.\nBy mapping these methods to the stages of verifier engineering, we gain a clearer understanding of how each approach in the search, verify, and feedback stage enhances foundation model capabilities.\n6.2. Discussion on three stages of Verifier Engineering\nThe three stages of verifier engineering play a distinct and critical role in enhancing the capabilities of foundation models. This discussion delves into current challenges and proposes future research directions, focusing on search efficiency, verifier design, and the effectiveness of feedback methods.\n6.2.1. ADVANCED SEARCH METHODS\nThe efficiency and effectiveness of candidate response generation are crucial for model performance. The challenge lies in balancing exploration and exploitation, as well as"}, {"title": "6.2.2. OPEN QUESTIONS TO VERIFIER DESIGN", "content": "Designing effective verifiers is pivotal for enhancing the performance and reliability of foundation models. However, several unresolved issues remain regarding the optimal design and integration of verifiers. In this subsection, we first examine the considerations involved in verifier design and the need for systematic evaluation frameworks. Subsequently, we explore the complexities associated with combining multiple verifiers, highlighting the challenges that must be addressed to achieve comprehensive and scalable verification systems.\nVerifier Design and Systematic Evaluation Designing specific verifiers for different types of instructions is essential. For instructions with explicit output constraints (such as grammatical format and length limitations), rule-based verifiers should be implemented due to their definitive reliability. For question-answering instructions, LLM-based verifiers are typically more effective in evaluating subjective metrics like response fluency and information content. However, recent studies (Wen et al., 2024c) have revealed only weak correlations between existing verifier evaluation metrics and downstream task performance, highlighting significant limitations in the current verifier evaluation framework. Therefore, a systematic evaluation framework is needed to comprehensively assess the effectiveness, applicable scope, and limitations of different types of verifiers. Such a framework would not only guide the selection and combination of verifiers but also establish best practices for verifier deployment across various task scenarios.\nChallenges of Verifier Combinations As mentioned above, it is impossible to obtain effective verification results using a single verifier alone. Therefore, integrating multiple verifiers is essential to address the diverse requirements of candidate response evaluation. To comprehensively enhance foundation model performance across various task scenarios, an effective verifier combination system must be developed. Building an effective verifier combination system faces three key challenges:\n\u2022 Instruction Coverage: The verifier combination must be capable of handling various types of instructions to ensure the completeness of the evaluation system. Building a comprehensive verifier framework requires a deep understanding of different task characteristics and evaluation needs, including structured output validation, open-ended question assessment, creative task evaluation, etc.\n\u2022 Automatic Routing Mechanism: Different tasks typically require verifiers with varying forms and granularity, demanding an intelligent verifier routing system. This system needs to analyze instruction characteristics and select appropriate verifier combinations accordingly. Based on the PAC theory discussed in Section 2, the chosen verifier combinations should effectively approximate our ultimate optimization objective.\n\u2022 Verification Results Integration Strategy: When multiple verifiers produce different verification results, a reliable decision-making mechanism is needed. This in-"}, {"title": "6.2.3. EFFECTIVENESS OF FEEDBACK METHODS", "content": "The effectiveness of feedback methods plays a crucial role in shaping the performance and adaptability of foundation models. In particular, we focus on two key questions: 1) whether the feedback method can accurately and efficiently improve the model's performance, and 2) whether it can generalize effectively to other queries.\nKey Factors in Designing Feedback Methods Different feedback methods have different impacts on foundation model capabilities. When selecting feedback methods, it is essential to carefully balance several key factors. Firstly, the algorithm should demonstrate sufficient robustness to noise in verification results, ensuring the stability of the feedback process. Secondly, it is crucial to assess how the feedback algorithm based on specific types of verification results impacts the model. Over-optimizing certain capabilities can undermine the model's fundamental capabilities and overall generalization, potentially leading to performance degradation. Furthermore, foundation models with different capacities may require distinct optimization approaches. Larger models might benefit from more sophisticated feedback methods, while smaller models may need more conservative feedback methods to prevent capacity saturation. It is crucial to find an appropriate balance between these factors while considering the model's inherent capabilities.\nGeneralization over Queries Equipped with reliable verifiers and effective feedback methods, the ideal scenario is to achieve comprehensive improvements in foundation model capabilities through optimization on a limited set of queries. This necessitates feedback methods to possess strong cross-query generalization abilities. Specifically, when we enhance certain foundation model capabilities through feedback on specific queries, these improved capabilities should effectively transfer and persist when handling novel queries. However, generalization also faces significant challenges: different queries may require the model to invoke distinct capabilities, and incorrect generalization might lead to the model inappropriately applying certain capabilities in unsuitable scenarios, potentially degrading performance. Therefore, feedback methods must not only promote effective generalization but also prevent over-generalization and incorrect transfer of capabilities."}, {"title": "6.3. Verifier Engineering Implementation", "content": "In this section, we will discuss the problems we may meet in verifier engineering implementation in different stages to ensure efficiency, scalability, and reliability.\nSearch Achieving high search efficiency is a key objective in verifier engineering. Excessive search often slows down the entire verifier engineering pipeline.\nTo address this, most current LLM-based PPO algorithms sample only a single response for optimization. On the other hand, RLHF-like algorithms commonly incorporate importance sampling (Schulman et al., 2017; Xie et al., 2019) techniques to enhance search efficiency, minimizing the need for frequent switching between search, verify, and feedback stages while simultaneously improving sample utilization.\nVerify Verifier efficiency is also a key goal in verifier engineering for giving timely and effective verification results.\nWhen handling multiple instructions from various sources, it's crucial to employ different combinations of verifiers with different abilities to ensure accurate verification results. Determining the optimal approach to deploy all verifiers online and dynamically schedule them each time to minimize resource consumption while maximizing efficiency presents a challenging problem.\nDelivering effective verification results involves addressing two major challenges: (1) ensuring the verifier's knowledge remains synchronized with the policy model, and (2) selecting the optimal verifier combination when capabilities vary or conflict across verifiers. For instance, InstructGPT (Ouyang et al., 2022a) employs a human-annotated reward model as a verifier, and to counter the limitations of a static verifier, it periodically re-annotates reward model data to align its evaluation capabilities with the evolving policy model outputs. Furthermore, Quan (2024) leverages a Mixture of Experts architecture to combine multiple verifiers with different strengths. Experiential Co-Learning (Qian et al., 2023) also draws on the knowledge of diverse foundation models to provide more robust verification results.\nFeedback For highly efficient feedback, it's essential not only to enhance the feedback algorithm itself but also to optimize the entire workflow.\nTo increase training and inference efficiency, LoRA (Hu et al., 2021; Xin et al., 2024) improves training efficiency by reducing the number of trainable parameters and VLLM (Kwon et al., 2023) enhance inference efficiency.\nTo optimize the entire workflow, deciding when to apply feedback methods is crucial (Tang et al., 2024). For training-based feedback, understanding the performance gap be-"}, {"title": "7. Conclusion", "content": "This paper introduces the concept of verifier engineering and explores the significant shift in research paradigms from feature engineering to data engineering, and ultimately to verifier engineering. Our framework provides implications and insights demonstrating that verifier engineering can optimize the capabilities of foundation models through a closed-loop feedback cycle encompassing search, verification, and feedback. Furthermore, we categorize existing search algorithms based on their granularity and schemes, review current verifiers, and classify feedback methods from both training-based and inference-based perspectives. Finally, we discuss the challenges that verifier engineering currently faces. Through this paper, we aim to stimulate further discussions and promote practical applications in the field of verifier engineering for achieving Artificial General Intelligence."}]}