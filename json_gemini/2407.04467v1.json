{"title": "Are Large Language Models Strategic Decision Makers? A Study of Performance and Bias in Two-Player Non-Zero-Sum Games", "authors": ["Nathan Herr", "Fernando Acero", "Roberta Raileanu", "Mar\u00eda P\u00e9rez-Ortiz", "Zhibin Li"], "abstract": "Large Language Models (LLMs) have been\nincreasingly used in real-world settings, yet\ntheir strategic abilities remain largely unex-\nplored. Game theory provides a good frame-\nwork for assessing the decision-making abili-\nties of LLMs in interactions with other agents.\nAlthough prior studies have shown that LLMs\ncan solve these tasks with carefully curated\nprompts, they fail when the problem setting\nor prompt changes. In this work we investi-\ngate LLMs' behaviour in strategic games, Stag\nHunt and Prisoner Dilemma, analyzing perfor-\nmance variations under different settings and\nprompts. Our results show that the tested state-\nof-the-art LLMs exhibit at least one of the fol-\nlowing systematic biases: (1) positional bias,\n(2) payoff bias, or (3) behavioural bias. Sub-\nsequently, we observed that the LLMs' perfor-\nmance drops when the game configuration is\nmisaligned with the affecting biases. Perfor-\nmance is assessed based on the selection of\nthe correct action, one which agrees with the\nprompted preferred behaviours of both play-\ners. Alignment refers to whether the LLM'S\nbias aligns with the correct action. For exam-\nple, GPT-40's average performance drops by\n34% when misaligned. Additionally, the cur-\nrent trend of \"bigger and newer is better\" does\nnot hold for the above, where GPT-40 (the cur-\nrent best-performing LLM) suffers the most\nsubstantial performance drop. Lastly, we note\nthat while chain-of-thought prompting does re-\nduce the effect of the biases on most models,\nit is far from solving the problem at the funda-\nmental level.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have become in-\ncreasingly ubiquitous, as indicated by a significant\nincrease in research containing keywords relating\nto LLMs, with close to a 500% increase since 2018\n(Naveed et al., 2023). There has also been an in-\ncrease in people using them to solve everyday tasks\nin fields such as medicine, education, finance, and\nlaw (Hadi et al., 2023; Duan et al., 2024). However,\nas LLMs are deployed in the real world, where they\ninteract with other humans or artificial agents, there\nis an underserved need to understand the capabili-\nties of LLMs to operate in complex social scenarios.\nThe ability to reason strategically about interactions\nwith other agents is a fundamental aspect of human\nintelligence (Qiao et al., 2022; Huang and Chang,\n2022; Sahoo et al., 2024; Zhang et al., 2024). In\nfact, many works have begun asking the question\n(Huang et al., 2024; Xu et al., 2023; Duan et al.,\n2024): can LLMs assist in everyday tasks which\nrequire the ability to understand the complex en-\nvironment in which they are operating, anticipate\npotential outcomes, infer the intentions and beliefs\nof others (with whom they are collaborating or com-\npeting), and think critically about all these factors\nto come to the best possible strategic decision. To\nanswer this question, game theory, which is already\napplied to many existing real-world tasks (Martin,\n2017), is looked to as a well of wisdom (Huang\net al., 2024; Xu et al., 2023; Duan et al., 2024; Fan\net al., 2024; Guo et al., 2024; Zhang et al., 2024;\nLor\u00e8 and Heydari, 2023; Li et al., 2023a; Brookins\nand DeBacker, 2023; Gandhi et al., 2023; Akata\net al., 2023; Phelps and Russell, 2023; Guo, 2023;\nGemp et al., 2024).\nGame theory studies how the choices of inter-\nacting agents, with specific preferences, produce\noutcomes, intentional and not (Ross, 2024). For\nthe reader's interest, a more in-depth definition of\nGame Theory models and rational agents can be\nfound in Appendix A.1. Game theoretic tasks ab-\nstract complex real-life scenarios as mathematical\nmodels designed to be easy to understand but re-\nquire the above skills to be mastered. While many\ndifferent qualifications exist for these tasks, this\nwork focuses on non-zero-sum games. Non-zero-\nsum games have both competitive and cooperative\nelements, which provide a fair representation of\nagent interactions for many important real-world\nscenarios. Popular examples of such games are\nStag Hunt and Prisoners Dilemma, which will be\nthe focus of this research. Further details on these\ngames can also be found in Appendix A.1.\nPrompt engineering (Sahoo et al., 2024) has\nemerged as an effective technique for improving\nLLM's performance on complex tasks involving\nreasoning. However, the majority of existing works\ndo not test the robustness of their specifically cu-\nrated prompts and fail drastically when the setting\nin which they are operating changes (Guo et al.,\n2024). This frailty has been discovered and investi-\ngated in many of the state-of-the-art (SOTA) LLMs\n(Fan et al., 2024; Chen et al., 2024; Papadatos\nand Freedman; Zheng et al., 2023, 2024; Wang\net al., 2023) such as LLama-2, GPT-3.5, and GPT-\n4. Specifically, LLMs suffer from what we refer\nto as systematic biases, which guide the LLMs'\ndecisions and not their strategic reasoning. This\nphenomenon has been investigated mainly in mul-\ntiple choice question answering and similar tasks\nbut has not been investigated thoroughly in game\ntheoretic tasks.\nWe believe it is clear, and hope that the reader\nis starting to agree, that LLMs must be capable of"}, {"title": "Preliminary Study: Humans vs AI", "content": "Belloc et al. (2019) present an experiment where\na random sample of participants (88 people) play\n4 games (352 games) of Stag Hunt under a time\nconstraint, while a second set of participants (97)\nplay 4 games (388 games) unconstrained. The con-\nstrained set of participants will be referred to as\nmaking a reflexive choice (without reasoning) and\nthe unconstrained set make a reasoned choice.\nProcessing the data made available by Belloc\net al. (2019), we found that when humans are mak-\ning a reflexive choice, they typically hunt the Stag\n62% of the time and the Hare 38% of the time.\nWhen making a reasoned choice, they still prefer\nhunting the Stag, choosing it 52.5% of the time,\nwith the Hare being hunted more often at 47.5%.\nAs a preliminary study, to further motivate our\nwork, we prompted GPT-40 with similar instruc-\ntions and settings (where label A is Hare and\nB is Stag) the human participants were provided\nin (Belloc et al., 2019) to compare the frequency\nat which Stag and Hare are selected. We prompt\nGPT-40 to provide an answer only for the reflexive\nchoice (400 games) and to reason over the task be-\nfore providing the answer for the reasoned choice\n(400 games). We can see that GPT-40\nexhibits a similar but stronger trend to the human\nparticipants: it selects Stag 100% of the time when\nmaking the reflexive choice and 95% of the time\nunder the reasoned choice, with a slight increase in\nHare hunting during the reasoned choice.\nWe also prompted GPT-40 under different set-\ntings, where label A is Stag and B is Hare. We\nnotice that the frequency at which Stag and Hare\nare selected changes drastically, picking the hare\n75.3% of the time under the reflexive choice and\n100% of the time under the reasoned choice. While\nthe human participants are not tested under this\nsetting, it's assumed humans wouldn't change their\nchoice based on how the actions are labelled. This\nobservation indicates that GPT-40 does not reason\nover these tasks in a human-like way and is instead\ninfluenced by other signals, such as the positional\nbias we see here. We, therefore, aim to explore\nfurther what these signals are and how they affect\nthe performance of LLMs in these types of game\ntheoretic tasks."}, {"title": "Related Work", "content": "Having established the importance of investigat-\ning how systematic biases affect LLMs' ability to\nmake strategic decisions in two-player non-zero-\nsum games, we now explore existing related work.\nLLMs and Game Theory. In recent years, the\nuse of LLMs as single-agent planners/decision\nmakers has evolved into LLM-based multi-agent\nsystems (Guo et al., 2024) where agents are re-\nquired to solve strategic and logical reasoning\nproblems. These capabilities are often evaluated\nthrough game-theoretic tasks (Zhang et al., 2022;\nLor\u00e8 and Heydari, 2023; Gandhi et al., 2023). In\nfact, this shift has prompted many new benchmarks\ntesting LLMs in game theoretic tasks with the aim\nof progressing the work within the field (Xu et al.,\n2023; Huang et al., 2024; Chen et al., 2023; Duan\net al., 2024; Li et al., 2023b; Aher et al., 2023).\nFurthermore, several existing works focus on game\ntheoretic matrix games, such as Prisoners Dilemma,\nStag Hunt, and Dictator Game, to name a few (Fan\net al., 2024; Xu et al., 2023; Lor\u00e8 and Heydari,\n2023; Brookins and DeBacker, 2023; Gandhi et al.,\n2023; Phelps and Russell, 2023; Guo, 2023), which\nare discussed below. In Fan et al. (2024)'s work\nthey show that LLMs, even when explicitly given\nthe correct belief, from which they should reason\nto take correct action, tend to ignore or modify this\nbelief. They also note that LLMs tend to select spe-\ncific action labels more frequently than others (they\nnote that GPT-3 prefers U to V), but do not investi-\ngate this any further. Xu et al. (2023) and Brookins\nand DeBacker (2023) show that LLMs tend to se-\nlect the cooperative action more frequently than\nhumans, despite it not being the optimal choice\nin most cases. Lastly, several works test how\nthe LLMs' behaviour changes as they modify the\nLLMs' preferences or contextual frameworks, such\nas being selfish or cooperative (Fan et al., 2024;\nPhelps and Russell, 2023; Guo, 2023; Lor\u00e8 and\nHeydari, 2023). They all note that LLMs are seem-\ningly capable of following simple preferences, such\nas selecting the selfish action when prompted to be\nselfish. However, they do not investigate the effect\nthe chosen prompt configuration has on the LLMs'\nperformance.\nBias in LLMs. The presence of systemic biases\u00b9\n(such as favouring a specific action label U over\nlabel V (Fan et al., 2024)) has recently become\na topic of interest. Specifically, these biases are\nfound and tested in multiple choice question evalu-\nation (Zheng et al., 2023), multi-turn question an-\nswer evaluation (Zheng et al., 2024), response qual-\nity evaluation (Wang et al., 2023), and tasks such\nas text classification, fact retrieval, and information\nextraction (Zhao et al., 2021; Chen et al., 2024;\nBerglund et al., 2023; Golovneva et al., 2024). It\nwas found that LLMs suffer from what is referred\nto as selection bias (Zheng et al., 2023, 2024; Wang\net al., 2023; Zhao et al., 2021), which is a combina-"}, {"title": "Methodology", "content": "This paper aims to investigate how the identified\nbiases affect the capability of LLMs to solve non-\nzero-sum two-player games. The biases identified,\nare as follows:\n(1) Positional Bias, where the order in which the\naction labels are stated in the prompt affects the\nfrequency of the selected action label. For example,\nLlama-3-8B, when prompted with action label A\nfirst and B second, tends to select the first action\nlabel A more frequently.\n(2) Payoff Bias, where the payoffs associated\nwith the different action labels, PayOff(label_1,\nlabel_2), affects the frequency of the selected ac-\ntion label. In particular, a model may be biased to-\nwards, (1) selecting the action that leads to the max-\nimum possible self-gain or (2) selecting the action\nthat leads to the maximum possible common-gain,\nrather than the action that maximizes the expected\ngain (which takes into account all possible actions\nthe other agent can take). Note that in Stag Hunt\nthese two actions are the same and for Prisoners\nDilemma they are not (see Table 5 in Appendix A.1\nfor further details). For example, GPT-4-Turbo in\nPrisoners Dilemma, when PayOff(A, A)=2, Pay-\nOff(A, B)=0, PayOff(B, A)=3, and PayOff(B, B)=1,\ntends to select action label A (the action associated\nwith the maximum possible common-gain). An-\nother example, Llama-3-8B in Prisoners Dilemma,\nwith the same payoffs, tends to select action label B\n(the action associated with the maximum possible\nself-gain).\n(3) Behavioural Bias, where the preferred be-\nhaviour of the Acting Player (AP) and Fellow\nPlayer (FP) affects the frequency of the selected\naction. For example, when GPT-3.5 (the AP) is\nprompted to prioritise Common-Gain (CG) it tends\nto select action label B, irrespective of the FP's pre-\nferred behaviour, and when prompted to prioritise\nSelf-Gain (SG), tends to select action label A.\nTo perform this investigation, we methodically\nadjust the base prompt, seen in Appendix A.4,\nover all combinations of positions, payoffs, and\nbehaviours, making up 16 different experimental\nsetups. Additionally, each of these experimental\nsetups is run with and without prompting the LLM\nto first reason over the problem. To do this, we\nuse the following prompting schemes; (1) Answer-\nOnly (AO) prompt, which requires the LLM to\nrespond only with their answer without any rea-\nsoning, and (2) Zero-shot Chain-of-Though (CoT)\nprompt (Kojima et al., 2022) (the full prompts can\nbe found in Appendix A.4). All experiments dis-\ncussed are applied to both Prisoners Dilemma and\nStag Hunt, however, it is important to note that the\nnames of each game are not explicitly mentioned in\nthe prompt and are only identifiable by their payoff\nmatrices. The reason for this is to promote rea-\nsoning over the payoff matrix and not rely on its\nexisting knowledge of the games. We run all experi-\nments on 4 SOTA LLMS: GPT-3.5, GPT-4-Turbo,\nGPT-40, and Llama-3-8B."}, {"title": "Experiment Analysis", "content": "The observed output of the LLMs is affected by the\nmodels' temperature (T)4. Since we are interested\nin the models' underlying behaviour, independent\nof this randomness, we perform all analyses on\nthe models' top token (the token with the highest\nprobability associated with it). Therefore, for AO\nprompting, we test at T=1.0, a commonly used\ndefault value for temperature. However, since zero-\nshot CoT prompts the LLM to reason, it would be\namiss to not take into consideration the effect of the"}, {"title": "Results", "content": "In the following paragraphs, we will discuss the\nfine-grained observations made for each identified\nbias: (1) Positional Bias, (2) Payoff Bias, and (3)\nBehavioural Bias. Subsequently, we will discuss\nthe higher-level effects the biases have on the per-\nformance of LLMs playing these games.\nPositional Bias. In Figure 3, we can see that the\npositional bias is particularly strong in GPT-3.5\nwhen using the AO prompt. It becomes signifi-\ncantly weaker when asked to reason over the task\nfirst when using the CoT prompt. GPT-4-Turbo,\non the other hand, shows an overall much weaker\nbias towards the position of the action labels, for\nboth prompting methods. Interestingly, the newer\nGPT-40 regresses and shows a strong positional\nbias under both prompting methods. Lastly, Llama-\n3-8B, much like GPT-3.5, shows a strong positional\nbias under AO prompting and a much weaker bias\nunder CoT prompting. More specifically, in Table\n3, we see that GPT-3.5 tends to select the action\nin the first position more frequently. Conversely,\nGPT-4-Turbo and GPT-40 tend to select the action\nin the second position more frequently. Llama-3-\n8B, under AO prompting, selects the first position\nmore frequently and the second position under CoT\nprompting.\nPayoff Bias. In Figure 3, we can see that both\nGPT-3.5 and Llama-3-8B show either a very weak\nor insignificant payoff bias for both prompting\nmethods. Interestingly, both models tend to (1)\nmaximise the common-gain in Stag Hunt and (2)\nmaximise the self-gain in Prisoners Dilemma. This\ncan be seen in Table 4. Looking at Figure 3 again,\nwe see that the payoff bias is strong in both GPT-4-\nTurbo and GPT-40. It is the strongest bias in GPT-\n4-Turbo for both prompting methods in both games,\nwhereas for GPT-40 the payoff bias is weaker un-\nder CoT prompting in Prisoners Dilemma. Pre-\nsumably, the reason for this is that the maximum\npossible self-gain and common-gain in Prisoners\nDilemma is less than in Stag Hunt and is, therefore,\na weaker signal. In tables 4 and 2, we note that\nboth GPT-4-Turbo and GPT-40 tend to maximise\nthe common-gain more frequently. While in pre-\nvious works, this behaviour has led to conclusions\nsuch as \"LLMs have a propensity to be cooperative\"\n(Xu et al., 2023; Brookins and DeBacker, 2023),\nour results suggest that this phenomenon is not in-\ndicative of \"cooperative behaviour\u201d and instead is\na result of a skewed attention towards the action"}, {"title": "Conclusion", "content": "In this work, we first motivated our work by com-\nparing the way humans and GPT-40 play Stag Hunt,"}, {"title": "Limitations", "content": "Through extensive testing and evaluation, possible\nlimitations are identified below.\n(1) We tested only 2 two-player non-zero-sum\ngames, both of which are somewhat similar to one\nanother, and thus cannot make any conclusions on\nhow the effects of the identified biases change for\nmore complicated versions of such games or en-\ntirely different types of games.\n(2) Similarly, we did not test the games under dif-\nferent payoff matrices, and that could affect the\nidentified biases and their impact.\n(3) We only tested 4 SOTA models and therefore\ncannot make any conclusive remarks on how the\nidentified biases change for models of varying size,\ntraining time, training data and so on.\n(4) We purposefully chose to not explicitly mention\nthe games which were being played, namely Stag\nHunt and Prisoners Dilemma, as a means to ensure\nthat the LLM reasons over the payoff matrix, in-\nstead of relying on its existing knowledge of the\ngames from the training data. However, it could be\ninteresting to observe how the biases change when\nthe models are explicitly told which game they are\nplaying.\n(5) We did not collect the data for our preliminary\nstudy and thus were limited to the kind of human-\nAI comparisons we could make."}, {"title": "Ethical Considerations", "content": "This paper aims to investigate the following re-\nsearch questions via a series of experiments: (1)\nHow do LLMs perform on cognitive tasks, such\nas reasoning, navigation, planning, and theory of\nmind? and (2) What are the fundamental limita-\ntions of language models in terms of cognitive abil-\nities? We believe highlighting biases in LLMs has\nsignificant academic and societal impacts. Aca-\ndemically, it suggests possible future directions for\nresearch to be taken. Societally, highlighting these\nbiases provides a clear understanding of the limi-\ntations of people who employ LLMs in real-world\nscenarios. On the other hand, it also exposes the\nweaknesses in current applications, which could\npotentially be exploited by those with malicious\nintent."}, {"title": "Extended Details", "content": "A.1 Game Theory\nGame Theory is the study of how the choices of interacting agents with specific preferences produce\noutcomes, intentional and not (Ross, 2024). Game theory is currently applied to many existing real-world\ntasks in domains such as economics, politics, and psychology (Martin, 2017).\nGame theory models assume that the interacting agents make rational choices, which can be modelled\nas follows (Osborne and Rubinstein, 1994):\n1. A set of actions A from which the agents select their choice.\n2. A set of possible consequences C to action set A.\n3. A function g : A \u2192 C that maps actions to consequences.\n4. A preference relations > on set C. Note that > can be defined by a utility/payoff function U : C \u2192 R\nwhere x > y \u21d4 U(x) \u2265 U(y).\nTherefore, a rational agent chooses a* \u2208 A if g(a*) \u2265 g(a) for all a \u2208 A.\nThese situations, in which rational agents interact with each other by taking action simultaneously, are\nreferred to as strategic games (Osborne and Rubinstein, 1994). The following defines a strategic game:\n1. A finite set of players N.\n2. A nonempty set of actions available to agent; Ai\n3. A preference relation for each agent; \u2265i on set A = A; \u00d7 Aj for j \u2208 N. This is what distinguishes\na strategic game from a decision problem.\nWhile many different qualifications exist for these games, this article focuses on non-zero-sum games.\nThe reason for this is as follows; In zero-sum games, an optimal solution can always be found due to\nits strictly competitive nature, which is not a fair representation of rational agent interactions for many\nimportant real-world scenarios. This is not the case for non-zero-sum games, which can have both\ncompetitive and cooperative elements. Popular examples of such games are Stag Hunt and Prisoners\nDilemma, which will be the focus of this research."}, {"title": "Fisher Exact Test", "content": "The Fisher Exact Test is used to analyse the statistical significance of the relationship between the rows\nand the columns of contingency tables (Kim, 2017). Specifically, the null hypothesis is that the columns\nand rows are independent (McDonald, 2009). Following this, the Fisher Exact test is used to calculate the\np-value and for p < 0.05 (the null hypothesis has less than a 5% chance of being true) we reject the null\nhypothesis. Typically, the Fisher Exact Test is used for smaller sample sizes but is valid for all sizes (Kim,\n2017). It is one of the Exact Tests since the calculation of the p-value does not rely on approximations\n(McDonald, 2009).\nThe Fisher Exact Test uses the hypergeometric distribution to calculate the p-values, which takes on the\ngeneral form (Hoffman, 2015);\nP(A = r, A' = (n \u2212 r)) =\n\\frac{\\binom{X}{r} \\binom{N-X}{n-r}}{\\binom{N}{n}} = \\frac{\\frac{X!}{r!(X-r)!} \\frac{(N-X)!}{(n-r)!(N-X-n+r)!}}{\\frac{n!(N-n!)}{N!}} = \\frac{X! (N-X)! n! (N-n!)}{r!(X-r)! (n-r)!(N-X-n+r)!}\nwhere N is the total population of objects, X \u2208 N have a specific characteristic A and (N \u2212 X) \u2208 X\ndo not, and n is the sample size drawn from N. Essentially, the above is calculating the probability that r\nsamples have a specific characteristic from the n samples drawn.\nNow, given the example contingency Table 6, where N in the sample population, where c\u2081 \u2208 N has\ncharacteristic A and c2 \u2208 N has characteristic A'. Then, from N, r1 \u2208 N samples are drawn and a \u2208r\nsamples have characteristic A. Then to calculate the 2-sided p-value, the Hypergeometric Distribution\nfunction is used to calculate the probabilities that x \u2208 r\u2081 where 0 < x < a have characteristic A. These\nprobabilities are finally summed together. Specifically;\nFET_{2-sided} = \\sum_{x=0}^{a} P(A = x, A' = (r_1 - x))"}, {"title": "Behaviour Preferences", "content": "We test the LLMs under different combinations of preferred behaviours, namely; the acting player (AP)\nwill be prompted with both the preferred behaviours (prioritise Common-Gain or prioritise Self-Gain) of\ntheir fellow player (FP) as well as their own (AP, FP = {CG, SG}). The following states which action\nthe acting agent should select based on the preferred behaviours of both players:\nStag Hunt: Preferred Behaviours\nActing Agent: Common-Gain + Fellow Agent: Common-Gain \u2192 Action = Common-Gain\nActing Agent: Common-Gain + Fellow Agent: Self-Gain \u2192 Action = Self-Gain\nActing Agent: Self-Gain + Fellow Agent: Common-Gain \u2192 Action = Common-Gain\nActing Agent: Self-Gain + Fellow Agent: Self-Gain \u2192 Action = Self-Gain\nNote: selecting the action which maximises the Common-Gain and Self-Gain are both\ncorrect 50% of the time over all setups.\nPrisoners Dilemma: Preferred Behaviours\nActing Agent: Common-Gain + Fellow Agent: Common-Gain \u2192 Action = Common-Gain\nActing Agent: Common-Gain + Fellow Agent: Self-Gain \u2192 Action = Self-Gain\nActing Agent: Self-Gain + Fellow Agent: Common-Gain \u2192 Action = Self-Gain\nActing Agent: Self-Gain + Fellow Agent: Self-Gain \u2192 Action = Self-Gain\nNote: selecting the action which maximises the Common-Gain is the correct action only\n25%, while Self-Gain is correct 75% of the time of over all setups."}, {"title": "Prompts", "content": "For each combination of preferred behaviour, different prompt configurations will be tested. The base\nprompt is as follows:\nBASE PROMPT\nGame Context:\nYou are one player in a 2-player game. In this 2-player game, you and your fellow\nplayer balance risk and reward. Both of you make simultaneous choices between actions label_1\nand label_2. The outcomes are based on the combination of your choices. If both choose label_1,\nyou both gain PO(label_1, label_1) point(s). Opting for label_1 while your opponent picks label_2\nearns you PO(label_1, label_2), and they get PO(label_2, label_1). If both go for label_2, you\nboth receive PO(label_2, label_2) point(s). Choosing label_2 while your opponent selects label_1\nresults in PO(label_2, label_1) points for you and PO(label_1, label_2) for them.\nPlayer Preferences:\nYou: acting agent's preferred behaviour (CGV SG)\nFellow Player: fellow agent's preferred behaviour (CGV SG)\nwhere action labels {label_1, label_2} = {A, B} and label_1 \u2260 label_2. The notation PO() refers to\nthe payoff matrix and the corresponding gain associated with those actions being taken. Varying this\nprompt over the defined sets will result in a total of 16 different experimental setups, which together will\nprovide a sufficient test suit on which the biases of the LLMs can be investigated.\nBelow are the two prompting methods used: (1) Action-Only and (2) Zero-shot Chain-of-Thought\nprompting.\nAction-Only PROMPT\nWhat action should you take given the game context and player preferences?\nYou should only respond in JSON format as described below:\n{\n\"action\": \"selected action, either label_1 or label_2\"\n}\nZero-shot Chain-of-Thought PROMPT\nWhat action should you take given the game context and player preferences?\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"step-by-step reasoning\",\n\"action\": \"selected action, either label_1 or label_2\"\n}\nLet's work this out in a step by step way to be sure that we have the right answer."}, {"title": "Additional Results", "content": "Performance Comparison\nFigure 5 shows a comparison in the average performance (measured in accuracy) between (1) Unmatched:\nthe LLM biases and the experiment setup match and (2) Matched: the LLM biases and the experiment\nsetup do not match.\nStatistical Significance vs A Accuracy: Stag Hunt\ny = 20.12 + 0.16x\nr = 0.85, p-value = 0.01\nStatistical Significance vs A Accuracy: Prisoners Dilemma\ny = 14.46 + 0.24x\nr = 0.70, p-value = 0.05\nFigure 6: Figure showing average statistical significance vs the difference in accuracy achieved when misaligned\nand aligned. (LEFT) Stag Hunt - the correlation coefficient calculated, r = 0.85, is shown to be statistically\nsignificant with a p = 0.01 and (RIGHT) Prisoner Dilemma - the correlation coefficient calculated, r = 0.70, is\nshown to be statistically significant with a p = 0.05. We can, therefore, say that the linear relationship between the\naverage statistical significance of the three measured biases and the performance drop when the experiment setting\nis misaligned is statistically significant in both games.\nLLM Reasoning\nThe following blocks of text show common patterns of reasoning for GPT4-Turbo under the 4 different\nexperimental configurations when both agents are prompted with a preference to prioritise their self-gain\n(similar patterns are observed for other combinations of preferred behaviours).\nSTAG HUNT: REASONING\nConfiguration 1\nGiven the game's rules and the principles of both players, we need to consider the outcomes\nbased on mutual self-interest. If both players choose A, each gains 5 points, which is the\nmaximum possible for mutual choice. If one chooses A and the other B, the player choosing\nB gains 4 points while the other gains 0. If both choose B, each gains 2 points. Since\nboth players prioritize their own points, choosing A could risk gaining 0 points if the other\nplayer chooses B, aiming for 4 points. However, choosing B guarantees at least 2 points,\nand potentially 4 if the other player chooses A. Given the risk of gaining 0 points with\nA and the guarantee of at least 2 points with B, the safer and potentially more rewarding\nchoice is B.\nConfiguration 2\nGiven the game's rules and the principles of both players, we need to consider the outcomes\nbased on mutual decisions. If both choose A, each gains 2 points. If one chooses A and\nthe other B, the one who chooses A gets 4 points, and the one who chooses B gets 0. If\nboth choose B, each gains 5 points. If one chooses B and the other A, the one who chooses\nB gets 0 points, and the one who chooses A gets 4. Since both players prioritize their\nown points, choosing B is the safer and potentially more rewarding option because the\nworst-case scenario (choosing B while the opponent chooses A) results in 0 points, but\nthe best-case scenario (both choosing B) results in 5 points each, which is the maximum\npossible gain for both players simultaneously.\nConfiguration 3\nGiven the game's rules and the principles of both players, we can analyze the outcomes. If\nboth players choose B, each gains 5 points, which is a good outcome. If one chooses B and\nthe other A, the one choosing A gains 4 points while the other gets 0. If both choose A,\neach gains 2 points. Since both players prioritize their own points, choosing B could risk\ngetting 0 points if the other player chooses A, aiming for 4 points. However, choosing A\nguarantees at least 2 points regardless of the other player's choice, and potentially 4 points\nif the other chooses B. Therefore, choosing A maximizes the guaranteed points and also\noffers a chance for a higher reward if the opponent chooses B.\nConfiguration 4\nGiven the game'sGiven the game's rules and the principles of both players, we need to consider the outcomes\nbased on each combination of choices. If both players choose B, each earns 2 points. If\none chooses B and the other A, the B chooser gets 4 points and the A chooser gets 0. If\nboth choose A, each earns 5 points. Since both players prioritize their own points, choosing\nA could be risky if the other player chooses B, resulting in 0 points. However, choosing\nB is safer as the worst outcome is 2 points (if both choose B) and the best is 4 points (if\nthe opponent chooses A). Given that both players aim to maximize their own points, it's\nlikely the opponent might also consider choosing B to avoid the risk of earning 0 points.\nTherefore, choosing B seems to be the safer and potentially more rewarding strategy based\non the game setup and player principles.\nPRISONERS DILEMMA: REASONING\nConfiguration 1\nGiven the game's rules and the principles of both players, we need to consider the outcomes\nof each combination of choices. If both choose A, each gains 2 points. If one chooses A\nand the other B, the one who chooses B gains 3 points while the other gains 0. If both\nchoose B, each gains 1 point. Since both players prioritize their own points, choosing B is a\nsafer option because it guarantees at least 1 point and potentially 3 points if the opponent\nchooses A. Choosing A risks gaining 0 points if the opponent chooses B.\nConfiguration 2\nGiven the game's rules and the principles of both players, we need to consider the outcomes\nof each combination of choices. If both players choose A, each gains 1 point. If one chooses\nA and the other B, the one who chooses A gains 3 points while the other gets 0. If both\nchoose B, each gains 2 points. If one chooses B and the other A, the one who chooses B\ngets 0 points and the other gets 3. Since both players prioritize their own points, choosing B\nis safer as it guarantees at least 2 points if both choose B, whereas choosing A risks getting\n0 points if the opponent chooses B. Therefore, choosing B maximizes the minimum gain\n(minimax strategy).\nConfiguration 3\nGiven that both players prioritize their own points, choosing action B is risky unless both\nplayers choose B, which results in 2 points each. However, if one chooses B and the other\nchooses A, the player who chose A gets 3 points while the one who chose B gets 0. If both\nchoose A, each player gets 1 point. Since both players aim to maximize their own points,\nits likely that the opponent will choose A to avoid the risk of getting 0 points by choosing\nB. Therefore, to maximize your own points in anticipation of the opponents likely choice\nof A, you should also choose A to ensure you get at least 1 point rather than risking getting\n0 points.\nConfiguration 4\nGiven the game's rules and the principles of both players, we need to consider the outcomes\nbased on each possible combination of choices. If both players choose B, each earns 1\npoint. If one chooses B and the other A, the player choosing B earns 3 points while the\nother earns 0. If both choose A, each earns 2 points. If one chooses A and the other B,\nthe player choosing A earns 0 points while the other earns 3. Since both players prioritize\ntheir own points, choosing A could risk earning 0 points if the other player chooses B.\nHowever, choosing B guarantees at least 1 point and can earn up to 3 points if the other\nplayer chooses A. Therefore, choosing B maximizes the potential gain while minimizing\nthe risk of earning no points."}]}