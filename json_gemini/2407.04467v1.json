{"title": "Are Large Language Models Strategic Decision Makers? A Study of Performance and Bias in Two-Player Non-Zero-Sum Games", "authors": ["Nathan Herr", "Fernando Acero", "Roberta Raileanu", "Mar\u00eda P\u00e9rez-Ortiz", "Zhibin Li"], "abstract": "Large Language Models (LLMs) have been increasingly used in real-world settings, yet their strategic abilities remain largely unex- plored. Game theory provides a good frame- work for assessing the decision-making abili- ties of LLMs in interactions with other agents. Although prior studies have shown that LLMs can solve these tasks with carefully curated prompts, they fail when the problem setting or prompt changes. In this work we investi- gate LLMs' behaviour in strategic games, Stag Hunt and Prisoner Dilemma, analyzing perfor- mance variations under different settings and prompts. Our results show that the tested state- of-the-art LLMs exhibit at least one of the fol- lowing systematic biases: (1) positional bias, (2) payoff bias, or (3) behavioural bias. Sub- sequently, we observed that the LLMs' perfor- mance drops when the game configuration is misaligned with the affecting biases. Perfor- mance is assessed based on the selection of the correct action, one which agrees with the prompted preferred behaviours of both play- ers. Alignment refers to whether the LLM'S bias aligns with the correct action. For exam- ple, GPT-40's average performance drops by 34% when misaligned. Additionally, the cur- rent trend of \"bigger and newer is better\" does not hold for the above, where GPT-40 (the cur- rent best-performing LLM) suffers the most substantial performance drop. Lastly, we note that while chain-of-thought prompting does re- duce the effect of the biases on most models, it is far from solving the problem at the funda- mental level.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have become in- creasingly ubiquitous, as indicated by a significant increase in research containing keywords relating to LLMs, with close to a 500% increase since 2018 (Naveed et al., 2023). There has also been an in- crease in people using them to solve everyday tasks in fields such as medicine, education, finance, and law (Hadi et al., 2023; Duan et al., 2024). However, as LLMs are deployed in the real world, where they interact with other humans or artificial agents, there is an underserved need to understand the capabili- ties of LLMs to operate in complex social scenarios. The ability to reason strategically about interactions with other agents is a fundamental aspect of human intelligence (Qiao et al., 2022; Huang and Chang, 2022; Sahoo et al., 2024; Zhang et al., 2024). In fact, many works have begun asking the question (Huang et al., 2024; Xu et al., 2023; Duan et al., 2024): can LLMs assist in everyday tasks which require the ability to understand the complex en- vironment in which they are operating, anticipate potential outcomes, infer the intentions and beliefs of others (with whom they are collaborating or com- peting), and think critically about all these factors to come to the best possible strategic decision. To answer this question, game theory, which is already applied to many existing real-world tasks (Martin, 2017), is looked to as a well of wisdom (Huang et al., 2024; Xu et al., 2023; Duan et al., 2024; Fan et al., 2024; Guo et al., 2024; Zhang et al., 2024; Lor\u00e8 and Heydari, 2023; Li et al., 2023a; Brookins and DeBacker, 2023; Gandhi et al., 2023; Akata et al., 2023; Phelps and Russell, 2023; Guo, 2023; Gemp et al., 2024).\nGame theory studies how the choices of inter- acting agents, with specific preferences, produce outcomes, intentional and not (Ross, 2024). For the reader's interest, a more in-depth definition of Game Theory models and rational agents can be found in Appendix A.1. Game theoretic tasks ab- stract complex real-life scenarios as mathematical models designed to be easy to understand but re- quire the above skills to be mastered. While many different qualifications exist for these tasks, this work focuses on non-zero-sum games. Non-zero- sum games have both competitive and cooperative elements, which provide a fair representation of agent interactions for many important real-world scenarios. Popular examples of such games are Stag Hunt and Prisoners Dilemma, which will be the focus of this research. Further details on these games can also be found in Appendix A.1.\nPrompt engineering (Sahoo et al., 2024) has emerged as an effective technique for improving LLM's performance on complex tasks involving reasoning. However, the majority of existing works do not test the robustness of their specifically cu- rated prompts and fail drastically when the setting in which they are operating changes (Guo et al., 2024). This frailty has been discovered and investi- gated in many of the state-of-the-art (SOTA) LLMs (Fan et al., 2024; Chen et al., 2024; Papadatos and Freedman; Zheng et al., 2023, 2024; Wang et al., 2023) such as LLama-2, GPT-3.5, and GPT- 4. Specifically, LLMs suffer from what we refer to as systematic biases, which guide the LLMs' decisions and not their strategic reasoning. This phenomenon has been investigated mainly in mul- tiple choice question answering and similar tasks but has not been investigated thoroughly in game theoretic tasks.\nWe believe it is clear, and hope that the reader is starting to agree, that LLMs must be capable of"}, {"title": "2 Preliminary Study: Humans vs AI", "content": "Belloc et al. (2019) present an experiment where a random sample of participants (88 people) play 4 games (352 games) of Stag Hunt under a time constraint, while a second set of participants (97) play 4 games (388 games) unconstrained. The con- strained set of participants will be referred to as making a reflexive choice (without reasoning) and the unconstrained set make a reasoned choice.\nProcessing the data made available by Belloc et al. (2019), we found that when humans are mak- ing a reflexive choice, they typically hunt the Stag 62% of the time and the Hare 38% of the time. When making a reasoned choice, they still prefer hunting the Stag, choosing it 52.5% of the time, with the Hare being hunted more often at 47.5%.\nAs a preliminary study, to further motivate our work, we prompted GPT-40 with similar instruc- tions and settings (where label A is Hare and B is Stag) the human participants were provided in (Belloc et al., 2019) to compare the frequency at which Stag and Hare are selected. We prompt GPT-40 to provide an answer only for the reflexive choice (400 games) and to reason over the task be- fore providing the answer for the reasoned choice (400 games). We can see that GPT-40 exhibits a similar but stronger trend to the human participants: it selects Stag 100% of the time when making the reflexive choice and 95% of the time under the reasoned choice, with a slight increase in Hare hunting during the reasoned choice.\nWe also prompted GPT-40 under different set- tings, where label A is Stag and B is Hare. We notice that the frequency at which Stag and Hare are selected changes drastically, picking the hare 75.3% of the time under the reflexive choice and 100% of the time under the reasoned choice. While the human participants are not tested under this setting, it's assumed humans wouldn't change their choice based on how the actions are labelled. This observation indicates that GPT-40 does not reason over these tasks in a human-like way and is instead influenced by other signals, such as the positional bias we see here. We, therefore, aim to explore further what these signals are and how they affect the performance of LLMs in these types of game theoretic tasks."}, {"title": "3 Related Work", "content": "Having established the importance of investigat- ing how systematic biases affect LLMs' ability to make strategic decisions in two-player non-zero- sum games, we now explore existing related work.\nLLMs and Game Theory. In recent years, the use of LLMs as single-agent planners/decision makers has evolved into LLM-based multi-agent systems (Guo et al., 2024) where agents are re- quired to solve strategic and logical reasoning problems. These capabilities are often evaluated through game-theoretic tasks (Zhang et al., 2022; Lor\u00e8 and Heydari, 2023; Gandhi et al., 2023). In fact, this shift has prompted many new benchmarks testing LLMs in game theoretic tasks with the aim of progressing the work within the field (Xu et al., 2023; Huang et al., 2024; Chen et al., 2023; Duan et al., 2024; Li et al., 2023b; Aher et al., 2023). Furthermore, several existing works focus on game theoretic matrix games, such as Prisoners Dilemma, Stag Hunt, and Dictator Game, to name a few (Fan et al., 2024; Xu et al., 2023; Lor\u00e8 and Heydari, 2023; Brookins and DeBacker, 2023; Gandhi et al., 2023; Phelps and Russell, 2023; Guo, 2023), which are discussed below. In Fan et al. (2024)'s work they show that LLMs, even when explicitly given the correct belief, from which they should reason to take correct action, tend to ignore or modify this belief. They also note that LLMs tend to select spe- cific action labels more frequently than others (they note that GPT-3 prefers U to V), but do not investi- gate this any further. Xu et al. (2023) and Brookins and DeBacker (2023) show that LLMs tend to se- lect the cooperative action more frequently than humans, despite it not being the optimal choice in most cases. Lastly, several works test how the LLMs' behaviour changes as they modify the LLMs' preferences or contextual frameworks, such as being selfish or cooperative (Fan et al., 2024; Phelps and Russell, 2023; Guo, 2023; Lor\u00e8 and Heydari, 2023). They all note that LLMs are seem- ingly capable of following simple preferences, such as selecting the selfish action when prompted to be selfish. However, they do not investigate the effect the chosen prompt configuration has on the LLMs' performance.\nBias in LLMs. The presence of systemic biases\u00b9 (such as favouring a specific action label U over label V (Fan et al., 2024)) has recently become a topic of interest. Specifically, these biases are found and tested in multiple choice question evalu- ation (Zheng et al., 2023), multi-turn question an- swer evaluation (Zheng et al., 2024), response qual- ity evaluation (Wang et al., 2023), and tasks such as text classification, fact retrieval, and information extraction (Zhao et al., 2021; Chen et al., 2024; Berglund et al., 2023; Golovneva et al., 2024). It was found that LLMs suffer from what is referred to as selection bias (Zheng et al., 2023, 2024; Wang et al., 2023; Zhao et al., 2021), which is a combina-"}, {"title": "4 Methodology", "content": "This paper aims to investigate how the identified biases affect the capability of LLMs to solve non- zero-sum two-player games. The biases identified, are as follows:\n(1) Positional Bias, where the order in which the action labels are stated in the prompt affects the frequency of the selected action label. For example, Llama-3-8B, when prompted with action label A first and B second, tends to select the first action label A more frequently.\n(2) Payoff Bias, where the payoffs associated with the different action labels, PayOff(label_1, label_2), affects the frequency of the selected ac- tion label. In particular, a model may be biased to- wards, (1) selecting the action that leads to the max- imum possible self-gain or (2) selecting the action that leads to the maximum possible common-gain, rather than the action that maximizes the expected gain (which takes into account all possible actions the other agent can take). Note that in Stag Hunt these two actions are the same and for Prisoners Dilemma they are not (see Table 5 in Appendix A.1 for further details). For example, GPT-4-Turbo in Prisoners Dilemma, when PayOff(A, A)=2, Pay- Off(A, B)=0, PayOff(B, A)=3, and PayOff(B, B)=1, tends to select action label A (the action associated with the maximum possible common-gain). An- other example, Llama-3-8B in Prisoners Dilemma, with the same payoffs, tends to select action label B (the action associated with the maximum possible self-gain).\n(3) Behavioural Bias, where the preferred be- haviour of the Acting Player (AP) and Fellow Player (FP) affects the frequency of the selected action. For example, when GPT-3.5 (the AP) is prompted to prioritise Common-Gain (CG) it tends to select action label B, irrespective of the FP's pre- ferred behaviour, and when prompted to prioritise Self-Gain (SG), tends to select action label A.\nTo perform this investigation, we methodically adjust the base prompt, seen in Appendix A.4, over all combinations of positions, payoffs, and behaviours, making up 16 different experimental setups. Additionally, each of these experimental setups is run with and without prompting the LLM to first reason over the problem. To do this, we use the following prompting schemes; (1) Answer- Only (AO) prompt, which requires the LLM to respond only with their answer without any rea- soning, and (2) Zero-shot Chain-of-Though (CoT) prompt (Kojima et al., 2022) (the full prompts can be found in Appendix A.4). All experiments dis- cussed are applied to both Prisoners Dilemma and Stag Hunt, however, it is important to note that the names of each game are not explicitly mentioned in the prompt and are only identifiable by their payoff matrices. The reason for this is to promote rea- soning over the payoff matrix and not rely on its existing knowledge of the games. We run all experi- ments on 4 SOTA LLMS: GPT-3.5, GPT-4-Turbo, GPT-40, and Llama-3-8B.\nExperiment Analysis\nThe observed output of the LLMs is affected by the models' temperature (T). Since we are interested in the models' underlying behaviour, independent of this randomness, we perform all analyses on the models' top token (the token with the highest probability associated with it). Therefore, for AO prompting, we test at T=1.0, a commonly used default value for temperature. However, since zero- shot CoT prompts the LLM to reason, it would be amiss to not take into consideration the effect of the"}, {"title": "5 Results", "content": "In the following paragraphs, we will discuss the fine-grained observations made for each identified bias: (1) Positional Bias, (2) Payoff Bias, and (3) Behavioural Bias. Subsequently, we will discuss the higher-level effects the biases have on the per- formance of LLMs playing these games.\nPositional Bias. In Figure 3, we can see that the positional bias is particularly strong in GPT-3.5 when using the AO prompt. It becomes signifi- cantly weaker when asked to reason over the task first when using the CoT prompt. GPT-4-Turbo, on the other hand, shows an overall much weaker bias towards the position of the action labels, for both prompting methods. Interestingly, the newer GPT-40 regresses and shows a strong positional bias under both prompting methods. Lastly, Llama- 3-8B, much like GPT-3.5, shows a strong positional bias under AO prompting and a much weaker bias under CoT prompting. More specifically, in Table 3, we see that GPT-3.5 tends to select the action in the first position more frequently. Conversely, GPT-4-Turbo and GPT-40 tend to select the action in the second position more frequently. Llama-3- 8B, under AO prompting, selects the first position more frequently and the second position under CoT prompting.\nPayoff Bias. In Figure 3, we can see that both GPT-3.5 and Llama-3-8B show either a very weak or insignificant payoff bias for both prompting methods. Interestingly, both models tend to (1) maximise the common-gain in Stag Hunt and (2) maximise the self-gain in Prisoners Dilemma. This can be seen in . Looking at Figure 3 again, we see that the payoff bias is strong in both GPT-4- Turbo and GPT-40. It is the strongest bias in GPT- 4-Turbo for both prompting methods in both games, whereas for GPT-40 the payoff bias is weaker un- der CoT prompting in Prisoners Dilemma. Pre- sumably, the reason for this is that the maximum possible self-gain and common-gain in Prisoners Dilemma is less than in Stag Hunt and is, therefore, a weaker signal. In and 2, we note that both GPT-4-Turbo and GPT-40 tend to maximise the common-gain more frequently. While in pre- vious works, this behaviour has led to conclusions such as \"LLMs have a propensity to be cooperative\" (Xu et al., 2023; Brookins and DeBacker, 2023), our results suggest that this phenomenon is not in- dicative of \"cooperative behaviour\u201d and instead is a result of a skewed attention towards the action that leads to maximum possible gains, despite it not always being the optimal choice.\nBehavioural Bias. In , we can see that both GPT-4-Turbo and GPT-40 are weakly affected by the behavioural bias, whereas, both GPT-3.5 and Llama-3-8B are strongly affected. Specifi- cally, GPT-3.5 is weakly biased when using the AO prompt and strongly biased when using the CoT prompt. Llama-3-8B shows a different pat- tern where it is strongly biased when using the AO prompt and weakly biased when using the CoT prompt. However, in Prisoners Dilemma, it is still the strongest bias under the CoT prompt. In , we observed that GPT-3.5 tends to select action label A when prompted to prioritise Common-Gain and action label B when prompted to prioritise Self-Gain. Again, Llama-3-8B shows a different pattern, where it tends to select action label A when prompted to prioritise Self-Gain and action label B when prompted to prioritise Common-Gain.\nAlignment. Now that the fine-grained details of each identified bias have been discussed, let us con- sider the high-level effects these biases have on the performance of LLMs playing these games. In , it can be seen that in all models, under both prompting methods, when the bias of the LLM and the experimental setup are misaligned, the per- formance is much worse. Specifically, GPT-3.5, GPT-4-Turbo, GPT-40, and Llama-3-8B show an average performance drop, over both prompt- ing methods, of 32%, 25%, 34%, and 28%, re- spectively in Stag Hunt, and 28%, 16%, 34%, and 24% respectively in Prisoners Dilemma. It's worth noting that employing CoT prompting gen- erally lessens the performance drop due to bias misalignment, however, this isn't the case with GPT-4-Turbo. This is because there is a significant relationship between the performance drop and the strength of the biases experienced by the models and it can be seen that GPT-4-Turbo's biases strengthen with CoT prompting. Additionally, all models, ex- cept GPT-4-Turbo under CoT prompting, perform equally or better in the Prisoner's Dilemma com- pared to the Stag Hunt. The suspected reason is that in Prisoners Dilemma the action which maximises self-gain is the correct action 75% of the time (in Stag Hunt it is 50%, refer to Appendix A.3 for more detail), for all experimental setups, and that GPT-4-Turbo is the only model which is primarily biased to select the action which maximises the common-gain which causes it to select the incorrect action more frequently."}, {"title": "6 Conclusion", "content": "In this work, we first motivated our work by com- paring the way humans and GPT-40 play Stag Hunt,"}, {"title": "7 Limitations", "content": "Through extensive testing and evaluation, possible limitations are identified below.\n(1) We tested only 2 two-player non-zero-sum games, both of which are somewhat similar to one another, and thus cannot make any conclusions on how the effects of the identified biases change for more complicated versions of such games or en- tirely different types of games.\n(2) Similarly, we did not test the games under dif- ferent payoff matrices, and that could affect the identified biases and their impact.\n(3) We only tested 4 SOTA models and therefore cannot make any conclusive remarks on how the identified biases change for models of varying size, training time, training data and so on.\n(4) We purposefully chose to not explicitly mention the games which were being played, namely Stag Hunt and Prisoners Dilemma, as a means to ensure that the LLM reasons over the payoff matrix, in- stead of relying on its existing knowledge of the games from the training data. However, it could be interesting to observe how the biases change when the models are explicitly told which game they are playing.\n(5) We did not collect the data for our preliminary study and thus were limited to the kind of human- AI comparisons we could make."}, {"title": "8 Ethical Considerations", "content": "This paper aims to investigate the following re- search questions via a series of experiments: (1) How do LLMs perform on cognitive tasks, such as reasoning, navigation, planning, and theory of mind? and (2) What are the fundamental limita- tions of language models in terms of cognitive abil- ities? We believe highlighting biases in LLMs has significant academic and societal impacts. Aca- demically, it suggests possible future directions for research to be taken. Societally, highlighting these biases provides a clear understanding of the limi- tations of people who employ LLMs in real-world scenarios. On the other hand, it also exposes the weaknesses in current applications, which could potentially be exploited by those with malicious intent."}, {"title": "A Extended Details", "content": "A.1 Game Theory\nGame Theory is the study of how the choices of interacting agents with specific preferences produce outcomes, intentional and not (Ross, 2024). Game theory is currently applied to many existing real-world tasks in domains such as economics, politics, and psychology (Martin, 2017).\nGame theory models assume that the interacting agents make rational choices, which can be modelled as follows (Osborne and Rubinstein, 1994):\n1. A set of actions A from which the agents select their choice.\n2. A set of possible consequences C to action set A.\n3. A function $g : A \\rightarrow C$ that maps actions to consequences.\n4. A preference relations > on set C. Note that > can be defined by a utility/payoff function $U : C \\rightarrow R$ where $x > y \\Leftrightarrow U(x) \\geq U(y)$.\nTherefore, a rational agent chooses $a^* \\in A$ if $g(a^*) \\geq g(a)$ for all $a \\in A$.\nThese situations, in which rational agents interact with each other by taking action simultaneously, are referred to as strategic games (Osborne and Rubinstein, 1994). The following defines a strategic game:\n1. A finite set of players N.\n2. A nonempty set of actions available to agent; $A_i$\n3. A preference relation for each agent; $\\geq_i$ on set $A = A_i \\times A_j$ for $j \\in N$. This is what distinguishes a strategic game from a decision problem.\nWhile many different qualifications exist for these games, this article focuses on non-zero-sum games. The reason for this is as follows; In zero-sum games, an optimal solution can always be found due to its strictly competitive nature, which is not a fair representation of rational agent interactions for many important real-world scenarios. This is not the case for non-zero-sum games, which can have both competitive and cooperative elements. Popular examples of such games are Stag Hunt and Prisoners Dilemma, which will be the focus of this research. The contingency tables of which can be seen in"}, {"title": "A.2 Fisher Exact Test", "content": "The Fisher Exact Test is used to analyse the statistical significance of the relationship between the rows and the columns of contingency tables (Kim, 2017). Specifically, the null hypothesis is that the columns and rows are independent (McDonald, 2009). Following this, the Fisher Exact test is used to calculate the p-value and for p < 0.05 (the null hypothesis has less than a 5% chance of being true) we reject the null hypothesis. Typically, the Fisher Exact Test is used for smaller sample sizes but is valid for all sizes (Kim, 2017). It is one of the Exact Tests since the calculation of the p-value does not rely on approximations (McDonald, 2009).\nThe Fisher Exact Test uses the hypergeometric distribution to calculate the p-values, which takes on the general form (Hoffman, 2015);\n$P(A = r, A' = (n - r)) = \\frac{{X \\choose r} {N-X \\choose n-r}}{{N \\choose n}} = \\frac{\\frac{X!}{r!(X-r)!} \\frac{(N-X)!}{(n-r)!(N-X-n+r)!}}{\\frac{N!}{n!(N-n!)}}$\nwhere N is the total population of objects, X \u2208 N have a specific characteristic A and (N \u2212 X) \u2208 X do not, and n is the sample size drawn from N. Essentially, the above is calculating the probability that r samples have a specific characteristic from the n samples drawn.\nNow, given the example contingency Table 6, where N in the sample population, where $c_1 \\in N$ has characteristic A and $c_2 \\in N$ has characteristic A'. Then, from N, $r_1 \\in N$ samples are drawn and $a \\in r$ samples have characteristic A. Then to calculate the 2-sided p-value, the Hypergeometric Distribution function is used to calculate the probabilities that x \u2208 r\u2081 where 0 < x < a have characteristic A. These probabilities are finally summed together. Specifically;\n$FET_{2-sided} = \\sum_{x=0}^{a} P(A = x, A' = (r_1 - x)) $"}, {"title": "A.3 Behaviour Preferences", "content": "We test the LLMs under different combinations of preferred behaviours, namely; the acting player (AP) will be prompted with both the preferred behaviours (prioritise Common-Gain or prioritise Self-Gain) of their fellow player (FP) as well as their own (AP, FP = {CG, SG}). The following states which action the acting agent should select based on the preferred behaviours of both players:"}, {"title": "A.4 Prompts", "content": "For each combination of preferred behaviour, different prompt configurations will be tested. The base prompt is as follows:"}, {"title": "A.5 Additional Results", "content": "Performance Comparison\nshows a comparison in the average performance (measured in accuracy) between (1) Unmatched: the LLM biases and the experiment setup match and (2) Matched: the LLM biases and the experiment setup do not match."}]}