{"title": "AI-generated Image Quality Assessment in Visual Communication", "authors": ["Yu Tian", "Yixuan Li", "Baoliang Chen", "Hanwei Zhu", "Shiqi Wang*", "Sam Kwong*"], "abstract": "Assessing the quality of artificial intelligence-generated images (AIGIs) plays a crucial role in their application in real-world scenarios. However, traditional image quality assessment (IQA) algorithms primarily focus on low-level visual perception, while existing IQA works on AIGIs overemphasize the generated content itself, neglecting its effectiveness in real-world applications. To bridge this gap, we propose AIGI-VC, a quality assessment database for AI-Generated Images in Visual Communication, which studies the communicability of AIGIs in the advertising field from the perspectives of information clarity and emotional interaction. The dataset consists of 2,500 images spanning 14 advertisement topics and 8 emotion types. It provides coarse-grained human preference annotations and fine-grained preference descriptions, benchmarking the abilities of IQA methods in preference prediction, interpretation, and reasoning. We conduct an empirical study of existing representative IQA methods and large multi-modal models on the AIGI-VC dataset, uncovering their strengths and weaknesses.", "sections": [{"title": "Introduction", "content": "Image generation has undergone significant advancements with the help of artificial intelligence (AI) technology. Recent research has demonstrated the potential benefits of AI in various visual communication fields, particularly in advertising. For example, Coca-Cola used an AI platform to create a series of advertisements (ads) for its brand, creating deeper engagement than existing ones. Some large e-commerce platforms, such as Amazon and Alibaba, utilize AI technology to generate personalized ad content, enhancing the efficiency of ad development and increasing the impact of the ads. For applications that require visual communication, high-quality images not only fully and clearly convey a certain message, but also evoke the inner emotion that the visual designer wants to reflect . However, due to hardware limitations and technical proficiency, the quality of AI-generated images (AIGIs) varies widely, necessitating refinement and filtering before distributing them to practical applications.\nThere have been substantial efforts in establishing benchmarks to facilitate research on AIGI quality assessment. However, these benchmarks emphasize the quality of generated content for general purposes, overlooking the effectiveness of AIGIs in real-world applications. For practical applications in visual communication, the primary challenges in evaluating the quality of AIGIs arise from two aspects: 1) information clarity: each element in the text message must be present and clearly depicted in the image; 2) emotional interaction: the image must powerfully evoke the intended emotion in the viewers. It is crucial to develop an IQA benchmark that is closely aligned with practical use cases. In this work, we contribute a dataset called AIGI-VC, the first-of-its-kind database to study the communicability of AI-Generated Images in Visual Communication. The overview of the AIGI-VC dataset is shown in Fig. 1. The AIGI-VC dataset comprises a diverse collection of 2,500 images, encompassing 14 distinct ad topics and representing 8 different types of emotions. We conduct subjective experiments via pairwise comparisons on two evaluation dimensions"}, {"title": "Related Works", "content": "We present a summary of representative datasets for AIGI quality assessment in Table 1."}, {"title": "Dataset Construction", "content": "To cover diverse content and emotions, the AIGI-VC dataset involves two common ad types, i.e., product ads and public service announcements (PSAs). Product ads promote commercial products or services and generally aim to evoke pos-"}, {"title": "Human Preference Annotation", "content": "Coarse-grained Preference Choices We collect human opinions via the pairwise image comparison method, directly asking participants to choose their preferred image from a pair. Generally speaking, global ranking results of N test stimuli are derived from exhaustive pairwise comparisons, which involve conducting  pairwise comparisons. However, this process is time-consuming and expensive. Therefore, as suggested in , we employ Thurstone's Case V model  to estimate the missing human labels using a subset of the exhaustive pairwise comparison data. Let Xi and xj represent two images generated from the same prompt. We collect the preference entry Ci,j, which indicates the number of times x\u2081 is preferred over xj. The global ranking scores Q = {q}=1 can be estimated by solving the following maximum a posterior (MAP) estimation problem:\narg max  \u2211 Ci, j log (\u03a6(qi \u2013 qj)) - \\frac{(qi)^2}{2},\nsubject to \u2211 qi = 0,\nwhere (.) is the standard normal cumulative distribution function.\nTo verify the reliability and effectiveness of MAP estimation, we compare the correlation between the scores estimated from a subset of the exhaustive pairwise comparison data and the true scores obtained through exhaustive pairwise comparisons. Specifically, we selected 250 images generated from 50 prompts in the AIGI-VC dataset. In each round, each image was randomly paired with another image with the same prompt. We repeated this process for M rounds and calculated the accuracy of preference choices derived from the estimated ranking scores. Following the data reliability recommendations in , we collected responses from 20 participants (11 males and 9 females) aged 21 to 31. Due to the preference ambiguity caused by the similar quality of the two images in a pair , we focus on cases with strong estimated preferences, namely those where the preference probabilities fall outside the range of [0.3, 0.7].\nThe results are shown in Fig. 3, we can find that the estimated preferences can recover the true preferences when M is 4. Therefore, in our subjective experiment, 20 participants are employed to label 2,000 pairs randomly sampled from the whole AIGI-VC dataset, reducing the required number of exhaustive pairwise comparisons by 60% while producing the same preferences. We provide a visualization of the estimated preference probabilities in the AIGI-VC database, shown in Fig. 4, from which one can observe the Spearman Rank-Order Correlation Coefficient (SRCC) and Pearson"}, {"title": "Fine-grained Descriptions", "content": "We further provide detailed descriptions to determine the reasons that influence human judgments of images, enhancing the interpretability and transparency of the AIGI-VC dataset. As illustrated in Fig. 5, we adopt a humans-in-the-loop strategy  to reduce workload and enhance data reliability. Specifically, to obtain more detailed and comprehensive descriptions, we treat the top-ranked image in each evaluation dimension for each prompt as a pseudo-reference and incorporate GPT-40 to identify why the other image under the same prompt is worse than the pseudo-reference. Furthermore, we provide various visual cues for each evaluation dimension. For information clarity, the visual cues include text-image alignment, sharpness, texture details, and rationality . For emotional interaction, the visual cues include layout, emphasis, movement, rhythm, human action, brightness, and colorfulness . To avoid subjective divergence, we remove image pairs where both images have similar quality. We collect responses from GPT-40 as the initial descriptions and recruit human experts to verify and supplement each GPT-generated description, creating a golden standard description. To better illustrate how those visual factors contribute to quality assessment, we present the frequently occurring words in golden standard descriptions. The results and analyses are provided in the supplementary materials."}, {"title": "Evaluation on AIGI-VC", "content": "Baselines We employ 14 objective metrics for performance comparisons, including one emotion classifier , one vanilla quality assessment metrics designed for natural images , five CLIP-based metrics tailored for AIGIs , and seven LMMs that accept multiple images as input . Detailed information of these LMMs is summarized in supplementary materials. It is worth noting that we re-train the WSCNet from scratch on a large-scale visual emotion dataset . To ensure fairness, we use the default hyperparameters provided by the original models.\nCriteria We exploit various evaluation criteria to quantify the capabilities of the competing models in terms of preference prediction, interpretation and reasoning. Regarding preference prediction, we use three criteria: 1) Correlation (p): the linear correlation between the ground-truth and predicted preference probabilities; 2) Accuracy (\u03b1): the ratio of image pairs correctly predicted by the model; 3) Consistency (\u03ba): the criteria is designed for LMMs, which measures whether the predictions from LMMs are robust to the presentation order of two images. More specifically, Given an image pair (x, y) and its reference information z (text or emotion category). f is the model to be tested, where f((x, y), z) = 1 if x is preferred over y given z, and f((x,y), z) = 0 otherwise. The accuracy, consistency, and correlation of the model can be computed as follows,\n\u03c1 = PLCC(P(x,y)|z, P(x,y)|z),\n\\kappa=\\frac{1}{|D|} \\sum_{((x, y), z) \\in D} \\mathbb{I} [f((x, y), z)+f((y, x), z)=1],\n\u03b1 = \\frac{1}{|D|} \\sum_{((x, y), z) \\in D} \\mathbb{I} [f((x, y), z)=\\mathbb{I} [P_{(x, y) \\mid z}>0.5]],\nwhere |D| and \\mathbb{I} are the total number of pairs and the indicator function, respectively. P(x,y)|z denotes the ground-truth preference probability that x is preferred over y given reference information z.  and  represent the ground-truth and the predicted preference probabilities of all pairs in the whole dataset. PLCC is the Pearson linear correlation coefficient measure. It is worth noting that all predicted scores by the model are fitted before computing the preference probabilities. The higher values of \u03b1, \u03c1, and \u03ba signify a better performance of the model.\nRegarding preference interpretation and reasoning, we employ the GPT-assisted evaluation method to evaluate LMM responses against the golden descriptions. Following the suggestions in , we employ three evaluation criteria: (1) Completeness (Comp.): Encouraging LLM outputs that closely align with the golden description; (2) Preciseness (Prec.): Penalizing outputs that include informa-"}, {"title": "Performance on Preference Prediction", "content": "We input image pairs and their corresponding reference information into the models (except for HyperIQA, as it only supports image inputs) to evaluate performance regarding information clarity and emotional interaction. For information clarity evaluation, the reference information is the text; for emotion interaction evaluation, the reference information is the emotion category. The results are shown in Tables 2&3, where Dall and Dsub represent all pairs with the full range of preference probabilities from 0 to 1 and a subset of image pairs where humans show strong preferences, respectively. We can see that 1) in terms of prediction accuracy on information clarity and emotional interaction dimensions, GPT-40 significantly outperforms all other competing models, particularly surpassing other LMMs; 2)"}, {"title": "Performance on Interpretation and Reasoning", "content": "We evaluate the interpretation and reasoning abilities of the LMMs using golden descriptions. During the interpretation process, the LMMs analyze human choices and infer the reasons behind these preferences. During the reasoning process, we provide two images and require the LMMs to conduct a detailed comparison, ultimately making a preference decision based on the comparative analysis. The results are shown in Tables 6&7. We can draw the following findings: 1) GPT-40 achieves the best performance in preference interpretation and reasoning across all criteria; 2) for both preference interpretation and reasoning, most LMMs exhibit high relevance values but lower completeness and precision values. The results suggest that while LMMs responses effectively address visual cues within each evaluation dimension, they often lack comprehensive coverage and include conflicting information, leading to less accurate and less complete responses."}, {"title": "Conclusion", "content": "In this work, we introduce AIGI-VC, a quality assessment dataset containing 2,500 AIGIs across 14 ad topics and 8 emotion types. AIGI-VC facilitates the quality assessment of AIGIs in terms of information clarity and emotional interaction, providing coarse-grained and fine-grained human preference annotations. Our experimental results highlight the need for an IQA metric to effectively handle the unique characteristics of AIGIs in visual communication. We hope that our dataset and analysis will shed light on the development of more robust and accurate IQA metrics, enhancing the effectiveness of AIGIs in practical applications."}]}