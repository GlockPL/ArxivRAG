{"title": "Who Brings the Frisbee: Probing Hidden Hallucination Factors in Large Vision-Language Model via Causality Analysis", "authors": ["Po-Hsuan Huang", "Jeng-Lin Li", "Chin-Po Chen", "Ming-Ching Chang", "Wei-Chao Chen"], "abstract": "Recent advancements in large vision-language models (LVLM) have significantly enhanced their ability to comprehend visual inputs alongside natural language. However, a major challenge in their real-world application is hallucination, where LVLMs generate non-existent visual elements, eroding user trust. The underlying mechanism driving this multimodal hallucination is poorly understood. Minimal research has illuminated whether contexts such as sky, tree, or grass field involve the LVLM in hallucinating a frisbee. We hypothesize that hidden factors, such as objects, contexts, and semantic foreground-background structures, induce hallucination. This study proposes a novel causal approach: a hallucination probing system to identify these hidden factors. By analyzing the causality between images, text prompts, and network saliency, we systematically explore interventions to block these factors. Our experimental findings show that a straightforward technique based on our analysis can significantly reduce hallucinations. Additionally, our analyses indicate the potential to edit network internals to minimize hallucinated outputs.", "sections": [{"title": "1. Introduction", "content": "Large vision-language models (LVLM) can comprehend multimodal data and respond to human commands [8,21,45, 54]. Alongside advancements in network architectures, significant research focuses on improving response accuracy and reducing deviations from human instructions [8, 21]. Despite these efforts, modern LVLMs struggle with real-world challenges due to their notorious hallucinations [2, 22] jeopardizing downstream reliability and safety.\nLVLM hallucinations occur when the generated contents do not align with the provided visual cues or include unrelated or incorrect texts [22]. Mitigating hallucinations by fine-tuning LVLMs with human preferences is effective but expensive, requiring extensive human annotations [49]. To reduce costs, recent research employs auxiliary models, such as object grounding and language refinement models, to automatically generate pseudo annotations [47]. Alternatively, approaches that require LVLMs to answer multiple verification questions iteratively incur significant computational overhead [38]. Besides directly optimizing human preferences, post-training inference calibration, for example, enforcing the decoding with contrast to erroneous variations, can partly reduce hallucination probability [14].\nDespite various strategies proposed to reduce hallucination in LVLMs, a limited understanding of their response behaviors still hinders further research. Clearly, various uncontrolled hidden factors contribute to these intricate hallucinations when the LVLMs process multi-modality data. For example, Figure 1 shows an example where InstructBLIP [8] erroneously describes a nonexistent frisbee. This mistake likely arises from the presence of a large green grass field in the photo, where frisbees frequently co-occur in the training data. Spurious correlations lead to cross-modality retrieval errors, where models predict objects based on their frequent occurrences in training data [12]. Mitigating these errors requires fine-grained data augmentation and balancing across modalities [4]. Additionally, poor image-text alignment biases the decoding mechanism towards language, which tends to neglect image contents [11, 14]. Analytical studies, such as [53], have identified factors like occurrence, uncertainty, and object position through statistical analyses, but lack a unified framework to analyze the hidden factors inducing hallucination. These studies have not scrutinized hidden context factors, such as the people, trees, or grass in the image, that might induce hallucinations.\nIt is crucial to understand the conditions that lead to LVLM hallucinations and analyze their causal patterns. When there are green fields, it is likely to have a frisbee in the training data, leading to the hallucination mentioned above in Figure 1. However, the reverse scenario is less likely, due to the relatively fewer training data focusing mainly on frisbees. Additionally, there is a significant gap in studies connecting hallucination analysis with hallucination reduction strategies. To address this, we systematically investigate four fundamental research questions about LVLM hallucination concerning visual objects: (1) Are semantic structures affecting hallucination? (2) What are the effects of non-hallucinated objects that are potentially accompanied by hallucinated objects? (3) How likely can we intervene in LVLM regarding hallucinated objects to reduce the effects of hidden factors? (4) Are there salient network internals implying network hallucination?\nThis study explores object hallucination patterns and introduces a novel causal intervention scheme to analyze LVLM behaviors. We examine the causal relations between visual objects in the dataset and observe the generated LVLM outputs, overviewed in Figure 3. Our findings highlight the key relations between a main subject and the context. We demonstrate that simple intervention to the observed structures can notably reduce hallucinations. Additionally, we investigate the network internal saliency of latent embeddings based on causally-related inputs. Our analysis results pave the way for seamless interference in model generation to mitigate hallucination occurrences.\nOur contributions are summarized in the following:\n\u2022 Investigating hallucination relations between hallucination-inducing words and hallucinatory words\n\u2022 Identifying a unified causality graph to develop hallucination reduction solutions\n\u2022 Analyzing intervention on text, image, and embedding\n\u2022 Probing embedding properties of high-hallucinatory and non-hallucinatory images"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Large Vision-Language Model (LVLM)", "content": "Large pre-trained models have heralded a new era of vision language models. InstructBLIP [8], mPLUG-Owl [44, 45], MiniGPT [54], and LLaVa [21] all leverage autoregressive pre-trained large language models (LLMs) paired with fine-tuned vision encoders and multimodal alignment modules. Researchers construct extensive instruction datasets for instruction tuning, aiming to enhance response quality [21]. Additionally, automatic instruction generation approaches can effectively scale up training [36]. Despite addressing known shortcomings of models, challenges persist in fully understanding their behaviors."}, {"title": "2.2. Hallucination Detection and Elimination", "content": "Mitigating hallucination issues was initially studied in the field of image captioning and later extended to LVLMs. Traditional image captioning metrics such as CIDEr [34] and METOER [3] often fail to capture the object hallucination within a sentence. The polling-based hallucination detection method (POPE) involves a large set of questions asking whether a specific object is present in an image [16]. Despite the LVLM evaluation rapidly expanding to various types of hallucinations [30,33,40], the fundamental issue of visual object hallucination remains unsolved.\nInstruction tuning: LRV-Instruction [20] dataset is designed to balance positive and negative instructions for robust instruction tuning. Direct preference optimization improves the model using annotated data of both hallucinated and non-hallucinated samples, supported by a reward model providing feedback during fine-tuning [9]. However, the quality of collected instructions and the computational overhead continue to be major inconveniences.\nSelf-check and auxiliary models: Volcano [13] is a self-feedback guided revision model that iteratively asks itself questions to improve response quality. To mitigate the self-bias of LVLMs leading to hallucinations, object existence verification can be achieved by prompting another open-vocabulary object detector [41,51]. These verification results serve as a reward model during LVLM fine-tuning. More complex multiple-step strategies including question generation, object grounding, and language refinement are subsequently designed [46, 47]. These methods avoid up-"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Datasets, LVLM Setup, and Hallucination Evaluation Metrics", "content": "Datasets: We utilize the AMBER dataset [37] and COCO dataset [17] for evaluation. AMBER is a benchmark dataset assessing LVLM hallucination, including human annotations on both truly appeared objects and potentially hallucinated ones. The setting of COCO is followed by prior works [10] with randomly selected 500 images from the 2014 validation set. Here, we prompt LVLMs with \u201cDescribe this image.\u201d to obtain descriptions for the images.\nLarge vision-language model (LVLM): We utilize an auto-regressive transformer-based LVLM denoted as f. The input token sequence $X \\in \\mathbb{R}^{N \\times T}$ with N samples and T timesteps is fed into the LVLM to the latent embedding $E \\in \\mathbb{R}^{N \\times T \\times D}$ for D latent dimensions and generate the text A = f(X). We employed two established LVLMs including InstructBLIP [8] and mPLUG-Owl2 [45].\nHallucination evaluation metrics: The evaluation function H assigns a hallucinatory score H(A) \u2208 R to a response A, indicating hallucination rates with given metrics. We follow the metrics used in the dataset papers for evaluation. We assess LVLM hallucination using several common metrics: CHAIR, Cover, HAL, and Cog scores on AMBER dataset [37] as well as CHi, CHs, and Recall on COCO [10]. CHAIR [31] measures the hallucination generation rate. CHi and CHs, represent the image-level and sentence-level hallucination rates, respectively. Cover and Recall measure the ratio of mentioned existing objects to all existing objects in the image. HAL indicates whether the CHAIR score is non-zero for a sentence. Cog measures the hallucinatory object rate resembling human cognition."}, {"title": "3.2. Hallucination Statistics", "content": "We investigate the hallucination results using human-annotated labels from the AMBER dataset to illustrate the underlying data and model properties.  lists the top 5 most common hallucination cases, including a single-hallucinatory word, co-occurring hallucinatory words, and the hallucinatory-inducing words. The hallucinatory-inducing words are non-hallucinatory while associated with other hallucinatory words. These cases were detected using InstructBLIP and mPLUG-Owl2 on the AMBER dataset, suggesting that LVLMs exhibit different hallucination inclinations.\nCo-occuring hallucinatory words: Frequently hallucinated words tend to co-occur with other hallucinated words. Once a hallucination occurs, other hallucinations are likely to follow. We present the top 5 most common co-occurring hallucination words. Observe that co-occurring hallucinatory words differ from single-hallucinatory words. The appearance of these paired hal-\nlucinated words is usually syntactically correct in image descriptions, which might be one reason that paired cooccurrence words appear. These observations suggest that the underlying syntactic and semantic structures influence the relationships between words of hallucinations.\nHallucinatory inducing words: Aside from the co-occurrence of hallucinatory words, we are also interested in an important causal factor \u201cWhat induces a hallucination?\u201d. Given a generated response A, the mentioned object set is denoted as $S = \\{$s_1,...,s_n$\\}$ and the ground-truth object set is $O = \\{$o_1,...,o_m$\\}$. The hallucinatory words are the objects absent in the ground-truth object set, i.e., $O_h = S \\setminus O$, and the set of non-hallucinatory words is denoted as $O_n = S \\cap O$. We regard the unrevealed relation between hallucinatory and non-hallucinatory words using conditional probability $P(O_h | O_n)$ [28].  shows the $O_n \\rightarrow O_h$ relations in AMBER. Words like 'tree', 'water', 'sky', 'beach', and 'road' are scene-like nouns, which usually describe the background information. The appearance of these types of nouns often induces other hallucinatory words. For example, a tree induces a bicycle to hallucinate probably because they are likely to appear in the same scene. However, the observed hallucination word relations remain unclear regarding causality, raising concerns about spurious correlations and risks of misinterpretation. Therefore, we utilize the causality analysis to identify solutions for hallucination reduction."}, {"title": "3.3. Causality Analysis", "content": "We formulate a causal graphical model involving the input random variables: image I, text query Q, a latent variable of target object $Z_o$, context factor $Z_c$, and the resulting answer A. This model is represented by a directed acyclic graph (DAG) as shown in Figure 2. In this graph, a directed edge between variables indicates a direct causal influence of the parent node on the child node.\nWe distinguish and abstract the variables $Z_o$ and $Z_c$ at a cognitive level. $Z_o$ represents the ideal semantic representation of target objects (e.g., the concept of a car), while $Z_c$ as a confounding variable denotes a context pattern that could diversify the comprehension of the car. Figure 2a depicts an ideal LVLM generation, where A is independent of $Z_c$. However, the inherent bias in the training data introduces $Z_c$ into the causal graphical model for LVLM generation in Figure 2b, shaping the unwanted causal effect $Z_c \\rightarrow Z_o$.\nWe adopt the standard causality analysis approach [27, 29]; performing an intervention on input nodes to block the undesired effects from the confounding factor $Z_c$. We set up the causal effect metric $\\delta(P, P')$ between a distribution P and the distribution after intervention P' is defined as the fewer hallucinations after the intervention; that is, $\\delta = I(H(A) > H(A'))$, where A' denotes the resulting answer after the intervention and H is the hallucination evaluation metric defined in \u00a73.1. The causal effect metric $\\delta$ reflects whether an intervention successfully reduces hallucination. We measure the total causal effect (TCE) [26, 32] over an evaluation test set. For the intervention results, TCE calculates the expected value E of $\\delta$ over the evaluation test set X by the equation:\n$TCE = E_{X' \\sim P(X)}[\\delta(P, P')]. \\qquad(1)$\nBased on the causal relation depicted in Figure 2, we can leverage intervention approaches to interrupt the undesired effects of $Z_c$. The intervention is denoted as $do(X: x \\rightarrow x')$ and simplified as $do(X)$, where x and x' are inputs before and after an intervention. The path $Z_o \\leftarrow Z_c \\rightarrow A$ forms a backdoor path and thus and thus we aim to perform an intervention to block the path $Z_c \\rightarrow Z_o$. To achieve this, we consider intervening image I (\u00a73.4), text query Q (\u00a73.5), or latent embedding E (\u00a73.6), shown in Figure 3 and described in the following subsections."}, {"title": "3.4. Image (I) Intervention", "content": "In Figure 2c, the effect of the query text Q is minimized by using a fixed, simple prompt as described in \u00a73.3. This allows us to focus on the interactions between I, $Z_o$, $Z_c$, and A. We aim to perform intervention do(I) on the image I while assuming that the image after intervention I' minimally changes $Z_c$. Formally, we assume that $P(Z_c | I') \\approx P(Z_c | I)$. This assumption holds if I' primarily affects $Z_o$ and has a much weaker effect on $Z_c$. We expect this to hold in our design, which involves small object modifications and focuses on the region related to $Z_o$.\nThe confounding effect of $Z_c$ can thus be mitigated using the backdoor adjustment. We can examine the TCE based on Eq. (1) with $P' = P(A|Q, do(I))$. We implement this image intervention do(I) using two different object manipulation designs: pasting a small object in the background of I, based on the observation of the differing hallucinatory tendencies of LVLMs regarding the target objects and the rest context elements, as shown in Figure 3; and removing a hallucinatory-inducing object from I where the object is specified by statistics in \u00a73.2. Specifically, for the image-pasting intervention, we paste a small image featuring a single object, sized to one-sixth of the shortest side of I, at the top left corner. This ensures the object is recognizable and in the background, implicitly affecting $Z_o$. For the object removal intervention, we remove one hallucinatory-inducing object in the image based on the highest hallucinatory frequency $\\Sigma_{O_i \\in O_n} P(O_i | O_n)$, where $O_n$ is the non-hallucinatory object set and Or is its corresponding hallucinatory object set. We report these prior statistics in Table 1. For example, if 'person' is a non-hallucinatory object, it is most likely associated with a hallucinatory object; thus, it has the highest priority for removal when present in I. We utilize the combination of the GroundingDINO [23] and IA [48] to detect and segment the object and then fill the masked area using the inpainting technique."}, {"title": "3.5. Text (Q) Intervention", "content": "Our proposed text intervention technique comprises two steps, separately prompting for the foreground (FG) and background (BG) generation. This foreground-background (FGBG) technique is designed based on the idea of interrupting the effects from $Z_c$ via changing the object concepts $Z_o$ on the backdoor path, $Z_o \\leftarrow Z_c \\rightarrow A$ shown in Figure 2d. We directly intervene in the text query as Q' with the FGBG strategy and thus $Z'_o$ changes accordingly, paving an alternative causal way to prevent $Z_c$ from affecting $Z_o$. The quantified TCE of Q intervention involves the $P(A|do(X))$ probability distribution. Our designed FGBG approach is a Chain-of-Thought (CoT)-like prompting technique with the front-door adjustment [29, 50]. By introducing a mediator variable S to perform the two-step prompting, the probability distribution to generate A becomes as below:\n$P(A|do(X)) = \\Sigma_S P(A|do(S))P(S|do(X)). \\qquad(2)$\nThis approach avoids the intractable access to variable $Z_c$ as the context variable $Z_c$ can hardly be specified. Therefore, we seek an estimation of the term $P(S|do(X))$ and $P(A|do(S))$ for tractable results. Here, $P(S|do(X))$ and $P(A|do(S))$ correspond to the FG and BG, respectively. First, $P(S|do(X))$ is computed in the path of $X \\leftarrow Z_c \\rightarrow A \\leftarrow S$ between X and S. The collision structure of $Z_c \\rightarrow A \\leftarrow S$ allows us to block the backdoor path and derive $P(S|do(X)) = P(S|X)$. Second, $P(A|do(S))$ is computed by blocking the path $S \\leftarrow X \\leftarrow Z_c \\rightarrow A$ using backdoor adjustment: $P(A|do(S)) = \\Sigma_X P(X)P(A|S, X) = E_X[P(A|S, X)]$. Instead of navigating the unconstrained space of X, we follow the estimation based on the expectation value of X [39].\n$E_X[P(A|S, X)] \\approx P(A|S, E[X]) \\approx P(A|S \\oplus E[X]), \\qquad(3)$\nwhere $\\oplus$ denotes vector concatenation. We avoid multiple iterations to form a CoT by specifying the prompt to represent $Z_o$ and minimize the variation. Our observed foreground-background description structure and the non-hallucinatory tendency of the first sentence [53] ensure FG to be specific and consistent. We thus empirically use X in a single run to replace E[X]."}, {"title": "3.6. Embedding Intervention", "content": "Our embedding intervention approach inspired by model editing research [7, 15] can generate a direct intervention to $Z_o$ in Figure 2e without model parameter updates. Depending on samples X in a proxy dataset with hallucination annotations, we derive a hallucinated group $X_h = \\{x \\in X | H(f(x)) > 0\\}$ and a non-hallucinated group $X_n = \\{x \\in X | H(f(x)) = 0\\}$. The corresponding embeddings for $X_h$ and $X_n$ are denoted as $E_h$ and $E_n$. In contrast to other studies aiming to edit specific words, our targeted generative tasks lack fixed ground truth. Therefore, we propose to measure salient latent embedding dimensions in terms of hallucination and edit these dimensions by retrieving values of the dimensions from non-hallucinated data.\nEmbedding saliency map: We measure the saliency dimensions over the whole sequence via statistical significance. Specifically, we utilize the Student\u2019s t-test to examine each dimension between $E_h$ and $E_n$. We select the dimensions with a p-value smaller than 0.001 and derive the saliency maps $M \\in \\mathbb{R}^{T \\times D}$ indicating the dimensions statistically significant in discriminating hallucination and non-hallucination groups in the proxy dataset.\nEmbedding editing: We calculate the distance between a query embedding $E_q$ and the proxy non-hallucinated embeddings $E_n$ from $X_n$. The average embedding $E_K$ of the most similar top-K embedding is then selected through the $l_2$-distance k-nearest neighbor approach. $E_K = \\frac{1}{K} \\Sigma^K_i E_i$. Each query sample obtains the mean embedding from most similar and non-hallucinated samples in the proxy dataset. Then, $E_K$ serves as a non-hallucinated prototype to edit the identified hallucination embedding saliency map M. Therefore, the derived embeddings become: $E'_q = (1 \u2212 p) * E_q + p * M * E_K$, where p denotes a hyperparameter determining editing strength. The embedding $E_q$ is then used to decode the output texts A'. This embedding editing technique is an intervention to $Z_o$ to lessen the effects of $Z_c$ in Figure 2b."}, {"title": "4. Experiments", "content": "We carry out experiments to show the impact of interventions on reducing hallucinations as detailed in \u00a73.4, \u00a73.5, and \u00a73.6. Our comparisons are aligned with previous studies on hallucination reduction using the AMBER dataset [37] and COCO validation subset. We test our three intervention approaches on InstructBLIP [8] and mPLUG-Owl2 [45] using the default parameter settings of the original paper. For mPLUG-Owl2, we set the hyperparameters temperature and max new tokens to 0.7 and 512, respectively, to derive similar results in AMBER benchmark [37].\nBaselines: We compare our approach with the following baselines: (1) Opera [10]: reduces hallucinations by lowering the attention weight on the summary tokens based on observation occurring on these tokens. (2) VCD [14]: distorts detected hallucinated objects and performs contrastive decoding to avoid generating these objects."}, {"title": "4.1. Evaluation of Image Intervention", "content": "We evaluate our image-pasting and object-removal intervention methods (\u00a73.4) for reducing LVLM hallucinations. The image-pasting intervention uses a single rabbit as the pasted object. Table 2 compares our method with two state-of-the-art approaches (Opera and VCD) using two LVLMs (InstructBLIP and mPLUG-Owl2). Our method shows consistent improvements in reducing hallucinations on both LVLMs across the AMBER and COCO datasets and performs comparably to Opera and VCD. This supports our finding that focusing LVLMs away from non-informative backgrounds reduces hallucinations.\nWe also explore various pasting factors, such as style and semantic relationships, using the COCO dataset; see Table 3. We find that inpainting, which blends the pasted object into the background, achieves a more consistent style than direct pasting. For semantic pasting and inpainting, we use objects from the same or different supercategories as the original image. Specifically, non-semantic pasting, such as inserting a bird into a kitchen image, performs better by reducing spurious correlations, while semantic inpainting, which inserts objects with similar semantics, tends to increase related hallucinations.\nIn contrast, the object removal intervention, as shown in Table 2, results in lower CHAIR and HAL scores but achieves a high Cover rate and a comparable Cog score on the AMBER dataset. Object removal may not effectively reduce hallucinations due to complications like residual background after removing large objects, which can lead to additional hallucinatory artifacts. To better understand the impact of object removal, we also measure its effect on frequently hallucinated objects and their associated inducing words. These findings are detailed in the Supplementary materials."}, {"title": "4.2. Evaluation of Text Intervention", "content": "We compare the overall hallucination rate and coverage rate in Table 2. Our proposed Foreground-Background (FGBG) prompt can be separated into FG and BG steps as described in \u00a73.5. We additionally consider another baseline: Stop prompt provides a hint for non-existing objects for the model by the prompt: \u201cThere are no [Oh] in the image. Then, describe the image.\u201d The object terms Oh are derived from a prior output of LVLM on the same sample which is compared with the annotated ground-truth. This is a hard upper bound for intervention and suggests the possibility of hardly corrected samples.\nIn Table 2, the FGBG prompt achieves 5.6 CHAIR, 27.8 HAL, and 2.6 Cog scores using InstructBLIP on the AMBER dataset. Similarly, mPLUG-Owl2 shows low hallucination results with 5.6 CHAIR, 26.5 HAL, and 2.1 Cog scores. The improved results with low hallucination did not compromise coverage, with the highest 53.2 and 54.3 Cover scores using InstructBLIP and mPLUG-Owl2, respectively, indicating a more precise response correction ability compared to other baseline methods. Additionally, the FGBG prompt outperforms the baselines with significantly lower CH5 and CHi scores for both LVLMs. InstructBLIP shows a 31.2% reduction in CH5 and 9.1% reduction in CHi, while mPLUG-Owl2 demonstrates a 30.0% reduction in CH5 and 8.0% reduction in CHi, indicating an overall relative hallucination reduction of around 50%.\nWe examine the two-step results of FGBG prompt. LVLMs generally respond to the FG prompt with short responses similar to image captioning. This leads to notably low CHAIR, HAL, and Cog scores on the AMBER dataset using InstructBLIP (2.2, 4.5, and 0.2) and mPLUG-Owl2 (3.9, 14.1, and 0.7). In contrast, the BG prompt induces more hallucinations compared to the FG, with CHAIR scores increasing by 6.4% and 7.8%. The higher HAL scores (27.1% with InstructBLIP and 19.6% with mPLUG-Owl2) further indicate uncertainty in background descriptions. However, FG prompt results in a Cover score reduction of 16.5% and 10% with InstructBLIP and mPLUG-Owl2, respectively. A similar disadvantage is seen with the Stopping prompt, which leads to a Cover score decline of 5.7% and 3.3% using InstructBLIP and mPLUG-Owl2, respectively. These results are intriguing, as the LVLMs continue to hallucinate even when informed that certain objects do not exist. This suggests a tradeoff between being verbose and conservative for LVLMs. Notably, InstructBLIP tends to replicate foreground descriptions more frequently, resulting in higher Cover scores. However, when combined with foreground descriptions, the results are not superior to those of mPLUG-Owl2. Another noteworthy observation is that the effects of hallucination reduction are not uniform across each Zo."}, {"title": "4.3. Evaluation of Embedding Intervention", "content": "The quantitative evaluation of our embedding intervention approach (\u00a73.6) is reported in Table 2. Compared to the baseline results on the AMBER dataset, Embedding reduces a 5.2% HAL score while maintaining a 52.7% Cover score for InstructBLIP. Meanwhile, Embedding results for mPLUG-Owl2 sacrifices 1.1% Cover score but achieves a 7% reduction in the HAL score. Promising results are observed on the COCO dataset which achieves declined CHAIR scores with kept recall scores.\nSaliency visualization: Figure 4 visualizes the calculated embedding saliency map M described in \u00a73.6. The salient dimensions can be observed in light green for pos-eh\nitive values ($e_h > e_n$) and dark blue for negative values ($e_h < e_n$) given the index (i, j) with embedding values $e_h = E_h(i, j)$ and $e_n = E_n(i,j)$. The sparsity of the saliency map illuminates the limited underlying causal feature dimensions affecting hallucination. The visualized results averaging over attention heads are shown in Figure 4 and the other figures for different heads, timestamps, and dimensions are reported in the supplementary.\nEmbedding property analysis: In the previous experiments, a few data are less likely to be hallucinated which can be used as stable targets to assess the property of an embedding. Specifically, we regard a group of data $X_{stable}$ that is never hallucinated with our three text prompts in \u00a74.2. When an arbitrary embedding E retrieves its nearest neighbors Eo, we can identify if Eo is in the $X_{stable}$. For a sample set, we obtain a ratio of the retrieved data belonging to $X_{stable}$ and term this ratio as a \u201cretrieval safe score\u201d. A higher retrieval safe score signifies that this set of embeddings is located near less hallucinated samples.\nWe regard the cases that comprise either a common hallucinatory-inducing word ($O_n$) or a hallucinatory word ($O_h$) described in \u00a73.2 narrated in the response. The word can occur in a response that belongs to a group $X_h$ being hallucinated with the raw LVLM inference or the stable group $X_{stable}$ less likely to be hallucinated. Figure 4 (c) demonstrates the results of $O_n$ within the groups of $X_h$ or $X_{stable}$ and (d) shows $O_h$ results.\nIn both Figure 4 (c) and (d), $X_h$ consistently yields lower retrieval safety scores than $X_{stable}$, indicating distinct properties in the embedding space. Aside from the general finding, it is necessary to consider the scores under the condition that a given word occurred in that the contexts might be specified differently. Figure 4 (c) shows that samples mentioned 'desk' and 'sign' attain low scores in $X_{stable}$ yet still higher than the scores in $X_h$. A given word of $O_n$ in Figure 4 (c) exhibits consistently low safe scores for $X_{stable}$ while $O_h$ in Figure 4 (d) includes unexpected high safe scores such as the frisbee. That is, $O_n$ stands closer to hallucinatory embeddings while $O_h$ does not. This suggests that non-hallucinatory objects ($O_n$) pose hidden risks for inducing hallucinations, offering new insights into managing hallucinations via embedding space."}, {"title": "5. Conclusion", "content": "We introduce a causal hallucination probing scheme to analyze potential approaches to mitigate hallucination and identify the underlying hallucination structure that non-hallucinatory objects can induce hallucination. Our analyses explore image, text, and embedding interventions in the causal framework that can block unwanted causality relations of the inducing objects. Our proposed simple approaches achieve significant reduction over other hallucination mitigation methods without model parameter updating. Further, our investigation in embedding intervention uncovers the potential to manipulate the representation space directly. For future works, we will apply causal effects to more complex multimodal tasks and unveil the complex relation in large-scale training and testing datasets. Further exploring the cross-modality causality and model internals in reflecting hallucination under various circumstances is still critical research to extend this study."}]}