{"title": "Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody Prompting", "authors": ["Wooseok Han", "Minki Kang", "Changhun Kim", "Eunho Yang"], "abstract": "Speaker-adaptive Text-to-Speech (TTS) synthesis has attracted considerable attention due to its broad range of applications, such as personalized voice assistant services. While several approaches have been proposed, they often exhibit high sensitivity to either the quantity or the quality of target speech samples. To address these limitations, we introduce Stable-TTS, a novel speaker-adaptive TTS framework that leverages a small subset of a high-quality pre-training dataset, referred to as prior samples. Specifically, Stable-TTS achieves prosody consistency by leveraging the high-quality prosody of prior samples, while effectively capturing the timbre of the target speaker. Additionally, it employs a prior-preservation loss during fine-tuning to maintain the synthesis ability for prior samples to prevent overfitting on target samples. Extensive experiments demonstrate the effectiveness of Stable-TTS even under limited amounts of and noisy target speech samples.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in zero-shot TTS models have showcased their ability to generate near-human quality speech resembling the voice of any speaker, using only a few seconds of target speech [1]\u2013[5]. However, these models frequently face two major issues. First, extensive pre-training on large datasets, comprising thousands of hours of speech, is necessary to achieve high-quality zero-shot TTS for any speaker [2]. Second, despite extensive pre-training on vast amounts of speech corpora, it remains challenging to generate natural-sounding speech that perfectly mirrors the voice of any speaker, particularly given the distribution shift between source corpora and the target speaker.\nTo tackle these challenges, an alternative approach called transfer learning, which leverages knowledge from the source domain and involves fine-tuning on speech samples of target speaker, has been widely employed in TTS synthesis [6]\u2013[8]. These models have achieved high stability with small pre-training datasets and effective personalization through fine-tuning on diverse few-shot speech samples, usually under a minute. However, they often struggle with generating speech that satisfies both naturalness and speaker similarity criteria, particularly in situations where the few-shot samples suffer from issues such as noise, clipping, or distortion. For instance, fine-tuning a TTS model with 'in-the-wild' recordings [9] is challenging due to unclear pronunciation and severe background noise, unlike the clear speech data in pre-training [10]. In such cases, fewer fine-tuning steps may reduce voice similarity, while excessive fine-tuning can compromise the TTS model's integrity to produce clear and intelligible speech. Motivated by these limitations, we introduce Stable-TTS, a novel speaker-adaptive TTS framework designed to ensure prosody consistency and enhance timbre, even under conditions of limited amount of noisy target samples. Stable-TTS achieves this by leveraging a small subset of high-quality prior samples from the pre-training dataset. Specifically, Stable-TTS utilizes a prosody encoder and a Prosody Language Model (PLM) [4] to guide prosody generation while using a style encoder primarily as a timbre encoder to reinforce the target speaker's timbre. Furthermore, we exploit a prior-preservation loss [11] during fine-tuning to maintain synthesis ability for prior samples, thereby preventing overfitting on target samples. Extensive experiments demonstrate the effectiveness of Stable-TTS in producing high-quality speech samples, excelling in intelligibility, naturalness, and speaker similarity, even under challenging conditions of limited and noisy target samples.\nTo summarize, our contributions are as follows:\nWe propose Stable-TTS, a speaker-adaptive TTS framework using a small subset of the high-quality prior samples from a pre-training dataset.\nStable-TTS leverages prior samples by incorporating prosody encoder and PLM to ensure prosody consistency, and employs prior-preservation loss to prevent overfitting.\nExtensive experiments show that Stable-TTS produces high-quality speech with strong naturalness and speaker similarity, even with limited and noisy target samples."}, {"title": "II. STABLE-TTS", "content": "In this section, we introduce Stable-TTS, a speaker-adaptive TTS framework that ensures prosody consistency even with limited noisy target speech samples. This is achieved by leveraging high-quality prior samples from the pre-training dataset during both fine-tuning and inference stages. The architecture of Stable-TTS incorporates prior samples to stabilize the fine-tuning process, as shown in Fig. 2, allowing for high-quality, consistent output even under challenging conditions. The following sections detail the architecture (Section II-A), the use of prior samples in inference (Section II-B), and fine-tuning stabilization (Section II-C).", "subsections": [{"title": "A. Diffusion-Based Zero-Shot TTS Models", "content": "Our model stems from diffusion-based zero-shot TTS models [3], [12]. Specifically, our model consists of five key modules: text encoder $T$, prosody encoder $P$ [4], variance adaptor $V$ [13], [14], timbre encoder $S$ [15], and the diffusion model [12]. To train this model, we use the training dataset consisting of paired samples of the text $t$ in the form of phoneme sequences and its speech $X$ in the form of a mel-spectrogram. First, the text encoder $T$ and prosody encoder $P$ encode text $t$ and speech $X$ into latent representations, $\\mu_{ph} = T(t)$ and $p_{ph} = P(X)$, respectively. Here, $p_{ph}$ represents prosody codes at the phoneme level, matching the length of $\\mu_{ph}$. Notably, the prosody encoder includes a vector quantization layer [16] at the end, to discretize the prosody representations into the fixed set of discrete prosody codes. The variance adaptor $V$ then expands both $\\mu_{ph}$ and $p_{ph}$ to sequences $\\mu = V(\\mu_{ph})$ and $p = V(p_{ph})$ respectively, where both sequences are in the same length with the target mel-spectrogram $X$.\nNext, the timbre encoder $S$ extracts timbre features from mel-spectrograms. During training, we sample $k$ mel-spectrograms from the same speaker as $X$ and concatenate them to strengthen the timbre characteristics of the target speaker [4] using the timbre encoder as follows:\n$s = S(X_R)$, $X_R = concat(X_{r_1}, ..., X_{r_k}),$ (1)\nwhere $concat$ is the concatenation operator and $X_{r_1}, ..., X_{r_k}$ are the mel-spectrograms from the target speaker."}, {"title": "B. Prosody Language Model for Prior Prosody Prompting", "content": "As detailed in Section II-A, our TTS model encodes prosody $p$ and timbre $s$ into distinct representations [4], [19]. The timbre vector $s$ is continuous and unconstrained, while the prosody codes $p$ are discrete and limited to a predefined codebook. These codes are designed to align with the input text $t$, as their purpose is to apply appropriate prosodic elements to the textual content.\nDuring inference, since the target speech and input text are not directly aligned, an additional module is required to predict the prosody codes for the input text. Mega-TTS [4] introduced a Prosody Language Model (PLM) to predict prosody codes in an auto-regressive manner, similar to a decoder-only language model used in GPT-2 [20]. The PLM is trained with a language modeling objective [4], predicting the prosody codes $p_t$ given the prompt speech $X$, as follows:\n$\\arg \\max_{p_t} P_{LM}(p_t|p, \\mu, p_{<t}, \\mu_{<t}),$ (4)\nwhere $P_{LM}$ is the trained PLM, $p = V(P(X))$ represents the prompt prosody codes derived from the prompt speech $X$, and $\\mu = V(T(t))$ is the encoded text from the prompt speech $t$.\nBy constraining the predicted prosody codes $p$ to the predefined set in the codebook, we ensure that the prosody"}, {"title": "C. Prior-Preservation Loss for Fine-Tuning", "content": "Fine-tuning the TTS model with the target speaker's speech samples is crucial for accurately cloning the speaker's voice while preserving naturalness [7]. However, this becomes challenging when the available samples are limited and noisy, which is common in real-world recordings. A well-known issue is that fine-tuning on a few noisy samples can lead to overfitting, causing the model to generate noisy speech [3].\nTo address this, we focus on fine-tuning the diffusion model, considering its critical role in synthesizing speech from representations. However, overfitting to limited noisy samples often results in noisy outputs and reduces the effectiveness of prosody codes, making it harder to generate clean speech.\nTo mitigate this, we introduce prior-preservation loss during fine-tuning, inspired by a personalized text-to-image diffusion model [11]. This loss maintains the pre-trained distribution by minimizing the difference in estimated noise between clean and noisy samples. As illustrated in Fig. 3, we use a subset of high-quality prior samples from the pre-training dataset, aiming to minimize the mean squared error between the estimated noise of two diffusion models\u2014one with frozen pre-trained weights and the other with modified weights during fine-tuning\u2014based on these prior samples as follows:\n$\\mathcal{L}_{ppl} = E_{t,x} ||\\epsilon(X_t, t, \\mu,p, s; \\theta') - \\epsilon(X_t, t, \\mu, p, s; \\theta)||^2,$ (5)\nwhere $X$ is prior samples from the pre-training dataset, and $X_t$ is a forwarded noisy input $X$ at timestep $t$. In this process, $\\theta'$ is the pre-trained and frozen parameter of the noise estimator, and $\\theta$ is a parameter to be fine-tuned. The prior-preservation loss ensures that the diffusion model retains the ability to synthesize clean speech. During fine-tuning, the noise estimator is fine-tuned by minimizing both $\\mathcal{L}_{diff}$ in Equation (3) and an auxiliary loss $\\mathcal{L}_{ppl}$. This approach allows the diffusion model to generate high-quality speech with the voice of the target speaker, even when only a few noisy samples of the target speaker are available."}]}, {"title": "III. EXPERIMENTS", "content": "", "subsections": [{"title": "A. Experimental Setup", "content": "Datasets. Stable-TTS is pre-trained on the clean-100 and clean-360 subsets of LibriTTS-R [22], an English multi-speaker dataset with 245 hours of audio from 553 speakers. For fine-tuning, we utilize the configuration from 24 speakers in the test-clean subset of LibriTTS [10], and the configuration from 24 speakers in the VCTK [21] dataset, with each containing 20 audio clips averaging 1-4 seconds in length. Furthermore, to validate Stable-TTS in real-world noisy scenarios, we utilize 20 speakers from VoxCeleb [9], comprising audio-visual short clips extracted from the interview video dataset. The transcripts are generated using Whisper [23] ASR, and each dataset contains 5-7 audio clips, ranging from 4-8 seconds in length.\nBaselines. We compare Stable-TTS with two recent diffusion-based TTS models, Grad-StyleSpeech [3] and UnitSpeech [6]. Grad-StyleSpeech is an any-speaker adaptive TTS model, comprising a diffusion and a style-adaptive encoder with style-adaptive layer normalization [15]. UnitSpeech, based on Grad-TTS [12], enables speaker adaptation with untranscribed speech using self-supervised unit representation as a pseudo transcript, along with a unit encoder.\nEvaluation metrics. For objective evaluation, we employ Word Error Rate (WER) to assess the intelligibility of synthesized speech and Speaker Embedding Cosine Similarity (SECS) to measure the speaker similarity, utilizing speaker verification model of Resemblyzer [24]. For subjective evaluation, we employ Mean Opinion Score (MOS) for naturalness and Similarity Mean Opinion Score (SMOS) for speaker similarity, where 20 evaluators rate the synthesized speech on a scale from 1 to 5, measuring each attribute separately.\nImplementation details. We set the audio config to 16khz sampling rate and 80 mel bin. Our diffusion-based zero-shot TTS model, based on Grad-StyleSpeech [3], closely follows its pre-training setup. For the prosody encoder, we use a vector quantization module, using a low band with rich prosodic contents of size 15 from paired mel-spectrogram as input for the prosody vector. As mentioned in Section II-A, three random mel-spectrograms from the same speaker are concatenated into one for the timbre vector input, i.e., $k = 3$ in Equation (1). We randomly choose prior samples from the pre-training dataset, where each sample has a mel-spectrogram length ranging from 100 to 150. Moreover, for the prior prosody prompts in inference, we carefully choose one speech sample for each male and female which are exclusively used in most cases. We adopt HiFi-GAN [25] as a vocoder."}, {"title": "B. Main Results", "content": "We compare the performance of Stable-TTS against baselines across diverse datasets to ascertain its efficacy in enhancing intelligibility, naturalness, and similarity. As shown in TABLE I, Stable-TTS exhibits superior performance in terms of both MOS and SMOS across all datasets. It is worth noting that Stable-TTS significantly reduces the WER of"}, {"title": "C. Ablation Study", "content": "We validate the core strategies of Stable-TTS, namely PLM for prior prosody prompting (Section II-B) and prior-preservation loss (P.P., Section II-C) as well as utilizing prior samples (Section II-C). As shown in TABLE I, we find that removing either component results in a degradation of performance in terms of MOS, SMOS, and WER across all scenarios. Notably, even Stable-TTS without P.P. already outperforms baselines, underscoring the efficacy of prosody prompting with PLM. Removing P.P. maintains SECS but significantly degrades MOS, SMOS, and WER, showing that prior preservation prevents overfitting to the target samples, enabling intelligible and natural speech synthesis. It is worth highlighting that using prior samples on PLM for prosody prompting rather than target samples significantly enhances naturalness, especially for out-of-domain datasets (VCTK, VoxCeleb)."}, {"title": "D. Evaluation under Limited Amounts of Target Samples", "content": "Since our objective is to excel not only in noisy datasets (e.g., VoxCeleb in TABLE I) but also in scenarios with limited amounts of data, we conduct experiments on restricted samples of the target speaker by decreasing the number of samples used during fine-tuning using VCTK dataset. In TABLE III, we prove that Stable-TTS maintains stable WER of less than 1% even with limited target samples. This is in stark contrast to Grad-StyleSpeech, where the performance deteriorates as the number of target samples decreases."}]}, {"title": "IV. CONCLUSION", "content": "In this paper, we have introduced Stable-TTS, a speaker-adaptive TTS framework under limited and noisy speech samples. The core idea of Stable-TTS is to use a small subset of high-quality prior samples from a pre-training dataset. Stable-TTS integrates a prosody language model into our TTS system to conduct prior prosody prompting and incorporates a prior-preservation loss during fine-tuning. Extensive experiments demonstrated the effectiveness of Stable-TTS in terms of naturalness and speaker similarity, even with limited amounts and poor-quality speech samples."}]}