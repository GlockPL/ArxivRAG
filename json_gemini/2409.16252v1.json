{"title": "Fields of The World: A Machine Learning Benchmark Dataset For Global Agricultural Field Boundary Segmentation", "authors": ["Hannah Kerner", "Snehal Chaudhari", "Aninda Ghosh", "Caleb Robinson", "Adeel Ahmad", "Eddie Choi", "Nathan Jacobs", "Chris Holmes", "Matthias Mohr", "Rahul Dodhia", "Juan M. Lavista Ferres", "Jennifer Marcus"], "abstract": "Crop field boundaries are foundational datasets for agricultural monitoring and assessments but are expensive to collect manually. Machine learning (ML) methods for automatically extracting field boundaries from remotely sensed images could help realize the demand for these datasets at a global scale. However, current ML methods for field instance segmentation lack sufficient geographic coverage, accuracy, and generalization capabilities. Further, research on improving ML methods is restricted by the lack of labeled datasets representing the diversity of global agricultural fields. We present Fields of The World (FTW)-a novel ML benchmark dataset for agricultural field instance segmentation spanning 24 countries on four continents (Europe, Africa, Asia, and South America). FTW is an order of magnitude larger than previous datasets with 70462 samples, each containing instance and semantic segmentation masks paired with multi-date, multi-spectral Sentinel-2 satellite images. We provide results from baseline models for the new FTW benchmark, show that models trained on FTW have better zero-shot and fine-tuning performance in held-out countries than models that aren't pre-trained with diverse datasets, and show positive qualitative zero-shot results of FTW models in a real-world scenario - running on Sentinel-2 scenes over Ethiopia.", "sections": [{"title": "1 Introduction", "content": "Crop field boundary datasets are urgently needed in agricultural monitoring, sustainable agriculture, and development applications (Nakalembe and Kerner 2023). However, these datasets do not exist for most of the world. Automatic field delineation in globally available satellite imagery offers a promising solution, but semantic reasoning about globally diverse agricultural landscapes in satellite imagery remains challenging. Field morphologies, agricultural practices, and"}, {"title": "2 Dataset Description", "content": "Annotations\nField boundary representations Field boundary annotations are typically in the form of geo-referenced polygons. Since field boundaries at the same location may change across growing seasons, these polygons should also be temporally referenced to specify when the boundary is valid. Field polygons may be farmer-reported, manually drawn on high-resolution satellite images with GIS software, or recorded by walking the field perimeter with a handheld location-recording device. Polygons can then be paired with satellite data from the same location and time.\nWe conducted a comprehensive search for field polygons from government databases, published literature, and other websites. We looked for datasets with diverse geographic coverage, high-quality and trustworthy polygon annotations, and licenses that permit reuse. We included all datasets meeting these criteria in FTW. We considered author-reported quality assessment in each dataset's documentation, previous use of the dataset in ML analyses, and visual inspection (e.g., closed polygons, polygons consistent with satellite images from reported dates, etc). Table 1 lists the 24 source datasets selected for Fields of The World.\nPresence/absence labels Presence/absence labels are binary labels providing information about both the occurrence and non-occurrence of a phenomenon across sampled locations or time periods-for example, the presence of a field boundary or its absence. Most of the datasets in Fields of The World have presence/absence labels. However, some have presence-only labels, i.e., they are partially labeled. These indicate the presence of some, but not necessarily all, field boundaries in the sampled locations. Some pixels in presence-only label masks have unknown labels that might be labeled as background. Partial labels are a common challenge in field boundary segmentation (Wang, Waldner, and Lobell 2022). The Rwanda example in Figure 1 (row 3) illustrates presence-only labels while the Cambodia example (row 4) illustrates presence/absence labels.\nSemantic filtering We focused Fields of The World on field boundaries for annual crops. Annual crops, also called temporary crops, are planted, grown, and harvested within a single growing season or year. Common examples include wheat, rice, maize, soybeans, and barley. This does not include permanent or perennial crops, which are cultivated for longer than one year and are not replanted annually, such as fruit trees, nut trees, and some grasses. We also excluded parcels used for pasture, fallow land, or other non-crop agricultural activities, such as grazing, orchards, vineyards, and forestry. If a dataset included parcels that were not active annual crops, we filtered them out (details in supplement).\nSample grids Many datasets in Table 1, particularly those sourced from the European Union government websites and EuroCrops (Schneider and K\u00f6rner 2022), have millions of dense annotations spanning the entire country. Including all of these annotations in Fields of The World would bias the dataset toward large European countries. We sub-sampled these datasets by: 1) creating a bounding box enclosing the"}, {"title": "4 Baseline Experiments", "content": "Setup and metrics A common approach to field instance segmentation is binary segmentation followed by polygonization of predicted raster masks (Persello et al. 2023). We follow this approach in our experiments. Unless specified otherwise, we used a U-net with EfficientNet-b3 backbone with inputs consisting of concatenated 4-channel RGB-NIR images from both Window A and B (in that order). We initialized the RGB channels using ImageNet RGB weights and NIR channels using random weights. We optimized cross-entropy loss with class weights inversely proportional to each class's frequency in the training set for each experiment. We trained all models for 100 epochs. We used a fixed random seed for all experiments (randomly chosen). We did not perform any further hyperparameter tuning. Experiments required 4 A100 and 8 V100 GPUs for approximately one week.\nWe evaluate models using semantic (pixel-level) and instance (object-level) segmentation metrics. We provide evaluation functions in the FTW code repository for pixel-level intersection over union (IoU), precision, and recall, and"}, {"title": "5 Discussion and Conclusion", "content": "ML research on automatic extraction of agricultural field boundaries from remotely sensed imagery is currently limited by a lack of ML-ready datasets and benchmarks to train and evaluate models on the global diversity of crop fields. These datasets are urgently needed in many applications for agriculture, climate change, and development. We designed Fields of The World to enable improved performance of ML models for field boundary segmentation in diverse global agricultural landscapes and enable granular country-scale evaluation for more countries than any existing dataset. We performed experiments to establish a performance baseline for the new FTW benchmark and showed that models trained with FTW improve performance over more geographically limited datasets analogous to existing benchmark datasets.\nAn exhaustive modeling search or proposal of novel field instance segmentation methods is beyond the scope of this work. Future work could build on the baselines we established by including more model architectures, including instance segmentation architectures (e.g., Mask-RCNN (He et al. 2017) or SAM (Kirillov et al. 2023)). We also did not"}, {"title": "A Supplementary Information", "content": "Annotation filtering\nAdditional details on semantic filtering As described in the Semantic filtering section, we excluded all classes that were not annual (temporary) crops if they were included in a field boundary dataset. In ftw-semantic-filters.csv (found at https://github.com/fieldsoftheworld/ftw-datasets-list), we list and justify the classes included and excluded in each dataset. We also give the exact dates used to filter each country's satellite images for Window A and Window B. In Figure 6, we visualize the selected and discarded (filtered-out) fields in Luxembourg.\nField boundary datasets not included in FTW We conducted a comprehensive search for field polygons from government databases, published literature, and other websites to use as annotations in FTW. We looked for datasets with diverse geographic coverage, high-quality and trustworthy polygon annotations, and licenses that permit reuse. We included all datasets meeting these criteria in FTW. We considered author-reported quality assessment in each dataset's documentation, previous use of the dataset in ML analyses, and visual inspection (e.g., closed polygons, polygons consistent with satellite images from reported dates, etc).\nThere were a few datasets that did not meet our criteria and thus we decided not to include in FTW:\nZambia: The same source data provider of our Kenya dataset, ECAAS, also published a dataset in Zam- bia. The polygons in this dataset were extremely sparse and did not appear to align with satellite imagery from the same year. The dataset can be ob- tained from https://drive.google.com/drive/folders/ 1nEhHxWzsZxqozO2LZa-uUl6D0KNVYZVZ and metadata from https://ecass-project-documentation. readthedocs.io/en/latest/modules/data_access.html.\nRomania: This documentation of this dataset did not specify the year the field boundaries were valid for. We decided not to use the dataset because we did not know what year of satellite imagery it should be paired with. The dataset can be found at https://github.com/maja601/ EuroCrops/wiki/Romania.\nKenya: Kehs et al. (2021) published a crop type dataset with field boundary polygons in Kenya. However, the pa- per describes limited quality assessment and our visual inspection showed some fields did not align well with with contemporaneous satellite imagery.\nBrazil: The Cadastro Ambiental Rural (CAR) (https://dados.agricultura.gov.br/it/dataset/cadastro- ambiental-rural) provides geo-referenced data for land parcels including agriculture (Jung et al. 2017). How- ever, we were not able to determine the appropriate attributes and attribute values to determine how to filter the parcels for temporary crops. We will try to obtain this information in future work to include in a later version of FTW.\nGloCAB (Brazil, Ukraine, USA, Canada, and Russia): Hall, Argueta, and Giglio (2024) published the Glo- CAB of 190,832 manually-digitized field boundaries in 22 regions of various sizes spanning 5 countries: Brazil, Ukraine, United States of America, Canada, and Russia. While this dataset seems promising for inclusion in FTW, visual inspection revealed many boundaries that did not align with the apparent field extent from the satellite im- agery, particularly around center-pivot irrigated fields in Ukraine (see Figure 7). We will continue to investigate using this dataset in future work and hope to include a filtered version of it in future FTW versions.\nUSA (California): The Kern County Department Of Agriculture And Measurement publishes crop field boundaries annually since 1997. We were not able to in- clude this dataset in FTW because their website does not specify a license for the data that allows for reuse.\nEffect of random spatial splits\nAs described in the Dataset splits section, we perform a blocked random splitting strategy to partition 3 \u00d7 3 groups of patches into training, validation, and test splits for each country in the dataset. Figure 8 shows an example of this splitting strategy for a section of the France dataset. As such, patches in the test sets are adjacent to patches in the train sets, which may allow for leakage between train and test due to spatial autocorrelation in imagery in labels.\nWe tested for this effect by grouping test patches by the number of training patches they are adjacent to, then com- puting model performance for each group (using the entire FTW dataset). If autocorrelation was causing data leakage, then we would expect to observe higher model performance among test patches that are adjacent to training patches com- pared to test patches that are isolated (e.g., in the middle of the 3x3 blocks). We compared the distribution of Pixel IoU per patch using the 2-class (ignore presence-only back- ground) model between the group with no adjacent train- ing patches to those with some adjacent training patches per country with an independent sample t-test. We did not find a statistically significant difference in performance for any country and concluded that spatial autocorrelation is not in- fluencing test set results.\nDataset characteristics\nIn this section, we provide additional dataset visualizations to show the diversity in field morphology between countries and within the Fields of The World dataset.\nFigure 9 shows the distribution of field polygon elonga- tion across four countries. To compute elongation, we first compute a minimum bounding rectangle for each field poly- gon. The elongation is then computed as the ratio of height (i.e., short-side length) to width (i.e., long-side length) of the minimum bounding rectangle, resulting in a value between 0 and 1. This shows, for example, that Austria has many long- narrow fields while the fields in Vietnam and South Africa are typically less elongated."}, {"title": "", "content": "Figure 10 shows distributions of the Convex Hull Devia- tion Ratio for different countries within the FTW dataset. Let f be the area of the field polygon and c be the area of its convex hull. The Convex Hull Deviation Ratio is defined as\n$\\frac{C}{f}$.\nThis ratio is zero when the field is convex and increas- ingly close to one for highly non-convex field polygons. This shows that South Africa has heavy tails for the distri- bution, reflecting the relatively high number of highly non- convex field polygons, especially when compared to Brazil and Vietnam.\nFigure 11 presents a comparative analysis of the ge- ographical coverage of various field boundary datasets. AI4Boundaries and PASTIS/PASTIS-R datasets primarily cover European countries, while AI4SmallFarms focuses on two Asian countries. In contrast, the FTW dataset spans mul- tiple continents, including South America, Europe, Africa, and Asia.\nTable 6 compares the FTW dataset with other field bound- ary datasets across current K\u00f6ppen climate zones of the world (Beck et al. 2018). It shows that the AI4SmallFarms dataset exists only in a single climate zone, the equatorial savannah with dry winter, while the AI4Boundaries dataset spans 9 different climate zones, including two unique zones: Polar tundra and Warm temperate fully humid with a cool summer. The FTW dataset is the most diverse among these, covering 17 different climate zones, including 7 unique zones where no other dataset is present.\nFigure 12 shows the distribution of field orientations across all FTW countries. Most countries exhibit diverse field orientations, while a few, such as Austria, Denmark, and Slovenia, have predominantly north-south orientations, and others, like Luxembourg, Portugal, and South Africa, have predominantly east-west orientations.\nFigure 13 shows the Convex Hull Index distributions for selected countries within the FTW dataset. Let p be the perimeter of the original field polygon, and pc be the perime- ter of its convex hull; the Convex Hull Index is defined as\n$\\frac{P}{P}$.\nThis ratio provides insight into the complexity of a field's boundary, where values close to 1 indicate that field poly- gons are nearly convex, and values significantly less than 1 suggest that the polygons are non-convex with more com- plex boundaries. The figure shows that the field boundaries in South Africa and Estonia are more non-convex/ complex than those in Rwanda and Cambodia.\nPrediction heatmaps\nIn Figure 14 and Figure 15, we visualize the class pre- diction heatmaps for the U-Net with EfficientNet-b3 back- bone trained on the full FTW dataset (FTW-Full) with 3-class masks (ignore presence-only background). We also visual- ize the heatmaps for the same model trained on the subset of European countries (FTW-EU) that are in AI4Boundaries (Austria, Spain, France, Luxembourg, Netherlands, Slove- nia, and Sweden).\nFigures 14 and 15 illustrate these predictions for a sam- ple of 8 countries, representing both Presence/Absence and Presence-only regions (respectively). The heatmaps use three classes for prediction: Red for the Background class, Green for the Field Extent (Interior) class, and Blue for the Boundary class.\nOur results show that the FTW-Full predictions are more aligned with the ground truth for both European and non- European countries. Notably, the FTW-Full model provides more accurate boundary predictions (blue channel) in coun- tries with smaller fields, such as Cambodia, Vietnam, India, and Kenya.\nIn contrast, the FTW-EU model struggles with accurate predictions in Presence-only regions, particularly for the Field Extent and Boundary classes. However, in some cases, such as France, the FTW-EU confidently predicts the Field Extent class, sometimes more accurately aligning with the ground truth than FTW-Full.\nThese visualizations help illustrate how using different datasets for training affects the model's predictions in dif- ferent regions. By comparing the FTW-Full with the FTW- EU model heatmaps, we can see that the FTW-Full heatmaps align better with the ground truth masks across diverse field patterns globally.\nExperiments\nModel architecture experiment results We evaluated two semantic segmentation model architectures, U-Net (Ronneberger, Fischer, and Brox 2015) and DeepLabv3+ (Chen et al. 2018), with five different backbones: ResNet-18, ResNet-50, ResNeXt-50, EfficientNet-b3, and EfficientNet- b4. We report performance in Table 7 for Slovenia, France, and South Africa. U-Nets performed slightly better than DeepLabv3+ models, and U-Nets with EfficientNet back- bones performed best.\nPer-country experiment results In the experiment results in Tables 3 and 4 of the main paper, we reported results for a subset of test countries in FTW (Slovenia, France, and South Africa). We provide the full results of those experiments for all test countries in Tables 8-9 and Tables 10-12 (respec- tively) of the supplement.\nMultiple random seeds The results in Tables 3, 4, and 5 of the main paper were run with one arbitrarily-chosen ran- dom seed. In supplement Tables 13-14, we report the aver- age results across three random seeds for all test countries for the mask type experiment (Table 3 in the main paper). The standard deviation across random seeds for each experi- ment and test country are very small (0 or close to 0 for most metrics)."}]}