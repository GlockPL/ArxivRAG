{"title": "Research on Reliable and Safe Occupancy Grid Prediction in Underground Parking Lots", "authors": ["Jiaqi Luo"], "abstract": "Against the backdrop of advancing science and technology, autonomous vehicle technology has emerged as a focal point of intense scrutiny within the academic community. Nevertheless, the challenge persists in guaranteeing the safety and reliability of this technology when navigating intricate scenarios. While a substantial portion of autonomous driving research is dedicated to testing in open-air environments, such as urban roads and highways, where the myriad variables at play are meticulously examined, enclosed indoor spaces like underground parking lots have, to a significant extent, been overlooked in the scholarly discourse. This discrepancy highlights a gap in understanding the unique challenges these confined settings pose for autonomous navigation systems.\nThis study tackles indoor autonomous driving, particularly in overlooked spaces like underground parking lots. Using CARLA's simulation platform, a realistic parking model is created for data gathering. An occupancy grid network then processes this data to predict vehicle paths and obstacles, enhancing the system's perception in complex indoor environments [1]. Ultimately, this strategy improves safety in autonomous parking operations.\nThe paper meticulously evaluates the model's predictive ca-pabilities, validating its efficacy in the context of underground parking. Our findings confirm that the proposed strategy success-fully enhances autonomous vehicle performance in these complex indoor settings. It equips autonomous systems with improved adaptation to underground lots, reinforcing safety measures and dependability. This work paves the way for future advancements and applications by addressing the research shortfall concerning indoor parking environments, serving as a pivotal reference point.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving has emerged as a crucial area of explo-ration within the automotive realm, with perception systems playing a vital role in guiding vehicles' actions and reactions. These systems act as the eyes of autonomous cars, deciphering the environment to inform navigation decisions and ensure safe maneuvers.\nResearchers integrated Bird's Eye View (BEV) [2] perception into the mix to further enhance the safety and dependabil-ity of autonomous vehicles. This innovative perspective offers a comprehensive overhead visual of the vehicle's surroundings, synthesizing data from diverse sensors. BEV perception en-ables vehicles to meticulously discern roads, vehicles, pedes-trians, and any potential obstructions by presenting a top-down view of the world, thereby significantly upgrading the accuracy and responsiveness of the autonomous driving system.\nThe essence of BEV perception revolves around consoli-dating sensory inputs from various detectors into a single, co-hesive framework-effectively translating them into a bird's-eye-view format. This transformation allows for the fusion and analysis of environmental features under a unified spatial context, forming the basis for sophisticated decision-making processes. In implementing multi-sensor strategies within BEV perception, two principal methodologies prevail:\n1) Multi-modal Fusion Approach\nThis methodology harnesses the synergy of LiDAR and millimeter-wave radar, among other sensors. The system acquires robust and precise data on both stationary and moving elements in the vehicle's vicinity by leveraging the active sensing capabilities of these devices, which emit signals and analyze their reflections. This comprehensive dataset, enriched with depth and positional accuracy, forms a cornerstone for numerous autonomous driving systems, reflecting the industry's widespread endorsement.\n2) Vision-Based Scheme\nConversely, this approach banks on passive visual sensors, primarily cameras, to perceive the environment. Vision-based systems derive understanding solely from ambi-ent light, interpreting visual cues to discern objects and navigate surroundings without emitting any probing sig-nals. This method demands advanced image processing algorithms to compensate for the lack of direct distance measurement, making it a compelling alternative or sup-plementary pathway in autonomous vehicle design.\nTaking Tesla's FSD vision-based system as an example, multiple cameras are placed around the car body. The space around the car is represented as a two-dimensional grid in BEV coordinates. BEV perception fuses features from multiple images into the corresponding two-dimensional BEV grid to provide a global view, and each grid corresponds to a part of the input image. The vision-based scheme is more technically difficult than the \"multi-mode fusion\" scheme that relies on lidar. However, the cost is lower after successful research and development, and the production cost can be greatly reduced. As research progresses, vision-based approaches steadily increase in precision, demonstrating that they can approximate the perception performance of \"multi-modal fusion\" systems while maintaining a significantly lower cost profile [3]. This narrowing gap in effectiveness, coupled with reduced expenses, underscores the growing appeal and practicality of vision-based BEV perception in the autonomous driving landscape, fostering an environment where cost-efficient yet highly capable autonomous systems are increasingly feasible. Therefore, BEV visual perception has set off a wave of research in academia and industry.\nAs one of the core technologies of BEV's visual perception, the occupying grid network is a 3D reconstruction method based on deep learning that enables the understanding of 3D space by dividing it into fixed-size voxels and predicting whether each voxel grid is occupied and which targets cat-"}, {"title": "II. RELATED WORK", "content": "In the realm of Bird's-Eye-View (BEV) perception, vision-based approaches have garnered significant interest within academic circles. They stand out due to their potential to decrease both the financial outlay and intricacy associated with LiDAR-driven 3D perception systems, while also offer-ing enhanced versatility in application. This makes them an intriguing avenue for further examination. Specifically, among vision-based BEV methodologies, two frameworks have risen to prominence: LSS and Transformer networks.\nIn the LSS (Lift, splat, shooting) [7] scheme, the camera projects the real world onto the image plane. It is a process of 3d to 2d, which will lose the depth. The purpose of lift is to restore the depth of each pixel in the image, lifting the image from a 2d plane to a 3d space. Following the lift operation in the LSS scheme, two distinct 3D point clouds are generated:\n1) Cone Point Cloud\nThis cloud precisely locates each point within the au-tonomous vehicle's coordinate framework, effectively map-ping their real-world positions in 3D space.\n2) Context Feature Point Cloud\nComplementarily, this cloud encapsulates the contextual attributes affiliated with every point such as color, texture, or object characteristics, enriching the spatial information with descriptive details.\nThe purpose of splat is to project context features into the BEV grid and construct BEV features. In general, it can be divided into two aspects: Use 2D features to construct depth information and lift 2D features into 3D space. Encode 2D features into 3D space through 3D to 2D projection mapping.\nThe shoot component, unrelated to the perception process, is omitted from this discussion.\nInspired by LSS, more excellent BEV perception algorithms have subsequently appeared, such as BEVDepth [8], which utilizes the point cloud of lidar on the basis of LSS to supervise the predicted depth and make the predicted distance closer to the real value. BEVDet [9] innovatively unified ring view"}, {"title": "III. METHODOLOGY", "content": "This paper will introduce detailed methods for the above two problems.\nA. Data Collection\nTraining SurroundOcc models requires data sets built with the nuScenes structure, upon that, the study configured the same sensors as nuScenes to ensure that the collected data is consistent with the nuScenes data set structure, which can also avoid the impact of training the network when the desired data is not found.\nUsing the CARLA simulator, we can \"install\" several cam-eras, including lidar, IMU, millimeter-wave radar, and other simulation sensors on the vehicle. We also set semantic lidar and other semantic sensors for GT data generation. These simulation sensors have no physical structure and can be placed in any position of the vehicle. According to Figure 4 the sensor configuration of the acquisition vehicle given in the official document of nuScenes, the acquisition vehicle used in this research is equipped with six cameras, five millimeter-wave radars distributed in different directions, and one lidar on the top of the vehicle. In this study, the lidar was replaced by the semantic lidar provided by CARLA to facilitate the subsequent semantic tag extraction.  shows camera parameters, millimeter wave radar parameters, and semantic Lidar parameters, respectively. Figure 5 shows a frame of data of the underground parking lot collected by the camera. This research employs a custom-built model of the underground parking facility at the College of Engineering, Southern University of Science and Technology, serving as the foundational parking lot map for the study.\n1) Basic Configuration: A centralized configuration ap-proach is adopted to comprehensively configure all scenarios in the simulation, wherein all elements\u2014including vehicles, sensors, and map details are consolidated within a single configuration file. Specifically, the config.yaml file acts as an"}, {"title": "IV. EXPERIMENTS", "content": "A. Experimental Environment Settings\n1) Hardware Configuration: Current Occupancy networks training is typically computationally intensive and is typically completed on GPU clusters. The SurroundOcc models used in this study also require a large amount of computing resources for training, the occupancy models used in underground park-ing lots are trained in the Computer Department of Southern University of Science and Technology.  lists the hardware parameters of the GPU cluster.\n2) Model Hyperparameter:  shows the hyperpa-rameter Settings for the model. In this paper, two NVIDIA L40s graphics cards were used for training, with a total of 36 epochs. The initial learning rate was 0.0002 and the learning rate was 0.1 in the backbone part. There are 60 training scenarios, 20 verification scenarios, and a total of 80 training data scenarios.\nB. Evaluation Metrics\nFor 3D semantic occupancy prediction of the underground parking lot, IoU and mIoU are used as evaluation indexes in this paper. For the occupation prediction of the scene, the semantic label of voxel is ignored, and the intersection ratio of the predicted sample and the actual sample (IoU) is used as the evaluation index. The IoU metric measures the degree of overlap between the predicted segmentation results and the actual segmentation results. It can be calculated by the following formula: IoU=(intersection area of the predicted result and the true result)/(union area of the predicted result and the true result). All semantic categories are considered for the semantic occupation prediction of the scene, and mIoU is used as the evaluation index. The mIoU metric is an average calculation of the IoU of multiple samples and is used to evaluate the performance of the entire dataset. The steps to calculate mIoU are as follows:\n1) Calculate the IoU for each sample in the data set.\n2) Add the IoU for all samples.\n3) Divide the sum by the number of samples to get the average IoU.\nIn SurroundOcc, $TP$, $FP$ and $FN$ represent TruePositive (true), FalsePositive (false) and FalseNegative (false negative), respectively. They are indicators used to evaluate the accuracy and completeness of model prediction results in object detec-tion or segmentation tasks.\nTP (TruePositive): refers to the number of samples that the model correctly predicts to be positive. That is, the number of positive cases that the model judges as positive cases. FP (FalsePositive) : refers to the number of samples that a model incorrectly predicts as a positive example. That is, the number of negative cases judged by the model as positive cases. FN (FalseNegative) : refers to the number of samples in which the model incorrectly predicts negative cases. That is, the number of positive cases judged by the model as negative cases.\nThese indicators can be calculated by comparing them with real labels. Suppose there are N samples, where the intersection of the prediction result and the real label is TP, the part of the prediction result that is a positive example but empty set with the real label is FP, and the part of the real label that is a positive example but empty set with the prediction result is FN, then the formula for calculating these indicators is as follows: TP= Intersection of the predicted result with the real label\nFP= the part of the predicted result that is a positive example but the union with the true label is an empty set\nFN= Evaluation index of the final model of the part of the real label that is a positive example but the union with the predicted result is an empty set\nIoU and mIoU are calculated as follows:\n$IoU = \\frac{TP}{TP + FP + FN}$ (5)\n$mIoU = \\frac{1}{\u03a3_{i=1-C}}\u03a3_{i=1}^{C} \\frac{TP_{i}}{TP_{i} + FP_{i} + FN_{i}}$ (6)\nC. Experimental Procedure\nIn order to verify the prediction effect of the model in the parking lot scene, relevant experiments were carried out in this study:\n1) SurroundOcc pretraining models were first used to predict the ineffectiveness of underground parking lots.\n2) SurroundOcc models are trained using collected under-ground parking data to validate the effectiveness of the proposed approach.\nIn this paper, the self-made nuScenes data set of parking lot scenarios is used. There are 80 scenarios in the training data, of which 75% are training sets and 25% are verification sets. The training set contains 60 scenarios with 1200 keyframes. The validation set contains 20 scenarios with 400 keyframes.\nA comprehensive set of densely occupied voxel representa-tions for underground parking lots was acquired by employing SurroundOcc to produce dense occupancy ground truth values, serving as the training dataset for further enhancing the model's capabilities."}, {"title": "V. RESULTS", "content": "A. Predict underground parking using SurroundOcc pretrain-ing models\nObservation of Figure 16 reveals inconsistencies between the predicted outcomes and the actual architectural configura-tion of the underground parking facility. Notably, the model fails to discriminate between structural elements such as walls and columns accurately. Erroneously, vegetation predictions (depicted by the green regions) infiltrate areas designated for walls, which starkly contradicts the typical environment of an underground parking lot. The pre-training model provided by SurroundOcc is completely ineffective in underground parking.\nB. The underground parking lot prediction\nThe model is used to predict the occupancy of the under-ground parking lot after the training. The prediction results are shown in Figure 17. The corresponding visual point cloud data is shown in Figure 18.\nThe model's accuracy was evaluated by calculating the IoU and mIoU values. After calculation, the model's comparison of IoU and mIoU before and after training is shown in. The chart shows that the model's performance has undergone"}, {"title": "VI. DISCUSSION", "content": "Despite its progress, this study identifies several lingering factors that pose challenges and interfere with the experimental outcomes, with a primary focus on generating true value representations. Addressing these issues remains a crucial avenue for future research. Here are the problems:\n1) In the generated dense occupancy ground truth, two pil-lars in an underground parking lot may inadvertently be rendered as a wall. It is postulated that this anomaly arises from coordinate transformation errors during the concatenation of radar point clouds. These inaccuracies lead to subsequent pillar point clouds being appended in front of the initial frame's pillar point cloud, visually consolidating them into what appears as a continuous wall surface.\n2) In the synthesized ground truth, instances of semantic inter-penetration occur among different objects, such as vehicle voxels infiltrating into the ground. This phenomenon is potentially attributed to the nearest neighbor algorithm, which, when assigning semantics to densely packed vox-els, inadvertently allocates the semantics of an object to neighboring voxels that do not genuinely belong to it, thereby causing the object's representation to exceed its actual voxel boundaries.\n3) The confusion of lane markings with wall semantics could stem from discrepancies in the mapping process between the semantic outputs of CARLA's LiDAR scans and the predefined semantics in nuScenes."}, {"title": "VII. CONCLUSIONS", "content": "A. Work Summary\nThis study is dedicated to applying occupancy grid tech-niques within the intricate environment of underground park-ing lots for scene prediction, thereby enhancing the environ-mental perception capabilities of autonomous driving systems and improving driving safety. The specific research contents and contributions can be summarized into four main aspects:\n1) By integrating the CARLA Advanced Driving Simulation platform, a comprehensive dataset of underground parking is systematically collected in this study, which covers multi-dimensional information, such as camera images, radar scanning data, fine object labeling, and precise po-sition and attitude information of vehicles and various sensors. It is normalized according to the industry standard nuscenes data format, which provides high-quality basic materials for subsequent deep-learning models.\n2) Based on precise truth generation strategies from the Sur-roundOcc research team, this study not only replicated this process in depth but optimized the coordinate transforma-tion of the CARLA simulation environment to produce a detailed set of scene occupancy truth values.\n3) Further model optimization [29], [30], [31] and training was carried out in the SurroundOcc framework. This pro-cess involves fine-tuning the pre-trained model on a solid basis, aiming to make it more suitable for the unique spatial structure and dynamic environmental characteristics of un-derground parking lots, so as to improve the model's ability to understand and predict complex parking scenarios.\n4) Finally, using a trained and tuned occupancy network model, this study realizes occupancy prediction for un-derground parking lot scenarios. This result verifies the validity of the proposed method.\nB. Future Work\nThere are still many shortcomings in this study, including the following:\n1) Only one SurroundOcc model was used to predict under-ground parking scenes.\n2) The method of generating the true value of occupancy is not completely accurate enough to obtain a completely accurate true value of occupancy.\n3) No real underground parking lot data was used for verifi-cation.\nIn view of the shortcomings in this study, we will continue to complete in the future work:\n1) Survey more methods of Occupancy network and compare other methods to forecast underground parking lots.\n2) Make improvements on the original method of generating true values to ensure the accuracy of generated true values.\n3) Use real-world parking lot data set to train and verify the model.\n4) Use the predicted results to plan the automatic driving of the vehicle and verify the practical value of the results."}, {"title": "Appendix", "content": "Here we provide the pseudocode of two algorithms."}]}