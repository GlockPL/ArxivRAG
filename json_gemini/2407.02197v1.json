{"title": "Research on Reliable and Safe Occupancy Grid Prediction in Underground Parking Lots", "authors": ["Jiaqi Luo"], "abstract": "Against the backdrop of advancing science and technology, autonomous vehicle technology has emerged as a focal point of intense scrutiny within the academic community. Nevertheless, the challenge persists in guaranteeing the safety and reliability of this technology when navigating intricate scenarios. While a substantial portion of autonomous driving research is dedicated to testing in open-air environments, such as urban roads and highways, where the myriad variables at play are meticulously examined, enclosed indoor spaces like underground parking lots have, to a significant extent, been overlooked in the scholarly discourse. This discrepancy highlights a gap in understanding the unique challenges these confined settings pose for autonomous navigation systems.\nThis study tackles indoor autonomous driving, particularly in overlooked spaces like underground parking lots. Using CARLA's simulation platform, a realistic parking model is created for data gathering. An occupancy grid network then processes this data to predict vehicle paths and obstacles, enhancing the system's perception in complex indoor environments [1]. Ultimately, this strategy improves safety in autonomous parking operations.\nThe paper meticulously evaluates the model's predictive capabilities, validating its efficacy in the context of underground parking. Our findings confirm that the proposed strategy successfully enhances autonomous vehicle performance in these complex indoor settings. It equips autonomous systems with improved adaptation to underground lots, reinforcing safety measures and dependability. This work paves the way for future advancements and applications by addressing the research shortfall concerning indoor parking environments, serving as a pivotal reference point.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving has emerged as a crucial area of exploration within the automotive realm, with perception systems playing a vital role in guiding vehicles' actions and reactions. These systems act as the eyes of autonomous cars, deciphering the environment to inform navigation decisions and ensure safe maneuvers.\nResearchers integrated Bird's Eye View (BEV) [2] perception into the mix to further enhance the safety and dependability of autonomous vehicles. This innovative perspective offers a comprehensive overhead visual of the vehicle's surroundings, synthesizing data from diverse sensors. BEV perception enables vehicles to meticulously discern roads, vehicles, pedestrians, and any potential obstructions by presenting a top-down view of the world, thereby significantly upgrading the accuracy and responsiveness of the autonomous driving system.\nThe essence of BEV perception revolves around consolidating sensory inputs from various detectors into a single, cohesive framework-effectively translating them into a bird's-eye-view format. This transformation allows for the fusion and analysis of environmental features under a unified spatial context, forming the basis for sophisticated decision-making processes. In implementing multi-sensor strategies within BEV perception, two principal methodologies prevail:\n1) Multi-modal Fusion Approach\nThis methodology harnesses the synergy of LiDAR and millimeter-wave radar, among other sensors. The system acquires robust and precise data on both stationary and moving elements in the vehicle's vicinity by leveraging the active sensing capabilities of these devices, which emit signals and analyze their reflections. This comprehensive dataset, enriched with depth and positional accuracy, forms a cornerstone for numerous autonomous driving systems, reflecting the industry's widespread endorsement.\n2) Vision-Based Scheme\nConversely, this approach banks on passive visual sensors, primarily cameras, to perceive the environment. Vision-based systems derive understanding solely from ambient light, interpreting visual cues to discern objects and navigate surroundings without emitting any probing signals. This method demands advanced image processing algorithms to compensate for the lack of direct distance measurement, making it a compelling alternative or supplementary pathway in autonomous vehicle design.\nTaking Tesla's FSD vision-based system as an example, multiple cameras are placed around the car body. The space around the car is represented as a two-dimensional grid in BEV coordinates. BEV perception fuses features from multiple images into the corresponding two-dimensional BEV grid to provide a global view, and each grid corresponds to a part of the input image. The vision-based scheme is more technically difficult than the \"multi-mode fusion\" scheme that relies on lidar. However, the cost is lower after successful research and development, and the production cost can be greatly reduced. As research progresses, vision-based approaches steadily increase in precision, demonstrating that they can approximate the perception performance of \"multi-modal fusion\" systems while maintaining a significantly lower cost profile [3]. This narrowing gap in effectiveness, coupled with reduced expenses, underscores the growing appeal and practicality of vision-based BEV perception in the autonomous driving landscape, fostering an environment where cost-efficient yet highly capable autonomous systems are increasingly feasible. Therefore, BEV visual perception has set off a wave of research in academia and industry.\nAs one of the core technologies of BEV's visual perception, the occupying grid network is a 3D reconstruction method based on deep learning that enables the understanding of 3D space by dividing it into fixed-size voxels and predicting whether each voxel grid is occupied and which targets cat-"}, {"title": "II. RELATED WORK", "content": "In the realm of Bird's-Eye-View (BEV) perception, vision-based approaches have garnered significant interest within academic circles. They stand out due to their potential to decrease both the financial outlay and intricacy associated with LiDAR-driven 3D perception systems, while also offering enhanced versatility in application. This makes them an intriguing avenue for further examination. Specifically, among vision-based BEV methodologies, two frameworks have risen to prominence: LSS and Transformer networks.\nIn the LSS (Lift, splat, shooting) [7] scheme, the camera projects the real world onto the image plane. It is a process of 3d to 2d, which will lose the depth. The purpose of lift is to restore the depth of each pixel in the image, lifting the image from a 2d plane to a 3d space. Following the lift operation in the LSS scheme, two distinct 3D point clouds are generated:\n1) Cone Point Cloud\nThis cloud precisely locates each point within the autonomous vehicle's coordinate framework, effectively mapping their real-world positions in 3D space.\n2) Context Feature Point Cloud\nComplementarily, this cloud encapsulates the contextual attributes affiliated with every point such as color, texture, or object characteristics, enriching the spatial information with descriptive details.\nThe purpose of splat is to project context features into the BEV grid and construct BEV features. In general, it can be divided into two aspects: Use 2D features to construct depth information and lift 2D features into 3D space. Encode 2D features into 3D space through 3D to 2D projection mapping.\nThe shoot component, unrelated to the perception process, is omitted from this discussion.\nInspired by LSS, more excellent BEV perception algorithms have subsequently appeared, such as BEVDepth [8], which utilizes the point cloud of lidar on the basis of LSS to supervise the predicted depth and make the predicted distance closer to the real value. BEVDet [9] innovatively unified ring view"}, {"title": "III. METHODOLOGY", "content": "This paper will introduce detailed methods for the above two problems.\nTraining SurroundOcc models requires data sets built with the nuScenes structure, upon that, the study configured the same sensors as nuScenes to ensure that the collected data is consistent with the nuScenes data set structure, which can also avoid the impact of training the network when the desired data is not found.\nUsing the CARLA simulator, we can \"install\" several cameras, including lidar, IMU, millimeter-wave radar, and other simulation sensors on the vehicle. We also set semantic lidar and other semantic sensors for GT data generation. These simulation sensors have no physical structure and can be"}, {"title": "V. RESULTS", "content": "Observation of Figure 16 reveals inconsistencies between the predicted outcomes and the actual architectural configuration of the underground parking facility. Notably, the model fails to discriminate between structural elements such as walls and columns accurately. Erroneously, vegetation predictions (depicted by the green regions) infiltrate areas designated for walls, which starkly contradicts the typical environment of an underground parking lot. The pre-training model provided by SurroundOcc is completely ineffective in underground parking.\nThe model is used to predict the occupancy of the under-ground parking lot after the training. The prediction results are shown in\nThe model's accuracy was evaluated by calculating the IoU and mIoU values. After calculation, the model's comparison of IoU and mIoU before and after training is shown in."}, {"title": "VI. DISCUSSION", "content": "Despite its progress, this study identifies several lingering factors that pose challenges and interfere with the experimental outcomes, with a primary focus on generating true value representations. Addressing these issues remains a crucial avenue for future research. Here are the problems:\n1) In the generated dense occupancy ground truth, two pillars in an underground parking lot may inadvertently be rendered as a wall. It is postulated that this anomaly arises from coordinate transformation errors during the concatenation of radar point clouds. These inaccuracies lead to subsequent pillar point clouds being appended in front of the initial frame's pillar point cloud, visually consolidating them into what appears as a continuous wall surface.\n2) In the synthesized ground truth, instances of semantic inter-penetration occur among different objects, such as vehicle voxels infiltrating into the ground. This phenomenon is potentially attributed to the nearest neighbor algorithm, which, when assigning semantics to densely packed voxels, inadvertently allocates the semantics of an object to neighboring voxels that do not genuinely belong to it, thereby causing the object's representation to exceed its actual voxel boundaries.\n3) The confusion of lane markings with wall semantics could stem from discrepancies in the mapping process between the semantic outputs of CARLA's LiDAR scans and the predefined semantics in nuScenes."}, {"title": "VII. CONCLUSIONS", "content": "This study is dedicated to applying occupancy grid techniques within the intricate environment of underground parking lots for scene prediction, thereby enhancing the environmental perception capabilities of autonomous driving systems and improving driving safety. The specific research contents and contributions can be summarized into four main aspects:\n1) By integrating the CARLA Advanced Driving Simulation platform, a comprehensive dataset of underground parking is systematically collected in this study, which covers multi-dimensional information, such as camera images, radar scanning data, fine object labeling, and precise position and attitude information of vehicles and various sensors. It is normalized according to the industry standard nuscenes data format, which provides high-quality basic materials for subsequent deep-learning models.\n2) Based on precise truth generation strategies from the SurroundOcc research team, this study not only replicated this process in depth but optimized the coordinate transformation of the CARLA simulation environment to produce a detailed set of scene occupancy truth values.\n3) Further model optimization and training was carried out in the SurroundOcc framework. This process involves fine-tuning the pre-trained model on a solid basis, aiming to make it more suitable for the unique spatial structure and dynamic environmental characteristics of underground parking lots, so as to improve the model's ability to understand and predict complex parking scenarios.\nmodel, this study realizes occupancy prediction for underground parking lot scenarios. This result verifies the validity of the proposed method.\nThere are still many shortcomings in this study, including the following:\n1) Only one SurroundOcc model was used to predict underground parking scenes.\n2) The method of generating the true value of occupancy is not completely accurate enough to obtain a completely accurate true value of occupancy.\n3) No real underground parking lot data was used for verification.\nIn view of the shortcomings in this study, we will continue to complete in the future work:\n1) Survey more methods of Occupancy network and compare other methods to forecast underground parking lots.\n2) Make improvements on the original method of generating true values to ensure the accuracy of generated true values.\n3) Use real-world parking lot data set to train and verify the model.\n4) Use the predicted results to plan the automatic driving of the vehicle and verify the practical value of the results."}]}