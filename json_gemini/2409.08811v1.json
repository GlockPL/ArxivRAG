{"title": "Mutual Theory of Mind in Human-Al Collaboration: An Empirical Study with LLM-driven Al Agents in a Real-time Shared Workspace Task", "authors": ["SHAO ZHANG", "XIHUAI WANG", "WENHAO ZHANG", "YONGSHAN CHEN", "LANDI GAO", "DAKUO WANG", "WEINAN ZHANG", "XINBING WANG", "YING WEN"], "abstract": "Theory of Mind (ToM) significantly impacts human collaboration and communication as a crucial capability to understand others. When AI agents with ToM capability collaborate with humans, Mutual Theory of Mind (MTOM) arises in such human-AI teams (HATs). The MToM process, which involves interactive communication and ToM-based strategy adjustment, affects the team's performance and collaboration process. To explore the MTOM process, we conducted a mixed-design experiment using a large language model-driven AI agent with ToM and communication modules in a real-time shared-workspace task. We find that the agent's ToM capability does not significantly impact team performance but enhances human understanding of the agent and the feeling of being understood. Most participants in our study believe verbal communication increases human burden, and the results show that bidirectional communication leads to lower HAT performance. We discuss the results' implications for designing AI agents that collaborate with humans in real-time shared workspace tasks.", "sections": [{"title": "1 Introduction", "content": "Al agent refers to an AI-driven system or entity that can perceive the environment and make decisions based on the perceived information to achieve specific goals independently [95]. With the advancement of technology, an increasing number of AI agents are being deployed in various scenarios to complete tasks, including medical diagnosis [50, 108, 109, 128], scientific research [1, 41, 72, 107, 127], and industry applications [8, 43, 88]. As large language models continue to evolve, the range of tasks that AI agents can handle is expanding, and they are increasingly being applied in scenarios such as home robots [14, 120], gaming [70, 110], and education [102, 132]. Humans and AI agents collaborate in these scenarios in the same space and can observe each other's actions, which is considered a shared workspace setting [29, 96]. These developments have also led AI agents to transition from independently performing tasks to collaborating with humans and form human-AI teams (HATs), where humans and AI are recognized as unique contributors, role-specific, and working together toward a common goal [86].\nTheory of Mind (ToM) is an important ability of human beings to infer mental states, intentions, emotions, and beliefs of others for the dynamic adjustment of behaviors [92, 93], which has been applied in many AI agent frameworks [60, 118, 129]. When closely collaborating with humans in the shared workspace setting, ToM plays a more critical role in the AI agent framework [47, 75]. Researchers use ToM to help AI agents understand, infer, and predict human behavior, enabling them to dynamically adjust their strategies to achieve better team performance [47, 93]. From a human perspective, humans also build mental models of AI agents through the ToM capability in the human-Al collaboration process [38]. Humans typically expect AI agents to align with their anticipated capabilities or roles [58], and they attribute mental states to AI agents accordingly [94, 98]. When human beings are interacting with an agent with ToM capability, Mutual Theory of Mind (MToM) framework, which refers to a constant process of reasoning and attributing states to each other, is considered the analysis of the collaboration process in some studies [111, 113, 117]. In the study of human cognitive activities, ToM processes are considered closely related to communication [80]. People use their ToM abilities to infer others' mental states and intentions, which helps them decide whether the communication is necessary and what the content and style of that communication should be [37]. This effect is even more pronounced in team collaboration. People determine their plans based on their partner's movements and verbal communication, and they use communication to coordinate with their partners to achieve alignment [59]. The interactivity of communication, which refers to both parties being able to simultaneously send and receive"}, {"title": "2 Related Works", "content": "With continuous technological advancements, AI agents are gradually becoming teammates collaborating with humans in shared workspaces [20, 33]. In shared workspace settings, AI agents can independently take on some traditional human tasks and roles, contributing to the team's success [16, 28, 96]. Due to the human and the AI agent being both recognized as a unique, role-specific contributor working collectively towards a common goal, the human and AI agent form a Human-AI Team (HAT) [86]. This section presents the related works of Theory of Mind and Communication in Human-AI Teams (HATs) to provide the context of these two factors in HATs research and their relationships. In our study, we consider that communication and individual ToM processes mutually influence each other, and the interactivity of communication is part of the MToM process. We further present the current AI technology related to the agent's capabilities in communication and ToM to provide the background of the AI agent that was used in our experiments."}, {"title": "2.1 Theory of Mind in Human-Al Teams", "content": "Theory of Mind refers to an important ability of human beings to infer mental states, intentions, emotions, and beliefs of others for dynamic adjustment of behaviors [10, 11, 92]. Using shared plans and goals as the foundation for collaborative task completion, ToM enables humans to recognize and adjust plans to achieve cooperation [9, 17]. These ToM processes and outcomes impact human collaborative performance and overall team performance.\nThe AI community uses ToM to build AI agents that can infer partners' intention [28, 118], including partner agents and humans [24, 47, 101]. Many studies suggest that the ToM is a crucial factor for AI in achieving dynamic autonomy adjustment [96], which significantly impacts team performance [38]. ToM can help improve understanding of which tasks are suitable for which teammates [91, 103] and enable better task allocation among them [46], leading to more effective coordination. At the same time, researchers are also studying human perceptions and reactions when machines exhibit behaviors that suggest an understanding of human capabilities [113]. From the human perspective, when collaborating with AI, humans with good ToM capability will naturally have a mental model of AI [6, 38, 67]. Based on the mental model, people tend to attribute mental states to AI teammates [75, 91, 98] and expect AI to perform specific roles in interactions that align with their expectations and the mental models of AI teammates [58].\nWhen a human collaborates with an agent with ToM capability, the mutual Theory of Mind (MToM) framework is considered to analyze the collaboration process [112]. MToM refers to a constant process of reasoning and attributing mental states to each other during interaction when humans and AI have ToM capabilities [111, 117]. In past MToM studies, language communication was used as a cue for understanding [111, 112]. However, in shared workspace settings, in addition to verbal communication, there are more actions and task dependencies between individuals, leading individuals to rely on actions and behaviors to understand others' intentions. We still need to fully understand the MToM process in HATs in shared workspace settings, including greater immediate and extensive dependence on actions and tasks between humans and AI agents."}, {"title": "2.2 Communication in Human-Al Teams", "content": "Communication plays a key role in supporting the human-human team collaboration process, including verbal and non-verbal communication [37, 56, 63, 78, 97]. Verbal communication is typically a direct and explicit exchange of information [51, 125], while non-verbal communication is more implicit and requires understanding based on mental states and additional contextual information [3, 79].\nSimilar to human communication [36, 62, 100], communication in HATs is also the primary factors influencing human perception on the AI teammates. Many studies indicate that different factors of communication have impacts on humans' trust on AI agents [61, 99]. Research on the role of communication in HATs has also examined the quantity and frequency of communication [77, 86]. Communication in HATs is typically less than in the human-human team [25]. However, some studies have found that AI agents that disclose more information to humans and provide more explanations via non-verbal communication are not always associated with better decision-making or higher human trust on the Al agent [61]. Communication also influences the social behavior of AI and human preferences [71]. Some studies have explored the verbal and non-verbal communication strategies that humans prefer AI teammates to use in team environments, finding that humans tend to prefer proactive communication from teammates and expect them to respond [125, 126]. Since communication is one of the primary means of information exchange within teams, its interactivity has received significant attention [4, 83]. Interactive verbal communication is typically considered bidirectional [76], meaning that messages can be sent and received simultaneously [30]. Ashktorab et al. [4] reports when Al agent responds to human cues, humans perceive the AI agent as more intelligent. In the presence of bidirectional communication, the communication style can affect the objective task load on humans. Specifically, communication styles that involve feedback can increase the human objective task load [53].\nVerbal and non-verbal communication is also influenced by the Theory of Mind (ToM) process in human-Al collaboration. In HATs, humans and AI agents typically need to understand the status of their teammates [52], as well as what their teammates know and plan [26], to facilitate cooperation. Communication is regarded as an important channel to help HATs exchange information [125], which can be sent in verbal and non-verbal ways. Humans use ToM to infer the mental states and intentions of others, which directly affects whether they choose to communicate, the emotional tone of the communication, the way of communication, and the content of the communication. The amount of communication in HATs is positively correlated with stronger shared mental models, which leads to better team performance [27]. In current HATs research, studies indicates how an agent's ToM abilities can enhance its social attractiveness in communication tasks [12, 75, 89]. In studying the MToM process, the mutual influence between communication and individual ToM processes makes communication an essential factor that cannot be overlooked in the MToM process [111]."}, {"title": "2.3 LLM-driven Theory of Mind Al Agents", "content": "Considering the Theory of Mind's (ToM) importance in human-human interaction and collaboration, the AI community has developed many AI agent frameworks based on ToM to improve AI agents' capability to interact with humans and other agents [60, 118]. In multi-agent system research, researchers primarily focus on how to construct agents capable of collaborating with any unseen partners (including humans) as generalization problems [2, 115]. Considering the problem as the Ad Hoc teamwork scenario [2], researchers use many methods based on ToM to help agents adapt to different partners, including intention sharing [40], classifying types of human teammates [131], using latent variables for inference [74], and employing human behavior data to model humans [16]. Most human modeling approaches assume that human behavior patterns are static and unchanging, leading to limitations in the AI agent's ToM process, as it cannot make real-time inferences about humans [69].\nIn the past, agents built using reinforcement learning (RL) could only achieve team coordination through non-verbal communication [61, 130] or network parameter sharing between agents [114], making it difficult to communicate with humans. Some works have attempted to enable language communication with the RL agents with language model [14, 34, 85], but still facing the grounding problem [55]. This issue also made it challenging to study communication processes in many HATs research using real AI technology [27, 125]. With the emergence of large language models (LLMs), Al agents' communication capabilities have improved significantly. The advent of techniques such as Chain of Thought (CoT) [116] and ReAct [123] has further advanced the capabilities of LLMs in reasoning and decision-making [110]. Autonomous agents driven by LLMs are beginning to emerge, capable of performing real-world tasks, such as gaming [7, 39, 110], online shopping [35, 73, 122], and housekeeping [42, 64]. Many conversational agents have also incorporated ToM into their frameworks to help understand human intentions [7], thus improving the user experience [112, 119]. Since simulations involving multiple LLMs cooperating have confirmed the LLMs' ability to understand human behavior and adjust their actions accordingly [87], LLMs have also been used to adapt human behavior via ToM [7, 104, 106, 119].\nMany studies propose frameworks for LLM-driven agents to facilitate the collaboration between LLM-driven agents and humans. MindAgent [39] constructs an LLM-based agent to achieve multi-agent coordination in Minecraft and cooperation with humans in a kitchen environment. Liu et al. [70] used two LLMs to control the agent's slow-mind and fast-mind thinking, enabling real-time communication and cooperation with humans in a simulated Overcooked game. ToM capabilities enabled by LLMs have also been used for competitive operations in board games, helping AI agents achieve victory in the poker game [129].\nThese technological foundations provide a viable pathway for real AI agents to be involved with communication and ToM capabilities in HAT research. In our study, we build real LLM-driven AI agents based on GPT-40 mini [84] with communication and ToM capabilities to experiment with humans. Our study can serve as a complement to previous research that used Wizard-of-Oz [21, 125] or rule-based agents [96], by exploring the performance and impact of real AI agents."}, {"title": "3 Cooperative Task Setup and Formalization in Shared Workspace", "content": "To better validate the impact of MToM and communication on the cooperation process in HATs, we designed a shared cooperative workspace and tasks based on Overcooked. Overcooked is an important simulation environment used for human-AI collaboration [16, 65, 66, 105, 121, 124] derived from the Overcooked video game\u00b3.\nIn this section, we introduce our environment layout design, the definition of cooperative tasks, the communication system design, and the specific metrics we use to measure the cooperation process. The environment layout design and task design are closely related to MToM and the interactivity of communication. We explain the specific connections between the design and MToM, as well as communication. We also formalize the task process to help understand the design of the agent and our experimental procedure."}, {"title": "3.1 Layout Design", "content": "In our study, we set the shared workspace as a kitchen. Since we focus on the MToM process, especially the communication and individual ToM process in the HAT cooperation process, we arrange the space so that both parties must coordinate their movements to avoid collisions. The layout requires communication and action coordination within the team. We redesign the basic layout Counter Circuit from the original overcooked-ai environment [16], and implement it in the gym-cooking enviroment [121]. The layout features a ring-shaped kitchen with a central counter, elongated counter, and a circular path between the counter and the operational area, which is simulating a real kitchen. In this configuration, pots, cutting board, ingredients (bread, beef, and lettuce), and serving spots are positioned in four distinct directions within the operational area, as shown in Figure 2. Although the layout does not forcibly require cooperation, players may find themselves obstructed by narrow aisles, prompting the need for coordination to maximize performance. At the same time, the circular space creates potential movement patterns (moving clockwise or counterclockwise), which can be used by humans or agents with ToM to infer their teammates' behavior and help facilitate coordination."}, {"title": "3.2 Task Design", "content": "Since we aim to explore the MToM process, we incorporate a mechanism in the burger-cooking task that requires communication for coordination. Ingredients for the burgers overlap, and without explicit indication, it is difficult for individuals to infer which burger their teammate is working on through a single action. Therefore, such a task design requires the players to communicate and infer the teammate's intention on the ongoing task to prevent redundant work and effectively complete orders. As shown in Figure 3, we design three burgers: LettuceBurger, BeefBurger, and BeefLettuceBurger. Humans and agents jointly control the chefs and handle the continuously incoming burger orders in our environment. The task provides three ingredients for cooking burgers: bread, beef, and lettuce. The lettuce needs to be chopped, and the beef needs to be cooked. All ingredients are assembled into a burger on a plate, with no specified assembly order. A LettuceBurger requires bread and chopped lettuce, a BeefBurger requires bread and well-cooked beef, and a BeefLettuceBurger requires bread, chopped lettuce, and well-cooked beef. Each order is only available for a limited time, indicated by a countdown on the interface. To coordinate their teamwork effectively, both the human and the agent need to pay attention to the remaining time of each order.\nIn addition to the cooperation, we introduce additional failure events that will damage performance in the cooperation process. If well-cooked beef is not promptly removed using a plate during cooking, the pan will catch fire. The pan becomes unusable until a team member uses a fire extinguisher to put out the fire. Overcooked beef must be removed using a plate; otherwise, the pan will remain occupied by the overcooked beef, rendering it unusable for further cooking. This process does not directly result in a score penalty (i.e., there is no explicit punishment), but it will impact the team's cooperation process."}, {"title": "3.3 Communication System Design", "content": "We design a communication system for HATs within the task to control the interactivity of communication. In the game, we implement a dialogue module where humans could click buttons to send messages in a dialog box. The buttons in the communication system (as shown in Figure 2) are designed to represent the items needed for the task, including burgers, all ingredients, and plates. The needs for all items are expressed as \u201cWe need (specific item)\u201d to the agent. Additionally, we include options for handling special situations (such as extinguishing fires). Beyond task-related information, there are also two buttons to express emotions. The human participants can see messages sent by the agent in the same dialog box."}, {"title": "3.4 Formulation", "content": "We formulate this scenario as a two-player decentralized Markov decision process (DEC-MDP) [13]. The DEC-MDP containing one agent and one human can be formalized as < S, {A}, {Ah}, p, T,r >, where S is the state space, p : S \u2192 [0, 1] is the distribution of the initial state s0. Ai and Ah are the action spaces of the agent and the human, and A = Ai \u00d7 Ah is the joint action space. T : S \u00d7 A \u00d7 S \u2192 [0, 1] denotes the transition probability and r : S \u00d7 A \u2192 R is the reward function. At time step t, the agent and the human take action $a^i_t$ and $a^h_t$ simultaneously.\nState. Both the agent and the human have full access to the game states and each other's actions. Players can directly see the status of all items in the game interface, such as the location where items are placed and their current state (e.g., beef cooking in a pan). Players can also view the remaining game time and current score through the information displayed. The remaining time for each order, the progress of chopping lettuce, the process of cooking beef, and the process of extinguishing a fire are shown through progress bars. All actions taken by teammates, the teammates' location, and the items they are holding are fully visible to each other. Players can also view messages in real-time, including both the messages they have sent and the messages they have received, along with the corresponding timestamps.\nAction. In this environment, the actions that the human and the agent can take to control the chefs include moving up, down, left, and right, as well as \u201cinteract\u201d. All activities such as picking up items, serving dishes, and extinguishing fires are considered as \u201cinteract\u201d actions. The specific interaction rules are illustrated in Figure 3. We denote the actions to control the chefs as $A^{control}$. The agent and the human share the same $A^{control}$, while the communication actions are designed individually for them, denoted as $A^{comm,i}$ and $A^{comm,h}$ respectively. The communication action space for the agent $A^{comm,i}$ consists of any possible sentences less than 10 words from an LLM, while the communication action space for the human $A^{comm,h}$ consists of 11 message templates. Details about the communication system are presented in Section 3.3 and Section 4.3.\nReward. The scores for completing the three different types of orders vary and serving the wrong burger or missing an order will result in a penalty. The specific rewards are detailed in Table 1."}, {"title": "3.5 Objective Metrics", "content": "To capture the dynamics of the cooperation process and observe the impact of MToM and communication on collaboration, we defined a set of task-related metrics to measure team performance and the team collaboration process.\nTask Score. We define the team's objective performance as the team's score. The game is set to be completed in 500 time-steps, and the specific reward calculation method is shown in Table 1.\nContribution Rate. We first define key task events KE to capture which team member completes specific tasks. Based on the burger-making process, each of the three types of burgers involves"}, {"title": "4 LLM-driven Agent with Theory of Mind and Communication Module", "content": "This section introduces our agent implementation based on GPT-40 mini [84]. As shown in Fig. 4, we design three main modules for the agent, including Theory of Mind, Policy, and Communication. The Theory of Mind module infers the intentions of human partners based on human behavior, summarizing these behaviors to guide further the agent's strategies for cooperating better with humans. The Policy module controls how the agent interacts in our environment and continually"}, {"title": "4.1 Theory of Mind Module", "content": "With a ToM capability, individuals can form hypotheses about the mental states of others as a belief through their actions and communication history, thus understanding and predicting others' behaviors [92]. Belief in Theory of Mind refers to an individual's cognition of things, which further influences their behavior [11, 93, 118]. For example, in our Overcooked environment, if the human player believes that \u201cmy partner will cook the beef,\u201d the human player might focus on other tasks, assuming that their partner will take care of the beef. Based on the Theory of Mind mechanism, we design the Theory of Mind module, which enables the agent to establish the belief about the human, including the tendency, convention, and plan, using the human partner's behavioral history and communication messages."}, {"title": "4.2 Policy Module", "content": "Our task is time-sensitive, requiring the agent and human player to make decisions and adjust their policies in time to avoid missing orders and overcooking the dishes. Due to the real-time"}, {"title": "4.2.1 Code-as-Policy Generator", "content": "Given the latency in API calls with GPT-40 mini and the real-time decision-making requirements of our task, we implement an initial policy with predefined rules. This policy is further empowered by a code-as-policy [68] generator, which plays a crucial role in enabling the agent to effectively handle real-time emergencies. Our initial policy is structured upon a Finite-State Machine (FSM) framework. This framework is instrumental in allowing the agent to"}, {"title": "4.2.2 Policy Reflection", "content": "Equipped with the FSM-based initial policy and Code-as-Policy generator, the agent already has the basic capability to interact with humans in our environment. However, the agent needs to improve its policy in such a long-horizon interaction process for higher performance. Using the low-overhead in-context learning approach, the agent maintains and iteratively updates a \u201cBehavior Guideline,\u201d which summarizes the improvements to current policy. After a \u201cBehavior Guideline\" is generated, the agent incorporates the \u201cBehavior Guideline\u201d into the current policy. The entire process can be formalized as:\n$B^m = LLM (H_{0:t_m}, b^n, B^{m-1}),$\n$\\pi^m = \\pi^{m-1} \\cup B^M,$\nwhere m means the reflection process executes m times, $b^n$ is the latest inferred belief about human, $B^m$ is the \u201cBehavior Guideline\u201d that is updated m times."}, {"title": "4.2.3 Action Executor", "content": "The actions output by the policy module are macro actions. These actions are then executed by an action executor, transforming them into atomic actions that can be executed"}, {"title": "4.3 Communication Module", "content": "Communication is the most direct means for human teams to express their intentions. The message sent within the team is the most flexible content for individuals to adjust. To avoid affecting agent autonomy and to allow dynamic adjustment of communication, we do not require the agent to communicate continuously. Instead, the agent is required to autonomously decide whether communication is necessary and determine the content of the communication. The communication message generation process can be present as:\n$a^{comm, i}_t = LLM (H_{t-n:t}, b^n, B^m),$\nwhere $b^n$ and $B^m$ are the latest belief about humans and the latest \u201cBehavior Guideline\u201d respectively, and n is the interval for the communication process to execute, which is set as 25 in our experiment. The core prompt for generating communication is shown in Figure 12. We implement different communication conditions between groups in the experiment: the communication process will not be executed and will not output any message in scenarios where the agent cannot communicate with the human.\nAdditionally, under conditions where communication is enabled, each message emergence from humans triggers the communication process to ensure that humans' explicit communicative intentions are clearly captured."}, {"title": "4.4 Validation of The Agent Framework", "content": "We conducted a validation experiment to validate the capabilities of the LLM-based agent we designed and to understand the impact of the ToM capability on agent performance. We use an"}, {"title": "5 Methods", "content": "To investigate MToM's effects in HATs, especially the effects of communication interactivity and the agent's ToM capability, we designed a 4x2 mixed-design experiment. The communication interactivity factor includes four levels as between-group variables: bidirectional communication (Bi-Comm), human-only message sending (H-Comm), agent-only message sending (A-Comm), and no communication (No-Comm). We set the agent's ToM ability as a within-subject condition.: agent with ToM (w/ ToM) and agent without ToM (w/o ToM). This within-subject condition directly affects whether MToM exists. When the agent does not have ToM abilities, the MToM process becomes a one-way ToM process, where only the human possesses ToM capabilities."}, {"title": "5.1 Procedure", "content": "We recruited participants from the university via the university's internal social platform. Each participant received 50 RMB for their participation. We gave the participants bonuses based on their performance to motivate them to be more engaged and attentive. We ranked participants in each group based on their self-play performance and performance in two different agent games. The top 25% in each condition within a group received an additional bonus of 5 RMB. The bonus could be accumulated for each condition, allowing for a maximum bonus of 15 RMB. The experiments were conducted online, where each participant completed the experiments on a certain web page using a computer with a keyboard and a mouse, and each experiment took about 20 minutes. Participants controlled the chef using arrow keys and interacted with objects using the spacebar on the keyboard. They could issue messages to the agent by clicking buttons with the mouse. We did not require humans to send messages in the Bi-Comm and H-Comm groups. The participant were free to decide whether to send the message or not. And participants were also free to decide whether to adopt the agent's messages in A-Comm and Bi-Comm groups. We recorded the entire experimental process and provided playback support for data validation.\nSince we used a mixed experimental design, with the between-group condition Communication (includes four levels: Bi-Comm, H-Comm, A-Comm, and No-Comm), participants were randomly assigned to one of the four groups, with 20 participants in each group. Each participant was exposed to two different agents (i.e., the agents with and without a ToM capability) within a group, with three trials conducted for each agent, totaling six trials. The experiment was conducted in both the participants' native languages, Chinese and English. Specifically, to investigate whether humans could perceive the agent's ToM, participants were not informed of the agent's specific capabilities; they were only told that there were two types of agents in the experiment, distinguished by color. All participants first completed an informed consent form and read instructions about the game rules, game operations, and communication methods. After the instructions, participants underwent a non-scored trial to familiarize themselves with the environment, rules, and operations, followed by a scored trial to assist with data validation. In the formal experiment, after each trial, we asked"}, {"title": "5.3 Data Analysis", "content": "Considering individual random differences, we conducted a regression analysis using a mixed-effects linear model [54, 90] to examine fixed effects and control the random effects of individual differences. Additionally, we utilized bootstrapping techniques [81] (Sample Size = 2000) to enhance the robustness of our estimates [31, 32], ensuring greater accuracy and reliability in the presence of non-normality or small sample sizes. We considered the main effects of the two conditions, communication interactivity, agent's ToM capability, and their interaction effects, and applied the Bonferroni correction [82] for analysis.\nFor the open questions, the first author and the second author first thoroughly read the text data from the open questions and conducted thematic analysis [22]. Subsequently, both two authors independently coded the data and then reached a consensus through repeated discussions with all authors."}, {"title": "6 Results", "content": "We analyzed the task metrics, including performance, contribution rate, failure count, and message count, as well as the subjective scale of the participant's perception of their AI teammates. We also obtained qualitative results from subjective human answers to the questions after the experiment."}, {"title": "6.1 Team Performance", "content": "To answer RQ1 (How does the MToM process influence the overall team performance of HATs?), we first analyzed the impact of two independent variables on team performance, using the best performance as the dependent variable. The best performance refers to the maximum value of repeated measurements, which can eliminate the influence of low-level outliers on the results when the learning effect exists and the task is complex. In the Bi-Comm group, the mean score for the"}, {"title": "6.2 Team Collaboration Process", "content": "In addition to the scores, we recorded other metrics during the collaboration process to answer RQ2 (How does the MToM process affect the team collaboration process?).\nWhen MToM was present (agent w/ ToM), fixed effects estimation shows that the contribution rate of the Al agent $CR_A$ increased by 0.02 (p < 0.001, Cohen's d = 0.169) compared to agent w/o ToM condition. Differences in communication interactivity conditions had no significant impact on the AI agent $CR_A$. The median and distribution of the Agent Contribution Rate are illustrated in Figure 13(b). As for the Failure Count, the Bi-Comm group showed a difference compared to the H-Comm group, with an average increase of 0.77 (p < 0.01).\nWe further examined the impact of MToM on the frequency of communication. We found that in both Bi-Comm and H-Comm groups where humans were allowed to send messages, the vast majority of participants sent fewer than one message on average. Only one participant in the H-Comm group sent a large number of messages (> 15 times in a game). All participants rarely used non-item-related messages (\u201cGood Job\u201d and \u201cNeed Improved\u201d) toward the agent. Among all participants, only one person used the \u201cGood Job\u201d message during one game.\nIn our qualitative analysis of the open questions, participants in the group where humans could send messages reported that sending messages increased workload and negatively impacted their task performance. This result aligns with the message count statistics, which show that humans,"}, {"title": "6.3 Human Preference and Perceptions of Al Agents", "content": "For RQ3 (How do humans perceive Al teammates in the MToM process?)", "result": "participants with higher scores may prefer the agent without ToM abilities. While it has a trend in the relationship between team performance and human preference on agent w/ ToM, these results are not statistically significant in the point-biserial correlation coefficient [15", "48": ".", "I feel the agent understands me,": "noticeable difference was observed in each group as shown in Figure 13(c). We conducted a fixed effects test on all results, and participants in each group perceived that the agent with ToM (w/ MToM condition) understood them better (p < 0.001, Cohen's d = 0.336). For the statement \u201cI understand the agent,\u201d there was a significant main effect between w/ MToM and w/o MToM (p < 0.001), but the effect size was small (Cohen's d = 0.177), not reaching the threshold for a small effect. The detailed results of the questionnaires we used in the experiment can be found in Appendix A.\nWe combined the qualitative results from the open questions to understand human perceptions of the agent's ToM capability. Our qualitative"}]}