{"title": "TWO STAGES DOMAIN INVARIANT REPRESENTATION\nLEARNERS SOLVE THE LARGE CO-VARIATE SHIFT IN\nUNSUPERVISED DOMAIN ADAPTATION WITH TWO DI-\nMENSIONAL DATA DOMAINS", "authors": ["Hisashi Oshima", "Tsuyoshi Ishizone", "Tomoyuki Higuchi"], "abstract": "Recent developments in the unsupervised domain adaptation (UDA) enable the\nunsupervised machine learning (ML) prediction for target data, thus this will ac-\ncelerate real world applications with ML models such as image recognition tasks\nin self-driving. Researchers have reported the UDA techniques are not working\nwell under large co-variate shift problems where e.g. supervised source data con-\nsists of handwritten digits data in monotone color and unsupervised target data\ncolored digits data from the street view. Thus there is a need for a method to\nresolve co-variate shift and transfer source labelling rules under this dynamics.\nWe perform two stages domain invariant representation learning to bridge the\ngap between source and target with semantic intermediate data (unsupervised).\nThe proposed method can learn domain invariant features simultaneously between\nsource and intermediate also intermediate and target. Finally this achieves good\ndomain invariant representation between source and target plus task discriminabil-\nity owing to source labels. This induction for the gradient descent search greatly\neases learning convergence in terms of classification performance for target data\neven when large co-variate shift. We also derive a theorem for measuring the gap\nbetween trained models and unsupervised target labelling rules, which is neces-\nsary for the free parameters optimization. Finally we demonstrate that proposing\nmethod is superiority to previous UDA methods using 4 representative ML clas-\nsification datasets including 38 UDA tasks. Our experiment will be a basis for\nchallenging UDA problems with large co-variate shift.", "sections": [{"title": "INTRODUCTION", "content": "These days UDA is attracting attentions from researchers and engineers in ML projects, since it can\nautomatically correct difference in the marginal distributions of the training source data (supervised)\nand test target data (unsupervised) and learn the better model based on labeling rule from source\ndata. Especially domain invariant representation learning methods including the domain adversarial\ntraining of neural networks (DANNs) (Ganin et al., 2017), the correlation alignment for deep domain\nadaptation (Deep CoRALs) (Sun & Saenko, 2016), and the deep adaptation networks (DANs) (Long\net al., 2015) have achieved performance improvements in a variety of ML tasks for instance digits\nimage recognition and the human activity recognition (HAR) with accelerometer and gyroscope\n(Wilson et al., 2020). There are emerging projects in a fairly business-like setting with UDA, and"}, {"title": "PRELIMINARY", "content": ""}, {"title": "UNSUPERVISED DOMAIN ADAPTATION", "content": "Sets of data are comprised of $D_s = \\{(x_i,y_i)\\}_{i=1}^{N_s}$, $D_I = \\{x_i\\}_{i=1}^{N_I}$, $D_T = \\{x_i\\}_{i=1}^{N_{T'}}$ (Source\ndomain, intermediate domain, target domain respectively and we denote $N_s, N_T, N_{T'}$ as the sample\nsizes for source, intermediate and target.), in our setting $D_I$ is e.g. from (UserA, Summer) when\n$D_s$ is from (UserA, Winter) and $D_{T'}$ is from (UserB, Summer). Then let $P_s(y|x), P_s(x)$ denote the\nmarginal and conditional distribution of source, defined for intermediate and target similarly. They\nare in the homogeneous domain adaptation assumption, namely they share same sample space but\ndifferent distribution (Wilson & Cook, 2020). Also this research is in co-variate shift problem, that\nis generally sharing conditional distribution but different marginal distribution of co-variate (Zhao\net al., 2019). The objective is learning $\\hat{F}(\u00b7) = F \\circ C$ to predict ground truth labels for $D_{T'}$, using\nthree sets of data without access to target ground truth labels, $F$ corresponds to feature extractor\nwith arbitrary neural networks parameter $\\Theta_f$ and $C$ task classifier with $\\Theta_c$ to be optimized by gradient\ndescent."}, {"title": "WHAT IS TWO DIMENSIONAL CO-VARIATE SHIFTS", "content": "Let's dive into what is intermediate domain $D_I$, we assume data domain is not one dimensional but\nis two dimensional e.g. (UserA or UserB, Accelerometer Model A and Accelerometer Model B)\nin HAR, (UserA or UserB, Winter or Summer) in occupancy detection problem and (Monotone or\nColor, In the street or not) in image recognition. In this paradigm, $D_s, D_I, D_{T'}$ may be (UserA,\nAccelerometer Model A), (UserB, Accelerometer Model A), (UserB, Accelerometer Model B) for\ninstance. These two dimensional factors are influencing data distribution (we call this as two di-\nmensional co-variate shifts), energy consumption data differs between users and also seasons for\ninstance. Basically under this two dimensional co-variate shifts problem correcting difference be-\ntween domains in UDA is quite difficult but more natural case closer to businesses or real world\nprojects. Additionally we assume gathering unsupervised data $D_I$ is quite easy, so UDA problem\nwith three sets $D_s, D_I, D_{T'}$, is natural and there is demand for solving this problem. Two dimen-\nsional domains assumption is novel in this field, although uses of an intermediate domain to resolve\nlarge co-variate shift are explored in (Lin et al., 2021; Zhang et al., 2019; Oshima et al., 2024) as\nwell. Their experiments showed a synthetic intermediate domain can improve UDA (Lin et al.,\n2021; Zhang et al., 2019), but limited to computer vision tasks technically or experimentally. Our\nproposition is not limited to a specific data type. We investigated whether or not each dataset follows\nthis assumption in the Appendix E."}, {"title": "UDA WHEN TWO DIMENSIONAL CO-VARIATE SHIFTS", "content": ""}, {"title": "LARGE CO-VARIATE SHIFT SOLVER: TWO STAGES DOMAIN INVARIANT LEARNERS", "content": "To begin with previous domain invariant learning abstraction, their objective functions and optimiza-\ntion are below (we call this as Normal). Domain invariant learning is parallel learning including the\nfeature extractor and task classifier's learning labelling rule based on $D_s$ by minimizing $L_{task}$ and\nthe feature extractor's learning domain invariance between $D_s, D_T$, by $L_{domain}$ (measurement of\ndistribution gap between source and target, we elaborate later). The constant value $\\lambda$ is coefficient\nof weight to adjust the balance between task classification performance and domain invariant perfor-\nmance. We define $L_{task} = \\sum_{j=1}^{num-class} -y \\log(\\hat{y})$ as the cross entropy loss with a"}, {"title": "FREE PARAMETERS TUNING", "content": "Free parameters(e.g. learning rate for gradient descent optimizer, layer-wise configurations) selec-\ntion has been regarded as a crucial role because deep learning methods generally are susceptible\nto, additionally UDA require us to do this in a agnostic way for target ground truth labels at all.\nWe propose free parameter tuning method specialized in this two stages domain invariant learners\nby extending RV (same as Normal (Ganin et al., 2017)). RV applies pseudo-labeling to the target\ndata and uses the pseudo-labeled data and the source unsupervised data in an inverse relationship\n(Zhong et al., 2010), authors proved this can measure conditional distribution difference between\ntarget data and learned model. We apply the RV idea to two stages domain invariant representation"}, {"title": "EXPERIMENTAL VALIDATION", "content": ""}, {"title": "SETUP AND DATASETS", "content": "The evaluation follows the hold-out method. The evaluation score is the percentage of correct labels\npredicted by the task classifier of the two stages domain invariant learners for the target data i.e.\n$accuracy(x_{T'}) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}[C(F(x)) = y_{T'}] $ (n as the sample size of target data for testing). The\ntarget data is divided into training data and test data in 50%, the training data is input to the training\nas unsupervised data, and the test data is not input to the training but used only when calculating\nthe evaluation scores. In order to take into account the variations in the evaluation scores caused by\nthe initial values of each layer of deep learning, 10 evaluations are carried out for each evaluation\npattern and the average value is used as the final evaluation score. In addition, hyperparameters\noptimisation with 3.2 is performed on the learning rate using the training data. The all codes we\nused in this paper are available on GitHub 1.\nWe validate our method with 4 datasets and 38 tasks including simulated toy data, image digits\nrecognition, HAR, occupancy detection. We adopt six benchmark models (conventional Train on\nTarget model, Ste-by-step with DANNs or CoRALs, Normal with DANNs or CORALs, conventional\nWithout Adapt model) as a comparison test to ours in Table 14 (Appendix I). If our hypothesis is\ntrue, our method should be close to its Upper bound and better than previous studies and Lower\nbound models therefore ideal state \"Upper bound > Ours > Max(Step-by-step, Normal, Lower\nbound)\" should be expected."}, {"title": "sklearn.datasets.make_moons", "content": "Source data is two interleaving half circles with binary class. Intermediate data is ro-\ntated $\\alpha$ degrees (semi-clockwise from the centre), target data is rotated 2$\\alpha$ degrees ($\\alpha \\in\n\\{15, 20, 25, 30, 35\\}$). These rotations correspond to toy versions of two dimensional co-\nvariate shifts. We set noise=0.1."}, {"title": "MNIST, MNIST-M, SVHN(Lecun et al., 1998; Ganin et al., 2017; Netzer et al., 2011)", "content": "Source data is modified national institute of standards and technology database (MNIST),\n\u00b9please check v1.0.0(release soon) compatible to this paper [Put URL later], also we elaborated experimen-\ntal configurations in the Appendix D"}, {"title": "QUANTITATIVE RESULTS", "content": "We confirmed the Ours' obvious performance advantage compared to Step-by-step and Normal in\nDataset A with DANNs when larger co-variate shift exists i.e. target data with 60 and 70 degrees\nrotated (highlighted in red in left of Figure 2). The difference was 0.174 compared to Normal when\n60 degrees rotated target, 0.074 compared to Step-by-step and 0.091 compared to Normal when 70\ndegrees rotated target, and variance was much smaller. In Dataset A with CoRALs, increasing the\nangle of rotation significantly reduces the evaluation scores of Normal, but Step-by-step and Ours\nwere somewhat able to cope with this (highlighted in red in right).\nThe Figure 3 shows our methods' superiority to previous studies in Dataset A-D(with DANNs),\nD(with CORALs), namely the ideal state \"Upper bound > Ours > Max(Step-by-step, Normal, Lower\nbound)\" was observed in the UDA experiment. In cases Dataset A and C, a clear difference in\naccuracy was identified compared to Step-by-step or Normal, the difference in accuracy was 0.074\nfor Ours and Step-by-step in Dataset C, 0.061 for Ours and Normal, 0.053 for Ours and Normal in\nDataset A. In cases other than the above, the degree of deviation from the ideal state is case-by-case.\nThe superiority to Step-by-step i.e. \"Ours > Step-by-step\" is confirmed 5 out of 8 settings and\nthis demonstrated effectiveness of proposing method i.e. end-to-end domain invariant learning with\nthree sets of data. The superiority to Normal i.e. \"Ours > Normal\" is confirmed 8 out of 8 settings,\nhighlighting the positive impact of two times domain invariant learnings itself.\nCounting the cases where evaluation score is at least higher than the Lower bound, which is impor-\ntant in the UDA setting (\"Ours > Lower bound\"), 8/8 settings. This result means that when UDA\nis performed in business and real-world settings, a better discriminant model can be built than when\nit is not performed, highlighting its practical usefulness. Evaluation patterns and results for each\nDataset, before aggregation, are provided in the Appendix C."}, {"title": "QUALITATIVE RESULTS", "content": "To begin, we investigate how our method performs learning at each epoch in terms of domain in-\nvariance and task classifiability. We simultaneously visualise the learning loss of domain invariance\nper epoch (i.e. $-L_{domain} = CrossEntropy(\u00b7)$), the learning loss of task classifiability and the\nsynchronised evaluation of task classification on test target data. Figure 4 shows that in any case,\ndomain invariance between the source and the intermediate domain and invariance between the in-\ntermediate domain and the target are learnt in an adversarial manner and eventually a solution with\nhigh invariance is reached. Also we found that the task classifiability to the source is simultaneously\noptimised and eventually asymptotically approaches zero or small value. Correspondingly to the\nabove three learnings, the evaluation scores are improving and we can recognise that our proposed\noptimisation algorithm is effective in the point of task classification performance for target data.\nTo get more insights into our method, we investigated neural networks' learned representation at\nboth of feature level and classifier level. The Figure 5 includes feature extractor's learned repre-"}, {"title": "CONCLUSION AND FUTURE RESEARCH DIRECTION", "content": "We proposed novel UDA strategy whose domain invariance and task variant nature could overcome\nUDA problem with two dimensional co-variate shifts. Also our proposing free parameters tuning\nmethod is useful since it can validate UDA model automatically without access to target ground\ntruth labels.\nIn terms of research directions for the method, further improvements in accuracy can be expected\nwhen the method is used in combination with other UDA methods e.g. (Yang et al., 2024; French\net al., 2018; Sun et al., 2022; Yang et al., 2021; Singha et al., 2023). Our method is attractive because\nit is a broad abstraction that encompasses layers of deep learning and domain invariant representation\nlearning methods internally, and can be used in combination with many other methods (not limited\nto specific data type e.g. table data, image, and signal). The hyper-parameter optimisation method of\n(Yang et al., 2024) uses not the conditional distribution gap that can be measured by this method, but\nwith the hopkins statistics (Banerjee & Dave, 2004) and mutual information, transferability at clas-\nsifier level and transferability and discriminability at feature level can be measured in a combined\nmanner. Also (French et al., 2018) is based on that consistency regularisation ensures that the neural\nnetworks' outputs are close each other for stochastic perturbations. It is well-matched with tons\nof image augmentation methods of image processing (Shorten & Khoshgoftaar, 2019) and is likely\nto improve experiments with Dataset B in particular.Safe Self-Refinement for Transformer-based"}, {"title": "APPLIED RESEARCHES' FUTURE RESEARCH DIRECTION", "content": "A promising direction for applied research is to evaluate the method with data and tasks that are in\nhigh demand for other social implementations, and to promote the application of UDA in business\nand the real world. For example, the problem of determining whether a patient with acute hypoxemic\nrespiratory failure died during hospitalisation from medical data (e.g. blood pH and arterial blood\noxygen partial pressure) and the problem of classifying the name of the disease,(Purushotham et al.,\n2017) may result in distribution shifts due to two dimensional data domains between different ages\nand different sexes. Semantic image segmentation in self-driving also may put a need for UDA\nbetween whether conditions and between a.m. or p.m. e.g. (Noon, Sunny)\u2192(Noon, Rainy)\u2192(Night,\nRainy) (Liu et al., 2020)."}, {"title": "JDOT VERSION TWO STAGES DOMAIN INVARIANT LEARNERS AND\nEXPERIMENTAL VALIDATION", "content": "Algorithm and schematic diagram are in Algorithm 4 and Figure 11. In the pseudo code, we denote\noptimal transport solution for sample i and sample j as $OT_{i,j}$ for short. Figure 12 shows two-stages\nJDOT's superiority to previous studies, namely we found that \"Upper bound > Ours > Max(Step-\nby-step, Normal, Lower bound)\" for 4 out of 4 datasets. Figure 13 also says that evaluation for ours\nis always better than Normal and Step-by-step when any rotated target data in Dataset A."}, {"title": "EMPIRICAL STUDY: EFFECTS OF TWO STAGES DOMAIN INVARIANT\nLEARNERS FREE PARAMETER INDICATOR", "content": "We analyzed whether there is a positive correlation between RV-based indicators (left of Theorem\n3.1) and actual losses (1st term, right of Theorem 3.1). Using dataset A, learning rate was varied\nfrom 0.00000001 0.1, and the RV-based score (cross entropy loss) was calculated for the model\nas the result of UDA learning. At the same time, actual losses (cross entropy loss) were calculated\nusing target data for testing. Finally, pearson correlation coefficient was calculated to determine\nif there was a positive correlation between the two scores. Table 13 shows that there is a high\npositive correlation for all patterns. This result supports that two stages domain invariant learners\nfree parameter indicator is useful in UDA experiments."}]}