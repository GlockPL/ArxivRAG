{"title": "Who is Helping Whom? Analyzing Inter-dependencies to Evaluate Cooperation in Human-AI Teaming", "authors": ["Upasana Biswas", "Siddhant Bhambri", "Subbarao Kambhampati"], "abstract": "The long-standing research challenges of Human-AI Teaming(HAT) and Zero-shot Cooperation(ZSC) have been tackled by applying multi-agent reinforcement learning(MARL) to train an agent by optimizing the environment reward function and evaluating their performance through task performance metrics such as task reward. However, such evaluation focuses only on task completion, while being agnostic to 'how' the two agents work with each other. Specifically, we are interested in understanding the cooperation arising within the team when trained agents are paired with humans. To formally address this problem, we propose the concept of interdependence - measuring how much agents rely on each other's actions to achieve the shared goal - as a key metric for evaluating cooperation in human-agent teams. Towards this, we ground this concept through a symbolic formalism and define evaluation metrics that allow us to assess the degree of reliance between the agents' actions. We pair state-of-the-art agents trained through MARL for HAT, with learned human models for the the popular Overcooked domain, and evaluate the team performance for these human-agent teams. Our results demonstrate that trained agents are not able to induce cooperative behavior, reporting very low levels of interdependence across all the teams. We also report that teaming performance of a team is not necessarily correlated with the task reward.", "sections": [{"title": "1. Introduction", "content": "Developing agents that can learn to cooperate and interact with unseen partners, especially humans, remains a well-established challenge in the field of multi-agent reinforcement learning (MARL) (Ajoudani et al., 2018; Lucas & Allen, 2022; Stone et al., 2010; Du et al., 2023). Several approaches have been proposed for learning agents to team with humans in an ad-hoc setting, such as Fictitious coplay (Strouse et al., 2022), Maximum Entropy Population-based Training(MEP) (Zhao et al., 2022b), Hidden-Utility Self-Play(HSP) (Yu et al., 2023), Cooperative Open-ended Learning(COLE) (Li et al., 2024). These methods commonly use task performance metrics for evaluating cooperation such as mean episode rewards over multiple runs (Yu et al., 2023; Strouse et al., 2022; Lou et al., 2023) or the time-steps taken to complete the task in the environment (Sarkar et al., 2023; Zhao et al., 2022a). Evaluating a team using metrics which objectively measure only the task performance obscure critical details about the performance of the individual teammates and the interactions that arise between them, especially in cases where the they can successfully complete the task without necessarily cooperating with each other. Here, we borrow from the distinction introduced in (Zhang et al., 2016). Required cooperation(RC) needs the participation of all the agents to achieve the goal. Non-required cooperation(Non-RC), on the other hand, can be achieved independently by a single agent, without requiring participation of the other agents. Therefore, it is unclear whether agents trained through MARL actually learn how to induce cooperation when paired with an unseen teammate for problems which do not satisfy conditions of RC. This becomes a particularly worrisome problem when these agents are paired with human teammates, where the human may pick the slack of the non-performing AI teammate and still be able to complete the task efficiently.\nFor example, consider the environment layout shown in Figure 1 from the Overcooked Game. The agents act together in the environment to cook and deliver soups by collecting onions, cooking them in a pot, transferring the soup to a dish, and delivering it at the serving station. This setting does not satisfy RC i.e. the participation of both agents is not required to complete the task. Consider the case of a human-agent team trying to achieve the goal. The team could successfully complete the task using a strategy with minimal interactions between the team members, such as one where the human is doing the task by themselves and the agent is merely staying out of the human's way. Here, the task reward provided by the environment will be shared"}, {"title": "2. Related Works", "content": "Previous works in human-agent teaming use task performance or episodic reward (Strouse et al., 2022; Yu et al., 2023; Zhao et al., 2022b; Li et al., 2024; Wang et al., 2024; Lou et al., 2023) to evaluate the team's performance. (Zhao et al., 2022a; Knott et al., 2021; Fontaine et al., 2021) emphasize the significance of designing different metrics for evaluation such as collaborative fluency, robot and human idle time etc. (Zhao et al., 2022a) and subjective user studies to measure trust, engagement and fluency of the agents when paired with a human (Zhao et al., 2022a; Ma et al., 2022; Nalepka et al., 2021). (Johnson et al., 2014) places interdependence at the center of their model for designing human-machine systems, making it the organizing principle around which the rest of the team's structure and behavior revolves. (Johnson et al., 2020) emphasizes that an effective integration of AI into human teams depends the ability of AI agents to collaborate with humans by managing interdependencies."}, {"title": "3. Preliminaries", "content": null}, {"title": "3.1. Two-Player Markov Game", "content": "A two-player Markov game for a human-AI cooperation scenario can be defined as (S, A, T, R) where S is the set of world states, A: A1 \u00d7 A2 where A\u00bf is set of possible actions for agent i, $T : S \\times A_1 \\times A_2 \\rightarrow S$ is the transition function mapping the present state and the joint action of the agents to the next state of the world, $R_i : S \\times A_1 \\times A_2 \\rightarrow R_i$ is the reward function mapping the state of the world and the joint action to the global reward. For a 2-player cooperative markov game, R = R1 = R2 where R is the global environment reward function. The joint policy is defined as \u03c0 = (\u03c01, \u03c02) where the policy $\u03c0_i : S \\rightarrow A_i$ is defined for an agent i over set of possible actions Ai."}, {"title": "3.2. Multi-Agent Planning Problem", "content": "A STRIPS problem is represented as (P, A, I, G) where P is the set of propositions which can be used to denote facts about the world, A is the set of planning actions, I is the initial state and G is the goal state. Each fluent p\u2208 Pis a symbolic, binary variable that describes the current state of the environment, with each proposition representing a specific property. The possible fluents for the Overcooked environment can be counter-empty - describes whether the counter is empty or not, pot-ready - indicates whether the soup is ready in the pot, soup-served - indicates whether the soup has been served at the serving station etc. I denotes the propositions representing the initial state of the world and G denotes the propositions corresponding to the goal state of the world. A planning action can be defined as a =< pre(a), add(a), del(a) > where pre(a) is the set of propositions that must be true before the action can be executed, add(a) are the propositions that become true after the action is performed and del(a) are the propositions that become false after the action is performed. Extending this to multiple agents, a Multi-agent Planning task can be denoted as (P, N, {Ai}N1, I, G) where N is the number of agents and Ai is the set of actions for the agent i. We assume that the agents take turns to act and not in parallel. A plan is defined as a sequence of actions ({a1}N1, {a2}N1,..., {an\u22121}N1) where n is the number of steps in the plan. A plan is a solution II if it is a sequence of actions that can be applied to the initial state I and results in a world state which satisfies G i.e. \u03a0 = ({a1}N1, {a2}N1,..., {an\u22121}N1) is a valid solution plan if {an}1 (......({a2}N1 ({a1}N1(I)))) \u2286 G"}, {"title": "4. Interdependencies", "content": null}, {"title": "4.1. Problem Statement", "content": "We pose the human-agent teaming problem as a two-player Markov game, where the actions of the teammates take place sequentially. We focus on the case where the team is trying to reach a set of goal states SG such that SGC S. The states in SG are absorbing i.e. Vs \u2208 SG and a \u2208 Ai, we have T(s, {ai}N1) = 0. We represent the solution trajectory for a single agent Ti as Ti = (at, at+1,...ak...an) and the joint-action solution trajectory \u03c4 of two agents starting from timestep t and reaching a goal state at timestep n as $\u03c4 = ((a^1_t, a^2_t), (a^1_{t+1}, a^2_{t+1})... (a^1_n, a^2_n))$. An execution trace Tr of a policy \u03c0 from an initial state st as is denoted as (st, at, st+1, at+1, . . . sn), where Tr corresponds to the state-action sequence that starts at timestep t and terminates"}, {"title": "4.2. Mapping the Markov Game to STRIPS", "content": "In a Markov Game, the state at a current timestep st \u2208 S is typically a high-dimensional vector. st can be denoted as a symbolic state with a set of true propositions pt which denotes the current state of the world. Doing this, we effectively describe each state as a finite set of relevant symbolic facts. Therefore, there exists a function F : S \u2192 P mapping the states to symbolic propositions. Here, we refer to Fig. 2. We consider the predicate counter-empty to denote if the middle counter is empty. We consider the transition when the green-hat agent (A2) takes an action to place the onion on the counter. The state at which the agent performs this action has the proposition counter-empty set as True, while the action sets counter-empty as False in the next state. Therefore, mapping the state to a symbolic state helps us capture the effect of the agents' actions in terms of relevant symbols. We can recall from the execution trace Tr of a Markov Game that the state of the world at time t is st. From st, taking action at causes the state of the world to change to st+1. We can map each transition (st, st+1, at) to the symbolic formulation with the help of F. st+1 can be represented as a set of true propositions Pt+1 and st can be represented as pt. Similarly, we now map the action at = (a1, a2) to a symbolic representation. Recall that since the teammates take turns to play, at = (a1, no-op) or at = (no-op, a2). For action a1, there exists a mapping from (st, a1, st+1) to a STRIPS style planning action such that pre(a1)\u2286 pt, add(a1) Pt+1 and del(a1) P\\Pt+1. Therefore, the solution trajectory \u03c4 can be represented as a joint solution plan II, where each single-agent action a1i in the trajectory can be represented as a1i = (pre(a), add(a), del(a)). This way we can track the preconditions and effects of the actions of individual agents in the trajectory as symbolic propositions and track the interdependencies between them."}, {"title": "4.3. Agent Interdependencies", "content": "Given a joint-action solution trajectory \u03c4 and the solution trajectory Ti for an agent i, we define the following properties about \u03c4 and Ti to formalize the concept of interdependence for the solution trajectory:\nDefinition 4.1. For Ti, Independent actions Ii are the set of actions which have no possible interactions with the actions of the other agent. This is defined as: $Ind_i = (a_i | \\forall a_j \\in A_j \\cap j \\neq i, eff(a_i) \\cancel{\\subseteq} pre (a_j) \\cup pre (a_i) \\cancel{\\subseteq} eff(a_j))$ where ai \u2208 Ti. The set of Coordination actions for all agents in a team as: $C_i = T_i - I_i$.\nIn Figure 1, an example of an Independent action is A1 picking an onion from the onion dispenser. The effect of this action is the proposition holding-onion getting set to True for A1. This action cannot interact with any actions of A2, since there is no direct passing of onions between the agents. An example of a Coordination action is A1 putting the third onion into the pot, leading to the soup being ready to be picked up by A2. This action leads to the proposition soup-ready getting set to True, which interacts with the action of A2 if they perform the action of collecting soup from the pot. Referring to Figure 3, the outermost ring shows the distribution of all the actions into Independent and Coordination actions.\nDefinition 4.2. For Ti, the set of Trigger actions is $Tr_i = (a_i | \\forall a_j \\in A_j \\cap j \\neq i, eff(a_i) \\subseteq pre)$ where ai \u2208 Ci. The set of Accept actions is $Ac_i = (a_i | \\forall a_j \\in A_j \\cap j \\neq i, pre (a_i) \\subseteq eff(a_j))$ where ai \u2208 Ci.\nA Trigger action for A1 is placing the onion on the counter, since it could potentially be the precondition for A2 picking that onion from the counter. Similarly, a Accept action for A2 is picking an onion from the counter, since it's precondition onion-on-counter could potentially be satisfied by the A1's action of placing the onion on the counter. Note that the $C_i = Tr_i + Ac_i$ as shown in Figure 3. Therefore, Ci consists of all the actions of agent i which have the possibility of interacting with the other agent's actions, either as Trigger actions or Accept actions.\nDefinition 4.3. For \u03c4, Interdependent actions are pair of actions $(a^i_t, a^j_{k})$ where $add(a^i_t) \\subseteq pre(a^j_k)$.\nDefinition 4.4. An Interdependent pair of actions $(a^i_t, a^j_{k})$ has two agents, a Giver agent performing the action $a^i_t$ and a Receiver agent performing the action $a^j_k$.\nDefinition 4.5. For \u03c4, an agent i has a set of Giver actions which is the set of actions where agent i acts as the giver in an interdependent pair and Receiver actions which is the set of actions where agent i acts as the receiver in an interdependent pair. This can be defined as :\n$G_i = (a \\in T_i || \\exists a_{k+t} \\land add(a^i_k) \\subseteq pre(a^i_{k+t}))$\n$R_i = (a \\in T_i || \\exists a_{k-t} \\land add(a^i_k) \\subseteq pre(a^i_k))$.\nThe set of Interdependent actions $Int_i = G_i + R_i$ as we can see in Figure 3. We define Interdependence as the property of a solution trajectory \u03c4 if there exists an interdependent pair of actions in \u03c4. This is intended to capture the"}, {"title": "4.4. Human-AI Cooperation Problem", "content": "Now, we extend the above definitions to the case where one of the agents is a human user i.e. the human and the agent are working together to finish a task. In such human-agent teams, we propose a quantitative measure of teamwork to judge the team through interdependencies, Rewriting the definitions above for this case, we define Interdependent pair of actions (ak, at)i\u2260j for cases where :\n\u2022 Human is the giver agent i.e. (ak, at\u2212k)i\u2260j such that $add(a^k_t) \\subseteq pre(a^{at-k})$\n\u2022 Human is the receiver agent i.e. (ak, at\u2212k)i\u2260j such that $add(a^{t-k}_t) \\subseteq pre(a^k_t)$\nwhere ah refers to the human user and aa refers to the trained agent. We can also define the Giver actions for the human as $G_H = (a \\in T_H||\\exists a^{a}_{k+t} \\land add(a^h_k) \\subseteq pre(a^{a}_{k+t}))$ and the receiver list $R_H = (a \\in A_a ||\\exists a^{a}_{k-t} \\land add(a^h_k) \\subseteq pre(a^{a}_{k-t}))$, and giver list for the agent as $G_i = (a \\in A_ia^{a}_{k+t} \\land add(a^a_k) \\subseteq pre(a^{h}_{k+t}))$ and the receiver list as $R_i = (a \\in A_i|| \\exists a^{a}_{k-t} \\land add(a^a_k) \\subseteq pre(a^{h}_{k-t}))$. The contribution of each agent is assessed by examining the size and distribution of the human giver list GH and receiver list RH, along with the agent counterparts Gi and Ri. A balanced team performance is indicated when both the human and the agents exhibit a similar number of giver and receiver actions. If GH is significantly larger than RH, it suggests that the human is contributing disproportionately to the task. Conversely, if the agent giver list Gi exceeds the agent receiver list Ri, the agent is actively supporting human task execution, signifying effective human-AI collaboration."}, {"title": "Algorithm 1 Analyzing interdependencies in multi-agent solution trajectory", "content": "1: procedure EVALUATE(T, N)\n2: for i in 1 to N do \u25b7 Initialize all giver and receiver lists\n3: to be empty for all agents\n4: add-list = 0\n5: Gi, Ri = 0\n6: end for\n7: while t < n do \u25b7 While game is not over\n8: at = (a1, . . . a2, ...aN)\n9: for i in 1 to N do\n10: St, St+1 a pre(a), add(a), del(a)\n11: a\n12: add-list = add-list; U pre(a)\n13: forj in 1 to N do\n14: if j\u2260 i and pre(a) \u2208 add-list; then\n15: Ri = Ri U pre(a)\n16: Gj = GjUpre(a)\n17: add-listi = add-list; \\ pre(a)\n18: end if\n19: end for\n20: end for\n21: end while\n22: end procedure"}, {"title": "5. Experiments", "content": "In this section, we evaluate the performance of state-of-the-art methods in the counter circuit layout (Fig.1 from the Overcooked domain when teamed with a learned human model as presented in (Wang et al., 2024). The performance of these teams is assessed using the concept of interdependence, which captures the cooperative interactions between agents. We utilize metrics derived from the trigger list, the giver list, the receiver list of the teammates. We also conduct a comprehensive analysis of the coordination dynamics within the team reflected by these metrics."}, {"title": "5.1. Environment", "content": "The team of 2 players is in a gridworld environment with onion dispensers, dish dispensers, pots, serving stations, and empty counters. The players can either move in the environment or interact with these objects. The objective of the game is to cook and deliver three soups as quickly as possible. To do this, the team must do the following tasks: pick and drop three onions from the onion dispenser, place them in the cooking pot, and wait for the soup to be done. The next steps are to pick a dish from the dish dispenser, transfer the cooked soup to the empty dish, and deliver the soup to the serving station. Each player and each counter can hold only one object at a time. On successful delivery of a soup, both the players receive the task reward. Therefore, both players are incentivised to collaborate to prepare the soup and deliver it as many times as possible. The environment is fully observable and communication is not allowed between agents in the environment. Subtasks : At each step, players can perform either of these eight actions: stay in the same cell, move one cell up, move one cell down, move one cell to the right, move one cell to the left and interact with the object in front. The result of this action depends on the item the player is holding (empty, onion, empty dish, filled dish) and the type of object they are facing(dispenser, pot, empty counter, serving station). Since the environment we are working with has a distinct interact action, we can enumerate all possible outcomes of the the interact action, and use these as our sub-tasks - Pick up onion from onion dispenser, Pick up onion from counter, Pick up dish from dish dispenser, Pick up dish from counter, Place onion in pot, Place onion on counter, Get soup from pot, Place dish on counter, Get soup from pot, Place soup on counter, Serve soup in serving station."}, {"title": "5.2. SOTA Methods", "content": "FCP (Strouse et al., 2022), MEP (Zhao et al., 2022b), HSP (Yu et al., 2023) and COLE (Li et al., 2024) are trained using a two-stage training framework, where a diverse partner population is created through self-play in the first stage, followed by the second stage where the ego agent is iteratively trained by having it play against sampled partners from the population and optimizing mainly the task reward using reinforcement learning. All these methods focus on improving the diversity of the partner population in the first stage. While MEP adds maximum entropy to the reward for increasing the diversity of the population, HSP tries to model the human teammate's reward as event-based rewards to construct a set of behavior-preferring agents. COLE presents cooperative games as graphic-form games and calculates the reward from the cooperative incompab- ility distribution. The ego agent in all these approaches are trained to optimize the episodic task reward, which is also the objective metric being used to measure cooperation when these agents are paired with an unseen teammate (also human). We use the evaluation partners generated in (Wang et al., 2024) as learned models of human behavior (Hproxy)."}, {"title": "6. Results", "content": null}, {"title": "6.1. Task vs Teaming Score", "content": "We compare the task reward for a team, as measured by the time it takes for the team to cook and deliver 3 soups, while the teaming performance is measured by the inter-dependencies occurring within the team. Specifically, we monitor the ratio of Interdependent Actions (Def.4.4) compared to the total number of actions performed by the team, to monitor the proportion of actions in the solution trajectory \u03c4 which involves interactions between the teammates. Additionally, we compare the length of the Giver List and Receiver List (Def.4.5) of the team members to understand the individual contribution of each team member towards task success achieved through successful cooperation. also write about symmetry of the roles. We evaluate the SOTA methods in the domain by pairing it with a itself as well as H-proxy with task and teaming performance averaged over 10 runs, and present it in Table.1. We observe that for all the teams evaluated, the %Interdependent Actions are much lower than expected for a layout where the optimal joint policy involves cooperation and multiple interdependencies arising between the teammates. Also, the contribution of the human teammate is higher as against when the agents are paired with themselves. Our results in also indicate that a quicker soup delivery aka higher task reward does not necessarily correlate with better teamwork. HSP-Hproxy takes a shorter time to deliver soup as compared to COLE-Hproxy. In contrast, COLE-Hproxy outperforms HSP-Hproxy when it comes to teaming performance suggesting strong cooperation occuring in the team. We observe a similar trend where MEP-Hproxy outperforms FCP-Hproxy in terms of efficiency of task completion, but performs similarly when it comes to teaming performance. Comparing the contribution of the SOTA agents and Hproxy to cooperation occurring in the team, we can observe that Hproxy gives more interdepend-"}, {"title": "6.2. Trigger vs Giver List for Agent", "content": "We analyze the proportion of the triggered interdependencies (Def.4.2) that are subsequently accepted by the human (Def.4.5. The acceptance of an event by H-proxy indicates the human's willingness to engage with the agent. However, the agent's trigger list may include multiple attempts to initiate interactions or request help which, if perceived as unnecessary or irrelevant, could lead to negative consequences, such as frustration or disengagement on the part of the human (Endsley, 2023; Zhang et al., 2021). In such scenarios, the human may perceive the agent's actions as interruptive (Zhang et al., 2023; Chen & Barnes, 2014). From Table.2, we observe that the interdependencies triggered by the agent are not accepted by the Hproxy less than 50% of the times. Most of the interdependencies triggered by the agent fail. FCP paired with Hproxy tries to initiate an interdependence most frequently, most of which the Hproxy rejects, lowering the efficiency of task completion as well as lowering the teaming score. In contrast, COLE paired with Hproxy triggers interdependencies equally frequently, but get accepted by Hproxy more often, leading to improved task and teaming performance. Whereas, HSP triggers interdependencies least frequently, out of which only 15% get accepted, leading to a lower contribution to the teaming score."}, {"title": "6.3. Trigger vs Giver List for Human", "content": "We analyze the the proportion of the interdependencies triggered by the Hproxy that are subsequently accepted by the agent. The acceptance of an event by the agent is characterized as a cooperative response, signifying the agent's capacity to understand and engage with the human's proposed action. However, a lack of acceptance or recognition by the agent may indicate its failure to detect or respond to the opportunistic interdependencies initiated by the human. This could impede effective collaboration, as the agent's refusal or inability to act may result in missed opportunities for mutual benefit while working towards the shared goal. Additionally, repeated non-responses from the agent might lead to the human perceiving the agent as either uncooperative or incapable of understanding complex interdependencies. Thus, while the agent's behavior may not be perceived as intrusive in this case, its failure to respond adequately to human-initiated triggers could undermine the efficiency of teaming in this case. We observe in Table.2 that Hproxy triggers interdependencies more often than the SOTA agents, suggesting human's willingness to cooperate with the agent. In particular, Hproxy tries to initiate interdependencies most often when paired with HSP. However, the triggered interdependencies are accepted less than 30%times, similar to the trend in the last section. This suggests significant misalignment and cooperative incompatibility ?? between SOTA agents and Hproxy when they are trying to work together as a team."}, {"title": "6.4. Event Distribution", "content": "In this section, we investigate the distribution of the coordination subtasks for Counter Circuit and Asymmetric Advantages. For each layout, we pick the teams with the highest levels of teamwork (COLE- Hproxy). We aim to focus on the key patterns of interactions that emerge during the teaming process. COLE and Hproxy are highly involved in onion-passing tasks, and there is some coordination being achieved when it comes to passing onions. While both the agents in this layout are symmetrical in terms of the tasks they can reach, the agents converge to the strategy where Hproxy is putting the onion on the counter and COLE is picking up the onion from it. However, an important observation"}, {"title": "7. Conclusion", "content": "Our analysis reveals that, in general, the level of cooperation observed within the evaluated teams is significantly lower than expected, even in scenarios where optimal joint policy should necessitate extensive collaboration. The measured proportion of interdependent actions remains minimal across all SOTA agents when paired with a human proxy, indicating that agents are not effectively leveraging cooperative strategies to achieve task success. A key observation from our experiments is that the human proxy contributes more to cooperation when paired with agents, as compared to when agents are paired with themselves. This suggests that human teammates take on the burden of initiating cooperative behaviors, compensating for the lack of proactive engagement from the agents. Another crucial takeaway is that higher task reward does not necessarily equate to improved teamwork. While some teams demonstrate efficiency in task completion, their teaming performance remains suboptimal. Our results also highlight that a significant portion of interdependencies triggered are not accepted by their teammates, leading to both diminished teaming scores and reduced task performance. This suggests a fundamental issue of misalignment between SOTA agent and humans, where the teammates fail to generate meaningful interdependencies or produce them in a manner that is ineffective or disruptive to their partner. Currently, most layouts in the Overcooked domain lack the sufficient coordination events necessary to adequately test cooperation, highlighting the need for improved layouts with increased potential inter-dependencies to facilitate the development of agents. This decoupling of task success from teaming performance allows us to actually evaluate the cooperation within a team. We conclude that agents trained using MARL for HAT are not inducing cooperative behavior when paired with a human teammate. This paper proposes an objective framework for assessing team performance by presenting a novel formalization for cooperation within a team, which will be a significant contribution to guide the design and development of improved MARL approaches that can achieve robust teaming. We also establish a novel method to judge how well agents collaborate, enabling the development of more effective learning strategies that can induce cooperation better."}]}