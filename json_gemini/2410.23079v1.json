{"title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference", "authors": ["Junqi Zhao", "Zhijin Fang", "Shu Li", "Shaohui Yang", "Shichao He"], "abstract": "Large language models (LLMs) are essential in natural language processing but often struggle with inference speed and computational efficiency, limiting real-time deployment. The key-value (KV) cache mechanism reduces computational overhead in transformer models, but challenges in maintaining contextual understanding remain. In this paper, we propose BUZZ, a novel KV caching algorithm that leverages structured contextual information to minimize cache memory usage while enhancing inference speed. BUZZ employs a beehive-structured sparse cache, incorporating a sliding window to capture recent information and dynamically segmenting historical tokens into chunks to prioritize important tokens in local neighborhoods. We evaluate BUZZ on four real-world datasets: CNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ (1) reduces cache memory usage by 2.5\u00d7 in LLM inference while maintaining over 99% accuracy in long-text summarization, and (2) surpasses state-of-the-art performance in multi-document question answering by 7.69% under the same memory limit, where full cache methods encounter out-of-memory issues. Additionally, BUZZ achieves significant inference speedup with a logn time complexity. The code is available at: https://github.com/JunqiZhao888/buzz-llm.", "sections": [{"title": "1 Introduction", "content": "In the realm of artificial intelligence, the research on Large Language Models [1, 2] has, in recent years, become one of the most compelling areas of focus. With the introduction of the attention mechanism [3], transformer-based network structures have been proven to perform well in various fields, such as personalized content recommendation on streaming platforms, drug discovery through molecular property prediction, and enhancing customer service chatbots for more natural and accurate responses. These structures have been widely adopted in these practical scenarios.\nSince that seminal work, a plethora of pre-trained models has been introduced, each lever-aging extensive linguistic datasets. Prominent among these are Llama [4, 5], GPT [4, 6], Gemini [7], and Mistral [8], models that have demonstrated remarkable proficiency in language processing and generation.\nAs application scenarios continue to evolve and more models wish to tackle longer context window, they impose increasingly stringent demands on computational efficiency, storage requirements, and performance efficacy. In response to the imperative to mitigate compu-tational overhead, the KV cache [9] mechanism has been introduced. During the prefilling"}, {"title": "2 Related Work", "content": "Window methods. In the realms of multi-turn dialogues and long-text processing, re-searchers have proposed various window-based strategies to address the challenges of mem-ory consumption and computational efficiency. There is a local attention mechanism [23] that restricts the model to focus on only a portion of the sequence at any given time, thereby reducing computational load and enhancing processing speed. StreamingLLM [21] maintains model performance by retaining a limited number of initial token key-value pairs and in-tegrating a sliding window mechanism. Longformer [24] employs evenly spaced windowed attention mechanisms, combining local and global attention to handle lengthy texts. LM-Infinite [25] effectively processes long sequences without additional learning by introducing lambda-shaped attention masks and distance constraints. MInference [26] accelerates the prefilling phase by dynamically constructing sparse indices that recognize unique patterns in long contexts.\nKV Cache Eviction methods. These methods enhance the inference efficiency of LLMs by selecting more important tokens and compressing KV cache. The H2O [20] algorithm stands out in this field: based on the discovery that a small subset of tokens, known as \"Heavy Hitters,\" contribute most of the value when calculating attention scores, researchers have achieved effective compression of KV cache by dynamically balancing the most re-cent tokens with these heavy hitter tokens. Scissorhands [27] sequentially predicted the potentially pivotal tokens with the attention score above average within a history window."}, {"title": "3 Methodology", "content": "The goal of this section is to propose the algorithm of our method BUZZ and to show some preliminary and guarantees. We first present the preliminaries of KV Cache Eviction and the motivation of BUZZ. Then we detail BUZZ from an algorithmic perspective.", "subsections": [{"title": "3.1 Preliminary", "content": "We first give a basic preliminary about the inference of LLMs based on transformer [3] layers and the problem formulation of KV Cache Eviction. Here we only consider one head attention block. Let the attention weight be $W_Q \\in \\mathbb{R}^{d \\times d}$, $W_K \\in \\mathbb{R}^{d \\times d}$ and $W_V \\in \\mathbb{R}^{d \\times d}$, and the embedding of prompt or hidden state of last layer be $X \\in \\mathbb{R}^{\\text{seq} \\times d}$, where $d$ represents the hidden dimension of the model and seq represents the length of prompt in prefilling part and 1 in decoding part.\nThe inference of the LLMs follows the autoregressive way. KV Cache is initialied as\n$K = X_{\\text{prompt}}W_K, V = X_{\\text{prompt}}W_V$ (1)\nAt each decoding step $t$, the model generates next token based on KV Cache and update $K$ and $V$ it into $[K, K_t]$ and $[V, V_t]$, where $K_t$ and $V_t$ are computed by\n$Q_t = X_tW_Q, K_t = X_tW_K, V_t = X_tW_V$ (2)\nThen the attention outputs are calculated by\n$A_t = \\text{Softmax}(\\frac{1}{\\sqrt{d}}Q_tK^T), O_t = AV$ (3)\nHence, KV Cache reduces the time complexity of computation of attention into linear level based on length of sequence. However, the GPU memory usage of KV Cache is a new challenge. Here we give the formulaton of KV Cache Eviction.\nAt decoding step $t$, given the known matrices $Q_t, K, V$, our objective is to determine matrices $\\hat{K}, \\hat{V}$ that minimizes the function concerning $K, V$ defined as the difference between two attention matrices:\n$\\min |\\hat{O}(\\hat{K}, \\hat{V}) - O_t|$ (4)\nwhere the new attention output are computed as follows:\n$\\hat{A} = \\text{Softmax}(\\frac{1}{\\sqrt{d}}Q_t\\hat{K}^T), \\hat{O} = \\hat{A}\\hat{V}$ (5)\nThis minimization ensures that the modified attention output $\\hat{O}(\\hat{K}, \\hat{V})$ closely approximates the original one."}, {"title": "3.2 Motivation", "content": "Prior work[21] has shown that deploying a sliding window and ultilizing attention sink tokens which often refer to initial tokens to capture head and tail information can significantly reduce cache memory while maintaining model performance. Additionally, preserving top-k tokens based on attention scores A outside the sliding window which are referred to as heavy hitters has been found to enhance model potential without increasing cache memory usage[20]. However, these methods do not fully leverage the coherence and structure of natural language, as they apply a standard greedy algorithm to select important tokens, often overlooking the holistic importance of past tokens in terms of structure.\nMoreover, previous studies [28] have explored evicting KV Cache and reconstructing an ideal attention score distribution, assuming it should follow a Gumbel distribution, which is a skewed left Gaussian distribution that favors earlier tokens and penalizes later ones. Although this approach introduces a bias that tends to preserve structured information, BUZZ offers a more flexible method by segmenting tokens outside the sink tokens and sliding window to identify local heavy-hitters, thus compressing KV Cache while protecting structured attention distribution and recovering attention distribution without distorting its inherent shape[30]. This segmentation ensures that token generation attends to every part of the preceding text, avoiding excessive focus on localized tokens. Furthermore, prior KV Cache eviction methods usually evict one token immediately after decoding a new one, making the accumulated cost of step-by-step eviction non-negligible. Considering this, at each eviction step, BUZZ removes tokens from each segment and introduces a threshold parameter, allowing the model to buffer KV Cache of tokens and conduct eviction only when the threshold is reached, reducing the additional overhead of the eviction operation and enhancing the efficiency of algorithm.\nOur approach enables the model to utilize the information from most of the tokens from different parts including head, tail, and structured body while still limiting cache memory. And according to the objective in 4, our aim is to achieve a significant increase in compu-tational speed while minimizing the loss of information. This is accomplished by retaining the structured and informational content of $K, V$ in optimized matrices $\\hat{K}, \\hat{V}$ as much as possible via our method, BUZZ."}, {"title": "3.3 BUZZ Algorithm", "content": "Our approach consists of three components that address the head, body, and tail of the cached KV, each employing distinct strategies for pruning before concatenating them to form a updated cache. Before describing our algorithm, we first list the relavant notations which we will explain in detail. As Figure 3 illustrates, the KV Cache of BUZZ has three main features:"}, {"title": "3.4 Parameter Estimation", "content": "In our algorithm, there are numerous parameters, including $k, w, s, T$, and selecting the appropriate ones is a crucial issue. The following theorem provides some guidance on the selection of these parameters before conducting eviction.\nTheorem 3.1 (Parameter Estimation Theorem). Maintaining a constant stride $s$ and cache size $C$, the performance of the LLM is expected to be optimal when the following condition is satisfied($T$ denotes the threshold for eviction, $w$ denotes sliding window size):\n$\\frac{T}{w} =\\begin{cases}\\frac{s^2 + 1}{s+1}, & \\text{if } s \\text{ is odd};\\\\s - 1, & \\text{if } s \\text{ is even}.\\end{cases}$ (6)"}, {"title": "3.5 Augmented Inference with log n", "content": "Many researchers believe that the softmax function in the attention mechanism of all Trans-former models has certain drawbacks, such as forcing each attention head to annotate, even when there is no information to be added to the output vector [21]. To address this issue, we have optimized the function with logn[33] to obtain the BUZZ with logn model. To demonstrate the importance and efficiency of this technique, we first state a proposition:\nIn order to enable the model's results to generalize better to unknown lengths,\nthe design of the Attention mechanism should maintain entropy invariance as\nmuch as possible. We then prove the following lemma:\nLemma 3.1. Let $t_1, t_2,..., t_n$ be a set of data points, and define the probability $p_i$ associated\nwith each data point $t_i$ as\n$p_i = \\frac{e^{\\lambda t_i}}{\\sum_{j=1}^{n} e^{\\lambda t_j}}$\nwhere $\\lambda > 0$ is a hyperparameter. Then the entropy $H(p)$ of the probability distribution\n$p = (p_1, p_2, ..., p_n)$ is minimized when $\\lambda$ is proportional to $\\log n$.\nIn the lemma, $\\lambda$ is actually the coefficient in front of $Q_tK^T$, and $p_i$ represents the components of $\\text{softmax}(Q_tK^T)$. Therefore,\nwe have sufficient reason to add a $\\log n$ coefficient. As for the determination of the base\nnumber, we believe that when the length is the mainstream model's pre-training length\n$(n = 512)$, our optimization formula should degrade to the traditional attention formula.\nTherefore, we take 512 as the base number, and the specific augmented attention formula\nis:\n$\\hat{A}(Q, K, V) = \\text{Softmax}(\\frac{\\log_{512}n}{\\sqrt{d}} Q_tK^T)V$ (7)\nThen we combined it with the KV Cache updated policy and get the following formula:\n$\\hat{A}(Q, K, V) = \\text{Softmax}(\\frac{\\log_{512}n}{\\sqrt{d}} Q_tK^T)V$ (8)\nBUZZ with logn will serve as an auxiliary to our main BUZZ method and augment the performance of inference in various scenarios."}]}, {"title": "4 Evaluation", "content": "The goal of this section is to show the competitive performance of our method BUZZ. We first describe the evaluation settings and datasets. Then we present the testing results in various scenarios in detail. We also do some ablation studies to show the robustness of BUZZ.", "subsections": [{"title": "4.1 Settings", "content": "Setup. Our experiments are based on the famous and representative model family of LLMS, LLaMA2[34] with different model sizes. By default, we all use LLaMA2-7B. We use a total of 4 datasets which focus on long-context tasks, including CNN Daily [35], XSUM [36], Wikitext [37], and 10-document-QA [38]. We followed official HuggingFace/GitHub guidelines for data preprocessing and loading.\nBaselines. The baselines we compare BUZZ with include H2O which use greedy accumu-lated attention scores to evict KV Cache of non-important tokens, StreamingLLM which use recomputation of position embeddings based on sink tokens and sliding window, and local sliding window method. The upper bound of these methods is dense attention with full KV Cache.\nEvaluation Metrics To comprehensively assess BUZZ's performance on long-text tasks, we evaluated it across different metrics, such as ROUGE-1, ROUGE-2, ROUGE-L, perplexity, Self-BLEU, and EM accuracy. For each task, we conducted extensive hyperparameter tuning and applied theoretical approximations to post-eviction cache size, thereby reducing the number of hyperparameters.\nImplementations Details. BUZZ is implemented on major open-source transformer frameworks by modifying their inference forward function. Our experiments were selec-tively conducted on NVIDIA A100 (40GB GPU RAM) and NVIDIA L4 GPUs (22.5GB GPU RAM), depending on the required query context length of each task."}, {"title": "4.2 Experiments", "content": null, "subsections": [{"title": "4.2.1 Summarization", "content": "Task description. Summarization tasks are crucial for evaluating a language model's ability to distill long articles into concise, coherent summaries while preserving essential information. To assess BUZZ's effectiveness in handling long-context scenarios, we employed"}, {"title": "4.2.2 Context-based Q&A", "content": "Task description. Context-based QA is a common scenario in online streaming, where context may refer to user-provided documents or multiple chunks of conversation history. Therefore, a model's ability to extract relevant information from any part of the context is critical. In our evaluation, we used the multi-documents QA dataset [38], where each task includes 10 contextual documents of similar size and a question, with the correct answer located in a controlled position within one of the documents.\nTesting details. We set the cache size threshold to 300, approximately 25% of the average input token count. The full cache method encountered OOM issues on NVIDIA L4 on the first question and was therefore excluded from this comparison.\nResults. Figure 6 compares the performance of different methods on three types of multi-document QA datasets, where the reference answers are located at position 0, 4, and 9. We also computed the average EM accuracy to assess overall performance on long-sequence question answering. BUZZ significantly outperformed other methods in terms of average EM accuracy, particularly in position 4 QA datasets, and achieved relatively high scores in both position 0 and position 9 datasets. This superior performance indicates BUZZ's consistency and applicability in real-life QA scenarios, where target answers may be located at the beginning, middle, or end of the provided context.\nAnalysis. As mentioned above, BUZZ outperforms other methods, especially in position 4 QA datasets. This result further confirms the importance of structured information, which BUZZ utilizes, while H2O employs a special mechanism to capture important tokens outside its window module, the high-variance loss suggests that the window plays a larger role in retaining information than keeping top-k heavy hitters in practical usage. StreamingLLM, which focuses mechanically on the head and tail by discarding middle information, performs as expected according to its approach.\nAlso, BUZZ's low-variance loss ensures that as the number of documents or the length of the context increases, the model's ability to capture key information scales effectively. Additionally, we anticipate that traditional heavy-hitter methods will see a significant drop in accuracy when dealing with 20 or more documents. As the reference answer becomes sparser in the context, this method could become increasingly unpredictable, often capturing only narrow, localized information."}, {"title": "4.2.3 Diversity and Perplexity", "content": "Diversity. The Self-BLEU metric quantifies the degree of similarity among sentences gen-erated by a model, commonly utilized to assess the diversity in language generation tasks, particularly for LLMs."}]}]}, {"title": "4.3 Ablation Study", "content": "The sampling module of BUZZ is of paramount importance. When compared against the geometric mean ROUGE score, StreamingLLM without the sampling module exhibited an average decrease of 17% relative to BUZZ under various KV Cache budgets. Conversely, the H2O model, which employs a top-k module based on accumulated attention scores instead of the sampling module, demonstrated an average reduction of 4%.\nIn order to further investigate the mechanisms of BUZZ, we conduct an analysis of the mechanisms underlying the stride and the local max function.\nInfluence of small stride for old tokens. We swapped the value of small stride \u015d and pre-defined stride s, measured the corresponding composite ROUGE scores for the new parameter set across 10%-60% of the KV Cache budget, and compared them with the old parameters. The results are depicted in Figure 8(a). It is obvious that as the KV Cache budget decreases, the performance of BUZZ with swapped strides drops sharply. Theoretically, the main purpose of introducing the separate sampling size is to recover the original distribution of the attention scores. Giving new tokens a smaller stride would cause the system to favor retaining new tokens and quickly discarding old ones, thereby disrupting the distribution.", "subsections": []}, {"title": "5 Conclusion", "content": "The KV Cache reduction problem in long context processing and multi-turn dialogues has long been a focal point of research in LLMs. Our study reveals the limitations of window methods in accessing information from extensive contexts and the inefficacy of previous sparse method in accurately capturing overall contextual structure. In response, this paper introduces BUZZ, a KV cache method that reconstructs sequence attention distributions and utilizes a locally greedy selection of important tokens, all maintained within logn time complexity. By replacing global search with local search, BUZZ significantly reduces complexity while preserving the original distribution of attention scores as much as possible. Extensive evaluations demonstrate BUZZ's superior performance across various tasks. For example, in long-article summarization, BUZZ achieves a 2.5\u00d7 reduction in cache size while exceeding over 99% accuracy. In multi-document question answering, BUZZ outperforms state-of-the-art methods by 7.69%. These results substantiate the practicality of our novel approach to sub-optimally selecting important tokens for KV caching."}, {"title": "6 Acknowledgement", "content": "We would like to thank Professor David Woodruff for sparking new ideas of inference op-timization and Professor Beidi Chen for connecting us with first authors. We also thank Zhenyu Zhang for helping with reproducing H2O experiment results."}]}