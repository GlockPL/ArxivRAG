{"title": "Implicit Language Models are RNNs: Balancing Parallelization and Expressivity", "authors": ["Mark Sch\u00f6ne", "Babak Rahmani", "Heiner Kremer", "Fabian Falck", "Hitesh Ballani", "Jannes Gladrow"], "abstract": "State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens-representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks.", "sections": [{"title": "1. Introduction", "content": "Transformers, despite their dominance on contemporary language benchmarks, exhibit fundamental limitations in computational expressiveness. Both theoretically and empirically, they cannot fully recognize regular languages (Bhattamishra et al., 2020) or, equivalently, represent finite state machines (FSMs) (Merrill et al., 2022). This limitation is significant because FSMs form the backbone of many real-world state-tracking problems, including evaluating code, tracking object permutations (e.g., in games like chess or structured narratives), and modeling sequential dependencies in logic (Li et al., 2021), location tracking (Guan et al., 2023), games (Li et al., 2023) and scientific applications such as protein generation, genetics, and chemistry (Briand et al., 2023; Chowdhury et al., 2022; Boiko et al., 2023). This raises questions about the ability of transformers to maintain coherent world models based on transitions between states (Vafa et al., 2024) and hence, their suitability for tasks requiring robust state-tracking. These shortcomings appear to stem from a fundamental trade-off between parallelizability at training time and the ability to track state (Merrill & Sabharwal, 2023).\nSurprisingly, recently emerging state-space models (SSM), a class of linear recurrent neural networks, are bound by the same trade-off: despite their seemingly sequential nature they cannot express some inherently sequential problems such as certain regular languages (Merrill et al., 2024). In contrast, non-linear recurrent neural networks (RNNs) are not bound by these restrictions on compute complexity and can track state (Siegelmann & Sontag, 1992; Merrill, 2019) but lack parallelizability at scale. This raises the question: How much sequential processing does one have to accept to solve the state tracking problem?\nPrevious attempts to address these limitations in transformers have leveraged non-linear transitions through self-iteration in the depth dimension (Dehghani et al., 2019; Banino et al., 2021). However, backpropagation through unrolled networks is computationally prohibitive at scale. Deep equilibrium (DEQ) models (Bai et al., 2019), in contrast, define a function implicitly via the fixed-points of a neural network; their output is the result of self-iteration until convergence. Training such networks requires back-propagation solely at the fixed point, eliminating the need to traverse the iterative path and thereby decoupling memory usage from the depth of iterations. Emerging hardware promising rapid computation of fixed-points of neural networks (Brunner et al., 2025) may tilt the hardware lottery (Hooker, 2020) in favor of such implicit models, making this an opportune moment to explore their potential.\nOur approach to balancing state tracking and parallelization relies on two key observations. First, we demonstrate that implicit models naturally adapt their compute load to the difficulty of the learning problem (see Figure 3Left). At both training and test time, such models effectively interpolate between their parallelizable form, when all tokens in the sequence are resolvable, and RNNs, when there are no resolvable tokens. Further, we show theoretically that implicit models have indeed non-linear token-to-token transitions similar to RNNs. Second, based on the success of transformers on many practical language modeling problems, we hypothesize that natural language contains only a sparse set of tokens that cannot be resolved by transformers (and SSMs). Such non-solvable transitions are critical for state tracking but remain intractable for the class of circuits representable by transformers and SSMs (Merrill et al., 2022; 2024). Exploiting these properties, we devise implicit models that combine the expressive power of RNNS with the parallelizability of transformers and SSMs (see Figure 2). In contrast to conventional transformers and SSMs, implicit models can track state, even out-of-distribution (see Figure 1Right). In contrast to RNNs, these models permit a much larger degree of parallelization as the depth of self-iteration is much smaller than the sequence length (see Figure 3Mid).\nContributions. (a) We propose implicit SSMs and show theoretically that they represent non-linear and non-diagonal state-to-state transitions similar to RNNs. (b) We confirm empirically that implicit SSMs can solve the S5 word problem, which conventional SSMs and transformers fail to solve. (c) We show by constructing distributions with varying difficulty level over the word problem that implicit SSMs as well as transformers require much fewer non-parallelizable transitions to learn word problems than RNNs (d) We demonstrate scalability of implicit models through a carefully chosen training curriculum that bounds the number of iterations, training implicit SSM and transformers up to 1.3B parameters on 207B tokens of the deduplicated PILE (D-PILE) (Gao et al., 2020)\u2014 see Figure 1 Bottom, the largest self-iterated model with dynamic halting condition to date, to the best of our knowledge. (e) We highlight a set of properties of our pretrained implicit language models such as favorable length generalization, and path-independent auto-regressive generation."}, {"title": "2. Background", "content": null}, {"title": "2.1. State-Space Models", "content": "SSMs are linear recurrent models which produce an output $Y_t \\in \\mathbb{R}^{d_{out}}$ given an input $x_t \\in \\mathbb{R}^{d_{in}}$ and a sequentially updated hidden state vector $h_t \\in \\mathbb{R}^{n}$ via the recurrence\n$h_t = A(z_t, x_t)h_{t-1} + u(z_t, x_t)$  (1)\n$Y_t = f(h_{t-1}, x_t),$ (2)\nwhere $u$ and $f$ are possibly non-linear learned functions. The learned matrix $A \\in \\mathbb{R}^{n \\times n}$ is typically diagonal and can be constant (Gu et al., 2022; Smith et al., 2023) or an input-dependent matrix-valued function (Qin et al., 2023; Gu & Dao, 2023; Dao & Gu, 2024). A SSM combines a number of these blocks with non-linear feed-forward blocks. In contrast to non-linear RNNs, the linear state recurrence (1) allows for training parallelism along the sequence dimension, and avoids the quadratic scaling of self-attention."}, {"title": "2.2. Limitations of Transformers and SSMs", "content": "Efficient parallelization is one of the central features enabling transformers and SSMs to scale to large machine learning problems such as language modeling. Parallel circuits, however, face fundamental trade-offs regarding the class of problems that they can address. In particular, transformers and SSMs theoretically fail to recognize certain regular languages, or equivalently, to simulate FSMs (Merrill et al., 2022; 2024). Empirical studies have confirmed that neither of the models are capable of learning the algorithms constituting certain regular languages (Bhattamishra et al., 2020; Sarrof et al., 2024). By contrast, the sequential nature of RNNs allows them to express all regular languages (Merrill, 2019). A detailed discussion is given in Appendix A.1."}, {"title": "2.3. Deep Equilibrium Models", "content": "Most deep learning architectures explicitly parametrize a function $x \\rightarrow y$ with a neural network. Deep Equilibrium Models (DEQ), in contrast, define a function implicitly via"}, {"title": "3. Implicit Sequence Models", "content": null}, {"title": "3.1. Implicit State-space Models", "content": "The linear recurrence of SSMs shown in equation (1) cannot resolve elaborate sequential problems (Merrill et al., 2024). Here, we propose to exploit self-iterations along the depth of neural networks to close the expressivity gap between SSMs and RNNs. Following the DEQ paradigm (see Section 2.3), we implicitly define a model via the fixed points of a SSM. Introducing the iteration variable $z_t^{(s)} \\in \\mathbb{R}^{d_{out}}$ to equations (1) and (2) yields the fixed point iteration\n$h_t^{(s)} = A\\left(z_t^{(s-1)}, x_t\\right)h_{t-1} + u\\left(z_t^{(s-1)}, x_t\\right)$ (6)\n$z_t^{(s)} = f\\left(z_t^{(s-1)}, h_t^{(s)}, x_t\\right),$ (7)\nwhere $z_t^{(0)} = 0$ for $t = 0,...,T$ and $h_t^{(0)} = 0$ for $s = 0, . . ., S$ respectively. The output $z_t^{(s)}$ of $f_e$ is fed back to the self-iteration until (approximate) convergence to a fixed-point. Note, how this adds a new dependency to the functions $A$ and $u$ in equation (6) that is not present in equation (1). Notably, this minor technical change leads to fundamental differences between explicit SSMs and the implicit SSM defined above.\nComputing the output, as well as the gradient, of our implicit SSM requires to iterate the two loops defining equations (6) and (7): A loop $t = 1,..., T$ over the sequence dimension, and a loop $s = 0, ..., S$ to find the fixed point. The two loops give rise to two modes of evaluation visualized in Figure 2. The simultaneous mode simultaneously finds the fixed points for all $t$ (see Figure 2A), and exploits parallelization strategies for SSMs (Dao & Gu, 2024). The sequential mode resolves the $s$ and $t$ loops in the transpose order, and processes sequences sequentially just like classical SSMs or RNNs (see Figure 2B). While the simultaneous mode allows for highly parallel training, the sequential mode enables efficient inference at constant memory, e.g. for language generation. For both modes, equation (6) in the limit $s \\rightarrow \\infty$ reads\n$h_t = A(z_t, x_t)h_{t-1} + u(z_t, x_t),$ (8)\nwhere $z_t = \\lim_{s\\rightarrow \\infty} z_t^{(s)}$ and $h_t = \\lim_{s\\rightarrow \\infty} h_t^{(s)}$ denote the fixed points. The fixed point $z_t$ depends on $h_t$, and hence by equation (7) on $h_{t-1}$. Notably, our self-iteration introduces a non-linear dependency to the originally linear recurrence (1) via the functions $A$ and $u$. Thereby, our implicit SSM inherits one of the crucial properties of RNNs, as formalized next."}, {"title": "Theorem 1.", "content": "Consider an implicit SSM defined by equations (6) and (7). Then the transition function $h_{t-1} \\rightarrow h_t$ defined by equation (8) is non-linear and non-diagonal, i.e. each hidden state $h_t$ is a non-linear function of the previous hidden state $h_{t-1}$. Consequently, the state-to-state Jacobian is a non-diagonal operator."}, {"title": "4. Implicit SSMs Adapt to Hard Languages", "content": "Implicit SSMs Lift the Illusion of State The Illusion of State (Merrill et al., 2024) reveals that SSMs cannot simulate arbitrary finite state machines. A hard state tracking problem in the sense that all state tracking problems can be reduced to it is given by the word problem for the symmetric group $S_5$ (Barrington, 1989). The word problem for a monoid $(M, \\circ)$ is to resolve arbitrary length products of the form $m = m_1 \\circ m_2 \\circ \\cdot \\cdot \\cdot \\circ m_k$ for $m_1, m_2, ..., m_k \\in M, k \\in \\mathbb{N}$. A comprehensive introduction to the word problem and our particular learning setting is provided in Appendix D.1.\nWe train a set of Mamba2 SSMs (Dao & Gu, 2024) to reproduce the results of Merrill et al. (2024). Figure 1Left highlights that Mamba2 requires more layers as the sequences get longer. For example resolving sequences of 32 elements from S5 requires a minimum of 16 layers. Extending the result of Merrill et al. (2024), Figure 1Right shows that the same Mamba2 model with 16 layers does not generalize beyond the training distribution when evaluated on sequences longer than 32 elements. Our implicit Mamba2, however, can utilize additional self-iterations at test-time to resolve longer sequences of up to 128 elements. This result establishes that implicit SSMs effectively learn to be RNNs. However, with naive unrolling in implicit SSMs, parallelization would still be challenging. In the following, we show a subtle yet important result: Implicit SSMs can adapt to word problems of varying difficulty even when trained with bounded depth.\nLanguages with Sparse Non-Solvable Transitions SSMs excel in natural language processing tasks despite being theoretically constrained to the simple class of star-free formal languages (Sarrof et al., 2024). We conject that natural language is mostly composed of simple to comprehend tokens, while harder tokens appear only sparsely. To study implicit models in a controlled learning environment closer to natural language than the S5 word problem, we construct a word problem that mixes simple and hard examples. Let $M = M^a \\times G$ be a direct product of an aperiodic monoid $M^a$ and a non-solvable group $G$. A sequence $m_0, ..., m_y$ is sampled from $M$ with replacement. To control the number of hard examples and simple examples, we define a family of distributions $D_p$ over $M$ as follows. An element $m^a \\in M^a$ is sampled uniformly at each step $k$, representing the presence of simple examples. On the other hand, we sample elements $g_k \\in G\\setminus {e}$ from $G$ without the identify transformation, each with probability $|G|^{-1}$. The identity element $g_k = e \\in G$ is sampled with probability $1 - p$. The resulting transformations $(m^a, g_k)$ are aperiodic at least when $g_k = e$, i.e. with probability $1 - p$.\nInterpolating between SSMs and RNNs We will identify minimally sequential models that parallelize to a high degree"}, {"title": "5. Implicit Large Language Models", "content": "We investigate whether implicit models can be effectively pretrained to function as language models. Motivated by the results of Section 4, we implement a pretraining strategy for implicit models with two stages of bounded and free self-iterations. Transformer (LLama) Touvron et al. (2023) and SSM (Mamba2) (Dao & Gu, 2024) architectures serve as the core backbones for our implicit models. In the bounded stage, we train with four self-iterations and a single step of phantom gradient, which we refer to as the (4 + 1)-model. The (s + k)-notation refers to s gradient-tape-free self-iteration steps and k phantom gradient steps. k refers to Equation (5), see also Figure 5. The free stage starts from a checkpoint of the (4+1)-model and increases the number of self-iterations to 24/32 followed by four steps of phantom gradient. We refer to these models as (24 + 4)/(32+4)-models for Mamba2/Llama, respectively. We employ four model sizes: 125M, 350M, 760M, and 1.3B. These models are pretrained in an autoregressive manner for next-token prediction across all sizes on the D-PILE (Gao et al., 2020) dataset, which consists of 207B tokens. For baselines, we use both Mamba2 (Dao & Gu, 2024) and Llama (Beck et al., 2024) models previously trained on a corpus of 300B tokens. Additionally, we reproduce Mamba2* and Llama\u2020 as baselines trained with the same code and data as our implicit models. We evaluate the pretrained models on the test set of the D-PILE, examine their length extrapolation capabilities, and assess their common sense reasoning performance on downstream tasks. See Appendix D.3 for pretraining details.\nPretraining Results and Downstream Performance. We report in Table 1 the next-token perplexity performance of all models trained on the entire 207B token corpus using a test split of the D-PILE\u00b9 . We observe our implicit models consistently achieve a lower perplexity compared to their explicit counterparts see also Figure 1Bottom. For details related to the dynamics of the implicit models on D-PILE, refer to Table 2. Additionally, we evaluate the models' performance on common sense reasoning tasks using the LM Evaluation Harness (Gao et al., 2024). The results show that implicit Mamba2 outperform the explicit Mamba2*, which are pretrained on the same number of tokens, on most tasks. This difference becomes more pronounced as the size of the models increases, specifically with the 760M and 1.3B variants. Compared to the original Mamba2 baseline, trained on 1.5 times more data, the implicit models do better on HELLASWAG, PIQA, ARC-E, and are competitive in LAMBADA and ARC-C. Across all scales, the implicit Mamba2 models significantly outperform Mamba2 in the HELLASWAG task, yet they underperform in WINO-GRANDE and OPENBOOKQA.\nIt is also noteworthy that our implicit Llama models substantially outperform the baseline Llamas, including both the results reported in (Beck et al., 2024) and the Llama\u2020. This improvement is consistent across all tasks and model sizes. Strikingly, we note that our implicit Llama (32+4) 760M is competitive to the explicit Llama\u2020 1.3B."}, {"title": "6. Related Work", "content": "Adaptive-Compute Time The idea of an adaptive compute budget goes back to (Schmidhuber, 2012) who employ a halting neuron to delimit the computation on a particular input. Graves (2017) generalized the idea and regularised the halting condition to encourage the network to stop early. They implemented an adaptive-depth RNN and demonstrated the network adjusting the compute budget based on the difficulty of instances in a parity-check task. This idea was later applied to Transformers, resulting in \"Universal Transformers\" (UT) (Dehghani et al., 2019). UTs can either be unrolled to a fixed depth or augmented with a dynamic halting condition (DHC) per token. UTs were later shown to exhibit improved scaling laws compared to standard transformers (Kaplan et al., 2020). PonderNet (Banino et al., 2021) introduced a principled probabilistic model for determining the halting condition. This approach improved on the UT on the BABI benchmark. Recently, a mixture-of-experts (MoE) variant of the UT (MoEUT) was presented (Csord\u00e1s et al., 2024) with 1B parameters, seeking to improve the parameter-to-compute ratio of UTs. The MoEUT is an unrolled model with fixed iterations and does not employ a DHC. While our models presented here are dense, they could, in principle, be turned into MoE. Gatmiry et al. (2024) show that looped linear transformers implement gradient-descent until convergence on the prediction loss defined by previous input-output examples in the context window. Lim et al. (2024) take the opposite approach to our work: Instead of augmenting SSMs or transformers, they propose an approach based on fixed-point iterations to enable parallel training of RNNs. However, their method incurs cubic cost in terms of state size, limiting the method to smaller models.\nReasoning and out-of-distribution generalization. The ability of looped models to generalize better to input lengths not seen during training is empirically well established: For example Yang et al. (2024a) show this for looped transformers, while Anil et al. (2022) demonstrate length generalization for DEQs, particularly when they are path independent. Du et al. (2022) show that energy-based models trained to map energy-gradient-descent steps to algorithmic steps, can length generalize in summation, and complex algorithms such as shortest-path. On the theoretical side, The pioneering work of Siegelmann & Sontag (1992) shows that iterated RNNs are Turing complete at infinite numerical precision. More recently, Deletang et al. (2023) studied a number of sequence models and report that grouping tasks by their rung in the Chomsky hierarchy is predictive of models ability to length-generalize. While the works of Merrill et al (Merrill, 2019; Merrill et al., 2020; Merrill & Sabharwal, 2023; Merrill et al., 2024), which we discuss inSection 2.2, showed that both transformers and SSMs are restricted to TC\u2070; several studies sought to find more precise constraints. Weiss et al. (2021) observe that programs written in a specific language (RASP) can be mapped to transformer models of sufficient capacity. Zhou et al. (2023) then showed that transformers tend to length-generalise if the underlying data-generating process can be expressed in RASP. Sarrof et al. (2024) derived a similar refined constraint for SSMs and showed that they can precisely express star-free regular languages. Grazzi et al. (2024) demonstrate that SSMs can track state in simple problems, such as parity, when their (diagonal) recurrence matrix A in Equation (1) permits negative eigenvalues. Moreover, they illustrate that a variant of DeltaNet (Yang et al., 2024b) with (possibly) negative eigenvalues can solve the S5 problem when only swaps of two values are considered in the transition. However, no variant of Mamba or DeltaNet was capable of learning S5 and achieving length generalization."}, {"title": "7. Discussion and Conclusion", "content": "This work demonstrates that models implicitly defined by a fixed point iteration can solve hard state tracking problems that resist the capabilities of transformers and SSMs. We provide theoretical insight how implicit SSMs can deviate from pure diagonal and linear token-to-token transitions and effectively become an RNN in the limit. When trained with a relatively small number of self-iterations, our models seamlessly generalize from simpler to harder word problems (see Figure 3). This property is of special interest in language modeling where 'hard' sequences are rare but might occur clustered in applications requiring state tracking.\nOur extensive study of synthetic state tracking problems informs a pretraining schedule for large language models. The implicit Llama and Mamba2 models improve over the baselines in many cases, and prove particularly beneficial on downstream tasks such as HELLASWAG (see Table 1). Performance on language modeling is typically primarily determined by parameter count which traditionally caused weight-shared models to underperform (Tay et al., 2023). While implicit models lift the limitations of state-of-the-art language models, self-iteration comes at a cost that only amortizes over the long tail of natural language. However, emerging hardware that accelerates such self-iteration would alleviate this overhead (Brunner et al., 2025). Furthermore, as LLMs make more progress on reducing perplexity, they may eventually face tokens requiring RNN-like transitions.\nFinally, given the recent rise of test-time compute (Snell"}, {"title": "A. Algebraic Structure of Finite State Machines", "content": "This section provides a basic introduction to the word problem and it's relation to simulating finite state machines (FSMs). We start with some results in the circuit complexity and then relate them to the properties of FSMs."}, {"title": "A.1. Circuit Complexity", "content": "Efficient parallelization is one of the central features enabling transformers and SSMs to scale to large machine learning problems such as language modeling. Parallel circuits, however, face fundamental trade-offs regarding the class of problems that they can address. Circuit complexity theory provides a framework to characterize the types of problems that parallel circuits can solve. TC\u00ba is the class of circuits with constant depth and polynomial width composed of unbounded fan-in AND-gates, OR-gates, NOT-gates and MAJORITY-gates. The second class of interest, NC\u00b9, is represented by logarithmic depth circuits with a polynomial number of bounded fan-in gates. From the perspective of formal languages, NC\u00b9 is equivalent to the class of circuits recognizing the regular languages. Since the unbounded fan-in gates allowed in TC0 circuits can be constructed from log-depth circuits with bounded fan-in, it follows that TC\u00ba \u2282 NC\u00b9. It is open if TC\u00ba is a proper subset of NC\u00b9, and we will discuss a regular language for which no TC\u00ba circuit construction is known.\nBoth transformers and SSMs can be simulated by TC circuit families under mild assumptions (Merrill et al., 2022; 2024). If TC\u00ba is a proper subset of NC\u00b9, the leading sequence models today cannot even recognize all regular languages. Consequentially, they cannot execute arbitrary finite state machines (FSMs), a fundamental skill to execute tasks, or to form world models (Vafa et al., 2024). Many empirical studies confirm these theoretical limitations of transformers and SSMs to learn regular languages (Deletang et al., 2023; Sarrof et al., 2024; Strobl et al., 2024). At the same time, recurrent neural networks are known to recognize regular languages (Kleene, 1951; Elman, 1991; Merrill et al., 2020), and to effectively implement internal FSMs to solve language problems (Omlin & Giles, 1996)."}, {"title": "A.2. Algebraic Concepts of Finite State Machines", "content": "Monoids and Groups There is a tight relationship between finite state machines and algebraic concepts such as monoid and groups. We define the relevant concepts for our state tracking problem described in Section 4"}, {"title": "Definition 2 (Monoid).", "content": "A set $M$ and a binary operation $\\circ : M \\times M \\rightarrow M$ are called a monoid $(M, \\circ)$ if\n1. there exists an identity element $e \\in M$ with $e \\circ m = m \\circ e = m$ for all $m \\in M$\n2. the operation $\\circ$ is associative, i.e. $(m_1 \\circ m_2) \\circ m_3 = m_1 \\circ (m_2 \\circ m_3)$ for all $m_1, m_2, m_3 \\in M$.\nStraight forward examples for monoids are natural, rational or real numbers with multiplication, or strings with string concatenation. Since monoid are associative, we can simplify notation and write $m \\circ m = m^2$, and so on for all powers $k \\in \\mathbb{N}$."}, {"title": "Definition 3 (Aperiodic Monoid).", "content": "A monoid $(M, \\circ)$ is called aperiodic if for all $m \\in M$ there is a $k \\in \\mathbb{N}$ s.t. $m^k = m^{k+1}$.\nMonoid whose elements can be inverted have a particularly right structure."}, {"title": "Definition 4 (Group).", "content": "A group $(G, \\circ)$ is a monoid with the additional property that for every $g \\in G$ there is $g^{-1} \\in G$ s.t. $g \\circ g^{-1} = g^{-1} \\circ g = e$.\nExamples for groups are rational numbers with multiplication, or the orthogonal matrices with matrix multiplication. Notably, permutations on a set of $k$ elements for $k \\in \\mathbb{N}$ form a group, called the symmetric group $S_k$.\nOur synthetic learning problem discussed in Section 4 will be constructed based on a classical problem in computer science."}, {"title": "Definition 5 (Word Problem).", "content": "Let $M^*$ denote the set of all sequences over elements of $M$. The Word Problem on a monoid $(M, \\circ)$ is defined by the function\n$WP : M^* \\rightarrow M$\n$WP (m_0, m_1..., m_k) \\rightarrow m_0 \\circ m_1 \\circ \\cdot \\cdot \\cdot \\circ m_k,$, (9)\ni.e. a word over $M$ is resolved by composition to a single element in $M$."}, {"title": "Theorem 6 ((Barrington, 1989)).", "content": "The word problem for any fixed non-solvable group $G$ is complete for $NC^1$ under $AC^0$ reductions."}, {"title": "Algebra of FSMs", "content": "Tracking the state of a system can be formalized as executing a finite state machine (Merrill et al., 2024). To characterize the limits of certain FSMs, we define a few formal concepts."}, {"title": "Definition 7 (FSM).", "content": "A finite state machine (FSM) consists of a finite set of states $Q$, a finite set of input symbols $\\Sigma$ called the alphabet, and a transition function $\\delta : Q \\times \\Sigma \\rightarrow Q$.\nGiven an initial state $q_0 \\in Q$ and a sequence of symbols $w = a_1 a_2 . . . a_k \\in \\Sigma^*$, a FSM transitions from the initial state into a final state."}, {"title": "Definition 8 (Syntactic Monoid).", "content": "For each symbol $a \\in \\Sigma$, define the function $\\delta_a : Q \\rightarrow Q$. The transformation monoid $M$ generated by $\\delta_a$, $a \\in \\Sigma$ and the composition of functions $o$, is called the syntactic monoid $(M, o)$ of the finite state machine.\nThe algebraic structure of $M$ is tightly coupled to the programs that the original FSM can execute. Our investigation is based on the classical result stated in Theorem 6. The simplest example of a non-solvable group is the permutation group of five elements $S_5$. A corollary from theorem 6 is that the FSM whose syntactic monoid is $S_5$ is complete in $NC^1$ and hence in the class of regular languages. We have thus identified a hard state tracking problem: Permutations of five elements.\nAnother classical result tightly related to state-space models is"}, {"title": "Theorem 9 ((Sch\u00fctzenberger, 1965)).", "content": "Let $L$ be the regular language defined by a FSM, and let $M$ be the syntactic monoid of the same FSM. Then $L$ is a star-free language if an only if $M$ is aperiodic.\nIt is intuitive that the word problem for finite aperiodic monoids is in $TC^0$. The maximal depth of the circuit is driven by the number of elements of the monoid and it's maximal $k$ for the aperiodicity condition. Empirical studies have shown that transformers and SSMs can simulate a range of regular languages (Deletang et al., 2023; Strobl et al., 2024), but they struggle to learn the S5 word problem in line with their characterization as TC\u2070 circuits (Merrill et al., 2024). SSMs can be further restricted to the star-free languages (Sarrof et al., 2024), i.e. those with aperiodic syntactic monoid."}, {"title": "Theorem 1.", "content": "Consider an implicit SSM given by Equations (7) and (6). Then the transition function $h_{t-1} \\rightarrow h_t$ is non-linear and non-diagonal, i.e. each hidden state $h_t$ is a non-linear function of the previous hidden state $h_{t-1}$. Consequently, the state-to-state Jacobian is a non-diagonal operator."}, {"title": "5. Figure Fixed-point iteration and phantom gradients:", "content": "A neural network is iterated until convergence in the forward pass. When employing the phantom gradient principle, only a fraction of the forward steps is however considered for the backward pass."}, {"title": "Figure Comparison of implicit Mamba2, unrolled Mamba2, and Mamba2 for p = 0.05. All models were trained and evaluated on sequences of length L = 256.", "content": "Unrolled Mamba2 refers to a single layer being unrolled multiple times with a full backpropagation trace, while the implicit Mamba2 receives only 4 steps of Phantom Gradient. The training time depth of all models is limited to 16, i.e. 16 layers for Mamba2, and 16 self-iterations for the implicit and weight tied models. Implicit and unrolled models use unbounded test-time computation to converge to a fixed point. The comparison shows that the implicit model with 4 steps of Phantom Gradient succeeds over the unrolled model."}, {"title": "Figure Hyperparameter sweeps for explicit Mamba2 models over batch sizes 128, 256, layers 1,2,3 and various learning rates for training on the CATBABI dataset.", "content": null}, {"title": "D.3. Language Modeling", "content": "Pretraining Details We have trained a suite of Implicit SSM models with the core architecture of Mamba2 and Implicit Transformer models with the core of Llama3. For each implicit model", "models": 1.3}]}