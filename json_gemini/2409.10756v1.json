{"title": "VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching", "authors": ["Arastoo Zibaeirad", "Marco Vieira"], "abstract": "Large Language Models (LLMs) have shown promise in tasks like code translation, prompting interest in their potential for automat-ing software vulnerability detection (SVD) and patching (SVP). To further research in this area, establishing a benchmark is es-sential for evaluating the strengths and limitations of LLMs in these tasks. Despite their capabilities, questions remain regarding whether LLMs can accurately analyze complex vulnerabilities and generate appropriate patches. This paper introduces VulnLLMEval, a framework designed to assess the performance of LLMs in identi-fying and patching vulnerabilities in C code. Our study includes 307 real-world vulnerabilities extracted from the Linux kernel, creating a well-curated dataset that includes both vulnerable and patched code. This dataset, based on real-world code, provides a diverse and representative testbed for evaluating LLM performance in SVD and SVP tasks, offering a robust foundation for rigorous assessment. Our results reveal that LLMs often struggle with distinguishing between vulnerable and patched code. Furthermore, in SVP tasks, these models tend to oversimplify the code, producing solutions that may not be directly usable without further refinement.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as OpenAI's Codex [6], Meta's Llama [9, 51], CodeBERT [11], DeepMind's AlphaCode [26], Sales-force's CodeGen [35] and CodeT5 [56], PLBART [1], and InCoder [12], have shown remarkable capabilities in understanding program-ming languages [33]. These models have potential for assisting de-velopers by providing code suggestions, test generation [2, 46, 54], code documentation [21], and automating various coding tasks. In fact, by employing advanced methods to train billions of parameters on data from large datasets [14, 15, 17, 50], LLMs demonstrate sig-nificant capabilities in various areas [10]. These models show great potential for automating Software Vulnerability Detection (SVD) and Software Vulnerability Patching (SVP), making software secure by improving efficiency, accuracy, and coverage in identifying and fixing vulnerabilities.\nThe number of identified and patched software vulnerabilities has surged in recent years, with over 29,000 Common Vulnerabil-ities and Exposures (CVEs) addressed in 2023, up from 25,084 in 2022 and 20,153 in 2021 [8]. This trend underscores the need for automated SVD and SVP to keep up with rapidly evolving digital technologies. There are, however, several limitations and challenges in SVD and SVP from the perspective of both LLMs and traditional vulnerability detection techniques, such as Automatic Program Repair (APR), fuzzing, and Static Analysis Tools (SAT).\nFrom the perspective of LLMs, one major challenge lies in under-standing the inherent complexity of vulnerability patterns, which involve intricate interactions between multiple functions or mod-ules within the entire system rather than isolated code snippets. For instance, a vulnerability might only surface when a specific sequence of function calls is made, or when data flows through various parts of the code in a particular order. Detecting these in-teractions requires a deep understanding of how different parts of the code interact over time.\nA key problem with state-of-the-art LLMs is that they are trained on billions of lines of code without distinguishing between vul-nerable and non-vulnerable code, which can lead to ineffective identification and prevention of software vulnerabilities. For in-stance, while many LLMs are trained on data including the Linux kernel project and know the entire codebase, they lack awareness of previous versions of the Linux kernel that contained numerous vulnerabilities. Furthermore, from a software engineering perspec-tive, it is crucial that these models can detect vulnerabilities in code that they have never seen before, such as new projects or entirely new codebases. To enhance their effectiveness in SVD and SVP, a large dataset that differentiates between vulnerable code versions and their patched counterparts is necessary.\nTraditional vulnerability detection techniques also present sig-nificant challenges. APR faces issues with patch quality and gener-alization [19, 61]. Fuzzing struggles with achieving comprehensive coverage and detecting logic errors [24, 30]. Similarly, SATs often suffer from a high rate of false positives and difficulty in under-standing dynamic behaviors [39]. These limitations highlight the need for development of more sophisticated approaches, includ-ing effectively leveraging LLMs for SVD and SVP, which in turn requires a better understanding of their performance.\nThere are, however, notable gaps in the evaluation of LLMs for SVD and SVP tasks:\n(1) Data leakage: The evaluation of state-of-the-art LLMs is compromised by data leakage, where models are tested on datasets they were trained on, leading to inflated perfor-mance metrics that do not reflect real-world capabilities [44, 57].\n(2) Comprehensive evaluation with real-world data: Exist-ing evaluations often assess LLMs in narrow contexts using"}, {"title": "2 Background and Related Work", "content": "Benchmarking LLMs. Ullah et al. [52] evaluated eight LLMs on their ability to detect and reason about security vulnerabilities using 228 code scenarios, including 30 real-world examples in C and Python. Despite extensive testing, the study found current LLMs to be inconsistent and unreliable for automated vulnerability detection.\nKhare et al. [22] examined state-of-the-art LLMs like GPT-4 and CodeLlama, focusing on their ability to identify security vulnerabil-ities across Java and C/C++ projects. The study highlighted LLMs' struggle with complex, context-dependent vulnerabilities and the need for specialized fine-tuning strategies.\nSun et al. [48] developed a framework for evaluating LLMs' reasoning capabilities in detecting smart contract vulnerabilities. The study emphasized the importance of context and knowledge integration for improving performance in complex scenarios.\nLiu et al. [28] benchmarked 17 LLMs across various tasks us-ing datasets from projects like Apache Subversion and synthetic sources. The study stressed the significance of context and prompt design in enhancing LLM performance.\nGao et al. [13] introduced VulBench, a benchmark aggregating data from CTF challenges and other high-quality sources, evaluat-ing LLMs on their ability to detect vulnerabilities and root causes in both binary and multi-class classification tasks.\nData Collection. The CVEfixes dataset [3] automatically com-piles vulnerabilities from the National Vulnerability Database, link-ing them with corresponding fixes in open-source repositories. It includes both vulnerable and patched code samples, enhanced with metadata like programming languages and code metrics. However, it primarily focuses on modified code snippets rather than providing the full context of the surrounding code."}, {"title": "3 Benchmarking Approach", "content": "In this section, we present our framework and methodology to evaluate the performance of LLMs in SVD and SVP. We start by introducing VulnLLMEval in Section 3.1, outlining the steps and tools we used in our evaluation. Section 3.2 then describes the dataset and how we extracted real-world vulnerabilities from the Linux kernel, with potential extensions to other projects in the future. In Section 3.3, we cover the different metrics used to measure the effectiveness of LLMs in both SVD and SVP tasks, ensuring a thorough and balanced evaluation. Finally, Section 3.4 explains the prompt templates created to standardize the input queries for the LLMs."}, {"title": "3.1 VulnLLMEval", "content": "VulnLLMEval is a framework designed to evaluate the effective-ness and robustness of LLMs in tasks related to SVD and SVP, as shown in Figure 1. The process starts with the extraction of initial metadata, such as commit hashes linked to vulnerabilities with CVE and CWE identifiers, forming the foundation for building our dataset of vulnerable and patched code blocks, as discussed in Sec-tion 3.2. The systems under benchmarking (SUB) are ten LLMs pre-trained on various programming languages and code structures (detailed in Table 3), and VulnLLMEval evaluates their performance across 8 tasks to benchmark how effectively these models can iden-tify and patch vulnerabilities in real-world codebases. The outputs generated by the LLMs are automatically filtered using regular expressions, ensuring consistency and reducing biases or errors. Finally, the evaluation involves calculating precision, recall, accu-racy, F1 scores, and other relevant metrics for detection, along with ROUGE, CodeBLEU, and cyclomatic complexity for patching, as detailed in Section 3.3.\nScope of the Benchmark. VulnLLMEval targets C/C++ code, given its widespread use in system-level programming, particularly in projects like the Linux kernel, one of the most widely used and essential systems globally. Due to its prevalence and the critical nature of its vulnerabilities, the Linux kernel serves as a highly rele-vant target for our benchmark. While this benchmark focuses on the Linux kernel, it can be extended to other programming languages. It considers over 30 CWEs, with an emphasis on prevalent vulnera-bilities such as buffer overflows (CWE-119), improper input vali-dation (CWE-20), information exposure (CWE-200), use-after-free (CWE-416), and null pointer dereference (CWE-476). The evalua-tion assesses code at both the file and function levels, incorporating non-functional elements like global variables and configuration settings linked to functional components. Including both functional and non-functional elements is essential as it captures the intercon-nections between files, functions, and non-functional components, providing a comprehensive view of the codebase. This approach is crucial for understanding the complexity of vulnerable code since patches often involve changes to variables or settings that LLMs must accurately interpret for effective vulnerability detection and patching.\nEvaluation Scenarios and Metrics. VulnLLMEval can be used to assess LLMs through two primary scenarios: detection and patch-ing. In the detection scenario, the models are evaluated on their ability to identify vulnerabilities within given code blocks using"}, {"title": "3.2 Dataset", "content": "Manually labeling datasets is labor-intensive, time-consuming, and difficult to scale, often leading to incomplete or inconsistent data [25], so we developed a custom dataset to address these challenges efficiently. Our approach focuses on creating datasets that distin-guish between vulnerable and non-vulnerable code in real-world scenarios, rather than just using isolated code snippets of vulnerable code. By leveraging publicly available CVE records and metadata such as commit hashes, CVEs, and CWEs, we can systematically extract vulnerable code blocks and their corresponding patched versions at various levels of abstraction, including files, functions, and non-functional elements. This ensures a comprehensive un-derstanding of vulnerabilities and their contexts, providing a more realistic and effective dataset for training and evaluating LLMs. The detailed steps of this automated data collection process are outlined in Algorithm 1.\nDataset Acquisition and Preparation. Our study centers on the Linux kernel project, which offers a wealth of vulnerability data crucial for refining our data collection process. We systematically gathered essential metadata, including commit hashes, CVEs, and CWEs, utilizing resources introduced by Pereira et al. [38]. Ad-ditionally, metadata was sourced from platforms such as MITRE [32] and the National Vulnerability Database (NVD) [34]. This approach enabled us to trace the history and progression of vulner-abilities-from their initial discovery to their resolution-providing a comprehensive understanding of how vulnerabilities evolve and are patched over time. The output of this step is the collection of commit hashes linked to specific CVEs and their corresponding CWEs, ensuring a detailed and accurate dataset for further analysis. This comprehensive gathering process allows for a thorough analy-sis of the Linux kernel's codebase, capturing the complete lifecycle of each vulnerability from introduction to remediation."}, {"title": "3.3 Metrics", "content": "To evaluate our models, we employ various metrics that characterize the accuracy and relevance of predictions. These metrics are tailored for both SVD and SVP.\n3.3.1 SVD Metrics\n(1) F1 Score, Recall, Accuracy, Precision are standard met-rics for evaluating the performance of classification models. Accuracy measures the ratio of correctly predicted instances to the total instances. Precision is the ratio of correctly pre-dicted positive observations to the total predicted positives. Recall (Sensitivity) is the ratio of correctly predicted positive observations to all actual positives. The F1 Score, the har-monic mean of precision and recall, provides a single metric that balances both precision and recall. These metrics are fundamental for assessing the effectiveness of binary and multi-class classification tasks.\n$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$$\n$$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n$$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n$$\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n(2) Top-k accuracy is a metric used to evaluate classification models where the model's prediction is considered correct if the true label is within the top k predicted labels. This metric is particularly useful in scenarios where ranking is crucial, such as recommendation systems, where it is important to measure how often the correct answer is among the top predictions. In this study, we specifically use Top-5 accuracy, as it provides a balanced assessment of model performance by focusing on the most likely correct answers while keeping the evaluation manageable [42]. The equation is:\n$$\\frac{\\text{Number of correct predictions in top k}}{\\text{Total number of predictions}}$$\n(3) Mean Reciprocal Rank (MRR) [7] is a statistical mea-sure commonly used in information retrieval and question-answering systems to assess how effectively a system ranks the correct answers. In the context of LLMS, MRR evaluates the quality of responses in classification tasks by measuring how high the correct answer is positioned in a ranked list of possible answers based on their probabilities. MRR provides an average score that reflects the ability to rank relevant items at the top.\n$$\\text{MRR} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\text{rank}_i}$$\nHere, N is the number of queries, and rank; is the rank position of the first relevant document or correct answer for the i-th query.\n3.3.2 SVP Metrics\n(1) ROUGE Score [27] is used to evaluate the similarity be-tween the generated patches and reference patches by com-paring their n-gram overlap. ROUGE-L, which focuses on the longest common subsequence, is particularly relevant for measuring how well the generated patch aligns with the reference in terms of sequence structure.\nIn practice, after generating a patch, we calculate the ROUGE-L score by comparing it to the reference patch. A higher ROUGE-L score indicates that the model has produced a patch that closely matches the reference in terms of sequence similarity, which can be an indicator of the model capturing relevant patterns.\n$$\\text{ROUGE-L} = \\frac{\\text{ELCS(X,Y)}}{\\Sigma\\text{Count(X)}}$$\nwhere LCS(X, Y) represents the length of the longest com-mon subsequence between the generated text X and refer-ence text Y, and Count(X) is the total length of the reference text.\n(2) CodeBLEU [40] is an advanced metric designed specifi-cally for evaluating code generation. Traditional metrics like BLEU [36] are inadequate for code as they primarily focus on n-gram matching, overlooking essential aspects such as syntactic structure and semantic correctness. While metrics like ROUGE can capture surface-level similarities, they fall short in evaluating code since they only measure textual overlap and do not account for the underlying structure or logic. CodeBLEU goes beyond these limitations by assess-ing not only surface similarity but also the syntactic and semantic accuracy of the generated code, making it a more comprehensive tool for code evaluation.\nCodeBLEU combines several dimensions of code quality, including:\n\u2022 Surface similarity through n-gram matching.\n\u2022 Syntactic accuracy using abstract syntax tree (AST) match-ing.\n\u2022 Semantic correctness via data-flow analysis.\nThe overall CodeBLEU score is calculated as a weighted sum of these components:\n$$\\text{CodeBLEU} = \\alpha \\cdot \\text{BLEU} + \\gamma \\cdot \\text{Match}_{AST} + \\delta \\text{Match}_{DF}$$\nwhere BLEU refers to the standard BLEU score, BLEU weight applies weights to different n-grams, emphasizing the im-portance of certain tokens, particularly keywords, MatchAST evaluates syntactic correctness by comparing sub-trees in the abstract syntax trees (ASTs) of the candidate and ref-erence code, and MatchDF assesses semantic accuracy by comparing data-flow graphs, which represent variable de-pendencies. This approach ensures that the generated code not only matches the reference text in form but also aligns with it in structure and meaning, making CodeBLEU a more reliable metric for evaluating code generation.\n(3) Cyclomatic Complexity [31] is a software metric used to measure the complexity of a program by quantifying the number of linearly independent paths through the source code. This metric helps identify complex sections of code that may require more rigorous testing and maintenance, ul-timately aiding in managing code quality and maintainability. By comparing the complexity of the original vulnerable code to the patched version, we assess whether the LLMs are gen-erating simplified but functional code or producing overly complex fixes that may introduce maintenance challenges. A decrease in complexity often indicates more maintainable code, while excessive reductions may suggest oversimplifica-tions that overlook important functionality. In our study, the cyclomatic complexity ranges from 0 to 5 (low complexity), 5 to 10 (moderate complexity), 10 to 15 (high complexity), and 15 to 20 (very high complexity), offering a framework for evaluating whether the patches generated by LLMs simplify the code appropriately without compromising functionality. It is calculated using the formula:\n$$\\text{M} = \\text{E} - \\text{N} + 2\\text{P}$$\nwhere M is the cyclomatic complexity, E is the number of edges representing control flow transitions between nodes in the control flow graph of the program, N refers to the number of nodes which indicate decision points or statements, and P is the number of connected components that represent independent sections of code, such as separate functions or methods."}, {"title": "3.4 Prompt Templates", "content": "In this section, we outline the prompt templates used for zero-shot and few-shot scenarios in vulnerability detection and patching. However, we excluded few-shot prompts for vulnerability detection to prevent bias. Table 2 summarizes the prompt types, inputs, and descriptions."}, {"title": "4 Benchmarking Results", "content": "In this study, we leverage 10 pre-trained LLMs to assess their capa-bilities in SVD and SVP tasks. The models include CodeLlama [41], Gemma2 [49], Llama3, Llama3.1 [9], and Mistral [20], each available in different parameter scales. We primarily used instruct models, as they are fine-tuned to better follow instructions and generate more relevant outputs for complex tasks. While ChatGPT is not included in this study, our primary goal is to demonstrate and validate the benchmark. Our approach is designed to be flexible and can easily be adapted to evaluate other LLMs, including different versions of GPT. The specifications of the models we used are detailed in Table 3. To maintain consistency across evaluations, we set the temperature parameter to 0.0 for all models.\n4.1 RQ1: Effectiveness of LLMs in SVD\nIn this section, we examine three critical aspects of evaluating the effectiveness of LLMs in SVD tasks using our real-world dataset. First, we assess the models' ability to accurately rank CVEs and CWEs by comparing their outputs against ground truth data. Next, we analyze how well the models differentiate between vulnera-ble and patched code through four targeted prompts. Finally, we evaluate the effectiveness in handling the most frequently occur-ring CWEs, providing detailed insights into their applicability and reliability in real-world SVD tasks.\n4.1.1 Ranking In SVD1 and SVD2, the task is to assess the models' ability to rank CVEs and CWEs accurately. To evaluate this, we calculate Top-5 accuracy and MRR, which measure the precision and relevance of the rankings. MRR provides a more comprehensive evaluation by considering the exact rank of the correct answer, reflecting the overall quality of the models' rankings. To ensure fair benchmarking, we included the year of the CVE in the rankings, preventing confusion from similarly characterized CVEs across different years. In instances where LLMs provided descriptions instead of explicit CVE IDs or varied in the number of ranked responses, we dynamically adjusted the assessment, flagging such responses to avoid skewing the results. Logic was incorporated into the script to ensure that MRR was calculated on valid, comparable rankings, even when response lengths varied.\nObservations. Our analysis revealed significant variability in the LLMs' ability to accurately rank CVEs and CWEs. Notably, the performance in ranking CWEs was generally better than for CVEs, potentially due to the broader and more structured nature of CWEs, which may align better with the LLMs' training data. Among the models, Llama3-70b demonstrated the highest perfor-mance across both CVE and CWE rankings, consistently placing the correct vulnerabilities higher in the list and achieving the best MRR scores. Conversely, Codellama often ranked the correct vulnera-bilities lower on the list, indicating lower accuracy. Additionally, while the models were expected to provide the top 5 most probable CVEs and CWEs, the length of their responses varied-ranging from one to five entries. Despite our inclusion of the year for CVE rankings, some LLMs returned results with CVEs from incorrect years, showing difficulties in maintaining context. Furthermore, certain models provided only descriptions of the vulnerable code without listing CVE IDs, or indicated that they could not generate a list of potential CVEs or lacked sufficient information to identify CWEs in the code. These inconsistencies highlight the challenges in obtaining reliable and consistent outputs from the models.\n4.1.2 Is Vulnerable? We assess the effectiveness of LLMs in deter-mining whether a code block is vulnerable. This evaluation focuses on key metrics-precision, recall, accuracy, and F1 score-applied to both vulnerable and patched code blocks across various LLMs. The analysis offers insights into how well these models can dis-tinguish between vulnerable code, patched code (which has been modified to address vulnerabilities). This distinction is crucial for ensuring accurate and reliable security assessments. Specifically, prompts SVD3 and SVD5 focus on code blocks where the ground truth indicates vulnerability (labeled as 1), while SVD4 and SVD6 address code blocks that are not vulnerable (labeled as 0). SVD3 and SVD4 evaluate whether a given code block, either vulnerable or patched, remains vulnerable, while SVD5 and SVD6 challenge the LLMs to determine if the code block is vulnerable based on its corresponding CVE and CWE identifiers. To ensure consistent and accurate evaluations, we implemented a comprehensive set of regular expressions to filter and analyze the LLMs' responses effectively.\nObservations. Table 5 summarizes the results for four evalua-tion prompts. SVD3 and SVD4 are paired to assess how well the models can identify vulnerabilities, with SVD3 focusing on vul-nerable code blocks and SVD4 on patched code blocks. Similarly, SVD5 and SVD6 evaluate the models' ability to determine if a code block is vulnerable by considering whether it is susceptible to its corresponding CVE and CWE identifiers. The combined analysis of SVD3 and SVD4, as well as SVD5 and SVD6, reveals several key trends. Notably, providing the ground truth CVE and CWE in SVD5 and SVD6 results in an average improvement of 9% in accuracy, indicating that LLMs perform better when given specific contextual information.\nHowever, the models generally tend to classify code as vulnerable regardless of whether the block is the original vulnerable code or its patched version. This tendency explains the higher accuracy observed in SVD3 and SVD5 compared to SVD4 and SVD6, as shown in Figure 2. Interestingly, in the case of the Llama3-70b model, the"}, {"title": "4.1.3 CWE Distribution", "content": "We analyzed the distribution of CWEs to evaluate the effectiveness of LLMs in identifying different types of vulnerabilities. The results were calculated for both SVD5 and SVD6 scenarios. The primary CWEs examined were CWE-119 (Buffer Overflow) with 50 instances, CWE-416 (Use After Free) with 37 instances, CWE-200 (Information Exposure) with 31 in-stances, CWE-476 (NULL Pointer Dereference) with 27 instances, and CWE-20 (Improper Input Validation) with 26 instances. Obser-vations. The Table 6 shows significant variation in model perfor-mance across different CWE types, with strengths and weaknesses emerging in handling specific vulnerabilities. Models like Llama3.1-8b and Gemma2-9b show consistently high recall, particularly with CWE-20 and CWE-200, indicating strong capabilities in identifying true vulnerabilities. In contrast, models such as Llama3-70b and Mistral-7b struggle, especially with more complex vulnerabilities like CWE-416 and CWE-476, exhibiting lower precision and recall. Codellama-7b generally performs well across most CWEs, but its effectiveness drops significantly with CWE-416. Additionally, the models tend to have lower precision, signaling a higher rate of false positives, where non-vulnerable code is mistakenly flagged as vulnerable. At the same time, the higher recall scores suggest fewer false negatives, meaning that vulnerabilities are identified effectively, though sometimes at the cost of overestimating the risk. This balance between precision and recall highlights the need for refining model accuracy to reduce false positives without sacrificing the ability to detect true vulnerabilities.\nObservations. The analysis of LLM performance across the five most frequent CWEs reveals key trends. A general observation is that while models displayed higher recall rates, indicating an effective detection of vulnerabilities, they often suffered from lower precision. This means that the models tended to misclassify non-vulnerable or patched code as vulnerable, leading to a significant number of false positives.\nSpecifically, for CWE-119 (Buffer Overflow) and CWE-416 (Use After Free), the models, such as Llama3-8b and Gemma2-9b, showed strong recall, reflecting their ability to identify these vulnerabilities. However, their lower precision and F1 scores suggest difficulties in filtering out irrelevant instances, resulting in a higher rate of false positives. Similarly, for CWE-476 (NULL Pointer Dereference), CWE-200 (Information Exposure), and CWE-20 (Improper Input"}, {"title": "4.2 RQ2: Effectiveness of LLMs in SVP", "content": "To understand the capabilities of LLMs on SVP, we calculated ROUGE scores to assess the similarity between generated and ref-erence patches, CodeBLEU scores to evaluate the quality of the generated code, and cyclomatic complexity to measure the com-plexity of the code. We considered both zero-shot and few-shot learning scenarios to observe how commit description affects the models' performance.\nObservations. The cyclomatic complexity analysis, illustrated in Figure 3, shows that the LLMs consistently generated code with lower complexity compared to the original vulnerable code. This trend was observed across almost all models, both in zero-shot (Z) and few-shot (F) settings. Notably, while reducing complex-ity can lead to more maintainable code, it also suggests that the LLMs might be simplifying the code too much, potentially over-looking important details required for a robust and secure patch. The simplified patches often lacked the depth needed to address the vulnerabilities comprehensively, which is a critical concern in SVP tasks. Additionally, the models exhibited a tendency to simplify the structure of the code without fully addressing the underlying logic, as indicated by placeholders like \"...\" or \"the rest of the code remains unchanged,\" in their outputs.\nFigure 4 offers a comparative view of the ROUGE and CodeBLEU scores across different models, highlighting the varying degrees of similarity and quality in the generated patches. The scores reveal that although some LLMs perform relatively well in producing code"}, {"title": "4.3 RQ3: Impact of Code Block Abstraction Level and Length", "content": "In this section, we examine how the number of files and functions within a vulnerable code block affects the accuracy of LLMs in SVD tasks. Additionally, we analyze how the number of lines in these code blocks influences the ability to accurately identify vul-nerabilities. This analysis helps us understand how complexity and size of the code impact the performance of LLMs in detecting vulnerabilities.\n4.3.1 Abstraction Levels The abstraction level of a code block, de-fined by the number of files and functions it spans, plays a critical role in determining the effectiveness of LLMs in SVD tasks. We assess the abstraction level by examining both the number of files affected in a commit patch and the number of functions within vulnerable code blocks, which are extracted using regular expres-sions. This abstraction level-ranging from a single file with a single function to multiple files with multiple functions-can significantly influence the model's effectiveness. In this analysis, we explore how LLMs respond to these varying levels of code block abstraction,"}, {"title": "5 Discussion and Research Directions", "content": "VulnLLMEval represents a significant advancement in the evalua-tion of LLMs for automated SVD and SVP. By providing a compre-hensive framework for assessing LLM capabilities across a range of tasks, our work opens new avenues for research in this critical area of cybersecurity. This section discusses the challenges and limitations encountered during our study, providing insights into areas where further improvements are needed, and outlines future research directions.\n5.1 Challenges and Limitations\nData Bias and Model Variability. To mitigate bias introduced by mod-els that may have been inadvertently trained on similar datasets, we created a custom dataset ensuring data uniqueness. This dataset, which can be extended with other programming languages and open-source projects, was essential in reducing potential skew-ing of results. However, LLMs exhibited inconsistencies in CVE and CWE rankings due to their non-deterministic nature. Despite efforts to stabilize outputs using consistent temperature settings, variability persisted, particularly in balancing recall and precision for different vulnerability types.\nCWE Complexity and Evaluation Metrics. Certain CWE types, like CWE-416, presented significant challenges, especially in the data-flow match component of the CodeBLEU metric. This was par-ticularly evident when code blocks lacked functions or consisted mainly of non-functional elements, leading to incomplete evalua-tions. These difficulties underscore the need for functionally rich code samples in assessments, as their absence may prevent models from fully capturing the nuances of specific vulnerabilities.\nLimitations in Real-World Application. Despite our efforts, the LLMs often produced oversimplified patches that failed to address vul-nerabilities comprehensively. In addition, in SVP tasks, some LLMs tended to produce empty responses, which posed a significant challenge in generating usable solutions. Challenges in evaluating complex CWE types, such as CWE-416, highlighted the difficulty of achieving accurate code analysis using existing metrics. While our study primarily focused on benchmarking and evaluation, it opens the door to further research in areas like prompt engineering and advanced techniques such as functionality testing with Pass@K [6, 59]. These techniques, although not the focus of our current work, hold promise for significantly improving LLM performance in software vulnerability detection and patching.\n5.2 Research Directions\nFuture research in automated SVD and SVP with LLMs should pri-oritize developing models that can accurately differentiate between vulnerable and patched code. Training LLMs on general code alone is insufficient; instead, targeted fine-tuning, as illustrated by the data collection and labeling techniques in this study, is crucial for improving the precision of these models in identifying and mitigat-ing security vulnerabilities [58].\nWhile it is important to continuously adapt LLMs to new vulner-abilities, we must also address the challenge of memorization. Over time, as LLMs are exposed to new data, they tend to forget previ-ously learned information-a phenomenon known as catastrophic"}, {"title": "6 Threads to Validity", "content": "Internal Validity. The non-deterministic nature of LLMs posed a significant challenge, as their outputs can vary between runs. To mitigate this, we set the temperature to 0, ensuring more stable and consistent results across both SVD and SVP tasks. This stability allowed us to reduce output variability, though minor variations might still occur. Additionally, we avoided the potential for hu-man error by eliminating manual labeling, relying on real-world vulnerabilities from our comprehensive dataset. Our decision to use real-world data reflects the inherent complexity and intercon-nectedness of vulnerabilities, providing a more reliable evaluation. However, limitations remain due to the complexity of differentiat-ing between vulnerable and patched code, as models often struggled with this distinction."}, {"title": "7 Conclusion", "content": "This paper introduced VulnLLMEval, a comprehensive benchmark-ing framework and dataset designed to evaluate the effectiveness of LLMs in SVD and SVP. Our approach automates dataset collec-tion, focusing on real-world vulnerabilities from the Linux kernel, to ensure consistent and realistic evaluations. The results reveal that while LLMs perform well in detecting vulnerabilities, they often struggle to differentiate accurately between vulnerable and patched code, sometimes oversimplifying patches. The study also shows that LLM performance improves with increased code context but declines with added complexity, offering valuable insights for enhancing LLMs in software security tasks. Finally, we propose research directions to address these challenges, including mitigat-ing catastrophic forgetting in LLMs through meta-learning, and integrating reinforcement learning to improve dynamic decision-making in software vulnerability detection and patching."}]}