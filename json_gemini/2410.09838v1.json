{"title": "Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense", "authors": ["Rui Min", "Zeyu Qin", "Nevin L. Zhang", "Li Shen", "Minhao Cheng"], "abstract": "Backdoor attacks pose a significant threat to Deep Neural Networks (DNNs) as they allow attackers to manipulate model predictions with backdoor triggers. To address these security vulnerabilities, various backdoor purification methods have been proposed to purify compromised models. Typically, these purified models exhibit low Attack Success Rates (ASR), rendering them resistant to backdoored inputs. However, Does achieving a low ASR through current safety purification methods truly eliminate learned backdoor features from the pretraining phase? In this paper, we provide an affirmative answer to this question by thoroughly investigating the Post-Purification Robustness of current backdoor purification methods. We find that current safety purification methods are vulnerable to the rapid re-learning of backdoor behavior, even when further fine-tuning of purified models is performed using a very small number of poisoned samples. Based on this, we further propose the practical Query-based Reactivation Attack (QRA) which could effectively reactivate the backdoor by merely querying purified models. We find the failure to achieve satisfactory post-tuning robustness stems from the insufficient deviation of purified models from the backdoored model along the backdoor-connected path. To improve the post-purification robustness, we propose a straightforward tuning defense, Path-Aware Minimization (PAM), which promotes deviation along backdoor-connected paths with extra model updates. Extensive experiments demonstrate that PAM significantly improves post-purification robustness while maintaining a good clean accuracy and low ASR. Our work provides a new perspective on understanding the effectiveness of backdoor safety tuning and highlights the importance of faithfully assessing the model's safety.", "sections": [{"title": "1 Introduction", "content": "Backdoor attacks [2, 7, 16] have emerged as one of the most significant concerns [4, 5, 20, 25] in deep learning. These attacks involve the insertion of malicious backdoor triggers into the training set, which can be further exploited to manipulate the behavior of the model during the inference stage. To defend against these threats, researchers have proposed various safety tuning methods [20, 27, 32, 44, 49, 50, 55] to purify well-trained backdoored models. These methods can be easily incorporated into the existing model deployment pipeline and have demonstrated state-of-the-art effectiveness in reducing the Attack Success Rate (ASR) of backdoored models [32, 48].\nHowever, a critical question arises: does achieving a low Attack Success Rate (ASR) through current safety tuning methods genuinely indicate the complete removal of learned backdoor features from the pretraining phase? If the answer is no, this means that the adversary may still easily reactivate the implanted backdoor from the residual backdoor features lurking within the purified model, thereby exerting insidious control over the model's behavior. This represents a significant and previously unacknowledged safety concern, suggesting that current defense methods may only offer superficial safety [1]. Moreover, if an adversary can successfully re-trigger the backdoor, it raises another troubling question: how can we assess the model's robustness against such threats? This situation underscores the urgent need for a more comprehensive and faithful evaluation of the model's safety.\nIn this work, we provide an affirmative answer to these questions by thoroughly investigating the Post-Purification Robustness of state-of-the-art backdoor safety tuning methods. Specifically, we employ the Retuning Attack (RA) [36, 42] where we first retune the purified models using an extremely small number of backdoored samples and tuning epochs. Our observations reveal that current safety purification defense methods quickly reacquire backdoor behavior after just a few epochs, resulting in significantly high ASR levels. In contrast, the clean model (which does not have backdoor triggers inserted during the pretraining phase) and Exact Purification (EP)\u2014which fine-tunes models using real backdoored samples with correct labels during safety purification, maintain a low ASR even after the RA. This discrepancy suggests that existing safety tuning methods do not thoroughly eliminate the learned backdoor, creating a superficial impression of backdoor safety. Since the vulnerability revealed by the Retuning Attack (RA) relies on the use of retuned models, we further propose the more practical Query-based Reactivation Attack (QRA). This attack is capable of generating sample-specific perturbations that can trigger the backdoor in purified models, which were previously believed to have eliminated such threats, simply by querying these purified models.\nTo understand the inherent vulnerability of current safety purification methods concerning post-purification robustness, we further investigate the factors contributing to the disparity in post-purification robustness between EP and other methods. To this end, we utilize Linear Mode Connectivity (LMC) [13, 33] as a framework for analysis. We find that EP not only produces a solution with low ASR like other purification methods but also pushes the purified model further away from the backdoored model along the backdoor-connected path, resulting in a more distantly robust solution. As a result, it becomes challenging for the retuning attack to revert the EP model back to the basin with high ASR where the compromised model is located. Inspired by our findings, we propose a simple tuning defense method called Path-Aware Minimization (PAM) to enhance post-purification robustness. By using reversed backdoored samples as a proxy to measure the backdoored-connected path, PAM updates the purified model by applying gradients from a model interpolated between the purified and backdoored models. This approach helps identify a robust solution that further deviates our purified model from the backdoored model along the backdoor-connected path. Extensive experiments have demonstrated that PAM achieves improved post-purification robustness, retaining a low ASR after RA across various settings. To summarize, our contributions are:\n\u2022 Our work first offers a new perspective on understanding the effectiveness of current backdoor safety tuning methods. Instead of merely focusing on the commonly used Attack Success Rate, we investigate the Post-Purification Robustness of the purified model to enhance our comprehensive understanding of backdoor safety in deep learning models.\n\u2022 We employ the Retuning Attack by retuning purified models on backdoored samples to assess the post-purification robustness. Our primary observations reveal that current safety purification methods are vulnerable to RA, as evidenced by a rapid increase in the ASR. Furthermore, we propose the more practical Query-based Reactivation Attack, which can reactivate the implanted backdoor of purified models solely through model querying.\n\u2022 We analyze the inherent vulnerability of current safety purification methods to the RA through Linear Mode Connectivity and attribute the reason to the insufficient deviation of purified models from the backdoored model along the backdoor-connected path. Based on our analysis, we propose Path-Aware Minimization, a straightforward tuning-based defense mechanism that promotes deviation by performing extra model updates using interpolated models along the path. Extensive experiments verify the effectiveness of the PAM method."}, {"title": "2 Related Work", "content": "Backdoor Attacks. Backdoor attacks aim to manipulate the backdoored model to predict the target label on samples containing a specific backdoor trigger while behaving normally on benign samples. They can be roughly divided into two categories [48]: (1) Data-poisoning attacks: the attacker inserts"}, {"title": "3 Revealing Superficial Safety of Backdoor Defenses by Accessing\nPost-purification Robustness", "content": "While current backdoor purification methods can achieve a very low Attack Success Rate (ASR) against backdoor attacks, this does not necessarily equate to the complete elimination of inserted backdoor features. Adversaries may further exploit these residual backdoor features to reconstruct and reactivate the implanted backdoor, as discussed in Section 3.3. This is particularly important because purified models are often used in various downstream scenarios, such as customized fine-tuning [37] for critical tasks [20]. Therefore, it is crucial to provide a way to measure the robustness of purified models in defending against backdoor re-triggering, which we define as \"Post-Purification Robustness\".\nIn this section, we first introduce a simple and straightforward strategy called the Retuning Attack (RA) to assess post-purification robustness. Building on the RA, we then present a practical threat known as the Query-based Reactivation Attack (QRA), which exploits the vulnerabilities in post-purification robustness to reactivate the implanted backdoor in purified models, using only model querying. First, we will introduce the preliminaries and evaluation setup."}, {"title": "3.1 Problem Setup", "content": "Backdoor Purification. In this work, we focus on the poisoning-based attack due to its practicality and stealthiness. We denote the original training dataset as ${D}_{r}\\mathcal{C}\\left(\\mathcal{X},\\mathcal{Y}\\right)$. A few training examples $(x, y) \\in D_r$ have been transformed by attackers into poisoned examples $(x_p, y_t)$, where $x_p$ is poisoned example with inserted trigger and a target label $y_t$. Following previous works [29, 32, 39, 48, 49], only a limited amount of clean data $D_t$ are used for fine-tuning or pruning. For trigger-inversion methods [44, 50], we denote the reversed backdoored samples obtained through reversing methods as $(x_r, y) \\in D_r$. We evaluate several mainstreamed purification methods, including pruning-based defense ANP [49]; robust fine-tuning defense I-BAU [53] (referred to as BAU for short), FT-SAM [55] (referred to as SAM for short), FST [32], as well as the state-of-the-art trigger-reversing defense BTI-DBF [50] (referred to as BTI for short). BTI purifies the backdoored model by using both reversed backdoored samples ${D}_{r}$, and the clean dataset $D_t$ while the others use solely the clean dataset $D_t$. We also include exact purification (EP) that assumes that the defender has full knowledge of the exact trigger and fine-tunes the models using real backdoored samples with correct labels $(x_p, y)$.\nAttack Settings. Following [32], we evaluate four representative data-poisoning backdoors including three dirty-label attacks (BadNet [16], Blended [7], SSBA [28]), and one clean-label attack (LC [43]). All experiments are conducted on BackdoorBench [48], a widely used benchmark for backdoor learning. We employ three poisoning rates, 10%, 5%, and 1% (in Appendix) for backdoor injection and conduct experiments on three widely used image classification datasets, including CIFAR-10 [24], Tiny-ImageNet [8], and CIFAR-100 [24]. For model architectures, we following [32], and adopt the ResNet-18, ResNet-50 [17], and DenseNet-161 [18] on CIFAR-10. For CIFAR-100 and Tiny-ImageNet, we adopt pretrained ResNet-18 on ImageNet1K to obtain high clean accuracy as suggested by [32, 50]. More details about experimental settings are shown in Appendix B.\nEvaluation Metrics. Following previous backdoor works, we take two evaluation metrics, including Clean Accuracy (C-Acc) (i.e., the prediction accuracy of clean samples) and Attack Success Rate (ASR) (i.e., the prediction accuracy of poisoned samples to the target class) where a lower ASR indicates a better defense performance. We further adopt O-ASR and P-ASR metrics. The O-ASR metric represents the defense performance of original defense methods, while the P-ASR metric indicates the ASR after applying the RA or QRA."}, {"title": "3.2 Purified Models Are Vulnerable to Retuning Attack", "content": "Our objective is to investigate whether purified models with low ASR completely eliminate the inserted backdoor features. To accomplish this, it is essential to develop a method for assessing the degree to which purified models have indeed forgotten these triggers. In this section, we begin with a white-box investigation where the attacker or evaluator has access to the purified model's parameters. Here we introduce a simple tuning-based strategy named the Retuning Attack (RA) [37, 42] to conduct an initial evaluation. Specifically, we construct a dataset for model retuning, which comprises a few backdoored samples (less than 1% of backdoored samples used during the training process). To maintain C-Acc, we also include benign samples from the training set, resulting in a total RA dataset with 1000 samples. We subsequently retune the purified models using this constructed dataset through a few epochs (5 epochs in our implementation). This approach is adopted because a clean model can not be able to learn a backdoor; thus, if the purified models quickly regain ASR during the retuning process, it indicates that some residual backdoor features still exist in these purified models. Implementation details of the RA can be found in the Appendix B.2.\nAs shown in Figure 1, we observe that despite achieving very low ASR, all purification methods quickly recover backdoor ASR with Retuning Attack. Their quickly regained ASR presents a stark contrast to that of clean models and remains consistent across different datasets, model architectures, and poisoning rates. Note that the pruning method (ANP) and the fine-tuning method (FST), which achieve state-of-the-art defense performance, still exhibit vulnerability to RA, with an average recovery of approximately 82% and 85% ASR, respectively. In stark contrast, the EP method stands out as it consistently maintains a low ASR even after applying RA, demonstrating exceptional post-purification robustness. Although impractical with full knowledge of the backdoor triggers, the EP method validates the possibility of maintaining a low attack success rate to ensure post-purification robustness against RA attacks."}, {"title": "3.3 Reactivating Backdoor on Purified Models through Queries", "content": "Although our previous experiments on RA demonstrate that current purification methods insufficiently eliminate learned backdoor features, it is important to note that the success of this tuning-based method relies on the attackers' capability to change purified models' weights. This is not practical in a real-world threat model. To address this limitation, we propose Query-based Reactivation Attack (QRA), which generates sample-specific perturbations that can reactivate the backdoor using only model querying. Specifically, instead of directly retuning purified models, QRA captures the parameter changes induced by the RA process and translates them into input space as perturbations. These perturbations can then be incorporated into backdoored examples, facilitating the successful reactivation of backdoor behaviors in purified models."}, {"title": "4 Investigating and Mitigating Superficial Safety", "content": "While our previous evaluations indicate that only the EP model demonstrates exceptional post-purification robustness compared to current backdoor safety tuning methods, the factors contributing to the effectiveness of EP remain unclear. Motivated by prior studies examining fine-tuned models [13, 14, 33], we propose to investigate this intriguing phenomenon from the perspective of the loss landscape using Linear Mode Connectivity (LMC)."}, {"title": "4.1 Investigating the Superficial Safety through Linear Mode Connectivity", "content": "While our previous evaluations indicate that only the EP model demonstrates exceptional post-purification robustness compared to current backdoor safety tuning methods, the factors contributing to the effectiveness of EP remain unclear. Motivated by prior studies examining fine-tuned models [13, 14, 33], we propose to investigate this intriguing phenomenon from the perspective of the loss landscape using Linear Mode Connectivity (LMC).\nFollowing [13, 33], let $E(W; D_t)$ represent the testing error of a model $f(W; x)$ evaluated on a dataset $D_t$. For $D_t$, we use backdoor testing samples. $E_t(W_0, W_1; D_t) = E((1 - t)W_0 + tW_1; D_t)$ for $t \\in [0,1]$ is defined as the error path of model created by linear interpolation between the $f(W_0; x)$ and $f(W_1; x)$. We also refer to it as the backdoor-connected path. Here we denote the $f(W_0;)$ as backdoored model and $f (W_1;)$ as the purified model. We show the LMC results of the backdoor error in Figure 4 and 5. For each attack setting, we report the average results among backdoor attacks. More results on other datasets and models are shown in Appendix C.2.\nThe backdoored model and purified models reside in separate loss basins, linked by a backdoor-connected path. We present the results of LMC between purified and backdoored models in Figure 4. It is clear from the results that all purified models exhibit significant error barriers along the backdoor-connected path to backdoored model. This indicates that backdoored and purified models reside in different loss basins. Additionally, we conduct LMC between purified models with EP and with other defense techniques, as depicted in Figure 5. We observe a consistently high error without barriers, which indicates that these purified models reside within the same loss basin. Based on these two findings, we conclude that backdoored and purified models reside in two distinct loss basins connected through a backdoor-connected path.\nEP deviates purified models from the backdoored model along the backdoor-connected path, resulting in a more distantly robust solution. Although introducing a high loss barrier, we observe notable distinctions between the LMC of the EP model (red solid line) and purified models (dotted lines). We observe a stable high backdoor error along the backdoor-connected path of EP until $t < 0.2$, where the interpolated model parameter W has over 80% weight from the backdoored model. In"}, {"title": "4.2 Enhancing Post-Purification Robustness Through Path-Aware Minimization", "content": "Motivated by our analysis, we propose a simple tuning defense method called Path-Aware Minimization (PAM), which aims to enhance post-purification robustness by promoting more deviation from the backdoored model along the backdoor-connected path like the EP method.\nSince there are no real backdoor samples $x_p$ available, we employ the synthetic backdoored samples $x_r$ from the trigger-reversing method BTI [50] as a substitute to get the backdoor-connected path. Although BTI has a similar LMC path curve with the EP model in Figure 5, as we have discussed, tuning solely with $x$ would lead to finding a non-robust solution with low ASR.\nTo avoid converging to such a solution, we propose utilizing the gradients of an interpolated model $W_a$ between $W_0$ and $W$ to update the current solution $W$. As illustrated in Figure 4, the interpolated model, which lies between $W_0$ and $W$, exhibits a higher ASR compared to $W$. By leveraging the gradients from the interpolated model, we can perform additional updates on the $W$ which prevents premature convergence towards local minima and results in a solution that deviates from the backdoored model along this path. Specifically, for $W$, we first take a path-aware step $P{\\Vert W}_{a}{\\Vert }_2$ ($W_a = W_0 - W$) towards to $W_0$ and obtain the interpolated model $W + P \\frac{\\left|{\\mid W}_{a}{\\mid }\\right|_2}{\\left|{\\mid W}{\\mid }\\right|_2}$. Then we compute its gradient on x to update W. We formulate our objective function as follows:\n$\\underset{W}{\\min }\\left\\{E_{\\left(x,y\\right)\\sim D_r\\cup D_t}\\left[L\\left(f\\left(W+P\\frac{{\\Vert W}_{a}{\\Vert }_2}{\\Vert W\\Vert_2};x\\right),y\\right)\\right]\\right\\}, \\text{s.t.}\\ {W}_{a}={W}_{0}-{W},$\n${{\\Vert W}_{a}\\Vert }_2$"}, {"content": "where p represents the size of the path-aware step. Typically, a larger p indicates a larger step towards the backdoored model $W_0$ along our backdoor-connected path and also allows us to obtain a larger gradient update for $W$, which results in more deviation from the backdoored model along the backdoor-connected path. The detailed algorithm is summarized in the Algorithm 1."}, {"title": "5 Conclusions and Limitations", "content": "In this paper, we seek to address the following question: Do current backdoor safety tuning methods genuinely achieve reliable backdoor safety by merely relying on reduced Attack Success Rates? To investigate this issue, we first employ the Retuning Attack to evaluate the post-purification robustness of purified models. Our primary experiments reveal a significant finding: existing backdoor purification methods consistently exhibit an increased ASR when subjected to the RA, highlighting the superficial safety of these approaches. Building on this insight, we propose a practical Query-based Reactivation Attack, which enables attackers to re-trigger the backdoor from purified models solely through querying. We conduct a deeper analysis of the inherent vulnerabilities against RA using Linear Mode Connectivity, attributing these vulnerabilities to the insufficient deviation of purified models from the backdoored model along the backdoor-connected path. Inspired by our analysis, we introduce a simple tuning defense method, Path-Aware Minimization, which actively promotes deviation from the backdoored model through additional model updates along the interpolated path. Extensive experiments demonstrate the effectiveness of PAM, surpassing existing purification techniques in terms of post-purification robustness.\nThis study represents an initial attempt to evaluate post-purification robustness via RA. While we propose the practical QRA method, future work is essential to develop more efficient evaluation techniques that can faithfully assess post-purification robustness. The need for such evaluations is critical, as they ensure that the perceived safety of purified models is not merely superficial. Additionally, we recognize the significant potential for enhancing QRA in the context of transfer attacks, which we aim to explore in future research. Furthermore, our paper primarily focuses on the most common types of backdoor attacks and defenses; we plan to broaden our research by incorporating additional backdoor attack strategies and safety tuning methods applicable to generative models, such as LLMs and diffusion models [1, 20, 37, 40], in future work."}, {"title": "A Social Impact", "content": "The prevalence of Deep Neural Networks (DNNs) in modern society relies heavily on massive amounts of training data from diverse sources. However, in the absence of rigorous monitoring mechanisms, these data resources become susceptible to malicious manipulation, resulting in unforeseen and potentially harmful consequences. Among the various concerns associated with the training dataset, backdoor attacks pose a significant threat. These attacks can manipulate the behavior of a well-trained model by poisoning the training set with backdoored samples, often at a low cost and without requiring complete control over the training process. While existing defense methods have demonstrated effective backdoor purification by achieving low Attack Success Rates (ASR), they still exhibit vulnerabilities that allow adversaries to reactivate the injected backdoor behavior easily. In our work, instead of solely focusing on backdoor ASR, we investigate the effectiveness of modern purification techniques from the perspective of post-purification robustness. We aim to enhance the post-purification robustness of backdoor defense, mitigating the potential for malicious manipulation of deployed models even after backdoor purification. In sum, our work hopes to move an initial step towards improving post-purification robustness while also contributing to another aspect of understanding and enhancing machine learning security."}, {"title": "B Experimental Settings", "content": "In this section, we provide detailed information about the experimental settings used in our evaluations. This includes the dataset, training details, and the selection of hyperparameters. All experiments were conducted using 4 NVIDIA 3090 GPUs. We ran all experiments 3 times and averaged all results over 3 random seeds."}, {"title": "B.1 Datasets and Models", "content": "We follow previous studies [26, 32, 48, 49] on backdoor learning, and conduct our experiments on three widely used datasets including CIFAR-10, CIFAR-100, and Tiny-ImageNet.\n\u2022 CIFAR-10 is a widely used dataset in the backdoor literature, comprising images with a resolution of 32 \u00d7 32 and 10 categories. For backdoor training, we utilize the ResNet-18 model for main evaluation, a commonly used architecture in previous studies [32, 50, 55]. Additionally, we explore other architectures, including the ResNet-50 and DenseNet-161.\n\u2022 CIFAR-100 and Tiny-ImageNet are two large-scale datasets compared to the CIFAR-10, which include 100 and 200 different categories, respectively. Similar to previous work [32, 50], we utilize the pretrained ResNet-18 on ImageNet-1K provided by PyTorch to implement backdoor attacks since directly training from scratch would result in an inferior model performance on C-Acc, hence is not practical in real-world scenarios."}, {"title": "B.2 Implementation Details", "content": "Attack Configurations We implement 4 representative poisoning-based attacks and generally follow the implementation from the BackdoorBench\u00b9. For the BadNet, we utilize the 3 \u00d7 3 checkerboard patch as triggers and choose the lower right corner of the image for backdoor injection by default; for the Blended, we adopt the Gaussian Noise as the backdoor trigger. We set the blend ratio to 0.1 for backdoor training and increase the blend ratio to 0.2 during the inference phase; for SSBA and LC, we follow the original implementation from BackdoorBench without making modifications. In our implementation, we set the default poisoning rate to 5%, which is commonly used in previous studies [32, 55] and additionally explore various poisoning rates including both 1% and 10%. Note that we do not adopt a lower poisoning rate since most of the methods suffer from effectively removing backdoor effects when the poisoning rate is extremely low as indicated by [32]. For all backdoor attacks, the target label is set to be 0 by default.\nFor CIFAR-10, we adopt an initial learning rate of 0.1 to train all the backdoored models for 100 epochs. For both the CIFAR-100 and Tiny-ImageNet, we utilize pretrained backbones and initialize the classifiers with appropriate class numbers. We adopt a smaller learning rate of 0.001 and fine-tune"}]}