{"title": "Getting By Goal Misgeneralization With a Little Help From a Mentor", "authors": ["Tu Trinh", "Nguyen X. Khanh", "Mohamad H. Danesh", "Benjamin Plaut"], "abstract": "While reinforcement learning (RL) agents often perform well during training, they can struggle with distribution shift in real-world deployments. One particularly severe risk of distribution shift is goal misgeneralization, where the agent learns a proxy goal that coincides with the true goal during training but not during deployment. In this paper, we explore whether allowing an agent to ask for help from a supervisor in unfamiliar situations can mitigate this issue. We focus on agents trained with PPO in the CoinRun environment, a setting known to exhibit goal misgeneralization. We evaluate multiple methods for determining when the agent should request help and find that asking for help consistently improves performance. However, we also find that methods based on the agent's internal state fail to proactively request help, instead waiting until mistakes have already occurred. Further investigation suggests that the agent's internal state does not represent the coin at all, highlighting the importance of learning nuanced representations, the risks of ignoring everything not immediately relevant to reward, and the necessity of developing ask-for-help strategies tailored to the agent's training algorithm.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has successfully enabled artificial intelligence (AI) agents to achieve human-level performance in various applications [9; 11; 16]. However, these applications remain confined to artificial environments [2; 19; 18] or low-stakes tasks, such as folding bedding [21], sorting books [7], or redirecting customers [14]. Deployments in higher-stakes settings such as chemistry labs [4], construction [5], or hospital work [1] are much more limited. One reason is that RL agents often behave unpredictably under distribution shift [13; 6], i.e., when the deployment environment differs significantly from the training environment. Distribution shift is often inevitable due to the real world having dynamics not present in controlled training settings, such interfering humans and agents, materials incompatible with robot appendages, mechanical wear and tear, and more. Some solutions to this include increasing the diversity of training environments or training the agent directly in the real-world, but these can be both costly and risky.\nWe study a different approach: accept that distribution shift is inevitable but train agents to identify unfamiliar situations and subsequently act cautiously, by refusing to take any action and instead re-"}, {"title": "2 Experimental Setup", "content": "2.1 Training and Testing Environments\nOur experiments were conducted using Open-\nAl's procgen package [3] and a package built\non top of it called procgenAISC [8]. Procgen-\nAISC introduces additional difficulty settings\nfor some procgen games that change the proce-\ndural generation distribution in significant qual-\nitative ways that lead to goal misgeneralization.\nWe focus on the flagship environment pair used\nto demonstrate goal misgeneralization: coinrun\nfrom [3] and coinrun_aisc from [8]. In both en-\nvironment distributions, the agent tries to reach\na coin, receiving a reward of 10. All other out-\ncomes such as colliding with a monster or run-\nning out of time receive 0 reward.\n2.2 The Cost of Asking For Help\nIn a real-world application, while asking for\nhelp is preferable to harmful behavior, it still incurs a cost: the supervisor's time and effort. Therefore, the agent must balance the cost of harmful behavior with the cost of requesting help. However, since the relative magnitudes of these costs vary greatly across individuals, systems, and applications, we did not incorporate this cost of asking for help into the agent's reward function; rather we only study agent performance as a function of help request frequency. Specifically, we look at the ask-for-help percentage (AFHP), the proportion of timesteps in a run that the agent asks for help.\nEach method we test (introduced in Sections 3 and 4) uses a different uncertainty metric to determine if it should ask for help. Consequently, each method results in the agent asking for help at different times and a different number of times, yielding a different AFHP for each run. Each run also yields a final reward of either 10 or 0, depending on whether the agent reached the coin. Thus for each method, and for each method's threshold, we compute the realized AFHP and average reward across all runs. Since choosing a specific threshold is arbitrary, we look at each method's performance across all AFHPs. If one method outperforms another for any fixed AFHP, we can conclude that the former is superior. More generally, one could compute the area under the curve of performance on the y-axis and AFHP on the x-axis, where a larger area indicates that the agent was able to get the coin more often compared to other methods which asked for help at the same rate.\n2.3 Experimentation Pipeline\nThe training and testing pipeline is as follows. We first train an agent using PPO on coinrun. The resulting stochastic policy samples an action from its outputted distribution for the agent to take. We"}, {"title": "3 Using Agent's Action Distribution", "content": "We started by using the weak agent's action distribution as a signal for when to ask for help. We hypothesized that if the agent is in an unfamiliar situation, it would be unsure of what is best to do, which would lead to the action distribution having high uncertainty. We studied five different policies that ask for help based on different measures of uncertainty: (1) max probability: if the probability of the agent's highest-probability action is less than some threshold; (2) max logit: if the highest logit in the agent's action distribution is less than some threshold; (3) sampled probability: if the probability of the agent's sampled (chosen) action is less than some threshold; (4) sampled logit: if the logit corresponding to the agent's sampled (chosen) action is less than some threshold; and (5) entropy: if the entropy of the agent's action distribution is more than some threshold.\nThe thresholds for the methods above are created after running the weak agent on coinrun. Using max probability as an example: for every run, every timestep, we recorded the max probability of the agent's action distribution. Then we computed percentiles of these values to use as thresholds. Thus, in practice, the weak agent might ask for help when its current max probability in coinrun_aisc is less than, for example, the 20th percentile of max probabilities seen in coinrun. As shown in Figure 2, a higher percentile corresponds to more frequent help requests each pth percentile in fact results in very close to AFHP = p%.\nFigure 3 shows the results of the five action-based methods in coinrun_aisc. The horizontal lines show the weak agent's performance in coinrun_aisc (dashed gray) and expert agent's performance in coinrun_aisc (dotted brown). The action-based ask-for-help methods always outperform the weak agent's no-help performance in the test environment, showing that asking for help based on action distribution uncertainty can mitigate goal misgeneralization, even when the agent asks for help very rarely. There was no significant performance difference between these five methods.\nThorough examination of the agent's runs in coinrun_aisc revealed that the agent most often asks for help when it reaches the far right wall, the location of the coin during training. One interpretation is that the agent realizes there is no coin to be found and that the level is not ending with high reward as expected. Only then does it repeatedly ask for help. It actually rarely ever asks for help when it encounters the coin in the middle, the actual stark difference with the training environment. We argue that this is not ideal: an effective ask-for-help strategy should be able to recognize anomalies immediately, not wait for the agent to make a mistake before asking, which can be risky, costly, and essentially defeats the purpose of being able to ask for help in the first place."}, {"title": "4 Using Anomalous Observations", "content": "This realization motivated us to try observation-based methods instead of action-based methods. Simply put, if we desire the agent to recognize an anomalous feature of the environment immediately when it appears, then we should work with the environment observations, whether as raw images or latent representations. In order to determine if an observation warranted asking for help, we used the state-of-the-art anomaly detection method Deep-SVDD [15]. Details on how the Deep-SVDD model was used can be found in Appendix B. We tested two different observation input types to the Deep-SVDD model: (1) raw: look at the raw observation image to determine anomalies; and (2) latent: look at the weak agent's policy's latent representation of the observation (i.e. the environment from the agent's \u201cpoint-of-view\u201d), taken from its penultimate layer, to determine anomalies.\nWe hypothesized that the Deep-SVDD model would give higher anomaly scores to observations with the coin in the middle which never appear during training, thus allowing the agent to ask for help as soon as it recognizes the misplaced coin. Like with the action-based methods above, we use percentiles on the anomaly scores of observations seen in training as thresholds for Deep-SVDD.\nFigure 5 shows that the observation-based methods outperform the action-based methods (and trivially the no-help baseline). Using latent observation representations has a clear benefit, but using raw observations only barely outperforms the action-based methods. One possible explanation is that it is difficult to detect different coin locations in a raw image since the coin takes up very few pixels compared to other aspects, such as background pattern, ground colors, monsters, and more.\nUnfortunately, we further discovered that neither the action-based nor the observation-based methods outperform a very important baseline: the random baseline, defined by the agent asking for help with some fixed probability at every timestep, independent of the current observation. Figure 6 shows that the random baseline significantly outperforms our action-based methods while performing approximately equally well to our observation-based methods.\nWe suspect that the random baseline results in the agent asking for help more in the beginning and middle of the level instead of waiting to reach the right wall at the end or even waiting to approach the coin. This means that the agent has a higher chance of receiving expert guidance early on, before"}, {"title": "5 Ask-For-Help Skylines and the Importance of Learned Representations", "content": "Our discovery that the random baseline induces high-performing, proactive ask-for-help behavior motivated us to ask if there was still much room for improvement for an effective ask-for-help strategy. To answer this, we built skylines: ask-for-help methods trained directly on the test environment. The skylines are meta-policies with an action space of size 2 (use the weak agent's action or use the expert agent's action) that try to learn the best cooperation dynamic between the two agents. We emphasize that these skylines are purely theoretical and not viable agent policies, as access to the test distribution is unrealistic for most applications. This is in contrast to our previous methods, which use no knowledge of the test environment.\nWe tested two different skyline architectures, both trained with PPO: (1) S_obs: this skyline takes in the raw environment observation image as input to its (meta) policy; and (2) S_weak_feat: this skyline takes in the weak agent policy's latent observation representation as input. Figure 7 shows the two different skyline performances on coinrun_aisc. We can see that they profoundly outperform all other ask-for-help methods, including the random baseline and even the expert agent in its own training environment. These results answered definitively that there was significant room for improvement before our methods could reach optimal performance. Yet the question still remained: why couldn't the previous methods recognize that having a coin in the middle of the level was an anomaly?\nA qualitative analysis revealed key insights into the skylines' behavior. S_obs asks for help consistently when the coin appears in the middle of the level. On the other hand, it appears that S_weak_feat's ask-for-help policy does not depend at all on the coin's location! Rather, it tends to asks for help near ledges to determine if the agent should jump (and if so, how high or far out to jump) or if it should continue walking. (Figure 1 (right) is an example of the agent jumping over the coin on a ledge if it doesn't ask for help.) Indeed, we found that S_obs asks for help, on average, 11.4% of the time when the coin is present and 0% of the time when it's absent. S_weak_feat, meanwhile, only asks for help 6.1% of the time when the coin is present and 13.4% of the time when it's absent. We confirmed that a sizable amount of the skyline training data has the coin on a ledge. That means asking for help near a ledge is an excellent \"coin-agnostic\" strategy and one that S_weak_feat ultimately decided was the best if the agent doesn't have any concept of a coin, it would associate ledges with high rewards and seek assistance near them to have the expert lead it to the best landing.\nThis analysis suggests an explanation for why methods based on the agent's internal state (action-based methods and Deep-SVDD-latent) are ineffective: these representations do not capture the coin's existence. Without a comprehensive environment representation, any unsupervised ask-for-help strategy on top of an existing trained agent will be fundamentally limited. With our PPO agents, this suggests that PPO cannot emulate true intelligent agent behavior because it fails to capture elements of the world which are not immediately relevant to the reward. It also suggests that a good ask-for-help strategy must be tailored to the specific training algorithm used."}, {"title": "6 Conclusion", "content": "In this paper we demonstrated that asking for help from a supervisor can mitigate goal misgeneralization in RL agents. However, current methods struggle to proactively detect anomalies, often asking for help only after mistakes have been made. Future work should focus on improving agents' internal environment representations and thus early anomaly detection abilities to fully realize the potential of ask-for-help strategies. Additional areas for future work are discussed in Appendix D."}, {"title": "A Social Impact Statement", "content": "Our work focuses on improving the safety and reliability of reinforcement learning agents by enabling them to recognize when they are in unfamiliar situations and act cautiously in response by requesting supervisor assistance. We hope that our work contributes to reducing risks induced by distribution shift and goal misgeneralization, especially in high-stakes real-world applications such as healthcare or assistive robotics."}, {"title": "B Deep-SVDD Model", "content": "The Deep-SVDD model as built by [15] is trained to form the tightest hypersphere possible around the latent representations of in-distribution datapoints. For our purposes, we trained a Deep-SVDD model on observations collected from running the weak agent in the training environment, coinrun. During testing, the Deep-SVDD model assigns an anomaly score to its input datapoint based on how far its latent representation is from the center of the hypersphere. This score is then compared with a provided threshold to determine if the datapoint is far enough away from the cluster of in-distribution datapoints to be considered anomalous. That means in practice, we feed in the agent's observation from when it's running in coinrun_aisc into the Deep-SVDD model at each timestep and receive an anomaly score. If this score is above the given threshold, the agent will ask for help. One great feature of the Deep-SVDD model is that it is lightweight and executes rapidly, making it very suitable for assessing every timestep if the agent should ask for help."}, {"title": "C Related Work", "content": "Several studies have explored RL agents asking for help to improve performance in challenging environments. For instance, Nguyen et al. [12] introduced a hierarchical ask-for-help framework where agents attempt to learn more about the surroundings of objects they interact with to decompose their tasks into subtasks. Liu et al. [10] proposed the asking-for-knowledge (AFK) agent, which generates natural-language questions for agents to discover new knowledge about their environment. Both papers approach the ask-for-help problem by enabling the agent to decrease its uncertainty by increasing its environmental knowledge.\nXie et al. [22] developed a proactive strategy for RL agents to ask for help with states or actions they have predicted to be potentially irreversible. Vandenhof [20] treated asking for help as an additional, costly action in the agent's action space and proposed a deep Q-learning algorithm to support it. These methods involve additionally training the RL agent in some way, whether to be adept at detecting unfamiliar situations or balancing exploration, exploitation, and asking for help.\nOur approach differs from previous work in two key ways: (1) we aim to study unsupervised methods of asking for help that can potentially be applied on top of a pretrained RL agent, and (2) to the best of our knowledge, we are the first to study asking for help in a goal misgeneralization setting."}, {"title": "D Future Work", "content": "Additional areas where future work can help improve our methods or address some limitations include the following.\n1.  Currently our weak agent only asks for help one timestep at a time. Future work can investigate how the agent can best ask for help for longer periods of time, which can alleviate the cost of switching control back and forth between the supervisor and the agent, a more realistic model of real-world interactions.\n2.  Our weak agent currently uses the action provided by the expert to replace its own action when it requests help. However, it would be better if in addition to this, the weak agent is also able to incorporate this knowledge into its own policy, leading to a continuous learning paradigm. This could eventually decrease the necessity of asking for help and supervisor resources as the agent grows more experienced."}]}