{"title": "META-LEARNING APPROACHES FOR IMPROVING DETECTION OF UNSEEN SPEECH DEEPFAKES", "authors": ["Ivan Kukanov", "Janne Laakkonen", "Tomi Kinnunen", "Ville Hautam\u00e4ki"], "abstract": "Current speech deepfake detection approaches perform satisfactorily against known adversaries; however, generalization to unseen attacks remains an open challenge. The proliferation of speech deepfakes on social media underscores the need for systems that can generalize to unseen attacks not observed during training. We address this problem from the perspective of meta-learning, aiming to learn attack-invariant features to adapt to unseen attacks with very few samples available. This approach is promising since generating of a high-scale training dataset is often expensive or infeasible. Our experiments demonstrated an improvement in the Equal Error Rate (EER) from 21.67% to 10.42% on the InTheWild dataset, using just 96 samples from the unseen dataset. Continuous few-shot adaptation ensures that the system remains up-to-date.", "sections": [{"title": "1. INTRODUCTION", "content": "Speech deepfakes are artificial or manipulated (fake) speech samples generated using deep learning techniques. Although the underlying methodologies, voice conversion (VC) and text-to-speech (TTS), serve the needs for many useful applications ranging from smart assistants and audiobooks to public announcement systems and games, the term 'deepfake' itself tends to be associated with exploitative use cases, including spoofing voice biometric systems [1] and telephony fraud [2, 3].\nBy taking a source speaker's utterance as an input, VC aims at converting the utterance to sound like the target speaker, but with the original content. TTS, in turn, generates speech from a text input. Concerning voice characteristics, the early TTS systems were developed for a small and closed population of 'stock speakers'. Recent advances in neural waveform models [4] and deep speaker embedding [5] have enabled conditional speech generation by using target speaker embeddings as an additional input. This has greatly expanded the flexibility of adapting TTS, in principle, to anyone's voice using limited training data from that person. These few-shot voice cloning approaches, such as VALL-E [6] and AudioBox [7], produce high-quality artificial speech in the targeted person's voice.\nWith the imminent threat posed by deepfakes, especially in telephony, teleconferencing, and social media domains, there is a timely need to develop solutions for detecting speech deepfakes reliably. Despite the recent, popularized 'deepfake' terminology, this is not a new task for the speech community. On the contrary, speech deepfakes have received considerable attention throughout the past decade in the context of spoofing voice biometrics (automatic speaker verification or ASV) using artificial or modified inputs. The attack vectors of biometric systems have been known for a long time [9]. For speech, analysis of vulnerabilities and development of deepfake detectors have been spearheaded by the ASVspoof initiative [10], a challenge series that provides common evaluation data, protocols, and performance metrics for comparing alternative speech deepfake detectors. In this context, a classifier aimed at detecting speech deepfakes is known as a (spoofing) countermeasure (CM). The two subsystems are assumed to produce (class-conditionally) independent decisions, leading to a cascaded spoofing-robust classifier architecture. An associated performance metric [11] measures the detection cost of the combined system.\nTo sum up, deepfake detector is a spoofing countermeasure. While originating from different contexts, they are the same task \u2013 differentiating spoofed (deepfake) from real (bonafide) speech utterances.\nThe general goal of machine learning is that learned model parameters work well for unseen data [12]. By \"working well\" we mean that the performance on the unseen data is similar to the validation set. The main assumption behind the generalization goal is that essentially all samples in the training set, validation set, and the unseen data are all independent and identically distributed (iid) samples from the same original dataset. In the case where we cannot guarantee that unseen samples are samples from the same distribution, we talk about the domain generalization task. In the first glance, the task appears not to be possible, how one can train a model for data it cannot observe? A slightly easier variant of the domain generalization problem is the so-called few-shot learning task [13], where we allow few samples to be observed from the unknown dataset, such as example audio files of the new speech deepfake attack type. According to [14] three broad categories of approaches are currently being investigated for the domain generalization: 1) data manipulation, such as different data augmentations, 2) tools from representation learning, such as disentanglement learning and 3) learning based methods. Data manipulation methods, such as data augmentation, adversarial data augmentation and different data generation methods aim to increase the training dataset size and at the same time hope that generated samples match the unseen data. Representation learning, on the other hand, aims to design a model where the latent space is disentangled leading to better generalization performance [15].\nLearning-based methods [12, 13], on the other hand, approach the domain generalization task from the point of view of explicitly learning a model that generalizes well. We start with an assumption that data cannot completely change from the validation set to the unseen data, there has to be some invariant that once found can be utilized to generalize to the unseen data. For example, we assume that even in the unseen data there will be bonafide samples and also samples from some, possibly unknown fake attacks. The simplest learning-based method is to use domain adversarial training [16], where one trains two classifier heads for the same backbone, one for the bonafide/deepfake classifier, and the other that will classify attack types. The learning goal is to minimize loss on the bonafide/deepfake head while maximizing loss on the attack-type head [17]. The goal of the learning procedure is to force the backbone to be attack-type agnostic, at least to the known attack types.\nIn this work, we utilize meta-learning [12] based domain generalization schemes. In meta-learning the aim is to explicitly minimize the generalization loss by the attack type for testing and updating the parameters based on the loss on the few-shots of the unseen cases. The final result is, hopefully, a model that generalizes well to an unknown attack type. We are not the first to use meta-learning in speech deepfake detection task. In [18], the authors used meta-learning, but not in the domain generalization context. We notice that observing just a few examples, less than a hundred, from the unseen attack type can significantly improve speech deepfake detection performance. To summarize questions we study in this work are:\n\u2022 Two meta-learning approaches are explored to address the generalization challenge by adapting to unseen fake speech data with limited samples setup.\n\u2022 Explored a minimal number of samples needed for observing a trend in performance improvements as in-domain as well as out of domain datasets.\n\u2022 Explored the cross-corpus generalization improvements, which makes meta-learning as an option for domain adaptation with limited training samples from unseen fake attacks with no access to TTS/VC generator."}, {"title": "2. RELATED WORK", "content": "Most of the speech deepfake research has focused on training and testing on the same corpus. While sometimes test attacks might not be the same as in the training portion, the conditions are similar enough that reported EERs are now less than 1%. Cross-corpus performance was systematically evaluated in studies such as [19, 20] with the results that countermeasures do not generalize well to unseen corpora. Regularization is one of the approaches that have been tried to solve cross-corpus generalization problem, such as in [21] where authors used large margin cosine loss and frequency masking augmentation. Another approach was taken in [22], where authors finetuned model with unseen data but used continual learning to ensure that the model still worked on the original corpus. The idea is that parameters should not diverge too far from the base model and thus still work in the original corpus. Main conceptual difference to the proposed is that our aim is to use minimal amount of unseen data in the adaptation. In [23], the authors approached generalization by combining three corpora and performing 5-fold cross-validation and using double frontend LCNN model. One approach to improving generalization is to train a massively large SSL model and then finetune it with some speech deepfake corpus, with the idea that such a model will then generalize to unseen corpus [24, 25]. In the present work we take a large SSL model as a basis, but instead of finetuning the whole network we finetune only a small part using meta-learning based strategy."}, {"title": "3. META-LEARNING", "content": "The basic idea of meta-learning [12] is that given multiple different tasks, it is beneficial to learn a general set of parameters that are shared by all the tasks. After that, learning (adapting) an individual task becomes easier. The whole dataset is then divided into multiple sets, such as $D = \\{T_t\\}_{t=1}^f$ is the training set for the tasks of interest and $x_i$ is the input data and $y_i$ is the corresponding label. Each task $T_t$ consists of support $S = \\{S_i\\}_{i=1}^N$ and query $Q = \\{Q_i\\}_{i=1}^N$ sets with N-classes, where $S_i = \\{(x_j, y_j)\\}_{j=1}^K$ is K-shot samples from class i.\nThe meta-learning process comprises two phases: meta-training and meta-testing. In meta-training, the model encounters multiple tasks $T_t$, each divided into a support set $S_t$ and a query set $Q_t$. The model adapts to each task using the support set through a few gradient updates; then it is evaluated on the query set. The goal is to optimize the model's initial parameters on the support sets to perform well on the corresponding query sets. During meta-testing, the model's generalization ability is assessed on new, unseen tasks, similarly divided into support and query sets. This time, the model adapts to these new tasks using few-shots from the support set; and it is evaluated on the query set representing the whole evaluation set. The tasks alternation, during meta-training, learns the ability to adapt to new tasks quickly."}, {"title": "4. DOMAIN ADAPTATION IN SPEECH DEEPFAKES", "content": "Meta-learning, or \"learning to learn,\" has emerged as a powerful paradigm for enabling models to learn new tasks quickly with minimal data [12]. Meta-learning is valuable in fields where data is scarce or expensive to obtain. Among the plethora of meta-learning algorithms, two have shown notable promise in current research: prototypical networks (ProtoNet) [26], model-agnostic meta-learning (ProtoMAML) [13]. This section provides an overview of these methods, highlighting their foundational concepts."}, {"title": "4.1. Prototypical network", "content": "Prototypical network (ProtoNet) [26] offers an effective approach to few-shot learning tasks. It operates in the principle of learning a metric space in which classification can be performed by computing distances to prototype representations of each class. This method excels in scenarios where the goal is to rapidly adapt to new tasks with a limited number of examples [26].\nPrototypical network computes a prototype centroid vector $v_i$ for every class i based on samples $x_j$ from a support set $S_i$:\n$V_i = \\frac{1}{\\mid S_i \\mid} \\sum_{x_j \\in S_i} f_\\theta(x_j),$\nwhere N-way and K-shot indexes are i = 1, N (class index) and j = 1, K (sample index). Each query example $x^*$ is assigned to the"}, {"title": "4.2. ProtoMAML", "content": "The hybrid approach ProtoMAML integrates the prototypical concept into the MAML structure to improve the adaptation step in a few shots [13]. Compared to non-parametric ProtoNet, ProtoMAML is optimization-based, i.e. updates the network parameters during the adaptation step to a new task. ProtoMAML aims to optimize a model's parameters such that a small number of gradient updates will lead to significant improvement on a new task.\nProtoMAML networks are interpreted as ProtoNet with a linear classifier applied to a learned representations $f_\\theta(x)$. In ProtoMAML [13], the squared Euclidean distance from (2)\n$-||f_\\theta(x) - v_i||^2 = 2v_i^T f_\\theta(x) \u2013 ||v_i||^2 + const,$\nwhere we denote $W_{i,.}= 2v_i$ and $b_i= -||v_i||^2$, which gives trainable output linear layer $W f_\\theta(x) + b$."}, {"title": "5. EXPERIMENTAL SETUP", "content": "Training Corpus: The ASVspoof2019 LA [27] dataset is used to train all systems in this study, it has 6 attacks in training/validation"}, {"title": "5.2. Training Strategy", "content": "We explore the top performing fake speech detection model derived from ASVspoof community [31, 32]. It is based on SSL front-end and graph neural network back-end.\nSelf-supervised learning (SSL) model. The system Wav2Vec-AASIST in Table 1 is trained using Wav2Vec 2.0 XLSR-53 with 1024 output embedding dimension coupled with an integrated spectro-temporal graph attention network (AASIST) [31] as a back-end. In the Wav2Vec-AASIST* scenario, the SSL model is frozen during training, only parameters of the graph attention are trained. We train the Wav2Vec-AASIST and the Wav2Vec-AASIST* model using the same setup. We use Adam [33] optimizer with an initial learning rate of 1 \u00d7 10-6. Negative log-likelihood loss is optimized with 2 class logSoftmax outputs for bonafide and spoof. Training stops using early stopping if there is no improvement in validation performance for more than 15 epochs. The best performing model on the validation set is used for testing.\nMeta-Learning: ProtoNet and ProtoMAML. In this experiment we focus on few-shot classification task, where the classes in the training and test sets are different (or partially unseen) as in the case of ASVspoof19 dataset which has different type of attacks in training and evaluation splits. In meta-training phase, we randomly sample 3 classes out of 7 classes (A01-A06 and bonafide) for each task. However, at meta-test time, we classify between spoof and bonafide.\nThe Wav2Vec-AASIST* network backbone was used in meta-learning. The Wav2Vec 2.0 checkpoint is used to initialize front-end weights and it is kept frozen for all meta-learning experiments. The graph attention network back-end (AASIST) is randomly initialized with values using a Xavier normal distribution [34]. Only AASIST parameters are optimized during training. The training was done in few-shot manner with 3-class and 5-shot setting. That is, every iteration task sampler randomly picks 5 samples of 3-classes for support and query sets from ASVspoof2019 training set. For each update step, we train on support batch and validate on query batch. Typically, it is advised to keep the number of shots during training equal to the number intended for adaptation/evaluation [26]. However, we explore the number of shots as a hyper parameter later in the adaptation section to obtain the best-performing model.\nThe ProtoNet learning strategy was applied to Wav2Vec-"}, {"title": "5.3. Few-shot Adaptation and Evaluation", "content": "To evaluate the adaptation capability, we adapt ProtoNet and ProtoMAML to new deepfakes from unseen domain. We randomly select k speech samples from spoof and bonafide classes to extract the prototypes and assess the model's equal error rate (EER) performance metric using the remaining samples of the evaluation set. Meta-testing process is similar to a meta-training, the support set is used for adaptation on unseen domain and the remaining of the evaluation dataset is a query set.\nWe repeat random sampling of support set for 9 times. Each time we run adaptation followed by an evaluation on each evaluation set. We calculate mean and standard deviation confidence boundaries, see Figure 3 and 4. The averaged performance across all support sets indicates the expected efficacy of ProtoNet and ProtoMAML approaches when exposed to a limited number of examples per class. During training, we used k = 5 shots, however, for the testing phase, we vary the number of shots to examine its impact on performance. We experiment with k varying from 2 \u2013 256 for ProtoNet and from 2 \u2013 96 for ProtoMAML. Computational restrictions allowed us to explore only up to this number of shots.\nThe ProtoMAML is evaluated the same way as ProtoNet, specifically by selecting 2-class and k-shot random examples from the test set to form a support set, while the remainder of the evaluation dataset serves as the query set. However, unlike simple calculating prototypes for all classes in support set, ProtoMAML requires finetuning a separate model for each support set inside the inner loop in Algorithm 1. This adaptation step makes the evaluation process resource-intensive compared to ProtoNet, however, more adaptable to the new acoustic environment. In contrast to training phase, it is recommended to use many more inner-loop updates during adaptation on the support set. To obtain results in Figure 4 and Table 1, we used 25 inner-loop adaptation steps for all ProtoMAML models. Finally, in Figure 5, we investigate the effect of number of adaptation steps on performance."}, {"title": "6. RESULTS", "content": "Number of adaptation shots. Number of few-shot adaptation for across datasets is shown in Figures 3 and 4. It shows that increasing the number of examples in a support set for adaptation on unseen domain improves the performance and facilitate the investigation at scales. The k-shot from 2 - 256 is explored for ProtoNet; and from 2 - 96 is for ProtoMAML. For both ProtoNet and ProtoMAML, the EER decreases the more adaptation samples are introduced. We see dramatic exponential error drop after introducing up to 16-shot per class, e.g., for ProtoNet, the EER on InTheWild dropped from 26.05% to 21.37% and for ProtoMAML from 24.42% to 17.89%. After 16-shot the improvement stagnates across all evaluation corpus, and the variation drops between different support samples, the confidence boundaries narrows down. The most challenging dataset for adaptation is InTheWild, especially adapting on small support set below 16-shot. However, even with just 256-shot per class, ProtoNet brings EER to 20.94% and ProtoMAML brings to 16.29% on 96-shot.\nFrom Table 1, due to few-shot adaptation, meta-learning approaches outperform supervised approach Wav2Vec-AASIST* with almost the same number of trainable parameters across all datasets. The ProtoMAML, as optimization-based adaptation, outperforms non-parametric ProtoNet on majority of datasets, except ASV21:LA.\nNumber of adaptation steps. Analysing the number of adaptation steps for ProtoMAML_96, Figure 5, we observed exponential performance improvement on InTheWild dataset. Finetuning ProtoMAML on 96-shot and 2-class (bonafide and spoof) support set, the EER dropped significantly from 17.61% to 10.42% when using 5K adaptation steps. We experimented with the adaptation steps only on InTheWild datasets as the most challenging for adaptation, other datasets require further investigation.\nIn summary, it is evident that ProtoMAML standing out over ProtoNet when it comes to adaptation step tuning. However, a notable drawback of ProtoMAML is its significant computational requirements. ProtoNet, with its simplicity, offers a robust baseline for ProtoMAML and may be the preferred choice in scenarios where resources are constrained."}, {"title": "7. CONCLUSIONS", "content": "Previous work had already shown that in the cross-corpus task, where a speech deepfake detector is trained using one corpus and applied to another corpus, the generalization performance degrades considerably. We confirm it by showing that the performance of the Wav2Vec-AASIST decreases from 0.84% EER to 13, 93% EER when applied to the InTheWild corpus. Our aim to solve the generalization problem is to use meta-learning in the way of adapting the trained model to a new corpus. Specifically, we used ProtoNet and ProtoMAML training and adaptation schemes. Resulting in that when we use only 96 samples per class from the InTheWild corpus, we are able to push the EER to 10.42%. This result was obtained with 0.14% of the trainable parameters used in the baseline model.\nThis is a promising direction for continuous system adaptation when we have few open-sourced samples from a new realistic synthetic speech attacks. In that case, generating a high-scale training dataset is expensive or not feasible, however, running few-shot adaptation will, at least, guarantee that the system is up to date. The future goal of our work is to achieve a reasonable level of performance without sampling from the evaluation set."}]}