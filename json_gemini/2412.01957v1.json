{"title": "Usage Governance Advisor: from Intent to AI Governance", "authors": ["Elizabeth M. Daly", "Sean Rooney", "Seshu Tirupathi", "Luis Garces-Erice", "Inge Vejsbjerg", "Frank Bagehorn", "Dhaval Salwala", "Christopher Giblin", "Mira L. Wolf-Bauwens", "Ioana Giurgiu", "Michael Hind", "Peter Urbanetz"], "abstract": "Evaluating the safety of AI Systems is a pressing concern for organizations deploying them. In addition to the societal damage done by the lack of fairness of those systems, deployers are concerned about the legal repercussions and the reputational damage incurred by the use of models that are unsafe. Safety covers both what a model does; e.g., can it be used to reveal personal information from its training set, and how a model was built; e.g., was it only trained on licensed data sets. Determining the safety of an Al system requires gathering information from a wide set of heterogeneous sources including safety benchmarks and technical documentation for the set of models used in that system. In addition, responsible use is encouraged through mechanisms that advise and help the user to take mitigating actions where safety risks are detected. We present Usage Governance Advisor which creates semi-structured governance information, identifies and prioritizes risks according to the intended use case, recommends appropriate benchmarks and risk assessments and importantly proposes mitigation strategies and actions.", "sections": [{"title": "Introduction", "content": "Organizations run a significant risk of legal, financial and reputational damage because of misuse and biased outcomes from Al systems when no appropriate governance techniques are employed. AI governance, therefore, is not just an obligatory requirement but a strategic necessity to mitigate these threats and, on a bigger scale, promote trust in AI technologies.\nCompanies using AI in their products are duty-bound to implement responsible governance structures and have a strategic incentive to do so. Having oversight and a comprehensive understanding of one's AI Systems will mitigate threats posed by improper governance and make monitoring and updating operational practices in line with evolving risks and regulations easier. Additionally, with the introduction of the EU AI Act (European Parliament and Council of the European Union 2024) and similar regulations, companies that proactively implement responsible AI governance practices will have a competitive advantage.\nThis paper presents Usage Governance Advisor, supporting human-in-the-loop automation to ease the barrier of entry for governance in AI systems. Our solution prioritizes the use and deployment of Large Language Models (LLMs) as integral components of the governance and risk assessment framework. The broad and diverse capabilities of LLMs make it difficult to scope and monitor risks and performance measures when considering broad capabilities. The risks associated with foundation models differ depending on their intended use as well as the context, e.g. the geography, in which they are deployed. For example, if an LLM is being used in the context of entity extraction, it would be excessive to analyze the output of the LLM for the presence of toxic language when precision, recall and a check for any strings that did not appear in the original input is sufficient. In the context of a text generation task however toxic language becomes a very important aspect to consider. Additionally, if the generation task is in the context of HR related decisions, compliance with external policies might need to be included in the assessment. To address this problem we first aim to elicit semi-structured information on the intended use of the foundation model. This is then used to prioritize risks and identify the AI tasks involved. This information is then leveraged to recommend candidate models and the appropriate risk assessment evaluations to assess those risks. Finally, our solution leverages the prioritized risk specification to evaluate the models across the relevant dimensions, creating an audit trail of the intended use, the relevant risks identified, and the recommended mitigation actions and guardrails required before the solution should be approved for deployment. At the core of the solution is a risk taxonomy which is used to link the use to the model, the models to assessments, and the outcomes to mitigations. We also employ a Knowledge Graph (KG) based approach that organizes the risk-related information about AI systems with a dedicated ontology. We describe how we populate this KG and leverage it for tooling to aid in decision making when assessing risks associated with an AI solution. We also explain the development of the auto-assist questionnaire, model recommendation, risk evaluations and mitigation recommendations."}, {"title": "Related Work", "content": "Implementing a comprehensive enterprise Al governance system poses significant challenges due to the numerous stakeholders involved, as well as the need for human oversight to ensure trustworthiness in decision-making processes. Multiple studies have been conducted that provide a conceptual overview on the disparate aspects of AI governance. (Papagiannidis et al. 2023) describes AI governance applications to three firms in the energy sector based in Norway. The study describes the structural, relational, and procedural dimensions of governance by leveraging the knowledge gained through the functioning of these companies.\n(Cihon, Schuett, and Baum 2021) discuss governance by considering the stakeholders or actors (like managers, investors etc) and their roles for AI governance. (\u00c5ngstr\u00f6m et al. 2023) discuss the challenges companies face with AI implementation and governance obtained through a survey of decision makers from multiple countries. Finally, (Ferdaus et al. 2024) and citations within find the opportunities, challenges and limitations of trustworthy AI with particular focus on LLMs. The paper also states the role of government initiatives for AI governance.\nKnowledge graphs have been employed in legal governance by (Schwabe, Laufer, and Casanovas 2020) to address concerns around trust, privacy, transparency, and accountability by incorporating legal resources into their framework. That paper presents a proof-of-concept framework demonstrating how knowledge graphs can be utilized to understand and address key components such as privacy, transparency, and trust. The Artificial Intelligence Ontology (AIO) (Joachimiak et al. 2024) is an ontology of AI concepts and relationships. Among its goals is a standardized language of AI terms and concepts. AIO is structured into six domains: Networks, Layers, Functions, LLMs Preprocessing and Bias. The ontology presented in this work shares common goals, however with a focus on broad-based AI risk assessment.\nDespite research and conceptual frameworks that have explored individual facets of governing AI and LLMs, a comprehensive, systemic approach has yet to be undertaken. This paper focuses on developing an automated human-in-the-loop methodology to identify and address risks associated with LLMs that can be deployed in live production environments."}, {"title": "User scenario", "content": "The Usage Governance Advisor system (Figure 1) targets AI model developers and AI risk officers to help them uncover the risk profile of a use case and recommend appropriate models to perform the underlying AI tasks, and as well as any mitigating actions to offset the risks. This information allows them to make an informed decision about the trade-offs involved in using a model, to track risks for different models, and meet their regulatory and compliance goals. To illustrate how the system can achieve these aims, consider an example scenario of AI developers who are investigating the AI risks that might be involved with the training and deployment of an AI system for a medical chatbot. The intent of such a system could be\n\"In a medical chatbot, create a triage system that assesses patients' symptoms and provides advice based on their medical history and current condition. The chatbot will identify potential medical issues, and offer recommendations to the patient or healthcare provider.\"\nThe AI developers are tasked with gaining an understanding of the potential risks associated with their use case and to identify potential actions to best mitigate those risks. The outputs of the system are designed to assist them in communicating this to other non-technical stakeholders in their organisation. The user inputs their use case to the system as an intent, and the system auto-fills a model usage compliance questionnaire by leveraging a Chain-of-Thought, LLM-as-a-judge approach to connect questions/answer pairs to risks and identify an AI task.\nThe user inspects the questionnaire output (as displayed in Figure 2), and is asked to confirm the system suggestion for the AI task (\"Generation\" in this case) or select an alternate task. The list of potential risks, AI task, and intent are used to recommend different models in terms of suitability, based on mining the knowledge graph for relevant evaluation scores from benchmarks and related information. To provide additional transparency in the recommendation process, the evaluation scores are provided to the user as evidence of the reasoning behind the recommendation. The user confirms the suggested model is suitable for their case and the system computes a risk evaluation report. The user inspects the report to understand the categories of risks likely to be associated with their use case. For each risk identified, explanations are given about the concerns, to allow users to better understand and interpret the system recommendations. The user sees that \"Toxic output\" is a risk associated with output that is potentially related to the use case. They can expand the card to gather more details. Potential mitigating manual actions for that risk are proposed by the system in Figure 3 and they can also see the scores of the related risk evaluations which have been run against the proposed AI model for the relevant risks, such as social bias and safety. The report is stored by the system and the AI developer can use it as a supporting tool to illustrate the risks to the other project stakeholders with varying levels of technical ability."}, {"title": "System Overview", "content": "This section describes Usage Governance Advisor, a system that provides workflows taking the user-intent and multiple sources of information to 1) create semi-structured information through compliance questionnaires, 2) prioritize these risks based on the inferred answers and 3) recommend appropriate models. The system collects risk evaluations associated with the proposed models in relation to the intended use case. As described above, the system recommends potential mitigation strategies and action items, leveraging a knowledge graph to help organize the information about AI models."}, {"title": "AI Systems Knowledge Graph", "content": "To bring structure to disparate resources about AI model usage risk we use a knowledge graph. The details of the defined ontology, ingestion process and entity/relationship extraction are described in the following sections."}, {"title": "Ontology Definition", "content": "Underlying the Usage Governance Advisor is a common semantic model which is an ontology organizing concepts and relations within the domain of AI, with a focus on AI Governance concepts such as lineage, data sets, licensing, technical characteristics, evaluation results, and risk assessment. Given the rapid evolution of the field of AI, there is a need for unifying concepts, mappings and the ability to integrate existing vocabularies.\nThe AI ontology is defined with the LinkML modeling language and framework (Moxon et al. 2021). Compared with OWL, LinkML's YAML syntax presents a more familiar definition language for developers and subject matter experts, enabling inter-disciplinary AI governance teams. Namespaces and import semantics allow the modular definition and inclusion of sub-domain models such as risk taxonomies. The LinkML framework provides generators for converting LinkML models into other schema languages such as JSON Schema, SQL data definitions, RDF and OWL. Due to its technology-independence and convertibility, LinkML is a bridge between ontologies and the broad ecosystem of technologies found in modern development run-time environments. Key elements and relationships in the ontology are shown in Figure 4. With consideration for the EU AI Act, which emphasizes integrated AI systems in its scope, the class AiSystem is comprised of one or more AiModels. LargeLanguageModel, a sub-class of AiModel, is trained on Datasets. Both models and datasets have Licenses. AiEvaluations are associated with AiEvalResults and Risks which can belong to a RiskTaxonomy. Risks may have mappings to Risks in other taxonomies.\nWe materialize this ontology in the KG by ingesting information from multiple sources. Essential data about the AI models includes: information about the how the model was built, i.e., technical documents associated with the model and information about how the model behaves i.e., output from dedicated test frameworks, such as lm-eval-harness.\nThe KG thus encompasses both a domain graph of AI entities and relationships as well as an evidence graph associating AI entities with evidence (e.g., AI evaluation results). By merit of the relationships between these two graphs, there is a navigable indication of the source of a given information thus providing confidence in the entity/relationship triples extracted by the ingestion process."}, {"title": "Applying the Ontology to Risk Mapping", "content": "A core goal of the Usage Governance Advisor is supporting a risk assessment of an Al model or system. To model Al risks, an existing AI risk ontology, AIRO (Golpayegani, Pandit, and Lewis 2022), is used for the base risk vocabulary. The IBM AI Risk Atlas (IBM 2023) was chosen for the base risk taxonomy. However, communities (e.g., OWASP AI security) and regulatory jurisdictions (e.g., EU, US Federal Systems) may orient on different risk taxonomies. As a result, mappings were defined between the base taxonomy and three leading risk taxonomies: the NIST AI RMF Gen AI Profile (NIST 2023), the MIT AI Risk Database (MIT 2024) and the OWASP Top 10 for LLM Applications (OWASP 2024).\nChallenges in drawing equivalences between risk taxonomies arise due to differences in focus, granularity and structure between taxonomies. Although there are conceptual overlaps, the differences are nevertheless significant enough to rule out straightforward isomorphic equivalences. As an illustrative example, Figure 5 shows mappings between the IBM AI Risk Atlas and the NIST Gen AI Profile. We use the (SKOS) (W3C 2009) schema to capture the different relationships of skos:closeMatch, skos:exactMatch, skos:broadMatch and skos:relatedMatch."}, {"title": "Knowledge Graph Construction", "content": "As much of the information stored in the KG exists in unstructured documentation our system uses a generative AI pipeline to ingest data from these sources. We maintain both a domain graph containing the extracted facts and an evidence graph pointing to the sources of information for those facts and an indication of our confidence in their truth. This pipeline consists of: ingestion, entity/relationship extraction, and augmenting with evidence and confidence measures. The entire pipeline is coordinated using the langchain (LangChain 2024) framework."}, {"title": "Ingestion:", "content": "We first classify documents to determine their nature, helping identify entities and relationships for extraction. Term frequency highlights the document's main subject, such as an Al model or regulation, which focuses the extraction process and narrows the relevant subset of the ontology. Similar work is discussed in (Jiang et al. 2024).\nAfter classifying the document, we decompose the text of the document into chunks that can be treated in isolation by the generative pipeline. Sections, where available, guide the process with the aim to retain related information and context for extraction. These chunks are labeled with additional information such as the classification, section title, and their location in the document. Some overlap with the previous and next chunk is included. We leverage methods from (Mishra et al. 2024), enabling chunking of PDFs in a manner suitable for Retrieval-Augmented Generation (RAG) systems, labelling chunks, applying a windowing strategy, preserving relationships between documents parts and translates tables in a format suitable for RAG."}, {"title": "Entity/Relationship Extraction:", "content": "To organize the information from the documents into the KG, it needs to be converted to entities and relationships following the types described in the ontology. To reduce the complexity of the problem, we make the following assumptions: 1) the entity types relevant to the document have been extracted in the ingestion phase, so we do not need to consider the entire ontology when processing a document 2) there is no need to extract the specific relationship types between two entities, only whether two entities are related; the relationship should be implicit from the ontology. Additionally, we may assume that the main entity or entities that the document describes have been identified. A further simplification is that we do not try to identify entities with properties as defined in the ontology, but we rather promote properties to entities themselves, as in an RDF representation of the ontology. No information is lost in this process, but the task is reduced to find any of the resulting entities, as opposed to entities with a given set of properties."}, {"title": "Auto-assist questionnaire", "content": "To help manage the risks of deploying LLMs, organizations follow a process to identify and mitigate risks. A key component of this process is the completion of questionnaires to aid in assessing the risks associated with the specific AI use case and model being deployed. Thus, this process can be time-consuming and cumbersome for end-users, who must navigate lengthy questionnaires prior to deployment. To address this challenge, we have developed an auto-assist functionality that utilizes a Chain-of-Thought (CoT) approach and few-shot examples to assist users in completing questionnaires as shown in Figure 6 and 7. This functionality provides suggestions for answers to compliance questions based on the user's initial intent. The auto-assist functionality supports three types of questions: multiple choice, binary and freeform."}, {"title": "Risk Prioritization", "content": "Risks are identified by analyzing questions and their corresponding answers. Accordingly, we use an LLM-as-a-judge approach to connect questions/answer pairs to risks and whether specific answers reduce or amplify a risk (Zheng et al. 2023). Each risk is assigned a severity level, which classifies the degree of potential impact of a specific risk into three classes: High, Medium, and Low. Figure 8 presents an example prompt used to classify the risk severity level based on the risk description, the questions/answer pairs from which the risk was inferred, and the normalized average score of all the individual metrics scores associated with the risk. In this example, the context variables in the prompt are substituted with the relevant factors discussed above. When assessing the risk, the LLM evaluates whether the response to the question helps mitigate the risk in relation to the question's context and the risk description."}, {"title": "Model Recommendation", "content": "The model recommender uses the KG related to a model inventory, taking into account the list of prioritized risks associated with the use case. In addition to the KG information, customer policy, e.g. business guidelines, is used to define acceptance criteria for recommended models. The KG also contains historical risk evaluations, i.e., benchmarks and questionnaires, that quantify risk exposure of AI models. This can be combined with information about a deployed model's behavior from logging data.\nThe model recommender combines both the prioritized risk, evaluation results and acceptance criteria to estimate a total risk value for each candidate model, thus identifying the least risky model for the given use case. If a candidate model has already been specified for a similar use case, the model recommender can compare this model with other challenger models and list strengths and weaknesses of the candidate model. In case of incomplete information about risk evaluations, additional automated risk evaluations are proposed."}, {"title": "Automated Risk Evaluations", "content": "General purpose LLMs have a broad range of capabilities and with this comes a wide variety of risks (Weidinger et al. 2021; Schillaci 2024). The research community has responded by creating an ever increasing number of benchmarks (Zhang et al. 2023; Xu et al. 2023; Zhu et al. 2024). Given the rapidly evolving collection of benchmark and evaluations we developed a flexible approach with two goals. First, given the large number of risk dimensions it is untenable to run all evaluations for all use cases, as a result, we run only a subset of prioritized evaluations based on the prioritized list based on the risk identification process. Second, we allow easy creation of new assessments leveraging the Unitxt framework (Bandel et al. 2024). Usage Governance Advisor maintains linkages between risk atlas definitions and Unitxt specified risk evaluations. The catalogue is regenerated frequently to support new benchmarks being included in the risk assessment evaluation.\nIndividual risks may be specified as quantitative or categorical. A quantitative risk could be a numeric score from a safety benchmark. These scores are diverse and not easily combined. As a result, we normalize these raw results into three numbers: $1, 0, -1$, where 1=\u201cAbove average\u201d, 0=\u201cAverage\u201d, -1=\u201cBelow average", "scale": 1, "unacceptable": "The term \"acceptable\" is configurable via the deployers' policy. This is similar to the approach used in the Stanford Transparency Index (Bommasani et al. 2023) that measures whether some aspect of a model is known. We extend this to distinguish whether the known value is acceptable to the deployer. For example, the policy might define that the license is known and corresponds to a set of approved licenses, or that the model vendor claims that approval for usage has been obtained for all data sets. Categorical risks can be used to filter the models that are considered."}, {"title": "Mitigation Recommender", "content": "Identified risks are used to recommend mitigation strategies. Our solution recommends two types of mitigation strategies: deployment guardrails and manual mitigating actions.\nGuardrails Detectors and guardrails are common practice for LLMs in deployment (Inan et al. 2023; Achintalwar et al. 2024; Magooda et al. 2023). Detectors can act as real-time filters to mitigate the potential for harmful outcomes (e.g. generating toxic output). In a similar manner for evaluations, Usage Governance Advisor maintains links between risk dimensions and guardrails. Assessments conducted as part of the automatic risk evaluation can be augmented to include a post-processing guardrail step to measure the impact of a given guardrail on mitigating a given risk. As a result, the system can recommend mitigation guardrails that should be used for any system deployment along with evidence of the anticipated impact of such a guardrail."}, {"title": "Manual Curation of Recommended actions", "content": "The different types of risks identified require different types of actions. Whereas some actions are directly identifiable from the risk, other actions require different types of (human) action. An example of the former is the following: Detected situation: For all data used in building the model, copyright status is not disclosed. Risk: Data usage rights (Data Provenance), Action: Provide copyright information). An example for the latter case: Detected situation: A model's limitations are not demonstrated. Risk: Explaining output (Output bias). Action:1.) Contact developer/model provider to demonstrate the model's limitations. 2.) Understand whether limitations impact the output in expect use context. 3.) Implement guardrails. 4.) Assess model output after guardrail implementation. 5.) If 4 not satisfactory, reconsider use for specific use context. These examples also illustrate the importance of manual curation of recommendation actions: depending on the risk identified, the suggested actions can be a straightforward provision of lacking data or information. In other cases, a series of actions with different dependencies has to be followed. Often, these actions require human intervention including intervention in existing business processes. Thus, as mitigation actions can be very context-specific, it is also advisable to test and assess the mitigation actions that Usage Governance Advisor provides, within the specific context of use.\nA risk can only be marked as resolved once the deployment guardrail has been executed and/or the mitigating action implemented, with the actions taken documented and assessed. This documentation also allows for evaluating the effectiveness of these measures over the model's lifetime."}, {"title": "Analysis", "content": "The following section evaluates two primary stages of the Usage Governance Pipeline."}, {"title": "Knowledge Graph Construction", "content": "The KG is populated by the processing of technical documents about AI models, obtaining related entities (triples) relevant to the ontology previously described. We evaluate different approaches using generative AI against manually obtained ground truth from the granite-8b-code-base-4k AI model card in Hugging Face (IBM 2024). The generated KG is then measured against the ground truth by considering entity/relationship triples and calculating how many of the ground truth triples exist in the prediction (recall) and how many of the predict triples exist in the ground truth (precision). We also provide the F1 metric relating the two for completeness. This F1 metric is computed following the same approach as in (Mihindukulasooriya et al. 2023). Results are provided in Table 1.\nIn our case, we assume that for two triples to match, the two related entity types must match, the entity labels (names) must also match and the relationship is implicit from the ontology; i.e., it does not matter what the relationship type is, only that the two entities in the triple are related. As an example, consider that an \u201cAI Model\u201d entity is related to a \u201cLicense\u201d entity; independently of what the process obtains as a relationship type, we assume that the triple refers to the license of said AI model.\nPreliminary results showed that having an exact match for triples in the ground truth and triples generated by an LLM is too strict a metric. As an example, consider the following:\nA triple ['ibm research\u2019, 'organization\u2019, 'granite-8b-code-base-4k', 'aimodel']\ndoes not match\n['granite-8b-code-base-4k', 'aimodel', 'ibm research', 'organization'], although for our purposes they both contain the exact same information. To support these cases, we extend the pipeline with an LLM-as-a-judge solution using granite-3-8b-instruct which improved results as shown in Table 1. Accumulating results over multiple runs for the same document improves the results for the LangChain APIs (LLMGraphTransformer and GraphRag), especially for recall. The process was run up to 15 times, however, performance reached the upper limit within 2-4 passes. Granite-3-8b-instruct as Judge recovered the most information, and was correct around 80% of the time."}, {"title": "Auto-assist questionnaire", "content": "We evaluated the effectiveness of the Chain-of-Thought (CoT) approach in auto-assisting questionnaire completion with synthetically generated user intents. We generated 42 artificial user intents and corresponding answers using an LLM. These answers were validated by human-annotated ground truth. Accuracy is used to measure binary and dropdown questions. To evaluate free-form questions, the answers generated by CoT were condensed to bullet points to better support accuracy measures and compared with the ground truth.\nFigure 6 provides an illustrative example of user intent, response, and bullet-point summary generated by the CoT approach and consequent user summarization. Table 2 shows a subset of the questions used to assess the effectiveness of CoT approach. For each question, we provided few-shot or Chain-of-Thought reasoning examples to facilitate consistent response generation, as shown in Figure 7.\nThe performance of the CoT approach was evaluated using granite-3-8b-instruct. The results presented in Table 3 demonstrate that the CoT method outperforms zero-shot inference for all three types of questions, yielding significantly higher accuracy. Furthermore, when users are offered the option to select from a subset of suggested answers, the performance of CoT is enhanced. Notably, the upper limit for additional suggestions provided by the LLM has been set to two options above the ground truth. If the LLM proposes more than this threshold, it is considered an incorrect response."}, {"title": "Perspectives and Future Work", "content": "Effective AI governance requires a multifaceted approach that considers diverse stakeholder perspectives (service providers, users, auditors) to mitigate risks. Governance rules are inherently dynamic, subject to frequent updates and new models emerging in the market, making it challenging to ensure compliance. This paper explores the potential of knowledge graphs as a solution for governing AI systems. By integrating new rules, models, and mitigation strategies seamlessly, while maintaining relationships with existing information, KGs can provide a comprehensive framework for managing AI governance complexities. This paper presents an essential first step in addressing regulatory and compliance requirements for deploying AI models into production, supporting alignment with evolving laws and regulations. Real-time monitoring and auditing of AI system performance are also crucial to identify risks associated with Al failures or misuse. To further enhance the framework, future work will focus on developing a robust mechanism for continuously auditing and verifying AI system performance through integration of real-time monitoring capabilities. Additionally, we aim to incorporate policy guidelines from stakeholders directly in the knowledge base, enabling a more tailored approach for AI governance. By addressing these aspects, the framework can provide a comprehensive and proactive approach to AI governance, ensuring responsible deployment and continuous improvement of AI systems."}]}