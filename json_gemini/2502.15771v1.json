{"title": "Learning to Reason from Feedback at Test-Time", "authors": ["Yanyang Li", "Michael R. Lyu", "Liwei Wang"], "abstract": "Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OPTUNE, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OPTUNE achieve superior scalability and performance\u00b9.", "sections": [{"title": "Introduction", "content": "Leveraging external feedback from interactions with the environment during test time has emerged as a promising approach for large language models (LLMs). This includes applications such as LLMbased agents (Yao et al., 2023; Shinn et al., 2023) and, more recently, test-time scaling (Wu et al., 2024; Snell et al., 2024; Liu et al., 2025). Such methods further enhance the potential of LLMs to solve challenging tasks, e.g., Olympiad-level math problems (Guan et al., 2025) and competitive programming (OpenAI et al., 2025).\nSignificant progress in this area typically falls into two categories (Snell et al., 2024), as illustrated in Figure 1: sequential revision and parallel sampling. Sequential revision methods (Shinn et al., 2023; Madaan et al., 2023) incorporate previous attempts into the LLM\u2019s context, while parallel sampling methods (Brown et al., 2024; Xie et al., 2023) generate new attempts independently of prior failures. However, both approaches have notable limitations. Sequential revision is computationally expensive due to long context lengths and faces challenges (Muennighoff et al., 2025), such as position bias (Liu et al., 2024) and attention noise (Ye et al., 2024b). In contrast, parallel sampling, while efficient, fails to learn from previous errors (Brown et al., 2024). Unlike these paradigms, human reasoning follows a different pattern: humans store recent experiences in \u201cfast weights\" (Ba et al., 2016), enabling them to neither revisit past errors explicitly nor start each attempt without any prior knowledge. Recent research suggests that the weights of neural networks could serve as a natural memory mechanism during test time (Wang et al., 2024).\nBuilding on these observations, we propose a novel paradigm that leverages Test-Time Training (TTT) (Sun et al., 2020, 2023) to store past experiences in model weights rather than in the context. This approach bridges the gap between sequential revision and parallel sampling by indirectly incorporating knowledge into the LLM without disrupting in-context reasoning. Specifically, we introduce Feedback-based Test-Time Training (FTTT), which employs a carefully designed TTT task enriched with feedback through self-reflection. We demonstrate that FTTT improves test-time computation scalability on two mathematical reasoning and two code generation datasets, using Llama-3.1-8B-Instruct (Dubey et al., 2024) and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023).\nInspired by advancements in learning to optimize (Chen et al., 2022), we explore training learnable test-time optimizers to yield Pareto-optimal cost-performance tradeoffs. Our proposed learnable optimizer, OPTUNE, is a lightweight neural network that predicts weight updates from the gradients of the previous attempt. Unlike traditional parameter-efficient fine-tuning (PEFT) methods, OPTUNE works on the gradient rather than the activation space. Experiments on three reasoning datasets and two different LLMs demonstrate the effectiveness of OPTUNE, outperforming five widely used PEFT baselines."}, {"title": "Feedback-based Test-Time Training", "content": "2.1 The Test-Time Training Task\nThe problem of exploiting test-time feedback is as (Shinn et al., 2023): given a question Q, a model M attempts to solve Q within a budget of N attempts. A verifier V evaluates each attempt, such as the n-th attempt An, and provides feedback V(An). This work focuses on binary verifiers, which determine whether An is correct. These verifiers are well-established, rule-based systems that are both cost-effective and efficient to evaluate.\nWhen the model generates attempts sequentially, our goal is to enable M to learn from previous attempts to improve subsequent ones. To achieve this, we frame learning from previous attempts as a training problem: at each step n, we optimize M using Q, An, and V(An), aiming for M to generate a better An+1. This way internalizes the past attempts into weights for efficient inference of An+1. As a result, the sequence of attempts can be viewed as an N-step optimization process.\nA key challenge is designing an effective supervised task using Q, An, and V(An) to improve the model\u2019s ability to solve Q. We build on the intuition that a model capable of judging the correctness of a solution should also be able to solve the question itself. Concretely, given Q and An, we train M to predict verbal feedback F that aligns with V(An). This leads to our FTTT loss:\n$L_{FTTT}(Q, A_n) = -\\frac{1}{l_F} log M_{n-1}(F | Q, A_n)$"}, {"title": "Self-Reflected Feedback", "content": "Since we are working with a binary verifier, the learning signal is limited at each interaction. Previous research suggests that LLMs can self-correct errors when provided with external signals (Huang et al., 2024). Inspired by this, we aim to enhance the learning signal by leveraging the model to generate silver-standard training labels.\nWe first sample a reflection Rn from the model given Q, An, F and the instruction P:\n$R_n \\sim M_0(R | Q, A_n, F, P)$\nIn practice, we use Mo to generate Rn to mitigate the risk of degraded self-reflection ability after training. The auxiliary loss is then defined as:\n$L_{aux} (Q, A_n, R_n) = -\\frac{1}{l_R} log M_{n-1}(R_n | Q, A_n, F)$\nwhere ln is the length of Rn. Eq. 3 can be interpreted as a sequence-level distillation loss (Kim and Rush, 2016), where knowledge from the raw model Mo is distilled into the trained model Mn-1 to prevent overfitting. Finally, the overall loss is as:\n$L_{final} = L_{FTTT} + L_{aux}$\nBelow is a training example with self-reflection, where underlined sentences are the training target:"}, {"title": "A Learnable Test-Time Optimizer", "content": "3.1 The Learning to Optimize Problem\nAlthough FTTT achieves success (see Section 4.2), it simply accumulates the gradients of the feedback received so far to update the weights. This raises the question: can we design a better test-time optimizer that more effectively exploits feedback?\nMotivated by learning to optimize (Chen et al., 2022), we adopt a neural network as the test-time optimizer. Concretely, this learnable test-time optimizer is formulated as $f_\\theta(Q, \\{A_i, V(A_i)\\}_{i=1}^n)$, which predicts updates for all LLM weights based on the previous n attempts, and \u03b8 is the optimizer parameter. However, this direct formulation leads to prohibitively large networks due to high-dimensional input and output spaces. For a maximum number of m tokens per attempt and an l-layer LLM, the input space grows to n \u00d7 m \u00d7 l, even when updating only a scalar (we exclude the token count of Q, as it is significantly smaller than m). Since updates for all weight matrices across all layers are predicted jointly, the dimensionality of the input and output spaces becomes unmanageable. We therefore simplify f\u03b8 by introducing the following assumptions:\n(A1): Markov Property: The latest attempt captures all relevant information from previous attempts.\n(A2): Independent Update: The optimizer predicts updates for each parameter independently, similar to conventional optimizers.\nA1 eliminates the dependency on n and A2 enables updates to be predicted independently for each weight, significantly reducing the size of the output space. The learnable test-time optimizer now becomes $W_i \\leftarrow f_{\\theta_i}(Q, A_n, V(A_n))$, where it predicts the update for the weight Wi in the i-th layer based on Q and the latest attempt An. To train all f, we define the following loss:\n$L_{meta} = - log M(\\hat{A} | Q, \\{W + w | W \\in w\\})$\nwhere \u00c2 is the ground-truth for Q and W is the set of LLM weights. Eq. 5 encourages f to predict updates that increase the likelihood of generating the correct answer after applying the updates."}, {"title": "A Parameter-Efficient Architecture in The Gradient Space", "content": "Given the limited learning signal at test time, we design the learnable optimizer to be parameter-efficient to alleviate overfitting. However, the input and output spaces of $f_{\\theta_i}(Q, A_n, V(A_n))$ are large due to their lengths, making even a simple linear projection parameter-intensive. Additionally, V(An) may be heterogeneous to Q and An, e.g., a scalar, posing challenges for modeling.\nInspired by the success of FTTT in Section 2 and recent works (Mitchell et al., 2022; Wang et al., 2024), we propose a parameter-efficient architecture in the gradient space as the learnable optimizer.\nGradient-based Input Compression. Instead of directly inputting Q, An, and V(An), we first project them into the gradient space, since recent work suggests that long context can be effectively compressed by gradients (Wang et al., 2024). This way reduces the token count m in An to a constant and unifies the spaces of Q, An, and V(An) to ease the modeling. To compress Q and An, we use the next token prediction loss, while for V(An), we include LFTTT in Eq. 1. The final loss for compressing the optimizer input is:\n$L_{compress} \\frac{1}{m} log M(A_n | Q) + L_{FTTT}$\nThe input of f\u03b8wi to predict the update of Wi now is the gradient $\\nabla_{W_i}$ of $L_{compress}$ w.r.t. $W_i$. Consequently, f\u03b8wi receives a fixed-size tensor as input rather than a variable-length sequence.\nGradient Decomposition. Although fewi operates on a smaller space after compression, the dimensionality of the gradient space remains large for direct processing. We utilize the observation that Wi \u2208 Rd\u00d7d (assuming Wi \u2208 Rd\u00d7d) can be decomposed into two vectors to further reduce the dimensionalities: the input to a linear projection with weight Wi, ui \u2208 Rd\u00d71, and the gradient of Lcompress w.r.t. the output of the projection, \u03b4i+1 \u2208 Rd\u00d71 (Mitchell et al., 2022). In this framework, f\u03b8wi takes the decomposed ui and \u03b4i+1 as its input and predicts \u0169i and \u03b4i+1. The update is then reconstructed as $\\nabla_{W_i} = \\delta_{i+1} \\tilde{u_i}$. This approach reduces the dimension from d\u00b2 to 2d.\nModel Architecture. The architecture of f\u03b8wi (ui, \u03b4i+1), named OPTUNE, is shown in Figure 2 and defined as follows:\n$[\\tilde{u_i}, \\tilde{\\delta_{i+1}}] = Norm ([u_i, \\delta_{i+1}])$\n$h_i = O_2Dropout (O_1 [\\tilde{u_i}, \\tilde{\\delta_{i+1}}])$\n$[\\tilde{u_i}, \\tilde{\\delta_{i+1}}] = h_i + [u_i, \\delta_{i+1}]$\nwhere $O_1 \\in R^{r \\times 2d}$ and $O_2 \\in R^{2d \\times r}$ are the optimizer parameters with r < d. [\u00b7] denotes the vector concatenation. Norm normalizes ui and \u03b4i+1 to have zero mean and unit variance separately. Dropout is the dropout regularization (Srivastava et al., 2014). In practice, O1 and O2 are shared across all weights with the same shape. OPTUNE is similar to the Bottleneck Adapter (Houlsby et al., 2019), with the key difference that its input is gradients and its output is the weight update. As such, OPTUNE can also be regarded as a specialized PEFT technique tailored to reasoning."}, {"title": "Experiments", "content": "4.1 Setup\nDatasets. We evaluate both baselines and our method on math and coding reasoning tasks: (a) Mathematical reasoning: MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021), using the test split from Lightman et al. (2024) for MATH. (b) Code generation: MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021). For all datasets, we report results on subsets where models fail with greedy decoding. We use Exact Match as the evaluation metric as well as the verifier for math tasks and Pass@1 for code."}, {"title": "Training-Free Results", "content": "Table 2 compares FTTT with various baselines across four reasoning datasets. FTTT, both with and without self-reflected feedback, outperforms conventional test-time scaling methods on average. This success is partially explained by the findings of Ye et al. (2024a), which show that training with error-correction data enhances reasoning capabilities and models do not retry during inference. FTTT is also efficient. For instance, the inference time of Llama-3.1-8B-Instruct on GSM8K with a budget of 32 is 3 GPU hours for the best parallel sampling method (Best-of-N) and 20 GPU hours for the best sequential revision method (Self-Refine). In contrast, FTTT achieves inference times of approximately 3 GPU hours without self-reflected feedback and 4 GPU hours with selfreflected feedback.\nNotably, self-reflected feedback does not always improve results. Its effectiveness appears to depend on the LLM\u2019s self-reflection ability. To test this, we computed the Spearman rank correlation between FTTT and Self-Refine, a self-reflection-based algorithm. The Spearman coefficient (r = 0.8656, p < 0.05) indicates a strong positive correlation, supporting our hypothesis. We also observe that Self-Consistency performs poorly on code tasks because sampled code snippets rarely match exactly, making majority voting akin to random selection.\nFigure 3 illustrates performance for FTTT and baselines under varying budgets. FTTT consistently outperforms baselines, with greater gains under constrained budgets. In contrast, Revision and Self-Consistency do not scale well. Revision struggles with long-context reasoning due to length generalization issues (Li et al., 2024), while Self-Consistency fails to leverage feedback, often discarding correct answers during majority voting due to long-tailed distributions of correct answers (Brown et al., 2024)."}, {"title": "Fine-Tuning Results", "content": "We present the results of PEFT baselines and OPTUNE with a budget of 32 in Table 3. Best-ofN is applied to PEFT baselines to exploit testtime feedback. HumanEval is excluded as it lacks a training set. Table 3 highlights the effectiveness of OPTUNE, outperforming all PEFT baselines by at least 2.58% on average. OPTUNE is also parameter-efficient, with 439K trainable parameters that are comparable to the most lightweight PEFT method (LN-Tuning, 266K parameters), while surpassing the best PEFT method (LoRA, 1.7M parameters) with an order of magnitude fewer parameters. However, OPTUNE shows suboptimal performance on MATH for Mistral-7B-Instruct-v0.3, which is consistent with other PEFT methods with few trainable parameters (e.g., (IA)\u00b3, LoRA, LN-Tuning). This is likely due to Mistral-7B-Instruct-v0.3\u2019s limited mathematical reasoning capabilities, requiring significant parameter updates to improve performance in this domain.\nOPTUNE incurs negligible inference overhead. For example, on GSM8K with Llama-3.1-8B-Instruct and a budget of 32, the best test-time scaling baseline (FTTT) requires 4 GPU hours, whereas OPTUNE uses only 1.5 GPU hours, benefiting from shorter yet accurate predictions.\nFinally, Figure 4 examines the scaling behavior of PEFT baselines and OPTUNE. Initially, OPTUNE underperforms compared to other PEFT methods and FTTT, but it mostly achieves superior results when the budget exceeds 2. OPTUNE\u2019s weaker performance with smaller budgets arises from its reliance on an initial attempt sampled from the raw LLM to initiate the process. This initial attempt often fails but is still counted as a valid attempt, making OPTUNE less competitive in lowbudget settings."}, {"title": "Analysis", "content": "Ablation Study. Table 5 presents an ablation study on the architecture design of OPTUNE. The results demonstrate that all components are essential, as removing any of them significantly degrades performance. Notably, normalization is the most critical component, as it addresses the varying gradient scales of different weights.\nCase Study. Table 4 provides two examples on GSM8K where the leading PEFT method, LORA, fails, but OPTUNE succeeds using Llama-3.1-8B-Instruct. These examples highlight OPTUNE\u2019s superior ability to correctly interpret and reason through questions, unlike LoRA."}, {"title": "Related Work", "content": "Learning from Feedback. Other than the heuristic binary feedback studied in this work, prior research has explored feedback from various sources, such as humans (Ouyang et al., 2022), other models (Yang et al., 2022), tools (Schick et al., 2023), and knowledge bases (Gao et al., 2023). This paper focuses on demonstrating the effectiveness of the proposed method and other feedback types are beyond the scope of this paper.\nTest-Time Training. Test-Time Training (TTT) has shown success in the image modality by addressing distribution shifts and enhancing model capacity through self-supervised fine-tuning on each test case (Sun et al., 2020; Liu et al., 2021; Sun et al., 2023). Recent studies have extended TTT to the text modality (Hardt and Sun, 2024; Wang et al., 2024). The most relevant work, by Aky\u00fcrek et al. (2024), uses TTT to enhance the reasoning ability of LLMs. However, their method relies heavily on human scaffolding for self-supervision and does not generalize beyond ARC-AGI (Chollet, 2019). In contrast, FTTT is generally applicable.\nLearning to Optimize. Learning to Optimize (L2O) trains a network to act as an optimizer for another network (Chen et al., 2022). Early approaches used reinforcement learning to train such optimizers (Li and Malik, 2017; Chen et al., 2017), while recent work focuses on discovering analytical white-box optimizers (Bello et al., 2017; Chen et al., 2023). The most relevant work, MEND (Mitchell et al., 2022), trains a network to predict weight updates from training gradients. OPTUNE builds on this idea, extending it to learn from test-time feedback with a distinct architecture."}, {"title": "Conclusion", "content": "In this paper, we propose a novel paradigm that leverages optimization to address the challenge of exploiting test-time feedback, resulting in improved scaling performance. We further present a learnable test-time optimizer, OPTUNE, which surpasses various PEFT baselines. Both FTTT and OPTUNE are efficient in terms of speed and trainable parameter count."}, {"title": "Limitations", "content": "The current evaluation setting limits FTTT\u2019s potential by providing only binary feedback (i.e., correct or incorrect) for each attempt. However, developing complex reasoning environments with rich feedback is beyond the scope of this work. Additionally, while continuous feedback, such as that from reward models (Yang et al., 2024), has been extensively studied, it is not examined here. Our method can be adapted to continuous feedback with minimal modifications, such as using REINFORCE (Williams, 1992). For coherence, we leave this exploration to future work."}]}