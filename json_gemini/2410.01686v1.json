{"title": "Positional Attention: Out-of-Distribution Generalization and Expressivity for Neural Algorithmic Reasoning", "authors": ["Artur Back de Luca", "George Giapitzakis", "Shenghao Yang", "Petar Veli\u010dkovi\u0107", "Kimon Fountoulakis"], "abstract": "There has been a growing interest in the ability of neural networks to solve algorithmic tasks, such as arithmetic, summary statistics, and sorting. While state-of-the-art models like Transformers have demonstrated good generalization performance on in-distribution tasks, their out-of-distribution (OOD) performance is poor when trained end-to-end. In this paper, we focus on value generalization, a common instance of OOD generalization where the test distribution has the same input sequence length as the training distribution, but the value ranges in the training and test distributions do not necessarily overlap. We propose that using fixed positional encodings to determine attention weights - referred to as positional attention - enhances empirical OOD performance while maintaining expressivity. We support our claim about expressivity by proving that Transformers with positional attention can simulate parallel algorithms.", "sections": [{"title": "1 Introduction", "content": "Transformers [Vaswani et al., 2017] are versatile models used in various applications, including vision [Yuan et al., 2021, Khan et al., 2022, Dehghani et al., 2023] and natural language processing [Wei et al., 2022b, Touvron et al., 2023]. Their effectiveness in complex tasks is particularly notable in Large Language Models (LLMs) [Wang et al., 2018, Hendrycks et al., 2021], where they excel at generating coherent text and understanding context. This strong performance has led to an increased interest in understanding the Transformer architecture as a computational model capable of executing instructions and solving algorithmic reasoning problems.\nIn this context, P\u00e9rez et al. [2021], Wei et al. [2022a] show that Transformers are Turing Complete, and Giannou et al. [2023], Back De Luca and Fountoulakis [2024], Yang et al. [2024] demonstrate that Transformers can effectively encode instructions to solve linear algebra and graphs problems. Additionally, it has been shown that Transformers can perform reasoning tasks using far fewer layers than the number of reasoning steps [Liu et al., 2023], indicating a connection between Transformers and parallel algorithms. To this end, Sanford et al. [2024] further demonstrates that Transformers can simulate the Massively Parallel Computation (MPC) model [Andoni et al., 2018], which is based on the MapReduce framework for large-scale data processing [Dean and Ghemawat, 2008].\nComplementing this theoretical framework, empirical studies have demonstrated the capabilities of Transformers, among other models, in executing algorithms [Veli\u010dkovi\u0107 and Blundell, 2021]. Notable applications include basic arithmetic [Lee et al., 2024], sorting [Tay et al., 2020, Yan et al., 2020], dynamic programming"}, {"title": "2 Related work", "content": "Empirical: Several studies focus on the empirical aspects of training neural networks to execute algorithms, achieving promising results in OOD generalization. Notable examples include Yan et al. [2020], Ibarz et al. [2022a], Diao and Loynd [2023], Bevilacqua et al. [2023], Engelmayer et al. [2023], Rodionov and Prokhorenkova [2023]. However, all these works rely on some form of additional supervision during training, with some differing in the type of supervision employed. Specifically, Rodionov and Prokhorenkova [2023] leverages problem-specific information within a self-supervised training framework, whereas the other studies utilize intermediate labels to guide the learning process. For instance, Engelmayer et al. [2023] demonstrates that using intermediate labels derived from parallel algorithms leads to better performance for parallelizable tasks.\nFrom the perspective of OOD generalization, most works focus on length generalization [Veli\u010dkovi\u0107 et al., 2022, Minder et al., 2023], i.e., testing on longer inputs, with a few exceptions addressing graphs with different connectivity distributions [Georgiev et al., 2023] as well as graphs with varying sizes, edge weights, and"}, {"title": "3 Preliminaries and notation", "content": "We denote by \\( \\mathbb{N} = \\{1,2,3,... \\} \\) the set of natural numbers. We use [n] to refer to the set \\( \\{1,2,..., n\\} \\) for \\( n\\in \\mathbb{N} \\). For a set S we denote by P(S) its power set (i.e. the set containing all subsets of S).\nOut-of-distribution generalization. Out-of-distribution (OOD) generalization broadly refers to the ability of a supervised learning model to \u201cperform well\u201d when evaluated on data that are drawn from a distribution that is different from the one used to generate the training data. Two quantitative measures of OOD generalization are given below.\nDefinition 1 (OOD risk). Let X be the feature space, y be the set of labels, and let \\( h : X \\rightarrow Y \\) be the hypothesis returned by a supervised learning algorithm where the training data are sampled from a distribution \\( D_{\\text{train}} \\) on X x Y. Let \\( D_{\\text{test}} \\) be a different distribution on X \u00d7 Y. The out-of-distribution risk of h with respect to \\( D_{\\text{test}} \\) is defined as \\( R_{D_{\\text{test}}}(h) = \\mathbb{E}_{(x,y)\\sim D_{\\text{test}}}[l(h(x), y)] \\)\nDefinition 2 (Empirical OOD risk). Using the same setting as Definition 1 we define the Empirical OOD risk as \\( R_S(h) = \\sum_{i=1}^n l(h(x_i), Y_i) \\), where \\( S = \\{(x_1,Y_1),..., (x_n, Y_n)\\} \\) are drawn i.i.d. from the distribution \\( D_{\\text{test}} \\).\nModels that achieve low (empirical) OOD risk are said to OOD-generalize. We now define value generalization, which is the type of OOD generalization that we are concerned with in this work.\nDefinition 3 (Value generalization). We use the term value generalization to refer to the following particular case of OOD generalization. The feature space is Euclidean, i.e., \\( X \\subseteq \\mathbb{R}^k \\) for some \\( k\\in \\mathbb{N} \\). There exists a ground-truth labeling function h which maps every \\( x \\in X \\) to its true label \\( y = h(x) \\in y \\), i.e., \\( D_{\\text{train}} \\) and \\( D_{\\text{test}} \\) are completely characterized by their marginalization onto X, denoted by \\( D_{\\text{train}} (X) \\) and \\( D_{\\text{test}} (X) \\), respectively. We say that a model value-generalizes from \\( D_{\\text{train}} \\) to \\( D_{\\text{test}} \\) if it achieves low OOD risk and \\( \\text{supp}(D_{\\text{test}}(x)) \\backslash \\text{supp}(D_{\\text{train}}(X)) \\neq \\emptyset \\).\nNote that the quantities used to measure OOD generalization (and value generalization in particular) do not assume anything regarding the overlap between the training and test distributions. In the context of learning, this can lead to artificially low OOD risk when there is significant overlap between \\( D_{\\text{train}} \\) and \\( D_{\\text{test}} \\). Therefore, the interesting cases are those where samples from \\( D_{\\text{test}} \\) are unlikely to have been sampled from \\( D_{\\text{train}} \\). In the context of value generalization, this translates to \\( P_{x\\sim D_{\\text{test}}(x)} (x \\in \\text{supp}(D_{\\text{train}}(X))) \\) being low. That is, the probability that a test sample lies in the domain of the training distribution should be small. In our experiments, this probability is sufficiently small (see Appendix D for details), so a low test error indicates \"true\" value generalization.\nIn the context of neural algorithmic reasoning, good value generalization (with minimal overlap between test and training distributions) provides a strong indication that a model has learned to execute an algorithm. This is explained by the fact that an algorithm consists of a fixed sequence of instructions that does not change when the input values change."}, {"title": "4 The Positional Transformer architecture", "content": "We now define the positional Transformer, as an adaptation of the Transformer model [Vaswani et al., 2017]. For an input \\( X \\in \\mathbb{R}^{n\\times d_x} \\), we define the \\( l^{th} \\) layer of our architecture as follows:\n\\begin{equation}\nX^{(l)}(X) = \\|_{h=1}^H A^{(l,h)} X_V W_V^{(l,h)} W_O^{(l)} X W_V\n\\end{equation}"}, {"title": "5 Expressivity of Positional Transformers", "content": "We prove that our architecture can simulate algorithms within a parallel computational model, which we refer to as Parallel Computation with Oracle Communication (PCOC). We first describe the main features of PCOC, followed by its definition, and then discuss its limitations. It is important to note that the simulation result is theoretical. In practice, the model could converge to a parameter setting that does not correspond to an interpretable algorithm. However, the simulation result is significant as it demonstrates the minimal capabilities of our architecture in theory. Such theoretical approaches have been employed in previous works. For example, see Sanford et al. [2024], Loukas [2020]."}, {"title": "5.1 Parallel Computation with Oracle Communication (PCOC)", "content": "The PCOC model consists of two steps at each round. The first step is communication, where machines send and receive data from other machines. The communication pattern can change at every round. The second step is computation, where all machines perform some local computation on data stored in their local memory.\nOracle communication. For each length n and input data, we assume the existence of an oracle that provides the destination for each machine and message at each round. The oracle executes a communication pattern without explicit access to destination information. This contrasts with other parallel models, such as the Massively Parallel Computation (MPC) model [Andoni et al., 2018], where it is assumed that the destination information is given explicitly as a part of the input. At first glance, introducing such an oracle might not seem particularly useful, especially because it is fixed for each input data, where the data can be real-valued, which implies potentially unaccountably many communication oracles for a particular task. However, its importance lies in the fact that for a variety of parallel algorithms, the communication pattern at each round depends only on the length of the input, and it is independent of other input data. For example, in algorithms that compute basic statistics such as the sum or minimum, or tasks like sorting a list of numbers, the communication between machines is determined not by their values but by their positions. This means that if the values at each machine change, the communication pattern established between machines for a"}, {"title": "5.2 Limitations of PCOC", "content": "While PCOC offers great flexibility in executing parallel tasks, it should be noted that the oracle communication scheme can be limiting. The computational model in PCOC is subjected to a given oracle. In contrast, in"}, {"title": "5.3 Positional Transformers can simulate PCOC", "content": "Having established the PCOC model, we now show that the positional Transformer model in Equation (1) can simulate it. More specifically, our results show that a R-round PCOC protocol can be simulated by a positional Transformer with R layers. We first present the corresponding theorem, followed by a proof overview.\nTheorem 1. Consider a PCOC instance P with R rounds, N machines with local memory s, and data type \\( T = \\mathbb{R} \\). Let M be a model following the architecture in equation 1 with n = N + 1 nodes, R layers and s attention heads. Then, for any instance P with Borel measurable local functions, there exists a configuration of M that approximates P to any desired degree of accuracy.\nProof overview: The proof starts by demonstrating that a single layer of the positional Transformer can simulate each individual round of PCOC. The constructive proof can be further divided into two main components: communication and computation.\nCommunication: The communication stage leverages the oracle \\( \\text{RCV}_{n, \\text{Data}} \\) to specify the subsets of machines and local memory positions from which each machine receives information. These subsets can be transformed into binary encodings, which are represented by distinct attention heads, one for each local memory position, for a total of s positional attention heads. This part of the proof relies on the capability of attention to represent binary patterns. It is important to note that the number of nodes exceeds the number of machines by one, as an additional node is necessary to represent unsent messages. In the computation of attention, no attention matrix can contain a row of all zeros, implying that a machine is not receiving any information. Due to such cases, we introduce an additional sink node to account for information not directed to any machine. Therefore, in this framework, we can show that any communication pattern defined by the oracle can be effectively represented by s attention heads across N + 1 nodes.\nComputation: The computation stage accounts for the local computations executed by each machine. To this end, we invoke the universal approximation results of multilayer perceptrons (MLPs) to establish that, in each round, the local computations initiated by each machine can be approximated by \u0424 detailed in Equation (1). One important consideration is the inclusion of unique node identifiers in the input to ensure the injectivity of the MLP approximation. Even if two input rows may have the same input values, the unique identifiers guarantee that each row corresponds to a distinct local function. Furthermore, the node identifiers must be preserved at every layer to maintain this injectivity. This is also achieved by the MLP when processing both the output of the attention heads and the residual input X, ensuring that identifiers are consistently retained.\nTherefore, by demonstrating that our architecture can approximate any oracle and local functions, we show its ability to simulate any algorithm in PCOC. In practice, finding an oracle and local functions for a specific task can be posed as a learning problem. Our proposed architecture adopts this approach and can learn to execute parallel algorithms using fixed positional encodings in the attention mechanism. As our experiments illustrate, this approach helps mitigate OOD generalization issues. In our experiments we do not assume access to explicit supervision for communication or computation, both are learned indirectly through ground truth, as all models are trained end-to-end."}, {"title": "6 Experiments", "content": "In this section, we evaluate the performance of positional Transformers across various tasks. Specifically, we compare the effectiveness of positional attention against standard self-attention. Both models utilize the architecture defined in Equation (1), with the distinction that in self-attention, the attention weights are computed based on the input X:\n\\begin{equation}\nA^{l,h} (X) = \\text{softmax} \\left( \\left( \\frac{X W_Q^{(l,h)}}{\\sqrt{d_k}} \\right) \\left( X W_K^{(l,h)} \\right)^T \\right).\n\\end{equation}\nIn this setting, standard Transformers also incorporate positional encodings concatenated with the input values. In Appendix C, we examine other configurations for standard Transformers (including one using Rotary Positional Embedding (RoPE)) and find no major differences in performance.\nNext, we outline the tasks used in this work, followed by a detailed description of the experimental setup. Finally, we present and discuss the results.\nTasks: To analyze the performance of positional attention in contrast to self-attention, we train the models on the following tasks:\n1. Cumulative sum: Given \\( x \\in \\mathbb{R}^n \\), output \\( y \\in \\mathbb{R}^n \\) where each element \\( y_i \\) is the sum of the first i elements of x, i.e. \\( y_i = \\sum_{j=1}^i x_j \\).\n2. Cumulative min: Given \\( x \\in \\mathbb{R}^n \\), output \\( y \\in \\mathbb{R}^n \\) where each element \\( y_i \\) is the minimum value among the first i elements of x, i.e. \\( y_i = \\min\\{x_j | 1 \\leq j \\leq i\\} \\)\n3. Cumulative median: Given \\( x \\in \\mathbb{R}^n \\), output \\( y \\in \\mathbb{R}^n \\) where each element \\( y_i \\) is the median of the first i elements of x, i.e. \\( y_i = \\text{median}\\{x_j | 1 \\leq j \\leq i\\} \\).\n4. Sorting: Given \\( x \\in \\mathbb{R}^n \\), output sort(x), a vector containing the entries of a sorted in ascending order.\n5. Cumulative maximum sum subarray: given \\( x \\in \\mathbb{R}^n \\), output \\( y \\in \\mathbb{R}^n \\) where each element \\( y_i \\) is the sum of elements of a maximum sum subarray of the first i elements of x, i.e. \\( y_i = \\max_{1 \\leq j \\leq k \\leq i} \\left( \\sum_{l=j}^k x_l \\right) \\)\nThe tasks selected were chosen to ensure a balanced representation of varying levels of complexity. Furthermore, we adopt cumulative versions of algorithms when feasible for several reasons: they naturally provide n to n training settings, they are more challenging than non-cumulative versions, and the non-cumulative versions for tasks such as summing and taking the minimum have trivial one-layer constructions for fixed input size n.\nExperimental setting: All tasks employ the same model configuration. The model uses the structure of Equation (1), augmented with encoding and decoding layers, which are linear operators.\nWe compare the standard Transformer, which utilizes the attention mechanism in Equation (3), and the positional Transformer, which employs the attention defined in Equation (2). Both variants share the same number of layers and dimensional configurations, with any specific differences explicitly noted. In all configurations, the total number of layers is set to \\( \\lceil\\log_2 n\\rceil + 1 \\), where n denotes the maximum input length, and each layer uses 2 attention heads. Along with each input sequence, we also append an empty scratchpad entry. This extra entry does not count toward the total number of layers and is not used to compute the loss. It is included solely to aid in the computation of the tasks. For the function \u0424 , we employ a 2-layer MLP with ReLU activation functions. The embedding dimension of the encoder and the hidden dimensions of the MLP are both set to 64."}, {"title": "6.1 Variable length inputs", "content": "In this section, we present value generalization results for models operating on variable-length inputs. This setting aims to verify the models' ability to generalize across different scales while maintaining the flexibility to handle inputs of varying lengths.\nValue Generalization: In this experiment, we evaluate the models' ability to process sequences of varying lengths up to a maximum size of n = 8. Specifically, the model is required to perform tasks on input sequences with lengths ranging from 1 to 8. We train models with 500,000 samples and ensure that all input lengths are equally represented. We then evaluate the OOD loss across different scale factors \\( c \\in \\{1, 2, ..., 10\\} \\). Note that when c = 1, the setting actually corresponds to in-distribution generalization. The losses reported are calculated using 3,000 samples for each scale. As shown in Figure 5, positional Transformers consistently outperform standard Transformers across all scales and tasks. Additionally, our architecture maintains robust OOD performance even in tasks where the output can exceed the input magnitude (e.g., sum and maximum sum subarray)."}, {"title": "6.2 Fixed length inputs", "content": "In this section, we present a more in-depth analysis of value generalization as a function of additional factors such as sample size and input length. Due to the resource demands of variable-length experiments, we present results obtained by training with a single fixed input length.\nSample Size vs. Value Generalization: In this setting, we fix the input length n = 8 and examine value generalization for c = 3, which is three times the training range, i.e., [-6, 6]. We then analyze OOD loss as a function of the number of training samples, ranging from 5,000 to 50,000. Figure 3 shows that for all tasks, the OOD loss of positional Transformers steadily decreases with an increasing number of samples, whereas the performance of standard Transformers remains roughly constant. Additionally, Appendix C provides training and validation error results, demonstrating that standard Transformers not only converge but also generalize well in-distribution. To rule out potential overfitting due to model complexity, Appendix C includes further analyses showing that standard Transformers with reduced depth also fail to value-generalize."}, {"title": "7 Limitations and future work", "content": "Our present work shows strong evidence that the positional attention mechanism is a better alternative to standard attention in the context of neural algorithmic reasoning. However, more research is needed to uncover the true potential of this mechanism. We identify three main future research directions which we believe are important.\n1. OOD generalization theory: It is very often the case that existing OOD generalization bounds are not tight (see Appendix F for an extended discussion). For specific tasks, there is often a gap between what theory says about the worst-case performance and what one observes empirically. This highlights the need for a more fine-grained analysis that will be able to capture the difference in OOD generalization capabilities among different architectures.\n2. Length generalization capability: Our current proposal uses fixed positional encodings, making it difficult to test it on bigger length inputs. Designing positional encodings that can work with arbitrary input lengths will allow us to explore the length generalization capabilities of positional attention.\n3. Complementary tasks: Testing the positional attention mechanism on complementary tasks, such as graph algorithms, requires special treatment. In particular, graph algorithms require that the model effectively process graph connectivity rather than merely treating it as input for the data matrix."}, {"title": "A.1 Massively Parallel Computation (MPC)", "content": "The MPC model is a computational model of the MapReduce framework, widely used for computations over massive datasets. It defines a parallel computing model that jointly executes a function across multiple machines, each constrained by limited memory capacity. The MPC model is capable of representing various parallel algorithms and is more powerful than other established parallel models, such as parallel random-access machine (PRAM) .\nFor completeness, we provide a simplified version of the definition of the MPC protocol by, which makes the connection to our PCOC model more apparent.\nDefinition 5 (MPC protocol, Def. I.1 , simplified). Let s be a parameter. There are p \u2265 1 machines (processors), each with local memory of size s. The input is distributed on the local memory of some of the machines. The computation proceeds in rounds. In each round, each machine computes the data in its local memory and sends messages to other machines at the end of the round. The total size of messages sent or received by a machine in a round is bounded by s. In the next round, each machine only holds the received messages in its local memory. At the end of the computation, the output is in the memory of some machines."}, {"title": "A.1.1 Relation between PCOC and MPC", "content": "Why we do not use the MPC model. We describe two main differences between MPC and PCOC, which justify introducing the latter. First, the MPC model requires explicit destination information for the data to be routed among the machines in the network. In our agnostic model, we no longer require the data to contain information about their destination. This information is provided by an oracle, which, in practice, can be realized as a learning problem. Second, the original MPC definition contains assumptions about the relations between the memory size s, the input size, and the number of machines p, as well as an assumption about the number of machines to which the input gets distributed. Although these assumptions make sense when implementing algorithms for MPC in practice, and we could consider such assumptions for our simulation results, it is unclear what additional value they provide within the context of neural algorithmic reasoning. Thus, they are not part of the definition of the PCOC model.\nPCOC can simulate MPC. For a given task, length n and input Data, it is easy to observe that PCOC can simulate an algorithm on the MPC model that does not utilize the memory and processor restrictions mentioned above. In such cases, PCOC allows two different simulation approaches for an MPC protocol.\nFirst, assuming the existence of an oracle, which has information about the communication at each round of a specific R-round MPC algorithm as well as the destinations of each element in the memories of the machines at each round, a PCOC algorithm on n machines and R rounds with the aforementioned oracle can simulate an R round MPC protocol on n machines. At each round, the oracle essentially routes all data according to the underlying MPC algorithm's requirements. The parameters for PCOC coincides with the one from the MPC model since no more data are routed among machines during the execution of the PCOC algorithm.\nAlternatively, PCOC can simulate MPC with a fixed oracle at each round, though it is significantly less efficient. This is feasible if the destinations are encoded within the data (even if not used for routing). In this scenario, the oracle sends all relevant data to all machines, allowing each local function to determine which memory slots should be used in the local computation based on the destinations. However, this method is considerably more expensive as each machine requires significantly more memory than the simulated MPC protocol. Moreover, the local functions become more complex, as they must conditionally execute computations based on the destination of each memory slot."}, {"title": "A.2 Illustration of parallel algorithms", "content": "In this section, we further expand on the discussion of Section 5, stating that communication in several parallel algorithms depends only on the identification of the machines rather than their current values. To illustrate this, we provide some concrete examples.\nThese tasks are examples of those presented in Section 6. Note that these illustrations do not indicate the computational graphs derived by our architecture, as there are multiple ways to achieve the same goal, and they do not necessarily involve the neat graph structures shown in Figure 6. For a more in-depth analysis of the results obtained by our architecture, we refer the reader to Appendix C.\nIn these computational graphs, we represent each machine by a circle, distinguished by a subscript (from 1 to 4, since n = 4). Furthermore, we use superscripts to denote the rounds of the algorithm, with superscript 0 representing the initial stage where no computation or communication is performed. Note that no specific values are provided in these examples. This indicates that the correct results can be achieved by following the computational graph for any set of values. In the subsequent rounds, each machine receives information from other machines (including itself) and performs some computation. For each algorithm in Figure 6, we will briefly describe the computations involved, noting that this is not the main focus of the paper and serves only as motivating examples.\nFor the computation of the minimum and the summing function, each machine applies the minimum (or sum)"}, {"title": "B.1 Hardmax patterns using positional attention", "content": "In this section, we show that the positional attention architecture in Equation (2) can approximate any unique hardmax pattern, a concept we define later in this section. We begin by stating the definition of the row-wise hardmax transformation for a p\u00d7 q matrix X from Section 3:\n\\begin{equation}\n\\text{hardmax}(X)_{i,j} = \\begin{cases} 1 & \\text{if } X_{i,j} = \\max_{k\\in[q]} X_{i,k} \\\\ 0 & \\text{otherwise} \\end{cases} \\quad \\text{for } i \\in [p], j\\in [q],\n\\end{equation}\nwhere we implicitly extend the definition for vectors in \\( \\mathbb{R}^n \\) by viewing them as 1 \u00d7 n matrices.\nWe use the term hardmax pattern to refer to any matrix in the image of hardmax (i.e. a binary matrix with at least one non-zero element in every row). Furthermore, we use the term unique hardmax pattern to refer to hardmax patterns with exactly one non-zero element in every row. Unique hardmax patterns occur when the input matrix has a unique maximum value in every row."}, {"title": "B.2 Positional Transformers simulate PCOC", "content": "We begin this section by outlining the key concepts utilized in the routing protocol employed in our constructions. First, we describe the general structure of the input matrix X."}, {"title": "B.2.1 Encoding", "content": "Input matrix: In alignment with the PCOC model, the input matrix X represents N machines, where each machine is denoted by a row in the matrix, and its local memory, \\( MEM_i \\in T^s \\), is represented by the corresponding columns. The maximum size of data that any machine can send or receive is s bits, with each bit corresponding to a column in X."}, {"title": "B.2.2 Simulation results", "content": "We now demonstrate that, with the established encoding, the architecture provided in Section 4 can simulate the execution of any PCOC instance. Each round of such a PCOC instance can be decomposed into two stages: communication and computation. Our objective is to provide existential results for both stages.\nIn the communication stage, routing assigns destination machines for each message. In our architecture, this assignment is analogously captured by the attention weights, which determine whether a message should be received by a node using binary values.\nThe no-collision assumption ensures that all routing patterns can be represented by unique hardmax patterns. As expressed in Lemma 1, since any unique hardmax pattern can be approximated by our attention layer using softmax, for simplicity, the subsequent proofs use hardmax instead of softmax. With all details now established, we re-state our main simulation result:\nTheorem 1. Consider a PCOC instance P with R rounds, N machines with local memory s, and data type \\( T = \\mathbb{R} \\). Let M be a model following the architecture in equation 1 with n = N + 1 nodes, R layers and s attention heads. Then, for any instance P with Borel measurable local functions, there exists a configuration of M that approximates P to any desired degree of accuracy."}, {"title": "B.3 Softmax patterns using positional attention", "content": "We conclude the discussion on expressivity by showing a final, standalone, result, namely that the positional attention architecture in Equation (2) can represent any softmax pattern. We begin by stating the definition of the row-wise softmax transformation for a matrix \\( X \\in \\mathbb{R}^{p\\times q} \\):\n\\begin{equation}\n\\text{softmax}(X)_{i,j} = \\frac{\\text{exp}(X_{i,j})}{\\sum_{k=1}^q \\text{exp}(X_{i,k})} \\quad \\text{for } i \\in [p], j\\in [q]\n\\end{equation}\nAs with hardmax, the definition is implicitly extended to vectors in \\( \\mathbb{R}^n \\) by viewing them as 1 \u00d7 n matrices. The image of the softmax function is the set of row-stochastic matrices with entries in (0,1). Indeed, it is easy to see that when softmax is applied to a matrix, the resulting matrix satisfies the above property. On the other hand, for a matrix B = \\( (b_{ij})_{i\\in[p],j\\in[q]} \\) with \\( b_{ij} \\in (0,1) \\) and \\( \\sum_{j\\in[q]} b_{ij} = 1 \\) for all \\( i \\in [p] \\) we have softmax(X) = B where \\( X_{i,j} = \\text{ln}(b_{ij}) \\). We use the term softmax pattern to refer to any matrix in the image of softmax.\nConsider attention weights \\( A^{(l,h)} \\) that are defined by positional encodings in equation 2. Let B \u2208 (0,1) n\u00d7n be a softmax pattern. We would like to find parameters \\( W_Q^{(l,h)} \\) and \\( W_K^{(l,h)} \\) that induce B, that is \\( A^{(l,h)} = B \\). From the properties of softmax described above, it suffices to solve the matrix equation (PW(,)). (PW())) = B where \\( B_{ij} = \\text{ln}(B_{ij}) \\). This equation always has a solution when \\( d_P = n \\) and P is invertible. We summarize the above observation in the following expressivity remark:\nRemark 2 (Positional attention is expressive). Positional attention can realize all softmax patterns at every layer provided that \\( d_P = n \\) and P is invertible. This is not necessarily true in the case of standard attention where, in subsequent layers, positional encodings are modified and, therefore, not guaranteed to be linearly independent."}, {"title": "C Experiments", "content": "This section presents detailed results for the experiments reported in Section 6. All experiments in this section are performed on lists of fixed length. Briefly, we examine the following:\n\u2022 The capability of more compact standard Transformers to achieve value generalization.\n\u2022 The capability of a standard Transformer with Rotary Positional Embedding (ROPE) to achieve value generalization\n\u2022 How the number of training samples affects value generalization.\n\u2022 The OOD performance of standard and positional Transformers for various fixed input lengths\nUnless otherwise specified, all configurations are consistent with those described in Section 6.2. Both training and testing lists are generated using the sampling strategy discussed in Section 6."}, {"title": "C.1 Value generalization of compact Transformers", "content": "This section presents additional value generalization results for simpler models, aiming to rule out potential overfitting caused by the excessive complexity of the standard Transformer. We examine two configuration variants: one with log n + 1 layers (4 layers) and another with a single layer. The plots also illustrate the outcomes for different hidden dimensions in the MLP. We report the value generalization results for the cumulative sum and cumulative minimum  tasks. As observed, in both cases, the OOD performance deteriorates as the network size decreases. Although single-layer networks exhibit slightly better performance, they remain inferior to the performance of positional attention reported in the main paper."}, {"title": "C.2 Value generalization for Transformers with Rotary Positional Embedding (ROPE)", "content": "We compare Positional Transformers with standard Transformers using Rotary Positional Embedding (ROPE), a widely adopted technique in natural language processing contexts, which has also been applied to algorithmic tasks . Even though RoPE manages to decrease the OOD test loss, this improvement is not enough to claim value generalization. Our architecture still performs significantly"}, {"title": "C.3 Sample size vs. Value Generalization experiments", "content": "In this section, we provide detailed results showcasing the training, validation, and OOD test performance for each of the five tasks as a function of the number of training samples used. From the results, we can draw two conclusions about the behavior of the models as the number of samples increases. First, both modes achieve better in-distribution performance. Second, only the positional Transformer achieves better OOD performance. The results for this experiment are presented in Figures 10 to 14."}]}