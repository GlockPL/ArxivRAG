{"title": "TE-SSL: Time and Event-aware Self Supervised Learning for Alzheimer's Disease Progression Analysis", "authors": ["Jacob Thrasher", "Alina Devkota", "Ahmed P. Tafti", "Binod Bhattarai", "Prashnna Gyawali"], "abstract": "Alzheimer's Disease (AD) represents one of the most pressing challenges in the field of neurodegenerative disorders, with its progression analysis being crucial for understanding disease dynamics and developing targeted interventions. Recent advancements in deep learning and various representation learning strategies, including self-supervised learning (SSL), have shown significant promise in enhancing medical image analysis, providing innovative ways to extract meaningful patterns from complex data. Notably, the computer vision literature has demonstrated that incorporating supervisory signals into SSL can further augment model performance by guiding the learning process with additional relevant information. However, the application of such supervisory signals in the context of disease progression analysis remains largely unexplored. This gap is particularly pronounced given the inherent challenges of incorporating both event and time-to-event information into the learning paradigm. Addressing this, we propose a novel framework, Time and Event-aware SSL (TE-SSL), which integrates time-to-event and event and data as supervisory signals to refine the learning process. Our comparative analysis with existing SSL-based methods in the downstream task of survival analysis shows superior performance across standard metrics. The full code can be found here: https://github.com/jacob-thrasher/TE-SSL", "sections": [{"title": "1 Introduction", "content": "Advancement in deep-learning-based medical image analysis have shown remarkable promise in revolutionizing the study of Alzeimer's Dementia (AD) through"}, {"title": "2 Methods", "content": "We consider a set of labeled training examples X with the corresponding labels Y. Since, we are interested in modeling disease progression, we consider a setup of survival analysis, where the labels y contains both time and event information. Specifically, for each instance i, Vi = (Ti, di), where Ti denotes the time to event or censoring and di is the event indicator, with d\u2081 = 1 if the event (disease progression) occurred, and d\u2081 = 0 if the data is censored. Here, censoring refers to instances where the event has not yet occurred. Importantly, this does not mean the event will never occur, only that it was not observed during the study. The goal of survival analysis is to model the survival function S(t) = P(T > t), which estimates the probability of an event not occurring by time t. Building upon this setup, we first learn appropriate representations using our proposed self-supervised learning approach, TE-SSL (section 2.1), which is a contrastive learning paradigm that utilizes the additional time labels afforded by a survival analysis to strengthen the pull of elements at similar stages of development to improve feature extraction. Fig. 1 provides an illustration of our proposed SSL framework. We then finetune the network with a task-specific objective function to predict survival outcomes at individual time points (section 2.2)."}, {"title": "2.1 TE-SSL: Time and Event-aware Self Supervised Learning", "content": "We provide a background of contrastive self-supervised learning, a learning paradigm designed to use unlabeled data to learn representations by performing some pretext task. We then provide details for enhancing the SSL learning"}, {"title": "Self-supervised learning", "content": "Self-supervised learning (SSL) is an unsupervised method for learning representations through pretext tasks, without labeled data. Formally, given training examples X, they are transformed into a modified version X using a set of transformations T to create multi-viewed examples. Transformations T include a set of augmentation functions that do not alter the intrinsic information contained within the data, thus creating a separate view of the original data. SSL frameworks are trained with these multi-viewed batches. For each index i \u2208 I = 1,2,..., 2N in such a multi-viewed batch, let j represent the corresponding index of the transformed pair to sample i. In such a setup, a loss function for the self-supervised learning objective can be represented as:\n$$\\mathcal{L}_{SSL} = \\sum_{i \\in I} \\log \\frac{\\exp (z_i \\cdot z_j/\\tau)}{\\sum_{a \\in A(i)} \\exp (z_i \\cdot z_a/\\tau)}$$\nwhere z is the corresponding projection of the input x, \u03c4 is a temperature parameter, and A(i) = I\u2212{i}. Since augmented images are semantically identical to one another, this method provides anchor points for the model to \"pull\" together two augmented views while \"pushing\" other samples in a batch."}, {"title": "Supervisory signal for SSL training", "content": "The standard SSL setup described above cannot maximize similarity between samples of the same class (e.g., disease category or event vs. censored) in a batch due to the absence of a supervisory signal. While it may seem that standard SSL could eventually differentiate such attributes over time, recent work has shown that incorporating supervisory signals significantly enhances the standard SSL setup [11].\nTo incorporate supervisory signals using available label information (in our case, event labels), the SSL objective (in Eqn. 1) can be generalized as follows:\n$$\\mathcal{L}_{E-SSL} = \\sum_{i \\in I} -\\log \\frac{\\frac{1}{|P(i)|} \\sum_{p \\in P(i)} \\exp (z_i \\cdot z_p/\\tau)}{\\sum_{a \\in A(i)} \\exp (z_i \\cdot z_a)/\\tau}$$\nwhere P(i) = {p \u2208 A(i)|\u1ef9p = \u1ef9i} represents the set of indices for all positive samples in the multi-viewed batch that are distinct from i, and |P(i)| is the cardinality of this set. If sample i has an event indicator during the study period, this objective classifies all samples with an event indicator as positive cases and those that are censored as negative cases, and vice-versa. Clinically, this would translate to maximizing feature representations of multiple patients' imaging data (within a batch) who eventually convert to AD."}, {"title": "Time-to-event and Event labels for SSL training", "content": "Progression analysis uniquely benefits from having both event indicators and time-to-event information as labels, providing a comprehensive view of patient outcomes. When considering disease progression, it is reasonable to assume that the features of two patients at similar stages of progression will be more similar than those from a late-stage and early-stage patient pair. We therefore hypothesize that utilizing both types of labels in SSL training enhances the learning process, leading to nuanced models that can accurately predict the event timing and occurrence, thus significantly improving disease progression analysis.\nTowards this, we devise weighing schemes for each sample in the batch relative to the anchor point, based on their time-to-event information. We calculate the time difference between the anchor point and other samples in a batch, using the maximum and minimum differences to assign weights to each pair. These weights determine the strength of feature similarity, enforcing that patients at similar stages in development will have a stronger pull than an early/late stage patient pair. Our proposed SSL learning objective function incorporating both time-to-event and labels is represented as:\n$$\\mathcal{L}_{TE-SSL} = \\sum_{i \\in I} -\\log \\frac{\\frac{1}{|P(i)|} \\sum_{p \\in P(i)} w_{i,p} \\exp (z_i \\cdot z_p/\\tau)}{\\sum_{a \\in A(i)} w_{i,a} \\exp (z_i \\cdot z_a)/\\tau}$$\nwhere the weight term w*,* for each anchor point is calculated as:\n$$w_{i,j} = \\frac{\\alpha - \\beta}{s-l} \\Delta_{i,j} + \\frac{\\beta l - \\alpha s}{s-l}$$\nwhere Ai,j = |T; - T;| is the time difference associated with data points i and j. Additionally, we compute A = {\u2206i,j[i,j\u2208 I,i \u2260 j} and take s = min A and l = max A to establish the maximum and minimum time span differences between samples in the batch. Finally, a and \u1e9e serve as hyperparameters defining the maximum and minimum weight values, respectively. Specifically, the pair with the smallest time difference is assigned the highest weight, a, and the pair with the largest time difference receives the lowest weight, \u03b2."}, {"title": "2.2 Time-to-event prediction", "content": "To leverage the feature space learned with SSL frameworks, we construct a deep learning framework consisting of an encoder network E(\u00b7) and a projection head P(.). During pretraining (Section 2.1), multiviewed data (original and it's augmented copy X) is passed through the encoder module to obtain r = E(X) \u2208 Rde, where de is the dimension of r. A final representation z = P(r) \u2208 RdP (dp is the dimension z) is computed and normalized for the pretraining procedure. After pretraining, P(.) is discarded and replaced with task-specific head, which is then finetuned together with the encoder network on the time-to-event objective.\nFor our task head, we adopt the DeepHit framework [13], a deep learning approach to survival analysis. With DeepHit framework, instead of predicting a single hazard coefficient for a given input, we output a distribution of hazards at discrete time points. This allows the model to learn the first hitting times (predicted time until the occurrence of the first event of interest for each subject) directly without making assumptions about the underlying form of the data. In specific, the model learns to minimize the loss function Ltotal = L1 + L2, where L\u2081 is the log-likelihood of the distribution of the hitting time, defined as\n$$\\mathcal{L}_1 = - \\sum_{i=1}^{N} [1(\\delta_i = 1) * \\log h_i + 1(\\delta_i \\ne 1) * \\log (1 - F(T_i|x_i)]$$\nwhere, 1 is an indicator function evaluating to 1 iff d\u2081 = 1 (event occurred), hit corresponds to the predicted hazard for input X\u2081 at time T\u2081, and F(Ti|xi) is the estimated cumulative incidence function (CIF) which approximates the probability that the event will occur on or before time Ti. L2 incorporates a combination of cause-specific ranking loss functions and is defined as:\n$$\\mathcal{L}_2 = \\sum_{i\\ne j} A_{i,j} \\exp(F(T_i|x_i), F(T_i|x_j))$$\nwhere y is a hyperparameter which indicates the intensity of the ranking loss and Ai,j = 1(Ti < Tj) represents an indicator function which evaluates to 1 if a pair (i, j) experience an event at different times."}, {"title": "3 Experiments and Results", "content": "ADNI dataset: Our data consists of a cohort of 493 unique patients in the Alzheimer's Disease Neuroimaging Initiative (ADNI) [16] dataset. Each subject has one or more visits containing a 3D T1-weighted MR Image, yielding a total of 2007 data points. Patients are diagnosed as being cognitively normal (CN), having mild cognitive impairment (MCI), or Alzheimer's dementia (AD) at every visit. We define converters as subjects whom were CN or MCI during their initial visit, but developed AD within the duration of the study. Additionally, each visit contains the number of months since the baseline observation, which acts as the time-to-event signal. The data were preprocessed via the pipeline laid out by [14] and divided based on the unique participants to avoid data leakage. For patients with multiple visits, we treat each visit as a unique data point.\nImplementation details: We utilize a 3D CNN adapted from [14] as our backbone MRI encoder for both pretraining and finetuning tasks. The encoder takes in X \u2208 RN\u00d796\u00d796\u00d796 and outputs a representation E(X) = r \u2208 RN\u00d71024, where N is the batch size.\nPretrain phase: Contrastive based SSL techniques require large batch sizes to train properly. Due to hardware constraints, we selected N = 16 and accumulated gradients for 8 iterations before backpropagation to simulate a batch size of 128. The encoded representation r is then passed through the projection head to achieve z = P(r) \u2208 RN\u00d7128. We assigned a temperature of 7 = 0.07 for all contrastive loss functions and optimized the model using LARS with a learning rate of .3 \u00d7 N/256 = .15 [8] and a momentum of 0.9."}, {"title": "3.1 Results", "content": "Our primary results in Table 1 showcase a comparison between our E-SSL and TE-SSL frameworks and baseline models: No Pretraining and SSL. For fairness, all models, including E-SSL and TE-SSL, were finetuned using the same model as No Pretraining. We trained each model with three random seeds and reported their average results. Our frameworks outperform others in both C-td (higher is better) and IBS (lower is better) metrics. Notably, E-SSL introduces the novel use of event labels in SSL training for progression analysis, while TE-SSL's innovative incorporation of both time-to-event and event information leads to the best performance, highlighting its efficacy in progression analysis."}, {"title": "3.2 Ablation analysis", "content": "We conducted the ablation analysis to better understand the roles of a and \u1e9e in our proposed TE-SSL (Eqn. 4), which serve to define the intensity of the weight values for a pair of inputs (i, j). The difference between a and \u1e9e dictates how strongly to differentiate distant pairs. For instance, in a batch of N samples if (i, j) represents the pair with the largest time difference, setting \u03b2 = 0, effectively considers j as a negative sample relative to the anchor i. Therefore, we explored sensible configurations of a and \u03b2, with 1 \u2264 a \u2264 1.5 and 0.5 \u2264 \u03b2 \u2264 1, noting that an a - \u03b2 < 0.5 would inappropriately diminish the negative impact of distant pairs. The results, presented in Table 2, demonstrate the method's relative stability within these selected ranges. It is also noteworthy that TE-SSL outperforms both standard SSL and the baseline no-pretraining time-to-event prediction model in five out of six experiments, highlighting its efficacy."}, {"title": "4 Conclusion", "content": "We introduce the Time and Event-aware SSL framework, which integrates both event and time-to-event information to guide the learning process of feature representations. As demonstrated, our approach surpasses existing self-supervised learning methods, including those supervised versions that incorporate only the event label. This underscores the critical importance of utilizing both event and time-to-event information in the progression analysis of Alzheimer's disease. Our evaluation using the ADNI dataset showcases the practical applicability and effectiveness of our proposed method, significantly contributing to the advancement of AD progression study."}]}