{"title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples", "authors": ["Chengqian Gao", "Haonan Li", "Liu Liu", "Zeke Xie", "Peilin Zhao", "Zhiqiang Xu"], "abstract": "The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: \"Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity.\" Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs.", "sections": [{"title": "1. Introduction", "content": "Data selection focuses on identifying the most valuable subset of data from a dataset while excluding ineffective samples (Albalak et al., 2024). It significantly improves the first two stages of training large language models (LLMs): pre-training (Lee et al., 2021; Penedo et al., 2023; Tang et al., 2024) and supervised fine-tuning (SFT) (Cao et al., 2023; Qin et al., 2024; Zhou et al., 2023), by adhering to well-established principles. However, in the third stage, i.e., preference alignment, data selection principles are often implicit and superficial, potentially limiting the alignment between LLM outputs and human preferences (Askell et al., 2021; Weidinger et al., 2021).\nPrior studies in alignment underscore the importance of selecting error-free data by demonstrating the presence and negative impacts of mislabeled data (Wang et al., 2024a; Gao et al., 2024), noisy feedback (Mitchell, 2023; Chowdhury et al., 2024), and data with low annotator agreement (Argilla, 2024). Those practices implicitly assume that all error-free data are beneficial for alignment regardless of the model's capacity. However, we argue this assumption overlooks the relationship between data difficulty and model capacity. Our experiments show that overly difficult examples not only fail to improve alignment but can actually hinder the performance (see Figure 1). This observation motivates our systematic investigation into how example difficulty affects alignment performance.\nOur main contribution is a new principle for preference data selection, which emphasizes the match between model capacity and example difficulty:\nPreference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity.\nThis principle has three key claims: (1) preference data can be categorized by difficulty levels, (2) overly difficult examples can harm alignment performance, and (3) difficulty is relative to the model's capacity-larger models, with greater capacity, can benefit from more difficult examples. We validate this principle through systematic experiments. Specifically:\nPreference examples vary in difficulty level (Section 3). We show that, in DPO (Rafailov et al., 2024), the order in which testing examples are correctly classified by the implicit reward model is consistent across different runs and training data. This robust ordering reflects the existence of inherent example difficulties. Based on this observation, we use validation loss as a computational proxy to systematically identify and rank example difficulty.\nDifficult examples hinder alignment (Section 4). We reveal that difficult examples-identified by high validation loss-significantly hinder alignment. Our experiments across two datasets and four pre-trained models show consistent performance drops when including these difficult examples. These challenging examples emerge naturally during data collection, rather than through artificial construction. This highlights the imperfections of the previous principle and calls for a new data selection principle for alignment tasks.\nDifficult examples exceed the model's capacity (Section 4). We demonstrate that example difficulty interacts directly with model capacity. Experiments with models of 3B, 8B, and 14B parameters show that larger models benefit from higher proportions of difficult examples, confirming that difficulty must be calibrated to the model's capacity.\nFiltering out overly difficult examples yields remarkably gains (Section 5 and 6). Finally, we validate our principle with a new method, Selective DPO, which filters out overly difficult examples. This approach achieves a 9-16% higher win rate on AlpacaEval 2 (Dubois et al., 2024) compared to standard DPO (Rafailov et al., 2024), outperforming state-of-the-art methods such as SimPO (Meng et al., 2024) and R-DPO (Park et al., 2024) while maintaining better perplexity and implicit reward margins."}, {"title": "2. Preliminaries", "content": "2.1. Preference Alignment with DPO\nPreference alignment (Ouyang et al., 2022) aims to align the outputs of LLMs with human ethics and styles, ensuring that these models are safe, reliable, and effective for real-world applications (Christiano et al., 2017). In this study, we focus on direct preference optimization (DPO) (Rafailov et al., 2024), a method known for its simplicity and robust performance in alignment tasks (Dubey et al., 2024). DPO trains a policy model, \\( \\pi_{\\theta} \\), on a dataset D containing prompt x, preferred response yw, and rejected response yr. The training objective incorporates a reference SFT model, \\( \\pi_{ref} \\), and a hyper-parameter, \\( \\beta \\), to control the divergence between \\( \\pi_{\\theta} \\) and \\( \\pi_{ref} \\):\n \\( L_{DPO}(\\pi_{\\theta}, D) = = -E_{(x,y_w,y_l)\\sim D} \\bigg[ log \\sigma \\bigg( \\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)} \\bigg) \\bigg] \\)."}, {"title": "2.2. Quantifying the Example Difficulty", "content": "Learned Step as a Measure of Difficulty. An example's learned step is defined as the earliest training step after which the model reliably distinguishes preferred responses from rejected answers. This is formalized as:\n \\( LS(x, y_w, y_l) = \\min_{t \\in T} \\bigg\\{ t | \\beta log \\frac{\\pi_{\\theta_t}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_{\\theta_t}(y_l|x)}{\\pi_{ref}(y_l|x)} > \\delta, \\forall t' > t \\bigg\\} \\).\nA similar metric has been explored by Wu et al. (2021). The difference is that we calculate Eq. (2) exclusively on held-out examples, ensuring it reflects intrinsic difficulty rather than the order of data presentation (Zhu et al., 2024a). Higher learned steps indicate more difficult examples. For all experiments, we set d = 0.4.\nValidation Loss as an Alternative Difficulty Proxy. We borrow validation loss (Wu et al., 2021; Rampp et al., 2024) as a computationally cheaper alternative to the learned step. Specifically, for a specific example (x, yw, Y\u0131) from D\\D, validation loss is defined as:\n \\( VL(x, y_w, y_l) = log \\sigma \\bigg( \\beta log \\frac{\\pi_{\\hat{\\theta}}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_{\\hat{\\theta}}(y_l|x)}{\\pi_{ref}(y_l|x)} \\bigg) \\),"}, {"title": "3. Preference Examples Vary in Difficulty", "content": "One surprising finding here is that the order in which examples are learned is remarkably consistent across runs. Such robustness reveals the underlying presence of example difficulty. We then validate the effectiveness of validation loss as a measure of example difficulty for alignment tasks."}, {"title": "3.1. The Underlying Example Difficulty", "content": "While various metrics such as length (Spitkovsky et al., 2010; Tay et al., 2019; Nagatsuka et al., 2023) and perplexity (Wu et al., 2024) have been proposed to measure difficulty of text samples, their ability to reliably capture example difficulty remains controversial (Campos, 2021). We address this concern by demonstrating: (1) examples have distinct learned steps (see Eq.2), indicating different difficulty levels, and (2) these learned steps are consistent across runs with different training data and random seeds.\nIn Figure 2 (left), we visualize the learned steps of 300 test examples from Ultrafeedback-binarized,where darker colors indicate more training steps needed for model comprehension. Results from 10 runs show consistent learning order across different models (Jiang et al., 2023; AI@Meta, 2024; Team et al., 2024) varying in size (2B-9B), training stage, and data sampling. This consistency confirms that examples vary in difficulty, allowing us to discuss difficult examples without debating various definitions of difficulty."}, {"title": "3.2. Validation Loss as a Proxy for Learned Step", "content": "The robust learning order suggests the existence of difficult examples some examples are consistently harder for LLMs to understand. However, identifying these examples at scale is computationally expensive, as the computing of learned step requires evaluating the model after each gradient update. To address this, we adopt validation loss from the curriculum learning literature (Wu et al., 2021; Rampp et al., 2024) (see Eq(3)). Specifically, we train six reference models using the DPO objective on the randomly sampled half training set and evaluate the validation loss for examples on the other half. We refer the difficult examples to examples with large validation loss."}, {"title": "Definition 3.1 (Difficult example).", "content": "A preference example (x, yw, yl) is considered a difficult example if its validation loss exceeds or equals a specified value:\n \\( VL(x, y_w, y_l) \\geq Q(\\tau) \\).\nWe introduce a flexible threshold Q(T), i.e., the T-quantile of the validation loss distribution. This variability arises mainly from: (1) There is no formal definition of the easy and difficult samples (Zhu et al., 2024b), and (2) Different pre-train models have different training dynamics and thus different scales and distributions of validation loss.\nTo assess whether validation loss effectively approximates the learned step, we examine the correlation between difficulty rankings produced by these two measures. Using Spearman's rank correlation, we compared rankings across different runs and models. As shown in the middle panel of Figure 2, validation loss exhibits patterns remarkably similar to the learned step. Furthermore, the high correlation coefficients between average learned step and average validation loss across the four models (0.9258, 0.9227, 0.9336, and 0.9283) validate the effectiveness of validation loss as a computationally efficient proxy for learned step. Additionally, the Jaccard similarity between difficult example sets (defined as top 50% by either metric) remains consistently high for each model (Figure 2, right), confirming that both measures identify similar sets of difficult examples."}, {"title": "4. Difficult Examples Hinder the Alignment", "content": "In this section, we first demonstrate that difficult examples significantly degrade alignment performance across various datasets and model scales. We then investigate the factors that contribute to their difficulty through a series of systematically designed empirical studies."}, {"title": "4.1. Investigation Setup", "content": "Models: We evaluate SFT models trained on the UltraChat-200k dataset: Mistral-7B-SFT (Jiang et al., 2023), Qwen-2.5-7B-SFT (Team, 2024), Llama3-8B-SFT (AI@Meta, 2024) and Gemma-2-9B-SFT (Team et al., 2024). We focus on SFT models as they better demonstrate the effects of alignment procedures (Meng et al., 2024).\nDatasets: We use UltraFeedback-binarized, a widely adopted alignment dataset (Tunstall et al., 2023; Meng et al., 2024; Zhou et al., 2024; Pattnaik et al., 2024), and Argilla-dpo-mix-7k, a small but high-quality dataset.\nHyper-parameters: Following prior work, we set \u03b2 = 0.01 (Zhou et al., 2024). The learning rate is sweeped for DPO with random ordering and directly applied to DPO with other settings. We conduct the alignment with one epoch following Meng et al. (2024).\nEvaluation: We employ WR', the win rate against gpt-4-turbo on 805 testing examples from AlpacaEval 2 (Dubois et al., 2024) with ArmoRM (Wang et al., 2024b), a reward model with impressive performance on the Reward-Bench (Lambert et al., 2024), as the evaluator."}, {"title": "4.2. Difficult Examples Hinder Preference Alignment", "content": "As shown in Figure 3, training on difficult examples leads to significant performance declines. We compare three example-ordering strategies: (1) random ordering (standard DPO), (2) easy-to-difficult sorting by validation loss, and (3) random ordering with only easy examples. Despite using the same training recipes, models consistently perform better when trained on easier examples across all four architectures and both datasets. Notably, the benefits are mainly unlocked by excluding difficult examples rather than the ordering itself, as shown by the similar performance of sorted and shuffled easy examples (setting 2 and 3).\nThe performance drop from difficult examples is more pronounced in Ultrafeedback-binarized. This aligns the observation that Ultrafeedback-binarized contains mislabeled examples (Argilla, 2024; Bartolome et al., 2023) and Argilla-"}, {"title": "4.3. Difficult Examples Are Not Necessarily Data Errors", "content": "Before proposing our solution to filter out difficult and harmful examples, we here investigate their characters, ensuring their removal is justified. For statistics and case study on difficult examples, please refer to Appendix E and F.\nMislabeled data (Figure 4 (a)). Prior work suggests that difficult examples might be mislabeled (Argilla, 2024; Bartolome et al., 2023). To test this hypothesis, we sort the examples by their validation loss and flip the labels of last 40% (the most difficult) examples. However, this modification does not alleviate the performance drop, suggesting that label noise is not the primary cause.\nDistribution shift (Figure 4 (b)). Another possibility is that difficult examples represent a distinct distribution, causing catastrophic forgetting when models transition from easy to difficult examples. We test this using e-greedy sorting: each mini-batch contains e portion of randomly sampled examples and (1 - \u20ac) portion of examples sorted by validation loss. This ensures continuous exposure to both distributions, yet shows no improvement over the greedy sorting.\nLearning rate sensitivity (Figure 4 (c)). We argue that the performance drop is not caused by simply the improper learning rate. We investigate this by varying the learning rate. However, adjusting the learning rate neither alleviates performance drops nor delays the decline, demonstrating that the issue is unrelated to improper optimization settings."}, {"title": "4.4. Difficult Example Exceeds Model's Capacity", "content": "We hypothesize that difficult examples represent tasks beyond the model's current capabilities, requiring larger models to properly understand the nuanced preference differences. To validate this hypothesis, we conduct experiments using Qwen-2.5 models (Team, 2024) of three sizes: 3B, 7B, and 14B. The dataset is Argilla-dpo-mix-7k. Figure 5 shows a clear relationship between model size and manageable example difficulty: the optimal percentage of training data (the sweet spot) increases from 64% for the 3B model to 81% for the 14B model. This scaling pattern demonstrates that larger models can effectively learn from more difficult examples, confirming the direct relationship between model capacity and example difficulty threshold."}, {"title": "5. Selective DPO", "content": "Having verified the three key claims underpinning our data selection principle, we are now well-positioned to propose an instantiated algorithm, Selective DPO. It extends the standard DPO (Rafailov et al., 2024) by selectively training on examples within the model's capacity. The algorithm consists of three main steps, as illustrated in Figure 6:\nTrain reference models. The training dataset is randomly split into two partitions. Using the standard DPO loss (Eq. 1), SFT models are trained separately on each partition, resulting in two reference models per split. This process is repeated three times, yielding six reference models. Unlike the reference SFT model used in the DPO objective to control KL divergence, these reference models are specifically employed for computing validation loss.\nRank examples by their validation loss. The trained reference models evaluate held-out examples from their respective complementary partitions (D\\D). Each example is assessed three times using different reference models, and the mean validation loss is computed to rank the examples in ascending order.\nAlign with the selected data. The easiest examples, comprising the lowest 7 percent of validation loss rankings, are selected for alignment training. The alignment algorithm, such as DPO, is applied exclusively to these examples. To fully utilize the difficulty ranking, examples are processed sequentially from easy to difficult."}, {"title": "Remark 5.1 (Flexible hyper-parameter \u03c4).", "content": "The hyper-parameter 7 determines the percentage of data selected for training. Optimal 7 values depend on the data difficulty distribution and the model's capacity. In practice, T can be tuned using a third-party evaluator such as AlpacaEval 2 (Dubois et al., 2024).\nFor the evaluation in the next section, we set T 50 for the UltraFeedback-binarized dataset, based on insights from Figure 3. For clarity and reproducibility, pseudo-code for Selective DPO is provided in Appendix A."}, {"title": "6. Experiments", "content": "We evaluate the benefits of the proposed preference data selection principle by benchmarking the Selective DPO algorithm, against state-of-the-art alignment algorithms using the formal benchmarks: AlpacaEval 2 (Dubois et al., 2024), Arena-Hard v0.1 (Li et al., 2024b), and MT-Bench (Zheng et al., 2023). We report scores following each benchmark's evaluation protocol."}, {"title": "6.1. Performance Comparison", "content": "Baselines. We compare Selective DPO with DPO (Rafailov et al., 2024) and its variants, including IPO (Azar et al., 2023), KTO (Ethayarajh et al., 2024), and ORPO (Hong et al., 2024), borrowing their results from the SimPO paper (Meng et al., 2024) to ensure consistency. For SimPO"}, {"title": "6.3. In-Depth Analysis of DPO vs. Selective DPO", "content": "Selective DPO outperforms DPO in terms of likelihood distribution and reward margin distribution. As shown in Figure 8(c), Selective DPO achieves a distribution of negative log-likelihoods (NLLs) closer to zero on test prompts, indicating higher confidence in generated responses. Additionally, the implicit reward model learned by Selective DPO exhibits better accuracy and larger reward margins on testing examples (Figure 8(d)). Together, these results suggest that by filtering out overly difficult examples, Selective DPO produces more robust reward models and reduces undesired hallucinations."}, {"title": "6.4. Weak-to-Strong Curriculum", "content": "To investigate whether difficult examples can be identified using smaller reference models, we compare alignment experiments where a 7B SFT model is trained with its own curriculum versus a curriculum derived from a smaller 3B model. Results in Figure 9 show moderate benefits from the smaller model's curriculum, though slightly inferior to the model's own curriculum. This suggests that while smaller models can provide insights, data selection remains more effective when tailored to the target model's capacity."}, {"title": "7. Related Work", "content": "Response selection. The importance of selecting high-quality responses as preferred choices has been highlighted in several studies (Bai et al., 2022; Ethayarajh et al., 2022; Tunstall et al., 2023). These works focus on ensuring that preferred responses align with human values and ethics. Our work builds upon these efforts in two key ways: (1) the datasets we consider already incorporate these response selection techniques, and (2) we prioritize whether preference examples fall within the capabilities of the target LLM, rather than solely emphasizing their alignment with human values. Data correction. Efforts to address noisy labels in datasets include techniques such as label flipping (Wang et al., 2024a) and confidence-based data filtering (Gao et al., 2024). Approaches like cDPO (Mitchell, 2023) and rDPO (Chowdhury et al., 2024) aim to mitigate the impact of mislabeling without explicitly removing mislabeled examples. In our study, we incorporate label flipping and label smoothing experiments to support our claim that difficult examples are not necessarily mislabeled, but instead exceed the model's capacity. Seemingly relevant work. Our study differs from general data selection research, such as Liu et al. (2024); Xia et al. (2024), which uses the term alignment but actually focuses on the SFT stage. For a comprehensive review of data selection for LLMs and curriculum learning, we refer readers to Appendix B."}, {"title": "8. Conclusion and Future Work", "content": "In this work, we reveal and address a critical gap in LLM alignment: the mismatch between data difficulty and model capacity. Challenging the assumption that more clean data uniformly improves alignment, we propose a novel principle for alignment tasks:\nPreference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity.\nComprehensive experiments validate the three key claims underlying this principle. Building on this data selection principle, we introduce Selective DPO, an alignment algorithm that selectively trains on examples within the model's capacity. Selective DPO achieves state-of-the-art results on benchmarks including AlpacaEval 2, Arena-Hard, and MT-Bench, with up to 16% gains in win rates over DPO. Our work advocates a paradigm shift in alignment: alignment should prioritize data difficulty relative to model capacity rather than treating all preference data equally.\nHowever, limitations remain: (1) Selective DPO tends to favor longer responses due to potential data bias; and (2) the proposed principle is designed and validated specifically for the DPO setting, limiting its direct applicability to RLHF. These gaps highlight opportunities for future work."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the alignment between large language model behaviors and human values. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "Algorithm 1 Selective DPO", "content": "Input:\nSFT: An SFT model that serves as the starting point for preference alignment.\nD: A dataset consisting of preference examples.\nRandomSampler: A utility for sampling elements randomly without replacement.\nSequentialSampler: A utility for sampling elements sequentially.\nLDPO: DPO loss function with the form: \\( L_{DPO}(x, y_w, y_l) = log \\sigma \\bigg[ \\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)} \\bigg] \\).\n # Step 1: Train six reference alignment models: \\( \\pi_{\\theta_{01}}, \\pi_{\\theta_{02}}, \\pi_{\\theta_{11}}, \\pi_{\\theta_{12}}, \\pi_{\\theta_{21}}, \\pi_{\\theta_{22}} \\).\n for t = 0, 1, 2 do\n Randomly split the dataset D into two subsets, D\u2081 and D2.\n Initialize \\( \\pi_{ref} \\) \u2190 SFT and \\( \\pi_{\\theta} \\) \u2190 SFT.\n while RandomSampler has not finished do\n Sample a mini-batch of examples from D\u2081 using RandomSampler.\n Update \\( \\pi_{\\theta} \\) by minimizing the DPO loss function: \\( \\pi_{\\theta} \\) \u2190 arg \\( \\min_{\\pi_{\\theta}} E_{(x,y_w,y_l)\\sim D_1} \\bigg[ L_{DPO}(x, y_w, y_l) \\bigg] \\).\n end while\n Save the model: \\( \\pi_{\\theta_{t1}} \\) \u2190 \\( \\pi_{\\theta} \\).\n Reinitialize: \\( \\pi_{\\theta} \\) \u2190 SFT.\n while RandomSampler has not finished do\n Sample a mini-batch of examples from D2 using RandomSampler.\n Update \\( \\pi_{\\theta} \\) by minimizing the DPO loss function: \\( \\pi_{\\theta} \\) \u2190 arg \\( \\min_{\\pi_{\\theta}} E_{(x,y_w,y_l)\\sim D_2} \\bigg[ L_{DPO}(x, y_w, y_l) \\bigg] \\).\n end while\n Save the model: \\( \\pi_{\\theta_{t2}} \\) \u2190 \\( \\pi_{\\theta} \\).\n Reinitialize: \\( \\pi_{\\theta} \\) \u2190 SFT.\n end for\n # Step 2: Rank examples by their validation loss.\n for each example (x, yw, Y\u0131) in dataset D do\n Compute the validation loss using the three held-out reference alignment models:\n \\( VL(x, y_w, y_l) = \\frac{1}{3} ( L_{DPO}^{\\pi_{\\theta_{01}}} or L_{DPO}^{\\pi_{\\theta_{02}}} ,  L_{DPO}^{\\pi_{\\theta_{11}}} or L_{DPO}^{\\pi_{\\theta_{12}}} , L_{DPO}^{\\pi_{\\theta_{21}}} or L_{DPO}^{\\pi_{\\theta_{22}}} ) \\).\n end for\n Sort the examples by their validation losses and select the top 50% of examples to form \\( D_{selected} \\).\n # Step 3: Conduct alignment on the selected data Dselected.\n while SequentialSampler has not finished do\n Sample a mini-batch of examples from Dselected using SequentialSampler.\n Update \\( \\pi_{\\theta} \\) by minimizing the DPO loss function: \\( \\pi_{\\theta} \\) \u2190 arg \\( \\min_{\\pi_{\\theta}} E_{(x,y_w,y_l)\\sim D_{selected}} \\bigg[ L_{DPO}(x, y_w, y_l) \\bigg] \\).\n end while\nOutput:\n\\( \\pi_{\\theta} \\): The aligned model obtained by Selective DPO."}, {"title": "B. Related Work", "content": "B.1. Data Selection for Pre-Training\nSelecting training corpus brings significant performance gains in the pre-training stage (Wenzek et al., 2019; Mann et al., 2020; Zhao et al., 2023a; Penedo et al., 2023; Tang et al., 2024). Approaches can be broadly categorized into two categories: Sample-level selection focuses on filtering out undesired content such as non-target languages, duplicated data, toxic materials, and low-quality information (Albalak et al., 2024). This is often achieved through model-based filters (Joulin, 2016; Engstrom et al., 2024; Wettig et al., 2024) or heuristic filters (Wenzek et al., 2019; Lee et al., 2021; Lauren\u00e7on et al., 2022), each applying specialized filters for specific objectives. Token-level selection, an emerging strategy, down-weights low-quality tokens to enhance data quality (Lin et al., 2024), complementing sample-level filtering.\nB.2. Data Selection for Supervised Fine-Tuning\nRecent study suggests that SFT changes only the format of generation (Zhou et al., 2023). In light of this, various methods are proposed for finding the most informative subset for SFT, mainly following three principles: data quality, diversity, and importance (Qin et al., 2024). The measurement of data quality can be manual indicators such as the linguistic DQI (Mishra & Sachdeva, 2020), human scores (Zhou et al., 2023). Model-based quality measurement includes predictions from ChatGPT (Chen et al., 2024a), reward models (Cao et al., 2023), small reference models (Ankner et al., 2024) and the LLM itself (Li et al., 2024a). Measurements of data diversity are mainly manually defined, such as the source diversity (Mukherjee et al., 2023; Wang et al., 2023) and distance in the embedding space (Wu et al., 2023; Xu et al., 2023; Du et al., 2023; Liu et al., 2024). Data importance, which evaluates an example's contribution to a specific task, measured using performance scores (Engstrom et al., 2024), data influence models (Yu et al., 2024), or relevance to desired skills (Chen et al., 2024b).\nB.3. Scoring the Example Difficulty\nScoring data difficulty is central to curriculum learning, which prioritizes training on simpler examples before progressing to more complex ones (Bengio et al., 2009). Heuristic scoring functions mirror human priors of difficulty understanding, such as sentence length (Spitkovsky et al., 2010; Tay et al., 2019; Nagatsuka et al., 2023), word rarity (Chang et al., 2021), and linguistic perplexity (Campos, 2021). In contrast, principled scoring functions leverage model behavior to indicate example difficulty, including reward margins from third-party reward models (Croitoru et al., 2024), model perplexity on responses (Wu et al., 2024), or attention scores from transformer models (Kim & Lee, 2024). In this work, we employ two principled scoring measures, demonstrating their robustness and consistency in ranking examples. This allows us to analyze difficult examples objectively, avoiding ambiguities inherent in heuristic definitions.\nB.4. Curriculum Learning for Alignment\nCurriculum learning (CL) mimics human cognition by structuring learning from simpler to more complex concepts (Avrahami et al., 1997; Bengio et al., 2009). However, CL remains a highly debated technique. While some studies show that it accelerates convergence, enhances generalization, and/or improves robustness in models like convolutional neural networks (Jiang et al., 2014; Tudor Ionescu et al., 2016), recurrent neural networks (Zaremba & Sutskever, 2014; Sachan & Xing, 2016), transformers (Platanios et al., 2019), and diffusion models (Croitoru et al., 2023), other research finds little or no benefit (Platanios et al., 2019; Campos, 2021; Wu et al., 2021). In preference alignment for LLMs, the results are similarly mixed. Kim & Lee (2024) explored CL for preference alignment and concluded that sorting examples according to prompt length and attention score offered no clear benefits. On the other hand, Pattnaik et al. (2024) reported positive results, although with other tricks such as multiple candidate pairs data and iterative reference model. Our study suggests that CL, when paired with robust difficulty scoring, can positively impact LLM alignment by aligning data difficulty with model capacity."}, {"title": "C. Experiment Details", "content": "C.1. Computation Environment\nAll training experiments in this paper were conducted on compute nodes equipped with 8 \u00d7 H100 GPUs. To facilitate reproduction with limited computational resources, we also provide key benchmarking results for selected models trained using 4 x A100 40G GPUs with LoRA. Reproducing our SelectiveDPO on 7B models takes about 8 GPU hours (H100).\nC.2. SFT Hyper-Parameters\nIn this work, we limited our alignment experiments to SFT models, which is expected to better demonstrate the effects of different preference alignment procedures. We prepared these SFT models using the the UltraChat-200k dataset. We try our best to use the SFT models from community to facilitate the reproduction. However, there were no available SFT checkpoints for some pre-trained models (e.g., Qwen-2.5 models). We in this part list the hyper-parameters for training these community-released SFT models as well as the SFT models trained by ourselves in Table 2."}, {"title": "C.3. Key Hyper-Parameters for Alignment", "content": "Figure 3 We conducted a series of alignment experiments with LoRA on two datasets for generating Figure 3. Key hyper-parameters used in the Argilla-dpo-mix-7k experiments are listed in Table 3 where we report the sweep range and the selected best learning rate for DPO with bold font. These parameters are then directly applied to other two settings (sorted and selected by VL) for generating Figure 3. The key parameters used for the UltraFeedback-binarized dataset are list in Table 4."}, {"title": "C.4. LoRA Configuration for Alignment", "content": "We conduct all our analytics experiments using LoRA. Its detailed configurations are described in Table 7."}, {"title": "C.5. Decoding Configuration", "content": "AlpacaEval 2. For this benchmark", "follows": 0.7}]}