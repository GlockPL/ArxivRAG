{"title": "On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis", "authors": ["Yekun Ke", "Xiaoyu Li", "Yingyu Liang", "Zhizhou Sha", "Zhenmei Shi", "Zhao Song"], "abstract": "Recently, Visual Autoregressive (VAR) Models introduced a groundbreaking advancement in the field of image generation, offering a scalable approach through a coarse-to-fine \"next-scale prediction\" paradigm. However, the state-of-the-art algorithm of VAR models in [Tian, Jiang, Yuan, Peng and Wang, NeurIPS 2024] takes $O(n^4)$ time, which is computationally inefficient. In this work, we analyze the computational limits and efficiency criteria of VAR Models through a fine-grained complexity lens. Our key contribution is identifying the conditions under which VAR computations can achieve sub-quadratic time complexity.\nSpecifically, we establish a critical threshold for the norm of input matrices used in VAR attention mechanisms. Above this threshold, assuming the Strong Exponential Time Hypothesis (SETH) from fine-grained complexity theory, a sub-quartic time algorithm for VAR models is impossible. To substantiate our theoretical findings, we present efficient constructions leveraging low-rank approximations that align with the derived criteria.\nFormally, suppose that $n$ is the height and width of the last VQ code map in VAR models, $d$ is the hidden dimension, $R$ is the bound of the entries of the input matrices for attention calculations in VAR models. We present two results:\n\u2022 On the positive side, we show that when $d = O(\\log n)$ and $R = o(\\sqrt{\\log n})$, there is an $O(n^{2+o(1)})$-time algorithm that approximates the output of VAR model up to $1/\\text{poly}(n)$ additive error.\n\u2022 On the negative side, we show that when $d = O(\\log n)$ and $R = \\Theta(\\sqrt{\\log n})$, assuming SETH, it is impossible to approximate the output of VAR model up to $1/\\text{poly}(n)$ additive error in truly sub-quartic time $O(n^{4-\\Omega(1)})$.\nThis work initiates the study of the computational efficiency of the VAR model from a theoretical perspective. Our technique will shed light on advancing scalable and efficient image generation in VAR frameworks.", "sections": [{"title": "1 Introduction", "content": "Visual generation technologies now underpin a broad array of applications, ranging from image enhancement [LHC+25, GLD+25] and augmented reality [AWT+24] to medical diagnostics [AKH+24, MHL+24, LLL+24] and creative pursuits like game development [RHR+20, CGX+25].\nBy translating text descriptions or other input into detailed and diverse visuals, these models are reshaping both how machines interpret images and how new visual content is created. Leading methods in the field include Variational AutoEncoders (VAE) [Doe16], Generative Adversarial Networks (GAN) [GPAM+20], and Diffusion models [HJA20]. Their advancements in producing high-resolution, high-fidelity, and varied imagery have significantly broadened the scope of visual generation, driving improvements in realism, diversity, and overall quality.\nThe emergence of the Visual AutoRegressive model (VAR) [TJY+24] marks a notable paradigm shift in image generation. Rather than relying on conventional \"next-token prediction\", the VAR model introduces a coarse-to-fine \u201cnext-scale prediction\" approach, enabling autoregressive transformers to more efficiently learn visual distributions and outperform diffusion-based alternatives. Moreover, the VAR model demonstrates robust zero-shot capabilities in tasks like image inpainting and editing, underscoring its potential for advancing autoregressive models in visual generation.\nDespite its demonstrated strengths, there remains a critical need to investigate the VAR model's computational limits and to design efficient algorithms. In [TJY+24], the authors report that the VAR model has a computational cost of $O(n^4)$, improving upon the $O(n^6)$ complexity associated with earlier autoregressive (AR) methods, where $n$ is the height and width of the last (largest) VQ code map. In this work, we aim to investigate the computational limits and potential efficient algorithms of VAR models. Specifically, we ask the following questions:\nCan we perform the computations of VAR models faster than $O(n^4)$ time?\nWe answer this question affirmatively and summarize our contributions as follows:\n\u2022 Computational Limits: We analyze the computation of the VAR models under the Strong Exponential Time Hypothesis. Let $R$ represent the upper bound of the elements in the input matrices used for attention calculations in VAR models. We establish an upper bound criterion $R^* = \\Theta(\\sqrt{\\log n})$. Crucially, only when $R$ is below this threshold, one can compute VAR models in $O(n^{4-\\Omega(1)})$ time (truly sub-quartic time).\n\u2022 Provably Efficient Criteria: We further show that when $R = o(\\sqrt{\\log n})$, it becomes possible to design an algorithm that approximates the VAR model in almost quadratic time, specifically $O(n^{2+o(1)})$."}, {"title": "1.1 Our Results", "content": "Our first result shows that when $R > \\Omega(\\sqrt{\\log n})$, it is impossible to design a truly sub-quartic time algorithm. Our results for the lower bound make use of the Strong Exponential Time Hypothesis (SETH) [IP01] from the area of fine-grained complexity regarding the time required to solve k-SAT.\nTheorem 1.1 (Computational Limits of VAR Models, informal version of Theorem 4.4). Suppose $d = O(\\log n)$ and $R = \\Theta(\\sqrt{\\log n})$. Assuming SETH, there is no algorithm that approximates the VAR model up to $1/\\text{poly}(n)$ additive error in $O(n^{4-2\\Omega(1)})$ time.\nOur second result shows that when $R$ is $o(\\sqrt{\\log n})$, an almost quadratic time algorithm exists:\nTheorem 1.2 (Existence of Almost Quadratic Time Algorithm, informal version of Theorem 5.8). Suppose $d = O(\\log n)$ and $R = o(\\sqrt{\\log n})$. There is an algorithm that approximates the VAR model up to $1/\\text{poly}(n)$ additive error in $O(n^{2+o(1)})$ time."}, {"title": "2 Related Work", "content": "Roadmap. Section 2 offers a summary of related work. In Section 3, we outline the mathematical formulation of both the VAR model and its fast version and divide the model into three stages: the VAR Transformer, the feature map reconstruction block, and the VQVAE-Decoder. Section 4 delves into analyzing the computation limits of the VAR model. In Section 5, we examine the running time and error propagation for each block in the fast VAR model and establish the conditions under which the model can be accelerated with proven efficiency. In Section 6, we discuss the potential impacts and future directions. In Section 7, we conclude our contributions."}, {"title": "2.1 Visual Generation Models", "content": "Recent years have witnessed remarkable advancements in visual generation models, driven by progress in several prominent architectures."}, {"title": "AutoRegressive Models", "content": "AutoRegressive models for visual generation [DYH+21, DZHT22] transform 2D images into 1D token sequences for processing. Early works like PixelCNN [VdOKE+16] and PixelSNAIL [CMRA18] pioneered pixel-by-pixel generation using a raster-scan approach. Subsequent studies [RVdOV19, ERO21, LKK+22] extended this concept by generating image tokens in a similar raster order. For example, VQ-GAN [ERO21] employs a GPT-2-style decoder-only transformer for image generation, while models such as VQVAE-2 [RVdOV19] and RQ-Transformer [LKK+22] enhance this method with additional hierarchical scales or stacked representations. More recently, Visual AutoRegressive (VAR) modeling [TJY+24] introduced a novel coarse-to-fine \u201cnext-scale prediction\" approach. This method improves scalability, inference speed, and image quality, outperforming traditional autoregressive techniques and diffusion transformers."}, {"title": "Diffusion Models", "content": "Diffusion models [HJA20, RBL+22] are known for their ability to generate high-resolution images by progressively refining noise into coherent visuals. Models such as DiT [PX23] and U-ViT [BNX+23] exemplify this approach, leveraging probabilistic frameworks to capture underlying data distributions. Recent advancements in diffusion-based generation focus on improving sampling techniques and training efficiency [SE19, SME20, LZB+22, HWL+24], exploring latent-space learning [RBL+22, WSD+24, WXZ+24, LZW+24], enhancing model architectures [HSC+22, PX23, LSSS24, WCZ+23, XSG+24], and 3D generation [PJBM22, WLW+24, XLC+24]."}, {"title": "2.2 Acceleration via Low-rank Approximation", "content": "Low-rank approximation has emerged as a powerful technique for addressing the computational challenges associated with modern transformer architectures. By approximating key operations such as attention and gradient computations, these methods significantly reduce the time and resource requirements of training and inference."}, {"title": "Accelerating Attention Mechanisms", "content": "Due to its quadratic computational complexity with respect to context length, the attention mechanism faces increasing difficulty as the sequence length grows in modern large language models [Ope24, AI24, Ant24]. To tackle this problem, polynomial kernel approximation methods [AA22] have been proposed, leveraging low-rank approximations to construct an efficient approximation of the attention matrix. These approaches lead to notable improvements in computation speed, enabling a single attention layer to handle both training and inference tasks with near-linear time complexity [AS23, AS24b]. Additionally, these methods can"}, {"title": "Gradient Approximation", "content": "The low-rank approximation is a widely used technique for optimizing transformer training by reducing computational complexity [LSS+24a, LSSZ24b, AS24b, HWSL24, CLS+24, LSS+24b]. Specifically, [AS24b] builds upon the low-rank approximation framework introduced in [AS23], which originally focused on forward attention computation, to approximate the gradient of attention mechanisms. This method effectively reduces the computational cost associated with gradient calculations. In [LSS+24a], this low-rank gradient approximation approach is further extended to multi-layer transformers, demonstrating that backward computations in such architectures can be approximated in nearly linear time. Additionally, [LSSZ24b] generalizes the work of [AS24b] to a tensor-based attention model, leveraging the forward computation results from [AS24c] to enable efficient training of tensorized attention mechanisms. Finally, [HWSL24] utilizes low-rank approximation methods in the training process of Diffusion Transformers (DiTs)., highlighting the versatility of these methods in various transformer-based models."}, {"title": "3 Model Formulation", "content": "In Section 3.1, we give some definitions which will be used later. In Section 3.2, we present the mathematical formulation for the token map generation phase. In Section 3.3, we provide the mathematical formulation for the feature map reconstruction phase. Finally, in Section 3.4, we detail the mathematical formulation of the VQ-VAE Decoder within the VAR model."}, {"title": "3.1 Notations and Definitions", "content": "We give the following notations in our setting. Given an integer $n \\in \\mathbb{Z}^+ \\cup \\{0\\}$, the set $\\{1, 2, ..., n\\}$ is represented by $[n]$. In our paper, nearly linear time is defined as $O(n \\text{poly} \\log n)$, and almost linear time is defined as $O(n^{1+o(1)})$. Given a vector $c$, the diagonal matrix formed from $c$ is denoted as $\\text{diag}(c)$, where $c_i$ is the $i$-th diagonal entry of this matrix. Given a matrix $U$, we use $U^T$ to denote the transpose of $U$. Given two vectors $a$ and $b$, which have the same length. The element-wise multiplication of $c$ and $d$ is denoted as $c \\odot d$ with $i$-th entry being $c_i d_i$. Given a matrix $U$, we use $||U||_F$ to represent the Frobenius norm of $U$. Specifically, we have $||U||_F := \\sqrt{\\sum_{i,j} U_{i,j}^2}$. Given a matrix $U$, we use $||U||_{\\infty}$ to represent the maximum norm of $U$. Specifically, we have $||U||_{\\infty} := \\max_{i,j} |U_{i,j}|.$"}, {"title": "3.2 Phase 1: Token Maps Generation Phase", "content": "The VAR model uses the VAR Transformer to convert the initial tokens of the generated image into several pyramid-shaped token maps. And in the token maps generation phase, the token maps for the next scale, $M_{k+1}$, are generated based on the previous $k$ token maps $M_1, ..., M_k$. This phase has the main modules as the following:"}, {"title": "Up Sample Blocks", "content": "Before inputting the $k$ token maps into the VAR Transformer, the VAR model upsamples the $(i)$-th token map to the size of the $(i + 1)$-th token map. Specifically, the VAR model employs an upsampling method using interpolation for the image. Here, we define the up-interpolation layers:\nDefinition 3.1 (Bicubic Spline Kernel). A bicubic spline kernel is a piecewise cubic function $W : \\mathbb{R} \\rightarrow \\mathbb{R}$ that satisfies $W(x) \\in [0,1]$ for all $x \\in \\mathbb{R}$.\nDefinition 3.2 (Up-interpolation Layer). The Up-Interpolation layer is defined as follows:\n\u2022 Let $h \\in \\mathbb{N}$ and $h' \\in \\mathbb{N}$ denote the height of the input feature map and output feature map, respectively.\n\u2022 Let $w \\in \\mathbb{N}$ and $w' \\in \\mathbb{N}$ denote the width of the input feature map and the output feature map, respectively.\n\u2022 Let $c \\in \\mathbb{N}$ denote the number of channels of the input feature map and the output feature map.\n\u2022 Let $X \\in \\mathbb{R}^{h \\times w \\times c}$ denote the input feature \u0442\u0430\u0440.\n\u2022 Let $Y \\in \\mathbb{R}^{h' \\times w' \\times c}$ denote the output feature ma\u0440.\n\u2022 Let $s,t \\in \\{-1, 0, 1, 2\\}$.\n\u2022 Let $W: \\mathbb{R} \\rightarrow \\mathbb{R}$ be a bicubic spline kernel as defined in 3.1.\nWe use $\\phi_{up} : \\mathbb{R}^{h \\times w \\times c} \\rightarrow \\mathbb{R}^{h' \\times w' \\times c}$ to denote the up-interpolation operation then we have $Y = \\phi_{up}(X)$. Specifically, for $i\\in [h'], j\\in [w'], l \\in [c]$, we have\n$Y_{i,j} = \\sum_{s=-1}^{2} \\sum_{t=-1}^{2} W(s) \\cdot X_{\\frac{ih}{h'} + s, \\frac{jw}{w'} + t, l} \\cdot W(t)$"}, {"title": "Transformer Blocks", "content": "After the up-sample process, the generated token maps above will be input into the Transformer to predict the next token map. Here, we define several blocks for the VAR Transformer.\nWe first define the attention matrix.\nDefinition 3.3 (Attention Matrix). Define the model weights as $W_Q, W_K \\in \\mathbb{R}^{d \\times d}$, and let $X \\in \\mathbb{R}^{n \\times d}$ represent the input of length $n$. The attention matrix $A \\in \\mathbb{R}^{n \\times n}$ is given by the following, for $i, j\\in [n]$,\n$A_{i,j} := \\exp(X_i, W_Q W_K X_j^T)$\nTo move on, we define the single attention layer in the following way:\nDefinition 3.4 (Single Attention Layer). Let $X \\in \\mathbb{R}^{n \\times d}$ be the input matrix. Let $W_V \\in \\mathbb{R}^{d \\times d}$ represent the weighted matrix of value. Similar to the standard attention mechanism, the goal is to produce an $n \\times d$ output matrix, where $D := \\text{diag}(A1_n) \\in \\mathbb{R}^{n \\times n}$. We define attention layer $\\text{Attn}$ as the following:\n$\\text{Attn}(X) := D^{-1}AXW_V$.\nFirstly, we give the definition of the VAR Transformer Layer."}, {"title": "3.5 (VAR Transformer Layer)", "content": "Given that the following conditions are true:\n\u2022 Define $X \\in \\mathbb{R}^{n \\times d}$ as the input data matrix.\n\u2022 Let the up-interpolation layer $\\phi_{up}$ as given in Definition 3.2.\n\u2022 Let the $\\text{Attn}(X)$ as given in Definition 3.3.\nThen, we defined one VAR Transformer Layer as\n$F_{var}(X) := \\text{Attn}(\\phi_{up}(X))$\nThen, we can give the definition of the Fast VAR Transformer Layer.\nDefinition 3.6 (Fast VAR Transformer Layer). Given that the following conditions are true:\n\u2022 Let $X \\in \\mathbb{R}^{n \\times d}$ denote the input data matrix.\n\u2022 Let the up-interpolation layer $\\phi_{up}$ as given in Definition 3.2.\n\u2022 Let the $\\text{AAttC}(X)$ as given in Definition 4.2.\nThen, we defined one Fast VAR Transformer Layer as\n$F_{fvar}(X) := \\text{AAttC}(\\phi_{up}(X))$\nDefinition 3.7 (VAR Model). Given that the following conditions are true:\n\u2022 Let the initial input be defined as $x_1 \\in \\mathbb{R}^{1 \\times d}$\n\u2022 Let the VAR Transformer layer $F_{var}$ as given in Definition 3.5.\n\u2022 Let $r \\in \\mathbb{N}$ represent the total iteration of VAR model.\nThen, for $i \\in \\{0,1,2,...,r\\}$, we defined the intermediate variable $T_i(X)$ as follows:\n$T_i(X) = \\begin{cases} x_1, & i = 0 \\\\ F_{var}(T_{i-1}(X)), & i \\in [r] \\end{cases}$\nThen, the final output of the VAR model is $T_r(X)$.\nDefinition 3.8 (Fast VAR Model). Given that the following conditions are true:\n\u2022 Define $x_1 \\in \\mathbb{R}^{1 \\times d}$ as the initial input.\n\u2022 Let the Fast VAR Transformer layer $F_{fvar}$ be defined as Definition 3.6.\n\u2022 We take $R$ to be the upper bound on the entries of the input matrices involved in attention computations for VAR models.\n\u2022 Let $r \\in \\mathbb{N}$ denote the total iteration of the Fast VAR model.\nThen, for $i \\in \\{0,1,2,...,r\\}$, we defined the intermediate variable $T_i(X)$ as follows:\n$T_i(X) = \\begin{cases} x_1, & i = 0 \\\\ F_{fvar}(T_{i-1}(X)), & i \\in [r] \\end{cases}$\nThen, the final output of the VAR model is $T_r(X)$."}, {"title": "3.3 Phase 2: Feature Map Reconstruction", "content": "In phase 2, the VAR model will transform the generated token maps into feature maps. This phase has the following main modules:"}, {"title": "Up Sample Blocks", "content": "The VAR model performs up-sampling on token maps of different sizes, scaling them to the size of the final output feature map. In this process, the VAR model will use the up-interpolation blocks defined in Definition 3.2. To mitigate information loss during token map up-scaling, the VAR model employs convolution blocks to post-process the up-scaled token maps. We define the convolution layers as the following:\nDefinition 3.9 (Convolution Layer). The Convolution Layer is defined as follows:\n\u2022 Let $h \\in \\mathbb{N}$ denote the height of the input and output feature map.\n\u2022 Let $w \\in \\mathbb{N}$ denote the width of the input and output feature map.\n\u2022 Let $C_{in} \\in \\mathbb{N}$ denote the number of channels of the input feature map.\n\u2022 Let $C_{out} \\in \\mathbb{N}$ denote the number of channels of the output feature m\u0430\u0440.\n\u2022 Let $X \\in \\mathbb{R}^{h \\times w \\times C_{in}}$ represent the input feature map.\n\u2022 For $l \\in [C_{out}]$, we use $K^l \\in \\mathbb{R}^{3 \\times 3 \\times C_{in}}$ to denote the $l$-th convolution kernel.\n\u2022 Let $p = 1$ denote the padding of the convolution layer.\n\u2022 Let $s = 1$ denote the stride of the convolution kernel.\n\u2022 Let $Y \\in \\mathbb{R}^{h \\times w \\times C_{out}}$ represent the output feature map.\nWe use $\\Phi_{conv}: \\mathbb{R}^{h \\times w \\times C_{in}} \\rightarrow \\mathbb{R}^{h \\times w \\times C_{out}}$ to represent the convolution operation then we have $Y = \\Phi_{conv}(X)$. Specifically, for $i \\in [h], j\\in [w],l \\in [C_{out}]$, we have\n$Y_{i,j,l} := \\sum_{m=1}^{3} \\sum_{n=1}^{3} \\sum_{c=1}^{C_{in}} X_{i+m-1,j+n-1,c} \\cdot K_{m,n,c}^l + b$\nRemark 3.10. Assumptions of kernel size, padding of the convolution layer, and stride of the convolution kernel are based on the specific implementation of [TJY+ 24]."}, {"title": "3.4 Phase 3: VQ-VAE Decoder process", "content": "VAR will use the VQ-VAE Decoder Module to reconstruct the feature map generated in Section 3.3 into a new image. The Decoder of VQ-VAE has the following main modules:"}, {"title": "ResNet Blocks", "content": "In the VQVAE decoder, the ResNet block, which includes two (or more) convolution blocks, plays a crucial role in improving the model's ability to reconstruct high-quality outputs. The convolution blocks help capture spatial hierarchies and patterns in the data, while the residual connections facilitate better gradient flow and allow the model to focus on learning the residuals (differences) between the input and output. The definition of convolution block is given in Definition 3.9."}, {"title": "Attention Blocks", "content": "The Attention block helps the Decoder fuse information from different locations during the generation process, which can significantly improve the clarity and detail of the generated images. When applied to a feature map, the attention mechanism computes attention scores for all pairs of pixels, capturing their pairwise relationships and dependencies. The definitions of blocks in attention are given in Section 3.2."}, {"title": "Up Sample Blocks", "content": "The VQ-VAE decoder uses Up-Sample Blocks to progressively increase the spatial resolution of the latent representation. The Up-Sample Blocks in VQVAE combine up-interpolation and convolution blocks to restore the spatial dimensions of the feature maps, facilitating the reconstruction of the high-resolution output image. The convolution block has already been defined in Definition 3.9, and the up-interpolation block has already been defined in Definition 3.2."}, {"title": "4 Computational Limits", "content": "In this section, we delve into the computational limits of VAR Models, particularly in the context of solving key problems under the assumptions of the Strong Exponential Time Hypothesis (SETH). Section 4.1 introduces SETH as the basis for our complexity analysis. In Section 4.2, we discuss a key result from [AS23] that establishes the hardness of Approximate Attention Computation. Finally, Section 4.3 presents the lower bound for VAR model efficiency, pinpointing the limitations for sub-quartic performance."}, {"title": "4.1 Strong Exponential Time Hypothesis", "content": "We begin by presenting the foundational hypothesis (SETH) [IP01], which underpins much of our complexity analysis:\nHypothesis 4.1 (Strong Exponential Time Hypothesis (SETH) [IP01]). For every $\\epsilon > 0$, there exists a positive integer $k \\geq 3$ such that no randomized algorithm can solve k-SAT on formulas with $n$ variables in $O(2^{(1-\\epsilon)n})$ time."}, {"title": "4.2 Hardness of Approximate Attention Computation", "content": "We begin by introducing the definition of Approximate Attention Computation (AAttC).\nDefinition 4.2 (Approximate Attention Computation AAttC(n, d, R, $\\delta$), Definition 1.2 in [AS23]). Let $d > 0$. Let $R > 0$. Let $X \\in \\mathbb{R}^{n \\times d}$ denote the input of the attention mechanism. Given three matrices $Q, K, V \\in \\mathbb{R}^{n \\times d}$, with the guarantees that $||Q||_{\\infty} < R$, $||K||_{\\infty} < R$ and $||V||_{\\infty} < R$, output a matrix $T \\in \\mathbb{R}^{n \\times d}$ that approximately represents $\\text{Attn}(X)$, meaning\n$||T - \\text{Attn}(X) ||_{\\infty} \\leq \\delta$\nNext, we state a result for the Approximate Attention Computation (AAttC) from [AS23].\nLemma 4.3 (Theorem 4.6 in [AS23]). Suppose $d = O(\\log n)$ and $R = \\Theta(\\sqrt{\\log n})$. Assuming SETH, there is no algorithm that solves the Approximate Attention Computation (AAttC) up to $1/\\text{poly}(n)$ additive error in $O(n^{4-2\\Omega(1)})$ time."}, {"title": "4.3 Computational Limits of Fast VAR Models", "content": "We now present a main theorem detailing the lower bound for VAR model computation.\nTheorem 4.4 (Computational Limits of Fast VAR Models, formal version of Theorem 1.1). Suppose $d = O(\\log n)$ and $R = \\Theta(\\sqrt{\\log n})$. Assuming SETH, there is no algorithm that approximates the VAR model up to $1/\\text{poly}(n)$ additive error in $O(n^{4-2\\Omega(1)})$ time.\nProof. By Lemma 4.3, in the K-th step ($\\text{K} = \\log_a n$), the VAR Transformer must compute attention with a computational cost at least\n$\\Omega(L_K^2 \\cdot d) = \\Omega(\\sum_{i=1}^{K} (\\frac{a^{2^i}-1}{a^2-1})^2 \\cdot d)$\n$= \\Omega((\\frac{a^{2^K}-1}{a^2-1})^2 \\cdot d)$\n$\\geq \\Omega((a^{2^K}-1)^2 \\cdot d)$\n$\\geq \\Omega((n^4-2 \\Omega d)$.\nIn the first step above, we use the definition of $L_K$. The second step applies the standard geometric series formula. The third step involves basic algebra, and the final inequality is due to the fact $\\text{K} = \\log_a n$."}, {"title": "5 Provably Efficient Criteria", "content": "Section 5.1 details the running time of the fast VAR Transformer, feature map reconstruction block, and Fast VQ-VAE Decoder. In Section 5.2, we analyze the error propagation in both the Fast VAR Transformer and the Fast VQ-VAE Decoder. Section 5.3 presents our findings regarding the existence of an almost quadratic time algorithm."}, {"title": "5.1 Running Time", "content": "Here, we present an overview of the computational cost associated with the Fast VAR Transformer, feature map reconstruction block, and Fast VQ-VAE Decoder.\nFirstly, we show that the runtime of the VAR Transformer can be sped up to $O(n^{2+o(1)})$.\nLemma 5.1 (Running time of Fast VAR Transformer, informal version of Lemma E.3). Assuming the conditions below are satisfied:\n\u2022 Let $K$ denote the total number of the generated token maps.\n\u2022 Let $k \\in [K]$.\n\u2022 Let $r_1 \\in \\mathbb{R}^{1 \\times 1 \\times d}$ denote the first scale token \u0442\u0430\u0440.\n\u2022 Let $a > 1$ denote the growth rate of the height and width of the token map at each level. Then for $k \\in [K]$, the $k$-th token map $r_k \\in \\mathbb{R}^{a^{k-1} \\times a^{k-1} \\times d}$.\n\u2022 Let $r_K \\in \\mathbb{R}^{n \\times n \\times d}$ denote the last scale token map, where $n = a^{K-1}$.\n\u2022 Let $d = O(\\log(n))$."}, {"title": "5.2 Error Propagation Analysis", "content": "Then, the total runtime of the VAR Transformer for generating token maps can be accelerated to $O(n^{2+o(1)})$.\nThen, we proceed to show that the runtime of the feature map reconstruction layer is $O(n^{2+o(1)})$.\nLemma 5.2 (Running time of Feature Map Reconstruction Layer, informal version of Lemma E.4). Assuming the conditions below are satisfied:\n\u2022 Let $K$ denote the total number of generated token maps.\n\u2022 Let $k \\in [K]$.\n\u2022 Let $r_1 \\in \\mathbb{R}^{1 \\times 1 \\times d}$ denote the first scale token \u0442\u0430\u0440.\n\u2022 Let $a > 1$ denote the growth rate of the height and width of the token map at each level. Then for $k \\in [K]$, the $k$-th token map $r_k \\in \\mathbb{R}^{a^{k-1} \\times a^{k-1} \\times d}$.\n\u2022 Let $r_K \\in \\mathbb{R}^{n \\times n \\times d}$ denote the last scale token map, where $n = a^{K-1}$.\n\u2022 Let $d = O(\\log(n))$.\nThen, the total runtime of the Feature Map Reconstruction Layer is $O(n^{2+o(1)})$.\nFinally, we show that the runtime of the VQVAE Decoder can be sped up to $O(n^{2+o(1)})$.\nLemma 5.3 (Running time of Fast VQ-VAE Decoder, informal version of Lemma E.6). Assuming the conditions below are satisfied:\n\u2022 Define $k_1, k_2, k_3 \\in \\mathbb{N}$ as constant numbers.\n\u2022 Let $X \\in \\mathbb{R}^{n \\times n \\times d}$ represent the input feature \u0442\u0430\u0440.\n\u2022 We assume there are $k_1$ up-interpolation layers $\\Phi_{up}$ defined in Definition 3.2.\n\u2022 Given a feature map $M \\in \\mathbb{R}^{h \\times w \\times d}$. For $i\\in [k_1]$, we assume $i$-th up-interpolation layer's output $\\phi_{up}(M) \\in \\mathbb{R}^{O(h) \\times O(w) \\times d}$.\n\u2022 We assume there are $k_2$ attention layers Attn defined in Definition 3.4.\n\u2022 Given a feature map $M \\in \\mathbb{R}^{h \\times w \\times d}$. For $i \\in [k_1]$, the $i$-th attention layer's output $\\text{Attn}(M) \\in \\mathbb{R}^{h \\times w \\times d}$.\n\u2022 We assume there are $k_3$ convolution layers $\\phi_{conv}$ defined in Definition 3.9.\n\u2022 Given a feature map $M \\in \\mathbb{R}^{h \\times w \\times d}$. For $i\\in [k_1]$, we assume $i$-th up-interpolation layer's output $\\phi_{up}(M) \\in \\mathbb{R}^{h \\times w \\times O(d)}$.\n\u2022 Let $d = O(\\log(n))$."}, {"title": "Here", "content": "we present the error analysis results of the fast Transformer layer and the fast VQ-VAE Decoder.\nWe begin with analyzing the error propagation of AAttC (see Definition 4.2).\nLemma 5.4 (Error analysis of AAttC(X') and AAttC(X)", "satisfied": "n\u2022 Define $X \\in \\mathbb{R}^{n \\times d}$ as the input matrix.\n\u2022 Define $X' \\in \\mathbb{R}^{n \\times d}$ as the approximation version of input matrix.\n\u2022 Let $\\epsilon \\in (0,0.1)$ denote the approximation error.\n\u2022 Suppose we have $||X' \u2013 X||_{\\infty} \\leq \\epsilon$.\n\u2022 Let $M > 1$.\n\u2022 Assume the value of each entry in matrices can be bounded by $M$.\n\u2022 Let the polynomial approximation of attention matrix AAttC(X) as given in Definition 4.2.\n\u2022 Let $U, V \\in \\mathbb{R}^{n \\times k}$ represent low-rank matrices constructed for polynomial approximation of attention matrix AAttC(X).\n\u2022 Let $f$ be a polynomial with degree $g$.\nThen, we can show that\n$||\\text{AAttC}(X') \u2013 \\text{AAttC}(X)||_{\\infty} \\leq O(kM^{g+2d}) \\cdot \\epsilon$\nThen, we move on to analyzing the approximation error between AAttC(X') and the ground truth Attn(X), where X' denotes the approximated version of the original input X.\nLemma 5.5 (Error analysis of AAttC(X') and Attn(X), informal version"}]}