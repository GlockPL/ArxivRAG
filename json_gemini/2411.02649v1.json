{"title": "M-CELS: Counterfactual Explanation for Multivariate Time Series Data Guided by Learned Saliency Maps", "authors": ["Peiyu Li", "Omar Bahri", "Souka\u00efna Filali Boubrahimi", "Shah Muhammad Hamdi"], "abstract": "Over the past decade, multivariate time series classification has received great attention. Machine learning (ML) models for multivariate time series classification have made significant strides and achieved impressive success in a wide range of applications and tasks. The challenge of many state-of-the-art ML models is a lack of transparency and interpretability. In this work, we introduce M-CELS, a counterfactual explanation model designed to enhance interpretability in multidimensional time series classification tasks. Our experimental validation involves comparing M-CELS with leading state-of-the-art baselines, utilizing seven real-world time-series datasets from the UEA repository. The results demonstrate the superior performance of M-CELS in terms of validity, proximity, and sparsity, reinforcing its effectiveness in providing transparent insights into the decisions of machine learning models applied to multivariate time series data.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, there has been widespread adoption of time series classification methods across various domains [1], [2]. The success of data-driven time series classification methods is largely attributed to the availability of large-scale data, which has facilitated the development of highly accurate models for real-world applications. However, most of the time series model development efforts, especially deep learning models [3], consider accuracy as their foremost priority without necessarily providing a tracing mechanism of the model's decision-making process which limits their interpretability. This lack of interpretability poses a challenge, as scientific communities often require explanations to support their trust in the models. Therefore, the establishment of trust between humans and decision-making systems has become critically important. This can be achieved by incorporating interpretable explanations during the model development phase or by developing post-hoc explanation solutions. By doing so, we can enhance transparency and foster trust in decision-making processes, particularly in critical domains where human judgment and accountability are paramount. To address concerns regarding the lack of transparency in machine learning models, experts from different domains are proposing EXplainable Artificial Intelligence (XAI) methods to provide trustworthy explanations of the decision-making processes of machine learning models [4]. XAI methods aim to provide reliable explanations for the decision-making processes of these models while maintaining high levels of performance. There are two dominant paradigms of XAI: intrinsic interpretable and post-hoc explanation methods for opaque models [5]. Examples of intrinsic interpretable models include logistic regression and decision tree-based induction, which can provide an intrinsic explanation by their simple structure models. On the other hand, post-hoc explanation methods can be further categorized into feature attribution methods and counterfactual explanation methods. Feature attribution methods aim to explain the rationale behind a model's decision, while counterfactual explanation methods identify the smallest input modifications that would result in a different decision. In this paper, we focus only on post-hoc XAI methods, which have gained significant attention in the field. These methods play a crucial role in enhancing the interpretability of opaque models, enabling stakeholders to gain a better understanding of the factors driving the models' predictions.\nIn recent years, significant research advancements have been accomplished on explainability in the computer vision and natural language processing (NLP) domains, but there are still many challenges to be addressed to provide interpretability for the time series domain [6]. A lot of efforts have been made to provide post-hoc XAI for image [7], vector-represented data [8] and univariate time series data [9], [10], while significantly less attention has been paid to multivariate time series data [11].\nIn this work, we aim to fill this gap by introducing M-CELS, a novel extension of counterfactual explanations via learned saliency maps on univariate time series to enhance transparency in multivariate time series classification models. To the best of our knowledge, this is the first effort to learn a saliency map specifically for the purpose of producing high-quality counterfactual explanations for multivariate time series data."}, {"title": "II. RELATED WORK", "content": "In the post-hoc interpretability paradigm, the wCF method by Wachter et al. [12] stands as a prominent approach that first introduced the counterfactual explanation. wCF generates counterfactuals by minimizing a loss function to alter decision outcomes while preserving the minimum Manhattan distance from the original input instance. Extensions like TimeX [13] and SG-CF [14] have augmented counterfactual quality by integrating extra terms into the loss function. TimeX introduces a Dynamic Barycenter Average Loss term for a more contiguous counterfactual, while SG-CF leverages previously mined shapelets for better interpretability. Nonetheless, these methods are primarily designed for univariate time series data, posing challenges for direct application to multivariate time series.\nNG-CF employs Dynamic Barycenter (DBA) averaging of the query time series and its nearest unlike neighbor for counterfactual generation. Recent methods like those employing mined shapelets, temporal rules or Matrix Profile [10], [15]\u2013[17] aim to enhance interpretability. However, they often encounter limitations of extensive processing time for shapelet or rule extraction.\nCOMTE [11], proposed by Ates et al., targets generating counterfactuals for multivariate time series by evaluating the impact of turning off one variable at a time. However, COMTE tends to generate low sparsity as it modifies the entire time series data [16]. Another approach, AB-CF proposed by [18], underscores minimum discriminative contiguous segment replacement for sparser and higher-validity counterfactuals. Nonetheless, it presupposes the presence of discriminative segments across all dimensions within the same time-step interval.\nIn addressing these challenges, our work introduces optimization techniques tailored to multivariate time series, guiding perturbations through saliency maps. Our contributions are summarized below:\n1) Optimization Techniques for Multivariate Time Series: We introduce novel optimization methods specifically tailored to the complexities of multivariate time series data. These techniques guide the generation of counterfactuals, ensuring they are both meaningful and interpretable across multiple dimensions.\n2) Use of Saliency Maps for Enhanced Sparsity: By leveraging saliency maps, our approach identifies and targets the most influential features within the time series data. This focused perturbation promotes higher sparsity. As a result, fewer changes are made to the original time series, making the counterfactuals more actionable for users.\n3) Improved Interpretability in Time Series Classification: The combination of targeted perturbations and enhanced sparsity directly contributes to better interpretability in time series classification tasks. Users can more easily discern the rationale behind model decisions and the influence of different variables over time."}, {"title": "III. BACKGROUND WORKS: CELS", "content": "Recently, the CELS model [19] introduced the concept of generating counterfactual explanations for univariate time series classification using learned saliency maps. Building on this approach, we present M-CELS, an extension of CELS, to generate high-sparsity and high-validity counterfactual explanations for multivariate time series data. Our methodology adapts the principles of CELS to handle the added complexity of multivariate datasets, ensuring meaningful and interpretable counterfactuals across multiple dimensions. In this section, we briefly describe the CELS background and the main components we utilized in M-CELS."}, {"title": "A. Problem definition for CELS", "content": "CELS [19] is a novel counterfactual explanation method originally designed for univariate time series data, offering high proximity, sparsity, and interpretability in post-hoc explanations for time series classification.\nGiven a set of N univariate time series $D = \\{x_1,...,x_N\\}$ and a univariate time series classification model $f : X \\rightarrow Y$, where $X \\in R^T$ is the T-dimensional feature space and $Y = \\{1, 2,..., C\\}$ is the label space with C classes. For an instance-of-interest $x \\in D$, a time series with T time steps along with a predicted probability distribution $\\hat{y} = f(x)$ over C classes where $\\hat{y} \\in [0, 1]^C$ and $\\sum_{i=1}^{C} \\hat{y_i} = 1$. The top predicted class z where $z = \\text{argmax }\\hat{y}$ is the class of interest for which a counterfactual explanation is sought. The instance of interest x is defined as the original query instance, and the perturbed result x' is defined as the counterfactual instance or counterfactual.\nThe goal of CELS is to learn a saliency map $\\theta \\in [0, 1]^T$ for class z, where each element represents the importance of its corresponding time step. Values close to 1 indicate strong evidence for class z, while values near 0 indicate no importance. CELS assumes that the importance of a time step reflects how much $P(\\hat{y_z} | x)$ changes when x is perturbed to x'. A successfully learned saliency map $\\theta$ will assign high values to the regions in a time series where the perturbation function would shift the model's predictions away from z.\nCELS contains three key components that work together: (1) the Nearest Unlike Neighbor Replacement strategy learns which time series from the background dataset D are the best for replacement-based perturbation. (2) the Learning Explanation aims to learn a saliency map $\\theta$ for the instance of interest by highlighting the most important time steps that provide evidence for a model prediction. (3) the Saliency Map Guided Counterfactual Perturbation function which perturbs the instance of interest x to force the perturbation shift the model's prediction away from the class of interest z."}, {"title": "B. Replacement Strategy: Nearest Unlike Neighbor (nun)", "content": "NG [9] generates in-distribution perturbations by replacing time steps with those from the nearest unlike neighbor in a background dataset D. Inspired by NG, CELS similarly selects the nearest unlike neighbor time series from D (usually the training dataset) to guide the perturbation of the instance x. The nearest unlike neighbor is defined as the instance nun from a different class z', with the smallest Euclidean distance to x. In binary classification, z' is the opposite class of z, while in multi-class classification, it is the class with the second-highest probability. This approach helps explain model predictions and generates counterfactual explanations by identifying the minimal changes needed to alter the prediction [9]."}, {"title": "C. Learning Explanation", "content": "Saliency values $\\theta \\in R^{1\\times T}$ are learned using a novel loss function (Equation 4) including three key components.\nFirst, a $L_{Max}$ is designed to promote the perturbation function to generate high-validity counterfactual instances. $P(\\hat{y}_{z'} | x')$ represents the class probability of class z' (the target class) predicted by the classification model f for the perturbated counterfactual instance x'. Specifically, $L_{Max}$ is incorporated to maximize the class probability of class z' predicted by the classification model f for the perturbated counterfactual instance x', if $P(\\hat{y}_{z'} | x')$ is optimized to be close to 1, then we can get high validity counterfactual explanation:\n$L_{Max} = 1-P(\\hat{y}_{z'} | x')$ (1)\nNext, to encourage simple explanations with minimal salient time steps, the $L_{Budget}$ loss is introduced. This component promotes the values of $\\theta$ to be as small as possible, where intuitively, values that are not important should be close to 0 according to the defined problem:\n$L_{Budget} = \\frac{1}{T} \\sum_{t=1}^{T} \\theta_{t}$ (2)\nThe saliency map is then encouraged to be temporally coherent, where neighboring time steps should generally have similar importance. To achieve this coherence, a time series regularizer $L_{TReg}$ is introduced, minimizing the squared difference between neighboring saliency values:\n$L_{TReg} = \\frac{1}{T-1} \\sum_{t=1}^{T-1} (\\theta_{t} - \\theta_{t+1})^2$ (3)\nFinally, all the loss terms are summed, with $L_{Max}$ being scaled by a $\\lambda$ coefficient to balance the three components in counterfactual explanation behavior.\n$L(P(\\hat{y} | x); \\theta) = \\lambda * L_{Max} + L_{Budget} + L_{TReg}$ (4)"}, {"title": "D. Counterfactual Perturbation", "content": "A saliency map is derived for the top predicted class z to identify the key time steps contributing to the prediction, which also plays an important role in guiding the generation of counterfactual explanations. CELS perturbs the instance x by replacing its important time steps with those from the nearest unlike neighbor nun, as guided by the saliency map $\\theta$. The counterfactual perturbation function is defined as:\n$x' = x \\odot (1 - \\theta) + nun \\odot \\theta$ (5)\nSpecifically, if $\\theta_{t} = 0$, which means that the time step t is not important for the prediction decision on class z, then the original time step $x_t$ remains unchanged. In other words, if $\\theta_{t} = 1$, which means that the time steps t provide important evidence for the prediction decision on class z, then $x_t$ is replaced with the corresponding time step of nearest unlike neighbor nun. In summary, the perturbation function generates the counterfactual explanation x' by performing time step interpolation between the original time steps of x and the replacement series nun."}, {"title": "IV. M-CELS", "content": "The Nearest Unlike Neighbor replacement strategy and the counterfactual perturbation function, initially designed for univariate time series data, seamlessly extend to multivariate time series data with direct applicability. The key adaptation lies in the saliency map learning process. When addressing multi-dimensional time series data, the conventional approach of learning a one-dimensional saliency map is replaced by the exploration of a multi-dimensional saliency map, denoted as $\\theta \\in [0, 1]^{T\\times D}$, capturing the nuances of the multi-dimensional temporal sequence. This section introduces the notation and problem definition for counterfactual explanation using learned saliency maps on multivariate time series data. It elucidates the saliency map learning process's significant deviation from its univariate counterpart, providing a comprehensive understanding of the intricate adjustments required for effective application in the multivariate context."}, {"title": "A. Notation", "content": "We assume a univariate time series $x = \\{x_1, x_2, ..., x_T\\}$ is an ordered set of real values, where T is the length of the time series. In the case of multivariate time series, the time series is a list of vectors over D dimensions and T observations, $x = [x_1, x_2, ..., x_D]$. Then we can define a multivariate time series dataset $D = \\{x_0, x_1, ..., x_n\\}$ as a collection of n multivariate time series. Assume we are given a multivariate time series classification model $f : x \\rightarrow Y$, where $x \\in R^{T\\times D}$ is the feature space of the input multivariate time series, where each multivariate time series has mapped to a mutually exclusive set of classes $y = \\{1, 2, . . ., C\\}$, where C is the number of classes in dataset D."}, {"title": "B. Problem Definition.", "content": "Let us consider an instance-of-interest $x \\in D$, a multivariate time series with D dimensions and T time steps per dimension along with a predicted probability distribution $\\hat{y} = f(x)$ over C classes where $\\hat{y} \\in [0, 1]^C$ and $\\sum_{i=1}^{C} \\hat{y_i} = 1$. The top predicted class z where $z = \\text{argmax }\\hat{y}$ has predicted confidence $\\hat{y}_z$ is the class of interest for which we seek a counterfactual explanation.\nOur goal is to learn a saliency map $\\theta \\in [0, 1]^{D\\times T}$ for the class of interest z where each element represents the importance of a corresponding time step at a corresponding dimension at the very first step. Values in $\\theta$ close to 1 indicate strong evidence for class z, while values in $\\theta$ close to 0 indicate no importance. We assume the importance of a time step t should reflect the expected scale of the change of $P(\\hat{y}_z | X)$ when x is perturbed: $|P(\\hat{y}_z | x) - P(\\hat{y}_z | x')|$, where x' is perturbed version of x. Then we design a perturbation function based on the learned saliency map to generate a counterfactual explanation x' for the class of interest z. A successfully learned saliency map $\\theta$ will assign high values to the regions"}, {"title": "C. Learning Explanation for Multivariate Time Series Data", "content": "In the case of multivariate time series data, the learning explanation would be different from the process on univariate time series data. Saliency values $\\theta \\in R^{T\\times D}$ are learned where $\\theta_{t,d}$ is the saliency value of dimension d at time step t. The loss function utilized to optimize saliency maps for deriving an intuitive instance-based counterfactual explanation, denoted as x, undergoes distinct adaptations.\nThe $L_{MMax}$ remains unchanged to ensure the high validity of the counterfactual explanation generation, where z' is the target class of the counterfactual explanation.\n$L_{MMax} = (1 - P(\\hat{y}_{z'} | x'))$ (6)\nHowever, approaches to calculating the Budget loss and TV norm loss differ. Equation 7 introduces the multivariate budget loss, computing the average absolute saliency values across dimensions and time steps. Enforcing sparsity in the saliency map offers insights into the overall perturbation magnitude, ensuring judicious distribution of changes across dimensions.\n$L_{MBudget} = \\frac{1}{D \\cdot T} \\sum_{d=1}^D \\sum_{t=1}^T |\\theta_{t,d}|$ (7)\nEquation 8 presents the multivariate temporal regularization term, capturing the smoothness and temporal consistency of saliency maps across dimensions. It discourages abrupt changes in saliency values over time, promoting coherent explanations across consecutive time steps.\n$L_{MTReg} = \\frac{1}{D} \\sum_{d=1}^D \\frac{1}{T-1} \\sum_{t=1}^{T-1} (\\theta_{t,d} - \\theta_{t+1,d})^2$ (8)\nFinally, the overall loss function in Equation 9 integrates the three components ($L_{MMax}, L_{MBudget}, L_{MTReg}$), reflecting diverse objectives in the multivariate time series setting. These equations collectively establish a comprehensive framework tailored for the nuanced characteristics of multivariate time series data, ensuring an effective and meaningful learning explanation process.\n$L(P(\\hat{y} | x); \\theta) = \\lambda * L_{MMax} + L_{MBudget} + L_{MTReg}$ (9)"}, {"title": "V. EXPERIMENTAL STUDY", "content": "We evaluated our proposed M-CELS model with the other three baselines, Alibi, Native guide counterfactual (NG), and Attention-based counterfactual (AB).\n\u2022 Alibi Counterfactual (Alibi): Alibi follows the work of Wachter et al. [12], which constructs counterfactual explanations by optimizing an objective function,\n$L = L_{pred} + L_{L1}$, (10)\nwhere $L_{pred}$ guides the search towards points x' which would change the model prediction and $L_{L1}$ ensures that x' is close to x.\n\u2022 Native guide counterfactual (NG-CF): NG-CF uses Dynamic Barycenter (DBA) averaging of the query time series X and the nearest unlike neighbor from another class to generate the counterfactual example [9].\n\u2022 Attention-based Counterfactual Explanation (AB-CF): AB-CF leverages the Shannon entropy to extract the most important k subsequences by measuring the information embedded in each segment given the probability distribution and introducing the nearest unlike neighbor replacement to generate the counterfactual example [18]."}, {"title": "VI. CONCLUSION", "content": "In this paper, we introduce M-CELS as an innovative solution that advances counterfactual explanations for multivariate time series classification. First, we present novel optimization techniques specifically designed to handle the complexities of multivariate time series, ensuring the generation of meaningful and interpretable counterfactuals across multiple dimensions. Second, M-CELS leverages learned saliency maps to target the most influential time steps, leading to focused perturbations and promoting higher sparsity. This results in fewer changes to the original instance, making the counterfactuals more actionable and easier to interpret. Finally, these targeted and sparse perturbations enhance the overall interpretability of time series classification, enabling users to better understand model decisions and the influence of key variables over time. Our experiments across seven widely used datasets show that M-CELS outperforms three state-of-the-art counterfactual models, delivering significantly sparser, more proximate, and more valid counterfactual explanations. As a future step, we aim to improve M-CELS' computational efficiency for better scalability on larger datasets and real-time applications."}, {"title": "D. Evaluation metrics", "content": "The goal of our experiments is to assess the performance of the baseline methods concerning all the desired properties of an ideal counterfactual method. To evaluate our proposed method M-CELS, we compare our method with the other three baselines in terms of validity, proximity, and sparsity.\nThe first one is target probability, which is used to evaluate the validity property. We define the validity metric by comparing the target class probability for the prediction of the counterfactual explanation result. The closer the target class probability is to 1, the better.\nThe second evaluation metric we used is the L1 distance to demonstrate the proximity, which is defined in Equation 11. It measures the closeness between the generated counterfactual x' and the original instance of interest x, a smaller L1 distance is preferred.\n$L1 distance = \\sum_{d=1}^D \\sum_{t=1}^T |(x_{t,d} - x'_{t,d})|$, (11)\nThen we use the sparsity level to evaluate the sparsity. Sparsity is evaluated by calculating the percentage of data points that remain unchanged after the perturbation. The highest sparsity is an indicator that the time series perturbations made in x to achieve x' are minimal. Therefore, a higher sparsity level is desirable. The equations designed by [13] are shown in 12-13.\n$Sparsity = 1 - \\frac{1}{D \\cdot T} \\sum_{d=1}^D \\sum_{t=1}^T g(x_{t,d}, x'_{t,d})$, (12)\n$g(x,y) = \\begin{cases} 1, & \\text{if } x \\neq y \\\\ 0, & \\text{otherwise} \\end{cases}$ (13)"}, {"title": "E. Evaluation results", "content": "Figure 1 presents the outcomes of our experiments, comparing our proposed method, M-CELS, against several baseline models. Each figure highlights a different aspect of performance, providing a comprehensive evaluation of the effectiveness of our approach.\nIn Figure 1a, we depict the target probability of the generated counterfactuals. Notably, our approach achieves the highest target probability, indicating superior validity. In contrast, ALIBI's target probability falls below 50%, suggesting invalid counterfactuals. NG achieves a target probability slightly over 50%, which means the perturbation stops whenever the target probability exceeds 50%. This indicates that while NG can generate valid counterfactuals, it does so just barely, often stopping at the minimal threshold. This suggests that our method provides more reliable and valid counterfactual explanations compared to the other baselines, ensuring that the generated counterfactuals align more closely with the desired outcomes.\nNext, in Figure 1b, we assess the proximity property using the L1 distance metric across all counterfactual explanation models. M-CELS achieves the second lowest L1 distance compared to NG, AB, and ALIBI. Although ALIBI exhibits the lowest L1 distance, its counterfactuals are considered invalid due to low target probabilities. This demonstrates that our method strikes a good balance between closeness to the original instance and the validity of the counterfactuals. The ability to generate counterfactuals that are both valid and proximate to the original data points is crucial for interpretability and practical applicability.\nMoving on to Figure 1c, we compare the sparsity properties (the percentage of unchanged data points after perturbation) for generated counterfactuals across all models. M-CELS outperforms the other baselines in terms of sparsity, indicating minimal alterations to crucial data points during perturbation. High sparsity is essential as it preserves the integrity of the original data while ensuring that the counterfactual explanation remains meaningful and interpretable. By making fewer changes, M-CELS maintains the context and relevance of the original instance, making it easier for users to understand the necessary adjustments for a different outcome.\nIn summary, counterfactual explanations generated by M-CELS exhibit superior performance in terms of validity, sparsity, and proximity. This comprehensive performance profile underscores the effectiveness of M-CELS in generating high-quality counterfactual explanations that are both meaningful and practical for real-world applications."}]}