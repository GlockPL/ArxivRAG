{"title": "Utilizing Data Fingerprints for Privacy-Preserving Algorithm Selection in Time Series Classification: Performance and Uncertainty Estimation on Unseen Datasets", "authors": ["Lars B\u00f6cking", "Leopold M\u00fcller", "Niklas K\u00fchl"], "abstract": "The selection of algorithms is a crucial step in designing Al services for real-world time series classification use cases. Traditional methods such as neural architecture search, automated machine learning, combined algorithm selection, and hyperparameter optimizations are effective but require considerable computational resources and necessitate access to all data points to run their optimizations. In this work, we introduce a novel data fingerprint that describes any time series classification dataset in a privacy-preserving manner and provides insight into the algorithm selection problem without requiring training on the (unseen) dataset. By decomposing the multi-target regression problem, only our data fingerprints are used to estimate algorithm performance and uncertainty in a scalable and adaptable manner. Our approach is evaluated on the 112 University of California riverside benchmark datasets, demonstrating its effectiveness in predicting the performance of 35 state-of-the-art algorithms and providing valuable insights for effective algorithm selection in time series classification service systems, improving a naive baseline by 7.32% on average in estimating the mean performance and 15.81% in estimating the uncertainty.", "sections": [{"title": "1. Introduction", "content": "Time series classification involves analyzing sequences of data points, indexed in time order, to categorize them into predefined classes. It is crucial in various services such as health record analysis (W. K. Wang et al., 2022), predictive maintenance (Rudolph et al., 2020), cyber-security (MontazeriShatoori et al., 2020), and earthquake prediction (Arul & Kareem, 2021), reflecting its wide-ranging impact in both scientific and practical applications.\nThe plethora of algorithms developed for time series classification presents a significant challenge: the selection of the most appropriate algorithm in a service to achieve the best performance for a specific dataset-and, therefore, the related domain problem-is a complex task (Oreski & Redep, 2018). This task is formalized by Rice and is known as the algorithm selection (AS) problem (Rice, 1976). According to the \"no free lunch\" theorem, under certain assumptions, no algorithm uniformly dominates all others (Wolpert & Macready, 1997). It is not known which algorithm will perform best on the new dataset. Therefore, we present an approach to estimate these performances based on a data fingerprint, describing the dataset in an aggregated manner Figure 1. As a result, our approach allows us to differentiate variance in classification accuracies among different algorithms on real-world datasets, such as the UCR Yoga-dataset (Bagnall et al., 2017), an observation highlighted in Figure 2b.\nOrganizations, researchers and service provider alike are constrained by limited resources-time, computation, data, and expertise. Other methods, such as neural architecture search (NAS) (Elsken et al., 2019), automated machine learning (AutoML) (Feurer et al., 2022; Hutter et al., 2019), transfer learning (TL) (Lu et al., 2015), hyperparameter optimization (HPO) (Bischl et al., 2023) and algorithm configuration (Lindauer et al., 2015) engage in broad algorithmic experimentation or adaptation of pre-trained models for similar tasks. These methods, despite their effectiveness, face substantial resource demands and complexity in selecting the most appropriate algorithm for possible deployment within an AI service (Elsken et al., 2018). In addition, they necessitate access to data points to run their optimizations, compromising data privacy. In scenarios where data privacy is a concern, the AI service provider needs a solution that allows for informed decision-making without requiring full access to the dataset. For a new, unseen time series classification dataset, the service provider wants to assess which state-of-the-art algorithm is most promising without training the algorithms themselves, running HPO or NAS. Instead, this can be achieved by analyzing the dataset's characteristics, enabling the service to act as an assistant in the AI development process while maintaining data privacy.\nTo address these shortcomings, we want to answer the following research questions (RQs): (RQ1) How can diverse time series datasets be translated into a standardized input format to facilitate comparison and analysis? (RQ2) To what extent can this standardized input format be used to estimate the expected performance of various state-of-the-art classification algorithms? (RQ3) How can the uncertainty associated with the predicted algorithm performance on these standardized inputs be estimated?\nIn our work, we propose a data fingerprint to characterize datasets. We translate the algorithm selection (AS) problem into a multi-target regression problem that estimates algorithm performance and uncertainty, as illustrated in Figure 1. We train various regressors on data fingerprints from benchmark datasets to predict the performance of time series classification algorithms on these benchmarks. Once trained, these regressors can be applied to new, unseen data fingerprints to predict how each algorithm within a service system will perform on the new datasets. As a result, our approach effectively suggests the most suitable algorithm for any new dataset in a privacy-preserving manner, as only the data fingerprint is shared, not the actual data points. Therefore, a service provider does not need to access the dataset but can assist in the AI development process by suggesting the most promising classification algorithm-only by processing the fingerprint, which does not expose any data points. Our approach is highly customizable and can provide tailored suggestions by predicting other target variables in addition to accuracy and uncertainty to provide a basis for informed decision-making for AS in Al services.\nOur contributions are threefold:\n1. We introduce novel data fingerprints to form feature maps for AS representing whole time series datasets. They capture the essential attributes of any time series dataset, making it easier to compare datasets and providing a standardized input for regression models.\n2. We present a customizable approach that utilizes the fingerprints and benchmark results to decompose any multi-target regression problem related to AS. This includes estimating algorithm performance and its uncertainty in the work at hand-but can easily be adapted to other target objectives, to select the algorithm that will deliver"}, {"title": "2. Related Work", "content": "In this section, we review various methods for algorithm selection, highlighting the computational challenges and privacy concerns associated with these methods to give an overview of the state-of-the-art in the field.\nAlgorithm selection: AS generally describes the selection of the most suitable algorithms for novel tasks (Rice, 1976). Unlike the broader approach that does not distinguish between general and machine learning-specific algorithms, our focus is squarely on the latter. We adopt a meta-learning perspective, utilizing machine learning algorithms not for direct selection but for estimating performance and uncertainty of potential algorithm choices. Based on these results, we use the predictions of the best algorithm selectors to make a final prediction about the expected performance and uncertainty. However, the predictions could also be used for the selection itself. For example, the algorithm with the highest performance could be selected. Other target values, such as the expected running time, could also be estimated and used as a basis for decision-making. A distinction can be made between online and offline AS (Degroote, 2017). Online AS describes the case where no training data is available in advance, and the selection is made iteratively. Our approach is to be considered offline AS.\nDistinction from other methods: The selection of hyperparameters for an algorithm can also be optimized, which is referred to as HPO (Bischl et al., 2023); if they are generalized for a set of tasks, this is called algorithm configuration (Schede et al., 2022). As we use benchmarks as a data basis, we assume that the benchmarks utilized already incorporate potential performance improvements achievable through HPO or algorithm configuration. This is also an advantage over other methods such as NAS (Elsken et al., 2019) or AutoML (Hutter et al., 2019), which have also been applied to the algorithm selection problem in time-series classification (Mu et al., 2023; Parmentier et al., 2021). These methods are not only computationally intensive, as they attempt to find an optimal architecture or algorithm through targeted experimentation, but they also require full access to data points, compromising data privacy. There is ongoing research addressing the limitation of data privacy in NAS and AutoML (F. Wang et al., 2022; Yan et al., 2022; Zhang et al., 2021). These works, although primarily focused on domains other than time series classification, highlight the importance of privacy-preserving techniques in algorithm selection and model training. If both AS and HPO are carried out, it is named the combined algorithm selection and hyperparameter optimization (CASH) problem (Thornton et al., 2013). A large number of algorithms already exist for time series classification. Our work is intended to help estimate the performance and uncertainty for new datasets and thus support the selection process instead of searching for new architectures in a computationally intensive manner. TL (Lu et al., 2015) is also based on existing algorithms, which are adapted to the new task. However, here too, the expected performance is unknown in advance."}, {"title": "3. Approach", "content": "Consider an AI service that supports the development of time series classification solutions by recommending the most suitable algorithms based on data characteristics, enabling more informed AS and solutions tailored to the specific data of new clients. While the current state-of-the-art time series classification algorithm can be employed, this approach often overlooks the complexities and nuances inherent in real-world data. These algorithms typically rely on benchmark datasets that may not fully capture the intricacies of diverse datasets. Instead, our approach identifies the most promising algorithm based on the unique characteristics of the new client's dataset, thereby delivering a truly smart service as defined by Jussen et al., 2020.\nTo formalize our approach, we start by defining the time series classification problem, its performance assessment, and key concepts of our fingerprint aggregation. For a given time series x, a time series classification algorithm predicts its target class y. Each task is represented by an individual dataset d with an arbitrary number of instances $x^i$ to be classified. Let"}, {"title": "3.1. Instance-level fingerprint", "content": "On the first level, we want to describe each instance by representations of fixed size instead of the actual values themselves. Thus, we introduce an instance-level fingerprint. This instance-level fingerprint is described by $f_1(.)$ and is calculated for each instance $x^{i,d}$ of a dataset d separately: $ \\forall i \\in I : f_1(x^{i,d})$. One exemplary representation could be to identify the deviation of change in one time step to the average deviation given by $\\Delta x^{i,d}$, such as $f_1(x^{i,d}) = $ \nOther statistical measures can also be used to capture different aspects of the time series data. The descriptive statistics are combined in a $(1 \\times L_1)$ vector, where $L_1$ is the number of statistical measures used, to form the instance-level fingerprint $f_1(x^{i,d})$. An overview of the proposed statistical measures like Skewness $Skew[X]$ and Kurtosis $Kurt[X]$ can be found in the code of this work."}, {"title": "3.2. Class-level fingerprint", "content": "On the next level, we want to aggregate the instance-level fingerprints of each class into class-level fingerprints of fixed size. So each class is assigned a class-level fingerprint, which results from transforming all instance-level fingerprints belonging to that class.\nTo aggregate the instance fingerprint for all instances of a given class c in a dataset d, we first define the set of indexes related to a specific class as $I_c = {i|y^{i,d} = c}$. We can then derive $f_c(.)$ as: $ \\forall c \\in C : f_c(\\left\\{f_1(x^{i,d})\\right\\}_{i\\in I_c})$. One option is to calculate the average of a given fingerprint across all instances of a class, so $f_c(.) = \\frac{1}{|I_c|} \\sum_{i \\in I_c} f_1(x^{i,d})$. Another option is to take the median value as a representation for the class instances. Again, $f_c(.)$ can be independently selected from $f_1(.)$ or the later defined data fingerprint $f_D(.). Our approach provides a generalizing concept that can be easily extended and adapted by choosing different aggregation functions."}, {"title": "3.3. Dataset-level fingerprint", "content": "On the last level, we aggregate the previously calculated class-level fingerprint $f_c(.)$ and extend them by meta characteristics to form a standardized dataset-level fingerprint that describes any time series classification dataset as a function of $f_c(.)$, as: $ \\forall d \\in D : f_D(f_c(.), ..., f_c(.)).$ One example aggregation function is the standard deviation of each class-level fingerprint across the available classes:\n$\\sqrt{\\frac{\\sum_{c\\epsilon C} (f_c(.) - \\bar{f_c})^2}{|C|}}$\nBesides the aggregated $f_c(.)$, we also add meta characteristics of our dataset, such as the total number of training and test instances, the length of each instance expressed as $||x^{i,d}||$, number of target classes. Moreover, the distribution of instances across classes is characterized by the minimum and maximum number of instances in any class, represented by $min(||I_c||)$ and $max(||I_c||)$, respectively as well as the average"}, {"title": "3.4. Performance estimation", "content": "There is no single algorithm that performs best on all available tasks the \u201cno free lunch\" theorem (Wolpert & Macready, 1997). Our approach addresses this by mapping our proposed fingerprint $f_D(.)$ to any multi-objective performance measures defined by the multi-target regression problem. It does so by decomposing the performance and its uncertainty, as well as the algorithm h(.), learning a regressor r(.) separately as shown in Figure 1 on page 1.\nMotivated by the central limit theorem (Rosenblatt, 1956) and the asymptotic characteristics of k-folds (Li, 2023), we derive the estimation of the performance and its uncertainty for our approach. We learn a regressor r(.) such that $r(f_D(.) \\rightarrow \\mu(E_d))$, estimating the mean classification performance of algorithm h(.) on a dataset d as well as the observed standard deviation in performance $f_D(.) \\rightarrow \\sigma(E_d)$. Note that our approach and code allow us to estimate various characteristics of an algorithm's performance, e.g., lower percentiles of the k-folds for estimating lower bounds in a risk-averse setting like earthquake prediction (Arul & Kareem, 2021). Details on the regressors r(.) applied to the multi-target regression AS problem can be found in Section 4.4."}, {"title": "4. Experiment & results", "content": "Our approach estimates the performance of an algorithm h(.) on a dataset d, described by $E_d$, solely through the computation of select characteristics that describe the dataset. To train and test this mapping, we need various datasets and the related performance of multiple classification algorithms. We evaluate our approach on the 112 univariate time series datasets established in the UCR classification benchmark (Bagnall et al., 2017; Dau et al., 2019). The performances are established by Middlehurst et al., 2023 in their back-off paper and available as part of the time series machine learning package (Middlehurst et al., 2023). We run our evaluation on all 35 algorithms h(.) referenced in this most recent benchmark, such as BOSS (Sch\u00e4fer, 2015), HC2 (Middlehurst et al., 2021), InceptionT (Ismail Fawaz et al., 2020), ROCKET (Dempster et al., 2020), among others 1. For each dataset $d \\in D$ we calculate the instance fingerprint $f_1(.)$, the class fingerprint $f_c(.)$, and finally accumulate the data fingerprint $f_D(.)$. Our approach estimates the classification performance $\\mu(E_d)$ and uncertainty $\\sigma(E_d)$ of algorithms h(.) based on this final fingerprint.\nWe split the 112 datasets of the UCR benchmark $d \\in [1,..., D]$ by a .2/.2/.6 train-validation-test split. For each of the individual datasets in $D_{train}, D_{validation}$ and $D_{test}$, we calculate their fingerprint $f_D(.)$ and pair them with the achieved performance of each classification algorithm h.\nThe performance regressors r(.) are trained on the fingerprint and classification performances of h(.) on all datasets in $D_{train}$. The regressors r(.) are selected based on their accuracy in performance estimation of classification algorithms h(.) on $D_{validation}$. We evaluate our approach by running the regressor r(.) on the fingerprints of $D_{test}$ and compare the estimated performances and uncertainty to the benchmark results. The code of this work is publicly available2."}, {"title": "4.1. Naive baseline", "content": "We derive a naive baseline $\\bar{\\mu_h}$ for the mean performance $E_d$ of an algorithm h on a dataset d building upon the common concept of a single best solver (Bischl et al., 2016). We define $\\bar{\\mu_h} = \\frac{1}{|D_{train}|} \\sum_{d \\epsilon D_{train}} \\mu(E_d)$. It reflects the average performance of algorithm h on $D_{train}$, the datasets used for training our performance estimator. Correspondingly a naive baseline $\\bar{\\sigma_h}$ for the expected uncertainty in performance can be calculated by $\\bar{\\sigma_h} = \\frac{1}{|D_{train}|} \\sum_{d \\epsilon D_{train}} \\sigma(E_d)$. Our approach estimates $\\mu(E_d)$ and $\\sigma(E_d)$ for any $d \\in D_{test}$ is benchmarked against this baseline."}, {"title": "4.2. Fingerprints", "content": "Instance-level fingerprints: The instance-level fingerprint $f_1(.)$ describes instances of any length by a fixed-size vector. Accurately differentiating each individual instance just by its fingerprint seems challenging, as shown in Figure 2a. Still underlying patterns can be identified, e.g. instances of target class 1 in the Yoga-dataset have higher Skewness $Skew[X]$ while instances of target class 2 have higher mean change $\\Delta x^{i,d}$, as shown in Figure 4.\nClass-level fingerprint: Our approach aggregates instance-level fingerprint $f_1(.)$ across all instances I for each target class c. Figure 4 provides a fingerprint for the first ten individual instances of each target class in the Yoga-dataset, as well as their class-level aggregation $f_c(.)$. Note that this visualization highlights the concept for a reduced number of instances (ten in this case). The actual class-level fingerprint $f_c(.)$ is aggregated on all instances I in the training subset of the given dataset d. Still, only these ten instances result in a class fingerprint with certain distinguishing characteristics, e.g., Kurtosis and Skewness.\nDataset-level fingerprint: Finally, to build a fixed-size fingerprint that can be utilized to describe any time series classification task, the class-level fingerprints $f_c(.)$ are aggregated on dataset granularity. For the dataset-level aggregation, standard deviation, interquartile range, and the range between the minimum and maximum value are calculated. Each of those fixed-sized fingerprints representing an individual dataset is then mapped to an algorithm performance $f_D(.) \\rightarrow E_d$."}, {"title": "4.3. Performance estimation for a given algorithm", "content": "We evaluate various regression models on the decomposed multi-target of estimating an algorithm's (h) mean performance on a dataset d, described by $\\mu(E_d)$ and the uncertainty across the k-folds, described by the standard deviation $\\sigma(E_d)$. The mean performance $\\mu(E_d)$ is shown in Figure 5 and the estimated uncertainty $\\sigma(E_d)$ is shown in Figure 6 for h 1NN-DTW (exemplarily). Algorithms ridge and random forest are selected based on their performance on $D_{val}$ shown in the upper half and evaluated on $D_{test}$ shown in the lower half. Reporting average relative improvement to account for the different baseline levels.\nThe hatched area indicates predictions with a relative improvement, while un-hatched areas cover points where the performance estimation is further off than the naive baseline. Our performance estimation achieves a relative improvement, if $|\\mu(E) - \\mu(E_d)| < |\\mu_h - \\mu(E)|$. This dynamic applies both to comparing the ground truth mean performance to the estimated mean performance $\\mu(E_d)$ as well as the ground truth standard deviation across the k-folds compared to the estimated uncertainty $\\sigma(E_d)$. Algorithms ridge and random forest are selected based on their performance on $D_{val}$ shown in the upper half and evaluated on $D_{test}$ shown in the lower half."}, {"title": "4.4. Estimation improvements benchmark", "content": "In Table 1, we report the absolute error level on the test set as the mean absolute error (MAE) and the relative improvement compared to the naive baseline. For each algorithm (row) we report: (1) Which model is selected based on its MAE performance on the validation set. (2) The performance of naive baselines $\\bar{\\mu_h}$ and $\\bar{\\sigma_h}$. (3) MAE in predicting the mean performance $\\mu(E_d)$ and the std. across the k-folds $\\sigma(E_d)$. (4) The relative change of MAE in %.\nFor example, when a ridge regressor estimates the performance of the INN-DTW algorithm, as shown in Table 1, we observe a significant improvement: Just by analyzing the fingerprint $f_D(.)$ of the unseen test datasets, our approach is 18.61% more accurate in estimating the mean ground truth performance on these datasets $E_d$, compared to the naive baseline measured by the MAE. Estimating the uncertainty of the 1NN-DTW-algorithm $\\sigma(E_d)$ our approach improves the naive baseline by 37.31% in MAE. A visual interpretation of these results as well as a comparison of the performance on the validation datasets $D_{val}$ and the test datasets $D_{test}$ is given in fig. 7. In summary, based on the validation set performance for each algorithm h(.), our approach outperforms the naive baseline by an average of -7.32% for the MAE when predicting the mean performance and by -15.81% in the MAE of the std. deviation. Our approach is capable of differentiating performances on individual datasets. Instead of selecting an algorithm based on its average performance on some publicly available benchmark, our approach allows for precise estimation of the exact performance each algorithm will achieve on a specific dataset. This enables a more informed and tailored algorithm selection process, ensuring that the chosen algorithm is the most suitable for the unique characteristics of the new dataset."}, {"title": "5. Limitations & Future Work", "content": "While our current approach demonstrates significant advancements, it also has certain limitations that opens various directions for future work. The decomposition of the multi-target regression overlooks the intricate dependencies between algorithms and the collective objectives (Lorena et al., 2008). In future work, this can be investigated by a regressor that predicts the performance of multiple algorithms at once. The selection requires domain experts to balance different objectives, such as mean performance and uncertainty, which can complicate the AS process. Further development into an AI service could incorporate relaxations of multiple objectives and include domain experts in a human-in-the-loop manner. Our approach tests a range of regressors, including those with transparent internal mechanics, but does not inherently prioritize regressors based on their interpretability. This may restrict its usefulness when understanding a model's internal decision-making process, which is often crucial in real-world applications. The predictions and properties, such as the interpretability, could be combined in a decision rule for the final AS that meets the user's preferences. The effectiveness of our estimation strategy depends on the chosen metric, with MAE showing different levels of robustness and volatility in performance improvements across validation and test sets (as documented in Table 1). To improve the robustness of our approach, we encourage researchers to share their data fingerprint and the corresponding performances (Koester et al., 2020). For the next steps, our performance estimations can guide service providers in the domain, so instead of auto-correcting human decisions, we can provide feedback on which algorithm would be more suited (Balla et al., 2023). Such an extension follows the trajectory of leveraging technology to advance service, as suggested by Ostrom et al., 2010. Further developments of our approach can follow up on the ongoing discussion about which additional objectives to assess (e.g., expected running time (Bossek & Trautmann, 2019)). Our adaptable and extensible approach allows us to estimate such objectives instead or aside from the performance and uncertainty."}, {"title": "6. Conclusion", "content": "This paper introduces a novel data fingerprint for time-series classification, offering an approach to support more effective and privacy-preserving AI development without having access to all data points. We predict algorithmic performance and associated uncertainties by strategically decomposing the multi-target regression problem.\nOur assessment across 112 datasets of the University of California riverside benchmark showcases its capability in accurately forecasting the outcomes of 35 state-of-the-art algorithms, surpassing a naive baseline by an average of 7.32% in estimating mean performance and 15.81% in quantifying uncertainty. Our approach will assist researchers and professionals in the field of algorithm selection in time series classification to set up successful Al services. We encourage other researchers and practitioners to use and extend the approach with the proposed fingerprints for further objectives. A promising field of research lies ahead."}]}