{"title": "ADVERSARIAL TRAINING FOR DEFENSE AGAINST LABEL POISONING ATTACKS", "authors": ["Melis Ilayda Bal", "Volkan Cevher", "Michael Muehlebach"], "abstract": "As machine learning models grow in complexity and increasingly rely on publicly sourced data, such as the human-annotated labels used in training large language models, they become more vulnerable to label poisoning attacks. These attacks, in which adversaries subtly alter the labels within a training dataset, can severely degrade model performance, posing significant risks in critical applications. In this paper, we propose FLORAL, a novel adversarial training defense strategy based on support vector machines (SVMs) to counter these threats. Utilizing a bilevel optimization framework, we cast the training process as a non-zero-sum Stackelberg game between an attacker, who strategically poisons critical training labels, and the model, which seeks to recover from such attacks. Our approach accommodates various model architectures and employs a projected gradient descent algorithm with kernel SVMs for adversarial training. We provide a theoretical analysis of our algorithm's convergence properties and empirically evaluate FLORAL'S effectiveness across diverse classification tasks. Compared to robust baselines and foundation models such as ROBERTa, FLORAL consistently achieves higher robust accuracy under increasing attacker budgets. These results underscore the potential of FLORAL to enhance the resilience of machine learning models against label poisoning threats, thereby ensuring robust classification in adversarial settings.", "sections": [{"title": "1 INTRODUCTION", "content": "The susceptibility of machine learning models to the integrity of their training data is a growing concern, particularly as these models become more complex and reliant on large volumes of publicly sourced data, such as the human-annotated labels used in training large language models (Kumar et al., 2020; Cheng et al., 2020; Wang et al., 2023). Any compromise in training data can severely undermine a model's performance and reliability (Dalvi et al., 2004; Szegedy et al., 2013)\u2014 leading to catastrophic outcomes in security-critical applications, such as fraud detection (Fiore et al., 2019), medical diagnosis (Finlayson et al., 2019), and autonomous driving (Deng et al., 2020).\nOne of the most insidious forms of threat is the data poisoning (causative) attacks (Barreno et al., 2010), where adversaries subtly manipulate a subset of the training data, causing the model to learn erroneous input-output associations. These attacks can involve either feature or label perturbations. Unlike feature poisoning, which alters the input data itself, (triggerless) label poisoning is particularly challenging to detect because only the labels are modified, leaving the input data unchanged, as illustrated in Figure 2. Deep learning models are inherently vulnerable to random label noise (Zhang et al., 2017), and this susceptibility is magnified when the noise is adversarially crafted to be more damaging. Figure 1b illustrates this vulnerability: The RoBERTa model (Liu et al., 2019) fine-tuned for sentiment analysis suffers substantial performance degradation under label poisoning attacks (Zhu et al., 2022), with severity growing as the attacker's budget increases. In contrast, Figure 1c highlights FLORAL's effectiveness in mitigating these attacks. Here, the adversarially labelled dataset is generated by poisoning the labels of the most influential training points (see Appendix C.3 for details)."}, {"title": "2 PROBLEM STATEMENT AND BACKGROUND", "content": "We tackle the problem of robust binary classification in the presence of label poisoning attacks (see Section 3 for an extension to multi-class classification). Given a training dataset \\(D = \\{(x_i, y_i) \\in (\\mathcal{X}, \\mathcal{Y})\\}_{i=1}^n\\), where \\( \\mathcal{X} \\subset \\mathbb{R}^d \\) are the input features and \\( \\mathcal{Y} = \\{\\pm 1\\} \\) are the binary labels (potentially involving adversarial labels), we consider a kernel SVM classifier \\(f_\\lambda(x) := \\text{sign}(\\sum_{j} \\lambda_j y_j k(x, x_j)+b)\\), parametrized by \\( \\lambda \\in \\mathbb{R}^n \\) and bias \\(b \\in \\mathbb{R}\\), which assigns a label to each data point and is derived from the following quadratic program (dual formulation) (Boser et al., 1992; Hearst et al., 1998):\n\\[D(f_\\lambda; D): \\min_{\\lambda \\in \\mathbb{R}^n} \\frac{1}{2} \\lambda^T \\Omega \\lambda - 1^T \\lambda\\]\nsubject to\n\\[\\lambda^T y = 0, \\quad 0 \\leq \\lambda \\leq C,\\]\nwhere \\( \\Omega \\in \\mathbb{R}^{n \\times n} \\) is a positive semi-definite matrix, with elements \\( \\Omega_{ij} = y_i y_j K_{ij} \\) and 1 is the n-dimensional vector of all ones. Here, \\(K\\) is the Gram matrix with entries \\(K_{ij} = k(x_i, x_j), \\forall i, j \\in [n] := \\{1, ..., n\\}\\), derived from a kernel function \\(k\\). A common kernel choice is the radial basis function (RBF), given as \\(k(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)\\), with width parameter \\(\\gamma\\). The parameter \\(C \\geq 0\\) is a regularization term, balancing the trade-off between maximizing the margin and minimizing classification errors. In this formulation, each dual variable \\(\\lambda_i, i \\in [n]\\) corresponds to the Lagrange multiplier associated with the misclassification constraint for the training point \\(x_i\\)."}, {"title": "3 THE FLORAL APPROACH", "content": "In the context of label poisoning attacks, the attacker's objective is to maximize the model's test classification error by subtly altering the labels in the training dataset to an optimal adversarial configuration. Adversarial training (Goodfellow et al., 2015; Madry et al., 2017) can be extended to counter these attacks and minimize model sensitivity to disruptive labels by actively optimizing for robustness under worst-case scenarios. In this setting, the attacker generates the optimal label attack within a budget of \\(k\\) flips to maximize the model's loss, while the model seeks parameters that minimize this worst-case loss. A straightforward, yet naive (Robey et al., 2024), way to implement this approach would be to use the following minimax formulation:\n\\[\\min_{\\lambda \\in \\mathbb{R}^n} \\frac{1}{n} \\sum_{i=1}^{n} \\max_{\\tilde{y}_i \\in \\mathcal{Y}, i \\in [n] \\atop \\sum_{i \\in [n]} 1\\{\\tilde{y}_i \\neq y_i\\} = k} \\mathcal{L}(f_\\lambda(x_i), \\tilde{y}_i),\\]\nwhere \\(\\mathcal{L}\\) denotes a loss function, which in the case of the kernel SVM is related to the hinge loss (Smola & Sch\u00f6lkopf, 1998), and \\(\\tilde{y}\\) represents the adversarial label set. This formulation is problematic for multiple reasons:\n1.  Misaligned objectives: The loss is only a surrogate for the test accuracy, which is the actual quantity of interest to both the learner and the attacker. However, from an optimization perspective, maximizing an upper bound (such as the hinge loss in SVMs) on the classification error as in (3) is not meaningful as such a bound does not represent the true objective of the attacker. Hence, a non-zero-sum formulation would allow for a more nuanced representation of the attacker's objectives (Yasodharan & Loiseau, 2019)."}, {"title": "4 EXPERIMENTS", "content": "In this section, we showcase the effectiveness of FLORAL across various robust classification tasks, utilizing the following datasets:\n*   Moon (Pedregosa et al., 2011): We employed a synthetic benchmark dataset, \\(D = \\{(x_i, y_i)\\}_{i=1}^{2010}\\) where \\(x_i \\in \\mathbb{R}^2\\) and \\(y_i \\in \\{\\pm 1\\}\\). Adversarial versions are generated by flipping the labels of points farther from the decision boundary of a linear classifier trained on the clean dataset, using label poisoning levels (%) of \\{5, 10, 15, 20, 25\\}. The details on the adversarial datasets are given in Appendix C.1.\n*   IMDB (Maas et al., 2011): A benchmark review sentiment analysis dataset with \\(D = \\{(x_i, y_i)\\}_{i=1}^{50000}\\) where \\(x_i \\in \\mathbb{R}^{768}\\) and \\(y_i \\in \\{\\pm 1\\}\\). For SVM training, we extracted 768-dimensional embeddings from the fine-tuned RoBERTa (Liu et al., 2019). We created adversarial datasets by fine-tuning the RoBERTa-base model on the clean dataset to identify influential training points based on the gradient with respect to the inputs, then flipping their labels at poisoning levels (%) of \\{10, 25, 30, 35, 40\\}.\n*   MNIST (Deng, 2012): In Appendix E.3, we provide the additional experiments with the MNIST dataset in detail.\nExperimental setup. For all SVM-based methods, we used RBF kernel, exploring various values of C and \\(\\gamma\\). We conducted five replications with different train/test splits, including the corresponding adversarial datasets for each dataset. In all FLORAL experiments, we constrain the attacker's capability with a limited budget. That is, the attacker identifies the most influential candidate points, with \\(B = 2k\\), from the training set and randomly selects \\(k \\in \\{1, 2, 5, 10, 25\\}\\) to poison, where \\(k\\) represents the % of points relative to the training set size. Detailed experimental configurations are provided in Appendix C (see Table 3).\nBaselines. We benchmark FLORAL against the following baselines:\n1.  (Vanilla) SVM with an RBF kernel, which serves as a basic benchmark (Hearst et al., 1998).\n2.  LN-SVM (Biggio et al., 2011) applies a heuristic-based kernel matrix correction."}, {"title": "4.1 EXPERIMENT RESULTS", "content": "In this section, we report the performance of FLORAL against the baseline methods on the Moon dataset, followed by the results of its integration with RoBERTa on the IMDB dataset.\nMoon. As reported in Table 1 and Figure 3, FLORAL achieves higher robust accuracy across almost all settings compared to baseline methods. Notably, in scenarios with more severe poisoning levels, FLORAL significantly outperforms all baselines, which experience a marked drop in their accuracy. We report results under various kernel hyperparameters in Appendix E.1 (see Table 4 and"}, {"title": "5 CONCLUSION", "content": "In this paper, we address the vulnerability of machine learning models to label poisoning attacks and propose FLORAL, an adversarial training defense strategy based on kernel SVMs. We formulate the problem using bilevel optimization and frame the adversarial interaction between the learning model and the attacker as a non-zero-sum Stackelberg game. To compute the game equilibrium that solves the optimization problem, we introduce a projected gradient descent-based algorithm and analyze its local stability and convergence properties. Our approach demonstrates superior empirical robustness across various classification tasks compared to robust baseline methods.\nFuture research includes exploring SVM-based transfer attacks or integrating our approach to robust fine-tuning of foundation models for supervised downstream tasks. Additionally, a detailed analysis of how FLORAL alters the most influential training points for model predictions, e.g. when integrated with foundation models such as RoBERTa could provide interesting insights."}, {"title": "A THEORETICAL ANALYSIS PROOFS", "content": "In this section, we present the proofs for the local asymptotic stability analysis of FLORAL (Algorithm 1). We begin by proving Lemma 1 in Section A.1, which establishes that the distance of the updates of Algorithm 1 from the equilibrium of the game is bounded. In Section A.2, we prove Lemma 2, demonstrating that the distance of the non-projected updates from the equilibrium of the game is also bounded. Lastly, in Section A.3, we provide the proof of Theorem 3.1, which shows the local asymptotic stability of our algorithm, with a derivation of a global convergence result presented in Section A.4."}, {"title": "A.1 PROOF OF LEMMA 1", "content": "Our objective is to prove that the distance of the iterates of Algorithm 1 from the Stackelberg equilibrium \\((\\hat{\\lambda}, \\hat{y}(\\hat{\\lambda}))\\), specifically \\(||\\lambda_t - \\hat{\\lambda}||\\), is bounded. We begin by recalling the update rule at round t, \\(\\lambda_t := PROX_{S(y_t)}(z_t) = PROX_{S(y_t)}(\\lambda_{t-1} - \\eta \\nabla_\\lambda f(\\lambda_{t-1}, Y_t))\\), where \\(y_t = \\hat{y}(\\lambda_{t-1})\\), \\(S(y_t)\\) is the feasible region defined by constraints (12), using the labels at round t. The operator PROX is defined below.\nDefinition 1 (PROX operator). The operator \\(PROX_{S(y_t)}(z_t) : \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) denotes the projection of \\(z_t \\in \\mathbb{R}^n\\) onto the convex set \\(S(y_t)\\) at round t of Algorithm 1. PROX minimizes the Euclidean distance and is defined by the following optimization problem:\n\\[PROX_{S(y_t)}(z_t): \\quad \\lambda \\in \\mathbb{R}^n \\quad \\underset{\\lambda}{\\text{arg min}} \\frac{1}{2} || \\lambda - z_t ||^2,\\]\nsubject to\n\\[\\lambda^T y = 0\\]\n\\[0 \\leq \\lambda \\leq C\\]\nEquivalently, \\(PROX_{S(y_t)}\\) solves the following optimization problem:\n\\[\\underset{\\lambda}{\\text{min}} \\quad \\underset{\\mu \\in \\mathbb{R}}{\\text{sup}} \\quad \\frac{1}{2} || \\lambda - z_t ||^2 + \\mu y^T \\lambda.\\]\nLemma 3 (Bounded iterates). The sequence {\\(\\lambda_t\\)} generated by the iterative update rule \\(\\lambda_t := PROX_{S(y_t)}(z_t) = PROX_{S(y_t)}(\\lambda_{t-1} - \\eta \\nabla_\\lambda f(\\lambda_{t-1}, Y_t))\\) is bounded, i.e., \\(||\\lambda_t||_{\\infty} < C, \\forall t \\geq 0\\).\nProof. This follows immediately from the definition of \\(S(y_t)\\).\nIn the following, our aim is to quantify the sensitivity of (19) with respect to its arguments \\(Y_t\\) and \\(Z_t\\). Let \\(\\lambda^*\\) denote the optimal solution to the projection operation. We can express this solution through the following steps. First, we simplify the expression by omitting the index t in (19). Then, we exploit the fact that the objective function is convex-concave with convex constraints, which allows us to interchange the order of the min and the sup. This yields\n\\[\\underset{\\lambda \\in \\mathbb{R}^n}{\\text{min}} \\quad \\underset{\\mu \\in \\mathbb{R}}{\\text{sup}} \\frac{1}{2} || \\lambda - z_t ||^2 + \\mu y^T \\lambda\\]"}, {"title": "A.2 PROOF OF LEMMA 2", "content": "Our objective is to prove that the distance of the non-projected updates of Algorithm 1 from the Stackelberg equilibrium \\((\\hat{\\lambda}, \\hat{y}(\\hat{\\lambda}))\\), specifically \\(z_t  \\hat{z}\\), is bounded.\nWe begin by recalling the update rule at round t, \\( \\lambda_t := PROX_{S(y_t)}(z_t) = PROX_{S(y_t)}(\\lambda_{t-1} - \\eta \\nabla_\\lambda f(\\lambda_{t-1}, Y_t))\\), where \\(y_t = \\hat{y}(\\lambda_{t-1})\\), \\(S(y_t)\\) is the feasible set defined by constraints (12), using the labels at round t. We further recall the Stackelberg equilibrium \\((\\hat{\\lambda}, \\hat{y}(\\hat{\\lambda}))\\), i.e.,\n\\[\\hat{\\lambda} := PROX_{S(\\hat{y}(\\hat{\\lambda}))}(\\hat{z}) = PROX_{S(\\hat{y}(\\hat{\\lambda}))}(\\lambda - \\eta \\nabla_\\lambda f(\\lambda, \\hat{y}(\\lambda)))\\]\n\\[\\hat{y}(\\lambda) := LFLIP(\\lambda),\\]\nwhere the operator LFLIP : X \\(\\times\\) Y -> Y defines the label poisoning attack formulated in (7-9). We conclude the following:\n\\[z_t = \\lambda_{t-1} - \\eta \\nabla_\\lambda f(\\lambda_{t-1}, y_t)\\]"}, {"title": "A.3 PROOF OF THEOREM 3.1", "content": "Let \\((\\hat{\\lambda}, \\hat{y}(\\hat{\\lambda}))\\) denote the Stackelberg equilibrium, i.e.,\n\\[\\hat{\\lambda} := PROX_{S(\\hat{y}(\\hat{\\lambda}))}(\\hat{z}) = PROX_{S(\\hat{y}(\\hat{\\lambda}))}(\\lambda - \\eta \\nabla_\\lambda f(\\lambda, \\hat{y}(\\lambda)))\\]\n\\[\\hat{y}(\\lambda) := LFLIP(\\lambda),\\]\nwhere the operator LFLIP : X \\(\\times\\) Y -> Y defines the label poisoning attack formulated in (7-9). We further assume that the LFLIP operator returns a unique set of adversarial labels at the Stackelberg equilibrium \\((\\hat{\\lambda}, \\hat{y}(\\hat{\\lambda}))\\), which implies that there are no ties with respect to \\(\\hat{\\lambda}\\) values. As a result, there exists a small enough constant \\(\\delta'\\) > 0 such that for any \\(\\lambda_0\\) with \\(||\\lambda_0 - \\hat{\\lambda}||_{\\infty} < \\delta'\\), the corresponding \\(\\hat{y}(\\lambda_0)\\) satisfies \\(\\hat{y}(\\lambda_0) = \\hat{y}(\\lambda)\\). (Indeed, as long as \\(\\delta'\\) is small enough, such that the top-k entries between \\(\\lambda\\) and \\(\\lambda_0\\) agree, \\(\\hat{y}(\\lambda_0) = \\hat{y}(\\lambda)\\) will be satisfied.)\nBy combining Lemma 1 and Lemma 2 we conclude\n\\[||\\lambda_1 - \\hat{\\lambda}||_{\\infty}  k_y ||y_t - \\hat{y}(\\hat{\\lambda})||_{\\infty} + k_\\lambda ||\\lambda_0 - \\hat{\\lambda}||_{\\infty}  k_\\lambda \\delta',\\]\nwhere we used the fact that \\(\\hat{y}(\\lambda_0) = \\hat{y}(\\lambda)\\). The learning rate \\(\\eta\\) is chosen small enough, such that \\(k_\\lambda < 1\\) and therefore \\(||\\lambda_1 - \\hat{\\lambda}||_{\\infty}  k_\\lambda \\delta' < \\delta'\\). We therefore conclude by induction on t that \\(||\\lambda_t - \\hat{\\lambda}||_{\\infty}  k_\\lambda^t \\delta'\\) for all t > 0. This readily implies \\(\\lambda_t \\rightarrow \\hat{\\lambda}\\). Moreover, choosing \\(\\delta = min\\{\\epsilon, \\delta'\\}\\) concludes \\(||\\lambda_t - \\hat{\\lambda}||_{\\infty} < \\epsilon\\) and concludes the proof."}, {"title": "A.4 GLOBAL CONVERGENCE RESULT", "content": "The previous section provides the proof of Theorem 3.1, which provides a local stability and convergence result. Under additional assumptions on the constants \\(k_Y\\) and \\(k_X\\) that capture the sensitivity of the iterates \\(\\lambda_t\\) with respect to changes in the labels, one can derive a global convergence result, as summarized by the following proposition:\nProposition 1. Let (\\(\\lambda\\), \\(\\hat{y}(\\lambda)\\)) denote the Stackelberg equilibrium as before and let d' = (\\(\\lambda^{\\{k\\}}\\) - \\(\\lambda^{\\{k+1\\}}\\))/2 > 0, where {\\(\\lambda^{\\{1\\}}\\)} denotes the largest entry of \\(\\lambda\\), \\(\\lambda^{\\{2\\}}\\) the second larges entry of \\(\\lambda\\), etc. Provided that\n\\[\\frac{2(\\kappa_Y + \\kappa_Y)k}{1 - \\kappa_\\lambda} < \\delta'\\]\nholds and that the step-size \\(\\eta\\) is chosen to be small enough, the iterates {\\(\\lambda_t\\)} of FLORAL are guaranteed to converge to \\(\\lambda\\) from any initial condition \\(\\lambda_0\\).\nProof. As a result of Lemma 1 and Lemma 2 we conclude that\n\\[||\\lambda_t - \\lambda||_{\\infty} < \\kappa_Y ||\\hat{y}(\\lambda_{t-1}) - \\hat{y}(\\lambda)||_{\\infty} + ||z_{t-1} - \\hat{z}||_{\\infty}\\]"}, {"title": "B THE GRADIENT OF THE OBJECTIVE (4)", "content": "We begin by recalling the kernel SVM dual formulation (Boser et al., 1992; Hearst et al., 1998):\n\\[D(f_\\lambda; D): \\underset{\\lambda}{\\text{min}} \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\lambda_i \\lambda_j y_i y_j K_{ij} - \\sum_{i=1}^{n} \\lambda_i,\\]\nsubject to\n\\[\\sum_{i=1}^{n} \\lambda_i y_i = 0\\]\n\\[0 \\leq \\lambda_i \\leq C, \\forall i \\in [n],\\]\nwhere K represents the Gram matrix with entries \\(K_{ij} = k(x_i, x_j), \\forall i, j \\in [n] := \\{1,...,n\\}\\), derived from a kernel function k. We consider the pth data point and apply differentiation of a double summation to the objective, which yields\n\\[\\frac{d}{d \\lambda_p} (D(f_\\lambda; D)) = \\frac{1}{2} \\left( \\sum_{i=1}^{n} \\lambda_i y_i y_p K_{ip} +  \\sum_{j=1}^{n} \\lambda_p y_p y_j K_{pj} \\right) - 1\\]"}, {"title": "C EXPERIMENT DETAILS", "content": "For our experiments, we set the hyperparameter values as given in Table 3. We provide the experiment details as follows:\n*   We initialize the model f0 with parameters set to 0. In FLORAL, however, the attacker uses a randomized top-k rule to identify the B most influential support vectors based on the \\(\\lambda\\) values. Due to the 0 initialization of \\(\\lambda\\), a warm-up period is required, which we set to 1 round for all SVM-related methods.\n*   To train kernel SVM classifiers for all SVM-related methods other than FLORAL, We use our PGD-based Algorithm 1 with a dummy attack, that is, we eliminate the adversarial dataset generation step and employ vanilla PGD training.\n*   For large datasets such as IMDB, we implement projection via fixed point iteration as given in Algorithm 2 in Section 3.2 instead of constructing a quadratic program as defined in (11-12)."}, {"title": "C.1 DATASETS", "content": "*   Moon is a benchmark dataset for binary classification tasks, generated directly using the scikit-learn library (Pedregosa et al., 2011). It contains two-dimensional input examples with each feature taking value in the range [-2.5, 2.5]. We generate its adversarial versions by flipping the labels of farthest points from the decision boundary of a linear classifier trained on the clean dataset, using label poisoning levels (%) of {5, 10, 15, 20, 25}. We provide the visualizations of the Moon training dataset with clean and adversarial labels in Figure 5.\n*   IMDB review sentiment analysis benchmark dataset (Maas et al., 2011) contains train and test datasets, each containing 25, 000 examples. We used randomly selected 20, 000 points from the training set as training examples, and the rest as validation examples. We fine-tuned the RoBERTa-base model (Liu et al., 2019) on this dataset and extracted features (768-dimensional embeddings) to train SVM-related models on this dataset. We generated adversarially labelled datasets using the fine-tuned RoBERTa-base model on the clean dataset. Specifically, we identified the most influential training points based on the gradient of loss with respect to the inputs and flipped their labels under various poisoning levels (%) of {10, 25, 30, 35, 40}."}, {"title": "C.2 BASELINES", "content": "In our main experiments, we compared FLORAL against the baseline methods by carefully selecting their hyperparameters using the domain knowledge, which we detail below.\n*   LN-SVM (Biggio et al., 2011) applies a heuristic-based kernel matrix correction by assuming that every label in the training set is independently flipped with the same probability. It requires a predefined noise parameter \\(\\mu\\), which we set to \\(\\mu \\in \\{0.05, 0.1, 0.15, 0.2, 0.25\\}\\) by leveraging the domain label poisoning knowledge, i.e. using the poisoning levels of the adversarial datasets.\n*   For Curie (Laishram & Phoha, 2016), we set the confidence parameter to \\{0.95, 0.90, 0.85, 0.8, 0.75\\}. To compute the average distance, we considered k = 20 neighbors in the same cluster for the Moon dataset and k = 1000 neighbors for the IMDB dataset experiments.\n*   For LS-SVM (Paudice et al., 2018), we use the relabeling confidence threshold from \\{0.95, 0.90, 0.85, 0.8, 0.75\\}, again aligning with the poisoning level of the adversarial datasets. For its k-NN step, we considered k = 20 and k = 1000 neighbors for the Moon and IMDB datasets, respectively.\n*   NN baseline is a fully connected multi-layer perceptron with two hidden layers with 32 units each, trained using the SGD optimizer with 0.9 momentum and binary cross-entropy loss. For additional experiments on the MNIST dataset, a similar architecture with two hidden layers having {32, 10} units is employed.\n*   NN-PGD is based on the same NN architecture as above, trained with PGD-AT (Madry et al., 2017) using a standard perturbation budget of 8/255 and a step size of 2/255."}, {"title": "C.3 ROBERTA EXPERIMENT DETAILS", "content": "We fine-tune the RoBERTa-base model\u00b9 on the IMDB review sentiment analysis dataset\u00b2. We fine-tune the model for three epochs with no warm-up steps, using the AdamW optimizer, weight decay 0.01, batch size 16, and learning rate 2e-05 with a linear scheduler, using a single NVIDIA A100 40GB GPU. We extract the last layer embeddings of the trained model for experiments with FLORAL integration."}, {"title": "D EFFECTIVENESS ANALYSIS OF FLORAL DEFENSE", "content": "As explained in Section 3, FLORAL takes a proactive defense when the initial training data is clean, iteratively adjusting the model to reduce sensitivity to potential label poisoning attacks by exposing it to adversarial decision boundary configurations through adversarial training. Conversely, when the training data is already contaminated with adversarial labels, FLORAL mitigates their effect by implicitly sanitizing the corrupted labels.\nTo demonstrate how FLORAL defenses under already poisoned training data, we further analyze the efficacy of FLORAL by measuring its \"recovery\" rate of poisoned labels. That is, we quantify FLORAL's rate of disrupting the initial attack (%) on the adversarially labelled training sets, averaged over replications."}, {"title": "E ADDITIONAL EXPERIMENTAL RESULTS", "content": "We provide additional experimental results under various hyperparameter settings for the Moon dataset in Appendix E.1. In Appendix E.2, we first report a comprehensive comparison of FLORAL against other baselines on the IMDB dataset, followed by an analysis of how FLORAL shifts the most influential training points for RoBERTa's predictions on the IMDB dataset. In Appendix E.3, we provide experiments on the MNIST (Deng, 2012) dataset. Finally, we present a sensitivity analysis with respect to the attacker's budget in Appendix E.4."}, {"title": "E.1 MOON", "content": "We report the clean and robust test accuracy of methods under different (non-optimal) kernel hyperparameter choices and considering label poisoning levels \\{5, 10, 25\\}(%) in Figure 7 and Table 4.\nWhen the kernel hyperparameters are not optimally chosen, NN-PGD shows superior performance in less adversarial scenarios compared to SVM-based methods. However, it also demonstrates significant sensitivity to label attacks in 25% adversarial settings, against all other baselines. FLORAL particularly advances by maintaining a higher robust accuracy in more adversarial settings."}, {"title": "E.2 IMDB", "content": "We report the test accuracy and loss performance of FLORAL against RoBERTa on the IMDB dataset in Figure 8 and Table 2. As demonstrated, FLORAL consistently exhibits superior accuracy and a smaller loss in more adversarial problem instances, without sacrificing the clean performance. This shows the effectiveness of FLORAL in achieving robust classifiers when integrated with foundation models such as RoBERTa."}, {"title": "E.4 SENSITIVITY ANALYSIS", "content": "In our experiments with the Moon dataset under varying label poisoning levels, we consider attacker budgets B = 2k under varying k values, and report the best performing setting in Figure 3 in Section 4.1.\nHowever, we further investigate the sensitivity of FLORAL to the attacker's budget B, by considering levels B \\(\\in\\) \\{5, 10, 25, 50, 125\\}, with results presented in Figure 12. As demonstrated, FLORAL shows superior performance under a constrained attacker budget in the clean label scenario, as expected, since an increasing number of adversarially labelled examples during training degrades clean test accuracy. In contrast, baseline methods operate on a fixed dataset. However, as the dataset gets more adversarial, FLORAL outperforms under higher attacker budgets."}, {"title": "F EXTENSION TO MULTI-CLASS CLASSIFICATION", "content": "We extend our algorithm to multi-class classification tasks, as detailed in Algorithm 3. The primary modification involves adopting a one-vs-all approach (Hsu & Lin, 2002) by employing kernel SVM model \\(f_m\\) for each class \\(m \\in M\\) and associating multiple attackers \\(a_m, m \\in M\\) for the corresponding classifiers. In each round t, the attackers identify the \\(B_m\\) most influential data points with respect to \\(\\Lambda_t\\) values of the corresponding models under their constrained budgets \\(B_m\\), and gather them into a set \\(B_t\\). Among the points in \\(B_t\\), the labels of top-k influential data points are poisoned according to a predefined label poisoning distribution q. The dataset with adversarial labels is then shared with each kernel SVM model and local training is applied via PGD training step."}, {"title": "G COMPARISON AGAINST ADDITIONAL METHODS", "content": "We additionally compare FLORAL against least squares classifier using randomized smoothing (RS) (Rosenfeld et al., 2020), and regularized synthetic reduced nearest neighbor (RSRNN) (Tavallali et al., 2022) methods on the Moon and MNIST-1vs7 datasets. RS provides a robustness certification for a linear classifier under label-flipping attacks. Whereas, RSRNN, conceptually similar to Curie (Laishram & Phoha, 2016), provides a filtering-out defense based on clustering."}, {"title": "H EXPERIMENTS UNDER DIFFERENT LABEL ATTACKS", "content": "To show the generalizability of our approach in the presence of otherlabel poisoning attack types, we further compare FLORAL against baselines on adversarial datasets generated using the alfa, alfa-tilt (Xiao et al., 2015) and LFA attacks (Paudice et al., 2018)."}, {"title": "H.1 EXPERIMENTS WITH THE ALFA-TILT ATTACK", "content": "We further evaluate FLORAL's performance in the presence of alfa-tilt attack (Xiao et al., 2015) on the Moon and MNIST-1vs7 datasets. We report the results on the Moon datasets in Table 11, whereas we present the results for MNIST-1vs7 dataset in Figure 13 and Table 12.\nAs shown with the results on the Moon dataset, FLORAL is able to achieve a higher \"Best\" robust accuracy level throughout the training process. Furthermore, FLORAL's effectiveness under alfa-tilt attack is best shown on the MNIST dataset. As reported in Figure 13 and Table 12, FLORAL achieves an outperforming robust accuracy level compared to baseline methods on all adversarial settings. This demonstrates the potential of FLORAL defense against other label poisoning attacks."}, {"title": "H.2 EXPERIMENTS WITH THE ALFA ATTACK", "content": "The alfa attack is generated under the assumption that the attacker can maliciously alter the training labels to maximize the empirical loss of the original classifier on the tainted dataset"}]}