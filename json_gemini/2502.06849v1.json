{"title": "Model Fusion via Neuron Transplantation", "authors": ["Muhammed \u00d6z", "Nicholas Kiefer", "Charlotte Debus", "Jasmin H\u00f6rter", "Achim Streit", "Markus G\u00f6tz"], "abstract": "Ensemble learning is a widespread technique to improve the prediction performance of neural networks. However, it comes at the price of increased memory and inference time. In this work we propose a novel model fusion technique called Neuron Transplantation (NT) in which we fuse an ensemble of models by transplanting important neurons from all ensemble members into the vacant space obtained by pruning insignificant neurons. An initial loss in performance post-transplantation can be quickly recovered via fine-tuning, consistently outperforming individual ensemble members of the same model capacity and architecture. Furthermore, NT enables all the ensemble members to be jointly pruned and jointly trained in a combined model. Comparing it to alignment-based averaging (like Optimal-Transport-fusion), it requires less fine-tuning than the corresponding OT-fused model, the fusion itself is faster and requires less memory, while the resulting model performance is comparable or better. The code is available under the following link:\nhttps://github.com/masterbaer/neuron-transplantation .", "sections": [{"title": "1 Introduction", "content": "Nowadays, it is not uncommon to have neural networks with billions of parameters [5]. The inference time and memory demand for large pretrained models is costly, even more so when the model is only one of many base learners in an ensemble. Ensemble learning techniques are known to increase generalization [23], but require even more memory and computational resources compared to a single model. This makes the deployment of deep ensembles difficult, if not even unattainable [39]. Knowledge distillation [13] is one solution to compress ensembles but it requires the ensemble members in the fine-tuning process. An alternative to this is weight-based model fusion such as weight averaging and"}, {"title": "2 Related Work", "content": "The field of model fusion has been categorized by Li et al. [19] into four categories, of which \"weight averaging\", and \"alignment\" is of relevance to our work.\nWeight Averaging. Weight averaging of trained models [37] is the process of combining models in a defined space. The outputs of the individual models are weighted and summed. Weight averaging has the problem of a loss barrier which increases the loss for fused models through averaging. For large pre-trained models, it has been shown that they lie in a single low error basin [25, 38], rendering vanilla averaging feasible. Another such scenario is the case of merging similar models [19]. In such fusion settings [4,17], no loss-barrier seems to be present making average-based methods viable. Unlike the approaches detailed here, we do not interpolate between different models, but instead transplant all of them into the fused model to create a new initialization that is able to reach near-ensemble performance with the memory and speed of a single member.\nAlignment. Entezari et al. conjecture that models can be permuted in such a way that there is no loss barrier on the linear interpolation between them [9]. This solves the problem of the loss barrier in weight averaging, but the search space to probe all possible permutations in a reasonable time is too large. To solve this, Ainsworth et al. propose activation matching, weight matching and using a straight-through estimator to find good permutations [1]. Singh and Jaggi use Optimal Transport (OT) to align the neurons (layerwise, in a greedy way) minimizing the total transportation cost between them [33]. In both works, a certain loss barrier remains when the actual averaging is done, especially when layers with small widths are fused. In contrast to these methods, we do not try to \"align and average\" neurons, but to \"select\" the most relevant ones from the ensemble members, hence avoiding the averaging-induced loss.\nDistillation. Knowledge distillation [13] is a compression method that can be used as a fusion method [2,31,36]. It is orthogonal to averaging or transplanting and can be used alongside other fusion methods such as OT [33] or ours. In knowledge distillation, knowledge from the teacher is distilled into the student by having access to the logits of the teacher model as soft targets, in addition to the original labels as hard targets. Sun et al. use knowledge distillation on a diverse ensemble as a synchronization method for SGD [34]. After each fusion, workers train independently until the next synchronization step. To ensure diversity of the ensemble members, they add a similarity term to the loss function. We use distillation to further study the behaviour of our method in experiments.\nPruning. Pruning is the method of removing elements from a model, the smallest unit being a single weight, which can be set to zero to decrease model complexity while retaining predictive performance. The field of pruning is large [22]. Structured pruning considers compositions of elements to be pruned concurrently, for linear layers these might be neurons, their equivalent for CNNs might be"}, {"title": "3 Neuron Transplantation", "content": "Our proposed Neuron Transplantation method (see Figure 2) is an ensemble fusion method that merges important neurons in a layer-wise fashion. We do not try to \"align and average\" neurons, but to \"select\" the most relevant ones from each ensemble member. Consider an ensemble which consists of k members with equal architecture and equal size of all layers and parameters. NT is the composition of the following steps:\n1. Initialize all models with different random seeds, and train individually using the full data set.\n2. Concatenate the non-output layers vertically, and average the classification layers.\n3. Prune all non-output layers via structured magnitude pruning to a sparsity of $1 - \\frac{1}{k}$ to obtain the original architecture of a single model.\n4. Using the new initialization obtained from the steps above, fine-tune the resulting model on the full data set to compensate for the lost smaller magnitude neurons.\nConcatenation. Step two of our method is done in a way that the resulting model is equivalent to output averaging. Layers connected to the input stay connected to the input, the output layer is an average of all individual output layers and all others are concatenated. All weights between the layers, which we call cross"}, {"title": "3.1 Analysis of NT", "content": "Memory and Time Requirements. Let the ensemble consist of k equal sized models. For parallel training and joint fusion, all individual models need to be stored in memory. Storing the L2-norms for each neuron is the only additional memory needed.\nFor time constraints, one only needs to consider the upper bound of sorting the computed L2-norms of each neuron. For a layer with k * N neurons and M inputs each, computing the L2-norms is of order O(kNM) and sorting them an additional O(kNlog(kN)). This is negligible compared to training and fine-tuning, especially if k is small. For very large k, it may be beneficial to use a reduction scheme for the fusion.\nReduction Scheme for Multiple Models. To speed up the fusion process for multi-ple models and to limit the required memory, we introduce an iterative and a re-cursive version of NT, which we call NT-iterative and NT-recursive respectively. The iterative version iterates through the models and merges the current model with the next one until no model is left, i.e., similar to exponential weighted averaging. We expect a smaller accuracy drop post-fusion as a lot of weight is given to the last model of the iteration. The recursive variant recursively merges each half of the available models and then combines both. Both versions have the advantage that they only have to merge two models simultaneously while the latter is better parallelizable.\nFusing Duplicate Models. A fundamental limitation of transplanting neurons is the case of fusing a model with itself, or a highly similar one, as the gained"}, {"title": "4 Experiments", "content": "In this section we conduct experiments on NT as a fusion method.In section 4.2 we analyze how Neuron Transplantation is performed in an effective way and how it fares in different settings. We deliberately prioritize demonstrating the feasibility and efficacy of our method over achieving the highest accuracy with the analyzed models on the chosen datasets. By focusing on validating the underlying principle of transplantation, we aim to provide a simple drop-in replacement for other more costly fusion methods. We therefore compare our method to state-of-the-art fusion techniques in section 4.3."}, {"title": "4.1 Experimental settings", "content": "We first study NT's properties through ablation experiments and afterwards compare its performance against related model fusion approaches. For the ab-lation studies, if not stated otherwise, we use an ensemble of two small neural networks with three hidden layers of width 512, referred to as \"MLP\", and train it on the SVHN real-world image dataset [24] with the following hyperparame-ters: batch size of 256, 100 epochs, SGD-optimizer with a momentum of 0.9 and a constant learning rate of 0.01 and the cross entropy loss as the loss function.\nFor the comparison to other methods, we use the models LeNet [16], VGG11 [32] and Resnet18 [12] and the additional image datasets MNIST [7], CIFAR10 and CIFAR100 [15]. LeNet and MNIST act as an easy classification task whereas CI-FAR10/CIFAR100 with VGG11 or Resnet18 are slightly more challenging and allow a closer comparison to Optimal-Transport-fusion [33] as they used a sim-ilar setting. Since OT-fusion does not support biases and batch normalization, we remove them for these experiments. We make the following alterations to the hyperparameters to get a more accurate comparison to OT-fusion: we train the MLP and LeNet for 60 epochs (as the training process already converges) and VGG11/Resnet18 for 300 epochs. For VGG11 and Resnet18 we use a batch size of 128 and a learning rate of 0.05 with a decay of 0.5 every 30 epochs.\nHardware and Software Environment. The experiments were conducted on a GPU4 node using 12 Intel Xeon Platinum 8368 CPUs and an NVIDIA A100-40 GPU with 40 GB memory. We trained our models using PyTorch [27], a popular deep learning framework, and utilized CUDA [26], NVIDIA's parallel computing platform, to leverage GPU acceleration for faster training. For our NT-fusion we utilize the pruning framework torch-pruning [10] and for OT-fusion the Optimal-Transport solver POT [11]."}, {"title": "4.2 Main Properties and Ablation Studies", "content": "Figure 4 shows the key property of NT: At the cost of some current accuracy the potential to reach greater performance via fine-tuning is brought. The initial performance drop is a result of the pruning process. The loss of small-magnitude neurons is overcompensated by newly transplanted ones after fine-tuning.\nOrder of Operations. We consider the order of pruning, merging and potentially fine-tuning. We differentiate between three cases:\n1. Prune-merge-fine-tune: The ensemble members are first pruned individually and then merged. This requires the least amount of memory and local prun-ing can be done in parallel.\n2. Merge-prune-fine-tune: The models are first merged and then jointly pruned. This has the advantage of considering all neurons in the pruning process.\n3. Merge-fine-tune-prune-fine-tune: The same as above with additional fine-tuning after merging, exploiting the potential of training the merged ensem-ble.\nTable 1 shows that the additional fine-tuning step of option three does not pay off and that the second option, i.e., merging, then pruning, slightly outper-forms the first one at early epochs. We conclude that jointly selecting the neurons with the largest L2-norm across all models is better than doing so locally, though the improvement is marginal at later epochs. We deliberately left out a possible"}, {"title": "5 Discussion and Conclusion", "content": "The inference time and memory consumption of large pretrained ensembles can be large making it difficult to adapt them to downstream tasks with limited hardware. Distillation does not solve this issue as all ensemble members are present as the ensemble teacher during fine-tuning and recent alignment-based model fusion methods suffer from large memory requirements or search times.\nIn this work, we present a novel model fusion method called Neuron Trans-plantation, that is able to fuse ensembles to a single model's size through com-putationally cheap pruning and little fine-tuning, retaining most of the ensemble performance. NT jointly prunes all models by removing neurons with the small-est L2-norms and concatenates the remaining ones, setting up the fused model to retrain to a higher accuracy than any individual model. We theorize that only the large weights of the models are needed to set the fused model into a \"good\" loss neighborhood while the loss of the small weights can be compensated with further fine-tuning.\nCompared to vanilla averaging, NT does not suffer from severe loss-barriers, and compared to OT, no costly permutation matrices need to be computed, enabling NT to fuse models with large widths. It can be used in combination with distillation to give a slight but cheap performance boost. Though our approach suffers from a saturation when fusing too many models (in our experiments >8) and from redundant neurons when the models are too similar, NT can be applied to various architectures like MLPs and CNNs. It's application to transformers [35] and other use cases like federated learning or parallel SGD we leave for future work.\nWe advocate NT as model fusion technique. Though its application as a pure ensemble-pruning technique also seems apparent, it is - by design as a canonical way of applying pruning to ensembles for model fusion - almost equivalent to pruning each ensemble member individually. Whether the joint pruning and joint training of NT can overcompensate for the newly introduced cross-weights remains to be seen. We also leave that for future work.\nIt might also be possible to heuristically decide whether the neurons are diverse enough for Neuron Transplantation to succeed. Such an approach could solve the information loss issue of using NT on really similar models."}]}