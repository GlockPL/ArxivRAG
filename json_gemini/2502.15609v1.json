{"title": "On the Robustness of Transformers against Context Hijacking for Linear Classification", "authors": ["Tianle Li", "Chenyang Zhang", "Xingwu Chen", "Yuan Cao", "Difan Zou"], "abstract": "Transformer-based Large Language Models (LLMs) have demonstrated powerful in-context learning capabilities. However, their predictions can be disrupted by factually correct context, a phenomenon known as context hijacking, revealing a significant robustness issue. To understand this phenomenon theoretically, we explore an in-context linear classification problem based on recent advances in linear transformers. In our setup, context tokens are designed as factually correct query-answer pairs, where the queries are similar to the final query but have opposite labels. Then, we develop a general theoretical analysis on the robustness of the linear transformers, which is formulated as a function of the model depth, training context lengths, and number of hijacking context tokens. A key finding is that a well-trained deeper transformer can achieve higher robustness, which aligns with empirical observations. We show that this improvement arises because deeper layers enable more fine-grained optimization steps, effectively mitigating interference from context hijacking. This is also well supported by our numerical experiments. Our findings provide theoretical insights into the benefits of deeper architectures and contribute to enhancing the understanding of transformer architectures.", "sections": [{"title": "1. Introduction", "content": "Transformers (Vaswani et al., 2017) have demonstrated remarkable capabilities in various fields of deep learning, such as natural language processing (Radford et al., 2019; Achiam et al., 2023; Vig & Belinkov, 2019; Touvron et al., 2023; Ouyang et al., 2022; Devlin, 2018). A common view of the superior performance of transformers lies in its remarkable in-context learning ability (Brown, 2020; Chen et al., 2022; Liu et al., 2023a), that is, transformers can flexibly adjust predictions based on additional data given in context contained in the input sequence itself, without updating parameters. This impressive ability has triggered a series of theoretical studies attempting to understand the in-context learning mechanism of transformers (Olsson et al., 2022; Garg et al., 2022; Xie et al., 2021; Guo et al., 2023; Wu et al., 2023). These studies suggest that transformers can behave as meta learners (Chen et al., 2022), implementing certain meta algorithms (such as gradient descent (Von Oswald et al., 2023; Ahn et al., 2023; Zhang et al., 2024b) based on context examples, and then applying these algorithms to the queried input.\nDespite the benefits of in-context learning abilities in transformers, this feature can also lead to certain negative impacts. Specifically, while well-designed in-context prompts can help generate desired responses, they can also mislead the transformer into producing incorrect or even harmful outputs, raising significant concerns about the robustness of transformers (Chowdhury et al., 2024; Liu et al., 2023c; Zhao et al., 2024). For instance, a significant body of work focuses on jailbreaking attacks (Chao et al., 2023; Niu et al., 2024; Shen et al., 2024; Deng et al., 2023; Yu et al., 2023), which aim to design specific context prompts that can bypass the defense mechanisms of large language models (LLMs) to produce answers to dangerous or harmful questions (e.g., \"how to build a bomb?\"). It has been demonstrated that, as long as the context prompt is sufficiently long and flexible to be adjusted, almost all LLMs can be successfully attacked (Anil et al., 2024). These studies can be categorized under adversarial robustness, where an attacker is allowed to perturb the contextual inputs arbitrarily to induce the transformer model to generate targeted erroneous outputs (Shi et al., 2023; Pandia & Ettinger, 2021; Creswell et al., 2022; Yoran et al., 2023).\nHowever, in addition to the adversarial attack that may use harmful or incorrect context examples, it has been shown that the predictions of LLMs can also be disrupted by harmless and factually correct context. Such a phenomenon is referred to as context hijacking (Jiang et al., 2024; Jeong, 2023), which is primarily discovered"}, {"title": "2. Related works", "content": "In-context learning via transformers. The powerful performance of transformers is generally believed to come from its in-context learning ability (Brown, 2020; Chen et al., 2022; Min et al., 2022; Liu et al., 2023a; Xie et al., 2021). A line of recent works study the phenomenon of in-context learning from both theoretical (Bai et al., 2024; Guo et al., 2023; Lin et al., 2023; Chen et al., 2024; Frei & Vardi, 2024; Huang et al., 2023; Siyu et al., 2024; Li et al., 2024) and empirical (Garg et al., 2022; Aky\u00fcrek et al., 2022a; Li et al., 2023a; Ravent\u00f3s et al., 2024; Pathak et al., 2023; Panwar et al., 2023; Bhattamishra et al., 2023; Fu et al., 2023; Lee et al., 2024) perspectives on diverse settings. Brown (2020) first showed that GPT-3 can perform in-context learning. Chen et al. (2024) studied the role of different heads within transformers in performing in-context learning focusing on the sparse linear regression setting. Frei & Vardi (2024) studied the ability of one-layer linear transformers to perform in-context learning for linear classification tasks.\nMechanism interpretability of transformers. Among the various theoretical interpretations of transformers (Friedman et al., 2024; Yun et al., 2020; Dehghani et al., 2019; Lindner et al., 2024; Pandit & Hou, 2021; P\u00e9rez et al., 2021; Bills et al., 2023; Wei et al., 2022; Weiss et al., 2021; Zhou et al., 2023; Chen & Zou, 2024), one of the most widely studied theories is the ability of transformers to implement optimization algorithms such as gradient descent (Von Oswald et al., 2023; Ahn et al., 2023; Zhang et al., 2024b; Bai et al., 2024; Wu et al., 2023; Cheng et al., 2023; Aky\u00fcrek et al., 2022b; Dai et al., 2023; Zhang et al., 2024a). Von Oswald et al. (2023) theoretically and empirically proved that transformers can learn in-context by implementing a single step of gradient descent per layer. Ahn et al. (2023) theoretically\nanalyzed that transformers can learn to implement preconditioned gradient descent for in-context learning. Zhang et al. (2024b) considered ICL in the setting of linear regression with a non-zero mean Gaussian prior, a more general and common scenario where different tasks share a signal, which is highly relevant to our work.\nRobustness of transformers. The security issues of large language models have always attracted a great deal of attention (Yao et al., 2024; Liu et al., 2023b; Perez & Ribeiro, 2022; Zou et al., 2023; Apruzzese et al., 2023). However, most of the research focuses on jail-breaking black-box models (Chowdhury et al., 2024), such as context-based adversarial attacks (Kumar et al., 2023; Wei et al., 2023; Xu et al., 2023; Wang et al., 2023a; Zhu et al., 2023; Cheng et al., 2024; Wang et al., 2023b). There is very little white-box interpretation work of attacks on the transformer, the foundation model of LLMs (Qiang et al., 2023; Bailey et al., 2023; He et al., 2024; Anwar et al., 2024; Jiang et al., 2024). Qiang et al. (2023) first considered attacking large language models during in-context learning, but they did not study the role of transformers in robustness. Jiang et al. (2024) proposed the phenomenon of context hijacking, which became the key motivation of our work. They analyzed this problem from the perspective of associative memory models instead of the in-context learning ability of transformers."}, {"title": "3. Preliminaries", "content": "In this section, we will provide a detailed introduction to our setup of the context hijacking problem, including the data model, transformer architecture, and evaluation metric."}, {"title": "3.1. Data model", "content": "To understand the mechanism of context hijacking phenomenon, we model it as a binary classification task, where the query-answer pair is modeled as the input-response pair $((x, y) \\in \\mathbb{R}^d \\times \\{\\pm 1\\})$. In particular, we present the definition of the data model as follows:\nDefinition 3.1 (Data distribution). Let $w^* \\in \\mathbb{R}^d$ be a vector drawn from a prior distribution on the $d$ dimensional unit sphere $\\mathbb{S}^{d-1}$, denoted by $p_{\\beta^*}(\\cdot)$, where $\\beta^* \\in \\mathbb{S}^{d-1}$ denotes the expected direction of $w^*$. Then given the generated $w^*$, the data pair $(x, y)$ is generated as follows: the feature vector is $x \\sim \\mathcal{N}(0_d, I_d)$ and the corresponding label is $y = \\text{sign}(\\langle w^*, x \\rangle)$.\nBased on the data distribution of each instance, we then introduce the detailed setup of the in-context learning task in our work. In particular, we consider the setting that the transformer is trained on the data with clean context examples and evaluated on the data with hijacked context.\nTraining phase. During the training phase, we are given"}, {"title": "3.2. Transformer model", "content": "Following the extensive prior theoretical works for transformer (Zhang et al., 2024a;b; Chen et al., 2024; Frei & Vardi, 2024; Ahn et al., 2023), we consider linear attention-only transformers, a prevalent simplified structure to investigate the behavior of transformer models. In particular, We define an $L$-layer linear transformer $TF$ as a stack of $L$ single-head linear attention-only layers. For the input matrix $Z_{i-1} \\in \\mathbb{R}^{(d+1)\\times(n+1)}$, the $i$-th single-head linear attention-only layer $TF_i$ updates the input as follows:\n$Z_i = TF_i(Z_{i-1}) = Z_{i-1} + P_i Z_i M (Z_{i-1}^T Q_i Z_{i-1}),$ (3.3)\nwhere $M := \\begin{pmatrix} I_n & \\\\ & 0 \\end{pmatrix} \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ is the mask matrix. We design this architecture to constrain the model's focus to the first $n$ in-context examples. Moreover, the matrix $P := W_v W_q^T \\in \\mathbb{R}^{(d+1)\\times(d+1)}$ serves as the value matrix in the standard self-attention layer, while the matrix $Q := W_q W_q^T \\in \\mathbb{R}^{(d+1)\\times(d+1)}$ consolidates the key matrix and query matrix. This mild re-parameterization has been widely considered in numerous recent theoretical works (Huang et al., 2023; Wang et al., 2024; Tian et al., 2023; Jelassi et al., 2022). To adapt the transformer for solving the linear classification problem, we introduce an additional linear embedding layer $W_E \\in \\mathbb{R}^{(d+1)\\times(d+1)}$. Then the output of the transformer TF is defined as\n$\\hat{y}_{query} = TF(Z_0; W_E, \\{P_i, Q_i\\}_{i=1}^L)$"}, {"title": "3.3. Evaluation metrics", "content": "Based on the illustration regarding the transformer architecture, we first define the in-context learning risk of a $L$-layer model TF in the training phase. In particular, let $D_{tr}$ be the distribution of the input data matrix $Z$ in (3.1) and the target $y_{query}$, which covers the randomness of both $(x, y)$ and $w^*$, then the risk function in the training phase is defined as:\n$\\mathcal{R}(TF) := \\mathbb{E}_{Z, y_{query} \\sim D_{tr}}[(TF(Z; \\theta) - y_{query})^2]$. (3.5)\nwhere $\\theta = \\{W_E, \\{P_i, Q_i\\}_{i=1}^L\\}$ denotes the collection of all trainable parameters of TF. This risk function will be leveraged for training the transformer models (where we use the stochastic gradient in the experiments).\nAdditionally, in the test phase, let $D_{te}$ be the distribution of the input data matrix $Z_{hc}$ in (3.2) and the target $y_{query}$, we consider the following population prediction error:\n$\\mathcal{E}(TF) := \\mathbb{P}_{Z_{hc}, y_{query} \\sim D_{te}} [TF(Z_{hc}; \\theta) \\cdot y_{query} < 0]$. (3.6)"}, {"title": "4. Main theory", "content": "In this section, we present how we establish our theoretical analysis framework regarding the robustness of transformers against context hijacking. In summary, we can briefly sketch our framework into the following several steps:\n*   Step 1. We establish the equivalence between the $L$-layer transformers and $L$ steps gradient descent, converting the original problem of identifying well-trained transformers to the problem of finding the optimal parameters of gradient descent (i.e., initialization and learning rates).\n*   Step 2. We derive the optimal learning rates and initialization of gradient descent, revealing its relationship with the number of layers $L$ and training context length $n$.\n*   Step 3. By formulating the classification error of a linear model obtained by $L$ steps gradient descent with optimal parameters on hijacking distribution $D_{te}$, we characterize how the number of layers $L$, the training context length $n$ and test context length $N$ affect the robustness."}, {"title": "4.1. Optimizing over in-context examples", "content": "Inspired by a line of recent works (Zhang et al., 2024b; Bai et al., 2024; Chen et al., 2024; Ahn et al., 2023; Olsson et al., 2022) which connects the in-context learning of transformer with the gradient descent algorithm, we follow a similar approach by showing that, in the following proposition, multi-layer transformer can implement multi-step gradient descent, starting from any initialization, on the context examples.\nProposition 4.1. For any $L$-layer single-head linear transformer, let $y_{query}$ be the output of the $l$-th layer of the transformer, i.e. the $(d + 1, n + 1)$-th entry of $Z_l$. Then, there exists a single-head linear transformer with $L$ layers such that $y_{query} = -\\langle w_{gd}^{(L)}, x_{query} \\rangle$. Here, $w_{gd}^{(L)}$'s are the parameter vectors obtained by the following gradient descent iterative rule and the initialization $w_{gd}^{(0)}$ can be arbitrary:\n$\\begin{aligned} w_{gd}^{(l+1)} &= w_{gd}^{(l)} - \\Gamma_l \\nabla \\mathcal{L}(w_{gd}^{(l)}), \\\\ \\mathcal{L}(w) &= \\frac{1}{2} \\sum_{i=1}^n (\\langle w, x_i \\rangle - y_i)^2. \\end{aligned}$ (4.1)\nHere $\\Gamma_l$ can be any $d \\times d$ matrix.\nAs $\\Gamma_l$ could be any $d \\times d$ matrix, Proposition 4.1 demonstrates that the output of the $L$-layer transformers is equivalent to that of a linear model trained via $L$-steps of full-batch preconditioned gradient descent on the context examples, with $\\{\\Gamma_l\\}_{l=0}^{L-1}$ being the learning rates. This suggests that each $L$-layer transformer defined in (3.4), with different parameters, can be viewed as an optimization process of a linear model characterized by a distinct set of initialization and learning rates $\\{w_{gd}^{(0)}, \\Gamma_0, ..., \\Gamma_{L-1}\\}$. Therefore, it suffice to directly find the optimal parameters of the gradient descent process, without needing to infer the specific parameters of the well-trained transformers.\nAmong all related works presenting similar conclusions that transformers can implement gradient descent, our result is general as we prove that transformers can implement multi-step gradient descent from any initialization. In comparison, for example, Zhang et al. (2024b) shows that a single-layer transformer with MLP can implement one-step gradient descent from non-zero initialization. Ahn et al. (2023) demonstrate that linear transformers can implement gradient descent, but only from 0 initialization."}, {"title": "4.2. Optimal multi-step gradient descent", "content": "Based on the discussion in the previous section, Proposition 4.1 successfully transforms the original problem of identifying the parameters of well-trained transformers into the task of finding the optimal learning rates and initialization for the gradient descent process (4.1). In this section, we present our conclusions regarding these optimal parameters. As we consider optimizing over the general training distribution $D_{tr}$, where the tokens $x_i$'s follow the isotropic distribution, it follows that the updating step size should be"}, {"title": "4.3. Robustness against context hijacking", "content": "The previous two subsections illustrate that for any input with context examples, we can obtain the corresponding prediction for that input from the well-trained transformers by applying gradient descent with the optimal parameters we derived in Theorem 4.3. As we model $D_{te}$ the distribution of hijacking examples, to examine the robustness of $L$-layer transformers against hijacking, we only need to check whether the linear model achieved by $L$-step gradient on $(x_{hc}, y_{hc})$ can still conduct successful classification on $x_{query}$. Specifically, we consider the classification error of the parameter vector $w_{gd}^{(L)}$ as,\n$\\mathcal{E}(w_{gd}^{(L)}) := \\mathbb{P}_{T, w^* \\sim D_{te}} (y_{query} \\langle w_{gd}^{(L)}, x_{query} \\rangle < 0),$ where $T = \\{(x_{hc}, y_{hc}), (x_{query}, y_{query})\\}$, and $w_{gd}^{(L)}$ is obtained by implementing gradient descent on $(x_{hc}, y_{hc})$ with $L$ steps and the optimal $\\alpha$ and $w_{gd}^{(0)}$. Similar to the previous result, $\\mathcal{E}(w_{gd}^{(L)})$ is identical to $\\mathcal{E}(TF)$ defined in (3.6). Based on these preliminaries, we are ready to present our results regarding the robustness against context hijacking. We first introduce the following lemma illustrating that when the context length of hijacking examples is small, we hardly observe the label flipping phenomenons of the prediction from well-trained transformers.\nLemma 4.4. Assume that all assumptions in Theorem 4.3 still hold. Additionally, assume that the length of hijacking examples $N$ is small such that $N < O(\\frac{d^{3/2}}{n})$ and $\\sigma$ follows any continuous distribution. Based on these assumptions, it holds that\n$\\mathcal{E}(w_{gd}^{(L)}) \\leq \\mathcal{E}(w_{gd}^{(0)}) + o(1).$\nLemma 4.4 demonstrates that when the context length of hijacking examples is small, the classification error of the"}, {"title": "5. Experiments", "content": "In this section, we conduct experiments based on the setting in Section 3 to verify the theory we proposed in Section 4. We first verify the consistency of our theory with optimal multi-step gradient descent. Then we train a series of linear transformers to examine their robustness on test data."}, {"title": "5.1. Optimal gradient descent with different steps", "content": "In our theoretical framework, for the same optimization objective, the optimal gradient descent with more steps $L$ or longer training context length $n$ will have a smaller learning rate per step (Theorem 4.3), and this combination of more steps with small learning rates will perform better on the optimization process over context samples (Theorem 4.5). Our theory shows that a trained transformer will learn the optimal multi-step gradient descent, which will make it more robust during testing. Therefore, we directly verified the consistency between practice and theory in the multi-step gradient descent experiment.\nWe construct a single-layer neural network to conduct optimal multi-step gradient descent experiments. Each training sample $(x_i, y_i)$ is drawn i.i.d. from the distribution $D_{tr}$ defined in Section 3.1. We consider the learning rate that minimizes the loss of the test sample which is also drawn from $D_{tr}$ when the single-layer neural network is trained using 1 to 8 steps of gradient descent, that is, the optimal learning rate $\\alpha_L$ corresponding to $L$-step gradient descent, which can be obtained by grid search. shows that $\\alpha_L$ decreases as $L$ and $n$ increases, which is aligned with our theoretical results (Theorem 4.3).\nNext, we discuss the second part of the theoretical framework, i.e., gradient descent with more steps and small step size performs a more fine-grained optimization (Theorem 4.5), which can be verified by our experiment results. We apply the optimal learning rate searched in the training phase to the test phase, and perform gradient descent optimization on the test samples drawn from $D_{te}$ with the optimal"}, {"title": "5.2. Robustness of linear transformers with different number of layers", "content": "Applying our theoretical framework to the context hijacking task on transformers can explain it well, indicating that our theory has practical significance. We train linear transformers with different depths and context lengths on the training dataset based on distribution $D_{tr}$. We mainly investigate the impact of training context length $n$, and model depth $L$ and the testing context length $N$ on model classification accuracy.\nWe first test the trained transformers on the training dataset to verify that the model can fine-tune the memorized $\\beta^*$ to $w^*$. According to the  4, we can find that the model has a high classification accuracy when there are very few samples at the beginning. This means that the model successfully memorizes the shared signal $\\beta^*$. As the context length increases, the accuracy of the model gradually increases and converges, meaning that the model can fine-tune the pre-trained $\\beta^*$ by using the context samples. In addition, deeper models can converge to larger values faster, corresponding to the theoretical view that deeper models can perform more sophisticated optimization.\nThen we conduct experiments on the test set. Observing the experiment results ( 3), we can see that as the context length increases, the accuracy of the model decreases significantly and converges to 50%, showing that the model is randomly classifying the final query $x_{query}$. This is consistent with the context hijacking phenomenon that the model's robustness will deteriorate as the number of interference prompt words increases. When the number of layers increases, the models with different depths show the same trend as the context length increases, but the accuracy of the model will increase significantly, which is consistent with the phenomenon that deeper models show stronger robustness in practical applications. In addition, the model becomes significantly more robust as the training context length increases, which is reflected in the fact that the classification accuracy converges more slowly as the length increases."}, {"title": "6. Conclusion and discussion", "content": "In this paper, we explore the robustness of transformers from the perspective of in-context learning. We are inspired by the real-world problem of LLMs, namely context hijacking (Jiang et al., 2024), and we build a comprehensive theoretical framework by modeling context hijacking phenomenon as a linear classification problem. We first demonstrate the context hijacking phenomenon by conducting experiments on LLMs with different depths, i.e., the output of the LLMs can be simply manipulated by modifying the context with factually correct information. This reflects an intuition: deeper models may be more robust. Then we develop a comprehensive theoretical analysis of the robustness of transformer, showing that the well-trained transformers can achieve the optimal gradient descent strategy. More specifically, we show that as the number of model layers or the length of training context increase, the model will be able to perform more fine-grained optimization steps over context samples, which can be less affected by the hijacking examples, leading to stronger robustness. Specifically considering the context hijacking task, our theory can fully explain the various phenomena, which is supported by a series of numerical experiments.\nOur work provides a new perspective for the robustness explanation of transformers and the understanding of in-context learning ability, which offer new insights to understand the benefit of deeper architecture. Besides, our analysis on the optimal multi-step gradient descent may also be leveraged to other problems that involve the numerical optimization for linear problems."}, {"title": "A. Proof of Proposition 4.1", "content": "In this section we provide a proof for Proposition 4.1.\nProof of Proposition 4.1. Our proof is inspired by Lemma 1 in Ahn et al. (2023), while we consider a non-zero initialization. We first provide the parameters $W_E$, $P_i$, $Q_i \\in \\mathbb{R}^{(d+1)\\times(d+1)}$ of a $L$-layers transformer.\n$\\begin{aligned} W_E = \\begin{bmatrix} \\frac{1}{\\sqrt{d}} w_{gd}^{(0)} & \\\\ & I_d \\end{bmatrix}, P_i = \\begin{bmatrix} 0 & \\\\ & \\Gamma_i \\end{bmatrix}, Q_i = \\begin{bmatrix} -\\Gamma_i & \\\\ & \\Gamma_i \\end{bmatrix}, \\text{where } \\Gamma_i \\in \\mathbb{R}^{d \\times d}. \\end{aligned}$\nFor the linear classification problem, the input sample $Z_0 \\in \\mathbb{R}^{(d+1)\\times(n+1)}$ consists of $\\{(x_i, y_i)\\}_{i = 1}^n$ and $(x_{query}, y_{query})$ in (3.1), which will first be embedded by $W_E$. Let $X^{(0)} \\in [\\cdot]^{d \\times (n+1)}$ denote the first $d$ rows of $W_E (Z_0)$ and let $Y^{(0)} \\in \\mathbb{R}^{1 \\times (n+1)}$ denote the $(d + 1)$-th row of $W_E(Z_0)$. In subsequent iterative updates in (3.3), the values at the same position will be denoted as $X^{(l)}$ and $Y^{(l)}$, for $l = 1, ..., L$. Similarly, define $\\hat{\\Sigma}^{(l)} \\in \\mathbb{R}^{d \\times n}$ and $\\hat{Y}^{(l)} \\in \\mathbb{R}^{1 \\times n}$ as matrices that exclude the last query sample $(x_{query}, y_{query})$. That is, they only contain the first $n$ columns of the output of the $l$-th layer. Let $x_i^{(l)}$ and $y_i^{(l)}$ be the $i$-th pair of samples output by the $l$-th layer. Define a function $g(x, y, l) : \\mathbb{R}^d \\times \\mathbb{R} \\times \\mathbb{Z} \\rightarrow \\mathbb{R}$: let $x_{query}^{(0)} = \\langle w_{gd}^{(0)}, x \\rangle$, then $g(x, y, l) := y_{query}^{(l)}$. Next, based on the update formula (3.3) and the parameters constructed above, we have:\n$\\begin{aligned} X^{(l+1)} &= X^{(l)} = ... = X^{(0)}, \\\\ Y^{(l+1)} &= Y^{(l)} - Y^{(l)} M (X^{(0)})^T \\Gamma_l X^{(0)}. \\end{aligned}$\nThen for all $i \\in \\{1, ..., n\\}$,\n$y_i^{(l+1)} = y_i^{(l)} - \\sum_{j = 1}^n y_j^{(l)} \\langle \\Gamma_l x_j^{(0)}, x_i^{(0)} \\rangle.$\nSo $y_i^{(l+1)}$ does not depend on $y_{query}^{(l)}$. For query position,\n$\\begin{aligned} y_{query}^{(l+1)} &= y_{query}^{(l)} - \\sum_{j = 1}^n y_j^{(l)} \\langle \\Gamma_l x_j^{(0)}, x_{query}^{(0)} \\rangle. \\end{aligned}$\nThen we obtain $g(x, y, l)$ and $g(x, 0, l)$:\n$\\begin{aligned} g(x, y, l) &= y_{query} - \\sum_{j = 1}^n x_{query}^{(0)} \\langle \\Gamma_l x_j^{(l-1)}, y_j \\rangle - \\\\ &= y_{query}^{(l)} - \\sum_{j = 1}^n y_j^{(l-1)} \\langle \\Gamma_{l-1} x_j^{(0)}, x_{query}^{(l-2)} \\rangle - \\\\\n&...\\\\ &= y_{query}^{(0)} - \\sum_{j = 1}^n \\langle \\Gamma_{0} x_j^{(0)}, x_{query}^{(0)} \\rangle - ... - \\sum_{j = 1}^n \\langle \\Gamma_{l-1} x_j^{(l-1)}, y_j \\rangle \\\\ &= - \\langle w_{gd}^{(0)}, x_{query} \\rangle - \\sum_{j = 1}^n \\langle \\Gamma_{0} x_j^{(0)}, x_{query}^{(0)} \\rangle - ... - \\sum_{j = 1}^n \\langle \\Gamma_{l-1} x_j^{(l-1)}, y_j \\rangle;\\\\ g(x, 0, l) &= y_{query} - \\sum_{j = 1}^n x_{query}^{(0)} \\langle \\Gamma_l x_j^{(l-1)}, y_j \\rangle \\\\ &= y_{query}^{(l)} - \\sum_{j = 1}^n y_j^{(l-1)} \\langle \\Gamma_{l-1} x_j^{(0)}, x_{query}^{(l-2)} \\rangle - \\\\\n&...\\\\ &= y_{query}^{(0)} - \\sum_{j = 1}^n \\langle \\Gamma_{0} x_j^{(0)}, x_{query}^{(0)} \\rangle - ... - \\sum_{j = 1}^n \\langle \\Gamma_{l-1} x_j^{(l-1)}, y_j \\rangle. \\end{aligned}$"}, {"title": "B. Gradient descent updates of parameters", "content": "In this section", "as\n$A_{l,k}": "sum_{(j_1, j_2, ..., j_k) \\in S_{l,k}} \\prod_{\\kappa=1}^k \\alpha_{j_{\\kappa}}$.\nThen we can observe that the permutation of elements of $\\{\\alpha_0, \\alpha_1, ..., \\alpha_{l-1}\\}$ would not change the value of $A_{l,k}$. Then based on these notations, we present mathematical derivation in the following.\nBy some basic gradient calculations, we can re-write the iterative rule of gradient descent (4.1) as\n$\\begin{aligned} w_{gd}^{(l+1)} &= w_{gd}^{(l)} - \\alpha_l \\nabla \\mathcal{L}(w_{gd}^{(l)}) \\\\ &= w_{gd}^{(l)} - \\alpha_l \\sum_{i=1}^n (\\langle w_{gd}^{(l)}, x_i \\rangle - \\text{sign}(w^*, x_i)) \\cdot x_i \\\\ &= (I_d - \\alpha_l \\sum_{i=1}^n (x_i x_i^T)) w_{gd}^{(l)} + \\alpha_l \\sum_{i=1}^n (\\text{sign}(w^*, x_i)) \\cdot x_i. \\end{aligned}$ (B.1)\nBased on this detailed iterative formula, and the definition of $S_{i,k}$ and $A_{l,k}$ above, we present and prove the following lemma, which characterizes the closed-form expression for $w^{(l)}$.\nLemma B.1. For the iterates of gradient descent, i.e. $w$'s with $l \\in \\{0, 1, ..., L-1\\}$, it holds that\n$\\begin{aligned} w_{gd}^{(l)} = \\Big(I_d + \\sum_{k=1}^l A_{l,k} (-\\sum_{i=1}^n x_i x_i^T)^k \\Big) w_{gd}^{(0)} + \\sum_{k=1}^l \\Big( A_{l,k} (-\\sum_{i=1}^n x_i x_i^T)^{k-1} \\Big) (\\sum_{"}]}