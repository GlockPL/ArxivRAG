{"title": "Exploring the Integration of Large Language Models in Industrial Test Maintenance Processes", "authors": ["Ludvig Lemner", "Linnea Wahlgren", "Gregory Gay", "Nasser Mohammadiha", "Jingxiong Liu", "Joakim Wennerberg"], "abstract": "Much of the cost and effort required during the software testing process is invested in performing test maintenance-the addition, removal, or modification of test cases to keep the test suite in sync with the system-under-test or to otherwise improve its quality. Tool support could reduce the cost-and improve the quality-of test maintenance by automating aspects of the process or by providing guidance and support to developers.\nIn this study, we explore the capabilities and applications of large language models (LLMs)-complex machine learning models adapted to textual analysis-to support test maintenance. We conducted a case study at Ericsson AB where we explored the triggers that indicate the need for test maintenance, the actions that LLMs can take, and the considerations that must be made when deploying LLMs in an industrial setting. We also proposed and demonstrated implementations of two multi-agent architectures that can predict which test cases require maintenance following a change to the source code. Collectively, these contributions advance our theoretical and practical understanding of how LLMs can be deployed to benefit industrial test maintenance processes.", "sections": [{"title": "1 Introduction", "content": "Software testing-the application of selected input to a system-under-test and inspection of the resulting behavior-is a crucial component of the development process. However, it is also a notoriously expensive component, said to represent up to half of the total development cost of the system [8]."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Software Testing", "content": "Software testing is an activity performed to ensure that software meets its stated requirements, including functional correctness [50] and non-functional requirements such as performance or usability [20]. During software testing, a test suite-consisting of one or more test cases-is executed against the system-under-test [50]. Each test case performs a series of input actions, records observations of how the software behaved following the application of input, then compares those observations to a set of expectations (known as a test oracle [10]).\nSoftware testing is an important activity. However, it is also notoriously expensive [80], especially due to the significant manual effort involved in test management activities such as test case creation or manual execution of test cases [116]. Automation has a role to play in reducing the cost of testing by, e.g., transforming manual test cases into a form that can be automatically executed or suggesting what test input to apply [57, 110]. In particular, significant research efforts have been invested in automating the generation of test input [9], including recent investigations of the capabilities of LLMs [114, 125]. However, there are still significant concerns about the readability and maintainability of generated tests [40, 85]. In addition, any form of test automation requires significant up-front effort to implement and additional effort to maintain [8, 33].\nTest maintenance is the process of updating the test suite as the source code evolves-including, e.g., adding new test cases to test new functionality, removing test cases that are no longer relevant, and adapting test cases to ensure their continued relevance to changed code [8]. The test suite may also evolve to improve its efficiency, e.g., by removing redundant tests, or to improve its thoroughness, e.g., by covering more diverse input or more paths through the codebase [103]. Test maintenance is understood to be an important part of quality assurance [102], and can account for a significant proportion of testing effort over the lifespan of a project [8, 103]. However, the area has received comparatively little research attention [102]. For example, \"test smells\" are known to negatively impact test maintainability, but are significantly less well-understood than \"code smells\" in the source code [109]."}, {"title": "2.2 Generative Artificial Intelligence and Large-Language Models", "content": "Generative Al refers to machine learning models that produce content in the form of test, images, videos, or music in response to instructions delivered in the form of a prompt [21]. Prompts can include text and other forms of media, e.g., images or source code. Models are trained to infer the semantic meaning of a prompt, and use that meaning to produce an appropriate response.\nCurrent Generative AI models generally are implemented using a transformer-based architecture, which is based on attention mechanisms designed to imitate the cognitive attention (i.e., the ability to focus on select stimuli) seen in humans [112]. Powerful models, known as foundation models, are trained on large and diverse data sets with the intention that they perform up to a minimal level on new tasks [43]. These models can then be adapted to new domains through fine-tuning using additional domain-specific training data [15, 134].\nWe focus in this research on large language models (LLMs), a form of Generative Al trained on massive corpora of text. LLMs are suited for language analysis and transformation tasks such as translation, summarization, and decision support due to their ability to infer semantic meaning from textual input [84]. LLMs generate output by iteratively predicting the next token or word in the generated sequence [129]. Because of their ability to understand and output both natural language and code, LLMs are well-suited for software development tasks including test generation, automated program repair, and code review [39, 45, 93, 114, 128].\nAn LLM and a human may not draw the same meaning from a prompt, which has led to the development of different prompting strategies [132]. The process of developing a prompt or a series of prompts, known as prompt engineering, can be likened to a form of natural language programming intended to steer the generated output of the LLM [92]. Two common prompt engineering strategies include few-shot prompting-where input-output examples are supplied along with the prompt [18]-and chain-of-thought prompting-where examples are augmented with reasoning explaining how the correct output was reached [117].\nAlthough LLMs have shown significant potential to automate software engineering tasks, many challenges and limitations have also emerged. LLMs are known to hallucinate, issuing output that appears initially plausible, but is incorrect and not justified by the underlying dataset [52]. Additionally, there is the risk of data leakage, where the LLMs may have seen a benchmark during training, leading to artificially high performance when it is asked to make predictions based on data from that benchmark [114].\nAn LLM agent is an advanced AI-based system coupling an LLM with additional tool access and memory capabilities [111]. These mechanisms allow LLM agents to reason, plan, and perceive or interact with an environment [32, 111]. For example, an LLM agent may have access to an organization's version control system or issue tracker. This would allow it to reason about the code, make changes to it, and push those changes to a remote repository. Multiple LLM agents can work together to address complex tasks. For example, agents can be given different roles or sub-tasks, e.g., simulating how human development teams would cooperate [119].\nA common tool built into LLM agents is Retrieval-Augmented Generation (RAG) [63]. RAG allows an LLM to access information from external sources, and use that information to better respond to prompts. For example, even if a particular source code file was not part of the training data for an LLM, RAG could be used to retrieve that file and make inferences from its contents."}, {"title": "3 Related Work", "content": ""}, {"title": "3.1 Test Maintenance", "content": "Kochar et al. examined testers' perspectives on the characteristics of good test cases, finding that maintainability was one of the primary aspects mentioned [60]. However, in practice, Gonzalez et al. found that only a quarter of examined open-source projects implemented tests following patterns related to maintainability [38]. Imtiaz et al. also found that there is a need for studies on test maintenance in an industrial context [49].\nMultiple authors have identified factors that influence the probability that test maintenance will be required, frequency that maintenance must be performed, or the difficulty of performing maintenance. For example, Pinto et al. found that many of the tests \"added\" to test suites are actually older tests that have been updated and renamed [87]. They also found that tests are generally deleted because they are obsolete, not because they are difficult to update. When tests are added, it is to cover new functionality, validate bug fixes, and validate refactored code. Al\u00e9groth et al. identified thirteen factors that affect the difficulty of test maintenance for GUI-based automated test suites [8], with test case length having the greatest impact. Berglund et al. also identified five factors that affect test maintenance for machine learning systems-e.g., non-determinism and explainability of the system-under-test-and nine further factors that affect both traditional and machine learning systems-e.g., the required precision of the test oracle and consistency of testing practices between teams [13].\nResearchers have also examined the co-evolution of test and source code, under the assumption that changes to the source code and its corresponding test case within a short time-frame indicate that the changes are linked [106]. Kita et al. proposed a metric, Tconf, to assess the extent to which the source and test code have co-evolved [59]. Ens et al. have presented a visualization tool that shows how source and test code have changed over time [30]. Sohn and Papadakis developed a tool that infers traceability links between source and test code based on co-evolution [105].\nCo-evolution has been used to identify source code changes that may trigger a need for test maintenance. We draw on this literature as part of answering our first research question (see Section 5.1). For example, Shimmi and Rahimi extracted patterns of co-evolution, classified as triggering addition, deletion, or modification of test cases [100]. Reich and Maalej extracted similar patterns, dividing code changes into low-level (changes to a single file, e.g., a change to a variable's data type) and high-level changes to the project (e.g., merging two packages) [90]. Levin and Yehudai also identified source code changes that tend to lead to test changes, e.g., removing a class or changing a method signature [62]. Marsavina et al. used association rule mining to identify patterns of co-evolution [74]. Their findings were later confirmed by Vid\u00e1cs and Pinzger [113].\nWang et al. also identified fine-grain source code change patterns-e.g., changes to if-statements-that may trigger test maintenance, and used these patterns to train a model to predict whether a test case is outdated [116]. Liu et al. improved the accuracy of this approach with more complex method of establishing traceability between source and test code and by automatically assigning code change patterns through static analysis, rather than manually labeling change patterns [69]. Huang et al. also proposed a machine learning approach that predicts whether test code needs to be updated based on semantic inference from the code and code complexity metrics [47].\nMirzaaghaei et al. have developed an approach to automatically repair test cases based on changes to source code [75, 76, 78]. Their approach is based on changes to method parameters and return values. Hu et al. have also proposed an approach to adapt test cases when the source code changes by inferring edit patterns from datasets of source and test code changes [46].\nOur study is influenced by and expands on prior work. We expand the range of triggers considered, we broadly explore the theoretical applications of automation in test maintenance, and we demonstrate one proof-of-concept for automation-predicting which test cases need to be updated. Our proof-of-concept is similar to the approaches of Wang et al. [116] and Liu et al. [69]. However, we (a) demonstrate how LLMs could be used for this task, and (b), employ different forms of data in making predictions. Our study is also one of the few conducted in an industrial setting, and incorporates the experience and opinions of practitioners as an important source of data."}, {"title": "3.2 Large Language Models", "content": "Due to their ability to infer semantic meaning from both natural language and source code [86], large language models hold immense potential for automating software engineering tasks and activities [39, 45, 127]. This includes software testing, where Wang et al. have presented a survey on the use of LLMs in the field [114]. They found that there has been a heavy focus on test case generation-in particular, on unit test generation (e.g., [97, 101, 125]). LLMs overcome some issues with traditional test generation techniques, e.g., generating non-trivial assertions [97] and more readable test cases [34, 125]. They can also generate tests from natural language information, such as bug reports [55]. However, there are still challenges to address, including non-compilable test cases [125], hallucinated calls to non-existent code [125], and issues with conforming to strong type systems [101]. Gay also found that LLMs can improve the readability of existing test cases, which could improve their maintainability as well [34].\nLLM agents are a very recent research topic, and there is limited work on their use in software engineering. Feldt et al. developed a taxonomy of LLM agents in the area of testing, and outlined an example conversational testing agent [32]. Yoon et al. have also demonstrated how LLM agents can perform Android GUI testing [123]. Hong et al. also proposed a framework based on standard operating procedures to facilitate cooperation between agents and humans [44]. As an example of how humans and LLM agents can interact, they model the roles in a software development team. Rasheed et al. have shown that cooperating LLM agents-each working on a particular subtask (e.g., code design, review, testing)-can generate higher quality code than a single LLM or LLM agent working alone [89].\nTo date, we are aware of no research examining the use of LLMs or LLM agents within test maintenance. We address this gap in our study by, first, broadly exploring how LLMs could be integrated into the maintenance process, then presenting a proof-of-concept-predicting which test cases may need to be updated-in an industrial setting."}, {"title": "4 Methods", "content": "In this study, we are interested in exploring the potential applications that LLMs or LLM agents could have-and the potential benefits or limitations of such applications-within the test maintenance process. Specifically, we address the following research questions:\nRQ1 When changes occur in a project, what factors (e.g., changes to specific code elements) trigger the need for maintenance of the corresponding test code?\nRQ2 What applications could current-generation LLMs or LLM agents have in the test maintenance process?\nRQ2.1 Which of the triggers from RQ1 could be acted upon by an LLM or LLM agent?\nRQ2.2 What viable test maintenance actions could an LLM or LLM agent perform, based on these triggers?\nRQ2.3 What considerations must be taken when deploying an LLM within an industrial environment?\nRQ3 What is the performance of our prototype multi-LLM frameworks when predicting the need for test maintenance?\nThe purpose of RQ1 is to identify and categorize the specific factors (such as source code changes) that could trigger the need for test maintenance. The goal of RQ2 is to explore how these triggers could be acted upon by LLMs or LLM agents, reducing the need for manual test maintenance. We explored three aspects of this application. First, we identified which triggers could be acted on. Second, we explored what actions an LLM or LLM agent could take, based on these triggers. We then investigated the considerations to be made when deploying LLMs in an industrial setting. Finally, we implemented four prototypes, following two multi-LLM architectures, to demonstrate how LLM agents could be applied to predict which test cases need to be updated following a change to the source code. The purpose of RQ3 is to evaluate the current performance of these prototypes, including strengths, limitations, and ideas for future research and development work.\nWe address these research questions through a case study at Ericsson AB, a Swedish telecommunications company. Our case study follows the guidelines from Runeson and H\u00f6st [95]. To address the research questions, we performed the following steps (shown visually in Figure 1):"}, {"title": "4.1 Literature Review to Identify Test Maintenance Triggers (RQ1)", "content": "We conducted a literature review to identify aspects of the source code or development process of a project that-when updated-may trigger a need for test maintenance. In particular, we focused on identifying individual project changes (e.g., concrete changes to code, requirements or other artifacts) that previous research literature noted as potential triggers. Our review was inspired by Keele's guidelines for systematic literature reviews [58].\nWe applied the following search strings to ACM, IEEE, Science Direct, SCOPUS, and Google Scholar:\n\u2022 (\"test case\" OR \"test suite\") AND (update OR create OR refactor OR generate OR maintenance OR evolution OR management OR repair OR co-evolution) AND (\"source code\" OR codebase) AND (factors OR criteria)\n\u2022 (\"test case\" OR \"test suite\") AND (update OR create OR refactor OR generate OR maintenance OR evolution OR management OR repair OR co-evolution) AND ( \"foundational model\u201d OR \u201cmachine learning\u201d OR \u201cLLM\u201d OR \"large language model\")\nThe first string was developed iteratively through informal experimentation with synonyms, with the intent of capturing relevant research on test maintenance while filtering publications that do not discuss potential triggers. The second string adds an additional filter for publications related to LLMs, and was applied to ensure that no related work was missed. The strings were adapted for each database. In Scopus, the subject area was limited to computer science.\nDatabase results were sorted by relevancy, then we selected publications by inspecting the title, followed by the abstract and conclusion. Inspection of results from each database ended after no new relevant papers were identified from three pages of results. We applied the following inclusion criteria:\n\u2022 Publications must have been published within the past 15 years-i.e., from 2009-2024-to ensure relevancy to modern software development and testing paradigms.\n\u2022 Publications must have been written in English.\n\u2022 Peer-reviewed and pre-print articles were considered. However, preference was given to peer-reviewed articles.\n\u2022 Publications must relate to test maintenance.\n\u2022 Publications must discuss specific factors that could trigger a need for test maintenance.\nA total of 48 publications were found through inspection to potentially meet these criteria. The publications were then read in full. If they discussed test maintenance triggers, these were recorded for later analysis. Eight publications discussed such triggers. We performed backward and forward snowballing on these publications, yielding 64 additional potentially-relevant publications. From these, four more discussed test maintenance triggers."}, {"title": "4.2 Interviews (RQ1-2)", "content": "Interviews were held with Ericsson employees to better understand their test maintenance process, triggers, and challenges. The interviews also included questions about the potential use of LLMs and tool support.\nParticipant Selection: Our target population was Ericsson employees with experience in testing at the company. We utilized convenience sampling, with elements of purposive sampling, to identify participants. Based on our knowledge of the organization, relevant teams were identified and invited to participate. Ultimately, we conducted five interview sessions, with a total of eight participants. The interviews were conducted in the same manner regardless of the number of participants present at the same time.\nInterview Instrument: We conducted semi-structured interviews, where we followed a prepared interview instrument (Table 2), but asked follow-up questions to gain further insight. Interview sessions were conducted using Microsoft Teams and were recorded for transcription. All participants signed a consent form before beginning the interview. On average, each interview lasted for approximately 40 minutes.\nWe performed a pilot interview to test the instrument. This pilot session led to removal of an irrelevant question and minor clarifications of other questions. Because changes were minor, the pilot interview was used in the final analysis.\nInterview Analysis: Interviews were first transcribed automatically using functionality provided by Microsoft Teams, then the transcripts were manually corrected. After the interviews were transcribed, we performed thematic analysis following the guidelines described by Braun and Clarke [16].\nDuring the analysis process, we highlighted relevant parts of the transcripts and assigned \"code labels\"-short identifiers-to each. We then developed \"codes\" describing each highlighted segment. Then, in an iterative process, the codes and code labels were grouped into themes and sub-themes. This process was completed by the first two authors, with feedback from the other authors. We judged that we had reached saturation in codes after analyzing the transcripts from these eight participants. Therefore, we decided against conducting further interviews."}, {"title": "4.3 Survey (RQ1-2)", "content": "As interviews require significant effort to perform, we decided to conduct a survey with a broader set of Ericsson employees to gain further insight into the test maintenance process, triggers, and challenges, as well opinions regarding LLMs and tool support.\nParticipant Selection: Our target audience was, like the interviews, Ericsson employees with testing experience. We again employed convenience sampling with elements of purposive sampling. We initially distributed the survey to a \"developer community\" within Ericsson, consisting of over 100 developers across different countries and sections within the company that work within the same product area. Due to a low initial response rate, we then distributed the survey to additional developer communities. The survey was open for a total of 54 days.\nUltimately, there were 29 participants who completed the survey.  Therefore, we hypothesize that the data from interviews and the survey responses are complementary.\nSurvey Instrument: The instrument for the survey was designed based on guidelines from Ghazi et al. [37] and Kasunic [56], and is presented in Table 3.\nThe survey was designed to take 5-10 minutes to ensure a reasonable completion rate. The intention of the survey was to provide quantitative data to augment the qualitative data gathered in interviews. Thus, questions were designed to multiple-choice, rather than open-ended. Respondents could provide multiple answers to some questions, but were asked to limit the number of selected options to force them to prioritize their selections. It should also be noted that the survey was designed prior to the analysis of interview data.\nWe evaluated the wording of all questions using understandability criteria established by Kasunic [56]. These criteria offer rules for the phrasing and structure of questions to minimize misunderstandings. The survey instrument was evaluated by a data analytics expert at Ericsson. It was also pilot-tested by three Ericsson developers to ensure there were no ambiguities in the questions, as well as to ensure it could be completed in less than ten minutes.\nSurvey Analysis: The results of the survey were analyzed using descriptive statistics, and were contextualized by the thematic analysis of the qualitative data obtained during the interviews."}, {"title": "4.4 Literature Review on LLM Capabilities for Test Maintenance (RQ2)", "content": "To gain an understanding of how LLMs could potentially assist with test maintenance as well as considerations for industrial deployment, we conducted a literature review focusing on how LLMs have been applied within the field of software testing. This review followed a similar procedure to the review discussed in Section 4.1.\nWe applied the following search strings to ACM, IEEE, Science Direct, SCOPUS, and Google Scholar:\n\u2022 (llm OR \"large language model\u201d OR \u201cgenerative ai\") AND (\"test management\" OR \"test maintenance\" OR \"software testing\")\n\u2022 (llm OR \"large language model\u201d OR \u201cgenerative AI\u201d) AND (\u201ctest case\u201d OR \u201ctest suite\u201d) AND (\"software\")\n\u2022 (llm OR \"large language model\" OR \u201cgenerative AI\u201d) AND (\u201csoftware engineering\u201d OR code) AND (suitability OR appropriateness OR abilities OR methods)\n\u2022 (llm OR \"large language model\" OR \u201cgenerative AI\u201d) AND (trigger OR \"trigger point\" OR action OR apply OR automate OR improve OR simplify OR update OR updating OR create OR creating OR develop OR enable OR interact OR understand OR analyze OR generate OR generation) AND (\u201ctest management\u201d OR \u201ctest maintenance\" OR \"test suite\" OR \"test case\" OR test OR \"test code\" OR \"quality assurance\" OR \"software testing\" OR \"software documentation\" OR \"test scenario\" OR \"test design\") AND (suitability OR appropriateness OR abilities OR methods)\nThese strings were iteratively developed. The first two capture publications where LLMs are used in the area of software testing. The third expands more broadly to the capabilities of LLMs in performing tasks involving source code. Finally,"}, {"title": "4.5 Mapping Test Maintenance Triggers and Tasks with LLM Capabilities (RQ2)", "content": "To identify how LLMs or LLM agents could assist with test maintenance tasks, we compared the data gathered on triggers, practices, and challenges from the literature review on test maintenance triggers (Section 5.1), the interviews (Section 5.2), and the survey (Section 5.3). We then mapped this data to the capabilities of LLMs demonstrated in previous literature (Section 5.4).\nWe developed a list of potential matches between test maintenance tasks or challenges and ways that LLMs have been applied through an open-ended brainstorming session. To decide which specific test maintenance tasks a LLM or LLM agent could be implemented for, we utilized a mind-mapping process. This was done by brainstorming which LLM actions and applications might fit specific triggers, and how these could be combined to solve the test maintenance tasks and challenges identified through the interviews and survey. We also considered how the use of LLMs should be adapted to match developers' preferences and the teams' way of working.\nAs part of addressing RQ1, we split test maintenance triggers into high- and low-level triggers, where high-level triggers relate to process-level decisions (e.g., requirement changes or a decision to improve the efficiency of the test suite) and low-level triggers referring to individual changes to project artifacts. We created one map for high-level and one for low-level triggers.\nDuring this mapping process, we also decided that LLM agents are more appropriate for addressing test maintenance tasks than using LLMs on their own. This is because an LLM agent can utilize external tools, like retrieval-augmented generation [63], to obtain relevant additional information without retraining the core model. Additionally, an LLM agent generally has memory capabilities and can, therefore, perform complex multi-step processes as well as potentially correct previous mistakes. LLM agents typically demonstrate improved performance over non-agent LLMs, especially in zero-shot prompting interactions [115, 120]."}, {"title": "4.6 Proof-of-Concept Design (RQ3)", "content": "The results of RQ1-2 illustrate the theoretical applicability of LLM agents within the test maintenance process. To explore the practical applicability of current-generation LLM agents, we decided to implement a proof-of-concept that could act on the source and test code developed at Ericsson. To establish a scope for this proof-of-concept, we decided to focus on a use case where a multi-LLM framework would observe a change to the source code, then-based on the specific changes made-would predict which test cases require maintenance.\nWe iteratively explored multiple potential architectures for a framework that could predict the need for test maintenance. Ultimately, we implemented four prototypes in Python, based on two proposed architectures. This section will present the architectures and implementation details for the four prototypes.\nOverall Architectures: We developed two core architectures for performing the prediction tasks. Both employ a multi-LLM architecture to improve performance [41, 44, 66]. This allows each LLM agent or LLM instance to focus on a particular subtask, and means that each agent can be developed to employ specialized tools. The first architecture (Figure 3) employs a planning agent that coordinates the effort of other LLM instances and agents. The second (Figure 4) is architected as a chain of calls between LLMs instances and an agent.\nAn alternative to an LLM agent would be to pre-train an LLM for a specific task. However, as the costs of pre-training can be very expensive, we decided that an LLM agent would be more appropriate. In addition, pre-training is not as adaptive as an agent architecture as the model would still need to be retrained to gain access to updated information. However, these two concepts are complementary-a pre-trained model could be used within an agent architecture-and their combination could be explored in future work."}, {"title": "4.7 Proof-of-Concept Evaluation (RQ3)", "content": "We performed an evaluation of the four prototypes to understand their current capabilities and limitations and to provide inspiration for future research and development work.\nIt is worth clarifying our expectations regarding the performance of the prototypes prior to the evaluation. Mistral 7B - Instruct is not a state-of-the-art model. The model has also neither been trained, nor fine-tuned, on the Ericsson codebase. Therefore, we did not expect the prototypes to attain sufficient performance to be directly usable in a real-world application. Rather, our intention was to show the feasibility of employing LLMs for such a use case, to provide a model for future implementations, and to explore the current limitations that must be overcome.\nEvaluation Dataset: To evaluate the accuracy of the four prototypes, we developed a dataset from the commit history of a Java-based git repository at Ericsson. The project contained in the repository is a microservice that performs data processing tasks. The test code contains both unit and integration tests.\nWe compare the predictions of each prototype to a ground truth obtained from the commit history. Our ground truth is based on the assumption that source code and test cases that are changed in the same commit are examples of co-evolution. The dataset spans 57 commits, which together contain 374 distinct code changes.\nThe prototypes are designed to return test case recommendations for a code change. Therefore, each commit has been manually split into individual code changes. We define an individual code change as a chunk of source code (containing the changed code) of 20 lines of code or the entire modified method, whichever is smaller. The reason for using individual code changes instead of entire commits at once was due to limitations in Mistral 7B - Instruct's ability to understand and work with large contexts. The following types of code changes were excluded from the dataset:\n\u2022 Commits with no changes to the source code.\n\u2022 Changes to source code that only modified imports or comments.\n\u2022 Additions or removal from source code. This evaluation focused solely on modifications.\n\u2022 All changes to test code that were not within a test case, such as changes to test support code.\n\u2022 The addition and deletion of files and new test cases in the test code. Deletion of existing test cases was included as it can be seen as a modification of an existing test case.\nEvaluation Procedure: For each commit, we performed the following procedure:\n(1) The information stored in the RAG tools was updated to reflect the current commit.\n(2) Each individual code change was separately fed into the agent setup, which would output test cases that needed to be updated or modified based on that change. These test cases suggested by the agent were saved as results.\n(3) In the dataset, ground truth is known for a commit, not for each code change. To assess accuracy, the suggested test cases from all code changes in that commit were combined into a set, which was compared to the test cases that had actually been changed in the commit (the ground truth).\nWe define a true positive (TP) as a correctly identified test case, a false positive (FP) as a test case that was identified for modification that should not have been identified, and a false negative (FN) as a test case that was not identified for modification that should have been identified. These measurements are calculated individually for each commit. Then, to assess performance across the full dataset, we sum together the TP, FP, and FN from each commit."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Literature Review to Identify Test Maintenance Triggers (RQ1)", "content": "We identified 12 publications that explicitly define concrete project changes that can trigger a need for test maintenance. Broadly, these triggers can be divided into changes to the documentation-information about the software, such as comments, requirements, or annotations in the code-and changes to the functionality of the source code.\nIn Figure 6, we list the triggers that we discovered. We also group changes to the source code based on the level of granularity-functionality, class-level, method-level, and line-level. We define these triggers in Tables 4-7. There is some overlap in the triggers-for example, \"addition of functionality\" could include \"addition of a class\" or \"addition of a method\". However, we retain all three to preserve as much information as possible about changes to the code and to capture cases where it was not clear how the new functionality was added."}, {"title": "5.2 Thematic Analysis of Interviews (RQ1-2)", "content": "In total, five major themes were identified during the analysis of the interview transcripts-Ways to Assure Quality, Reasons to Change Tests, Issues Related to Test Maintenance, Wishlist for Tool Support, and Attitudes Towards Generative Al. Each has multiple sub-themes.\nAn overview of the themes is presented in Table 8, and we discuss each theme and sub-theme below. In addition, in Figure 7, we indicate the number of codes corresponding to each theme and sub-theme. The number of occurrences of a theme or sub-theme does not necessarily indicate its relative importance, but it does indicate that the interview participants had more to say about some subjects."}, {"title": "5.2.1 Reasons to Change Tests", "content": "This theme concerns test maintenance triggers. The primary triggers are summarized as sub-themes in Table 9.\nProduction: The most prominent reason for test maintenance-discussed in 68% of codes in this theme-is a change in the source code. Naturally, if functionality is changed or new functionality is introduced, the test suite must generally evolve as well. Participants, in particular, described test maintenance occurring following new feature implementations, changes in requirements, modifications to existing code methods, and changes to conditional statements-all of which were also derived as triggers in Section 5.1.\nRefactoring: The source code often evolves as a result of refactoring:\nParticipants noted that it is difficult to completely separate logic and code. Though the \"functionality\" remains unchanged, how that functionality is tested may still require modifications. Refactoring at a small scale-e.g., changing the implementation of a function to improve its performance-should ideally not require test maintenance. However, large-scale refactoring, such as a change to the fundamental architecture of the project, may require such maintenance. For example, methods and classes invoked by tests may no longer exist, or interface changes may have been implemented.\nBug: Naturally, if a bug is discovered, the source code will evolve. This evolution may induce evolution in the test suite as well. For example, developers may discover that an area of the code is under-tested:\nParticipants also discussed cases where tests were added to the suite to reproduce a bug, as well as cases where bugs were discovered in the test themselves.\nTech Stack: Changes to the tech stack-e.g., programming languages, third-party libraries, testing frameworks, or build systems-can also trigger the need for test maintenance. An unstable tech stack can intensify test maintenance effort, and may constrain the forms and quantity of testing that can take place.\nTestability: At times, the source code is modified to improve its testability, with the aim of improving the efficiency and effectiveness of testing. These changes also can trigger the need for test maintenance, as the test suite must be adapted to the changing system and new forms of testing may now be possible. For example, a participant noted efforts to make components more modular or to make them mockable in tests:"}, {"title": "5.2.2 Ways to Assure Quality", "content": "This theme encapsulates the different ways the interviewees discussed to ensure the quality of test cases and test suites", "Measures": "Participants strongly stressed the importance of ensuring both code and"}]}