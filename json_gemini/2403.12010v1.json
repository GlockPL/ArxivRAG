{"title": "VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model", "authors": ["Qi Zuo", "Xiaodong Gu", "Lingteng Qiu", "Yuan Dong", "Zhengyi Zhao", "Weihao Yuan", "Rui Peng", "Siyu Zhu", "Zilong Dong", "Liefeng Bo", "Qixing Huang"], "abstract": "Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects.", "sections": [{"title": "Introduction", "content": "The creation of 3D content plays a crucial role in virtual reality, the game and movie industry, 3D design, and so on. However, the scarcity of large-scale 3D data and the high time consumption of acquiring them pose significant obstacles in learning a strong 3D prior from them for high-quality 3D content creation. To"}, {"title": "Related Works", "content": "Distillation-based Generation. Score Distillation Sampling was first proposed by DreamFusion [55] to generate 3D models by distilling from pre-trained 2D image generative models without using any 3D data. Fantasia3D [5] further disentangled the optimization into geometry and appearance stages. Magic3D [37] uses a coarse-to-fine strategy for high-resolution 3D generation. ProlificDreamer [81] proposes variational score distillation (VSD), which models the 3D parameter as a random variable instead of a constant. CSD [32] considers multiple samples as particles in the update and distills generative priors over a set of images synchronously. NFSD [29] proposes an interpretation that can distillate shape under a nominal CFG scale, making the generated data more realistic. SteinDreamer [76] reduces the variance in the score distillation process. LucidDreamer [36] proposes interval score matching to counteract over-smoothing. HiFA [98] and DreamTime [22] optimize the distillation formulation. RichDreamer [57] models geometry using a multi-view normal-depth diffusion model, which makes the optimization more stable. RealFusion [46], Make-it-3D [72], HiFi-123 [90], and Magic123 [56] use multi-modal information to improve generation fidelity. DreamGaussian [71] and GaussianDreamer [88] use an efficient Gaussian Splitting representation to accelerate the optimization process. However, distillation-based generation requires tens of thousands of iterations of the 2D generator and can take hours to generate a single asset.\nFeed-forward-based Generation. Many works attempt to use a neural network to directly learn the 3D distribution by fitting 3D data. OccNet [47] encodes shapes into function space and infers 3D structure from various inputs. Mesh-VAE [69] also learns a reasonable representation in probabilistic latent space for various applications. 3D-GAN [85] designs a volumetric generative adversarial network for shape generation from latent space. With the development of differentiable rendering, HoloGAN [51] and BlockGAN [52] learn 3D representation from natural images in an unsupervised manner. To maintain multi-view consistency, some prior works [2, 3, 10, 14, 54, 86, 94] incorporate implicit 3D representations in generative adversarial networks for 3D-aware generation. GET3D [13], DG3D [99], and TextField3D [21] leverage DMTet [61] for accurate textured shape modeling. Accompanied by the development of 2D diffusion models [19, 58], 3D diffusion-based approaches [6, 7, 16, 26, 35, 42, 44, 49, 50, 64, 91, 95, 96] use variants of diffusion models for generative shape modeling. Point-E [53] and Shap-E [25] expand the scope of the training dataset for general object generation. LRM [20], PF-LRM [79], and LGM [70] choose to use a deterministic"}, {"title": "Method", "content": "3.1 Problem Statement and Approach Overview\nProblem Statement. Given a text or single-image prompt, VideoMV aims to generate consistent multi-view images under user-specified camera poses.\nApproach Overview. The key idea of VideoMV is to combine a large video generative model for initializing a multi-view generative model and a novel 3D-Aware Denoising Sampling strategy to further improve multi-view consistency. Figure 1 illustrates the pipeline of VideoMV. In the first stage, we fine-tune a pre-trained video generation model to obtain the multi-view generative model G (Section 3.2). We focus on how to specify camera poses of multi-view images to connect with object-centric videos. In the second stage, we employ a feed-forward reconstruction module to obtain an explicit global 3D model based on the images generated by G (Section 3.3). The explicit model uses a variant of the"}, {"title": "Fine-tuning Generation", "content": "The first stage of VideoMV fine-tunes a video generative model for multi-view image generation. This is achieved by generating videos from rendered images of 3D models for fine-tuning. In the following, we first introduce the pre-trained video generative models. We then describe how to generate video data from 3D models for fine-tuning.\nPre-trained video generative models. We choose two open-source video generative models, Modelscope-T2V [75] and I2VGen-XL [93], which are used for the text-based and single-image-based multi-view generation, respectively. Both belong to the video latent diffusion model (VLDM), which uses a pre-trained encoder and a pre-trained decoder and performs diffusion and denoising in the latent space.\nSpecifically, consider a video $x \\in \\mathbb{R}^{F\\times H\\times W\\times 3}$ where F is the number of frames. They use a pre-trained encoder E of VQGAN [12] to compress it into a low-dimensional latent feature $z = E(x)$, where $z \\in \\mathbb{R}^{F\\times h\\times w\\times c}$. In the training stage, the diffusion process samples a time step t and converts $z_0$ to $z_t$ by injecting Gaussian noise $\\epsilon$. Then a denoising network $\\epsilon_\\theta$ predicts the added noise $\\epsilon_\\theta(z_t, y, t)$. The corresponding optimized objective can be simplified as follows:\n$L_{VLDM} = \\mathbb{E}_{z_t,y,\\epsilon \\sim \\mathcal{N}(0,1),t} ||\\epsilon - \\epsilon_\\theta(z_t, y, t)||^2,$\nwhere y denotes the conditional text or image. In the denoising sampling loop, given an initial Gaussian noise, the denoising network predicts the added noise"}, {"title": "Feed-Forward Reconstruction", "content": "The second stage of VideoMV learns a neural network that reconstructs a 3D model from images generated by the model G trained in the first stage. In the last stage of VideoMV, we will use rendered images of this 3D model to guide the denoising step in G to achieve improved multi-view consistency.\nWe employ 3D Gaussians [31] as the representation of the 3D model, which has a fast rendering pipeline for image generation. Instead of using the optimization scheme that gets 3D Gaussians parameters via fitting rendering images to input images (which is time-consuming), we employ a feed-forward manner to directly regress the attributes and number of 3D Gaussians. In the following, we first review the 3D Gaussian Splatting [31] representation. We then present the reconstruction network.\n3D Gaussians. The 3D Gaussian representation uses a set of 3D Gaussians to represent the underlying scene. Each Gaussian is parameterized by a center $p \\in \\mathbb{R}^3$, a scaling factor $s \\in \\mathbb{R}^3$, a rotation quaternion $q \\in \\mathbb{R}^4$, an opacity value $\\alpha \\in \\mathbb{R}$, and a color feature $c \\in \\mathbb{C}$. To render the image, 3DGS projects the 3D Gaussians onto the camera imaging plane as 2D Gaussians and performs alpha compositing on each pixel in front-to-back depth order."}, {"title": "3D-Aware Denoising Sampling", "content": "As shown in Figure 1, we adopt a 3D-Aware Denoising Sampling strategy that involves the rendered images produced by our reconstruction module in a denoising loop to further improve the multi-view consistency of the resulting images. We use the DDIM [65] scheduler with 50 denoised steps for fast sampling. The sampling step from $z_t$ to $z_{t-1}$ of DDIM [65] can be formulated as follows:\n$z_{t-1} = \\sqrt{\\alpha_{t-1}} \\cdot (\\frac{z_t - \\sqrt{1-\\alpha_t} \\cdot \\epsilon_\\theta(t)}{\\sqrt{\\alpha_t}}) + \\sqrt{1-\\alpha_{t-1} - \\sigma^2} \\cdot (\\epsilon_t) + \\sigma \\epsilon_{t-1},$\nwhere $\\alpha_t$ and $\\sigma$ are constants, $\\epsilon_t$ is the standard Gaussian noise independent of $z_t$, and we use $\\epsilon_\\theta(t)$ rather than $\\epsilon_\\theta(z_t, y, c, t)$ to denote the predicted noise for simplicity. Note that during the training of the reconstruction network, we convert the predicted noise to \"predicted $z_0$\" and decode it to $x_0$ as the input of the training data."}, {"title": "Experiments", "content": "We perform experimental evaluation on two tasks, i.e., text-based multi-view generation and image-based multi-view generation. For text-based multi-view generation, we adopt MVDream [63] as the baseline approach, and report metrics including PSNR, SSIM [82], LPIPS [92], and flow-warping RMSE. For image-based multi-view generation, we adopt Zero123 [40], Zero123-XL [8, 40], and SyncDreamer [41] as baseline approaches, and report metrics that include PSNR, SSIM [82], and LPIPS [92]. Note that in text-based multi-view generation, we evaluate by NeRF-based novel view synthesis since no ground truth is provided."}, {"title": "Text-based Multi-View Generation", "content": "We use 100 single-object prompts from T3Bench [17] for quantitative evaluation. For MVDream [63], we feed circular camera poses into it and generate 24 views simultaneously. MVDream [63] was trained on 32 uniformly distributed azimuth angles, and the objects were rendered twice with different random settings."}, {"title": "Image-based Multi-View Generation", "content": "VideoMV can be reformulated to image-based multi-view generation. In our experiments, we fine-tune VideoMV from I2VGen-XL [93], which is an open-source image-based video generation method and shares the same architecture as modelscopeT2V [75]. Since I2VGen-XL [93] accepts both an input image and a text prompt, we set the text prompt to an empty string in the fine-tuning stage. We similarly train a feed-forward reconstruction module and apply consistent sampling in VideoMV. Evaluation is carried out on 50 objects from the GSO dataset (Google Scanned Objects) dataset [11], including the 30 objects from SyncDreamer [41]. Since SyncDreamer [41] is trained to generate fixed 16 views with an elevation of 30 degrees and the azimuth spans evenly in [0, 360] degrees, we only compute metrics on the [0, 3, 6, 9, 12, 15, 18, 21]th frames that correspond to the [0, 2, 4, 6, 8, 10, 12, 14]th frames of SyncDreamer [41]. For Zero123 [40] and Zero123-XL [8], we report metrics on the generated frames with azimuths of [0, 45, 90, 135, 180, 225, 270, 315] degrees. We also compare our method with ImageDream [78], which is an image-prompt-based multi-view generation method. We use BLIP2 [34] to caption the input image and evaluate under settings of 4 views and 24 views, respectively. Note that we always evaluate under the elevation settings of our baselines for fairness, which means that we use an input image of elevation = 5 for ImageDream [78] and an input image of elevation = 30 for SyncDreamer [41]."}, {"title": "Analysis on 3D-Aware Denoising Sampling", "content": "We have shown in Tab. 1 and Tab. 2 that our proposed 3D-Aware Denoising Sampling faithfully improves the consistency-related metrics. However, observing"}, {"title": "Applications", "content": "5.1 Dense View Reconstruction\nOur proposal can generate 24 dense views with specified camera poses using DDIM [65] sampling in 5 seconds. The dense view generated is enough for a reconstruction pipeline such as NeRF [48] or Neus [77]. We show reconstruction results on both image-based multi-view generation and text-based multi-view generation tasks.\nFor the image-based multiview generation task, we visualize results from NVS-based methods(Zero123(XL) [40], SyncDreamer [41]) and reconstruction-focused methods(Wonder3D [43], OpenLRM [18, 20], and Shap-E [25]) accompanied with our approach. As shown in Fig. 5, VideoMV achieves better reconstruction results among NVS-based methods due to the highly consistent"}, {"title": "Distillation-based Generation", "content": "Our proposal can also be applied as the priori of score distillation sampling [55]. As shown in Fig. 7, we can distillate faithful shapes and texture from a multi-view score distillation loss and avoid the Janus problem most of the time. Note that we are focusing on consistent multi-view image generation, so we do not fully optimize the distillation pipeline. Distillation from dense views is also an interesting task for future work."}, {"title": "Conclusions", "content": "In this paper, we present a consistent dense multi-view generation method that can generate 24 views at various elevation angles. By fine-tuning large video generative models for several GPU hours, our proposal can effectively produce dense and consistent multi-view images from an input image or a text prompt. Future directions may focus on developing a robust neural reconstruction pipeline based on the provided consistent dense views. Moreover, we have shown that there are rich opportunities in connecting videos and multi-view based 3D vision tasks. We hope our findings in turning a video generative model into a consistent multi-view image generator can also inspire other 3D generation and video-related tasks."}, {"title": "Architecture Details", "content": "We provide comprehensive information on the architectures of the networks employed. In Sec. 7.1, we elaborate on the text-based video generation network (ModelScopeT2V [75]), including its transformation into a text-based multi-view image generator. In Sec. 7.2, we discuss the image-based video generation network (I2VGen-XL [93]) and its conversion into an image-based multi-view image generator. Finally, in Sec. 7.3, we present our large GaussianSplatting-based reconstruction model (LGM [70]) and how it is utilized for noise reconstruction fine-tuning."}, {"title": "ModelScopeT2V (Text-Based Video Generation)", "content": "The main paradigm of ModelScopeT2V is shown in Fig. 9. Overall, it comprises two main components: VQGAN and Denoise UNet. VQGAN aims to reconstruct the original video as precisely as possible. Given a training video ugt, the encoder of VQGAN compresses it into the latent space as z\u0151t. Then, a quantizer is applied to zat. Note that the function of the quantizer is to find the closest quantized discrete vector of z\u0151t, and we denote the output from the quantizer as qgt. Finally, the decoder of VQGAN takes qot as input and outputs the reconstruction vrec.\nThe overall objective of VQGAN is the weighted summation of the quantization loss between $L_q = f_q(z_{gt}, q_{gt})$, the reconstruction loss $L_r = f_r(v_{gt}, v_{rec})$, and an optional adversarial loss $L_{adv} = f_{adv}(v_{gt}, v_{rec})$.\nDenoiseUNet aims to recover zgt from the noise-corrupted zit. The optimization objective is the denoise reconstruction loss $L_{dr} = f_{dr}(z_{gt}, z_{pr})$. zor is predicted by the network mapping function e as $z_{pr}^\\theta = e_\\theta(z_{it}, y, t)$. As depicted in Fig. 10, DenoiseUNet is built with several Spatial-Temporal convolutional"}, {"title": "I2VGen-XL (Image-Based Video Generation)", "content": "The primary paradigm of I2VGen-XL is illustrated in Fig. 11. It is important to note that the framework follows a one-staged approach, distinguishing it from the two-staged architecture proposed by [93]. Upon careful examination, we discovered that the available open-source implementation actually corresponds to a one-staged model; hence, we decided to adopt this version as depicted in Fig. 11. This one-staged model allows for the incorporation of both images and text as conditions, providing additional global (G.Enc.) and detailed (D.Enc) information extracted from the image.\nI2VGen-XL [93] shares the same DenoiseUNet architecture and VQGAN architecture with ModelScopeT2V [75]. However, they differ in terms of dataset"}, {"title": "Feed-Forward Reconstruction", "content": "We have adopted the identical architecture proposed in LGM [70]. As illustrated in Fig. 12, this asymmetric UNet architecture offers advantages in terms of memory efficiency by mitigating the increase in points within the GaussianSplatting representation caused by high-resolution output. It incorporates dense self-attention, similar to MVDream [63]. Considering computing resources, we did not extend it to accommodate 24 views. Yet, we have future plans for developing a dense view reconstruction model.\nSequentially, we fine-tune the asymmetric UNet using the \"predicted xo\". Inspired by LGM [70], we randomly select four views from the output of VideoMV"}, {"title": "Text-to-Multi-View Image Generation", "content": "The VideoMV demonstrates the ability to generate diverse outcomes by employing different random noises while maintaining the same prompt. As depicted from Fig. 13 to Fig. 17, VideoMV produces a range of astonishing results across various prompts selected from the multi-object list in T3Bench [17]."}, {"title": "More Qualitative Results", "content": "Despite the limited data used for fine-tuning large-scale video generative models, the alignment between prompts and visual information in both video datasets and image datasets (consisting of 1-frame videos) remarkably enhances generalizability to open-vocabulary scenarios, enabling VideoMV to generate multi-view images beyond the limitations of training datasets. To further enhance qualitative visualization, we adopt highly abstract prompts previously employed in DreamFusion [55]. As illustrated in Fig. 18 to Fig. 26, VideoMV consistently generates dense multi-view images based on abstract prompts, showing its ability to understand out-of-distribution data."}, {"title": "Image-to-Multi-View Image Generation", "content": "Although the input image provides some guidance for dense pixel generation in multi-view scenarios, VideoMV is capable of generating various plausible results even from invisible angles. We present two typical examples in Fig. 27 to illustrate that VideoMV can produce diverse yet faithful outputs based on the given image input."}, {"title": "More Qualitative Results on Google Scanned Object", "content": "Due to the page limitations in the main paper, we have included additional qualitative results of Google Scanned Objects in Fig. 28 for a comprehensive analysis. Note that the first image serves as the input for VideoMV."}, {"title": "More Qualitative Results on Web Images", "content": "The VideoMV technique can also be applied to web images that lack underlying 3D models. In this study, we present a visualization of limited cases and encourage readers to experiment with our code on a wider range of web images. As illustrated in both Fig. 29 and Fig. 30, VideoMV demonstrates its capability to generate feasible results using either generated or in-the-wild images."}, {"title": "Numerical results on image-to-3D", "content": "As a dense multi-view generative model, VideoMV aims to tackle the challenging task of synthesizing novel views with higher density and consistency based on a given prompt or single image. Unlike previous approaches [40, 41], we do not employ any reconstruction optimization in VideoMV. Instead, inspired by prior work [77], we present relevant Volume IOU and Chamfer Distance metrics on the GSO dataset using the off-the-shelf MVS method, such as NeuS [77]. As depicted in Tab. 3, VideoMV outperforms state-of-the-art methods in terms of Chamfer Distance and Volume IOU metrics, indicating that leveraging increased consistency in multi-view images for reconstruction can result in improved accuracy in 3D geometry."}]}