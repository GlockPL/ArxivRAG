{"title": "ReCapture: Generative Video Camera Controls for\nUser-Provided Videos using Masked Video Fine-Tuning", "authors": ["David Junhao Zhang", "Roni Paiss", "Shiran Zada", "Nikhil Karnad", "David E. Jacobs", "Yael Pritch", "Inbar Mosseri", "Mike Zheng Shou", "Neal Wadhwa", "Nataniel Ruiz"], "abstract": "Recently, breakthroughs in video modeling have allowed\nfor controllable camera trajectories in generated videos.\nHowever, these methods cannot be directly applied to user-\nprovided videos that are not generated by a video model.\nIn this paper, we present ReCapture, a method for generat-\ning new videos with novel camera trajectories from a single\nuser-provided video. Our method allows us to re-generate\nthe reference video, with all its existing scene motion, from\nvastly different angles and with cinematic camera motion.\nNotably, using our method we can also plausibly halluci-\nnate parts of the scene that were not observable in the ref-\nerence video. Our method works by (1) generating a noisy\nanchor video with a new camera trajectory using multiview\ndiffusion models or depth-based point cloud rendering and\nthen (2) regenerating the anchor video into a clean and\ntemporally consistent reangled video using our proposed\nmasked video fine-tuning technique.", "sections": [{"title": "1. Introduction", "content": "Recently, diffusion models have enabled significant ad-\nvances in video generation and editing [10, 27, 31, 32,\n35, 55, 66, 76, 94, 99, 103, 105, 107, 109], revolution-\nizing workflows in digital content creation. Camera con-\ntrol plays a vital role in practical applications of video\ngeneration and editing, allowing for greater customization\nand stronger user experience. Recent efforts have intro-\nduced camera control capabilities to video diffusion mod-\nels [2, 29, 38, 89], yet, in this case the videos are entirely\ngenerated by the video model from a text-prompt and are\nneither captured in the real world, nor provided by a user.\nEffectively generating new videos with user-specified cam-\nera motion from an existing user-provided video that con-\ntains complex scene motion is still an open and challenging\nproblem.\nThe task is inherently ill-posed due to the limited amount\nof information in the reference video: one cannot know ex-\nactly how the scene looks like from all angles if there is not\nfull knowledge of the scene's 4D content. However, this\ndoes not preclude an approximate solution that is plausi-\nble and appreciated by users. Previous studies have shown\npromising results by generally assuming the availability of\nsynchronized multi-viewpoint videos [64] and constructing\n4D neural representations. Later works [45, 53, 80, 87, 92]\nenable 4D reconstruction using a single monocular video,\nbut require accurate camera pose and depth estimation, and\ncannot capture content outside the original field of view. In\nthis paper, we reformulate this problem as a video-to-video\ntranslation task. Camera Dolly [82] also develops a video-to-video pipeline, but requires 4D video data with differ-\nent camera poses obtained via simulation, which limits it to\nin-domain scenes like driving or cubic objects. Given the\nchallenge of obtaining paired videos in the wild with vary-\ning camera movements, it is hard to solve this problem with\na video-to-video pipeline in an end-to-end manner and we\nseparate it into two steps instead. Our approach leverages\nthe prior knowledge of diffusion generative models, in both\nimage and video domains, to effectively reangle the video\nas if filmed from the requested camera trajectory.\nFor the first stage of our method, we want to generate an\nincomplete anchor video conditioned on the user-provided\ncamera trajectory and reference video. We initially obtain a\npartial frame-by-frame depth estimation of the target video.\nWe project each frame into 3D space using a depth estimator\nto obtain a sequence of point clouds. Then, we simulate the\nuser-specified camera movement, which can include zoom,\npan, and tilt, and render the point cloud sequence according\nto the new camera trajectory. This estimation is only partial\nsince, as illustrated in Figure 4, these camera movements\ncan introduce black areas outside the original video bound-\naries, cause some blurring due to the nature of point cloud\nprojection and have poor temporal consistency since they\nare generated frame-by-frame. Another way to obtain the\nnoisy anchor video, that uses recent advances in 3D recon-\nstruction, is to use a multiview diffusion model [24] condi-\ntioned on camera pose and individual video frames. This\nmethod also results in an anchor video that has poor tem-\nporal consistency, along with blurring, artifacts and black\nareas outside the scene.\nUsing this anchor video our method is able to generate a\nclean output with the desired camera trajectory. To achieve\nthis we propose the novel technique of masked video fine-\ntuning. This technique consists of training a context-aware\nspatial LoRA and temporal motion LoRA on the known pix-\nels from the generated anchor video, as well as from addi-\ntional reference frame data. Specifically, the spatial LoRA\nis incorporated into the spatial layers of the video diffusion\nmodel and finetuned on augmented frames extracted from\nthe source video. This enables the model to learn the sub-\nject's appearance and the background context of the source\nvideo. The temporal LoRA enables the model to learn the\nscene motion with respect to the new camera trajectory, and\nis inserted into the temporal layers of the video diffusion\nmodel and finetuned using a masked loss on the anchor\nvideo. Unknown regions are masked, which excludes them\nfrom the loss computation, enabling the model to focus on\nmeaningful and known regions and motion while ignoring\nthe unknown areas.\nDuring inference, equipped with both video specific spa-\ntial and temporal LoRAs, the diffusion model can automati-\ncally fill the unknown regions of the anchor video with plau-\nsible content, leveraging the video diffusion model's prior\nand the context provided by the spatial LoRA. It also signif-\nicantly improves temporal consistency and removes anchor\nvideo jittering. This results in a coherent and meaningful\nvideo output, preserving the motion and layout of the orig-\ninal anchor video as learned through the temporal LoRA\ntraining. Finally, as a refining step, we can remove the tem-\nporal LoRA and retain only the context-aware spatial LoRA\nto apply SDEdit [56] to the generated video, thereby further\nreducing blurring and improving temporal consistency.\nIn the end, we generate a video with new camera tra-\njectories while preserving the original complex scene mo-\ntion and the full content of the source video. Notably,\nthis is accomplished without the need for paired video\ndata. Ultimately, our method outperforms the generative\napproach Generative Camera Dolly [82], which requires\npaired videos as training data, and other 4D reconstruc-\ntion methods [48, 93] on the Kubric dataset [82]. Further-\nmore, each component of our proposed method is validated\nthrough ablation studies on VBench [40]."}, {"title": "2. Related Work", "content": "Video Diffusion Model. Recent video generation meth-\nods [4, 15, 16, 25, 30, 37, 41, 70, 97, 105], predominantly\nutilize diffusion models [34, 60, 79] due to their state-\nof-the-art performance and robust open-source communi-\nties. Some approaches use a 3D-UNet architecture, inflat-\ning an image diffusion model with trainable temporal layers\n[4, 7, 10, 26, 37]. Other works explored the transformers\narchitecure, with both discrete [43] and continous tokens\n[11, 27, 54]. In this paper, we use the open-source Stable\nVideo Diffusion (SVD) [7] as our video diffusion model.\nPersonalization of Video Diffusion Models. At this stage\nthe problem of personalization of image generative models\nhas been well explored in the last several years, with work\non subject-driven generation [18, 22, 71, 72, 100], style-\ndriven generation [33, 69, 78, 86], style+subject-driven gen-\neration [73, 75] and image-level personalization for inpaint-\ning [81]. The research direction of personalization of video\nmodels is more sparse, albeit with important recent work\nsuch as Dreamix [58] which proposed to finetune video\nmodels on a given video, Still-Moving [14] which miti-\ngates the need for customized video data by elevating a cus-"}, {"title": "3. Method", "content": "Our method works in two stages, first by generating an in-\ncomplete and noisy anchor video with respect to the new"}, {"title": "3.1. Preliminaries", "content": "3D U-Net. Video diffusion models train a 3D U-Net\n(spatial-temporal for 3D) to denoise a sequence of Gaus-\nsian noise samples to generate videos, guided by text or im-\nage prompts [8, 10]. The 3D U-Net architecture comprises\ndown-sampling, middle, and up-sampling blocks. Each\nblock includes multiple spatial convolution layers, spatial\ntransformers, cross-attention layers, and either temporal\ntransformers or convolution layers.\nLow-Rank Adaptation (LoRA). Low-rank adaptation was\nintroduced to fine-tune large pre-trained language mod-\nels [39] and has since been applied to text-to-image and\ntext-to-video tasks for appearance customization [14, 71].\nIt updates the weight matrix W using low-rank factoriza-\ntion as:\n$W \\leftarrow W_0 + \\Delta W = W_0 + BA,$\nwhere $W_0$ represents the original weights, and B and A\nare low-rank factors with much fewer dimensions. LORA\nis computationally efficient and can be more easily regular-\nized, which preserves more of the model's prior knowledge\ncompared to fine-tuning the entire network."}, {"title": "3.2. Anchor Video with New Camera Motion", "content": "At this stage, we are given a reference video $V =\n[I_0,...,I_{N-1}] \\in R^{N \\times 3 \\times H \\times W}$, where N represents the\nnumber of frames. The main objective is to transform these\nframes into a new sequence, denoted as $V^a$, based on a dif-\nferent camera trajectory provided by the used. We refer to\nthis video as an anchor because it anchors the final output\nof our method and serves as a condition for the next stage.\nThis first draft contains artifacts from out-of-scene regions\nand temporal inconsistencies, which will be corrected in the\nsecond stage.\nWe present two approaches to obtain an anchor video\nbased on a new camera trajectory. The first method uses a\npoint cloud rendering technique, suitable for typical camera"}, {"title": "Point Cloud Sequence Rendering", "content": "We begin by lifting\nthe pixels from the input image plane into a 3D point cloud\nrepresentation. For each frame of the source video $I_i, i \\in\n\\{0, ..., N-1\\}$, we independently estimate its depth map $D_i$\nusing an off-the-shelf monocular depth estimator [6]. By\ncombining the image with its depth map, the point cloud $P_i$\ncan be initialized as:\n$P_i = \\phi([I_i, D_i], K),$\nwhere $\\phi$ denotes the mapping function from RGBD to a\n3D point cloud in the camera coordinate system, and K rep-\nresents the camera's intrinsics using the convention in [19].\nNext, we take as input the camera motion as a pre-\ndefined trajectory of extrinsic matrices $\\{P_1,..., P_{N-1}\\}$,\nwhere each includes a rotation matrix and a translation ma-\ntrix representing the camera's pose (position and orienta-\ntion), which are used to rotate and translate the point cloud\nin the camera's coordinates.\nWe then project the point cloud of each frame back\nonto the anchored camera plane using the function $\\psi$ to\nobtain a rendered image with perspective change: $I'_i =$\n$\\psi(P_i, K, P'_i)$. By calculating the extrinsic matrices corre-\nsponding to the camera's movement, we can express a va-\nriety of camera motions including zoom, tilt, pan, pedestal,\nand truck, enabling flexible camera control to yield anchor\nvideos:\n$V^a = \\{I'_0, ..., I'_{N-1}\\} = \\{\\psi(P_i, K, P'_i) | i \\in \\{0, ..., N - 1\\}\\}.$\nSimultaneously with the color frames, we obtain a bi-\nnary mask for each frame. Valid pixels after projecting the\npoint cloud, represented with a value of '1'. Regions miss-\ning due to camera movement as shown in Fig. 4, which ex-\ntend beyond the original video scene, are marked as '0'.\nWe denote the corresponding sequence of binary masks as\n$M^a \\in R^{N \\times 1 \\times H \\times W}$."}, {"title": "Multiview Image Diffusion for Each Frame", "content": "When a\ncamera trajectory involves significant rotation and view-\npoint changes, point cloud rendering usually fails [102]. To\naddress this, we employ a multiview diffusion model [24].\nThis approach leverages the fact that multiview image\ndatasets are generally easier to obtain compared to multi-\nview video datasets. Specifically, as shown in Fig. 3, for\neach frame $I_i$ of the source video, which represents the con-\ndition view, along with its corresponding camera parame-\nters $P_{cond}$, the model learns to estimate the distribution of\nthe target image $I'_i$\n$p (I'_i | I_i, P_{cond}, P_i) .$\nwhere $P_i$ is the target camera parameters which are also\nprovided as input. The multiview diffusion model employs\na 3D U-Net as its backbone, and for simplicity of notation,\nwe omit the latent VAE encoder. The 3D U-Net is adapted\nfrom a 2D text-to-image U-Net by inflating the 2D self-\nattention mechanism into 3D, allowing it to operate across\nboth 2D spatial dimensions and multiple view images. Due\nto the difficulty in obtaining the camera pose with the con-\ndition frame, we follow CAT3D and use a raymap with the\nsame dimensions as the condition images, computed rela-\ntive to the first image's camera pose. This makes the pose\nrepresentation invariant to rigid transformations. Raymaps\nare then concatenated channel-wise to the corresponding\ncondition image.\nBy applying this multiview diffusion model to each\nframe, we can generate the anchor video $V^a$. However, pro-\ncessing each frame independently leads to significant tem-\nporal inconsistencies and artifacts (Fig. 4). While CAT3D\ncan complete unseen regions, it hallucinates these region\ndifferently at each frame. To mask out this regions, we ob-\ntain the mask $M^a$ with the same approach as point cloud\nrendering, which indicate the regions missing due to added\ncamera movement."}, {"title": "3.3. Masked Video Fine-tuning", "content": "At this stage, our objective is to take an incomplete anchor\nvideo (with significant artifacts and inconsistencies) as in-\nput and produce a clean, high-quality output. To achieve\nthis, we introduce the technique of masked fine-tuning of a\nvideo diffusion model, incorporating a context-aware spa-\ntial LoRA and a temporal motion LoRA. By leveraging the\nstrong prior of the video diffusion model, we can generate\na high-quality video conditioned on the anchor video. Next\nwe present our components:\nTemporal LoRAs with Masked Video Fine-tuning. The\nanchor video from the first stage may exhibit significant ar-\ntifacts, such as revealed occlusions due to camera move-\nment and temporal inconsistencies such as flickering. To\naddress these issues, we propose a masked video fine-tuning\nstrategy using temporal motion LoRAs. LoRAs are applied\nto the linear layers of the temporal transformer blocks in the\nvideo diffusion model. Since LoRA operates in a low-rank\nspace and the spatial layers remain untouched, it focuses\non learning fundamental motion patterns from the anchor\nvideo without over-fitting to the entire video. The strong\ntemporal consistency prior from the video diffusion model\nhelps minimize temporal inconsistencies. We introduce a\nmasked diffusion loss, where the invalid regions in the an-\nchor video are excluded from the loss calculation, ensuring"}, {"title": "Context-Aware Spatial LoRAs", "content": "Although the video dif-\nfusion model with masked fine-tuning automatically fills\nthe invalid regions of the anchor video, the filling may not\nbe consistent with the original context or appearance, and\nmight appear pixelated, as shown in Fig. 8 Line 2. To ad-\ndress this issue, we propose enhancing the spatial atten-\ntion layers of the video diffusion model by incorporating\na spatial LoRA, which is fine-tuned on the frames of the\nsource video. At each training step, a frame is randomly\nselected from the source video, and the temporal layers are\nbypassed. The spatial LoRA loss is defined as follows\n$L_{spatial} = E_{\\epsilon, t, i\\sim U\\{0,...N-1\\}} [||\\epsilon - \\epsilon_{\\theta}((I_{i,t}), t, y)||],$\nwhere $I_{i, t}$ denotes the noisy frame $I_i$ of the source video\nat time step t. The spatial LoRA captures the original con-\ntext from the source video, ensuring seamless integration of\nfilled pixels with the original pixels.\nConsequently, our final diffusion loss is the sum of\n$L_{temp}$ and $L_{spatial}$. To ensure compatibility between the"}, {"title": "4. Experiments", "content": "We conduct a comprehensive qualitative and quantitative\nevaluation to assess the performance of our method, com-\nparing it against prominent baselines for the task of novel\nview synthesis in existing videos. Our quantitative analy-\nsis in Sec. 4.1 encompasses two complementary subsets of\nautomatic metrics: low-level statistics, including PSNR and\nSSIM, and high-level semantic measures, such as subject"}, {"title": "4.1. Quantitative Evaluation", "content": "Low-level evaluation metrics. We utilize the Kubric-4D\ndataset, consisting of 3,000 scenes, including an evaluation\nsubset of 100 scenes. These scenes, generated with the\nKubric simulator, showcase complex multi-object interac-\ntions and dynamic movement patterns. Each scene includes\nsynchronized video from 16 fixed camera viewpoints at a"}, {"title": "4.2. Qualitative Comparisons", "content": "For qualitative results, we compare our method with Gen-\nerative Camera Dolly [82], as shown in Figure 4. The com-\nparison demonstrates that our method more accurately fol-\nlows the camera trajectory, generates fewer artifacts, and\nexhibits less blurriness than Generative Camera Dolly. No-\ntably, even when the camera position differs significantly\nfrom the original video, our method faithfully preserves\nthe subject's motion and appearance, whereas Dolly suffers\nfrom significant artifacts under these conditions."}, {"title": "4.3. Ablation Studies", "content": "We evaluate the impact of each component of our method in\nTable 3. As demonstrated, fine-tuning the temporal LoRA\nwith masking significantly improves temporal consistency\nand visual quality. This effect is also visible in Fig. 8, where\ngenerating the video with temporal LoRA helps to fill re-\ngions not visible in the anchor video, enhancing both visual\nquality and temporal coherence. Additionally, incorporat-\ning the context-aware spatial LoRA further boosts visual\nquality and subject consistency by leveraging prior knowl-\nedge of the original video. Finally, integrating SDEdit fur-\nther enhances overall quality by reducing artifacts. These\nresults affirm the effectiveness of each component in our\nmethod."}, {"title": "4.4. Implementation Details", "content": "We use the the CAT3D [24] multi-view model without any\nfurther adjustments. We employ SVD [9] as our video dif-\nfusion model in all our experiments, since it is I2V model,\nwe use Vo as the image prompt. For the LoRA finetuin-\ning we use rank of 16 for both spatial and temporal LoRAs."}, {"title": "5. Conclusion", "content": "In this work we present ReCapture, our proposed method\nto generate new videos with novel camera trajectories from\nexisting user-provided videos. Compared to previous work,\nand thanks to the strong prior of video models, ReCapture\nhas a surprisingly strong ability to generalize to vastly dif-\nferent videos and scenes, and in many cases faithfully pre-\nserves complex scene and subject motion, as well as the\ndetails of the scene."}]}