{"title": "Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy with Pre-training, Data Augmentation and Dual Flow UNet", "authors": ["Litingyu Wang", "Wenjun Liao", "Shichuan Zhang", "Guotai Wang"], "abstract": "Head and neck tumors and metastatic lymph nodes are crucial for treatment planning and prognostic analysis. Accurate segmentation and quantitative analysis of these structures require pixel-level annotation, making automated segmentation techniques essential for the diagnosis and treatment of head and neck cancer. In this study, we investigated the effects of multiple strategies on the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT) images. For the segmentation of pre-RT images, we utilized: 1) a fully supervised learning approach, and 2) the same approach enhanced with pre-trained weights and the MixUp data augmentation technique. For mid-RT images, we introduced a novel computational-friendly network architecture that features separate encoders for mid-RT images and registered pre-RT images with their labels. The mid-RT encoder branch integrates information from pre-RT images and labels progressively during the forward propagation. We selected the highest-performing model from each fold and used their predictions to create an ensemble average for inference. In the final test, our models achieved a segmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on aggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at https://github.com/WltyBY/HNTS-MRG2024_train_code.", "sections": [{"title": "1 Introduction", "content": "Head and neck (H&N) cancers are among the most prevalent types of cancer. Imaging in H&N cancer serves multiple purposes, including quantitative assessment of tumors, evaluation of nodal disease, and differentiation between recurrent tumors and post-treatment changes [11]. In recent years, convolutional neural networks (CNNs) within the realm of deep learning have profoundly impacted medical image analysis [2,4]. The detection and segmentation of H&N"}, {"title": "2 Methods", "content": "We proposed multiple training strategies for the two tasks, and selected the best-performing model in each of the five-fold cross-validation divisions. For the segmentation of pre-RT images in Task-1, we employed:\n\u2022 Fully supervised learning utilizing an encoder-decoder architecture, as depicted in Fig. 1 (a).\n\u2022 The same fully supervised learning approach, enhanced with pre-trained weights and the MixUp data augmentation method."}, {"title": "2.1 Networks", "content": "We designed two network architectures in this study. The first network is basic segmentation network, shown as Fig. 1 (a)), which is an encoder-decoder architecture. For task-1, the input of basic segmentation network is the single-channel pre-RT images. For Task-2, the input of basic segmentation network is three-channel images, which are consisted of mid-RT image, registered pre-RT image and its mask. The second network is DFUNet, shown as Fig. 1 (b), which is only used for Task-2. The DFUNet comprises two encoders, whose inputs are a single-channel mid-RT image and a two-channel input (registered pre-RT image and its mask), respectively. To facilitate information fusion, the information from the pre-RT image and its mask is integrated into the mid-RT stream at various encoder stages using a CNN-based cross attention block [13]. This CNN-based attention mechanism differs from the attention blocks described in [14] by employing both spatial and channel attention [16]. This approach enhances or fuses features while significantly reducing the computational demands of training and inference. The operational details of the cross attention block are illustrated in Fig. 2. Details of the network's configuration are elaborated in Section 3.3."}, {"title": "2.2 Model Pre-training", "content": "We utilized the SegRap2023 Challenge Dataset [9] and the basic segmentation network depicted in Fig.1 (a) for fully supervised pre-training. However, since SegRap2023 is based on CT images, we encountered challenges due to the inherent intensity differences between CT and MRI data. To minimize these differences, we employed histogram matching during preprocessing to align the intensity distributions of CT and MRI as closely as possible. Despite this effort, intensity variations across various anatomical structures remained a challenge. To further address these issues, we incorporated techniques from domain generalization [19], applying nonlinear transformations to the image intensities. This approach was designed to reduce the model's reliance on specific intensity values, thereby enhancing its generalization capabilities. The effectiveness of these nonlinear transformations is visualized in Fig. 3."}, {"title": "2.3 MixUp Augmentation Strategy", "content": "Since our training methodology is based on patches, patches in one mini-batch inevitably contain negative samples, which entirely belong to the background category. To mitigate the impact of these negative samples and to encourage the network to learn the distribution across different classes, we employ the MixUp [18] technique to augment our dataset. MixUp creates new training examples by linearly interpolating between pairs of samples, which helps the model"}, {"title": "2.4 Loss", "content": "The inputs to our network can be categorized into two types: the raw input patches and the new cases generated by MixUp. For the raw input patches, we utilize a combination of CrossEntropy loss LCE and Dice loss LDice for both pre-training and downstream training:\n$\\displaystyle Loss_{raw} = \\frac{1}{2I} \\sum_{d=0}^{I-1} (LCE(x_d, y_d) + LDice(x_d, y_d))$ \nwhere d represents the identifier for different resolutions, with smaller d values corresponding to higher resolutions of xa and ya. I represents the number of resolutions used in computing the loss. The low-resolution labels ya are obtained by down-sampling the original labels. In contrast, for the cases generated through MixUp, we only compute LCE and omit deep supervision:\n$Loss_{MixUp} = LCE(x, y)$"}, {"title": "3 Experiments", "content": "We conducted our experiments using the nnUNetv2 [6] framework, which streamlined the hyperparameter selection process. This toolkit automates many decisions, allowing us to focus on the specific architectural adjustments needed for our tasks. As depicted in Fig. 1, we selected a VNet-like [10] architecture for our network, which was trained using deep supervision."}, {"title": "3.1 Datasets", "content": "SegRap2023 Challenge Dataset Segmentation of Organs-at-Risk and Gross Tumor Volume of NPC for Radiotherapy Planning Challenge Dataset [9] comprises 120 pairs of H&N CT and contrast-enhanced CT images. We ultimately used only the 120 enhanced CT scans because, during the training process, we found that the model's performance with enhanced CT was superior to that with CT or a combination of both."}, {"title": "3.2 Data Preprocessing", "content": "SegRap2023 First, we performed morphological operations to crop the images to the region of interest within the human body based on intensity. Second, we randomly selected a pre-RT image from the HNTS-MRG2024 dataset and applied histogram matching to align the grayscale histograms, as illustrated in Fig. 4. Finally, we applied Z-score standardizing and resampled each volume into a resolution of 1.2mm \u00d7 0.5mm \u00d7 0.5mm.\nHNTS-MRG2024 We selected pixels with intensity values greater than 60 to identify the region of interest. We then located the largest connected component and applied morphological operations to create a mask of the human body. Next, we cropped the image along the x and y axes to match the dimensions of the head area. Finally, we normalized the image using Z-score standardization and resampled it to the same resolution used for the SegRap2023 dataset."}, {"title": "3.3 Models", "content": "As illustrated in Fig. 1, we proposed two distinct network architectures. Fig. 1 (a) displays a basic encoder-decoder model designed for semantic segmentation, which consists of six resolution stages. Each stage includes two convolutional layers with instance normalization, a LeakyReLU activation function, and a residual connection. This is followed by a DownPool block that contains an additional convolutional layer, instance normalization, and LeakyReLU activation. The structure of the decoder is symmetrical to that of the encoder, but it utilizes UpSample with transposed convolution. Features are pooled five times along the x and y axes, and three times along the z axis. In Fig. 1 (b), the Dual Flow UNet (DFUNet) introduces an additional encoder and cross-attention blocks for merging two data streams. Both decoders share a design with four 1\u00d71 \u00d71 convolutions to output three channels for deep supervision."}, {"title": "3.4 Implementations", "content": "All the models were trained for 1000 epochs with a batch size of 2. When applying the MixUp [18] augmentation strategy, the batch size was increased to 4, as illustrated in Fig. 1 (a). We used the Stochastic Gradient Descent (SGD) optimizer, with a momentum of 0.99, an initial learning rate of 0.01, and a weight decay of 3 \u00d7 10-5. Training was conducted on a single NVIDIA RTX 4090 with 24GB of VRAM, using a patch size of 56 \u00d7 224 \u00d7 160. Inference was performed with a sliding window strategy, enhanced by Test Time Augmentation (TTA) that included operations such as axes flips. Model performance was evaluated based on the aggregated Dice Similarity Coefficient (DSC)."}, {"title": "4 Results", "content": "The results of the various training strategies based on a five-fold cross-validation of the training set, as outlined in the methods section, are detailed in Table 1 for Task-1 and Table 2 for Task-2. The bolding in Table 1 and Table 2 indicates the models selected for the corresponding data split, representing the models that yielded the best results in those particular folds. Overall, the average cross-validation performances for the selected models, as measured by the aggregated DSC, are 80.65% for Task-1 and 74.68% for Task-2.\nFor Task-1, as shown in Table 1, all strategies struggle to achieve significant improvements in GTVp segmentation, although they positively impact GTVn. The combination of pre-trained weights and MixUp marginally enhance GTVp segmentation by 0.47% in aggregated DSC, while GTVn sees a more substantial improvement of 1.30%. As illustrated in Fig.5(a), pre-trained weights facilitate goal exploration to some extent, whereas MixUp significantly aids in the identification of foreground categories.\nIn Task-2, the situation is more complex. Although incorporating pre-RT images and their corresponding labels significantly enhances the segmentation"}, {"title": "5 Discussion and Conclusion", "content": "In this study, we developed GTV segmentation models using two variations of the VNet-like [10] architecture on the MRI T2w H&N dataset from the HNTS-MRG2024 Challenge. Our investigation focused on expanding the dataset and refining network structures. To augment the dataset, we initially used an external public CT dataset [9] for pre-training and subsequently applied MixUp technology to generate more training data. For Task-2, we also utilized additional registered pre-RT images and masks to enhance the segmentation of mid-RT. Beyond the standard segmentation network structure, we introduced the DFUNet, which includes two encoders, one decoder, and CNN-based cross attention blocks. This architecture enables the network to differentiate between the primary image for segmentation and supplementary information.\nThe 5-fold cross-validation results indicate that our proposed strategies have led to significant improvements in the segmentation of simpler categories, such as GTVn. However, the advancements in the segmentation of a more challenging category, like GTVp, have been marginal and could even introduce adverse effects, especially for Task-2, as shown in Fig. 6 (b). We suspect that the suboptimal learning of GTVp is due to the severe class imbalance and the learning of full background samples. While nnUNetv2 [6] samples foreground categories during training, its methodology does not adequately address the imbalance among foreground categories. A potential solution might involve an expanded oversampling strategy that targets each foreground and background category individually. Although the DFUNet underperformed compared to the basic segmentation model in cross-validation, it demonstrated notable enhancements in GTVp segmentation in some folds. We are confident that with further refinements to the DFUNet architecture, it could outperform the basic model. Additionally, it is worth noting that pre-training is not universally effective, and in some cases, the use of pre-trained weights can significantly degrade segmentation performance. As illustrated in Fig. 5 (b), the weaker segmentation performance of GTVn in CT scans may hinder the model's ability to segment MRI samples that are difficult to segment for GTVn.\nFurthermore, during pre-training with the SegRap2023 Challenge dataset [9], we observed that the segmentation performance for GTVp was markedly higher on the CT dataset compared to GTVn. Interestingly, this trend was reversed for T2-weighted MRI in HNTS-MRG2024. Although the types of cancer in the two datasets are different, a model that can combine the advantages of the two modalities can improve the segmentation performance while greatly reducing the dependence on multi-modal data and minimizing the patient's exposure during image acquisition. Potential approaches to achieve this include cross-modal distillation [15], domain adaptation [7], or the use of generative models [17].\nIn conclusion, we investigated the enhancement of fully supervised learning through dataset expansion and domain generalization techniques. We observed a notable improvement in the performance of GTVn with pre-training and the application of the MixUp strategy. Additionally, DFUNet led to significant enhancements in GTVp segmentation in some folds. Our average aggregated DSC across the folds is 80.65% for Task-1 and 74.68% for Task-2. In the final test, we achieve scores of 82.38% for Task-1 and 72.53% for Task-2. Future work could concentrate on refining the DFUNet architecture, effectively initializing its weights, and leveraging pre-RT data to boost the segmentation performance of mid-RT."}]}