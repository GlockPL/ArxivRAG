{"title": "TEAFormers: TEnsor-Augmented Transformers for Multi-Dimensional Time Series Forecasting", "authors": ["Linghang Kong", "Elynn Chen", "Yuzhou Chen", "Yuefeng Han"], "abstract": "Multi-dimensional time series data, such as matrix and tensor-variate time series, are increasingly prevalent in fields such as economics, finance, and climate science. Traditional Transformer models, though adept with sequential data, do not effectively preserve these multi-dimensional structures, as their internal operations in effect flatten multi-dimensional observations into vectors, thereby losing critical multi-dimensional relationships and patterns. To address this, we introduce the Tensor-Augmented Transformer (TEAFormer), a novel method that incorporates tensor expansion and compression within the Transformer framework to maintain and leverage the inherent multi-dimensional structures, thus reducing computational costs and improving prediction accuracy. The core feature of the TEAFormer, the Tensor-Augmentation (TEA) module, utilizes tensor expansion to enhance multi-view feature learning and tensor compression for efficient information aggregation and reduced computational load. The TEA module is not just a specific model architecture but a versatile component that is highly compatible with the attention mechanism and the encoder-decoder structure of Transformers, making it adaptable to existing Transformer architectures. Our comprehensive experiments, which integrate the TEA module into three popular time series Transformer models across three real-world benchmarks, show significant performance enhancements, highlighting the potential of TEAFormers for cutting-edge time series forecasting.", "sections": [{"title": "Introduction", "content": "In the era of big data, multi-dimensional time series data, such as matrix and tensor-valued time series, are increasingly common in applications like economics, finance, and climate science. For instance, policymakers track quarterly economic indicators like GDP growth and inflation across multiple countries [13]; investors monitor financial metrics such as asset/equity ratios and revenue from various companies [11]; and scientists observe hourly environmental variables like PM2.5 and ozone levels at multiple stations [15; 14]. All these datasets naturally present themselves as time series of matrices (order-2 tensors).\nTransformers [37] have demonstrated substantial promise in modeling sequential data such as language[51], text[22], and time series [40]. However, existing Transformer models fail to preserve the tensor structure of data. The calculations within these architectures effectively flatten multi-dimensional observations into vectors, losing the inherent multi-dimensional relationships and patterns."}, {"title": "", "content": "This paper fill this gap by introducing the Tensor-Augmented Transformer (TEAFormer), a novel approach designed to preserve and leverage multi-dimensional tensor structures. Multiple benefits are achieved by TEAFormer: 1) Its tensor expansion step effectively aggregate all possible useful information by multi-view feature learning from multiple channels, including those constructed from multi-head attention mechanism; 2) Its tensor compression step effectively compress information to a more efficient feature by automatic tensor decomposition, reduce the computational burden associated in the tensor expansion step such as self-attention, and simultaneously enhance their performance. By computing self-attention on this reduced-size core tensor, we achieve significant computational savings without sacrificing the model's ability to capture intricate tensor structures and dependencies in the data; 3) TEAFormer is not merely a specific model architecture. Its tensor expansion and compression steps are highly compatible with attention mechanism and the encoder-decoder structure. Thus, we generalize it as a TEA-module that can be incorporated in any Transformer model.\nWe incorporate the Tensor-Augmentation module into three widely used time series Transformer models: Transformer [37], Informer [54], and Autoformer [43]. We conduct extensive experiments across a diverse range of datasets, demonstrating that our approach not only reduces computational costs but also enhances prediction accuracy. Our results show substantial improvements in evaluation metrics, such as Mean Absolute Error (MAE) and Mean Squared Error (MSE) scores, compared to baseline models. This underscores the potential of tensor decomposition as a powerful tool for advancing matrix and tensor time series forecasting.\nRecent statistical studies have explored the benefits of maintaining the multi-dimensional structure in tensor time series using linear factor models [38; 13; 11; 14; 18; 23; 24; 27; 26] and linear vector auto-regressive (VAR) models [17; 47; 31]. They have shown great advantages of preserving the multi-dimensional structure in tensor time series. However, all of them adopt linear approaches and have limited capacity to capture potential nonlinear relationships, especially in applications involving language and text.\nThis work is the first to formally introduce a TEnsor-Augmented Transformer (TEAFormer), distinct from statistical VAR models. Maintaining tensor structure in Transformers is much more challenging due to the architecture's complexity, including encoding, decoding, and multi-head attention mechanisms, as detailed in Section 2. Our contributions are summarized in three key points:\n\u2022 Introduction of TEAFormer. We introduce TEAFormer, a novel multi-dimensional (tensor) time series Transformer architecture that integrates a Tensor-Augmentation module with any Transformer-based models. This is the first work to utilize tensor learning in Transformer.\n\u2022 Development of Tensor-Augmentation Module. We develop a Tensor-Augmentation module that involves tensor expansion and compression, effectively aggregating information through multi-view feature learning while reducing the computational burden associated with self-attention and information aggregation.\n\u2022 Comprehensive Empirical Evaluation of Appliction to Existing Tranformers. Our extensive experimental results show that all implemented TEAFormers outperform their baseline counterparts in 34 out of 42 trials across three real-world benchmarks. Ablation studies reveal that TEAFormer holds significant potential for future research: (1) the tensor compression structure can be further optimized; (2) it excels in learning from small datasets with shorter sequences; (3) tensor compression enhances performance when tensor expansion does not sufficiently aggregate information for learning.\nThe rest of this paper is organized as follows: Section 1.1 shows related work on Neural Networks and Transformer models for time series forecasting. Section 2 provides a detailed description of the TEAFormer architecture and its integration of tensor decomposition. In Section 3, we present our experimental setup, including datasets, evaluation metrics, and implementation details, followed by a discussion of the results and an analysis of performance improvements. Finally, Section 4 concludes the paper and outlines potential directions for future research."}, {"title": "Related Work", "content": "Our research lies at the intersection of tensor learning, multidimensional time series analysis, and transformer-based forecasting. Given the extensive literature across these fields, we focus our survey on the most relevant works."}, {"title": "", "content": "Multi-dimensional Time Series Analysis. Matrix- and tensor-variate time series analysis has emerged as a rapidly growing field in statistical learning, with substantial developments in recent years. The fundamental challenge in multi-dimensional time series analysis is understanding the intricate relationships among multiple time-dependent variables. Traditional approaches for handling high-dimensionality and inter-variable correlations involve vectorizing tensor time series into vector time series and applying factor models. Factor models have emerged as a crucial tool for capturing common dependencies among multiple covariates [3; 2; 1; 5]. Recent research has extended factor model methodologies to matrix- and tensor-variate time series analysis. These studies predominantly employ Tucker low-rank structures [11; 12; 13; 14; 16; 18; 23; 24; 50; 53] and Canonical Polyadic (CP) low-rank structures [4; 6; 26; 27]. Additionally, researchers have developed matrix- and tensor-variate autoregressive models with bilinear or multilinear forms, supported by theoretical guarantees demonstrating that preserving multi-dimensional structure enhances performance [17; 28; 31; 32; 47; 15; 33; 10; 7; 25]. The computer science literature focuses primarily on empirical approaches, leveraging neural network architectures for forecasting. Variables exhibit distinct patterns and cycles, interacting through both lagged relationships and simultaneous influences. While CNNs and RNNs have been traditional tools, recent innovations include models like DeepGLO [36], which employs multiple Temporal Convolutional Networks (TCNs) to capture global and local dependencies. Graph Neural Networks (GNNs) represent another promising direction: STGCL [34] utilizes contrastive learning at both graph and node levels for spatiotemporal GNNs, while GNN-RNN [20] processes diverse data types through CNNs and RNNs before using GraphSage GNN to extract geospatial representations. CausalGNN [39] combines various embeddings to obtain latent matrix representations, integrating them with causal embeddings through GNN-based non-linear transformations for disease forecasting. Despite these advances, the incorporation of tensor structures, particularly Tucker or CP low-rank constraints, into transformer architectures remains largely unexplored, presenting an important gap in the current literature.\nTransformer for Multivariate Time Series Forecasting. Transformers [37] offer significant advantages for handling sequence data, particularly due to their attention mechanisms, which allow for greater parallelization and faster training times on modern hardware. Informer [54] modifies the traditional transformer architecture for time series forecasting by using a probsparse attention mechanism for improved computational efficiency and a generative decoder to reduce complexity and prevent cumulative errors. Building on Informer, Spacetimeformer [21] incorporates spatial correlations by adding time and space information during embedding, calculating both global and local attention. Autoformer [43] enhances efficiency and decomposition ability through an autocorrelation mechanism and time series decomposition methods. Fedformer [55] leverages signal processing techniques, using Fourier transformation for global information decomposition. ETSformer [42] combines traditional time series decomposition with exponential smoothing, creating interpretable and decomposable forecasting outputs. Crossformer [52] introduces a two-stage attention mechanism, computing attention along the time axis first and then along the dimension axis, with an innovative router mechanism to manage attention distribution and reduce time complexity. DSformer [49] collects global and local information through sampling methods, rearranging data along the time and dimension axes.\nTensor-Augmented Neural Networks. While neural networks can effectively process high-dimensional tensor inputs, most existing approaches utilize tensors primarily for computational efficiency rather than leveraging their statistical properties. [19] established a foundational connection between deep networks and hierarchical tensor factorizations. Building on this, tensor contraction layers [29] and regression layers [30] were developed to regularize models, successfully reducing parameter counts while preserving model accuracy. Recent innovations, including the Graph Tensor Network [48] and Tensor-view Topological Graph Neural Network [41], have introduced novel frameworks for processing large-scale, multi-dimensional data. Additionally, significant progress has been made in uncertainty quantification and representation learning for tensor-augmented neural networks [8; 9; 44; 45; 46]. Despite these advances, the development of tensor-augmented transformers specifically designed for multi-dimensional time series remains an open challenge."}, {"title": "Tensor-Augmented Transformer", "content": "For a clear presentation, we consider the most common case of a two-dimensional matrix-variate time series \\(X_t \\in \\mathbb{R}^{D_1 \\times D_2}\\) for \\(t \\in [L]\\). In sequential forecasting, one aims to predict the future values \\(X_{t+1}\\) based on historical observations \\(X_1, ..., X_t\\).\nWe preserve the inherent multi-dimensional structure in the TEAFormer. For matrix-variate time series, we let \\(X^{(raw)} \\in []\\mathbb{R}^{L\\times D_1\\times D_2}\\) be one sample of raw input data, where \\(L\\) denotes the sequence length, \\(D_1\\) and \\(D_2\\) denote the sizes of two-dimensional matrix observation.\nEmbedding with Tensor Mode and Size Expansion. During embedding, we transform the raw input \\(X^{(raw)}\\) to feature \\(X \\in \\mathbb{R}^{L\\times L^{(mdl)} \\times D^{(mdl)}}\\). Here, the temporal embedded dimension \\(L^{(mdl)}\\) is not restricted to a scalar value of one-dimension. Instead, it can represent multiple dimensions \\(L^{(mdl)} := L^{(mdl)} \\times...\\times L^{(mdl)} \\ L^{mdl}\\), where \\(E \\geq 1\\) is the total number of modes extended from the temporal mode of \\(X^{(raw)}\\). For example, CrossFormer [52] expands the raw temporal mode of length \\(L\\) to \\(L^{(mdl)} = L^{(mdl)} \\times L^{(mdl)}\\) where \\(L^{(mdl)}\\) represents the number of sub-sequences and \\(L_{omd}\\) represents the embedding of each sub-sequence. This corresponding to Tensor mode expansion.\nAnalogously, the feature embedded dimensions \\(D^{(mdl)} := D_1^{(mdl)} \\times...\\times D_M^{(mdl)}\\) can be multi-dimensional with \\(M > 2\\). If \\(M > 2\\) where 2 is the number of dimensions of the raw time series observation \\(X_t\\), the embedding results in a mode-expanded feature space, or Tensor mode expansion, that is the number of modes increases. If \\(\\prod_{m=1}^{M}D_m^{(mdl)} > \\prod_{i=1}^{2}D_m\\), this embedding results in a size-expanded feature space, or Tensor size expansion, that is the size of the feature space increases. For example, CrossFormer [52] expands the raw temporal mode of length \\(L\\) to \\(L^{(mdl)} = L^{(mdl)} \\times L^{(mdl)}\\) by segmenting the \\(L\\)-length sequence to \\(S\\)-number of sub-sequences of size \\(L/S\\). The embedding also results in a size-expanded feature space since \\(L^{(mdl)} = S\\) and \\(L \\ L^{(mdl)} > L/S\\). Finally, we note that this setting also include the classic transformer as sub-cases. Specifically, for vector (order-1 tensor) data, \\(D^{(mdl)} := D^{(mdl)}\\) corresponds to the common transformer; for matrix (order-2 tensor) data \\(D^{(mdl)} := D_1^{(mdl)} \\times D_2^{(mdl)}\\) corresponding to matrix time series without Tensor mode expansion.\nTensor-Augmented Sequence-to-Sequence Multi-Head Attention (MHA). We first characterize the formulation of a Single-Head Attention (SHA). The embedded feature is \\(X \\in \\mathbb{R}^{L\\times L^{(mdl)} \\times D^{(mdl)}}\\). One attention head is structured as a tensor with dimension expressed as \\(D^{(attn)} := D_1^{(attn)} \\times...\\times D_M^{(attn)}\\). The value weight tensor is \\(W_v \\in \\mathbb{R}^{L^{(mdl)}\\times D^{(mdl)} \\times D^{(attn)}}\\) the query and key weight tensors are \\(W_Q, W_K \\in \\mathbb{R}^{L^{(mal)}\\times D^{(mdl)} \\times D^{(attn)}}\\), and the output weight tensor is \\(W_O \\in \\mathbb{R}^{D^{(attn)} \\times L^{(mdl)} \\times D^{(mdl)}}\\), all of which are trainable. The output of one layer of the Transformer is a \\((L \\times L^{(mdl)} \\times D^{(mdl)})\\)-dimensional tensor, and can be described as follows:\n\\[T^{SHA}(X; W_Q, W_K,W_v,W_O) := \\sigma\\left(\\text{RowSoftmax}\\left(\\langle(X, W_Q), (X, W_K)\\rangle\\right)(X, W_v)\\right) W_O,\\]\nwhere \\(\\langle X, W_Q\\rangle, \\langle X, W_K\\rangle, \\langle X, W_v\\rangle\\) denote tensor inner products that result in a \\((L \\times D^{(attn)})\\)-dimensional matrix, \\([\text{RowSoftmax}(M)]_{e,:}:=\\text{softmax}(M_{e,:})\\) that runs softmax on row of its input, and \\(\\sigma\\) is a \\(L_0\\) Lipshitz activation function that is applied element-wise and has the property \\(\\sigma(0) = 0\\)."}, {"title": "", "content": "For MHA, let \\(H\\) denote the number of head, we collect all weight tensors in \\(W := \\{W_{Q,h}, W_{K,h},W_{V,h},W_{O,h}\\}_{h=1}^{H}\\) for all heads \\(h \\in [H]\\) and define an extra head weight vector \\(w_H\\) of dimension \\(H\\). One layer of the Transformer with MHA can be described as \\(T\\)\n\\[T(X;W,w_H) = \\left[T^{SHA}(X;W_{Q,h}, W_{K,h},W_{V,h},W_{O,h})\\right]_{h=1}^{H} \\times_4 w_H.\\]\nThe dimension of \\(T(X)\\) is \\(L\\times L^{(mdl)} \\times D^{(mdl)}\\). Thus, a \\(S\\)-multi-layer Transformer can be constructed by iteratively compose \\(T(X)\\) for \\(S\\) times, denoted as\n\\[T_S(X; \\{W^{(s)}, w^{(s)}\\}_{s=0}^S) := T^{(S)} o... o T^{(0)} (X),\\]\nwhere \\(T^{(s)} = (\\cdot; W^{(s)}, w^{(s)})\\).\nIn our study, multi-head self-attention (MSA) is used. MSA is a specific type of MHA, in which queries, keys, and values all come from the same sequence, i.e. \\(W_Q = W_K = W_V\\), thus allowing the model to capture dependencies within that sequence.\nAutomated Information Aggregation and Compression. The initial feature input and the hidden throughputs are all of order-\\( (1 + E + M) \\) tensor structure of dimension \\(L \\times L^{(mdl)} \\times D^{(mdl)}\\) where \\(E\\) is the number of modes of the hidden temporal embedding and \\(M\\) is the number of modes of the feature model embedding. Current transformers treat all dimensions equally and carry out calculation by flattening all the tensors. The idea of this paper is to preserve the tensor (multi-dimensional) structure. As such, we are able to automate information aggregation and information compression through tensor expansion and compression.\nTensor compression refers to the procedure that incorporates low-rank tensor decomposition in the procedure, which can only be achieved when we keep the tensor structure in (2). We incorporate tensor low-rank structures such as CP low-rank, Tucker low-rank, and Tensor Train low-rank.\nTucker low-rank structure is defined by\n\\[X = C \\times_1 U_1 \\times_2 \\cdots \\times_M U_M + \\mathcal{E},\\]\nwhere \\(\\mathcal{E} \\in \\mathbb{R}^{D_1^{(mdl)} \\times...\\times D_M^{(mdl)}}\\) is the tensor of the idiosyncratic component (or noise) and \\(C\\) is the latent core tensor representing the true low-rank feature tensors and \\(U_m\\), \\(m \\in [M]\\), are the loading matrices.\nPractical Experiment Implementation. To our knowledge, there does not exist any Transformer-based time series forecasting model that has more than one hidden dimension in the \\(D^{(mdl)}\\) set. In addition, even though different models have different embedding methods, they are either expanding based upon sequence length or features, and would always result in a three-dimensional embedding. Hence, though it is theoretically possible to have tensor-augmented multi-head attention for higher dimensions, we are only able to test our theories under circumstances where there is only one hidden dimension and the embedded data is in three dimension.\nBefore passing into the attention block, the embedded input as a three-dimensional tensor will be passed into a Tensor-Augmentation module to be decomposed using Tucker decomposition method and be transformed into a group of factorized tensors.\n\\[X = \\text{Embed}(x^{(raw)}),\\\\X = C \\times_1 U_1 \\times_2 U_2 \\times_3 U_3,\\]\nwhere \\(C\\in \\mathbb{R}^{R_1\\times R_2\\times R_3},\\ R_1 \\ll L, R_2 \\ll L^{(mdl)},\\ R_3 \\ll D^{(mdl)}\\) is a core tensor that acts as a framework to capture the complex interactions between cross-dimensional and temporal features across the dataset, thus could be leveraged as a latent representation for global information in MTS."}, {"title": "", "content": "\\(U_1 \\in \\mathbb{R}^{L \\times R_1}, U_2 \\in \\mathbb{R}^{L^{(mdl)}\\times R_2}, U_3 \\in \\mathbb{R}^{D^{(mdl)} \\times R_3}\\) are factor matrices. They can extract the most predominant features in each dimension while maintaining the original dimension's structure.\nAs we intend to reduce the computational cost of self-attention while capture the global information about the interactions within the data, we will pass the core tensor, which is much smaller in size comparing to the embedded input while preserving essential information, into the attention block to conduct computation. To be more specific, in each encoder layer, we have,\n\\[C = \\text{LayerNorm}(C + MSA(C,C,C)),\\\\x^{enc} = C\\times_1 U_1\\times_2 U_2 \\times_3 U_3,\\]\nwhere \\(X^{enc}\\) is the output of the current encoder layer. And the rest of the model structure will be the widely adopted Transformer encoder-decoder architecture."}, {"title": "Experiments", "content": "Datasets We conduct experiments on three real world datasets that are very popular in recent multi-dimensional time series forecasting studies: (1) ETTh1: The temperature of electricity transformers (ETT)\u00b9 is a vital metric in the long-term deployment of electric power. This dataset contains two years of data from two different counties in China. The ETTh1 subset is the data on 1-hour-level granularity; (2) ETTm1: The subset of ETT dataset on 15-minutes-level granularity; and (3) WTH\u00b2:\nEvaluation Metrics We use two evaluation matrics: Mean Square Error and Mean Absolute Error,\n\\[MSE = \\frac{1}{LL^{(mdl)} D^{(mdl)}}\\sum_{t=1}^{L}\\sum_{i=1}^{L^{(mdl)}}\\sum_{j=1}^{D^{(mdl)}} ||X_{tij} - \\hat{X}_{tij}||^2,\\]\n\\[MAE = \\frac{1}{LL^{(mdl)} D^{(mdl)}}\\sum_{t=1}^{L}\\sum_{i=1}^{L^{(mdl)}}\\sum_{j=1}^{D^{(mdl)}} |X_{tij} - \\hat{X}_{tij}|.\\]"}, {"title": "Experiment Results and Analysis", "content": "As shown in Table 1, results of 34 out of 42 trials from our proposed tensor-augmented models surpass baselines, suggesting a significant improvement in model performance. For all dataset, the TEAFormer models have the better performance for the majority of the window sizes. Our model performs the best on WTH dataset. We argue that it is due to the WTH dataset has far more features and weather data has a more regular and predictable pattern than ETT. It has already been mentioned in many previous studies that ETT data has severe distribution shift challenge that has not been countered yet, which might impact TEAFormer's ability to forecast. We also notice that, the simpler the model structure, the better TEAFormer performs. Transformer is the start point of all attention-based time series forecasting studies, thus it has the simplest structure, followed by Informer, which is a modification to cope with multi-dimensional time series tasks. Autoformer is the latest and most complicated model among all three baselines, and we can observe that the baseline of Autoformer produces more better results than its corresponding TEA-counterparts than the other two models."}, {"title": "Ablation Study", "content": "TEA-module in decoder In this study, we aim to address the current architecture challenge mentioned in Section 2, i.e., whether conducting attention operation on the core tensors throughout the entire model is a feasible solution. Here we decompose \\(X^{dec}\\) and compute self-attention on the core tensor \\(C^{dec}\\) to test our hypothesis.\n\\[\\begin{aligned}&\\tilde{x}^{dec} = C^{dec} \\times_1 U_1 \\times_2 U_2 \\times_3 U_3,\\\\&\\hat{C}^{dec} = \\text{LayerNorm}(C^{dec} + MSA(C^{dec},C^{dec},C^{dec}, mask)),\\end{aligned}\\]\nSince we are only testing the effect of decomposing \\(X^{dec}\\) during self-attention computation in this approach, we will not be using \\(C^{dec}\\) for the rest of the computation. Thus we have\n\\[\\begin{aligned}&\\tilde{x}^{dec} = C^{dec} \\times_1 U_1 \\times_2 U_2 \\times_3 U_3,\\\\&\\hat{X}^{out} = \\text{LayerNorm}(\\tilde{X}^{dec} + MSA(X^{dec}, X^{enc}, X^{enc}, mask)),\\end{aligned}\\]\nwhere \\(\\hat{X}^{out}\\) is the output of the current decoder layer, \\(X^{enc}\\) is the input from the encoder layer. We used this new TEAFormer method in Informer on ETTh1, denoted as TEA-Informer-ENC-DEC. As shown in Table 2, TEA-Informer-ENC-DEC does not perform well across all window sizes on\nETTh1 dataset. On the contrary, it even performs worse than the baseline model. This result indicates that conducting attention operation on the core tensors throughout the entire model is not a feasible solution. We have two assumptions for why TEA-module cannot be implemented in decoder. First of all, as mentioned in Section 2, \\(X^{dec}\\) is essential for forecasting, and we argue that the reduced dimension of \\(C^{dec}\\) may not be sufficient to retain as much information as \\(X^{dec}\\). Moreover, the factor matrices \\(U_1\\), \\(U_2\\), \\(U_3\\) are not part of the learning process, thus it would not be accurate if we directly recombine the original \\(U_1\\), \\(U_2\\), \\(U_3\\) that have not learned anything with the trained core tensor \\(C^{dec}\\).\nApart from that, one key difference between the MSA modules in encoder and decoder is that attention mask is used in decoder. This attention mask is used to prevent the model from attending to certain positions in the input sequence, which is particularly important in time-series models, where the model should not have access to future tokens when predicting the current token. Theoretically, as a more abstract latent representation of the input tokens, it would be more difficult to learn with mask for a core tensor, and the prediction results would have more variance.\nTEA-Crossformer: Tensor expansion is a necessary step in input embedding as it allows the Transformer-based model to learn more key information. However, the larger expansion, the more computationally expensive the model becomes. In this part of ablation study, we want to test whether TEAFormer's usage of tensor compression techniques can reduce model's reliance on tensor size expansion. Crossformer [52] employs a segment-wise embedding, and manipulates data extensively using a two-stage attention architecture, in which attention is computed first along the time dimension then the feature dimension, such that hidden dimension is involved more frequently than our other baseline models, making it an ideal candidate for this part of study. It also implements a router mechanism that uses a fixed number of learnable matrix as intermediary to receive and send attention to reduce runtime complexity caused by computing attention across the batch. In this work, we only implement TEA-module in the cross-time stage where data is divided into segments along time dimension. We reduce the model's hidden dimension from 256 to 4 and obtain the following results."}, {"title": "Conclusions and Future Work", "content": "We have proposed TEAFormer, a novel approach to multi-dimensional time series forecasting tasks that leverages tensor decomposition techniques in transformer through a Tensor-Augmented attention module, and deploys such module in time-series transformer models. The embedded data is treated as tensor mode and size expansion transformation, and is then tucker-decomposed into a core tensor and factor matrices. We use the core tensor as an aggregation of latent cross-dimensional and temporal representations, which contains valuable information about the interactions and contributions within the data. Multi-head self-attention is computed on core tensor such that self-attention's computational cost would be significantly reduced. By implementing TEAFormers with Transformer, Informer, and Autoformer, we successfully enhanced the performance of the three baseline models, proving Tensor Augmentation's effectiveness.\nWe explored some potential directions for improvement and briefly discuss them: 1) While computing MSA on core tensor indeed improves efficiency, extra computational cost is produced when decompose and re-construct the tensor objects. Our study has indicated that passing the core tensor solely throughout the transformer architecture is not a feasible solution due to dimension mismatch, insufficient information retained, as well as its incompatibility with the attention mask mechanism. An alternative way of implementing tensor-augmentaion needs to be found in order to ensure optimized efficiency. 2) Our experiment of implementing TEAFormer with Crossformer suggests that tensor-augmentation module contributes significantly to learning with small data in short sequence forecasting tasks, given the minimal hidden dimension. This result demonstrates that TEAFormer has potential for performance enhancement when insufficient data or representation is given.\nThe potential of TEAFormer is not limited to our findings in this paper. The successful integration of tensor-augmented attention within established models underscores the versatility and adaptability of the TEAFormer methodology. This adaptability is particularly crucial in real-world applications where data characteristics and requirements can vary significantly. The ability of TEAFormer to maintain performance with reduced computational overhead highlights its practical value in resource-constrained environments, making advanced forecasting techniques more accessible and feasible across different industries.\nThe exploration of tensor-augmentation in handling small data and short sequence forecasting tasks further emphasizes its potential to revolutionize how models are trained and deployed, particularly in domains where data is limited or expensive to obtain. TEAFormer invites future research to delve deeper into the interplay between tensor decomposition and neural network architectures, potentially leading to more efficient and powerful models."}]}