{"title": "Early-exit Convolutional Neural Networks", "authors": ["Edanur Demir", "Emre Akbas"], "abstract": "This paper is aimed at developing a method that reduces the computational cost of convolutional neural networks (CNN) during inference. Conventionally, the input data pass through a fixed neural network architecture. However, easy examples can be classified at early stages of processing and conventional networks do not take this into account. In this paper, we introduce 'Early-exit CNNs', EENets for short, which adapt their computational cost based on the input by stopping the inference process at certain exit locations. In EENets, there are a number of exit blocks each of which consists of a confidence branch and a softmax branch. The confidence branch computes the confidence score of exiting (i.e. stopping the inference process) at that location; while the softmax branch outputs a classification probability vector. Both branches are learnable and their parameters are separate. During training of EENets, in addition to the classical classification loss, the computational cost of inference is taken into account as well. As a result, the network adapts its many confidence branches to the inputs so that less computation is spent for easy examples. Inference works as in conventional feed-forward networks, however, when the output of a confidence branch is larger than a certain threshold, the inference stops for that specific example. The idea of EENets is applicable to available CNN architectures such as ResNets. Through comprehensive experiments on MNIST, SVHN, CIFAR10 and Tiny-ImageNet datasets, we show that early-exit (EE) ResNets achieve similar accuracy with their non-EE versions while reducing the computational cost to 20% of the original. Code is available at https://github.com/eksuas/eenets.pytorch", "sections": [{"title": "1 Introduction", "content": "Deep neural networks are power-hungry. They typically need powerful processing units (i.e. GPU cards) in order to run in a reasonable amount of time. Reducing their computational cost with minimal degradation in accuracy is an important goal that has been approached from several different directions. One promising way to this end is to make the network adapt its computational cost to the input during inference. This idea has recently been explored in many ways. Researches have proposed early termination networks (Huang et al., 2017; Teerapittayanon et al., 2016; Berestizshevsky and Even, 2018; Figurnov et al., 2017; Panda et al., 2016), layer skipping networks (Veit and Belongie, 2018; Wang et al., 2018; Wu et al., 2018), specialized branches with wide networks (Mullapudi et al., 2018), adaptive neural trees (Tanno et al., 2018), cascaded networks (Bolukbasi et al., 2017) and pruning methods such as channel gating networks (Hua et al., 2018).\nConventionally, the input data pass through a fixed neural network architecture. However, easy examples can be classified at early stages of processing and conventional networks do not take this account. In order to reduce the computational cost, the methods mentioned above aim to adapt the computation graph of the network to the characteristics of the input instead of running the fixed model that is agnostic to the input. Our work in this paper can be categorized under the \"early termination networks\" category.\nIn this paper, we introduce \"Early-exit CNNS\", EENets for short, which adapt their computational cost based on the input itself by stopping the inference process at certain exit locations. Therefore, an input does not have to flow through all the layers of the fixed network; on the contrary, the computational cost can be significantly decreased based on the characteristics of inputs. In EENets, there are a number of exit blocks each of which consists of a confidence branch and a softmax classification branch. The confidence branch computes the confidence score of exiting (i.e. stopping the inference process) at that location; while the softmax branch outputs a classification probability vector. Both branches are trainable and they are independent of each other. These exit blocks constitute a very small part of the overall network (e.g. a single exit blocks constitutes ~0.0002% of the parameters in a EENet-110 (akin to ResNet-110) designed for a 10-class dataset). In short, the additional parameters coming from early-exit blocks do not significantly increase the computational cost, hence, they can be ignored.\nDuring training of EENets, in addition to the classical classification loss, the computational cost of inference is taken into account as well. As a result, the network adapts its confidence branches to the inputs so that less computational resources are spent for easy examples. That is, when the confidence (i.e. $h_i$ in Fig. 1) at a certain exit block is larger than a predetermined threshold, the inference process stops for that specific example.\nDefining a proper way to train a EENet network is important because the model could be biased towards wrong decisions such as early or late termination. These wrong decisions could either decrease accuracy or increase the computational cost unnecessarily. Deciding at which point an input can be classified and the execution can be terminated is the key challenge of the problem. To address this problem, we propose a novel loss function which balances computational cost and classification loss in a single expression enabling the training of the base neural network and all the exit blocks simultaneously.\nOur experiments show that EENets achieve similar accuracy compared to their counterpart ResNets (He et al., 2016) with relative computational costs of 30% on SVHN (Netzer et al., 2011), 20% on CIFAR10 (Krizhevsky, 2009) and 42% on Tiny-ImageNet (Fei-Fei et al., 2014) datasets."}, {"title": "1.1 Contributions", "content": "In the context of previous related work, our contributions with the introduction of EENets are as follows:\n*   EENet has a single stage training as opposed similar previous work which are trained in multiple stages.\n*   EENets are compact models not requiring additional hyper parameters such as non-termination penalty or confidence threshold variables.\n*   The confidence scores of EENets are learnable and they do not depend on heuristic calculations. As a consequence, their initialization is not an issue and they can be initialized just like other parameters of the network.\n*   Our loss function considers both accuracy and cost simultaneously and provides a trade-off between them via an hyper-parameter.\n*   All exit blocks of an EENet are fed by all inputs even if some inputs are classified in early stages of the model. This avoids a possible dead unit problem (which is a frequent problem in previous work) where some layers are not trained at all."}, {"title": "2 Related Work", "content": "Neural networks that adapt their computations based on the input's characteristic can be examined in the following main categories: early termination networks, layer skipping networks, specialized branches with wide networks, neural trees, cascaded networks and pruning methods such as channel gating networks. In addition, some studies focus on the confidence degree of image classification (Wan et al., 2018)."}, {"title": "2.1 Early Termination Networks", "content": "Early termination network are based on the idea that it might not be necessary to run the whole network for some inputs. Similar to EENets, early termination networks (Huang et al., 2017; Teerapittayanon et al., 2016; Berestizshevsky and Even, 2018; Figurnov et al., 2017; Panda et al., 2016) have multiple exit blocks that allow early termination based on an input's characteristics. All of these studies have some kind of confidence scores to decide early termination.\nOne of the early termination networks, BranchyNets (Teerapittayanon et al., 2016) have multiple exit blocks each of which consists of a few convolutional layers followed by a classifier with softmax. In other words, BranchyNets have one head just for classification at their exit blocks. The exit blocks of Berestizshevsky and Even (2018)'s model are composed of pooling, two fully-connected (FCs) and batch normalization layers. Like BranchyNets, one conventional head at an exit block is trained for classification. The confidence scores are derived via some heuristics. In the training procedure of the model of Beretizshevsky and Even, the weights of only convolutional and the last FC layers are firstly optimized. Later, the remaining FC layers are optimized, one by one. On the other hand, MSDNets (Huang et al., 2017) have multi-scaled features with dense connectivity among them. Exits of MSDNets consist of two convolutional layers followed by a pooling and a linear layer. However, similar to BranchyNets, MSDNets do not have confidence branches at their exit blocks.\nIn these models, the confidence scores are derived from the predicted classification results (i.e. the maximum over the softmax). Because such confidence scores are not learnable, deciding on the termination criteria or threshold of an exit branch becomes an important issue. The exit threshold providing the maximum accuracy should be empirically discovered in these models. Unlike EENets, the loss functions of these models do not encourage an early-exit by considering the computational cost. In addition, they have a dead layer problem coming from improper initialization of the confidence scores in the training. The scores may be biased to exit always early and deeper layers may not receive learning signals properly. To avoid this situation, the model of Berestizshevsky and Even (2018) use a multi-stage training. In the first stage, it optimizes all the convolutional weights together with the weights of the last FC layers. After that, it optimizes the weights of the remaining FC components, one by one.\nSpatially Adaptive Computation Time for Residual Networks, shortly SACTs (Figurnov et al., 2017), is another study in the category. Exit blocks of the model consist of a pooling and a fully-connected layer followed by a sigmoid function like our model. However, the final confidence score of early termination (namely \"halting score\" in their paper) is calculated by the cumulative learnable scores of the previous exit blocks. As soon as the cumulative halting score reaches a constant threshold (i.e. $T > 1.0$), the computation is terminated. Unlike EENets, the classification output vector of SACTS (i.e. the output of the softmax branch) is derived from weighted summation of the inputs of the confidence branches so far. While EENets directly train the confidence scores by taking them into account in the loss function, SACTs employ the number of executed layers as non-termination penalty in the loss function. Another work, Conditional Deep Learning (CDL) (Panda et al., 2016) has multiple exit blocks each of which consists of just a linear classifier. Starting from the first layer, linear classifiers are added to the end of each convolutional layer iteratively as long as this addition process does not decrease the accuracy. In CDL, a user defined threshold is used to decide if the model is sufficiently confident to exit. The training procedures of SACTs and CDLs are also multi-stage."}, {"title": "2.2 Layer Skipping Networks", "content": "Layer skipping networks (Veit and Belongie, 2018; Wang et al., 2018; Wu et al., 2018) adapt their computation to the input by selectively skipping layers. In these networks, a gating mechanism determines, for a specific input, whether the execution of the layer can be skipped. The main challenge here is learning the discrete decisions of the gates. AdaNets (Veit and Belongie, 2018) use Gumbel Sampling (Jang et al., 2016) while SkipNets (Wang et al., 2018) and BlockDrop (Wu et al., 2018) apply reinforcement learning to this end. None of these models has a separate confidence branch at the gate blocks. Similar to the early-exit blocks of early termination nets, the gates of the layer skipping networks may die and lose their functionality if they incline to be too much turned off during training. Thus, the actual capacity usage decreases. On the other hand, if the gates tend to be turned on, the networks cannot reduce computational cost effectively. As a result, the networks can not only perform as counterpart static models but also spend additional computational cost for the gate functions (i.e. the same capacity with more cost). In order to avoid such cases, the gate blocks require to be initialized carefully and trained properly. Thus, models in this category (i.e. layer skipping networks) have a complicated multi-stage training."}, {"title": "2.3 Specialized Branches with Wide Networks", "content": "As wide networks, HydraNets (Mullapudi et al., 2018) is another approach in the area. HydraNets contain distinct branches specialized in visually similar classes. HydraNets possess a single gate and a combiner. The gate decides which branches to be executed at inference. And the combiner aggregates features from multiple branches to make a final prediction. In training, given a subtask partitioning (i.e. dividing dataset into visually similar classes), the gate and the combiner of the HydraNets are trained jointly. The branches are indirectly supervised by the classification predictions after combining the features computed by the top-k branches."}, {"title": "2.4 Neural Trees", "content": "Adaptive Neural Trees, ANTs (Tanno et al., 2018), can be considered as a combination of decision trees (DTs) with deep neural networks (DNNs). It includes the features of the conditional computation of DTs with the hierarchical representation learning and gradient descent optimization of DNNS. ANTs learn routing functions of a decision tree thanks to the training feature of DNNs. While doing this, instead of a classical entropy, ANTs use stochastic routing, where the binary decision is sampled from Bernoulli distribution with mean $r^\\phi(x)$ for input $x$ ($r^\\phi$ can be a small CNN). However, ANTs are trained in two stages: growth phase during which the model is trained based on local optimization and refinement phase which further tunes the parameters of the model based on global optimization."}, {"title": "2.5 Cascaded Networks", "content": "Some other approaches focus on cascaded systems. The model by Bolukbasi et al. (2017) adaptively chooses a deep network among the-state-of-arts such as AlexNet (Krizhevsky et al., 2012), GoogleNet (Szegedy et al., 2014), and ResNet (He et al., 2016) to be executed per example. Each convolutional layer is followed by the decision function to choose a network. But it is hard to decide if termination should be performed just by considering a convolutional layer without employing any classifier. It has a multi-stage training procedure where the gates are trained independently from the rest of the model."}, {"title": "2.6 Pruning Methods", "content": "Channel Gating Neural Networks (Hua et al., 2018) dynamically prune computation on a subset of input channels. Based on the first p channels, a gate decides whether to mask the rest of the channels. Similar to SACTS (Figurnov et al., 2017), when the classification confidence score reaches a threshold, the remaining channels are not computed."}, {"title": "2.7 Novelties of EENets", "content": "As discussed above, many models from different categories have the dead layer/unit problem. In EENets, we avoid this problem with our novel loss function (described in Section 3) which enables the training of all exit blocks by all inputs, even if some inputs are classified in the early stages of the model.\nAnother contribution of EENets is the separate confidence branches at their exit blocks. Unlike most of the previous adaptive computational approaches, the confidence scores of EENets are trainable and do not depend on heuristic calculations. Having separate learnable parameters allows the confidence branches to be not biased towards classification results. Their initialization is not an issue and they can be initialized just like other parameters of the network. This separate confidence branches approach makes EENets easier to use/train compared to the previous work.\nAnother novelty of EENets is the loss function that takes both accuracy and the cost spending into account simultaneously and provides a trade-off between them through the confidence scores. In contrast to most of the previous studies, our cost values employed in the loss function are not hyper-parameters but are based on the actual number of floating-point operations. Unlike most of the previous studies, EENets have a single stage training in spite of having multiple exit-blocks. EENets do not require additional hyper-parameters such as non-termination penalty or confidence threshold variables."}, {"title": "3 Model", "content": "In this section, we describe the architecture of EENets, the details about types of exit blocks, how to distribute early-exit blocks to a network, feed-forward and backward phases of the model and the proposed loss function."}, {"title": "3.1 Architecture", "content": "Any given convolutional neural network (CNN) can be converted to an early-exit network by adding early-exit blocks at desired locations. To achieve this, first, one has to decide how many exit-blocks are going to be used. This is a design choice. Next, the locations where to connect the exit-blocks need to be decided. We propose various ways of doing this in Section 3.4. Finally, one needs to decide which type of exit-blocks to use. In the following paragraphs, we describe three different types of exit-blocks.\nEach EE-block consists of two fully-connected (FC) heads, namely the confidence branch and the classification softmax branch. Both take the same channel-based feature maps (from the previous layer) as input.\nWe define three types of early-exit blocks, namely, Plain, Pool and Bnpool. The Plain-type exit is composed of two separate fully-connected (FC) layers and input feature maps are directly fed to these FC branches. The Pool exit has a global average pooling layer before the FC branches. Lastly, the Bnpool-type exit block consists of a batch normalization layer followed by a ReLU activation and a global average pooling layer before the FC confidence and classification branches."}, {"title": "3.2 Inference", "content": "The Early-exit Convolutional Neural Networks (EENets) have a certain threshold in order to decide early termination in the inference procedure. If the confidence score of an early-exit block is above the threshold, the classification results of the current stage will be the final prediction. Each input is classified based on their individual confidence scores predicted by the early-exit blocks. Thus, one input can be classified and terminated early while others continue being processed by the model.\nEarly termination threshold is $T = 0.5$. It is the midpoint of the output range of the sigmoid function (used by the confidence branches). The threshold is employed only in the inference phase. During training, all examples are processed by the entire network; thus, all early-exit blocks contribute to the loss function (see Section 3.3) for all examples even if some of them can be classified early.\nThe pseudo-code of the inference procedure of EENets is given in Algorithm 1 where $EEBlock_i$ represents the $i^{th}$ early-exit (EE) block of the model and $CNN\\_Layers_i$ denotes the sequence of intermediate blocks (CNN layers) between $(i - 1)^{th}$ EE-block and $i^{th}$ EE-block. $CNN\\_Layers_0$ is the initial CNN layers of the model before entering any EE-block. $N$ denotes the total number of early-exit blocks. $h_i$ and $\\hat{y_i}$ shows the confidence score and classification output vector of $i^{th}$ EE-block."}, {"title": "3.3 Training", "content": "During training, the goal is to learn the parameters of the CNN and all the early-exit blocks simultaneously so that an input is processed minimally on average to predict its label correctly. This leads us to combine both losses in a single loss function:\n$L = L_{MC} + \\lambda L_{Cost}$\nwhere $L_{MC}$ is the multi-class classification loss, $L_{Cost}$ is the computational cost and $\\lambda$ is a trade-off parameter between accuracy and cost.\nLet $\\hat{y_i}$ be the classification vector output by the $i^{th}$ early-exit block and $c_i$ be the computational cost of the network, measured in number of floating-point operations (FLOPs), up to this early-exit block. The inference procedure (Section 3.2) dictates the following final classification output vector:\n$\\hat{y} = I_{\\{h_0 \\geq T\\}} \\hat{y_0} + I_{\\{h_0 < T\\}}\\{I_{\\{h_1 > T\\}} \\hat{y_1} + I_{\\{h_1 < T\\}}\\{... I_{\\{h_{N-1} > T\\}} \\hat{y_{N-1}} + I_{\\{h_{N-1} < T\\}} \\hat{y_N}...\\}\\}$\nwhere $I\\{\\}$ is the indicator function and $N$ is the number of early-exit blocks. $\\hat{y_n}$ denotes the final softmax output of the CNN (it is not the output of an early-exit block). \nWe cannot directly use the expression in Eq. (4) for training because it is not differentiable due to the indicator functions. The indicator function can be approximated by the sigmoid function, and because our confidence scores ($h_i$) are produced by sigmoid activation functions, we obtain the following soft classification output vector:\n$\\hat{Y_0} = h_0y_0 + (1 - h_0)\\{h_1y_1 + (1 - h_1)\\{...h_{N-1}\\hat{y_{N-1}} + (1 - h_{N-1})\\hat{y_N}...\\}\\}$\nwhich can be more conveniently expressed as a recursive formula:\n$\\hat{Y_i} = h_i\\hat{y_i} + (1 - h_i)\\hat{Y_{i+1}} \\forall i = 0, 1, ..., N-1\\ \n\\hat{Y_N} = \\hat{y_N}.$\nWe can similarly define the soft version of the computational cost as:\n$C_i = h_ic_i + (1 - h_i)C_{i+1}$ \n$C_N = c_N$\nwhere $c_N$ denotes the computational cost of the whole network from start to the final softmax output.\nGiven the definitions above, we can finally write the first version of our loss function:\n$L_{v1} = CE(y, \\hat{Y_0}) + C_0$\nwhere $CE()$ is the cross-entropy loss and $y$ denotes the ground-truth label.\nThe problem with $L_{v1}$ is that, due to the recursive natures of $\\hat{Y_0}$ and $C_0$, the later an early-exit block, the smaller its contribution to the overall loss. To see this, consider the multiplicative factor of $\\hat{y_n}$ in Eq. (5): $\\prod_{i=0}^{N-1}(1 - h_i)$. Since each $h_i \\in [0,1]$, as $i$ grows (i.e. going deeper), the contribution of early-exit block i to the overall loss goes down, consequently, it receives less and less supervisory signal. In our experiments, we observed that EENets trained using $L_{v1}$ showed little diversity in the exit blocks preferred by the inputs and an early stage exit-block (small i) was dominant. Hence, EENets trained with $L_{v1}$ performed poorly.\nTo address the shortcoming of $L_{v1}$, we consider the exit block from which the input would possibly not exit as a latent variable and minimize an expected loss over it.\nSuppose for a specific input, we knew upto which early-exit block it would not exit. For example, if we knew that a specific input would exit at the final output of the CNN (therefore, it will not exit from any of the early-exit blocks), then, for this example, it would be sufficient to consider only the loss term related to $\\hat{y_N}$, and ignore the loss term related to earlier exits. Similarly, if we knew that an example would not exit from early-exit block 0, then we would not add the losses related to this block into the overall loss.\nHowever, we do not apriori know from which early-exit block a specific example would exit (or not exit). For this reason, we can consider the index of the block from which the example would pass (without exiting) as a latent variable. If we assume a uniform prior over all exit blocks, minimizing the expected value of the loss over this latent variable, we arrive at:\n$L_{v2} = \\sum_{i=0}^{N}(CE(y, \\hat{Y_i}) + \\lambda C_i).$\nWe propose to use $L_{v2}$ to train EENets, where each early-exit block has a chance to contribute to the loss and, hence, receive supervision signal.\nOverall, all exit blocks contribute to the loss function for all examples, even if easy examples can be classified at the earlier early-exit blocks. Multiple outputs coming from all exit blocks are trained jointly in $L_{v2}$. Thanks to our novel loss function, $L_{v2}$, (i) EENets avoid the dead layer problem occurring in many previous work (Huang et al., 2017; Teerapittayanon et al., 2016; Berestizshevsky and Even, 2018), and (ii) EENets do not require a complicated multi-stage training process neither."}, {"title": "3.4 Distributing Early-exit Blocks to a Network", "content": "The number of early-exit blocks (EE-blocks) and how they are distributed over the base CNN are other important factors in the architecture of EENets. The additional parameters introduced by the EE-blocks are very small and usually negligible (e.g. ~0.0002% of the total parameters of EENet-110 for each EE-block on a 10-class datasets), the early-exit blocks can be added as much as desired.\nWe propose five different ways for determining where a given number of EE-blocks should be added in a given base CNN: (i) Pareto, (ii) Golden Ratio, (iii) Fine, (iv) Linear and (v) Quadratic. According to the Pareto principle, 80% of the results come from 20% of the work. Our Pareto distribution is inspired by this principle: the first EE-block splits the network according to the Pareto principle where 20% of the total computational cost is calculated in terms of the number of floating-point operations (FLOPs). Similarly, the second EE-block splits the rest of the network (i.e. starting right after the first EE-block until the end) again into 20%-80%. This pattern continues until all EE-blocks are added. In the Fine distribution method, each EE-block divides the network at 5%-95% based on the total FLOPs. The Golden ratio distribution uses the golden ratio, 61.8%-38.2%.\nThe Linear and Quadratic distributions split the network in such a way that the computational cost of the layers between two consecutive EE-blocks increases in linear or quadratic form, respectively. Note that there is not a best distribution method for all EENets or datasets. The effects of the distribution method used should be observed empirically on the specific problem."}, {"title": "4 Experiments", "content": "In our experiments, we chose ResNets (He et al., 2016) as our base CNNs for their widespread use (although, our early-exit blocks can be applied to any CNN architecture). We obtained early-exit (EE) versions of ResNets and compared their performance to that of non-EE (i.e. original) versions on MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky, 2009), SVHN (Netzer et al., 2011) and Tiny-ImageNet (Fei-Fei et al., 2014) datasets. In addition to ResNets, we also experimented with a small, custom CNN on the MNIST dataset.\nThe experiments are diversified in order to observe the effects of EENets in different aspects and certain conditions. In this section, we try to answer the following questions through comprehensive experiments:\n*   Do EENets really work? That is, is the inference process terminated for individual examples at different exit locations? Is there a variety in the exit locations chosen per example by the network?\n*   Are EENets successful when compared to their counterparts in terms of computational cost and accuracy?\n*   Which type of early-exit block yield better results?\n*   How does the distribution of early-exit blocks affect the accuracy and the computational cost?\nWe conducted our experiments on a machine with a i7-6700HQ CPU processor with 16GB RAM and two NVIDIA Tesla PICe P100 16GB. We implemented EENets both in two different frameworks (Keras (v2.1.5) and PyTorch (v1.0.1)) to verify behavior and performance. We chose PyTorch for its flexibility. Both Keras and PyTorch implementations are available at GitHub.\nUnless otherwise noted, all results reported in this section were produced by the PyTorch code.\nIn MNIST (LeCun et al., 1998) and Tiny-ImageNet (Fei-Fei et al., 2014) experiments, the models were optimized by Adam with learning rate = 0.001. The mini-batch size in the experiments was 32. Most of the models were trained up to 200 epochs unless otherwise stated.\nOn SVHN (Netzer et al., 2011) and CIFAR10 datasets (Krizhevsky, 2009), we trained the models using the configurations given in the ResNet paper (He et al., 2016). In these experiments, we used SGD with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 per 100 epochs. The models were trained for up to 350 epochs. We used weight decay = 0.0001 and momentum = 0.9."}, {"title": "4.1 Experimented Architectures", "content": "We added early-exit blocks to ResNet models (He et al., 2016) with both basic and bottleneck architectures. The early-exit (EE) ResNets based on bottleneck architectures consist of 50, 101 and 152 layers. By modifying the 6n+2 layers ResNet models (He et al., 2016), we have constructed 20, 32, 44 and 110 layers EENets. Various numbers of early-exit blocks were distributed based on the capacity of the models. The models which have a large capacity were trained on CIFAR10 (Krizhevsky, 2009) and SVHN (Netzer et al., 2011). In addition, the early-exit version of Naive ResNets such as EENet-18 were evaluated on Tiny-ImageNet (Fei-Fei et al., 2014).\nOn the other hand, the models having a smaller capacity were evaluated on MNIST (LeCun et al., 1998) to observe how EENets perform in the situation of a dataset forcing the capacity of the model. These small capacity networks are composed of 6, 8 and 18 layers with a small number of filters.\nSome of the ResNet based architectures that are evaluated in our experiments are shown in Our own design EENet-8 is a very small CNN having between 2 to 8 filters in its layers. We ran this low capacity model on the MNIST dataset."}, {"title": "4.2 Results on MNIST", "content": "First, we performed a set of experiments on the MNIST dataset (LeCun et al., 1998) to see if the confidence scores are meaningful (i.e. they are related to the accuracy of the predictions made by these early-exit blocks) and if they have a variety in inputs. In these basic tests, the EENet-8 model was employed with quadratically distributed two Pool-type early-exit blocks whose number of floating-point operations (FLOPs) and costs are given in Table 2. The exit distribution of the MNIST examples on the EENet-8 models trained with different loss functions are given in Table 2 as well. We used $\\lambda$ = 1 in these experiments.\nAs expected, the model EENet-8-LCost terminates the executions at the first early-exit block by considering only the computational cost while EENet-8-LMC classifies all examples at the last exit block to get the highest accuracy. On the other hand, the EENet-8 model trained with Lv2 takes both the cost and accuracy into account; as a consequence, it maintains the accuracy by spending less computational cost (0.82 of the original). EENet-8-Lv1 performs poorly as expected (see Section 3.3 for the discussion). Moreover, we observe that examples exit at a variety of locations (Table 2): 47, 2247 and 7706 numbers of test examples of MNIST are classified at the early-exit (EE) block-0, EE-block-1 and the last exit layer of the EENet-8 model, respectively. This experiment shows that our loss function, $L_{v2}$, performs as expected and maintains the balance between the accuracy and computational cost.\nThe computational cost, accuracy and loss values per epoch are We evaluate the model with different optimizers and learning rates. Adam optimizer with learning rate 0.001 gives the best results.\nWe performed another set of experiments on MNIST to observe the effects of A trade-off on the loss function $L_{v2}$. The results are presented in Table 3 and Figure 7.\nThe best balance between the accuracy and the computational cost is observed with $\\lambda$ = 0.95. However, the effects of $L_{MC}$ or $L_{Cost}$ can be changed through the A trade-off if more accurate results or less computational cost consumption are desired (e.g. A can be decreased to obtain more accurate results if the computational cost is not an issue).\nRandom MNIST examples classified with EENet-8 which consists of two early-exit (EE) blocks are shown in Figures 8, 9 and 10 as classified at the EE-block-0, EE-block-1 and the last exit blocks of the model, respectively. We observe that the early-exit blocks are specialized in visually similar examples of the same class or in a few visually similar classes. For example, the EE-block-0 is only specialized in the class of the digit eight in this model (i.e. visually similar examples of the same class). On the other hand, the EE-block-1 seems to be specialized in digits one, four and seven. Note that these classes are visually similar as well.\nFinally, we tested different distributions of the early-exit blocks. We observe that keeping the first early-exit blocks in the very beginning of the model decreases the cost excessively. Due to this, the Quadratic distribution with a small number of EE-blocks can be a good choice in this situation."}, {"title": "4.3 Results on Tiny-ImageNet", "content": "We evaluated EENet-18 on the Tiny-ImageNet dataset (Fei-Fei et al., 2014) which consists of 200 classes with 500 training and 50 validation images (down-sampled to 64-by-64) per class, from the original ImageNet dataset (Russakovsky"}]}