{"title": "Simplex-enabled Safe Continual Learning Machine", "authors": ["Yihao Cai", "Yanbing Mao", "Hongpeng Cao", "Lui Sha", "Marco Caccamo"], "abstract": "This paper proposes the SeC-Learning Machine: Simplex-enabled safe continual learning for safety-critical autonomous systems. The SeC-learning machine is built on Simplex logic (that is, \u201cusing simplicity to control complexity\") and physics-regulated deep reinforcement learning (Phy-DRL). The SeC-learning machine thus constitutes HP (high performance)-Student, HA (high assurance)-Teacher, and Coordinator. Specifically, the HP-Student is a pre-trained high-performance but not fully verified Phy-DRL, continuing to learn in a real plant to tune the action policy to be safe. In contrast, the HA-Teacher is a mission-reduced, physics-model-based, and verified design. As a complementary, HA-Teacher has two missions: backing up safety and correcting unsafe learning. The Coordinator triggers the interaction and the switch between HP-Student and HA-Teacher. Powered by the three interactive components, the SeC-learning machine can i) assure lifetime safety (i.e., safety guarantee in any continual-learning stage, regardless of HP-Student's success or convergence), ii) address the Sim2Real gap, and iii) learn to tolerate unknown unknowns in real plants. The experiments on a cart-pole system and a real quadruped robot demonstrate the distinguished features of the SeC-learning machine, compared with continual learning built on state-of-the-art safe DRL frameworks with approaches to addressing the Sim2Real gap.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (DRL) has been integrated into many autonomous systems (see exampels in Figure 1) and have demonstrated breakthroughs in sequential and complex decision-making in broad areas, ranging from autonomous driving [1, 2] to chemical processes [3, 4] to robot locomotion [5, 6]. Such learning-integrated systems promise to revolutionize many processes in different industries with tangible economic impact [7, 8]. However, the public-facing AI incident database [9] reveals that machine learning (ML) techniques, including DRL, can deliver remarkably high performance but no safety assurance [10]. Hence, the high-performance DRL with verifiable safety assurance is even more vital today, aligning well with the market's need for safe ML technologies."}, {"title": "1.1 Related Work on Safe DRL", "content": "Significant efforts have been devoted to promoting safe DRL in recent years, including developing safety-embedded rewards and residual action policies and deriving verifiable safety, as detailed below.\nThe safety-embedded reward is crucial for a DRL agent to learn a high-performance action policy with verifiable safety. The control Lyapunov function (CLF) is the potential safety-embedded reward [11, 12, 13, 14]. Meanwhile, the seminal work [15] revealed that a CLF-like reward can enable DRL with verifiable stability. At the same time, enabling verifiable safety is achievable by extending CLF-like rewards with given safety conditions or regulations. However, systematic guidance for constructing such CLF-like rewards remains open.\nThe residual action policy is another shift in the focus of safe DRL, which integrates data-driven DRL action policy and physics-model-based action policy. The existing residual diagrams focus on stability guarantee [16, 17, 18, 19], with the exception being [20] on safety guarantee. However, the physics models considered are nonlinear and intractable, which thwarts delivering a verifiable safety guarantee or assurance, if not impossible.\nThe verifiable safety (i.e., having verifiable conditions of safety guarantee) is enabled in the recently developed Phy-DRL (physics-regulated DRL) framework [21, 22]. Meanwhile, Phy-DRL can address the aforementioned open problems. Summarily, Phy-DRL simplifies a nonlinear system dynamics model as an analyzable and tractable linear one. This linear model can then be a model-based guide for constructing the safety-embedded (CLF-like) reward and residual action policy."}, {"title": "1.2 Challenges and Open Problems", "content": "Although safe DRL has developed significantly, DRL-enabled autonomous systems still face formidable safety challenges, rooting in the Sim2Real gap and unknown unknowns on real plants.\nChallenge 1: Sim2Real Gap. Due to the expense of data acquisition and potential safety concerns in real-world settings, the prevalent DRL involves training a policy within a simulator using synthetic data and deploying it onto the physical platforms. The discrepancy between the simulated environment and the real scenario thus leads to the Sim2Real gap that degrades the performance of the pre-trained DRL in a real plant. Numerous approaches have been developed to address Sim2Real gap [23, 24, 25, 26, 27, 28, 29, 30, 31]. The common aim of these approaches is to enhance realism in the simulator through, for example, domain randomization [32] and delay randomization (mimicking asynchronous communication and sampling) [28]. These approaches can address the Sim2Real gap to different degrees. However, the unrevealed gap still persistently impedes safety assurance if DRL (also other ML) is not trained or learning in the real plant in real environments, using real-time data.\nChallenge 2: Unknown Unknowns. The unknown unknowns generally refer to outcomes, events, circumstances, or consequences that are not known in advance and cannot be predicted in time and distributions [33]. The dynamics of many learning-enabled systems (e.g., autonomous vehicles [34] and airplanes [35]) are governed by conjunctive known knowns (e.g., Newton's laws of motion), known unknowns (e.g., Gaussian noise without knowing mean and variance), and unknown unknowns (due to, for example, unforeseen operating environments and DNNs' huge parameter space, intractable activation and hard-to-verify). The safety assurance also requires resilience to unknown unknowns, which is very challenging. The reasons root in characteristics of unknown unknowns: almost zero historical data and unpredictable time and distributions, leading to unavailable models for scientific discoveries and understanding."}, {"title": "1.3 Contribution: Simplex-enabled Safe Continual Learning Machine", "content": "If successful, continual learning in a real plant can directly address the Sim2Real gap and learn to tolerate unknown unknowns. However, the DRL's real-time action policy during continual learning cannot be fully verified and can have software faults. Continual learning shall run on a fault-tolerant architecture to address this safety concern. Simplex \u2013 using simplicity to control complexity [36, 37] is a successful software architecture for complex safety-critical autonomous systems. The core of Simplex uses verified and simplified high-assurance controller to control the unverified high-performance and complex controller. Meanwhile, recently developed Phy-DRL theoretically and experimentally features fast training with verifiable safety [21, 22]. These motivate us to develop the Simplex-enabled safe continual learning (SeC-learning) machine to address Problem 1.1 and Problem 1.2, which is built on Simplex architecture and Phy-DRL. As shown in Figure 1, the SeC-learning machine constitutes HP (high performance)-Student, HA (high assurance)-Teacher, and coordinator. The HP-Student is a pre-trained Phy-DRL and continues to learn to tune the action policy to be safe in a real plant. The HA-Teacher action is a verified, mission-reduced, and physics-model-based design. As a complementary, HA-Teacher has two missions: backing up safety and teaching to correct unsafe learning of HP-Student. The coordinator triggers the switch and the interaction between the HP-Student and HA-Teacher to ensure lifetime safety (i.e., safety guarantee in any stage of continual learning, regardless of HP-Student's success or convergence)."}, {"title": "2 Preliminaries: Safety Definition", "content": "The dynamics of a real plant can be described by\n$$s(k + 1) = As(k) + Ba(k) + f(s(k), a(k)), k\\in\\mathbb{N}$$\nwhere $f(s(k), a(k)) \\in \\mathbb{R}^n$ is the unknown model mismatch, $A \\in \\mathbb{R}^{n\\times n}$ and $B \\in \\mathbb{R}^{n\\times m}$ denote known system matrix and control structure matrix, respectively, $s(k) \\in \\mathbb{R}^n$ is real-time system state of real plant, $a(k) \\in \\mathbb{R}^m$ is real-time action from SeC-learning machine.\nThe actions of the SeC-learning machine aims to constrain the states of a real plant to the safety set:\nSafety set : $X \\triangleq \\{s\\in\\mathbb{R}^n | \\underline{v} \\leq D\\cdot s \\leq \\overline{v}, D \\in \\mathbb{R}^{h\\times n}, \\text{ with } \\underline{v}, \\overline{v}, v \\in\\mathbb{R}^h\\}.\nwhere $D, \\underline{v}, \\overline{v}$ and $v$ are given in advance for formulating $h\\in \\mathbb{N}$ safety conditions. In the SeC-learning machine, the safety set is not directly used to embed high-dimensional safety conditions (indicated by $h\\in \\mathbb{N}$ in Equation (2)) into the DRL reward since the reward is a real one-dimensional"}, {"title": "3 Design Overview: SeC-Learning Machine", "content": "The proposed SeC-learning machine aims to address Problem 1.1 and Problem 1.2 with capabilities of assuring lifetime safety, addressing the Sim2Real gap, and tolerating unknown unknowns. To do so, as shown in Figure 1, the learning machine is designed to have three critical interactive components:\n\u2022 HP-Student is a pre-trained Phy-DRL (physics-regulated DRL [21, 22]) model and contin-ues to learn in a safety-critical real plant to tune his action policy to be safe.\n\u2022 HA-Teacher is a verifiable and analyzable physics-model-based action policy with two missions: backing up the safety of a real plant and correcting unsafe learning of HP-Student.\n\u2022 Coordinator triggers the switch and interaction between HP-Student and HA-Teacher by monitoring the real-time system states. Specifically, when the real-time states of the real plant under the control of HP-Student approach the safety boundary, the coordinator triggers the switch to HA-Teacher and the correction of unsafe actions in learning. In other words, the HA-Teacher takes over the HP-Student to control the real plant to safe (i.e., backing up safety). Meanwhile, the HA-Teacher uses his safe actions to correct the HP-Student's unsafe actions in the replay buffer for learning. Once the real-time states return to a safe region, the coordinator triggers the switch back to HP-Student and terminates the learning correction.\nNext, we detail the designs of the three interactive components in Sections 4 to 6, respectively."}, {"title": "4 SeC-Learning Machine: HP-Student Component", "content": "The HP-Student builds on Phy-DRL (physics-regulated deep reinforcement learning) proposed in [21, 22]. The critical reason is that Phy-DRL's training mission is pre-defined: searching for an action policy that renders the assigned safety envelope invariant. In this way, HP-Student can share his mission with HP-Student and Coordinator, so they can have a common goal in the learning machine: rendering the safety envelope invariant in a safety-critical real plant in the face of Sim2Real gap and unknown unknowns. We next detail the designs of HP-Student."}, {"title": "4.1 HP-Student: Residual Action Policy and Safety-embedded Reward", "content": "Following Phy-DRL in [21, 22], the HP-Student adopts the concurrent residual action policy and safety-embedded reward, as they can offer fast and stable training and successfully encode the safety envelope \u03a9. The residual action formula is\n$$a_{HP}(k) = a_{drl}(k) + a_{phy} (k) (:= F \\cdot s(k)),$$\ndata-driven\\ model-based\nwhere $a_{drl}(k)$ denotes a date-driven action from DRL, while $a_{phy}(k)$ is a physics-model-based action (F is our design). Meanwhile, the safety-embedded reward:\n$$R(\\vec{s}(k), a_{drl}(k)) = s^T(k) \\cdot H \\cdot s(k) \u2013 s^T(k + 1) \\cdot P \\cdot s(k+1) + w(s(k), a_{HP}(k)),$$\n$$r(s(k), s(k+1))$$\nwhere the sub-reward $w(s(k), a(k))$ aims at high-performance operations (e.g., minimizing energy consumption of resource-limited robots [39, 40]). In contrast, the sub-reward $r(s(k), s(k + 1))$ is safety-critical. Equation (5) also defines:\n$$H\\triangleq A^T\\cdot P\\cdot A, with A\\triangleq A + B\\cdot F and 0\\preceq H\\preceq\\alpha\\cdot P, \\alpha\\in (0,1).$$\nThe matrices P and F are design variables, using the available physics-model knowledge (A, B). Their automatic computations will be discussed in Section 4.2."}, {"title": "4.2 HP-Student: Controllable Contribution Ratio", "content": "In residual diagram (4), the contribution ratio between data-driven and model-based policies controls HP-Student's safety and system performance. To understand this, we define the contribution ratio as $\\gamma = \\frac{space\\{|a_{drl} (k)|,\\forall k\\in\\mathbb{N}\\} }{space\\{|a_{drl}(k)|,\\forall k\\in\\mathbb{N}\\} + space\\{|a_{phy}(k)|,\\forall k\\in\\mathbb{N}\\}}$. As depicted in Figure 2, if $\\gamma$ approaches 0, i.e., the physics-model-based action policy dominates the integration, featuring analyzable and verifiable behavior but limited performance. If $\\gamma$ approaches 1, i.e., the data-driven policy dominates the integration, featuring high performance but hard-to-analyze and hard-to-verify. Therefore, a controllable contribution ratio is desired. The controllable space of data-driven actions is innate, as Phy-DRL is built on DDPG [41], which directly maps states to actions within the interval [-1,1] using the Tanh activation function. The action space can be rescaled with a controllable magnitude factor m to expand the space to [-m, m]. So, the remaining job is to enable the controllable space of physics-model-based actions:\nAction Space: $A_{phy} \\triangleq \\{a_{phy} \\in \\mathbb{R}^m | \\underline{z} < C\\cdot a_{phy} < \\overline{z}, \\text{ with } C \\in \\mathbb{R}^{I\\times m}, \\underline{z}, \\overline{z}, z \\in \\mathbb{R}^m\\}.$\nwhere $C, \\underline{z}, \\overline{z}$ and $z$ are users' options for controlling the space of model-based actions. Equation (4) shows the model-based action completely depends on F. So,we shall redesign F to control model-based action to space (7). Due to the page limit, the proposed redesign for delivering F, reward (5) and controllable action space (7) is presented in Appendix B.2."}, {"title": "4.3 HP-Student: Continual Learning", "content": "HP-Student is a Phy-DRL model pre-trained in a simulator or another domain, which takes the actor-critic architecture-based DRLs such as [41] [42] for training. The pre-trained Phy-DRL model has an action policy and an action-value function. When deployed to a new safety-critical environment, the SeC-learning machine enables the pre-trained policy to continually and safely search for a safe policy that maximizes the expected return.\nSampling efficiency is one of the important considerations for continual learning in the real world. Experience replay (ER) [43] allows off-policy algorithms to reuse the experience collected in the past, greatly improving the sampling efficiency and avoiding forgetting the learned knowledge [44]. ER is also beneficial for breaking the correlation between adjacent transitions to avoid sampling bias for a stable learning process. Those features are very important in continual learning, where online data is limited due to the expensive interaction on the physical system. During the online inference, we continuously store the real transitions realized by safe high-performance action of HP-Student or corrected unsafe data-driven action by HA-Teacher to the replay buffer. Specifically, as illustrated in Figure 1, if the HP-Student's action $a_{Hp} (k)$ leads to unsafe behavior of a real plant, HA-Teacher takes over his role of controlling real plant to be safe, and corrects his unsafe data-driven action $a_{drl}(k)$ to $\\hat{a}_{HA} (k)$ according to\n$$a_{drl}(k) \\leftarrow \\hat{a}_{HA}(k) \\equiv a_{HA}(k) - A_{phy}(k),$$\nwhere $a_{phy} (k)$ is HP-student's model-based action in residual action policy (4), and $a_{HA}(k)$ is the action from HA-Teacher, whose design is presented in Section 6. Meanwhile, the online learning process will uniformly sample a minibatch of transitions for learning or training [45].\nRemark 4.1. Equation (8) indicates that for HP-Student's residual action policy (4), the correction in performed only on data-driven action $a_{drl}(k)$, i.e., not including model-based action $a_{phy}(k)$. The reason is that although the model-based design has limited performance and a small safe operation region due to model mismatch, it is analyzable and verifiable, and its policy is invariant because of his invariant physics-model knowledge (A, B)."}, {"title": "5 SeC-Learning Machine: Coordinator Component", "content": "The Coordinator triggers the switch between HP-Student and HA-Teacher to control the real plant by monitoring the system's state in real-time. The switching logic of terminal action applied to a real"}, {"title": "6 SeC-Learning Machine: HA-Teacher Component", "content": "HA-Teacher has tasks in the SeC-leaning machine:\n\u2022 Back up Safety via Patching Safety Envelope. As soon as the HP-Student leads to an unsafe real-time system behavior, the HA-Teacher intervenes to safely control the system through patching the safety envelope, depicting in Figure 3.\n\u2022 Correct Unsafe Learning. The HA-Teacher uses safe actions to correct the real-time and potentially (in dwell time \u03c4 horizon) unsafe actions of the HP-Student for learning.\nHA-Teacher is a physics-model-based design whose function is reduced to be safety-critical only. Compared with the HP-Student, the HA-Teacher has relatively rich dynamics knowledge about real plants. Hereto, the dynamics model leveraged by HA-Teacher updates from (1) as\n$$s(k + 1) = A(s(k))s(k) + B(s(k)) a_{HA}(k) + g(s(k)), k\\in\\mathbb{N}$$\nwhere $g(s(k)) \\in \\mathbb{R}^n$ is the unknown model mismatch for HA-Teacher. The physics-model knowledge available to HA-Teacher is thus $(A(s(k)), B(s(k)))$. Because of the Parallel Running configuration, HA-Teacher is always active to compute a safe action policy with control goal as\nPatch center: $s^* = \\chi\\cdot s(k)$ with $\\chi \\in (-1,1)$.\nIn other words, with real-time sensor data and physics-model knowledge, the HA-Teacher is always in 'active' status to compute a model-based action policy to control the real plant to reach the goal s*. To achieve this, HA-Teacher first obtains tracking-error dynamics from Equation (11) as\n$$e(k + 1) = A(s(k)) \\cdot e(k) + B(s(k)) a_{HA}(k) + h(e(k)), \\text{ with } e(k) \\triangleq s(k) - s^*.$$\nwhere h(e(k)) \u2208 Rn denotes unknown model mismatch, and HA-Teacher's action policy is\n$$a_{HA}(k) = F e(k),$$\nwhose aim is to track the goal s* while constraining system states to the envelope patch:\n$$Envelope \\ patch: \\Omega_{patch} = \\{s | (s-s^*)^T\\cdot P \\cdot (s-s^*) \\leq (1-\\chi)^2\\cdot s^T (k) \\cdot P s(k), P>0\\}.$$\nThe matrices F and P in Equation (14) and Equation (15) are HA-Teacher's design variables for backing up safety and correcting unsafe learning. To have them, we present a practical and common assumption on unknown model mismatch for computing them.\nAssumption 6.1. The model mismatch in h(\u00b7) in Equation (13) is locally Lipschitz in set $\\Omega_{patch}$, i.e.,\n$(h (e_1) -h (e_2))^T\\cdot P \\cdot (h (e_1) - h (e_2)) \\leq \\kappa\\cdot (e_1 \u2013 e_2)^T\\cdot P \\cdot (e_1 \u2013 e_2), \\forall e_1, e_2 \\in \\Omega_{patch}$,\nwhere P > 0 is shared by HP-Student, which defines his safety envelope (3) and safety-embedded reward (5).\nThe designs of F and P for delivering HA-Teacher's capabilities of backing up safety and correcting unsafe learning are formally presented in the following theorem, whose proof appears in Appendix C.\nTheorem 6.2. Consider the HA-Teacher's action policy (14) and the envelope patch $\\Omega_{patch}$ (15), whose matrices F and P are computed according to\n$$F =R Q^{-1}, P = Q^{-1},$$\nwith the matrices $R$ and $Q$ satisfying\n$$Q \\cdot P < I_n < \\eta \\cdot Q \\cdot P, \\text{ with a given } \\eta > 1$$\n$$\\begin{bmatrix}\n\\beta-\\kappa\\cdot\\eta\\cdot (1+\\frac{1}{w})\\cdot Q & Q A^T (s(k)) + R^T B^T (s(k))\\\\\nA(s(k)) Q+B(s(k)) \\cdot R & \\frac{1+w}{\\eta} Q\n\\end{bmatrix} > 0,$$\nwhere \u03b2\u2208 (0,1) and w > 0 are given parameters. Under Assumption 6.1, the system (13) controlled by HA-Teacher has the following properties:\n1. The $e^T (k + 1) \\cdot P \\cdot e (k + 1) \\leq \\beta \\cdot e^T (k) \\cdot P \\cdot e (k)$ holds for any $k \\in \\mathbb{N}$.\n2. The $\\Omega_{patch} \\subseteq \\Omega$ holds if the parameters \u03b7 in Equation (17) and \u03c7 in Equation (12) satisfy\n$$(1 - \\chi)^2 \\cdot \\eta\\cdot \\varepsilon + \\chi^2 \\cdot \\varepsilon \\leq 0.5.$$\nRemark 6.3 (Suggestion from Item 1: Dwell time \u03c4 of HA-Teacher). We obtain from Item 1 that\n$e^T (k)\\cdot P e(k) \\leq \\beta^{k-t}\\cdot (e^*)^T\\cdot P \\cdot (e^*),$\nwherein the t and e* denote the activation time of HA-Teacher and the initial distance with goal s* (12), respectively. The real-time tracking error e (k) can be understood as the distance to the goal s* (i.e., the center of envelope patch). Meanwhile, we can use $e^T(k)\\cdot P e(k)$ as distance function or performance metric. Hereto, we consider a safety criteria as $e^T (k)\\cdot P e (k) < \u03b4$. Illustrated in Figure 3, a very small $\u03b4$ means \u201cbeing very close to patch center\u201d and that HA-Teacher can preserve sufficient fault-tolerance space for HA-Teacher's continual learning. Meanwhile, when the preset safety criteria hold, HP-Student takes back the control role from HA-Teacher. According to Equation (20), the condition of HA-Teacher's dwell time for satisfying the safety criteria is\n$$\\tau \\geq \\frac{\\ln \\delta \u2013 \\ln ( (e^*)^T \\cdot P \\cdot (e^*))}{\\ln \\beta}$$\nIn other words, if $k \u2013 t > \u03c4$ and \u03c4 satisfies Equation (21), we have $e^T (k) \\cdot P \\cdot e (k) \\leq \u03b4$."}, {"title": "7 Experiment", "content": "We perform the experiments on a cart-pole system (simulator) and a real quadruped robot."}, {"title": "7.1 Cart-Pole System", "content": "This experiment aims to demonstrate the effectiveness of the SeC-Learning Machine from perspectives of concurrent safety and training performances in the face of the Sim2Real gap. The pre-training of HP-Student (i.e., Phy-DRL) is performed on the simulator provided in Open-AI Gym [48]. To address the Sim2Real gap, the pre-training adopts domain randomization [24, 32] through introducing random force disturbances and randomizing friction force. We also use the simulator to mimic a real plant whose Sim2Real gap is intentionally created by inducing a friction force that is out of the distribution of the random friction force used in pre-training.\nThe system's mechanical analog is characterized by the pendulum's angle \u03b8, the cart's position x, and their velocities w = \u03b8 and v = \u017c. The mission of HP-Student is to stabilize the pendulum at equilibrium s* \u225c [0,0,0,0] while constraining the system state to safety set:\n$$X = \\{s \\in \\mathbb{R}^4| \u2013 0.9 \\leq x \\leq 0.9, - 0.8 < \\theta < 0.8\\} .$$\nGiven the safety set, we set the space of physics-model-based action policy as\n$$A_{phy} \\triangleq \\{a_{phy} \\in \\mathbb{R} | -25 \\leq a_{phy} \\leq 25\\}.$$\nWith them, the designs of HP-Student and HA-Teacher are presented in Appendix D.3 and Appendix D.4, respectively.\nFor continual learning in mimicked real plants, the maximum length of one episode is 1500 steps. For the comparisons, we consider three models: 'Pre-trained Policy' (i.e., Phy-DRL pre-trained in a simulator, using randomization approaches for addressing Sim2Real gap), 'Unsafe Continual Learning' (i.e., pre-trained Phy-DRL continues to learn in the real plant but no Simplex support), and our proposed \u2018SeC-Learning Machine'.\nA very distinguished feature of our SeC-Learning Machine is lifetime safety, i.e., a safety guarantee in any stage of continual learning, regardless of the success of HP-Student. To demonstrate this and have fair comparisons, HP-Student in the 'SeC-Learning Machine' and the 'Unsafe Continual Learning' model are picked after training for only two episodes. Given the three different initial conditions, phases plots of these three models are shown in Figure 5. To further convincingly demonstrate the feature, additional phase plots are shown in Figure 7, Figure 8, and Figure 9, where the models are picked after training for only three episodes, four episodes, five episodes, respectively. Meanwhile, the reward's training curves (five random seeds) are shown in Figure 4. Observing Figures 4, 5 and 7 to 9, we conclude:\n\u2022 In the face of a large Sim2Real gap with a pre-training environment, the SeC-Learning Machine can always guarantee safety (system states never leave the safety envelope; see red curves in Figures 5 and 7 to 9) in any stage of continual learning. In contrast, the pre-trained model and continual learning without Simplex cannot guarantee safety (system states left the safety envelope; see blue and green curves in Figures 5 and 7 to 9)."}, {"title": "7.2 Real Quadruped Robot", "content": "The mission of the action policy is to control the robot's COM height, COM x-velocity, and other states to track the corresponding commands $r_v$, $r_h$, and zeros, under safety constraints: |yaw| \u2264 0.2 rad, CoM x-velocity | - $r_v$| < |$r_v$|, |CoM z-height - $r_h$| \u2264 0.12 m, and |CoM yaw velocity| < 0.3 rad/s. For the Coordinator's switching logic (9) and correcting logic (10), we let $\\varepsilon = 0.65$ and $\\tau = 10$. The designs of HP-Student and HA-Teacher are presented in Appendix E.3 and Appendix E.4, respectively. For HP-Student's pre-training of addressing the Sim2Real gap, we consider the approaches of delay randomization proposed in [28] and force randomization. During pre-taining in the simulator, the ground friction is set as 0.7, and $r_v$\u2081 = 0.6 m/s and $r_h = 0.24$ m. In the real quadruped robot, one episode is defined as \u201crunning the robot for 15 sec.\u201d To better demonstrate the performance of the SeC-Learning Machine, the velocity command for the real robot is $r_v$\u2081 = 0.35 m/s, which very different from the one for HP-Student's training in the simulator.\nWe compare three models in the real quadruped robot: \u2018Phy-DRL\u2019 (a pre-trained Phy-DRL model in the simulator, directly deployed on the real robot), 'Continual Learning' (a pre-trained Phy-DRL model in simulator that continues learning in the real robot for 20 episodes but without Simplex logic), and our proposed 'SeC-Learning Machine' in the 1st episode in real robot. We consider the 1st episode for the 'SeC-Learning Machine' model, owning to its claimed feature of lifetime safety, i.e., safety guarantee in any stage of continual learning in a real plant regardless of HP-Student's convergence or success. Therefore, showing system trajectories in the 1st episode will be most convincing, as HP-Student cannot converge so fast for a safe action policy within only one episode. We also note that Phy-DRL or HP-Student is pre-trained well in the simulator.\nThe comparisons are first viewed from the trajectories of the robot's CoM height and x-velocity regarding tracking performance and safety guarantee, which are shown in Figure 6 (a) and (b). Meanwhile, the comparison video of the three models in real robots is available at real-robot-video-1 link. In addition, the real robot's trajectories under the control of the SeC-Learning Machine in the 5th episode, 10th episode, 15th episode, and 20th episode are shown in Figures 10 to 13 in Appendix E.5.1, respectively. These figures straightforwardly depict that the SeC-Learning Machine guarantees the safety of real robots across all selected episodes or stages of continual learning. Observing Figure 6 (a) and (b), Figures 10 to 13, and the comparison video, we discover that Phy-DRL (i.e., HP-Student), a well pre-trained model in simulator, cannot guarantee the safety of the real robot due to the existing Sim2Real gap or unknown unknowns in the physical environment that the delay and force randomization failed to capture. Thus, continuous real-time learning is needed for the real robot. However, without Simplex logic, the safety of real robot during continual learning is not guaranteed. The SeC-Learning Machine successfully addresses these challenges by ensuring lifetime safety for continual real-time learning.\nFinally, we emphasize that another critical mission of HA-Teacher is correcting HP-Student's unsafe behavior. In other words, HP-Student learns from HA-Teacher to be safe when his actions are unsafe. To demonstrate this, we compare the episode-average reward curves of HP-Student (with HA-Teacher support) against the one (without Simplex logic or HA-Teacher support) in continual learning, as"}, {"title": "8 Conclusion and Discussion", "content": "This paper develops the SeC-learning machine for safety-critical autonomous systems. The SeC-learning machine constitutes HP-Student (a pre-trained Phy-DRL, continuing to learn in a real plan), HA-Teacher (verified physics-model-based design), and Coordinator. In the learning machine, the HA-Teacher backs up safety and corrects unsafe learning of HP-Student. The SeC-learning machine aims to assure lifetime safety, address the Sim2Real gap, and learn to tolerate unknowns unknowns in real plants. Experiments also demonstrate that the SeC-learning machine features remarkably fast, stable, and safe training compared with continual learning without Simplex logic.\nIntuitively, the SeC-learning machine is also an automatic hierarchy learning machine. Specifically, the HP-student first learns from the HA-teacher to be safe. If safe enough, i.e., the HP-student can be independent of the HA-teacher, the HP-student will learn by himself to achieve the goal of high performance with verifiable safety. This investigation continues our future research direction."}, {"title": "9 Computation Resources", "content": "In all case studies, we train and test the deep reinforcement learning (DRL) algorithm on a desktop equipped with Ubuntu 22.04, a 12th Gen Intel(R) Core(TM) i9-12900K 16-core processor, 64 GB of RAM, and an NVIDIA GeForce GTX 3090 GPU. The DRL algorithm was implemented in Python using the TensorFlow framework. We used the open-source Python CVX solver to solve LMIs problems.\nIn our architecture, the computation of F and P for HA-Teacher at each patch need to be done in time when the Safety Coordinator is triggered. To enable real-time computation of CVX and interaction with the environment, we implement a multi-processing pipeline to control the robot and solve LMIs in parallel in real-time. For solving LMIs, we always let the solver take the latest state so that whenever the safety coordinator is triggered, the latest F and P will always be ready, where the delay issue was considered and formulated in the LMIs problems.\nWe also noticed that the MATLAB-based CVX solver could solve the LMIs problem more consistently than the Python-based one, yielding more reliable solutions. However, the data interfacing overhead between Matlab and Python will introduce extra delay when updating F and P for HA-Teacher. Besides, the multiprocessing implementation for Matlab and Python is another technical challenge due to software compatibility issues. Therefore, we took the Python-based CVX solver for real-"}, {"title": "A Auxiliary Lemmas", "content": "Lemma A.1 (Schur Complement [49"}]}