{"title": "MPC-Minimized Secure LLM Inference", "authors": ["Deevashwer Rathee", "Dacheng Li", "Ion Stoica", "Hao Zhang", "Raluca Ada Popa"], "abstract": "Many inference services based on large language models (LLMs) pose a privacy concern, either revealing user prompts to the service or the proprietary weights to the user. Secure inference offers a solution to this problem through secure multi-party computation (MPC), however, it is still impractical for modern LLM workload due to the large overhead imposed by MPC. To address this overhead, we propose MARILL, a framework that adapts LLM fine-tuning to minimize MPC usage during secure inference. MARILL introduces high-level architectural changes during fine-tuning that significantly reduce the number of expensive operations needed within MPC during inference, by removing some and relocating others outside MPC without compromising security. As a result, MARILL-generated models are more efficient across all secure inference protocols and our approach complements MPC-friendly approximations for such operations. Compared to standard fine-tuning, MARILL results in 3.6 - 11.3\u00d7 better runtime and 2.4 -6.9\u00d7 better communication during secure inference across various MPC settings, while typically preserving over 90% performance across downstream tasks.", "sections": [{"title": "1 Introduction", "content": "Transformer-based large language models (LLMs) have revolutionized machine learning (ML). Since the announcement of ChatGPT, we have seen the release of a plethora of proprietary LLMs (e.g., GPT-4 [57], Claude 2 [2], Bard [24]), as well as open-source LLMs (e.g., Llama [69], Mistral [34]) that are now competitive against their proprietary counterparts [11, 72, 76, 48]. Recently, companies have started to finetune these models on domain-specific data to improve their performance on downstream tasks such as chatbots, virtual assistants, and copilots [57, 3, 15].\nUsing these finetuned models to power such user-facing services, however, raises significant privacy concerns. On one hand, the providers of these finetuned models do not want to expose their models' weights, as these models are often trained on proprietary data and represent competitive differentiation. On the other hand, users do not want to send their queries to these providers as these queries might contain sensitive or proprietary information (e.g. IP-protected code or user data). In fact, some enterprises prohibit their users from using LLM services, e.g., Samsung recently banned the use of external LLM services after an employee accidentally leaked sensitive code to ChatGPT [62].\nSecure inference is a promising solution to address this challenge as it can provide privacy for both parties through secure multi-party computation (MPC) [22, 77]. There is a long line of work on secure inference [56, 54, 61, 60, 70, 68, 26, 32] offering different performance and security tradeoffs, with the recent work focusing on secure transformer inference [45, 73, 16, 50, 29, 25]. In principle, the service provider can use any of these recent secure inference protocols to support its privacy-preserving service. However, despite massive strides in efficiency, these protocols are still impractical for today's LLMs. For instance, the state-of-the-art solution [25] requires 23 s and 15.9 GB of communication for the first token generation on a small 137M parameter model with 1024 input tokens. We expect the runtime and communication to degrade to around 6.5 minutes and 240 GB for a more typical 7B parameter model, which is impractical.\nTo minimize this overhead, prior works have focused on low-level operations that are expensive to compute within MPC, and have proposed MPC-friendly approximations for those operations (\u00a7 2). In this work, we consider an orthogonal approach targeting high-level architectural changes, that offer a complementary way to minimize the MPC overhead. Instead of simplifying operations, such architectural changes reduce the number of expensive low-level operations needed within MPC. Critically, this strategy does not (necessarily) eliminate these operations from the inference process entirely; rather, it relocates them outside of MPC without compromising security, where their cost is relatively negligible. Our work is the first to explore this high-level strategy, which we term MPC-minimization. We achieve this through fine-tuning, and our key insight is that fine-tuning, when carefully tailored to secure inference, can unlock significant opportunities for MPC-minimization.\nFollowing this insight, we propose a fine-tuning framework MARILL\u00b2 that makes strategic modifica- tions to standard fine-tuning, guided by the unique characteristics of MPC. The fine-tuned models output by MARILL are (i) MPC-minimized while maintaining the same level of security (\u00a7 3), and (ii) achieve ML performance close to that of standard fine-tuned models through knowledge distil- lation (\u00a7 5). Crucially, since MARILL essentially compresses the model within MPC, the resulting models are significantly more efficient across all secure inference protocols (\u00a7 6.1). Furthermore, as mentioned earlier, MARILL introduces only high-level architectural changes that complement MPC-friendly approximations. We demonstrate that integrating these approximations with MARILL leads to further efficiency improvements (\u00a7 6.3). Now, we present a brief overview of our techniques and the model component (in bold) they minimize within MPC:\n\u2022 Leveraging open-sourced models: As alluded to earlier, open-source LLMs have become more powerful and are now competitive against proprietary models [11, 72, 76, 48]. Consequently, a trend has emerged where an increasing number of service providers opt to fine-tune these open-source models with their private datasets instead of pre-training their own proprietary models [3, 15]. Standard fine-tuning updates all the model weights with the private data, necessitating the entire model to run within MPC and precluding any potential benefits of the publicly available pre-trained weights. In light of this, we propose two fine-tuning strategies that effectively leverage the public weights to minimize MPC:\n\u2013 Layer Freezing (\u00a7 5.1): We reduce the number of transformer layers that need to be evaluated within MPC by restricting fine-tuning updates (and thus, private weights) to just the final layers"}, {"title": "2 Related Work", "content": "Secure Inference Protocols. In this work, we focus on MPC-based secure inference protocols for neural networks which started with the seminal work of SecureML [56]. SecureML considers the two- party setting that only involves the service provider and the client, and after many follow-up works in this setting [56, 37, 47, 54, 61, 60, 80, 31, 4, 27, 29, 50, 58], the performance has improved by orders of magnitude. Despite these improvements, 2PC still poses very large overheads. Thus, subsequent works have considered other settings that introduce an additional helper party such as 3PC with honest majority [70, 41, 63, 55, 71, 16] and 2PC with trusted dealer (2PC-Dealer) [39, 26, 32, 25]. Other works have accelerated secure inference protocols by leveraging GPU acceleration [39, 68, 74, 32, 25].\nRecent work [27, 29, 50, 58, 16, 73, 25, 6] in all these settings have focused on secure transformer inference since they represent the majority of the AI workload today. Our work is orthogonal to these protocols and can be used to accelerate secure inference with any of them (Appendix F).\nMPC-friendly Approximations. Several works [45, 56, 21, 19, 14, 9, 54, 51, 33, 59, 13, 12, 49, 42, 81] have proposed approximate implementations for non-linear activations like softmax and GeLU to make them more MPC-friendly. These approximations typically introduce a large drop in model performance. MPCFormer [45] proposed a two-stage distillation process to bridge this gap. Majority of these works [54, 33, 20, 59, 13, 12, 42, 49, 81] also use Neural Architecture Search (NAS) to employ multiple approximations within the same network depending on the precision level required.\nOur work is complementary to these approximations as we make high-level changes to the architecture, as opposed to the underlying operations. We show in \u00a7 6.3 that these approximations can be combined with MARILL to yield further performance improvements. Additionally, MARILL differs from these works in two key aspects: (i) while these works output models where all weights are private, MARILL produces models that have a mix of public and private weights, and (ii) the model architecture in NAS-based works depends on the private training data and leaks additional information, whereas MARILL is statically configured independent of the training data."}, {"title": "3 Threat Model", "content": "We inherit the threat model from prior secure LLM inference works which all assume a semi-honest (or passive) adversary that follows the protocol exactly but tries to learn information about the private inputs from the messages it sees during the protocol. This adversary controls an unknown subset of the MPC participants, where the size of the subset is defined by the MPC setting. Like prior works, we also assume that the model architecture is public and the service only wants to hide the model weights. We formally prove security in Appendix C. We note that our work is not limited to a semi-honest adversary and discuss extensions to malicious security in Appendix D."}, {"title": "4 Performance Characteristics of Secure Inference", "content": "Secure inference relies on secure multi-party computation (MPC) [22, 77], a cryptographic primitive that allows mutually distrusting parties to compute any function on their private inputs without reveal- ing anything beyond the function output. Prior secure inference works, specifically, have considered three MPC settings (Appendix A), each making different assumptions about the participants. In this section, we highlight the unique cost profile of MPC in these settings and discuss how it motivates the design of our techniques in \u00a7 5.\nInteraction costs. Unlike plaintext computation, most operations within MPC require interaction among the MPC participants. This imposes two additional performance overheads in addition to computation size, namely, communication size and rounds of communication. For most MPC protocols, this cost of interaction ends up being the bottleneck and it is the primary reason why MPC is orders of magnitude slower than plaintext computation.\nMultiplications with public weights come for free. Since MPC operates natively over integers, recent secure inference works use fixed-point representation to emulate real-number arithmetic. Additionally, prior works maintain the invariant that the intermediate state after every network layer is arithmetically secret-shared (ASS) among MPC participants. This approach minimizes the cost of arithmetic operations, such as integer multiplications and additions, which dominate ML workloads. In an ASS scheme, a secret value $x$ is split among $n$ MPC participants such that (i) each party $P_i$ receives a share $x_i$ and any set of $n - 1$ shares reveals nothing about $x$, and (ii) the sum of all shares reconstructs the secret $x = x_1 + ... + X_n$. The linear nature of this reconstruction function allows secret-shared values to be added locally (without interaction) by simply adding the corresponding secret shares, making additions within MPC relatively so inexpensive that they are considered \"free\". Similarly, any affine operation with public coefficients on secret-shared values, such as a matrix multiplication with public weights, also becomes free. In \u00a7 5.2, we show how low-rank adaptations can leverage this property to reduce the number of multiplications between secret-shared values.\nNon-arithmetic operations are the bottleneck in the most efficient MPC settings. Non-arithmetic operations are used to implement comparisons in maxpool, activation functions such as ReLU and GeLU, exponentiation and division in softmax, as well as the truncation operations in fixed-point multiplications. We analyzed state-of-the-art secure inference frameworks (\u00a7 6.1) in the most efficient MPC settings, namely, 3PC and 2PC-Dealer (Appendix A), and found that non-arithmetic operations account for over 88% of the runtime and communication during secure inference with a sequence length of 2048. This is in stark contrast to plaintext computation where non-arithmetic operations have a minimal contribution to the total FLOPs and the inference latency. Guided by this insight, we proposed head-merging in \u00a7 5.3, a technique that preserves the FLOPs and still yields significant performance improvements.\nA mix of public and private weights typically does not speedup secure inference. Since multi- plications with public weights come for free, one would expect significant improvements to secure inference if most of the weights were public. However, to preserve the standard guarantees of the MPC, an intermediate state that depends on both the private input and any private weight must not be revealed to any party. Consequently, once the computation involves a single private weight, all subsequent non-arithmetic operations need to be performed within MPC, which as we just discussed are the bottleneck in the most efficient MPC settings for secure inference. This restriction motivated the design of layer-freezing in \u00a7 5.1, which separates the public and private weights across layers such that the non-arithmetic operations in public layers are performed outside MPC."}, {"title": "5 Techniques", "content": "In this section, we describe our techniques that minimize the need for expensive operations within MPC. We start with layer-freezing (\u00a7 5.1) that reduces the number of layers evaluated within MPC. Next, we discuss LoRA (\u00a7 5.2) and head-merging (\u00a7 5.3) that minimize arithmetic and non-arithmetic operations, respectively, in the private layers. Distillation details are deferred to Appendix E.\n5.1 Layer Freezing\nOur starting point is the observation that when an open-source model is fine-tuned on a private dataset, only the fine-tuned weights need to be kept private during inference. To leverage this insight, consider using a technique from prior work that only fine-tunes a fraction of model weights [17]. However, as explained in \u00a7 4, these techniques typically do not significantly speed up inference. This is because they update weights throughout the network, including near the input, which means that almost all non-arithmetic operations \u2013 typically the bottleneck must be performed within MPC.\nTo this end, our solution (Fig. 2a) effectively leverages public weights by deferring fine-tuning to only the final layers of the transformer, thereby also deferring MPC to these final layers. During inference, the client receives the weights for the bottom layers (identical to the open-source pre-trained model) from the server, computes the output of these layers locally, and then engages in MPC with the server for the top layers. Consequently, if only a fraction $f$ of the layers are fine-tuned, all MPC overheads are reduced by a factor of $f$ \u00d7 (\u00a7Table 2). Although delegating the computation of the bottom layers to the client might seem like a limitation, this approach actually reduces client overheads by the same factor, since the MPC overhead on the client in secure inference protocols is orders of magnitude higher than the overhead of plaintext inference\u00b3.\n5.2 LORA Adaptation\nIn \u00a7 4, we discussed how multiplication with public weights is free during secure inference. Here, we demonstrate how LoRA [30], a technique developed for parameter-efficient fine-tuning, can be repurposed to minimize integer multiplications during inference. These operations account for up to 95% of the runtime in the state-of-the-art 2PC work Bumblebee [50]. Beyond the 2PC setting, we found that multiplications also dominate the decoding (see Appendix B) runtime in 3PC and 2PC- Dealer settings, which are otherwise bottlenecked by non-arithmetic operations (\u00a7 4). This occurs because the linear layers during decoding perform matrix-vector multiplications instead of matrix multiplications, making key matrix-multiplication optimizations from [56] no longer applicable.\nA LORA adapter on a weight matrix $W \\in R^{n \\times k}$ is a product of two low-rank matrices $A \\in R^{n \\times r}$ and $B \\in R^{r \\times k}$, where $r < \\text{min}(n, k)$. During fine-tuning, only the low-rank matrices are updated, and at inference time, $A \\times B$ is merged into the pre-trained weight $W$ to minimize inference overhead."}, {"title": "5.3 Head Merging", "content": "The most efficient secure inference works [16, 39, 25] operate in the 3PC and the 2PC-Dealer settings (Appendix A). In these settings, non- arithmetic operations are the bottleneck. Among these operations, those in the self-attention mod- ule are of particular interest because: (i) the self-attention mechanism is the only compo- nent that scales quadratically with the sequence length $b$, (ii) the state-of-the-art works in both 3PC [16] and the 2PC-Dealer [25] settings ex- hibit a super linear blowup in runtime when $b \u2265 1024$, highlighting that self-attention is in- deed the bottleneck for large $b$, and (iii) applica- tions such as chatbots and copilots which have real-time requirements require a large sequence length. Thus, we focus on minimizing the non- arithmetic operations in the self-attention mod- ule in this work.\nReducing number of heads. The self-attention mechanism has two non-arithmetic operations: (i) softmax, and (ii) truncations (from fixed-point multiplications), and the complexity for both is $O(b^2h)$, where $h$ is the #heads. Hence, we seek to reduce $h$ by a factor $m$ so that all operations are reduced proportionally. The standard technique for minimizing heads is head-pruning [53], which analyzes the importance of each head over the training dataset, and prunes the insignificant heads. This achieves our goal, however, we have to prune 75% of the heads (as well as their parameters) for $m = 4$, and this results in a large accuracy loss (\u00a7 6.4).\nPreserving the pre-trained parameters. To this end, we observe that unlike plaintext inference, FLOPs do not dictate the secure inference cost (\u00a7 4) and it is possible to achieve similar speedups as head-pruning despite preserving all the parameters (\u00a7 6.4). This is also evident in the complexity of non-arithmetic operations in self-attention, which are independent of the head-dimension $d$. Thus, we propose a technique called head-merging that reduces the number of heads $h$ by $m$, while simultaneously increasing the head dimension $d$ proportionally, thereby preserving all parameters from the pre-trained model. Specifically, $h$ heads are divided into groups of $m$, and the QKV matrices for heads within the same group are concatenated as shown in Fig. 3. Concretely, given matrices ${Qi, Ki, Vi}i\u2208[h]$ of dimension $R^{b\u00d7d}$, the head attention outputs ${headj}j\u2208[h/m]$ after merging are as follows: $head_{j} = softmax(\\frac{ (Q_{jm}||...||Q_{(j+1)m})(\\frac{K_{jm}||...||K_{(j+1)m}}{\\sqrt{md}})) . (V_{jm}||...||V_{(j+1)m}) \u2208 R^{b\u00d7md}$\nMerging similar heads. In the expression above, adjacent heads are grouped such that heads $jm$ to $(j + 1)m$ belong to group $j$. This strategy does not consider the similarity among heads, resulting in minimal accuracy improvement over head-pruning (\u00a7 6.4). To group heads based on similarity, we follow the strategy from [5] that computes the pairwise Jensen-Shannon distance between all heads within the same layer. Once we have the pairwise distances, we perform K-Medoid clustering [38] to organize heads into $h/m$ groups. Finally, to get groups of the same size, we redistribute heads based on a linear sum assignment that minimizes the sum of distances from the medoid within each group. We found that merging similar heads using this method performs significantly better, leading to up to 8% gain in accuracy \u00a7 6.4."}, {"title": "6 Evaluation", "content": "In this section, we first evaluate the secure inference cost (\u00a7 6.1) of MARILL-generated models and their ability to preserve ML performance (\u00a7 6.2). Next, we perform the same analysis for prior MPC-friendly approximations integrated with MARILL (\u00a7 6.3). Finally, we do an ablation study in \u00a7 6.4 that considers alternative designs for MARILL's techniques.\nSecure Inference Setup. We perform the secure inference experiments on state-of-the-art (open- sourced) frameworks in all MPC settings considered by prior work, namely, 2PC [50, 52], 3PC [16, 52], and 2PC-Dealer [39, 73]. The experiments were run on two or three machines (depending on the MPC setting) connected via LAN connection with 16 Gbps bandwidth and 0.1 ms latency. Each machine was equipped with an Intel Xeon Platinum 8173M Processor with 16 vCPUs, 128 GB RAM, and a V100 GPU with 16 GB memory. Since the 2PC-Dealer framework [39] supports GPU acceleration, we ran it on the V100. Experiments on other MPC frameworks were run on CPU. All experiments were multi-threaded. All reported numbers consider end-to-end costs.\nModels and Datasets. We consider three privacy-sensitive tasks for LLMs: chatbot, coding, and machine translation. For the chatbot task, we fine-tune open-llama3b-v2 on the ShareGPT dataset and evaluate it on the MTBench dataset, following [82, 44]. OpenLLaMA is a popular open-source model that replicates the LLaMA model [18, 69]. For the coding task, we fine-tune deepseek-coder-1.3b-base on the MagiCoder dataset [75] and evaluate it on the HumanEval benchmark [8]. For the machine translation task, we fine-tune open-llama3b-v2 on the ParroT dataset [35] and evaluate it on the WMT22 (De\u21d2En) benchmark [40].\nFine-Tuning Hyperparameters. We set the fine-tuning hyperparameters according to the papers that curated the corresponding fine-tuning dataset: [82] for MTBench, [75] for HumanEval, and [35] for WMT22. We only vary the batch size and number of training epochs to better suit some techniques. For instance, we observed that LoRA favors a smaller batch size in our setting. We include the detailed hyperparameters in Appendix G.\n6.1 Secure Inference Performance\nIn this section, we compare the secure inference performance of MARILL-generated models vs the baseline a fully fine-tuned model. Fig. 4 summarizes these results for openllama-3b-v2 as the pre-trained model. We first analyze the improvements from head-merging (\u00a7 5.3) and LoRA (\u00a7 5.2) in the three MPC settings from prior work, and then discuss layer-freezing (\u00a7 5.1) improvements.\n2PC: LORA improves the pre-filling runtime by 4.9\u00d7 (Fig. 4a) because 92% of the 2PC runtime is spent in performing multiplications for openllama-3b-v2 inference. Decoding runtime is improved by 2.2x, which is less pronounced because the 2PC framework [50] does not amortize well over"}, {"title": "6.2 ML Performance", "content": "Fig. 5 summarizes the ML performance of MARILL, the pre-trained model and the fully fine- tuned model on our three benchmarks. First, we note that full fine-tuning significantly improves the performance of the pre-trained model across all three tasks. MARILL's layer-freezing is also effective on all three tasks, preserving 93 \u2013 100% of the full fine-tuning performance. On WMT and HumanEval benchmark, head-merging preserves 92 - 95% performance, while on MTBench, it achieves 87% performance. The combination of layer-freezing and head-merging works well, incurring an additional loss of at most 4% compared to head-merging alone. LoRA preserves over 95% performance on all benchmarks. While combining LoRA with layer freezing sometimes leads to a big drop in performance (MTBench and HumanEval), we note that using LoRA alone provides significant speed-ups, ranging from 2.2\u00d7 to 4.9\u00d7. Overall, we observe that MARILL's techniques typically preserve over 90% of the fully fine-tuned performance."}, {"title": "6.3 Integration of prior MPC-friendly approximations with MARILL", "content": "In this section, we analyze the performance of MARILL when combined with prior MPC-friendly ap- proximations, namely, Quad [45] and ReLU [10, 79] as GeLU/SiLU approximations, and 2Quad [45], L2Quad [81] and 2ReLU [56] as softmax approximation. First, we analyzed the ML performance of each approximation independently and found that the quadratic approximations from recent works led to a catastrophic loss on our benchmarks. Specifically, on the HumanEval benchmark, Quad only achieves 31.7% accuracy compared to 61% of the baseline, and the fine-tuning diverges for L2Quad and 2Quad, resulting in 0% accuracy. In contrast, ReLU-based approximations work very well, with"}, {"title": "6.4 Ablation Study", "content": "Layer-freezing vs layer-pruning. In layer-freezing, we froze the bottom layers of the transformer to move some layers outside of MPC. An alternative strategy to minimize layers within MPC is to simply prune some layers. We experimented with layer-pruning on the HumanEval benchmark and evaluated the best-performing strategy from [64], namely, top-layer pruning. For half of the layers pruned, we found that the accuracy drops from 61% for the baseline to just 49.4% post layer-pruning. In contrast, layer-freezing achieved an accuracy of 56.7%, a 12% increase in relative performance, highlighting the importance of preserving the pre-trained model weights of the pruned layers.\nHead-merging vs head-pruning. We compared head-pruning [53] and head-merging \u00a7 5.3 on HumanEval, configuring head-pruning to prune the same number of heads from each layer so that it does not leak additional information about the private dataset. ML performance of head-merging, on the other hand, is much better since it preserves all the head parameters. In particular, head-merging has up to 8% better accuracy than head-pruning, and HM= 4 even outperforms HP=2 in both ML and secure inference performance. Note that these improvements only apply to similar head-merging, not adjacent head-merging, which na\u00efvely combines adjacent heads. These results demonstrate the significance of preserving head parameters as well as merging heads based on similarity."}, {"title": "7 Conclusion", "content": "In this work, we designed a framework MARILL, that leverages open-sourced LLMs and introduces high-level architectural changes during fine-tuning to minimize MPC usage during secure inference. We demonstrated that MARILL is effective in minimizing secure inference costs across MPC settings in exchange for a reasonable accuracy tradeoff. In particular, MARILL-generated models are 2.4 - 11.3\u00d7 more efficient for secure inference compared to a standard fine-tuned model, and they typically preserve over 90% relative performance across multiple challenging LLM tasks."}, {"title": "A MPC Settings", "content": "\u2022 2-party computation (2PC): this setting assumes two MPC participants who do not trust each other, and thus, it is the most natural setting for secure inference.\n\u2022 Honest-majority 3-party computation (3PC): this setting has an additional helper party that also participates in MPC, and the adversary can corrupt at most any one of the three parties. Prior works considered this setting because having this helper party improves the MPC performance by orders of magnitude.\n\u2022 2PC with trusted dealer (2PC-Dealer): in this setting, there is an additional trusted dealer that is only responsible for distributing input-independent correlated randomness to the computing parties in a pre-processing phase. The parties can then use this randomness to accelerate 2PC on their private inputs."}, {"title": "B LLM Inference Stages - Prefilling and Decoding", "content": "In this section, we briefly describe the two stages in LLM inference. Firstly, users provide a prompt in natural language to the system. The system then uses tokenizers to map the natural language into a vector x1,...In through a process called tokenization [66]. Then the system performs the main inference process using LLMs. The inference process consists of two phases - the pre-filling phase and the decoding phase. Formally, the pre-filling phase computes probablity of the first token conditioned on the previous n tokens $P(X_{n+1}|X_1, ...X_n)$ [67]. It then samples from the distribution and predicts the first token $X_{n+1}$. The decoding phase iteratively computes the next token based on the same logic. For instance, the first step in the decoding computes $P(X_{n+2}|x_1,...X_{n+1})$ and samples to obtain $X_{n+2}$. The decoding phase terminate when the new token is an ending token, often referred to as the \u201cend-of-sentence\" token (EOS). Interestingly, the left-to-right decoding nature has made the computation characteristics different [43, 78, 67] in these two stages. Thus, we distinguish between the two phases when evaluating our techniques in this work."}, {"title": "C Security Proof", "content": "Secure Inference Ideal Functionality $F_{M,n}$\nThis functionality is parameterized by the model architecture $M$ and #outputs tokens $n$.\n\u2022 Client Prompt: Receive prompt $p$ for $M$ from client $C$, and store $p$ internally.\n\u2022 Server Weights: Receive model weights $W$ for $M$ from server $S$, store $W$ internally.\n\u2022 Pre-filling: Perform pre-filling on the prompt to get state $st \\leftarrow M.\\text{prefill}(W, p)$. Set $i \\leftarrow 0$.\n\u2022 Decoding: If $0 < i < n$, receive token $x$ from the $C$, update the state $st \\leftarrow M.\\text{update}(st, x)$, and increment $i$. Then, perform a decoding step on $st$ to get an output token $y \\leftarrow M.\\text{decode}(st)$ and send $y$ to the client $C$.\nWe prove the security of our protocol in the standard simulation paradigm [7, 22, 46] that argues that whatever an adversary can learn in the real-world while interacting with honest parties in a protocol, it can also learn in an ideal-world while interacting with an ideal functionality that is incorruptible. In particular, the proof shows that there exists a simulator in the ideal-world that can simulate the adversary's real-world view by only interacting with the adversary and the ideal-functionality. Since the ideal functionality is designed to be trivially secure and not reveal anything about honest parties inputs beyond the function output, this proves that the adversary also can not learn this information from the actual protocol. We describe the ideal functionality $F$ that captures the security guarantees provided by any secure (transformer) inference protocol in Fig. 6. Note that the functionality does not leak any information to the server, and the client learns nothing beyond the output tokens. The ideal functionality also allows the client to choose the latest token, which is not a problem in the semi-honest setting as the client will follow the protocol. We discuss how to ensure a malicious client inputs the right tokens in Appendix D. We designed MARILL to make black-box use of"}, {"title": "MARILL's Secure Inference Protocol in the F-hybrid model", "content": "Let $M$ denote the entire model architecture (including LoRA and head-merging changes), $M_{pb}$ denote the part of the architecture with public layers, and $M_{pr}$ denote the part with private layers. Note that $M = M_{pb} || M_{pr}$ due to the design of layer-freezing. Let $W_{pb}$ and $W_{pr}$ denote the corresponding weights for these parts. Client C has prompt p and server S has weights $W_{pr}$. Both parties have $W_{pb}$. Let n be the number of tokens to be generated.\n1. Both parties initialize an instance of $F_{M_{pr},n}$ and the S sends $W_{pr}$ to $F_{M_{pr},n}$.\n2. The client locally evaluates the public part of the model on its prompt to get the hidden state for the prompt $h \\leftarrow M_{pb}.evaluate(W_{pb}, p)$, and sends $h$ to $F_{M_{pr},n}$. Note that this is the input that $M_{pr}$ expects to perform pre-filling on the prompt.\n3. C receives $y_1$ from $F_{M_{pr},n}$.\n4. For $i = 2,..., n$:\n(a) C locally evaluates the public part of the model on its prompt to get $h \\leftarrow M_{pb}.evaluate(W_{pb}, y_{i-1})$, and sends h to $F_{M_{pr},n}$. Note that this is the input $M_{pr}$ expects to update its context state with $y_{i-1}$.\n(b) C receives $y_i$ from $F_{M_{pr},n}$.\n5. Coutputs $(y_1,..., y_n)$."}, {"title": "Simulator for MARILL's Secure Inference Protocol", "content": "The simulator Sim internally runs the adversary A, has access to its input prompt p (since A is semi-honest), interacts with ideal functionality $F_{M,n}$ on behalf of the party controlled by the adversary, and simulates $F_{M_{pr},n}$ in the ideal-world.\nIf client C is corrupted:\n1. Sim sends prompt p to $F_{M,n}$ and receives $y_1$ from it.\n2. As $F_{M_{pr},n}$, Sim receives h from A, ignores it, and sends $y_1$ to A as the output.\n3. For $i = 2,..., n$:\n(a) Sim sends $y_{i-1}$ to $F_{M,n}$ and receives $y_i$ from it.\n(b) As $F_{M_{pr},n}$, Sim receives h from A, ignores it, and sends $y_i$ to A as the output.\nIf server S is corrupted:\n1. Receive model weights $W_{pr}$ from A, append it to the public weights $W_{pb}$ to get $W = W_{pb} || W_{pr}$ and forward W to $F_{M,n}$. There is nothing else to simulate since the server does not receive any messages in our protocol in the F-hybrid model."}, {"title": "D Malicious Security", "content": "Our work is not limited to a semi-honest adversary and can also support a malicious adversary that deviates from the protocol arbitrarily. Given a maliciously-secure protocol, our work inherits malicious security against the server directly as the server does not have any additional capabilities in our system. The simulator for a corrupted server also remains the same. Security against client needs careful assessment because the client in our system inputs a hidden state (output of a transformer layer), as opposed to a sequence of tokens in traditional secure LLM inference. This does not impact semi-honest security because the client will follow the protocol and input the right hidden state. However"}]}