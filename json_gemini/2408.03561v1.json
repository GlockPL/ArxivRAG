{"title": "MPC-Minimized Secure LLM Inference", "authors": ["Deevashwer Rathee", "Dacheng Li", "Ion Stoica", "Hao Zhang", "Raluca Ada Popa"], "abstract": "Many inference services based on large language models (LLMs) pose a privacy\nconcern, either revealing user prompts to the service or the proprietary weights\nto the user. Secure inference offers a solution to this problem through secure\nmulti-party computation (MPC), however, it is still impractical for modern LLM\nworkload due to the large overhead imposed by MPC. To address this overhead,\nwe propose MARILL, a framework that adapts LLM fine-tuning to minimize MPC\nusage during secure inference. MARILL introduces high-level architectural changes\nduring fine-tuning that significantly reduce the number of expensive operations\nneeded within MPC during inference, by removing some and relocating others\noutside MPC without compromising security. As a result, MARILL-generated\nmodels are more efficient across all secure inference protocols and our approach\ncomplements MPC-friendly approximations for such operations. Compared to\nstandard fine-tuning, MARILL results in 3.6 - 11.3\u00d7 better runtime and 2.4 -6.9\u00d7\nbetter communication during secure inference across various MPC settings, while\ntypically preserving over 90% performance across downstream tasks.", "sections": [{"title": "Introduction", "content": "Transformer-based large language models (LLMs) have revolutionized machine learning (ML). Since\nthe announcement of ChatGPT, we have seen the release of a plethora of proprietary LLMs (e.g.,\nGPT-4 [57], Claude 2 [2], Bard [24]), as well as open-source LLMs (e.g., Llama [69], Mistral [34])\nthat are now competitive against their proprietary counterparts [11, 72, 76, 48]. Recently, companies\nhave started to finetune these models on domain-specific data to improve their performance on\ndownstream tasks such as chatbots, virtual assistants, and copilots [57, 3, 15].\nUsing these finetuned models to power such user-facing services, however, raises significant privacy\nconcerns. On one hand, the providers of these finetuned models do not want to expose their models'\nweights, as these models are often trained on proprietary data and represent competitive differentiation.\nOn the other hand, users do not want to send their queries to these providers as these queries might\ncontain sensitive or proprietary information (e.g. IP-protected code or user data). In fact, some\nenterprises prohibit their users from using LLM services, e.g., Samsung recently banned the use of\nexternal LLM services after an employee accidentally leaked sensitive code to ChatGPT [62].\nSecure inference is a promising solution to address this challenge as it can provide privacy for\nboth parties through secure multi-party computation (MPC) [22, 77]. There is a long line of work\non secure inference [56, 54, 61, 60, 70, 68, 26, 32] offering different performance and security\ntradeoffs, with the recent work focusing on secure transformer inference [45, 73, 16, 50, 29, 25]. In\nprinciple, the service provider can use any of these recent secure inference protocols to support its\nprivacy-preserving service. However, despite massive strides in efficiency, these protocols are still\nimpractical for today's LLMs. For instance, the state-of-the-art solution [25] requires 23 s and 15.9\nGB of communication for the first token generation on a small 137M parameter model with 1024\ninput tokens. We expect the runtime and communication to degrade to around 6.5 minutes and 240\nGB for a more typical 7B parameter model, which is impractical.\nTo minimize this overhead, prior works have focused on low-level operations that are expensive to\ncompute within MPC, and have proposed MPC-friendly approximations for those operations (\u00a7 2).\nIn this work, we consider an orthogonal approach targeting high-level architectural changes, that\noffer a complementary way to minimize the MPC overhead. Instead of simplifying operations, such\narchitectural changes reduce the number of expensive low-level operations needed within MPC.\nCritically, this strategy does not (necessarily) eliminate these operations from the inference process\nentirely; rather, it relocates them outside of MPC without compromising security, where their cost\nis relatively negligible. Our work is the first to explore this high-level strategy, which we term\nMPC-minimization. We achieve this through fine-tuning, and our key insight is that fine-tuning, when\ncarefully tailored to secure inference, can unlock significant opportunities for MPC-minimization.\nFollowing this insight, we propose a fine-tuning framework MARILL that makes strategic modifica-\ntions to standard fine-tuning, guided by the unique characteristics of MPC. The fine-tuned models\noutput by MARILL are (i) MPC-minimized while maintaining the same level of security (\u00a7 3), and\n(ii) achieve ML performance close to that of standard fine-tuned models through knowledge distil-\nlation (\u00a7 5). Crucially, since MARILL essentially compresses the model within MPC, the resulting\nmodels are significantly more efficient across all secure inference protocols (\u00a7 6.1). Furthermore,\nas mentioned earlier, MARILL introduces only high-level architectural changes that complement\nMPC-friendly approximations. We demonstrate that integrating these approximations with MARILL\nleads to further efficiency improvements (\u00a7 6.3). Now, we present a brief overview of our techniques\nand the model component (in bold) they minimize within MPC:\n\u2022 Leveraging open-sourced models: As alluded to earlier, open-source LLMs have become more\npowerful and are now competitive against proprietary models [11, 72, 76, 48]. Consequently, a trend\nhas emerged where an increasing number of service providers opt to fine-tune these open-source\nmodels with their private datasets instead of pre-training their own proprietary models [3, 15].\nStandard fine-tuning updates all the model weights with the private data, necessitating the entire\nmodel to run within MPC and precluding any potential benefits of the publicly available pre-trained\nweights. In light of this, we propose two fine-tuning strategies that effectively leverage the public\nweights to minimize MPC:\n\u2022 Layer Freezing (\u00a7 5.1): We reduce the number of transformer layers that need to be evaluated\nwithin MPC by restricting fine-tuning updates (and thus, private weights) to just the final layers"}, {"title": "Related Work", "content": "Secure Inference Protocols. In this work, we focus on MPC-based secure inference protocols for\nneural networks which started with the seminal work of SecureML [56]. SecureML considers the two-\nparty setting that only involves the service provider and the client, and after many follow-up works in\nthis setting [56, 37, 47, 54, 61, 60, 80, 31, 4, 27, 29, 50, 58], the performance has improved by orders\nof magnitude. Despite these improvements, 2PC still poses very large overheads. Thus, subsequent\nworks have considered other settings that introduce an additional helper party such as 3PC with honest\nmajority [70, 41, 63, 55, 71, 16] and 2PC with trusted dealer (2PC-Dealer) [39, 26, 32, 25]. Other\nworks have accelerated secure inference protocols by leveraging GPU acceleration [39, 68, 74, 32, 25].\nRecent work [27, 29, 50, 58, 16, 73, 25, 6] in all these settings have focused on secure transformer\ninference since they represent the majority of the AI workload today. Our work is orthogonal to these\nprotocols and can be used to accelerate secure inference with any of them (Appendix F).\nMPC-friendly Approximations. Several works [45, 56, 21, 19, 14, 9, 54, 51, 33, 59, 13, 12, 49, 42,\n81] have proposed approximate implementations for non-linear activations like softmax and GeLU\nto make them more MPC-friendly. These approximations typically introduce a large drop in model\nperformance. MPCFormer [45] proposed a two-stage distillation process to bridge this gap. Majority\nof these works [54, 33, 20, 59, 13, 12, 42, 49, 81] also use Neural Architecture Search (NAS) to\nemploy multiple approximations within the same network depending on the precision level required.\nOur work is complementary to these approximations as we make high-level changes to the architecture,\nas opposed to the underlying operations. We show in \u00a7 6.3 that these approximations can be combined\nwith MARILL to yield further performance improvements. Additionally, MARILL differs from these\nworks in two key aspects: (i) while these works output models where all weights are private, MARILL\nproduces models that have a mix of public and private weights, and (ii) the model architecture in\nNAS-based works depends on the private training data and leaks additional information, whereas\nMARILL is statically configured independent of the training data."}, {"title": "Threat Model", "content": "We inherit the threat model from prior secure LLM inference works which all assume a semi-honest\n(or passive) adversary that follows the protocol exactly but tries to learn information about the private\ninputs from the messages it sees during the protocol. This adversary controls an unknown subset\nof the MPC participants, where the size of the subset is defined by the MPC setting. Like prior\nworks, we also assume that the model architecture is public and the service only wants to hide the\nmodel weights. We formally prove security in Appendix C. We note that our work is not limited to a\nsemi-honest adversary and discuss extensions to malicious security in Appendix D."}, {"title": "Performance Characteristics of Secure Inference", "content": "Secure inference relies on secure multi-party computation (MPC) [22, 77], a cryptographic primitive\nthat allows mutually distrusting parties to compute any function on their private inputs without reveal-\ning anything beyond the function output. Prior secure inference works, specifically, have considered\nthree MPC settings (Appendix A), each making different assumptions about the participants. In this\nsection, we highlight the unique cost profile of MPC in these settings and discuss how it motivates\nthe design of our techniques in \u00a7 5.\nInteraction costs. Unlike plaintext computation, most operations within MPC require interaction\namong the MPC participants. This imposes two additional performance overheads in addition\nto computation size, namely, communication size and rounds of communication. For most MPC\nprotocols, this cost of interaction ends up being the bottleneck and it is the primary reason why MPC\nis orders of magnitude slower than plaintext computation.\nMultiplications with public weights come for free. Since MPC operates natively over integers,\nrecent secure inference works use fixed-point representation to emulate real-number arithmetic.\nAdditionally, prior works maintain the invariant that the intermediate state after every network layer\nis arithmetically secret-shared (ASS) among MPC participants. This approach minimizes the cost of\narithmetic operations, such as integer multiplications and additions, which dominate ML workloads.\nIn an ASS scheme, a secret value $x$ is split among $n$ MPC participants such that (i) each party $P_i$\nreceives a share $x_i$ and any set of $n-1$ shares reveals nothing about $x$, and (ii) the sum of all shares\nreconstructs the secret $x = x_1 + ... + X_n$. The linear nature of this reconstruction function allows\nsecret-shared values to be added locally (without interaction) by simply adding the corresponding\nsecret shares, making additions within MPC relatively so inexpensive that they are considered \"free\".\nSimilarly, any affine operation with public coefficients on secret-shared values, such as a matrix\nmultiplication with public weights, also becomes free. In \u00a7 5.2, we show how low-rank adaptations\ncan leverage this property to reduce the number of multiplications between secret-shared values.\nNon-arithmetic operations are the bottleneck in the most efficient MPC settings. Non-arithmetic\noperations are used to implement comparisons in maxpool, activation functions such as ReLU and\nGeLU, exponentiation and division in softmax, as well as the truncation operations in fixed-point\nmultiplications. We analyzed state-of-the-art secure inference frameworks (\u00a7 6.1) in the most efficient\nMPC settings, namely, 3PC and 2PC-Dealer (Appendix A), and found that non-arithmetic operations\naccount for over 88% of the runtime and communication during secure inference with a sequence\nlength of 2048. This is in stark contrast to plaintext computation where non-arithmetic operations\nhave a minimal contribution to the total FLOPs and the inference latency. Guided by this insight, we\nproposed head-merging in \u00a7 5.3, a technique that preserves the FLOPs and still yields significant\nperformance improvements.\nA mix of public and private weights typically does not speedup secure inference. Since multi-\nplications with public weights come for free, one would expect significant improvements to secure\ninference if most of the weights were public. However, to preserve the standard guarantees of the\nMPC, an intermediate state that depends on both the private input and any private weight must not\nbe revealed to any party. Consequently, once the computation involves a single private weight, all\nsubsequent non-arithmetic operations need to be performed within MPC, which as we just discussed\nare the bottleneck in the most efficient MPC settings for secure inference. This restriction motivated\nthe design of layer-freezing in \u00a7 5.1, which separates the public and private weights across layers\nsuch that the non-arithmetic operations in public layers are performed outside MPC."}, {"title": "Techniques", "content": "In this section, we describe our techniques that minimize the need for expensive operations within\nMPC. We start with layer-freezing (\u00a7 5.1) that reduces the number of layers evaluated within MPC.\nNext, we discuss LoRA (\u00a7 5.2) and head-merging (\u00a7 5.3) that minimize arithmetic and non-arithmetic\noperations, respectively, in the private layers. Distillation details are deferred to Appendix E.\n5.1 Layer Freezing\nOur starting point is the observation that when an open-source model is fine-tuned on a private dataset,\nonly the fine-tuned weights need to be kept private during inference. To leverage this insight, consider\nusing a technique from prior work that only fine-tunes a fraction of model weights [17]. However, as\nexplained in \u00a7 4, these techniques typically do not significantly speed up inference. This is because\nthey update weights throughout the network, including near the input, which means that almost all\nnon-arithmetic operations \u2013 typically the bottleneck must be performed within MPC.\nTo this end, our solution (Fig. 2a) effectively leverages public weights by deferring fine-tuning to only\nthe final layers of the transformer, thereby also deferring MPC to these final layers. During inference,\nthe client receives the weights for the bottom layers (identical to the open-source pre-trained model)\nfrom the server, computes the output of these layers locally, and then engages in MPC with the server\nfor the top layers. Consequently, if only a fraction $f$ of the layers are fine-tuned, all MPC overheads\nare reduced by a factor of $\\frac{1}{f}$ \u00d7 (Table 2). Although delegating the computation of the bottom layers to\nthe client might seem like a limitation, this approach actually reduces client overheads by the same\nfactor, since the MPC overhead on the client in secure inference protocols is orders of magnitude\nhigher than the overhead of plaintext inference\u00b3.\n5.2 LORA Adaptation\nIn \u00a7 4, we discussed how multiplication with public weights is free during secure inference. Here,\nwe demonstrate how LoRA [30], a technique developed for parameter-efficient fine-tuning, can be\nrepurposed to minimize integer multiplications during inference. These operations account for up to\n95% of the runtime in the state-of-the-art 2PC work Bumblebee [50]. Beyond the 2PC setting, we\nfound that multiplications also dominate the decoding (see Appendix B) runtime in 3PC and 2PC-\nDealer settings, which are otherwise bottlenecked by non-arithmetic operations (\u00a7 4). This occurs\nbecause the linear layers during decoding perform matrix-vector multiplications instead of matrix\nmultiplications, making key matrix-multiplication optimizations from [56] no longer applicable.\nA LORA adapter on a weight matrix $W \\in \\mathbb{R}^{n \\times k}$ is a product of two low-rank matrices $A \\in \\mathbb{R}^{n \\times r}$ and\n$B \\in \\mathbb{R}^{r \\times k}$, where $r \\lt \\text{min}(n, k)$. During fine-tuning, only the low-rank matrices are updated, and\nat inference time, $A \\times B$ is merged into the pre-trained weight $W$ to minimize inference overhead."}, {"title": "Head Merging", "content": "The most efficient secure inference works [16,\n39, 25] operate in the 3PC and the 2PC-Dealer\nsettings (Appendix A). In these settings, non-\narithmetic operations are the bottleneck. Among\nthese operations, those in the self-attention mod-\nule are of particular interest because: (i) the\nself-attention mechanism is the only compo-\nnent that scales quadratically with the sequence\nlength $b$, (ii) the state-of-the-art works in both\n3PC [16] and the 2PC-Dealer [25] settings ex-\nhibit a super linear blowup in runtime when\n$b \\geq 1024$, highlighting that self-attention is in-\ndeed the bottleneck for large $b$, and (iii) applica-\ntions such as chatbots and copilots which have\nreal-time requirements require a large sequence\nlength. Thus, we focus on minimizing the non-\narithmetic operations in the self-attention mod-\nule in this work.\nReducing number of heads. The self-attention mechanism has two non-arithmetic operations:\n(i) softmax, and (ii) truncations (from fixed-point multiplications), and the complexity for both is\n$O(b^2h)$, where $h$ is the #heads. Hence, we seek to reduce $h$ by a factor $m$ so that all operations are\nreduced proportionally. The standard technique for minimizing heads is head-pruning [53], which\nanalyzes the importance of each head over the training dataset, and prunes the insignificant heads.\nThis achieves our goal, however, we have to prune 75% of the heads (as well as their parameters) for\n$m = 4$, and this results in a large accuracy loss (\u00a7 6.4).\nPreserving the pre-trained parameters. To this end, we observe that unlike plaintext inference,\nFLOPs do not dictate the secure inference cost (\u00a7 4) and it is possible to achieve similar speedups as\nhead-pruning despite preserving all the parameters (\u00a7 6.4). This is also evident in the complexity of\nnon-arithmetic operations in self-attention, which are independent of the head-dimension $d$. Thus,\nwe propose a technique called head-merging that reduces the number of heads $h$ by $m$, while\nsimultaneously increasing the head dimension $d$ proportionally, thereby preserving all parameters\nfrom the pre-trained model. Specifically, $h$ heads are divided into groups of $m$, and the QKV matrices\nfor heads within the same group are concatenated as shown in Fig. 3. Concretely, given matrices\n${Q_i, K_i, V_i}_{i \\in [h]}$ of dimension $\\mathbb{R}^{b\\times d}$, the head attention outputs ${head_j}_{j \\in [h/m]}$ after merging are\nas follows: $head_j=\\text{softmax}(\\frac{(Q_{jm} \\|\\|...\\|\\|Q_{(j+1)m})\\text{K}^T}{\\sqrt{md}}).(V_{jm}\\|\\|...\\|\\V_{(j+1)m}) \\in \\mathbb{R}^{b\\times md}$\nMerging similar heads. In the expression above, adjacent heads are grouped such that heads jm to\n(j + 1)m belong to group j. This strategy does not consider the similarity among heads, resulting in\nminimal accuracy improvement over head-pruning (\u00a7 6.4). To group heads based on similarity, we\nfollow the strategy from [5] that computes the pairwise Jensen-Shannon distance between all heads\nwithin the same layer. Once we have the pairwise distances, we perform K-Medoid clustering [38] to\norganize heads into h/m groups. Finally, to get groups of the same size, we redistribute heads based\non a linear sum assignment that minimizes the sum of distances from the medoid within each group.\nWe found that merging similar heads using this method performs significantly better, leading to up to\n8% gain in accuracy \u00a7 6.4."}, {"title": "Evaluation", "content": "In this section, we first evaluate the secure inference cost (\u00a7 6.1) of MARILL-generated models and\ntheir ability to preserve ML performance (\u00a7 6.2). Next, we perform the same analysis for prior\nMPC-friendly approximations integrated with MARILL (\u00a7 6.3). Finally, we do an ablation study in\n\u00a7 6.4 that considers alternative designs for MARILL's techniques.\nSecure Inference Setup. We perform the secure inference experiments on state-of-the-art (open-\nsourced) frameworks in all MPC settings considered by prior work, namely, 2PC [50, 52], 3PC [16,\n52], and 2PC-Dealer [39, 73]. The experiments were run on two or three machines (depending on\nthe MPC setting) connected via LAN connection with 16 Gbps bandwidth and 0.1 ms latency. Each\nmachine was equipped with an Intel Xeon Platinum 8173M Processor with 16 vCPUs, 128 GB\nRAM, and a V100 GPU with 16 GB memory. Since the 2PC-Dealer framework [39] supports GPU\nacceleration, we ran it on the V100. Experiments on other MPC frameworks were run on CPU. All\nexperiments were multi-threaded. All reported numbers consider end-to-end costs.\nModels and Datasets. We consider three privacy-sensitive tasks for LLMs: chatbot, coding, and\nmachine translation. For the chatbot task, we fine-tune open-llama3b-v2 on the ShareGPT\ndataset and evaluate it on the MTBench dataset, following [82, 44]. OpenLLaMA is a popular\nopen-source model that replicates the LLaMA model [18, 69]. For the coding task, we fine-tune\ndeepseek-coder-1.3b-base on the MagiCoder dataset [75] and evaluate it on the HumanEval\nbenchmark [8]. For the machine translation task, we fine-tune open-llama3b-v2 on the ParroT\ndataset [35] and evaluate it on the WMT22 (De\u21d2En) benchmark [40].\nFine-Tuning Hyperparameters. We set the fine-tuning hyperparameters according to the papers that\ncurated the corresponding fine-tuning dataset: [82] for MTBench, [75] for HumanEval, and [35] for\nWMT22. We only vary the batch size and number of training epochs to better suit some techniques.\nFor instance, we observed that LoRA favors a smaller batch size in our setting. We include the\ndetailed hyperparameters in Appendix G."}, {"title": "Secure Inference Performance", "content": "In this section, we compare the secure inference performance of MARILL-generated models vs the\nbaseline a fully fine-tuned model. Fig. 4 summarizes these results for openllama-3b-v2 as the\npre-trained model. We first analyze the improvements from head-merging (\u00a7 5.3) and LoRA (\u00a7 5.2)\nin the three MPC settings from prior work, and then discuss layer-freezing (\u00a7 5.1) improvements.\n2PC: LORA improves the pre-filling runtime by 4.9\u00d7 (Fig. 4a) because 92% of the 2PC runtime is\nspent in performing multiplications for openllama-3b-v2 inference. Decoding runtime is improved\nby 2.2x, which is less pronounced because the 2PC framework [50] does not amortize well over"}, {"title": "ML Performance", "content": "Fig. 5 summarizes the ML performance of MARILL, the pre-trained model and the fully fine-\ntuned model on our three benchmarks. First, we note that full fine-tuning significantly improves\nthe performance of the pre-trained model across all three tasks. MARILL's layer-freezing is also\neffective on all three tasks, preserving 93 \u2013 100% of the full fine-tuning performance. On WMT\nand HumanEval benchmark, head-merging preserves 92 - 95% performance, while on MTBench,\nit achieves 87% performance. The combination of layer-freezing and head-merging works well,\nincurring an additional loss of at most 4% compared to head-merging alone. LoRA preserves over\n95% performance on all benchmarks. While combining LoRA with layer freezing sometimes leads\nto a big drop in performance (MTBench and HumanEval), we note that using LoRA alone provides\nsignificant speed-ups, ranging from 2.2\u00d7 to 4.9\u00d7. Overall, we observe that MARILL's techniques\ntypically preserve over 90% of the fully fine-tuned performance."}, {"title": "Integration of prior MPC-friendly approximations with MARILL", "content": "In this section, we analyze the performance of MARILL when combined with prior MPC-friendly ap-\nproximations, namely, Quad [45] and ReLU [10, 79] as GeLU/SiLU approximations, and 2Quad [45],\nL2Quad [81] and 2ReLU [56] as softmax approximation. First, we analyzed the ML performance of\neach approximation independently and found that the quadratic approximations from recent works\nled to a catastrophic loss on our benchmarks. Specifically, on the HumanEval benchmark, Quad only\nachieves 31.7% accuracy compared to 61% of the baseline, and the fine-tuning diverges for L2Quad\nand 2Quad, resulting in 0% accuracy. In contrast, ReLU-based approximations work very well, with"}, {"title": "Ablation Study", "content": "Layer-freezing vs layer-pruning. In layer-freezing, we froze the bottom layers of the transformer\nto move some layers outside of MPC. An alternative strategy to minimize layers within MPC is to\nsimply prune some layers. We experimented with layer-pruning on the HumanEval benchmark and\nevaluated the best-performing strategy from [64], namely, top-layer pruning. For half of the layers\npruned, we found that the accuracy drops from 61% for the baseline to just 49.4% post layer-pruning.\nIn contrast, layer-freezing achieved an accuracy of 56.7%, a 12% increase in relative performance,\nhighlighting the importance of preserving the pre-trained model weights of the pruned layers.\nHead-merging vs head-pruning. We compared head-pruning [53] and head-merging \u00a7 5.3 on\nHumanEval, configuring head-pruning to prune the same number of heads from each layer so that it\ndoes not leak additional information about the private dataset. Table 1b summarizes the results for\nboth techniques when the heads are reduced by 2\u00d7 and 4\u00d7. First, we note that head-merging achieves\nsimilar efficiency improvements to head-pruning for both head reduction factors, with head-pruning\nbeing at most 10% faster and 2% more communication efficient. ML performance of head-merging,\non the other hand, is much better since it preserves all the head parameters. In particular, head-merging\nhas up to 8% better accuracy than head-pruning, and HM= 4 even outperforms HP=2 in both ML\nand secure inference performance. Note that these improvements only apply to similar head-merging,\nnot adjacent head-merging, which na\u00efvely combines adjacent heads. These results demonstrate the\nsignificance of preserving head parameters as well as merging heads based on similarity."}, {"title": "Conclusion", "content": "In this work, we designed a framework MARILL, that leverages open-sourced LLMs and introduces\nhigh-level architectural changes during fine-tuning to minimize MPC usage during secure inference.\nWe demonstrated that MARILL is effective in minimizing secure inference costs across MPC settings\nin exchange for a reasonable accuracy tradeoff. In particular, MARILL-generated models are 2.4\n11.3\u00d7 more efficient for secure inference compared to a standard fine-tuned model, and they typically\npreserve over 90% relative performance across multiple challenging LLM tasks."}, {"title": "MPC Settings", "content": "\u2022 2-party computation (2PC): this setting assumes two MPC participants who do not trust each\nother, and thus, it is the most natural setting for secure inference.\n\u2022 Honest-majority 3-party computation (3PC): this setting has an additional helper party that also\nparticipates in MPC, and the adversary can corrupt at most any one of the three parties. Prior works\nconsidered this setting because having this helper party improves the MPC performance by orders\nof magnitude.\n\u2022 2PC with trusted dealer (2PC-Dealer): in this setting, there is an additional trusted dealer that is\nonly responsible for distributing input-independent correlated randomness to the computing parties\nin a pre-processing phase. The parties can then use this randomness to accelerate 2PC on their\nprivate inputs."}, {"title": "LLM Inference Stages - Prefilling and Decoding", "content": "In this section, we briefly describe the two stages in LLM inference. Firstly, users provide a prompt\nin natural language to the system. The system then uses tokenizers to map the natural language\ninto a vector x1,...In through a process called tokenization [66]. Then the system performs the\nmain inference process using LLMs. The inference process consists of two phases - the pre-filling\nphase and the decoding phase. Formally, the pre-filling phase computes probablity of the first token\nconditioned on the previous n tokens P(Xn+1|X1, ...In) [67]. It then samples from the distribution\nand predicts the first token Xn+1. The decoding phase iteratively computes the next token based\non the same logic. For instance, the first step in the decoding computes P(Xn+2|x1,...Xn+1) and\nsamples to obtain Xn+2. The decoding phase terminate when the new token is an ending token, often\nreferred to as the \u201cend-of-sentence\" token (EOS). Interestingly, the left-to-right decoding nature has\nmade the computation characteristics different [43, 78, 67] in these two stages. Thus, we distinguish\nbetween the two phases when evaluating our techniques in this work."}, {"title": "Security Proof", "content": "We prove the security of our protocol in the standard simulation paradigm [7, 22, 46] that argues that\nwhatever an adversary can learn in the real-world while interacting with honest parties in a protocol,\nit can also learn in an ideal-world while interacting with an ideal functionality that is incorruptible.\nIn particular, the proof shows that there exists a simulator in the ideal-world that can simulate the\nadversary's real-world view by only interacting with the adversary and the ideal-functionality. Since\nthe ideal functionality is designed to be trivially secure and not reveal anything about honest parties\ninputs beyond the function output, this proves that the adversary also can not learn this information\nfrom the actual protocol. We describe the ideal functionality F that captures the security guarantees\nprovided by any secure (transformer) inference protocol in Fig. 6. Note that the functionality does\nnot leak any information to the server, and the client learns nothing beyond the output tokens. The\nideal functionality also allows the client to choose the latest token, which is not a problem in the\nsemi-honest setting as the client will follow the protocol. We discuss how to ensure a malicious\nclient inputs the right tokens in Appendix D. We designed MARILL to make black-box use of"}, {"title": "Malicious Security", "content": "Our work is not limited to a semi-honest adversary and can also support a malicious adversary\nthat deviates from the protocol arbitrarily. Given a maliciously-secure protocol", "protocol": "n\u2022 In step 2", "23": "nproving that the hidden state it is secret-sharing corresponds to an actual sequence of tokens of the\nappropriate length.\n\u2022 The secure inference protocol will output the token as well as a hiding commitment and its\nrandomness to the client. Now", "follows": "n\u2022 The ideal functionality FM.n will track the generated tokens and abort if the token provided by the\nclient C does not match the last generated token. In case M's output and input don't match (as is\nthe case for Mpr), the functionality will be parameterized by the function Mp\u266d, and the functionality\nwill check that h received is equal to Mpb.evaluate(y), where y was the last generated token.\n\u2022 Since the adversary is now malicious, the simulator does not have direct access to its input"}]}