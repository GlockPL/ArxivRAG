{"title": "Low-Rank Compression for IMC Arrays", "authors": ["Kang Eun Jeon", "Johnny Rhe", "Jong Hwan Ko"], "abstract": "In this study, we address the challenge of low-rank model compression in the context of in-memory computing (IMC) architectures. Traditional pruning approaches, while effective in model size reduction, necessitate additional peripheral circuitry to manage complex dataflows and mitigate dislocation issues, leading to increased area and energy overheads. To circumvent these drawbacks, we propose leveraging low-rank compression techniques, which, unlike pruning, streamline the dataflow and seamlessly integrate with IMC architectures. However, low-rank compression presents its own set of challenges, namely i) suboptimal IMC array utilization and ii) compromised accuracy. To address these issues, we introduce a novel approach i) employing shift and duplicate kernel (SDK) mapping technique, which exploits idle IMC columns for parallel processing, and ii) group low-rank convolution, which mitigates the information imbalance in the decomposed matrices. Our experimental results demonstrate that our proposed method achieves up to 2.5\u00d7 speedup or +20.9% accuracy boost over existing pruning techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of in-memory computing (IMC) architecture heralds a transformative shift in the computing domain, primarily driven by the escalating demands for processing large-scale data on complex deep neural networks. By unifying computation and data storage, IMC overcomes the von Neumann bottleneck inherent in traditional architectures that separate memory and processing units. This integration facilitates direct matrix-vector multiplication (MVM) within the memory itself, exploiting the parallel computation capabilities for expedited processing at lower energy costs [1].\nDespite these advantages, IMC architectures face challenges in handling convolution operations, which require reshaping of convolutional weights and input data for MVM compatibility. The image-to-column (im2col) method [2] unrolls convolutional weights into IMC columns for MVM but often suffers from low column utilization. To address this, techniques such as shift and duplicate kernel (SDK) [3] and variable-window SDK (VW-SDK) [4] have been proposed. These methods enhance array utilization and computational performance by enabling input data reuse and parallel processing, effectively exploiting idle columns where duplicated kernels are situated.\nWhile mapping techniques [2]-[4] improve array utilization, they do not compress the weights themselves for additional performance gains. Pruning methods [4]-[8], particularly structured pruning [6], [7] tailored to the unique hardware constraints of IMC arrays, emerged as a promising solution. Structured pruning reduces the computational workload by omitting non-essential weights in a way that complements the IMC's MVM functionality.\nHowever, pruning techniques encounter hurdles due to the necessity of additional peripheral circuitry, such as zero-skipping hardware [9], [10] or multiplexers [7], to translate model sparsity into performance benefits (see Fig. 1). Zero-skipping hardware leverages sparsity by deactivating unnecessary wordlines-rows containing zero-valued weights-while multiplexers realign input data with pruned weights to counteract dislocation. These requirements introduce extra area and energy overheads, hindering the practical adoption of pruning methods despite their theoretical advantages.\nTo overcome the drawbacks associated with pruning, this research advocates for low-rank matrix decomposition technique as an alternative for compressing neural network weights. Unlike pruning, low-rank compression does not necessitate complex peripheral circuitry or realignment mechanisms, offering a more straightforward integration into IMC arrays. However, this method typically involves a trade-off between compression and accuracy, with low-rank compression often resulting in lower performance compared to pruning. Moreover, low-rank compressed matrices frequently lead to suboptimal utilization of IMC arrays. Our work, as shown in Fig. 1d, introduces new techniques, namely, SDK and group low-rank compression, aiming to balance accuracy retention, and IMC array utilization. Our experimental results show that"}, {"title": "II. BACKGROUNDS AND RELATED WORKS", "content": "IMC and Convolutional Weight Mapping. IMC architecture marks a paradigm shift towards memory-centric computing, where MVM operation is performed directly within the memory that hosts the deep learning model parameters. While IMC architecture is adept at MVM operations, it is not inherently equipped for convolution operations. To address this, convolutional weight mapping methods such as image to column (im2col) have been employed. Im2col, as illustrated in Fig. 2a and c, maps a sliding window of the input feature map (IFM) to the input port of the IMC array. Concurrently, it unrolls and maps each output channel of the kernel to the columns of the IMC array, thus facilitating the convolution operation in the form of MVM. However, since the number of utilized columns equals the number of output channels, the array utilization of im2col mapping is contingent upon the number of output channels. Hence, im2col mapping often delivers suboptimal array utilization with smaller convolutional filters and consequently resulting in additional computing cycles.\nTo address the low array utilization issue of im2col [2], Zhang et al. [3] and Rhe et al. [4] proposed shift and duplicate kernel (SDK) mapping method. The SDK method uses parallel window (PW) and duplicated kernels to facilitate parallel processing of multiple sliding windows concurrently unlike the single window processing inherent to the im2col method. By situating duplicated kernels in previously idle columns of the IMC array, the SDK method significantly enhances array utilization. The extent of this enhancement is governed by the size of the PW; for instance, employing a 4\u00d74 PW allows for the duplication of three additional kernels, thereby increasing the number of simultaneously processed sliding windows. However, as illustrated in Fig. 2b and d, SDK mapping introduces structural sparsity by its very nature. Specifically, larger PW sizes improve idle column utilization, but at the expense of increased sparsity within the rows. Based on the generated mapping, the computing cycle of IMC array can be calculated, as proposed by Rhe et al. [4], using array row (AR) and array column (AC) cycles. AR cycle defines the number of arrays required to process the rows in a mapping, and AC cycle the columns.\nPruning Methods and Challenges on IMC. Weight pruning technique [4], [8] is a strategy to reduce computational requirements by eliminating redundant or non-contributory weights within neural networks. Within the IMC community, recent pruning techniques are tailored to compress the weight matrix to fit the constraint of IMC arrays, thereby enhancing computational efficiency. For example, Rhe et al. [4] have proposed the column-wise pruning method to exploit the structural column sparsity of the weight matrix through channel pruning, achieving 1.38\u00d7 inference speed in ResNet-20. Similarly, pattern-based pruning has been used to compress the weight"}, {"title": "Low-Rank Compression", "content": "To exploit the low-rank properties inherent in neural network weights, various low-rank compression techniques have been applied with considerable success [14]. While the low-rank compression method is often considered to be less effective compared to other compression methods, it holds a significant advantage in terms of fast inference, especially on GPUs [15]. This is attributed to its use of dense matrices, which exhibit local, regular, and parallelizable memory access patterns, facilitating quicker computations. The previous research efforts [15], [16], while pioneering in advancing low-rank compression techniques for deep neural networks, are mostly tailored for optimization on GPUs. However, this focus has inadvertently left a gap in the exploration of low-rank compression techniques for other forms of hardware, particularly IMC arrays. IMC arrays, known for their potential to significantly reduce energy consumption and latency in performing matrix operations, present a unique architecture that could benefit from specialized compression methods. Yet, the application of low-rank compression within the context of IMC arrays remains unexplored, signifying a critical research gap. This oversight underscores the need for a dedicated investigation into how low-rank compression techniques can be adapted or reimagined to exploit the distinctive advantages and architecture of IMC arrays, a challenge that our current research endeavors to address."}, {"title": "III. MOTIVATION", "content": "Given a weight matrix $W \\in \\mathbb{R}^{m\\times n}$, low-rank decomposition approximates it as $W = LR$, where $L \\in \\mathbb{R}^{m\\times k}$ and $R \\in \\mathbb{R}^{k\\times n}$. Each element $w_{ij}$ is the dot product of the $i$th row of $L$ and the $j$th column of $R$ (see Fig. 3). The parameter $k$ balances approximation accuracy and computational savings; smaller $k$ means more compression but potentially more information loss. The adaptation of the low-rank compression technique, which involves decomposing a larger matrix into two smaller ones, to IMC architecture presents two significant impediments: low array utilization and diminished accuracy in machine learning tasks. Fig. 4 illustrates the computational difficulties of applying low-rank matrix compression within an IMC framework. For instance, when original, uncompressed convolutional weights are mapped onto IMC arrays using the prevalent im2col mapping strategy, a rectangular-shaped weight matrix, $W$, is produced. This matrix extends across more rows than columns and requires three computing cycles to generate a single output, as shown in Fig. 4a. Conversely, Fig. 4b showcases low-rank compression on IMC arrays, where $W$ is decomposed into two matrices that do not fully utilize the IMC array's capacity. This decomposition, intended to reduce computational load, paradoxically introduces an additional computing cycle due to low array utilization.\nMoreover, the inherent rectangular shape of convolutional kernels leads to a significant imbalance in information encoding between the $L$ and $R$ matrices. This imbalance causes a notable loss of information in the weight matrix's rows, thereby reducing computation accuracy, a crucial aspect for the effectiveness of neural network models. To address the first challenge of low array utilization, we propose the integration of SDK mapping with the low-rank compression technique. This approach enhances array utilization through input data reuse and the added parallelism of duplicated kernels. For the second challenge, concerning reduced machine learning task accuracy, we introduce grouped low-rank decomposition. This method partitions the weight matrix into multiple groups prior to low-rank compression, effectively mitigating the information imbalance initially present in $L$, while capturing essential weight features with a minimal increase in parameters."}, {"title": "IV. PROPOSED METHOD", "content": "Group Low-Rank Compression. To address the severe accuracy degradation and low row utilization issues associated with the $L$ matrix in traditional low-rank approximations, we propose a group low-rank decomposition technique, as illustrated in Fig. 5a. In this approach, the weight matrix is partitioned into $g$ submatrices or groups, denoted in block matrix notation as $W = [W_1, W_2, ..., W_g]$, where $g$ is the number of groups. Each submatrix $W_i$ is then independently compressed using low-rank decomposition:\n$D_g(W) := [D(W_1), D(W_2), \u2026, D(W_g)]$.   (1)\nHere, $D_g(\\cdot)$ denotes the grouped low-rank decomposition operator for a specified number of groups $g$, and $D(\\cdot)$ represents the traditional low-rank decomposition operator without matrix partitioning, such that $D(W_i) := L_iR_i$.\nTheorem 1. Given a weight matrix $W$ and a target rank $k$, the reconstruction error of its group low-rank approximation, $E_g := ||W - D_g(W)||_F$, is upper-bounded by that of the traditional low-rank approximation, $\\epsilon := ||W - D(W)||_F$, for an arbitrary number of groups, $g$:\n$||W - D_g(W)||_F \\le ||W - D(W)||_F$  (2)\n$E_g$         $\\epsilon$\nwhere both reconstruction errors are measured in Frobenius norm, denoted by $F$.\nProof. We begin by approximating $W$ using truncated singular value decomposition (SVD), i.e., $D(W) = U\\Sigma V^T$. We know that this is an optimal approximation with respect to the Frobenius norm according to the Eckart-Young-Mirsky theorem. The decomposed matrices can be expressed in a block matrix form:\n$D(W) = L  [R_1, R_2, \u2026, R_g]$\n$\\{ U\\Sigma\\atop TV^T}\\[R_1, R_2, \u2026, R_g]$\nwhere $L=UE$ and $R$ is the $i$-th submatrix of $V^T$ which is partitioned into $g$ groups.\nFollowing the distributive property of block matrices, $L$ is multiplied with all $R_i$ matrices, approximating $W_i$ (i.e., $W_i \\approx L R_i$). However, according to the Eckart-Young-Mirsky theorem, we know that $L R_i$ is not necessarily the optimal approximation of $W_i$, since $L R_i$ may not be the SVD of $W_i$. Hence,\n$||W_i - D(W_i)||_F \\le ||W_i - LR_i||_F \\forall i$   (4)\nwhere $D(W_i)$ is the truncated SVD of $W_i$. Note that RHS represents the reconstruction error of $W$ of the group low-rank compression method, and LHS represents that of the traditional method."}, {"title": "SDK for Low-Rank Compression", "content": "To improve on the low column utilization issue, we seek to integrate SDK mapping [4] together with the low-rank decomposition technique. Since the SDK mapping inherently uses more columns than the im2col mapping, its low-rank decomposed version should also utilize more columns for parallel processing. However, the formulation to derive the low-rank decomposition of SDK mapping is non-trivial. To this end, we first propose a rigorous mathematical description of the SDK mapping method and then derive a low-rank decomposition formula with respect to the SDK mapping.\nTheorem 2. Given a weight matrix $W$, and its low-rank decomposed matrices, $L$ and $R$, low-rank approximation of the SDK mapping of $W$ is given by:\n$D(SDK(W)) = (I_N \\otimes L) SDK(R)$  (6)\nwhere $I_N$ is the identity matrix of size $N \\times N$, $N$ is the number of parallel outputs in the SDK mapping, $ \\otimes$ denotes the Kronecker product, and SDK($\\cdot$) denotes the SDK operator that generates SDK mapping for a given matrix.\nProof. A convolutional kernel matricized by im2col mapping method can be described as $W = [W_1, W_2,\u2026, W_m]$ where $w_i \\in \\mathbb{R}^{1 \\times n}$ is a vectorized output channel of a convolutional kernel. Then the SDK mapping, SDK(W) $ \\in \\mathbb{R}^{Nn \\times b}$ can be expressed as a linear transformation of $W$.\n$SDK(W) = [P_1W^T, P_2W^T,\u2026, P_NW^T]^T$\n$ = \\begin{bmatrix}\nW & 0 & 0 \\\\\n0 & W & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n0 & 0 & W\n\\end{bmatrix}\\begin{bmatrix}\nP_1^T \\\\\nP_2^T \\\\\n\\vdots \\\\\nP_N^T\n\\end{bmatrix} \\newline \\qquad \\qquad \\qquad \\qquad W \\in \\mathbb{R}^{Nn \\times b}$ (7)\nwhere $P_s \\in \\mathbb{R}^{b \\times n}$ is the $s$-th padding matrix. $N$ is the total number of parallel outputs, which is determined by the PW dimension, and $b$ is the input dimension of the flattened PW. The role of the padding matrix is to insert zero column vectors into $W$, such that the elements of the kernels are appropriately shifted and aligned with the PW input. $P_s$ can be built from a square identity matrix followed by the insertion of zero row vectors in a specific pattern that is dictated by the SDK mapping. Then the element of $P_s$ at index $i, j$ is defined as:\n$[P_s]_{i,j} = \\begin{cases}\n1 & \\text{ if } i = f(j) \\\\\n0 & \\text{ otherwise}\n\\end{cases}$   (8)\nwhere $f()$ is a mapping function that describes the insertion locations of the zero column vectors.\nNow we can substitute low-rank compressed matrix of the im2col mapping, $W = LR$, in to equation (7).\n$SDK(W) = [P_1R^TL^T, P_2R^TL^T,\u2026, P_NR^TL^T]^T$ (9)\nThen, instead of factoring out the entire $LR$, which would give us the equivalent formulation as in (7), we can solely factor out $L$ in the form of block diagonal matrix:\n$\\begin{bmatrix}\nL & 0 & & 0 \\\\\n0 & L & & 0 \\\\\n\\vdots & & \\ddots & \\vdots \\\\\n0 & 0 & & L\n\\end{bmatrix}\\begin{bmatrix}\nP_1R^T \\\\\nP_2R^T \\\\\n\\vdots \\\\\nP_NR^T\n\\end{bmatrix}$  (10)"}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "Experimental Setup. To evaluate and demonstrate the effectiveness of the proposed method, we employed ResNet-20 and Wide-ResNet16-4 (WRN16-4) for image classification tasks on CIFAR-10 and CIFAR-100 datasets, respectively. Here, ResNet-20 was trained with expansion parameter set to 1 (i.e., the first basic block has 16 input/output channels). Weights and activations of all deep learning models were both quantized 4 bit, and the models were trained following the quantization aware training framework proposed in [17]. We did not compress the very first convolution layer and the last linear layer, as they are known to be highly sensitive to perturbations and are often processed on digital computing units that support floating point operations [3], [4]. Proposed low-rank compressed models were trained from scratch for 250 epochs, where as the pattern-pruned counterparts were fine-tuned for 20 epochs from a pre-trained model. The pre-trained model was trained for 200 epochs. We experimented for three trials using different seeds.\nComparisons with pattern pruning methods. Table I presents the model accuracy and computing cycle for a low-rank compressed model for different combinations of group and rank. The rank of each layer was configured uniformly to the number of output channels, m, divided by a constant factor, in this case, 2, 4, 8, and 16. Also, the number of groups is set to either 1, 2, 4, or 8. Fig. 6 presents a comprehensive overview and comparisons of the proposed low-rank compression method versus the existing pattern-pruning approaches. Baseline accuracies and computing cycles of unpruned models are presented by the orange dotted line. The first row of figures presents experiments conducted on ResNet-20, whereas the second row shows the results for WRN16-4. For pattern-pruning baselines, we plotted the results for entries ranging from 1 to 8, whereas, for our proposed method, we selectively plotted the combinations of rank and group that form the Pareto front for conciseness and clarity. The result demonstrates the effectiveness of the proposed compression method, achieving on-par performance with pattern-pruning approaches on ResNet-20, and significantly outperforming them on WRN16-4. From Fig. 6d, we can see that our proposed approach can achieve up 2.5\u00d7 speedup and +20.9% accuracy boost compared to the pruning counterparts.\nTo evaluate and compare the hardware performance, we have built a simulator based on NeuroSIM [18] and ConvMapSIM [19] that measures the energy consumption of the proposed and pattern pruning methods. We measured the energy consumption for both ResNet-20 and WRN16-4 networks for varying array sizes. Fig. 7 plots the normalized energy consumption of the two compression methods against im2col method. For low-rank compressed models, we employ the model with group = 4 and rank = m/8, which exhibits high accuracy (less than 1 or 2% drop from the uncompressed model) while achieving significant computing cycle reduction. For pattern-pruned models, we employ the model pruned with 6-entries, which achieves almost identical accuracy performance as our low-rank model. The results show that the proposed method is more energy-efficient than the pattern-pruned models for both networks across all array dimensions. For smaller arrays, the proposed method could improve energy saving by up to 71% when compared against the pattern-pruning method and up to 80% against im2col method.\nWe highlight once again, that unlike pruning approaches that necessitate additional peripheral circuitry to combat misalignment and dislocation issues, the proposed method can be adopted on any IMC array and is free of such overheads, and yet achieves better performance in both accuracy and computing cycles. This result is significant and underscores the potential impact of the proposed method when integrated with various deep learning networks and IMC architectures.\nComparisons with quantization methods. To enrich our evaluation, we also compare our proposed method against quantization with varying bit precision. We trained dedicated 1, 2, 3, and 4 bit quantized models of ResNet-20 using a QAT framework and a DoReFa quantizer. The accuracies and computing cycles of the quantized models for array dimensions of 64\u00d764 and 128\u00d7128 are plotted in Fig. 8. It can be seen that the proposed low-rank compression method outperforms quantized models, achieving up to 1.8\u00d7 speed-up.\nComparisons with traditional low-rank compression. As shown in Fig. 9 and Table I, the proposed method consistently outperforms the traditional low-rank compressed baseline models (where the proposed SDK mapping and group low-rank compression technique are not applied). Whereas the prior low-rank method can reduce the computing cycles to 54K and 40K in the WRN16-4 and ResNet-20 networks, respectively, the proposed method significantly reduces them to 37K in WRN16-4 and 25K in ResNet-20. This is equivalent to 1.5\u00d7 and 1.6\u00d7 speedup in WRN16-4 and ResNet-20, respectively, due to better array utilization with SDK mapping. The gain is more notable on larger arrays, where SDK mapping can be better explored for more parallel computation. On the other hand, the proposed method also boasts significant boosts in accuracy even at lower values of rank, thanks to the use of group low-rank compression. It can be seen from Table I, that with the increasing number of groups, even with just 2, we witness significant mitigation of accuracy drop."}, {"title": "VI. CONCLUSION", "content": "In this study, we tackled the challenge of efficiently compressing models tailored to IMC architectures to enhance computational efficiency without the significant area and energy overheads typical of traditional pruning methods. Our approach introduced low-rank compression techniques integrated with novel SDK and group low-rank convolution strategies, mitigating issues such as suboptimal IMC array utilization and accuracy compromises. Through rigorous experiments on ResNet-20 and WRN16-4 using CIFAR-10 and CIFAR-100 datasets, our method demonstrated its potential by matching or surpassing the performance of existing pruning techniques while significantly reducing computational cycles. This research not only offers a viable alternative to conventional pruning but also opens new avenues for optimizing deep neural networks for IMC architectures, offering paving the way for their more efficient deployment in real-world applications."}]}