{"title": "shapiq: Shapley Interactions for Machine Learning", "authors": ["Maximilian Muschalik", "Hubert Baniecki", "Fabian Fumagalli", "Patrick Kolpaczki", "Barbara Hammer", "Eyke H\u00fcllermeier"], "abstract": "Originally rooted in game theory, the Shapley Value (SV) has recently become an important tool in machine learning research. Perhaps most notably, it is used for feature attribution and data valuation in explainable artificial intelligence. Shapley Interactions (SIs) naturally extend the SV and address its limitations by assigning joint contributions to groups of entities, which enhance understanding of black box machine learning models. Due to the exponential complexity of computing SVs and SIs, various methods have been proposed that exploit structural assumptions or yield probabilistic estimates given limited resources. In this work, we introduce shapiq, an open-source Python package that unifies state-of-the-art algorithms to efficiently compute SVs and any-order SIs in an application-agnostic framework. Moreover, it includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values to systematically assess computational performance across domains. For practitioners, shapiq is able to explain and visualize any-order feature interactions in predictions of models, including vision transformers, language models, as well as XGBoost and LightGBM with TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and consolidate the application of SVs and SIs in machine learning that facilitates future research. The source code and documentation are available at https://github.com/mmschlk/shapiq.", "sections": [{"title": "1 Introduction", "content": "Assigning value to entities collectively performing a task is essential in various real-world applications of machine learning (ML) [60, 71]. For instance, when reimbursing data providers based on the value of data [30, 76], or justifying a model's prediction based on value of feature information [13, 18, 19, 52, 73]. The fair distribution of value among a group of entities is a central aspect of cooperative game theory, where the Shapley Value (SV) [72] defines a unique allocation scheme based on intuitive axioms. The SV is applicable to any game, i.e. a function that specifies the worth of all possible groups of entities, called coalitions. In ML, application-specific games were introduced [5, 30, 71, 76, 80], which typically require a definition of the overall worth and a notion of entities' absence [19]. The SV fairly distributes the overall worth among individuals by evaluating the game for all coalitions. However, it does not give insights on synergies or redundancies between entities. For instance, while two features such as latitude and longitude convey separate information, only their joint consideration reveals the synergy of encoding an exact location. The value of such a group of entities is known as an interaction [33], or in this context feature interaction [29], and is crucial to understand predictions of complex ML models [20, 45, 46, 58, 63, 74, 78, 82], as illustrated in Figure 1.\nShapley Interactions (SIs) [10, 33, 74, 77] distribute the overall worth to all groups of entities up to a maximum explanation order. They satisfy axioms similar to the SV, to which they reduce for individuals, i.e. the lowest explanation order. In contrast, for the highest explanation order, which comprises an interaction for every coalition, the SIs yield the M\u00f6bius Interaction (MI), or M\u00f6bius transform [27, 35, 69]. The MIs are a fundamental concept in cooperative game theory that captures the isolated joint contribution, which allows to additively describe every coalition's worth by a sum of the contained MIs. With an increasing explanation order, the SIs comprise more components that finally yield the MIs as the most comprehensive explanation of the game at the cost of highest complexity [10, 77]. While the SV and SIs provide an appealing theoretical concept, computing them without structural assumptions on the game requires exponential complexity [22]. For tree-based models, it was shown that SVs [51, 83] and SIs [58, 84] can be efficiently computed by exploiting the architecture. Moreover, game-agnostic stochastic approximators estimate the SV [11, 17, 43, 52, 54, 61, 65] and SIs [28, 29, 44, 74, 77] with a limited budget of game evaluations.\nDiverse applications of the SV have led to various techniques for its efficient computation [13]. Recently, extensions to any-order SIs addressed limitations of the SV and complemented interpretation of model predictions with higher-order feature interactions [10, 29, 74, 77, 78]. While stochastic approximators are applicable to any game, their evaluation is typically performed in an isolated application [29, 47], such as feature interactions. Moreover, implementing such algorithms requires a strong mathematical background and specific design choices. Existing Python packages, such as shap [52], provide a relatively small number of approximators, which are limited to the SV and feature attributions.\nContribution. In this work, keeping within the scope of the NeurIPS 2024 Datasets & Benchmark track, we present shapiq, an open-source Python library for any-order SIs that consolidates research for computing SVs and SIs across ML domains. Therein, we contribute\n1. a general approximation interface for state-of-the art SI algorithms and methods without focus on a specific application like explanations or data valuation,\n2. an explanation API for using SIs to explain ML models and visualizing interactions,\n3. a benchmarking suite to evaluate SI approximators across several real-world scenarios,\n4. and a cross-domain empirical evaluation of approximators guiding practitioners."}, {"title": "2 Theoretical Background", "content": "In ML, various concepts are based on synergies of entities to optimize performance in a given task. For example, weak learners construct powerful model ensembles [70], collected data instances and features are used to train supervised ML models [16, 30], where feature values collectively predict outputs. To better understand such processes, XAI quantifies the contributions of these entities to the task, most prominently for feature values in predictions (local feature attribution [52, 73]), features in models (global feature importance [16, 17, 66]), and data instances in model training (data valuation [30]). Assigning such contributions is closely related to the field of cooperative game theory [71], which studies the notion of value for players that collectively obtain a payout. To adequately assess the impact of individual players, it is necessary to analyze the payout for different coalitions. More formally, a cooperative game $v : P(N) \\rightarrow R$ with $v(0) = 0$ is defined by a value function on the power set of $N := \\{1, ..., n\\}$ entities, which describes such payouts for all possible coalitions of players. We later summarize such prominent examples in the context of ML in Table 3. Here, we summarize existing contribution concepts for individuals and groups of entities, outlined in Table 1.\nThe SV [72] and Banzhaf Value (BV) [8] are instances of semivalues [23]. Semivalues assign contri- butions to individual players and adhere to intuitive axioms: Linearity enforces linearly composed contributions for linearly composed games; Dummy requires that players without impact receive zero contribution; Symmetry enforces that entities contributing equally to the payout receive equal value. The SV [72] is the unique semivalue that additionally satisfies efficiency, i.e. the sum of all contributions yields the total payout $v(N)$. In contrast, the BV [8] is the unique semivalue that additionally satisfies 2-Efficiency, i.e. the contributions of two players sum to the contribution of a joint player in a reduced game, where both players are merged. The SV and BV are represented as a"}, {"title": "3 Overview of the shapiq Python package", "content": "The shapiq package accelerates research on SIs for ML, and provides a simple interface for ex- plaining any-order feature interactions in predictions of ML models. Its code is open-source on GitHub at https://github.com/mmschlk while the documentation with notebook examples and API reference is available at https://shapiq.readthedocs.io."}, {"title": "3.1 shapiq Facilitates Research on Shapley Interactions for Machine Learning", "content": "Approximators. We implement 7 algorithms for approximating SIs across 4 different interaction indices, and another 7 algorithms for approximating SVs. Table 2 provides a comprehensive overview of this effort, where the shapiq.Approximator class is extended with each implementation. We unify common approximation methods by including a general shapiq. CoalitionSampler interface offering approximation performance increases through sampling procedures like the border- and pairing-tricks introduced in [17, 29]. Algorithms are primarily benchmarked based on how well they approximate the ground truth SIs values that often cannot be computed in practice due to exponential complexity and constrained resources.\nExact computer. A key functionality of shapiq lies in computing the SIs exactly, which is feasible for smaller games, but reaches its limit for growing player numbers. The shapiq. ExactComputer class provides an interface for computing 18 interaction indices and game-theoretic concepts, includ- ing the MIs (see Table 1).\nGames. Approximators and computers work given a specified cooperative game. Table 3 describes in detail 11 benchmark games implemented in shapiq. Beyond synthetic games, our benchmark spans the 5 most prominent domains where SIs can be applied for ML. The shapiq. Game class can be easily extended to include future benchmarks in the package. We pre-compute and share exact SIs for 2042 benchmark game configurations in total (Appendix B), facilitating future work on improving the approximators, which we elaborate on further in Section 4."}, {"title": "3.2 Explaining Machine Learning Predictions with shapiq", "content": "Explainer. The shapiq. Explainer class is a simplified interface to explain any-order feature interactions in ML models. Figure 2 goes through exemplary code used to approximate SIs for a single prediction and visualize them on a graph plot. Currently two classes are further distinguished within the API, but we envision extending shapiq. Explainer to include more data modalities and model algorithms. shapiq. TabularExplainer allows for model-agnostic explanation based on feature marginalization with either marginal or conditional imputation (refer to Appendix C for details). shapiq.TreeExplainer implements TreeSHAP-IQ [58] for efficient explanations specific to decision tree-based models, e.g. random forest or gradient boosting decision trees, with native support for scikit-learn [64], xgboost [15], and lightgbm [38]. Figure 3 goes through exemplary code for explaining a set of predictions and visualizing their aggregation in a bar plot, which represents the global feature interaction importance.\nUtility functions. shapiq offers additional useful tools that are described in detail in the documenta- tion. Interaction values are stored and processed using the shapiq. InteractionValues data class, which is rich in utility functions. The shapiq. plot module supports the visualization of interaction values, including our custom network plot, but also wrapping the well-known force and bar plots from shap [52]. Finally, shapiq.datasets loads datasets used for testing and examples."}, {"title": "4 Benchmarking Analysis", "content": "The shapiq library enables computation of various SIs for a broad class of application domains. To illustrate its versatility, we conduct benchmarks across a wide variety of traditional ML-based SV application scenarios. The ML benchmark demonstrates how higher-order SIs enable an accuracy-complexity trade-off for model interpretability (Section 4.1) and highlights that different approxi- mation techniques in shapiq achieve the state-of-the-art performance depending on the application domains (Section 4.2). Tables 3, 4 and 5 present an overview of different application domains and associated benchmarks. Depending on the benchmark, it can be instantiated with different datasets, models, player numbers or benchmark-specific configuration parameters, e.g. uncertainty type: epis- temic for Uncertainty Explanation or imputer: conditional for Local Explanation. In total, shapiq offers 100 unique benchmark games, i.e. applications times dataset-model pairs.\nFor all games that include $n < 16$ players, the value functions have been pre-computed by evaluating all coalitions and storing the games to file. Reading a pre-computed game from file, instead of performing up to $2^{16} = 65536$ value function calls with each new experiment run, saves valuable computational time and contributes to reproducibility as well as sustainability. This is particularly"}, {"title": "4.1 Accuracy \u2013 Complexity Trade-Off of Shapley Interactions for Interpretability", "content": "In this experiment, we empirically evaluate faithfulness of SIs for varying explanation orders $k$. To this end, we rely on the Shapley-weighted faithfulness $L(\u03bd, \u03a6_k)$ introduced in Section 2. SIs range from the SV (least complex) to the MI (most complex) explanation, where the SV minimizes $L(\u03bd, \u03a6_1)$ for $k = 1$ [12, 52], and MIs are faithful to all game values with $L(v, \u03a6_n) = 0$ for $k = n$ [10, 77]. Figure 4 shows the Shapley-weighted $R^2$ value for $k = 1, . . ., n$ for a synthetic game with a single interaction of varying size (a) and real-world ML applications (b). Here, we used $\u03bc_\u221e = 1$ instead of $\u03bc_\u03b1 \u226b 1$, which affects FBII that violates efficiency. The results show that in general SIs become more faithful with higher explanation order. Notably, the difference between pairwise SIs and SVs (SHAP textbox) is remarkable, where pairwise interactions ($k = 2$) already yield a strong improvement in faithfulness. If higher-order interactions dominate, then SIs require a larger explanation order to maintain faithfulness. While FSII and FBII are optimized for faithfulness, STII and $k$-SII do not necessarily yield a strict improvement in this metric. In fact, it was shown that SII and $k$-SII optimize a slightly different faithfulness metric, which changes for every order [28]. Yet, we observe a consistent strong improvement of pairwise $k$-SII over the SV (SHAP). While FSII and FBII optimize faithfulness, $k$-SII and STII adhere to strict structural assumptions, where STII projects all higher-order interactions to the top-order SIs, and $k$-SII is consistent with SII. Pracitioners may choose SIs tailored to their specific application, where $k$-SII is a good default choice for shapiq."}, {"title": "4.2 Comparison of Approximation Methods", "content": "Various approximation methods for computing SIs are included in shapiq for a variety of SIs (cf. Table 2). The possibility of attributing (domain-specific) state-of-the-art performance to a single algorithm has been investigated empirically by multiple works [28, 29, 43, 44, 54, 61, 65, 77, 79]. We use the collection of 100 unique benchmark games in shapiq to evaluate the performance of different SV and SI approximation methods on a broad spectrum of ML applications. For each domain and configuration (see Table 4 and 5 in Appendix B), we compute ground truth SVs, 2-SIIs, and 3-SIIs and compare them with estimates provided by all approximators from Table 2. The approximators are run with a wide range of budget values and assessed by their achieved mean squared error (MSE) or precision at five (Precision@5). Figure 5 summarizes the approximation results.\nMost notably, the ranking of approximators varies strongly between the different applications domains, which is depicted in Figure 5 (a) and (b). This observation holds for both SVs and SIs. In general, two types of approximation methods dominate the application landscape in terms of MSE and Precision@5. First, kernel-based approaches including KernelSHAP, KADD-SHAP, KernelSHAP-IQ and Inconsistent KernelSHAP-IQ perform best for Local Explanation, Uncertainty Explanation, and Unsupervised Feature Importance. Second, the two stratification-based estimators SVARM and SVARM-IQ achieve state-of-the-art performance for Data Valuation, Dataset Valuation, or Ensemble Selection. Traditional mean-estimation methods including Permutation Sampling (SV and SII), Unbiased KernelSHAP, SHAPIQ, and Owen Sampling achieve moderate estimation qualities in comparison. Our findings give rise to the conclusion that stratification-based estimators perform superior in settings where the size of a coalition naturally impacts its worth (e.g. training size for Dataset Valuation), which is plausible as these methods group coalitions by size and thus leverage this dependency. Meanwhile, kernel-based estimators achieve state-of-the-art in settings where the dependency between size and worth of a coalition is less pronounced (e.g. sudden jumps of model predictions in Local Explanation)."}, {"title": "5 Conclusion", "content": "As SIs are increasingly employed to analyze ML models, it becomes pivotal to ensure that these are accurately and efficiently approximated. To this end, we contributed shapiq, an open-source toolbox that implements state-of-the-art algorithms, defines a dozen of benchmark games, and provides ready-to-use explanations of any-order feature interactions. shapiq comes with a comprehensive documentation and is designed to be extendable by contributors.\nLimitations and future work. We identify three main limitations of shapiq that provide natural opportunities for future work. First, the TreeSHAP-IQ algorithm is currently implemented in Python, but by-design requires no access to model inference, which allows for a more efficient implementation in C++ alike TreeSHAP [51, 83]. Second, SIs can be misinterpreted based on choosing the wrong index for the application scenario, which we comment on across Sections 2 and 4.1. The selection of a particular SI index, enabled by shapiq, offers great opportunities for application-specific research. We also acknowledge that visualization of higher-order feature interactions is itself challenging and a potential research direction in human-computer interaction. Certainly, a human-centric evaluation of explanations may be required for their broader adoption in practical applications [68].\nBroader impact. A potential negative societal implication of visualizing higher-order feature interactions may be an information overload [7, 67] that leads to users misinterpreting model explanations. Nevertheless, we hope our contribution sparks the advancement of game-theoretical indices motivated by various applications in ML. Specifically in the context of explainability, shapiq may impact the way users interact with ML models when having access to previously inaccessible information, e.g. higher-order feature interactions."}, {"title": "A Extended Theoretical Background", "content": "In this section, we introduce further theoretical background. Specifically, we discuss in more detail the class of GVs and IIs in Appendix A.1, and the MIs in Appendix A.2.\nA.1 Probabilistic and Cardinal-Probabilistic GVs and IIs\nProbabilistic GVs [55] extend semivalues with a focus on monotonicity, i.e. games that satisfy $\u03bd(S) \u2264 \u03bd(T)$, if $S \u2286 T \u2286 N$. GVs satisfy the positivity axiom, which requires non-negative joint contributions, i.e. $GV(S) \u2265 0$, for all $S \u2286 N$ in monotone games [55]. It was shown that GVs are uniquely represented as weighted averages over (joint) marginal contributions $v(T\u222aS) \u2013 v(T)$. On the other hand, cardinal-probabilistic IIs [27] are centered around synergy, independence and redundancy between entities. IIs are based on discrete derivatives, which extend (joint) marginal contributions by accounting for lower-order effects. IIs focus on $k$-monotonicity, i.e. games that have non-negative discrete derivatives $\u2206_S(T) \u2265 0$ for $S \u2286 U \u2286 N$ with $2 \u2264 |S| \u2264 k$. IIs satisfy the $k$-monotonicity axiom, i.e. non-negative interactions $I(S) \u2265 0$ for $k$-monotone games. Both, GVs and IIs are uniquely represented [27, 55] as\n$GV(S) := \\sum_{T\u2286N\\backslash S} P_T^{(n)}. (n) (v(T\u222aS) \u2013 \u03bd(T))$ and $I(S) := \\sum_{T\u2286N\\backslash S} P_T^{(n)}. \u0394_S(T)$,\nwhere $p_T^{(n)}$ are index-specific weights based on the sizes of $S, T$ and $N$. The SGV [56] and the SII [33] with\nShapley: $p_t^{(n)} = \\frac{1}{(n-|s|+1) {n\\choose |S|}^{-1}}$\nnaturally extend the SV. An alternative extension for the SV is the Chaining GV (CHGV) [55] and Chaining II (CHII) [57] with\nChaining: $p_t^{(n)} = {n\\choose |s+t|}^{-1}$.\nThe main difference of the SGV/SII and the CHGV/CHII is the quantification of so-called partnerships [27, 55], i.e. coalitions that only influence the value of the game if all members of the partnership are present. The CHGV and CHII adhere to the partnership-allocation axiom [27, 55], which states that the contribution of an individual member of the partnership and the interaction of the whole partnership are proportional. In contrast, the SGV and SII satisfy the reduced partnership consistency axiom [27, 55], which states that the interaction of the whole partnership is equal to the contribution of the partnership in a game, where the partnership is a single player.\nOn the other hand, the Banzhaf GV (BGV) [56] and Banzhaf II (BII) [33] extend the BV with\nBanzhaf: $p_t^{(n)} = \\frac{1}{2^{n-s}}$.\nA.2 M\u00f6bius Interactions (MIs)\nThe MIs $I_n$ are a prominent concept in discrete mathematics, which appears in many different forms. In discrete mathematics, it also known as the M\u00f6bius transform [69]. In cooperative game theory, the concept is known as Harsanyi dividend [35] or internal II [27]. The MI for $S \u2286 N$ is defined as\n$\u03a6_\u03b7(S) := \\sum_{T\u2286S}(-1)^{|S|-|T|}v(T)$.\nIn this context, the MIs are the unique measure that satisfy the recovery property\n$\u03bd(T) = \\sum_{S\u2286T} \u03a6_\u03b7 (S)$ for every $T \u2286 N$.\nThe MIs are a basis of the vector space of cooperative games, and thus every game can be uniquely represented in terms of its MIs. The Co-M\u00f6bius transform (Co-MI) [32] is another fundamental concept linked to the MIs of the conjugate game, i.e. $\u03bd(T) := \u03bd(N \\backslash T)$ [31]."}, {"title": "C Marginal and Conditional Imputers", "content": "When computing SVs and SIs, especially for structured tabular data that has a natural interpretation of feature distribution, there is a choice for marginalizing feature influence over either a marginal or a conditional distribution [1, 14, 18, 49, 51, 52, 59, 60, 75].\nFor a concrete example [49], consider a supervised learning task where a model $f : X \u2192 R$ is used to predict the response variable given an input x, which consists of individual features $(X_1, X_2, ..., X_n)$. Let p(x) to represent the data distribution with support on $X \u2286 R^n$. We use bold symbols x to denote random variables and normal symbols x to denote values. Let $x_S$ and $x_S$ denote a subset of features, i.e. players in a game, and values for different $S \u2286 N$, respectively. Then, a cooperative game $v : P(N) \u2192 R$ for estimating Shapley-based feature attributions and interactions is defined as\n$v(S) := f_S(x_S) := E_{q(x_S)} [f(x_S, x_S)] = \\int f(x_S, x_\\bar{S})q(x_\\bar{S})dx_\\bar{S}$,\nwhere $S = N \\backslash S$ denotes the set complement. The feature distribution $q(x_S)$ most often considered in the literature is either a marginal distribution when $q(x_S) := p(x_S)$ [18, 52], or a conditional distribution when $q(x_S) := p(x_S | x_S)$ [1, 26, 59].\nIn general, empirical estimation of a conditional feature distribution is challenging [1, 18, 52, 59]. Most recently in [60], the authors benchmark several methods for approximating SVs based on marginalizing features with a conditional distribution $p(x_S | x_S)$, without a clear best, i.e. different methods are appropriate in different practical situations. Thus, we combine the deci- sion tree-based and sampling approaches [60] to implement a baseline conditional imputer in shapiq.ConditionalImputer. The class can be easily extended to include more algorithms, which we leave as future work. The rather standard imputation with a marginal distribution $p(x_S)$ is imple- mented in shapiq.MarginalImputer. Both imputers are used by the appropriate game benchmarks, and available for approximating feature interaction explanations in shapiq. TabularExplainer via the imputer parameter."}, {"title": "D Details of the Experimental Setting and Reproducibility", "content": "D.1 Generated Cooperative Games\nThe game-theoretical quantification of interaction demands a formal cooperative game specified by a player set N and value function $\u03bd : P(N) \u2192 R$. The players for each benchmark game are already given in Table 3, leaving the value functions left open to be specified, with what we catch up here.\nLocal Explanation. For a specified datapoint x, the worth of a coalition of features S is given by the model's predicted value $h(x_S)$ using only the features in S. The features outside of S are made absent in xs by imputing them with a surrogate value in order to remove their information. For tabular datasets such as AdultCensus, BikeSharing, and CaliforniaHousing this is done by marginal or conditional imputation. For the language model predicting the sentiment of movie review excerpts, missing words are set to the masked token. Missing pixels (patches) for the vision transformer image classifier are also set to the masked token. For the ResNet image classifier removed superpixels are collectively set to a mean value (gray).\nGlobal Explanation. Instead of specifying a single datapoint and considering the model's output, the model's loss is averaged over a number of fixed datapoints $x_1,..., x_M$. The model's loss for a coalition S and datapoint $x_m$ is computed by comparing its prediction $h(x_{mS})$ with the ground truth target value. The imputation of absent features is done as for local explanations.\nTree Explanation. This is a specialization of local explanations for tree models, made feasible by the capabilities of TreeSHAP-IQ to compute ground-truth SVs and SIs values, which allows the evaluation games with substantially more features. Features are imputed according to the tree distribution [51, 58]. Consequently, the worth of the empty coalition containing no features is the tree's average prediction, e.g. baseline value.\nUncertainty Explanation. Similar to local explanations, the model's prediction with missing features imputed to a fixed datapoint is evaluated. Instead of referring to the predicted value, the value function is given by the prediction's uncertainty for which three measures are available: total, epistemic, and aleatoric uncertainty. Hence, the Shapley values of the features attribute their individual contribution to the decrease in uncertainty caused by their information.\nFeature Selection. The available data is split into a training set $D_{Train}$ and test set $D_{Test}$. Given a learning algorithm A, a coalitions worth v(S) is given by the generalization performance of the model hs on DTest that results from applying A on $D_{Train}$ using only features in S, known as remove-and-refit. The worth of the empty coalition is set to 0.\nEnsemble Selection. Replacing features in feature selection by weak learners, and adapting the learning algorithm to construct an ensemble out of those, leads to ensemble selection. Each coalition S of base learners is evaluated by the performance of the resulting ensemble on a separate test set, known as remove-and-re-evaluate. Likewise, we set v(0) = 0.\nData Valuation. Continuing in the spirit of remove-and-refit, a new model is fitted to each coalition of datapoints. The generalization performance of the resulting model on a separate test set is set to be the coalition's worth. The value of the empty coalition is set to 0.\nDataset Valuation. The setup is analogous to data valuation, where instead of single datapoints are being understood as players, the available data is partioned and each subset is viewd as a player.\nCluster Explanation. Similar to feature selection, remove-and-refit is applied. Instead of fitting a model, a clustering algorithm forms multiple clusters on the dataset using only the available features of a coalition S. The worth v(s) is given by a cluster evaluation score (see Tables 4 and 5 for details). A cluster score of 0 is assigned to the empty coalition.\nUnsupervised Feature Importance. Given a coalition of features S, a set of datapoints can be understood as observations generated by a joint distribution of S and used to estimate this distribution"}, {"title": "D.2 Computational Resources", "content": "This section contains additional information regarding the computational resources required for the empirical evaluation of this work. The main computational burden stems from pre-computing the benchmark games for $n \u2264 16$ players and from running all of shapiq's SV and 2-SII approximation methods. Still, the experiments require only a modest range of computational resources. The games are pre-computed on a \u201c11th Gen Intel(R) Core(TM) i7\u201311800H 2.30GHz\u201d machine requiring around 240 CPU hours. The approximation experiments have been run on a compute cluster using 80 CPUs of four \"AMD EPYC 7513 32-Core Processor\" units for 24 hours resulting in about 1920 CPU hours."}, {"title": "D.3 Data Availability and Reproducability", "content": "The data to the pre-computed games is available at https://github.com/mmschlk/shapiq/ tree/v1. Utility functions exist in shapiq that automatically download and instantiate the games. The code for reproducing the experimental evaluation can be found at https://github.com/ mmschlk/shapiq/tree/v1/benchmark and https://github.com/mmschlk/shapiq/tree/ v1/complexity_accuracy."}, {"title": "E Additional Benchmark Approximation Results", "content": "This section contains additional experimental results. Mainly, this section contains exemplary MSE and Precision@5 approximation curves for a benchmark game of each application domain. These results can be found in Figures 8 to 10. The dataset names used to for the benchmark games are abbreviated as described in Appendix B. Figure 6 shows the overview of the benchmark results additionally to Figure 5 for the mean absolute error (MAE)."}]}