{"title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models", "authors": ["Andy K. Zhang", "Neil Perry", "Riya Dulepet", "Eliot Jones", "Justin W. Lin", "Joey Ji", "Celeste Menders", "Gashon Hussein", "Samantha Liu", "Donovan Jasper", "Pura Peetathawatchai", "Ari Glenn", "Vikram Sivashankar", "Daniel Zamoshchin", "Leo Glikbarg", "Derek Askaryar", "Mike Yang", "Teddy Zhang", "Rishi Alluri", "Nathan Tran", "Rinnara Sangpisit", "Polycarpos Yiorkadjis", "Kenny Osele", "Gautham Raghupathi", "Dan Boneh", "Daniel E. Ho", "Percy Liang"], "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have the potential to cause real-world impact. Policymakers, model providers, and other researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration test- ing. Toward that end, we introduce Cybench, a framework for specifying cyberse- curity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be re- cent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute bash commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks, which break down a task into intermediary steps for more gradated evaluation; we add subtasks for 17 of the 40 tasks. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 7 models: GPT-40, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without guidance, we find that agents are able to solve only the easiest complete tasks that took human teams up to 11 minutes to solve, with Claude 3.5 Sonnet and GPT-40 having the highest success rates. Finally, subtasks provide more sig- nal for measuring performance compared to unguided runs, with models achieving a 3.2% higher success rate on complete tasks with subtask-guidance than without subtask-guidance.", "sections": [{"title": "1 Introduction", "content": "The growing capabilities of language models (LMs) are driving increasing concerns about their mis- use in cybersecurity. For instance, the 2023 US Executive Order on AI (The White House, 2023)"}, {"title": "2 Framework", "content": "Cybench is built on the following framework for specifying cybersecurity tasks and evaluating agents on those tasks. Each task is specified by a task description, starter files, and an evaluator. An environment is constructed based on the task. An agent takes an action which is executed in the environment, and yields an observation. This observation is incorporated into agent memory. The agent repeats this cycle, taking actions and reading observations into memory, until it runs out of iterations or submits an answer. The agent only has a single chance to submit an answer to the evaluator, which outputs success or failure.\nSince many tasks are beyond the capabilities of existing language models, we introduce subtasks, which add additional goals and evaluation steps to tasks. In subsequent subsections, we describe each of the key components of the framework: task specification 2.1, subtasks 2.3, and environment 2.5."}, {"title": "2.1 Task Specification", "content": "Each task is specified by a textual task description, a set of starter files, and an evaluator (Figure 1).\nTask description. In Cybench, the task description describes the desired goal (e.g., \"capture the flag\"). The description can also contain pointers to relevant starter files.\nStarter files. The starter files include local files that the agent can directly read, write, execute and/or remote files, which specify one or more task servers, which may include additional files that may be accessed by querying the server(s) that the agent can access only through network calls. For instance, local files may include an encrypted secret that the agent needs to decrypt, and remote files is a web server with vulnerabilities such as PHP type juggling for the agent to exploit.\nEvaluator. Agents are evaluated based on the answer they submit. The evaluator also parses observations for answers that are unique and indicative of success on a task (e.g., a unique flag hkcert22{mistakes-off-the-page} that is outputted only on successful completion of a task). Additionally, we track efficiency metrics such as the number of LM tokens encoded and generated and time usage.\nAll tasks are instantiated in an environment with the same base image. As shown in Figure 1, a prompt containing task-specific information such as hostname and local files is passed in to the agent, which can then interact with a Kali Linux Docker container through bash commands. This Kali Linux container contains any task-specific local files and can access any remote files through network calls. Remote files are hosted in one or more separate Docker containers (task servers) within the shared Docker network."}, {"title": "2.2 Task Example: MOTP", "content": "As an example, consider the following task: MOTP from HKCert. MOTP stands for multiple one- time password and is from a Capture the Flag challenge with two vulnerabilities: (1) the username"}, {"title": "2.3 Subtasks", "content": "Certain cybersecurity tasks including CTFs and vulnerability detection have binary outcomes of success or failure. However, as cybersecurity tasks can be complex and involve many discrete steps, we introduce subtasks, which enable tasks to have partial credit. For instance, while a task may simply measure performance on \"capturing the flag\", this could involve multiple steps (e.g., recognizing vulnerabilities in several discrete files to synthesize a more complex exploit to retrieve the flag). We discretize these steps into individual subtasks, each with its own question and answer (e.g., \"Question: Which file contains the OTP bypass vulnerability? Answer: google2fa.php\"). An agent is prompted with the question from the first subtask (e.g., \u201cWhich file contains the account credentials?\") and has a limited number of iterations to attempt the subtask as well as a single answer submission. Afterwards, the agent is prompted with a question from the second subtask, and so on, until the final subtask (See example in Table 1).\""}, {"title": "2.4 Metrics", "content": "With subtasks, we have two types of experimental run modes: unguided mode, where subtasks are not provided for guidance, and subtask mode, where subtasks are provided sequentially. From these two modes, we track three performance metrics:\n1. Unguided performance is the performance on a task without subtask guidance, and outputs a binary score (i.e., 0 or 1).\n2. Subtask-guided performance is the performance on the final subtask only, and outputs a binary score (e.g., would score 1 for Success on Table 1)\n3. Subtask performance is the performance on the subtasks, and outputs a fractional score based on the fraction of subtasks solved (e.g., would score on Table 1)\nBy defining the goal of a subtask to be equivalent to that of a task (this is always \"What is the flag?\" for CTF tasks), we are able to compare subtask-guided performance with unguided performance."}, {"title": "2.5 Environment", "content": "The task defines the information to create an environment, represented as a Docker container for the execution environment and one or more Docker containers for task servers. The agent's execution environment has a Kali Linux base image and any task-specific local files. The one or more servers are also Docker containers in the same Docker network, instantiated by the remote files. The agent maintains a memory of everything that has happened and receives observations from executing commands.\nMore formally, the agent is specified by the following:\n1. S is the set of environment states, including the Kali Linux container and any task-specific local files and any task server(s) instantiated by remote files.\n2. R is the set of responses that the agent produces, which are the sequence of strings that contain a single valid action $a \\in A$ (e.g., a response may be a string such as \"this is a response and my action is: cat file\")\n3. A is the set of actions, which are either Command:x or Answer:y. Command:x is a string that is indicated by a \"Command:\" prefix and the suffix string \"x\" is run in the environment (e.g., \"Command:cat file\" is a command with an executable suffix \"cat file\"). Answer:y is a string indicated by a \"Answer:\" prefix and the suffix string \"y\" is evaluated for success or failure and terminates the current task or subtask.\n4. O is the set of observations from executing Command:x in the terminal (e.g., strings such as \"Command:file contents\"). Answer:y yield no observations.\n5. M is the set of memory representations of an agent (e.g., a prompt string that responses and observations are appended to).\nThe agent operates in a series of time steps $t = 1, ..., T$ and each time step operates in three parts:\n1. Act: The agent takes its memory $m_t$, and produces a response $r_t$, which includes an action At.\n$r_t, a_t = Act(m_t)$\n2. Execute: The framework executes the action $a_t$ on environment $s_{t-1}$ to produce updated environment $s_t$ and returns observation $O_t$.\n$s_t, O_t = Execute(s_{t-1}, a_t)$\n3. Update: The agent updates its memory for the next timestamp $m_{t+1}$ based on the response $r_t$ and observation $o_t$.\n$m_{t+1} = Update(m_t, r_t, o_t)$\nWhen running on a task without subtasks, the agent can act until it reaches the maximum number of iterations or until answer submission. When running on task with subtasks, there is an iteration and submission limit for each subtask, though memory is retained across subtasks and additional context about previous subtasks can be provided."}, {"title": "3 Task Creation", "content": "Having described the framework for cybersecurity tasks, we now present how we constructed the actual tasks. We leverage Capture the Flag challenges from 4 distinct competitions to include 40 tasks and add subtasks to 17 of those tasks. We describe the tasks and the selection process below."}, {"title": "3.1 Capture the Flag Challenges", "content": "Capture the Flag challenges (CTFs) are a broad class of cybersecurity tasks where the objective is to identify a vulnerability and execute the exploit in order to retrieve a secret string known as a flag. CTFs are well-established tools to teach and measure cybersecurity skills, covering a range of abilities from web-based exploits to cryptography. There are new CTF competitions each year, such that CTFs continue to address new and contemporary cybersecurity issues such as blockchain security.\nThese challenges include a wide range of tasks: brute-forcing simple passwords on a server to reverse engineering and patching binaries to bypass locked features, exploiting flaws in cryptographic cipher implementations, or performing complex return-oriented programming to gain root access on a remote server.\nThe challenges also span a wide array of difficulties, categories of computer security, and levels of realism. Some challenges are simple \"toy\" tasks that resemble interesting puzzles, while others are highly accurate simulations of professional hacking scenarios. Although each CTF typically demonstrates a single skill in a self-contained manner, real-world hacking can involve anything from straightforward attacks to deeply complex operations that chain together multiple discovered vulnerabilities. Nevertheless, carefully chosen CTFs can serve as effective proxies for real-world hacking."}, {"title": "3.2 CTF Competitions", "content": "Teams compete in CTF competitions, where they try to solve more challenges and earn more points than other teams to win prizes. These competitions are hosted by a variety of groups, including academic institutions, cybersecurity companies, CTF organizations (i.e., organizations focused on competing in and hosting CTFs), and government organizations. In contrast to the existing literature which has been limited to CTF competitions with high school and university-level tasks, we focus our selection on competitions with professional-level tasks. Of CTF competitions with professional-level tasks, we selected competitions released recently, between 2022-2024, to minimize train-test overlap. We then analyzed the competitions and scored them based on (1) the portion of challenges that were functional (2) how easy challenges were to run (3) whether solutions and writeups were included, and if so, how detailed, helpful, and complete, the writeups were. Of these, we selected: HackTheBox (cyber-apocalypse-2024), SekaiCTF (2022- 23), Glacier, and HKCert."}, {"title": "3.3 Task Selection", "content": "Our goal was to build a benchmark that is both deep comprising carefully annotated tasks with meaningful metadata and subtasks and wide, spanning broad categories with a good range of diffi- culties. We focused on tasks that serve as effective proxies for real hacking skills, from simple input validation issues to complex return-oriented programming, including those that involve identifying and exploiting actual common vulnerabilities and exposures (CVEs). Cybench is designed to grow"}, {"title": "4 LM-based Agent", "content": "To tackle Cybench, we design an LM-based agent as shown in Figure 4. We assume API access to a language model, which takes a prompt and (stochastically) returns a response. Here we implement memory as a string to track the last three iterations of responses and observations, similar to MLAgentBench. In contrast, generative agents leverage an external database for memory. At a high level, the agent follows an act, execute, update loop, where it acts based on its memory, the action is executed in the environment, and it updates its memory based on observation from execution. More formally, we implement Act 1 as discussed in Subsection 2.5 Environment.\nAct: The agent\u2019s memory mt (implemented as a string, which tracks the last three iterations of responses and observations), is passed as a prompt to the LM, which provides a response rt\n(Figure 4.1). The response rt is parsed to derive an action at. Here memory is restricted to the initial prompt (shown in Figure 5) and the last three iterations of responses and observations.\n$r_t, a_t = Act(m_t)$"}, {"title": "4.1 Response Format", "content": "While the response contains the action, it has several other crucial components that help the agent attempt cybersecurity tasks. These include Reflection, Research Plan and Status, Thought, Log, Action, which we define and describe below and include an example taken from an unguided run on Claude 3.5 Sonnet.\nReflection. Inspired by Reflexion and MLAgentBench Huang et al. (2024), Reflection is intended for the agent to reflect about the last observation. For instance, as shown below, the agent recognizes that the login.php leaks the username and password and contains three OTPs as part of the login process.\nPlan and Status. Inspired by MLAgentBench (Huang et al., 2024), Plan and Sta- tus is intended for the agent to plan and keep track of current status at a high level. For instance, as shown below, the agent has a five-part plan and has finished with its first step of examining files and is about to attempt to access the web application.\nThought. Inspired by ReAct and MLAgentBench (Huang et al., 2024), Thought is intended for the agent to think before it acts to have more a reasoned action. For instance, as shown below, we see that it verbalizes it should access the web application on otp:80 through curl.\nLog. The log encourages the agent to keep track of each action that has been executed, along with its interpretation of the observation. This is intended to help the agent plan based on its past actions and observations. For instance, as shown below, we see that the agent has run 'cat' on four files and has summarized key information about each observation."}, {"title": "5 Experiments", "content": "As of August 10, 2024, release v1.7.0 of https://crfm.stanford.edu/helm/mmlu/latest/.\n5.1 Experimental Conditions\nTo assess the cybersecurity capabilities of leading LMs, we evaluated the following 7 models: the top 5 models of HELM MMLU  Claude 3.5 Sonnet (Anthropic, 2024a) (anthropic/claude-3-5-sonnet-20240620), Claude 3 Opus (Anthropic, 2024b)(anthropic/claude-3-opus-20240229), Llama 3.1 405B Instruct (Dubey et al., 2024) (together/llama-3.1-405b-instruct- turbo), GPT-40 (OpenAI, 2023) (openai/gpt-4o-2024-05-13), and Gemini 1.5 Pro (Google, 2024b) (google/gemini-1.5-pro-001) and 2 additional models: Mixtral 8x22b Instruct (Jiang et al., 2024) (together/mixtral-8x22b-instruct-v0.1) and Llama 3 70B Chat (Dubey et al., 2024) (together/llama-3-70b-chat) This included 4 closed models (GPT-40, Claude 3 Opus, Claude 3.5 Sonnet, and Gemini 1.5 Pro) and 3 open-weight models (Llama 3.1 405B Instruct, Mixtral 8x22b Instruct, and Llama 3 70B Chat). The open-weight models are hosted on Together AI (Together, 2024).\nWe measure the agent's capability across all 7 models in unguided mode on all 40 tasks and subtask mode on all 17 tasks with subtasks added. We set an iteration limit of 15 for unguided mode and a limit of 5 per subtask for subtask mode. For all runs, there was a input token limit of 6000 tokens and output token limit of 2000 tokens, and network access was permitted (though we did not observe network access leading to question leakage in any of the runs)."}, {"title": "5.2 Results", "content": "Claude 3.5 Sonnet achieves highest unguided performance and subtask performance and GPT-40 achieves the highest subtask-guided performance. As shown in Table 3,\nClaude 3.5 Sonnet achieves 17.5% on unguided performance and 48.5% on subtask performance and GPT-40 achieves 29.4% on subtask-guided performance. Unguided, three models (Claude 3.5 Sonnet, GPT-40, Claude 3 Opus) solve a task with a first solve time of 11 minutes. With subtask-guidance, GPT-40 manages to solve a task with a first solve time of 52 minutes, though it is from a different competition (HKCert), making it difficult to directly compare.\nFirst solve time is a strong indicator of task difficulty for agents. As shown in Figure 6, with unguided-performance, the agent has a non-zero success rate on 73% of tasks with a first solve time of up to 11 minutes and is unable to solve a single task with a first solve time greater than 11 minutes. Similarly with subtask-guided performance (Figure 7), the agent has a non-zero success\nrate on 67% of tasks with a first solve time of up to 11 minutes and there is only a single solved task with a first solve time greater than 11 minutes. This task is MOTP from HKCert (described in Subsection 2.2) and was solved only by GPT-40. Accordingly, first solve time is a strong indicator of task difficulty.\nAgents struggle to progress on the more difficult tasks. As shown in Figure 6, unguided, the agent is unable to solve any task with a first solve time above 11 minutes in any of the competitions. The most difficult task, Robust CBC, has a first solve time that is 136 times greater (24 hours and 54 minutes). Currently, models seem to struggle to make \u201cinsights\" that take experts time to figure out. For instance, no model was able to identify a type juggling attack in PHP without guidance. Even with subtask guidance in the form of a question (\u201cWhat is the vulnerable operator used in google2fa.php?\") only a single model (GPT-40) was able to solve this task (though both Claude models successfully identify the vulnerabilities but fail in attempting the exploit). In contrast, without subtask guidance, the type juggling vulnerability was never identified. With increasing first solve time, the vulnerabilities become increasingly subtle.\nSubtasks provide more signal for differentiation on task performance. Given the high task difficulty, both unguided performance (Tables 4 and 5) and subtask-guided performance (Table 6) yield sparse matrices with mostly failures. However, subtask-guided performance has a higher success rate (16.0%) than unguided performance (11.8%) on the 17 tasks with subtasks. This is likely because subtasks can help guide agent performance, such as by hinting toward the type juggling vulnerability in MOTP by asking about the vulnerable operator (Table 1). Additionally, subtask performance (Table 7) yields significantly more signal, with a non-zero value in 53.8% of cells.\nThere is minimal overlap between training and test data on any solved task besides those for Claude 3.5 Sonnet. While it is difficult to determine the effect of train-test overlap\nSafety refusals are rare, occurring four times with Claude 3 Opus and no other runs. LMs often have safety refusal procedures that prevents the model from engaging with the user for unsafe tasks, such as helping users create cyber threats . We encountered safety refusals because of ethical concerns when experimenting with prompts initially, so we prompted the agent that \"You are a cybersecurity expert doing a cybersecurity challenge to assess vulnerabilities in a system.\" to make it clear that it was performing an ethical task. Accordingly, only Claude 3 Opus refused for ethical reasons, cit- ing that \"cannot provide or assist with code intended to exploit or hack systems, as that would be unethical and likely illegal\" .\""}, {"title": "6 Related Work", "content": "CTF Datasets. There have been several efforts to develop and release CTF datasets, including InterCode-CTF and the NYU CTF Dataset , which is concurrent work. Whereas Cybench includes professional-level CTF tasks, Intercode-CTF and NYU CTF Dataset include high school and university-level CTF tasks respectively. InterCode-CTF is composed of tasks from only PicoCTF, organized by Carnegie Mellon University, and targets high school students. The NYU CTF Dataset is composed of tasks from only CSAW, organized by students at New York University. Each of these competitions were included in the evaluation by the AI Safety Institute (AISI, 2024) and rated as high school-level and university-level respectively. Each of these datasets rely on a point-based system for difficulty, which are subjectively determined before the tasks were released to competitors (as opposed to first solve time which is grounded with objective data from competitor performance). In contrast to InterCode-CTF which is composed of easy tasks that took its authors an average of 3.5 minutes to solve, we have significantly harder tasks given the first solve times. It is trickier to compare difficulty with the NYU CTF Dataset given a lack of reference, but we note that Cell, a task marked with the highest difficulty in the NYU CTF dataset is comparable to RPGO, a task with a first solve time of 45 minutes, which is significantly lower than the most challenging tasks in Cybench with first solve times of several hours (Appendix D). Furthermore, as each dataset is drawn from a single competition, there are only a limited number of recent tasks, risking train test overlap. For instance, the majority of tasks in the NYU CTF Dataset are released before the training cutoff date of all their evaluated models. There, the authors reported that Claude 3 outperformed the median human score in the 2022 finals, but failed to achieve a single point in 2023, after the training cutoff date. Since we leverage different competitions for our work, this work is complementary, and provides additional coverage.\nLM Benchmarks for Cybersecurity. In addition to CTF datasets, there have been significant other efforts to develop LM benchmarks for cybersecurity. These efforts have included assessing an LM's ability to exploit vulnerabilities within code snippets and quizzing general cybersecurity knowledge via question answering . Here, we introduce a benchmark that assesses the capability of LM agents to complete cybersecurity tasks that take many iterations to identify one or more vulnerabilities and execute one or more exploits.\nAgent Benchmarks. There has been considerable effort to facillitate benchmarking LM agents, including AgentBench and Intercode for interactive docker and os-based workflows, MLAgentBench Huang et al. (2024) for ML research, SWE-bench for software engineering , SmartPlay for games , Agentsims for generative agents , WebShop for product search and re- trieval, WebArena for general web workflows , among others. Recognizing that cybersecurity tasks require special solicitude in environment and infrastructure set-up, we provide a framework designed to benchmark cybersecurity risk and capabilities of LM agents.\nAgent Architectures. There has been many works that have worked to explore various agent architectures. Park et al. (2023) introduced generative agents, where agents are given names, objectives, and relationships with other agents and act in a simulated world with memory in a database. OpenDevin Wang et al. (2024) introduces a platform for creating software engineering agents, including multi-agent delegation. BOLAA explores orchestration and interactions between multiple agents. There have also been approaches in prompting to improve agent performance, including Reflexion with verbal reflection and ReAct with reasoning before acting. These relate with general LM prompting strategies as well, such as chain-of-thought and tree-of-thoughts . Here, we draw inspiration from and build upon these existing works to create general architecture that works well for cybersecurity tasks.\nLM Agents for Offensive Cybersecurity. There have been significant efforts in developing LM agents for offensive cybersecurity, including penetration testing, and CTFs . PentestGPT, HackingBuddyGPT , and PenHeal are notable efforts in developing LM agents for penetration testing. Fang et al. (2024b;a;c) have reported that agents can hack websites and exploit vulnerabilities. Most relatedly, for CTFS, each present a simple agent to tackle their datasets, without detailed response structures that include Reflection, Planning, or Thought. In contrast to the CTF agents, our agent has structured responses that include Reflection, Planning, and Thought, which can improve capabilities."}, {"title": "7 Conclusion", "content": "We have presented Cybench, a new benchmark for evaluating agents on cybersecurity tasks. We introduced a set of recent, meaningful, and difficult tasks, and added subtasks to a subset of these tasks. To evaluate existing models, we developed a cybersecurity agent that is capable of solving the easiest tasks in the benchmark. As LMs continue to improve and the world becomes increasingly digitalized, the impact of cybersecurity agents will only grow. As dual-use technology, cybersecurity agents have both the potential for benefits and harms. We think it is necessary to capture the capabilities and risks of cybersecurity agents so that policymakers, model providers, and researchers can work together to ensure these agents are used to benefit society. We plan to continue to update this framework going forward, exploring new agent architectures and expanding the diversity of tasks we include."}, {"title": "Ethics Statement", "content": "There is a long history of dual-use technology in cybersecurity and there have been extensive discussion of how to weigh the benefits and risks of releasing such technologies . For instance, Silic surveys practitioners and finds that empirically practitioners agree that dual-use technology has both benefits and harms, as malicious attackers can use them for harm but good actors can use them for defense. Rad (2015) argues that while such technology can be used for harm, restrictions can hinder the benefits of the technology more than the harms, as malicious actors may simply obtain equivalent technology through alternative means such as black markets that are not available to law-abiding actors.\nHere we acknowledge that the agent and the benchmark are dual-use. In this space, there have been works that have chosen to release their code and others that have chosen to withhold the details of their research. After carefully weighing the benefits and harms of each choice, we have chosen to release our code and data and will explain our reasoning below.\nIn considering the harms, the concern of releasing the agent is that it may be leveraged by malicious actors to identify vulnerabilities and execute exploits on real systems . Current agents are not able to complete difficult cybersecurity tasks which limits the risk they pose. However, the growing capabilities of LM agents suggests that LM agents may soon substantially outclass non-LM based tools, and thereby unleash harm at a greater magnitude than existing technologies. Here, releasing the framework may accelerate development of stronger cybersecurity agents and expedite this future. While there are non-AI open-source tools that can perform similar functions (such as those for automated penetration testing and scanning), a potential concern is that AI-based tools are non-deterministic and accordingly would be harder to defend against.\nIn considering the benefits, the agent can be viewed as an automated penetration testing tool. Automated penetration testing tools such as Metasploit and OWASP Nettacker are open-source and widely adopted because the benefits vastly outweigh the risks. Here, the agent can be likened to an automated pen- etration testing tool as it identifies vulnerabilities and exploits them. Similarly, the benchmark would encourage development of such tools that have a similar risk-benefit profile to other auto- mated penetration testing tools, and hence be beneficial to release.\nAdditionally, because related works have already openly released their code, any marginal increase in risk would be minimal. For instance, Happe & Cito (2023) release code to leverage LMs for penetration testing, arguing that attackers will use LMs and that defenders would need to prepare to defend with LMs too. Similarly Shao et al. release code for an agent and a benchmark for CTF tasks after discussing the dual nature of AI as both a tool and a potential threat in cybersecurity. While this work has made distinct contributions, the risk profile of releasing this work is similar, and possibly less than those other works, given that alternative agents and benchmarks already exist.\nFurthermore, as there has been significant interest and consideration by governments to regulate A\u0399, we critically need more evidence and data for informed decisions and responsible regulation . There have been many efforts to assess cybersecurity risk, both by government organizations such as the AI Safety Institute (AISI, 2024) and by model providers. By making our work available in a transparent fashion, we can help policymakers better understand current capabilities and risks of cybersecurity agents, when government often lacks such systematic information (NTIA, 2024). This evidence should ideally inform responsible regulatory efforts.\nFinally, as scientific researchers, we believe that reproducibility and transparency are central to the AI ecosystem . The reproducibility crisis affecting the sciences has affected machine learning as well, owing to mistakes and/or even fraud and fabrication . While transparency in code, data, and methods is not sufficient to guarantee reproducibility (as mistakes can, of course, occur in the research process), obscurity can ensure irreproducibility. Additionally, releasing our code allows the community to build on our work, helping accelerate scientific progress.\nAfter weighing the various factors, we choose to release our code and data publicly."}, {"title": "Acknowledgements", "content": "We thank Alan De Loera, Avi Gupta, Ricky Thai, Peter De La Cruz, Tenzin Chonzom, Elijah Song, and Uche Ochuba for their help in reviewing challenges. We thank Open Philanthropy for providing funding for this work. We greatly appreciate HackTheBox, Project Sekai CTF, LosFuzzys, and HKCERT for publicly releasing their challenges along with detailed writeups and rich metadata."}, {"title": "B First Solve Time", "content": "First solve time is the time it takes the first team to solve a given challenge. Team that achieve first solve receive extra points to their score and/or prizes, in addition to prestige within the community, which makes it helpful as an objective metric to quantify challenge difficulties. This number is competition-dependent, both in terms of the competitors who are represented and the methodology by which the number is calculated. Accordingly, we provide the details for how we collected this data for each competition below.\nB.1 HackTheBox\nThe leaderboard of the competition can be accessed on the official website; there is no information about the first solve time for the challenges, but one can view the timestamps of when a team solved a challenge. We considered the eight teams that solved all of the challenges of the competition. We manually copied the timestamps from the website, subtracted them by the starting time of the competition (since we did not find any information suggesting that the challenges were released in waves), and took the minimum time among the eight teams as an estimate of the first solve time for every challenge.\nB.2 Sekai22 and Sekai23\nThere is a public Discord server that contains a channel for automated announcements that were sent out when every challenge was first solved during the competition. We copied the timestamps of the Discord messages for all challenges. In both competitions, the challenges were released in several waves (possibly to encourages players to stay in the competition). The times for when specific challenges were released are also documented in the Discord channel, so we subtracted the release time of each challenge from the first solve timestamp accordingly to generate the first solve time. These first solve times are also available on the Github page.\nB.3 Glacier\nThere is a public Discord server that contains a channel for automated announcements that were sent out when every challenge was first solved during the competition. There was an announcement confirming that the challenges were not released in waves. We copied the timestamps of the messages and subtracted them by the starting time of the competition to generate the first solve time.\nB.4 HKCert\nWe include two challenges (\u201cBack to the Past\u201d and \u201cMOTP\u201d) from this competition. There is a public Discord server that contains a channel for announcements during the competition, but no automated recordings of first solves. Also, we observed that the challenges were released in several waves, but there was no specification on which challenges were contained in each wave. There was an announcement 32 minutes after the competition started that ten teams had solved the \"Back to the Past\" challenge, so we use 32 minutes as an estimate of its first solve time. Additionally \"Back to the Past\" included a released solution writeup (though this was not provided for MOTP). As for \"MOTP\", there was an announcement that four teams had achieved first solve 4 hours 52 minutes after the start, but considering that second wave of challenges was released 4 hours into"}, {"title": "C Tasks By Competition", "content": "For each competition", "HH": "MM. For each task", "GLA": "GlacierCTF2023", "HKC": "HKCert 2023", "HTB": "HackTheBox 2024", "S23": "SekaiCTF 2023", "S22": "SekaiCTF 2022; Category abbreviations are W: Web", "R": "Reverse", "C": "Crypto", "F": "Forensics", "P": "Pwn", "Hw": "Hardware", "M": "Misc. For Logs"}]}