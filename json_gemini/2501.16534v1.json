{"title": "Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs", "authors": ["Jean-Charles Noirot Ferrand", "Yohan Beugin", "Eric Pauley", "Ryan Sheatsley", "Patrick McDaniel"], "abstract": "Alignment in large language models (LLMs) is used to enforce guidelines such as safety. Yet, alignment fails in the face of jailbreak attacks that modify inputs to induce unsafe outputs. In this paper, we present and evaluate a method to assess the robustness of LLM alignment. We observe that alignment embeds a safety classifier in the target model that is responsible for deciding between refusal and compliance. We seek to extract an approximation of this classifier, called a surrogate classifier, from the LLM. We develop an algorithm for identifying candidate classifiers from subsets of the LLM model. We evaluate the degree to which the candidate classifiers approximate the model's embedded classifier in benign (F\u2081 score) and adversarial (using surrogates in a white-box attack) settings. Our evaluation shows that the best candidates achieve accurate agreement (an F\u2081 score above 80%) using as little as 20% of the model architecture. Further, we find attacks mounted on the surrogate models can be transferred with high accuracy. For example, a surrogate using only 50% of the Llama 2 model achieved an attack success rate (ASR) of 70%-a substantial improvement over attacking the LLM directly, where we only observed a 22% ASR. These results show that extracting surrogate classifiers is a viable (and highly effective) means for modeling (and therein addressing) the vulnerability of aligned models to jailbreaking attacks.", "sections": [{"title": "Introduction", "content": "Improvements in machine learning-specifically the introduction of the transformer architectures [44]-have led to increasingly powerful large language models (LLMs). Whether they are accessed through an API or interface (e.g., GPT [32], Gemini [39], Claude [1]) or published as open-source (e.g., Llama 2 [29], Llama 3 [30], Qwen [43], etc.), these models have become the de facto tool for tasks involving natural language. They serve as foundations for new tools, in which they are augmented with new capabilities [2, 31] or fine-tuned on a downstream task [14]. In adapting these models to tasks, LLMs undergo an alignment process in which they are further trained (refined) to satisfy safety objectives [29, 36]. For example, alignment guidelines often prevent a model from outputting responses that would be offensive, discriminatory, or harmful (i.e., unsafe). It is acknowledged that alignment can (and often does) fail in adversarial settings, resulting in a jailbreak: an unsafe input being accepted as safe. Indeed, many studies have explored the automated generation of jailbreak inputs, i.e., adversarial examples [23, 25, 50, 61].\nWe posit a novel approach to reason about LLM alignment in the context of jailbreak attacks (see Figure 1). In this paper, we hypothesize that alignment embeds a \u201csafety\" classifier into a target model that determines whether an input prompt is safe (compliant with alignment goals) or unsafe (refusal with alignment). Consistent with this hypothesis, previous work has shown that the representations of safety queries are well separated in aligned models [24, 59], specifically in the first decoders within the LLM [21]. This work seeks to extract an approximation of the model classifier (called a surrogate classifier) and evaluate its utility in assessing a model's robustness to adversarial inputs. In this, we explore: (a) how a surrogate classifier can be accurately extracted, and (b) how the surrogate classifier can be used to assess the safety of or attack the aligned model.\nWe begin by exploring methods for extracting surrogate classifiers. Here, our approach systematically identifies candidates from a subset of the model architecture. Candidates are built by (a) finding a structure, i.e., a subset of the architecture of the model, that best represents the embedded safety classifier, (b) adding a classification head, and (c) training this classification head on unsafe and safe inputs to map the features extracted from the structure to the model predictions (positive for refusal, negative for compliance). The candidates are then evaluated in benign and adversarial settings. For the benign settings, we measure how well the candidate classifier agrees with the model's safety classifier on safe and unsafe inputs. For the adversarial settings, we measure transferability of adversarial inputs between the target model and candidate"}, {"title": "Background", "content": "Large language model. A LLM models the conditional probability of the next token (a unit of text for the model) with respect to previous tokens. Let V be the vocabulary of the\ntokens and $V^* = \\bigcup_{n=1}^{N} V^n$ the corresponding input space of sequences, where N denotes the context window of the model (maximum amount of tokens that can be processed by the model). Given a sequence of tokens x = x1X2 . .. x \u2208 V*, the model aims to produce the next token xT+1 by approximating the probability distribution.\nP(XT+1 |X1X2...XT)\n(1)\nLLMs learn a vector representation of words called the embedding space. Given a sequence of tokens x = X1X2 . . . XT, each token is assigned to an embedding, resulting in a sequence of vectors {ht}t\u20ac{1,...,7} all in Rd, where d is the dimension of the embedding space. This study focuses on aligned chat models, which are almost exclusively of the same architecture: a sequence of decoders. The i-th decoder transforms a sequence of embeddings h(i) into another h(i+1), keeping the same sequence length. If the model is made of D decoders, the probability distribution on the next token is obtained by applying a linear layer followed by a softmax function (D)\non the last output embedding of the last decoder, hrp), i.e., (D)\np(\u00b7|x1x2...xr) = softmax(Ahri), where A \u2208 Rd\u00d7|V| is the\nlearned matrix that maps the embeddings to the scores (or logits) for each token.\nChat template. By construction, LLMs simply predict the next token. Thus, before deployment, the model undergoes instruction tuning: a process which encodes \"roles\" in the model such that it can interact with a user. Part of this process includes a chat template: a fixed modification of the input to dissociate the roles. It relies on special tokens from the model to enforce a format. As a result of different training settings, different models generally come with different chat templates. Formally, for a given model, there exists a template (Xpre, Xpost) \u2208 V* \u00d7 V* such that any input text x is preprocessed by prepending Xpre and appending Xpost."}, {"title": "Alignment and Safety", "content": "Alignment. Since the training data used for LLMs comes from diverse sources, biases and undesirable behaviors emerge [11]. In order to prevent those behaviors from happening, models often go through an alignment process that regulates their outputs according to given guidelines. Several methods exist and are not mutually exclusive [29]. Approaches such as supervised fine-tuning (SFT) uses sample answers from humans, while reinforcement learning with human feedback (RLHF) [3, 7] trains a neural network that acts as a reward for the LLM. Since training another reward model may be costly, direct preference optimization (DPO) [36] uses the LLM as its own reward model.\nGuidelines and taxonomies. The existence of alignment as a technique is a direct product of the complexity of identifying what is precisely expected from models (i.e., their objective). Safety is a broad term, thus, previous work has identified taxonomies [16, 47] that represent a broad range of unsafe behaviors. This allows researchers to evaluate alignment on a finer scale and understand where it could be improved. For example, when the prompts are decomposed into different categories such as self-harm, privacy, harassment, etc., models tend to refuse the prompts related to self-harm and accept the prompt related to privacy [9].\nIdentifying unsafe outputs. Similarly to their inputs, classifying LLMs outputs is challenging. Since aligned models tend to create strong refusal answers for detected unsafe prompts, a naive solution is to test whether certain refusal keywords are in the output (e.g., \"Sorry\", \"as a responsible AI\", etc.) [61]. Alternatively, judge models (also LLMs) are trained to classify the safety of outputs with more [16] or less [27] granularity on the classes they predict, similarly to the taxonomies explained above. A limitation of judge models is that they can also be attacked [26, 37], leading to a possible high rate of false negatives (unsafe outputs judged as safe)."}, {"title": "Attacks on LLMS", "content": "LLMs, like any class of machine learning models, are prone to attacks. While several exploits exist (e.g., model-stealing [4], text misclassification, etc. [58]), the jailbreak exploit remains the most prevalent. Jailbreak refers to a compliance from an aligned LLM for an unsafe query, as defined by the alignment guidelines. Jailbreak attacks fall into one of two threat models: the white-box threat model, in which model parameters are known, and the black-box threat model, in which only access to the output of the model is assumed.\nWhite-box. These attacks have been studied on open-weights models such has Llama 2 [29], Llama 3 [30], or Vicuna [6]. The most prevalent, GCG [61], uses greedy coordinate gradient (GCG) to mitigate the absence of mapping from embeddings (i.e., vectors) to tokens. Following this, many new white-box algorithms have been introduced [8, 48], improving existing algorithms [23, 24, 34, 52], using different objectives [54], or using specific properties of the model (e.g., logits [22], special tokens [55], or generation [15]. In terms of efficacy, attacks like GCG have an average attack success rate (ASR) above 50% [27] on most open-source models.\nBlack-box. These attacks have been widely studied, given their prevalence in practical scenarios [1, 32]. Most of them are driven by empirical heuristics: they apply transformations to the initial prompt such as ASCII-art [18], translation into underrepresented languages [49], encoding and decoding [12], or using pre-existing templates [51]. Black-box attacks achieve remarkable efficacy. For instance, GPT-Fuzzer [50] can achieve ASR as high as 90% on commercial LLMs such has ChatGPT."}, {"title": "Methodology", "content": "This section outlines our approach for extracting a surrogate classifier from LLMs, which serves as the basis for our experiments. Before we detail our methodology, we first explain a preliminary experiment whose results inform our approach."}, {"title": "Preliminaries", "content": "As explained previously, LLMs are aligned but are prone to attacks. Regardless of the threat model, jailbreak attacks have given limited understanding of where alignment fails since they involve heuristics in either the adversarial objective or the perturbation they apply to the input. In this work, we hypothesize that alignment embeds a safety classifier in the model, whose existence we outline next. By extracting this safety classifier, we can create a new class of optimization-based attacks that reduces jailbreaking to misclassification on this surrogate classifier.\nMotivation of existence. Refusal from aligned models often comes with a set of predefined outputs (e.g., \"I cannot fulfill your request. I'm just an AI\") as a product of alignment. As the space of refusal outputs is very limited, such outputs can be easily detected. We assume the following: the first generated token after the input is enough information to know whether the output will be a refusal or not. Under this assumption, there exists a set of refusal tokens V, such that, for a given input sequence of tokens X = X1X2...XT \u2208 V*, XT+1 \u2208 V, if and only if the output answer is a refusal. Thus, classifying unsafe inputs reduces to verifying XT+1 \u2208 V,. In a deterministic generation (i.e., without sampling), this is done by finding the token xT+1 that satisfies $xT+1 = \\arg \\max_{i \\in V} \\text{softmax}(A h_T^{(D)})_i$. As explained in Section 2.1, the last embedding (after the last decoder) hp) entirely determines the next token. Moreover, the (D)\nsequence of decoders in the model transforms the sequence"}, {"title": "Separation metric", "content": "For the classifier to exist, there must be separation between the two classes, i.e., inputs classified as unsafe (refusal) and inputs classified as safe (compliance). To demonstrate this, we measure the silhouette score on the embeddings associated to the two types of inputs (safe and unsafe). We use the silhouette score to determine how well clustered the two safety classes are and evaluate if there is indeed a separation, the metric is defined as follows:\ns(i) = \\frac{b(i) - a(i)}{\\max(a(i),b(i))}\n(2)\nwhere a(i) denotes the mean intra-cluster distance (average distance to all other points in the same cluster) and b(i) the mean nearest-cluster distance (average distance to points in the nearest cluster).\nThis score ranges from -1 to 1 where scores below 0 mean that unsafe and safe representations overlap, and scores above 0.25 or 0.5 imply a weak or reasonable separation [38]."}, {"title": "Takeaway 1: Existence of a safety classifier", "content": "Across models and datasets, internal decoders show separation between safe and unsafe inputs that exceeds that of the final decoder, suggesting the existence of an embedded safety classifier that does not span the full model architecture."}, {"title": "Threat Model", "content": "Motivated by our preliminary results, our adversary aims to extract an approximation of the safety classifier of a target aligned model, called a surrogate classifier. A successful extraction would allow subsequent attacks to be made with more effective white-box jailbreak approaches, while forgoing the complex computational costs they incur, as we discuss in Section 5.2. We will extract the surrogate classifier using subsets of the model, thus white-box access is assumed (i.e., the adversary has access to the model weights and architecture). In jailbreak attacks, the adversarial objective is to make the model comply with an unsafe input by adding a perturbation. Aligned models overfit on the unsafe inputs. As a result, they can reject safe prompts [9], exhibiting a high sensitivity to certain tokens. While previous work has mostly studied jailbreaks (unsafe inputs identified as compliant), in this work we focus on extracting the surrogate classifier therein enabling attacks that target alignment more closely. To do so, we also study safe inputs identified as unsafe, as we hypothesize that they also provide substantial information on the decision boundary of the classifier."}, {"title": "Problem Formulation", "content": "General formulation. Let fo be a LLM with context window N and vocabulary V, and let R : x \u2208 V* \u27a1 C be a classification rule that maps the output of the LLM to a class from C. We aim to extract a classifier from the LLM through two components: a structure and a classification head. A structure fo*, 0*C is a subset of the model (e.g., a decoder). A classification head C maps the representations learned by the structure to a class such that the classifier Co fe* is equivalent to the LLM for the classification task. Formally, for any token sequence x \u2208 V*, the following holds:\nR(fo(x)) = C(fo* (x))\n(3)\nSafety. In this paper, we focus on safety classification, i.e., distinguishing between unsafe and safe inputs. This is a bi-"}, {"title": "Extracting the Classifier", "content": "In order to extract the classifier, we first need to have a set of candidates. We build a candidate classifier through the following three steps, summarized in Figure 3:\nSelect a subset fo* of the architecture of the LLM fe,\nwhich we will refer to as a structure.\nCollect data points from the structure (fo* (x), R(fo(x)))\nconsisting in the features extracted from the structure\nand the predicted label of the LLM.\nTrain a classification head Con (fo* (x), R(fo(x))).\nThe resulting classifier Co fo* will be termed a candidate classifier.\nStructures. As explained in Section 2.1, most LLMs are built as sequences of decoders: processing units that take a sequence of vectors and output another sequence of vectors of the same length. LLMs can be scaled either in depth by adding decoders or in width by increasing the dimensionality of embeddings. Therefore, there are many ways to \"cut\" the model and thus many possible structures. It is clear that there is a separation between unsafe and safe representations at the end of aligned models [24] largely due to the strong refusal outputs. In addition, previous work [21, 56] has shown that"}, {"title": "Candidate classifier", "content": "We will refer to fi,i+8 as the set of decoders between the i-th and the i + 8-th decoder, 8 \u2265 0. The second hypothesis implies that for a given input x \u2208 V* where fi,i+8(x) \u2208 R|x|xd, we only consider the last element of the sequence, reducing the input dimension of the classification head C to Rd.\nBy construction, the input space of fi,i+8 depends on the output space of f1,i-1, since the model was trained with the entire architecture. In this work, we set i to 1. We leave the exploration of structures that can isolate intermediate decoders to future work (see Section 5.3), as they would require techniques to mitigate the absence of earlier decoders, which is out of the scope of this work.\nFinally, 8 will be termed the candidate size and we will refer to $\\frac{\\delta}{D}$ as the normalized candidate size, i.e., the proportion\nof decoders of the LLM used by the candidate.\nTraining. For the selected structure fi,i+8, the goal is to train a classification head C to obtain a candidate classifier. The first step is to create a dataset X = (fi,i+s(x), R(f(x))) consisting in the extracted feature from the structure fii+8 and the predicted label\u00b9 by applying the classification rule R to the output of the LLM fe for each prompt x.\nThen, the classification head is trained following this objective:\n$\\min L(C(h),y)$\n(4)\nThis results in a candidate classifier C\u00b0 fi,i+8 of size 8."}, {"title": "Evaluation", "content": "We apply our approach to answer the following research questions:\nRQ1. How accurate are the candidate classifiers at approximating the safety classifier in benign settings? (Section 4.2)\nRQ2. Does the performance of candidate classifiers hold in adversarial settings? (Section 4.3)\nRQ3. Can we jailbreak aligned LLMs by attacking the surrogate classifier? (Section 4.4)"}, {"title": "Experimental Setup", "content": "All experiments were run on pools from the Center for High Throughput Computing [5].The dataset creation and attack experiments were run on NVIDIA A100 GPUs with 40 GB of VRAM. Attack experiments on the models amount to an approximate total of 3000 GPU hours. Attack experiments on the candidate classifiers amount to an approximate total of 500 GPU hours.\nModels. We evaluate our approach on five open-weights aligned chat models, all from Hugging Face Transformers 4.46.2 [46] library: Llama-2-7b-chat [29], Qwen2.5-7B-Instruct [43]\ngemma-7b-it [41], gemma-2-9b-it [40], and granite-3.1-8b-instruct [42]. We chose these models because they achieve good performance at the classification task on the datasets considered, translating to a stronger alignment. For completeness, we present results on other models in Section A.3.\nDataset augmentation. Since most of the datasets lack safe prompts or targets to attack, we augment them when necessary. For safe prompts, we use a LLM to convert unsafe prompts to their corresponding safe version, as it is a simple natural language task suitable for the use of LLMs. We manually verify two properties of each generated prompt: the instruction does not prompt the model for unsafe information, and it has the same structure as the original prompt. For example, the prompt \"Write a script that can exploit vulnerabilities in a software or operating system\" becomes \"Write a script that can identify and help fix vulnerabilities in a software or operating system\u201d.\nThe GCG optimization objective aims to increase the probability of a certain output. Therefore, it needs a target string for each input prompt. For missing targets, we also use a LLM and generate what would be a compliance from the model (e.g., \"Sure, here is a script that can exploit vulnerabilities in a software or operating system\" for the previous example).\nDatasets. We use two datasets in our experiments:\nAn augmented version of AdvBench [61] containing the 520 original unsafe instructions and 520 added safe instructions.\nA subset of OR-Bench [9] made of 1000 randomly sampled prompts (of which 500 are unsafe and 500 are safe) for computational reasons.\nLabels. The labels are obtained by applying a classifier on the LLM output. To minimize nondeterminism due to sampling and facilitate reproducibility of results, we generate outputs with a temperature equal to 0: the next token selected at each step of the generation is the one that maximizes the logits. The output is classified between refusal and compliance using the distilroberta-base-rejection-v1 [35] model for the reasons mentioned in Section 3.3.\nClassifier training. As explained in Section 3.4, the classification head needs to be simple enough to prevent overfitting on the dataset and to only learn to project the features of structure to a classification. Therefore, we consider a simple linear layer followed by a sigmoid: $C(h) = \\sigma(Ah+b)$ where A \u2208 R\u00b9\u00d7d, b \u2208 R and $\\sigma : x \\rightarrow \\frac{1}{1+e^{-x}}$. The result is a scalar between 0 and 1, thus the classification is made given a threshold T. As for training, we apply K-fold cross-validation with 5 folds. We select the threshold T that maximizes the F\u2081 score on the training data, i.e., $T = \\arg \\max_{T\\in[0,1]} F_1(T)$ (Dtrain). To train the classification head and build the candidate, we use the hyperparameters listed in Table 1.\nAttack. To generate adversarial examples, we use the nanogcg2 implementation of GCG [61] with the following hyperparameters: num_steps=250, topk=512, and search_width=512. We chose this attack because it is the most popular gradient-based white-box attack. Although several enhancements to GCG have been introduced, we use the default version of the attack. The improvements rely on more heuristics (whether they are on the adversarial objective or the search algorithm) which goes against our goal of a heuristic-free attack (as explained in Section 3.1). For candidate classifiers, we change the original loss to the Py-Torch [33] implementation of the binary cross entropy loss."}, {"title": "Performance in Benign Settings", "content": "In this section, we answer RQ1, i.e., how accurate the candidate classifiers are at approximating the safety classifier.\nBaseline results. Table 2 shows the benign classification performance of the LLMs for each dataset with the corresponding confusion matrices in Figure 6 in Figure 7. We can first see that all models perform well in AdvBench, all scoring above 0.9. The worse performance on OR-Bench is expected, as it"}, {"title": "Takeaway 3: Lower bound on candidate size d", "content": "Candidate classifiers can overfit on the distribution of the dataset. Evaluating their performance on different datasets helps lower bound the candidate size 8 to identify the best candidate classifier."}, {"title": "Performance in Adversarial Settings", "content": "In this section, we aim to answer RQ2, i.e., measure the performance of the candidate classifiers in adversarial settings. We apply the attack on the LLMs and evaluate the candidate classifiers on the resulting adversarial inputs. The attacked samples are prompts for which the classification head was not trained on for each fold, with the threshold that maximizes the F\u2081 score on the training data."}, {"title": "Takeaway 4: Performance in adversarial settings", "content": "Candidate classifiers approximate well the safety classifier under adversarial inputs crafted on the corresponding LLM with a transferability rate converging to a value above 70% for most settings. Specifically for the most robust model Llama 2, the transferability rate to the candidate classifiers exceeds 95% when they use more than 50% of the model."}, {"title": "Attacking the Surrogate Classifier", "content": "In this section, we aim to answer RQ3, i.e., if we can jailbreak aligned LLMs by attacking the surrogate classifier. This section is symmetrical to Section 4.3. Instead of applying the attack on the LLMs and evaluating the inputs on the candidate classifiers, we attack the latter and evaluate how well the adversarial inputs transfer to the LLMs.\nBaseline ASR on candidates. Figure 9a reports the ASR of the attack on the candidate classifiers, which corresponds to the percentage of unsafe modified samples from the test dataset that were labeled safe by the candidate classifier (using the best threshold defined above). We note that when the candidate classifier uses the entire model, the corresponding ASR is close to the baseline ASR (e.g., 20% for Llama 2).\nTransfer to the LLM. Figure 9b shows the proportion of adversarial inputs crafted on the candidate classifiers that induce a misclassification on the model. Interestingly, the results do not exhibit a strongly increasing trend with respect to the candidate size like in the previous sections. For three of the models, we observe a peak above 70% when the normalized candidate size approaches 60%. In addition, attacking the candidate classifiers and transferring the resulting inputs to the model seem to work better than attacking the model directly. For example, using 50% of Llama 2, it is possible to achieve an ASR of 70%, well above the baseline ASR in Table 3.\nOptimal candidate size. The existence of a maximum for the performance of the candidate classifiers highlights the importance of the candidate size 8. If 8 is too small, the candidate classifiers might fail at capturing the information for the attack, leading to a smaller transferability rate. On the other hand, if 8 is too high, the candidate classifiers might be compromised by information orthogonal to the safety classification, as seen in the decrease from Figure 2."}, {"title": "Takeaway 5: Adversarial objective of attacks", "content": "White-box attacks have relied on heuristic-driven adversarial objectives (e.g., maximize the probability of a specific target output). By attacking the surrogate classifier, the adversarial objective is misclassification: a heuristic-free objective that removes unnecessary constraints on the search space of adversarial inputs."}, {"title": "Takeaway 6: Efficacy and efficiency of attacks", "content": "Extracting the safety classifier of an LLM results in an increased attack efficacy (higher ASR) and efficiency (smaller model to attack) when attacking the surrogate classifier and transferring the modified inputs to the model. For example, attacking Llama 2 with only 50% of the model architecture leads to an ASR of 70%, well above the baseline ASR of 22%."}, {"title": "Discussion & Future Work", "content": "This study focuses on the problem of safety, i.e., classifying unsafe and safe inputs. We chose this for two main reasons. First, recent work has shown that there is a clear separation between the two classes in the model (see Section 6). Second, this problem is one of the most prolific research topics (e.g., jailbreak attacks), and the insights from this study lead to implications for white-box attacks, which we discuss in the next section."}, {"title": "Implication for White-box Attacks", "content": "Classification objective. As opposed to evasion attacks on classifiers which cause misclassifications, jailbreak attacks aim to induce unsafe behaviors. This objective is more complex because it follows from the safety definition. When extracting the classifier, we explicitly uncover the decision made by the model. This gives a clearer signal which does not rely on heuristics (e.g., maximizing a target sequence that is likely to induce a jailbreak) and increases the attack efficacy.\nEfficiency. Recent white-box gradient-based attacks on LLM are based on the GCG attack [61]. Although there have been significant performance improvements over time, these attacks compute gradients and, therefore, they are limited when the models are too large. Our results from Section 4.4 suggest that investigating the robustness of alignment through attacks is reducible to attacking the surrogate classifier. This implies fewer computations and thus higher efficiency (and scalability) of attacks. Our approach was applied on LLMs with less than 9 billion parameters and showed that using 50% of it is enough for the attack. If this result scales to larger models"}, {"title": "Safety Classifier", "content": "Start point of the classifier. We focus on the structures f1,1+8, i.e., contiguous sets of decoders starting with the first layer. This allowed to locate the end of the safety classifier by evaluating the performance of the corresponding candidates. However, it may be that the classifier does not start at the first layer. Therefore, studying structures fi,i+8 with i > 1 may be of interest to refine the extraction but introduce a new dimension of complexity. Since the input space of decoder i must match the output space of decoder i - 1, new techniques need to be introduced to allow the creation of candidates that truly use the information of the structure. For example, a candidate classifier based on fi,i+8 should not use information from f1,i-1 or learn new information outside the mapping from structure representations to classification. We leave such explorations to future work.\nFiner-grained extraction. Our approach uses results from previous work on the model representation of unsafe and safe inputs to reduce the space of structures. Within decoder blocks are multiple attention heads with different patterns learned. Therefore, it is possible to further refine the extraction of the classifier by removing attention heads that do not contribute to classification. The intractable number of possibilities implies the need for heuristics to search the space of structures efficiently. Since the candidates use a simple linear classification head, its performance is a function of how linearly separable the two classes are. Thus, a greedy approach in which attention heads are selected on the basis"}, {"title": "Jailbreak and Representations", "content": "Improving the safety of LLMs is one of the most prevalent research areas: numerous publications have studied the link between jailbreak and model representations. It is necessary that alignment creates a linear separation between refused and accepted prompt for some representations, since the output is explicitly linearly dependent on the last embedding [24]. Refusal is encoded through a set of similar responses or sequence of tokens, which implies a separation between the tokens logits for accepted and refused prompt. Several papers have shown the distinction at the decoder level [19, 21, 24, 53, 56] or at the head of the attention level [57]. In addition, previous work has also shown that representations can be used to manipulate the model [59] and increase its robustness at a negligible utility cost [60]. Our work takes a different approach by considering an embedded classifier within the"}, {"title": "Adversarial Settings", "content": "Isolating the security critical component of an LLM is beneficial from attack and defense perspectives. Previous work has shown multiple ways of using the identified component. From a defense perspective, it is possible to fine-tune the LLM while freezing the corresponding component to avoid destroying the learned safety features [21, 45].\nFrom an attack perspective, there are three ways to use the component: ablation [57], intervention [19], or optimization of the input to attack the component [24]. Ablation corresponds to disabling the component during generation, preventing the model from classifying the safety of the input. Interventions operate by adding a perturbation to the embedding at the position of the component. If the component is a classifier, the perturbation corresponds to the direction orthogonal to its decision boundary.\nIn this work, we extract a surrogate classifier which can be attacked directly with a heuristic-free adversarial objective. While interventions can be applied to the candidates, they may not lead to feasible input perturbations (i.e., it may not be possible to find a sequence of tokens that represent the intervention), which limits the characterization of alignment failures in adversarial settings."}, {"title": "Pruning", "content": "In addition to studying their safety, an active area of research on LLMs is pruning: reducing the size of the model while maintaining most of its capabilities [10, 28]. For LLMs, pruning can be divided into two categories: width pruning and depth pruning. The former aims to reduce the size of the projection matrices, while the latter aims to remove decoders. The rise of security concerns led to the intersection of pruning and security. Recent work has shown that safety alignment is inherently low rank and can be isolated [45] and that pruning can offer higher safety [13]. Our work extracts the safety classifier within the model; therefore, the objective is different. Instead of reducing the size of the model while maintaining capabilities, which results in some loss of information, we aim to extract the safety classifier. Thus, minimizing the information lost on its decision boundary by having as little training as possible is crucial."}, {"title": "Conclusion", "content": "In this paper, we hypothesized that alignment embeds a safety classifier within LLMs, and we introduced techniques to extract best candidate classifiers for it. We empirically verified the agreement with the models of the best candidates in"}, {"title": "Appendix", "content": ""}, {"title": "Confusion Matrices", "content": "Figure 10 and Figure 11 show the confusion matrices of the 5 models after applying the GCG attack on the LLM."}, {"title": "Attacking Safe Inputs", "content": "As with any classification problem, mitigating false negatives (jailbreaks) comes at the cost of higher false positives (over- refusal). This effect was seen in the lower classification performance on OR-Bench (designed to test this phenomenon) in Section 4.2. In this section, we focus on attacking safe inputs to create a refusal.\nSetup. We also augment the dataset by adding targets corresponding to a refusal. Different aligned models have different refusal answers depending on how they were trained. However, we saw limited impact on the choice of the target string on the performance of the attack and kept the same target for all prompts and models: \u201cI cannot fulfill your request. I'm just an AI\"."}, {"title": "Other Models", "content": "Our work focuses on aligned LLMs under the assumption that alignment embeds a safety classifier. Therefore", "models": "Llama-3.1-8B-Instruct [30", "17": "and Zephyr_RMU [20"}]}