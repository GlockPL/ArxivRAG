{"title": "Maximize Your Diffusion: A Study into Reward Maximization and Alignment for Diffusion-based Control.", "authors": ["Dom Huh", "Prasant Mohapatra"], "abstract": "Diffusion-based planning, learning, and control methods present a promising branch of powerful and expressive decision-making solutions. Given the growing interest, such methods have undergone numerous refinements over the past years. However, despite these advancements, existing methods are limited in their investigations regarding general methods for reward maximization within the decision-making process. In this work, we study extensions of fine-tuning approaches for control applications. Specifically, we explore extensions and various design choices for four fine-tuning approaches: reward alignment through reinforcement learning, direct preference optimization, supervised fine-tuning, and cascading diffusion. We optimize their usage to merge these independent efforts into one unified paradigm. We show the utility of such propositions in offline RL settings and demonstrate empirical improvements over a rich array of control tasks.", "sections": [{"title": "1. Introduction", "content": "Denoising diffusion probabilistic models (DDPM) have demonstrated incredible successes in numerous domains as one of the de facto standards for generative modeling in continuous domains (Sohl-Dickstein et al., 2015; Ho et al., 2020; Yang et al., 2024), garnering heightened attention in research communities with the model's exceptional ability to capture highly complex data distributions and flexibility in constraint-based optimization that can be applied to diverse modalities. A notable direction is their adoption and realizations in decision-making applications (Zhu et al., 2024), contributing to an evolving class of model-based planning as well as model-free algorithms we will refer to as diffusion-based planning, learning, and control. Despite the appealing promises of DDPMs, the algorithm's sample generation inefficiencies loom as a notorious and significant concern (Cao et al., 2024), causing a severe bottleneck as an efficient solution for closed-loop control.\n\nThe focus of this work addresses this limitation, proposing a general investigation into reward maximization, or more aptly called reward alignment, directly on the diffusion process for control. In turn, our objective is to reduce the number of samples required to generate to maximize the reward signal. We adopt modern optimization procedures to facilitate a guided and constrained distributional shift biased towards maximal returns. Specifically, we investigate four reward alignment approaches reward maximization with reinforcement learning, direct preference optimization, supervised fine-tuning, and cascading diffusion and compare their performance in various control settings, more so in offline RL settings. We evaluate various design choices as well as an unification of the studied alignment approaches on a diverse testbed of challenging control tasks consisting of a variety of complex tasks from benchmarks such as D4RL (Fu et al., 2020), MetaWorld (Yu et al., 2021), PushT (Florence et al., 2022), Relay-Kitchen (Gupta et al., 2019), and RoboMimic (Mandlekar et al., 2021) as well as a novel evaluation task that enables in-depth analysis and visualization of the various features of the environment, the controller and their interactions."}, {"title": "2. Background", "content": "Diffusion Models DDPMs (Ho et al., 2020) are commonly known as a class of generative models, which aim to estimate and sample from unknown data distributions X. DDPMs define a Markov chain of forward Gaussian kernels $q(x_t|x_{t-1})$ with a scheduled (Nichol & Dhariwal, 2021) sequence of variance coefficients {$\u03b2_t \u2208 (0, 1)$}$_{t=1}^{T}$ that gradually add noise to input data\n\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 \u2013 \u03b2_t}x_{t-1}, \u03b2_tI)$ (1)\n\nand then learn to reverse the diffusion process by denoising with learnable reverse Gaussian kernels $p_\u03b8(x_{t-1}|x_t)$, parameterized by \u03b8, to generate data samples $x_0$ from a standard Gaussian noise prior $x_T \u223c N(0, I)$.\n\n$p_\u03b8(x_{t-1}|x_t) = N(x_{t-1}; \u03bc_\u03b8(x_t, t), \u03a3_\u03b8(x_t, t))$ (2)\n\nA common practice is to parameterize $\u03bc_\u03b8(x_t, t)$ to perform \u03f5-prediction, motivated by its resemblance to the Langevin dynamics, which enables a simplification of the training objective.\n\n$\u03bc_\u03b8(x_t, t) := \\frac{1}{\\sqrt{\u03b1_t}} (x_t + \\frac{\u03b2_t}{\\sqrt{1 - \\bar{\u03b1}_t}}\u03f5_\u03b8(x_t, t))$ (3)\n\nThe training objective of DDPM uses the variational lower bounds of the marginal likelihood to minimize the negative log-likelihood (Kingma et al., 2021), as shown in Equation 4, where in practice, a simplified yet equivalent objective $L_{DDPM}(\u03b8)$ is commonly used (Ho et al., 2020).\n\n$L_{NLL}(\u03b8) = E_{q(x_{1:T}|x_0)}[\u2212log p_\u03b8(x_0)] \u2264 -E_{q(x_{1:T}|x_0)}[log \\frac{q(x_{1:T}|x_0)}{p_\u03b8(x_{0:T})}] $(4)\n\n$L_{DDPM}(\u03b8) = E_{x_0, \u03f5} [||\u03f5 \u2212 \u03f5_\u03b8(\\sqrt{\\bar{\u03b1}_t}x_0 + \\sqrt{1 \u2212 \\bar{\u03b1}_t}\u03f5, t)||^2]$ (5)\n\nwhere $t \u223c [0, T]$ with T is the total number of diffusion steps, $\u03b2_t = 1 \u2212 \u03b1_t, \\bar{\u03b1}_t = \u03a0_{i=1}^{t}\u03b1_i$, and $\u03f5 \u223c N(0, I)$. A common and useful intuition is that DDPMs learn information regarding a time-dependent vector field $u_t(x_t, 0)$ that generates a probability density/diffusion path $p_t(x_{t\u22121}|x_t)$ from a simple and known distribution (e.g., standard Gaussian) to the unknown target distribution (e.g., data distribution).\n\nSeveral recent improvements and reformulations of this DDPM paradigm have been explored (Cao et al., 2024), most notably and relevantly, towards efficient sample generation with implicit probabilistic models (DDIM (Song et al., 2020a)), v-prediction (Salimans & Ho, 2022) and rectification (Liu et al., 2022; Wang et al., 2024), conditional guidance (Dhariwal & Nichol, 2021; Ho & Salimans, 2022), model selection (Rombach et al., 2022; Ho et al., 2022; Peebles & Xie, 2023), novel DDPM training paradigms (Fan & Lee, 2023; Black et al., 2023) and related classes of generative modeling such as score/flow-matching (Song et al., 2020b; Lipman et al., 2023).\n\nAlignment of Diffusion Paths Training deep learning models, such as a DDPM, is often done in two stages. In the first stage, the foundation model is optimized, where a general underlying data distribution is learned by training on large-scale datasets. In the second stage, this foundation model is fine-tuned to align to a reference distribution (e.g., towards downstream tasks, human preference, or stitching with multiple distributions) through supervised fine-tuning (SFT) (Ziegler et al., 2019), RL with AI/human feedback (RLAIF/RLHF) (Bai et al., 2022; Ouyang et al., 2022a), preference optimization (DPO) (Rafailov et al., 2024; Wallace et al., 2024; Xiao et al., 2024), or reward gradients (Prabhudesai et al., 2023; Clark et al., 2023). In this work, we apply and modify these techniques toward DMCs to better align using a scalar feedback signal that defines the reward distribution for sequential decision-making tasks.\n\nDiffusion Models for Control (DMC) Like many generative modeling algorithms, the role of DDPMs in decision-making applications has spanned from diffusion-based planning (Janner et al., 2022) to policy learning (Ren et al., 2024), with significant successes and advancements in online and more prevalently in offline RL settings. Specifically, its utilization has been quite diverse, taking various forms such as diffusion-based policies, value functions, and world models, and these models can either indirectly support the training of RL policies (Rigter et al., 2023) or be more of an active component of the decision-making loop (Wang et al., 2022; Chen et al., 2022). In this work, we extend these prior efforts, specifically Diffuser (Janner et al., 2022), Diffusion Policy (DP) (Chi et al., 2023), SynthER (Lu et al., 2024) and some of their following works (Ajay et al., 2023; Dong et al., 2024a) to act as our foundation to build upon with our alignment efforts."}, {"title": "3. Reward Alignment of DMC", "content": "We define a general equation that defines the reward alignment optimization problem, in which we align our diffusion model $\u03c0_\u03b8$ to generate samples $x_0$ that maximize a reward function.\n\n$L_{align}(\u03b8) := E_{x_0\u223c\u03c0_\u03b8}[r(x_0)] \\quad s.t. \\quad div(\u03c0_\u03b8, X) \u2264 \u03b4$ (6)\n\nThe constraint subjects the optimization to ensure the generated samples remain within a plausible region of the target data distribution X, or the distribution learned by the foundation DDPM $f_{\u03b8_{fdt}}$.\n\nWe illustrate this objective in Figure 2, where a foundation DDPM is fine-tuned to maximize a given reward function on a toy task. In this example, the fine-tuned diffusion paths largely adhere to the foundation model, with minor yet impactful adjustments towards maximizing their respective reward functions, exceeding \u00d73.21, \u00d77.09, and \u00d74.18 improvements in orange, green and blue respectively within 32 generated samples while maintaining $P_r(x_0|X) > 0$. Specifically, with the orange reward function, despite having the potential to converge towards out-of-distribution (OOD) regions for maximal rewards, the constraints are in place to adequately address this issue in a desired manner. Hence, to achieve this alignment, we study and extend four approaches - RL, DPO, SFT, and cascading to optimize the objective in Equation 6.\n\nAn important distinction between DDPM and DMC is that, at denoising step t, DMC specifies that $x_t$ is either a N-step state-action trajectory ${s_0, a_0, s_1 ... s_N}_t$ (e.g. Diffuser and DD), or actions $a^\u2217_t$, (e.g. DP) with a conditional information regarding the state $s_tr$ as shown in Figure 3. Hence, our reward function r($x_0$) can either represent the return of the state-action trajectory or the Q-value of each state-action pair for greater granularity as a way to take advantage of the temporal aspect of the latent representation."}, {"title": "3.1. Reward Alignment with Reinforcement Learning", "content": "A natural approach to reward maximization is to extend traditional RL optimization to align the denoising process towards sample generation with higher returns. To do so, we view the DMC as a denoising policy\n\n$\u03c0_\u03b8(\u03bc_t|x_t, c) = p_\u03b8(x_t|x_{t\u22121}, c)$ (7)\n\nwith a condition input c, treating the denoising process as a multi-step MDP (Black et al., 2023). We build upon two classic RL algorithms that directly optimize Equation 6: REINFORCE (Williams, 1992) and Q-value policy gradient (QV-PG) (Silver et al., 2014; Haarnoja et al., 2018).\n\nTo account for the sequential nature of the denoising process, we consider two important training concerns: gradient truncation and denoising credit assignment.\n\nGradient truncation clips the gradient such its calculation only propagates through the last K steps (Clark et al., 2023; Prabhudesai et al., 2023), where K can be dynamically set during training. This truncation implements truncated backpropagation through time/denoising steps, facilitating a more memory-efficient and stable optimization. In practice, we utilize the stop-gradient function sg(\u00b7, K), which truncates the gradient computation to the last K denoising steps.\n\nAs each sampling step contributed differently towards reaching $x_0$, we consider the concept of credit assignment over the denoising steps. Empirically, we define a sequence of hyperparameters {$\u03ba_0, \u03ba_1, . . .$} that scale the objective along the denoising steps. In this work, we consider two options, where \u03ba can be a decreasing sequence or $K_t$ can be the similarity measure between ($x_t$, $x_{t\u22121}$) and ($x_T$, $x_0$), a computation that is done retrospectively.\n\nTo prevent large divergence from the reference policy/foundation DMC and adhere to the constraint in Equation 6, we can use trust regions with clipping, i.e., proximal policy optimization (PPO) (Schulman et al., 2017), similar to the procedure introduced with DDPO (Black et al., 2023), or add a KL regularization term. We found both REINFORCE/PPO and QV-PG approaches to be competitive, though each has pros and cons that make their comparison seem moot, and the one that performed better is kept during evaluation."}, {"title": "3.2. Reward Alignment with Preference Optimization", "content": "Direct preference optimization (DPO) (Rafailov et al., 2024) offers an alternative approach to align DMC by utilizing a dataset of ranked pairs x < x\u2032 of losing and winning samples and maximizing the likelihoods of the DMC\u2019s sample generation following the preference operator \u227a. This preference operation is defined in this work as follows:\n\n$x < x' \u2192 E[r(x')] < E[r(x')]$ (10)\n\nFollowing DiffusionDPO adaptation (Wallace et al., 2024), the DPO objective $L_{DPO}(\u03b8)$ is simplified and can be expressed as follows:\n\n$L_{DPO} (\u03b8) = -E[log \u03c3(-\u03b3w(\\Delta_t)(F_w(\u03b8) \u2212 F_l(\u03b8)))]$ (11)\n\nwhere \u03c3(\u00b7) is the sigmoid function, $\u03bb = \\frac{\u03b1}{\u03c3^\u2217_l}$ is the signal-to-noise ratio, w($\\Delta_t$) is a weighting function and\n\n$F_\u2217(\u03b8) = ||\u03f5_\u2217 \u2212 \u03f5_\u03b8(x, t)||^2 \u2212 ||\u03f5_\u2217 \u2212 \u03f5_{ref}(x, t)||^2$ (12)\n\nA functional representation of the preference operator can be expressed either heuristically or, more generally, as a trajectory-wise value function (Zhang et al., 2024)."}, {"title": "3.3. Reward Alignment with Supervised Fine-Tuning", "content": "Similar to DPO, we can instead fine-tune the foundation DMC only using the winning samples, or samples with the highest rewards, by updating along $L_{DDPM}(\u03b8)$ on these samples. Therefore, we apply traditional diffusion updates to shift and concentrate the generative distribution towards higher rewards under a stable optimization scheme, i.e., supervised learning.\n\nThis process can be performed iteratively, using either the foundation or the fine-tuned DMC as a data synthesizer to generate samples of increasingly higher returns. However, if we use the fine-tuned DMC, we face potential instability and OOD issues. In this work, we prevent this using a threshold constraint that prevents samples with lower likelihoods of the foundation DMC from being used for an SFT update."}, {"title": "3.4. Up-sampling Reward with Cascading Diffusion", "content": "Cascading DDPM (Ho et al., 2022) is an up-sampling technique used for super-resolution, where lower resolution samples are processed as the conditional inputs to sample higher resolution samples. This concept can be extended to control applications, where a DMC that is conditioned on $x_{cond}^{LO}$ is trained to generate a sample $x^H_0$ that yields greater rewards, hence r($x_{cond}^{LO}$) < r($x^H_0$).\n\nIn prior efforts with similar motivations (Zhang et al., 2024), the objective followed classifier-free guidance (CFG), training with the conditional information at some probability p \u2208 [0, 1]. Since, in our case, the cascading DMC is connected to a foundation DMC, it does not necessarily have to emphasize the generation of the target distribution; we set p = 1. Transferring the trained weights from a foundation DMC as a warm-start proved beneficial and was likely sufficient to understand the target distribution.\n\nAn important consideration for training cascading DMC is determining the pairs ($x_{cond}^{LO}$, $x^H_0$) of samples to condition and reference during training, where their alignment may be mismatched, especially for multi-step samples. To address this, when curating ($x_{cond}^{LO}$, $x^H_0$), we promote consistency between the pairs. This consistency can be achieved by in-painting a contiguous subset of the sample or sharing the latent state $x_t$ at a denoising step t. These approaches force some level of relations between the two samples, such that the cascading DMC avoids up-sampling towards random samples during inference.\n\nTo further boost the up-sampling capability of the cascading DMC, we can extend the other alignment approaches to greaten the difference between r($x_{cond}^{LO}$), r($x^H_0$) with proper modifications. Our exploration of this study is limited, and we encourage future works to research this topic further."}, {"title": "3.5. An Unification of Reward Alignment Approaches", "content": "The four alignment methods discussed above RL, DPO, SFT, and cascading share a unified goal of solving the constrained optimization problem defined by Equation 6. While each approach is suited for different use cases, with their unique advantages and limitations, we find that they can be applied together. As the cost of increasing model alignment efforts is modest relative to training the reference policy (Ouyang et al., 2022b), we propose a sequential optimization scheme, where each alignment approach (i.e., RL, DPO, and SFT) is used individually, and repeatedly until convergence. This iterative sequential scheme is practiced to avoid potential catastrophic learning variances of a naive multi-objective optimization approach. We find that after 2-3 passes of this sequence, there are diminishing returns in the reward maximization efforts. Again, the cascading module is trained separately and appended onto the fine-tuned model during inference. Similarly, we experimented with performing multiple passes through the cascading module but found high variances in the up-sampling after 2 passes."}, {"title": "3.6. Design and Training Choices", "content": "Diffusion Sampling and Process We apply the proposed alignment methods to widespread advancements to the classical DDPM formulation. For instance, accelerated sampling methods such as DDIM (Ho et al., 2020) and other representations of the iterative diffusion-like posterior, such as score-matching (SM) (Song et al., 2020a) and flow-matching (FM) (Lipman et al., 2023), are also studied in this work under the Diffuser, DD and DP framework utilizing these alignment approaches.\n\nIn-painting The in-painting ability of DDPMs refers to reconstructing missing regions within the data based on the context provided. In our alignment efforts, we rely on this capability to bootstrap training samples to enforce consistency and avoid undesired changes to the sample. To ensure that our model can in-paint effectively, we trained the foundation DMC directly on the in-painting task, where we utilize causal masks along the temporal axis following the Repaint procedure for conditioning (Lugmayr et al., 2022). This in-painting procedure is followed during inference for planning-based DMC. In our experiments, we found that including this masked training improved not only the DMC\u2019s ability to in-paint but also the overall performance of the DMC.\n\nContext Window and Recurrence The context window for the DMC defines the number of time steps contained in input for planners and conditional input for the policy. Under specific model architectures, such as a temporal convolution backbone, this window does not face significant computing constraints. However, we opt for a sliding window approach to handle this constraint for backbones that scale poorly along this dimension and for tasks with longer time horizons. We found the introduction of a recurrent state appended to the latent state to cause instability in some of the tasks, and even in certain tasks with less complex observations with greater partial observability, we found minor but negligible improvements.\n\nEvaluation Function For all alignment methods, we use a separate Q-value function trained under implicit Q-learning in an offline RL fashion to evaluate the samples generated and calculate the learning signal. The Q-value can also be used during action-selection.\n\nOnline Fine-tuning We additionally study the inclusion of online fine-tuning, where we inject samples from online interactions during the alignment training to compare and analyze the changes in performance.\n\nPEFT For all alignment processes, we use LORA (Hu et al., 2021), a parameter-efficient fine-tuning approach that introduces a separate set of low-rank weights on linear layers that are trained during fine-tuning. Separate LORA weights were not introduced between different alignment methods."}, {"title": "4. Experiments", "content": "4.1. Tasks\n\nWe evaluate our alignment approaches on various offline RL benchmarks. We use D4RL and MetaWorld for planning-based DMC; for policy-based DMC, we use Robomimic, Relay-Kitchen, and PushT. There is a standardized and publicly available offline RL dataset for many of these tasks, and we direct readers toward their respective original works for more details.\n\nNav1D We also introduce simple 1D navigation (Nav1D) task to our testbed, shown in Figure 4, where the agent start at a random position $s_t \u2208 [-1, 1]$ and must move left or right. The agent must select actions under a continuous yet restricted action space with a deterministic transition dynamics\n\n$T(s_t, a_t) = l(clip(s_t + \u03b3a_t, \u22121, 1))$ (13)"}, {"title": "4.2. Implementation Details", "content": "The basis of our implementation is built upon CleanDiffuser (Dong et al., 2024b), with adjustments made for the alignment methods. A central difference in our implementation of the DMC is the lack of a return conditional input, as it is generally unknown what return is desired. Hence, for action selection, we use the learned Q-Value function to greedily select actions, similar to IDQL (Hansen-Estruch et al., 2023). For DP, we also experiment with expectation-based action selection, where the average over all generated action samples is taken. The model architecture for each algorithm we studied follows the original work, similar to CleanDiffuser. All experiments presented were executed on 3 Nvidia RTX A6000 and Intel Xeon Silver 4214R @ 2.4GHz."}, {"title": "4.3. Results For Planning-based DMC", "content": "We summarize our finding for planning-based DMC in Table 2, showing the notable improvements from our proposed alignment process, regardless of the sampling approaches such as DDPM, DDIM, SM, FM, and DD frameworks, with decision-making that uses a small number of roll-outs during its planning. Specifically, for each task and algorithm, we first trained the reference DMC and finetuned it using varying passes and iterations through {RL, DPO, SFT} alignments.\n\nWe closely examined the learning for these training to ensure no training collapses occurred, with model check-pointing and lowered learning rates. This phenomenon includes sudden drops in return and instabilities/spikes in the training objectives. We did observe some instances of this, especially during the RL finetuning, which we redacted. We trained the cascading DDPM separately, applying the same alignment process, and attached it for the final evaluation. In general, we found that even with multiple rounds, the return did improve, as shown in Figure 5.\n\nAn important trend we noticed as a result of the alignment is that, on average, over all tasks and algorithms, the average standard deviation over the return on all trials significantly decreased, from 38.1% \u2192 5.3%. This result empirically indicates the distributional shift that occurs, which narrows the diffusion process towards higher returns, as the average return improved by +43.0%. Using only the D4RL tasks, we introduced an entropy regularization term to counter this distributional narrowing as much as possible. However, this resulted in the average return improvement to decrease from 32.2% \u2192 20.3%.\n\nAdditionally, with the alignment process under the Nav1D task, we found the coherency score remained relatively stable with repeated passes and iterations. We monitored this value during training to ensure stable learning, tuning the divergence control hyperparameters as needed. However, we did notice large decreases in the coherency score when we performed multiple passes through the cascading DDPM.\n\nLastly, we experimented with online fine-tuning, enabling interactions with the environment during the alignment process to collect more data points. We also continued training the Q-value function on these samples. We evaluated this inclusion under D4RL benchmark tasks, and we observed a significantly more stable and efficient training stage with no training collapses and notable improvements in the return achieved."}, {"title": "4.4. Results For Policy-based DMC", "content": "In Table 3, we evaluate the performance of Diffusion Policy (DP) using two different action selection methods using the DiT architecture (Peebles & Xie, 2023). Similar to our approach for the planning-based DMC, we applied the alignment approach in sequential order and found multiple rounds still to have return improvements. We find overall improvements in return with our proposed alignment process, more so with the expectation action selection models.\n\nIn some cases, we found the energy-based action selection approaches such as IDQL are sufficient to achieve perfect success rates, where we still observed that the fine-tuning did affect the narrowness of the posterior, measured by significantly lower entropy between the generated action candidates.\n\nRegarding the coherency score in the Nav1D task, we found that the alignment process did shift the action distribution away from the behavioral policy, more notably in models trained on the partial and full datasets and upon online fine-tuning. This trend was not as starkly observed in the expert dataset, which retained a comparable coherency score from its reference counterpart."}, {"title": "5. Conclusion", "content": "In this work, we studied four alignment methods \u2013 RL, DPO, SFT, and cascading \u2013 for reward maximization in diffusion-based control models (DMC). While these alignment methods were applied independently in prior works, we proposed a unification of these fine-tuning processes using a sequential optimization scheme. Our findings were evaluated in both planning-based and policy-based DMC on various control tasks, and we established the empirical utility of using our proposed alignment approach. For future directions, we suggest finding methods of automating the sequential process, avoiding manual progression on the alignment sequence upon convergence or training failures."}]}