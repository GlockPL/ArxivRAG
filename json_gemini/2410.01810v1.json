{"title": "Propaganda is All You Need", "authors": ["Paul Kronlund-Drouault"], "abstract": "As ML is still a (relatively) recent field of study, especially outside the realm of abstract mathe-matics, few works have been conducted on the political aspect of LLMs, and more particularly about the alignment process and its political dimension. This process can be as simple as prompt engineering but is also very complex and can affect completely unrelated questions. For example, politically directed alignment has a very strong impact on an LLM's embedding space and the relative position of political no-tions in such a space. Using special tools to evaluate general political bias and analyze the effects of alignment, we can gather new data to understand its causes and possible conse-quences on society. Indeed, by taking a socio-political approach, we can hypothesize that most big LLMs are aligned with what Marx-ist philosophy calls the 'dominant ideology.' As Al's role in political decision-making\u2014at the citizen's scale but also in government agen-cies-such biases can have huge effects on so-cietal change, either by creating new and in-sidious pathways for societal uniformity or by allowing disguised extremist views to gain trac-tion among the people.", "sections": [{"title": "Introduction", "content": "How many times, since the start of the current Ar-tificial Intelligence (AI) era, have I been told by friends, professors, or even colleagues to ask a Large Language Model (LLM) to get an unbiased opinion about a subject? Indeed, in the collective doxa, the belief that LLMs are still ruled by deter-ministic and logical rules implies, a fortiori, that they must be neutral, or even 'apolitical,' and that they thus represent a way to obtain an 'objective opinion' 1 While this might seem unimportant at first, it convinced me to dig a bit deeper into the underlying structure defining an LLM's ideology and how it can be expressed through its output. In order to do this, we will also focus on a pro-cess called alignment (\u00a73). Building upon multiple works highlighting the political nature of AI and pointing out specific biases, the goal of this re-search work is to understand the different ideologi-cal biases present in LLMs through a deep analysis of the inner workings of LLMs and particularly the alignment process, and to evaluate our findings using a socio-political framework to highlight their societal causes and possible consequences. We are first going to review the previous works on the mat-ter, their approaches to bias analysis and political classification, and try to elaborate new methods to better understand the nature of ideological ten-dencies in LLMs. We are going to dive into the embedding space and other crucial AI concepts to study how political biases form and how they might be stored, both in the MLP (3Blue1Brown, 2024) and the model's embedding space (Tennen-holtz et al., 2024). Then, using both experimental data we gathered for this project and the findings of other papers on the subject, we are going to try and formulate a socio-political hypothesis to explain such biases, the need for and dangers of alignment, and the possible societal impact of political AI."}, {"title": "Biases", "content": "The most difficult part when talking about the po-litical nature of AI is to first acknowledge its exis-tence. Indeed, most current mainstream models do seem very neutral in their approach to societal con-cerns. As of right now, the only public mentions of AI political bias have been made by reactionary tendencies opposing the progressive ideas of main-stream LLMs.\nThis neutrality effect can be explained by the Marxian concept of dominant ideology, which states that in a specific society, the global ideology seen as the objective and neutral truth depends on"}, {"title": "Political Alignment", "content": "The process we call here 'alignment' can refer to any means to direct the action of autonomous sys-tems towards a specific goal (Wiener, 1960). Some aspects of this topic have been largely covered by research work, but our field of interest will be Po-litical Alignment, or the action to modify an Al's behavior to agree (align) with a particular set of political views. Our interest in this subject will be two-fold:\n\u2022 First, we will try to understand how it relates to human-to-human ideological influence and how we can understand it using the usual human-directed tools of socio-political analy-sis.\n\u2022 Then we will dig into the underlying theoreti-cal structure and see what political alignment really does to the language model. This tech-nical understanding could then be used as a model and applied to human social dynamics."}, {"title": "Unsupervised Alignment", "content": "The most basic form of alignment in an AI model is unsupervised, text-based alignment. Indeed, al-though less effective and more computationally intensive than other forms of more supervised tun-ing, pure text training can be very useful to affect the deepest levels of AI models, particularly in the MLP. By training an NLP generative model on politically biased pure text data, the AI will inter-nalize the particular ideological biases of the text"}, {"title": "ORPO/ DPO Alignment", "content": "This concept of alignment has a lot of similari-ties with human-directed political influence. Some techniques, such as DPO (Rafailov et al., 2024) or ORPO (Jiwoo Hong and Thorne, 2023), use data sorted into ideological categories and assign weights to them. These techniques aim to modify the AI tendencies to align with them and can evoke quite literally propaganda posters from WWII or the Cold War, mixing desired ideologies with posi-tive signals and the 'enemy' position with negative brain stimuli. Most big players in today's LLM market use such techniques to prevent behaviors they find harmful. This method is very powerful as it doesn't require a lot of training to be effective, but I hypothesize that their impact is more on the 'surface level,' acting like a roleplay prompt rather than completely changing the inner structure of the model.\nAs for the unsupervised alignment, we also mea-sure a difference in embedding space, which, al-though smaller, affects a reduced set of vectors in more predictable ways."}, {"title": "Guarded Alignment", "content": "For example, Anthropic AI (ant) uses a special 'Election Interference' dataset on which they prob-ably train a 'Guard' Model (usually cheap 7-8b one) to detect any generation that might look alike. This process is different from traditional alignment because it involves a different AI component, usu-ally politically aligned with a specific ideological set, to try and detect 'bad' or 'unsafe' output."}, {"title": "Aligned Humans", "content": "Now that we have an overview of how Language Models can be influenced towards a specific set of views, it could be interesting to see how our analy-sis framework can be applied to human beings. Is ideological propaganda only a way to modify our brain's embedding dimensions? Is consciousness outside of this influenced space, or can it also be modified using reward/punishment techniques sim-ilar to ORPO/DPO? Is Freud's superego some kind of 'Guard' model, preventing us from misaligning ourselves?\nObviously, these questions are way out of the scope of this work, but they show how understand-ing thinking machines can allow us to better under-stand ourselves."}, {"title": "Evaluation", "content": "In order to gather data on political biases, we com-bine the different approaches we saw so far to cre-ate both a subjective metric and one grounded in real-world truth using a dataset of ideological bi-ases from sources known to have such biases. This approach is directly inspired by those cited in 2, especially the one used in (Agiza et al., 2024), but we try to gain a more profound analysis by giving more flexibility to the evaluator agent and making the scoring step more modular and multimodal 6."}, {"title": "Relative Position", "content": "To get compass-like data on a model, we use a process of multimodal relative evaluation. This process comprises two steps, the question phase, which we also call cuisine:\n1. A large evaluator model asks questions to the evaluated LM to try to elicit its political bi-ases.\n2. A combination of other models, chosen for their different relative biases, will evaluate the entire previous conversation and create a simple rating of the ideological tendencies of the evaluated LM.\nThis method can produce very good results since it allows for a very deep analysis of the model's leanings with the adaptive prompts created by the chief evaluator model"}, {"title": "Absolute Position", "content": "In order to obtain more traditional conservative-liberal-Marxist data on an LM's biases, we use a different method that will provide less precise insights but will be grounded in real-world data. First, we get a prompt related to actual, real-world economic or political events, and we synthe-size the answers different ideological groups would give to it. For this example, we used 'liberal,' 'con-servative,' and 'Marxist' data."}, {"title": "Socio-Political Impacts", "content": "AI as a political agent creates a new type of ideo-logical relationship that poses new challenges for societies. Before the rise of autonomous learning agents, we could distinguish two categories of po-litical and ideological vectors:\n\u2022 Human conscious actors, who can have differ-ent expressed and internal ideological stand-points. Humans are known to have multiple ideological levels.\n\u2022 Non-conscious, automated vectors, which can only transmit ideological information that was given to them by conscious actors. Such vec-tors also have a single ideological level, di-rectly expressed.\nWith the rise of learning machines, a third type of vector arises\u2014non-conscious but with multiple ideological levels\u2014that can either be directly ex-pressed or affect other functions through hidden functioning. This change will affect our political thinking in the same way the internet and social networks changed our relationship with informa-tion and facts. The new shift will be in our means of analysis.\nThis new paradigm in ideological transmission is already starting to create new challenges for both researchers and users. For example, any organiza-tion that would want to use an LLM-based solution to parse and classify textual data would find its output slightly biased, either towards an 'organic' bias (accidentally present in the training set) or consciously aligned by the AI provider.\nIn the current state of AI, all major models are produced not by governments but by privately owned corporations. As with social networks, we can hypothesize that this condition will contribute to the *power shift* from governments to private entities by giving them much more influence. To prevent this, AI regulation would need not only to control an LLM's output but also precisely mea-sure its biases, and raise awareness around these issues. Indeed, in a lot of countries, political dis-course-especially in election times\u2014is heavily regulated (CC), and as AI can convey ideological"}, {"title": "A Marxian Analysis", "content": "As we have already discussed, the natural bias of Language Models, coming either from its raw training data or an alignment process designed to fit social norms, can be interpreted as an expression of the dominant ideology (Marx and Engels, 1932). Indeed, training actors are almost always for-profit entities incentivized to protect their class interests, and thus to guide the alignment direction toward the capitalist/liberal side. This makes our work of alignment classification (4) much harder and makes it challenging to obtain objective metrics for an LLM's political leanings. Models trained in different political contexts even have somewhat different biases. For example, Mistral AI (a French company) made models will be slightly to the left of the GPT and Claude families, because the French political scene is globally more left-leaning than that of the US, for instance 7.\nOur analysis can even be deepened by bring-ing in the idea of hegemony, developed by An-tonio Gramsci (Gramsci, 1971). Indeed, Lan-guage Models represent in the current context an essences of the hegemonic ideology of the ruling class. Through the complex tensors of the Multi-layer perceptron, the main points of the capitalist thought framework are encoded, with alignment and/or natural bias. Furthermore, Gramsci theo-rizes that class hegemony goes through a socio-cultural process that alters ideas themselves. This concept has many similarities with the embedding space tuning in the alignment process: An LLM's world model is changed not only in its factual as-pects but directly in its analysis pathways. 8."}, {"title": "Examples", "content": "Despite the relatively recent rise of AI, we can already observe some instances of political bi-ases-intentional or not\u2014influencing an LLM's production. An article from the HKS Misinforma-tion Review highlights different biases in content produced by chat models such as Google Bard, Microsoft Bing Copilot, and Perplexity. Their"}, {"title": "Conclusion", "content": "While our work has shown some of the implica-tions of LLM's political biases, how they can be created or moved through alignment, and why they could have a big impact on society, our analysis barely scratched the surface of the ever-growing black boxes that are LLMs. Instead of answering my questions, this work has only created new ones, both around the socio-political aspects of this re-search and the peculiar mathematical realm where Language Models reside.\nAlong with the previous works we built upon, we have opened the door to a new intersection for research to focus on, between mathematics, com-puter science, and human studies like political sci-ence, sociology, and even psychology. Indeed, this research shows that the rise of this new kind of thinking machine is an incredible opportunity to learn more about ourselves and how we live to-gether. Studying Artificial Intelligence might fi-nally lead us to a new kind of science, joining inter-disciplinary knowledge to create a unified model for our brain and our social interactions.\nNow, as researchers, we need to consider the political implications of this study. We saw that LLMs are powerful means of ideological transmis-sion, and that most alignment decisions are taken by completely non-democratic entities pursuing only profit. We need to ask ourselves the same questions as with social networks, but with the added dimensions of the political alignment prob-lem. We already see chat models influencing, vol-untarily or not, political discourse. As of right now, most agents are quite simplistic, usually big models with a custom system prompt, that are protected by their corporate alignment, but soon we might see more subtle bots using a more moderately biased discourse to largely influence the masses. Beyond interference from external or non-governmental ac-tors, our Marxian framework pushes us to consider the consequences of AIs in intra-societal political and economic conflicts, such as class warfare or other structural antagonistic dynamics."}]}