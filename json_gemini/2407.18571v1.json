{"title": "SPEECH BANDWIDTH EXPANSION VIA HIGH FIDELITY\nGENERATIVE ADVERSARIAL NETWORKS", "authors": ["Mahmoud Salhab", "Haidar Harmanani"], "abstract": "Speech bandwidth expansion is crucial for expanding the frequency range of low-bandwidth speech\nsignals, thereby improving audio quality, clarity and perceptibility in digital applications. Its appli-\ncations span telephony, compression, text-to-speech synthesis, and speech recognition. This paper\npresents a novel approach using a high-fidelity generative adversarial network, unlike cascaded\nsystems, our system is trained end-to-end on paired narrowband and wideband speech signals. Our\nmethod integrates various bandwidth upsampling ratios into a single unified model specifically de-\nsigned for speech bandwidth expansion applications. Our approach exhibits robust performance across\nvarious bandwidth expansion factors, including those not encountered during training, demonstrating\nzero-shot capability. To the best of our knowledge, this is the first work to showcase this capability.\nThe experimental results demonstrate that our method outperforms previous end-to-end approaches,\nas well as interpolation and traditional techniques, showcasing its effectiveness in practical speech\nenhancement applications.\n\nKeywords Speech Bandwidth Expansion, Speech Enhancement, Signal Processing, Generative Adversarial Networks", "sections": [{"title": "1 Introduction", "content": "Speech Bandwidth Expansion (BWE) involves the conversion of a narrowband speech signal into a wideband one.\nThis process is also referred to as Audio Super Resolution, where the objective is to generate a high-resolution speech\nsignal from a low-resolution input containing only a fraction of the original samples [1]. BWE serves to enhance the\nquality and perceptibility of narrowband speech, a capability particularly valuable in contexts such as Public Switching\nTelephone Network (PSTN) environments [2]. Bandwidth expansion plays a vital role in many systems, and it has\nshown how the performance of automatic speech recognition systems degrades when encountering a narrowband speech\nsignal [3].\n\nAlthough the critical need for speech transmission bandwidth has diminished in modern times, numerous devices and\nequipment still operate with, receive, and even store narrowband speech. For example, many Bluetooth headphones\ncontinue to function based on narrowband speech [4].\n\nA speech signal can be described as a continuous function, denoted as f(t) over the interval [0, T], where T is duration\nof the speech in seconds, and f(t) is the amplitude at time t. To convert this continuous signal into a discrete form,\na sampling technique is employed, in such a way, a value of the signal is captured every T, seconds. This sampling\nprocess establishes the sampling rate of the signal, denoted as Fs (in Hz), which can be calculated as the reciprocal of\nthe sampling interval Ts. Consequently, the continuous function f(t) can be discretized into a vector x comprising\nsamples taken at regular interval Ts, such that x = {f(Ts), f(2T\uff61), f(3Ts), ... f(nTs)}. In terms of the sample\nrate, the discretized signal can be represented as x = {f(\\frac{1}{F_s}),f(\\frac{2}{F_s}),f(\\frac{3}{F_s})....f(\\frac{n}{F_s})}. Where n is the number of"}, {"title": "2 Related Work", "content": "Bandwidth expansion, also referred to as audio super-resolution in the literature, and it has been a subject of study for\ndecades due to its significance in many systems [6, 7, 8, 9, 10, 11, 12, 13, 14]. Early studies focused on estimating the\nspectral envelope of the high-frequency band and using excitation generated from the low-frequency band to recover the\nhigh-frequency spectrum [15]. Traditional techniques such as Gaussian mixture models, linear predictive coding, and\nhidden Markov models have also been used [16, 17, 18]. However, these methods generally perform worse compared to\nneural networks [19]. Additionally, matrix factorization techniques trained on very small datasets have been proposed\nto mitigate the computational cost of factorizing matrices [20, 21].\n\nRecent advancements in end-to-end deep neural networks generate wideband signals directly from narrowband signals\nwithout the need for feature engineering. For instance, [22] proposed training a deep neural network as a mapping\nfunction, using log-spectrum power as the input and output features to perform the required nonlinear transformation. A\ndense neural network with three hidden layers of size 2048 and ReLU activation function proposed in [3], showed that\nthis method is preferred over Gaussian mixture models in 84% of cases in a user study they made.\n\nInspired by image super-resolution algorithms, which use machine learning techniques to interpolate a low-resolution\nimage into a higher-resolution one, a convolutional U-net contains successive downsampling and upsampling blocks\nwith skip connections proposed in [23]. Building on that, a neural network component named Temporal Feature-Wise\nLinear Modulation (TFiLM) introduced in [24]. TFiLM captures long-range input dependencies in sequential inputs by\ncombining elements of convolutional and recurrent approaches in a U-net-like architecture. Furthermore, it modulates\nthe activations of a convolutional model using long-range information captured by a recurrent neural network. A\nblock-online variant of the temporal feature-wise linear modulation (TFiLM) model to achieve bandwidth extension\nwas proposed by [25]. This architecture simplifies the UNet backbone of the TFiLM to reduce inference time and\nemploys an efficient transformer at the bottleneck to alleviate performance degradation. It also utilizes self-supervised\npretraining and data augmentation to enhance the quality of bandwidth-extended signals and reduce sensitivity with\nrespect to downsampling methods.\n\nAttention-based Feature-Wise Linear Modulation (AFiLM) [26] proposed a network with a U-net-like architecture for\naudio super-resolution that combines convolution and self-attention. AFiLM uses a self-attention mechanism instead of\nrecurrent neural networks to modulate the activations of the convolutional model.\n\nIn [27], to to model the distribution of the target high-resolution signal conditioned on the log-scale mel-spectrogram\nof the low-resolution signal researchers utilized the WaveNet [28] model. Furthermore, [29] studied the use of the\nWave-U-Net architecture for speech enhancement."}, {"title": "3 Methodology", "content": "Given a dataset D = {(x1, x1), (x2, x2), ..., (XM, \u00c2M)}, where each pair consists of the same speech signal sampled\nat different frequencies, we define \u00eem as the signal sampled at a lower frequency Flow with bandwidth Blow, and Xm\nas the signal sampled at a higher frequency Fhigh with bandwidth Bhigh. Both signals originate from the same source.\nMathematically, Fhigh = s \u00d7 Flow and Bhigh = s \u00d7 Blow, where s is the super-resolution/upsampling ratio. Formally,\nwe represent xm as Xm = {x1, x2,..., XN} and \u00c2m as \u00c2m = {1,2,..., [\\frac{N}{s}] }.\n\nOur goal is to expand the bandwidth of the given signal \u00ee by a factor of s, resulting in a speech signal with an increased\nsampling frequency and bandwidth. The objective of neural super-resolution is to construct a function Fe such that\nx = Fe(x), where \u00e9 is the upsampled or reconstructed version of the input speech signal \u00ee with bandwidth Bhigh. In\nsuch a way, \u00e9 is as close as possible to the ground truth one x. The aim is to develop an effective yet efficient function\nFo. A straightforward approach involves designing Fe as a simple DNN [36] or CNN [37] that minimizes the following\nobjective function:\n\nmin$\\limits_{\\theta}$ $\\sum\\limits_{m=1}^{M}$ $\\sum\\limits_{n=1}^{N}$ $(F_\\theta(\\hat{x}_{m})_n \u2013 X_{m,n})^2$\n\nIf the network is not properly designed, several issues may arise. Speech signals tend to be lengthy, making it\ncomputationally inefficient to train a complex model. Additionally, distance-based objective functions have inherent\nlimitations. Training neural networks with such objectives often results in blurry outputs in computer vision, known as\nthe \"softness\" issue [38, 39], and similarly in speech. Therefore, our objective is to build an efficient Fe and use an\nappropriate objective function to train Fe on the dataset, ensuring the reconstructed speech signal closely matches the\nhigh-resolution signal. To achieve this, we employed an adversarial loss to eliminate the softness issue and utilized a\nconvolutional model architecture, which will be elaborated upon in the following sections."}, {"title": "3.1 Model", "content": "In the realm of speech audio, where signals exhibit sinusoidal characteristics with varying periods, accurately represent-\ning these periodic patterns is paramount for synthesizing authentic, high-fidelity speech from an original, low-fidelity\nsource. Building upon the methodology in [40], our model comprises a generator and two distinct discriminator\ntypes: multi-scale and multi-period discriminators. These elements undergo adversarial training, supplemented by two\nauxiliary loss functions to boost training stability. The multi-period discriminator comprises several sub-discriminators,\neach targeting specific periodic segments within the raw waveforms. Complementing this, the generator module\nincorporates multiple residual blocks, each adept at processing patterns of different lengths concurrently. This design\nensures the model comprehensively captures the diverse range of periodic characteristics present in speech audio. The\nentire model architecture is illustrated in Figure 1, and detailed explanations of each subcomponent follow in the\nsubsequent sections."}, {"title": "3.1.1 Generator", "content": "The generator is a convolutional neural network that's entirely dedicated to upscaling. It begins with a mel-spectrogram\ninput generated within the input speech signal's preprocessor module, and uses transposed convolutions to gradually\nmatch the temporal resolution of the high-resolution speech signal xm. Each transposed convolution operation is\naccompanied by a multi-receptive field fusion (MRF) module. This module is designed to analyze patterns of different\nlengths simultaneously. It achieves this by summing the outputs from several residual blocks. These blocks employ\nvarious kernel sizes and dilation rates to create a wide range of receptive field patterns. The architecture of the generator\nis identical to the generator proposed in [40]."}, {"title": "3.1.2 Discriminator", "content": "Following the methodology presented in [40], the design of the discriminator tackles two critical challenges. Firstly, it\nmust adeptly capture the extended dependencies within the input speech signal. Secondly, given the sinusoidal nature of\nthe input with variable periods, it needs to discern the diverse periodic patterns inherent in the audio data. Thus, we\nadopt a similar strategy as detailed in the original work [40]. This involves implementing a multi-period discriminator\n(MPD) composed of multiple sub-discriminators, each responsible for analyzing a segment of the periodic signals\nwithin the input audio. Additionally, a multi-scale discriminator (MSD) is employed to detect sequential patterns and\nlong-term dependencies effectively."}, {"title": "3.2 Training Loss", "content": "For the generator and discriminator, the training objectives follow the approach proposed in [41], where the binary\ncross-entropy loss function is replaced with the least square error loss. This substitution helps prevent the vanishing\ngradient issue. Additionally, an extra objective function is included, specifically mel-spectrogram reconstruction as\nproposed by [42], along with discriminator feature-wise matching across layers as in [43]. The discriminator is trained\nto classify ground truth samples as 1 and samples synthesized by the generator as 0. The generator, on the other hand, is\ntrained to deceive the discriminator by enhancing the sample quality so that it is classified as close to 1 as possible.\nThe objective function for the discriminator D shown in the Equation 2 and for the generator Ge is expressed in the\nEquation 3."}, {"title": "4 Experiments", "content": "Similar to prior studies, we utilized the VCTK corpus [44] for both training and evaluation purposes. The VCTK dataset\ncomprises approximately 44 hours of audio recordings, from 109 distinct speakers, offering a variety of voices and\naccents including Scottish, Indian, and Irish, among others. The dataset originally uses a bit width of 16-bit PCM with a\nsample rate of 48 kHz. Therefore, we first resampled the entire dataset to a sample rate of 16 kHz while maintaining the\nsame bit width. To create low-resolution audio, we applied an order 8 Chebyshev type I low-pass filter to the original\n16 KHz signals, followed by subsampling according to the desired upsampling ratio.\n\nOur research centers on a multi-speaker task, training on the first 100 VCTK speakers and testing on the remaining\nspeakers \u00b9, in alignment with the methodology presented in [23]."}, {"title": "4.2 Evaluation Metric", "content": "To evaluate the performance of our proposed method and to measure the quality of generated audio samples by\ncomparing them to the actual high-resolution audio, and following previous works, we assess the reconstruction quality\nof individual frequencies using the Log Spectral Distance (LSD). The LSD is calculated as shown in Equation 7."}, {"title": "4.3 Experimental Setup", "content": "For all the experiments, we used different upsampling ratios s, specifically 2, 4, and 8. For each upsampling ratio, we\ntrained a separate model, each for 500K steps. Our models were trained on a single machine with two NVIDIA 3080 TI\nGPUs, using a global batch size of 16 and the same model configuration as version 1 described in [40].\n\nSince the preprocessing handles the upsampling, we also trained a unified model on all the aforementioned upsampling\nratios. Additionally, we investigated the ability of the proposed technique to work in zero-shot settings, where the\nmodel is given a signal and tasked with generating a signal with a different upsampling ratio than those it was trained\non. We examined whether the model could generalize and predict the signal. The unified model was trained similarly to\nthe single upsampling ratio models, but for longer, specifically for 1.5M steps.\n\nAll the models were trained on mel-spectrogram input, calculated using 80 mel-filter bank with an FFT size of 1024,\na window size of 1024, and a hop length of 256. For the generator loss terms, we set Aadv = 1.1, Amel = 50, and\nAfeat = 2. In addition to that, the AdamW optimizer was used with \u03b2\u2081 = 0.8 and B2 = 0.999, with a weight decay\nx = 0.01, and the learning rate used on each epoch calculated using the equation 9, where lrinit is the initial learning\nrate set to 1.5 \u00d7 10\u22124, epoch is the current training epoch, and y is the learning rate decaying factor set to 0.999."}, {"title": "5 Results", "content": "The results of our experiments are presented in Table 1, which displays the Log Spectral Distance (in dB) for various\nupsampling ratios, specifically 2, 4, and 8. The table compares the performance of different baselines with two scenarios:\nfirst, when each upsampling ratio is trained on a standalone model, labeled as \"Single,\" and second, when all upsampling\nratios are trained jointly using a single model, labeled as \"Unified.\" Our model consistently outperforms all end-to-end\nbaselines. When compared to cascaded models such as NVSR [34], our approach performs better for high upsampling\nratio (i.e., s = 8) and is comparable for s = 4, although cascaded models outperform our model for low upsampling\nratio (i.e., s = 2)."}, {"title": "6 Conclusion", "content": "In this work, we presented an end-to-end approach for tackling speech bandwidth expansion using a high-fidelity\ngenerative adversarial network trained across different super-resolution/upsampling ratios. Empirically, our method\nsurpasses various end-to-end baselines and achieves comparable results when compared with cascaded approaches.\nAdditionally, we demonstrated the scalability of our method to unseen upsampling ratios during training in zeros-shot\nsetting. These findings underscore the potential of our unified model architecture, which not only simplifies the training\nand deployment process but also enhances performance across varying levels of upsampling ratios. Moving forward,\nour work opens avenues for further exploration and application of neural speech bandwidth expansion in real-world\nscenarios."}]}