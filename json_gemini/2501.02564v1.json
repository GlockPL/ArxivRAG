{"title": "Balanced Multi-view Clustering", "authors": ["Zhenglai Li", "Jun Wang", "Chang Tang", "Xinzhong Zhu", "Wei Zhang", "Xinwang Liu"], "abstract": "Multi-view clustering (MvC) aims to integrate information from different views to enhance the capability of the model in capturing the underlying data structures. The widely used joint training paradigm in MvC is potentially not fully leverage the multi-view information, since the imbalanced and under-optimized view-specific features caused by the uniform learning objective for all views. For instance, particular views with more discriminative information could dominate the learning process in the joint training paradigm, leading to other views being under-optimized. To alleviate this issue, we first analyze the imbalanced phenomenon in the joint-training paradigm of multi-view clustering from the perspective of gradient descent for each view-specific feature extractor. Then, we propose a novel balanced multi-view clustering (BMvC) method, which introduces a view-specific contrastive regularization (VCR) to modulate the optimization of each view. Concretely, VCR preserves the sample similarities captured from the joint features and view-specific ones into the clustering distributions corresponding to view-specific features to enhance the learning process of view-specific feature extractors. Additionally, a theoretical analysis is provided to illustrate that VCR adaptively modulates the magnitudes of gradients for updating the parameters of view-specific feature extractors to achieve a balanced multi-view learning procedure. In such a manner, BMvC achieves a better trade-off between the exploitation of view-specific patterns and the exploration of view-invariance patterns to fully learn the multi-view information for the clustering task. Finally, a set of experiments are conducted to verify the superiority of the proposed method compared with state-of-the-art approaches both on eight benchmark MvC datasets and two spatially resolved transcriptomics datasets. The demo code of this work is publicly available at https://github.com/guanyuezhen/BMvC.", "sections": [{"title": "1 INTRODUCTION", "content": "In real-world applications, data are often collected from multiple sensors or depicted by diverse feature descriptors. For example, the information captured from different sensors (e.g., cameras, LiDARs, Radars) is fused to achieve comprehensive scene understanding for autonomous vehicles [1]. The text, visual, and audio are jointly utilized in the video process [2]. The traditional shallow features descriptors, e.g., histogram of oriented gradients (HOG) [3], and deep features descriptors, e.g., transformer [4], are both leveraged to depict the objects in some computer vision tasks [5]. Accordingly, multi-view learning methods, which explore the complementary information from diverse views to improve the model performance, have witnessed great progress in these years [6], [7], [8].\nClustering algorithms aim to group the data points into their respective clusters without labels. As a useful unsupervised learning tool to reveal the underlying semantic structure of data for capturing meaningful information, clustering algorithms have been widely studied and employed in various fields, such as social network [9], image segmentation [10], and single-cell RNA-seq data clustering [11]. As one of the kinds of multi-view learning approaches, multi-view clustering expects to learn a shared and comprehensive clustering presentation by utilizing the complementary information from diverse views, to boost the clustering performance, which has received much attention recently [12].\nExisting multi-view clustering approaches usually employ a joint training paradigm to optimize the clustering models as shown in Fig. 1 (a), which integrates the view-specific features into a shared representation under a uniform joint training objective, i.e., reconstructing the features (Rec Loss) or clustering (Clu Loss) by the shared representation, to extract the comprehensive information for the clustering task. For example, Xie et al. [13] designed a joint learning framework, which learns diverse feature representations for images via different neural networks and fuses the information to obtain the clustering results. The work in [14], designs a framework to investigate the effects of self-supervision and contrastive alignment in deep multi-view clustering. In recent studies, some researchers claimed that the clustering performance of the multi-view model may degrade when the number of views increases [15], [16]. The main reason is that the particular views with noise information could be not only useless but also even detrimental for the clustering task [15]. This discrepancy makes it challenging for the multi-view clustering model to effectively learn from all views simultaneously under a uniform joint training objective. To alleviate this issue, previous methods usually select some informative views for clustering [16], [17], but it also leads to insufficient utilization of the complementary information among all views.\nBeyond the clustering performance degradation caused by diverse view qualities, we observe that even when the multi-view clustering models perform better than their single-view counterparts, they still fail to fully explore the potential of multiple views. As shown in Fig. 1, a set of experiments are conducted on the nuswide dataset [18] to evaluate the performance of diverse multi-view clustering settings. From the results, we note that the joint training multi-view clustering models outperform than single-view ones, but the clustering performance of view-specific features within the jointly-trained multi-view clustering models performs similarly even worse than the performance of view-specific features captured from solely trained single-view clustering models. For example, in Fig. 1 (c), 1-th view features in the jointly-trained multi-view clustering model exists a clear clustering performance drop, compared with that in the single-view model. Such observations indicate that the view-specific feature extractors are under-optimized with an imbalanced degree in the joint training multi-view clustering model. The main reason is that the multi-view datasets often contain some views with more discriminative information, which tend to be favored and dominant in the training procedure, consequently suppressing the learning process of other views. Such view preference within the datasets causes the observed imbalanced multi-view clustering issue among different views.\nAlthough many solid MVC methods have been proposed [6], [12], the efforts to tackle the imbalance multi-view clustering issue are still limited. To this end, we propose BMvC (short for balanced multi-view clustering) as illustrated in Fig. 1 (b), a novel solution that introduces a view-specific contrastive regularization (VCR) to modulate the optimization of each view. Concretely, VCR constrains the clustering distributions of view-specific features to preserve the sample similarities captured from the joint features and view-specific ones. As a result, the VCR provides an additional gradient for updating view-specific feature extractors so that a better trade-off between the exploitation of view-specific patterns and exploration of view-invariance patterns can be achieved to fully learn the multi-view information for the clustering task. As shown in Fig. 1 (c), the clustering performance of view-specific features extracted by our proposed method outperforms better than that of joint training and single-view training models. Finally, the proposed method achieves better results compared with the joint training model as given in Fig. 1 (d). The contributions are summarized as follows,\n\u2022 We observe and analyze the imbalanced learning phenomenon in multi-view clustering from the gradient descent perspective that view-specific encoders in joint training paradigm are imbalanced under-optimized and certain views could be worse optimized than others in the training procedure.\n\u2022 We propose BMvC to achieve a better trade-off between the exploitation of view-specific patterns and the exploration of view-invariance patterns to fully learn the multi-view information for the clustering task.\n\u2022 We introduce VCR to exploit the sample similarities captured from the joint features and view-specific ones to regularize the clustering distributions of view-specific features being structural so that the view-specific encoders are learned balanced. A theoretical analysis is formulated to demonstrate that VCR adaptively modulates the gradients for updating the parameters of view-specific encoders to achieve a balanced learning processing.\n\u2022 We perform extensive experiments on eight benchmark multi-view datasets to verify the effectiveness of the proposed method. Additionally, our BMvC method is extended to apply to the spatially resolved transcriptomics data clustering task to demonstrate the efficacy of the proposed method.\nThe rest of this paper is organized as follows. Section 2 gives a brief review of the most related work. In section 3, we present the details of the proposed BMvC method. Section 4 provides a series of experimental results and discussions. In Section 5, we provide a conclusion of this paper."}, {"title": "2 RELATED WORK", "content": "Multi-view Clustering, which aggregates comprehensive information from various views, has attracted considerable attention due to the increasing amount of multi-view data. Current methods can be classified into three primary categories based on their approach to generating clustering results: matrix factorization-based methods [19], [20], [21], graph-based methods [22], [23], [24], [25], [26], [27], and deep learning-based methods [13], [17], [28], [29].\nMatrix factorization-based methods aim to derive a shared feature representation from multiple views through matrix factorization techniques. For instance, Liu et al. [30] developed a shared factorization that provides compatible solutions across diverse views to achieve clustering outcomes. Building upon the deep matrix factorization methods [31], Zhao et al. [21] hierarchically decomposed the multi-view representations into a shared latent feature space, progressively learning complementary information from the various views. They employed the Laplacian regularization to maintain the locality properties of the data within this latent space. Yang et al. [20] introduced a tri-factorization-based non-negative matrix factorization approach to decompose multi-view data into a uniform distribution, thereby enhancing the separability of the learned consensus representation. To address the challenges posed by the incomplete multi-view data, Wen et al. [32] implemented an adaptive feature weighting constraint aimed at mitigating the effects of redundant and noisy features during the matrix factorization process. Additionally, they designed a graph-embedded consensus representation learning term to preserve the structural information inherent in incomplete multi-view data.\nGraphs are widely used data structures for representing the relationships among different samples. Graph-based multi-view clustering methods integrate graph similarities learned from various views, ultimately deriving clustering results through spectral clustering techniques. Self-representation [33] and adaptive neighbor graph learning [34] are two prevalent approaches for constructing high-quality similarity graphs in prior works. For instance, Zhang et al. [22] jointly learned both latent and subspace representations, employing neural networks to enhance the latent representation learning process and improve model generalization. Nie et al. [35] extended adaptive neighbor graph learning to a multi-view setting and captured a Laplacian rank-constrained consensus graph from multiple views for the clustering purpose. Tang et al. [36] unified k-means and spectral clustering to leverage information from both graphs and embedding matrices, thereby enhancing clustering performance. In [37], a joint learning framework that simultaneously learns the graph embedding matrix and clustering indicators, is developed to effectively unify the spectral embedding and spectral rotation manners. Additionally, they further employed an anchor graph to reduce the computational complexity of the model.\nDeep learning-based methods leverage the powerful representation capabilities of deep neural networks to derive consensus clustering results from multi-view data. Wang et al. [29] utilized a consistent generative adversarial network to capture a shared representation from incomplete multi-view data for clustering. Xie et al. [13] developed a joint learning framework that employs different neural networks to extract features for comprehensively depicting each image. This framework enables simultaneous feature embedding, multi-view information fusion, and data clustering to achieve joint training. Xu et al. [28] introduced a collaborative training scheme to capture complementary information from diverse views, facilitating the joint learning of feature representations and cluster assignments. In recent years, self-supervised learning with contrastive loss has made significant strides and received rapid developments across various fields [38], [39]. Drawing inspiration from the robust feature learning capabilities of self-supervised learning, contrastive loss has been extensively applied in multi-view clustering. For example, Xu et al [40] aligned multi-view information from both high-level semantics and low-level features through contrastive learning, effectively capturing the common semantics for clustering. Lin et al. [41] integrated feature representation learning and missing sample recovery into a unified framework, using contrastive learning to capture informative and consistent representations from different views. Additionally, the work in [14] designed a framework to explore the effectiveness of self-supervision and contrastive alignment in the multi-view clustering task.\nWhile previous multi-view clustering methods have made significant strides in improving clustering performance from diverse perspectives, the issue of imbalanced learning in multi-view clustering has been rarely studied. To this end, we propose a balanced multi-view clustering method aimed at enhancing cooperation among views."}, {"title": "2.2 Imbalanced Learning", "content": "Multi-view and multi-modal learning are two closely intertwined concepts. In the context of multi-modal fusion, studies have shown that joint-training supervised multi-modal learning often encounters modality competition [42], [43]. This issue arises when different modalities are optimized synchronously under a unified objective. During training, modalities with more discriminative information tend to dominate the learning process, converging more quickly than others. As a result, the other modalities struggle to update their learning parameters fully. Consequently, the under-optimized uni-modal feature extractors prevent the full exploitation of modality-specific information, generating a bottleneck in boosting the performance of multimodal learning.\nRecently, several methods have emerged to address this issue [44], [45], [46], [47]. For instance, Du et al. [44] employed a knowledge distillation scheme for uni-modal distillation to enhance multimodal model performance. The works in [45], [46] utilize the adaptive gradient modulation to suppress the learning process of the dominant modality and facilitate the training of other modalities to achieve more balanced multi-modal learning. Fan et al. [47] leveraged a prototypical cross-entropy loss to accelerate the learning of weaker modalities. Additionally, the study in [48] introduced a multi-loss objective to dynamically adjust the learning process for each modality, refining the balancing mechanism. However, these previous methods primarily address the imbalanced learning problem in supervised tasks, making them infeasible to be directly applied for the unsupervised multi-view learning task."}, {"title": "3 THE PROPOSED METHOD", "content": "Problem Definition: For convenience, the multi-view data is presented as $\\{X^r \\in \\mathbb{R}^{N \\times D^r}\\}_{r=1}^M$, where N and M are the number of samples and views. $D^r$ is the feature dimensions of r-th view. The goal of multi-view clustering is to formulate a projection to capture the underlying data structures, i.e., $\\{X^r \\in \\mathbb{R}^{N \\times D^r}\\}_{r=1}^M \\rightarrow Y$, where Y denotes the clustering results."}, {"title": "3.1 Imbalanced Multi-view Clustering", "content": "In this part, we analyze the imbalanced phenomenon in the joint training paradigm of previous multi-view clustering methods. The view-specific encoder $E^r(\\eta^r, X^r)$ is utilized to extract r-th view-specific latent features $Z^r$ from $X^r$ as,\n$Z^r = E^r(\\eta^r, X^r), r = 1, 2, ..., M,$\\"}, {"title": "3.2 Balanced Multi-view Clustering", "content": "Our proposed balanced multi-view clustering (BMvC) method follows the same network architecture as the joint training paradigm approaches depicted in Section 3.1. As shown in Fig. 2, the multi-view data $\\{X^r \\in \\mathbb{R}^{N \\times D^r}\\}_{r=1}^M$ are processed by their view-specific encoders $\\{E^r(\\eta^r, X^r)\\}_{r=1}^M$ to extract the view-specific latent features $\\{Z^r\\}_{r=1}^M$. Then, the fusion module $F = \\sum_{r=1}^M \\delta_r \\cdot Z^r$ is introduced to aggregate the multi-view representations into a joint one, which is utilized to reconstruct the multi-view data by view-specific decoders $\\{D^r(\\theta^r, F)\\}_{r=1}^M$. To boost the exploitation of view-specific patterns and the exploration of view-invariance patterns, we introduce a view-specific contrastive regularization (VCR), which aims to maximize the similarities of view-specific clustering indicators within their neighborhood ones based on the similarities extracted from the view-specific features and the aggregated joint ones. In such a paradigm, the feature extraction capabilities of the view-specific encoders are enhanced since the VCR introduces an extra gradient to optimize the parameters of view-specific encoders, which is analyzed in Section 3.2.2. Thus, the total objective function of the proposed method is,"}, {"title": "3.2.1 View-specific Contrastive Regularization", "content": "Preserving the similarity from the original feature space is a useful scheme to enhance the representation capability of the latent features [49], [50], [51]. Inspired by this, we tend to constrain the view-specific feature representation with structural information to boost their representation capabilities. To this end, we formulate three key components, i.e., the clustering projection layer, similarity graph construction, and contrastive loss."}, {"title": "3.2.2 The Resultant Gradient Analysis", "content": "During the optimization procedure, gradient from the view-specific contrastive regularization for update the parameters $\\eta^s$ in $E^s(\\eta^s, X^s)$ is represented as,\n\nThus, the parameters $\\eta^s$ in $E^s(\\eta, X^s)$ are updated as,"}, {"title": "Theorem 3.1", "content": "The proposed VCR adaptively modulates the gradients for updating parameters of view-specific encoders, facilitating balanced multi-view clustering.\nProof. Since $\\sum_{j=1}^N q_{ij} = 1, q_{ij} > 0$, the Eq. (12) can be further rewritten into the matrix form as,\n\nwhere $\\circ$ is the Hadamard product, and $A^s$ represents the correlation matrix for s-th view computed from $C^s$. Eq. (15) measures the alignment between matrix $\\hat{G}^s$ and $A^s$. And $\\zeta(.)$ achieves the minimum value when $\\hat{G}^s$ and $A^s$ share similar distributions as well as the clustering structures.\nWe assume that view 1 contains more discriminative information than view 2. Consequently, the fused representation F is expected to offer clearer clustering structures compared to view-specific features $Z^1$ and $Z^2$. As illustrated in Section 3.1, the view with more discriminative information will dominate the learning process. Thus, the representation F will contain a similar clustering structure with $Z^1$, since the clustering knowledge of F mainly comes from $Z^1$ when only employ the feature reconstruction loss. Consequently, the resultant $\\hat{G}^1$ and G both well align with A, resulting a small value for $\\zeta(F, Z^1)$. On the contrary, $\\zeta(F, Z^2)$ should be a large value due to the fewer contributions from $Z^2$ in the learning process of the model.\n$\\zeta(F, Z^1) < \\zeta(F, Z^2)$ suggests that a more discriminative view is better at ensuring that the local similarities captured by the indicators align with the neighborhood structures of both the joint features and the original view specific features. Consequently, view 2 must learn a clustering distribution that closely resembles the joint features to significantly reduce the loss $\\zeta(F, Z^2)$. The clustering distribution of the joint features should also be aligned more closely with that of view 2, further facilitating the reduction of the loss $\\zeta(F, Z^2)$. To implement this process, larger values of $\\frac{\\partial \\zeta(F, Z^2)}{\\partial Z^2}$ and $\\frac{\\partial F}{\\partial Z^2}$ should be achieved, while smaller values of $\\frac{\\partial \\zeta(F, Z^1)}{\\partial Z^1}$ and $\\frac{\\partial F}{\\partial Z^1}$ should be maintained. As a result, the VCR assigns a larger scale factor to the gradient of the view with less discriminative information and a lower scale factor to the gradient of the view with more discriminative information, thereby enhancing feature learning for its view-specific encoder. Ultimately, under the guidance of VCR, the model will achieve a more balanced multi-view clustering. This completes the proof."}, {"title": "4 EXPERIMENTS", "content": "In our experiments, we evaluate our proposed method on eight widely used multi-view datasets. The details are as follows:"}, {"title": "4.3 Ablation Studies", "content": "In this section, we conduct a series of experiments on the eight multi-view datasets to analyze the effectiveness of diverse components in the proposed method."}, {"title": "4.3.1 Effectiveness of View-specific Contrastive Regularization", "content": "The view-specific contrastive regularization, which leverages the similarities captured from original features and joint features to perform contrastive learning on the view-specific clustering space, ensuring additional gradients for the parameters updating in each view-specific encoder, is the key component of the proposed method. To verify its effectiveness, we formulate three methods here, i.e., Rec, VCR, and Rec + VCR. For the Rec method, the view-specific contrastive regularization is removed from the BMvC method and only the feature reconstruction loss is utilized to optimize the model. VCR method cut out the view-specific decoder parts and just leverages the view-specific contrastive regularization loss to optimize the model. Rec+VCR is set as same as our proposed BMvC methods, in which the feature reconstruction and view-specific contrastive regularization are jointly exploited. The clustering performance in terms of ACC, NMI, ARI, and Fscore on eight multi-view datasets of the above formulated methods is reported in Tab. 2. From the results, we find that 1) The VCR method performs better than Rec in most cases, which indicates the VCR part can obtain more balanced multi-view learning during training, ensuring that the complementary information of multi-view datasets can be more explored. 2) Rec + VCR is consistently superior to Rec and VCR, demonstrating jointly utilizing Rec and VCR is the best choice for multi-view clustering."}, {"title": "4.3.2 Impacts of Different Feature Fusion manners", "content": "Different multi-view feature fusion manners may achieve diverse impacts on the final multi-view clustering model. To study this, we construct three methods, i.e., BMvC-w-Asum, BMvC-w-Wsum, and BMvC-w-Cat, which fuse multi-view features via average feature addition, feature addition with sample-wise weights, and feature concatenation, respectively. As can be seen from the results in Tab. 2, our BMvC method equipped with average feature addition, feature addition with sample-wise weights, and feature concatenation can both obtain considerable clustering performance. This indicates that our proposed BMvC method is less sensitive to different multi-view feature fusion manners."}, {"title": "4.3.3 Parameter Sensitivity Analysis", "content": "Our proposed BMvC method consists of a key balance parameter $\\lambda$ to trade-off the feature reconstruction and view-specific contrastive regularization loss. To study the parameter sensitivity, we give the clustering performance measured by ACC, NMI, ARI, and Fscore on eight multi-view datasets varying with different $\\lambda$ in Fig. 3. From the results, we observe that the clustering performance of the BMvC method slightly fluctuates with the $\\lambda$. Additionally, when the parameter $\\lambda$ is in range [$\\10^0, 10^1, 10^2, 10^3$], our BMvC method can obtain considerable clustering results in most datasets. Therefore, we suggest to set the parameter $\\lambda$ in range [$\\10^0, 10^1, 10^2, 10^3$], when it is applied for some applications."}, {"title": "4.4 Application for Spatially Resolved Transcriptomics Data Clustering", "content": "The spatial arrangement of various cell types is closely linked to the functionality of complex tissues. Recent advancements in spatial transcriptomics (ST) have transformed our capacity to investigate this relationship. By combining transcriptomic data with their respective spatial contexts, researchers can achieve a more profound understanding of the biological roles of these tissues. Furthermore, spatial information aids in mapping intercellular communication, thereby deepening our comprehension of cell interactions and the regulatory mechanisms that influence cell behavior. The fusion of spatial and transcriptomic data provides a more holistic view of complex tissue functions. However, the heterogeneity between gene expression and spot spatial location results in an imbalanced multi-view learning issue in previous spatial transcriptomics clustering methods. For example, the spatial continuity of cellular tissue makes it preferable to learn the information from the spot spatial location rather than the gene expression for clustering. Both the gene expression and spot spatial location information are important for revealing the cellular relationships. Thus, it is necessary to formulate a balanced multi-view learning framework for the spatially resolved transcriptomics data clustering task.\nIn this section, we transfer our proposed BMvC method to deal with the spatially resolved transcriptomics data clustering task, as shown in Fig. 4. Specifically, two graph convolutional neural networks are employed to extract the transcriptomic expression-related representation and spatial distribution-related representation to comprehensively depict each cell. Then, the feature concatenation operator consequently with a linear layer is utilized to fusion such two different types of information. As done in previous methods [70], [71], the ZINB decoder [72], which captures the complex global information of data, is introduced to reconstruct the gene expression. Finally, the view-specific contrastive regularization is leveraged to achieve more balanced multi-view learning in the spatially resolved transcriptomics data clustering task."}, {"title": "4.4.1 Datasets", "content": "We evaluate the proposed method on two spatially resolved transcriptomics data:\nHuman Breast Cancer (HBC) [73]: This dataset features 20 meticulously annotated regions categorized into four main morphological types.\nMouse Brain Anterior Tissue (MBA) [74]: This dataset is labeled with 52 distinct class areas and captured using the 10X Visium device, and shows minimal inter-class variation."}, {"title": "4.4.2 Compared Methods", "content": "To verify the superiority of proposed method, we compare it with twelve benchmark methods: K-means, SCANPY [75], SpaGCN [76], DeepST [77], SCGDL [78], GraphST [79], Spatial-MGCN [80], stLearn [81], CellPLM [82], stMMR [83], MAFN [70], and stGCL [71]. In our experiments, we set the parameters of all compared methods as suggestions according to their papers to implement their best clustering performance."}, {"title": "4.4.3 Spatial Clustering Performance", "content": "We report the spatial clustering performance in terms of ARI and NMI of all compared methods in Tab. 3. From the results, we note that our BMvC method achieves superior results compared with previous well-designed spatial clustering methods on the two datasets. For example, our BMVC method obtains the best performance in terms of ARI on HBC and MBA datasets, and is better than the second performer by about 1.36 and 2.81 percentages. Previous spatial clustering methods usually follow the joint-training paradigm to perform multi-view learning. As mentioned in Section 3.2.2, the joint-training paradigm poses an imbalanced learning problem, making the view-specific encoders under-optimized. Thus, such joint-training paradigm based methods have limited spatial clustering performance. In contrast, our BMvC method introduces a view-specific contrastive regularization to find a better trade-off between the exploitation of view-specific patterns and the exploration of view-invariance patterns to fully learn the multi-view information for the clustering task."}, {"title": "4.4.4 Visualization Analysis", "content": "The clustering distribution visualization results of all compared methods are presented in Figs. 5 and 6. Compared with other methods, our method provides more compact and region-consistent visualization results, especially in the left bottom corner and left part of the HBC and MBA datasets, respectively. Furthermore, the proposed method obtains clearer spatial boundaries, indicating more accurate cell clustering. Such visualization superiority of the proposed method remarkably demonstrates the effectiveness of the proposed method and the efficiency of the well-designed view-specific contrastive regularization in dealing with the imbalanced multi-view problem."}, {"title": "5 CONCLUSIONS", "content": "In this paper, we proposed a novel balanced multi-view clustering method to achieve more balanced multi-view learning and further improve the clustering performance. We first analyzed the imbalanced multi-view clustering problem existing in the joint-training paradigm from the gradient view. Then, we introduced a view-specific contrastive regularization to make a better trade-off between the exploitation of view-specific patterns and the exploration of view-invariance patterns to fully learn the multi-view information for the clustering task. Additionally, the theoretical analysis was provided to verify the effectiveness of the VCR from the gradient perspective. Finally, Extensive experiments on various benchmark multi-view clustering datasets and spatially resolved transcriptomics datasets were conducted to verify the efficacy of our method."}]}