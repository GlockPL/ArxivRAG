{"title": "AdaScale: Dynamic Context-aware DNN Scaling via Automated Adaptation Loop on Mobile Devices", "authors": ["Yuzhan Wang", "Sicong Liu", "Bin Guo", "Boqi Zhang", "Ke Ma", "Yasan Ding", "Hao Luo", "Yao Li", "Zhiwen Yu"], "abstract": "Deep learning is reshaping mobile applications, with a growing trend of deploying deep neural networks (DNNs) directly to mobile and embedded devices to address real-time performance and privacy. To accommodate local resource limitations, techniques like weight compression, convolution decomposition, and specialized layer architectures have been developed. However, the dynamic and diverse deployment contexts of mobile devices pose significant challenges. Adapting deep models to meet varied device-specific requirements for latency, accuracy, memory, and energy is labor-intensive. Additionally, changing processor states, fluctuating memory availability, and competing processes frequently necessitate model re-compression to preserve user experience. To address these issues, we introduce AdaScale, an elastic inference framework that automates the adaptation of deep models to dynamic contexts. AdaScale leverages a self-evolutionary model to streamline network creation, employs diverse compression operator combinations to reduce the search space and improve outcomes, and integrates a resource availability awareness block and performance profilers to establish an automated adaptation loop. Our experiments demonstrate that AdaScale significantly enhances accuracy by 5.09%, reduces training overhead by 66.89%, speeds up inference latency by 1.51 to 6.2 \u00d7, and lowers energy costs by 4.69 x.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has significantly advanced mobile applications such as automated driving assistance on Roadside cameras [1]\u2013[3], video surveillance on UAV/UAG [4], biometric authentication on smartphones [5], and emotion detection on smartphones [6]-[8]. With demands for near-/real-time performance and privacy, deploying deep models to mobile and embedded devices [9], [10] has become a trend, enabling local data processing without cloud transmission. Various hand-crafted and on-demand deep model compression techniques have been proposed, including weight compression [11], [12], convolution decomposition [13], [14], and special layer architectures [15]-[17], to fit local resource constraints.\nHowever, mobile devices typically operate in dynamic and diverse deployment environments. For instance, a text-based emotion detection app utilizing a BERT model could be installed on millions of smartphones/wearables, spanning from low- (e.g. CPU) to high-end devices (e.g. GPU, NPU) with resource availabilities that vary up to 20x and fluctuate over time. Specifying deep models for each device type and hardware context to satisfy demands on latency, accuracy, memory, or energy cost is labor-intensive [18], [19]. Similarly, multi-modal recognition models in different cockpit systems may face varied and changing computational capacities under strict latency requirements for safety. Resource availability evolves due to processor states, memory availability, and competing processes. For example, loading parameters or activations from L2-cache is 20 \u00d7 faster than from DRAM, but cache-hit-rates change over time due to factors like operator size and cache availability. To maintain a stable user experience, developers often need to re-compress deep models to adapt to dynamic contexts.\nGiven this challenge, a variety of prior efforts for specifying deep models suited to particular environments. These techniques generally fall into two categories: the pre-deployment method [20]\u2013[23], where models are designed and finalized in the cloud before being sent to mobile devices, and the post-deployment method [10], [24]\u2013[26], which allows models to adjust to the dynamic context after being deployed. The latter scheme is often more cost-effective and preferable for mobile devices with diversity and dynamics, offering a novel approach that enables a more suitable model architecture within the target deployment context and enhances user privacy by avoiding the collection of user data. Specifically, methods like AdaSpring [10], AdaptiveNet [24], and LegoDNN [25] exemplify this trend by producing DNNs with adaptable structures to meet the specific demands of edge devices' runtime environments, thus optimizing accuracy within given resource and latency limits. These dynamically adaptive methods have significantly advanced the deployment of deep models on mobile devices. Despite these advances, developers continue to encounter the following challenges:\n\u2022 Lightweight Scalable Model. Existing methods often modify large networks like ResNet and VGG into reusable blocks to build scalable DNNs for various contexts []. However, these models are not inherently suited for mobile devices, leading to a surplus of discarded structures and a vast search space. Using lighter-weight scalable models is crucial to align model complexity with device resource limitations.\n\u2022 Adaptation Frequency. Prior scalable DNNs typically operate on a \"Once for all\" basis [20]. Frequent variant switching can introduce additional operational latency and limit the effective use of computational resources. A suitable trigger strategy for adaptation is necessary.\n\u2022 Runtime Profiler. Scalable methods for deploying DNNs typically focus on DNN-centric metrics, such as computational load and accuracy, and struggle with profiling hardware-dependent performance indicators like latency and energy consumption [27]. This results in a suboptimal evaluation of adaptation strategies. A timely and accurate resource and performance profiler is desired.\nGiven these challenges, we propose AdaScale, a multi-granularity, operator-ensembled, elastic inference framework designed to automate the dynamic adaptation of deep models to varying contexts. AdaScale leverages a pre-trained self-evolutionary model to facilitate this automation, significantly streamlining the creation of network structures by employing diverse compression operator combinations. This not only reduces the search space but also improves outcomes compared to random network generation. AdaScale features a multi-branch early exit structure to enhance dynamic resource adaptation and utilizes a multi-stage training mode that optimizes network performance while reducing training costs, boosting network efficiency and environmental sustainability. Additionally, it integrates blocks for resource availability awareness and performance profiling to precisely initiate adaptations, ensuring that models capitalize on real-time device capabilities and enhance overall adaptability. The main contributions of this work are as follows:\n\u2022 To the best of our knowledge, AdaScale represents the first work that integrates device resource awareness with model performance optimization into a context-adaptive deep neural network inference framework. It utilizes lightweight network blocks and multi-branch strategies to construct a self-evolutionary elastic network that continuously scales and adapts. Moreover, AdaScale incorporates a multi-stage training approach complemented by a multi-branch exit mechanism, significantly enhancing the network's adaptability to real-time changes.\n\u2022 AdaScale proposes an efficient runtime search strategy to effectively address the challenge of runtime resource adaptation. It features a flexible combination of lightweight compression operators, rapid resource sensing, search strategies, and a robust training mechanism that significantly enhances the network's adaptability, responsiveness, and performance during runtime.\n\u2022 Experiments show that the DNNs generated by AdaScale achieve comparable performance to existing handcrafted compression techniques under various user demands. Through extensive cross-platform experimentation involving multiple tasks and various advanced adaptive and lightweight methodologies, AdaScale has demonstrated substantial benefits in deploying DNNs under resource-constrained conditions. It significantly enhances accuracy by 5.09%, reduces training overhead by 66.89%, speeds up inference latency by 1.51 to 6.2 \u00d7, and lowers energy costs by 4.69 \u00d7, all while maintaining less than 4% accuracy loss in resource-deficient environments.\nIn the rest of this paper, we present the system overview in \u00a7 II, and elaborate the design in \u00a7 III and \u00a7 IV. And then we review the related work in \u00a7 IV, evaluate AdaScale in \u00a7 V, and conclude in \u00a7 VII."}, {"title": "II. OVERVIEW", "content": "This section starts with problem analysis and then presents an overview of AdaScale design.\nAs shown in Figure 1, we analyze three deployment methods for DNNs on mobile devices. The first method generates a sub-model in the cloud and deploys it to the device, requiring substantial data uploads and potentially compromising user privacy. The second method is on-device adaptation with a static network configuration, lacking flexibility to adapt to changes in device context. The third, our proposed method, introduces a dynamic deployment model where the DNN evolves in real-time, adapting to the device's changing context and offering a flexible, context-aware solution.\nOur approach focuses on continuously adapting DNN structures based on device context to optimize performance metrics such as latency, energy consumption, and accuracy. We have conducted extensive testing of various deep learning models across different contextual scenarios to evaluate their performance. Additionally, we have analyzed the correlation between DNN operations and device runtime states for more accurate"}, {"title": "A. Problem Study", "content": "As shown in Figure 1, we analyze three deployment methods for DNNs on mobile devices. The first method generates a sub-model in the cloud and deploys it to the device, requiring substantial data uploads and potentially compromising user privacy. The second method is on-device adaptation with a static network configuration, lacking flexibility to adapt to changes in device context. The third, our proposed method, introduces a dynamic deployment model where the DNN evolves in real-time, adapting to the device's changing context and offering a flexible, context-aware solution.\nOur approach focuses on continuously adapting DNN structures based on device context to optimize performance metrics such as latency, energy consumption, and accuracy. We have conducted extensive testing of various deep learning models across different contextual scenarios to evaluate their performance. Additionally, we have analyzed the correlation between DNN operations and device runtime states for more accurate and efficient modeling. This enables our DNNs to operate continuously in mobile environments, meeting stringent requirements for efficiency and effectiveness, thereby presenting a significant advancement in the dynamic deployment of neural networks on mobile devices.\n\u2022 Accuracy is crucial for high-quality task completion with DNNs. Efficient training of networks of varying sizes is essential to uphold performance standards.\n\u2022 Responsiveness requires that DNN needs to dynamically adapt to meet varying task requirements, especially with limited computing resources.\n\u2022 Energy efficiency is critical for the sustainable operation of DNNs on sensor platforms. Continually enhancing energy efficiency is vital to resolving a primary bottleneck in mobile computing.\n\u2022 Dynamic adaptivity requires DNNs to be hardware-aware and capable of dynamically adjusting their architecture and performance in response to different contexts. This adaptability necessitates real-time monitoring of device performance and optimization to balance response time, energy efficiency, and accuracy, thereby meeting the evolving needs of mobile deployments.\n\u2022 Dynamic adaptivity requires DNNs to be hardware-aware and dynamically adjust their architecture and performance based on contextual conditions. This adaptability necessitates real-time monitoring of device performance and optimization to balance response time, energy efficiency, and accuracy, meeting the evolving needs of mobile deployments.\nDespite previous efforts discussed in \u00a7 VI, current solutions have not fully met the demands for effectively deploying DNNs on mobile devices. To overcome these limitations, this paper introduces AdaScale, a resource-aware multi-variant elastic scaling inference framework. AdaScale is designed to optimize the interaction between mobile device capabilities and DNN requirements for continuous deep learning tasks, optimizing both performance and resource utilization."}, {"title": "B. Problem Formulation", "content": "As depicted in Figure 2, AdaScale utilizes a comprehensive strategy to balance deep neural network (DNN) performance by integrating multiple compression techniques. This approach targets the optimization of interrelated performance metrics, ensuring that the most effective model is selected for mobile devices with fluctuating computational resources. We tackle the issue of deploying deep learning models on mobile devices with constrained computational power by establishing a dynamic optimization framework. The primary objective is to reduce the discrepancy between a model's performance demands and a device's processing abilities, while also meeting strict accuracy and resource constraints. The dynamic optimization problem addressed by AdaScale is formulated as:\n$\\min_{P} \\sum_{i} \\sum_{j} (P[i] \u2013 Ca[j])2,$\ns.t. $Acc > Accu, avgP < avgCa$\n(1)\nwhere the model's performance requirements (P) encompass vectors for energy consumption (E), latency (L), accuracy (Acc), and response time (R): P = [E, L, Acc, R]. Device computational capabilities (Ca) include the device's CPU (Ccpu), GPU (Cgpu), and memory (Cmem) resources: Ca = [Ccpu, Cgpu, Cmem]. The optimization aims to minimize the Euclidean distance between P and Cd, ensuring that the model's demands align closely with the device's capabilities.\n\u00a7 III-A1 will provide a detailed introduction to the ensemble of compression operators. \u00a7 III-B will discuss the multi-stage branch pretraining approach. \u00a7 IV-A will explore the dynamic device resource awareness module, while \u00a7 IV-C will focus on the model performance awareness adaptation module. Additional details on runtime adjustments and network elasticity will be elaborated in \u00a7 IV."}, {"title": "C. Challenges", "content": "Deploying DNNs on mobile devices introduces several significant challenges:\nChallenge #1: Reducing redundancy in model spaces. Standard architectures like ResNet [28] and VGG [29] utilize reusable code blocks, which contribute to a large and redundant search space, inefficient for edge computing. Reducing generation costs and the search space demands redefining the model generation subspace, a task complicated by the diversity and complexity of current DNNs.\nChallenge #2: Dynamic adaptation under varying contexts. Traditional DNN methods usually follow a once-for-all strategy, resulting in prolonged adaptation, increased latency, and inefficient resource use under dynamic conditions. Developing networks that rapidly adapt to the variable contexts of mobile devices is essential yet challenging.\nChallenge #3: Managing device performance metrics. While research has optimized DNNs for accuracy, latency, and computational, key metrics like memory, power, and energy consumption often remain unaddressed. Real-time monitoring introduces substantial overheads. Effectively managing these metrics in adaptive operations, especially with the variable performance of mobile devices, is a critical challenge.\nEach of these challenges represents a barrier to efficient DNN deployment on mobile devices, and overcoming them is crucial for advancing mobile computational capabilities."}, {"title": "D. AdaScale Framework", "content": "As shown in Figure 2, AdaScale combines several key components to overcome critical deployment challenges:\n\u2022 Multi-variant scaling network block is a core component of the AdaScale Framework, which enhances model efficiency by ensembling lightweight compression operators. This design reduces the candidate space during training and generation, allowing the network to swiftly adjust to the variable runtime resource contexts of mobile devices. Its multi-stage parameter sharing mechanism ensures effective adaptation without the retraining.\n\u2022 Runtime elastic model adjustment block uses a multi-branch early exit mechanism and performance search strategy, adapting quickly to contexts, maximizing resources, and ensuring optimal deep learning performance.\n\u2022 Resource availability awareness block monitors and evaluates the resources available on mobile devices and the performance of DNNs in real-time. It provides accurate, dynamic assessments and facilitates the scaling of network variants, offering deep insights into both device capabilities and DNN efficiency."}, {"title": "III. COMPRESSION OPERATOR-ENSEMBLED SELF-EVOLUTIONARY NETWORK", "content": "This section details the design of the compression operator ensemble and the multi-branch self-evolutionary network. The ensemble of compression operators is composed of lightweight blocks from various lightweight networks. Concurrently, the multi-branch self-evolutionary network integrates a high-performance backbone network with multiple evolutionary branches, which allows for dynamic adaptation and scalability."}, {"title": "A. Multi-variant Network Architecture Ensemble", "content": "This section details the construction of variant networks, including the integration of compression operators and the establishment of multi-branch networks, to enhance performance in resource-constrained environments.\n1) Compression Operator Ensemble: In contrast to existing methods such as LegoDNN [25] and Adaptivenet [24], which generate vast and expensive search spaces encompassing up to 1.68 \u00d7 106 and 2.57 \u00d7 1017 configurations respectively. To address the problem of large search spaces and high model generation and training costs, we propose a lightweight operator integration method, offering a new approach to model generation. This method uses lightweight compression operators, proven effective in leading architectures. By integrating these operators, we simplify network construction and reduce the complexity and cost during the search and training phases. To maximize efficiency and maintain or enhance performance, our framework systematically categorizes and tailors compression operators to optimize specific aspects of neural network structures and operations.\n\u2022 Structural optimization operator integrates methods like depthwise separable convolutions, compact layer architectures, and optimized kernel allocation to decrease computational costs and simplify the convolutional framework. It enhances resource efficiency and streamlines connections within neural networks [30]\u2013[35].\n\u2022 Automated machine learning and NAS operator dynamically balances aspects such as network width, depth, and resolution. It leverages neural architecture search to automatically identify and configure optimal structures, significantly reducing manual intervention and potential bias in model design [36], [37].\n\u2022 Advanced connectivity operator implements dense connections to bolster information flow and gradient propagation across layers. This enhancement boosts the learning efficiency and robustness of neural networks [38].\n\u2022 Mathematical decomposition techniques employ batch normalization and decomposed convolutions to minimize model size and complexity, enabling deeper compression without sacrificing performance. Singular value decomposition further enhances efficiency by streamlining the network's weight matrix [34], [39].\n2) Multi-branch Establishment: Current dynamic adaptation methods like Adaspring [10] and Adadeep [40] only allow pre/post-deployment modifications and are too slow to unsuitable for real-time applications. Similarly, early exit strategies such as NeuLens [26] and AIMA [41] fail to adapt during runtime. As a result, there is a notable deficiency in mechanisms that can dynamically adjust DNNs in real-time to accommodate fluctuations in device resources.\nTo address the limitations, we introduce a novel multi-branch, self-evolving network architecture designed for real-time adaptability to computational fluctuations. This architecture merges a robust backbone network with variant branches, each employing unique convolutional compression strategies. Our multi-branch design supports varied performance demands and leverages multi-stage training with shared parameters, drastically cutting down the frequency of weight retraining and expediting inference speeds. This adjustment ensures that the network's performance consistently matches the available computational resources.\nThe backbone of our network is built for flexibility, facilitating the seamless interchange of compression operators to scale with varying demands. It features a dynamic path selection mechanism at each layer, optimizing computational efforts according to task specifics. Additionally, an integrated early exit strategy allows for processing termination at intermediate stages based on input complexity and classification confidence, boosting responsiveness and efficiency in mobile environments. Current architectures like Candidate DNNs, Intermediate Classifiers, and Multi-scale Architectures offer the basis for multi-branch early exits [42], [43], capitalizing on the varying complexity of real-world inputs. While effective for basic applications, these architectures falter in complex scenarios typical of intelligent IoT devices, often prioritizing accuracy at the expense of operational speed, which results in unacceptable latency for mobile applications.\nRecognizing the importance of resource for operational continuity in dynamic environments, we propose an early-exit network architecture for resource-constrained settings. This multi-branch approach dynamically adapts using early-exit blocks to terminate processing when outcomes are sufficiently determined, reducing computational load and latency.\nOur early-exit blocks features a 2D convolutional layer that efficiently extracts essential features using a 3x3 convolutional kernel with a stride of 2, significantly reducing data volume. This is followed by adaptive average pooling that further reduces dimensionality to a 1x1 format, cutting down the number of parameters and streamlining the data for classification. To mitigate the risk of overfitting, a Dropout layer is incorporated, succeeded by a sparse fully connected layer that handles classification tasks without excessive resource use.\nTo enhance scalability and manage resources efficiently in multi-branch early-exit networks, we introduce a testing methodology. This method integrates a simplified exit branch at every layer, tailored to adapt to varied terminal resources without the substantial overhead commonly associated with extensive branching. This overhead often impairs performance and enlarges the search space for optimal configurations.\nOur approach incorporates streamlined exit branches, each consisting of pooling and fully connected layers, enabling precise evaluation of performance distribution throughout the DNN. This strategic configuration reduces training expenses, accelerates the testing cycle, and mitigates performance skew typically introduced by downsampling layers during evaluations. Our method efficiently balances branching complexity and resource limitations, offering a scalable solution.\nIn the Establishment of scalable networks with multiple branches, we identified key phenomena referred to as \u201cbranch diversity establishment\u201d, highlighting differences in performance outcomes and enhancements among various branches. This encompasses:\n\u2022 Performance differences among branches: within a multi-branch scaling network structured around backbone architectures like ResNet, significant variations in performance across structurally repeated layers were observed. Deeper layers often exhibit enhanced performance later in the training cycle, whereas shallower layers consistently underperform, leading us to strategically integrate candidate branches after high-performing layers.\n\u2022 Performance improvement differences between neighboring branches: there is a pronounced disparity in performance improvement between adjacent layers. Some layers demonstrate significant accuracy improvements early in training, marking them as crucial, while others show minimal enhancements even after extended training periods and are deemed less critical. Identifying these less critical layers has allowed us to target them for the application of compression operators, optimizing network efficiency while maintaining robust performance, crucial for meeting the dynamic demands of mobile computing.\nThese observations guide the strategic placement and optimization of branches within the network, ensuring efficient operation and adaptation to diverse computing environments."}, {"title": "B. Multi-branch Self-evolutionary Network Pretraining", "content": "Multi-branch self-evolutionary network aim to deliver precise classification across multiple exit branches. Existing methodologies like Once-for-All and AdaSpring, while optimizing for high accuracy at the final classification stage, often overlook the performance at earlier exit points. This oversight not only results in poorer early classifications but also escalates time and resource expenditure.\nInspired by findings from Wei et al. [44], which suggest that networks of similar scale can achieve comparable accuracy, we propose a novel multi-stage training strategy. This approach not only targets enhancing precision at each exit but also focuses on reducing overall training costs. By incrementally adjusting model complexity and refining each branch, our method ensures balanced optimization of the entire network, promoting efficient resource utilization and robust performance across all branches.\nFigure 3 illustrates how the deep neural network (DNN) is structured by segmenting it into distinct parts at designated branch exits, resulting in partitioned networks termed Partnet (pi). Specifically, the network is divided into segments ranging from input ~ exit1, exit1 + 1 ~ exit2, to exitn\u22121 + 1 ~ final, where n denotes the number of partitions, thus forming the complete network: Net = p1 \u222a p2\u222a... \u222a pn\nOur training approach, detailed in Algorithm 1, starts with the training of p1 until its accuracy at exit1 (Acc1) meets predefined criteria, at which point p1 training is halted. Training for p2 leverages both Acc1 and the weights from the first segment (weight1 or w1), taking advantage of the shared core components between p1 and p2 (excluding exit-specific downsampling and fully connected layers). This approach allows the direct reuse of w1, ensuring parameter continuity and exploiting pre-trained weights.\nWe employed two weight updating methods: (a) Fixing w1 to prevent changes, while updates are focused solely on w2 within p2. This parameter freezing technique, novel in its application to training subsequent network stages, is akin to practices used in network pruning and quantization. (b) Updating w1 if Acc1new > Acc1, which allows for rapid progression in training p2 while fine-tuning p1. This method is viable for networks with fewer branches, where updating costs are manageable. The process continues in a similar fashion through p3 and beyond, based on the accumulated accuracies and weights from preceding segments, repeating until all segments are optimally trained.\nWe employ Algorithm 1 to thoroughly train the multi-branch self-evolving network, utilizing predefined branch positions that are determined during the network's branching construction. This training approach ensures each branch achieves precise accuracy, yielding a network that is both elastic and scalable. Such an architecture is particularly beneficial across a variety of mobile device scenarios, accommodating dynamic resource availability and computational demands. Further details on branch selection, compression operator configuration, and dynamic adaptive deployment are discussed in \u00a7 IV-C, where we detail how to optimize network performance tailored to specific operational environments."}, {"title": "IV. RUNTIME ELASTIC MODEL ADJUSTMENT", "content": "This section describes how AdaScale accurately predicts the computational resources of intelligent mobile devices. AdaScale then instructs the scalable network to select the most suitable operators from a pool of pre-trained compression blocks. This mechanism allows the network to dynamically adjust to the fluctuating resource capacities of devices, ensuring efficient, real-time performance customization."}, {"title": "A. Resource Availability Awareness", "content": "Deploying DNNs on mobile devices involves managing latency and adjusting model scales to match device capabilities, as demonstrated by AdaptiveNet [24], AdaSpring [10], and AdaDeep [40]. However, these methods predominantly address latency and model scaling, neglecting other critical computational resources such as memory, energy consumption, and storage, which are essential for effective deep learning deployment on mobile devices. This narrow focus limits their applicability in optimizing the full range of device capabilities.\nTo overcome the limitations of deploying DNNs on mobile devices, we propose a method for resource management, i.e. resource awareness module. The resource awareness module continuously monitors device resources and generates a device load index. Leveraging an autoregressive model, it forecasts short-term future loads, enabling proactive adaptation to the stringent computational, storage, and energy constraints characteristic of dynamic mobile device environments.\nOur method involves real-time monitoring of critical shared resources, including memory, CPU, and GPU. We focus on collecting data on utilization rates and operating frequencies. To enhance computational load analysis, we introduce a device load index, which integrates usage levels of the CPU, GPU, and memory resources. This index considers the usage levels of the CPU, GPU, and memory resources. The formula for the load index is defined as follows:\n$Iload = (Wcpu \u00d7 Ucpu + Wgpu \u00d7 Ugpu + WM \u00d7UM)/W,$ (2)\nwhere Iload represents the load index, Ucpu, Ugpu, and UM denote the utilization of the CPU, GPU, and memory, respectively. The coefficients Wcpu, Wgpu, and WM are the respective weights assigned to the CPU, GPU, and memory, with W being the sum of these weights. The monitoring of the CPU, for example, involves calculating the difference between system state snapshots taken from the /proc/stat file at two distinct times (prev and current) to measure CPU usage."}, {"title": "B. Model Performance Profiler", "content": "Evaluating DNNs requires a comprehensive analysis of performance metrics. These metrics can be divided into two categories: intrinsic performance and predictive performance.\n\u2022 Intrinsic performance includes metrics related to the model's structure, such as computational complexity, parameter amount, and storage. These metrics are crucial for assessing the model's efficiency and are typically evaluated early in development, independent of the specific deployment device or training outcomes.\n\u2022 Predictive performance encompasses hardware-related metrics, focusing on latency and energy consumption. These metrics are vital for understanding how the model performs on mobile devices with varying resources.\nA rapid and precise assessment of these performance metrics is essential for adapting DNNs to the dynamic computing environments of mobile devices. This early evaluation ensures that models are optimized for the constraints and capabilities of mobile platforms, leading to more efficient and effective deployment. This section describes how to evaluate these performance metrics in detail.\nEvaluate the intrinsic performance metrics of deep learning models through the following aspects: computational cost (C), parameter amount (P), and storage (S). Computational Cost measures the total operations needed during training or inference, typically quantified by floating-point operations across convolutional, pooling, and fully connected layers. Parameter Amount assesses the cumulative learnable parameters, including weights and biases, layer by layer, indicating model complexity. Storage quantifies the required space to save a model by summing the sizes of all parameters, adjusted for their data types and configurations, often stored in formats such as .pth, .pt, or .onnx. These metrics collectively provide essential insights into a model's resource demands, crucial for performance optimization in mobile and embedded systems.\nEnergy Consumption (E): this metric quantifies the electrical energy utilized by a model during its operation, integral for assessing predictive performance aspects like latency and energy consumption. It is crucial in mobile computing where both model architecture and device hardware impact energy demands. We calculate the energy consumption for a DNN using the following formula:\n$E = ((\u041c\u0441\u0440\u0438 \u00d7 Pcpu \u00d7 \\epsilon)\u00f7fcpu) + ((\u041c\u0434\u0440\u0438 \u00d7 \u0420\u0434\u0440\u0438)\u00f7fgpu) + \u041c\u0442\u0435\u0442 \u00d7 Pmem \u00d7 (1 \u2212 \\epsilon) \u00f7 fmem,$\n(3)\nwhere Mcpu, Mgpu, and Mmem represent the memory operations for the CPU, GPU, and general memory, respectively. \\epsilon denotes cache hits. Pcpu, Pgpu, and Pmem are the power consumption rates for the CPU, GPU, and memory, respectively. Finally, fcpu, \u0192gpu, and fmem reflect the clock frequencies of the CPU, GPU, and memory, indicating the operations they can perform per second.\nOur model considers a holistic view of energy requirements by accounting for computation and various memory operations, drawing on extensive empirical data from AdaEnlight [45] to estimate cache hit rates and energy coefficients. This comprehensive approach enables precise predictions and effective optimizations in the context of mobile computing. The memory access (M) and cache hit rate (e) are carefully designed and calculated using the following method:\n\u2022 Memory accesses is across various layers of a model, encompassing input, conv layers, pooling layers, and fc layers. It is quantified by the total number of memory accesses performed during the model's operation, which can be represented by the following formula:\n$M = \\sum_{i=1}^{n} (Ini + Outi + Wi + bi) \u00d7 byte,$ (4)"}, {"title": "C. Performance-Guided Search", "content": "Deploying network architectures with multi-variant models and integrated compression techniques across diverse edge devices poses significant challenges. Traditional adaptive methods, such as those described in NAS [21], LegoDNN [25] and AdaSpring [10], often overlook the unique characteristics of each device, leading to inefficient and time-consuming adaptation processes for edge environments.\nTo address this problem, we introduce a approach that using a subspace search tailored to their specific needs. This device-adaptive phase includes a performance evaluation module (described in IV-B), a resource availability awareness module(described in IV-A) and a model-guided search strategy, significantly reducing the search overhead and enabling rapid adaptation to the dynamic conditions of edge computing.\nThe primary goal of the model-guided search strategy is to effectively minimize the combined performance loss function J, which is defined as:\n$J(m,d) = \\alpha \\cdot L(m, d) + \\beta \\cdot E(m, d),$ (6)\nwhere \u03b1 and \u03b2 are weights balancing latency and energy consumption. We aim to optimize under two constraints: latency L(m, d) < T and energy E(m, d) \u2264 Eb. The objective is to find a model m' that minimizes J(m, d) while adhering to these constraints:\nm' = arg minm\u2208MJ(m,d),\ns.t. L(m, d) \u2264 T, E(m,d) \u2264 Eb\n(7)\nwhich optimizes the model's performance by balancing latency and energy consumption, ensuring that it operates efficiently within device-specific constraints.\nTo identify the optimal model, we have constructed comprehensive tables for all network variants, encompassing both backbone networks and compression operators. Our Performance Index Table catalogues each variant by the number of trainable parameters, storage, and accuracy. Additionally, the Predictive Performance Table documents the latency and energy consumption for each configuration. These tables, derived from the intrinsic performance and accuracy metrics of DNNs"}, {"title": "IV. EVALUATION", "content": "We aim to address the following research questions through experimentation: Q1: Can AdaScale achieve models with better trade-offs between latency and accuracy? Q2: Is AdaScale capable of adapting to the available resources of IoT devices? Q3: Does AdaScale exhibit better performance in dynamic scenarios? We will compare AdaScale with various methods reported in the latest literature to evaluate its effectiveness."}, {"title": "A. Experiment Setup", "content": "We first present the settings for our evaluation.\nSystem Implementation. We implemented our method using Python and C++. The training and deployment parts of the deep learning model are implemented using Python and PyTorch, while the device-side computation resource sensing and real-time monitoring are handled using C++. We have implemented the online component of AdaScale on mobile devices to dynamically adjust the DNN configuration to match the device's computational capabilities. The self-evolving networks generated by AdaScale's offline component (i.e., the backbone network and multiple compression operators) are then loaded into the target platform.\nEvaluation Applications/Datasets. We utilize three commonly used mobile datasets to assess the performance of AdaScale, as shown in Table I, which represents tasks of different difficulty levels. Specifically, we test AdaScale on"}, {"title": "B. Performance Comparison of Dynamic Adaptive Networks", "content": "We evaluate the quality of models generated by our method compared to generative and dynamic adaptive baseline models. Specifically, we assess the search space, training time, and deployment search time on mobile devices for three different generative network approaches: NAS, AdaptiveNet, and LegoDNN. To ensure generalizability and consistency with other studies, we use ResNet as our backbone model, as it is commonly employed in the experimental setups of AdaptiveNet, LegoDNN, and AdaSpring.\nPerformance. (i) As illustrated in Figure 5a, using lightweight network modules as our compression operators significantly reduces our search space compared to state-of-the-art network generation methods [24], [25], [49]. AdaScale's space is reduced to 0.48\u00d7103, which is one percent of that of AdaptiveNet and one thousandth of LegoDNN, yet achieves comparable results. This reduction minimizes the resources required to generate networks. (ii) As shown in Figure 5b, the smaller search space results in the shortest training time for our network. Despite the higher computational power of the servers used by NAS, LegoDNN, and AdaptiveNet, AdaScale demonstrates greater potential. (iii) In our deployment of AdaScale on the TurtleBot robot, as shown in Table II and Figure 5c, the runtime search overhead was only 0.043ms, approximately one percent of the average inference latency. This efficiency is due to our model performance evaluation and device resource sensing modules. The performance table uses a B+ tree data structure, ensuring each query is processed in O(log n) time, where b is the branching factor and n is the total number of elements (0.48\u00d7103). This approach significantly reduces search and evaluation costs.\nSummary: AdaScale dramatically reduces the search space from one thousandth to one percent of more advanced methods. This substantial reduction not only decreases the resources needed for network generation but also significantly shortens training time. In practical applications, AdaScale shows exceptionally low runtime search overhead on the TurtleBot robot, significantly enhancing both efficiency and performance."}, {"title": "C. Comprehensive Performance Comparison", "content": "We evaluate our approach (with a 4-stage network) against lightweight single-networks, selecting Shufflenet, Squeezenet, ResNet18, MobileNetv3 (small and large) and Transformer for comparison. Our baseline training configuration is as follows: batch size of 128, 16 workers, using SGD as the optimizer with a momentum of 0.9 and weight decay of 5e-4. MILESTONES are set at epochs 40, 80, 100, and 120, with an initial learning rate of 0.1. We test these methods and our approach on two datasets, Cifar10 and Cifar100, evaluating accuracy, training memory, training time, storage, parameters, deployment runtime latency, and energy consumption on Jetson Nx.\nPerformance. (i) As shown in Table III, our four-stage segmented training method significantly enhances model performance on the CIFAR-10 dataset. AdaScale's accuracy progressively improves, starting at 85.99% and peaking at 91.17% in the third stage, before stabilizing at 90.82% in the fourth. This progression underscores its effective adaptation and learning capabilities. Notably, AdaScale surpasses the peak accuracy of the Transformer model, which is 90.0%,"}, {"title": "D. Performance Under Dynamic Contexts", "content": "Evaluating AdaScale's ability to balance accuracy and latency in dynamic scenarios, we deployed it on Jetson Nx and Raspberry Pi 4B. We conducted comparative tests against state-of-the-art lightweight models under dynamic resource conditions. To ensure fair evaluation conditions, a performance control program written in C++ managed resource allocation preemptively in these dynamic contexts.\nPerformance. As demonstrated in Figure 6, 7a and 7b, AdaScale achieves the lowest inference latency compared to baseline models in all scenarios, with significant improvements under limited resource conditions. AdaScale's optimization strategies enhance resource utilization and reduce inference latency on Jetson Nx and Raspberry Pi 4B devices compared to other advanced lightweight networks. This is achieved by dynamically adjusting the computational load, leading to substantial performance enhancements. Inference speed increases range from 1.14 to 1.85 \u00d7 on the Jetson Nx and from 1.71 to 6.29 \u00d7 on the Raspberry Pi 4B, with speeds on the Raspberry Pi 4B increasing up to 6.29 \u00d7 faster than ShuffleNet. Additionally, Ada"}]}