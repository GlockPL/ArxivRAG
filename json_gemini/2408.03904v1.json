{"title": "Lightweight Video Denoising Using a Classic Bayesian\nBackbone", "authors": ["Cl\u00e9ment Bled", "Fran\u00e7ois Piti\u00e9"], "abstract": "In recent years, state-of-the-art image and video denoising\nnetworks have become increasingly large, requiring millions of trainable\nparameters to achieve best-in-class performance. Improved denoising\nquality has come at the cost of denoising speed, where modern trans-\nformer networks are far slower to run than smaller denoising networks\nsuch as FastDVDnet and classic Bayesian denoisers such as the Wiener\nfilter.\nIn this paper, we implement a hybrid Wiener filter which leverages\nsmall ancillary networks to increase the original denoiser performance,\nwhile retaining fast denoising speeds. These networks are used to refine\nthe Wiener coring estimate, optimise windowing functions and estimate\nthe unknown noise profile. Using these methods, we outperform several\npopular denoisers and remain within 0.2 dB, on average, of the popular\nVRT transformer. Our method was found to be over x10 faster than the\ntransformer method, with a far lower parameter cost.", "sections": [{"title": "I. INTRODUCTION", "content": "Denoising remains a crucial step in many applications of image\nand video processing, from the smartphone camera ISP pipeline to\ndenoising tools of the post-production industry. More recently, the\nmass adoption of over-the-top streaming services such as Netflix and\nDisney+, as well as social media driven by user-generated content\nsuch as YouTube, Twitch.tv, Instagram and Facebook have placed\ngreater importance on efficient video encoding, where denoising is\nessential in reducing frame entropy and reducing the bandwidth\nnecessary to distribute and receive content.\nClassic denoisers which rely on Bayesian modelling and frequency\nfiltering such as Wiener filters [1]\u2013[4] and Wavelet filters [5]-\n[7], or those which use patch similarity, as in BM3D [8], V-\nBM4D [9] and VNLB [10], have recently been outperformed by\ndeep learning approaches [11]\u2013[19]. In 2019, Maggioni et al. put\nforward DVDNet [12], which outperformed VNLB [10] using a\ntwo-step CNN architecture: a spatial denoising network applied to\nmotion-compensated frames, followed by a temporal denoising step\nwhich consolidates the output of three spatially denoised adjacent\nframes into a single frame. Originally 1.3M parameters in total,\nFastDVDNet [13] increased the network size to 2.5M in total, opting\nfor a U-Net architecture in its denoising blocks and replacing motion\ncompensation with overlapping, multi-frame input blocks.\nInspired by DVDNet, similar networks such as Videnn [14] (3.5M\nparameters) and PaCNet [16] (2.9M parameters) have been proposed.\nMore recently, following the success of image vision transformers\nsuch as SwinIR [20] (Liang et al.) and Restormer [21] (Zamir et\nal.), Liang et al. put forward the Video Restoration Transformer\n[17] (VRT), achieving best-in-class results with a network of 35.6M\nparameters. Unlike image denoisers, the most popular video denoising\nalgorithms (VRT, DVDNet, FastDVDNet, VNLB) are non-blind,\nmeaning the user is required to supply the denoiser with a measure\nof the noise variance.\nWhile transformer networks achieve greater PSNR quality scores,\nthey are slower to run than smaller networks (See Table IV), and their\nincreased parameter count results in high video memory consumption\nwhen running inference on high-resolution images, limiting the\nhardware on which they may be deployed.\nIn recent work from Bled and Piti\u00e9 [22], it was demonstrated\nthat this trend of increasingly larger networks is not a fatality\nand that the original Wiener filter can actually be optimised to\nachieve performances close to popular image denoising DNNs such\nas DnCNN [23].\nIn this paper, we adopt a similar approach for video denoising\nand explore how the Wiener filter could be used as the backbone\nof a state-of-the-art video denoiser architecture. We reconsider all\ntuneable parameters of Bled's Wiener filter, taking special care to\noptimise for denoising speed as temporal data is introduced. We\nintroduce trainable window functions, 4D FFTs and 3D CNNs, as\nwell as an ablation study on the use of motion compensation in\nvideo denoisers. We also modify Bled's blind denoiser to generalise\nto Video denoising.\nOur key contribution is the implementation of a denoiser which\ndemands far fewer parameters (0.29 M) than current denoising net-\nworks, outperforming DVDNet, FastDVDNet, and VNLB, on average\nin terms of PSNR. We outperform all tested networks in SSIM and\nachieve greater performance than the Vision transformer [17] at high\nnoise levels."}, {"title": "II. BACKGROUND", "content": "A. Baseline Video Wiener Filter\nGiven a noisy signal y, composed of the original, unknown signal\nx, and additive noise n, y = x + n; the Wiener filter [24] defines a\nlinear, minimum mean square error (MMSE) optimal filter. Assuming\nthat the image sequence and noise signal are second-order stationary\nand decorrelated, the optimal IIR Wiener filter is given by the\nfollowing transfer function $H(W_1, W_2, \\omega_t)$:\n$H (W_1, W_2, w_t) = \\frac{P_{xx} (W_1, W_2, w_t)}{P_{yy} (W_1, W_2, w_t)},$ (1)\nwhere $P_{yy}$ and $P_{xx}$ are the power spectrum densities at spatial and\ntemporal frequencies $W_1, W_2, w_t$ at frame t for the input signal y and\noriginal signal x. In practice, the PSD of the unknown, clean signal\nis estimated with the following coring function at each frequency\n$(W_1, W_2, w_t)$:\n$P_{xx} = max(P_{yy} \u2013 P_{nn}, 0).$ (2)"}, {"title": "III. ENHANCED WIENER DENOISING FOR VIDEO", "content": "A. A Video Wiener 4D Backbone Network\nIn this work, we propose to extend the idea from Bled et al. to form\na video denoising network based on a Wiener Filter backbone. As we\nintroduce the temporal dimension, we must revisit the optimisation\nmade by Bled, as previous optimal values no longer apply. We also\ntake extra care to optimise for denoising speed.\nWe start from the baseline 3D Wiener video denoising filter\nimplementation by Kokaram and include some of the ideas proposed\nin [22] to form a new method that we will call Wiener 4D.\nAs the name suggests, we first expand the Wiener filter to handle\ncolour as an additional dimension, thus Kokaram's 3D FFT becomes\na 4D FFT, using the RGB channels as the third dimension and a"}, {"title": "B. Video Wiener Coring Refinement Network", "content": "As an alternative to the default Wiener coring function of Eq. (2),\nwe propose, as in [22] a lightweight coring post-processing network\nthat operates on the 4D spectral tensor. This network aims to reduce\npotential ringing artefacts caused by the default coring estimation\nerrors. The network takes in the MSE-optimal Wiener filter transfer\nfunction $H(\\omega_1, \\omega_2, w_t, w_c)$ as computed by Eq. (2) as a 4D tensor"}, {"title": "C. Blind denoising", "content": "As is still typical in denoisers today, the classic Wiener filter\nis a 'nonblind' filter, requiring an estimate of the degraded image\nnoise standard deviation. As denoising networks move towards blind\ndenoising, we also implement a blind Wiener filter, requiring no extra\ninputs from the user.\nA small ancillary 2D CNN, is implemented for this purpose, which\ntakes in the centre target frame of the 5-frame sequence and returns a\nnoise standard deviation map of the same size. The map is repeated\nfor the outer four frames and fed to the Wiener filter. Unlike the\nuser-input noise STD, the predicted noise map is not constrained to\na single value across the 3-channel image. The network is trained\nalongside the coring refinement, with the additional L1 loss between\nthe predicted noise STD map and the ground truth uniform map\nadded. To allow the STD network to be trained unconstrained by\nthe uniform standard deviation loss and to maximise output quality,\na second training stage is run with only the target frame loss. All\nfive noise level datasets are combined to train this network.\nThis ancillary network consists of only four 2D convolution layers\nand three LeakyReLU activations. The network contributes only\n8,280 extra parameters to the coring refinement network, increasing\nits size to 287,595 parameters."}, {"title": "D. Motion Compensation and Multi-Scale Averaging", "content": "Many video denoising networks still implement forms of motion\nestimation to map pixels in non-target frames to their position in\ntarget frames. This spatial alignment is often necessary to increase\nperformance in classic Bayesian filters which rely on temporal\nconsistency but its effect on denoising networks is unclear. To assess\nthe impact of motion compensation in trained networks, we compare\nthe denoising performance of our optimised Wiener filter with, and,\nwithout motion compensation, for both our trained Wiener filter and\nour untrained filter.\nLastly, we examine the performance benefits of a multi-scale\nWiener filter, whereby the image is denoised at multiple block\nsizes, and the outputs are averaged. Capturing multi-scale frequency\ninformation in this manner increases the amount of information\navailable to the denoiser and creates a smoother final image."}, {"title": "IV. EXPERIMENTS/RESULTS", "content": "In this section, we iterate through the optimisations made to the\n4D Wiener filter, measuring quality improvements at each step. We\nevaluate our denoiser using ten, 64-frame sequences, taken from a\ncombination of Derf's Collection [28] (HD, gaming) and the BVI\n(SynTex [29], DVC [30]) datasets. Each clip is centre-cropped to\n500\u00d7500 for evaluation. For training, we choose 173 uncropped, full-\nlength videos from the corpus, omitting the test sequences. Additive\nGaussian noise is applied to the datasets at standard deviations of\n\u03c3 = [10, 20, 30, 40, 50]. At training time, five frames are randomly\nselected from each sequence and cropped into batches of 5\u00d7128\u00d7128.\nA. Optimising Block Overlaps and Block Size\nWe first optimised the 4D filter for the overlap between denoised\ntemporal blocks. We measure the stride of the sliding analysis window\nas a fraction of the block size, 32\u00d732, from 1/2 (Kokaram's standard)\na block width to 1/8 of a block. Our quality measurements are taken\nas the average across the 10 sequence dataset at a noise STD=20.\nIn Table I we observe a +0.17 dB PSNR gain by reducing the\nstride to 1/3 of a block width. Further decreasing the stride provides\nlittle improvement in quality while greatly increasing denoising time;\na quarter stride increases performance by only 0.01 dB and increases\nthe denoising time by 6.7 seconds, as more analysis blocks must be\ndenoised.\nNext, we optimise for window size using the same noise profile,\n\u03c3 = 20. In Fig.2 we plot denoising quality w.r.t block size for both\nPSNR and SSIM. We observe that PSNR peaks at a window size of\n18 (31.88 dB), and decreases as the analysis block size increases to\n126 (31.05 dB). This optimal is + 0.14 dB greater than the previous\nimplementation. SSIM peaks at a window of size 22 (0.8445), and the\nlowest quality (0.8336) is also recorded at the largest window size,\n126. Increasing window size was not found to significantly increase\nor decrease denoising time. While larger windows result in more\nexpensive FFT transforms, fewer windows are required to cover the\nframes.\nB. Optimising DC Offset Removal and Window Shape\nAs mentioned in Section IV-A, it is necessary to zero-mean\nthe temporal block before applying the FFT. For this purpose, we\ncompare classic mean subtraction to median subtraction. To measure\nthe performance lost by taking the noisy mean, we record the output\nquality when the denoiser is provided with the unseen ground truth\nmean. In table II, we show that for all noise levels, using the median\nof the block increases denoising performance. This effect is most\nnoticeable at higher noise standard deviations, where pixels close to\nbrightness boundaries deviate further from their ground truth values.\nAt a noise level of STD=50, we note a + 0.27 dB increase in\nperformance over using the noisy mean.\nNext, we study the impact of window shape on denoising perfor-\nmance. In their original paper, Kokaram used a raised cosine window\nwhich acted as both the analysis and spatial interpolation window for\nthe overlapping blocks. As mentioned in Section IV-A, changing the\nwindow stride means that overlapping blocks no longer sum to one.\nInstead, we use separate analysis and interpolation window pairs, and,\nto ensure the overlapping windows sum to one, a normalising weight\nmap is applied to the reconstructed frame.\nIn addition to the half cosine and Gaussian windows used by\nKokaram and Bled respectively, we introduce two new analysis-\ninterpolation window pairs: trainable Gaussian (non-isotropic)\nwindows and trainable isotropic windows. The trainable Gaussian\nwindows are initialised as normal Gaussian windows and their\nweights are set to be trainable. The trainable isotropic window is\ninitialised as a 1D Gaussian window, with the final window being\ninterpolated onto 2D space. In both cases, the windows are saved\npost-training and added to the filter as fixed weights.\nTraining: The weights are trained using the same loss function\ndescribed in section III-B, with the AdamW optimiser [31] and a\ncosine Annealing learning rate scheduler which reduces the learning\nrate from le-\u00b3 to le-5 every 300 epochs, over 1200 epochs.\nIn table III we show that the original Raised Cosine window is\noutperformed by our trainable Gaussian window by + 0.21 dB,\na small improvement over the non-trained Gaussian window. The\nisotropic window also outperforms the half-cosine window but we\nwere unable to exactly match Gaussian windowing in out training."}, {"title": "C. Coring Refinement Network and Blind Denoising", "content": "We now evaluate the performance of the coring refinement net-\nwork, as described in Section III-B. The network is trained using the\nsame scheme as outlined in the previous section (IV-B), with a 1/3\nblock stride and the learned Gaussian windows. The window sizes\nare set to 16x16 to maximise performance.\nFor our non-blind denoiser, we train five networks separately, each\non a separate Gaussian noise profile: \u03c3 = [10, 20, 30, 40, 50].\nThese denoisers are non-blind and require the user to provide the\ndenoiser with a noise STD. In Table IV we show that WienerNet\nperformance is on average + 3.5 dB (+ 0.1311 SSIM) greater than\nthe optimised baseline Wiener (non-coring refinement network). We\nalso show that WienerNet remains within 0.2 dB, on average, of the\nVRT transformer, outperforming it for noise STDs of \u03c3 = 40 and \u03c3\n= 50, while being over ten times faster on the same hardware.\nAs described in Section III-C, we also train and evaluate a single\nblind denoiser, WienerNet Blind, which requires no noise input,\ninstead generating its own noise map. We show that with no user\nnoise input, this network outperforms our optimised Wiener filter by\n+ 3.1 dB (+ 0.1231 SSIM) on average, remaining within 0.4 dB of\nour non-blind denoiser, with very few extra parameters and almost\nidentical run times. Like the non-blind version, this denoiser also\noutperforms the VRT transformer at high noise levels.\nLastly, we evaluate the blind denoiser using a multi-scale denoising\napproach, by denoising each sequence at block sizes of 16, 32 and 64,\nand averaging the output, as described in Section III-D. This method\nimproves the performance of the blind denoiser at \u03c3 = 10 and \u03c3 =\n20. This suggests an optimal weighted average exists, per noise level,\nwhich outperforms the single-scale approach."}, {"title": "D. Motion Compensation", "content": "Lastly, we evaluate motion compensation as a preprocessing step\nto denoising for both trained and untrained (WienerNetBlind) filters\nusing Deepflow [32] and RAFT [33] optical flow algorithms. This is\nimplemented in the same manner as DVDNet [12].\nIn the untrained case, DeepFlow and RAFT do not improve\ndenoising results in terms of PSNR but improve SSIM at all noise\nlevels except for \u03c3=10. This result may be attributed to occlusions\ncreated in the motion-compensated frames.\nFor the trained case, Deepflow matches the non-motion compen-\nsated network at \u03c3 = 10 and outperforms it at \u03c3 = 20 and \u03c3 = 30.\nHowever, SSIM results do not improve when motion compensation\nis applied to the trained network and no PSNR improvements are\nmade at \u03c3 = 40 and \u03c3 = 50. These results may indicate some\ndenoisers have been trained to handle the occlusions generated by\nmotion compensation algorithms. In our case, more performance may\nbe extracted if we discard or ignore frames which exceed a threshold\nvalue for occluded pixels."}, {"title": "V. CONCLUSIONS", "content": "In our work, we have demonstrated the efficiency of using small\nancillary CNNs to improve the performance of a classic, optimised\nBayesian filter, moving away from the black-box approach of CNN\nand transformer-based denoisers. Our denoiser is smaller in terms of\nparameters than all tested networks, and faster than the most competi-\ntive methods. We have also shown that current motion compensation\nmethods do not always improve denoising performance. This may\nbe optimised in future work, along with further improvements in\ndenoising speed and weighted averaging for multi-scale denoising."}]}