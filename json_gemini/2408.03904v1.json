{"title": "Lightweight Video Denoising Using a Classic Bayesian Backbone", "authors": ["Cl\u00e9ment Bled", "Fran\u00e7ois Piti\u00e9"], "abstract": "In recent years, state-of-the-art image and video denoising networks have become increasingly large, requiring millions of trainable parameters to achieve best-in-class performance. Improved denoising quality has come at the cost of denoising speed, where modern transformer networks are far slower to run than smaller denoising networks such as FastDVDnet and classic Bayesian denoisers such as the Wiener filter.\nIn this paper, we implement a hybrid Wiener filter which leverages small ancillary networks to increase the original denoiser performance, while retaining fast denoising speeds. These networks are used to refine the Wiener coring estimate, optimise windowing functions and estimate the unknown noise profile. Using these methods, we outperform several popular denoisers and remain within 0.2 dB, on average, of the popular VRT transformer. Our method was found to be over x10 faster than the transformer method, with a far lower parameter cost.", "sections": [{"title": "I. INTRODUCTION", "content": "Denoising remains a crucial step in many applications of image and video processing, from the smartphone camera ISP pipeline to denoising tools of the post-production industry. More recently, the mass adoption of over-the-top streaming services such as Netflix and Disney+, as well as social media driven by user-generated content such as YouTube, Twitch.tv, Instagram and Facebook have placed greater importance on efficient video encoding, where denoising is essential in reducing frame entropy and reducing the bandwidth necessary to distribute and receive content.\nClassic denoisers which rely on Bayesian modelling and frequency filtering such as Wiener filters [1]\u2013[4] and Wavelet filters [5]\u2013 [7], or those which use patch similarity, as in BM3D [8], V- BM4D [9] and VNLB [10], have recently been outperformed by deep learning approaches [11]\u2013[19]. In 2019, Maggioni et al. put forward DVDNet [12], which outperformed VNLB [10] using a two-step CNN architecture: a spatial denoising network applied to motion-compensated frames, followed by a temporal denoising step which consolidates the output of three spatially denoised adjacent frames into a single frame. Originally 1.3M parameters in total, FastDVDNet [13] increased the network size to 2.5M in total, opting for a U-Net architecture in its denoising blocks and replacing motion compensation with overlapping, multi-frame input blocks.\nInspired by DVDNet, similar networks such as Videnn [14] (3.5M parameters) and PaCNet [16] (2.9M parameters) have been proposed. More recently, following the success of image vision transformers such as SwinIR [20] (Liang et al.) and Restormer [21] (Zamir et al.), Liang et al. put forward the Video Restoration Transformer [17] (VRT), achieving best-in-class results with a network of 35.6M parameters.\nUnlike image denoisers, the most popular video denoising algorithms (VRT, DVDNet, FastDVDNet, VNLB) are non-blind, meaning the user is required to supply the denoiser with a measure of the noise variance.\nWhile transformer networks achieve greater PSNR quality scores, they are slower to run than smaller networks (See Table IV), and their increased parameter count results in high video memory consumption when running inference on high-resolution images, limiting the hardware on which they may be deployed.\nIn recent work from Bled and Piti\u00e9 [22], it was demonstrated that this trend of increasingly larger networks is not a fatality and that the original Wiener filter can actually be optimised to achieve performances close to popular image denoising DNNs such as DnCNN [23].\nIn this paper, we adopt a similar approach for video denoising and explore how the Wiener filter could be used as the backbone of a state-of-the-art video denoiser architecture. We reconsider all tuneable parameters of Bled's Wiener filter, taking special care to optimise for denoising speed as temporal data is introduced. We introduce trainable window functions, 4D FFTs and 3D CNNs, as well as an ablation study on the use of motion compensation in video denoisers. We also modify Bled's blind denoiser to generalise to Video denoising.\nOur key contribution is the implementation of a denoiser which demands far fewer parameters (0.29 M) than current denoising net- works, outperforming DVDNet, FastDVDNet, and VNLB, on average in terms of PSNR. We outperform all tested networks in SSIM and achieve greater performance than the Vision transformer [17] at high noise levels."}, {"title": "II. BACKGROUND", "content": "A. Baseline Video Wiener Filter\nGiven a noisy signal y, composed of the original, unknown signal x, and additive noise n, y = x + n; the Wiener filter [24] defines a linear, minimum mean square error (MMSE) optimal filter. Assuming that the image sequence and noise signal are second-order stationary and decorrelated, the optimal IIR Wiener filter is given by the following transfer function H(W1, W2, \u03c9t):\n$$\u0397 (W1, W2, wt) = \\frac{Pxx (W1, W2, Wt)}{Pyy (W1, W2, wt)},$$", "equations": ["$$\u0397 (W1, W2, wt) = \\frac{Pxx (W1, W2, Wt)}{Pyy (W1, W2, wt)},$$"]}, {"title": "III. ENHANCED WIENER DENOISING FOR VIDEO", "content": "A. A Video Wiener 4D Backbone Network\nIn this work, we propose to extend the idea from Bled et al. to form a video denoising network based on a Wiener Filter backbone. As we introduce the temporal dimension, we must revisit the optimisation made by Bled, as previous optimal values no longer apply. We also take extra care to optimise for denoising speed.\nWe start from the baseline 3D Wiener video denoising filter implementation by Kokaram and include some of the ideas proposed in [22] to form a new method that we will call Wiener 4D.\nAs the name suggests, we first expand the Wiener filter to handle colour as an additional dimension, thus Kokaram's 3D FFT becomes a 4D FFT, using the RGB channels as the third dimension and a temporal window of 5 frames as the fourth dimension. While the filter returns five filtered frames, only the target frame is saved.\nA summary of our 4D Wiener Filter is outlined in Alg. 2. A notable difference with Kokaram's Wiener filter baseline is that our window functions need to be explicitly normalised to one in the synthesis step. This is because we also explore the choice of analysis and synthesis windowing in terms of window overlap stride, window size, and, window shape. In section IV-A, we introduce trainable 3D windows and evaluate their performance compared to Raised-Cosine windows and Gaussian windows.\nIn section IV-A, we also show that the method of DC-offset removal, a necessary preprocessing step of the FFT, can have some significant impact. Because of range clipping, noise is biased in the black regions and white regions. This bias is rarely addressed in the literature but it usually means that denoised images blacks are not dark enough. In this paper, we show that using the median for DC-offset estimation is surprisingly effective in video denoising, suppressing any visible bias, and leading to similar performance as when using the Ground-Truth DC values.\nB. Video Wiener Coring Refinement Network\nAs an alternative to the default Wiener coring function of Eq. (2), we propose, as in [22] a lightweight coring post-processing network that operates on the 4D spectral tensor. This network aims to reduce potential ringing artefacts caused by the default coring estimation errors. The network takes in the MSE-optimal Wiener filter transfer function \u0397 (W1, W2, wt, wc) as computed by Eq. (2) as a 4D tensor"}, {"title": "V. CONCLUSIONS", "content": "In our work, we have demonstrated the efficiency of using small ancillary CNNs to improve the performance of a classic, optimised Bayesian filter, moving away from the black-box approach of CNN and transformer-based denoisers. Our denoiser is smaller in terms of parameters than all tested networks, and faster than the most competi- tive methods. We have also shown that current motion compensation methods do not always improve denoising performance. This may be optimised in future work, along with further improvements in denoising speed and weighted averaging for multi-scale denoising."}]}