{"title": "Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count", "authors": ["Hanseul Cho", "Jaeyoung Cha", "Srinadh Bhojanapalli", "Chulhee Yun"], "abstract": "Transformers often struggle with length generalization, meaning they fail to generalize to sequences\nlonger than those encountered during training. While arithmetic tasks are commonly used to study length\ngeneralization, certain tasks are considered notoriously difficult, e.g., multi-operand addition (requiring\ngeneralization over both the number of operands and their lengths) and multiplication (requiring generalization\nover both operand lengths). In this work, we achieve approximately 2-3\u00d7 length generalization on both tasks,\nwhich is the first such achievement in arithmetic Transformers. We design task-specific scratchpads enabling\nthe model to focus on a fixed number of tokens per each next-token prediction step, and apply multi-level\nversions of Position Coupling (Cho et al., 2024; McLeish et al., 2024) to let Transformers know the right\nposition to attend to. On the theory side, we prove that a 1-layer Transformer using our method can solve\nmulti-operand addition, up to operand length and operand count that are exponential in embedding dimension.\nAll our experiments are reproducible based on the codebase in github.com/HanseulJo/position-coupling.", "sections": [{"title": "1 Introduction", "content": "Transformer-based language models (Vaswani et al., 2017) have become a cornerstone of modern deep learning in\nrecent years (Chowdhery et al., 2023; Gemini et al., 2023; OpenAI, 2023; Thoppilan et al., 2022). Despite their\nseemingly limitless capabilities, they often struggle with a critical limitation known as the length generalization\nproblem, meaning that the model does not perform well on input sequences longer than those encountered during\ntraining (Anil et al., 2022; Deletang et al., 2023; Press et al., 2022; Wu et al., 2023; Zhang et al., 2023). Length\ngeneralization has recently dragged the attention of many researchers because of the following two aspects: (1)\nthe failure in length generalization corroborates the models' fundamental limitation that they may not genuinely\nunderstand the task-solving algorithm but may rely on short-cut learning that is only applicable to sequences of\ntrained lengths; (2) improving length generalization can automatically extend the applicability of the models in\nboth memory-efficient and computation-efficient way.\nAs manageable and intriguing test beds, arithmetic and algorithmic tasks are commonly used to study the\ncapabilities (including length generalization) of Transformers (Cho et al., 2024; Fan et al., 2024; Kazemnejad\net al., 2023; Kim et al., 2021; Lee et al., 2024; McLeish et al., 2024; Nogueira et al., 2021; Zhou et al., 2024a,b).\nIn this paper, we mainly focus on arithmetic tasks, specifically integer addition and multiplication. While\nhumans can easily generalize to longer examples of these tasks, recent works have shown that Transformers often\nstruggle in length generalization, and various approaches have been proposed to help Transformers learn the\ntrue underlying mechanisms that solve addition and multiplication (Cho et al., 2024; McLeish et al., 2024; Zhou\net al., 2024a). As a noteworthy recent achievement, independent works by Cho et al. (2024) and McLeish et al.\n(2024) propose Position Coupling, which is a modification of the standard absolute position encoding (APE) that\nassigns the same position ID to digits of the same significance. Position Coupling demonstrates a remarkable"}, {"title": "3 Warm-up: Length Generalization on Parity Task", "content": "Before moving on to our findings about addition and multiplication tasks, we begin with a warm-up example: the\nparity task. Given a binary sequence as a query, the goal of the parity task is to output 1 if the query contains an\nodd number of 1's, and 0 otherwise. Despite its simple description, it is well known that Transformers struggle\nwith achieving length generalization for the parity task (Anil et al., 2022; Deletang et al., 2023; Hahn and Rofin,\n2024; Kazemnejad et al., 2023; Zhou et al., 2024a). In this section, we will demonstrate an enhanced length\ngeneralization performance on the parity task by applying Position Coupling on top of the input sequence with a\nproperly designed scratchpad."}, {"title": "3.1 Method: Scratchpad & Position Coupling", "content": "We follow the scratchpad format proposed by Anil et al. (2022). Figure 2 illustrates an example of an input\nsequence consisting of a 4-bit parity problem (with query 0101) and its scratchpad (0110). The idea of this\nscratchpad is to record every intermediate parity state as a given binary query is processed starting from the\nleftmost token. That is, the k-th bit in the scratchpad is the parity of the subsequence containing the first k\nbits of the query. Then, the final rightmost token of the scratchpad is the desired answer for the parity task.\nRecording the process of solving the parity task is beneficial in the following two points:\n1. It makes the task-solving algorithm simpler. The first token the scratchpad is just a copy of the first token in\nthe query sequence. Also, we do not need to directly solve the intermediate parity task at once in order to\ninfer the k-th (k > 1) intermediate token in the scratchpad; instead, it is enough to focus on the k-th token\nof the query and the (k \u2212 1)-th token in the scratchpad and then sum them up modulo 2 (see Figure 2).\n2. It is straightforward to apply Position Coupling onto the input sequence with the scratchpad. The scratchpad\ngenerates a natural positional correspondence between the query and the response. Thus, we can assign the\nsame position ID to the k-th query token and k-th scratchpad token, for example, 234512345 in the example\ndepicted in Figure 2.\nTo sum up, such a scratchpad simplifies the task by allowing the model to perform step-by-step reasoning without\nhaving to attend to an increasing number of query tokens until it solves the full task, while Position Coupling\n(applied on top of the scratchpad) can explicitly let the model know \u201cwhere it should focus on\u201d to perform\nevery step of the reasoning. We thus can expect a synergy effect when we combine these two methods, and our\nexperiments do align with this expectation."}, {"title": "3.2 Experiments & Discussion", "content": "To test the efficacy of the combination of scratchpad and Position Coupling, we compare its performance against\ntwo other configurations: a model trained without positional embedding and without any scratchpad (NoPE),"}, {"title": "4 Length Generalization on Multi-Operand Addition Task", "content": "In the previous section, we demonstrated the potential of integrating the scratchpad with Position Coupling for\nbetter length generalization. We now aim to evaluate the effectiveness of this approach on a more challenging\ntask. Specifically, we address the problem of achieving length generalization in integer addition tasks, where\noperand length and count can both get longer at test time. Indeed, there already is a length extrapolation result\non additions with more than two operands: the \u201ctriple addition\u201d task presented by Cho et al. (2024). However,\ntheir approach still requires the number of operands to be fixed (e.g., 3), leaving the challenge of generalizing to\nvarying operand counts an open question. In this section, we demonstrate that this challenge can be overcome\nby employing a scratchpad in conjunction with a carefully designed multi-level Position Coupling."}, {"title": "4.1 Method: Scratchpad & Bi-level Position Coupling", "content": "Scratchpad for Multi-operand Addition. Similar to the parity task, we store the intermediate cumulative\nsums in the scratchpad, as in Figure 4. It makes the algorithm of solving the multi-operand addition task easier.\nThis is because a model can obtain the k-th intermediate cumulative sum by adding exactly two numbers: the\nmost recently generated ((k \u2013 1)-th) intermediate sum and the k-th number/operand in the query. As a minor\ndetail, we start the scratchpad with zeros in order to make the task-solving rule more clear and consistent.2"}, {"title": "4.2 Experimental Setup", "content": "Given two integers $a \\leq b$, we denote by $[a b] := \\{a, a + 1, \u2026, b\\}$ a set of consecutive integers.\nData Sampling. For the sake of simplicity of notation, we denote a training set by $S_A(n, m)$, consisting of\naddition problems where each operand can have at most $n$ digits and the operand count can range from 2 to $m$.\nThe dataset consists of two equally sized chunks. In the first chunk, for each sample, the number of operands is\nuniformly sampled from $[2: m]$, and the length of each operand is independently sampled from $[1: n]$; this means\nthat the operands can differ in length. Based on the chosen length (e.g., 4), each operand is then randomly\ngenerated (e.g., from $[1000: 9999]$). In the second chunk, for each sample, the number of operands is still\nuniformly sampled from $[2: m]$, but this time, the lengths of all operands are identical. The operand length is\nsampled from $[1 : n]$, and all operands are randomly chosen to be of the chosen length. We use $S_A(10, 10)$ with\nsize 500,000 as the baseline training set.\nWe also denote a test set by $T_A(n, m)$, consisting of a single component. In each sample, both the number of\noperands and the length of each operand are fixed specifically at $m$ and $n$, respectively. For model evaluation,\nwe draw a 30 \u00d7 29 grid heatmap and assess the performance of the model on the test set $T_A(i, j)$ of size 1,000 for\nevery entry $(i, j) \\in [1 : 30] \u00d7 [2:30]$.\nModel and Training. Our baseline model is a 6-layer 8-head decoder-only Transformers with embedding\ndimension 1024 (with approximately 63M parameters), trained from scratch. We do not incorporate weight\ndecay or dropout. Further details can be found in Table 2.\nRandom Offset of Position IDs During Training. An important detail about the training procedure\nis that we randomly choose offsets (for each level of position IDs) and add them to every position ID in each\ntraining sample. This is to promote learning all the position embedding vectors as evenly as possible, and this\ntraining-time random assignment of position IDs is already used in prior works for similar reasons (Cho et al.,\n2024; McLeish et al., 2024; Ruoss et al., 2023). As Cho et al. (2024) and McLeish et al. (2024) did, we pre-define\nthe maximum position IDs (for each level) as hyperparameters, which naturally determines a maximum testable\nrange of operand lengths and count. See Table 2 for our choice of the maximum position IDs."}, {"title": "4.3 Experimental Results", "content": "Position Coupling and Scratchpad Together Enable Powerful Length Generalization. We trained\nmodels using 3 different position embedding methods-Position Coupling, NoPE, and FIRE-both with and\nwithout the scratchpad. The implementation details of Position Coupling differ by the presence/absence of\nscratchpad: if we use scratchpads, we apply the bi-level Position Coupling explained in Section 4.1; if not, we\nuse a simple single-level Position Coupling that matches the position IDs for digits at the same significance in all\nnumbers, which is also used for \"triple addition\" task in Cho et al. (2024). The results are showcased in Figure 1.\nWe measure the exact-match accuracy for correct inference of the whole response, including scratchpad if it is used.\nThe top 3 heatmaps in the figure, which are the results without scratchpads, indicate that these models can only\ngeneralize to in-distribution samples and exhibit near-zero exact-match accuracy on out-of-distribution samples.\nNext, when either NoPE or FIRE is combined with the scratchpad, the models show a slight improvement in\nterms of generalizable operand lengths and counts. They barely solve problems involving 13 numbers, each 12\ndigits long. In contrast, the combination of the scratchpad and Position Coupling enables much stronger length\ngeneralization, achieving non-trivial accuracy even on test samples involving 30 numbers, each with 30 digits.\nWe emphasize that such problems are extremely difficult, as the model must accurately predict a total of 1023\nconsecutive tokens to solve the problem.\nEffect of Zero-padding. Figure 5 exhibits the exact-match accura-\ncies for models trained on input sequences with scratchpad but without\nzero-padding in both the query and the response. Although there is a\nmoderate degradation in overall performance, the models still generalize\nwell with respect to an increased number of operands. Also, they main-\ntain reasonable accuracy for operand lengths below 25 digits. It implies\nthat, although zero-padding aids in enhancing length extrapolation\ncapability, it is not an absolute necessity.\nEffect of Trained Length. We compare the models trained on\ndifferent lengths: $S_A(7, 7)$, $S_A(10, 10)$, and $S_A(13, 13)$. The results are\npresented in Figure 6. As expected, the model's ability to generalize\nto longer sequences improves as the training data covers a wider range\nof lengths."}, {"title": "Effect of Architecture.", "content": "To study whether our approach can be applied across various depth/width con-\nfigurations, we explore the performance as we vary the number of layers and heads. Specifically, we tested\nconfigurations with 1, 2, 4, and 6 layers, and 2, 4, and 8 heads, resulting in 12 distinct configurations. The\nresults are illustrated in Figure 10 (see Appendix C)."}, {"title": "4.4 Theoretical Analysis on 1-Layer Transformer", "content": "In this section, we explain the success of our approach by providing a theoretical analysis in Theorem 4.1.\nSpecifically, we construct a Transformer model (whose normalization layer is omitted for simplicity) that is\ncapable of solving the addition task involving both exponentially long operands and exponentially large number\nof operands when our approach is applied. Furthermore, the constructed model is a 1-layer 4-head Transformer,\nwhich supports the experimental results presented in Figure 10: the 1-layer 4-head model succeeds, but the\n1-layer 2-head model fails.\nTheorem 4.1. With a proper input format, scratchpad, and Position Coupling, there exists a 1-layer 4-head\ndecoder-only Transformer that solves the multi-operand integer addition task involving up to $m$ operands each\nwith up to $n$ digits. Here, a sufficient choice of the embedding dimension is $d = O(log_2(m + 1) + log_2(n + 1))$.\nTheorem 4.1 above states that a 1-layer 4-head model is enough to solve multi-operand additions involving\nexponentially many and exponentially long operands (in terms of the embedding dimensions), especially when a\nproper scratchpad and Position Coupling are both applied. Since it is a sufficiency result, it implies that larger\narchitectures (with more layers and attention heads) are capable of solving the same task as well. Its detailed\nconstructive proof is provided in Appendix D. We also remark that our theorem extends Theorem 5.1 of Cho\net al. (2024), which can only handle addition problems with a fixed number of operands.\nTo illustrate our key idea, consider an example problem 057 + 048 + 096 = 000 \u2192 750 \u2192 501 \u2192 102 (with the\noutput reversed). First, consider the case without a scratchpad: 057+048 + 096 = 102. To predict the least\nsignificant digit of the final answer, 1, the model must attend to the least significant digits of all the operands,\nwhich are 7, 8, and 6, in the query sequence. In a scenario with $m$ operands, the model would need to attend\nto $m$ digits. This property-the number of tokens the model has to attend to increases with the number of\noperands\u2014makes the construction difficult.\nWith a scratchpad, the process becomes a lot simpler. Instead of attending to every least significant digit of\nall operands, the model only needs to attend to two tokens: e.g., 6 from 096 and 5 from 501. Importantly, by\nutilizing the intermediate states stored in the scratchpad, the number of tokens the model needs to attend to\nremains fixed, even if the number of operands gets larger.\nScrutinizing the Attention Patterns of Trained Transformer. Surprisingly, our insight into the advantage\nof our scratchpad and its synergy with Position Coupling can be visually verified, supporting the significance\nof our method and theoretical finding. We probe the attention matrices of Transformer models trained with a\npractical Adam optimizer. We visualize the lower-triangular row-stochastic matrix $softmax(QK^T)$ as a heatmap\nin Figure 7. Thus, if you want to know the distribution of softmax logits over key tokens for an NTP at the q-th\nquery token, you should look at q-th row of the heatmap; the brighter the point, the more the model attends to\nthat key token position.\nNow first look at the attention pattern extracted from a model trained with our scratchpad and bi-level Position\nCoupling (Figure 7a). Since the scratchpad takes up about half of the total input sequence length, we may\nfocus on the bottom half of the heatmap. The attention pattern tells us that, for most of the NTP step, the\nmodel focuses on at most a fixed number of (say two) previous tokens: one on the short anti-diagonal line\n(corresponding to the token in the query sequence) and one on the long diagonal line (corresponding to the token\nin scratchpad). This property is strikingly similar to our theoretical construction of the attention pattern."}, {"title": "5 Length Generalization on Integer Multiplication Task", "content": "Achieving length generalization for multiplication when both multiplier and multiplicand can vary in length has\nlong been recognized as a challenging problem. To our knowledge, prior works on the multiplication task (Duan\nand Shi, 2023; Fan et al., 2024; Jelassi et al., 2023; McLeish et al., 2024) have never successfully addressed this\nissue. In this section, we demonstrate the combination of Position Coupling and the scratchpad can serve as a\npowerful solution to this obstacle."}, {"title": "5.1 Method: Two-stage Scratchpad & Tri-level Position Coupling", "content": "Different from the addition tasks, every digit of the first operand interacts with every digit of the second\noperand. This makes it difficult to come up with a single-stage cumulative scratchpad. To achieve strong\nlength generalization in multiplication, we take a step beyond the simple cumulative scratchpads. We propose a\ntwo-stage scratchpad, motivated by an observation that the usual (human) computation of integer multiplication\ncan be decomposed into two stages: (i) a series of M-digit \u00d7 1-digit multiplications and (ii) a (linearly shifted\nvariant of) multi-operand addition. Figure 8b illustrates a concrete example of our two-stage scratchpad and\ntri-level position ID assignment rule, and Figure 8a explains the intuitive motivation of the scratchpad. Note that\nwe concatenate two stages of scratchpad with a '=' token, which the model is required to infer as well as other\ndigit/symbol tokens. To elaborate, let us write two operands as A (with M digits) and B (with N digits).\nStage 1: M-digit\u00d71-digit Multiplications. The first stage of the scratchpad consists of N numbers: the\nfirst number is the product between A and the least significant digit (LSD) of B, the second number is the\nproduct between A and the second LSD of B, and so on. We reverse all the N numbers, zero-pad them to match\nthe length, and concatenate them in order with '+' tokens in between. Regarding the position ID assignment,\nobserve that it is natural to (i) couple a k-th LSD of A with k-th LSDs in every number of the scratchpad stage\n1 and to (ii) couple the k-th LSD in B with every digit in the k-th number of the scratchpad stage 1. This is\nreflected to PosID1 and PosID2 in Figure 8b, until the second '=' token.\nStage 2: (Modified) Multi-Operand Addition. The second stage is basically the same as a familiar\nmulti-operand addition, with a slight difference in that we shift the operands to the left one by one as we add\nthem sequentially. It can be done by viewing the LSD of the k-th number in stage 1 (i.e., the leftmost digit,\nas it is already reversed) as the k-th LSD when solving stage 2. This is semantically equivalent to converting\n\"581+470+333\" in the stage 1 into \"58100+04700+00333\". Because of this, we introduce a totally new level of\nposition ID (level-3) that reflects this change of viewpoint on digit significance. Luckily, we can reuse the level-2\nposition IDs since they only distinguish the numbers. As a result, we expect the model to utilize level-3 and 2 of\nposition IDs to solve the second stage. In this spirit, the position ID assignment rule is similar to that introduced\nin Section 4.1, reflected to PosID2 and PosID3 in Figure 8b. Lastly, we fill in unnecessary slots with O's."}, {"title": "5.2 Experimental Setup", "content": "Data Sampling. We denote the training set by $S_M (n, m)$, consisting of multiplication problems where the\nfirst operand can have up to $n$ digits and the second operand can have up to $m$ digits. Specifically, the length of\nthe first operand is uniformly selected from $[1 : n]$. The first operand is then randomly generated based on the\nchosen length. The second operand is chosen similarly, but its length is selected from $[1 : m]$. We use $S_M (10, 10)$\nas the baseline training set.\nThe test set $T_M (n, m)$ is constructed similarly, with the primary difference being that the lengths of both operands\nare strictly fixed at $n$ and $m$. For evaluation, we create a 30 \u00d7 30 grid heatmap and assess the performance of\nthe model on the test set $T_M(i, j)$ for every entry $(i, j) \\in [1 : 30] \u00d7 [1 : 30]$.\nModel and Training. We employ the same baseline architecture as in the addition task (See Table 3)."}, {"title": "5.3 Experimental Results", "content": "We evaluate models trained with 3 different position embedding methods, both with and without the scratchpad,\nand present the results in Figure 9. For models using Position Coupling without the scratchpad, we adopted a\nsimilar position ID assignment scheme proposed for solving N-digit \u00d7 2-digit multiplication in Cho et al. (2024).\nWhen the scratchpad is not applied, which corresponds to the top 3 plots, none of the models manage to generalize,\neven on in-distribution samples. When NOPE or FIRE is employed in conjunction with the scratchpad, the\nmodels show limited length generalization, achieving non-trivial accuracy on multiplication between two 12-digit\nintegers. However, the combination of Position Coupling and the scratchpad again dominates others.\nInterestingly, Position Coupling without the scratchpad shows weak length generalization when one of the\noperands is short. This should not come as a surprise, as Cho et al. (2024) already demonstrate that a model\ntrained with Position Coupling alone can generalize to N-digit \u00d7 2-digit multiplication task: models trained on\nN\u2264 40 can generalize to N \u2265 100."}, {"title": "6 Conclusion", "content": "While length generalization in arithmetic Transformers has drawn a lot of attention, especially on operand length\nfor the addition, the ability to generalize on operand counts is considered a difficult challenge and has not been\nexplored yet. To address this challenge, we propose a combination of two techniques: scratchpad and Position\nCoupling. We show that a Transformer trained on problems involving 1-10 digit integers with 1-10 operands\ncan solve addition tasks with up to 30 operands, each being as long as 30 digits. We also theoretically construct\na 1-layer Transformer model capable of adding exponentially many operands with exponentially long integers\nwhen our approach is applied. Finally, we demonstrate the effectiveness of our approach for length generalization\nin the multiplication task, where both operand lengths can vary.\nLimitation. One limitation of our approach is that it is only applicable to tasks whose structure is well-defined\nand can be effectively encoded by scratchpad and Position Coupling. This leaves us with two directions for\nfuture work. The first direction is to establish a clear principle for employing scratchpad and Position Coupling\nwhen the task structure is known, as the current design choice heavily relies on intuition. The second is to\nextend our method to the cases where the task structure is implicit or even entirely unknown."}, {"title": "A Related Works", "content": "Several works (Anil et al., 2022; Deletang et al., 2023; Nogueira et al., 2021) have scrutinized the Transformer's\nlack of ability to length generalize across algorithmic reasoning tasks. Here, we list the studies investigating and\nenhancing the length extrapolation capability of Transformers for arithmetic/algorithmic tasks.\nAddition Tasks. We first begin by exploring studies focused on encoder-only Transformers. Ruoss et al. (2023)\nintroduce Randomized Position Encodings to address the problem of the appearance of unseen position indices\nin longer sequences. Additionally, Jelassi et al. (2023) demonstrate that the RPE method enables generalization\nto 15-digit addition problems when the model is trained on problems up to 5 digits.\nWe next move on to the studies concerning decoder-only Transformers. Kazemnejad et al. (2023) investigate the\neffect of PE methods on length generalization performance, arguing that NoPE outperforms several popular\nPE methods such as ALiBi (Press et al., 2022), Rotary (Su et al., 2024), APE (Vaswani et al., 2017), and T5's\nRelative PE (Raffel et al., 2020). Meanwhile, Shen et al. (2023) propose the use of a scratchpad and random\nspacing, which facilitate generalization to 12-digit problems when trained on up to 10-digit problems. Zhou et al.\n(2024a) introduce the technique of \u201cindex-hinting\", which inserts appropriate position markers in the sequence.\nZhou et al. (2024b) integrate several existing techniques-FIRE (Li et al., 2024), index-hinting (Zhou et al.,\n2024a), and Randomized PE (Ruoss et al., 2023)\u2014achieving generalization to 100-digit problems while training\nexclusively on samples with fewer than 40 digits. Furthermore, Cho et al. (2024) and McLeish et al. (2024)\nindependently introduce Position Coupling (also called \u201cAbacus\u201d), demonstrating state-of-the-art performance in\nthe literature by generalizing to 100-digit problems after training on samples with up to 20 digits. We lastly\nremark there is a recent attempt to solve addition by utilizing a looped transformer (Fan et al., 2024).\nMultiplication Tasks. Most studies on multiplication focus on problems where one operand has a fixed digit\nlength. Jelassi et al. (2023) investigate N-digit \u00d7 3-digit multiplication but observe length generalization only\nwhen train set priming is applied, which involves adding a few long samples in the train set. Cho et al. (2024)\npresent the effectiveness of Position Coupling in N-digit \u00d7 2-digit multiplication, achieving generalization to\nN\u2265 100 after training on samples with N < 40. Fan et al. (2024) showcase the length generalization capability\nof looped Transformers to 16-digit problems from training up to 11 digits, where the numbers are encoded in\nbinary format and the length of the first operand is up to 2.\nWhile McLeish et al. (2024) train their models on multiplication tasks where both operand lengths can vary,\nthey observe only in-distribution generalization.\nThere is also a body of literature focused on length generalization in algorithmic tasks. We highlight a few\nkey contributions. Zhou et al. (2024a) introduce the concept of RASP-L, suggesting the conjecture of the\nTransformer's ability to length-generalize may depend on whether the task can be expressed in the RASP-L\nlanguage. Additionally, Awasthi and Gupta (2023) create an auxiliary task associated with sorting, resulting in\nsubstantial length generalization improvements through multitask learning."}, {"title": "A.1 Length Generalization in Arithmetic/Algorithmic Transformers", "content": "Several works (Anil et al., 2022; Deletang et al., 2023; Nogueira et al., 2021) have scrutinized the Transformer's\nlack of ability to length generalize across algorithmic reasoning tasks. Here, we list the studies investigating and\nenhancing the length extrapolation capability of Transformers for arithmetic/algorithmic tasks.\nAddition Tasks. We first begin by exploring studies focused on encoder-only Transformers. Ruoss et al. (2023)\nintroduce Randomized Position Encodings to address the problem of the appearance of unseen position indices\nin longer sequences. Additionally, Jelassi et al. (2023) demonstrate that the RPE method enables generalization\nto 15-digit addition problems when the model is trained on problems up to 5 digits.\nWe next move on to the studies concerning decoder-only Transformers. Kazemnejad et al. (2023) investigate the\neffect of PE methods on length generalization performance, arguing that NoPE outperforms several popular\nPE methods such as ALiBi (Press et al., 2022), Rotary (Su et al., 2024), APE (Vaswani et al., 2017), and T5's\nRelative PE (Raffel et al., 2020). Meanwhile, Shen et al. (2023) propose the use of a scratchpad and random\nspacing, which facilitate generalization to 12-digit problems when trained on up to 10-digit problems. Zhou et al.\n(2024a) introduce the technique of \u201cindex-hinting\", which inserts appropriate position markers in the sequence.\nZhou et al. (2024b) integrate several existing techniques-FIRE (Li et al., 2024), index-hinting (Zhou et al.,\n2024a), and Randomized PE (Ruoss et al., 2023)\u2014achieving generalization to 100-digit problems while training\nexclusively on samples with fewer than 40 digits. Furthermore, Cho et al. (2024) and McLeish et al. (2024)\nindependently introduce Position Coupling (also called \u201cAbacus\u201d), demonstrating state-of-the-art performance in\nthe literature by generalizing to 100-digit problems after training on samples with up to 20 digits. We lastly\nremark there is a recent attempt to solve addition by utilizing a looped transformer (Fan et al., 2024)."}, {"title": "A.2 Chain-of-Thoughts Prompting", "content": "While the main focus of this work is on training the model with a scratchpad, chain-of-thoughts (CoT) prompting- showing a series of intermediate natural language reasoning steps before reaching the answer is also extensively studied to enhance the reasoning ability of Transformers (Kojima et al., 2022; Suzgun et al., 2022; Wei et al., 2022). Similar to the spirit of scratchpad, CoT allows the model to decompose complex problems into several intermediate steps, which has demonstrated its importance in tasks that require arithmetic and reasoning. In an attempt to understand the success of CoT, Feng et al. (2024) investigate the expressivity of CoT. They prove that the autoregressive Transformers of constant size can solve basic arithmetic/equation tasks, while finite-depth Transformer models cannot directly produce correct answers to these tasks unless their size grows super-polynomially with input length. In arithmetic tasks, their experiments reveal that models trained with CoT-formatted data can generalize to different numbers of operands, but the generalization leap is limited to 3 (from 15 to 18)."}, {"title": "B Experimental Details", "content": "We modify and customize the codebase from Kazemnejad et al. (2023) for all our experiments.5 This codebase\nincludes a custom implementation of a decoder-only T5 model (Raffel et al., 2020) built upon PyTorch (Paszke\net al., 2019) and HuggingFace (Wolf et al., 2019), which incorporates several positional encoding methods.\nWe implemented a custom RMSNorm module (Zhang and Sennrich, 2019) and various normalization layer\npositioning schemes (e.g., PreNorm (Xiong et al., 2020), PostNorm (Vaswani et al., 2017)) to follow the\nimplementation details outlined by Cho et al. (2024); Zhou et al. (2024b).\nBelow, we provide detailed settings of experiments."}, {"title": "D Formal Construction of Multi-Operand Addition Transformer", "content": "In this section, we prove Theorem 4.1 by formally constructing a 1-layer 4-head Transformer model capable of\nsolving multi-operand addition problems. The framework of the proof mostly follows the proof by Cho et"}]}