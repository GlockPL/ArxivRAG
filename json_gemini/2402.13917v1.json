{"title": "What Linguistic Features and Languages are Important in LLM Translation?", "authors": ["Ryandito Diandaru", "Lucky Susanto", "Zilu Tang", "Ayu Purwarianti", "Derry Wijaya"], "abstract": "Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) and their utiliza-tions have been a popular research topic in Natural Language Processing (NLP) due to their remark-able performance on various tasks (Brown et al., 2020; OpenAl, 2023; Touvron et al., 2023a,b) and machine translation is no exception. Extensive eval-uations (Robinson et al., 2023; Hendy et al., 2023) on machine translation of the prominent GPT (Ope-nAl, 2023) models have shown that they exhibit a promising capability for high-resource machine translation although sometimes not for underrepre-sented languages (Robinson et al., 2023; Hendy et al., 2023; Stap and Araabi, 2023; Kadaoui et al., 2023).\nA popular approach for low-resource NLP is to leverage other higher-resourced languages. These approaches include using them as pivot (Xia et al., 2019), in transfer learning (Gu et al., 2018; Nguyen and Chiang, 2017) and for joint training (Neubig and Hu, 2018; Johnson et al., 2017). Performance gains from such methods indicate a strong influ-ence of their presence in the training data. Given that including related languages alongside the low-resource language can often improve performance (Xia et al., 2019; Poncelas and Effendi, 2022; Gu et al., 2018; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Johnson et al., 2017), it makes sense to evaluate the degree of proximity between these lan-guages, which can be done using the vectors from the URIEL typological database (Littell et al., 2017). The utilization of the URIEL database has facilitated a more guided approach to exploring the incorpo-ration of multiple languages into a single model by leveraging linguistically-aware feature vectors from which linguistic distances can be computed. These feature vectors have been utilized by previ-ous works in various ways including determining which language to use as transfer or pivot language (Lin et al., 2019; Nambi et al., 2023) and measuring language diversity (Ruder et al., 2021).\nIt is ideal for an LLM to be as transparent as possible to conduct a comprehensive performance evaluation. This includes details such as the spe-cific languages, the quantity of data for each lan-guage, and the nature of the data used for training. A significant constraint in studies that assess the GPT model series (Hendy et al., 2023; Robinson et al., 2023) is the fact that these models are pro-prietary, closed-source systems. This presents a fundamental challenge as it remains unclear which languages are included in the models. On the other hand, determining the significance of language fea-tures poses a non-trivial challenge due to its many dimensions. For instance, the URIEL database vec-tors encompass several linguistic features, which include FEATURAL, GENETIC, GEOGRAPHICAL, PHONOLOGICAL, INVENTORY, and SYNTACTIC distances.\nIn this work we are evaluating Llama2 (Touvron et al., 2023b) in machine translation to highlight its multilingual capability with an emphasis on lan-guages the model has and has not seen during training. We analyze the results against linguis-tic feature distances to gather insight about which"}, {"title": "2. Methodology", "content": "systems. The machine translation evaluation is conducted using the FLORES-200 Guzm\u00e1n et al. (2019) benchmark as it provides support for nu-merous low-resource languages, we exclude into-English translation directions to mitigate the risk of potential data leakage, given that the benchmark relies on Wikipedia as its English data source. We also exclude zero-shot translation as LLMs often get the language wrong in this prompting setup as reported by Robinson et al. (2023). We use a statistical machine translation metric, the BLEU (Papineni et al., 2002) score and a model-based machine translation metric when applicable, the default COMET-22 (Rei et al., 2022) model."}, {"title": "2.1. Language Similarity Metric Evaluation", "content": "We experiment with languages reported in the train-ing data of Llama2 (Touvron et al., 2023b), the list of which and their respective ISO 639-3 codes can be found in Table 3. We refer to this set of lan-guages as inllama. We also pick 15 languages not reported in the training data which we will refer to as outllama, presented in Table 1. It is impor-tant to highlight that languages not explicitly men-tioned in Llama2 might still be present in the training data, albeit at a minuscule proportion of less than 0.005% (Touvron et al., 2023b). Languages in out-Ilama cover various language genera and writingFor more detailed explanation of these distances, consult \nhttps://www.cs.cmu.edu/~dmortens/ \nprojects/7_project/"}, {"title": "3. Results and Analysis", "content": ""}, {"title": "3.1. Llama2 Translation Results", "content": "One-shot 7B Llama2 translation results are pre-sented in Table 3 and Table 4. From Table 3, we observe that none of the languages included in inllama produce a BLEU score below 10. This sug-gests that we can reasonably assume that Llama2 is capable of translating into all the languages it has encountered during training. However, many lan-guages in outllama yields a BLEU score under 10, this is expected as Llama2 is presumably unfamiliar with these languages. We move forward with lan-guages in outllama that yields a BLEU score below 10 and experiment with other variations of Llama2. We explore the effect of scale, chat version, and adding shot count and present the results on Ta-ble 5. Due to our limited compute resources we excluded the 70B and 70B-chat versions of Llama2.\nModel scale enhances translation ability, but not always for instruction tuning and not so much for adding shot count. Our experiment results presented in Table 5 demonstrate that the 13B versions of Llama2 outperforms the smaller 7B versions for all truly unseen languages. However, larger models do not seem to yield the same num-ber of gains for every language. In best cases, 13B models increase on average as high as 2.53 BLEU with a standard deviation of 1.64. We also observed that instruction-tuning through chat versions im-proves translations for some languages over mod-els with the same scale and shot count. This is observed best on Igbo and Javanese, with gains as much as 3.16 and 2.87 respectively, and worst on Tagalog, which performs worse on chat models with a decrease as big as 2.64. Adding the shot count generally improves performance although it is less drastic as model scale and instruction-tuned setups with a mean increase of 0.47 and 0.08 for non-chat and chat Llama-13B respectively. While previous three factors appears to greatly enhance Llama2's capacity to translate into some languages, there are languages where the prospects are quite limited. For instance, for Sinhala and Tamil, scaling the model results in gains only less than 1 BLEU score and neither does using chat models or adding shot count."}, {"title": "3.2. Language Proximity Analysis", "content": "The translation results using Llama2 7B with the one-shot prompting setup presented on Table 3 and Table 4 are used for linguistic proximity analysis. We first retrieve pre-computed distances\u00b2 from the2http://www.cs.cmu.edu/~aanastas/ files/distances.zip"}, {"title": "4. Related Work", "content": "Our work is in line with previous studies that assess LLMs for machine translation, resembling the work conducted by Hendy et al. (2023) and Robinson et al. (2023). We extend those evaluations by in-vestigating the influence of the languages included in the model. This aspect of the analysis was pre-viously unattainable in the two aforementioned pa-pers, mainly due to the lack of transparency of the data used for training LLMs. Our method of anal-ysis is closely related to the one included in the work of Robinson et al. (2023) where languages of interest are represented as feature vectors to do further evaluation. Our objective is to extend that exploration by encompassing other linguistic fea-tures obtained from the URIEL typological database (Littell et al., 2017). We are interested in the phe-nomenon observed in the work of Lin et al. (2019) which also uses distances retrieved from URIEL Ty-pological Database (Littell et al., 2017). This work shows that however important dataset statistics fea-tures are compared to linguistic features, there are still cases where using them alone results in poor performance. This phenomenon drives us to con-duct a more comprehensive exploration of linguistic features."}, {"title": "5. Conclusion", "content": "We provide a comprehensive evaluation of machine translation task in Llama2 in its seen and unseen languages. In this work, we provide machine trans-lation scores for 26 non-English languages cov-ered in the Llama2 model training data using the Llama2-7B model. We also added 15 additional languages not in the Llama2 training data using the 7B, 7B-chat, 13B, and 13B-chat versions. Our results show that Llama2 is capable of translat-ing into languages it is unfamiliar with, although this phenomenon is observed only for some lan-guages. We also demonstrate that the model scale has the most substantial impact on translation per-formance, while the impact of using the chat ver-sions of Llama2 and adding the shot count varies by language. We calculate and visualize the cor-relations between language feature distances and machine translation scores. Our analysis reveals that syntactic similarity is not always the predom-inant linguistic factor influencing machine transla-tion scores. Furthermore, despite English having the most training data, there are often other lan-guages, whose features consistently exhibit equally or even stronger correlations in determining ma-chine translation scores albeit having much fewer training data. Our findings pose a unique perspec-tive on state-of-the-art LLMs, suggesting that the prevailing focus on English-centric models may not be the most optimal approach, potentially opening the door to more effective multilingual systems that could be shaped by languages other than English."}, {"title": "Limitations", "content": "Our research heavily depends on the language distances obtained from the URIEL typological database, as introduced by Littell et al. (2017). The original authors noted that many languages in the database may have missing features, which means the accuracy of our findings is constrained by the methods used to estimate or compensate for these missing features. Our evaluation with the COMET-22 metric is only done for languages supported in their models. However, we are aware that the model may not be equally reliable for all languages, thus the reliability of the COMET-22 heatmap goes as far as the COMET-22 model.\nIn an ideal scenario, it would be advantageous to include all languages from the FLORES-200 bench-mark and all available versions of Llama2 to pro-vide more evidence of the effectiveness of model scale and the overall generalizability of our find-ings. Unfortunately, our research is constrained by limited computational resources, preventing us from achieving this comprehensive coverage. We exclude to-English translation directions as Llama2 is likely to be trained on English Wikipedia cor-pora. We also exclude prompting languages in outllama using various dictionary-based prompting techniques due to the challenging work required to collect accurate dictionary entries for low-resource languages. However, we leave this for future work. We are also aware that the chat versions of Llama2 have been intentionally trained to prevent the generation of harmful or toxic content, and this protective design may affect the quality of trans-lations. Moreover, the chat versions of the model generate numerous artifacts in addition to the trans-lated sentences. We have made diligent efforts to automate the output parsing process to ensure that metrics are calculated fairly. The task of human evaluation and manual parsing of the outputs is left for future work."}]}