{"title": "AtlasSeg: Atlas Prior Guided Dual-U-Net for Cortical Segmentation in Fetal Brain MRI", "authors": ["Haoan Xu", "Tianshu Zheng", "Xinyi Xu", "Yao Shen", "Jiwei Sun", "Cong Sun", "Guangbin Wang", "Dan Wu"], "abstract": "Accurate tissue segmentation in fetal brain MRI remains challenging due to the dynamically changing anatomical anatomy and contrast during fetal development. To enhance segmentation accuracy throughout gestation, we introduced AtlasSeg, a dual-U-shape convolution network incorporating gestational age (GA) specific information as guidance. By providing a publicly available fetal brain atlas with segmentation label at the corresponding GA, AtlasSeg effectively extracted the contextual features of age-specific patterns in atlas branch and generated tissue segmentation in segmentation branch. Multi-scale attentive atlas feature fusions were constructed in all stages during encoding and decoding, giving rise to a dual-U-shape network to assist feature flow and information interactions between two branches. AtlasSeg outperformed six well-known segmentation networks in both our internal fetal brain MRI dataset and the external FeTA dataset. Ablation experiments showed the efficiency of atlas guidance and the attention mechanism. The proposed AtlasSeg demonstrated superior segmentation performance against other convolution networks with higher segmentation accuracy, and may facilitate fetal brain MRI analysis in large-scale fetal brain studies.", "sections": [{"title": "1. Introduction", "content": "Magnetic resonance imaging (MRI) of the fetal brain has become an essential tool in prenatal studies, significantly improving the understanding of fetal growth and development [1], [2]. It plays an important role in clinical examination of prenatal brain disorders due to its superior image resolution and diverse tissue contrasts, which are considered more informative than ultrasonography [3], [4]. The advancements in fetal MRI were attributed to the development of fast imaging techniques [5], improved motion correction and advanced reconstruction algorithms [6], [7]. These technological improvements enabled the generation of detailed, high-resolution three-dimensional volumes of the fetal brain, providing benefits for both diagnostic and quantitative assessments [8]. Central to the quantification of fetal brain anatomy is automated segmentation [9]. Segmentation of fetal brain MRI is vastly different from that of the adult brains, due to the thin miniature sized fetal brain, weak contrast, and dynamically changing anatomy and tissue contrast of the fetal brains. The field has evolved from manual or semi-automatic approaches, such as multi-atlas registration techniques [10], to more advanced deep learning (DL) solutions. Most DL-based methods train U-shape convolutional neural networks (CNNs) with manually labeled ground truths in an end-to-end manner, such as the nnU-Net [11], which has become the state-of-the-art (SOTA) approaches in segmentation of the fetal brain [12]. Despite these efforts, segmentation accuracy of the fetal brain remains limited compared to that in the adult brains [13]. We think one primarily reason is associated with the rapidly changing fetal brain with gestational age (GA), as shown by the examples in Fig. 1a. This variability poses a major obstacle for the existing end-to-end networks, which can only implicitly learn GA-related information. As a result, the network's adaptation to the wide range of anatomical variations across different GA is limited. Furthermore, another issue is the imbalanced distribution of GA in the training data (Figure 1b), as less patients are scanned at early GA or very late GA close to delivery. This uneven distribution can adversely affect the network's learning, making it less effective in segmenting fetal brain tissues at early or late stages of pregnancy. Because fetal brain atlases from mid-to-late gestations have been generated [14], [15], [16] and made publicly available, we hypothesized that by borrowing the anatomical information of the atlases at corresponding GA"}, {"title": "1.1 Related work", "content": "In early studies, automatic tissue segmentation of 3D fetal brain MRI often relied on structural information provided by atlases [17], [18]. These multi-atlas segmentation (MAS) techniques usually involved registering the unlabeled source volume to several target atlases and then back-transforming atlas labels and fusing of the labels to obtain segmentation [15]. Traditional registration methods could be replaced by some DL-based deformable registration frameworks [19] like VoxelMorph [20], for improved accuracy and faster segmentation. However, manual correction remained necessary after MAS [21]. Recently, U-Net [22] had emerged as a benchmark in medical image segmentation. Most DL-based fetal brain MRI segmentation networks utilized U-Net or its variants [13]. Payette et al. [23]"}, {"title": "1.2 Contributions", "content": "To improve the segmentation of fetal brain MRI across various GAs, and to address the variability in anatomical structures observed during fetal development, we proposed an Atlas prior guided Segmentation network (AtlasSeg). The proposed AtlasSeg integrated GA prior information, as encoded in the corresponding atlas to assist the segmentation. AtlasSeg consisted a dual-U-Net architecture (Fig. 1c), simultaneously processing the MRI volume and corresponding atlas image and label at the matching GA. Stage-wise dense attention connections were constructed at different convolution stages for feature fusion to enhance the flow of features between the two branches. We evaluated the performance of AtlasSeg against six well-known networks: 3D U-Net [22], Squeeze & Excitation Fully Convolutional Network (SE-FCN) [36], DenseU-Net [37], UNet++ [25], Attention U-Net [26], and the SOTA model MixAttNet [27]. The paper is organized as follows: the model architecture and attention mechanism are discussed in Section II. The experiment details and the introduction of fetal MRI dataset are described in Section III. Section IV describes the experimental results, followed by a discussion in Section V and a conclusion in Section VI. The relevant code has been released on GitHub (https://github.com/xuhaoan/FetalAtlas-Seg)"}, {"title": "2. Method", "content": null}, {"title": "2.1 Model Architecture", "content": "Figure 2 illustrates the architecture of our proposed AtlasSeg for fetal brain MRI tissue segmentation. AtlasSeg utilizes two parallel U-Net structures, each featuring an encoder-decoder design and skip connections at every convolutional stage. These twin U-Nets serve distinct roles, one as the segmentation branch for processing the target fetal brain MRI volume, while the other as the atlas branch, handling the corresponding atlas comprising two channels of image and label. To ensure efficient information exchange, dense attentive connection modules are implemented between the dual U-Nets. The similar structure of two branches facilitates the establishment of attention connections, as corresponding feature maps are of the same size.\nThe input MRI volume first passes through a double convolution block (shown in Figure 2b), comprising two convolution operations, each followed by batch normalization (BN) [38] and Parametric Rectified Linear Unit (PReLU) [39] for activation. The initial feature map in the segmentation branch's encoder, denoted as $E^f$, captures the semantic features of the target tissue. Simultaneously, in the atlas branch, the corresponding atlas image and label are concatenated into a two-channel input and subjected to an identical double convolution block with separate parameters, resulting in $E^A$. Both $E^A$ and $E^f$ are subsequently fed into a deep attentive fusion module, which will be discussed in the following section. The output of fusion is further processed by the encoder block in Figure 2b, consisting of a double convolution block and a downsampling block. After encoding, the bottleneck feature maps, $B^f$ and $B^A$, passes through a fusion module and are then decoded via a block containing a trilinear interpolation-based upsampling and another double convolution block. In the final decoding stage, the output of segmentation branch undergoes convolution with 1*1*1 kernel size and single output channel, followed by a sigmoid activation function to generate the segmentation label."}, {"title": "2.2 Multi-scale Attentive Atlas Fusion Module", "content": "The dense multi-scale attentive atlas fusion module (MA2-Fuse) illustrated as the orange block in Figure 2a is a pivotal part of AtlasSeg. As shown in Figure 2c, this module is designed to fuse features from segmentation and atlas branches at corresponding stage i, which are denoted as $F_i^s$ and $F_i^A$, respectively. This setup allows the module to dynamically weight the features from the target image and prioritize anatomical details and contextual information guided by atlases.\nMA2-Fuse involves two primary operations: multi-scale spatial attention and late concatenation. In the first step, the feature map of segmentation branch $F_i^s$ with channel count of $c_s$ is processed by a group of convolutions with various kernel sizes and output channel count of $\\frac{c_s}{4}$,follow by BN and PReLU. This multi-scale approach is designed to broaden the receptive field of the attention map and capture contextual information at different spatial resolutions. The atlas feature $F_i^A$ undergoes a parallel process, albeit with an output channel count of $\\frac{c_A}{4}$. All attention-enhanced features are concatenated together to a channel count of $c_s + c^A$, then pass through a 1*1*1 convolution for merging, followed by sigmoid activation to produce the attention weighting map A. Subsequently, the features from both the segmentation and atlas branches are concatenated together, which are element-wise multiplied with attention weighting map A to generate the deep fusion output. The overall operation of MA2-Fuse can be summarized by the following form:\n$F_i^\u00a5 = A \\otimes (F_i^S \\oplus F_i^A) = \\sigma(f(F_i^S;\\theta^S_i) \\oplus f (F_i^A;\\theta^A_i)) \\otimes (F_i^S \\oplus F_i^A)$ \nwhere $\\oplus$ represents channel-wise concatenation, and $\\otimes$ is element-wise multiplication, $\\sigma$ represents the sigmoid activation, f represents the operation of multi-scale spatial attention, $\\theta^S_i$ and $\\theta^A_i$ denote the parameters of convolutional groups."}, {"title": "2.3 Loss Function", "content": "We trained our network with the unweighted sum of binary cross-entropy (BCE) loss and dice loss [40]. The predicted segmentation label and ground truth are donated as P and G, respectively. $p_i$ and $g_i$ denotes the predicted label and ground truth at voxel i. N is the number of voxels. BCE loss is defined as:\n$L_{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} [g_ilog(p_i) + (1 - g_i)log(1 - p_i)]$,\nand dice loss is defined as:\n$L_{Dice} = 1 - \\frac{2\\sum_{i=1}^{N}g_ip_i}{\\sum_{i=1}^{N}(g_i)^2+\\sum_{i=1}^{N}(p_i)^2}$.\nThe final loss L is defined as:\n$L = L_{BCE} + L_{Dice}$."}, {"title": "3. Experiment", "content": null}, {"title": "3.1 Data acquisition and preprocessing", "content": "A total of 180 fetal brain MRIs in at least three orthogonal orientations (axial, coronal, and sagittal) were collected under Institutional Review Board approval, namely, the ZJU dataset [14]. The data were acquired on a 3T Siemens Skyra scanner (Siemens Healthineers, Erlangen, Germany) with an abdominal coil, using a T2-weighted half-Fourier single-shot turbo spin-echo (HASTE) sequence with the following protocol: repetition time/echo time = 800/97 ms, in-plane resolution = 1.09 \u00d7 1.09 mm, field of few = 256 \u00d7 200 mm, thickness = 2 mm, partial Fourier factor = 5/8, echo train length = 102, and GRAPPA factor = 2, number of calibration line = 42 with an interleaved acquisition. Informed consent was obtained from all pregnant volunteers participating in the study. 31 cases were excluded before data preprocessing due to low image quality, such as low signal-to-noise ratio, signal voids, and severe fetal motion. The remaining 149 cases were processed with the following pipeline. First, bias field correction was performed using the N4 algorithm [41]. Then, fetal brains were extracted using a CNN-based brain extraction tool [8], followed by 3D non-local means denoising [42]. Each stack was normalized to its maximum intensity. Next, motion correction and super resolution reconstruction were executed utilizing the NiftyMIC toolkit [8] to generate isotropic volumes at a resolution of 0.8 mm. 47 cases were further excluded due to motion artifacts or poor image quality after reconstruction, resulting in 102 subjects (GA: 22.4-39.0 weeks) for further segmentation. The remaining fetal brain volumes were rigidly registered to the spatiotemporal fetal brain atlas [16] using FLIRT [43]. The registered volumes were resized to dimensions of 192\u00d7192\u00d7144 with zero-padding.\nTo generate ground truth cortical labels, all 3D reconstructed fetal brain volumes were segmented using an automatic MAS method provided by DrawEM. The coarse segmentations underwent several rounds of manual corrections by three raters over the span of a year to ensure accuracy [14]. The supporting atlas images and labels utilized in the network were derived from the 4D spatiotemporal fetal brain atlas from 23 to 38 weeks of GA, established by Xu et al. [14]. The GA of input MRI was rounded to the nearest GA and then paired with the corresponding atlas. For images outside the atlas range, such as those at 39 weeks of gestation, the closest available atlas was used.\nTo evaluate the generalizability of different networks on out-of-distribution (OOD) data from independent sites, we used the fetal brains from the FeTA challenge 2021 dataset [44] for inference. This dataset provided 80 isotropic fetal volumes from a single institution, all of which were manually segmented. From these, we selected 20 healthy and morphologically normal fetal brains with high reconstruction quality, to test the performance of different networks."}, {"title": "3.2 Evaluation Metrics", "content": "To quantitatively assess the performance of different networks, we employed three evaluation metrics: Dice Similarity Coefficient (DSC), 95 percent Hausdorff Distance (95HD), and Average Symmetric Surface Distance (ASSD). DSC is one of the most common metrics in medical segmentation [45], which is region-based and measures the spatial overlap between the ground truth and the predicted segmentation. 95HD and ASSD are both boundary-based metrics. HD calculates the distance of a set to the nearest point in the other set, which is used to measure the similarity between two boundaries:\n$HD(G, P) = max{sup_{g \\in S_G}inf_{p \\in S_P}d(g, p), sup_{p \\in S_P}inf_{g \\in S_G}d(p, g)$,\nwhere $S_G$ and $S_P$ represents the sets of surface points in the ground truth G and the predicted segmentation P, d(g, p) represents the Euclidean distance between points g and p. 95HD considers the distance below 95% of all points, which makes it more robust to outliers compared to the standard HD. ASSD assesses the average distance from points on the predicted segmentation surface to the ground truth:\n$ASSD(G, P) = \\frac{1}{|S_G|+|S_P|} [\\sum_{g \\in S_G} min_{p \\in S_P} d(g,p) + \\sum_{p \\in S_P} min_{g \\in S_G} d(p, g)]$"}, {"title": "3.3 Algorithm Comparison", "content": "We compared the performance of AtlasSeg with six well-known CNN-based models: 3D U-Net [22], SE-FCN [36], Dense U-Net [37], UNet++ [25], and Attention U-Net [26], and a SOTA model, MixAttNet [27]. All these networks were trained under the same schemes and hyper-parameters.\n\u2022 3D U-Net: An extension of the classic U-Net to 3D volumes, consisted of an encoder-decoder architecture with skip connections.\n\u2022 Squeeze & Excitation Fully Convolutional Network (SE-FCN): SE-FCN introduced a fundamental attention mechanism in CNNs, by incorporating Squeeze & Excitation (SE) blocks to adaptively recalibrate both spatial and channel-wise features.\n\u2022 Dense U-Net: Dense U-Net created a deeply interconnected architecture with each layer connected to every other layer, thus promoting the reuse of features.\n\u2022 UNet++: An advanced version of U-Net, which introduced nested, dense skip pathways to bridge the semantic gap between the encoder and decoder feature maps, and thereby enhancing the accuracy of segmentations.\n\u2022 Attention U-Net: This variant enhanced U-Net with attention gates into skip connections, allowing for selective focus on target structures.\n\u2022 MixAttNet: A 3D deep attentive FCN with mixed-kernel convolutions, specifically designed for fetal brain MRI cortical plate segmentation. It combined a U-Net backbone with stage-wise attention refinement modules, employing mixed-kernel convolutions to capture multi-scale contextual information."}, {"title": "3.4 Ablation study", "content": "We tested the design of the attention module using ablation experiments, which could be divided into two parts, including eight variant network configurations. First, we investigated the effects of positioning fusions at different stages of the network, including 1) Early fusion, introducing attention at the initial stage $E_1$; 2) Late fusion, applying attention at the final stage $D_1$; 3) Encoder fusion, integrating attention within the encoder stages; 4) Decoder fusion, incorporating attention within the decoder stages; and 5) the full AtlasSeg model, which integrated feature fusion at every stage. The second part of the ablation experiments tested various attention mechanisms. These included: 1) Add attention, involving element-wise addition of corresponding feature maps from both branches; 2) Late attention, where the corresponding feature maps were concatenated; 3) Spatial attention [46], which utilized single-kernel convolution to generate spatial attention maps; 4) Multi-scale spatial attention [47], employing convolutions with varying kernel sizes for creating attention maps; and 5) the full AtlasSeg model, combining multi-scale spatial attention with subsequent late concatenation."}, {"title": "3.5 Implementation and Training", "content": "The network was implemented using PyTorch [48] and trained on an NVIDIA GeForce RTX 3090 GPU with a batch size of 4. We trained our network using the Adam optimizer [49] with an initial learning rate of 1e-3 in an end-to-end manner, and reduced the learning rate by a factor of 0.9 if there was no improvement on the validation set for 5 epochs. Training stopped after 800 epochs, each comprising 25 iterations. Data augmentation of random 3D flip, rotation up to 35 degrees, contrast adjustment, and elastic deformation were used during training. All comparison and ablation networks were trained with the same datasets, augmentation techniques, and optimization hyperparameters. While MixAttNet was trained using its specific loss function, other networks used the combined BCE and dice loss for training."}, {"title": "4. Result", "content": "The comparison of fetal brain MRI segmentation algorithms were presented in Table 1, based on DSC, 95HD, and ASSD measurements. AtlasSeg demonstrated superior performance, with the highest average DSC of 0.9172, the lowest average 95HD of 1.0259, and the lowest average ASSD of 0.2531. Paired t-tests showed significant differences (at least p<0.001) between AtlasSeg with all other networks. In the inter-site test, as shown in Table 2, AtlasSeg consistently achieved the best segmentation performance in terms of DSC, 95HD, and ASSD, with paired t-tests indicating significant higher DSC than the other methods (at least p<0.001). Notably, the SOTA model MixAttNet exhibited a considerable decline of segmentation accuracy in the OOD experiment."}, {"title": "5. Discussion", "content": "Accurate fetal brain tissue segmentation was challenged by the rapid and complex changes in brain anatomy, contrast, and cortical morphology during pregnancy. To address these issues, AtlasSeg processed T2-weighted MRI and corresponding atlas in parallel using a dual-U-Net and integrated age-related prior information to aid the segmentation. Through dense multi-scale attention connections, AtlasSeg facilitated feature interactions between segmentation and supporting atlas branches. To demonstrate the efficacy of AtlasSeg, we focused on the segmentation of fetal brain cortex, whose segmentation presented significant challenges and held special importance in clinical diagnosis. Through comprehensive comparison and ablation experiments, we demonstrated that the integration of age-specific prior in AtlasSeg achieved superior segmentation performance compared to other CNNs.\nThere were several well-developed networks for medical image segmentation, such as U-Net and U-Net++. The fully convolutional operations with BN, non-linear activation and the encoder-decoder architecture in these networks made them a benchmark in the field of medical image segmentation. Other networks had incorporated attention mechanisms, such as Attention U-Net and SE-FCN, to improve the segmentation performance. Specifically, attention modules could generate attention maps, which were important for the interpretability of the segmentation outcome. However, these networks had not addressed the unique characteristics in fetal brain MRI segmentation, i.e., the segmentation performance is significantly affected by the changing anatomy during development and the segmentation accuracy was often GA-dependent. The end-to-end training manner in the current CNN framework did not incorporate fetal age information. Instead, they attempted to memorize the morphology of each training sample and implicitly learned the age-specific features.\nAtlasSeg was therefore proposed to explicitly encode the GA-specific information. At each GA, an atlas along with its label were inserted to provide guidance for the segmentation in terms of label location, cortical morphology, and tissue contrast. Upon the baseline U-Net as segmentation branch, AtlasSeg constructed an atlas branch with a similar structure, including the same convolution operations, stages, and feature map sizes. The main difference was that the base count of feature map in atlas branch was set to one-quarter of that in segmentation branch to reduce model complexity. After feature extraction, how to integrate the age-specific feature from the atlas branch into the segmentation branch was the core of the network. Lots of feature fusion approaches had been proposed, such as add fusion, late concatenation, spatial attention [46], channel attention [50], and self-attention [51]. In our work, we selected the Multi-scale Attentive Atlas Fusion (MA2-Fuse) as the interaction of two features, which was a combination of multi-scale spatial attention and late fusion. Multi-scale convolutions with a group of kernel sizes had been employed by many networks, including Inception [47], PSPNet [52], and MixAttNet [27]. This approach proved beneficial in our study, where the size of feature being fused varied across different stages. Specifically, in the shallower stages, larger features needed a combination of large and small receptive fields to extract contextual information at multiple scales. After attentive fusion, we further enhanced the atlas features by incorporating them into segmentation branch through late concatenation. The ablation results demonstrated the effectiveness of our proposed MA2-Fuse.\nWe compared AtlasSeg against six CNNs, on both an in-site fetal brain dataset and the inter-site FeTA dataset for OOD tests. As shown in Table I and Table II, AtlasSeg achieved superior performance in both scenarios. Visual comparison of segmentation results from various networks further demonstrated AtlasSeg's best performance, with the lowest levels of over- and under-segmentations. We also presented the feature maps and attention maps of AtlasSeg at the initial encoder and final decoder stages, illustrating the network's efficiency in capturing the edges of anatomical structures and its capability to delineate boundaries.\nThe present work still has several limitations. First, the core idea behind AtlasSeg makes it suitable for segmenting fetal brains with normal development. Handling of the fetal brains with prenatal disorders needs to be considered in future study. Moreover, providing GA-matched atlas to the network will introduce additional manual work, which needs to automated in the pipeline given a user input GA. The atlas branch and dense attentive connections also introduce additional network parameters and require excessive memory. Future work will focus on finding a more effective prior integration method, using multi-task learning [53] or multi-modal language-image model [54] to enhance the network's efficiency."}, {"title": "6. Conclusion", "content": "In this work, we presented AtlasSeg, an atlas prior guided convolutional neural network for accurate fetal brain MRI tissue segmentation. To handle fetal brains with various shapes and structures during pregnancy, AtlasSeg was designed with an incorporation of prior knowledge in the form of GA-specific atlases to provide contextual guidance. AtlasSeg had dual-U-Net architecture to process input MRI volumes and corresponding atlases, and also dense feature attentive fusion connections to facilitate a feature flow between two branches. AtlasSeg outperformed six SOTA CNNs for accurate cortical segmentation both in in-site and OOD experiments. Given its elevated accuracy, AtlasSeg stands as a promising tool for reliable fetal brain MRI tissue segmentation."}]}