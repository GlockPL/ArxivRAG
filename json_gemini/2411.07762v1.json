{"title": "ASER: Activation Smoothing and Error Reconstruction for Large Language Model Quantization", "authors": ["Weibo Zhao", "Yubin Shi", "Xinyu Lyu", "Wanchen Sui", "Shen Li", "Yong Li"], "abstract": "Quantization stands as a pivotal technique for deploying large language models (LLMs), yet it poses significant challenges particularly in achieving effective low-bit quantization. The limited numerical mapping makes the quantized model produce a non-trivial error, bringing out intolerable performance degradation. This paper is anchored in the basic idea of model compression objectives, and delves into the layer-wise error distribution of LLMs during post-training quantization. Subsequently, we introduce ASER, an algorithm consisting of (1) Error Reconstruction: low-rank compensation for quantization error with LoRA-style matrices constructed by whitening SVD; (2) Activation Smoothing: outlier extraction to gain smooth activation and better error compensation. ASER is capable of quantizing typical LLMs to low-bit ones, particularly preserving accuracy even in W4A8 per-channel setup. Experimental results show that ASER is competitive among the state-of-the-art quantization algorithms, showing potential to activation quantization, with minor overhead.", "sections": [{"title": "Introduction", "content": "The proliferation of large language models (LLMs) places increasing demands on computation and storage (Vaswani et al. 2017; Devlin et al. 2018), and stretches the boundaries of contemporary hardware prowess. Regarding typical generative models such as Llama3.1-310B (Llama Team 2024) or Qwen2-72B (Yang et al. 2024), they require hundreds of gigabytes of VRAM, and often rely on multi-GPU cluster data centers for deployment. Researchers have been trying to compress these large models while maintaining their performance in a more compact form. Quantization (Frantar et al. 2022; Xiao et al. 2023) of large language models emerges as a pivotal strategy in optimizing the deployment and computational efficiency of these heavy neural architectures.\nQuantization involves converting the model's weights and activations from high-precision floating-point numbers to lower-precision integers, whose primary goal is alleviating the computational and memory requirements without significantly compromising the model's performance, enabling edge deployment. Take post training quantization (PTQ) as an example, current advancements try to smooth (Xiao et al. 2023), rotate (Liu et al. 2024) or transform tensor representations in models, showing promise of achieving quantization-friendly data range and remarkable compression rate. However, when it comes to low-bit quantization, even with the application of the aforementioned techniques, the error introduced by the quantized model becomes evident, leading to notable degradation in the evaluation metrics.\nAs for typical quantization, the optimization objective can be formulated as minimizing $||WX-W_qX||_F$, where W and $W_q = Q(W)$ are original model weight and its quantized one respectively, and X is the activation. We empirically find this inevitable quantization error has low-rank property, which prompts us to consider using a method similar to LORA (Hu et al. 2021) to reconstruct this error. Specifically, the activation-weight quantization loss exhibits distinct low-rank property, with its singular value distribution featuring a small number of high values and a long-tail bulk of low ones, which varies both intra and inter Transformer blocks. Furthermore, we find the channels that produce the major error are consistent with the existence of the outliers.\nWe propose ASER (Activation Smoothing and Error Reconstruction), a low-rank compensation algorithm designed to enhance the efficacy of quantized models. Our method dynamically assesses the low-rank properties during the quantization process, whitens the relationship between quantization loss and singular values based on Cholesky factorization. Then, ASER identifies and smoothes these error-inducing outliers for separate compensation. Finally, we use LoRA-style skinny matrices to compensate the quantization error, with little computation overhead. The overall framework is shown in Figure 1. This lightweight yet powerful mechanism ensures robust performance of our quantized models, and this approach is orthogonal to any particular weight quantization method. Our experiments demonstrate that ASER remarkably recovers the quantization error, where W4A8 per-channel quantized model performs nearly equal capability related to the half-precision reference model. This work makes the following contributions:\n\u2022 We formulate the optimization objective of minimizing the discrepancy in model outputs, analyze the characteristics of quantization errors in LLMs quantization.\n\u2022 We propose a novel algorithm ASER, which includes error reconstruction using whitening SVD, and activation smoothing with outlier analysis.\n\u2022 Experimental results show ASER can significantly recover the performance of quantized model in W4A8 per-channel quantization, with little computational overhead."}, {"title": "Related Work", "content": "LLMS Compression and Quantization. In recent years, given the powerful emergent capabilities (Schaeffer, Miranda, and Koyejo 2024) of large language models (LLMs), there has been a growing number of model compression techniques specifically developed for Transformer-based models. Network pruning (Ma, Fang, and Wang 2023), including structured (Ma, Fang, and Wang 2023) and unstructured pruning (Kurtic et al. 2022), removes unnecessary neurons or structures to decrease model size, while distillation (Sun et al. 2020) directly uses a smaller model to learn the knowledge from the origin model. Though these conventional compression techniques have indeed yielded some benefits in LLMs, they pose challenges in deployment due to the modifications of model structure, and they often necessitate substantial consumption for retraining the models.\nPost-Training Quantization (PTQ) has emerged as a foundational technology for deploying large models, overcoming these limitations and significantly enhancing inference speed. GPTQ (Frantar et al. 2022), as an extension of OBQ (Frantar and Alistarh 2022), employs second-order methods to obtain a closed-form solution, theoretically ensuring the minimization of quantization error. AWQ (Lin et al. 2024) employs a scaling factor to reduce the quantization error of salient weights. But weight-only quantization methods do not consider decreasing the bit of activations to achieve further speedup. SmoothQuant (Xiao et al. 2023) uses empirical transformations to shift the quantization difficulty from activations to weights. However, it doesn't establish the relationship between transition matrix and quantization error, and pollutes the smoothness of weights.\nLow Rank Adapters of LLMs. The low-rank property is a prominent characteristic of LLMs, and researchers have attempted to structurally compress models based on this observation. Low-rank decomposition is a critical technique for compression, which has been widely utilized in LLMs (Wang et al. 2024). Linformer (Wang et al. 2020) introduces a low-rank attention mechanism, reducing the original O(N2) computational complexity. For decoder models, DeepSeek-V2 (DeepSeek-AI 2024) exploits the low-rank property of KV cache to adopt the MLA architecture, which effectively compresses the storage of the KV cache.\nAnother influential technique is LoRA (Hu et al. 2021), which fine-tunes large models, establishes a fundamental paradigm for large model fine-tuning. This approach of using low-rank matrices has been adopted across various domains. Recently, works like LoftQ (Li et al. 2023) and LQLORA (Guo et al. 2023) employ low-rank approximation to facilitate more efficient fine-tuning of lightweight models. Our focus lies in post-training quantization, aiming to maintain good performance when deploying low-bit models. LoRC (Yao et al. 2024) employs them to improve the quality recovery of quantized models. L2QER (Zhang et al. 2024) further scales the quantization error by empirically designed diagonal matrix before reconstruction. We are inspired by these LoRA-style method and propose a more compact quantization framework from the perspective of model compression. ASER designs whitening SVD for error reconstruction and smoothes activations via outlier analysis."}, {"title": "Analysis of Quantization Error", "content": "In this section, we formulate the quantization error from a model compression perspective, and anchor minimizing the quantization loss as the objective of our problem. We first analyze the properties of the quantization loss in the context of RTN (Round-To-Nearest) quantization, and then discuss how to reconstruct it with a small number of parameters."}, {"title": "Minimizing the Quantization Error", "content": "Tensors in quantized models have restricted range of representations, which brings out an unavoidable loss of dequantized data. In contrast to methods like LoRC that only account for the error arising from weight quantization, we aim to consider the integral error introduced by both the weights and the activations. Increasingly, quantization-related research (Xiao et al. 2023) acknowledges the impact of activations on quantization, primarily due to the critical role of outliers in the activations within LLMs (Tang et al. 2024). Thus, as a typical model compression, we formulate quantization objective optimization as:\n$\\arg \\min_{W_q} || WX \u2013 W_qX||_F,$\n(1)\nwhere $W_q = Q(W)$ is the quantized weights, and we use Frobenius norm to measure the magnitude of the error matrix. Here, we use $E_q = W \u2013 W_q$ to represent the quantization error of the weight matrix. We aim to estimate it with"}, {"title": "Characteristics of Quantization Error", "content": "We conduct empirical studies on Eq to guide our reconstruction strategy for the quantization error. We take Round-To-Nearest (RTN) quantization on LLaMA3-8B (Touvron et al. 2023) as an example to obtain the layer-wise distribution."}, {"title": "Activation Smoothing and Error Reconstruction", "content": "In this section, we introduce ASER, which consists of two main technologies. Error Reconstruction (ER) constructs the compensation matrices using a whitening method whose singular values are directly related to the integral error. Activation Smoothing (AS) analyzes outliers and transforms fluctuations in activation to weights, making low-rank compensation more effective."}, {"title": "Error Reconstruction via Whitening SVD", "content": "Firstly, we illustrate how to process quantization error using the whitening technique and establish the mapping between singular values and compression loss. To do this, we first obtain an orthogonal whitened activation $S^{-1}X$ where each channel is independent from each other:\n$(S^{-1}X)(S^{-1}X)^T = S^{-1}XX^T(S^{-1})^T = I,$\n(5)\nwhere S can be derived by Cholesky decomposition (Meyer 2000). Then we perform SVD on EqS, which produces $E_qS = U\\Sigma V^T$, where $U = [U_1, U_2, ..., U_n]$, $\\Sigma = diag(\\sigma_1, \\sigma_2, ..., \\sigma_n)$ and $V = [V_1, V_2, ..., V_n]^T$. Suppose we would approximate Eq by rank r, the smallest n \u2212 r singular values of \u2211 will be truncated to obtain the compressed Eq:\n$\\hat{E_q} = U_r\\Sigma_rV_r^TS^{-1}.$\n(6)\nNext, we provide a theoretical justification establishing the connection between singular values and error compensation, showing that the top singular values indeed contribute the most to mitigating the overall error.\nFor an arbitrary matrix $A \\in \\mathbb{R}^{m \\times m}$, its Frobenius norm can be deduced into the square root of the trace of its gram matrix (Horn and Johnson 1991):\n$||A||_F = \\sqrt{\\sum_{j=1}^m \\sum_{i=1}^n |A_{ij}|^2} = [Tr (A^TA)]^{\\frac{1}{2}},$\n(7)\nbased on which we can derive the approximation loss $L_i$ of $E_qX$ when truncating the ith singular value:\n$\\begin{aligned}L_i &= ||(E_q - \\hat{E_q})X ||_F \\\\&= ||\\sigma_i u_i v_i^TS^{-1}X||_F\\\\&\\stackrel{(i)}{=} \\sigma_i \\sqrt{Tr(u_i v_i^T S^{-1}XX^T(S^{-1})^T v_i u_i^T)} \\\\&\\stackrel{(ii)}{=} \\sigma_i \\sqrt{Tr(u_i u_i^T)} \\\\&\\stackrel{(iii)}{=} \\sigma_i,\\end{aligned}$\n(8)\nwhere (i) refers to Eq. 7, (ii) refers to Eq. 6 and (iii) equals since $v_i^Tv_i = u_i^Tu_i = 1; v_i^Tv_j = u_i^Tu_j = 0, \\forall i \\neq j; Tr(v_i^Tv_i) = Tr(u_i^Tu_i) = 1$.\nTherefore, $E_q$ can be replaced with a pair of LoRA-style matrices $L_A = U_r\\Sigma_r, L_B = V_r^TS^{-1}$ to compensate for the quantization error of the model. In addition to setting the same rank r for all layers, we use the ratio of the cumulative sum of sorted singular values to the total sum of singular values to set a threshold \u03b1 whose range is (0, 1).\n$\\max_r \\frac{\\sum_{i=1}^r \\sigma_i}{\\sum_{i=1}^n \\sigma_i} < \\alpha.$\n(9)"}, {"title": "Activation Smoothing via Outlier Analysis", "content": "Based on the empirical observations presented in the analysis section, we find that the outliers play a critical role in error reconstruction. Therefore, we devise a heuristic method to process these outliers. The activation outliers are extracted and migrated to the weights, after which the outlier parts of weights can be split and compensated along with the quantization error reconstruction.\nFirstly, inspired by SmoothQuant (Xiao et al. 2023), we use a scaling matrix M to migrate the quantization difficulty of the activation to weight:\n$WX = WMM^{-1}X,$\n(10)\nwhere $M = diag(m_1, m_2,...,m_n)$ is a specially designed matrix. Let $\\overline{X}$ and $\\overline{W}$ denote the absolute mean activation and weight value of each channel respectively. Suppose $I_f$ is a pre-defined hyper-parameter, we use $I_f = \\{i_1, i_2, i_3, ..., i_f\\}$ to represent the index set of $\\overline{X} \\overline{W}$ outliers. M can be obtained through the following formula:\n$m_i = \\begin{cases}\\overline{X_i}/\\overline{X_{\\min}} & i \\in I_f\\\\1 & else,\\end{cases}$\n(11)\nwhere $X_{\\min}$ is the minimum value in $\\overline{X}$. This makes the activation values easier to quantize and meanwhile allows us to use whitening SVD to uniformly treat and compensate for integral quantization error in the weight matrix:\n$\\begin{aligned}W_MM^{-1}X &= (W_s + W_o)M^{-1}X\\\\&= (Q(W_s) + (E_q + W_o))M^{-1}X.\\end{aligned}$\n(12)\nThe outlier part $W_o$ will be extracted from the adjusted weight $W_M$ based on the outlier index set $I_f$, and it will not be quantized. Together with the quantization errors from the rest of the weights, they are compensated by low-rank"}, {"title": "approximation matrices to minimize the loss incurred during the quantization process. Thus, we morph the objective of the whitening SVD into", "content": "$(E_q + W_o)$, and similar narrow matrices are used to approximate this error:\n$L_AL_B \\sim (E_q + W_o)S.$\n(13)\nNote that the error reconstruction technique is orthogonal to the weight-only quantization, which means ASER can be conducted based on the existing weight-only quantization methods, not limited to RTN. The pseudo-code of our algorithm is as follows:"}, {"title": "Experiments", "content": "This section presents experimental results by different PTQ methods on various model architectures. We further conduct visualization analysis to validate the effectiveness of the method, followed by an overhead analysis."}, {"title": "Experimental Setup", "content": "Models We consider quantization of typical open source large language models involving LLaMA3-8B (Touvron et al. 2023), Qwen1.5-7B, Qwen-72B (Bai et al. 2023). We obtain the pre-trained model from the official repository12.\nBaselines We consider several popular PTQ algorithms for comparison. The baseline approaches include act-and-weight quantization approaches LLM.int() (Dettmers"}, {"title": "Main Results", "content": "We demonstrate the main results of LLaMA3-8B and Qwen1.5-7B evaluation in Tables 5 and 2 respectively. The methods are divided into two parts, W4A8 and W4A6 quantization setups. We both test the perplexity and accuracy, and the best results are marked in bold. To ensure a fair comparison, we set the rank of compensation parameters in each layer of ASER, LORC, and L2QER to 64. The outlier threshold f is set by 32, which is equal in experimental setup.\nAs shown in the table 5, ASER achieves the best perplexity on language modeling tasks, while simultaneously performs competitive accuracy on language understanding. In both settings, ASER achieves perplexity improvement compared to the fp16 baseline ranging from 1.20 to 3.88. For the accuracy evaluation, particularly in the W4A6 per-channel setting, existing algorithms without error compensation, e.g., SmoothQuant and its variant, show significant performance degradation, while ASER outperforms its counterparts L2QER by 4.87%, which attributes to the significant role of activation smoothing.\nIn Table 2, for the W4A8 quantization, ASER shows competitive performance compared to the state-of-the-art method, with 3.12% close to the benchmark accuracy of the fp16 baseline averagely. Thanks to the activation smoothing, ASER achieves 5.09% higher accuracy compared to the one with only error reconstruction. For W4A6 quantization setup on language modeling datasets, as we can see, many existing methods fail to keep the performance without group-wise scaling. However, the quantized model using ASER achieves the perplexity that is 3.08 ~ 4.19 close to the fp16 baseline. Similarly, it also delivers the best accuracy on the commonsense evaluation dataset. Particularly, compared to L2QER, ASER performs better accuracy by 6.66%, which also far exceeds other act-and-weight quantization baselines."}, {"title": "Ablation Study and Analysis", "content": "Results of Larger Model Next, we validate the effectiveness of our method on the models with larger-scale. The model involved is Qwen-72B in Table 3. We mainly evaluate the setup of W4A8 per-channel quantization on commonsense benchmarks. As we can see, relying on our low-rank quantization error reconstruction algorithm, with further activation smoothing, ASER achieves an accuracy of 0.33% ~ 4.27%, which is close to the fp16 baseline. This demonstrates the scalability of our approach and its adaptability to larger models. Experimental results of more quantization setups and models and can be found in Appendix.\nEffect of Activation Smoothing As previously mentioned, ASER employs several techniques, including activation smoothing, migration of outliers, and low-rank estimation to compensate for integral quantization errors. These tech-"}, {"title": "Rank Selection", "content": "In the ASER framework, the choice of rank r, or its threshold \u03b1, influences the amount of quantization error compensation and the additional number of parameters introduced. Therefore, we conduct an experimental analysis on how the selection of rank affects downstream tasks. For Qwen1.5-7B, the performance of downstream tasks positively correlates with the selection of \u03b1, whereas this might not be the case for other models. As shown in Table 4, we find that using a larger rank is not necessarily better for all tasks in Qwen-7B. When we set smaller rank, we might achieve a better balance between performance of W4A8-quantized model and the extra parameter count."}, {"title": "Overhead Analysis", "content": "The proposed method can introduce additional computations and memory overheads due to the involvement of extra LoRA-style matrices LA, LB. Let s denote the sequence length, d the hidden dimension, r(\u226a d) the rank. The complexity analysis of a single layer is followed, where the computation represents floating-point operations (FLOPs), and the memory is measured by the number of parameters:\n$\\begin{array}{ccc} \text { Complexity } & \\text { Origin } & \\text { ASER } \\\\ \\hline \\text { Computation } & s d^{2} & s d^{2}+2 s r d \\\\ \\text { Memory } & d^{2} & d^{2}+2 r d \\end{array}$\nTaking Qwen-7B (s = 2048, d = 4096) as an example, we demonstrate the tradeoff of accuracy and the computational overhead among the listed configurations of Table 4. The choice can be deliberated based on the model generality or specific tasks. The additional parameter introduces minimum theoretical computational overhead of only 0.26%, which is minor to the main network inference. We leave it for future work to adaptively choose the best \u03b1."}, {"title": "Conclusion", "content": "In this work, from the perspective of model compression, we formulate and analyze the quantization error. Our study reveals that the error exhibits low-rank property, whose major components originate from the combined effect of outliers in both activations and weights. In response to these findings, we propose ASER, which utilizes whitening SVD to compensate for the integral error (Error Reconstruction), followed by analyzing and extracting outliers (Activation Smoothing). Experimental results demonstrate that ASER outperforms existing quantization algorithms on typical large language models in various quantization setups, with negligible theoretical overhead."}, {"title": "Appendix", "content": "Numerical Effect of Activation Smoothing\nThe idea of activation smoothing is to extract the outliers composed by both weights and activations, and transfer the difficulty of activation quantization, then the outliers part will be construct within the quantization error reconstruction. In Fig. 7, we demonstrate that, in the numerical perspective, the activation range is significantly smoothed, while the outliers transfered into the weight are extracted by $W_o$."}, {"title": "Effect of Rank Selection Strategy", "content": "As mentioned before, we observe the different effective dimentionality of quantization error across the layers in LLM. When it comes to the construction of LoRA-style matrix to compensate for the error, we design the strategy to dynamically select the rank configuration. As Fig. 8 shows, when using a ranging from 0.015 to 0.1, we employ different ranks across layers, whose trends are relatively consistent within the same layer across the Transformer blocks. The distribution also conforms to the effective rank in our analysis of quantization error."}, {"title": "More Evaluation Results", "content": "We extensively evaluate ASER on models of various scales and types, and on different datasets, to demonstrate its generalization ability. Specifically, we supplements the weight-only quantization on LLaMA3-8B (Table 5). Compared to the state-of-the-art approaches GPTQ and AWQ, ASER achieves competitive results both in causal language modeling perplexity and commonsense tasks accuracy. Similar results of weight-only quantization setup are shown in LLaMA2-13B (Table 6).\nNote that, with activation smoothing, the advantage of ASER lies in its quantization of activations. Therefore, we emphasize testing on act-and-weight quantization. Besides LLaMA2-13B (Table 6), larger models are involved in the evaluation. As for Qwen-14B (Table 7), Qwen1.5-32B (Table 8), ASER shows close average accuracy related to the fp16 baseline model in the W4A8 per-channel quantization setup."}]}