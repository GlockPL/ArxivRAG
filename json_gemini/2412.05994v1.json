{"title": "PIG: PHYSICS-INFORMED GAUSSIANS AS ADAPTIVE PARAMETRIC MESH REPRESENTATIONS", "authors": ["Namgyu Kang", "Jaemin Oh", "Youngjoon Hong", "Eunbyung Park"], "abstract": "The approximation of Partial Differential Equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accuracy due to the spectral bias of Multi-Layer Perceptrons (MLPs), which struggle to effectively learn high-frequency and non-linear components. Recently, parametric mesh representations in combination with neural networks have been investigated as a promising approach to eliminate the inductive biases of neural networks. However, they usually require very high-resolution grids and a large number of collocation points to achieve high accuracy while avoiding overfitting issues. In addition, the fixed positions of the mesh parameters restrict their flexibility, making it challenging to accurately approximate complex PDEs. To overcome these limitations, we propose Physics-Informed Gaussians (PIGs), which combine feature embeddings using Gaussian functions with a lightweight neural network. Our approach uses trainable parameters for the mean and variance of each Gaussian, allowing for dynamic adjustment of their positions and shapes during training. This adaptability enables our model to optimally approximate PDE solutions, unlike models with fixed parameter positions. Furthermore, the proposed approach maintains the same optimization framework used in PINNs, allowing us to benefit from their excellent properties. Experimental results show the competitive performance of our model across various PDEs, demonstrating its potential as a robust tool for solving complex PDES.", "sections": [{"title": "INTRODUCTION", "content": "Machine learning techniques have become promising tools for approximating solutions to Partial Differential Equations (PDEs) (Raissi et al., 2017; Yu et al., 2018; Karniadakis et al., 2021; Finzi et al., 2023; Gaby et al., 2024). A notable example is the Physics-Informed Neural Network (PINN) (Raissi et al., 2019), which leverages deep neural networks and gradient-based optimization algorithms. This approach circumvents the need for the time-intensive mesh design prevalent in numerical methods and allows us to solve both forward and inverse problems within the same optimization framework. With the increased computational power and the development of easy-to-use automatic differentiation software libraries (Abadi et al., 2015; Bradbury et al., 2018; Innes, 2018; Paszke et al., 2019), PINNs have successfully tackled a broad range of challenging PDEs Hu et al. (2024c); Li et al. (2024); Oh et al. (2024).\nAlthough the mesh-free neural network approach shows significant promise in solving PDEs, it has several limitations. Training PINNs typically requires numerous iterations to converge (Saarinen et al., 1993; Wang et al., 2021; De Ryck et al., 2023). Despite recent techniques aimed at reducing computational costs, multiple forward and backward passes of neural networks are still necessary to compute the PDE residual losses. Furthermore, obtaining more accurate approximations demands the use of wider and deeper neural networks, which enhances their expressiveness but significantly increases computational costs (Cybenko, 1989; Baydin et al., 2018; Kidger & Lyons, 2020). In addition, the inductive bias inherent in neural networks often hinders the accuracy of solution approximations. A well-known example is the spectral bias, which favors learning low-frequency components of solution functions and struggles to capture high-frequency or singular behaviors (Rahaman et al., 2019). Although some solutions to this issue have been proposed (Tancik et al., 2020; Sitzmann et al., 2020), eliminating inductive biases from neural networks remains a challenge.\nTo address these issues, recent studies have explored combining classical grid-based representations with lightweight neural networks (Hui et al., 2018; Cao et al., 2023). In this approach, the parametric grids map input coordinates to intermediate features, which are then processed by neural networks to produce the final solutions. By relying on high-resolution parametric grids for representational capacity, this method reduces the impact of neural networks' inductive biases. Additionally, using lightweight neural networks significantly reduces computational demands, leading to faster training speeds compared to traditional neural network-only methods.\nWhile promising, existing methods that combine parametric grids with neural networks face a fundamental challenge. The positions of the parameters (the locations of vertices) are predetermined by the grid resolutions and remain fixed during training. Since the optimal allocation of representational capacity (determining where to place more vertices) is unknown, these methods typically use high-resolution grids that uniformly distribute many vertices across the entire input domain to achieve more accurate solutions. This approach results in using a large set of learnable parameters, which often leads to overfitting issues, i.e., low PDE residual losses but inaccurate solutions. To mitigate this problem, a large number of collocation points are sometimes used during training at the expense of the increased computational costs.\nIn this work, we introduce a novel representation for approximating solutions to PDEs. Drawing inspiration from adaptive mesh-based numerical methods (Berger & Oliger, 1984; Seol et al., 2016) and the recent parametric grid representations (Li & Lee, 2021; Jang et al., 2023), we propose the Physics-Informed Gaussian (PIG) that learns feature embeddings of input coordinates, using a mixture of Gaussian functions. For a given input coordinate, PIG extracts a feature vector as the weighted sum of the feature embeddings held by Gaussians with their learnable parameters (positions and shapes). They are adjusted during the training process, and underlying PDEs govern this dynamic adjustment. To update the parameters of all Gaussians, we leverage the well-established PINNs training framework, which employs numerous collocation points to compute PDE residuals and uses gradient-based optimization algorithms.\nThe proposed approach offers several advantages over existing parametric grid methods. PIG dynamically adjusts the computational mesh structure and the basis functions (Gaussians) to learn the feature embeddings. By following the gradient descent directions, the Gaussians move towards regions with high residual losses or singularities, and this adaptive strategy allows for more efficient and precise solutions than the static uniform grid structures. In addition, Gaussian functions are infinitely differentiable everywhere, allowing for the convenient computation of higher-order gradients for PDE residuals, and they can be seamlessly integrated into deep-learning computation pipelines. The final architecture of the proposed approach, presented in 2-(c), that combines the learnable Gaussian feature embedding and the lightweight neural network is a new learning-based PDE solver that can provide more efficient and accurate solution approximations.\nWe have tested the proposed method on an extensive set of challenging PDEs (Krishnapriyan et al., 2021; Wang et al., 2024c; Cho et al., 2024). The experimental results show that the proposed PIG achieved competitive accuracy compared to the existing methods that use large MLPs or high-resolution parametric grids. When the number of Gaussians in our PIG model is comparable to the number of vertices in previous parametric grids, our method significantly outperformed existing approaches, demonstrating its superior efficiency. Furthermore, the proposed PIG shows significantly faster convergence speed than PINNs using large neural networks, demonstrating its effectiveness as a promising learning-based PDE solver. Our contributions are summarized as follows.\n\u2022 We introduce Physics-Informed Gaussians, an efficient and accurate PDE solver that utilizes learnable Gaussian feature embeddings and a lightweight neural network.\n\u2022 We propose a dynamically adaptive parametric mesh representation that effectively addresses the challenges encountered in previous static parametric grid approaches.\n\u2022 We demonstrate that our PIG model achieves competitive accuracy and faster convergence with fewer parameters compared to state-of-the-art methods, establishing its efficacy and paving the way for new research avenues."}, {"title": "RELATED WORK", "content": "PINNs are a class of machine learning algorithms designed to solve PDEs by integrating physical laws into the learning process. Introduced by Raissi et al. (2019), PINNs leverage neural networks to approximate the solutions of PDEs while ensuring that the learned solutions respect the underlying physics. This is achieved by incorporating the PDE residuals directly into the loss function, allowing the model to be trained using standard gradient-based optimization methods. PINNS have gained significant attention for their ability to handle high-dimensional (Wang et al., 2022b; Hu et al., 2024b;a) and complex problems (Yang et al., 2021; Pensoneault & Zhu, 2024) that are challenging for traditional numerical methods. They are particularly effective in scenarios where data is sparse or expensive to obtain, as they can incorporate prior knowledge about the physical system. Applications of PINNs span various domains, including fluid dynamics, solid mechanics, and electromagnetics, demonstrating their versatility and effectiveness in solving real-world problems (Cai et al., 2021; Khan & Lowther, 2022; Bastek & Kochmann, 2023). Key advantages of PINNs include their mesh-free nature, the ability to easily incorporate boundary and initial conditions, and their flexibility in handling various types of PDEs. However, they also face challenges, such as the need for extensive computational resources and the difficulty in training deep networks to achieve accurate solutions. For example, Wang et al. (2024b) typically uses around 9 hidden layers with 256 hidden units (sometimes up to 18 layers) to achieve high accuracy. This requires massive computations to run the neural network, which involves multiple forward and backward passes to compute the gradients for PDE residual loss. Furthermore, it slows down the convergence speed due to the large number of model parameters."}, {"title": "PHYSICS-INFORMED PARAMETRIC GRID REPRESENTATIONS", "content": "Physics-informed parametric grid representations combine traditional grid-based methods with neural networks to solve PDEs (Kang et al., 2023; Huang & Alkhalifah, 2024; Wang et al., 2024a; Shishehbor et al., 2024a). These representations have also been extensively explored in image, video, and 3D scene representations (Liu et al., 2020; Yu et al., 2021; Fridovich-Keil et al., 2022; M\u00fcller et al., 2022; Chen et al., 2022; Sun et al., 2022; Fridovich-Keil et al., 2023) by training the models as supervised regression problems. By discretizing the solution domain into a grid and associating each grid point with trainable parameters, these methods leverage the structured nature of grids to capture spatial variations effectively. This hybrid approach maintains high accuracy and reduces computational costs compared to purely neural network-based methods. Key benefits include the ability to handle high-resolution representations and integrate boundary conditions efficiently. However, the fixed grid structure can lead to suboptimal allocation of representational capacity during training. Despite this limitation, physics-informed parametric grid representations are promising for achieving accurate solutions in complex scenarios."}, {"title": "ADAPTIVE MESH-BASED METHODS", "content": "Adaptive mesh-based methods dynamically adjust the computational mesh to minimize the error between approximated and true solutions. This process involves a posteriori error analysis, which estimates errors after solving, allowing for targeted mesh refinement. Such adaptivity is crucial in the numerical analysis as it ensures efficient allocation of computational resources, focusing on regions with high errors and thus improving overall accuracy and efficiency (Ainsworth & Oden, 1993; 1997).\nThere are also some studies on non-uniform adaptive sampling methods in the context of PINNs. Lu et al. proposed a residual-based adaptive refinement method in their work with DeepXDE, aiming to enhance the training efficiency of PINNs (Lu et al., 2021; Wu et al., 2023). More recently, Yang et al. (2023b) introduced Dynamic Mesh-based Importance Sampling (DMIS), a novel approach that constructs a dynamic triangular mesh to efficiently estimate sample weights, significantly improving both convergence speed and accuracy. Similarly, Yang et al. (2023a) developed an end-to-end adaptive sampling framework called MMPDE-Net, which adapts sampling points by solving the moving mesh PDE. When combined with PINNs to form MS-PINN, MMPDE-Net demonstrated notable performance improvements. While these adaptive methods offer significant benefits, they also introduce additional complexity into the PINN framework."}, {"title": "POINT-BASED REPRESENTATIONS", "content": "Irregular point-based representations have long been considered promising approaches for representing, reconstructing, and processing data (Qi et al., 2017; Xu et al., 2022; Zhang et al., 2022). A recent study in 3D scene representation utilized Gaussians as a graphical primitive and showed remarkable performance in image rendering quality and training speed (Kerbl et al., 2023). The combination of Gaussian representation and neural networks has recently been explored in regressing images or 3D signed distance functions, showing its great expressibility (Chen et al., 2023). While those studies share some architectural similarities with our method, they all primarily focus on supervised regression problems to reconstruct the visual signals. We developed the architecture suitable for effective PDE solvers and first showed that the Gaussian features and neural networks can be trained in an unsupervised manner guided by the physical laws."}, {"title": "METHODOLOGY", "content": null}, {"title": "PRELIMINARY: PHYSICS-INFORMED NEURAL NETWORKS", "content": "Consider an abstract underlying equation,\n$D[u](x) = f(x), x\\in\\Omega\\subset\\mathbb{R}^d,$\n$B[u](x) = g(x), x \\in \\partial\\Omega,$\nwhere D is a differential operator, and B is a boundary operator which could contain the initial condition. The physics-informed neural network methods try to find an approximate solution by minimizing\n$\\mathcal{L}(\\theta) = \\int_{\\Omega} |D[u_{\\theta}](x) - f(x)|^2 dx + \\lambda \\int_{\\partial \\Omega} |B[u_{\\theta}](x) - g(x)|^2 d\\sigma(x)$"}, {"title": "PHYSICS-INFORMED GAUSSIANS", "content": "In this section, we present the proposed Physics-Informed Gaussian representation (PIG) for approximating solutions to PDEs. It comprises two stages: Gaussian feature embeddings (3.2.1) and solution approximation based on these features (3.2.2)."}, {"title": "LEARNABLE GAUSSIAN FEATURE EMBEDDING", "content": "Let $\\phi = \\{(\\mu_i, \\Sigma_i, f_i) : i = 1, ..., N\\}$ be the set of Gaussian model parameters, where $\\mu_i \\in \\mathbb{R}^d$ is a position of a Gaussian, and $\\Sigma_i \\in \\mathbb{S}_{++}^d$ is a covariance matrix and each Gaussian has a learnable feature embedding $f_i \\in \\mathbb{R}^k$. Given an input coordinate $x \\in \\mathbb{R}^d$, the learnable Gaussian feature embedding $FE_{\\phi} : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ is extracted as follows.\n$FE_{\\phi}(x) = \\sum_{i=1}^N f_i G_i(x), G_i(x) = e^{-(x-\\mu_i)^T \\Sigma_i^{-1} (x-\\mu_i)},$\nwhere N is the number of Gaussians and $G_i$ represents the i-th Gaussian function. $FE_{\\phi}$ maps an input coordinate to a feature embedding by a weighted sum of the individual features $f_i$ of each Gaussian. To enhance the expressive capability, we can use different Gaussians for each feature dimension. Further details are provided in Appendix A.1.\nGaussian features distant from the input coordinates do not contribute to the final feature embedding, while only neighboring Gaussian features remain significant. Similar to the previous parametric grid methods, which obtain feature embeddings by interpolating only neighboring vertices, this locality encourages the model to capture high-frequency details by effectively alleviating spectral bias.\nAll Gaussian parameters $\\phi$ are learnable and iteratively updated throughout the training process. This dynamic adjustment, akin to adaptive mesh-based numerical methods, optimizes the structure of the underlying Gaussian functions to accurately approximate the solution functions. For example, the regions with high-frequency or singular behaviors require more computational parameters, and Gaussians, updated based on the gradients, will migrate to these regions to reduce the loss (see Figure 1). Compared to the existing parametric grid approaches, which achieve this goal by uniformly increasing grid resolution, the proposed method can build a more parameter-efficient and optimal mesh structure."}, {"title": "SOLUTION APPROXIMATION WITH LEARNABLE GAUSSIANS FOLLOWED BY A LIGHTWEIGHT NEURAL NETWORK", "content": "Once the features are extracted, a neural network processes the feature to produce the solution outputs.\n$u_{\\phi,\\theta}(x) = NN_{\\theta}(FE_{\\phi}(x)),$\nwhere $NN_{\\theta}$ is a small and lightweight MLP with the parameter $\\theta$. We employed a single hidden layer MLP with a limited number of hidden units, resulting in negligible additional computational costs."}, {"title": "PIG AS A NEURAL NETWORK", "content": "The proposed Gaussian feature embedding admits a form of radial basis function network. The first layer contains N (the number of Gaussians) RBF units, and an input coordinate passes through all RBF units, $G_i(x)$, resulting in a N-dimensional vector. A single fully connected layer processes this vector to produce a k-dimensional feature vector. The weight matrix $W \\in \\mathbb{R}^{k \\times N}$ in this layer corresponds to the feature vectors held by each Gaussian, i.e., $W_{:,i} \\in \\mathbb{R}^k$ equals $f_i \\in \\mathbb{R}^k$.\nThe extracted feature vector is further processed by a single hidden layer MLP (we used the tanh activation function) to produce the final output, as depicted in Figure 3. Overall, the proposed PIG architecture can be interpreted as an MLP with one input layer with N RBF units and two hidden layers (no activation for the first hidden layer, and tanh for the second hidden layer)."}, {"title": "UNIVERSAL APPROXIMATION THEOREM FOR PIGS", "content": "Here, we present the Universal Approximation Theorem (UAT) for PIGs. A PIG consists of two functions: FE and $NN_{\\theta}$ (see equation 5). We will prove the UAT only for FE, as the UAT for PIGs follows directly from the standard UAT for MLPs. Given our earlier discussion on the relationship between PIGs and radial basis function networks, we begin with the following UAT specific to radial basis function networks.\nLet $K : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be an integrable bounded function such that K is continuous and\n$\\int_{\\mathbb{R}^d} K(x) dx \\neq 0.$\nThen the family $S_K$, defined as linear combinations of translations of K,\n$S_K = \\{ \\sum_{i=1}^n f_i K(x - \\mu_i) | f_i \\in \\mathbb{R}, \\mu_i \\in \\mathbb{R}^d, n \\in \\mathbb{N} \\}$\nis dense in $C(\\mathbb{R}^d)$.\nHowever, Theorem 1 does not apply to PIGs, as the feature embedding $FE_{\\phi}$ in PIGs takes a slightly different form:\n$FE_{\\phi}(x) = \\sum_{i=1}^n f_i K (x - \\mu_i; \\Sigma_i),$"}, {"title": "EXPERIMENTS", "content": null}, {"title": "EXPERIMENTAL SETUP", "content": "To validate the effectiveness of our proposed method, We conducted extensive numerical experiments on various challenging PDEs, including Allen-Cahn, Helmholtz, Nonlinear Diffusion, Flow Mixing, and Klein-Gordon equations (For more experiments, please refer to the Appendix). We used the Adam optimizer (Kingma & Ba, 2014) for all equations except for the Helmholtz equation, in which the L-BFGS optimizer (Liu & Nocedal, 1989) was applied for a fair comparison to the baseline method PIXEL. For computational efficiency, we considered a diagonal covariance matrix $\\Sigma = diag(\\sigma_1,...,\\sigma_d)$ and we will discuss more in Section 4.3.3."}, {"title": "EXPERIMENTAL RESULTS", "content": null}, {"title": "(1+1)D ALLEN-CAHN EQUATION", "content": "We compared our method against one of the state-of-the-art PINN methods on the Allen-Cahn equation, JAX-PI (Wang et al., 2023). For the detailed description, please refer to Appendix A.2.1. As shown in Figure 4, our method converges significantly faster and achieves competitive final accuracy (see Table 1). JAX-PI used a modified MLP architecture and 4 hidden layers with 256 hidden neurons. Thus, the number of parameters in JAX-PI is more than 250K, while ours used only around 20K parameters (($N, d, k$) = (4000,2,1)). Also, note that the $L^2$ error curve in Figure 4 is displayed per iteration, and computational costs per iteration of ours are significantly lower than JAX-PI, which requires multiple forward and backward passes of the wide and deep neural network."}, {"title": "2D HELMHOLTZ EQUATION", "content": "Figure 5 illustrates the numerical performance of our proposed PIG method for the 2D Helmholtz equation, comparing it to PIXEL (Kang et al., 2023), one of the state-of-the-art methods within the PINN family that uses parametric grid representations. A more detailed description of the experimental setup is available in Appendix A.2.2. The experiments were conducted using three different seeds, with PIG achieving the best relative $L^2$ error of 2.22 \u00d7 10\u22125 when employing the L-BFGS optimizer, and a relative $L^2$ error of 2.50 \u00d7 10\u20134 with the Adam optimizer (For fair comparison, we reported the result using L-BFGS since PIXEL used L-BFGS). Notably, the results show that PIG's error is four times lower than that of PIXEL, highlighting the efficiency and accuracy of our method. We did not compare against other state-of-the-art methods, such as JAX-PI or Pirate-Net, as they did not conduct experiments in this setting. While we could have used their codes, the sensitivity of PINN variants to hyperparameters complicates fair comparisons."}, {"title": "(2+1)D KLEIN-GORDON EQUATION", "content": "Figure 6 presents the predicted solution profile for the Klein-Gordon equation, comparing our results with SPINN. The best relative $L^2$ error achieved is 2.36 \u00d7 10-3, which outperforms SPINN by an order of magnitude. For further details, please refer to Appendix A.2.3."}, {"title": "(2+1)D NONLINEAR DIFFUSION EQUATION", "content": "We evaluated the performance of PIGs on the (2+1) dimensional nonlinear diffusion equation, with visualizations presented in Figure 18. The relative $L^2$ error achieved is 1.44 \u00d7 10\u22123. For details on the experimental setup, please refer to Appendix A.2.5."}, {"title": "(2+1)D FLOW-MIXING PROBLEM", "content": "Figure 7 displays the numerical solutions and absolute errors for the (2+1) flow mixing problem. Our solutions closely match the reference, with PIG achieving a maximum absolute error of 5.03 \u00d7 10-3, compared to 2.63 \u00d7 10-1 for SPINN, underlining the superior performance of PIG. Figure 19 presents solution profiles up to t = 4. Additional details can be found in Appendix A.2.4."}, {"title": "HYPERPARAMETER ANALYSIS AND ABLATION STUDY", "content": "In this section, we present the experimental results to show the effects of each component of the proposed PIG (Using MLP, learnable Gaussian positions, and dense covariance matrices), In addition, We study the effect of the number of Gaussians, the size of MLP and input dimensions."}, {"title": "THE NUMBER OF GAUSSIANS", "content": "In numerical analysis, there is a general trend that the quality of the solution improves as the mesh is refined. Given our approach of using Gaussians as mesh points, we expect that the accuracy of PIGs will improve with an increased number of Gaussians."}, {"title": "MLP IMPACT AND ADAPTIVE GAUSSIAN POSITIONS", "content": "While $FE_{\\phi}$ serves as a universal approximator, we found that adding a small MLP $NN_{\\theta}$ significantly enhances performance. Additionally, our ablation study explores the effectiveness of allowing adaptive Gaussian positions (learnable \u00b5 vs. fixed \u03bc)."}, {"title": "COVARIANCE MATRICES", "content": "Dense covariance matrices can represent the most general form of Gaussians, but they are more computationally expensive than diagonal covariance matrices. We compared these two types of covariance matrices across several equations: the 2D Helmholtz equation, the Klein-Gordon equation, the Flow-Mixing equation, and the Nonlinear-Diffusion equation. Despite the increased number of network parameters and generality associated with dense matrices, both dense and diagonal covariance matrices yielded similar error levels."}, {"title": "CONCLUSION AND LIMITATIONS", "content": "In this work, we introduced PIGs as a novel method for approximating solutions to PDEs. By leveraging explicit Gaussian functions combined with deep learning optimization, PIGs address the limitations of traditional PINNs that rely on MLPs. Our approach dynamically adjusts the positions and shapes of the Gaussians during training, overcoming the fixed parameter constraints of previous methods and enabling more accurate and efficient approximations of complex PDEs. Experimental results demonstrated the superior performance of PIGs across various PDE benchmarks, showcasing their potential as a robust tool for solving high-dimensional and nonlinear PDEs.\nDespite the promising results, our approach has certain limitations that warrant further investigation. Firstly, the dynamic adjustment of Gaussian parameters introduces additional computational overhead. While this improves accuracy, it may also lead to increased training times, particularly for very large-scale problems. However, by leveraging the locality of Gaussians, we can limit the evaluations to nearby Gaussians, which reduces the necessary computations and saves GPU memory. Secondly, the number of Gaussians is fixed at the beginning of training. Ideally, additional Gaussians should be allocated to regions requiring more computational resources to capture more complex solution functions. We believe it is a promising research direction and leave it to future work. Finally, a complete convergence analysis of the proposed method is not yet available. While empirical results show improved accuracy and efficiency, a theoretical understanding of the convergence properties would provide deeper insights and guide further enhancements."}, {"title": "REPRODUCIBILITY", "content": "We are committed to ensuring the reproducibility of our research. All experimental procedures, data sources, and algorithms used in this study are clearly documented in the paper. We already submitted the codes and command lines to reproduce the part of the results in Table 1 as supplementary materials. The code and datasets will be made publicly available upon publication, allowing others to validate our findings and build upon our work."}, {"title": "ETHICS STATEMENT", "content": "This research adheres to the ethical standards required for scientific inquiry. We have considered the potential societal impacts of our work and have found no clear negative implications. All experiments were conducted in compliance with relevant laws and ethical guidelines, ensuring the integrity of our findings. We are committed to transparency and reproducibility in our research processes."}, {"title": "ENHANCED GAUSSIAN FEATURE EMBEDDING", "content": "To enhance the expressive capability, different Gaussians can be used for each feature dimension.\nThe learnable Gaussian feature embedding $FE(\\textbf{x}; \\phi) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ and the set of Gaussian model parameters $\\phi = \\{(\\mu_i, \\Sigma_i, f_i) : i = 1, ..., N\\}$ are defined as previously described. Then, we have different Gaussians for each feature dimension, where $\\mu_i \\in \\mathbb{R}^{k \\times d}$ is the Gaussian position parameters, $\\mu_{i,j} \\in \\mathbb{R}^d$ denotes the position parameter for j-th feature dimension, and $f_{i,j} \\in \\mathbb{R}$ represents j-th feature value. Similarly, $\\Sigma_{i,j} \\in \\mathbb{S}_{++}$ is a covariance matrix for j-th feature dimension. Given an input coordinate $\\textbf{x} \\in \\mathbb{R}^d$, j-th element of the learnable Gaussian feature embedding $FE_j(\\textbf{x}; \\phi)$ is defined as follows,\n$FE_j(\\textbf{x}; \\phi) = \\sum_{i=1}^N f_{i,j} G_{i,j}(\\textbf{x}), G_{i,j}(\\textbf{x}) = e^{-(\\textbf{x}-\\mu_{i,j})^T \\Sigma_{i,j}^{-1} (\\textbf{x}-\\mu_{i,j})},$\nwhere $G_{i,j}$ is the Gaussian function using Gaussians parameters for j-th feature dimension."}, {"title": "DETAILED DESCRIPTION OF EXPERIMENTS", "content": null}, {"title": "(1+1)D ALLEN-CAHN EQUATION", "content": "The Allen-Cahn equation is a one-dimensional time-dependent reaction-diffusion equation that describes the evolutionary process of phase separation, which reads\n$u_t = 0.0001u_{xx} + 5u - 5u^3 = 0, (x, t) \\in [-1, 1] \\times [0, 1],$\nwith the periodic boundary condition\n$u(-1,t) = u(1,t), u_x(-1,t) = u_x(1,t).$\nThe initial condition for the experiment was $u(x,0) = x^2 cos(\\pi x)$. We used the NTK-based loss balancing scheme (Wang et al., 2022a) to mitigate the ill-conditioned spectrum of the neural tangent kernel (Jacot et al., 2018). We used N = 4000 Gaussians for training and a diagonal covariance matrix for parameter efficiency, where the diagonal elements of the initial $\\Sigma$ were set to a constant value of 0.025. The $\\mu_i$ was uniformly initialized following Uniform[0, 2]$^2$. We used shallow MLP with one hidden layer with 16 hidden units, and the dimension of the Gaussian feature was k = 1.\nReference solution was generated by Chebfun (Driscoll et al., 2014), which utilizes the Fourier collocation method with N = 4096 Fourier modes with ETDRK4 time stepping (Kassam & Trefethen, 2005) with a fixed time step $\\Delta t = 1/200$."}, {"title": "2D HELMHOLTZ EQUATION", "content": "The Helmholtz equation is the eigenvalue problem of the Laplace operator $\\Delta - k^2$. We consider the manufactured solution\n$u(x, y) = sin(\\alpha_1 \\pi x) sin(\\alpha_2 \\pi y), (\\alpha_1, \\alpha_2) = (4, 1),$\nto the two-dimensional Helmholtz equation with the homogeneous Dirichlet boundary condition given by\n$\\Delta u - k^2 u = q, (x, y) \\in [-1,1]^2, k = 1,$\nwhere\n$q(x, y) = k^2 sin(\\alpha_1 \\pi x) sin(\\alpha_2 \\pi y) - (\\alpha_1 \\pi)^2 sin(\\alpha_1 \\pi x) sin(\\alpha_2 \\pi y) - (\\alpha_2 \\pi)^2 sin(\\alpha_1 \\pi x) sin(\\alpha_2 \\pi y)$\nWe used N = 3000 Gaussians in this experiment. The weights and scales of Gaussians were initialized following Uniform[-1,1] and 0.1, respectively. The feature size of Gaussians was fixed at 4. The shallow MLP has 16 hidden nodes, and its network parameters were initialized by Glorot normal. The inputs for the Gaussians were rescaled into [0, 1]$^2$, therefore the positions were initialized following Uniform[0, 1]$^2$."}, {"title": "(2+1)D KLEIN-GORDON EQUATION", "content": "The Klein-Gordon equation is a relativistic wave equation, which predicts the behavior of a particle at high energies. We consider the manufactured solution\n$u(x, y,t) = (x + y) cos(2t) + xy sin(2t)$"}, {"title": "(2+1)D FLOW-MIXING PROBLEM", "content": "A mixing procedure of two fluids in a two-dimensional spatial domain could be described in the following equation\n$u_t + a u_x + b u_y = 0, (x, y, t) \\in [-4, 4]^2 \\times [0, 4],$\n$a(x,y) = \\frac{u_t}{\\sqrt{u_t,max} r'}, b(x,y) = \\frac{u_t}{\\sqrt{u_t,max} r'},$\n$u_t = sech^2(r)tanh(r),$\n$r = \\sqrt{x^2 + y^2}, u_{t,max} = 0.385.$\nThe analytic solution is u(x, y,t) = - tanh (cos(wt) - sin(wt)), where w = vt/(rvt,max); see e.g., Tamamidis & Assanis (1993). The initial condition can be extracted from the analytic solution.\nTo predict the solution to the PDE, we used N = 4000 Gaussians. The weights and scales were initialized to Normal(0, 0.012) and 0.1, respectively. The size of Gaussian features was fixed at 4. MLP had 16 hidden nodes, and its parameters were initialized by Glorot normal. Inputs for the Gaussians were rescaled to [0, 2]$^3$, hence the positions of Gaussians were initialized following Uniform [0, 2]$^3$."}, {"title": "(2+1)D NONLINEAR DIFFUSION EQUATION", "content": "The diffusion equation is a parabolic PDE describing the diffusion process of a physical quantity, such as heat. We consider a nonlinear diffusion equation for our benchmark, which reads\n$u_t = 0.05 (||\\nabla u||\u00b2 + u\\Delta u), (x, y,t) \\in [-1,1", "1": "n$u_0 (x) = 0.25g (x; 0.7, 0.3, \\frac{1}{\\sqrt{10}}) + 0.4g (x; -0.1, -0.5, \\frac{1}{\\"}]}