{"title": "Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "authors": ["Kaiwen Wang", "Rahul Kidambi", "Ryan Sullivan", "Alekh Agarwal", "Christoph Dann", "Andrea Michi", "Marco Gelmi", "Yunxuan Li", "Raghav Gupta", "Avinava Dubey", "Alexandre Ram\u00e9", "Johan Ferret", "Geoffrey Cideron", "Le Hou", "Hongkun Yu", "Amr Ahmed", "Aranyak Mehta", "L\u00e9onard Hussenot", "Olivier Bachem", "Edouard Leurent"], "abstract": "Reward-based finetuning is crucial for aligning language policies with intended behaviors (e.g., creativity and safety). A key challenge here is to develop steerable language models that trade-off multiple (conflicting) objectives in a flexible and efficient manner. This paper presents Conditioned Language Policies (CLP), a general framework for finetuning language models on multiple objectives. Building on techniques from multi-task training and parameter-efficient finetuning, CLP can learn steerable models that effectively trade off conflicting objectives at inference time. Notably, this does not require training or maintaining multiple models to achieve different trade-offs between the objectives. Through an extensive set of experiments and ablations, we show that the CLP framework learns steerable models that outperform and Pareto-dominate the current state-of-the-art approaches for multi-objective finetuning.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) finetuning is a crucial step for training language models (LMs) with intended behaviors [43] and human-aligned etiquette [4], with applications in summarization [35, 47], conversational agents [23] and encoding social norms [3]. In modern applications, RL finetuning is often a multi-objective problem [13] due to the diversity of human preferences (e.g., succinctness vs. detail, factuality vs. creativity) and of applications (e.g., summarization, coding, dialog). Since standard RL algorithms can only maximize a scalar reward function, it is common practice to linearly combine the multiple, often conflicting objectives with carefully tuned weightings that represent the relative importance of each reward [1, 3]. The reward weightings are critical for model quality and are often identified via multiple expensive finetuning runs.\nTo address the limitations of single-objective finetuning (SOFT), multi-objective finetuning (MOFT) learns a multi-objective LM (see Figure 1 left) that can be steered to generate desirable outputs over the continuum of possible reward weightings, without any retraining [27]. More specifically, a multi-objective LM takes a desired reward weighting at inference time and responds with outputs where this weighted combination of rewards is maximized (up to KL-regularization). A steerable, multi-objective LM also enables an interactive decision support system: instead of providing generations from the \"best\" LM picked by the model developer, a decision support system provides multiple diverse generations that cover a wide space of interests and lets the user pick their favorite [29].\nIn the context of LMs, MOFT has been explored via prompt-based approaches [12, 17, 38] and parameter-based approaches [17, 27]. Prompt-based approaches finetune a LM that is steered by simply including the reward weightings into the prompt. However, prompt-based MOFT is sub-optimal in steerability as we show in our experiments and can be sensitive to how weightings are encoded in the prompt. An alternative parameter-based approach, Rewarded Soups (RS) [27], (a) independently trains one LM per reward function, and (b) interpolates parameters from separate LMs with reward weightings to perform conditional generation at inference-time. Perhaps surprisingly, this zero-shot approach can effectively trade-off multiple rewards by relying on the linear mode connectivity [9]. However, we find that zero-shot MOFT is sub-optimal in performance on intermediate weightings that it hasn't seen during training.\nThis paper presents Conditional Language Policy (CLP), a general MOFT framework that employs parameter-space conditioning and multi-task training [7]. Using parameter-conditioning from RS, CLP is consistently more steerable than purely prompt-based approaches. Moreover, by finetuning on a diverse set of reward weightings, CLP produces higher quality responses than zero-shot approaches like RS while having comparable or better steerability. We conduct a systematic set of experiments, observing that CLP both Pareto-dominates RS and is more steerable than prompt-based MOFT (see Figure 1 right). We find that CLP robustly maintains the above benefits across many experimental conditions, including various choice of rewards and model sizes. We also conduct an automated evaluation with Gemini 1.0 Ultra [36] that further reinforces the fact that CLP is more steerable and generates higher quality responses than existing baselines. Finally, we provide novel theory proving that zero-shot methods can display near-Pareto-optimal behavior when optimal policies for individual rewards are aligned, but they provably fail otherwise. In those failure cases, multi-task training (as used by CLP) is necessary to learn to Pareto-optimal policy.\nIn summary, our contributions are:\n1. We propose CLP, a general framework for MOFT that learns multi-objective LMs through multi-task learning and parameter efficient model averaging (Section 3).\n2. We extensively evaluate CLP on summarization and show it robustly improves existing approaches in both output quality and steerability, across many experimental conditions and automated evaluator evaluations (Section 4)."}, {"title": "2. Problem Setup", "content": "Let $\\pi_{ref}(y | x)$ be a base policy with parameters $\\theta_{ref}$, where $x$ is the input prompt and $y$ is the output generation. In single-objective finetuning (SOFT), there is a fixed reward function $R(x, y)$ and the goal is to maximize the expected reward without drifting too far from $\\pi_{ref}$. Formally, SOFT learns a policy $\\pi_{\\theta}(\\cdot)$ to maximize the following value:\n$V_{\\alpha,R}(\\pi) := E_{x~D,y~\\pi(x)} [(1 - \\alpha)R(x, y) - \\alpha KL(\\pi(\\cdot | x) || \\pi_{ref}(\\cdot | x))]$,\nwhere $\\alpha \\in (0, 1)$ is a fixed, KL weighting that controls the deviation from $\\pi_{ref}$ as measured by the KL-divergence. SOFT can be solved by standard RL algorithms such as REINFORCE [40] or PPO [32]. Notice that the KL-term can be thought of as a special reward function to mitigate reward-hacking [35]. The main issue with SOFT, as mentioned earlier, is that both the reward function R and the KL weightings are fixed and unchangeable after training; thus, it is not possible to offer near-optimal behavior on other reward and KL weightings without retraining SOFT with multiple reward or \u03b1-values.\nMulti-objective finetuning (MOFT) solves the above issues by learning a conditional policy that generates y based not only on the prompt x, but also the \u03b1 and R specified at test-time without retraining. As standard in multi-objective RL [29], let $R(s, a) \\in \\mathbb{R}^m$ be a reward vector whose linear scalarizations {$w^T R(\\cdot) : w \\in \\Delta^m$} capture all desired trade-offs at test-time, where $\\Delta^m$ is the (m \u2013 1)-dimensional simplex, i.e., R is a basis for all desired rewards at test-time. The goal is to learn parameters $\\phi$ such that, for all weightings $\\alpha \\in [\\alpha_{min}, 1]$ and $w \\in \\Delta^m$, the conditioned policy $\\pi_{\\phi}(\\cdot; \\alpha,w)$ maximizes $V_{\\alpha,w^TR}$, the objective with KL-regularizer \u03b1 and reward function $w^T R(\\cdot)$. We frame MOFT as multi-task training over the weighting distribution Q, and aim to maximize:\n$V_{moft}(\\phi) = E_{(\\alpha,w)~Q}[V_{\\alpha,w^TR}(\\pi_{\\phi}(\\cdot; \\alpha,w))]$.\nIn theory, the optimal solution to multi-objective RL is a large (potentially exponential in m) set of policies called the convex coverage set [29]. However, the rich representational power of a single LLM may already be able to approximate such a policy cover and thus we aim to solve MOFT with a memory-efficient parameterization \u03c6.\nMOFT Desiderata \u2013 Pareto-dominance & steerability. A multi-objective LM $\\pi$ Pareto-dominates another $\\pi'$ if $V_{\\alpha,w^TR}(\\pi(\\cdot; \\alpha, w)) \\geq V_{\\alpha,w^TR}(\\pi'(\\cdot; \\alpha, w))$ for all values of \u03b1, w that one cares about. New MOFT algorithms should ideally Pareto-dominate existing baselines to ensure that generation quality is improved along all axes. Steerability is another important goal for MOFT algorithms. In Figure 1, full-CLP and prompting both satisfy the first goal of Pareto-dominance, but full-CLP is desirable since its Pareto-curve has much better spread, i.e., it is more steerable.\nNotation. $\\Theta$ refers to LM parameters, while $\\phi$ refers to CLP parameters (different structure from \u03b8), which can be conditioned on (\u03b1, w) to produce a conditioned LM parameter $\\theta_{\\alpha,w}$ (same structure as \u03b8). $\\Theta[S]$ or $\\Theta_S$ refer to the subset of parameters indexed by S. We use \u2295 to combine disjoint parameter subsets, i.e., $\\theta = \\Theta[S^C] \\oplus \\Theta[S]$. We assume $\\theta$ and $\\theta_{ref}$ have the same structure."}, {"title": "3. Conditional Language Policy (CLP)", "content": "This section presents the Conditional Language Policy (CLP) framework for MOFT, enabling a family of algorithms with varying trade-offs between quality (in terms of Pareto-dominance and steerability) and cost (in terms of parameter-count). In brief, CLP learns a set of parameters $\\phi$ that can be processed into a conditioned LM for any given weighting across rewards and KL, via a parameter-averaging mechanism described in Section 3.1. The learning algorithm samples a diverse set of weightings to push out its Pareto-front over all weightings simultaneously. Notably, this is multi-task learning across the continuum of weightings, which directly maximizes the MOFT objective defined in Equation (2), unlike existing zero-shot approaches [17, 27].\nWe now describe the CLP algorithm (Algorithm 1, illustrated in Figure 10), where each training round $t = 1, 2, . . ., T$ consists of three steps. First, we sample a prompt $x_t$ and reward & KL weightings $w_t, a_t ~ Q$ for this round (Line 3). Second, we condition CLP on weightings $(a_t, w_t)$ and sample generations $y_t ~ \\pi_t(x_t; a_t, w_t)$ (Line 5). Third, we compute the conditioned objective (Line 6) and update the CLP parameters with policy gradient (Line 7). This final policy optimization step uses gradient ascent to maximize the objective in Equation (2) and can be implemented by any standard policy gradient approach such as REINFORCE [2, 40], PPO [32], RLOO [15, 18], and also DPO [24] when given preference data."}, {"title": "3.1. Conditioning Mechanism", "content": "We now describe the parameter-based mechanism for computing conditional policies $\\pi(\\cdot; \\alpha,w)$ as described in Algorithm 2. Let S denote an index set on the LM parameters that we wish to use for parameter-conditioning [27]. In brief, S indexes parameters of the LM, such as attention weights, and its choice can trade-off the steerability and memory cost of CLP. Then, CLP maintains (1) m sets of conditioned parameters {$\\Theta_S^i$}$_{i\\in[m]}$ indexed by S, and (2) one set of unconditioned parameters $\\Theta_{S^c}$. To condition the S-part on (a,w), we linearly combine the m conditioning parameters with weightings w, and we then combine the result with $\\Theta_{ref}[S]$ weighted by $f_{mix}(a)$ (Equation (3)). Then, concatenate the conditioned part $\\Theta_S^{\\alpha,w}$ with the unconditioned part to obtain the full LM parameters $\\Theta_{\\alpha,w} = \\Theta_S^{\\alpha,w} \\oplus \\Theta_{S^c}$. The parameter count of CLP is thus O(m|S| + |Sc|). We remark that inference with CLP only requires one forward pass through the LM and the above parameter-averaging cost is amortized since it is done only once at the beginning. See Appendix A.3 for details on gradient propagation through condtioning.\nPrompt-based conditioning: One can also augment the prompt with the reward weightings (Line 3 in Algorithm 2); see also Appendix A.2 for more details on prompt-design. Prompting based MOFT has been explored in recent manuscripts [12, 17, 38] and has the advantage of not requiring additional parameters, but consumes part of the context and is sensitive to how these are encoded in the context. In Section 4.2.2, we consider augmenting parameter-space conditioning with prompting."}, {"title": "3.2. Three Instantiations of CLP", "content": "The choice of S influences both the steerability and memory usage of CLP. On one extreme, the most steerable and high parameter count choice is to condition on all LM parameters and we call this full-CLP, i.e., $S_{full}$ = {indices of all LM parameters}. This instance is inspired by model soups [42]. On the other extreme, logit-CLP only conditions on the final linear layer (a.k.a. logit layer), i.e., $S_{logit}$ = {indices of last linear layer of LM}. This instance is theoretically grounded [20] but we found it to have inferior steerability. Finally, a great middle ground for transformer LMs is to condition on the attention parameters and we call this attn-CLP, i.e., $S_{attn}$ = {indices of attention layers of LM}. attn-CLP is more parameter-efficient and nearly as steerable as full-CLP in our experiments. We highlight that CLP is agnostic to the LM architecture and S can be set appropriately for other models.\nIn our experiments, we observe that the more expressive parameterizations of S (e.g., $S_{full}$ and $S_{attn}$) robustly lead to Pareto-dominating and highly steerable behaviors than existing baselines such as Rewarded Soups. We remark that expressivity is determined not just by the number of parameters in S but also where those parameters are in the LM (e.g. earlier vs. later layers)."}, {"title": "4. Experiments", "content": "We consider the following questions:\n\u2022 Benchmarking: How do different methods perform in terms of performance (ability to push out the Pareto Front) and steerability (ability to generate content that trades-off different objectives)?\n\u2022 Ablations: How does the behavior of different approaches vary as a function of (a) number of finetuning steps, (b) model size? Furthermore, is parameter space conditioning composable with prompting based methods?\n\u2022 Automated Evaluations: Going beyond Pareto fronts, we present automated evaluations that compare generations from CLP against baselines by having Gemini [36] rate the summaries in terms of quality and steerability.\nData and models. A majority of our experiments/ablations are performed using summarization with the widely-used XSum dataset [21]. We initialize the reference policy $\\pi_{ref}$ and reward models from the instruction finetuned (FLAN) checkpoints for T5 [5]. We use the large size (770M parameters) for reward models and we mostly use the base size (220M parameters) for policies, except in our model size ablation where we also use large size for policies. For policy optimization, we use REINFORCE with control variate, which is a lighter implementation than PPO [32] and has been successfully used for summarization [31].\nReward functions. We consider three reward functions: (1) ROUGE, a formulaic reward that measures similarity of generation to the ground truth summary [19]; (2) natural language inference (NLI), a learnt reward model for textual entailment and factuality [22, 31]; and (3) a reward model for summary quality learnt from the \"too long; didn't read\" (TLDR) dataset [35]. ROUGE and the quality model (referred to as TLDR) tend to favor verbose and descriptive summaries while NLI favors concise summaries; this gives our setup a distinct tension between various reward pairs.\nMethods. We benchmark the three instances of CLP in Section 3.2: full-CLP, attn-CLP, logit-CLP. As a gentle reminder, full-CLP maintains one full LM per reward, akin to standard model soups; attn-CLP maintains replicas of all attention layers thus being more parameter efficient; and logit-CLP maintains replicas only the logit layer which is theoretically grounded but less expressive.\nBaselines. We consider (a) Rewarded Soups (RS) [27] which independently trains one policy per reward and linearly interpolates the parameters with weightings at inference time \u2013 this is a \u2018zero-shot' version of full-CLP; (b) a prompting baseline [12, 17], which encodes the reward weightings into the prompt and is trained with our multi-task objective \u2013 for details on the prompt and how it was selected, see Appendix A.2. For the \"single-reward, multi-KL\" setting (Section 4.1.1), instead of RS, we consider (c) the recent Decoding-time Realignment (DeRA) [20], which maintains two LMs (an LM optimized for $\\alpha_{min}$ and reference LM $\\pi_{ref}$) and linearly interpolates their logits.\nAll methods and baselines are run for same number of training iterations. See Appendix B for related details and hyper-parameters.\nNote. In addition to our main experimental results and ablations, we also run benchmarking experiments with the TLDR dataset [35, 37] to see if the general observations translate to other datasets/tasks as we scale up policy (to T5-xl size) and reward model sizes (to T5-xxl size). See section Section 4.4 for more details."}, {"title": "4.1. Core Benchmarking Results", "content": "4.1.1. Single Reward, Multi KL Regularizer\nIn the first setting, we fix a single reward function and vary the KL regularizer \u03b1 to test the KL-reward trade-off. We use the reward $R = 0.9R_{nli} + 0.1R_{rouge}$, where $R_{rouge}$ is mixed in to mitigate reward hacking the NLI model. In Figure 2(a), we see that all methods except logit-CLP are able to evenly trade-off reward and KL, enabling a smooth transition from $\\pi_{ref}$ (when \u03b1 = 1) to a maximally finetuned model (when \u03b1 = 0.01). This suggests that parameter-mixing trained with the multi-task objective is competitive with the baseline DeRa, which is state-of-the-art for reward-KL trade-off. Moreover, Figure 2(b) shows that CLP is also ~ 2\u00d7 more computationally efficient than DeRa at inference-time, because DeRa performs two LM calls (both $\\pi_{ref}$ and $\\pi_{\\alpha_{min}}$) per token. Hence, inference speed is a major benefit of parameter-mixing over logit-ensembling.\n4.1.2. Two Rewards, Fixed KL Regularizer\nHere, we consider a pair of rewards with a fixed KL regularizer, to test the trade-off between both rewards. Figure 3 shows the Pareto curves for (a) NLI v. Rouge and (b) NLI v. TLDR, where the x,y-axes are the KL-regularized rewards, i.e., $x = V_{\\alpha,R_1}(\\pi(\\cdot; \\alpha, w))$, $y = V_{\\alpha,R_2}(\\pi(\\cdot; \\alpha,w))$, which recall is the true objective being maximized by finetuning (Equation (1)). We see that CLP and prompting largely Pareto-dominate the baseline RS, which shows the benefits of multi-task training compared to RS's zero-shot approach. full-CLP and attn-CLP both exhibit Pareto-fronts that are more steerable (evenly spaced and spread out) than those of logit-CLP and prompting, which largely exhibit a mode-collapsed behavior. Importantly, while efficiently replicating only the attention weights, attn-CLP can Pareto-dominate the baseline RS while maintaining steerable Pareto-curves. Thus, attn-CLP offers the best trade-off between steerability and parameter count. See Appendix C.2 for more results.\n4.1.3. Three Rewards, Fixed KL Regularizer\nWe now consider the three-reward setting to test the trade-off between all reward functions. Figure 4 plots the KL-regularized value $V_{\\alpha,w^TR}(\\pi(\\cdot; \\alpha, w))$, normalized w.r.t. RS, for 13 different reward weightings w shown below the x-axis. At the extreme weights w = ei, i \u2208 {1, 2, 3}, RS is equivalent to single-objective finetuning and naturally outperforms CLP and prompting as expected. At the intermediate weights $w \\neq e_i$, full-CLP consistently outperforms RS. We find that attn-CLP is competitive to full-CLP despite incurring only 20% of the memory overhead. Thus, akin to the two-reward setting, attn-CLP achieves the best balance in terms of steerability and parameter count. For additional three reward results, see Appendix C.3.\nSummary of core benchmarking results. In terms of performance, we find that multi-task training enables CLP to improve over the zero-shot RS baseline. Importantly, we find that full-CLP and attn-CLP robustly maintain a steerable Pareto-front that is more spread out than logit-CLP and prompting baseline. In sum, attn-CLP presents a favorable trade-off in terms of Pareto-front and steerability, while using fewer parameters than existing baselines."}, {"title": "4.2. Ablation Studies", "content": "4.2.1. Effect of Training Iterations\nFigure 5 shows the progression of Pareto-curves over 90k training steps for CLP instances. full-CLP and attn-CLP are steerable after just 10k steps. In contrast, prompting becomes steerable at 60k steps; logit-CLP has a similar trend of being not as steerable at earlier training iterations. Furthermore, the optimization dynamics of full-CLP and attn-CLP are much smoother than prompting and logit-CLP. Complete results are presented in Appendix C.4.\n4.2.2. CLP With Prompt Conditioning\nFigure 6 shows the Pareto-fronts of full-CLP, the prompting baseline, and full-CLP with prompting (CONDPROMPT = TRUE). For NLI v. Rouge, full-CLP with prompting showed a slight improvement in steerability, whereas there was little difference for NLI v. TLDR. Hence, prompt conditioning does not hurt performance but can be slightly beneficial. See Appendix C.5 for more results.\n4.2.3. Model Size\nWe rerun our experiments with T5-small (60M) and T5-large (770M) to check how our findings change with different model sizes. Figure 7 shows the NLI v. TLDR Pareto-fronts. We see that full-CLP and attn-CLP still robustly Pareto-dominate the RS baseline and maintain steerable fronts. Interestingly, prompting collapses to a point for T5-small but has much better spread for T5-large, suggesting that prompting is sensitive to model size and a larger model can improve the steerability of prompting. We provide results for other reward pairs in Appendix C.6.\nSummary of ablations. While combining prompting with CLP did not significantly improve steerability, prompting may exhibit more steerability with larger models or training time. Across different settings, full-CLP and attn-CLP consistently maintain their superior performance and steerability suggesting they are robust conditioning architectures for MOFT when combined with multi-task training."}, {"title": "4.3. Automated Evaluation", "content": "In order to understand if CLP's improved Pareto-fronts translate to improvements in generations compared to baselines (prompting and Rewarded Soups), we conduct an automated evaluation of generation quality and steerability. For this evaluation, we consider the NLI v. TLDR setup with T5-large models (from Section 4.2.3) and use 2000 articles from the XSum validation set. We utilize Gemini 1.0 Ultra [36] as an automated evaluator to compare summaries from CLP instances to each baseline in terms of their conciseness and summary quality. Specifically, for each article, we sample conditioned summaries from both CLP and a baseline on weightings w = (0.8, 0.2) for high NLI (resp. weightings w = (0.2, 0.8) for high TLDR) and we ask the automated evaluator to compare which summary is more concise (resp. has higher quality). We permute the comparison order to account for position bias [39], marking a comparison as consistent if both permutations agree \u2013 please see Appendix D.1 for details. Then, for each article, we consider an algorithm to be the winner (i.e., more steerable) if either the auto evaluator prefers its summary in both comparisons, or the auto evaluator prefers its summary in one case and was inconsistent in the other. If neither algorithm is the winner, we consider it a tie. With this setup in place, Figure 8 shows the win-rate across different CLP variants against the two baselines. We observe that full-CLP (and attn-CLP) offers 11.6% (and 16.5%) improvement in raw win rates compared to the multi-task trained prompting baseline, and 4.9% (and 9.5%) improvements over RS. logit-CLP tends to fair comparably to prompting while being inferior to RS (dropping win rate by 3.1%). Notably, attn-CLP and full-CLP achieve the best win-rate relative to both baselines in this automatic evaluation, with attn-CLP having an additional desirable property of being more parameter-efficient. In sum, our automatic evaluation is consistent with prior Pareto-front results and validates that CLP produces higher quality multi-objective LMs with superior steerability both quantitatively and qualitatively."}, {"title": "4.4. Scaling up CLP with Larger Policy and Reward Models", "content": "In the following experiment, we perform multi-reward finetuning on the TLDR dataset [35, 37] and increase the model size of both the policy and reward models. Namely, we train two reward models with T5-XXL (11B parameters), one for factuality (which we call NLI) and one for summary quality (which we call TLDR) where we adopt the same reward model training procedure as in [8]. For our policy model, we use T5-XL (3B parameters) initialized from the FLAN checkpoint [5] and all methods were trained for 6000 steps. We keep other configurations unchanged from the two-reward experiments on XSum (e.g., Section 4.1.2). We repeat this experiment for three sampling distributions and show these results in Figure 9. We observe that attn-CLP and full-CLP both largely Pareto-dominate the baselines while maintaining smoothly steerable behaviors Pareto-curves. logit-CLP collapses to a small region likely due to its representational bottleneck. We also observe that the steerability of the learned policy may improve as the weight sampling distribution becomes more narrow, though this may come at a cost of less Pareto-optimality. Finally, we have observed this trend repeatedly across different model sizes, datasets and sampling distributions, which gives credence to the robustness and reliability of CLP for multi-objective finetuning."}, {"title": "5. Related Works", "content": "Approaches for multi-reward alignment (or MOFT) can be broadly classified into two categories: prompt-based and parameter-based conditioning. Prompt-based conditioning approaches include Personalized Soups [17] which use hand-crafted prompts for personalizing LMs based on binary weights on different rewards; CPO [12] which employs prompting within a DPO framework; RiC [44], DPA [38] use prompting within a supervised finetuning setup that differs from this paper which focuses on RL finetuning. On the parameter-conditioning front, Rewarded Soups (RS) [27] presents a zero-shot approach (i.e. without multi-task training) to multi-reward alignment by inference time averaging of parameters for LMs that are independently trained to optimize each of the rewards. A more recent manuscript [46] presents an approach where the reward weightings are embedded as singular values within the AdaLoRA framework [14, 45]; this can be framed as an instance of the proposed CLP framework. With regards to KL realignment, decoding time realignment (DeRa) [20] linearly mixes logits between $\\pi_{ref}$ and another LM learned via SOFT with the minimum KL weight $\\alpha_{min}$. A recent manuscript Shi et al. [34] showed that this idea is also effective for trading off multiple rewards. Finally, model souping [41, 42], learning policy sub-spaces [6, 10], and objective weight conditioning [7] have been applied in domains beyond LMs. We leverage these advances along with multi-task training to develop steerable LMs at inference time."}, {"title": "6. Theory for Logit Mixing and CLP", "content": "In this section, we perform a sensitivity analysis for logit mixing, a zero-shot method for MOFT, and derive regret bounds for its Pareto-front. While CLP uses parameter mixing instead of logit mixing, this analysis is instructive due to the similarity between parameter and ensemble mixing [26, 42].\n6.1. Sensitivity Analysis for Logit Mixing\nWe focus on the \u201ctwo-reward, fixed a\u201d setting for simplicity and our analysis can be extended to the general case. For any \u03bb \u2208 [0, 1], let $\u03b6_{\u03bb}$ be the logits of the optimal policy $\\pi_{\\lambda}$ = arg max\u03c0 $V_{\\alpha,w^TR}(\\pi)$ for weightings w = [1 \u2013 \u03bb, \u03bb]. Via the analytical solution of KL-regularized reward maximization, Liu et al. [20] observed that the optimal logits at a is expressible as mixture of the optimal logits for each individual reward, i.e., $\u03b6_\u03bb = (1 \u2212 \u03bb)\u03b6_0 + \u03bb\u03b6_1$. Thus, given optimal policies for $R_1$ & $R_2$, logit-mixing provides a zero-shot way to compute the optimal policy at any intermediate \u03bb\u2208 [0, 1].\nHowever, in practice, we of course are not given optimal policies and only have access to e-approximations, so it is important to understand the sensitivity of logit-mixing. We now bound the sub-optimality of logit-mixing in terms of \u025b and a concentrability coefficient that measures policy coverage, defined as $C_{\\pi_1,\\pi_2} := maxx,y \\pi_2(y | x)/\\pi_1(y | x)$, i.e. what is the least overlap in terms of ratio of probabilities of each policy (one per reward) over the actions (for e.g. tokens).\nTheorem 1. Suppose $\\pi_1$, $\\pi_2$ are \u025b-optimal policies for Equation (1) with $R_1, R_2$, respectively. For any \u03bb\u03b5 [0, 1], let $\\tilde{\\pi_{\\lambda}}$ be the logit mixture of $\\tilde{\\pi_1}$ and $\\tilde{\\pi_2}$. Then, the sub-optimality of $\\tilde{\\pi_{\\lambda}}$ is bounded by:\n$O(((1 - \u03bb)C_{\u03c0_1,\u03c0_2}^{\u03c0_2} + \u03bbC_{\u03c0_1,\u03c0_2}^{\u03c0_1} + \\frac{\\varepsilon}{p_{min}}) \\cdot \u03b5)$,\nwhere $p_{min}$ is the minimum probability of $\\pi_1$, $\\pi_2$ over all input-outputs (x, y) we may care about.\nThe proof is in Appendix E. The coverage terms however can be infinite if the policies $\\pi_1$, $\\pi_2$ don't cover each other. A zero-shot method will be robust to approximations when expert policies for each individual reward cover each other. But, as we show next, zero-shot approaches will fail when this coverage condition doesn't exist anymore."}, {"title": "6.2. Counterexample for zero-shot MOFT", "content": "Logit mixing cannot induce new behaviors since it can only mix behaviors from the two extremes, and so if an intermediate weighting requires a new behavior, logit mixing provably fails. Consider a toy problem with one context and three possible outputs $Y_1, Y_2, y_3$ with rewards $R_1(x, \\cdot) = (1,0,0.75), R_2(x, \\cdot) = (0, 1, 0.75)$. The optimal policies for $R_1, R_2$ (with \u03b1 = 0) are $\\pi_1^{\\dagger}(x) = (1, 0, 0)$ and $\\pi_2^{\\dagger}(x) = (0,1,0)$. However, $\\pi_{0.5} = (0, 0, 1)$, which is a qualitatively new behavior that cannot arise from zero-shot logit mixing thus being a failure case for zero-shot logit mixing. In appendix Figure 19, we show that the RS baseline, which is zero-shot empirically fails to learn the Pareto-optimal policy in this example, while CLP which uses multi-task training succeeds."}, {"title": "7. Conclusion", "content": "We introduced CLP, a flexible framework for MOFT that leverages techniques from multi-task training and parameter efficient finetuning to develop steerable LMs that adapt their generations to produce near Pareto optimal behavior across different weightings of individual rewards. We provide extensive benchmarking and ablations to better understand factors that enable the development of steerable LMs within the CLP framework. We supplement this with theoretical results that present conditions under which zero shot approaches work and when multi-task training is necessary to obtain near-optimal behavior. In terms of future directions, (a) understanding other conditioning mechanisms such as soft tokens [16], (b) automated tuning of the weight sampling distributions [11], (c) addressing non-linear reward scalarization [30] are questions of natural interest."}, {"title": "Limitations", "content": "This paper develops a framework for multi-objective finetuning and proposes variants that work out-of-the-box and are robust across different ablations, which are accompanied by auto-evaluations that present credence to these claims. We acknowledge that reward models are approximations to human interpretations of language. When considering applications of CLP for other problem setups, it is fairly likely that additional evaluations including human evals, red-teaming and other strategies need to be considered to mitigate any risks posed by a more flexible LM. The specific issues that one would encounter when pursuing extensions"}]}