{"title": "Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "authors": ["Kaiwen Wang", "Rahul Kidambi", "Ryan Sullivan", "Alekh Agarwal", "Christoph Dann", "Andrea Michi", "Marco Gelmi", "Yunxuan Li", "Raghav Gupta", "Avinava Dubey", "Alexandre Ram\u00e9", "Johan Ferret", "Geoffrey Cideron", "Le Hou", "Hongkun Yu", "Amr Ahmed", "Aranyak Mehta", "L\u00e9onard Hussenot", "Olivier Bachem", "Edouard Leurent"], "abstract": "Reward-based finetuning is crucial for aligning language policies with intended behaviors (e.g., creativity and safety). A key challenge here is to develop steerable language models that trade-off multiple (conflicting) objectives in a flexible and efficient manner. This paper presents Conditioned Language Policies (CLP), a general framework for finetuning language models on multiple objectives. Building on techniques from multi-task training and parameter-efficient finetuning, CLP can learn steerable models that effectively trade off conflicting objectives at inference time. Notably, this does not require training or maintaining multiple models to achieve different trade-offs between the objectives. Through an extensive set of experiments and ablations, we show that the CLP framework learns steerable models that outperform and Pareto-dominate the current state-of-the-art approaches for multi-objective finetuning.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) finetuning is a crucial step for training language models (LMs) with intended behaviors [43] and human-aligned etiquette [4], with applications in summarization [35, 47], conversational agents [23] and encoding social norms [3]. In modern applications, RL finetuning is often a multi-objective problem [13] due to the diversity of human preferences (e.g., succinctness vs. detail, factuality vs. creativity) and of applications (e.g., summarization, coding, dialog). Since standard RL algorithms can only maximize a scalar reward function, it is common practice to linearly combine the multiple, often conflicting objectives with carefully tuned weightings that represent the relative importance of each reward [1, 3]. The reward weightings are critical for model quality and are often identified via multiple expensive finetuning runs.\nTo address the limitations of single-objective finetuning (SOFT), multi-objective finetuning (MOFT) learns a multi-objective LM (see Figure 1 left) that can be steered to generate desirable outputs over the continuum of possible reward weightings, without any retraining [27]. More specifically, a multi-objective LM takes a desired reward weighting at inference time and responds with outputs where this weighted combination of rewards is maximized (up to KL-regularization). A steerable, multi-objective LM also enables an interactive decision support system: instead of providing generations from the \"best\" LM picked by the model developer, a decision support system provides multiple diverse generations that cover a wide space of interests and lets the user pick their favorite [29].\nIn the context of LMs, MOFT has been explored via prompt-based approaches [12, 17, 38] and parameter-based approaches [17, 27]. Prompt-based approaches finetune a LM that is steered by simply including the reward weightings into the prompt. However, prompt-based MOFT is sub-optimal"}, {"title": "2. Problem Setup", "content": "Let $\\pi_{ref}(y | x)$ be a base policy with parameters $\\theta_{ref}$, where x is the input prompt and y is the output generation. In single-objective finetuning (SOFT), there is a fixed reward function R(x, y) and the goal is to maximize the expected reward without drifting too far from $\\pi_{ref}$. Formally, SOFT learns a policy $\\pi_{\\theta}(\\cdot)$ to maximize the following value:\n$V_{\\alpha,R}(\\pi) := E_{x~D,y~\\pi(x)} [(1 - \\alpha)R(x, y) \u2013 \\alpha KL(\\pi(\\cdot | x) || \\pi_{ref}(\\cdot | x))]$,\nwhere $\\alpha \\in (0, 1)$ is a fixed, KL weighting that controls the deviation from $\\pi_{ref}$ as measured by the KL-divergence. SOFT can be solved by standard RL algorithms such as REINFORCE [40] or PPO [32]. Notice that the KL-term can be thought of as a special reward function to mitigate reward-hacking [35]. The main issue with SOFT, as mentioned earlier, is that both the reward function R and the KL weightings are fixed and unchangeable after training; thus, it is not possible to offer near-optimal behavior on other reward and KL weightings without retraining SOFT with multiple reward or $\\alpha$-values.\nMulti-objective finetuning (MOFT) solves the above issues by learning a conditional policy that generates y based not only on the prompt x, but also the $\\alpha$ and R specified at test-time without retraining. As standard in multi-objective RL [29], let R(s, a) $\\in \\mathbb{R}^m$ be a reward vector whose linear scalarizations {$w^T R(\\cdot) : w\\in \\Delta^m$} capture all desired trade-offs at test-time, where $\\Delta^m$ is the (m \u2013 1)-dimensional simplex, i.e., R is a basis for all desired rewards at test-time. The goal is to learn parameters $\\phi$ such that, for all weightings $\\alpha \\in [\\alpha_{min}, 1]$ and w $\\in \\Delta^m$, the conditioned policy $\\pi_{\\phi}(\\cdot; \\alpha,w)$ maximizes $V_{\\alpha,w^T R}$, the objective with KL-regularizer $\\alpha$ and reward function $w^T R(\\cdot)$. We frame MOFT as multi-task training over the weighting distribution Q, and aim to maximize:\n$V_{moft}(\\phi) = E_{(\\alpha,w)~Q}[V_{\\alpha,w^T R}(\\pi_{\\phi}(\\cdot; \\alpha,w))]$.\nIn theory, the optimal solution to multi-objective RL is a large (potentially exponential in m) set of policies called the convex coverage set [29]. However, the rich representational power of a single LLM may already be able to approximate such a policy cover and thus we aim to solve MOFT with a memory-efficient parameterization $\\phi$.\nMOFT Desiderata \u2013 Pareto-dominance & steerability. A multi-objective LM $\\pi$ Pareto-dominates another $\\pi'$ if $V_{\\alpha,w^T R}(\\pi(\\cdot; \\alpha, w)) \\geq V_{\\alpha,w^T R}(\\pi'(\\cdot; \\alpha, w))$ for all values of $\\alpha$, w that one cares about. New MOFT algorithms should ideally Pareto-dominate existing baselines to ensure that generation quality is improved along all axes. Steerability is another important goal for MOFT algorithms. In Figure 1, full-CLP and prompting both satisfy the first goal of Pareto-dominance, but full-CLP is desirable since its Pareto-curve has much better spread, i.e., it is more steerable."}, {"title": "3. Conditional Language Policy (CLP)", "content": "This section presents the Conditional Language Policy (CLP) framework for MOFT, enabling a family of algorithms with varying trade-offs between quality (in terms of Pareto-dominance and steerability) and cost (in terms of parameter-count). In brief, CLP learns a set of parameters $\\phi$ that can be processed into a conditioned LM for any given weighting across rewards and KL, via a parameter-averaging mechanism described in Section 3.1. The learning algorithm samples a diverse set of weightings to push out its Pareto-front over all weightings simultaneously. Notably, this is multi-task learning across the continuum of weightings, which directly maximizes the MOFT objective defined in Equation (2), unlike existing zero-shot approaches [17, 27].\nWe now describe the CLP algorithm (Algorithm 1, illustrated in Figure 10), where each training round t = 1, 2, . . ., T consists of three steps. First, we sample a prompt $x_t$ and reward & KL weightings $w_t, \\alpha_t ~ Q$ for this round (Line 3). Second, we condition CLP on weightings ($at, wt$) and sample generations $yt ~ \\pi_t(xt; at, wt)$ (Line 5). Third, we compute the conditioned objective (Line 6) and update the CLP parameters with policy gradient (Line 7). This final policy optimization step uses gradient ascent to maximize the objective in Equation (2) and can be implemented by any standard policy gradient approach such as REINFORCE [2, 40], PPO [32], RLOO [15, 18], and also DPO [24] when given preference data."}, {"title": "3.1. Conditioning Mechanism", "content": "We now describe the parameter-based mechanism for computing conditional policies $\\pi(\\cdot; \\alpha,w)$ as described in Algorithm 2. Let S denote an index set on the LM parameters that we wish to use for parameter-conditioning [27]. In brief, S indexes parameters of the LM, such as attention weights, and its choice can trade-off the steerability and memory cost of CLP. Then, CLP maintains (1) m sets"}, {"title": "3.2. Three Instantiations of CLP", "content": "The choice of S influences both the steerability and memory usage of CLP. On one extreme, the most steerable and high parameter count choice is to condition on all LM parameters and we call this full-CLP, i.e., Sfull = {indices of all LM parameters}. This instance is inspired by model soups [42]. On the other extreme, logit-CLP only conditions on the final linear layer (a.k.a. logit layer), i.e., Slogit = {indices of last linear layer of LM}. This instance is theoretically grounded [20] but we found it to have inferior steerability. Finally, a great middle ground for transformer LMs is to condition on the attention parameters and we call this attn-CLP, i.e., Sattn = {indices of attention layers of LM}. attn-CLP is more parameter-efficient and nearly as steerable as full-CLP in our experiments. We highlight that CLP is agnostic to the LM architecture and S can be set appropriately for other models.\nIn our experiments, we observe that the more expressive parameterizations of S (e.g., Sfull and Sattn) robustly lead to Pareto-dominating and highly steerable behaviors than existing baselines such as Rewarded Soups. We remark that expressivity is determined not just by the number of parameters in S but also where those parameters are in the LM (e.g. earlier vs. later layers)."}, {"title": "4. Experiments", "content": "We consider the following questions:\n\u2022 Benchmarking: How do different methods perform in terms of performance (ability to push out the Pareto Front) and steerability (ability to generate content that trades-off different objectives)?\n\u2022 Ablations: How does the behavior of different approaches vary as a function of (a) number of finetuning steps, (b) model size? Furthermore, is parameter space conditioning composable with prompting based methods?\n\u2022 Automated Evaluations: Going beyond Pareto fronts, we present automated evaluations that compare generations from CLP against baselines by having Gemini [36] rate the summaries in terms of quality and steerability.\nData and models. A majority of our experiments/ablations are performed using summarization with the widely-used XSum dataset [21]. We initialize the reference policy $\\pi_{ref}$ and reward models from the instruction finetuned (FLAN) checkpoints for T5 [5]. We use the large size (770M parameters) for reward models and we mostly use the base size (220M parameters) for policies, except in our model size ablation where we also use large size for policies. For policy optimization, we use REINFORCE"}, {"title": "4.1. Core Benchmarking Results", "content": "4.1.1. Single Reward, Multi KL Regularizer\nIn the first setting, we fix a single reward function and vary the KL regularizer $\\alpha$ to test the KL-reward trade-off. We use the reward R = 0.9$R_{nli}$ + 0.1$R_{rouge}$, where $R_{rouge}$ is mixed in to mitigate reward hacking the NLI model. In Figure 2(a), we see that all methods except logit-CLP are able to evenly trade-off reward and KL, enabling a smooth transition from $\\pi_{ref}$ (when $\\alpha$ = 1) to a maximally finetuned model (when $\\alpha$ = 0.01). This suggests that parameter-mixing trained with the multi-task objective is competitive with the baseline DeRa, which is state-of-the-art for reward-KL trade-off. Moreover, Figure 2(b) shows that CLP is also ~ 2\u00d7 more computationally efficient than DeRa at inference-time, because DeRa performs two LM calls (both $\\pi_{ref}$ and $\\pi_{\\alpha min}$) per token. Hence, inference speed is a major benefit of parameter-mixing over logit-ensembling.\n4.1.2. Two Rewards, Fixed KL Regularizer\nHere, we consider a pair of rewards with a fixed KL regularizer, to test the trade-off between both rewards. Figure 3 shows the Pareto curves for (a) NLI v. Rouge and (b) NLI v. TLDR, where the x,y-axes are the KL-regularized rewards, i.e., x = $V_{\\alpha,R_1} (\\pi(\\cdot; \\alpha, w))$, y = $V_{\\alpha,R_2} (\\pi(\\cdot; \\alpha,w))$, which recall is the true objective being maximized by finetuning (Equation (1)). We see that CLP and prompting largely Pareto-dominate the baseline RS, which shows the benefits of multi-task training compared"}, {"title": "4.1.3. Three Rewards, Fixed KL Regularizer", "content": "We now consider the three-reward setting to test the trade-off between all reward functions. Figure 4 plots the KL-regularized value $V_{\\alpha,w^T R} (\\pi(\\cdot; \\alpha, w))$, normalized w.r.t. RS, for 13 different reward weightings w shown below the x-axis. At the extreme weights w = ei, i \u2208 {1, 2, 3}, RS is equivalent to single-objective finetuning and naturally outperforms CLP and prompting as expected. At the inter-"}, {"title": "4.2. Ablation Studies", "content": "4.2.1. Effect of Training Iterations\nFigure 5 shows the progression of Pareto-curves over 90k training steps for CLP instances. full-CLP and attn-CLP are steerable after just 10k steps. In contrast, prompting becomes steerable at 60k steps; logit-CLP has a similar trend of being not as steerable at earlier training iterations. Furthermore, the optimization dynamics of full-CLP and attn-CLP are much smoother than prompting and logit-CLP.\n4.2.2. CLP With Prompt Conditioning\nFigure 6 shows the Pareto-fronts of full-CLP, the prompting baseline, and full-CLP with prompting (CONDPROMPT = TRUE). For NLI v. Rouge, full-CLP with prompting showed a slight improvement in steerability, whereas there was little difference for NLI v. TLDR. Hence, prompt conditioning does not hurt performance but can be slightly beneficial."}, {"title": "4.2.3. Model Size", "content": "We rerun our experiments with T5-small (60M) and T5-large (770M) to check how our findings change with different model sizes. Figure 7 shows the NLI v. TLDR Pareto-fronts. We see that full-CLP and attn-CLP still robustly Pareto-dominate the RS baseline and maintain steerable fronts. Interestingly, prompting collapses to a point for T5-small but has much better spread for T5-large, suggesting that prompting is sensitive to model size and a larger model can improve the steerability of prompting. We provide results for other reward pairs in Appendix C.6.\nSummary of ablations. While combining prompting with CLP did not significantly improve steerability, prompting may exhibit more steerability with larger models or training time. Across different settings, full-CLP and attn-CLP consistently maintain their superior performance and steerability suggesting they are robust conditioning architectures for MOFT when combined with multi-task training."}, {"title": "4.3. Automated Evaluation", "content": "In order to understand if CLP's improved Pareto-fronts translate to improvements in generations compared to baselines (prompting and Rewarded Soups), we conduct an automated evaluation of generation quality and steerability. For this evaluation, we consider the NLI v. TLDR setup with T5-large models (from Section 4.2.3) and use 2000 articles from the XSum validation set. We utilize Gemini 1.0 Ultra [36] as an automated evaluator to compare summaries from CLP instances to each baseline in terms of their conciseness and summary quality. Specifically, for each article, we sample conditioned summaries from both CLP and a baseline on weightings w = (0.8, 0.2) for high NLI (resp. weightings w = (0.2, 0.8) for high TLDR) and we ask the automated evaluator to compare which summary is more concise (resp. has higher quality). We permute the comparison order to account"}, {"title": "4.4. Scaling up CLP with Larger Policy and Reward Models", "content": "In the following experiment, we perform multi-reward finetuning on the TLDR dataset [35, 37] and increase the model size of both the policy and reward models. Namely, we train two reward models with T5-XXL (11B parameters), one for factuality (which we call NLI) and one for summary quality (which we call TLDR) where we adopt the same reward model training procedure as in [8]. For our policy model, we use T5-XL (3B parameters) initialized from the FLAN checkpoint [5] and all methods were trained for 6000 steps. We keep other configurations unchanged from the two-reward experiments on XSum (e.g., Section 4.1.2). We repeat this experiment for three sampling distributions and show these results in Figure 9. We observe that attn-CLP and full-CLP both largely Pareto-dominate the baselines while maintaining smoothly steerable behaviors Pareto-curves. logit-CLP collapses to a small region likely due to its representational bottleneck. We also observe that the steerability of the learned policy may improve as the weight sampling distribution becomes more narrow, though this may come at a cost of less Pareto-optimality. Finally, we have observed this trend repeatedly across different model sizes, datasets and sampling distributions, which gives credence to the robustness and reliability of CLP for multi-objective finetuning."}, {"title": "5. Related Works", "content": "Approaches for multi-reward alignment (or MOFT) can be broadly classified into two categories: prompt-based and parameter-based conditioning. Prompt-based conditioning approaches include Personalized Soups [17] which use hand-crafted prompts for personalizing LMs based on binary weights on different rewards; CPO [12] which employs prompting within a DPO framework; RiC [44], DPA [38] use prompting within a supervised finetuning setup that differs from this paper which focuses on RL finetuning. On the parameter-conditioning front, Rewarded Soups (RS) [27] presents a zero-shot approach (i.e. without multi-task training) to multi-reward alignment by inference time averaging of parameters for LMs that are independently trained to optimize each of the rewards. A more recent manuscript [46] presents an approach where the reward weightings are embedded as singular values within the AdaLoRA framework [14, 45]; this can be framed as an instance of the proposed CLP framework. With regards to KL realignment, decoding time realignment (DeRa) [20] linearly mixes logits between $\\pi_{ref}$ and another LM learned via SOFT with the minimum KL weight $\\alpha_{min}$. A recent manuscript Shi et al. [34] showed that this idea is also effective for trading off multiple rewards. Finally, model souping [41, 42], learning policy sub-spaces [6, 10], and objective weight conditioning [7] have been applied in domains beyond LMs. We leverage these advances along with multi-task training to develop steerable LMs at inference time."}, {"title": "6. Theory for Logit Mixing and CLP", "content": "In this section, we perform a sensitivity analysis for logit mixing, a zero-shot method for MOFT, and derive regret bounds for its Pareto-front. While CLP uses parameter mixing instead of logit mixing, this analysis is instructive due to the similarity between parameter and ensemble mixing [26, 42].\n6.1. Sensitivity Analysis for Logit Mixing\nWe focus on the \u201ctwo-reward, fixed $\\alpha$\u201d setting for simplicity and our analysis can be extended to the general case. For any $\\lambda \\in [0, 1]$, let $\\zeta_{\\lambda}$ be the logits of the optimal policy $\\pi_{\\lambda} = arg max_{\\pi} V_{\\alpha,w^T R}(\\pi)$ for weightings w = [1 \u2013 $\\lambda$, $\\lambda$]. Via the analytical solution of KL-regularized reward maximization, Liu et al. [20] observed that the optimal logits at $\\alpha$ is expressible as mixture of the optimal logits for each individual reward, i.e., $\\zeta_{\\lambda} = (1 \u2212 \\lambda)\\zeta_0 + \\lambda\\zeta_1$. Thus, given optimal policies for $R_1$ & $R_2$, logit-mixing provides a zero-shot way to compute the optimal policy at any intermediate $\\lambda \\in [0, 1]$.\nHowever, in practice, we of course are not given optimal policies and only have access to $\\epsilon$-approximations, so it is important to understand the sensitivity of logit-mixing. We now bound the sub-optimality of logit-mixing in terms of $\\epsilon$ and a concentrability coefficient that measures policy coverage, defined as $C_{\\pi_1,\\pi_2} := max_{x,y} \\frac{\\pi_2(y | x)}{\\pi_1(y | x)}$, i.e. what is the least overlap in terms of ratio of probabilities of each policy (one per reward) over the actions (for e.g. tokens).\nTheorem 1. Suppose $\\pi_1$, $\\pi_2$ are $\\epsilon$-optimal policies for Equation (1) with $R_1, R_2$, respectively. For any $\\lambda \\in [0, 1]$, let $\\hat{\\pi_{\\lambda}}$ be the logit mixture of $\\pi_1$ and $\\pi_2$. Then, the sub-optimality of $\\hat{\\pi_{\\lambda}}$ is bounded by:\n$O(((\\frac{(1 \u2212 \\lambda)C_{\\pi_2,\\pi_1} + \\lambda C_{\\pi_1,\\pi_2}}{p_{min}} + \\eta) \\cdot \\epsilon)$,\nwhere $p_{min}$ is the minimum probability of $\\pi_1$, $\\pi_2$ over all input-outputs (x, y) we may care about.\nThe proof is in Appendix E. The coverage terms however can be infinite if the policies $\\pi_1$, $\\pi_2$ don't cover each other. A zero-shot method will be robust to approximations when expert policies for each individual reward cover each other. But, as we show next, zero-shot approaches will fail when this coverage condition doesn't exist anymore."}, {"title": "6.2. Counterexample for zero-shot MOFT", "content": "Logit mixing cannot induce new behaviors since it can only mix behaviors from the two extremes, and so if an intermediate weighting requires a new behavior, logit mixing provably fails. Consider a toy problem with one context and three possible outputs $Y_1, Y_2, y_3$ with rewards $R_1(x, \\cdot) = (1, 0, 0.75), R_2(x, \\cdot) = (0, 1, 0.75)$. The optimal policies for $R_1, R_2$ (with $\\alpha = 0$) are $\\pi_1^{\\dagger}(x) = (1, 0, 0)$ and $\\pi_2^{\\dagger}(x) = (0, 1, 0)$. However, $\\pi_{0.5}^{\\dagger} = (0, 0, 1)$, which is a qualitatively new behavior that cannot arise from zero-shot logit mixing thus being a failure case for zero-shot logit mixing. In appendix Figure 19, we show that the RS baseline, which is zero-shot empirically fails to learn the Pareto-optimal policy in this example, while CLP which uses multi-task training succeeds."}, {"title": "7. Conclusion", "content": "We introduced CLP, a flexible framework for MOFT that leverages techniques from multi-task training and parameter efficient finetuning to develop steerable LMs that adapt their generations to produce near Pareto optimal behavior across different weightings of individual rewards. We provide extensive benchmarking and ablations to better understand factors that enable the development of steerable LMs within the CLP framework. We supplement this with theoretical results that present conditions under which zero shot approaches work and when multi-task training is necessary to obtain near-optimal behavior. In terms of future directions, (a) understanding other conditioning mechanisms such as soft tokens [16], (b) automated tuning of the weight sampling distributions [11], (c) addressing non-linear reward scalarization [30] are questions of natural interest."}, {"title": "Limitations", "content": "This paper develops a framework for multi-objective finetuning and proposes variants that work out-of-the-box and are robust across different ablations, which are accompanied by auto-evaluations that present credence to these claims. We acknowledge that reward models are approximations to human interpretations of language. When considering applications of CLP for other problem setups, it is fairly likely that additional evaluations including human evals, red-teaming and other strategies need to be considered to mitigate any risks posed by a more flexible LM. The specific issues that one would encounter when pursuing extensions along these directions is beyond the scope of this paper."}, {"title": "Ethical Considerations", "content": "This paper presents finetuning techniques for learning more flexible LMs that can provide generations that trade-off multiple potentially conflicting objectives at inference time. The design of objectives for alignment is an active area of research in itself. By having an LM that can adapt to provide optimized generations for different weighting of objectives, one can increase the risk of having LMs exhibit behaviors contrary to societal norms and values, and so must be subject to vetting, red-teaming and other protocols to ensure these models don't fall foul of societal norms."}]}