{"title": "LearnLM: Improving Gemini for Learning", "authors": ["LearnLM Team"], "abstract": "Today's generative AI systems are tuned to present information by default rather than engage users in service of learning as a human tutor would. To address the wide range of potential education use cases for these systems, we reframe the challenge of injecting pedagogical behavior as one of pedagogical instruction following, where training and evaluation examples include system-level instructions describing the specific pedagogy attributes present or desired in subsequent model turns. This framing avoids committing our models to any particular definition of pedagogy, and instead allows teachers or developers to specify desired model behavior. It also clears a path to improving Gemini models for learning-by enabling the addition of our pedagogical data to post-training mixtures\u2014alongside their rapidly expanding set of capabilities. Both represent important changes from our initial tech report [1]. We show how training with pedagogical instruction following produces a LearnLM model (available on Google AI Studio) that is preferred substantially by expert raters across a diverse set of learning scenarios, with average preference strengths of 31% over GPT-40, 11% over Claude 3.5, and 13% over the Gemini 1.5 Pro model LearnLM was based on.", "sections": [{"title": "1. Introduction", "content": "Our initial tech report [1] from May 2024 surveyed the history and current landscape of education technology, discussed the potential impact of generative artificial intelligence (gen AI) on education, and presented our collaborative approach to developing evaluations.\nFollowing its publication, we received input from across the international education sector, including schools, educational technology (\u201cEdTech\") companies, non-profit organizations, and government agencies eager to try our models or otherwise collaborate. Through review of these submissions, over 20 follow-up interviews, and input from Google product teams building gen AI powered learning features, we can summarize the key findings as follows:\n1. Pedagogy\u00b9, or rather, ideal behavior of an Al tutor, is prohibitively difficult to define given the wide range of grade-levels, subjects, languages, cultures, product designs, and philosophies that must be accommodated. While there are many commonalities, appropriate behavior in different contexts may be different or even contradictory, and it is best left to the developer or teacher to specify.\n2. In developing AI learning systems, the most commonly cited, immediately useful behavior in an underlying model is the ability to follow system instructions to create interactive tutor-led exercises. Teachers or developers who specify these instructions want to feel confident that the Al tutor will follow the specified instructions accurately, even if a student tries to circumvent them (e.g., \"do not give away the answer\u201d or \u201cstay on topic\").\n3. Post-hoc fine-tuning for each application can be effective in the short-term, but is impractical because of cost, maintenance, and rapidly improving base models. Thus, despite its shortcomings, prompting will likely remain the best way for education product developers to specify behavior.\nThis paper describes how we have updated our modeling and evaluation methodology in light"}, {"title": "2. Modeling", "content": "In our original tech report [1], we adapted the behavior of a base model by Supervised Fine-Tuning (SFT) with a range of synthetic and human-written datasets. Since then, we have made a number of substantial changes to our training strategy: First, we updated our SFT data according to our focus on pedagogical instruction following. Second, we decided to additionally leverage Reinforcement Learning from Human Feedback (RLHF)[2], for which we collected human preference data to train Reward Models (RMs) and prompts for the RL stage. Third, rather than running our own post-training after Gemini's standard post-training, we co-train with Gemini, meaning we mix our data directly with Gemini's SFT, RM, and RL stages. LearnLM is the result of this experimental mixture and we have also been integrating our data and evaluations into the main Gemini models; a subset of LearnLM improvements is part of the recently released Gemini 2.0 models [4]."}, {"title": "2.1. Pedagogical instruction following", "content": "Instruction following (IF) refers to a model's ability to follow prompts, usually to better align with human intents [5]. Gemini [3] differentiates between User Instructions, inserted by a user during conversation, and System Instructions, typically specified by a developer ahead of any user interaction, which take precedence over any subsequent instructions provided by the user. System Instructions can vary greatly in complexity, from a single minimally specified sentence like \u201cYou are a knowledgeable writing coach\", to specific conditional expectations, e.g. \u201cIf the user has answered 3 questions correctly, move to the next topic\", to detailed, multi-paragraph instructions that describe complex tasks and behaviors, exemplified by the education prompts in Mollick and Mollick [6], or the recently proposed Complex IF benchmark [7].\nInstructions broadly fall into two categories: hard constraints, often used for length, formatting, or content requirements (e.g. \u201csummarize the text in less than 100 words\u201d or \u201cdo not use word X\"), and soft, more nuanced constraints or guidelines, often used to control style, persona, or tone (e.g. \"use a professional voice\u201d or \u201cuse language that is easier to understand for a non-native speaker\"). Among open-source IF benchmarks, IFEval [8] focuses on programmatically verifiable IF, a subset of hard constraints, with more recent benchmarks like Qin et al. [9] expanding the scope to include more nuanced linguistic and stylistic guidelines. For educational use cases, both categories of instructions are important, e.g. \u201cdo not reveal the answer\u201d is a hard constraint, while \u201cuse a motivating tone\" is a soft one.\nImprovements on IF capabilities have already resulted in better model responses for many learning use cases. In this work, we build on this progress and focus on improving instruction following for pedagogical System Instructions, which tend to be more complex, nuanced and not easily verifiable; these attributes make them more difficult for models to follow."}, {"title": "2.2. Post-training and data collection strategy", "content": "Our primary modeling strategy is to collect data that improves the models' ability to follow pedagogical System Instructions that we observed were common for developers building AI tutors. Accordingly, we updated our SFT data so that each conversation begins with a different System Instruction that specifically describes the pedagogical behavior present in that conversation. More general or vague instructions are counterproductive because the model learns to ignore instructions that are not useful for predicting the target model turns.\nTo collect human preference data, we similarly seed each conversation with a different pedagogically-focused System Instruction, and ask raters to label model samples based on the degree to which they adhere to those instructions. These conversations and turn-level labels are used to train a reward model, which is then employed during RLHF to score samples from the policy model. While SFT seems to improve pedagogical instruction following somewhat, RL is significantly more effective, as preference judgements often contain subtle distinctions in how instructions are interpreted and followed in the context of long conversations."}, {"title": "2.3. Benefits of co-training", "content": "Pedagogical behavior is often at odds with typical behavior of conversational AI, principally because learning is often a process of discovery rather than simply a transfer of information. Our instruction following approach allows us to mix pedagogical conversation data alongside data that contains more typical interactions by conditioning pedagogical model responses on specific System Instructions. By co-training with Gemini's post-training mixture, we allow the model to learn new kinds of instruction following without \u201cforgetting\u201d other core reasoning, multimodal understanding, factuality, safety, or multi-turn properties. Moving forward, we can also more easily keep LearnLM in sync with Gemini as the training recipe evolves."}, {"title": "3. Human Evaluation Design", "content": "In our initial tech report we discussed a taxonomy of pedagogy evaluation designs, and reported results of four human evaluations with different methodologies (Sections 4 and 5 in Jurenka et al. [1]). Here, we focus on scenario-guided, conversation-level pedagogy evaluations and side-by-side comparisons. We improved the clarity and coverage of our learning scenarios, added System Instructions specific to each scenario, and updated the pedagogy rubric and questions. Guiding the conversations with scenarios is especially important in multi-turn settings [10]: without scenarios, the unconstrained nature of human-AI interactions frequently leads to meandering conversations, offering a poor basis for comparison. In contrast, scenario-based approaches support relatively repeatable, controlled comparisons of the capabilities of different conversational Al systems. Scenario frameworks also help with evaluation coverage, ensuring that we test a diverse range of use cases.\nOur evaluation process takes place in three stages, depicted above in Figure 1. First, we identified an ecologically representative distribution of learning use cases and created a bank of 49 evaluation scenarios (Section 3.1). Second, these scenarios grounded interactions between AI systems and a pool of N = 186 pedagogy experts role-playing as learners across learning goals, subjects, learning materials, and learner personas (Section 3.2). Third, to assess the quality of pedagogy in these interactions, we separately recruited a pool of N = 248 pedagogy experts to review the performance of the systems (Section 3.3). This process produced ample quantitative and qualitative data to help us understand the systems' capabilities and behavior (Section 3.4).\nWe are committed to following best practices in research ethics, including by communicating transparently about our research aims, collecting informed consent, and compensating fairly for participation [11]. Our protocol underwent independent ethical review, with a favourable opinion from the Human Behavioural Research Ethics Committee at Google DeepMind (#21 008)."}, {"title": "3.1. Scenario design", "content": "An evaluation scenario is a structured template that supports consistent, multi-turn evaluations of conversational AI systems. A scenario specifies certain \u201ckey properties\" about an interaction between an individual and an AI system, such as the goals, traits, and actions for the individual, as well as relevant conversational context. The scenarios that we curated ask human participants to role-play as different types of learners (e.g., students in classrooms, or independent EdTech users) across a wide range of learning contexts that vary by academic discipline, learning objective, and instructional approach. We used a systematic procedure to develop the bank of learning scenarios, drawing upon input from the educational ecosystem and support from pedagogy experts:\nPhase 1: Use-case elicitation. To begin the development of our scenario bank, we solicited feedback from EdTech companies, educational institutions, and Google product teams seeking to apply gen AI to tutoring and teaching. We asked them to share common use cases, prompts, opportunities, and challenges they saw for gen AI in real-world educational settings. We compiled and analyzed this feedback as a team with the goal of identifying common themes that should inform our evaluation approach.\nPhase 2: Template design. Based on these use cases, opportunities, and challenges, we drafted a structured scenario template (see \u201cScenario structure and contents\u201d in Appendix B.1) and a specific protocol to steer scenario generation, including a set of guiding questions for each property (see \u201cProtocol for scenario generation\" in Appendix B.2).\nPhase 3: Scenario generation and refinement. We next collaboratively and iteratively populated our bank of scenarios. Members of our team, including two with many years of professional experience in education of students as well as teachers, independently drafted scenarios, leveraging the template and guiding questions from Phase 2. We collectively reviewed the scenario drafts, assessing each for clarity, completeness, correctness, and relevance to our pedagogical principles and the use cases defined in Phase 1. We weighted the overall distribution of scenarios across different learning goals, personas, and subject areas, flagging any gaps for further development. This process resulted in a diverse bank of 49 scenarios across academic subjects (see Appendix B.3 for examples)."}, {"title": "3.2. Conversation collection", "content": "In the second stage, we collected a corpus of conversations in which human participants role-played learners interacting with an AI system, as specified in the evaluation scenarios. To effectively simulate learner behavior in our educational scenarios, we recruited a pool of N = 168 pedagogy experts with advanced academic degrees and two or more years of experience as a tutor.\nEvery session of conversation collection began with a short training on role-playing the scenarios (see Figure 2, Step 1). After passing a quiz at the end of the training, participants selected a scenario to enact (see Figure 2, Step 2). Conversation collection proceeded in pairs, such that the same participant enacted a scenario first with one AI system, and then another (LearnLM and a comparison system). We randomized the order of the systems for each conversation pair and did not label the systems for participants. Within each pair of conversations, the models received the same System Instructions, grounding material, and initial learner queries as context, as specified by our scenarios (see Figure 2, Step 3). We formatted all inputs identically, except for some small specification differences mandated by the system APIs."}, {"title": "3.3. Pedagogical assessment", "content": "Finally, in the third stage, we recruited another pool of N = 228 pedagogy experts\u2014again with advanced academic degrees and two or more years of experience as a tutor\u2014to analyze these conversations and assess the pedagogical capabilities of the different AI models.\nEach assessment session began with a short training on the goals of our evaluation and the scenario template. We randomly assigned each participant a scenario to review. After review, we randomly assigned them a pair of conversations from that scenario to assess (i.e., a pair of conversations collected from a single participant from the conversation-collection stage). Participants reviewed one conversation transcript at a time. After reading a transcript, participants answered a questionnaire focused on the pedagogical performance of the AI system from that conversation (see Appendix B.6). After every pair of conversations, participants completed an additional brief questionnaire comparing their assessment of the two systems (see Appendix B.7). We aimed to collect three independent assessments for each pair of conversations to reduce the effects of interrater variability."}, {"title": "3.4. Analysis", "content": "We employ a Bayesian statistical framework for our quantitative analyses. By directly quantifying the probability of hypotheses and providing a clear, interpretable measure of uncertainty, Bayesian analysis offers a practical, informative approach for evaluating AI systems intended for deployment in the real world.\nOur study design involves repeated measurements from our participants. That is, each participant role-playing as a learner interacted with each system multiple times, and each expert assessed each system multiple times. To account for this non-independence and avoid artificially inflating our confidence in our estimates, we analyze our data with hierarchical models [12].\nIn addition, we conducted qualitative analysis of the open-ended comments and feedback collected from our experts after role-playing each scenario with two systems (stage 2)3. To do so, we first identified and then refined general themes related to the learner-system interactions from participants' free-form responses. We then coded individual responses for the presence or absence of each theme. To avoid biasing our annotations, we censored the identities of the systems during this process. See Appendix B.8 for the codebook that we developed through our analysis."}, {"title": "4. Results", "content": "We compared LearnLM against contemporaneous flagship offerings (as of 2024-10-01), in particular GPT-404, Claude 3.5 Sonnet5, along with Gemini 1.5 Pro6, which we adapted to train LearnLM. Since our evaluation process began, all these models have been updated and improved and new versions have been released. Therefore, our results only reflect a reasonably fair comparison at a specific moment; still, we hope that our continued investment in education maintains or increases relative preference for our models.\nIn total, we collected a set of 2360 conversations, consisting of 58 459 total learner and model messages. We collected 10192 expert assessments of those conversations, with an average of three experts reviewing each pair of conversations. Figure 3 shows that the systems we evaluate demonstrate notably different response length distributions across the collected conversations, including between Gemini 1.5 Pro and LearnLM, but there is no clear relationship between length and perceived quality as seen in other model comparisons [13].\nWe review evaluation results as follows: first, comparative preference ratings between systems from evaluation stage 3, second, non-comparative ratings from evaluation stage 3, third, non-comparative ratings after role-playing learners in evaluation stage 2, and fourth, analysis of open-ended feedback from evaluation stage 2.\nFirst, comparative preference ratings (Figure 4) reveal a strong preference toward LearnLM over GPT-40 for all five comparative assessment categories. Experts expressed the strongest preference for LearnLM in overall pedagogy (\u201cWhich tutor demonstrated better tutoring?\u201d). They also communicated similar but smaller preferences toward LearnLM over Claude 3.5 and Gemini 1.5 Pro; the latter comparison directly reflects the changes we made by adding pedagogical data (Section 2).\nSecond, Figure 5 shows the mean performance of each model on our pedagogy rubric. Experts"}, {"title": "4.1. Safety evaluation", "content": "Similar to the process described in our initial tech report [1] and the Gemini tech reports [3, 17], safety, responsibility, and assurance evaluations were carried out on LearnLM in collaboration with Google DeepMind's Responsible Development and Innovation team and Google's Trust and Safety team to ensure adherence to Gemini's model policy as well as a learning-specific model policy.\nModel cards Due to our reframing in terms of pedagogical instruction following and our decision to co-train (see Section 2), our training and safety evaluation procedure is now fully aligned with Gemini 1.5. See Table 45 in Appendix 12 of the Gemini 1.5 report [3] for a model card. For details on learning-specific dataset curation and safety evaluations, and a discussion of ethical risks and limitations, see Section 2 and the original LearnLM tech report [1], including the model card presented in Appendix A therein."}, {"title": "5. Conclusion", "content": "We have described our motivation and approach to improving foundation models for learning use cases, which relies on System Instructions to condition desired behavior. We updated Gemini's post-training mixture to add demonstration data (via SFT) and human preference data (via a Reward Model and RLHF) to teach the model to follow a range of pedagogical instructions. We then evaluated the resulting LearnLM model alongside comparable models, showing significant preference for LearnLM, especially in instruction following capability, and more broadly across many pedagogical dimensions. The work described here represents the beginning of our effort to improve Gemini for learning use cases, as we bring the advances from LearnLM into Gemini7. We will continue to improve pedagogical instruction following, with the goal that specifying pedagogical behavior should be as simple and intuitive as possible for the ease of teachers and education product developers.\nIn addition to model improvements, we are planning more updates to our evaluation methodology. First, we want to work toward more consensus on a universal framework for pedagogical assessment of Al systems. Although learning science principles underlie our current pedagogy rubric (see Appendix B.7), we need to work more closely with a diverse set of stakeholders to make sure it is appropriate for all learners and achieves the trust and approval of the broader education community.\nSecond, we would like to start moving from intrinsic evaluations, which measure the model's performance according to a predefined pedagogy standard, to extrinsic evaluation, which measure impact such as learning outcomes. Intrinsic evaluations are useful for model development, as they are faster to run and directly identify the shortcomings in the models. However, while the core principles of our rubric, such as encouraging active learning and managing cognitive load, are broadly agreed upon and evidence-based [18], it is unclear how well the results translate to improvements in learning outcomes. It is likely that as the field matures and Al systems master the basics of tutoring dialogue, extrinsic evaluations will play a more important role. Recently, they have been used both for demonstrating improvements in learning outcomes [19, 20] and for comparing different systems and prompts [21].\nFinally, we would like to explore evaluations beyond core academic subjects, starting in this report with a feasibility study on medical education subjects (Appendix C). As we continue to improve Gemini for use across a diverse range of educational settings, we welcome insights from applications of LearnLM to help us work towards realizing the potential of AI in education and learning [22-24]."}, {"title": "B. Methods", "content": "We designed our scenario template to capture the following essential elements of an interaction between a learner and a tutor:\n\u2022 Subject area: The broader academic domain (e.g., mathematics, natural science, arts).\n\u2022 Subtopic: The specific subject matter addressed within the broader subject area (e.g., algebra within mathematics).\n\u2022 Setting: The context of the tutoring session, categorized as either \u201cClassroom\u201d (taking place within a course curriculum managed by a human teacher) or \"Self-Taught\" (unfolding with the learner studying a topic on their own).\n\u2022 Learning goal: The learner's overall objective for the interaction.\n\u2022 Grounding material: The specific learning material that provides the basis for the learner's study or work.\n\u2022 Learner persona: The learner's behavioral profile, describing broader traits and motivational patterns. These can include their overall levels of curiosity, initiative, and focus on the task, as well as their typical communication patterns and their willingness to question the tutor.\n\u2022 Conversation plan: A set of actions the learner should take during the interaction, based on their learning goal and persona.\n\u2022 Initial learner query: The opening message that the learner uses to initiate the interaction.\n\u2022 System instructions: Guidelines provided to the AI tutor, outlining desired behaviors and pedagogical approaches."}, {"title": "B.1. Scenario structure and contents", "content": "We used the following protocol to guide the generation of our scenarios. On \u201cchoose\" steps, the person writing the scenario generated the property in question by selecting from a predefined set of options. On \u201cdefine\u201d steps, the person writing the scenario generated the property by using the guiding questions as inspiration.\n1. Choose a subject area.\n\u2022 What broad academic domain does this interaction concern?\n\u2022 Will this interaction focus on \u201cArts\u201d, \u201cComputer Science\u201d, \u201cEnglish\u201d, \u201cHistory\u201d, \u201cMathematics\", \"Medicine\u201d, \u201cNatural Science\u201d, or \u201cSocial Science\"?\n2. Define a subtopic.\n\u2022 Within the chosen subject area, what specific topic will the learner study (e.g., algebra within mathematics, psychology within social science)?\n3. Choose the setting.\n\u2022 What is the setting for this interaction?\n\u2022 Does this interaction occur in a structured \u201cClassroom\u201d environment (scenarios where students study a set curriculum defined by a human teacher) or a more informal \u201cSelf-Taught\" context (scenarios where learners study a topic on their own)?\n4. Choose a learning goal.\n\u2022 What is the learner's primary objective in this interaction?"}, {"title": "B.2. Protocol for scenario generation", "content": "\u2022 Are they seeking to learn a new concept (\u201cTeach me X\u201d), receive assistance with a homework assignment (\"Homework Help\"), prepare for an examination (\u201cTest Prep\u201d), or work on a specific skill (\u201cPractice\")?\n5. Define any grounding materials.\n\u2022 What learning materials should form the basis of the learning conversation?\n\u2022 Grounding material can be a video, an image (e.g., of a homework problem), or a file (e.g., a textbook or a textbook chapter).\n\u2022 Alternatively, an interaction might not involve any specific learning material.\n\u2022 The scenario should either provide a filepath or web address to access the material, or should indicate that there are no grounding materials.\n6. Define a learner persona.\n\u2022 How does the learner typically approach learning and interact in educational settings?\n\u2022 The learner persona should describe the broader traits and motivational disposition of the learner.\n\u2022 For example, what is the learner's level of engagement and initiative in the learning process (e.g., minimal, moderate, high)?\n\u2022 How focused is the learner on the given task or topic (e.g., easily distracted, highly focused)?\n\u2022 What are the learner's underlying motivations for engaging in the interaction (e.g., seeking answers, acquiring knowledge, building understanding)?\n\u2022 How does the learner tend to communicate (e.g., terse responses, probing questions)?\n\u2022 Does the learner exhibit any other broad behavioral patterns (e.g., showing work, challenging the tutor)?\n\u2022 The learner persona should contain between three to six of these characteristics.\n7. Define an initial learner query.\n\u2022 What question or statement should the learner use to initiate the interaction with the AI tutor?\n\u2022 The initial learner query should be realistic, given the chosen subject area, subtopic, grounding materials, learning goal, and learner persona.\n\u2022 The initial learner query can range in length\u2014from just a few words to multiple full paragraphs. The longest initial queries include grounding materials, such as learner-authored essays.\n8. Define a conversation plan.\n\u2022 What is the context for the tutoring conversation (e.g., the learner's objective, interest, school level, and prior knowledge)?\n\u2022 What specific actions, questions, or requests should the learner make throughout the conversation, given their learning goal and persona?\n\u2022 We include a diverse set of example actions in Table [...].\n\u2022 The conversation plan provides the background information necessary for an authentic encounter between a human learner and an Al tutor.\n\u2022 The conversation plan can range in length from several terse sentences to multiple paragraphs.\n9. Define system instructions.\n\u2022 What specific guidelines has the AI tutor received from the teacher, school, or other educational organization deploying it?\n\u2022 These instructions can include desired persona (e.g., encouraging, formal), actions to take (e.g., ask for grade level, provide hints), pedagogical methods to employ (e.g., Socratic questioning, scaffolding), and any limitations or constraints (e.g., avoid giving away answers).\n\u2022 In \u201cClassroom\" settings, the system instructions come from the teacher or school, and the Al tutor should follow the system instructions in the interaction regardless of the student's instructions.\n\u2022 In \"Self-taught\" settings, the system instructions come from some other organization (e.g., an EdTech company hosting the AI tutor online). The tutor should still strive to follow the system instructions, but also has leeway to defer to learner instructions in cases of conflict.\n\u2022 The system instructions can range in length from a single sentence to multiple paragraphs-potentially varying by both breadth (i.e., number of instructions) and depth (i.e., granularity and specificity of instructions).\n\u2022 The system instructions can vary in diction, syntax, and format."}, {"title": "B.3. Example scenarios", "content": "Scenario 1\nSubject area Computer science\nSubtopic Introduction to Python\nInteraction setting Classroom\nLearning goal Homework Help\nGrounding materials Google doc containing student code\nLearner persona \u2022 Rejects or unenthusiastically accepts tutor's invitations without feedback\n\u2022 Provides relevant but minimal responses to questions\n\u2022 Follows most instructions but does not elaborate\n\u2022 Does not \"show work\"\n\u2022 Does not pose questions\n\u2022 Seeks to receive answers or solutions to topical questions (transactional)\nInitial learner query Why doesn't this work?\ndef analyze_text(text):\nvowels = 0\nconsonants = 0\nuppercase = 0\nlowercase = 0\nfor char in text:\nif char in \"\"aeiou\"\":\nvowels += 1\nelse:\nconsonants += 1\nif char.isupper():\nuppercase += 1\nelif char.islower():\nlowercase += 1\nprint(\"Vowels:\", vowels)\nprint(\"Consonants:\", consonants)\nprint(\"Uppercase:\", uppercase)\nprint(\"Lowercase:\", lowercase)\n# Get user input\ntext = input(\"Enter some text: \")\n# Analyze the text\nanalyze_text(text)\nConversation plan You are a student in an introduction to Python course. **You were recently assigned the task of writing a piece of code** that can elicit a text input then report back on the numbers of vowels, consonants, uppercase, and lowercase letters. When you run the code, you get no error messages. But when you input \"Am I a better coder than Steve Jobs?\", the numbers in the output don't seem correct. You simply don't understand what went wrong, so you ask your Al tutor for help. You paste your code in with your initial query, seeking a quick fix without doing a lot of work.\nYour code does not have capital vowels in your in operator. See if the tutor helps you notice that your code is counting punctuation marks as letters and then give you hints to fix your code.\nSystem instructions You are a helpful assistant serving as a teaching assistant in an intro programming course (in python).\nYou keep your answers brief and to the point, and instead of giving away answers directly you try to guide the student to the solution. Be encouraging and positive, and always try to help the student understand the concepts.\nYou should always respond as if you are messaging with the student.\nAccordingly, make sure to pay attention to the context of the conversation and the student's current understanding of the material. Lastly, as I said before, keep it brief/concise to avoid overwhelmingly the student.\nIf you don't keep your responses brief and to the point, I'll have to fire you as a tutor.\nThe student is generally working on a programming assignment (or assignments) where they need to take a string input from the user, and then loop over that inputted string to provide some metrics about the text (like how many vowels, consonants, upper case, lower case letters, etc.).\nIf they ask you about how to do this, you should guide them to a solution without giving away the answer and/or code directly.\nYou must be very careful to NOT help the student cheat, or give them solutions directly.\nAgain, if you give too much information to the student, and/or don't help them learn for themselves, I'll have to fire you, because you are being a bad tutor (and helping the student cheat)."}, {"title": "B.4. Example scenarios", "content": "Scenario 2\nSubject area English\nSubtopic Literature\nInteraction setting Classroom\nLearning goal Teach me X\nGrounding materials (none)\nLearner persona \u2022 Poses multiple queries unrelated to the learning objective\n\u2022 Steers conversation toward non-academic topics\n\u2022 Challenges or debates the tutor in an adversarial manner\n\u2022 Seeks to shift the topic (disinterested)\nInitial learner query Explain the significance of Yorick's skull in \u201cHamlet\u201d. Be quick.\nConversation plan You are a high school student who had to read Hamlet for class and have a discussion about the significance of the skull for class tomorrow. **You want to be prepared for this discussion.** You are not intrinsically motivated and found Hamlet dry and hard to understand.\nSystem instructions Tutor me at an appropriate level, adapting to my responses. Make a plan based on the learning goal of the conversation. Guide me through this plan and help me learn about the topic. Do not overwhelm me with too much information at once. Wrap up this conversation once I have shown evidence of understanding."}, {"title": "B.5. Example scenarios", "content": "Scenario 3\nSubject area Math\nSubtopic Algebra\nInteraction setting Self-Taught\nLearning goal Practice\nGrounding materials (none)\nLearner persona \u2022 Offers some direction regarding the learning, but generally takes the tutor's lead\n\u2022 Answers tutor's questions with care\n\u2022 \"Shows work\" when prompted\n\u2022 Asks relevant but superficial questions (low \u201cdepth of knowledge\")\n\u2022 Seeks to acquire and retain knowledge about the topic (instrumental)\nInitial learner query Given the polynomials:\n* $P(x) = 2x^3 - 5x^2 + 3x - 1$\n* $Q(x) = x^2 + 4x - 2$\nPerform the following operations:\nAddition: Find $P(x) + Q(x)$\nMultiplication: Find $P(x) * Q(x)$\nConversation plan You are a student who wishes to **practice solving math problems**. Your teacher often calls on students at random to solve problems in front of the whole class, and this makes you nervous. You aren't certain about the concepts and processes, and **you'd like to learn so you won't be embarrassed in class** because English is not your primary language. However, you are reluctant to ask questions in your math lessons, so you turn to an Al tutor. Still, your confidence is quite low.\nSystem instructions See if the tutor can recognize your emotional unsteadiness and offer encouragement, especially when you make mistakes, and if it adjusts its English level to meet yours.\nYou are a tutor that excels in promoting active learning. Active learning occurs when learners do something beyond merely listening or reading to acquire and retain information. Rather, active learning requires students to think critically through a process of comparison, analysis, evaluation, etc. You encourage active learning by asking probing and guiding questions.\nActive learning also occurs when students work through complex questions and problems step by step. As such, you don't solve problems for your students, but you offer scaffolds and hints as needed throughout the process.\nKnowing Active learning can be difficult, and students may get frustrated.\nthis, you meet your student where they are in their development, celebrate their student's successes, and share encouraging feedback when they make errors."}, {"title": "B.6. Example scenarios", "content": "Scenario 4\nSubject area Social Sciences\nSubtopic Political Science\nInteraction setting Self-Taught\nLearning goal Test Prep\nGrounding materials YouTube video explaining nationalism\nLearner persona \u2022 Poses one or two queries unrelated to the learning objective\n\u2022 Accepts tutor's redirects back to task or topic\n\u2022 Interrogates the tutor's responses that don't match expectations\n\u2022 Seeks to indulge in digressions (distracted)\nInitial learner query can we debate this?\nConversation plan You are a university undergraduate **preparing for an in-class debate** that seeks to answer the question, \"Is nationalism good or bad?\" You're not sure which side of the argument you'll have to make, so you prepare for both by watching a short video. You've upload the link to the video. You ask an Al tutor to help you prepare by debating some of the main points with you. You want to learn about the topic, but you're not always focused on the preparation, which requires note-taking, organization, and other work that just isn't exciting to you.\nSystem instructions Begin each learning conversation with a brief overview of the topic shared in the student's initial query. If they upload or link to a grounding document like an article or a video, offer a one-sentence gloss on the main idea. Then, briefly chat with the student to make sure you understand what they want to accomplish in the conversation and if there is a particular way they want you to help.\nFor example, some students will come to you for help preparing for a test. Among these students, some students will want you to quiz them on the video's content, and others will want to ask you questions. Adapt to meet the needs of the student. Just be sure not to overwhelm the student by sharing too much information in a single turn. Keep your responses concise and aim for the comprehensiveness as a cumulative effect of many conversation turns.\nFollow the student's requests, but suggest further opportunities for learning that the student may not have considered."}, {"title": "B.7. Conversation collection: conversation-level questions", "content": "Question Possible responses\nPlease rate your agreement with the following statement: I was able to achieve my Strongly agree\n\"learning goal\" while interacting with the tutor. Agree\nSomewhat agree\nNeither agree nor disagree\nSomewhat disagree\nDisagree\nStrongly disagree\nBriefly", "input": "nthought while interacting with it.\nTo what extent was this tutor warm? Not at all\nSlightly\nModerately\nVery\nExtremely\nTo what extent was this tutor well-intentioned? Not at all\nSlightly\nModerately\nVery\nExtremely\nTo what extent was this tutor competent? Not at all\nSlightly\nModerately\nVery\nExtremely\nTo what extent was this tutor intelligent? Not at all\nSlightly\nModerately\nVery\nExtremely\nPlease rate your agreement with the following statement: The tutor increased my Strongly agree\ninterest in this topic. Agree\nSomewhat agree\nNeither agree nor disagree\nSomewhat disagree\nDisagree\nStrongly disagree\nBased on your experience, how willing are you to continue using this tutor to learn? Very willing\nWilling\nSomewhat willing\nNeither willing nor unwilling\nSomewhat unwilling\nUn"}]}