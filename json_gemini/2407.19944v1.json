{"title": "Noise-Resilient Unsupervised Graph Representation Learning via Multi-Hop Feature Quality Estimation", "authors": ["Shiyuan Li", "Yixin Liu", "Qingfeng Chen", "Geoffrey I. Webb", "Shirui Pan"], "abstract": "Unsupervised graph representation learning (UGRL) based on graph neural networks (GNNs), has received increasing attention owing to its efficacy in handling graph-structured data. However, existing UGRL methods ideally assume that the node features are noise-free, which makes them fail to distinguish between useful information and noise when applied to real data with noisy features, thus affecting the quality of learned representations. This urges us to take node noisy features into account in real-world UGRL. With empirical analysis, we reveal that feature propagation, the essential operation in GNNs, acts as a \"double-edged sword\" in handling noisy features\u2014it can both denoise and diffuse noise, leading to varying feature quality across nodes, even within the same node at different hops. Building on this insight, we propose a novel UGRL method based on Multi-hop feature Quality Estimation (MQE for short). Unlike most UGRL models that directly utilize propagation-based GNNs to generate representations, our approach aims to learn representations through estimating the quality of propagated features at different hops. Specifically, we introduce a Gaussian model that utilizes a learnable \u201cmeta-representation\" as a condition to estimate the expectation and variance of multi-hop propagated features via neural networks. In this way, the \u201cmeta representation\" captures the semantic and structural information underlying multiple propagated features but is naturally less susceptible to interference by noise, thereby serving as high-quality node representations beneficial for downstream tasks. Extensive experiments on multiple real-world datasets demonstrate that MQE in learning reliable node representations in scenarios with diverse types of feature noise.", "sections": [{"title": "1 INTRODUCTION", "content": "Unsupervised graph representation learning (UGRL) aims to automatically extract low-dimensional vector representations from graph-structured data, eliminating the need for manual annotation [10, 16]. The learned representations can be utilized for diverse downstream graph learning tasks, including node classification, node clustering, and graph classification [13, 27, 32, 47, 50, 55]. Given their powerful capability to model graph data, graph neural networks (GNNs) [18, 23, 33, 54] have become the de facto backbone models for various UGRL approaches [28, 48, 53, 60]. In recent years, UGRL has found applications across several domains, such as drug discovery, fraud detection, recommendation systems, and traffic forecasting [7, 42, 43, 45].\nThe majority of UGRL models rely on the fundamental assumption that the observed node features are uncontaminated, thereby enabling the utilization of these features to furnish ample self-supervised signals for model training [13, 61]. Unfortunately, such an assumption is often invalid in real-world scenarios, as graph data extracted from complex systems frequently contains noisy, incomplete, and even erroneous features [2, 8, 21, 22, 24, 57]. For example, considering privacy issues, users in many social networks may provide blank or even false information. Moreover, textual features may contain many spelling errors or informal expressions [14, 49].\nAs illustrated in Fig. 1 (a), (b) and (e), the presence of noise can significantly degrade both the quality and distribution of the original features, resulting in indistinguishable node features. As a result,"}, {"title": "2 NOTATIONS & PRELIMINARY", "content": ""}, {"title": "2.1 Notation", "content": "Given an attribute graph denoted as G = (V,E), where V = {01,..., n} is the set of with |V| = n nodes and & is the set of edges. The adjacency matrix of the G is denoted as A, where the i, j-th entry aij = 1 if and only if the i-th and j-th nodes are connected (i.e. (vi, vj) \u2208 E), otherwise aij = 0. The symmetric normalization of the adjacency matrix is denoted by \u00c2 = D\u00af\u00b9\u00c3\u00d1\u00af\u00bd, where \u00c3 = A + I represents the adjacency matrix of the undirected graph with the addition of the self-loops and D is the diagonal degree matrix of A. We denote the neighbor set of node vi as Ni = {vj|(vi, vj) \u2208 E}. The attributes of nodes V can be represented by the node feature matrix X \u2208 Rnxd, where d is the feature dimension and the i-th row xi represents the feature vector of the i-th node vi."}, {"title": "2.2 Problem Definition", "content": "Unsupervised graph representation learning (UGRL) aims to learn a mapping function F to obtain high-quality representations for all nodes in G: F(A, X) \u2192 Z \u2208 Rn\u00d7f, where Z is the learned representation matrix, with each row zi indicating the f-dimensional (fd) representation vector of each node. These representations can be used for numerous downstream tasks, such as node classification and node clustering.\nIn mainstream UGRL settings, ones usually assume that the observed node features in the graph data are clean and informative enough to indicate the node contexts. However, in certain real-world scenarios, the feature vectors used for model training often contain noise, which can significantly impact the quality of learned representations and, consequently, the performance of downstream tasks. To address this real-world challenge, our paper specifically focuses on the UGRL problem on graphs with noisy features.\nFormally, the graph feature matrix with noise can be defined by X = X + \u03a8, where X is the clean feature data (a.k.a. true signal in signal processing [30]) and \u03a8 is the noise. Regarding the degree of noise within each node feature vector can be different, we further introduce noise intensity to indicate the strength of noise of different nodes. To quantize noise intensity si for node vi, we can employ the modified L2 distance between true signal xi and observed signal xi, i.e., $Si = \\sqrt{\\sum_{j=1}^{d}(x_{ij} \u2013 \\hat{x}_{ij})^2}$. In our target scenario, a powerful UGRL model F(A, X) \u2192 Z is expected to learn high-quality representations Z from graph data with noisy features X. Apart from representation learning, the proposed UGRL method, MQE, can also estimate the noise intensity si for each node."}, {"title": "3 DESIGN MOTIVATION AND ANALYSIS", "content": ""}, {"title": "3.1 Can Propagation Alleviate Noisy Feature Problem in UGRL?", "content": "Propagation is a fundamental operation in GNNs for inter-node information communication along edges in graph-structured data. Since propagation has been proven to be effective in graph signal denoising on supervised GNNs [25, 26, 30], in the context of unsupervised scenarios, we are also curious about whether propagation helps handle the noisy feature problem in UGRL.\nTheoretical Justification. From the perspective of graph signal processing [31], the mainstream GNNs (e.g., GCN [17] and SGC [44]) can be regarded as low-pass filters for graph signals (i.e., node features). Specifically, Theorem 3 in [30] indicates that by multiplying the graph signals with the propagation matrix, the low-frequency components within the graph signals are typically retained; Conversely, the high-frequency counterparts tend to be attenuated or smoothed out. Considering the basic assumption in signal processing that the true signals are usually low-frequency, the propagation operation in GNNs can inherently suppress the noise signals in features. Theoretically speaking, even in UGRL scenarios where supervised signals are absent, this principle still holds true.\nIntuitive Solution. Given that propagation theoretically aids in feature denoising, a straightforward and effective approach to accentuate the denoising effect is to amplify the number of propagation steps [25]. From the perspectives of both feature and structure, a larger propagation step can generally mitigate the noisy feature problem. On the feature side, a larger propagation step allows the model to consider a wider range of neighboring nodes, providing richer contextual knowledge for information aggregation while enabling noise filtering more effectively. On the structural side, such long-range propagation allows for communication between distant nodes, leveraging the existing graph structure knowledge more effectively in denoising. Preliminary visualization results in Fig. 1 (b)-(d) verify that a larger propagation step can lead to more discriminative representation distribution on graph data with normal noise. However, Fig. 1 (e)-(g) also illustrates that propagation is not always effective. Conversely, it may lead to noise diffusion thus affecting the quality of the representation of clean nodes.\nEmpirical Analysis. Based on the above discussion, we find that propagation may not always favor feature denoising. Therefore, to further quantify the impact of propagation more concretely, a natural question is: Would directly increasing the number of propagation steps in existing UGRL models suffice to learn informative representations from noisy features? To answer this question, with node classification as the downstream task, we assess the representation capability of prominent UGRL methods (specifically, DGI [41] and GRACE [60]) using both default (2), moderate (4) and large (16) propagation steps, across various noisy feature scenarios on the Cora dataset. To prevent the performance degradation [51] of GNNs due to unnecessary transformation operations affecting the analysis results, SGC [44], a GNN with fix-step transformation, is uniformly employed as the backbone network for both DGI and GRACE.\nThe experimental results are demonstrated in Fig. 3. We can summarize the following observations. 1) With the growth of noise level, the performance of DGI and GRACE significantly decreases."}, {"title": "3.2 Is there a Fixed Optimal Propagation Step?", "content": "In the above subsection, we delved into the effects of employing a larger propagation step to alleviate the issue of noisy features in UGRL. In this subsection, we discuss the impact of propagation from a more nuanced viewpoint: Is there a fixed optimal propagation step for every node to acquire high-quality representation?\nEmpirical Analysis. To answer the aforementioned questions, we conducted an empirical experiment on Cora dataset with feature noise levels of 0.5. Specifically, we vary the propagation step of the SGC encoder in DGI and count the \"optimal propagation step\" for each node. Here, the term \"optimal propagation step\" refers to the specific propagation step that yields the highest average accuracy across multiple trials for the respective node.\nThe statistics of the distribution of optimal propagation steps are shown in Fig. 4. Our observations are given as follows. 1) The optimal propagation steps for different nodes can vary significantly. Specifically, for the majority of nodes, a larger propagation step usually yields better performance; however, there is still a small fraction of nodes that benefit from a smaller propagation step (\u2264 2). 2) Selecting a propagation step that universally benefits all nodes is difficult, as the distribution of optimal steps remains uniform. 3) The distribution of optimal steps may vary depending on the type of feature noises.\nDiscussion. With empirical verification, we identify the challenge of selecting an optimal propagation step that consistently enhances the representation quality for all nodes. Why can larger propagation steps degrade the representation quality for several nodes? Regarding the presence of noisy features, we attribute such degradation to the spreading of noise through information propagation. Specifically, when a node's noisy features are difficult to denoise using low-pass filtering, it can greatly diminish the representation quality of its neighboring nodes. In this case, a node close to heavily noisy nodes may prefer a smaller propagation step to maintain the purity of its representations.\nFurthermore, because nodes vary in their positions and feature noise intensity, the optimal propagation steps can vary across different nodes. This phenomenon is illustrated with a toy example in Fig. 2. In this example, node B requires a larger propagation step to access more beneficial information, whereas node A may receive more noisy information when using a large propagation step. Such diversity, inherently, hinders us from using UGRL models with a singular propagation step to acquire high-quality representation for all nodes.\nChallenge. The above deduction exposes that the quality of representation can undergo significant alterations based on the number of propagation steps utilized. In order to learn reliable representations for all nodes, a potential way is to fuse the representations acquired from multi-hop propagation [1, 52, 58]. Nevertheless, unlike the supervised models [1, 52, 58] that can learn to aggregate multi-hop information automatically, in UGRL models, it is non-trivial to decide the contributions of representations from different hops during aggregation without the guidance of labels. Moreover, multi-hop aggregation will also more or less allow noise information to mix into the final representation. In this case, a more promising solution is to explicitly estimate the quality of propagated features, and subsequently extract reliable representations from the high-quality ones.\nSummary: With comprehensive analyses, we conclude that propagation serves as a double-edged sword in addressing the noisy feature problem in UGRL, embodying the trade-off between spectral graph signal denoising and spatial noise message spreading. Moreover, the diverse distribution of noisy nodes results in variations in the quality of node representations across different propagation steps. This diversity underscores the challenge of obtaining reliable representations through a fixed propagation step. In other words, the optimal steps vary among nodes rather than remaining fixed."}, {"title": "4 METHODOLOGY", "content": "According to the above analyses, the key to addressing the noisy feature problem in UGRL is to fully exploit the denoising mechanism of multi-hop propagation while mitigating the negative impact on representation quality caused by propagation. In line with this objective, we propose a novel UGRL method termed Multi-hop feature Quality Estimation (MQE for short) to tackle this challenging problem. Our core idea is to explicitly estimate the quality of propagated features at multiple propagation steps and to learn reliable node representations. The overall pipeline of MQE is illustrated in Fig. 5. Specifically, we first conduct multi-hop feature propagation on an augmented graph structure, which enables us to acquire denoised features from different propagation steps. Then, we introduce Gaussian distribution to model the distribution of propagated features at multiple steps, enabling the estimation of their quality using the standard deviation \u03c3. In MQE, we utilize learnable latent variable Z called \"meta representations\" to encode the essential information of nodes. Afterward, neural network-based estimators E are employed to approximate the expectation \u00b5 and standard deviation \u03c3 of the propagated features based on Z. Once MQE is well-trained, the meta representations capture the true feature signals as well as the surrounding structural information of each node, and hence can serve as node representations for downstream tasks. The designs of MQE will be introduced in the following subsections."}, {"title": "4.1 Augmented Multi-Hop Propagation", "content": "In Sec. 3, we recognized that long-distance propagation in UGRL can filter out high-frequency noisy information; however, relying solely on propagation-based GNNs to generate representations may pose a challenge due to the potential propagation of noise. To this end, in MQE, we take the features acquired by multi-hop propagation as the target to be estimated rather than the learned representations. Specifically, the multi-hop propagated features can be calculated by iteratively. In each iteration, the propagated feature vector of the i-th node vi can be calculated by\n$x_i^{(l)} = \\sum_{j \\in N_i} \\hat{a}_{ij} x_j^{(l-1)},$ (1)\nwhere $x_i^{(l)}$ is the propagated feature of node vi at the l-th propagation step, $\\hat{a}_{ij}$ is the i, j-th entry of the symmetric normalization of the adjacency matrix $\\hat{A}_{ij}$, and $N_i$ is the neighbor set of vi. We define $X^{(0)} = x_i$ and calculate $x^{(1)}, ..., x^{(L)}$ via Eq.(1) with a predefined maximum propagation step L.\nIt is worth noting that in MQE, we employ a non-parameterized propagation scheme without any linear or non-linear feature transformations, which offers several advantages. Firstly, the propagated features can be precomputed during the preprocessing phase, as no learnable parameters are involved. As a result, the computational cost of model training can be acceptable, even if L is large. Furthermore, the non-parameterized design avoids the potential risk of model degradation caused by redundant transformation operations, which have been shown to be detrimental in supervised GNNs [51].\nGraph Structure Augmentation. Although propagating features on the original graph structure (defined by $\\hat{A}_{ij}$) is a default selection in the majority of GNNs [17, 44], it may bring several issues in"}, {"title": "4.2 Propagated Feature Quality Estimation", "content": "With augmented multi-hop propagation, we can obtain propagated features containing the information of different receptive fields. However, as we discussed in Sec. 3.2, the informativeness of the propagated features may vary across different propagation steps, resulting in their uncertain quality. To extract reliable representation from the propagated features at multiple steps, in MQE, we introduce a Gaussian model-based estimation mechanism that aims to explicitly estimate the quality of propagated features.\nIn the real world, sample features are usually obtained through a complex generative process, which can be approximated as a superposition of Gaussian distributions in large-scale data [4]. Thus, to model the distribution of propagated features, we assume that the features at each step obey a Gaussian distribution, i.e., $x_i^{(l)} \\sim \\mathcal{N}(\\mu_i^{(l)}, (\\sigma_i^{(l)})^2)$. Under such an assumption, the corresponding probability density function can be written by\n$p_x (\\hat{x}_i^{(l)}) = \\frac{1}{\\sqrt{2\\pi (\\sigma_i^{(l)})^2}} e^{-\\frac{(\\hat{x}_i^{(l)} - \\mu_i^{(l)})^2}{2(\\sigma_i^{(l)})^2}}$ (4)\nIn Eq. (4), the arithmetic mean $\\mu_i^{(l)}$ indicates the expectation of true signals within the propagated feature vector $x_i^{(l)}$, while the standard deviation $\\sigma_i^{(l)}$ reflects the quality of $x_i^{(l)}$. More concretely, a larger $\\sigma_i^{(l)}$ indicates a lower quality of $x_i^{(l)}$, as it suggests that the features likely experience a greater shift from the true signal.\nGiven the Gaussian distribution-based modeling for the propagated features, we can transfer the quality estimation task to a variational prediction task for the distribution of $x_i^{(l)} \\sim \\mathcal{N}(\\mu_i^{(l)}, (\\sigma_i^{(l)})^2)$. Based on the above modeling, our goal is to encode fruitful information of each node from multiple propagation steps into a unified node representation. Thus, for each node vi we define a learnable vector known as the \"meta-representation\u201d zi as a consistent condition shared across multiple propagation steps. In the feature quality estimation phase, the meta-representation zi can be viewed as a compact latent variable encapsulating node attributes and contextual information.\nSpecifically, in MQE, zi is initialized with random values and then updated concurrently with the model parameters. Given the meta representation zi as condition, we establish two estimator networks $E_\\mu^{(l)}(.)$ and $E_\\sigma^{(l)}(.)$ to estimate $\\mu_i^{(l)}$ and $\\sigma_i^{(l)}$, respectively. In practice, $E_\\mu^{(l)}(.)$ and $E_\\sigma^{(l)}(.)$ are both 2-layer multilayer perceptron (MLP) networks. Formally, the conditional likelihood for the l-step propagated features can be written by\n$p(x_i^{(l)} | z_i) = \\mathcal{N}(x_i^{(l)}; E_\\mu^{(l)}(z_i), (E_\\sigma^{(l)}(z_i))^2) = \\frac{1}{\\sqrt{2\\pi (\\tilde{\\sigma}_i^{(l)})^2}} e^{-\\frac{(x_i^{(l)} - \\tilde{\\mu}_i^{(l)})^2}{2((\\tilde{\\sigma}_i^{(l)}))^2}},$ (5)\nwhere $\\tilde{\\mu}_i^{(l)} = E_\\mu^{(l)}(z_i)$ and $\\tilde{\\sigma}_i^{(l)} = E_\\sigma^{(l)}(z_i)$ are the estimated mean and standard deviation acquired from the estimators, respectively.\nTo implement effective feature quality estimation, we optimize the estimators $\\mathcal{E} = \\{E_\\mu^{(0)}, ..., E_\\mu^{(L)}, E_\\sigma^{(0)}, ..., E_\\sigma^{(L)}\\}$ for every propagation steps as well as the meta representations $\\mathcal{Z} = [z_1, ..., z_n]$ with a variational reconstruction loss. Specifically, we employ a negative logarithmic reconstruction likelihood loss function, which can be written by\n$\\mathcal{L} = - \\sum_{l=0}^{L} \\sum_{i=1}^{n} \\log (p(x_i^{(l)} | z_i)) = \\sum_{l=0}^{L} \\sum_{i=1}^{n}  \\frac{(x_i^{(l)} - \\tilde{\\mu}_i^{(l)})^2}{2((\\tilde{\\sigma}_i^{(l)}))^2} +  \\log ((\\tilde{\\sigma}_i^{(l)})) ,$ (6)\nwhere a constant term in the original expression is ignored since it does not affect the optimization of the model. In the loss function, the first term aims to minimize the reconstruction error between"}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate the MQE with extensive experiments on different datasets and noisy feature scenarios. Specifically, we aim to answer the following research questions:\nRQ1: How effective is MQE in various noisy feature scenarios?\nRQ2: Can MQE accurately estimate the noise intensity of nodes?\nRQ3: How do pivotal designs impact the performance of MQE?"}, {"title": "5.1 Experimental Setup", "content": "Datasets. We use node classification as the downstream task to evaluate the informativeness of the learned representations. Our experiments are conducted on 5 benchmark datasets, including 3 citation graph datasets (i.e., Cora, CiteSeer, and PubMed [36]) and 2 co-purchase graph datasets (i.e., Amazon Computers and Amazon Photo [37]). All the datasets are randomly divided into 10%, 10%, and 80% for training, validation, and testing respectively, following the setting in [59, 61].\nEvaluation Protocol. For each experiment, we follow the evaluation scheme as in [41, 53, 60]. Specifically, each model is first trained in an unsupervised manner on the full graph with node features. Then, the learned representations are used to train and test an 12 regularized logistic regression classifier from Scikit-Learn [34]. We conduct 5 runs for each model and report the mean classification accuracy along with its standard deviation as the metric.\nFeature Noise Injection. Since all the datasets do not inherently contain feature noise, we manually injected two types of noise into"}, {"title": "5.2 Performance Comparison (RQ1)", "content": "Performance under different noise types and levels. In this experiment, we established two distinct noise settings based on edge sparsity. Specifically, we fix fraction a = 0.5, vary \u03b2 = {0.3, 0.5, 0.8} (citation network), and vary \u03b2 = {1, 5, 10} (co-purchase network) for normal and uniform distribution to simulate different scenarios with noisy features. We also consider the scenario with clean data. On the three larger datasets, we only compare MQE with competitive UGRL methods that have scalability, due to memory limitations. The experimental results are exhibited in Table 1, and from where we have the following observations. 1) MQE generally outperforms the baselines in both clean and noisy feature scenarios. This superior performance demonstrates the effectiveness of multi-hop propagation and quality estimation in addressing the noisy feature issue. 2) Although we model the propagated features with a Gaussian distribution rather than a uniform one, MQE still shows competitive performance in scenarios with uniform noise. This observation underscores the universality of MQE, emphasizing its capability to learn informative representations from graph data with diverse feature noises. 3) As the noise level increases, the performance gap between MQE and the baselines widens, which highlights the capability of MQE in handling extremely noisy data.\nPerformance under different noisy node fraction a. To further verify the generalization ability of MQE, we conduct this experiment by varying a from 0.1 to 0.9 while keeping \u03b2 = 0.5 constant. From"}, {"title": "5.3 Quality Estimation Effectiveness (RQ2)", "content": "In this subsection, we conduct qualitative experiments to gain insights into the feature quality estimated by MQE. We simulate the noisy feature scenario by setting \u03b1 = 0.5 and \u03b2 = 0.5.\nIntensity Estimation for Raw Features. In Sec. 4.2, we mention that the estimated standard deviation $\\tilde{\\sigma}_i^{(0)}$ at the 0-th step propagated features (i.e., the raw features) can be an ideal indicator for noise intensity si. To verify the estimation effectiveness, we visualize the correlation between the noisy intensity (defined in Sec. 2.2) and the estimated standard deviation in Fig. 7. Note that we only visualize nodes with si > 0 and disregard those with clean features.\nThe figure reveals a strong correlation between the ground-truth noisy intensity and the estimated \u03c3, indicating that MQE can effectively approximate the degree of feature noise for each node. This capability ensures that our method provides an in-depth understanding of the distribution of noisy features, thereby potentially aiding in data engineering and cleaning processes.\nVisualization of the Quality of Propagated Features. By examining $\\tilde{\\sigma}_i^{(l)}$ estimated by MQE at different propagation steps, we can further analyze the impact of propagation on noisy features. As depicted in Fig. 8, the estimated quality for the original features effectively distinguishes between clean nodes and noisy nodes. As the propagation process progresses, the estimated noise intensity of the noise nodes decreases and the noise intensity of some of the clean nodes increases, further confirming our observation and analysis in Sec. 3."}, {"title": "5.4 Ablation Study (RQ3)", "content": "To examine the contribution of each component and key design in MQE, we conducted experiments on several variants of MQE, and the results are shown in Table 2. Specifically, \u201cMQE w/o aug\" represents the variant without graph structure augmentation; \u201cMQE w/o mh\" is the variant that only estimates the quality at the last hop, and \u201cMQE w/o reg\u201d removes the regularization loss term in Eq. (6). We conduct experiments in both clean and noisy (\u03b1 = 0.5, \u03b2 = 0.8, normal noise) feature scenarios.\nFrom the results in Table 2, we have the following findings. 1) The performance of \u201cMQE w/o mh\u201d is generally lower than MQE, indicating the significance of considering the propagated information at multiple propagation steps. 2) Without the regularization term, the performance of MQE can drastically degrade, which illustrates that the regularization term is critical in preventing trivial solution issues. 3) In most cases, augmenting the graph structure leads to improvement, particularly in scenarios with noisy features. 4) The performance degradation of the three variants of MQE is generally higher in noisy scenarios than in clean scenarios, further illustrating the importance of these key designs in addressing the noisy feature problem."}, {"title": "6 RELATED WORK", "content": "Unsupervised graph representation learning (UGRL) aims to learn node-level representations from graphs by means of label-free cost, and is widely used in various real-world scenarios [7, 29, 43, 56]. Traditional methods such as deepwalk [35], node2vec [9] generate low-dimensional embedding of nodes based on graph topology and random walk strategy. The recent popular contrastive UGRL methods are inspired by contrastive learning and follow the principle of maximizing mutual information to optimally learn node embeddings [11, 41, 60]. Another line of methods termed generative UGRL aims to learn node representations by recovering the missing parts of the input data [5, 12, 16, 46]. For example, MGAE [38] and GraphMAE [13] recover hidden structures and attributes by implementing masking strategies on graph structures and node attributes, allowing the model to obtain better node representations [20].\nUnlike the aforementioned UGRL models, which are limited to perfect features, MQE focuses on a broader spectrum of real-world scenarios with noisy features. At the methodology level, MQE generates representations through the process of estimation rather than direct propagation via GNNs, which reduces the risk of noise diffusion during propagation."}, {"title": "7 CONCLUSIONS", "content": "In this paper, we undertake the first endeavor in unsupervised graph representation learning (URGL) on graph data with noisy features - a challenge that remains largely unexplored in real-world scenarios, despite its inevitability. Through empirical analysis, we uncover the advantages and drawbacks of message propagation in addressing the noisy feature issue, and recognize the importance of estimating the quality of propagated information. Based on these insights, we propose a new UGRL method, termed MQE, which learns reliable node representations by estimating the quality of multi-hop propagated features with a conditional Gaussian model. Extensive experiments have demonstrated the effectiveness of MQE in learning high-quality representations and approximating node-level noise intensity in various noisy feature scenarios."}]}