{"title": "Noise-Resilient Unsupervised Graph Representation Learning via Multi-Hop Feature Quality Estimation", "authors": ["Shiyuan Li", "Yixin Liu", "Qingfeng Chen", "Geoffrey I. Webb", "Shirui Pan"], "abstract": "Unsupervised graph representation learning (UGRL) based on graph neural networks (GNNs), has received increasing attention owing to its efficacy in handling graph-structured data. However, existing UGRL methods ideally assume that the node features are noise-free, which makes them fail to distinguish between useful information and noise when applied to real data with noisy features, thus affecting the quality of learned representations. This urges us to take node noisy features into account in real-world UGRL. With empirical analysis, we reveal that feature propagation, the essential operation in GNNs, acts as a \"double-edged sword\" in handling noisy features it can both denoise and diffuse noise, leading to varying feature quality across nodes, even within the same node at different hops. Building on this insight, we propose a novel UGRL method based on Multi-hop feature Quality Estimation (MQE for short). Unlike most UGRL models that directly utilize propagation-based GNNS to generate representations, our approach aims to learn representations through estimating the quality of propagated features at different hops. Specifically, we introduce a Gaussian model that utilizes a learnable \u201cmeta-representation\" as a condition to estimate the expectation and variance of multi-hop propagated features via neural networks. In this way, the \u201cmeta representation\" captures the semantic and structural information underlying multiple propagated features but is naturally less susceptible to interference by noise, thereby serving as high-quality node representations beneficial for downstream tasks. Extensive experiments on multiple real-world datasets demonstrate that MQE in learning reliable node representations in scenarios with diverse types of feature noise.", "sections": [{"title": "1 INTRODUCTION", "content": "Unsupervised graph representation learning (UGRL) aims to automatically extract low-dimensional vector representations from graph-structured data, eliminating the need for manual annotation [10, 16]. The learned representations can be utilized for diverse downstream graph learning tasks, including node classification, node clustering, and graph classification [13, 27, 32, 47, 50, 55]. Given their powerful capability to model graph data, graph neural networks (GNNs) [18, 23, 33, 54] have become the de facto backbone models for various UGRL approaches [28, 48, 53, 60]. In recent years, UGRL has found applications across several domains, such as drug discovery, fraud detection, recommendation systems, and traffic forecasting [7, 42, 43, 45].\nThe majority of UGRL models rely on the fundamental assumption that the observed node features are uncontaminated, thereby enabling the utilization of these features to furnish ample self-supervised signals for model training [13, 61]. Unfortunately, such an assumption is often invalid in real-world scenarios, as graph data extracted from complex systems frequently contains noisy, incomplete, and even erroneous features [2, 8, 21, 22, 24, 57]. For example, considering privacy issues, users in many social networks may provide blank or even false information. Moreover, textual features may contain many spelling errors or informal expressions [14, 49].\nAs illustrated in Fig. 1 (a), (b) and (e), the presence of noise can significantly degrade both the quality and distribution of the original features, resulting in indistinguishable node features. As a result,\nhow can we develop a robust UGRL approach that effectively addresses the presence of noisy features?\nTo answer this question, this paper first conducts a comprehensive analysis to investigate the performance of UGRL models when learning from noisy features. Through empirical discussions, we find that propagation, the fundamental operation for information aggregation in GNNs, is a double-edged sword in the case of noisy features. On the one hand, under the assumption that noises are often high-frequency graph signals, propagation functions as a denoising mechanism in handling noisy graph data [30]. From the spectral perspective, propagation operation is essentially equivalent to low-pass filtering in graph signal processing, which effectively suppresses the high-frequency portion of the message and thus reduces the effect of noise in signal passing. In this case, propagation is not only a way of information broadcasting, but also a signal processing filter that enhances the robustness of GNN-based UGRL models against the noisy feature issue. As illustrated in Fig. 1 (c) and (d), propagation operations with different steps make noisy features more discernible, with features of the same category forming tightly knit clusters.\nOn the other hand, propagation is not always the perfect solution for handling graphs with noisy features. As shown in Fig. 1 (e), (f), and (g), for uniform noise, the propagation operation cannot remove the noise stably. In this case, the noisy signals will spread to the surrounding nodes along with the propagation process, affecting the representation quality of nodes with clean or\nBased on the above findings, an immediate core question is: How to measure the representation quality obtained from different steps of propagation to derive higher-quality representations?\nTo answer the question, we propose a robust and efficient UGRL model termed Multi-hop feature Quality Estimation (MQE for short). Our theme is to adaptively learn the optimal representations of each node by explicitly estimating the quality of propagated features at different hops. More specifically, to exploit the inherited denoising capability of long-range propagation, we first perform multi-hop propagation on the original features as a prior observation. Then, we introduce a Gaussian model to estimate the quality of propagated features by modeling the truth signals and noise information under different hops. In the process of feature quality estimation, we introduce a learnable matrix Z termed \"meta representation\" to indicate the essential information of each node for different hops. The meta representation Z can serve as the high-quality node representations learned by MQE, since it summarizes the most informative signals from propagated node features at different hops. Apart from learning reliable node representations, MQE can also quantize the noise intensity of node features during quality estimation, which provides potential explanation and analysis for graph data. Notably, MQE is different from the vast majority of UGRL approaches: rather than generating representations through propagation or GNNs, we attempt to estimate and fit different propagated features through learnable representations and neural networks. This characteristic enables MQE to leverage the denoising merit multi-hop propagation while alleviating the side effect of noise spreading. Overall, the contributions of this paper are as follows:\n\u2022 Problem. We conduct comprehensive analyses to study the impact of noisy features on UGRL models, which provides an"}, {"title": "2 NOTATIONS & PRELIMINARY", "content": "2.1 Notation\nGiven an attribute graph denoted as G = (V,E), where V = {01,..., n} is the set of with |V| = n nodes and & is the set of edges. The adjacency matrix of the G is denoted as A, where the i, j-th entry aij = 1 if and only if the i-th and j-th nodes are connected (i.e. (vi, vj) \u2208 E), otherwise aij = 0. The symmetric normalization of the adjacency matrix is denoted by \\(\\hat{A} = D^{-1}AD^{-}\\), where \\(\\tilde{A} = A + I\\) represents the adjacency matrix of the undirected graph with the addition of the self-loops and D is the diagonal degree matrix of A. We denote the neighbor set of node vi as Ni = {vj|(vi, vj) \u2208 E}. The attributes of nodes V can be represented by the node feature matrix X \u2208 \\(\\mathbb{R}^{nxd}\\), where d is the feature dimension and the i-th row x\u012f represents the feature vector of the i-th node vi.\n2.2 Problem Definition\nUnsupervised graph representation learning (UGRL) aims to learn a mapping function \\(F\\) to obtain high-quality representations for all nodes in G: \\(F(A, X) \u2192 Z \u2208 \\mathbb{R}^{n\u00d7f}\\), where Z is the learned representation matrix, with each row zi indicating the f-dimensional (fd) representation vector of each node. These representations can be used for numerous downstream tasks, such as node classification and node clustering.\nIn mainstream UGRL settings, ones usually assume that the observed node features in the graph data are clean and informative enough to indicate the node contexts. However, in certain real-world scenarios, the feature vectors used for model training often contain noise, which can significantly impact the quality of learned representations and, consequently, the performance of downstream tasks. To address this real-world challenge, our paper specifically focuses on the UGRL problem on graphs with noisy features.\nFormally, the graph feature matrix with noise can be defined by X = \\(X + \u03a8\\), where X is the clean feature data (a.k.a. true signal in signal processing [30]) and \u03a8 is the noise. Regarding the degree of noise within each node feature vector can be different, we further introduce noise intensity to indicate the strength of noise of different nodes. To quantize noise intensity si for node vi, we can employ the modified L2 distance between true signal x\u00a1 and observed signal\n\\(s_i = \\sqrt[2]{\\sum_{j=1}^d (x_{ij} - \\tilde{x}_{ij})^2}\\). In our target scenario, a powerful UGRL model \\(F(A, X) \u2192 Z\\) is expected to learn high-quality representations Z from graph data with noisy features X. Apart from representation learning, the proposed UGRL method, MQE, can also estimate the noise intensity si for each node."}, {"title": "3 DESIGN MOTIVATION AND ANALYSIS", "content": "3.1 Can Propagation Alleviate Noisy Feature Problem in UGRL?\nPropagation is a fundamental operation in GNNs for inter-node information communication along edges in graph-structured data. Since propagation has been proven to be effective in graph signal denoising on supervised GNNs [25, 26, 30], in the context of unsupervised scenarios, we are also curious about whether propagation helps handle the noisy feature problem in UGRL.\nTheoretical Justification. From the perspective of graph signal processing [31], the mainstream GNNs (e.g., GCN [17] and SGC [44]) can be regarded as low-pass filters for graph signals (i.e., node features). Specifically, Theorem 3 in [30] indicates that by multiplying the graph signals with the propagation matrix, the low-frequency components within the graph signals are typically retained; Conversely, the high-frequency counterparts tend to be attenuated or smoothed out. Considering the basic assumption in signal processing that the true signals are usually low-frequency, the propagation operation in GNNs can inherently suppress the noise signals in features. Theoretically speaking, even in UGRL scenarios where supervised signals are absent, this principle still holds true.\nIntuitive Solution. Given that propagation theoretically aids in feature denoising, a straightforward and effective approach to accentuate the denoising effect is to amplify the number of propagation steps [25]. From the perspectives of both feature and structure, a larger propagation step can generally mitigate the noisy feature problem. On the feature side, a larger propagation step allows the model to consider a wider range of neighboring nodes, providing richer contextual knowledge for information aggregation while enabling noise filtering more effectively. On the structural side, such long-range propagation allows for communication between distant nodes, leveraging the existing graph structure knowledge more effectively in denoising.\nEmpirical Analysis. Based on the above discussion, we find that propagation may not always favor feature denoising. Therefore, to further quantify the impact of propagation more concretely, a natural question is: Would directly increasing the number of propagation steps in existing UGRL models suffice to learn informative representations from noisy features? To answer this question, with node classification as the downstream task, we assess the representation capability of prominent UGRL methods (specifically, DGI [41] and GRACE [60]) using both default (2), moderate (4) and large (16) propagation steps, across various noisy feature scenarios on the Cora dataset.\n3.2 Is there a Fixed Optimal Propagation Step?\nIn the above subsection, we delved into the effects of employing a larger propagation step to alleviate the issue of noisy features in UGRL. In this subsection, we discuss the impact of propagation from a more nuanced viewpoint: Is there a fixed optimal propagation step for every node to acquire high-quality representation?\nEmpirical Analysis. To answer the aforementioned questions, we conducted an empirical experiment on Cora dataset with feature noise levels of 0.5. Specifically, we vary the propagation step of the SGC encoder in DGI and count the \"optimal propagation step\" for each node. Here, the term \"optimal propagation step\" refers to the specific propagation step that yields the highest average accuracy across multiple trials for the respective node.\nChallenge. The above deduction exposes that the quality of representation can undergo significant alterations based on the number of propagation steps utilized. In order to learn reliable representations for all nodes, a potential way is to fuse the representations acquired from multi-hop propagation [1, 52, 58]. Nevertheless, unlike the supervised models [1, 52, 58] that can learn to aggregate multi-hop information automatically, in UGRL models, it is non-trivial to decide the contributions of representations from different hops during aggregation without the guidance of labels. Moreover, multi-hop aggregation will also more or less allow noise information to mix into the final representation. In this case, a more promising solution is to explicitly estimate the quality of propagated features, and subsequently extract reliable representations from the high-quality ones.\nSummary: With comprehensive analyses, we conclude that propagation serves as a double-edged sword in addressing the noisy feature problem in UGRL, embodying the trade-off between spectral graph signal denoising and spatial noise message spreading. Moreover, the diverse distribution of noisy nodes results in variations in the quality of node representations across different propagation steps. This diversity underscores the challenge of obtaining reliable representations through a fixed propagation step. In other words, the optimal steps vary among nodes rather than remaining fixed."}, {"title": "4 METHODOLOGY", "content": "According to the above analyses, the key to addressing the noisy feature problem in UGRL is to fully exploit the denoising mechanism of multi-hop propagation while mitigating the negative impact on representation quality caused by propagation. In line with this objective, we propose a novel UGRL method termed Multi-hop feature Quality Estimation (MQE for short) to tackle this challenging problem. Our core idea is to explicitly estimate the quality of propagated features at multiple propagation steps and to learn reliable node representations. The overall pipeline of MQE is illustrated in Fig. 5. Specifically, we first conduct multi-hop feature propagation on an augmented graph structure, which enables us to acquire denoised features from different propagation steps. Then, we introduce Gaussian distribution to model the distribution of propagated features at multiple steps, enabling the estimation of their quality using the standard deviation \u03c3. In MQE, we utilize learnable latent variable Z called \"meta representations\" to encode the essential information of nodes. Afterward, neural network-based estimators E are employed to approximate the expectation \u00b5 and standard deviation o of the propagated features based on Z. Once MQE is well-trained, the meta representations capture the true feature signals as well as the surrounding structural information of each node, and hence can serve as node representations for downstream tasks. The designs of MQE will be introduced in the following subsections.\n4.1 Augmented Multi-Hop Propagation\nIn Sec. 3, we recognized that long-distance propagation in UGRL can filter out high-frequency noisy information; however, relying solely\non propagation-based GNNs to generate representations may pose a challenge due to the potential propagation of noise. To this end, in MQE, we take the features acquired by multi-hop propagation as the target to be estimated rather than the learned representations. Specifically, the multi-hop propagated features can be calculated by iteratively. In each iteration, the propagated feature vector of the i-th node vi can be calculated by\n\\(x_i^{(l)} = \\sum_{j \\in N_i} \\hat{a}_{ij} x_j^{(l-1)},\\)\nwhere \\(x_i^{(l)}\\) is the propagated feature of node vi at the l-th propagation step, \\(\\hat{a}_{ij}\\) is the i, j-th entry of the symmetric normalization of the adjacency matrix \u00c2\u00a1j, and N\u012f is the neighbor set of vi. We define x(0) = x\u00a1 and calculate \\(x^{(1)},..., x^{(L)}\\) via Eq.(1) with a predefined maximum propagation step L.\nIt is worth noting that in MQE, we employ a non-parameterized propagation scheme without any linear or non-linear feature transformations, which offers several advantages. Firstly, the propagated features can be precomputed during the preprocessing phase, as no learnable parameters are involved. As a result, the computational cost of model training can be acceptable, even if L is large. Furthermore, the non-parameterized design avoids the potential risk of model degradation caused by redundant transformation operations, which have been shown to be detrimental in supervised GNNs [51].\nGraph Structure Augmentation. Although propagating features on the original graph structure (defined by \u00c2\u00a1j) is a default selection in the majority of GNNs [17, 44], it may bring several issues in\n\\(\\pi(x) = \\frac{x \\cdot x^T}{\\lVert x \\rVert \\cdot \\lVert x \\rVert},\\)\n4.2 Propagated Feature Quality Estimation\nWith augmented multi-hop propagation, we can obtain propagated features containing the information of different receptive fields. However, as we discussed in Sec. 3.2, the informativeness of the propagated features may vary across different propagation steps, resulting in their uncertain quality. To extract reliable representation from the propagated features at multiple steps, in MQE, we introduce a Gaussian model-based estimation mechanism that aims to explicitly estimate the quality of propagated features.\nIn the real world, sample features are usually obtained through a complex generative process, which can be approximated as a superposition of Gaussian distributions in large-scale data [4]. Thus, to model the distribution of propagated features, we assume that the features at each step obey a Gaussian distribution, i.e., \\(x^{(l)} \\sim N(\\mu^{(l)}, (\\sigma^{(l)})^2)\\). Under such an assumption, the corresponding probability density function can be written by\n\\(p(x|z) = \\frac{1}{\\sqrt{2\\pi (\\sigma^{(l)})^2}}e^{-\\frac{(x^{(l)} - \\mu^{(l)})^2}{2(\\sigma^{(l)})^2}},\\)\nIn Eq. (4), the arithmetic mean \\(\\mu^{(l)}\\) indicates the expectation of true signals within the propagated feature vector \\(x^{(l)}\\), while the standard deviation \\(\\sigma^{(l)}\\) reflects the quality of \\(x^{(l)}\\). More concretely, a larger \\(\\sigma^{(l)}\\) indicates a lower quality of \\(x^{(l)}\\), as it suggests that the features likely experience a greater shift from the true signal.\nGiven the Gaussian distribution-based modeling for the propagated features, we can transfer the quality estimation task to a variational prediction task for the distribution of \\(x^{(l)} \\sim N(\\mu^{(l)}, (\\sigma^{(l)})^2)\\). Based on the above modeling, our goal is to encode fruitful information of each node from multiple propagation steps into a unified node representation. Thus, for each node vi we define a learnable vector known as the \"meta-representation\u201d zi as a consistent condition shared across multiple propagation steps. In the feature quality estimation phase, the meta-representation z\u012f can be viewed as a compact latent variable encapsulating node attributes and contextual information.\nSpecifically, in MQE, zi is initialized with random values and then updated concurrently with the model parameters. Given the meta representation z\u012f as condition, we establish two estimator networks \\(E_\\mu^{(l)}(\\cdot)\\) and \\(E_\\sigma^{(l)}(\\cdot)\\) to estimate \\(\\mu^{(l)}\\) and \\(\\sigma^{(l)}\\), respectively. In practice, \\(E_\\mu^{(l)}(\\cdot)\\) and \\(E_\\sigma^{(l)}(\\cdot)\\) are both 2-layer multilayer perceptron (MLP) networks. Formally, the conditional likelihood for the l-step propagated features can be written by\n\\(p(x|z) = \\frac{1}{\\sqrt{2\\pi (\\sigma^{(l)})^2}}e^{-\\frac{(x^{(l)} - \\mu^{(l)})^2}{2(\\sigma^{(l)})^2}},\\)\nTo implement effective feature quality estimation, we optimize the estimators \\(E = \\{E_\\mu^{(0)}, ..., E_\\mu^{(L)}\\} = \\{E_\\sigma^{(0)}, E_\\sigma^{(1)}, ..., E_\\sigma^{(L)}, E_\\sigma^{(L)}\\}\\) for every propagation steps as well as the meta representations Z = \\([z_1,..., z_n]\\) with a variational reconstruction loss. Specifically, we employ a negative logarithmic reconstruction likelihood loss function, which can be written by\n\\(L = - \\sum_{l=0}^L \\sum_{i=1}^n log(p(x^{(l)} | z_i)) = \\sum_{l=0}^L \\sum_{i=1}^n  \\frac{(x^{(l)} - \\mu^{(l)})^2}{2(\\sigma^{(l)})^2} + ln(\\sigma^{(l)}),\\)\nwhere a constant term in the original expression is ignored since it does not affect the optimization of the model. In the loss function, the first term aims to minimize the reconstruction error between"}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate the MQE with extensive experiments on different datasets and noisy feature scenarios. Specifically, we aim to answer the following research questions:\nRQ1: How effective is MQE in various noisy feature scenarios?\nRQ2: Can MQE accurately estimate the noise intensity of nodes?\nRQ3: How do pivotal designs impact the performance of MQE?\n5.1 Experimental Setup\nDatasets. We use node classification as the downstream task to evaluate the informativeness of the learned representations. Our experiments are conducted on 5 benchmark datasets, including 3 citation graph datasets (i.e., Cora, CiteSeer, and PubMed [36]) and 2 co-purchase graph datasets (i.e., Amazon Computers and Amazon Photo [37]). All the datasets are randomly divided into 10%, 10%, and 80% for training, validation, and testing respectively, following the setting in [59, 61].\nEvaluation Protocol. For each experiment, we follow the evaluation scheme as in [41, 53, 60]. Specifically, each model is first trained in an unsupervised manner on the full graph with node features. Then, the learned representations are used to train and test an 12 regularized logistic regression classifier from Scikit-Learn [34]. We conduct 5 runs for each model and report the mean classification accuracy along with its standard deviation as the metric.\nFeature Noise Injection. Since all the datasets do not inherently contain feature noise, we manually injected two types of noise into\n5.2 Performance Comparison (RQ1)\nPerformance under different noise types and levels. In this experiment, we established two distinct noise settings based on edge sparsity. Specifically, we fix fraction a = 0.5, vary \u03b2 = {0.3, 0.5, 0.8} (citation network), and vary \u03b2 = {1, 5, 10} (co-purchase network) for normal and uniform distribution to simulate different scenarios with noisy features. We also consider the scenario with clean data. On the three larger datasets, we only compare MQE with competitive UGRL methods that have scalability, due to memory limitations.\n5.3 Quality Estimation Effectiveness (RQ2)\n5.4 Ablation Study (RQ3)\nTo examine the contribution of each component and key design in MQE, we conducted experiments on several variants of MQE, and the results are shown in Table 2. Specifically, \u201cMQE w/o aug\u201d represents the variant without graph structure augmentation; \u201cMQE w/o mh\u201d is the variant that only estimates the quality at the last hop, and \u201cMQE w/o reg\u201d removes the regularization loss term in Eq. (6). We conduct experiments in both clean and noisy (\u03b1 = 0.5, \u03b2 = 0.8, normal noise) feature scenarios."}, {"title": "6 RELATED WORK", "content": "Unsupervised graph representation learning (UGRL) aims to learn node-level representations from graphs by means of label-free cost, and is widely used in various real-world scenarios [7, 29, 43, 56]. Traditional methods such as deepwalk [35], node2vec [9] generate low-dimensional embedding of nodes based on graph topology and random walk strategy. The recent popular contrastive UGRL methods are inspired by contrastive learning and follow the principle of maximizing mutual information to optimally learn node embeddings [11, 41, 60]. Another line of methods termed generative"}, {"title": "7 CONCLUSIONS", "content": "In this paper, we undertake the first endeavor in unsupervised graph representation learning (URGL) on graph data with noisy features - a challenge that remains largely unexplored in real-world scenarios, despite its inevitability. Through empirical analysis, we uncover the advantages and drawbacks of message propagation in addressing the noisy feature issue, and recognize the importance of estimating the quality of propagated information. Based on these insights, we propose a new UGRL method, termed MQE, which learns reliable node representations by estimating the quality of multi-hop propagated features with a conditional Gaussian model. Extensive experiments have demonstrated the effectiveness of MQE in learning high-quality representations and approximating node-level noise intensity in various noisy feature scenarios."}]}