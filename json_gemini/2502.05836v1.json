{"title": "LegalSeg: Unlocking the Structure of Indian Legal Judgments Through Rhetorical Role Classification", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Govind Sharma", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "abstract": "In this paper, we address the task of semantic segmentation of legal documents through rhetorical role classification, with a focus on Indian legal judgments. We introduce LegalSeg, the largest annotated dataset for this task, comprising over 7,000 documents and 1.4 million sentences, labeled with 7 rhetorical roles. To benchmark performance, we evaluate multiple state-of-the-art models, including Hierarchical BiLSTM-CRF, TransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and Role-Aware Transformers, alongside an exploratory RhetoricLLaMA, an instruction-tuned large language model. Our results demonstrate that models incorporating broader context, structural relationships, and sequential sentence information outperform those relying solely on sentence-level features. Additionally, we conducted experiments using surrounding context and predicted or actual labels of neighboring sentences to assess their impact on classification accuracy. Despite these advancements, challenges persist in distinguishing between closely related roles and addressing class imbalance. Our work underscores the potential of advanced techniques for improving legal document understanding and sets a strong foundation for future research in legal NLP.", "sections": [{"title": "Introduction", "content": "The increasing complexity of legal documents necessitates the use of advanced NLP techniques to aid in their understanding and analysis. Semantic segmentation of legal texts into rhetorical roles is essential for improving the efficiency of legal research, enhancing access to justice, and supporting automated legal decision-making systems. It also facilitates various downstream tasks, such as legal search, summarization, and case analysis. Traditional methods often struggle with the intricacies of legal language, making it imperative to develop models that can accurately classify and interpret these documents. This paper addresses the challenge of semantic segmentation in legal documents, with a focus on the Indian judiciary's legal judgments. Historically, the lack of large-scale annotated datasets has hindered the effective training of state-of-the-art ML models in this domain.\nPrevious research in this domain has highlighted the importance of annotated datasets for training effective models. However, many existing studies have relied on relatively small annotated datasets, limiting their applicability and effectiveness in real-world scenarios. For instance, datasets such as those compiled by Bhattacharya et al. (2019); Kalamkar et al. (2022) and Malik et al. (2022) provided valuable insights but were constrained in size, thereby restricting the scope of their findings. In contrast, this study leverages a newly compiled dataset, LegalSeg, which consists of 7,120 annotated legal documents and 14,87,149 sentences. This dataset is considerably larger than those used in previous research, particularly in terms of its volume and diversity.\nOur contributions to this work are as follows:\n1. Introduction of the LegalSeg dataset, the largest annotated dataset for rhetorical role classification in legal documents.\n2. Implementation and evaluation of SoTA models for semantic segmentation of legal texts.\n3. The development of novel models, including InLegalToBERT, Graph Neural Networks (GNNs), and Role-Aware Transformers, which enhance representation and context handling for rhetorical role classification."}, {"title": "Related Work", "content": "Recent advancements in legal text processing have spurred significant research efforts aimed at automating various tasks such as semantic segmentation, judgment prediction, and summarization of legal documents. However, much of this work relies heavily on manual annotation, with many studies focusing on the intricacies of annotation processes, including the development of annotation guidelines, IAA studies, and the curation of gold standard corpora. For instance, the TEMIS corpus, which consists of 504 sentences annotated both syntactically and semantically, was developed to enhance understanding of legislative texts Venturi (2012). Additionally, an in-depth annotation study highlighted low assessor agreement for labels such as Facts and Reasoning Wyner et al. (2013). In the Indian context, datasets like ILDC Malik et al. (2021), PredEx Nigam et al. (2024) and Nigam et al. (2022); Malik et al. (2022); Nigam et al. (2023a,b) have highlighted the growing role of AI in legal judgments, with an emphasis on explainability. Research in LJP with LLMs, such as Vats et al. (2023) and Nigam et al. (2024), has experimented with models like GPT-3.5 Turbo and LLaMA-2 on Indian legal datasets.\nSeveral efforts have been made to automate the annotation task itself. For example, Wyner (2010) discusses methodologies that employ NLP tools to analyze 47 criminal cases from California courts. Initial experiments aimed at understanding rhetorical roles within court documents were often intertwined with broader goals of document summarization Saravanan et al. (2008).\nFurther contributions include segmenting documents into functional parts (e.g., Introduction, Background) and issue-specific sections \u0160avelka and Ashley (2018). A semi-supervised training method for identifying factual versus non-factual sentences was explored by Nejadgholi et al. (2017) using a fastText classifier. The comparison between rule-based scripts and machine learning approaches"}, {"title": "Task Description", "content": "The goal of this research is to develop models capable of performing semantic segmentation on legal documents by identifying and classifying rhetorical roles (RR) within the text. Let D = {d\u2081,d\u2082,...,d\u2099} represent a collection of legal documents, where d\u1d62 \u2208 D consists of a sequence of sentences S\u1d62 = {S\u1d62\u2081, S\u1d62\u2082 , ..., S\u1d62\u2098}, with m representing the number of sentences in document d\u1d62. The task is to assign a rhetorical role label y\u1d62\u2c7c \u2208 Y to each sentence S\u1d62\u2c7c, where Y is the predefined set of 7 rhetorical role labels.\nFormally, the task can be described as:\nf: S\u1d62 \u2192 Y\nY =< Facts, Issue, Arguments of Petitioner,\nArguments of Respondent, Reasoning,\nDecision, None\nwhere f is a function that maps each sentence S\u1d62\u2c7c in a document d\u1d62 to its corresponding rhetorical role label Y\u1d62\u2c7c. Thus, the goal is to find:\nf(S\u1d62\u2c7c) = Y\u1d62\u2c7c, S\u1d62\u2c7c\u2208 S\u1d62, Y\u1d62\u2c7c \u2208 Y"}, {"title": "Dataset", "content": "In this research, we present the LegalSeg Judgment Dataset, the largest annotated dataset of legal judgments in the English language, specifically focused on rhetorical role segmentation. This dataset represents a significant advancement in the field of legal Natural Language Processing (L-NLP), especially in the context of the Indian judiciary. It aims to address existing gaps in annotation comprehensiveness by offering a rich resource of annotated legal judgments designed to facilitate semantic labeling task."}, {"title": "Dataset Compilation", "content": "The dataset comprises 16,000 legal judgments sourced from the IndianKanoon database, a widely used legal search engine for Indian legal documents. These judgments were collected from the Supreme Court of India and various High Courts, ensuring a diverse selection of cases across multiple domains of law, such as criminal, civil, and constitutional matters.\nDuring the data curation process, several documents were excluded for reasons such as corruption (e.g., containing unrecognized characters or missing segments) or being extremely short, often comprising procedural orders rather than substantive judgments. Additionally, after annotation, the final dataset was refined to 7,120 judgments by removing documents with incomplete or ambiguous annotations, ensuring a high-quality corpus that is also the largest of its kind by a significant margin."}, {"title": "Dataset Preparation and Preprocessing", "content": "To train and evaluate models for this task, the dataset was divided into training, validation, and test sets using a 70-20-10 split, which comprises 4,984, 1,424, and 712 documents correspondingly. This split ensures a robust set of data for both training and evaluating models. Additionally, we computed various statistics regarding the documents and sentences within the dataset, including the average number of sentences per document and token counts. Furthermore, the distribution of rhetorical roles within the dataset is visualized in a pie chart.\nTo improve the performance of our models, we modified the dataset by breaking the documents into individual sentences and assigning each sentence its respective label. For sentence segmentation, we utilized SpaCy."}, {"title": "Annotation Process", "content": "The annotation process was performed by a group of 10 legal experts, consisting of third and fourth year law students selected for their strong academic backgrounds and familiarity with legal processes. The annotation process spanned from April 2022 to October 2023. Each annotator was assigned 10 judgments per week, ensuring detailed attention to every document."}, {"title": "Quality Control", "content": "To ensure the annotation accuracy and consistency, we implemented multiple levels of quality control:"}, {"title": "Annotation Roles", "content": "Legal experts annotated each sentence with one of the following rhetorical roles:\n\u2022 Facts: Sentences that describe the sequence of events that led to the case. These typically involve details of the circumstances and actions related to the case, providing a factual narrative of the case's background, and details about the parties involved, including key dates, events, and parties involved.\n\u2022 Issue: Sentences that outline the legal issues or questions being addressed in the case. These often identify the core legal disputes or points of law that the court must resolve to make a ruling.\n\u2022 Arguments of Petitioner (AoP): Sentences representing the arguments made by the petitioner (the party bringing the case to court). These include claims, reasoning, and justifications presented by the petitioner to support their position and persuade the court to rule in their favor.\n\u2022 Arguments of Respondent (AoR): Sentences that summarize the arguments made by the respondent (the party defending against the case). Like the petitioner's arguments, these statements offer counterpoints, legal interpretations, and rebuttals designed to challenge the petitioner's claims and persuade the court to rule in the respondent's favor.\n\u2022 Reasoning: Sentences that provide the rationale or reasoning behind the court's decision. This includes the application of legal principles and precedents, as well as the logic that connects the facts and arguments to the final ruling. This label captures how the court justifies its decision in light of the legal issues presented.\n\u2022 Decision: Sentences that reflect the final ruling or judgment of the court. This label marks the conclusion of the case, where the court issues its verdict or order, stating the outcome of the case based on its reasoning, such as granting relief, compensation, or dismissing the case.\n\u2022 None: Sentences that do not fall under any of the defined rhetorical roles. These sentences may include procedural information, non-substantive remarks, legal jargon, or content that is not directly relevant to the legal analysis or judgment."}, {"title": "Methodology", "content": "This section outlines the methodology employed for the task of semantic segmentation of legal documents via rhetorical roles. We implemented several SoTA methods while also exploring new techniques. These methodologies collectively aim to enhance the model's ability to understand and classify rhetorical roles in legal texts by incorporating structural, contextual, and sequential information. Each technique addresses different aspects of the complex relationships between sentences in legal documents, contributing to more accurate and context-aware classification outcomes."}, {"title": "TransformerOverInLegalBERT (ToInLegalBERT)", "content": "This pipeline is inspired by Marino et al. (2023). While they employed a general-purpose BERT model, we utilized InLegalBERT, a transformer pre-trained specifically on the Indian legal domain. This substitution enhances the model's ability to capture domain-specific nuances, resulting in improved performance. The TransformerOverInLegalBERT (ToInLegalBERT) model follows a hierarchical architecture, consisting of four main components: (i) an InLegalBERT token-level encoder, (ii) a sentence-level positional encoder, (iii) a sentence-level encoder, and (iv) a prediction layer.\nThe process begins by splitting the document into sentences and tokenizing them. Each sentence is then input into the ToInLegalBERT token-level encoder, where the pooled output\u2014specifically, the hidden representation of the [CLS] token\u2014is extracted. These pooled outputs are subsequently fed into the positional layer to create a position-dependent encoding for each sentence within the document. The encoded representations are then passed to the sentence-level encoder, which captures the relationships between sentences in the document, and finally, these outputs are directed to the prediction layer for rhetorical role classification. This method incorporates both the local context of sentences and their position in the document,"}, {"title": "Hierarchical BiLSTM CRF Classifier", "content": "We also implemented the BiLSTM-CRF model proposed by Bhattacharya et al. (2019), which combines Bidirectional Long Short-Term Memory (BiLSTM) with a Conditional Random Field (CRF) layer. The input to this model is sentence embeddings generated using a sent2vec model trained on Indian Supreme Court judgments. These sentence embeddings are passed through a BiLSTM model, which captures sequential dependencies between sentences. The CRF layer on top of the BiLSTM ensures that the predicted rhetorical role labels adhere to the structured nature of legal documents. This model predicts the rhetorical role for each sentence by considering the context provided by neighboring sentences."}, {"title": "Multi-Task Learning (MTL)", "content": "Inspired by the Multi-Task Learning framework proposed by Malik et al. (2022), we adopt an MTL approach where rhetorical role prediction is the main task, and label shift prediction serves as the auxiliary task. The model consists of two components: a label shift detection component and a rhetorical role prediction component. The intuition is that the label shift between sentences (indicating a change in rhetorical role) helps improve role classification. The label shift detection component predicts whether a shift in rhetorical role occurs at the ith sentence, while the rhetorical role classification component predicts the rhetorical role for that sentence. The output from both components is concatenated and passed to the CRF layer for final role predictions. The overall loss function for the MTL model is: L = \u03bbLshift + (1 \u2212 \u03bb)LRR, where Lshift corresponds to label shift prediction, LRR corresponds to rhetorical role classification, and \u03bb is a hyperparameter balancing the two tasks. This method allows the model to learn dependencies between sentences more effectively."}, {"title": "InLegalBERT Variants", "content": "We experimented with different configurations of the InLegalBERT Paul et al. (2023) model to improve performance. These configurations vary in terms of the number of sentences provided as input during training and inference:\n\u2022 InLegalBERT(i): The model is trained and tested using only the current sentence i.\n\u2022 InLegalBERT(i-1, i): The model is trained with the previous sentence i \u2212 1 and the current sentence i.\n\u2022 InLegalBERT(i-2, i-1, i): The model is trained using the previous two sentences i - 2, i - 1 and the current sentence i.\n\u2022 InLegalBERT(i-1, i, i+1): The model is trained with the previous sentence, current sentence, and the next sentence."}, {"title": "Incorporate Previous Sentence and Label", "content": "We further explored methods where we provide the model with additional contextual information. In one variant, we concatenate the current sentence with the previous sentence and the true label of the previous sentence during training. This approach allows the model to leverage contextual information from preceding sentences to make better predictions. Another variant replaces the true label with the predicted label of the previous sentence during inference, simulating real-world conditions where true labels are unavailable. This method helps the model handle prediction errors and learn sequential dependencies between rhetorical roles."}, {"title": "Self-Supervised Pre-Training with Role-Aware Transformers", "content": "We propose a novel Role-Aware Transformer, which extends the standard transformer architecture by integrating role embeddings to represent rhetorical roles such as Facts, Issues, Arguments, and Reasoning. The model is pre-trained in a self-supervised manner on a large corpus of legal documents, allowing it to learn structural and contextual dependencies in legal discourse.\nDuring pre-training, the model predicts masked tokens while leveraging sentence-level role embeddings. Unlike standard transformers, which process sentences without explicit role awareness, our approach incorporates additional role-specific information into the input embeddings. Specifically, each token embedding is enriched with a learned role embedding that represents its rhetorical role, allowing the model to develop a deeper understanding of legal text organization. This enhances the ability to distinguish between similar rhetorical roles and improves overall classification performance."}, {"title": "GNN with Document Context", "content": "To capture the structural relationships between sentences, we propose a method that leverages Graph Neural Networks (GNNs). In this approach, each sentence in a document is represented as a node in a graph, and the edges between nodes are based on sentence order or semantic similarity. Sentence embedding generated via InLegalBERT, a pre-trained language model on the Indian legal domain, serves as a node feature. The GNN processes the graph by propagating information between connected sentences, allowing the model to capture both local and global contextual dependencies. The GNN processes this graph, allowing for information propagation and aggregation across connected sentences, which enhances understanding of interdependencies between sentences."}, {"title": "RhetoricLLaMA", "content": "To leverage the power of LLMs for rhetorical role prediction, we implemented RhetoricLLaMA, an instruction-tuned model based on LLaMA-2-7B Touvron et al. (2023). For this specific task, we fine-tuned the LLaMA-2-7B model on our LegalSeg dataset using instruction-tuning, a method designed to guide the model's understanding of specific tasks through a set of structured instructions.\nTo enhance the model's ability to segment legal documents accurately, we developed a set of 16 instruction sets tailored to the nature of rhetorical role classification in legal texts. These instructions provided the model with explicit guidance on how to handle the different rhetorical roles in a legal document. A complete list of these instruction sets can be found."}, {"title": "Evaluation Metrics", "content": "To evaluate the performance of models, we adopt a set of standard metrics commonly used in classification tasks. For each sentence in the dataset, the predicted label (rhetorical role) is considered correct if it matches the label assigned by the human expert annotator.\nWe utilize macro-averaged Precision, Recall, F-score, Accuracy, and Matthew Correlation Coefficient (MCC) Chicco and Jurman (2020) as our primary evaluation metrics. Macro-averaging involves calculating these metrics for each class separately and then taking their average. This method is particularly beneficial as it prevents bias towards high-frequency classes, ensuring that all rhetorical roles are treated equally in the evaluation process."}, {"title": "Results and Analysis", "content": "In this section, we present the results of our experiments on rhetorical role classification and analyze the performance of different models. summarizes the evaluation metrics for each model."}, {"title": "Model Performance", "content": "Among the evaluated models, the hierarchical BILSTM-CRF achieves the highest overall performance. The sequential nature of BiLSTM allows the model to capture dependencies between sentences, while the CRF layer explicitly models label transitions, refining predictions by enforcing structural coherence. This ability to learn the transition relationships between rhetorical roles plays a crucial role in classification, as labels in legal documents follow a structured sequence. For example, an issue is likely to be followed by supporting arguments and eventually a decision. The ability to maintain coherence in predictions by capturing dependencies between consecutive sentences makes the BiLSTM-CRF model more effective in comparison to models that classify each sentence independently. Prior studies in structured text classification have similarly observed the benefits of explicit modeling of transition relationships between labels,"}, {"title": "Impact of Transition Relationships in Classification", "content": "Our experiments highlight the critical role of transition relationships between rhetorical roles in improving classification performance. Models such as the BiLSTM-CRF explicitly model these transitions, allowing them to maintain coherence in predictions by capturing dependencies between consecutive sentences. This is particularly advantageous because legal documents are highly structured, with rhetorical roles appearing in predictable sequences. In contrast, models that classify each sentence in isolation struggle to maintain contextual consistency, leading to higher misclassification rates.\nFor instance, when a sentence is labeled as an issue, the subsequent sentences are highly likely to be arguments or facts rather than a decision. CRF layers enforce these structural constraints, making BiLSTM-CRF more effective than independent sentence classifiers. This aligns with previous findings in rhetorical role classification, where modeling dependencies between sequential labels significantly improved performance in structured text classification tasks."}, {"title": "Justification for Predicted Labels Showing Higher Performance", "content": "An interesting observation from Table 3 is that models using predicted labels for previous sentences sometimes outperform those using true labels. This initially appears counterintuitive, but a plausible explanation is that during training, both true labels and predicted labels were provided to the model, allowing it to learn effective dependencies. However, during testing, true labels are not available, meaning models trained exclusively with true labels may not learn to handle missing labels during inference. In contrast, models using predicted labels during training are already exposed to prediction noise, making them better adapted to real-world inference conditions where true labels are not available.\nThis suggests that training models to rely on predicted labels during both training and inference improves robustness, as the model learns to correct potential errors in label predictions over multiple steps. However, further research is needed to analyze whether explicitly modeling label uncertainty could further enhance performance."}, {"title": "Impact of Instruction-Tuning in RhetoricLLaMA", "content": "We conducted extensive experiments to analyze the impact of instruction-tuning in RhetoricLLaMA by comparing it against Vanilla LLaMA models in both quantized (4-bits) and unquantized forms. Despite leveraging large-scale pre-trained models, the instruction-tuned RhetoricLLaMA did not achieve the expected performance, suggesting that rhetorical role classification in legal texts requires more specialized adaptations."}, {"title": "Error Analysis", "content": "Our error analysis revealed that the models struggled primarily with distinguishing between closely related rhetorical roles, such as Facts and Reasoning, due to the overlap in their language and structure within legal documents. This challenge is clearly illustrated in the confusion matrix of the Hierarchical BiLSTM-CRF model , which shows frequent misclassifications between these roles. Similarly, confusion between Arguments of Petitioner and Arguments of Respondent was prevalent, as both often exhibit similar language patterns, further complicating accurate classification. Models that incorporated contextual information from preceding or following sentences demonstrated some improvement in reducing these errors, particularly for roles requiring a clear transition, such as Issue and Decision. However, despite this improvement, the context-aware models still encountered difficulties, suggesting that the rhetorical role boundaries within these transitions are not always well-defined. Another critical issue identified was class imbalance. More frequent labels like None and Facts were consistently overpredicted, leading to lower precision for less frequent labels such as Issue and Decision. This imbalance skewed the performance, resulting in models favoring high-frequency roles at the expense of accuracy for underrepresented roles. are provided in the Appendix due to space constraints. These figures further highlight the patterns of misclassification and the impact of various model architectures on error distribution. Addressing these issues, particularly through improved handling of context, mitigating class imbalance, and minimizing the propagation of sequential errors, remains a critical area for future research and model refinement."}, {"title": "Conclusion and Future Work", "content": "In this work, we addressed the challenging task of rhetorical role classification in legal documents by introducing the LegalSeg dataset, the largest annotated dataset for this task. LegalSeg, provides a significant resource for advancing research in this domain. We evaluated multiple models, including RhetoricLLaMA, ToInLegalBERT, Role-aware, and GNNs. Our results show that models incorporating both sequential and contextual information, such as Hierarchical BiLSTM-CRF and ToInLegalBERT, perform best in identifying and classifying rhetorical roles in legal texts. We also demonstrated that adding sentence-level context improves the model's ability to capture transitions between rhetorical roles, reducing errors caused by the inherent similarity between roles like Facts and Reasoning.\nDespite these advancements, our error analysis revealed several challenges, such as misclassification between similar roles and the cascading effect of label prediction errors. Furthermore, class imbalance remains a significant issue, with frequent misclassifications of minority labels.\nFor future work, we aim to explore more sophisticated techniques to handle class imbalance, such as advanced sampling strategies and loss function adjustments. Additionally, refining models' ability to capture long-range dependencies and leveraging more robust pre-training strategies could further enhance the performance of LLMs. We also plan to incorporate more domain-specific knowledge into the models and experiment with cross-domain transfer learning to improve their adaptability across different legal contexts."}, {"title": "Experimental Setup and Hyper-parameters", "content": "We conducted experiments across several models, utilizing different architectures and training techniques tailored to rhetorical role classification tasks. Below, we provide an overview of the key experimental setups and hyper-parameters used."}, {"title": "RhetoricLLaMA Training Procedure", "content": "RhetoricLLaMA, built on the LLaMA-2-7B model, was fine-tuned with Bfloat16 precision using a single A100 GPU with 40GB memory. Given the computational constraints, the model was optimized for efficiency, with training lasting 48 hours. A maximum token length of 1000 was used, and Low-Rank Adaptation (LoRA) was employed with a rank of 16, alpha set to 64, and a dropout rate of 0.1. The model leveraged flash-attention 2 for faster training. We applied a Paged Adam optimizer with a learning rate of le-4 and a cosine learning rate scheduler, along with gradient accumulation steps of 4. The model trained for 52,617 steps, corresponding to 3 epochs."}, {"title": "Transformers Training Hyper-parameters", "content": "For the Role-Aware Transformers, built upon the InLegalBERT model, pre-training involved self-supervised tasks such as Masked Language Modeling (MLM) with role embeddings added. The model processed a maximum sequence length of 512 tokens with a batch size of 4, running for 20 epochs. The learning rate was set to 2e-5, using the AdamW optimizer. Class weights were applied to handle the imbalance in rhetorical roles, and early stopping was used to prevent overfitting."}, {"title": "Graph Neural Networks (GNN) with Document Context", "content": "We utilized a Graph Neural Network (GNN) architecture to model sentence relationships within legal documents. A two-layer Graph Convolutional Network (GCN) processed sentence embeddings from InLegalBERT. The first and second GCN layers both had output dimensions of 128, using ReLU activations. The model was trained for 10 epochs with a learning rate of 1e-4 and employed a Cross-Entropy Loss function. Graphs were constructed with edges between consecutive sentences, capturing both sequential and semantic relationships."}, {"title": "Incorporating Previous Sentence and Actual Label", "content": "In this method, the input to the model combined the current sentence with the previous sentence and its actual rhetorical role label. The model used InLegalBERT with a maximum sequence length of 512 tokens, trained for 5 epochs with a learning rate of 2e-5. This approach provided explicit sequential context and utilized Cross-Entropy Loss with class weights to manage class imbalance."}, {"title": "Incorporating Previous Sentence and Predicted Label", "content": "Extending the previous approach, this variant incorporated the predicted label of the previous sentence, simulating real-world conditions. The same configuration was used as in the previous model, but the predicted label replaced the actual label during both training and inference."}, {"title": "Common Settings Across Models", "content": "All models were evaluated using Accuracy, Precision, Recall, F1 Score, and Matthews Correlation Coefficient (MCC). The experiments utilized PyTorch and Hugging Face Transformers libraries, with PyTorch Geometric handling the graph data in the GNN method. All models were trained on machines with NVIDIA GPUs for parallel computation."}]}