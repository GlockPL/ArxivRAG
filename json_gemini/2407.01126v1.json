{"title": "Investigating the potential of Sparse Mixtures-of-Experts for multi-domain neural machine translation", "authors": ["Nadezhda Chirkova", "Vassilina Nikoulina", "Jean-Luc Meunier", "Alexandre B\u00e9rard"], "abstract": "We focus on multi-domain Neural Machine Translation, with the goal of developing efficient models which can handle data from various domains seen during training and are robust to domains unseen during training. We hypothesize that Sparse Mixture-of-Experts (SMoE) models are a good fit for this task, as they enable efficient model scaling, which helps to accommodate a variety of multi-domain data, and allow flexible sharing of parameters between domains, potentially enabling knowledge transfer between similar domains and limiting negative transfer. We conduct a series of experiments aimed at validating the utility of SMoE for the multi-domain scenario, and find that a straightforward width scaling of Transformer is a simpler and surprisingly more efficient approach in practice, and reaches the same performance level as SMoE. We also search for a better recipe for robustness of multi-domain systems, highlighting the importance of mixing-in a generic domain, i.e. Paracrawl, and introducing a simple technique, domain randomization.", "sections": [{"title": "1. Introduction", "content": "The goal of multi-domain Neural Machine Translation (NMT) is to build a system capable of generating high-quality translations for inputs from various domains. Such domains are given in advance and are represented by different sources of data, eg. European Parliament speeches, scientific articles, etc. At the test time, input examples may come with or without known domain labels, depending on the practical scenario.\nMost popular approaches to equip multi-domain NMT model with domain knowledge do so either via domain tags or domain adapters. When domain knowledge is integrated via domain tags, prepended to input sequences as special tokens, the model is able to benefit fully from knowledge transfer between domains via shared representations. However, the model may suffer from negative transfer if it does not have enough parameters to accommodate contradictory signals from different domains. Domain adapters provide a separate set of parameters per each domain, therefore increasing the model size and being robust to negative transfer, but may not benefit from potential knowledge transfer across different domains.\nIn this work, we investigate the potential of Sparse Mixture-of-Experts models (SMoE, for multi-domain machine translation. SMoE belongs to a family of conditional-computation models, and uses a gating mechanism to decide which subset of model parameters will be activated for each input token. SMoE seems to be a good fit for multi-domain scenario due to several reasons. First, SmoE scales the model size, which appears to be necessary for a variety of multi-domain data the model needs to accommodate, but keeps constant the inference floating-point operations (FLOPs), thus enabling efficient scaling. Second, SMoE looks as a reasonable and effective middle ground between domain tags and domain adapters, due to the enabled \u201csoft\u201d sharing of parameters between domains. In particular, SMoE has a potential to learn which domains should be processed with the same set of parameters, i.e. to enable knowledge transfer between related domains, and which domains should not share parameters, i.e. to prevent negative transfer. To fully benefit from the domain information when it is available, we study three ways of utilizing domain labels in SMoE. We conduct series of experiments, aiming to validate the hypothesis that SMoE is an effective and efficient architecture for the multi-domain scenario. Our experiments lead us to non-conclusive results regarding the utility of the SMOE architecture, but highlight the advantages of model scaling, particularly straightforward width scaling, in both effectiveness and inference time efficiency.\nOur findings can be summarized as follows:\n\u2022 Effectiveness. SMoE significantly outperforms the baseline Transformer Base architecture, mostly due to the model scaling effect: SMoE performs on par with the horizontally scaled model of the same total size (width scaling of the Transformer dimension);\n\u2022 Efficiency. For models that fit into a single GPU, a straightforward width scaling of the Transformer dimension introduces almost no computational overhead compared to the non-scaled model, when modern GPUs such as Tesla A100 or V100 are used. On the contrary, SMoEs do introduce a substantial computational overhead at inference. As a result, the horizontally scaled model turns out to be an efficient scaling approach.\n\u2022 Domain knowledge in SMoE. We introduce and compare several methods for incorporating domain knowledge into SMoE and find that that the simplest approach of using domain tags performs similarly to the more advanced approaches (and to the horizontally scaled model also equipped with domain tags);\n\u2022 Robustness. We search for a recipe for more robust multi-domain systems, i.e. robust to out-of-domain examples and wrong domain labels, and highlight two important components. First, we show that mixing-in a generic domain improves model performance on out-of-domain examples. Second, we introduce a simple technique, domain randomization, targeted at enriching out-of-domain robustness. We observe that it also substantially increases robustness to wrong domain labels."}, {"title": "2. Related work", "content": "Multi-domain Neural Machine Translation. In Multi-domain NMT we assume that our training data comes from different sources, either with well identified domains, or more generic ones (eg. web-crawled data). The goal is to exploit this knowledge efficiently during training and/or inference. Berard et al. specify domain information via special domain tags prepended to the source or target sequence. Thus the model has access to domain information by accessing this domain tag via the attention mechanism, and its parameters are shared across all domains. Several works have tried to decouple domain-specific representations from domain independent representations by proposing different architectural changes and/or adversarial training.\nMost of the above mentioned approaches to multi-domain NMT are only interested in optimizing the performance on the domains observed during training and do not consider the performance of such models on new previously unseen domains. Moreover, these works assume that the domain is well identified at inference time, while in real life serrings NMT model may have to deal with wrong or unspecified domain label, or mixed-domain inputs.\nPham et al.  propose domain resampling strategies attempting to maximize performance across all the domains, and acting as a regularization to improve robustness to unseen domains. We address robustness from another (complementary) perspective, by including a generic domain in the training data and by explicitly encouraging models' reliance on generic data. We also study a wider and more diverse set of unseen domains. Pham et al. has performed and extensive benchmark of existing multi-domain NMT approaches by explicitly evaluating their Effectiveness (benefit from domain knowledge availability and benefit from domain transfer between training domains) and Robustness to the fuzzy or unknown domains at inference time. Authors concluded that none of the existing approaches is competitive with simply training a model on a mixture of all datasets and further fine-tuning on each domain's dataset, which implies that in existing approaches the impact of negative transfer across domains is higher than benefit of the positive transfer between training domains.\nMixture of Experts models for Neural Machine Translation. The Mixture-of-Experts architecture has been shown to be an efficient way for models parameters scaling . It has demonstrated promising results in NMT for multilingual settings , where efficient scaling of the parameters allowed to cope with the curse of multilinguality while keeping reasonable inference cost. To the best of our knowledge, the potential of SMoE architecture has not been explored in multi-domain NMT. Pham et al. propose an approach which learns the selection of domain-specific parameters end-to-end so that domains can share a part of parameters. The SMoE architecture in fact generalizes this idea further by enabling the selection of parameters specific to a combination of a token and a domain.\nIn this work we study whether Sparse Mixture of Experts architectures are beneficial in multi-domain NMT settings, and to what extent domain information is important when we scale up the model."}, {"title": "3. Methodology", "content": "Following , we define domains as different sources of data used to train a machine translation model. The effectiveness of the model is defined as the performance of the model on the seen domains, i.e. the domains included in the training data. We also evaluate out-of-domain robustness of the model, i.e. the performance on the domains unseen during training, i.e. on some held-out test domain data from different data distribution than the training domains. At the same time, we care about model efficiency, i.e. inference speed, as it is crucial in production systems."}, {"title": "3.1. Mixture of Experts", "content": "Sparsely-gated Mixture-of-Experts (SMoE) models activate a subset of their parameters per input token, contrary to dense models, where the entire network is used for each input token. Therefore, the total amount of parameters can be significantly increased with limited impact on the computational cost.\nIn the SMoE Transformer models proposed by Lepikhin et al. , the FFN sublayers in the dense model are replaced with SMoE layers consisting on N experts each (with the same architecture as an FFN layer). A SMOE layer takes an input token representation xt and then routes it to the top k out of N experts according to a learned gating mechanism, which computes a probability distribution over all the experts:\n$G_t = softmax(W_g x_t)$.\nThe output of the SMoE layer is a weighted sum of the outputs of the top k selected experts &:\n$Y_t = \\frac{1}{\\sum_{i \\in E} G_{ti}} \\sum_{i \\in E} G_{ti} E_i (x_t)$"}, {"title": "3.2. Domain Knowledge integration", "content": "Domain knowledge is presented to the model in the form of a domain label d. We compare different ways of integrating such domain label into the SMoE model, see their illustration in Figure 1.\nDomain Tags prepend domain embedding ed to the source sequence similar to . In this settings domain embeddings represent the only domain-specific parameters, the rest of the parameters are shared across all the domains. The model attends to domain information via self- and cross-attention during training and inference and modifies its internal representations accordingly.\nDomain Specialized Gate Gupta et al. introduce separate task-specialized gates in SMoE for multitask training of language models with the goal to encourage different experts to specialize to different tasks, while limiting the negative transfer between tasks. Similarly, we consider domain-specific gates Gd that devote specialized gating mechanism parameters (W) to each domain. In multi-domain settings, such specialization is expected to encourage the model to a select different set of experts for different domains.\nDomain-aware Gate. A fully domain-specialized gating mechanism may limit knowledge transfer between different domains. Moreover, learning gating mechanism parameters can be more challenging for very small domains, with very small amount of samples in the train-ing data. Therefore we propose a Domain-aware gating mechanism as an alternative to the domain-specialized gate. It takes as input the concatenation of the current token representation x\u2081 and the domain embedding ed:\n$G_t(d) = softmax(W_g . (x_t \\oplus e_d))$"}, {"title": "3.3. Domain randomization", "content": "As discussed above, our training data includes several given domains, as well as a large generic domain.\nIn practice, we would like to rely on the generic domain to translate text from unseen domains. In order to make the generic domain representation more robust to unseen domains, we introduce domain randomization during training.\nIt consists in randomly assigning training examples from well-defined domains to the generic domain, with some probability p. In our experiments, we use p = 0.5. We expect such randomization will allow the model to associate the generic tag with richer data samples and particularly to recognize situations when a test example is close to domains seen during training.\nBy introducing domain randomization, we assume that using only a half of sampled data points from a domain is enough to train a small amount of domain-specific parameters (domain tags, embeddings, or domain-specific gating mechanism), and our experiments confirm this assumption: using domain randomization improves robustness without hurting performance on seen domains. Another assumption is that the model capacity is sufficient to accommodate all the seen domains and at the same time associate the generic domain with all the seen domains. This assumption is well aligned with the suggested model scaling."}, {"title": "4. Experimental setup", "content": "Data. We experiment with the German-English (De-En), English-German (En-De) and English-French (En-Fr) translation directions. All models are trained on a mix of generic data (Paracrawl, and several seen domains: \"Law\", \"Medical\", \"Ted\", \"Subtitles\", and \"Patents\". The selection of seen domains is motivated by having diverse domains of different sizes and having a pair of close domains, \"Ted\" and \"Subtitles\" (they both relate to spoken language). We also test the generalization capabilities of models on a set of unseen domains: \"Flores\", \"IT\", \"Koran\", \"Patents (medical)\", \"Subtitles (Medical)\", and \"Medical abstracts\". The purpose of introducing the last three unseen domains is to test models' capabilities to recognize unseen domains related to some seen domains (\"Medical\", \"Patents\", and \"Subtitles\").\nWe deduplicate train-test splits to avoid data contamination, i.e. that test data does not leak into training data. We provide further details about datasets in the Appendix A.\nModel hyperparameters. For De-En and En-Fr translation, we use standard Transformer Base hyperparameters from Vaswani et al. : 6 encoder layers and 6 decoder layers, 8 heads, dmodel = 512, and dff = 2048. For En-De, we test the applicability of SMoE for a shallow configuration with only 3 decoder layers, which was shown to be much faster at inference, with a negligible performance drop . SMoE model uses 10 experts every second layer with top-2 token-level gating. Training details are given in Appendix A.\nModels. We consider two settings of multidomain NMT models: with access to the labels of seen domain and without it. In the setting where domain labels are not used, we compare the commonly-used Transformer Base , SMoE, and two additional dense variants: Transformer width \u00d71.5 (matches SMOE inference FLOPs SMOE)\u00b9 and Transformer width \u00d75 (matches total parameter count of SMoE). By increasing width we mean increasing dff.\nIn the setting with domain labels being available during both training and inference, we compare three ways of utilizing domain information in SMoE (see Section 3) with and without proposed domain randomization, and two commonly used baselines: Transformer with domain tags and Transformer with domain adapters. We treat domain adapters as domain-specific parameters and train them together with the rest of model parameters. We set the adapter dimension to 2048, to match the number of inference FLOPs and parameters with SMoE. The number of parameters is matched because the number of experts we use equals the number of seen domains.\nEvaluation. For each model, we report BLEU scores measured with SacreBLEU in the main text and COMET scores in Appendix, as well as inference FLOPs and the number of parameters. To estimate the number of floating point operations (FLOPs), we count matrix multiplications in the forward pass, assuming source and target sequence lengths of 10."}, {"title": "5. Results", "content": "5.1. Study of scaling methods"}]}