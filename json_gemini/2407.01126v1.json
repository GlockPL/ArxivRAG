{"title": "Investigating the potential of Sparse Mixtures-of-Experts for multi-domain neural machine translation", "authors": ["Nadezhda Chirkova", "Vassilina Nikoulina", "Jean-Luc Meunier", "Alexandre B\u00e9rard"], "abstract": "We focus on multi-domain Neural Machine Translation, with the goal of developing efficient models which can handle data from various domains seen during training and are robust to domains unseen during training. We hypothesize that Sparse Mixture-of-Experts (SMoE) models are a good fit for this task, as they enable efficient model scaling, which helps to accommodate a variety of multi-domain data, and allow flexible sharing of parameters between domains, potentially enabling knowledge transfer between similar domains and limiting negative transfer. We conduct a series of experiments aimed at validating the utility of SMoE for the multi-domain scenario, and find that a straightforward width scaling of Transformer is a simpler and surprisingly more efficient approach in practice, and reaches the same performance level as SMoE. We also search for a better recipe for robustness of multi-domain systems, highlighting the importance of mixing-in a generic domain, i.e. Paracrawl, and introducing a simple technique, domain randomization.", "sections": [{"title": "1. Introduction", "content": "The goal of multi-domain Neural Machine Translation (NMT) is to build a system capable of generating high-quality translations for inputs from various domains. Such domains are given in advance and are represented by different sources of data, eg. European Parliament speeches, scientific articles, etc. At the test time, input examples may come with or without known domain labels, depending on the practical scenario.\nMost popular approaches to equip multi-domain NMT model with domain knowledge do so either via domain tags [3,5,13] or domain adapters [2]. When domain knowledge is integrated via domain tags, prepended to input sequences as special tokens, the model is able to benefit fully from knowledge transfer between domains via shared representations. However, the model may suffer from negative transfer if it does not have enough parameters to accommodate contradictory signals from different domains. Domain adapters provide a separate set of parameters per each domain, therefore increasing the model size and being robust to negative transfer, but may not benefit from potential knowledge transfer across different domains.\nIn this work, we investigate the potential of Sparse Mixture-of-Experts models (SMoE, 24) for multi-domain machine translation. SMoE belongs to a family of conditional-computation models, and uses a gating mechanism to decide which subset of model parameters will be activated for each input token. SMoE seems to be a good fit for multi-domain scenario due to several reasons. First, SmoE scales the model size, which appears to be necessary for a variety of multi-domain data the model needs to accommodate, but keeps constant the inference floating-point operations (FLOPs), thus enabling efficient scaling. Second, SMoE looks as a reasonable and effective middle ground between domain tags and domain adapters, due to the enabled \u201csoft\u201d sharing of parameters between domains. In particular, SMoE has a potential to learn which domains should be processed with the same set of parameters, i.e. to enable knowledge transfer between related domains, and which domains should not share parameters, i.e. to prevent negative transfer. To fully benefit from the domain information when it is available, we study three ways of utilizing domain labels in SMoE. We conduct series of experiments, aiming to validate the hypothesis that SMoE is an effective and efficient architecture for the multi-domain scenario. Our experiments lead us to non-conclusive results regarding the utility of the SMOE architecture, but highlight the advantages of model scaling, particularly straightforward width scaling, in both effectiveness and inference time efficiency.\nOur findings can be summarized as follows:\n\u2022 Effectiveness. SMoE significantly outperforms the baseline Transformer Base architecture, mostly due to the model scaling effect: SMoE performs on par with the horizontally scaled model of the same total size (width scaling of the Transformer dimension);\n\u2022 Efficiency. For models that fit into a single GPU, a straightforward width scaling of the Transformer dimension introduces almost no computational overhead compared to the non-scaled model, when"}, {"title": "2. Related work", "content": "Multi-domain Neural Machine Translation. In Multi-domain NMT we assume that our training data comes from different sources, either with well identified domains, or more generic ones (eg. web-crawled data). The goal is to exploit this knowledge efficiently during training and/or inference. Berard et al. [3],Britz et al. [5], Kobus et al. [13],Tars and Fishel [26] specify domain information via special domain tags prepended to the source or target sequence. Thus the model has access to domain information by accessing this domain tag via the attention mechanism, and its parameters are shared across all domains. Several works [5,6,11] have tried to decouple domain-specific representations from domain independent representations by proposing different architectural changes and/or adversarial training.\nMost of the above mentioned approaches to multi-domain NMT are only interested in optimizing the performance on the domains observed during training and do not consider the performance of such models on new previously unseen domains. Moreover, these works assume that the domain is well identified at inference time, while in real life serrings NMT model may have to deal with wrong or unspecified domain label, or mixed-domain inputs.\nPham et al. [20], Wang et al. [30] propose domain resampling strategies attempting to maximize performance across all the domains, and acting as a regularization to improve robustness to unseen domains. We address robustness from another (complementary) perspective, by including a generic domain in the training data and by explicitly encouraging models' reliance on generic data. We also study a wider and more diverse set of unseen domains. Pham et al. [19] has performed and extensive benchmark of existing multi-domain NMT approaches by explicitly evaluating their Effectiveness (benefit from domain knowledge availability and benefit from domain transfer between training domains) and Robustness to the fuzzy or unknown domains at inference time. Authors concluded that none of the existing approaches is competitive with simply training a model on a mixture of all datasets and further finetuning on each domain's dataset, which implies that in existing approaches the impact of negative transfer across domains is higher than benefit of the positive transfer between training domains.\nMixture of Experts models for Neural Machine Translation. The Mixture-of-Experts architecture has been shown to be an efficient way for models parameters scaling [7,12]. It has demonstrated promising results in NMT for multilingual settings [12, 15, 18], where efficient scaling of the parameters allowed to cope with the curse of multilinguality while keeping reasonable inference cost. To the best of our knowledge, the potential of SMoE architecture has not been explored in multi-domain NMT. Pham et al. [21] propose an approach which learns the selection of domain-specific parameters end-to-end so that domains can share a part of parameters. The SMoE architecture in fact generalizes this idea further by enabling the selection of parameters specific to a combination of a token and a domain.\nIn this work we study whether Sparse Mixture of Experts architectures are beneficial in multi-domain NMT settings, and to what extent domain information is important when we scale up the model."}, {"title": "3. Methodology", "content": "Following [19], we define domains as different sources of data used to train a machine translation model. The effectiveness of the model is defined as the performance of the model on the seen domains, i.e. the domains included in the training data. We also evaluate out-of-domain robustness of the model, i.e. the performance on the domains unseen during training, i.e. on some heldout test domain data from different data distribution than the training domains. At the same time, we care about model efficiency, i.e. inference speed, as it is crucial in production systems."}, {"title": "3.1. Mixture of Experts", "content": "Sparsely-gated Mixture-of-Experts (SMoE) models activate a subset of their parameters per input token, contrary to dense models, where the entire network is used for each input token. Therefore, the total amount of parameters can be significantly increased with limited impact on the computational cost.\nIn the SMoE Transformer models proposed by Lepikhin et al. [16], the FFN sublayers in the dense model are replaced with SMoE layers consisting on N experts each (with the same architecture as an FFN layer). A SMOE layer takes an input token representation xt and then routes it to the top k out of N experts according to a learned gating mechanism, which computes a probability distribution over all the experts:\nGt = softmax(Wgxt).\nThe output of the SMoE layer is a weighted sum of the outputs of the top k selected experts &:\n \\begin{equation} Yt = \\frac{1}{\\sum_{i \\in \\epsilon} Gti} \\sum_{i \\in \\epsilon}Gt,Ei (xt) \\end{equation}\nIn addition to the efficient scaling, the SMoE architecture may provide more flexibility in multi-domain settings: if it is able to specialize experts for different domains it would lead to lower negative transfer between training domains."}, {"title": "3.2. Domain Knowledge integration", "content": "Domain knowledge is presented to the model in the form of a domain label d. We compare different ways of integrating such domain label into the SMoE model, see their illustration in Figure 1.\nDomain Tags prepend domain embedding ed to the source sequence similar to [5,13]. In this settings domain embeddings represent the only domain-specific parameters, the rest of the parameters are shared across all the domains. The model attends to domain information via self- and cross-attention during training and inference and modifies its internal representations accordingly.\nDomain Specialized Gate Gupta et al. [8] introduce separate task-specialized gates in SMoE for multitask training of language models with the goal to encourage different experts to specialize to different tasks, while limiting the negative transfer between tasks. Similarly, we consider domain-specific gates Gd that devote specialized gating mechanism parameters (W) to each domain. In multi-domain settings, such specialization is expected to encourage the model to a select different set of experts for different domains.\nDomain-aware Gate. A fully domain-specialized gating mechanism may limit knowledge transfer between different domains. Moreover, learning gating mechanism parameters can be more challenging for very small domains, with very small amount of samples in the train data. Therefore we propose a Domain-aware gating mechanism as an alternative to the domain-specialized gate. It takes as input the concatenation of the current"}, {"title": "3.3. Domain randomization", "content": "As discussed above, our training data includes several given domains, as well as a large generic domain.\nIn practice, we would like to rely on the generic domain to translate text from unseen domains. In order to make the generic domain representation more robust to unseen domains, we introduce domain randomization during training.\nIt consists in randomly assigning training examples from well-defined domains to the generic domain, with some probability p. In our experiments, we use p = 0.5. We expect such randomization will allow the model to associate the generic tag with richer data samples and particularly to recognize situations when a test example is close to domains seen during training.\nBy introducing domain randomization, we assume that using only a half of sampled data points from a domain is enough to train a small amount of domain-specific parameters (domain tags, embeddings, or domain-specific gating mechanism), and our experiments confirm this assumption: using domain randomization improves robustness without hurting performance on seen domains. Another assumption is that the model capacity is sufficient to accommodate all the seen domains and at the same time associate the generic domain with all the seen domains. This assumption is well aligned with the suggested model scaling."}, {"title": "4. Experimental setup", "content": "Data. We experiment with the German-English (De-En), English-German (En-De) and English-French (En-Fr) translation directions. All models are trained on a mix of generic data (Paracrawl, 1) and several seen domains: \"Law\", \"Medical\", \"Ted\", \"Subtitles\", and \"Patents\". The selection of seen domains is motivated by having diverse domains of different sizes and having a pair of close domains, \"Ted\" and \"Subtitles\" (they both relate to spoken language). We also test the generalization capabilities of models on a set of unseen domains: \"Flores\", \"IT\", \"Koran\", \"Patents (medical)\", \"Subtitles (Medical)\", and \"Medical abstracts\". The purpose of introducing the last three unseen domains is to test models' capabilities to recognize unseen domains related to some seen domains (\"Medical\", \"Patents\", and \"Subtitles\").\nWe deduplicate train-test splits to avoid data contamination, i.e. that test data does not leak into training"}, {"title": "5. Results", "content": null}, {"title": "5.1. Study of scaling methods", "content": "Tables 1 and 2 report German-English results for seen and unseen domains respectively. In the Appendix, we"}, {"title": "6. Conclusion", "content": "In this work we investigate the potential of the SMOE architecture for effective, efficient and robust multi-domain translation. Our experiments lead us to generally non-conclusive results regarding the utility of SMoE but highlight the necessity of model scaling and advantages of the straightforward width scaling. The width-scaled model performs on par with SMoE and adds almost no overhead at inference on modern GPUs compared to the non-scaled model. At the same time, for older and less optimized GPUs, SMoE may be a more efficient scaling approach. We also introduce domain randomization, a technique well suitable for scaled models which improves robustness on unseen domains, while preserving same performance on seen domains. We also find that DR improves robustness to wrong domain labels."}, {"title": "7. Limitations and discussion", "content": "This work is limited to the setting where all the seen domains are available at training time, i.e. we do not consider adaptation to newly appearing domains scenario after the model is trained. We leave this aspect for the future work.\nAnother limitation is that the notion of domain we rely on is restricted to the origin of the data. These labels can"}]}