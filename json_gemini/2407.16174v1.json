{"title": "Pixel Embedding: Fully Quantized Convolutional Neural Network with Differentiable Lookup Table", "authors": ["Hiroyuki Tokunaga", "Joel Nicholls", "Daria Vazhenina", "Atsunori Kanemura"], "abstract": "By quantizing network weights and activations to low bitwidth, we can obtain hardware-friendly and energy-efficient networks. However, existing quantization techniques utilizing the straight-through estimator and piecewise constant functions face the issue of how to represent originally high-bit input data with low-bit values. To fully quantize deep neural networks, we propose pixel embedding, which replaces each float-valued input pixel with a vector of quantized values by using a lookup table. The lookup table or low-bit representation of pixels is differentiable and trainable by backpropagation. Such replacement of inputs with vectors is similar to word embedding in the natural language processing field. Experiments on ImageNet and CIFAR-100 show that pixel embedding reduces the top-5 error gap caused by quantizing the floating points at the first layer to only 1% for the ImageNet dataset, and the top-1 error gap caused by quantizing first and last layers to slightly over 1% for the CIFAR-100 dataset. The usefulness of pixel embedding is further demonstrated by inference time measurements, which demonstrate over 1.7 times speedup compared to floating point precision first layer.", "sections": [{"title": "1 Introduction", "content": "After achieving record-breaking performance in various tasks represented by image classification [Krizhevsky et al., 2012; He et al., 2016], deep learning is expected to operate in every aspect of our life. Reducing neural network footprint to fit into embedded devices enables use of deep learning techniques in e.g. wearable devices, drones, or vehicles, which are often isolated from the network and have limited computational resources.\nThis paper focuses on quantization as a solution for accelerating inference in neural networks for embedded, on-device applications [Guo et al., 2017]. Quantized neural networks utilize low-bit parameters and operations, greatly reducing the model size and computation necessary for inference.\nWith FPGAs or customized ASICs, low-bit operations can be efficiently implemented in logical circuits with bit operations. A 32 bit floating point multiplication unit consumes more than 50 times more logic elements than 4 bit integer [Guo et al., 2017]. This means that the theoretical speed up factor of quantized computation for FPGAs or customized ASICs is over 50, if memory bandwidth is not a bottleneck.\nQuantizing the first convolutional layer is one of the largest technical issues for accelerating quantized neural networks. Indeed, work by Liu et al. [2019] shows that the first layer inputs are typically less resilient to quantization compared to the other layers.\nMost works on neural network quantization keep the first layer as non-quantized to avoid severe accuracy drops [Hubara et al., 2016; Rastegari et al., 2016]. However, utilizing high precision for the first layer is not an effective solution. In the case of FPGAs or ASICs, the first non-quantized convolutional layer is a huge performance bottleneck, since the intermediate (quantized) layers have been accelerated. We found that the non-quantized first convolutional layer of the ResNet-18 classification network consumes 54% of wall clock time on our FPGA-based neural network accelerator.\nIn this paper, we propose a technique we term pixel embedding to quantize both weights and activations of the first convolutional layer of deep convolutional neural networks (CNNs), improving the accuracy loss compared to the existing quantization methods. The pixel embedding method extends an idea from word embeddings in natural language processing [Bengio et al., 2003] to quantization.\nPixel embedding replaces input data with a more easily quantizable form (Section 3). The resulting operation at inference time is a lookup table of low-bit values. Experiments on CIFAR-100 and ImageNet show that pixel embedding achieves state-of-the-art for quantizing the first layer compared to the existing quantization methods (Section 4). Our main contributions are:\n\u2022 With the pixel embedding method, we show that it is possible to quantize both weights and input data for the first convolutional layer, breaking a general belief that quantizing the first or last layers is difficult.\n\u2022 We examine the strength of the pixel embedding method"}, {"title": "2 Existing quantization techniques", "content": "Convolutional neural networks are computationally heavy algorithms. It is desirable to reduce energy consumption and memory requirement by executing computation in lower precision like 8 bit integer [Krishnamoorthi, 2018], instead of 32 bit floating point. Such low-bit approximation is called quantization [Hubara et al., 2017]. Taking this to the extreme, 1 bit integer can be employed [Hubara et al., 2016; Rastegari et al., 2016]. This paper is concerned with low bitwidth quantization, which can be used for acceleration of neural networks on dedicated hardware.\nQuantization-aware training uses a combination of techniques that allows for very low-precision convolutions, while keeping high accuracy [Hubara et al., 2017; Zhou et al., 2016]. To learn quantized weight values during training, a quantization operator is used on the high bitwidth weights in the forward pass of training. For backpropagation, the straight-through estimator (STE) method [Bengio et al., 2013; Hubara et al., 2016] can be used. An STE can be thought of as an operator that has mismatched forward and backward operations.\nIn addition to weights, the activations of convolutional layers can also be quantized during training, and this quantization operation remains in the inference stage [Hubara et al., 2016], unlike the weight quantization operation. Piece-wise constant functions are used for the quantization of activations. These operations have low computation cost and have the property that similar activations get mapped to the same quantized values, preserving the notion of closeness.\nThe combination of both weight and activation quantization allows for convolution to be performed using low-bitwidth operations, giving a great boost to the algorithm's speed and efficiency. Although the first layer is typically relatively small compared to the neural network as a whole, using a floating point first layer becomes a serious bottleneck when the other layers have been optimized using quantization. Indeed, the first floating point layer takes up 54% of the inference time in our implementation.\nAlthough most previous works have used fully floating point precision for the first layer convolution, there are exceptions. In Zhou et al. [2016], a network with binary weights and 8-bit inputs is implemented and evaluated. Using binary weights eliminates multiplications, reducing the convolution operation to integer additions.\nIn the work of D\u00fcrichen et al. [2018], each integer input is separated bit-wise into separate channels and convolution is performed between those binary activations and binary weights (called direct binary input data), then an additional 1 \u00d7 1 convolution is used (called binary input layer). The binary input layer result for classification on the CIFAR-10 dataset incurs a loss of 4.58% accuracy; however, impressive results were achieved for the physical activity monitoring (PAMAP2) dataset, actually decreasing validation error by 1.92%."}, {"title": "3 Fully quantized networks", "content": "Pixel embedding is inspired by word embedding in the natural language processing field [Bengio et al., 2003], where each word is associated with a real-valued vector. Embeddings are powerful because they can be learned by backpropagation. To understand this, we need to introduce the concepts of 1-hot representations and embedding lookup tables. Using these two concepts, we then describe pixel embedding."}, {"title": "3.1 1-hot Representation", "content": "Let $w \\in V$ be a word id, where $V = \\{0,1,..., N - 1\\}$ is a vocabulary comprising all ids of unique words. The set of N-length 1-hot vectors is\n$\\mathbb{H}_N = \\{h \\in \\{0,1\\}^N | \\sum_{n=1}^N h_n = 1\\}.$\nThat is, only one element is 1 and the other elements are 0.\nLet $h: V \\rightarrow \\mathbb{H}_N$ be a vector-valued function that maps a word $w \\in V$ to its 1-hot representation. That is, $h(w) \\in \\mathbb{H}_N$ and $h_n(w) = 1$ if $n = w$ and $h_n(w) = 0$ if $n \\neq w$. For example, $h(1) = [0,1,0, . . .]^T$ (note that indexing is 0-based)."}, {"title": "3.2 Embedding Lookup Table", "content": "We denote the embedding of $w \\in V$ by $e_w \\in \\mathbb{R}^d$, where $d > 0$ is the dimensionality of the embedding space and $\\mathbb{R}$ is the set of real numbers. We can formulate the conversion process as a matrix multiplication. Let\n$E = [e_0, e_1, ..., e_{N-1}] \\in \\mathbb{R}^{d \\times N}$\nbe a matrix whose columns are embedding vectors. The matrix $E$ is called an embedding lookup table. Then, using the 1-hot representation, we have\n$e_w = E h(w).$\nThe matrix-vector formulation allows us to learn embeddings through backpropagation, because \u201cchoosing a column from a matrix\u201d becomes a differentiable operation. Several word embedding algorithms made use of the fact that embedding lookup tables can be optimised with backpropagation to learn embeddings [Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014]."}, {"title": "3.3 Pixel Embedding", "content": "Although an image pixel is usually represented by $[P_R, P_G, P_B] \\in \\{0,1,...,255\\}^3$, where each of the R, G, B channels has 8 bits, we can employ 1-hot representation to"}, {"title": "3.4 Quantizing the last layer", "content": "For the task of classification, quantization of the final layer is fairly straightforward. The immediate output of a quantized layer is integer-valued. The integer outputs of a final quantized layer can be fed directly to the loss function for network training. Note that for the inference stage, the softmax function can be removed and integer comparison can be used to give the class prediction.\nObject detection requires a little trick to be able to implement a quantized final layer while also keeping good accuracy. The outputs of the final layer convolution are used for box predictions. However, the outputs of a quantized convolution must be integer, and this will generally not be able to give the correct scaling for box predictions. Therefore, scaling the outputs by a floating point precision multiplication can be used to get the correct scale. Using a single factor for scaling is sufficient, which only introduces a very small computational overhead."}, {"title": "4 Experiments", "content": "To measure the accuracy of the proposed method compared to baseline methods, we perform experiments on ImageNet and CIFAR-100 datasets. To measure speed, we run inference on imagenet-size images, using our FPGA-based neural network accelerator. For the ImageNet experiments, the first and intermediate layers are quantized. For the CIFAR-100 experiments, all layers are quantized, including the last layer.\nThe quantization bitwidth used in this paper is 2 and 1 bits for activations and weights, respectively. For standard quantization layers, activations are quantized via uniform quantization and weights are quantized using the sign function and scaling factors.\nWhen using pixel embedding for the first layer, pixel embedding quantizes the inputs to 2 bits, and weights use standard quantization. Imagenet experiments use embedding dimension d = 16 and CIFAR-100 experiments use d = 8.\nWe compare pixel embedding with the following three methods of dealing with the first layer:\n1. Weight quantization: The weights are quantized to 1 bit.\n2. Input and weight quantization: The input pixel values are quantized to 2 bits and the weights to 1 bit.\n3. First layer full precision: Both the weights and activations are kept at full precision. This is the baseline for when the first layer is not quantized.\nWe evaluated our method on the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) classification dataset [Krizhevsky et al., 2012], to check the robustness of our method. The models were trained on the 1.28 million training images, and evaluated on the 50k validation images. We adopted an 18-layer residual network as the base network, and compared top-1 accuracy and top-5 accuracy with different quantization settings. We followed most of the training settings and hyper-parameters used in [He et al., 2016], with two exceptions: (i) we used a batch size of 160; (ii) we decreased the learning rate by a factor of 10 at 100k, 200k, and 250k steps. Model training took on the order of one or two days for 300k iterations on 8 GPUs.\nWe further verify our findings by evaluating on the CIFAR-100 dataset [Krizhevsky, 2009], which consists of 50k training images and 10k testing images in 100 classes. Each image is 32 by 32 pixels, and we used only basic data augmentation-pad, crop, and horizontal reflection. The top-1 accuracy was used to measure performance. For these experiments, we used two less downsampling operations compared to the usual ResNet-18 version of He et al. [He et al., 2016], to better suit CIFAR-size images."}, {"title": "4.1 Accuracy measurements", "content": "First, we evaluate pixel embedding in terms of classification accuracy using ImageNet and CIFAR-100 datasets."}, {"title": "4.2 Inference time measurements", "content": "We evaluate pixel embedding via inference time measurements, using the same model settings as described above for the ResNet-18 classifier on Imagenet. In particular, the last layer uses floating point precision and pixel embedding uses embedding dimension 16. The image size is 224 \u00d7 224 after nearest neighbour downsampling, which is not included in the inference time measurement. This set of experiments was performed using Intel Cyclone V FPGA embedded in"}, {"title": "5 Conclusion", "content": "In this paper, we have shown that it is possible to quantize both the input data and weights for deep convolutional neural networks. We proposed pixel embedding, which converts input data into more easily quantizable form. Furthermore, we conducted experiments that revealed the clear advantage of the proposed approach. The top-5 accuracy for ImageNet was almost 10% higher than just quantizing input data and weight. We found similar results for the CIFAR-100 dataset, where pixel embedding increases accuracy by 4.4% compared to standard quantization of the first layer.\nUsing pixel embedding quantization and floating point values in the last layer, we can reduce the increase of top-1 error due to first layer quantization to just 1.34% for the ImageNet dataset. In addition to accuracy, we also measured the inference time of classification with pixel embedding, which was over 1.7 times quicker than the floating point precision alternative. Because of this, pixel embedding enables both fast and accurate inference.\nAlthough we have obtained valuable insights into the new problem, there is a possibility to import more ideas from other research fields like natural language processing. For example, it might be possible to obtain better embeddings by employing recently proposed deep compositional code learning [Shu and Nakayama, 2018]."}]}