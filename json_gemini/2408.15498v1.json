{"title": "Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network", "authors": ["Yijun Zhou", "Om Arora-Jain", "Xia Jiang"], "abstract": "While machine learning has advanced in medicine, its widespread use in clinical applications, especially in predicting breast cancer metastasis, is still limited. We have been dedicated to constructing a DFNN model to predict breast cancer metastasis n years in advance. However, the challenge lies in efficiently identifying optimal hyperparameter values through grid search, given the constraints of time and resources. Issues such as the infinite possibilities for continuous hyperparameters like 11 and 12, as well as the time-consuming and costly process, further complicate the task.\nTo address these challenges, we developed Single Hyperparameter Grid Search (SHGS) strategy, serving as a preselection method before grid search. Our experiments with SHGS applied to DFNN models for breast cancer metastasis prediction focus on analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2, learning rate, decay, and momentum.\nWe created three figures, each depicting the experiment results obtained from three LSM-I-10+ year datasets. These figures illustrate the relationship between model performance and the target hyperparameter values. For each hyperparameter, we analyzed whether changes in this hyperparameter would affect model performance, examined if there were specific patterns, and explored how to choose values for the particular hyperparameter.\nOur experimental findings reveal that the optimal value of a hyperparameter is not only dependent on the dataset but is also significantly influenced by the settings of other hyperparameters. Additionally, our experiments suggested some reduced range of values for a target hyperparameter, which may be helpful for \"low budget\" grid search. This approach serves as a prior experience and foundation for subsequent use of grid search to enhance model performance.", "sections": [{"title": "INTRODUCTION", "content": "Machine learning (ML) has always been an important research topic of AI, and we have seen successful cases of using AI-based learning techniques such as deep learning to conduct various tasks and solve a wide range of problems in the biomedical domain. Machine learning and deep learning methods have been used in applications such as predicting drug-drug interactions using real world data collected via large-scale projects [1], modeling miRNA-mRNA interactions that cause phenotypic abnormality in breast cancer patients [2], prioritizing disease-causing gene using sequence-based features candidates[3-\n5] and predicting ubiquitination sites using physicochemical properties of protein sequences data [6,7].\nVarious learning methods have also been developed and applied to cancer related prediction such as predicting breast cancer local recurrence using language processing [8], identifying risk factors of prostate cancer recurrence using high-dimensional gene and clinical data[9], and predicting relapse in childhood acute lymphoblastic leukemia (ALL) [10].\nAn Artificial Neural Network (ANN) is a machine learning framework, which is designed to recognize patterns using a model loosely resembling the human brain [11,12]. Deep Neural Networks (DNNs), called deep learning, refers to the use of neural networks composed of more than one hidden layer [13\u2013\n15]. The DNN has obtained significant success in commercial applications such as voice and pattern recognition, computer vision, and image processing [16\u201319]. However, its power has not been fully explored or demonstrated in clinical applications, such as the prediction of breast cancer metastasis, which is in part due to modeling challenges resulted from the sheer magnitude of the number of variables involved in these problems [20].\nWe have previously developed deep feedforward neural network (DFNN) models that is able to predict n- year breast cancer metastasis [21]. The DFNN models we developed are fully connected neural networks that do not contain cycles. Figure 1 illustrates the structure and the inner connections of a DFNN that we have developed. It is a four-layer neural network that contains one input layer, two hidden layers, and one output layer. The input nodes to this neural network represent the clinical features contained in the input patient data, which we also refer to as predictors, and the output layer contains two nodes representing the binary status of n-year breast cancer metastasis, which also referred to as the target variable in this context.\nOne of the challenges of deep learning is that there are a large set of hyperparameters which must be tuned to obtain good prediction models [22]. In machine learning and deep learning, hyperparameters are the variables that determine the model's architecture and directly influence the training process and output"}, {"title": "METHODS", "content": "The Single Hyperparameter Grid Search (SHGS) strategy\nThe purpose of running a SHGS is to look for a promising but reduced range of values for a particular hyperparameter, which we call the target hyperparameter in the SHGS, to assist the task of pre-selecting input values for the subsequent grid search. In a SHGS, we will first give a large range of values for the target hyperparameters that has an infinity number of possible values such as L1 and L2, while all the other hyperparameters each will take only one preselected value. So, a SHGS can be considered as a special type of grid search in which the number of hyperparameter settings used for model training during the grid search is equal to the number of values of the target hyperparameter. In this study, we identified 8 hyperparameters that can take an infinity number of values and treat them as our target hyperparameters. These 8 hyperparameter are epochs, batch size, learning rate, dropout rate, momentum, decay, L1 weight decay, and L2 weight decay. In these experiments, to avoid bias, the fixed value used by each of the non-target hyperparameters is pre-picked randomly from the pool of all values considered for each of these hyperparameter following a uniform distribution. We call one value assignment of these non-target hyperparameters a background hyperparameter setting. For each of the target hyperparameter, we repeat the SHGS experiment 10 times and each time use a different randomly selected background hyperparameter setting from all possible settings, to see how a different background setting can affect the results. We used the Python programming language along with the TensorFlow and Keras libraries to implement SHGS, which is now available at [https://pypi.org/project/SHGS]."}, {"title": "The Evaluation of Model Performance Using 5-fold Cross Validation", "content": "For a given binary diagnostic test, a receiver operator characteristic (ROC) curve plots the true positive rate against the false positive rate for all possible cutoff values [25]. The area under a ROC curve (AUC) measures the discrimination performance of a model [26]. We followed a 5-fold cross validation (CV) procedure to train and evaluate models under each hyperparameter setting in a SHGS. The dataset was first split into two subsets: an 80% training dataset for optimizing the model's performance and a separate 20% independent test dataset to evaluate its generalization ability. The entire training dataset was then divided evenly into 5 sub-datasets. The division was mostly conducted randomly and, in the meantime, we made sure that each sub-dataset had approximately 20% of the positive cases and 20% of the negative cases to guarantee an overall balanced distribution of the two. For each target hyperparameter setting, we conducted both the training and testing five times. At each time a model is learned from a different set of 4 sub-datasets combined, and then it is tested using the remaining sub-datasets as the validation set. The average training and testing AUC across all five times, which are called mean_train_auc and mean_test_auc, were reported via the grid search package. Additionally, to evaluate the model's performance completely independently, we used the 20% set-aside test dataset to test the output model refitted using the entire training dataset under each target hyperparameter setting to obtain a test_auc. We used this procedure for all datasets involved in this research."}, {"title": "Datasets", "content": "In this research, we used three datasets LSM-I-10Year, LSM-I-12Year, and LSM-I-15Year [27,28]. The letter I stands for interactions. We previously developed the MBIL method to retrieve interactive features that has a direct influence on a target feature such as a disease outcome [27]. Such a target feature is often called the class feature in machine learning. In this context, the non-target features are also referred to as predictors. LSM-nYear datasets were developed in previous studies [27,28]. In this study we applied the MBIL method to obtain the sets of interactive features from the LSM Datasets, then we retrieved the LSM-I-10Year, LSM-I-12Year, and LSM-I-15Year datasets from the corresponding LSM-nYear datasets based on these interactive features. Each of the three datasets contains the target feature \u201cmetastasis\u201d."}, {"title": "RESULTS", "content": "Figures 3, 4, and 5 are the results obtained for the LSM-I-10Year, LSM-I-12Year, and LSM-I-15Year datasets, each respectively. Each figure consists of eight panels of scatter plots, one for each of the eight target hyperparameters. Within each panel are ten individual scatter plots, each showing the results of a specific SHGS experiment conducted using the corresponding dataset for the corresponding target hyperparameter. Such a scatter plot demonstrates how model performance, as measured by test_auc, changes when the values of a target hyperparameter increases during a SHGS experiment.\nBased on the results showed in Figure 3-5, the scatter plots of the 10 different experiments for the same target hyperparameter can be quite different. This is perhaps because each experiment used a different background setting that was randomly selected by the SHGS scheme. Note that with certain hyperparameter configurations that were randomly generated via grid search, the prediction performance can be very poor, indicated by the consistently low test_AUC values (approximately 0.5), as seen in figures such as Figure 3(a) experiments 7 and 8. An AUC value of 0.5 indicates that the model is unable to distinguish between positive and negative class points, performing no better than random guessing [30].\nAs to epochs (Figure 3(a) through 5(a)), the highest test_auc values for the 10-year, 12-year, and 15-year data are 0.683 (Figure 3(a)-5), 0.727 (Figure 4(a)-8), and 0.852 (Figure 5(a)-2), each respectively. We notice that optimal test_auc values are often reached at low epochs (examples: Figure 3(a)-5 and Figure 5(a)-2). Besides, we have seen three different patterns: 1) test_auc decreases once the number of epochs surpasses a specific threshold (examples: Figure 3(a)-2 and Figure 5(a)-4), 2) test_auc plateaus after passing certain point (example, Figure 4(a)-8) or throughout (example, Figure F5(a)-1), and 3) test_auc steadily goes up as number of epochs passes certain point (examples: Figure 4(a)-1, and Figure 5(a)-10. We also notice that in some cases test_auc has high variance and fluctuates rapidly while the number of epochs increases (example, Figure 4(a)-6), while it has very low variance in some other cases (example, Figure 5(a)-1).\nIn the batch size experiments (Figure 3(b) through 5(b)), the best test_auc values are 0.7 (Figure 3(b)-6) for 10-year, 0.738 (Figure 4(b)-8) for 12-year, and 0.856 (Figure 5(b)-10) for 15-year. There tend to be a short warm up period (batch size< 50) when performance is unstable but reaches a peak quickly, and in most experiments over all three datasets performance reaches the peak before batch size increases to 1/5 of its highest value. Overall, we see three patterns after performance reaches its peak: 1) a slight negative correlation between performance and batch size (Figure 3(b)-7, 4(b)-9, 5(b)-4, and 5(b)-5; 3) a sharp dip when batch size reaches about half its highest value, but trend remains stable at either side of the sharp dipping point (Figure 3(b)-5, 3(b)-9, Figure 4(b)-3 and 4(b)-7, and Figure 5(b)-6); and 3) trend remains constant, with or without a big variance, and this is seen in most cases, indicating that larger batch size tends to have limited influence on performance improvement.\nFigure 3-5(c) shows the patterns of dropout rate, the highest scores are 0.77 (10 Year), 0.726 (12 Year), and 0.886 (15 Year). Similar to batch size, in most cases, the performance trend remains constant or gets worse after it reaches a quick peak. The performance tends to become very poor after the dropout passes 0.5, with some exceptions (examples: Figure 3(c)-5, 4(c)-1, 5(c)-7).\nRegarding the L1 and L2 regularization parameters, denoted as L1 and L2 respectively in subsequent discussions, the best test_auc values for L1 in Figure 3(d)-5(d) are 0.689, 0.734, and 0.872, and the best test_auc results for L2 in Figure 3(e)-5(e) are 0.706, 0.734, and 0.86. The results of L1 reveal no increasing trend in performance as measured by test_auc when L1 values increase in any of the 10 experiments. Based on these results, using a small L1 value (<0.03) in grid search can be sufficient to obtain the best-performed model. L2 results demonstrate similar trends in most cases with some exceptions, in which it requires a larger L2 value to reach the performance peak in experiments 3(e)-2, 3 (3)-10, 4(e)-2, 4(e)-4, and 5(e)-5), but none of these experiments gives the best test_auc values.\nThe highest test_auc scores for learning rate (Figure 3(f) through 5(f)), are 0.734 (Figure 3(f)-6), 0.762 (Figure 4(f)-4), and 0.832 (Figure 5(f)-5), each respectively. The best test_auc values in most experiments were obtained when learning rate is below 0.03. After reaching the peak, the performance tends to stabilize (example, Figure 4(f)-2), or alternatively, it exhibits a significant decline with increased fluctuations after surpassing a certain point (example, Figure 5(f)-10).\nThe best test_auc values achieved for decay are 0.706 (Figure 3(g)-10 for 10 year), 0.733 (Figure 4(g)-2 for 12 year) and 0.87 (Figure 5(g)-4 for 15 year). All experiments that achieved the best test_auc values reach the best performance almost immediately and the increasing value of decay has no significant impact on performance. Certain patterns are seen for other experiments such as the 8th experiment of Figure 4(g), where the test_auc value initially rises as the decay increases but drops abruptly at a point (around 0.03) and remains suboptimal. Overall, the performance reaches the peak when decay assumes a small value (below 0.03), then the trend either maintains constant or goes down as decays increase, with exceptions as seen in Figure 5(g)-2 and 5(g)-6, where it takes longer for the performance to reach the peak.\nFor momentum, the best test_auc values are 0.675 (Figure 3(h)-3 for 10 year), 0.72 in Figure 4(h)-6 for 12 year), and 0.835 (Figure 5(h)-5 for 15 year). Considering all three datasets, performance trend mostly demonstrates two patterns: stays constant or goes up after momentum at least passes the half point, but the well-performed models tend to appear with the second pattern. For examples, for the 10-year data, the performance reaches the peak after momentum passes 0.5 in the experiment in which we identify the best model (Figure 3(h)-3), and for the 15-year data, we obtain the best model after momentum passes 0.7. For the 12-year dataset, the best test_auc is obtained when momentum is below 0.3 and the trend after the peak test_auc is going down as an exception, but we obtain near-best models in other experiments when the momentum passes 0.5 (Figure 4(h)-1) and when it passes 0.7 (Figure 4(h)-2)."}, {"title": "DISCUSSION", "content": "Previous studies reported that best model performance can be obtained at very high epochs [31], or at very low epochs values such as below 10 [32]. One possible explanation is that these experiments were done using different datasets, and a previous study reveals that there is no one-value-fits-all optimal value for epochs, and the value of epochs heavily depends on the dataset [33]. Our results demonstrate that epochs are not only dataset dependent, but also perhaps depending on the background hyperparameter setting. Recall that we refer to a background hyperparameter setting or a background setting as one unique value assignment of all non-target hyperparameters. For example, the 10 scatter plots in Figure 5(a) show that the effects of epochs can be quite different at different background settings for the same dataset. Optimal models are only obtained at low epochs with setting 2, 4, and 5, but at setting 1 and 3, model performance was not deteriorated as epochs goes high and we obtain well performed models when epochs reach 1000, the highest value we tested. For all three datasets, we see optimal models at low epochs (<100) as seen in Figure 3(a)-5, 4(a)-7, and 5(a)-2. This is consistent with a previous finding reported in [34], which states deep learning often converge within low epochs. Besides, our results showed (Figure 6) running time is often correlated with number of epochs, so using high epochs could significantly increase computation time, which renders it a lessor choice especially when time is limited to run a grid search.\nThe optimal batch size remains a subject of ongoing investigation, and no universally agreed-upon answer has been established. Several studies, such as [35] have observed that smaller batch sizes tend to yield improved results. Conversely, another study [36], contends that larger batch sizes lead to higher testing accuracy. This inconsistency may again be due to the different dataset and different background hyperparameter configures used in these studies. Based on our results, both small batch size (for example Figure 4(b)-8) and large batch size (example Figure 5(b)-2) can lead to optimal models, and at certain background hyperparameter settings model performance remains optimal regardless we use a small or a large batch size (examples Figure 5(b)-1 and 5(b)-2). So like epochs, batch size seems to be both dataset and background setting dependent. Our results also show that performance trend most often declines after quickly reaching a peak at a small batch size that is about below 1/5 of its maximum value. This observation supports that large-batch training methods tend to converge towards sharp minimizers, resulting in reduced generalization ability previously reported in[34,37]. It also suggests that choosing batch size values that are below 1/5 of the maximum value might be a good guideline for grid search, especially when computation time is limited and the number of batch size values that can be selected is very small due to the time constraint [38]."}, {"title": "CONCLUSIONS", "content": "Our experiments results demonstrate that an optimal value of a hyperparameter is not only dataset dependent, but also affected significantly by the value assignment of the other hyperparameters, which we call a background hyperparameter setting. This may indicate that hyperparameters can interact and therefore affect model performance jointly. All eight hyperparameters can lead to optimal models at small values after a short warm up period, and for most of them model performance trend maintains constant or declines after the peak except for epochs, batch size, and momentum. Based on our results, depending on a background hyperparameter configuration, both low and high values of epochs, batch size, or momentum can lead to optimal models, which helps explain the seemingly conflicting findings reported by previous studies. As to providing guidance to grid search, especially when computation time or resource is limited, our results suggest the following reduced range of values: epochs below 100, batch size below 1/5 of the size of datapoints, dropout below 0.5, momentum between 0.7 and 0.9, decay below 0.003, and learn rate, L1, and L2 each below 0.03. Finally, since the optimal values are dataset and background setting dependent, conducting some SHGS experiments can be very helpful to hyperparameter configuration prior to the main grid search."}, {"title": "DECLARATIONS", "content": "Ethics approval and consent to participate\nThe study was approved by University of Pittsburgh Institutional Review Board (IRB # 196003) and the U.S. Army Human Research Protection Office (HRPO # E01058.1a).\nThe need for patient consent was waived by the ethics committees because the data consists only of de-identified data that are publicly available.\nConsent for publication\nNot applicable.\nAvailability of data and material\nThe data used in this study are available at datadryad.org (DOI 10. 5061/dryad.64964m0).\nCompeting interests\nThe authors declare that they have no competing interests.\nFunding\nResearch reported in this paper was supported by the U.S. Department of Defense through the Breast Cancer Research Program under Award No. W81XWH-19-1-0495 (to XJ). Other than supplying funds, the funding agencies played no role in the research.\nAuthors' Contribution"}]}