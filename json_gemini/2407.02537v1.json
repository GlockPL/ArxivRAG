{"title": "Parameter Tuning of the Firefly Algorithm by Standard Monte Carlo and Quasi-Monte Carlo Methods", "authors": ["Geethu Joy", "Christian Huyck", "Xin-She Yang"], "abstract": "Almost all optimization algorithms have algorithm-dependent parameters, and the setting of such parameter values can significantly influence the behavior of the algorithm under con-sideration. Thus, proper parameter tuning should be carried out to ensure that the algorithm used for optimization performs well and is sufficiently robust for solving different types of opti-mization problems. In this study, the Firefly Algorithm (FA) is used to evaluate the influence of its parameter values on its efficiency. Parameter values are randomly initialized using both the standard Monte Carlo method and the Quasi Monte-Carlo method. The values are then used for tuning the FA. Two benchmark functions and a spring design problem are used to test the robustness of the tuned FA. From the preliminary findings, it can be deduced that both the Monte Carlo method and Quasi-Monte Carlo method produce similar results in terms of opti-mal fitness values. Numerical experiments using the two different methods on both benchmark functions and the spring design problem showed no major variations in the final fitness values, irrespective of the different sample values selected during the simulations. This insensitivity indicates the robustness of the FA.", "sections": [{"title": "1 Introduction", "content": "Many problems in engineering design and industry can be formulated as optimization problems with a main design objective, subject to multiple nonlinear constraints. Best design options correspond to the optimal solutions to such design optimization problems. To find such optimal solutions requires the use of sophisticated optimization algorithms and techniques [1, 18]. A recent trend is to use nature-inspired algorithms to solve engineering design optimization problems because nature-inspired algorithms tend to be effective, flexible and easy to implement.\nAlmost all optimization algorithms and techniques, including nature-inspired algorithms, have algorithm-dependent parameters, and these parameters need to be properly tuned. Tuning algorithmspecific parameters play a crucial role in determining the effectiveness of an algorithm, and the way these parameters are configured can significantly influence the performance of the algorithm under consideration [8, 15]. Consequently, fine-tuning algorithmic parameters is a vital aspect of implementing and applying algorithms for solving problems in real-world scenarios [13, 21, 22]."}, {"title": "2 Literature Review of Parameter Tuning", "content": "The literature of parameter tuning for evolutionary algorithms and metaheuristics is expanding, especially in the context of tuning parameters of nature-inspired metaheuristic algorithms. Here, a brief review is carried out on different approaches to parameter tuning, including online and offline approaches [18].\nFrom the perspective of parameter tuning, for a given algorithm, a tuning tool (or a tuner) should be used to tune the algorithm first, and then use the tuned algorithm to solve a set of problems. Thus, there are three key components here: an algorithm, a tuner, and a problem set. Since these three components are involved simultaneously in tuning, it is possible that the parameter values tuned may depend on both the algorithm under consideration and the problems to be solved. Therefore, parameter settings can be algorithm-specific as well as problem-specific.\nFor a given algorithm, its parameters can be tuned first before it is used for solving optimization problems. This approach is usually called offline tuning. Other studies also indicate that it may be advantageous to vary parameters during iterations, and this approach is often referred to as online tuning [8, 13]. Parameter tuning can be carried out either in sequence or in parallel and these different methods can be loosely divided into ten different categories:\n\u2022 Manual or brute force method\n\u2022 Tuning by systematic scanning\n\u2022 Empirical tuning as parametric studies\n\u2022 Monte Carlo based method\n\u2022 Tuning by design of experiments (DOE)\n\u2022 Machine learning based methods\n\u2022 Adaptive and automation methods\n\u2022 Self-tuning method [9]\n\u2022 Heuristic tuning with parameter control\n\u2022 Other tuning methods\nOther parameter tuning methods include sequential optimization approaches, multi-objective optimization approaches, self-parameterization, fuzzy methods as well as dynamic parameter adaptation approaches and hyper-parameter optimization [10, 11]. Although extensive studies have been dedicated to exploring parameter tuning methods, both offline and online, a lack of comprehensive"}, {"title": "3 Tuning Parameters by MC and QMC", "content": "Before the details of tuning parameters using the MC and QMC methods are discussed, the main idea of FA and its parameters are outlined."}, {"title": "3.1 Firefly Algorithm", "content": "The Firefly Algorithm (FA) is a nature-inspired algorithm that was developed by Xin-She Yang in 2008, based on the flashing characteristics and flying patterns of tropical fireflies [19]. FA has been applied to a diverse range of applications, including multi-modal optimization, multi-objective optimization [19, 17], clustering [20], software testing [3], vehicle routing problems [21], multi-robot swarming [22] and others.\nFor a given optimization problem, its solution vector x is encoded as the locations of fireflies. Thus, the locations of two fireflies i and j correspond to two solution vectors $x_i$ and $x_j$, respectively. The main updating equation of firefly locations or solution vectors is\n$x_i^{t+1} = x_i^t + \\beta e^{-\\gamma r_{ij}^2}(x_j - x_i) + \\alpha \\epsilon_i$\nwhere the random number vector $\\epsilon_i$ is drawn from a Gaussian normal distribution. In addition, the distance r between two solutions is given by the Euclidean distance or $L_2$-norm\n$r_{ij} = ||x_i - x_j||$.\nThe parameters to be tuned are the attractiveness parameter $\\beta$, the scaling parameter $\\gamma$ and the randomization strength parameter $\\alpha$. In most FA implementations, parameter $\\alpha$ is further rewritten as\n$\\alpha = \\alpha_0 \\theta^t$,\nwhere $\\alpha_0$ is its initial value, which can be set to $\\alpha_0 = 1$. Here, t is the pseudo-time or iteration counter, and 0 < $\\theta$ < 1 is the parameter to be tuned, instead of $\\alpha$."}, {"title": "3.2 Monte Carlo Method", "content": "One approach to offline tuning is to use MC-based methods. In this study, all the parameters in the FA are initialized randomly using MC and pseudo-random numbers that are uniformly distributed. Pseudo-random numbers are random numbers generated using generators, which are used in computer programs. They are not truly random numbers and are generated in a deterministic way with some sophisticated permutations.\nIn essence, the MC method is a statistical sampling method with statistical foundations and its errors tend to decrease as $O(1/\\sqrt{N})$ where N is the number of samples [12]. Though this inversesquare convergence may be slow, it can work well in practice [15], in comparison with manual or brute force methods.\nIn the current simulation for parameter tuning, the parameters of the FA are randomly initialized by drawing random samples from uniform distributions in a specific range of parameter values. Then, the discrete random samples are used as the parameter setting of the FA. With such settings, the FA is executed to solve the given optimization problems, such as the benchmark functions and the spring design problem [1, 14]."}, {"title": "3.3 Quasi-Monte Carlo Method", "content": "To obtain better estimates, the standard MC method requires a large number of samples. Theoretical analysis and studies from various applications suggest that a quasi-Monte Carlo method can potentially speed up the convergence because its errors decrease as $O(1/N)$ under certain conditions. Such QMC methods use low-discrepancy sequences or quasi-random numbers, and such sequences require some careful generation and random scrambling of the initial sequences [4, 12, 14]. Therefore, this study also uses QMC to tune parameters in the FA and comparison with the standard MC will be carried out.\nFor the generation of quasi-random numbers, there are efficient algorithms such as van der Corput sequence, Sobol sequence and Halton sequence. Most of these sequences will generate quasi-random numbers in the interval between 0 and 1. In the current simulation, the Sobol sequence with affine scramble and digital shift will be used [4, 5, 16], which is a standard implementation in Matlab."}, {"title": "4 Experiment Setup and Benchmarks", "content": "To investigate the possible effect of two different tuning methods on the performance of the FA, two benchmark functions and a design problem are used in this study. The two benchmark functions are the sphere function and Rosenbrock's banana function. The former is a convex, separable function, whereas the latter is a non-convex, non-separable function. The design problem is a non-convex, nonlinear spring design problem, subject to four constraints."}, {"title": "4.1 Experimental Setup for FA Parameters", "content": "In the standard FA, there are three parameters $\\theta$, $\\beta$ and $\\gamma$ to be tuned. These parameters of the FA can typically take the following values:\n\u2022 Population size: n = 20 to 40 (up to 100 if necessary).\n\u2022 $\\beta$ = 0.1 to 1, $\\gamma$ = 0.01 to 10, though typically, $\\beta$ = 1 and $\\gamma$ = 0.1.\n\u2022 $\\alpha_0$ = 1, $\\theta$ = 0.9 to 0.99 (typically, $\\theta$ = 0.97). where $\\alpha = \\alpha_0 \\theta^t$.\n\u2022 Number of iterations: tmax = 100 to 1000.\nFor simplicity in this study for both MC and QMC, the ranges of these parameter values will be further narrowed down, as shown in Table 1."}, {"title": "4.2 Benchmark Functions", "content": "Optimization algorithms are typically assessed using a diverse set of standard benchmark functions to validate their efficiency and reliability. Researchers evaluate these algorithms by comparing their performance across a wide range of more than two hundred benchmark functions. The choice of benchmarks lacks standardized criteria, but it is essential to use a diverse range of benchmark problems, including different modes, separability, dimensionality, linearity and nonlinearity. Numerous benchmark collections, including CEC suites, and those referenced in articles, for example, Jamil and Yang [6], are available online.\nWhile test functions are usually unconstrained, real-world benchmark problems originate from various applications with complex constraints and large datasets. This present work will assess the FA's parameter settings using three test benchmarks.\n1. The Rosenbrock function is a nonlinear benchmark [7], which is not convex in the D-dimensional space. It is written as\n$f(x) = (1 - x_1)^2 + \\sum_{i=1}^{D-1} [100 (x_{i+1}-x_i^2)^2], x \\in \\mathbb{R}^D,$\nwhere\n-30 \u2264 xi \u2264 30, i = 1, 2, ..., D.\nIts global minimum is located at $x^* = (1, . . ., 1)$ with $f_{min}(x^*) = 0$.\n2. The sphere function is a convex benchmark in the form\n$f(x) = \\sum_{i=1}^{D}x_i^2, x \\in \\mathbb{R}^D,$\nwhere\n-10 < xi < 10, i = 1, 2, ..., D.\nIts global minimum is located at $x^* = (0, . . .,0)$ with f(x*) = 0.\n3. The spring design is an engineering design benchmark with three decision variables and four constraints [1].\nMinimize $f(x) = (2 + x_3)x_1x_2,$"}, {"title": "5 Results and Hypothesis testing", "content": "To test the possible effects of different tuning methods on the performance of the FA, a set of 10 runs have been carried out using both MC and QMC methods over three different optimization problems. All problems and runs use the same maximum number of 1000 iterations. Based on the numerical experiments, two hypotheses are proposed and the paired Student's t-tests will be used for comparison.\nThe two hypotheses to be tested are as follows\nHypothesis H1: Parameter Tuning methods (MC or QMC) have no significant effect on the fitness values obtained, for a given optimization problem.\nHypothesis H2: For a given algorithm, its performance on different problems is not affected by the parameter tuning method used."}, {"title": "5.1 Testing the First Hypothesis", "content": "For the MC simulation, the parameters of the FA ($\\theta$, $\\beta$ and $\\gamma$) are taken from uniform distributions in the ranges given in Table 1. Similarly, for the QMC simulation, the parameters are taken from a scrambled Sobol sequence and then mapped into the proper ranges of the parameters. For every objective function, the optimal fitness value obtained along with the corresponding optimal solution, and the parameter values from MC and QMC are recorded for post-processing and hypothesis testing.\nThe t-tests are then used to test the hypotheses. According to the standard t-test criteria, the h and p values obtained from the paired t-tests will determine whether the null hypothesis should be rejected or not. The values for 10 runs obtained from the MC and QMC simulations for each objective function are listed in Table 2 to Table 4.\n1. Rosenbrock Function. From the results for the Rosenbrock function summarized in Table 2, a paired t-test has been carried out. As the p-value is much larger than the threshold value 0.05, the first hypothesis cannot be rejected. That is to say, there is no strong evidence to say that the fitness values obtained are affected by different tuning methods."}, {"title": "5.2 Testing the Second Hypothesis", "content": "The tests of the first hypothesis for all three problems give a consistent conclusion that there are no significant differences in the results obtained by the FA whatever the tuning methods were used. To see if this is consistent with the group means, the mean fitness values from three separate problems are tested using the same t-test.\nThe t-test results are summarized in Table 5, which again shows that the null hypothesis (H2) holds. That is to say, there is no statistically significant support for one tuning method being better than the other.\nThis conclusion is a bit surprising from the perspective that the QMC method usually produces better results than the standard MC method for multiple dimensional numerical integrals. This study seems to show that both MC and QMC methods produce similar results for the parameter tuning purpose."}, {"title": "5.3 F-Test for Variances", "content": "The hypothesis tests so far show that no significant differences were found in the mean objective values obtained by MC and QMC. However, the same level of mean values does not necessarily give the same level of variances. Thus, it is useful to carry out the test of variances. For this purpose, we use the two-sample F-test to see if the variances for the spring design problem obtained by MC and QMC are equal.\nThe F-test using the same data shown in Table 4 gives h = 1 and p-value p = 0.0262, which is smaller than the critical value 0.05. This means that there is a sufficient difference in variances to reject the null hypothesis. Therefore, it can be concluded that there are no significant differences in mean values obtained by the FA using MC and QMC, but there are some statistically noticeable differences in their corresponding variances of the objective values.\nHowever, it is worth pointing out that the statistical tests that have been carried out here are mainly to test the differences in means using paired t-tests. In addition, the sample size of 10 is relatively small, thus it may be possible that further more extensive tests may reveal that more comprehensive results may not be completely consistent with this preliminary conclusion."}, {"title": "6 Conclusion and Future Work", "content": "From the simulation results of the three different optimization benchmarks, there is not enough evidence to reject the null hypotheses. For hypothesis 1, surprisingly, there was no significant difference in the fitness values obtained via MC and QMC.\nFor Hypothesis 2, the fitness values obtained by MC and QMC simulations also fail to reject the null hypothesis. For all three benchmark functions, similar orders of fitness values were obtained for both tuning methods. The QMC method does not produce significantly better results, when compared to the standard MC method.\nThe preliminary study consists of only a small number of optimization problems, it may be the case that other benchmark problems and other algorithms may not show such robustness. Therefore, a further study is required to determine whether the parameter settings of the FA using these two parameter tuning methods exhibit the same property. Furthermore, some detailed statistical analysis and theoretical analysis will be needed to gain insights into the effect of parameter tuning and its potential link to the convergence behavior observed in these numerical experiments. These will form part of the authors' further research topics."}]}