{"title": "Harden Deep Neural Networks Against Fault Injections Through Weight Scaling", "authors": ["Ninnart Fuengfusin", "Hakaru Tamukoh"], "abstract": "Deep neural networks (DNNs) have enabled smart applications on hardware devices. However, these hardware devices are vulnerable to unintended faults caused by aging, temperature variance, and write errors. These faults can cause bit-flips in DNN weights and significantly degrade the performance of DNNs. Thus, protection against these faults is crucial for the deployment of DNNs in critical applications. Previous works have proposed error correction codes based methods, however these methods often require high overheads in both memory and computation. In this paper, we propose a simple yet effective method to harden DNN weights by multiplying weights by constants before storing them to fault-prone medium. When used, these weights are divided back by the same constants to restore the original scale. Our method is based on the observation that errors from bit-flips have properties similar to additive noise, therefore by dividing by constants can reduce the absolute error from bit-flips. To demonstrate our method, we conduct experiments across four ImageNet 2012 pre-trained models along with three different data types: 32-bit floating point, 16-bit floating point, and 8-bit fixed point. This method demonstrates that by only multiplying weights with constants, Top-1 Accuracy of 8-bit fixed point ResNet50 is improved by 54.418 at bit-error rate of 0.0001.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, deep neural networks (DNNs) have enabled applications such as image recognition [1], object detection [2], and natural language processing [3]. These applications form the backbones of many critical systems, including autonomous cars [4], healthcare [5], and more.\nHowever, as DNNs are deployed to hardware devices, they become vulnerable to fault injection, which can be caused by aging [6], temperature variance [7], and write errors [8]. These faults can manifest as bit-flips to DNN weights and can easily degrade DNN performances [9]. Without any protections, faults occurring in DNNs used in critical applications may lead to serious consequences.\nTo address this issue, several research directions have been proposed. One of the research directions is error-correction code (ECC) based methods [10]\u2013[13], which use ECC to encode redundancy in the form of additional bits to DNN parameters and enable ECC to correct a certain number of bit-flips. However, these methods often require high overheads in both memory to encode redundancy and computation to locate and correct error bits. The works in this direction [10]\u2013[12] explore weight parameter characteristics to reduce memory overheads; however, the computational overheads still remain high.\nIn this paper, our work proposes a simple yet efficient method that requires only element-wise multiplication before writing and element-wise division after reading weights from fault-prone mediums. We demonstrate that these processes reduce the overall absolute error caused by bit-flips. Furthermore, we propose a method that reduces the overall number of divisions by dividing output logits instead of weights. We show that our method can be applied across three data types: 32-bit floating point (FP32), 16-bit floating point (FP16), and 8-bit fixed point (Q2.5, with 1-bit sign, 2-bit integer, and 5-bit fraction)."}, {"title": "II. RELATED WORKS", "content": "This section discusses works related to hardening DNNs against fault injections. One approach to protecting DNNS against bit-flips is to use ECC. ECC encodes redundancy in the form of parity bits to DNN parameters, allowing it to correct a certain number of bit-flips in the parameters. However, storing parity bits requires additional memory, so ECC-based methods often focus on reducing memory overheads by exploiting weight distribution characteristics.\nFollowing this direction, in-place zero-space memory protection [12] is designed using ECC, specifically the extended Hamming code (64, 57), which can correct up to one bit-flip error. This method also proposes a training strategy that penalizes large weights, ensuring that large weights appear only at specific positions. This approach ensures that most weights small, making the most significant bits (MSB) after the sign bit likely to hold no information. As a result, this bit position can be used to insert a parity bit.\nIn the same direction, value-aware parity insertion ECC [11] is a method based on ECC (64, 50), which can correct double bit-flips. This method is designed for Q2.5 DNNs. To reduce memory overheads, it leverages the observation that the most weight values are less than 10.5, meaning the first two integer bits are likely to hold no information. As a result, these bit positions can be used to insert parity bits. If the weight values are greater than or equal to 10.5, the last two least significant bits (LSB) are used for parity bit insertion instead, ensuring minimal information loss.\nIn another approach, instead of protecting all bit positions equally, efficient repetition code for deep learning [10] is designed to protect only the bit positions close to the MSB, while avoiding protection for the bit positions near the LSB. This approach is based on the observation that bit-flips in bit positions near the MSB can cause dramatic changes compared to those near the LSB. Therefore, these bit positions near the MSB are prioritized for protection.\nECC-based approaches are similar to our method, as both exploit the characteristics of weight parameters and modify them either by inserting parity bits or scaling. A key distinction is that our method incurs significantly lower computational overhead compared to ECC-based methods. Our method only requires only element-wise multiplication and division, whereas ECC-based methods involve encoding and decoding processes that can be computationally expensive."}, {"title": "III. PRELIMINARIES", "content": "In this section, we describe the prerequisites for our proposed method. Given a i-th layer and j-th DNN weight denoted as $W_{i,j} \\in R$, its binary representation is denoted as $B_{i,j} \\in {0,1}^n$, where n is the number of bits of the data type. Here, $B_{i,j,1}$ denotes the MSB, and $B_{i,j,n}$ denotes the LSB.\nTo model fault injections, a bit-flip at $B_{i,j,k}$ is defined as $M_{i,j,k} \\in {0,1}$, where $M_{i,j,k} = 1$ indicates the presence of a bit-flip, while $M_{i,j,k} = 0$ indicates no bit-flip. The probability of $M_{i,j,k}$ is defined in (1), where BER is the bit-error rate, or probability of a bit-flip.\n$P(M_{i,j,k}) = \\begin{cases}BER & \\text{if } M_{i,j,k} = 1 \\\\ 1 - BER & \\text{if } M_{i,j,k} = 0\\end{cases}$ (1)\nTo model bit-flip errors, let $f_D$ be a function that converts a binary representation to its data type D and let $b_D$ be a function that converts a data type D to its binary representation, where D\u2208 {FP32, FP16, Q2.5}. Therefore, the i-th layer and j-th weight with bit-flips, denoted as $\\widetilde{W_{i,j}}$, is defined in (2), where $M_{i,j}$ is a bit-flip mask representing bit-flip occurrences for the weight at indices i and j.\n$\\widetilde{W_{i,j}} = f_D(b_D(W_{i,j}) \\oplus M_{i,j})$ (2)\nErrors from bit-flip are founded using e function defined in (3), where $\\oplus$ denotes as an element-wise XOR operation.\n$e(W_{i,j}, M_{i,j}) = W_{i,j} - f_D(b_D(W_{i,j}) \\oplus M_{i,j}) = W_{i,j} - \\widetilde{W_{i,j}}$ (3)"}, {"title": "IV. PROPOSED METHODS", "content": "After describing the prerequisites, we present our proposed method to harden DNNs against fault injections through weight scaling in this section. Our method scales the weights $W_i$ by $c_i$ before passing them to the e function that injects bit-flips into $c_iW_i$. After that, the weights are rescaled by dividing by the same $c_i$.\nTo enable FP32 and FP16 models to operate with faults, it is necessary to avoid bit-flips to the most significant bit of the exponent term or ensure $M_{i,j,2}$ = 0. A bit-flip in this position can significantly alter the magnitude of the weight. For example, 0.1 can be changed to \u2248 3.403 \u00d7 1037, and 1.0 can be changed to Infinity (Inf).\nTo enable this assumption, either ECC can be used to ensure data integrity, or values can be clamped to a range of (-2, 2), based on the observation that well-trained weights are typically small values [11], [12]. From our observations, only two weights from torchvision [14] pre-trained weights\u2014from AlexNet [15], ResNet18 [16], ResNet50 [16], and DenseNet169 [17] models\u2014fall outside this range (-2,2). For example, clamping AlexNet weights to this range results in a minor accuracy drop from 57.55 to 56.50. When all weights are within this range, we can assume that the MSB of exponent term is always zero.\nTo enable this assumption in a lossless manner, a sparse matrix method based on [11] can be used to store the positions of values that fall outside the range of (-2, 2). Since the"}, {"title": "B. Scaling, Rescaling, and Their Effects", "content": "Before deployment to fault-prone mediums, the i-th layer weight $W_i$ is scaled by a constant $c_i$, then written to the fault-prone mediums, as shown in (4). The weights with faults, based on our proposed method, are denoted as $\\widetilde{W_{i,j}}$.\n$c_i\\widetilde{W_{i,j}} = c_iW_{i,j} - e(c_iW_{i,j}, M_{i,j})$ (4)\nAfter receiving the scaled weight with fault injections, it can be recovered by dividing by $c_i$, as shown in (5),\n$\\widetilde{W_{i,j}} = W_{i,j} - \\frac{e(c_i\\widetilde{W_{i,j}}, M_{i,j})}{c_i}$ (5)\nFrom our observations, the e function is not scale-invariant, meaning that $e(c_iW_{i,j}, M_{i,j}) \\neq c_ie(W_{i,j}, M_{i,j})$. When $c_i > 1$, after scaling $W_{i,j}$ with $c_i$, we found that, on average, bit-flips to scaled weights cause higher absolute error from bit-flips: $|e(c_iW_{i,j}, M_{i,j})| > |e(W_{i,j}, M_{i,j})|$. However, after rescaling back to the original scale, the absolute error from bit-flips is reduced: $|\\frac{e(c_iW_{i,j}, M_{i,j})}{c_i}| < |e(W_{i,j}, M_{i,j})|$. These observations are based on the following analysis."}, {"title": "C. Analysis of the Effects of Scaling and Rescaling", "content": "In this analysis, pseudo-weights were generated from -0.5 to 0.5 with an increment of 0.01. This range is chosen based on [11], which indicates that the most of pre-trained weights from certain models fall within this interval.\nMonte Carlo simulations were conducted for 106 rounds by randomly injecting bit-flips into pseudo-weights with a BER = 10-1. This analysis was performed across several values of $c_i$. Note that when $c_i$ = 1, it represents the baseline method, without the use of our proposed method. Before injecting bit-flips, weights are scaled by $c_i$, and after injecting bit-flips, the weights are rescaled back to the original scale by dividing by $c_i$. The absolute errors from bit-flips, $| \\frac{e(c_iW_{i,j}, M_{i,j})}{c_i} |$, are averaged across simulations"}, {"title": "D. Finding the Optimal $c_i$", "content": "To find the optimal $c_i$, based on our analysis, the highest possible $c_i$ that scales $W_i$ into the range (-2, 2) for FP32 and FP16, or [-2, 1.984375] for Q2.5, must be determined. Since $W_i$ varies across layers, t is defined as the maximum value to which $W_i$ can be scaled. For FP32 and FP16, t = 1.9999 is selected, and for Q2.5, t = 1.97 is chosen instead of 1.984375 to avoid rounding issues. The optimal $c_i$ is founded by (6).\n$c_i = \\frac{t}{max(W_i)}$ (6)"}, {"title": "V. EXPERIMENTAL RESULTS AND DISCUSSION", "content": "In this paper, we conducted three experiments: fault injection with ResNet18 across different BERs, fault injection across models, and a demonstration of how to reduce the overall divisions by dividing output logits instead of weights."}, {"title": "A. Fault Injection Across Different BERs", "content": "In this experiment, we conducted fault injections with ResNet18 across different BERs and t values. Our ResNet18 weights were retrieved from torchvision package [14]. For FP16 and Q2.5, the FP32 weights were converted to their respective data types. All DNN operations were still performed using FP32 data type, while the weights were simulated to experience bit-flips in their respective data types. For all experiments with FP32 and FP16, we assumed that no bit-flips occur in the MSB of the exponent term to ensure that the FP32 and FP16 models can operate under faults.\nThis experiment was designed to demonstrate how t influences the Top-1 Accuracy of ResNet18 across different BER values. We performed fault injections with $BER \\in {10^{-3},10^{-4},10^{-5}}$ and $t\\in$ {0.5, 1.0, 1.5, 1.9999, 2.5, 3.0, 3.5, 4.0} for FP32 and FP16. For Q2.5, we used the same BER values but with $t\\in$ {0.5, 1.0, 1.5, 1.97, 2.5, 3.0, 3.5,4.0}. Monte Carlo simulations were conducted to inject bit-flips into the weights, with 10 rounds for each BER and t.\nThe experimental results"}, {"title": "B. Fault Injection Across Models", "content": "In this experiment, we injected faults or bit-flips into ImageNet 2012 [19] pre-trained models: AlexNet [15], ResNet18 [16], ResNet50 [16], and DenseNet169 [17]. These pre-trained weights were retrieved from the torchvision package [14]. We performed 10 rounds of Monte Carlo simulations to inject bit-flips into weights and reported the average Top-1 Accuracy with standard deviation, in format of Top-1 Accuracy \u00b1 Standard Deviation. Before the experiments, the original Top-1 Accuracy of these models"}, {"title": "C. Reducing Division Overhead", "content": "Our proposed method requires dividing the weights $W_i$ by $c_i$ after reading from fault-prone mediums. Since this division operation can be computationally expensive and it must be applied to all n weight parameters, we propose a method that reduces the number of divisions by performing the divisions only at the output logits. \nInstead of dividing the weights immediately after reading them from fault-prone mediums,"}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a simple yet effective method to harden DNNs against bit-flips by multiplying DNN weights by layer-wise constants before passing them to noisy mediums. When using these weights, the weights with faults are divided by the same constants to return them to their original scales. With this approach, we demonstrate that, within certain ranges,"}]}