{"title": "KODA: A Data-Driven Recursive Model for Time Series Forecasting\nand data assimilation using koopman operators", "authors": ["Ashutosh Singh", "Ashish Singh", "Tales Imbiriba", "Deniz Erdogmus", "Ricardo Borsoi"], "abstract": "Approaches based on Koopman operators have shown\ngreat promise in forecasting time series data generated\nby complex nonlinear dynamical systems (NLDS).\nAlthough such approaches are able to capture the\nlatent state representation of a NLDS, they still\nface difficulty in long term forecasting when applied\nto real world data. Specifically many real-world\nNLDS exhibit time-varying behavior, leading to\nnonstationarity that is hard to capture with such\nmodels. Furthermore they lack a systematic data-\ndriven approach to perform data assimilation, that\nis, exploiting noisy measurements on the fly in the\nforecasting task. To alleviate the above issues, we\npropose a Koopman operator-based approach (named\nKODA - Koopman Operator with Data Assimilation)\nthat integrates forecasting and data assimilation in\nNLDS. In particular we use a Fourier domain filter\nto disentangle the data into a physical component\nwhose dynamics can be accurately represented by\na Koopman operator, and residual dynamics that\nrepresents the local or time varying behavior that\nare captured by a flexible and learnable recursive\nmodel. We carefully design an architecture and training\ncriterion that ensures this decomposition lead to stable\nand long-term forecasts. Moreover, we introduce a\ncourse correction strategy to perform data assimilation\nwith new measurements at inference time. The\nproposed approach is completely data-driven and can\nbe learned end-to-end. Through extensive experimental\ncomparisons we show that KODA outperforms existing\nstate of the art methods on multiple time series\nbenchmarks such as electricity, temperature, weather,\nlorenz 63 and duffing oscillator demonstrating its\nsuperior performance and efficacy along the three\ntasks a) forecasting, b) data assimilation and c) state\nprediction.", "sections": [{"title": "Introduction", "content": "Many real world applications tackle phenomena which\nare dynamic in nature. Measuring and tracking the\nevolution of such phenomena holds extreme importance\nin many fields such as weather forecasting (Pathak\net al. 2022; Lam et al. 2022), evolution of PDEs\n(Li et al. 2020; Kovachki et al. 2023), modeling\nneural dynamics (Brunton et al. 2016; Singh et al.\n2021), etc. Most of these phenomena are characterised\nas nonlinear dynamical systems (NLDS). If prior\nknowledge about the underlying system's evolution is\nknown then it can inform modeling, see, e.g., (Cai\net al. 2021; Cuomo et al. 2022; Raissi, Perdikaris, and\nKarniadakis 2019; Imbiriba et al. 2023). However, in\nmost scenarios no direct information is available about\nthe physics of the underlying system. Therefore, for\nsuch cases we can only depend on the measurement\ndata to model system's dynamics (Guen and Thome\n2020; Afshar, Germ, and Morris 2023; Frion et al.\n2024b). Key motivations for such modeling is to\nevolve the model and generate accurate forecasts over\nlong horizon. But the non-linear and non-stationary\ncharacteristic of these system makes it hard to capture\nthe state representation or generate accurate forecasts.\nOne classical approach is to model the NLDS in a\nrepresentation space where the system dynamics is\nlinear. Using the tools from spectral theory and linear\nalgebra we can study various characteristics of the\nNLDS such as stability, asymptotic nature etc.\nKoopman operator theory (Koopman 1931; Phillips\n1961) generates a representation with a linear dynamics"}, {"title": "Related Works", "content": "Learning with Koopman Operator\nKoopman operator theory (Brunton et al. 2021) serves\nas a potent tool for unveiling the inherent dynamics of\nnon-linear systems. Notably, the recent surge in interest\nsurrounding Koopman operators can be attributed\nto the strong theoretical foundations and empirical\nsuccess of such algorithms. Most famous among them\nis Dynamic Mode Decomposition (Tu 2013), a data-\ndriven approach (Brunton, Proctor, and Kutz 2016;\nKutz et al. 2016) enabling a practical approximation\nof the Koopman operator. Recent works (see, e.g.,\n(Kawahara 2016; Lusch, Kutz, and Brunton 2018;\nYeung, Kundu, and Hodas 2019; Takeishi, Kawahara,\nand Yairi 2017; Bevanda et al. 2023)) further explored\nlearning theory for estimating Koopman operators. In\nparticularly, the Koopman autoencoder architecture\nproposed in (Azencot et al. 2020) and (Lusch, Kutz,\nand Brunton 2018) has become one of the most widely\nused deep learning models for this task. Since then\nseveral work have been proposed in the intersection of\nlearning theory and Koopman operators (Fathi et al.\n2024; Wang, Xu, and Mu 2023; Berman, Naiman, and\nAzencot 2023). The recent models proposed in (Liu\net al. 2024; Wang et al. 2023b), proposed Koopman-\nbased neural forecasting methods by disentangling the\nsystem's dynamics into local and global components.\nHowever, they lack a systematic approach for data\nassimilation during inference time.\nData Assimilation\nSome of the earlier work that combine Koopman\noperators with the Kalman filter are (Benosman,\nMansour, and Huroyan 2017), where the author\nformulates a linear observer design using Koopman\noperators to predict crowd flow, and (Jiang et al.\n2022) where the authors use a kernel-based Koopman\noperator for robotic systems. Unlike these works, the\nproposed approach is a deep learning-based recursive\napproach. (Frion et al. 2024a) is one of the first\nworks that motivates data assimilation with a neural\nKoopman operator. While the model proposed in\n(Frion et al. 2024a) uses a Koopman operator as\nprior for variational data assimilation, in KODA we\nadopt a branched prediction model within a Kalman\nfilter-inspired framework for both forecasting and\nassimilation. Additionally, unlike (Frion et al. 2024a),\nKODA can achieve online data assimilation during\ninference. Similarly, in (Singh et al. 2024) the authors\npropose a Kalman filter-based data assimilation using\nneural operators for semilinear PDEs. Differently,\nour method motivates data assimilation jointly with\na Koopman operator-based prediction model and a\nparallel residual model. We also don't assume access\nto a ground truth of a latent state representation of\nthe system and instead learn to evolve the model just\nfrom measurement data. In (Frion et al. 2024b) authors\npropose a way of uncertainty quantification for time"}, {"title": "Problem Setting", "content": "In this section, we formally define the problem of\nlearning a model that can not only produce solution\nfor LTSF problem, provided sufficient historical data,\nbut also utilise new data available during inference to\ncorrect the model prediction. The underlying physical\nquantity whose dynamic we observe is referred to as\nstates and the observed data itself is referred to as\nmeasurements of the state. Therefore, let us denote\n$s(t) \\in H$ as the state representation, and by $y(t) \\in Y$\nas the representation of the measurement snapshots.\n$H$ and $Y$ are finite dimensional vector spaces. Hence a\nNLDS can be described as:\n$\\frac{ds(t)}{dt} = A(s(t)) + \\eta(t)$\n$y(t) = B(s(t)) + \\epsilon(t)$\nwhere operator $A : H \\rightarrow H$ models the evolution of the\nstates, $B: H \\rightarrow Y$ represents the mapping from the\nstates to the measurements, $\\eta(t) \\in H$ denote a process\nnoise which represents possible stochasticity in the state\nevolution, and $\\epsilon(t) \\in Y$ denotes measurement noise.\nThe main objective of the forecasting problem is to\nobtain accurate estimates of the future measurement\ntrajectory $Y_{t+\\tau} = B(s(t + \\tau))$, $\\tau \\in \\{1,2,3,...\\}$ based\non past measurements $\\{y_0,...,y_t\\}$, where $y_t$ denote\na discrete measurement of $y(t)$. Note that while the\nNLDS (2) might be described in continuous time,\nwe consider the discrete-time forecasting problem,\nbased on discrete measurements. In order to keep\nnotation clear, we denote the discrete time index as\na subscript. Considering some training dataset with\ndifferent realizations of $\\{y_1,...,y_T\\}$, the forecasting\nobjective can be formulated as learning an operator\n$\\Phi: H^t \\times N^t \\rightarrow H$ that predicts the future measurements\nas $Y_{t+\\Tau} = \\Phi(y_1,..., y_t; \\Tau)$. The above objective is\ngenerally optimised by minimizing the prediction loss\nbetween the forecast and ground truth during training.\nThe data assimilation problem extends this task by\nincluding the possibility of incorporating a sparse set\nof measurements, irregularly present along the forecast\nhorizon, on the fly to improve the forecast estimate.\nNote that learning such a model is not easy since\nthe amount of training data might be limited, and\nthe NLDS under consideration can be highly nonlinear.\nMoreover the measurements $y(t)$ are often accompanied\nwith noise. This makes it harder to avoid drift in the\nforecast over time. In the section below, we introduce\nKODA a framework for generating long term forecasts\nand preforming data assimilation, while enabling us to\nstudy the properties of the NLDS through the learned\nkoopman operator."}, {"title": "KODA", "content": "In this section, we formally propose KODA and its\nvarious elements. KODA adopts a disentangled view of\nthe state representation separating it into a physical\nand a residual component 2. To achieve this purely\nfrom the measurement data, we separate the dominant\nspectrum across the data using a Fourier filter. We\nthen use an encoding model that learns the physical\nand residual components, of the state representation,\nfrom the dominant and non-dominant parts of the\nmeasurement data. Using separate prediction models,\nfor both the state space components, we generate their\nfuture estimates. Upon adding the predicate from the\ntwo models we compute the future state estimates. We\nthen use a decoder to map the state estimate back to\nthe measurement space, hence recovering the future\nforecast. We perform this recursively in a windowed\nfashion to generate future trajectory. At inference time\nwhenever the measurement data is available we use it\nin the correction mechanism of KODA to update the\nmodel predictions. Hence KODA presents a framework\nthat is purely learning based and data driven.\nData Encoding Scheme\nState Disentanglement In order to facilitate the\ntask of LTSF, we aim to learn a latent representation\n$h_t\\in H$. This representation act as the learning based\napproximation of the true $s_t$ in (2). Assume that $h_t$\ncomprises of multiple sources of variation (physical and\nresidual) we use the following disentanglement of the\nstate trajectory,\n$h_t = z_t + r_t,$\nwhere $z_t \\in H$ represents the underlying physical\ncomponent while $r_t \\in H$ represents the residual element\nof the state trajectory. By disentangling the dynamics\ninto these two components we aim at recovering the\nphysical component i.e. the stable component (time\ninvariant in the selected window). The benefit of\nsuch disentanglement is that we are able to separate\nphysical component from the local ones. The physical\ncomponent is expected to capture sources of variation\nthat cause long term change and are stable in smaller"}, {"title": null, "content": "window analysis of which could shed light on the\nspectral property of the underlying system. Due to its\nusefulness this disentangled view of the dynamics have\ngarnered interest in recent literature (Guen and Thome\n2020), (Liu et al. 2024) and (Wang et al. 2023b).\nMeasurement Disentanglement Since no ground\ntruth is available about the true state, we learn to\napproximate the state representations purely from the\nmeasurement data. We use Fourier filtering to find\nand separate the dominant spectrum present across the\nmeasurement space. To achieve the best performance\nfor LTSF we pre-computed a filter by taking all the\n$T$ length windows in the training data and extracted\nthe dominant spectrum across them. We define $Y_k =$\n$\\left\\{Y_t, Y_{t+1}, ..., Y_{t+\\tau}\\right\\}$ as the sliding window over the data\nwith $\\tau \\in N$ as the window length. We use this Fourier\nfiltering to represent the resulting disentanglement in\nthe measurement space as,\n$Y_k^p = F^{-1}(G_a(F(Y_k)))$\n$Y_k^r = F^{-1}(G_a^c(F(Y_k)))$\ns.t. $Y_k = Y_k^p + Y_k^r$\nhere $G_a$ represents the filter, where a is a hyper-\nparameter, and $G_a^c$ represents its complement which\nis designed such that (6) is satisfied, guaranteeing\nno information loss. $F$ and $F^{-1}$ represents Fourier\ntransform and its inverse applied in the time dimension.\nBoth $y_k^p$ and $y_k^r$ are defined such that $y_k^p \\in Y$ and\n$y_k^r \\in Y$. We assume that whenever the data is available\nit always exist as a block of $\\tau$ samples. In many\napplication, such as electricity, motor fault analysis\netc, it is quite common that the data is collected for\na small window of time at irregular intervals. Hence\nduring the data assimilation task we assume that data\nis irregularly present as a block of $\\tau$.\nSegmentation and Encoding Within each $k^{th}$\nwindow we create further $s$ segments of segment length\n$w = T/s$. Hence $Y_k^p = \\left\\{y_k1, y_k2,..., y_ks \\right\\}$ after reshaping,\nwhere $y_k^i$ represents the $i^{th}$ segment of $k^{th}$ window.\nSame segmentation is used for $Y_k^r$. Therefore both\n$y_k^i \\in y^w$ and $y_{ki} \\in y^w$. We use identical encoder\narchitecture $\\phi_g : Y^w \\rightarrow H$ and $\\phi_r : Y^w \\rightarrow H$ for $Y_k^p$ and\n$Y_k^r$ respectively, to then get $Z_k = \\{z_{k1}, z_{k2}, ..., z_{ks}\\}$ and\n$R_k = \\{r_{k1}, r_{k2}, ..., r_{ks}\\}$ the respective representations as\n$z_{ki} = g(y_k^i), r_{ki} = r(y_k^i).$\nPrevious work (Lin et al. 2023) have shown that\nsegment wise iteration leads to better forecast for the\nLTSF problem instead of point wise iteration.\nPrediction Model\nBased on the disentanglement of the latent state\nrepresentation (3), we design a prediction model which\nis used to propagate the states (7) forward along the\ntemporal dimension. The prediction model consists\nof two parallel branches: (i) a Global model, which\npropagates the component $Z_k$, and (ii) a Residual"}, {"title": null, "content": "model which propagates the component $R_k$. We\ndescribe each of these models below.\nResidual Model Modeling the residual is important\nfor the reconstruction and for the forecast of the\nmeasurement trajectory. We can define the residual $R_t$\ntrajectory as follows,\n$r_{ki} = F_r(r_{k(i-1)})$\nhere $F_r: H\\rightarrow H$ represents the residual model. We use\na gated recurrent unit (GRU) (Chung et al. 2014) to\nthen propagate the residual trajectory. Other recurrent\narchitecture such as LSTM (Cheng, Dong, and Lapata\n2016) or RNN (Salehinejad et al. 2017) could also be\nused here.\nPhysical Model Our prediction model for the\nphysical component of the signal learns koopman\noperator $K: H \\rightarrow H$ such that,\n$z_{k(i)} = K_O z_{k(i-1)}$\nmany methods in literature has been proposed to\nestimate the Koopman operator from data. Note that\n$z_{ki}$ in (9) and $z_{ki}$ in (7) are different as the later\nrepresents the encoding from the measurement while\nthe prior represents the koopman forecast. To align\nthe two representation, an alignment loss is computed\nand minimised. In (Lusch, Kutz, and Brunton 2018;\nAzencot et al. 2020) the $K$ is composed recursively\nwith $z_0$ to generate all the future $z$. This leads to poor\nperformance and drift especially for multivariate high\ndimensional non-stationary data as shown in (Fathi\net al. 2024). This also leads to poor generalisation across\nstate space not observed in the look-back window.\nTo mitigate this, during inference for LTSF task we\nmake use of periodic re-encoding that is after every\n$k^{th}$ windows we use the decoder to generate predicted\nmeasurement window $(k + 1)^{th}$ and use the encoding\nmodel to get state representation.\nUpon adding the output of (9) and (8) we can\nget the state representation for the next segment. For\neach prediction cycle we use (9) and (8) to predict\nall the segments in the future window and stack them\nin order to get $\\bar{Z}_k$ and $\\bar{R}_k$ respectively. Similarly we\nstack and add the output of the encoders in (7) to get\nReconstruction of $(k - 1)^{th}$ window.\n$\\hat{Y}_k = \\psi(\\bar{Z}_k + \\bar{R}_k), \\hat{Y}_{k-1} = \\psi(Z_{k-1} + R_{k-1})$\nhere $\\psi: H \\rightarrow y^{s \\times w}$ is the decoder that maps from\nstate space to the segmented view of the measurement\nspace. We reshape $\\hat{Y}_k$ and $\\hat{Y}_{k-1}$ back to $y$. Therefore\nby continuously evolving the physical and residual\ncomponents and decoding their output (10) we can\ngenerate $Y_k$ and reconstructed trajectory $\\hat{Y}_{k-1}$.\nCorrection Model\nWhen the model is used to make predictions over\nlong time horizons the predictions starts to experience\ndrift from the actual trajectory. In many use cases"}, {"title": null, "content": "the measurement data is sparsely and irregularly\navailable the long prediction window. Using a data\nassimilation-based framework we can make use of\nthese measurements to correct the drift present in the\npredictions in a systematic way.\nThe extended Kalman filter (EKF) framework is\na principled framework which updates the predicted\nstates in a NLDS as a function of the error between\nthe observed and predicted measurements (S\u00e4rkk\u00e4 and\nSvensson 2023). Assuming data $Y_k$ is available, we can\ndefine the correction equation inspired by the EKF as\n$\\begin{bmatrix} \n\\hat{Z}_{k} \\\\\n\\hat{R}_{k} \n\\end{bmatrix} = \n\\begin{bmatrix} \n\\hat{Z}_{k}^0 \\\\\n\\hat{R}_{k}^0\n\\end{bmatrix} + \\begin{bmatrix} \nL_g \\\\\nL_r\n\\end{bmatrix} \\begin{bmatrix} J_{Y} \\end{bmatrix}^T [Y_k - \\hat{Y}_k]$\nHere $J_Y$ is the Jacobian of the decoder function\nevaluated at the output of the prediction step i.e.\n$\\hat{h}_{k} = \\hat{Z}_{k} + \\hat{R}_{k}$. The Jacobian linearizes the nonlinear\nmeasurement function around the current predicted\nstate:\n$J_{Y} = \\frac{\\partial \\psi(.;0)}{\\partial h}|_{\\hat{h}_k}$\nThe output of the correction step $\\hat{h}_{k} = \\hat{Z}_{k} + \\hat{R}_{k}$ is then\nfed to the decoder to get the corrected prediction of the\nmeasurement trajectory.\nKalman Gain The gain matrices $L_g$ and $L_r$\nrepresent the component of Kalman gain for the global\nand residual branches. We use a gating mechanism\nparameterised by neural network to define the Kalman\ngains,\n$L_g = tanh(W(\\hat{Z}_k) + W_\\tau(\\phi_g(Y)) + b_g)$,\n$L_r = tanh(W(\\hat{R}_k) + W_\\tau(\\phi_r(Y)) + b_r).$\nhere $W, W, W, W$ are all single layer neural\nnetworks with ReLU activation. $b_r, b_g$ are bias terms\nfor $L_r, L_g$ respectively."}, {"title": "Learning criterion", "content": "Operator Learning Loss The physical model\nconsists of the following trainable components: (i) latent\nKoopman model $K$, (ii) state encoding model and (iii)\nstate decoding model. To ensure that the koopman\ndynamcis is respected by the learned physical model\nwe minimise the following losses,\n$L_{recon} = \\sum_{t=0}^H ||Y_k - \\hat{Y}_k||_2, L_{pred} = \\sum_{t=1}^T || Y_k - \\hat{Y}_k||_2,$\n$L_{align} = \\sum_{k=1}^{T/k} ||z_k - \\hat{Z}_k||_2$\n$H$ is forecast horizon and $T$ is lookback window length.\nTo achieve the best performance we train KODA in\nstages. First we train the prediction model with $L_{align}$,\n$L_{pred}$ and $C_{recon}$. We then add the correction model and\ntrain only using $L_{pred}$. This two stage training process\nallows us to first learn the prediction model and then in\nthe second stage learn the gain function while further\nimproving the prediction output.\nTraining Criteria for $L_g$ and $L_r$ The Kalman\ngain $L_g$ and $L_r$ depends on whether the measurement\ndata $Y_k$ is available for assimilation or not. The gating\nmechanism is first trained separately to allow correction\nto happen only when measurement data is made\navailable during the inference time. During inference\nwhen no data is available the correction component"}, {"title": null, "content": "should be 0 such that (14) is only driven by the\nprediction term. For further details on training criteria\nand network architecture please refer to the appendix.\nExperimental setting\nWe evaluate KODA across multiple datasets and\nconduct an extensive comparison across different\nbaselines. We evaluate KODA over two main tasks:\n(1) Forecasting and (2) Data Assimilation. We\nalso present comparison based on some well-known\nNLDS datasets for the (3) State Prediction task.\nThrough our experiments we showcase the comparable\neffectiveness of KODA with state of the art baselines\nfor forecasting and then further improvement with the\ndata assimilation strategy.\nDatasets: We compare KODA across 4 different real\nworld multivariate NLDS time series datasets: (1)\nweather (Wetterstation), (2) traffic (PeMS), (3) ECL\n(UCI), (4) ETT (Zhou et al. 2021)(including 4 subsets:\nETTh1, ETTh2, ETTm1, ETTm2). We also evaluate\nthe performance of KODA on M4 (SpyrosMakridakis\n2018) which is a real world univariate time series on\nperiodically collected marketing data. We use the same\ndata prepossessing and test time split ratio as presented\nin (Wu et al. 2023).\nWe use the same datasets for showing results for\ndata assimilation task. We present additional results\non state prediction task for some of the well known\nuni-variate NLDS: (1) Lokta Volterra, (2) Duffing\nOscillator, (3) pendulum and (4) Lorenz 63. These\nNLDS present extremely nonlinear dynamics with\nvarying dimensionality including multiple fixed points.\nAppendix B for NLDS description.\nBaseline: We choose state of the art methods to\ncompare KODA performance on forecasting and data\nassimilation tasks. We would like to make the note that\nmost deep learning methods discussed as our baseline\ndon't have a well defined way for data assimilation.\nFor the comparison on the forecasting task we evaluate\nour model against MLP based methods: Koopman\nforecaster (KNF) (Wang et al. 2023b), Koopa (Liu\net al. 2024), Dlinear (Zeng et al. 2022), TiDE (Das\net al. 2023); Transformer based methods: fedformer\n(Zhou et al. 2022), PatchTST (Nie et al. 2023);\nInformer (Zhou et al. 2021) Temporal convolution\nmethod: TimesNet (Wu et al. 2023), MICN (Wang\net al. 2023a). For the state prediction task from noisy\nmeasurements on the known NLDS we compare KODA\nagainst vanilla Koopman autoencoder (Lusch, Kutz,\nand Brunton 2018), MLP and Koopman autoencoder\nwith periodic re-encoding (Fathi et al. 2024). We\ncompare KODA against the method presented in (Frion\net al. 2024a) for the data assimilation and forecasting\ntask when data is sparsely present in the lookback\nwindow. KODA's framework allows us to assimilate\nmeasurement data into prediction recursively. Hence, to\nevaluate KODA's performance we use different amount\nof noisy measurement present in the forecast window"}, {"title": null, "content": "and evaluate model forecast on the rest of the forecast\nwindow. We compare this over different sets of noise\nlevels in the measurement data.\nEvaluation Metrics: The most commonly used\nmetric for comparing the accuracy of the forecasting\nmethod is: (1) Mean Absolute Error (MAE) and (2)\nMean Squared Error (MSE). For further details on the\nexperimental setup, please refer to Appendix B.\nResults\nLong Term Forecast In Table 1 we present the\nresults for long term forecasting across several recent\nstate-of-the-art methods. We compare all the methods\non four different multivariate time series datasets. The\nchoice of horizon $T_h$ is set to be \\{96,192,336, 720\\}.\nThrough this task we are solely evaluating KODA's\nability to generate long term accurate forecasts. The\nlookback window for all the models in Table 1 was\nset to be $T_L = 720$. We present additional results\nacross different lookback windows in the appendix. We\nobserve that except for the traffic benchmark, KODA\nalmost beats all the other methods across benchmarks.\nIn the case of traffic dataset we observe that the\nbest performing model were TiDE and PatchTST\nacross several of the prediction horizon length except\nwhen $H = 720$, where KODA provides the second\nbest performance. One interesting observation is that\nKODA outperforms all the other Koopman-based\nmodels: KOOPA and KNF. This shows that KODA\npresents a flexible model that can produce long\nterm accurate forecast for multivariate non-stationary\ntime series datasets. We provide additional results\nincluding results on uni-variate time-series benchmark\nM4 (SpyrosMakridakis 2018) in Appendix C.\nData Assimilation One of the main contribution\nof KODA is in its ability to perform online data\nassimilation during inference time hence making the\nmodel highly adaptive. In Table 2 we present the\nresult for the online data assimilation task. We define\n$\\alpha$ as the percentage of data available in the forecast\nhorizon for assimilation. We show results for $\\alpha \\in$\n\\{10\\%, 20\\%, 30\\%\\} across all the benchmarks. For this\nexperiment we chose $T = 24$ and $w = 8$. For the sake of\nsimplicity we distributed the available measurements\nalong the prediction horizon uniformly. We see a\nconsistent improvement in the forecast results when\ncorrection is used. This shows that KODA other than\nbeing able to generate accurate long term forecasts,\nKODA is also able to make use of the data available at a\nfuture time to further improve its forecast. This makes\nKODA framework really potent in tasks where data is\nirregularly present at different intervals and long term\nforecasting is required. Additionally KODA can also be\nused when the data are irregularly present instead of\nuniformly. We show additional results on assimilation\ntask across benchmarks in the appendix C."}]}