{"title": "DIVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level Relation Extraction", "authors": ["Yiheng Wu", "Roman Yangarber", "Xian Mao"], "abstract": "The remarkable capabilities of Large Language Models (LLMs) in text comprehension and generation have revolutionized Information Extraction (IE). One such advancement is in Document-level Relationa Triplet Extraction (DocRTE), a critical task in information systems that aims to extract entities and their semantic relationships from documents. However, existing methods are primarily designed for Sentence-level Relation Triplet Extraction (SentRTE), which typically handles a limited set of relations and triplet facts within a single sentence. Additionally, some approaches treat relations as candidate choices integrated into prompt templates, resulting in inefficient processing and suboptimal performance when determining the relation elements in triplets.\nTo address these limitations, we introduce a Discriminative and Voice-Aware Paradigm-DiVA. DiVA involves only two steps: performing document-level relation extraction (DocRE) and then identifying the subject/object entities based on the relation. No additional processing is required simply input the document to directly obtain the triplets. This streamlined process more accurately reflects real-world scenarios for triplet extraction. Our innovation lies in transforming DocRE into a discriminative task, where the model pays attention to each relation and to the often overlooked issue of active vs. passive voice within the triplet. Our experiments on the Re-DocRED and DocRED datasets demonstrate state-of-the-art results for DocRTE task.", "sections": [{"title": "Introduction", "content": "Relation Triplet Extraction (RTE) aims to extract the entity pair and the semantic relation type from the unstructured text, which plays a vital role in various downstream Natural Language Processing (NLP) applications, including knowledge graph construction and information retrieval (Li et al. 2023; Xiong, Power, and Callan 2017; Schlichtkrull et al. 2018).\nLarge-scale pre-trained language models (LLMs), such as LLAMA (Touvron et al. 2023) and GPT-3 (Brown et al. 2020) demonstrate that LLMs perform well on various NLP tasks with fine-tuning due to their strong capabilities in text understanding, generation, and generalization (Kalyan 2024). Therefore, LLMs are widely used in classical information extraction (Pang et al. 2023) tasks such as Named Entity Recognition (NER) (Nie, Shao, and Wang 2024; Li et al. 2024c), Relation Classification (RC) (Wadhwa, Amir,\nand Wallace 2023), Relation Extraction (RE) (Ma, Li, and Zhang 2023), Event Extraction (EE) (Sun et al. 2024b), and have already achieved remarkable accomplishments.\nIn sentence-level RE tasks (Yang et al. 2023a,b), the number of entities is limited and the relation is simple to distinguish (Roth and Yih 2004; Riedel, Yao, and McCallum 2010); then these entities can generate results through question-answering (Wan et al. 2023). However, in DocRE, as the text length increases, the task not only requires determining the relation between entities that may be far apart in the document, but also identifying whether a relation exists between them.\nConsequently, many SentRTE methods cannot be directly applied to document-level RE tasks (Wadhwa, Amir, and Wallace 2023; Wan et al. 2023). DocRE serves as the foundation for the DocRTE task. Most previous research has opted to sacrifice efficiency to address this issue. They generate triplets based on each relation in the relation list and then filter generated triplets based on probability and other factors (Ozyurt, Feuerriegel, and Zhang 2024) . This approach not only yields suboptimal results but also significantly increases the cost of training.\nMoreover, previous works seldom emphasize the importance of correctly distinguishing between subject and object entities, especially in the context of active and passive voice. Failing to account for the impact of voice can result in extracted triplets with reversed entity roles. Although entities themselves do not possess a voice, the distinction between subject and object entities is closely tied to the relation's direction, which can be influenced by whether the sentence is in the active or passive voice.\nTo overcome this we propose DiVA:\n\u2022 The conventional sequence-to-sequence method for di-rectly generating relations may result in the model not adequately focusing on each one. To address this, we have restructured DocRE into a discriminative task where the goal is to output each relation line by line along with a True/False text to indicate whether the relation exists in the given document\n\u2022 We prompt the model to be voice-aware, thereby enhancing its ability to distinguish between subject and object entities. Additionally we input all entities along with the document, prompting the model to identify which entities do not have relations with each other. This approach enhances the model's ability to understand the overall context.\n\u2022 We conducted extensive experiments on both the DoCRED and RE-DocRE datasets. Since our proposed method includes the DocRE stage, we also reported the results of the DocRE. The results demonstrate that DiVA achieves SOTA performance on the DocRTE task."}, {"title": "Related work", "content": "SentRTE with LLMs: today's Large Language Models (LLMs) have shown a remarkable ability to effectively extract triples from simple sentences through zero-shot or few-shot learning. Recent research has focused primarily on prompt engineering and in-context learning (ICL) (Brown et al. 2020). Among these approaches, Tabular Prompting has proven to be highly effective. For instance, Li et al. (2024b) employed tabular prompting to unify two different tasks and settings, generating organized and concise outputs in ICL. Specifically, they used a table header \"-Predicate-Subject-Object\" to structure the output. Particularly when combined with high-performance models like ChatGPT, even relatively simple retrieval methods can achieve highly competitive results Wan et al. (2023). Additionally, leveraging ChatGPT for reasoning has proven to be an effective technique (Liu et al. 2024). Li et al. (2024a) emphasized the integration of table prompting with ICL; they propose a tabular prompting approach, called TableIE,"}, {"title": "Methodology", "content": "In this section, we introduce our proposed paradigm. As shown in Figure 2, DiVA consists of two key steps: (1) Document-level relation extraction with discriminative paradigm, and (2) Extraction of subject and object entity pairs based on the extracted relation with voice-aware paradigm. In the following sections, we instantiate DiVA in zero-shot and fine-tuning scenarios, demonstrating its effectiveness in addressing the complexities of DocRTE."}, {"title": "Problem Formulation", "content": "Given a dataset $D$ with a set of pre-defined relation types $R$. Given a document $d_i \\in D$, DocRE aims to extract relation instances, formulated as $r \\subset R$. DocRTE aims to extract relation triplets with relation types, formulated as $(r_j, e_h, e_t), e_h, e_t \\in E_i, r_j \\in r$, where $e_h$ is the head entity, $e_t$ is the tail entity, $E_i$ is the set of entities of document $d_i$."}, {"title": "Pre-Experiment Preparation", "content": "Before conducting the experiment, we perform preparatory steps to obtain relation clusters and set of entity sets.\nFirst, the DocRTE task involves a vast number of relation categories, some of which are semantically related and may appear simultaneously (e.g., date of birth and date of death, both time-related, whereas others like head of government and start time are less related). Also, certain relations can be challenging to distinguish: e.g., participant and participant of. To address this complexity, we employed ChatGPT to cluster the relations and divided the relation list into several sub-lists, as outlined in Algorithm 1. Although this clustering approach slightly increases the linear time complexity, it significantly enhances the accuracy of the task. \nSecond, documents often contain many entities, not all of which are related or annotated (Yao et al. 2019; Tan et al. 2022). To handle this, we use ChatGPT to extract the entity set $E_i$ from document $D_i$ as a pre-processing step, detailed in Algorithm 2. This set is then integrated into the prompt, prompting the model to carefully evaluate potential relationships, even where none exist, thereby enhancing overall DocRTE accuracy."}, {"title": "Extracting Relation with Discriminative Paradigm", "content": "Providing a document and a pre-defined list of relations to an LLM and expecting it to output all possible relations often leads to suboptimal performance (Xue et al. 2024). This issue arises because the LLM must fully comprehend the nuances of each relation, review the entire relation list, and generate the correct output. While incorporating explanations for each relation might assist the LLM in understanding their meanings (Wan et al. 2023; Xue et al. 2024), this approach can be inefficient and akin to a reading comprehension task, particularly with lengthy texts, where the model may overlook or misinterpret complex sections. To enhance relation extraction accuracy, we restructured the task into a discriminative paradigm, prompting the LLM to generate outputs that specify whether each relation from the pre-defined list is present (True) or absent (False) in the document. This design compels the LLM to systematically examine each relation, ensuring that no critical details are missed during the extraction process. This significantly improves the model's performance in identifying and distinguishing complex relations within documents."}, {"title": "Extracting Entity Pair with Voice Consideration", "content": "An often overlooked aspect of DocRTE is the need to not only identify pairs of entities that may share a relation but also correctly distinguish between subject and object entities, particularly in the context of active and passive voice. While entities themselves do not have a voice, the directionality of the relation is influenced by whether the sentence is active or passive, which is crucial for relations like participant of and participant. Neglecting this can result in reversed entity roles, negatively impacting DocRTE performance.\nPrevious approaches have addressed this by sequentially extracting active and passive entities (Xue et al. 2024). In contrast, we integrate this process with a specific prompt: \"The subject entity does the relation to the object entity; the object entity is done relation by the subject entity.\" This prompt, although it is grammatically not quite correct, remains understandable to the LLM. This prompt guides the model in distinguishing between active and passive voice, ensuring accurate subject-object role identification."}, {"title": "Method Instantiation in Zero-shot Scenarios", "content": "In DiVA to zero-shot scenarios, we utilized ChatGPT as the foundational LLMs for experiments. Zero-shot learning involves the ability of a model to recognize and generate outputs for tasks it has not been explicitly trained on, relying instead on its generalization capabilities and pre-existing knowledge. Before the experiment, we split the relation list into m relation sublists using Algorithm 1, as shown in Figure 3, and we leverage ChatGPT to generate entities for each document, as shown in Algorithm 2.\nThe overall process is shown in Algorithm 3 which can be devided into 2 steps. At step 1, we process the DocRE task, then we obtain the triplets based on the generated relations. By providing chatGPT with carefully crafted prompts and structured queries, we aimed to extract meaningful relations and triplets from documents without prior specific training. To ensure fairness when comparing our results with those from other experiments, we incorporated in-context learning in the second step. Specifically, we retrieve 3 most similar examples from the train set which have the same relation category with evaluated example."}, {"title": "Method Instantiation in Fine-tuning Scenarios", "content": "The overall process is the same shown in Algorithm 3. Instead of utilizing ChatGPT (CallChatGPT in Algorithm 3 step 2 and step 4), we opt for employing open-source LLM. We first pre-process the dataset: we transform it into two new derived datasets. The first dataset is used for relation extraction as an intermediate task. Using this dataset, we train the extraction of relations from a given document. The second dataset (derived from the same original dataset) is used to train the extraction of subject and object entities given a document and a relation.\nDuring evaluation, we first perform the DocRE task on the given document. After obtaining the relations, we then generate the triplets. We also conducted evaluations using the ground truth relations, as detailed in Table 4. We do not incorporate in-context learning in our fine-tuning process. Instead, we focus on training the model directly from the provided data. This decision streamlined the training procedure and emphasized the model's ability to learn effectively from the data alone.\nAmong several open-source models, we employed the LLama3-7B because it demonstrated superior performance. To facilitate efficient training, we opted for QLORA (Dettmers et al. 2023) which combines the benefits of quantization and Low-Rank Adaptation, resulting in efficient fine-tuning and valuable for deploying LLMs on resource-constrained devices. The fine-tuning process was conducted over approximately 5 hours using two NVIDIA 4090 TI GPUs. DiVA allows a single training procedure to comprehensively address the entire DocRTE task."}, {"title": "Experiments", "content": "Dataset: We evaluate our paradigm on both DocRE and DocRTE tasks using two public datasets. DocRED is a popular large-scale, human-annotated document-level relation extraction dataset, which includes a human-annotated train set with 3053 documents, a human-annotated dev set with 998 documents and 96 predefined relation types, constructed from Wikipedia and Wikidata. Considering that the test set of DocRED does not provide a ground truth file, we follow the previous studies (Ozyurt, Feuerriegel, and Zhang 2024) and perform evaluations only on the development set.\nRe-DocRED is a revised version of DocRED by supplementing positive instances that are ignored in the DocRED dataset. Re-DocRED test consisting of 499 articles and 17,448 triplet facts, and a validation set containing 498 articles with 17236 triplets, ensuring a comprehensive and precise assessment.\nEvaluation Metrics: We adopted a stringent micro $F_1$ metric, considering a prediction correct only if it accurately captures the entire relation, including both the subject entity and object entity. We also report the number of True Positives (TP) and False Positives (FP) In both DocRED and Re-DocRED datasets, multiple entity names may refer to the same underlying entity. Consequently, if the predicted entity name aligns with any alias of the annotated entity, it is deemed correct. To ensure a rigorous and valid evaluation, regardless of the number of aliases an entity possesses, it will only be counted once in the triplet alignment evaluation. All incorrect predictions are classified as false positives. This method ensures a precise and statistically sound evaluation, bolstering the credibility of our results."}, {"title": "Comparing Methods", "content": "We selected two of the latest and most representative approaches for comparison.\nGenRDK (Sun et al. 2024a), is a zero-shot document-level relation triplet extraction framework that generates labeled data by retrieving and denoising knowledge from LLMs. Their model is fine-tuned with LLaMA2-13B-CHAT. The authors also reported results from the ChatGPT version (without fine-tuning, we name it as GENRDK*), and we include these results for comparison.\nAutoRE (Xue et al. 2024), is an end-to-end document-level relation extraction approach that adopts a novel RE extraction paradigm named RHF. This method explores various paradigms, and achieved state-of-the-art performance both on DocRE and DocRTE. The authors also reported results from the ChatGPT version (without fine-tuning, we name it as GENRDK*), and we include these results for comparison.\nWe report our results of three different experimental settings in Table 4: Dev\u00b2\u2014zero-shot task using ChatGPT to generate triplets; DevFT \u2014fine-tuning to generate triplets; and DevGT leveraging ground-truth relations to generate triplets. Analogous 3 settings are used with the Test dataset."}, {"title": "DocRE Results", "content": "This section compares DiVA with GenRDK and AutoRE, showing that DiVA consistently achieves higher micro $F_1$ scores and lower false positives. The results underscore the effectiveness of our discriminative paradigm over AutoRE's relation generation method.\n\u2022 As shown in Table 1, our approach\u2014DiVA\u2014 consistently outperforms the GenRDK and AutoRE models across both Re-DocRED and DocRED datasets. Specifically, on the Re-DocRED test set, DiVA achieves an micro $F_1$ score of 80.98 compared to GenRDK's 41.3 and AutoRE's 72.06.\n\u2022 As shown in Table 2. Our detailed error analysis reveals that DiVA maintains low false positives (590 on dev and 596 on test for Re-DocRED), indicating higher precision compared to GenRDK and Zero-shot models. In contrast, our zero-shot approach shows a significant discrepancy between true positives and false positives, reflecting its difficulty in accurately extracting relations.\n\u2022 The key difference between DiVA and the AutoRE method in the DocRE stage is that AutoRE opts for having the model directly generate relations, whereas we employ a discriminative paradigm. To help the model understand the specific meaning of each relation, AutoRE incorporates demonstrations for each relation category and modifies some category names to make them easier for the model to distinguish. However, the final results demonstrate that our proposed approach is not only much simpler but also more effective, easily surpassing AutoRE by over 8 micro $F_1$ scores on DocRE.\nOverall, DiVA excels in DocRE tasks on both Re-DocRED and DocRED datasets, significantly outperforming models like GenRDK and AutoRE. The high micro $F_1$ scores and low false positives demonstrate the effectiveness and precision of our discriminative paradigm. This highlights that DiVA, while simpler, is more efficient and robust in document-level relation extraction compared to more complex alternatives like AutoRE."}, {"title": "DocRTE Results", "content": "This section presents a comparative analysis of our fine-tuned and zero-shot approaches for the DocRTE task. The results highlight the superior performance of the fine-tuned model, particularly in precision and recall, while also emphasizing the significant role of the DocRE process in achieving these outcomes.\n\u2022 As shown in Table 3, our fine-tuned DiVA, achieves better performance than the compared approaches, with micro $F_1$ scores of 56.87 on the Re-DocRED test set and 47.09 on the DocRED dev set, significantly improving precision and recall by reducing false positives and increasing true positive identifications. Our zero-shot approach's high false positive count indicates a tendency to incorrectly label non-relevant relations, which significantly affects its precision. This issue highlights the limitations of zero-shot approaches in complex relation extraction tasks.\n\u2022 Table 4 shows that the zero-shot approach generates very many false positives-one of the reasons why its performance is far inferior to the fine-tuning methods. ChatGPT's output is difficult to control, and even with carefully crafted prompts, it can generate content unrelated to the target. We will analyze ChatGPT's generated results in more detail in the Error Analysis section.\n\u2022 The high performance of our DocRTE is largely due to the effectiveness of our DocRE. It can be seen that after using the ground truth relations, DocRTE achieved a significant improvement.\nIn summary, our DiVAFT demonstrates superior performance on DocRTE tasks, significantly outperforming zero-shot methods by reducing false positives and increasing true positive identifications. The analysis underscores the importance of fine-tuning in complex DocRE tasks, where precision and recall are crucial. The results also highlight the strong correlation between effective DocRE and successful DocRTE, as shown by the substantial improvement when ground truth relations are used."}, {"title": "Ablation Studies", "content": "\u2022 As shown in Table 5, the absence of the discriminative paradigm leads to noticeable variations in the performance of our model. Specifically, the results from triplet extraction on the Re-DocRED dataset exhibit slight fluctuations compared to results in Table 3 (58.27 vs. 58.24, 56.55 vs. 56.87, 46.90 vs. 47.09). However, these seemingly minor changes belie a more significant impact on the DocRE task, where we observe a performance drop of 2 to 3 points. This indicates that the discriminative paradigm plays a crucial role in maintaining high accuracy during DocRE, particularly in the more complex document-level settings of Re-DocRED.\nFor the DocRED dataset, the impact of removing the discriminative paradigm is even more pronounced, affecting both the DocRE and DocRTE tasks. The paradigm's absence likely introduces noise into the relation extraction process, thereby reducing the model's ability to accurately extract relevant triplets. This further emphasizes that the discriminative paradigm is not just beneficial but essential for achieving state-of-the-art results across different datasets, particularly those with a high degree of relational complexity.\n\u2022 Table 6 shows that the removal of the voice-aware paradigm also leads to performance degradation, especially on the Re-DocRED dataset, where both DocRE and DocRTE tasks are adversely affected. The degradation in performance highlights that without this paradigm, the model's ability to discern the correct directionality of relations (i.e., who is doing what to whom) is compromised, leading to inaccuracies in the extracted triplets.\nInterestingly, the impact on the DocRED dataset is minimal (46.90 vs. 47.09). We speculate that this reduced impact may be due to the less precise and incomplete annotations in the DocRED dataset, a limitation that has been frequently cited in previous studies. The incomplete nature of the annotations in DocRED might obscure the benefits of the voice-aware paradigm, as the dataset may not fully capture the subtleties of active and passive voice distinctions. This highlights a broader issue in the field: the quality of dataset annotations significantly influences the efficacy of advanced paradigms like the voice-aware approach.\nOverall, these findings reinforce the importance of both the discriminative and voice-aware paradigms in enhancing the performance of DocRTE models. They also suggest that future improvements in dataset annotation quality could further amplify the benefits of these paradigms, leading to even more accurate and reliable information extraction systems."}, {"title": "Error Analysis", "content": "In this section, we manually examine some examples in the zero-shot and fine-tuning scenario.\nIn Figure 4 (a) shows one text document and all relations in the document according to ground truth. It then shows the relations output from the zero-shot and fine-tuning scenarios. Blue relations are correctly extracted, green are incorrect but are among the pre-defined relations, red means it is not among the pre-defined relations.\n\u2022 Our fine-tuned LLaMA missed some relations, but the relations it generated were all correct. This is agrees with the fact that the number of false positives (FP) in Table 2 is relatively low. However, among the relations generated by ChatGPT, a relation called coaching appeared, which is not present in the pre-defined relation list. Although coaching may describe the relation between Jon Montgomery and Wilfried Schneider, it also highlights the need for strong prompts to constrain ChatGPT's output effectively.\n\u2022 Figure 4 (b) shows that only three correct triplets were generated by ChatGPT, alongside several incorrect triplets under the instance of relation. From a human evaluation standpoint, some of these triplets could be considered accurate, such as \u201c2002 Winter Olympics is an instance of Winter Olympics,\u201d even though this triplet is not part of the ground truth. However, the triplet \u201cTurin is an instance of the 2006 Winter Olympics\u201d is clearly incorrect. This inconsistency highlights a phenomenon that warrants further investigation. Annotating the DocRTE task is particularly challenging because it requires labeling all relations among numerous entities.\nWith effective filtering methods, ChatGPT could potentially streamline this annotation process and contribute to improving the accuracy of the DocRED and Re-DocRED datasets."}, {"title": "Conclusion", "content": "We introduced DiVA to Document-level Relation Triplet Extraction (DocRTE), addressing the challenges that traditional methods face when transitioning from Sentence-level tasks. By transforming Document-level Relation Extraction (DocRE) into a discriminative task and implementing a voice-aware paradigm, DiVA improves the model's accuracy in determining relations and correctly identifying subject-object pairs in complex texts. Experiments on Re-DocRED and DocRED datasets demonstrate that DiVA outperforms existing methods, achieving state-of-the-art DocRTE results. This end-to-end DocRTE paradigm simplifies the extraction process, reduces computational complexity, and offers a more precise solution for a wide range of relation categories, paving the way for future advancements in information extraction. Clearly there is much room for improvement. We are currently working on enhancing DiVA by incorporating fine-grained entity and relation types, expanding its application to multilingual datasets, and exploring semi-supervised learning to improve its adaptability and efficiency."}]}