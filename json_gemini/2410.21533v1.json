{"title": "L3MS \u2014 LAGRANGE LARGE LANGUAGE MODELS", "authors": ["Guneet S. Dhillon", "Xingjian Shi", "Yee Whye Teh", "Alex Smola"], "abstract": "Supervised fine-tuning (SFT) and alignment of large language models (LLMs) are key steps in providing a good user experience. However, the concept of an appropriate alignment is inherently application-dependent, and current methods often rely on heuristic choices to drive the optimization. In this work, we formulate SFT and alignment as a constrained optimization problem, where the LLM is trained on a task while being required to meet application-specific requirements, without resorting to heuristics. To solve this, we propose Lagrange Large Language Models (L3Ms), which employ logarithmic barriers to enforce the constraints. This approach allows for the customization of L3Ms across diverse applications while avoiding heuristic-driven processes. We demonstrate experimentally the versatility and efficacy of L3Ms in achieving tailored alignments for various applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) are used for a wide range of tasks: as chatbots (Brown et al., 2020; OpenAI, 2024), for code generation (Ahmad et al., 2021; Wang et al., 2021; Rozi\u00e8re et al., 2024), for medical assistance (Yang et al., 2022; Moor et al., 2023), and more. The key ingredients for their impressive downstream performance are supervised fine-tuning (SFT) and alignment; the former fine-tunes the LLM to a task of interest, while the latter instills it with preferential properties. Arguably, the right combination of preferential properties is highly application/task-dependent. For instance, a scholar might want a chatbot to be honest and factual to assist with their work, whereas a fiction writer might prefer the opposite behavior to help create fantastical imaginary worlds. There is also plenty of (anecdotal) evidence in support: some LLMs refuse to provide information on how to \"kill\" a process in Unix, recommending the use of less violent strategies for dealing with wayward computer programs instead. Therefore, we need frameworks for the customization of LLMs.\nConsequently, Li et al. (2021); Bai et al. (2022); Rame et al. (2023); Wu et al. (2023); Ji et al. (2023) train LLMs on varying combinations of such preferential properties. In practice, one tends to resort to trial and error to find the right preference combination for their particular task. In doing so, one verifies if certain minimum baselines are satisfied, such as ensuring the factual correctness of statements or confirming that the response lengths are capped at 100 words. Since there isn't a way to enforce such requirements directly, current methods resort to heuristics. Additionally, existing pipelines carry out SFT and alignment in sequential steps. Hence they need to ensure that the LLM does not forget relevant task information learned during the SFT stage. This is achieved by penalizing the LLM for drastic deviations: the strength of the penalty is determined heuristically.\nIn this work, we formulate SFT and alignment in LLMs as a constrained optimization problem. In particular, we fine-tune an LLM to minimize the task perplexity (the objective function) while simultaneously satisfying application-specific minimum requirements (the constraints). This merges the SFT and alignment stages and mitigates the reliance on heuristics altogether. Furthermore, we propose Lagrange Large Language Models, a.k.a. L3Ms, to solve such constrained optimization problems. Specifically, they do so by employing logarithmic barriers and enforcing the constraints gradually over the training procedure. Lastly, we empirically demonstrate how one can pick and choose different constraints and tailor L3Ms to a range of applications without resorting to heuristics."}, {"title": "2 OPTIMIZATION FOR LLMS", "content": "Training an LLM proceeds in multiple stages (Ouyang et al., 2022), which we discuss below."}, {"title": "2.1 PRE-TRAINING", "content": "The pre-training stage instills the LLM with a generic knowledge of language. It entails regenerating text/token sequences by minimizing their perplexity, i.e., the negative log-likelihood of the sequence normalized by its length. More formally, the perplexity on a sequence x is defined as:\n$l_{\\theta}(x) = \\frac{\\log \\pi_{\\theta}(x)}{|x|} = \\frac{1}{|x|}\\sum_{i=1}^{|x|} \\log \\pi_{\\theta}(x_i | x_{<i}),$\nwhere $x_i$ and $x_{<i}$ denote the i-th token in the sequence and its prefix, respectively. Additionally, the function $\\pi_{\\theta}(\\cdot)$ denotes the LLM's predicted probability distribution over token sequences, where the LLM is parameterized with weights $\\theta$. Then, the pre-training objective is given as follows:\n$\\min_{\\theta} \\mathbb{E}_{x \\sim q(x)} [l_{\\theta}(x)],$\nand the expectation is replaced by an empirical average over a large corpus with trillions of tokens."}, {"title": "2.2 SUPERVISED FINE-TUNING (SFT)", "content": "Next, one fine-tunes the LLM to a task of interest, such as instruction-following, summarization, or translation. The data are (prompt, response) pairs (x, y), and the LLM is trained to regenerate these responses. Thus, one minimizes the perplexity on the response conditioned on the prompt:\n$\\min_{\\theta} \\mathbb{E}_{(x,y)\\sim p(\\cdot)} [l_{\\theta}(y|x)],$\n(1)\nfor the distribution $p(\\cdot)$ over (prompt, response) pairs which reflects the task-related data."}, {"title": "2.3 ALIGNMENT", "content": "This stage aligns the LLM to generate responses with preferential properties. A common setup is to learn preference reward functions that represent properties like helpfulness and harmlessness (Bai et al., 2022), followed by reinforcement learning to adapt the LLM to maximize said rewards. This is referred to as reinforcement learning from human feedback (RLHF; Knox & Stone, 2008; Griffith et al., 2013; Christiano et al., 2017). Note that the preference reward functions need not always be learned; they could also be engineered or rule-based, such as the length of the response.\nGiven a single preference reward function $r(\\cdot)$, the alignment objective is given as follows:\n$\\max_{\\theta} \\mathbb{E}_{(x,y)\\sim p(\\cdot)} [r(y|x)] \\frac{\\psi_{\\omega \\pi_{\\theta}}(x)}{\\pi_{\\theta}(x)},$\n(2)\nThis maximizes the rewards for the LLM's responses to prompts sampled from the task distribution. To prevent over-optimization of the reward and avoid drastic deviation away from the SFT model, it is common practice to add a regularization penalty such as the KL divergence (Gao et al., 2023).\nWe are interested in k \u2265 1 different preferential properties. In such a scenario, one could learn individual preference reward functions $r_i$'s and optimize the LLM to maximize their combination. In particular, Li et al. (2021); Rame et al. (2023); Wu et al. (2023); Ji et al. (2023) use a linear combination of the rewards, substituting the single reward in Eq. (2) with $\\sum_{i=1}^{k} a_i r_i(y|x)$, for some choice of $a_i \\ge 0$. Alternatively, one could learn a single reward function by choosing the data proportions of the different properties used to train it. For instance, Bai et al. (2022) use a 3:1 proportion of helpfulness to harmlessness data to train a single preference reward function. Note that the proportions can also be represented via weights $a_i \\ge 0$ with $\\sum_{i=1}^{k} a_i = 1$. Therefore, the choice of $a_i$'s, combined with the strength of the regularization penalty, together steer the alignment. They are chosen based on the good judgment of the practitioner in a somewhat heuristic manner."}, {"title": "2.4 SHORTCOMINGS", "content": "While the above pipeline is commonly used to train LLMs for deployment, it has shortcomings.\nFirstly, how does one choose the weights $a_i$'s? Or equivalently, what is the right combination of preference properties? Choosing between properties such as truthfulness, verbosity, and humor depends on the application at hand. However, even when the application is known, the weights are chosen through trial and error: trying different combinations and inspecting the achieved rewards. For instance, if one wants the responses to be under 100 words for a summarization task, one might repeatedly examine the length of the responses and adjust the weights $a_i$'s to achieve that. This inherently involves verification against a set of minimum baselines being satisfied. Can we avoid heuristics and enforce such minimum requirements for the preference rewards in a principled way?\nSecondly, recall that the original task objective is in Eq. (1). However, optimizing a special purpose objective like Eq. (2) can lead to a decrease in performance on the original task. While penalizing the deviation away from the SFT model helps mitigate this to some extent, the strength of the penalty is again determined heuristically. Can we ensure the performance on the original task is maintained?"}, {"title": "3 RELATED WORK", "content": ""}, {"title": "3.1 ONE SIZE DOES NOT FIT ALL", "content": "Early work on LLM alignment assumed homogeneity of preferences (Bakker et al., 2022). How-ever, the reality is quite different: human preferences vary widely and are highly application-specific"}, {"title": "3.2 CONSTRAINED ALIGNMENT FOR LLMS", "content": "Independent of our work, Moskovitz et al. (2024); Dai et al. (2024) also introduced constrained optimization for LLM alignment but for varying reasons. Motivated to avoid over-optimization of preference rewards, Moskovitz et al. (2024) find \"proxy points\", values of the reward functions beyond which the performance of the LLM is negatively impacted. They constrain the average rewards to be in the vicinity of these proxy points. Dai et al. (2024) trade-off between helpfulness and harmlessness from a safety standpoint. They maximize the LLM's helpfulness while constraining its average harmlessness reward. Our motivation on the other hand is to tailor to custom preferences through sets of different constraints, while simultaneously learning to perform the task at hand.\nOur work is different in two important ways. Firstly, both Moskovitz et al. (2024); Dai et al. (2024) consider the alignment process in isolation, because of which their objective is either to maximize one of the preference rewards or to minimize the deviation away from the SFT model. On the other hand, we merge the SFT and alignment stages and choose task perplexity minimization as our objective. This directly ensures that the LLM's task-solving capabilities are preserved, and it avoids loading an additional reference (SFT) model during the training process to compute the deviations.\nSecondly, both Moskovitz et al. (2024); Dai et al. (2024) obtain solutions to the constrained problem by computing the saddle-point of the Lagrangian. This is done by formulating a min-max game where the LLM minimizes the Lagrangian and the Lagrange multipliers adapt to maximize it. How-ever, the non-convexity of the objective makes this nontrivial. Instead, we choose the logarithmic barrier approach as the associated Lagrange multipliers satisfy the KKT complementary slackness condition by design (cf. Section 5.2) rather than letting the learning procedure do so (which in our experience is extremely sensitive to the choice of the learning rate). We empirically observe that while both methods satisfy the constraints, our approach using log-barriers minimizes the task perplexity better (cf. Section 6.3). As a side effect, we avoid introducing new learning parameters.\nAdditionally, we provide experimental validation of our approach on more preferences than Moskovitz et al. (2024); Dai et al. (2024), and a 4.5\u00d7 larger LLM than Moskovitz et al. (2024)."}, {"title": "4 CONSTRAINED OPTIMIZATION", "content": "Our goal is to reduce the task objective in Eq. (1) for the LLM to solve the intended task, while also enabling custom alignment by having the LLM meet the application-specific minimum requirements on different preference rewards. To do so, we propose the constrained optimization problem:\n$\\min_{\\theta} \\mathbb{E}_{(x,y)\\sim p(\\cdot)} [l_{\\theta}(y|x)]$\nsubject to $\\mathbb{E}_{(x,.)\\sim p(\\cdot)} [r_i (y|x)] \\geq b_i$ for all $i \\in \\{1, 2, ..., k\\}.$\nHere, the objective is the same as the task objective in SFT, and the constraints are enforced to satisfy the custom requirements; this merges the SFT and alignment stages. The $b_i$'s signify the minimum baselines for each preference reward function $r_i(\\cdot)$ and the constraints are enforced on average.\nCompared to the previous approach, we no longer rely on heuristics to find a set of weights $a_i$'s to satisfy the minimum requirements; we can do so directly through the constraints. Furthermore, with the same objective as SFT, we can directly maintain task performance without any deviation penalty. Additionally, note that whenever a constraint is satisfied, its influence vanishes. For instance, if the LLM is naturally harmless and $r_{harmless}(y|x) \\geq b_{harmless}$, then the constraint is not active and the LLM will not be penalized. Conversely, the previous approach would further penalize the LLM."}, {"title": "4.1 TYPES OF CONSTRAINTS", "content": "While we write the constraints in Eq. (4) as expectation/average constraints, other forms exist. For instance, uniform constraints impose a minimum reward on every generated (prompt, response) pair:\n$r_i (y|x) \\geq b_i$ for all $(x,.) \\sim p (\\cdot), y \\sim \\pi_{\\theta} (\\cdot|x)$ and all $i \\in \\{1,2,...,k\\}.$\nAdditionally, chance constraints bound the probability of the inequality holding away from zero:\n$P_{(x,.)\\sim p(\\cdot)} [r_i (y|x) \\geq b_i] \\geq 1- \\epsilon_i$ for all $i \\in \\{1, 2,...,k\\}.$\nWhile these constraints are not equivalent, they are related. We can rewrite Eq. (7) in the form of expectation constraints by using $1 - \\epsilon_i$ as the threshold and taking the expectation of the indicator $1\\{r_i(y|x) \\geq b_i\\}$. Moreover, Eq. (6) implies Eq. (4), but the converse is not true. Unfortunately, Eq. (6) is difficult to achieve in practice, especially when the data distribution is unknown.\nWe continue using the expectation constraints, but similar discussions can extend to the other types."}, {"title": "4.2 LAGRANGE MULTIPLIERS", "content": "We can introduce Lagrange multipliers $\\lambda_i \\geq 0$ for the constraints and obtain the Lagrangian:\n$\\mathcal{L} (\\theta) = L (\\theta) + \\sum_{i=1}^{k} \\lambda_i C_i (\\theta).$\n(8)\nThere is a rich literature connecting the Lagrangian with constrained optimization. Notably, the KKT conditions (Karush, 1939; Kuhn & Tucker, 1951) provide sufficiency conditions for global optimality under convexity, where the solution can be obtained by finding the saddle point of the Lagrangian. However, these conditions are not enough for highly non-convex scenarios like ours.\nNonetheless, the Lagrangian is instructive in understanding the relative importance of the con-straints. For an active constraint, i.e., one satisfied with equality, the corresponding Lagrange mul-tiplier can be non-zero; the larger its value, the more important the constraint. Conversely, for an inactive constraint, i.e., one satisfied with strict inequality, the corresponding Lagrange multiplier must vanish to 0. This is known as complementary slackness and is one of the KKT conditions."}, {"title": "4.3 LOGARITHMIC BARRIER", "content": "A practical way to enforce constraints is with barrier functions. Consider the (relaxed) log-barrier:\n$B_{\\mu,\\varsigma}(z) = \\begin{cases} -\\mu \\log (-z), & z < -\\varsigma \\\\ z + \\mu - \\mu \\log \\varsigma, & z \\geq -\\varsigma \\end{cases}$ and hence $\\partial B_{\\mu,\\varsigma}(z) = \\frac{\\mu}{\\max(-z, \\varsigma)},$\n(9)\nwith parameters $\\mu, \\varsigma > 0$. This is a convex, continuous, and differentiable function, which is valid for all $z \\in \\mathbb{R}$. Importantly, for $\\varsigma = \\mu^2$, this barrier function converges to the characteristic function $\\chi\\{z \\leq 0\\}$ as $\\mu \\to 0$, i.e., it takes the value 0 when $z \\leq 0$ and $\\infty$ otherwise (Tal et al., 1992; Nash et al., 1994; Hauser & Saccon, 2006; Feller & Ebenbauer, 2017); the condition $\\varsigma = \\mu^2$ is sufficient, but not necessary (Kervadec et al., 2022). This convergence to the characteristic function is visually depicted in Fig. 1, showing the change in the log-barrier function as we gradually decrease $\\mu$.\nWe can now use the log-barrier to enforce the constraints in Eq. (5) and simply add them to the objective. We obtain an unconstrained objective, with $\\mu$ controlling the strength of the constraints:\n$G_{\\mu} (\\theta) = L (\\theta) + \\frac{1}{k} \\sum_{i=1}^{k} B_{\\mu,\\mu^2} (C_i (\\theta)).$\n(10)"}, {"title": "5 LAGRANGE LARGE LANGUAGE MODELS (L3MS)", "content": "We have thus far formulated the SFT and alignment stages as a constrained optimization problem in Eq. (5). We proceed to find solutions for the same by solving the unconstrained objective in Eq. (10). We call the family of models obtained in this way L3Ms, i.e., Lagrange Large Language Models."}, {"title": "5.1 OPTIMIZATION PROCEDURE", "content": "Since the log-barrier converges to the characteristic function as $\\mu \\to 0$, we want to find the minimizer of $G_{\\mu} (\\theta)$ for a very small $\\mu$. However, doing so directly leads to instabilities as the objective function is ill-conditioned. Instead, it is common practice to follow an iterative procedure where one finds the minimizer for a fixed $\\mu$, reduces $\\mu$, and repeats (Curtis et al., 2024). Specifically, the procedure is instantiated with initial values $\\theta_0$, $\\mu_1$, and $0 < \\gamma < 1$. On the t-th iteration, $\\mu_t \\gets \\gamma \\mu_{t-1}$ is reduced and $\\theta_t \\gets \\arg \\min_{\\theta} G_{\\mu_t} (\\theta)$ (with the initialization at $\\theta_{t-1}$). In doing so, the constraints are enforced gradually, nudging the LLM to satisfy them over the training procedure while avoiding instabilities. Furthermore, as $\\{\\mu_t\\} \\searrow 0$, the weights $\\theta_t$ converges to the minimizer of the constrained problem.\nIt is impossible to minimize $G_{\\mu_t} (\\theta)$ exactly in many practical applications. Instead, at each iteration, we take a single optimization step toward the solution. Doing so is amenable to stochastic gradient methods and mitigates computational overhead the training proceeds as normal while the value of $\\mu$ is reduced over the course of the training. One can guarantee convergence of such a proce-dure to the optimal solution in some settings; for instance, Curtis et al. (2024) prove convergence when dealing with box constraints. However, convergence in a scenario like ours is not guaranteed. Nonetheless, we will experimentally demonstrate its use for our constrained problems.\nWe employ stochastic gradient methods and derive the gradient of our objective function directly:\n$\\partial_{\\theta} G_{\\mu} (\\theta) = \\partial L (\\theta) + \\sum_{i=1}^{k} \\frac{\\partial_{\\theta}C_i (\\theta)}{\\max (-C_i (\\theta), \\mu^2)}.$ \n(11)\nThis follows immediately from Eqs. (9) and (10). Note that the $\\partial C_i (\\theta)$'s are also known as the policy gradients in reinforcement learning literature. We discuss our strategy for estimating these gradients in Appendix B and refer readers to Schulman et al. (2016) for a more detailed review."}, {"title": "5.2 CONNECTION TO LAGRANGE MULTIPLIERS", "content": "The log-barrier and the Lagrangian are intrinsically connected; this becomes evident when compar-ing Eq. (11) with the (gradient of the) Lagrangian in Eq. (8). In particular, we define the multipliers:\n$\\lambda_i = \\frac{\\mu}{k \\max (-C_i (\\theta), \\mu^2)},$"}, {"title": "5.3 IMPLEMENTATION DETAILS", "content": "Alternating objective and gradient clipping. A simple, yet effective way to ensure stable training of large models is gradient clipping (Goodfellow et al., 2016; Zhang et al., 2020). We employ this technique, albeit with the modification of clipping the gradients of both the task objective and the constraints separately, as they can have varying magnitudes. We achieve this by alternating between reducing the task objective and enforcing the constraints by flipping a fair coin to select one or the other. While this doubles the number of steps to achieve the same effect, it does not increase the amount of work done as now only one part of the objective or the other is evaluated at each step.\nLength normalization. The gradient of our objective function in Eq. (11) involves the log-likelihoods of the generated responses through the gradients $\\partial_{\\theta} C(\\theta)$'s (cf. Eq. (12)). To avoid a response length bias, we length-normalize the log-likelihoods, akin to the definition of perplexity.\nEstimating the mean preference rewards. We need to estimate the expectations involved in the gradient of our objective function in Eq. (11). The expectations in the numerators can be estimated with the per-mini-batch Monte Carlo averages (Mohamed et al., 2020). However, $C_i(\\theta)$ in the denominator needs careful consideration. Note that: (i) $C_i(\\theta)$ does not involve the gradient, so its estimates can include information from previous mini-batches to reduce variance, and (ii) since the weights are updated during training, $C_i(\\theta)$ is non-stationary. Consequently, we use an exponential moving average estimate for the mean (offset) preference rewards $C_i(\\theta)$ in the denominator."}, {"title": "6 EXPERIMENTAL RESULTS", "content": "To illustrate the versatility of L3Ms, we empirically evaluate them on two criteria: (i) the satisfaction of the imposed custom constraints, and (ii) the task perplexity objective. Note that L3Ms have essentially the same time complexity as traditional approaches: the SFT and alignment stages have been combined into one and the log-barrier parameter is adjusted during the training procedure itself."}, {"title": "6.1 SETUP", "content": "We use LLaMA-7B (Touvron et al., 2023) for all our experiments, as it is a lightweight LLM pre-trained on a large corpus. We are interested in the instruction-following task, for which we use UltraChat (Ding et al., 2023), a large-scale dataset of instructional conversations. We refer to the model trained via the SFT objective (without alignment) as the SFT model. We train LLMs using the min-max approach to find the saddle-point of the Lagrangian, as proposed by Moskovitz et al. (2024); Dai et al. (2024), referring to them as MMs. Lastly, we refer to models trained using our approach as L3Ms. We use the Transformers package (Wolf et al., 2020) for our implementation and run experiments on NVIDIA H100s. Further details on our setup are included in Appendix A.\nIn what follows, we use different preference reward functions and vary the custom constraint require-ments. All results are obtained on a held-out test dataset (not seen during training or validation)."}, {"title": "6.2 LENGTH CONSTRAINED L3MS", "content": "Consider tasks where the lengths of the responses need to be contained in the range $[l_{low}, l_{high}]$ to control its verbosity, in summarization tasks for example (Makino et al., 2019). The natural choice of reward functions in this case are the ones that compute the response length and its negation via $r_1(y|x) = |y|$ and $r_2(y|x) = -|y|$; correspondingly, these rewards are to be controlled with the requirements of $l_{low}$ and $l_{high}$ respectively. Note that these rewards are perfectly anti-correlated.\nIf we naively average the rewards, any unconstrained formulation of alignment (including RLHF) will be ineffective as the loss will always vanish due to the anti-correlation. We could use a weighted"}, {"title": "6.3 HELPFUL AND HARMLESS L3Ms", "content": "Next, we consider the Helpful and Harmless preferences that have been extensively used in the LLM alignment literature (Ji et al., 2023; Wang et al., 2024; Zhou et al., 2024; Guo et al., 2024). Specifically, we utilize the datasets by Bai et al. (2022) to train two preference reward functions respectively. These learned reward functions are negatively correlated (Bai et al., 2022; Dai et al., 2024). Furthermore, note that the numerical outputs of both these reward functions are interpreted as ratings such that a higher numerical value indicates higher helpfulness/harmlessness and vice versa.\nWe train several L3Ms with varying HH constraints. We compare our L3M approach of using log-barriers with the min-max optimization by Moskovitz et al. (2024); Dai et al. (2024) to find the saddle-point of the Lagrangian (MMs). In our experience, learning the Lagrange multipliers in this way is extremely sensitive to the choice of the learning rate. Moreover, to avoid loading an additional reference LLM during the training process, as is done by Moskovitz et al. (2024); Dai et al. (2024), our implementation of MM minimizes the task perplexity directly (as is done by L3Ms as well)."}, {"title": "7 CONCLUSIONS", "content": "In this work, we formulate the SFT and alignment processes for LLMs as one of constrained op-timization: we minimize the task perplexity while simultaneously imposing custom constraints on preferences. This enables customization of the LLM to different preferential properties while main-taining performance on the original task. Consequently, we propose Lagrange Large Language Models (L3Ms) to solve this constrained problem by incorporating the constraints in the objective function using the logarithmic barrier. We include experimental results to illustrate the customization qualities of L3Ms, which can fit to different preferences, providing a personalized user experience."}, {"title": "A EXPERIMENTAL SETUP", "content": "In addition to Section 6.1, here we provide further details of the experimental setup."}, {"title": "A.1 LEARNING PREFERENCE REWARD MODELS", "content": "While some preference reward functions are engineered or rule-based, others are learned. Such preferences can often be difficult to quantify. Alternatively, it is easier to compare responses with respect to the preference, e.g., ranking them from most to least helpful. Consequently, the data for learning preference reward models consist of tuples of the form (x, y+, y), where the prompt x is accompanied by two responses y+ and y-, with the former response being preferred over the latter. The preference reward model is denoted by $r_{\\phi}(\\cdot)$ (parameterized by $\\phi$). Assuming the Bradley-Terry model (Bradley & Terry, 1952), the model's predicted probability for preferring $y^+$ over y is:\n$P_{r_{\\phi}} (Y^+ > y^-|x) = \\sigma(r_{\\phi} (y^+|x) \u2013 r_{\\phi} (y^-|x)),$\nwith the standard logistic function $\\sigma(\\cdot)$. Then, the model minimizes the negative log-likelihood:\n$min \\mathbb{E}_{(x,y^+,y^-)\\sim t(\\cdot)} [-logP_{r_{\\phi}} (y^+ > y^-|x)] .$"}, {"title": "B POLICY GRADIENT", "content": "We are interested in the policy gradient $\\partial C_i (\\theta)$. Note that when taking derivatives with respect to the parameters of a distribution in an expectation, we can use the log-derivative trick:\n$\\mathbb{E}_{x\\sim p_{\\theta}(x)} [f(x)] = \\int dx f (x) \\partial_{\\theta} p_{\\theta}(x) = \\int dx f (x) \\frac{p_{\\theta} (x)}{\\partial_{\\theta} p_{\\theta} (x)}$\n$=\\int dx f (x) p_{\\theta} (x) \\partial_{\\theta} \\log p_{\\theta} (x) = \\mathbb{E}_{x\\sim p_{\\theta}(x)} [f (x) \\partial_{\\theta} \\log p_{\\theta} (x)]$.\nApplying this to the policy gradient $\\partial C_i (\\theta)$ yields,\n$\\partial_{\\theta}C_i (\\theta) = \\partial_{\\theta}\\mathbb{E}_{(x,\\cdot)\\sim p(\\cdot)} [C_i (y|x)] = \\mathbb{E}_{(x,\\cdot)\\sim p(\\cdot)} [c_i (y|x) \\partial_{\\theta} \\log \\pi_{\\theta} (y|x)],$\nwhere $c_i (y|x) = b_i-r_i(y|x)$. This is the simplest form of the policy gradient and can be estimated as Monte Carlo averages. We refer readers to Schulman et al. (2016) for a review of other estimates."}]}