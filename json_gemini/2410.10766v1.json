{"title": "Adaptive Diffusion Terrain Generator for Autonomous Uneven Terrain Navigation", "authors": ["Youwei Yu", "Junhong Xu", "Lantao Liu"], "abstract": "Model-free reinforcement learning has emerged as a powerful method for developing robust robot control policies capable of navigating through complex and unstructured terrains. The effectiveness of these methods hinges on two essential elements: (1) the use of massively parallel physics simulations to expedite policy training, and (2) an environment generator tasked with crafting sufficiently challenging yet attainable terrains to facilitate continuous policy improvement. Existing methods of environment generation often rely on heuristics constrained by a set of parameters, limiting the diversity and realism. In this work, we introduce the Adaptive Diffusion Terrain Generator (ADTG), a novel method that leverages Denoising Diffusion Probabilistic Models to dynamically expand existing training environments by adding more diverse and complex terrains adaptive to the current policy. ADTG guides the diffusion model's generation process through initial noise optimization, blending noise-corrupted terrains from existing training environments weighted by the policy's performance in each corresponding environment. By manipulating the noise corruption level, ADTG seamlessly transitions between generating similar terrains for policy fine-tuning and novel ones to expand training diversity. Our experiments show that the policy trained by ADTG outperforms both procedural generated and natural environments, along with popular navigation methods.", "sections": [{"title": "1 Introduction", "content": "Autonomous navigation across uneven terrains necessitates the development of control policies that exhibit both robustness and smooth interactions within challenging environments [1, 2, 3]. In this work, we specifically target the training of a control policy that allows the mobile-wheeled robots to adeptly navigate through diverse uneven terrains, such as off-road environments characterized by varying elevations, irregular surfaces, and obstacles.\nRecent advancements in reinforcement learning (RL) have shown promise in enhancing autonomous robot navigation on uneven terrains [4, 5, 6]. While an ideal scenario involves training an RL policy to operate seamlessly in all possible environments, the complexity of real-world scenarios makes it impractical to enumerate the entire spectrum of possibilities. Popular methods, including curriculum learning in simulation [7] and imitation learning using real-world collected data [8], encounter limitations in terms of training data diversity and the human efforts required. Without sufficient data and training, the application of learned policies to dissimilar scenarios becomes challenging, thereby hindering efforts to bridge the train-to-real gap. Additionally, existing solutions, such as scalar traversability classification for motion sampling [9, 10] and optimization methods [11, 12], may exhibit fragility due to sensor noise and complex characteristics of vehicle-terrain interactions."}, {"title": "2 Related Work", "content": "Offroad Navigation. Off-road navigation requires planners to handle more than simple planar motions. Simulating full terra-dynamics for complex, deformable surfaces like sand, mud, and snow is computationally intensive. Consequently, most model-based planners use simplified kinematics models for planning over uneven terrains [3, 18, 19, 20, 21] and incorporate semantic cost maps to evaluate traversability not accounted in the simplified model [2, 22]. Imitation learning (IL) methods [8, 23, 24] bypass terrain modeling by learning from expert demonstrations but require labor-intensive data collection. On the other hand, model-free RL does not require expert data and has shown impressive results enabling wheeled [7, 25, 4, 26] and legged robots [13, 5, 27, 28] traversing uneven terrains by training policies over diverse terrain geometries. However, the challenge is to generate realistic environments to bridge the sim-to-real gap. The commonly-used procedural generation methods [5, 13] are limited by parameterization and may not accurately reflect real-world terrain geometries. Our work addresses this by guiding a diffusion model trained on natural terrains to generate suitable terrain surfaces for training RL policies."}, {"title": "3 Preliminaries", "content": null}, {"title": "3.1 Problem Formulation", "content": "We represent the terrain using a grid-based elevation map, denoted as $e \\in \\mathbb{R}^{W \\times H}$, where W and H represent the width and height, respectively. This terrain representation is widely adopted in motion planning across uneven surfaces. Similar to most works in training RL policies for rough terrain navigation [13, 5], we use existing high-performance physics simulators [49] to model the state transitions of the robot moving on uneven terrains $s_{t+1} \\sim p(s_{t+1}|s_t, a_t, e)$. Here, $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$ represent the robot's state and action, and each realization of the elevation (i.e., terrain) e specifies a unique environment. An optimal policy $\\pi(a|s, e; \\theta)$ can be found by maximizing the expected cumulative discounted reward. Formally,\n$\\theta^* = \\arg \\max_{\\theta} \\mathbb{E}_{e\\sim p(e), s_0 \\sim p(s_0), a_t \\sim \\pi(a_t|s_t,e), s_{t+1} \\sim p(s_{t+1}|s_t, a_t,e)} [\\sum_{t=0}^T \\gamma^tR(s_t, a_t)],$ (1)\nwhere $p(s_0)$ is the initial state distribution and p(e) denotes the distribution over the environments. Due to the elevation e imposing constraints on the robot's movement, the policy optimized through Eq. (1) is inherently capable of avoiding hazards on elevated terrains. We aim to dynamically evolve the environment distribution p(e) based on the policy's performance, ensuring training efficiency and generating realistic terrain elevations. While constructing a realistic state transition $p(s_{t+1}|s_t, a_t, e)$ is also important for reducing the sim-to-real gap, we leave it to our future work."}, {"title": "3.2 Adaptive Curriculum Learning for Terrain-Aware Policy Optimization", "content": "A theoretically correct but impractical solution to Eq.(1) is to train on all possible terrains $\\Lambda = (e^1, ..., e^n)$, with p(e) as a uniform distribution over $\\Lambda$. However, the vast variability of terrain"}, {"title": "4 Adaptive Diffusion Terrain Generator", "content": "This section introduces the Adaptive Diffusion Terrain Generator (ADTG), a novel ACRL generator that manipulates the DDPM process based on current policy performance and dataset diversity. We begin by interpolating between \"easy\" and \"difficult\" terrains in the DDPM latent space to generate terrains that optimize policy training. Next, we modulate the initial noise input based on the training dataset's variance to enrich terrain diversity, fostering broader experiences and improving the policy's generalization across unseen terrains. We use e, eo, and ek to denote the environment in the training dataset, the generated terrain through DDPM, and the DDPM's latent variable at timestep k, respectively. All three variables are the same size $\\mathbb{R}^{W \\times H}$. Since in DDPM, noises and latent variables are the same [53], we use them interchangeably."}, {"title": "4.1 Performance-Guided Generation via DDPM", "content": "Latent Variable Synthesis for Controllable Generation. Once trained, DDPM can control sample generation by manipulating intermediate latent variables. In our context, the goal is to steer the generated terrain surface to maximize policy improvement after being trained on it. While there are numerous methods to guide the diffusion model [40, 44], we choose to optimize the starting noise to control the final target [46]. This approach is both simple and effective, as it eliminates the need for perturbations across all reverse diffusion steps, as required in classifier-free guidance [40], or fine-tuning of diffusion models [44]. Nevertheless, it still enhances the probability of sampling informative terrains tailored to the current policy.\nConsider a subset of terrain elevations $\\bar{\\Lambda} = (e^1, e^2, ..., e^n)$ from the dataset $\\Lambda$. To find an initial noise that generates a terrain maximizing the policy improvement, we first generate intermediate latent variables (noises) for each training environment in $\\bar{\\Lambda}$ at a forward diffusion time step k, $e_k^i \\sim q(e_k | e^i, k)$ for i = 1,...,n. Assume that we have a weighting function w(ei, \u03c0) that evaluates the performance improvement after training on each terrain map ei. We propose to find the optimized initial noise as a weighted interpolation of these latents, where the contribution of each latent $e_k^i$ is given by the policy improvement in the original terrain environment w(ei, \u03c0).\n$e_k = [\\sum_{i=1}^nw(e^i, \\pi) e_k^i] / [\\sum_{m=1}^nw(e^m, \\pi)].$ (2)\nThe fused latent variable $e_k$ is then processed through reverse diffusion, starting at time k to synthesize a new terrain $e_o$. The resulting terrain blends the high-level characteristics captured by the latent features of original terrains, proportionally influenced by their weights. We illustrate this blending effect in Fig. 1. By controlling weights $w_i$, we can steer the difficulty of the synthesized terrain.\nWeighting Function. Policy training requires dynamic weight assignment based on current policy performance. We define the following weighting function that penalizes terrains that are too easy or too difficult for the policy:\nw (e, \u03c0) = exp {r(e, \u03c0)}, r(e, \u03c0) = \u2212(s(e, \u03c0) \u2212 \u015d)^2/\u03c3^2. (3)\nIt penalizes the deviation of terrain difficulty, s(e, \u03c0), experienced by the policy \u03c0 from a desired difficulty level \u015d. This desired level indicates a terrain difficulty that promotes the most significant"}, {"title": "4.2 Diversifying Training Dataset via Modulating Initial Noise", "content": "The preceding section describes how policy performance guides DDPM in generating terrains that challenge the current policy's capabilities. As training progresses, the pool of challenging terrains diminishes, leading to a point where each terrain no longer provides significant improvement for the policy. Simply fusing these less challenging terrains does not create more complex scenarios. Without enhancing terrain diversity, the potential for policy improvement plateaus. To overcome this, it is essential to shift the focus of terrain generation towards increasing diversity. DDPM's reverse process generally starts from a pre-defined forward step, where the latent variable is usually pure Gaussian noise. However, it can also start from any forward step K with sampled noise as $e_k \\sim q(e_k | e_o)$ [59]. Fig. 1 shows the variance of generated terrains decreases with fewer forward steps and vice versa. To enrich our training dataset's diversity, we propose the following:\n1. Variability Assessment: Compute the dataset's variability $A_{var}$ by analyzing the variance of the first few principal components from a Principal Component Analysis (PCA) on each elevation map. This serves as an efficient proxy for variability.\n2. Forward Step Selection: The forward step $k \\propto A_{var}^{-1}$ is inversely proportional to the variance. We use a linear scheduler: $k = K(1 - A_{var})$, with K the maximum forward step and $A_{var}$ normalized to 0 ~ 1. This inverse relationship ensures greater diversity in generated terrains.\n3. Terrain Generation: Using the selected forward step k, apply our proposed Synthesize to generate new terrains, thus expanding variability for training environments."}, {"title": "4.3 ACRL with ADTG", "content": "We present the final method pseudo-coded in Alg. 1 using the proposed ADTG for training a privileged policy. The algorithm iterates over policy optimization and guided terrain generation, co-evolving the policy and terrain dataset until convergence. The algorithm starts by selecting a training environment that provides the best training signal for the current policy, which can be done in various ways [50]. For example, one can compute scores for terrains based on the weighting function in Eq. (3) and choose the one with the maximum weight. Instead of choosing deterministically, we sample the terrain based on their corresponding weights. The Optim step collects trajectories and performs one policy update in the selected terrain. After the update, we evolve the current dataset by generating a new one, as shown in lines 4 - 6 of Alg. 1. In practice, we run Alg. 1 in parallel across N terrains, each with multiple robots. In parallel training, Synthesize begins by sampling N \u00d7 n initial noises, where N is the number of new terrains (equal to the number of parallel environments) and n is the sample size in Eq. (2). It then optimizes over these noises to generate N optimized noises. Finally, these optimized noises are passed to the DDPM to generate N terrains. When the dataset grows large,"}, {"title": "5 Experiments", "content": "In this section, we start with the algorithmic evaluation to highlight the effectiveness of our ADTG. Then through sim-to-sim on two kinds of wheeled robot platforms and sim-to-real on one wheeled and one quadruped robot, we study the performance of our ADTG policy against ablations and competing methods.\nWe train in IsaacGym [49] and parallel 100 uneven terrains, each with 100 robots. Among the elevation dataset [62] as detailed in Appendix C.1, we select 3000 for DDPM training (E-3K), 100 for algorithmic evaluation (E-1H), and 30 for sim-to-sim experiments (E-30). Simulations run on an Intel i9-14900KF CPU and NVIDIA RTX 4090 GPU. Real-world tests use an NVIDIA Jetson Orin."}, {"title": "5.1 Algorithmic Performance Evaluation", "content": "This section evaluates whether the environment curriculum generated by ADTG enhances the generalization capability of the trained privileged policy across unfamiliar terrain geometries, on the wheeled ClearPath Jackal robot. We compare with the following baselines. Procedural Generation Curriculum (PGC), a commonly used method, uses heuristically designed terrain parameters [13]. Our implemented PGC follows ADTG, adapting the terrain via the score function Eq. (3) and dynamically updating the dataset. First, to ablate our Adaptive curriculum, Diffusion Terrain Generator (ADTG) generates terrain without curriculum. Procedural Generation (PGC) randomly samples parameters. To ablate our Diffusion Generator, Natural Adaptive Terrain (N-ADTG) selects terrains directly from E-3K. To ablate both, Natural Terrain (N-ADTG) randomly samples from E-3K without curriculum. Mono font means the ablated parts. Second, to test ADTG's robustness to initialization other than diffusion, N-ADTG and P-ADTG start with datasets sub-sampled from E-3K and procedural generations.\nAll methods use the same training and evaluation setup. After each training epoch, policies are tested in held-out evaluation environments with 200000 start-goal pairs. Fig. 3 shows the normalized RL return\u00b9 . ADTG outperforms PGC and N-AT within 40 epochs, exceeding the return by over 0.1,"}, {"title": "5.2 Zero-shot Sim-to-Sim and Sim-to-Real Experiments", "content": "This section evaluates the zero-shot transfer capability of sim-to-deploy environments. Metrics include the success rate, trajectory ratio, orientation vibration |\u03c9|, orientation jerk $|\\frac{d^2 \\omega}{dt^2}|$, and position jerk $|\\frac{d^2 a}{dt^2}|$, where \u03c9 and a denote angular velocity and linear acceleration. These motion stability indicators are crucial in mitigating sudden pose changes and enhancing overall safety. The trajectory ratio is the successful path length relative to straight-line distance and indicates navigator efficiency. All baselines use the elevation map [63] with depth camera and identify terrains as obstacles if the slope estimated from the elevation map exceeds 20\u00b0. For orientation costs, we obtain the robot's roll and pitch by projecting its base to the elevation map.\nBaselines. Falco [16], a classic motion primitives planner, and MPPI [10, 64], a sampling-based model predictive controller, are recognized for the success rate and efficiency. They use the point-cloud and elevation map to weigh collision risk and orientation penalty. TERP [4], an RL policy trained in simulation, conditions on the elevation map, rewarding motion stability and penalizing steep slopes. POVNav [17] performs Pareto-optimal navigation by identifying sub-goals in segmented images [65], excelling in unstructured outdoor environments.\nSimulation Experiment. We simulate wheeled robots, ClearPath Jackal and Husky, in ROS Gazebo on 30 diverse environments (E-30), equipped with a RealSense D435i camera. We add Gaussian noises to the robot state, depth measurement, and vehicle control to introduce uncertainty. 1000 start and goal pairs are sampled for each environment. We do not include ablations other than N-AT because of poor algorithmic performance. As Jackal's results shown in Table 1, our method outperforms the baselines. Appendix C.2 provides statistical results for Husky. While all methods show improved performance due to the Husky's better navigability on uneven terrains, our method consistently outperformed baseline methods. The depth measurement noise poses a substantial challenge in accurately modeling obstacles and complex terrains. Falco and MPPI often cause the robot to get stuck or topple over, and TERP often predicts erratic waypoints that either violate safety on elevation map or are overly conservative. Learning-based TERP and POVN lack generalizability, with their performance varying across different environments. This issue is mirrored in N-AT and PGC, highlighting the success of adaptive curriculum and realistic terrain generation properties of ADTG."}, {"title": "6 Conclusion, Limitations and Future Directions", "content": "We propose an Adaptive Diffusion Terrain Generator (ADTG) to create realistic and diverse terrains based on evolving policy performance, enhancing RL policy's generalization and learning efficiency. To guide the diffusion model generation process, we propose optimizing the initial noises based on the potential improvements of the policy after being trained on the environment denoised from this initial noise. Algorithmic performance shows ADTG's superiority in generating challenging but suitable environments over established methods such as commonly used procedural generation curriculum. Combined with domain randomization in a teacher-student framework, it trains a robust deployment policy for zero-shot transfer to new, unseen terrains. Extensive sim-to-deploy tests with wheeled and quadruped robots validate our approach against SOTA planning methods.\nLimitations and Future Directions: A key limitation of ADTG is that it evolves only the environment distribution, relying on physics simulators for state transitions, limiting deployment in the complex real world. While domain randomization helps, it's not a full solution. Future work will integrate environment distribution and physics to bridge the sim-to-real gap. Additionally, ADTG's environment scale is suited for local planning, but larger environments are needed for long-horizon tasks. We plan to explore hierarchical diffusion models for generating multi-layered environments."}, {"title": "Appendix", "content": null}, {"title": "A Initial Noise Optimization by Control as Inference", "content": "In this section, we justify our algorithm design through the lens of control-as-inference [1, 2] to optimize the initial noise of diffusion models."}, {"title": "A.1 Denoising Diffusion Probabilistic Models Preliminary", "content": "Denoising Diffusion Probabilistic Models (DDPMs) [5] are generative models that generate original data from Gaussian noise through a series of forward and reverse steps. In the forward process, Gaussian noise is gradually added to the original terrain sample $e_o$ until a final step K, with the terrain at each step k distributed as $q (e_k|e_{k-1}) = \\mathcal{N} (e_k|\\sqrt{1 - \\beta_k}e_{k-1}, \\beta_kI)$. We use cosine noise schedules [6] $(\\beta_1, ..., \\beta_K)$. The reverse process gradually reduces the noise from $e_K$ to generate the original data $e_o$ using a noise predictor $\\epsilon(e_k, k; \\phi)$. The training loss function for the noise predictor is $L(\\phi) = MSE(\\epsilon_k, \\epsilon(e_o + \\epsilon_k, k; \\phi))$."}, {"title": "A.2 DDPM Initial Noise Optimization", "content": "Let $p(e_{k-1} | e_k; \\theta)$ denote single reverse diffusion step, with \u03b8 the diffusion model's parameters. Based on [3], we can formulate iterative denoising of the DDPM as a Markov Decision Process (MDP), where the state evolution (policy and transition function combined) is the reverse diffusion itself $p(s_{k+1} | s_k, a_k) = p(e_{k-1} | e_k; \\theta)$, and the reward function is 0 unless at the initial step k = 0:\nR(e_k, \\pi) = \\begin{cases} r(e_0, \\pi) & \\text{if } k = 0 \\\\ 0 & \\text{otherwise} \\end{cases} (4)\nIn our work, r(eo, \u03c0) evaluates the improvements of the current policy \u03c0 if trained on the generated environment eo. The return of the noise ek starting at timestep k as\n\\mathcal{J}(e_k) = \\mathbb{E} \\Big [ r(e_0, \\pi) | e_{t-1} \\sim p(e_{t-1}|e_t;\\theta) \\Big ], (5)\nwhere t \u2208 [1, k]. Our goal is to find the noise that maximizes the Eq. (5), $e_k^* = \\arg \\max_{e_k} \\mathcal{J}(e_k)$. Gradient-based methods using differentiable rewards [4] are challenging to apply in our setup, as computing the gradient of policy improvement with respect to the initial noise is impractical. This is because evaluating policy improvement requires simulating the policy on each generated terrain to compute success rates. Instead, we frame this as a control-as-inference problem, using approximate sampling to optimize the initial noise for Eq. (5). Following the KL control theory derivation [1], we aim to find a distribution of the starting noise $\\hat{q}(e_k)$ that optimizes Eq. (5) while remaining close to a reference noise distribution $q(e_k)$. This is achieved by adding an extra KL cost to the return in Eq. (5) and taking expectation with respect to the random initial noise\n\\begin{aligned}\n\\hat{\\mathcal{J}}(e_k) &= \\mathbb{E} \\Big [ \\mathcal{J}(e_k) - \\log \\frac{\\hat{q}(e_k)}{q(e_k)} \\Big | e_k \\sim \\hat{q}(e_k) \\Big ] \\\\\n&= \\int \\hat{q}(e_k) \\Big ( \\mathcal{J}(e_k) - \\log \\frac{\\hat{q}(e_k)}{q(e_k)} \\Big ) de_k \\\\\n&= \\int \\hat{q}(e_k) \\Big ( \\log \\big ( \\exp \\{ \\mathcal{J}(e_k) \\} \\big ) - \\log \\frac{\\hat{q}(e_k)}{q(e_k)} \\Big ) de_k \\\\\n&= - \\int \\hat{q}(e_k) \\log \\frac{\\hat{q}(e_k)}{q(e_k) \\exp \\{ \\mathcal{J}(e_k) \\}} de_k \\\\\n&= - KL \\Big ( \\hat{q}(e_k) || \\psi(e_k) \\Big ),\n\\end{aligned} (6)\nwhere $\\psi(e_k) \\propto q(e_k) \\exp \\{ \\mathcal{J}(e_k) \\}$ is an unnormalized distribution with high density at the high return region. The Kullback-Leibler (KL) divergence is non-negative, implying that the optimal return is achieved when $\\hat{q} = \\psi$. Consequently, sampling from \u03c8 is equivalent to sampling from the optimal noise distribution. While there are multiple methods to sample from \u03c8, we employ importance"}, {"title": "A.3 Environment Difficulty Manipulated by Diffusion Synthesis", "content": "The core assumption underlying our method is that the success rates of the generated terrains by ADTG are consistent with the weighted combination of the success rates of selected terrains. This appendix provides empirical evidence to support this assumption. During training, we sample terrains with the success rate outside the 0.6 to 0.85 range for Synthesize. As our dataset expands rapidly, we uniformly sub-sample up to 1000 terrains as the current (sub-)dataset. If the synthesized terrain's predicted success rate falls outside the 0.6 to 0.85 range, we adjust by adding suitable samples while ensuring the total number under 16. We choose 16 based on the GPU memory"}, {"title": "B System Design", "content": null}, {"title": "B.1 Privileged Policy", "content": "The privileged (teacher) policy is trained using Proximal Policy Optimization (PPO) [7] for goal-oriented navigation on uneven terrains, minimizing motion vibration and jerk. The design is as following:\nState, Observation, and Action. The state $s_t = [q_t, v_t, w_t, d_t, a_{t-1}]$ at timestamp t is ground-truth motion information, including the orientation in quaternion $q_t \\in \\mathbb{R}^4$, linear velocity $v_t \\in \\mathbb{R}^3$, angular velocity $w_t \\in \\mathbb{R}^3$, relative goal distance $d_t \\in \\mathbb{R}^2$, and previous action $a_{t-1} \\in \\mathbb{R}^2$. The reference frame is anchored to the robot base. Normalization is applied to linear and angular velocities. We refrain from normalizing the relative goal distance, as this allows the policy to adapt to varying goal range scales. The observation has full access to the elevation map $o_{tt} \\in \\mathbb{R}^{128 \\times 128}$. Each observation remains constant during an ACRL episode when ADTG does not evolve terrains. The action $a = [\\alpha_{vv^\\intercal}, \\alpha_{\\omega \\omega^\\intercal}]$ applied to the robot represents proportional-derivative (PD) targets for forward linear and yaw angular velocities, where $a_f = [v^\\intercal, w^\\intercal]$ is the network output, and $[\\alpha_v, \\alpha_\\omega]$ are coefficients for the vehicle drive system.\nRewards. The total reward is structured as a sum of the weighted components: Goal Proximity $c_1 1_{\\delta pos \\leq \\delta_{pos}}(s_t)$ assesses the robot's closeness to the goal against thresholds $\\delta_{pos}$ and $\\delta_{rot}$, Orientation Regulation $c_2 1_{\\delta \\Theta > \\pi/12}(E(q_t))$ imposes penalties for exceeding safe roll and pitch angles, Movement Consistency $c_3 ||a_t - a_{t-1}||^2 + c_4 ||\\dot{a}_t - \\dot{a}_{t-1}||^2$ incentives consistent and gradual actions, Ground-contact and Safety $c_5 \\text{dim}(f = 0) + c_6 \\text{dim}(f > \\delta_F)$ discourages situations where the contact force mean ground contact loss or a collision risk. The total reward is parameterized as follows:\nr = 5.0 \u00d7 1_{ \\delta pos < 0.25}(s_t) + 0.001 \u00d7 1_{\\delta rot< \\pi/3}(s_t) \\\\\n- 0.01 \u00d7 1_{\\delta \\Theta > \\pi/12}(q_t) \\\\\n- 0.01 \u00d7 ||a_t - a_{t-1}||^2 - 0.0001 \u00d7 ||\\dot{a}_t - \\dot{a}_{t-1}||^2 \\\\\n- 0.01 \u00d7 \\text{dim}(f > 100) - 0.001 \u00d7 \\text{dim}(f < 0.00001). (10)"}, {"title": "Start and Termination.", "content": "Episodes start with robots in random poses and assigned navigation targets. The initial state $s_0$ is sampled by position, velocity, and yaw, with other parameters managed by the physics engine. The state z considers terrain geometry to avoid immediate failure. Initially, goals are within a circular sector, with a radius of $d_{max}$ = [0.5, 3.0]m and a central angle of [-\u03c0/3, \u03c0/3]rad. As the policy progresses to success rate of 60%, the radius and angle incrementally increase by 0.5m and \u03c0/18 until a semicircle with a radius of 6m. An episode ends upon reaching the goal or triggering safety or running length constraints. The maximum episode length is $d_{max}/\\Delta t/v_{avg}$, with $v_{avg}$ = 0.3 m/s the minimum average velocity and $\\Delta t$ = 0.005 s the simulator timestep. Early termination occurs if the roll or pitch angle exceeds $S_{quat}$ = \u03c0/9, or if the robot collides with the environment, or if two or more wheels are off the ground, risking toppling or entrapment."}, {"title": "B.2 Deployment Policy", "content": "The deployment (student) policy $\\hat{\\pi}$ is trained via Dataset Aggregation (DAgger) [8] to minimize the mean squared error to match the teacher's actions. At each timestamp t, the deployment policy observes $O_t = (S_t, I_t)$, where $S_t$ is the noisy state and $I_t$ is a depth image. The depth sensor captures calibrated depth images with [640 \u00d7 480] resolution and 87\u00b0 horizontal field-of-view, which mirror our sim-to-deploy experimental settings. Due to partial observability, the policy considers past information to decide the next action $a_t \\sim \\hat{\\pi}(a_t | a_{t}, o_{t}; \\theta)$, where $a_{t}$ and $o_{t}$ are action and observation histories, H is the maximum history length. To enhance generalization, we integrate physical domain randomization and perception domain randomization as following:\nPhysics Domain Randomization. An environment appears as geometry and is characterized by physics $\\epsilon_p = \\{ \\epsilon_{pv}, \\epsilon_{pg}; \\epsilon_{pm}, \\epsilon_{pf}, \\epsilon_{pa} \\} \\in \\mathbb{R}^{10}$, which is separated into environment properties and robot-environment interactions. $\\epsilon_{pv}$: Dynamic friction, static friction, and restitution coefficients, affecting slipperiness; $\\epsilon_{pg}$ : Gravity; $\\epsilon_{pm}$ : Mass, simulating extra burdens or flat tires; $\\epsilon_{pf}$ : External forces; $\\epsilon_{pa}$: Discrepancies in actuator setpoints.\nPerception Domain Randomization. Since velocity relies on the inertial measurement unit (IMU), which is typically noisy [14], we add Gaussian noise to simulate this: $\\tilde{v} \\sim \\mathcal{N}(v, \\sigma^2)$ for linear velocity and $\\tilde{\\omega} \\sim \\mathcal{N}(\\omega, \\sigma^2)$ for angular velocity, where v and w are ground-truth in the simulator. Based on the empirical study [15], depth cameras suffer from precision and lateral noise, as well as invalid values at full sensor resolution (nan ratio). We model precision noise as $\\mathcal{N} (0, p_0+p_1d+p_2d^2)$ and lateral noise as $\\mathcal{N}(0, l_0 + l_1 \u00b7 \\theta / (\\pi/2 - \\theta))$, where d is the measured depth of pixel (u, v) and \u03b8 is its azimuth. The nan ratio is modeled by a uniform distribution $\\mathcal{U}(0, d_{nan})$"}, {"title": "C Sim-to-Deploy Experiments", "content": "During training, the IsaacGym simulator runs at 200 Hz, and the policy runs at 50 Hz. In simulation experiments, ROS Gazebo provides odometry at 1000 Hz. In real-world experiments with Jackal, Faster-LIO [13] with IMU preintegration [14] fuses Lidar and IMU for 200 Hz odometry. Both settings use a depth camera RealSense D435i with [640 \u00d7 480] resolution at 30 fps, synchronized to odometry using ROS approximate time."}, {"title": "C.1 Simulation Training Setups", "content": "Uneven Terrain Datasets. To generate datasets, we utilize a digital elevation model (DEM) represented as a raster obtained from a real-world high-resolution topography dataset 2. The entire map is seamlessly divided into tiles of size [128 \u00d7 128] with resolution 0.1 m. 3000 training data for DDPM and 100 evaluation data for algorithmic performance are randomly selected within a specific geographical range, defined by latitude and longitude intervals: [33.5874, -116.0058], [33.5874, -115.9991], [33.5929, -116.0058], [33.5929, -115.9991]. For sim-to-sim deployment experiment in ROS Gazebo, we get 30 terrains, each of size [320 \u00d7 320] with resolu-"}, {"title": "C.2 Sim-to-Sim Experiment", "content": "In addition to benchmarking on the ClearPath Jackal robot, we train the ClearPath Husky robot using the same settings as the Jackal and tested in the same ROS Gazebo environments E-30 detailed in Sec. C.1. We evaluated across 30 environments, each containing 1000 pre-sampled start-goal pairs, resulting in total 30 000 trials. The results in Table 5 demonstrate that our method generalizes effectively across different wheeled platforms, maintaining an advantage over other methods in terms of success rate, orientation vibration, and position jerk. All methods performed better overall with the Husky, due to its better navigability on uneven terrains than Jackal. However, the performance differences between our method, Natural Adaptive Terrain (N-AT), and Procedural Generation Curriculum (PGC) relative to other methods mirror those observed in the Jackal experiment, with challenges such as perception noise and erratic prediction persisting for competing methods. In summary, our method generalizes well across different wheeled platforms and maintains a performance advantage over baseline methods."}, {"title": "C.3 Sim-to-Real Experiment", "content": "Wheeled Robot Navigation. In the Jackal real-world experiment, we ablate the physics (w/o Physics) and perception (w/o Percept) domain randomization (DR) to study their contributions, shown in Table 6 and Table 7. We observed varying contributions of physics and perception domain randomization. First, in forest, mud, and gravel, removing either domain decreased the success rate, but these ablations still performed better than baselines due to ADTG. In arid and rust, the perception domain was crucial because of increased perception noise. Second, we identified drawbacks in the perception domain, particularly in dunes where the camera depth quality heavily depended on exposure, which is difficult to model. Last, without the physics domain, the robot showed improved orientation smoothness but struggled to make reactive behaviors, since the learned arc movements in slippery areas benefited from this domain. Additionally, for computational"}]}