{"title": "SSL-TTS: Leveraging Self-Supervised Embeddings\nand KNN Retrieval for Zero-Shot Multi-speaker TTS", "authors": ["Karl El Hajal", "Ajinkya Kulkarni", "Enno Hermann", "Mathew Magimai.-Doss"], "abstract": "While recent zero-shot multispeaker text-to-speech\n(TTS) models achieve impressive results, they typically rely on ex-\ntensive transcribed speech datasets from numerous speakers and\nintricate training pipelines. Meanwhile, self-supervised learning\n(SSL) speech features have emerged as effective intermediate\nrepresentations for TTS. It was also observed that SSL features\nfrom different speakers that are linearly close share phonetic\ninformation while maintaining individual speaker identity, which\nenables straight-forward and robust voice cloning. In this study,\nwe introduce SSL-TTS, a lightweight and efficient zero-shot TTS\nframework trained on transcribed speech from a single speaker.\nSSL-TTS leverages SSL features and retrieval methods for simple\nand robust zero-shot multi-speaker synthesis. Objective and sub-\njective evaluations show that our approach achieves performance\ncomparable to state-of-the-art models that require significantly\nlarger training datasets. The low training data requirements\nmean that SSL-TTS is well suited for the development of multi-\nspeaker TTS systems for low-resource domains and languages.\nWe also introduce an interpolation parameter which enables fine\ncontrol over the output speech by blending voices. Demo samples\nare available at https://idiap.github.io/ssl-tts/.", "sections": [{"title": "I. INTRODUCTION", "content": "EURAL text-to-speech (TTS) synthesis has advanced\nsignificantly in recent years, achieving a level of nat-\nuralness comparable to human speech and allowing for an\nincreasingly expressive range of outputs [1], [2], [3], [4].\nNeural TTS systems can be categorized into two-stage and\nsingle-stage pipelines. Two-stage models convert phonemic\nfeatures into acoustic features and then use a vocoder to\ngenerate waveforms. These models can suffer from error\npropagation and limitations due to their dependence on low-\nlevel features like mel-spectrograms [5], [6], [7]. Single-stage\nmodels aim to address these issues by streamlining this process\ninto an end-to-end framework [2], [8], [9], but they may face\noversmoothing, mispronunciations, and reduced flexibility due\nto the lack of explicit linguistic information and entangled\nlatent representations [9], [10]. Recent research combines the\nstrengths of both approaches by using self-supervised learning\n(SSL) speech representations as intermediate elements in two-\nstage models [10], [11], [12], [13]. These representations help\nimprove word error rates, pronunciation of out-of-vocabulary\nwords [11], and robustness to noise [14], leading to high-\nquality, natural speech often surpassing end-to-end models.\nIn practice, end-user applications may need multiple voices.\nCollecting high quality speech data and building a TTS model\nfor each target speaker/voice is a challenging problem. So,\nthere has been a growing interest in zero-shot multi-speaker\nTTS systems which can synthesize speech in an unseen\nspeaker's voice based on short reference samples. State-of-\nthe-art models such as XTTS [15] and HierSpeech++ [16]\ndemonstrate impressive quality and similarity to unseen speak-\ners. However, these models require end-to-end training on\nthousands of hours of transcribed audio data from a large\nnumber of speakers to generalize effectively.\nSimultaneously, kNN-VC [17] has emerged as a promising\nany-to-any voice conversion method, leveraging SSL features\nfor zero-shot conversion. It uses a kNN algorithm to match\nframes from the source speaker with the target speaker's\nrepresentations, adjusting the speaker identity while preserving\nspeech content. This approach is similar to retrieval-augmented\ngeneration (RAG) techniques used in deep generative models\nsuch as language models [18] and image generators [19].\nThese methods can enhance accuracy, reliability, and enable\nstyle transfer by steering model outputs to mirror characteris-\ntics of a retrieval database [19], [20].\nBuilding on these insights, we propose SSL-TTS, a\nlightweight framework for multi-speaker zero-shot TTS that\nleverages SSL features encapsulating both speaker and lin-\nguistic information. In the first stage of the framework, text is\nconverted to SSL embeddings, which are then matched with\ncorresponding features from a target speaker by exploiting\ntheir linear relationship. Similar to kNN-VC, this modifies\nthe target voice in a non-parametric manner and obviates\nthe need for multi-speaker transcribed data for training. A\nwaveform is finally generated from the converted features\nusing a pre-trained vocoder. The only component that requires\ntraining is the Text-to-SSL model, which can be trained on\ntranscribed data from a single speaker only. Furthermore, we\nintroduce a linear interpolation parameter allowing for fine-\ngrained control over the influence of the target style on the\noutput. We validate the approach by implementing the Text-\nto-SSL block using two different lightweight models, namely\nGlowTTS [5] and GradTTS [21]. We train them on transcribed\nspeech from a single speaker, and compare them with state-of-\nthe-art zero-shot multi-speaker models using objective metrics\nand subjective listening tests. The code and trained models will\nbe made public upon publication, demo samples are available\nat https://idiap.github.io/ssl-tts/.\nThis paper is organized as follows. The next section intro-"}, {"title": "II. PROPOSED APPROACH", "content": "The SSL-TTS framework, illustrated in Fig. 1, begins with\na Text-to-SSL model that generates source speaker features\nfrom text input. A kNN retrieval algorithm then matches these\ngenerated features to units in a target speaker's unit database,\nwhich contains features pre-extracted from the target speaker's\nrecordings using a pre-trained, general-purpose SSL encoder.\nThe selected target speaker features are linearly interpolated\nwith the source speaker features to obtain the converted\nfeatures. Finally, these converted features are decoded back\ninto a waveform using a pre-trained vocoder.\nSSL encoder: For this framework, we need an intermediate\naudio representation that meets the following criteria: (1) the\nfeatures should encompass both linguistic and speaker-specific\ninformation; (2) features that are linearly close should exhibit\nsimilar phonetic properties while preserving speaker identity;\nand (3) it should be possible to decode the features back to\nwaveform without loss of information. Recent observations in-\ndicate that self-supervised models encode speech into general\nrepresentations that meet these criteria [22]. Therefore, such\nSSL representations are suitable for this framework.\nText-to-SSL: We train a Text-to-SSL model that generates\ncorresponding SSL features from a given text input. Notably,\nthis is the only component of our framework that requires\naudio data paired with text transcriptions for training. It is\nsufficient to train this model on the speech of a single speaker.\nKNN: To synthesize speech in a target speaker's voice,\nunits (or frames) from the target speaker unit database are\nselected to replace corresponding frames from the source\nspeaker features. The selection is performed by comparing\nsource and target frames in terms of a linear distance metric.\nThis process results in selected target speaker features which\nmaintain the phonetic information while replacing the voice\ncharacteristics with those of the target speaker.\nThe source and target speaker features are then linearly\ninterpolated with each other to obtain the converted fea-\ntures [18]. We use a variable parameter A which can be\nmodified to change the degree of influence the target features\nhave on the output. This allows for creating a blend of the\nsource and target styles.\n$Y_{\\text{converted}} = \\lambda Y_{\\text{selected}} + (1 - \\lambda) Y_{\\text{source}}$\nVocoder: We employ a vocoder capable of decoding the\nSSL features back into a waveform. To ensure robust gener-\nalization, the vocoder should be pre-trained on a large and\ndiverse dataset to maintain high-quality waveform reconstruc-\ntion across different speakers and contexts."}, {"title": "III. EXPERIMENTAL SETUP", "content": "SSL encoder: We employ a pre-trained WavLM-Large\nencoder from [23] to derive representations from speech utter-\nances. WavLM-Large fits the requirements we presented, and\nis specifically selected due to its effective audio reconstruc-\ntion capabilities, obtained through training on masked speech\ndenoising and prediction tasks [24]. Specifically, we utilize\nfeatures extracted from the 6th layer of the model, which\nencapsulate both phonetic and speaker characteristics [17],\n[24]. Each utterance yields T \u00d7 1024 features, where T is the\nsequence length. Each frame represents a 25ms window with\na 20ms hop between frames. These features are pre-extracted\nand cached prior to training or inference, eliminating the\nneed to load the WavLM model during training or synthesis,\nassuming the target speaker is known.\nText-to-SSL: We evaluate two Text-to-SSL implementa-\ntions: GlowTTS [5] and GradTTS [21]. GlowTTS employs a\nnon-autoregressive architecture with a transformer-based text\nencoder, a duration predictor, and a flow-based decoder [25].\nGradTTS follows a similar architecture but uses a diffusion-\nbased decoder [26]. These models, which were originally\ndesigned to predict mel-spectrograms, are trained to encode\ntext inputs into WavLM features corresponding to a single\nspeaker's speech. We maintain the models' default configura-\ntions and only adjust their output dimension to 1024 channels\nto align with WavLM-Large features. For the GradTTS diffu-\nsion decoder, we use 100 iterations for synthesis. Both models\nare trained on the LJSpeech dataset [27], which comprises\n24 hours of single-speaker English speech. GlowTTS is trained\nfor 650k steps, and GradTTS for 2M steps.\nKNN: For target speaker feature retrieval, we perform a\nkNN algorithm similar to [17]. For each source frame, we\ncompute its cosine distance with every target speaker frame\nwithin the unit database. We then select the k closest units,\nand average them with uniform weighting. We use k = 4\nwhich was determined to be suitable across different amounts\nof target audio [17].\nVocoder: We use a pre-trained HiFi-GAN V1 [28] model\ntrained to reconstruct 16kHz waveforms from WavLM-Large\nlayer 6 features. The model checkpoint, sourced from [17], was\ntrained on LibriSpeech train-clean-100 set which encompasses\n100 hours of clean English speech, with 251 speakers, and\n25 minutes of data per speaker. The prematched paradigm is\nemployed, where the training set is reconstructed by selecting\nk-nearest neighbors for each utterance and training the vocoder\nto predict original waveforms from these prematched features."}, {"title": "B. Baselines", "content": "We compare our models with the best performing open\nmodels for zero-shot multi-speaker TTS.\nYourTTS [8] builds on VITS [2], adding elements for\nmultilingual training and zero-shot multi-speaker capabilities.\nIt uses the H/ASP speaker encoder [29], pre-trained on the\nVoxCeleb2 dataset [30], to extract a speaker embedding from\nreference utterances. This embedding conditions the model's\nduration predictor, flow-based decoder, posterior encoder, and\nvocoder. YourTTS is trained end-to-end on 529 hours of\nmultilingual transcribed data from over 1000 speakers.\nXTTS [15] features a Vector Quantised-Variational AutoEn-\ncoder (VQ-VAE) that encodes mel-spectrograms into discrete\ncodes, a GPT-2 encoder that predicts these audio codes from\ntext tokens, and a HiFi-GAN-based decoder. The GPT-2 en-\ncoder is conditioned on speaker information using a Perceiver\nconditioner, which outputs 32 1024-dimensional embeddings\nfrom a mel-spectrogram. The decoder is also conditioned on\na speaker embedding extracted using H/ASP. XTTS is trained\nend-to-end on 27,282 hours of transcribed speech data across\n16 languages, including 14,513 hours of English speech.\nHierSpeech++ [16] comprises a text-to-vec module and a\nhierarchical speech synthesizer. The text-to-vec module gen-\nerates massively multilingual speech (MMS) representations\n[31] from text inputs and prosody prompts. The hierarchical\nspeech synthesizer produces a waveform from MMS features\nand a style prompt. Prosody and voice style representations\nare extracted from reference mel-spectrograms using style\nencoders comprising 1D convolutional networks, a multi-head\nself-attention temporal encoder, and a linear projection. Hier-\nSpeech++ is trained end-to-end on 2796 hours of transcribed\nEnglish and Korean speech, encompassing 7299 speakers."}, {"title": "C. Evaluation", "content": "We use the default checkpoints and configurations provided\nby the authors for each baseline model\u00b9 \u00b2. Since these models\nemploy various speaker encoders to convert a reference ut-\nterance into a style embedding, we ensure a fair comparison\nby averaging the embeddings across all reference utterances\nfor each target speaker. For zero-shot multi-speaker synthesis\ncomparisons, we use the LibriSpeech test-clean dataset for\ntarget speaker reference utterances. It includes speech of varied\nquality from 20 male and 20 female speakers, with 8 minutes\nof speech per speaker. For each model, we synthesize 100 En-\nglish sentences per speaker, selecting the sentences randomly\nfrom the FLoRes+ dataset [32], in accordance with the XTTS\nprotocol. Tests are performed with x = 1.\nObjective analysis: we evaluate each model's performance\nin terms of naturalness using UTMOS [33], intelligibility using\nthe word error rate (WER) and phoneme error rate (PER)\ncomputed with the Whisper-Large v3 model [34], and speaker\nsimilarity using speaker encoder cosine similarity (SECS) with\nthe ECAPA2 model [35].\nSubjective evaluation: we conduct a listening test to as-\nsess naturalness and similarity mean opinion scores (N-MOS\nand S-MOS). We randomly select utterances from 10 male\nand 10 female target speakers in the LibriSpeech test-clean\ndataset, choosing 3 synthesized sentences per speaker, totaling\n60 utterances per model. Each utterance is rated by 10 raters\non naturalness and similarity to a ground-truth recording, with\nscores ranging from 1 to 5 in 0.5 increments. We use Amazon\nMechanical Turk, with raters required to be native English\nspeakers based in the United States, having a HIT acceptance\nrate above 98% and more than 500 approved HITs. Attention\nchecks are implemented to filter out unreliable ratings.\nModel efficiency: we compare the models based on the\nnumber of parameters, peak GPU memory usage during test\nsample synthesis, and real-time factor (RTF). These tests are\nperformed using an NVIDIA RTX3090 GPU.\nControllability: to showcase this aspect of the framework,\nwe perform an experiment using the interpolation parameter,"}, {"title": "IV. RESULTS AND ANALYSIS", "content": "Results are presented in Table I. Objective metrics re-\nveal that the SSL-TTS models demonstrate the best speaker\nsimilarity, XTTS excels in intelligibility, and HierSpeech++\nachieves the highest naturalness. Subjective evaluations show\nthat listeners rated HierSpeech++ highest for naturalness and\nsimilarity, while the SSL-TTS models and XTTS performed\nsimilarly. These models' results fall within each other's con-\nfidence intervals, suggesting comparable performance. Re-\ngarding model efficiency, SSL-TTS models have the fewest\nparameters and lowest memory requirements among the top\nperformers. Notably, GlowTTS-SSL requires 3\u00d7 less memory\nthan HierSpeech++ with comparable speed. GradTTS-SSL's\nmemory usage and RTF are higher due to the 100 iterations\nused in the diffusion decoder. Further, the SSL-TTS models\nare trained on 100\u00d7 less transcribed data than HierSpeech++\nand 1000\u00d7 less data than XTTS.\nThe speaker similarity matrix (Figure 2) illustrates the\nresults of the controllability experiment. We can observe that\nthe similarity of the outputs to the target speaker gradually\nincreases as \u00e0 rises. This demonstrates the framework's ability\nto blend source and target styles in a fine-grained manner and\nsuggests the potential to combine multiple target styles.\nWe conduct ablation studies to evaluate the models' out-\nputs with varying amounts of reference utterances. Figure 3a\ncompares outputs using kNN retrieval from different amounts\nof LJSpeech data. We find that approximately 30 seconds of\nreference utterances are needed to achieve suitable intelligi-\nbility, while naturalness improves up to 5 minutes, surpassing\nthe model outputs without kNN retrieval. Figure 3b compares\nthe SSL-TTS models to the baselines for different amounts\nof reference utterances from a target speaker. Similarly, about\n30 seconds are required for suitable intelligibility, while sim-\nilarity plateaus at around 1 minute. In contrast, the baselines\nbenefit less from increasing the amount of reference utterances\nbeyond 10 to 30 seconds."}, {"title": "V. DISCUSSION AND CONCLUSIONS", "content": "State-of-the-art zero-shot multi-speaker TTS models rely on\nlarge datasets of transcribed speech from thousands of speakers\nfor training. In this paper, we demonstrated that by combining\nSSL features and kNN retrieval methods, we can develop a\nlightweight TTS system that achieves a comparable level of\nnaturalness and similarity to other approaches while requiring\ntranscribed data from only a single speaker. However, there is\na trade-off, with the other approaches performing better with\njust a few seconds of reference audio, while our method needs\nat least 30 seconds of reference audio, which is still a manage-\nable requirement. We further showed that fine-grained control\nover the influence of the target style on the output can be\nachieved using an interpolation parameter. This indicates that\nthis technique, which is originally inspired from other domains\nsuch as language modeling [18] and machine translation [36],\nalso applies to TTS.\nThe simplicity of the training process is one of the main\nadvantages of our approach, where only the Text-to-SSL model\nrequires training, and it can be trained on transcribed data\nfrom a single speaker. This simplicity, in conjunction with the\nKNN approach's cross-lingual capability [37], is particularly\nappealing for extending the model to new languages and\ndomains with fewer resources. This aspect is open for future\nwork. We also showed that the framework can be implemented\nusing different Text-to-SSL model architectures, allowing for\nmodel swapping to leverage different benefits. Our implemen-\ntations notably demonstrated efficiency in terms of parameters,\nmemory usage, and runtime speed in the case of GlowTTS-\nSSL, even without optimizing the retrieval process.\nTypically, different speakers exhibit different pronunciation\ndurations. In our framework, the duration aspect is deter-\nmined by the Text-to-SSL model, and the target voice is\nmodified through frame-by-frame selection, meaning that the\nduration of each utterance remains unchanged for different\nspeakers. Interestingly, despite this limitation, the SSL-TTS\nmodels were rated comparably to other approaches in terms\nof similarity. Our future work will explore techniques, such as\nUrythmic [38], to address this limitation."}]}