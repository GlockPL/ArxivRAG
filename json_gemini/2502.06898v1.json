{"title": "Large Language Models for In-File Vulnerability Localization Can Be \"Lost in the End\u201d", "authors": ["FRANCESCO SOVRANO", "ADAM BAUER", "ALBERTO BACCHELLI"], "abstract": "Traditionally, software vulnerability detection research has focused on individual small functions due to earlier language processing technologies' limitations in handling larger inputs. However, this function-level approach may miss bugs that span multiple functions and code blocks. Recent advancements in artificial intelligence have enabled processing of larger inputs, leading everyday software developers to increasingly rely on chat-based large language models (LLMs) like GPT-3.5 and GPT-4 to detect vulnerabilities across entire files, not just within functions. This new development practice requires researchers to urgently investigate whether commonly used LLMs can effectively analyze large file-sized inputs, in order to provide timely insights for software developers and engineers about the pros and cons of this emerging technological trend. Hence, the goal of this paper is to evaluate the effectiveness of several state-of-the-art chat-based LLMs, including the GPT models, in detecting in-file vulnerabilities. We conducted a costly investigation into how the performance of LLMs varies based on vulnerability type, input size, and vulnerability location within the file. To give enough statistical power (\u03b2 \u2265 .8) to our study, we could only focus on the three most common (as well as dangerous) vulnerabilities: XSS, SQL injection, and path traversal. Our findings indicate that the effectiveness of LLMs in detecting these vulnerabilities is strongly influenced by both the location of the vulnerability and the overall size of the input. Specifically, regardless of the vulnerability type, LLMs tend to significantly (p < .05) underperform when detecting vulnerabilities located toward the end of larger files-a pattern we call the 'lost-in-the-end' effect. Finally, to further support software developers and practitioners, we also explored the optimal input size for these LLMs and presented a simple strategy for identifying it, which can be applied to other models and vulnerability types. Eventually, we show how adjusting the input size can lead to significant improvements in LLM-based vulnerability detection, with an average recall increase of over 37% across all models.", "sections": [{"title": "Introduction", "content": "In the rapidly evolving field of software development, the integration of large language models (LLMs) has shifted from a novel concept to a mainstream practice. As generative artificial intelligence (AI) technologies continue to advance, tools like ChatGPT, Copilot, and similar models are now embedded as first-class citizens into development environments, assisting with tasks such as code generation, bug fixing, and security analysis. According to Sonatype's 2023 report [40], 97% of developers and security professionals now rely on LLMs in their workflows, marking a significant shift in how software vulnerabilities are detected and patched.\nIndeed, the integration of LLMs addresses a critical challenge identified in the 2022 GitLab Survey [17], which noted that developers often fail to detect security-relevant bugs early and do not prioritize bug fixing adequately. This typically leads to frequent vulnerabilities that are"}, {"title": "Background and Related Work", "content": "This section compares our study with other research on language models for vulnerability detection and explores the relationship to prior natural language processing research, focusing on the \u2018lost-in-the-middle' issue and how input size affects LLMs performance."}, {"title": "Vulnerability Detection via LLMs", "content": "There has been extensive research on the application of language models for vulnerability detection in software development and maintenance, primarily involving small specialized models operating at a function-level granularity [8, 9, 23, 31, 48, 50, 51]. The recent advent of ChatGPT has shifted some focus to the use of LLMs, but still at a function-level granularity [38, 41, 42, 51].\nThe majority of the aforementioned studies used vulnerability datasets extracted from the CVE catalog, a practice we also adopt in our research. The CVE MITRE catalog is a public database managed by the MITRE Corporation that lists and describes software and firmware vulnerabilities, assigning them unique identifiers for standardized handling. The catalog classifies vulnerabilities using the Common Weakness Enumeration, which categorizes software and hardware weaknesses with security implications. Each CWE entry details a specific type of vulnerability, helping organi-zations to better identify, understand, and mitigate potential threats. An example of CVE entry of type CWE-22, wherein the fix spans multiple functions. is shown in Figure 1.\nStudies such as the one by Ullah et al. [43] have shown significant non-robustness in advanced models like PaLM2 and GPT-4. Changes in function or variable names, or the addition of library functions, can cause these models to yield incorrect answers in 26% and 17% of cases, respectively. This highlights the need for further advancements before LLMs can serve as reliable security"}, {"title": "'Lost-in-the-Middle' Issue and Input Size Impact on LLMs", "content": "Although many modern LLMs, including ChatGPT, can process extensive inputs, they often struggle to effectively manage information within lengthy contexts. This challenge has been highlighted in many other studies sometimes referring to a \u2018lost-in-the-middle' issue [24]. \u2018Lost-in-the-middle' refers to a phenomenon in large language models where information presented in the middle of a long input sequence is more likely to be overlooked or forgotten compared to information at the beginning or end.\nThese problems seem to happen because these language models do not know well how to identify relevant details amid fairly large contexts, and this has an impact on their ability to consistently"}, {"title": "Research Questions", "content": "Recent advancements in artificial intelligence have made it possible for LLMs to process entire files or even repositories, rather than just small code segments. As the popularity of chat-based LLMs continues to grow and the use of these LLMs for in-file vulnerability detection becomes more widespread among software developers, it is crucial to assess their effectiveness across various vulnerability types to inform practitioners of the pros and cons of these technological trends. This urgency led us to our initial research question:"}, {"title": "RQ1: To what extent can popular chat-based LLMs detect vulnerabilities within entire files?", "content": "To address this question, we first obtained a dataset containing an equal distribution of vulnerable and patched (i.e., without the vulnerability) files for different CWE types. As detailed in Section 4, the dataset was extracted from the CVE catalog (which includes known vulnerabilities and their fixes/patches; cf. Section 2.1), by selecting all vulnerabilities wherein the patch targets a single file of code. Following the methodology described in Section 5, we then instructed popular state-of-the-art LLMs such as GPT-4 to find and return any line within the dataset files that may lead to a vulnerability of a given CWE type. This approach allowed us to compute metrics such as precision, recall, accuracy, and F1 score for each LLM and CWE type.\nOnce we determine the effectiveness of these models in detecting vulnerabilities within files, we can investigate the reasons behind their errors to inform practitioners about best practices. To do this, we decided to examine whether the position of the vulnerability or the file size impacts the LLMs' performance, specifically assessing whether they experience any \u2018lost-in-the-middle' issues (cf. Section 2.2). This leads to our next question:"}, {"title": "RQ2: To what extent input size and vulnerability position affect LLMs' ability to detect vulnerabilities?", "content": "To answer it, we conducted two experiments. The first experiment (cf. Section 6.1) used the dataset from RQ1 to examine the correlation between input size, the position of the vulnerability, and the correctness of the LLM's output, focusing only on the files containing a vulnerability.\nHowever, this first experiment presented a problem: the distribution of actual file sizes and vulnerability locations follows a tailed distribution, and this could distort the results. Therefore, we conducted a second experiment (whose methodology is detailed in Section 6.2), which com-plements the previous one by uniforming the distribution of vulnerability locations and file sizes. Consequently, this second experiment was designed to more effectively determine whether a given LLM suffers from a \u2018lost-in-the-middle' or \u2018lost-in-the-end' problem.\nFinally, building on the insights from RQ2, we aimed to establish best practices for practitioners when using mainstream LLMs in cases where they suffer from the \u2018lost-in-the-middle' issue or similar problems. If these LLMs have intrinsic limitations that hinder the safe detection of vulner-abilities in files approaching the size of their context windows, we seek to provide practitioners with guidance on the maximum input size that still ensures reliable detection capabilities. This led to the formulation of our third research question:"}, {"title": "RQ3: If smaller inputs are required, how small must they be?", "content": "We hypothesize that the optimal input size could be determined by analyzing how variations in input size affect the LLM's ability to accurately identify the location of a vulnerability. To test this, we divided the vulnerable files into chunks of varying sizes, following the methodology presented in Section 7 and adapting the experimental method from RQ1 to assess file chunks rather than entire files. After determining the optimal input size for an LLM and CWE type, practitioners can chunk files accordingly, improving vulnerability detection likelihood within their code."}, {"title": "Dataset Construction", "content": "To address RQ1, we need a dataset with an equal distribution of vulnerable and non-vulnerable files, categorized by CWE type, where each vulnerability fix affects only a single file. This ensures that the LLM has sufficient context to understand the vulnerability. As no such dataset was available in the related literature (see Section 2), we created one ourselves by extracting data from the CVE catalog, as has been done in related studies [8, 9, 31, 43, 45, 48]. Indeed, the CVE catalog contains thousands of entries, each linking to a specific vulnerability and its corresponding patch (i.e., the vulnerability fix).\nMethodology. Following an established procedure in vulnerability localization research [8, 9, 31, 43, 45, 48], we labeled the file before the patch as the \u201cvulnerable\u201d file and the file after the patch as the \u201cnon-vulnerable\u201d file. This ensures that the pre-fix code contains a vulnerability that the LLM must detect, while the post-fix code should be free of the same vulnerability. Although Rice's theorem [39] suggests it is impossible to determine whether a piece of code is entirely free of vulnerabilities, we use these \u201cnon-vulnerable\u201d files only for answering RQ1, which benchmarks LLMs. This does not affect our core findings and contributions which stem from RQ2 and RQ3.\nWe initiated the data extraction process by scraping all CVE entries from March 2018 to March 2024, covering a period of six years. These entries were filtered to include only those referring to single GitHub commits affecting a single source code file, excluding documentation files. For instance, if a commit targeted one code file and two documentation files, it was included, whereas"}, {"title": "RQ1: LLMs vs. In-File Vulnerability Localization", "content": "The first research question aims to benchmark popular chat-based LLMs on in-file vulnerability localization tasks.\nMethodology. For our analysis, we selected three cutting-edge open-source models (Mixtral 8x7b, Mixtral 8x22b, and Llama 3 70b) and three well-known commercial LLMs from OpenAI: ChatGPT 3.5 (version 0125), ChatGPT 4 (version 2024-04-09), and ChatGPT 4o (version 2024-05-13). The Mistral models have gained significant popularity as the first state-of-the-art open-source LLMs licensed under Apache 2.0. Llama 3, on the other hand, is the flagship open-source LLM from Meta, widely recognized in the AI community and arguably one of the most effective LLMs for vulnerability detection in the open-source landscape [3, 25], as well as the most mainstream.\nThroughout our experiments, we used these models in their default configurations without any fine-tuning. This approach was chosen to assess the general applicability and effectiveness of these models as they are, i.e., without customization, similar to how a typical software developer might use them. Indeed, our goal is to investigate whether commonly used LLMs can effectively analyze large file-sized inputs, to provide timely insights for software developers and engineers. Additionally, by avoiding model-specific fine-tuning, we can also understand the baseline capabilities of these LLMs in vulnerability detection without the confounding effects of specialized training, which may not be fully replicable or practical in many real-world scenarios.\nThe only default configuration hyperparameter we changed for all the LLMs was their temperature. The temperature of an LLM is a hyperparameter that controls the randomness of its output. It can have any positive real value, with lower values making the output more focused and deterministic, and higher values making it more diverse and creative. We set the temperature to 0, the lowest possible value. Setting an LLM's temperature to 0 for vulnerability detection ensures consistent, deterministic, and repeatable responses, which is critical for accurately identifying and analyzing security flaws with little unwanted variability in the results.\nWith each LLM, we used the same prompt, engineered to provide an example of the expected output. This prompting strategy is called in-context learning [27, 52]. Differently from [51], we always used the same example across all tested vulnerability types, specifically a CWE-79 bug involving a 'user_input' JavaScript variable being concatenated into HTML content without proper sanitation.\nThis is the prompt we used:\nAnalyze the file content below and tell me if there's any line that may contain a bug of type CWE-{bug_type_id} ( {bug_type_label} ). Your output must adhere to the following structure.\nExpected Output Structure:\nSE: very Short Explanation of why the line may contain a bug of given type (e.g., The 'user_input' is directly concatenated into HTML content without sanitation).\nBL: the Bugged Line, if any is found, else none (e.g., 'response = \"<html><body><h1>Welcome, \" + user_input + \"!</h1></body></html>\"').\nBUG FOUND: YES if a bug is found, else NO.\nExample output:\nSE: The 'user_input' is directly concatenated into HTML content without sanitation."}, {"title": "RQ2: The Impact of Vulnerability Position and File Size", "content": "Our study examines how the location of vulnerabilities within a file and the file size influence the performance of state-of-the-art LLMs in detecting the three CWE types identified in Section 5. This section details the two experiments conducted to address RQ2 as outlined in Section 3."}, {"title": "File Size and Position of Bug vs. Probability of Detection on Real-World Files", "content": "The first experiment examines the correlation between input size, the position of the vulnerability, and the ability of the LLM to detect the vulnerability within the 794 vulnerable files in our dataset.\nMethodology. Similarly to the previous experiments, also in this case we fed hundreds of vulnerable files to the selected LLMs, asking them to detect and locate the vulnerabilities based on"}, {"title": "Code-in-the-Haystack Experiment", "content": "Given that the probability of randomly identifying a vulnerable line decreases with increasing bug location and file size and that (as shown in Figure 2) the bug location and file size follow a tailed distribution, we conducted a follow-up experiment. This second experiment aims to determine whether the previously shown results are due to the inability of the LLMs to effectively process contextual information or to the tailed distribution of vulnerability features.\nMethodology. In the second experiment, called the \u2018code-in-the-haystack' experiment, we focused on a limited number of different vulnerability instances, specifically five per CWE type. For each instance, we relocated the vulnerable lines of code within the same file, varying the file sizes to create over 500 different file combinations per CWE type, to further verify whether the LLMs' detection capabilities correlate with file size and bug position. Similarly to the other RQ2 experiment, we then used the same simple logistic regressions to analyze vulnerability detection. This experiment was named after the \u2018needle-in-the-haystack' experiments discussed in Section 2.\nIn other words, our code-in-the-haystack experiment involves selecting a set of vulnerabilities for each considered CWE type and systematically altering the position of the vulnerability's core line within the file so that the distribution of bug positions is uniform. While in the first RQ2 experiment we examined potential effects at scale, in this second RQ2 experiments we focus on causal analysis, isolating bug position and file size. By design, this experiment holds all potential confounding factors constant, thus isolating the bug position's impact. In other words, controlling bug position while keeping files unchanged ensured other properties remained fixed. Specifically, in our approach to constructing the experiment, we adopted a methodology consisting of three primary steps.\nIn the first step, for each file, we identify and isolate the main line where the vulnerability occurs. If this vulnerable line resides within a function containing fewer than 500 characters, the entire function is marked as the block of vulnerable code to be moved. Conversely, if the function exceeds 500 characters, a segment encompassing a few lines before and after the vulnerable line is extracted. This segment is then refactored into a new function of approximately 500 characters, which is the block of vulnerable code to be moved.\nLet S be the size of the file, the second step involves creating $\\frac{S}{500}$ new files, positioning the buggy function at different intervals: at the beginning for the first file, at the 500th character for the second, and so on, increasing by 500 characters for each subsequent file. The remaining space in each file is filled with padding characters, using either the rest of the file or contents from files in the same repository.\nFor the final step, we employ the 0/1 knapsack algorithm [37] to construct $\\frac{S}{500}$ new files, each of size S, wherein the buggy function is positioned at different locations. The content of these new files is divided into two segments: one before the buggy function and one after it. Each padding segment is treated as a separate 'knapsack' with capacities determined by the function's position. For example, if the function is placed at position n, the first segment's capacity is 500 \u00d7 (n \u2212 1), and the second is S - 500 \u00d7 n.\nTo implement this, the original file content is divided into functions or independent logic units, which are then distributed into these knapsacks as padding. Each logic unit is measured in characters. To address cases where the padding does not perfectly match the knapsack's capacity constraints, we use a relaxed version of the 0/1 knapsack algorithm. Instead of requiring an exact match to the capacity, we aim for the best fit possible that is closest to the target capacity, whether it is slightly under or over. We define a tolerance value that allows the total size of selected functions to exceed the capacity by a small amount, i.e., approximately 200 characters."}, {"title": "RQ3: Input Size Identification", "content": "With RQ3, we aim to identify best practices for practitioners that use popular chat-based LLMs, such as GPT models, which suffer from the \u2018lost-in-the-end' issue. Since these LLMs struggle to detect vulnerabilities in files approaching their context window limits, we seek to provide guidance on the right input size for reliable detection. Specifically, we want to identify the largest input size for a CWE type at which the model's detection capabilities have the highest recall. We hypothesize that the best input size can be determined by observing changes in the LLM's ability to identify the exact location of a vulnerability when positioned into file chunks of different sizes. To investigate this hypothesis, we use the data from the first experiment of RQ2 (cf. Section 6.1), i.e., the 794 vulnerable files.\nMethodology. Our experimental procedure involves a naive chunking strategy where the content of a file is divided into blocks of lines totaling up to a maximum of k characters. Each line is preserved in its entirety to ensure at least one chunk contained the critical line for the LLM to identify. Although simplistic, this approach demonstrates that even basic chunking could enhance the LLM's performance.\nEach chunk was evaluated using the same experimental prompts used in the investigations of RQ1 and RQ2. Due to the increase in negative examples compared to RQ1, direct comparisons of accuracy and F1 scores are not viable. Instead, we focused on comparing the recall metric, representing the percentage of correctly identified vulnerabilities by the LLM. The chunking strategy was implemented using k values of {6,500, 3,000, 1,500, 500}. The largest chunk size, 6,500, corresponds to approximately the median file size for CWE-89 (see Table 1). A chunk size of 3,000 represents the lower quartile of file sizes across all CWE types. The 1,500 size is slightly above the average function size, while 500 is less than the average function size, providing a finer granularity.\nResults. According to the results detailed in Table 4, we have an average recall improvement of over +37% across all models and CWE types due to chunking, although this strategy led to a decrease in precision, indicated by an increase in false positives due to a greater number of chunks with no vulnerability."}, {"title": "Threats to Validity", "content": "In evaluating the findings, several extrinsic and intrinsic potential threats to validity need to be considered.\nExtrinsic Threats. While we included a variety of popular LLMs, such as Mixtral and Llama, as well as several versions of ChatGPT, the conclusions drawn are primarily applicable to the contexts and vulnerability types explicitly tested, i.e., CWE-22, CWE-79, and CWE-89. The performance of these models might differ when applied to other CWE types or in different software contexts, such as different programming languages.\nOpenAI's continuous updates to their models also pose a challenge; the accuracy and F1 scores presented might change if the experiments are rerun at a different time. Although we aimed for consistency and simplicity in the prompts used, future versions of ChatGPT may perform differently or require new types of prompts.\nLastly, our analysis using logistic regression to assess the impact of file size and vulnerability position on detection probability assumes a linear relationship, which may not capture more complex patterns adequately.\nIntrinsic Threats. Generally speaking, it is impossible to mathematically guarantee that a non-trivial piece of code (e.g., a vulnerability fix) is fully bug-free. This impossibility stems from Rice's theorem [39]. Nevertheless, we are certain that the post-fix code we used for answering RQ1 resolves at least one instance of vulnerability, and that this vulnerability must be detected by the LLM. Instead, our core findings and contributions, which stem from RQ2 and RQ3, are not affected by this problem.\nThe algorithmic construction and manual refactoring of our dataset's files for the code-in-the-haystack experiments could have introduced syntactical errors, such as misplaced import statements. However, this is not a major concern as the study focuses on the \u2018lost-in-the-end' phenomenon in detecting specific types of security weaknesses rather than on syntactical errors.\nThe non-deterministic nature of the AI models, particularly the non-commercial ones that lack control over the random seed, adds another layer of complexity. To mitigate this, we repeated the code-in-the-haystack experiments five times and averaged the results, incurring costs exceeding 400 USD. However, due to these costs, this approach was not feasible for other experiments, and even five runs may be insufficient. Nonetheless, the consistent observation of the \u2018lost-in-the-end' issue across all studied LLMs and the statistical significance of the logistic regression support the reliability of our findings, reducing the likelihood that the observed trends are due to chance.\nFinally, the range of chunk sizes k we considered is arbitrary, and better chunk sizes might exist. Due to the high costs of running large experiments with LLMs, we were limited to testing a few k values using a dichotomic search."}, {"title": "Discussion", "content": "The three research questions outlined in Section 3 focused on evaluating the effectiveness of several popular LLMs in detecting in-file vulnerabilities. Specifically, they examined the influence of file size and the position of the vulnerability, as well as the optimal input size for maximizing detection accuracy.\nRQ1. The findings presented in Section 5 address RQ1, indicating that the performance of off-the-shelf LLMs is generally low, with accuracy scores below .4, and varies significantly across different CWE types. No model consistently outperformed others across all aspects, though commercial models generally show superior performance compared to open-source models. Notably, the majority of the LLMs were more effective in detecting CWE-22 and CWE-89 compared to CWE-79. This suggests that the complexity and commonality of vulnerabilities influence detection capabilities."}, {"title": "Conclusion", "content": "This paper characterized and highlighted the lost-in-the-end issue in popular chat-based LLMs. Recognizing this issue is important for software developers and engineers who extensively use these specific LLMs for coding and debugging activities such as file-level vulnerability detection. In this study, we studied the effectiveness and limitations of popular chat-based LLMs in detecting vulnerabilities within entire source code files, focusing on three of the most dangerous and common CWE vulnerability types: CWE-22, CWE-89, and CWE-79. Our findings reveal significant variability in LLM performance across these vulnerabilities and highlight a new challenge: the \u2018lost-in-the-end' issue, where vulnerabilities located towards the end of files are less likely to be detected.\nWe showed that the \u2018lost-in-the-end' issue can considerably affect the performance of LLMs on file-level vulnerability detection. This also implies that the same issue could affect LLMs at similar tasks involving reasoning over large software files such as code review automation, generic bug localization, code summarization.\nThe implications of our findings are twofold. Firstly, they suggest that further improvements in LLMs are needed before they can be reliably used for vulnerability detection in software develop-ment. This includes enhancements in their ability to handle large inputs and in their sensitivity to the placement of vulnerabilities within files. Secondly, our study highlights the potential of simple yet effective strategies like input chunking to significantly enhance the performance of existing LLMs, which could be readily applied in current software development practices.\nAlthough our analysis (Figure 2) indicates that the smallest files are the most common, the \u2018lost-in-the-end' issue is still relevant because the largest files should not be ignored and \u201cthe devil is in the tails\u201d [50]. Therefore, to eliminate the most dangerous vulnerabilities, we strongly believe that future research should keep increasing the context window of these LLMs up to the repository level, but in a way that is more robust to the \u2018lost-in-the-end' issue."}, {"title": "Data Availability", "content": "All the data and scripts used for this paper are available in our replication package [2]."}]}