{"title": "BAMDP Shaping: a Unified Theoretical Framework\nfor Intrinsic Motivation and Reward Shaping", "authors": ["Aly Lidayan", "Michael Dennis", "Stuart Russell"], "abstract": "Intrinsic motivation (IM) and reward shaping are common methods for guiding\nthe exploration of reinforcement learning (RL) agents by adding pseudo-rewards.\nDesigning these rewards is challenging, however, and they can counter-intuitively\nharm performance. To address this, we characterize them as reward shaping in\nBayes-Adaptive Markov Decision Processes (BAMDPs), which formalizes the\nvalue of exploration by formulating the RL process as updating a prior over possible\nMDPs through experience. RL algorithms can be viewed as BAMDP policies;\ninstead of attempting to find optimal algorithms by solving BAMDPs directly, we\nuse it at a theoretical framework for understanding how pseudo-rewards guide\nsuboptimal algorithms. By decomposing BAMDP state value into the value of\nthe information collected plus the prior value of the physical state, we show how\npsuedo-rewards can help by compensating for RL algorithms' misestimation of\nthese two terms, yielding a new typology of IM and reward shaping approaches. We\ncarefully extend the potential-based shaping theorem [48] to BAMDPS to prove that\nwhen pseudo-rewards are BAMDP Potential-based shaping Functions (BAMPFs),\nthey preserve optimal, or approximately optimal, behavior of RL algorithms;\notherwise, they can corrupt even optimal learners. We finally give guidance on\nhow to design or convert existing pseudo-rewards to BAMPFs by expressing\nassumptions about the environment as potential functions on BAMDP states.", "sections": [{"title": "1 Introduction", "content": "RL algorithms are known to struggle when rewards are sparse. A common solution is intrinsic\nmotivation (IM) or reward shaping, which guides RL agents by adding pseudo-rewards to the true\nrewards [56, 15, 3]. Intrinsic rewards may depend on the entire history while shaping rewards depend\nonly on the action and previous and current MDP state. A wide variety of pseudo-rewards have\nbeen proposed and used successfully with scalable deep RL algorithms in complex environments\n[50, 5, 31]. However, designing them is challenging, and they can affect performance in counter-\nintuitive ways leading to degenerate behaviors [65, 10]. For instance, IM that rewards accurate\nprediction of the next percept may cause an agent to sit forever in front of a blank wall, while favoring\n\"surprise\"-states where the next percept is hard to predict-causes an agent to get stuck watching a\n\"noisy TV\" that randomly flips channels, rather than encouraging exploration as one might expect [7].\nThese sorts of problems can be avoided by analyzing and designing pseudo-rewards within a theoreti-\ncal framework for how a rational RL agent should behave. Such a framework already exists-the\nBayes-Adaptive MDP (BAMDP) [4, 43], a generalization of Bayesian bandits [26]. In a BAMDP,\nthe agent starts out not knowing which MDP it is operating in and learns more through observation.\nBAMDP states consist of the cumulative information observed from the actual MDP, i.e., the entire\nhistory of states, actions, and rewards, up to and including the current physical state. An RL algorithm\ncan be viewed as a policy mapping the BAMDP state to an action that updates it [17]. An optimal"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Markov Decision Processes and Reinforcement Learning Algorithms", "content": "Markov decision processes (MDPs) are defined by tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, R,T, \\mathcal{T}_0, \\gamma)$ with $\\mathcal{S}$ a set\nof states, $\\mathcal{A}$ a set of actions, $T(s'|s, a)$ and $R(r|s, a, s')$ the transition and reward probability func-\ntions with expected reward $R(s, a, s')$, $\\mathcal{T}_0(s_0)$ an initial state distribution, and $\\gamma$ a discount factor\n(when it is not critical, we write $R$ without $s'$ for brevity). MDP policies map from current states\nto distributions over next actions: $\\pi(a|s)$. The return is defined as the discounted sum of re-\nwards $G = \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1}$, and the value and action-value functions under policy $\\pi$ are $V^{\\pi}(s) =$"}, {"title": "2.2 Intrinsic Motivation and Reward Shaping", "content": "Intrinsic motivation (IM) is a method for guiding RL algorithms [2] by adding pseudo-rewards to the\noriginal reward at each step, generating composite reward signal: $r'_t = r_t + F(h_t)$, where $F$ denotes\nthe IM function which can depend on the entire history $h_t$ (and thus also the algorithm's internal\nstate or beliefs). Reward shaping functions are restricted to the form $F(s_t, a_t, s_{t+1})$, resulting in\nshaped reward function $R'(s_t, a_t, s_{t+1})$ (whereas IM does not generally lead to a valid MDP reward\nfunction of this form). The optimal policy for the shaped MDP, i.e. maximizing the composite return,\nis generally not optimal for the original MDP. Potential-based shaping functions (PBSFs) take the\nform $F(s_t, a_t, s_{t+1}) = \\gamma \\phi(s_{t+1}) - \\phi(s_t)$ and preserve optimal policies in all MDPs [48]."}, {"title": "2.3 Formulation of RL Problems as BAMDPS", "content": "We formulate RL problems as BAMDPs, in which the optimal policy is the Bayes-optimal RL\nalgorithm for the problem. Our conventions are inspired by Zintgraf et al. [69] and Guez et al.\n[28]. For clarity of exposition, we assume all possible MDPs in $p(M)$ share the same $\\mathcal{S}, \\mathcal{A}, \\gamma$, so\nonly $R, T, \\mathcal{T}_0$ are initially uncertain.\u00b2 $p(M|h_t)$ is the posterior after updating $p(M)$ on the evidence\nin history $h_t = s_0 a_0 r_1 s_1... a_{t-1} r_t s_t$ using Bayes' rule, i.e., $p(M|h_t) \\propto p(h_t|M)p(M)$. We use\noverlines, e.g., $\\bar{M}$, to denote the BAMDP version of any object.\nA BAMDP is a tuple $\\bar{M} = (\\bar{\\mathcal{S}}, \\mathcal{A}, \\bar{R}, \\bar{T}, \\bar{\\mathcal{T}}_0, \\gamma)$ where:\n$\\bullet$ $\\bar{\\mathcal{S}}$ is an augmented state space $\\mathcal{S} \\times \\mathcal{H}$, so $\\bar{s} = (s, h)$. This encapsulates all the information$\n\\qquad \\qquad \\qquad$ could use when choosing an action - though typically it maintains a lossy memory of $h$.\n$\\bullet$ $\\mathcal{A}$ and $\\gamma$ are shared with the underlying MDPs, although internally $\\bar{\\pi}$ may sample its actions\n$\\qquad$ from the MDP policy that it learnt: $\\pi_t = \\bar{\\pi}(\\bar{s}_t)$, so that $\\pi_t(a|s_t) = \\bar{\\pi}_t(a|\\bar{s}_t)$.\n$\\bullet$ $\\bar{R}(\\bar{s}_t, a) = E_{p(M|h_t)}[R(s_t, a)]$, the expected reward under the current posterior.\n$\\bullet$ $\\bar{T}(\\bar{s}_{t+1}|\\bar{s}_t, a_t) = E_{p(M|h_t)}[T(s_{t+1}|s_t, a_t)R(r_{t+1}|s_t, a_t)1[h_{t+1}=h_ta_tr_{t+1}s_{t+1}]]$\n$\\bullet$ $\\bar{\\mathcal{T}}_0((s_0, h_0)) = E_{p(M)}[\\mathcal{T}_0(s)]1[h_0 = s_0]$.\nWe illustrate the basic concepts of BAMDPs using the caterpillar problem shown in Fig. 2. In this\nproblem, $p(M)$ represents how butterflies usually lay eggs on the best food source in the area, but\n10% of the time a more rewarding source is nearby; upon hatching on the weed, the caterpillar must\ndecide whether exploring the neighboring bush is worth the energy and opportunity cost. The bush's\nreward varying across possible MDPs manifests as stochastic BAMDP dynamics when the caterpillar\nfirst observes it (e.g., taking stay from the state labelled $s_4$). After observing the reward, $p(M|h_t)$\ncollapses to the underlying $M$ and all dynamics become deterministic (e.g., at the last step in the\ntrajectories labelled A and B it had collapsed to $M_1$ and $M_2$, respectively)."}, {"title": "3 Pseudo-rewards Correct BAMDP Value Misestimation", "content": "We first use our framework to explain how IM and reward shaping can help RL algorithms by\ninfluencing their value estimates to align with the Bayes-Optimal BAMDP value."}, {"title": "3.1 The Relationship Between Value Misestimation and Regret", "content": "RL algorithms typically act, implicitly or explicitly, to maximize a value prediction estimated from the\nhistory, which we denote by $\\hat{Q}(\\bar{s}_t, a)$. This estimate is rarely equal to the true Bayes-Optimal value\n$Q^*$, because it is generally difficult to initialize the algorithm with the correct prior and intractable\nto plan through the space of future beliefs. Applying the performance difference lemma [37] at the\nBAMDP level tells us that algorithmic regret is directly related to how these estimates differ from the\nBayes-Optimal value over their trajectory:"}, {"title": "3.2 BAMDP Value Decomposition", "content": "We can now explain the role of pseudo-rewards as correcting RL algorithms' value estimates towards\nthe optimal BAMDP value. IM terms that reward novel observations or diverse actions encode the\nvalue of the information in the BAMDP state $h_t$; these are popular because the value of gaining\ninformation is ignored by commonly used RL algorithms (see section 3.3). On the other hand,\neven if the agent would reach a state in which no valuable information had been collected, i.e.,\n$V^*((s_{t+1}, h_{t+1})) = V^*((s_{t+1}, h_0))$, it could misestimate the value there due to incorrect initial\nbeliefs. Many other pseudorewards compensate for this type of misestimated value, typically when\nthere is significant prior knowledge about how to maximize rewards, e.g., when playing football\nit is advantageous for your team to have possession of the ball. This is helpful to express as a\npseudo-reward because it can be difficult to program such prior knowledge into non-tabular (e.g.,\ndeep RL) algorithms before they begin learning. We decompose the BAMDP value function into\nthese two values, which we call the Value of Information and Value of Opportunity, respectively."}, {"title": "Definition 3.1 (Value of Information).", "content": "The Value of Information (VOI) from state $s_t$ is the increase\nin $\\pi^*$'s expected return from $s_t$ due to the information in $h_t$ compared to its initial beliefs:\n$V_I^*((s_t, h_t)) = \\bar{V}^*((s_t, h_t)) - \\bar{V}^*((s_t, h_0))$"}, {"title": "Definition 3.2 (Value of Opportunity).", "content": "The Value of Opportunity (VOO) to from state $s_t$ is the\nexpected optimal value of state $s_t$ without having learnt anything, i.e.:\n$V_O^*((s_t, h_t)) = \\bar{V}^*((s_t, h_0))$"}, {"title": "Lemma 3.2 (BAMDP Value Decomposition).", "content": "The optimal BAMDP value can be decomposed into\nthe Value of Information and the Value of Opportunity: $\\bar{V}^*(\\bar{s}_t) = V_I^*(\\bar{s}_t) + V_O^*(\\bar{s}_t)$.\nWe can now categorize IM and reward shaping terms by which of these components they signal (see\nTable 1); note that some signal the value itself, and others the change in value from the previous\nstate to the next. We can understand the failure of pseudo-rewards to effectively guide RL algorithms\nas a result of them aligning insufficiently well with the true BAMDP value. See appendix B for\nmore in-depth discussion of these examples. This provides us with some guidance on how to design\npseudo-rewards, but we defer that discussion to section 4.4."}, {"title": "3.3 Certainty-Equivalent RL Algorithms Underestimate the Value of Information", "content": "Section 3.2 showed how we can understand many IM terms as encouraging and directing exploration\nthrough encoding the value of the resulting information. This type of IM is so popular because many\ncommonly used RL algorithms under-estimate the value of information; we now introduce a model\nof these algorithms to formalize this within the BAMDP framework. Many RL algorithms, from\npolicy-based methods like policy gradient [63, 57] to value-based methods like Q-Learning [68, 45],\navoid the intractability of belief-space planning by acting as if beliefs are fixed. We formalize this\nobjective with the Certainty-Equivalent\u2074 RL algorithm $\\bar{\\pi}$."}, {"title": "Definition 3.3 (Certainty-Equivalent RL Algorithm).", "content": "At each step, the Certainty-Equivalent Algo-\nrithm follows the MDP policy maximizing its estimated expected return under current beliefs:\u2075\n$\\bar{\\pi}(\\bar{s}_t) \\in arg \\max_{\\pi} E_{b^\\pi(h_t)} [V^\\pi (s_t)]$,\nwhere $b^\\pi(\\cdot)$ denotes how $\\bar{\\pi}$ interprets its experience, which could be anything from a distribution\nover world models maintained by updating a prior, to a point estimate of $Q^*$ maintained by training a\nrandomly initialized neural net on batches sampled from $h_t$ [45]. For ease of notation we use $\\pi_t$ and\n$a_t$ interchangeably, since $\\pi_t$ is only used at step $t$ to output $a_t$.\nFor example, policy gradient algorithms like Reinforce sample actions from an MDP policy $\\pi_\\theta$,\ni.e., $\\bar{\\pi}_{Reinforce}((\\bar{s}_t)) = \\pi_\\theta(s_t)$, where $\\pi_\\theta$ is learnt by gradient updates towards maximizing the\nexpected returns $R(\\tau)$ of the trajectories $\\tau$ that it generates, i.e. $J(\\theta) = E_{\\tau \\sim \\pi_\\theta} [R(\\tau)]$. The algorithm\nestimates $J(\\theta)$ from environment interactions so far, i.e. $h_t$, so $\\hat{J}(\\theta) = E_{b^\\pi(h_t)} [E_{\\tau \\sim \\pi_\\theta}[R(\\tau)]]$ where\n$b^\\pi(h_t)$ is concentrated on a point estimate of $J(\\theta)$. If, as a model, we assume that policy gradient\nwere to maximize $J(\\theta)$ between each interaction, then we find that it matches the behavior of $\\bar{\\pi}$:\n$\\arg \\max_\\theta \\hat{J}(\\theta) = \\arg \\max_\\theta E_{b^\\pi(h_t)} [E_{\\tau \\sim \\pi_\\theta} [R(\\tau)]] = \\arg \\max_\\theta E_{b^\\pi(h_t)} [V^{\\pi_\\theta} (s)].$\nThe Certainty-Equivalent algorithm effectively estimates the Q value as:\n$\\hat{Q}(s_t, a) = \\max_{\\pi} E_{b^\\pi(h_t)} [R(s_t, a) + \\gamma V^\\pi (s_{t+1})]$.\nComparing this to the optimal value $Q^*(\\bar{s}_t, a) = E_{p(M|h_t)}[R(s_t, a) + \\gamma \\bar{V}^*((s_{t+1}, h_{t+1}))]$, we see\nthat, even assuming accurate beliefs $b^\\pi(h_t) = p(M|h_t)$, the Certainty-Equivalent algorithm still\nmisses value from the ability to learn from the information in $h$, because with $V^\\pi (s_{t+1})$ it assumes it\nwould still follow the same $\\pi$ from step $t + 1$, when in reality it would have updated it with the latest\nobservation. E.g., if $s_t$ in Fig. 2 was observed to be empty, $\\bar{\\pi}$ would update $\\pi$ to return to $s_w$ forever,\nresulting in higher return than if it kept following the same $\\pi$ back to $s_b$ (see appendix D.2 for the\ncalculations). Thus, adding pseudo-rewards signalling VOI can compensate for this underestimation,\ncausing the algorithm to predict higher value for behavior leading to valuable information."}, {"title": "4 Potential-Based Shaping in the BAMDP", "content": "Section 3 explained how IM and reward shaping help suboptimal RL algorithms; however, when\nfollowing a process to develop increasingly optimal algorithms (either manually as a field, or automat-\nically, e.g., with meta-RL) we also care about what forms of pseudo-reward preserve optimality. To\ninvestigate this, we now extend existing theory on MDP potential-based shaping preserving optimal\npolicies, to BAMDP potential-based shaping and optimal RL algorithms. Note that extending the\noriginal proofs is non-trivial, because, unlike regular MDPs, rewards influence the BAMDP state."}, {"title": "Definition 4.1. (BAMPF)", "content": "Let any $\\mathcal{S}, \\mathcal{A}, \\gamma$ be given. Function $F$ is a BAMDP Potential-Based\nShaping Function if there exists a real-valued function $\\phi$ such that for all realizable $h_t$,\n$F(h_t) = \\gamma \\phi(h_t) - \\phi(h_{t-1})$"}, {"title": "4.1 A Motivating Example", "content": "IM terms based on increase in experience can be valid BAMPFs, e.g. information gain [33] can easily\nbe recast as a BAMPF with $\\phi(h) = -H(p(\\bar{T}|h))$, i.e., the certainty of the algorithm's posterior\nbelief over the MDP dynamics $\\bar{T}$ after updating on $h$: $F(h_t) = H(p(\\bar{T}|h_{t-1})) - \\gamma H(p(\\bar{T}|h_t))$\nMeanwhile, prediction error [50, 7] based on the error in predicting features of the latest state given\nthe experiences so far, i.e., $F(h_t) = ||f(s_t|h_{t-1}) - f(s_t)||^2$, is not a valid BAMPF, because it cannot\nbe expressed as a difference between potentials at successive histories.\nNote that both types of IM can sometimes be MDP potential-based shaping, e.g, there may eventually\nbe enough information in $h$ that they become constant, making them trivial PBSFs in future episodes.\u2076\nBut even if they do converge, we have no guarantees about how long that would take or how\ncatastrophically they might impact the RL agent's behavior before convergence.\nBAMPF theory goes beyond PBS theory; Theorem 4.2 tells us that we can always use information\ngain to guide our RL algorithms while preserving optimal behavior, whereas prediction error can\ndistract even optimal algorithms from maximizing the true return. E.g., in the Noisy TV problem,\neven Bayes-optimal algorithms would watch TV forever if the shaped return from unpredictably\nflipping channels was higher than that from exploring the more predictable corridors of the maze to\nreach the extrinsic reward. Concretely, if we use BAMPF pseudo-rewards then:\n$\\bullet$ As we develop increasingly Bayes-optimal RL algorithms, they will also be increasingly\n$\\qquad$ optimal with respect to the true rewards.\n$\\bullet$ Lifelong learning agents that learn to act more Bayes-optimally for the composite rewards\n$\\qquad$ over time will also act more Bayes-optimally for the underlying environment.\n$\\bullet$ We could shape the objective of meta-RL systems to help them discover better RL algorithms\n$\\qquad$ more quickly, without making them eventually converge to suboptimal algorithms."}, {"title": "4.2 Bayes-Optimal RL Algorithms are Invariant to BAMPFS", "content": "We model the effect of pseudo-reward function $F(h_t)$ as producing shaped BAMDP $\\bar{M}'$ with shaped\nreward $R'(s_t, a, s_{t+1}) = R(s_t, a) + F(h_{t+1})$, while $\\mathcal{S}$ and $\\mathcal{T}$ are unchanged, i.e., $h_t$ still only\ncontains the underlying rewards. This reflects the fact that $F(h_t)$ is fully known from the start, so\nit should not be treated as information or influence the posterior. We model the pseudo-rewards as\nbeing added inside the RL algorithm, rather than entering through $h_t$ in $\\bar{s}_t$. An optimal algorithm for\n$\\bar{M}'$ maximizes the expected shaped return from $R'$, i.e., $\\bar{\\pi}^{*'}=\\arg \\max_{\\pi} E_\\pi [\\bar{G}']$.\nWe now state our main theorem that BAMPFs always preserve Bayes-Optimality."}, {"title": "Theorem 4.2 (BAMDP Potential-Based Shaping Theorem).", "content": "For a pseudo-reward function to guar-\nantee that the optimal algorithm for any shaped BAMDP is optimal for the original BAMDP, i.e.,\nBayes-optimal for the underlying RL problem, it is necessary and sufficient for it to be a BAMPF."}, {"title": "4.3 Near Optimal Algorithms are Nearly Invariant to BAMPFS", "content": "Bayes-optimality is generally intractable, but we can extend our results to approximate algorithms."}, {"title": "Corollary 4.1.", "content": "With BAMPF shaping, a near-optimal algorithm for $\\bar{M}'$ will also be near-optimal\nwhen applied to $\\bar{M}$, i.e., $E_{\\bar{\\pi}^{*'}} [\\bar{G}'] - E_{\\bar{\\pi}}[\\bar{G}'] < \\epsilon \\Leftrightarrow E_{\\bar{\\pi}^{*}} [\\bar{G}] - E_{\\bar{\\pi}}[\\bar{G}] < \\epsilon$."}, {"title": "Definition 4.3 (k-Step learning-aware).", "content": "We define an RL algorithm as k-step learning-aware for a\nBAMDP if it maximizes the expected k-step return in it:\n$\\bar{\\pi}_k \\in arg \\max_{\\pi} E_{\\mathcal{T}_0,\\mathcal{T}} [\\sum_{t=0}^{k} \\gamma^t \\bar{R}(\\bar{s}_t, \\bar{\\pi}(\\bar{s}_t))]$"}, {"title": "Corollary 4.2.", "content": "If $F$ is a BAMPF with potential function of maximum magnitude $\\phi_{max}$, and the\nextrinsic reward has maximum magnitude $R_{max}$, then the k-step learning-aware algorithm for the\nshaped BAMDP, $\\bar{\\pi}_k'$, has regret bounded by $2\\gamma^k (\\phi_{max} + R_{max} \\frac{1}{1-\\gamma})$ in the underlying BAMDP, i.e.,\n$E_{\\bar{\\pi}^{*}} [\\bar{G}] - E_{\\bar{\\pi}_k'} [\\bar{G}] \\leq 2\\gamma^k (\\phi_{max} + R_{max} \\frac{1}{1-\\gamma}).$"}, {"title": "4.4 Designing Intrinsic Motivation and Reward Shaping as BAMPFS", "content": "We now synthesize the results from the previous sections to provide practical guidance for designing\neffective pseudo-rewards as BAMPFs. We showed in section 3.2 that pseudo-rewards can help by\ncompensating for components of BAMDP value that the RL agent misestimates; to achieve this in\nthe form of a BAMPF, simply expresswrite the missing value as a potential function on the BAMDP\nstate ($h_t$). A BAMPF based on those potentials encourages suboptimal RL agents to go to higher\npotential BAMDP states, while preserving the performance of near-optimal agents.\nIt is also straightforward to convert existing pseudoreward terms to BAMPFs. E.g., to convert\nVOI terms $\\phi(h_t)$ should capture the total expected value of information in $h_t$; e.g., Entropy bonus\nexpresses the value in the information from trying a wide spread of actions from each state, which can\nbe converted to $\\phi(h_t) = H(a|s)$ with $H$ calculated from the frequencies in $h_t$. For VOO terms, $\\phi(h_t)$\nshould capture the agent's missing prior knowledge of the value at $s_t$ encoded by the pseudo-reward;\ne.g., Negative Surprise corrects algorithms lacking the prior knowledge that unpredictable states have\nlower value because the environment is dangerous, so $\\phi(h_t)$ could be $-||f(s_t|h_{t-1}) - f(s_t)||^2$, or\nSubgoal Reaching signals the underestimated value of completing subgoals necessary for reaching\nthe final reward, with $\\phi(h_t)$ equal to the number of subgoals completed at $s_t$."}, {"title": "5 Related Work", "content": "Theory of Intrinsic Motivation Oudeyer and Kaplan [49] categorize IM as knowledge-based,\ncompetence-based or morphological, and evaluate them by their exploration (leading to exploratory\nand investigative behavior) and organization (leading to structured and organized behavior) potentials,\nwhich are similar to the effects of signalling $V_O$ and $V_I$. Singh et al. [62] propose an evolutionary\nframework for rewards as maximizing expected fitness over a distribution of environments, concluding\nthat optimal IM depends on regularities across the distribution and properties of the learning agent.\nAubret et al. [1] propose an information-theoretic taxonomy of IM, but this only captures IM terms"}]}