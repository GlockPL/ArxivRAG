{"title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting", "authors": ["Chengyou Jia", "Changliang Xia", "Zhuohang Dang", "Weijia Wu", "Hangwei Qian", "Minnan Luo"], "abstract": "Despite the significant advancements in text-to-image (T2I) generative models, users often face a trial-and-error challenge in practical scenarios. This challenge arises from the complexity and uncertainty of tedious steps such as crafting suitable prompts, selecting appropriate models, and configuring specific arguments, making users resort to labor-intensive attempts for desired images. This paper proposes Automatic T2I generation, which aims to automate these tedious steps, allowing users to simply describe their needs in a freestyle chatting way. To systematically study this problem, we first introduce ChatGenBench, a novel benchmark designed for Automatic T2I. It features high-quality paired data with diverse freestyle inputs, enabling comprehensive evaluation of automatic T2I models across all steps. Additionally, recognizing Automatic T2I as a complex multi-step reasoning task, we propose ChatGen-Evo, a multi-stage evolution strategy that progressively equips models with essential automation skills. Through extensive evaluation across step-wise accuracy and image quality, ChatGen-Evo significantly enhances performance over various baselines. Our evaluation also uncovers valuable insights for advancing automatic T2I.", "sections": [{"title": "1. Introduction", "content": "In recent years, text-to-image (T2I) generative models have attracted considerable attention [1, 23, 31, 32]. Building on advancements in large-scale T2I models such as DALL-E [31] and Stable Diffusion [32], the open-source community has significantly expanded the capabilities of T2I generation. Researchers fine-tune open-source models on specialized datasets, resulting in a diverse selection of task-specific models available on platforms like Civitai [6] and Hugging Face [8]. This variety provides users with a broader range of options to meet customization needs, facilitating the growing adoption of T2I models in real-world applications.\nHowever, the rapid development of T2I models within the open-source community has also introduced significant challenges for users. When non-experts attempt to create images with specific requirements, they often encounter a trial-and-error process involving several tedious steps. As shown in Figure 1, these steps include crafting suitable prompts, selecting appropriate models, and configuring specific model arguments. The complexity and uncertainty of each step turn the process into an arduous journey, resembling \u201cmice in a maze\u201d. In real-world scenarios, this iterative process consumes substantial time and resources as users continuously adjust settings to regenerate images. Therefore, we pose the challenging problem: can we automate these labor-intensive steps in T2I generation, allowing users to simply describe their needs in a chatting style and receive desired images effortlessly?"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Text-to-Image Generation", "content": "With the advancement of diffusion models [13, 27, 34], text-to-image (T2I) generation [1, 23, 31] has demonstrated exceptional capabilities in high-quality image generation and textual description alignment. Large-scale models such as DALL-E [1] enhance text-image alignment by leveraging the joint feature space of CLIP [30]. Moreover, Stable Diffusion [32], a well-established open-source model, has gained substantial attention. Numerous methods have been developed to fine-tune it or design additional modules for specialized tasks, such as customized image generation [17, 18, 33], layout-to-image generation [14, 19, 20, 47] and image edit [2, 16]. These diverse models have significantly expanded the capabilities of T2I. However, they also present significant learning challenges for non-expert users, underscoring the increasing need for automatic T2I generation."}, {"title": "2.2. LLMs for Text-to-Image Generation", "content": "Large language models (LLMs), like ChatGPT [24], Llama [37], have demonstrated impressive capability in language understanding [3] and problem solving [45]. Recently, LLMs have also begun to be applied to image generation. Recently, LLMs have also been applied to image generation. [7, 9, 20, 36, 43] leverage LLMs to generate detailed layout information from complex prompts, enabling the creation of sub-elements and control over their positioning. [28, 39, 49] propose employ LLMs for model selection, akin to tool usage [26, 29, 44]. However, the above"}, {"title": "3. Methodology", "content": "Our goal is to relieve users from tedious steps and automate to produce the desired images from user's freestyle input. To achieve this objective, we first introduce ChatGenBench, a benchmark comprising a large set of user inputs in a chatting style, built upon a foundation of over 6000 personalized models to evaluate automated image generation results. A comprehensive description of data collection, construction, and analysis is provided in Sec. 3.1. Then, we present ChatGen-Evo in Sec. 3.2, which uses a multi-stage evolution strategy to train MLLM for Automatic T2I."}, {"title": "3.1. ChatGenBench: Benchmarking Automatic T2I", "content": "For clarity, an example of this data is shown on the right side of Figure 2. The following sections detail the data collection and construction process, which primarily involves High-Quality Human Demonstration Collection and using LLM-driven Role-Play to simulate user input."}, {"title": "3.1.1 High-Quality Human Demonstration Collection", "content": "Civitai [6] is a vibrant community where users share customized AI models for generating high-quality images. Members contribute demonstrations with detailed prompts, model specifications, and arguments, supported by a feedback system that ensures quality validation. These features make Civitai an ideal platform for collecting raw data.\nCollection: We start by collecting demonstrations based on established evaluation metrics within Civitai, including download counts, upvotes, and other user feedback. These metrics enable us to collect data that has been validated through community engagement. By focusing on these indicators, we identify a subset of high-quality results.\nFiltering: Following the initial collection, we implement a rigorous filtering process to ensure data quality. This involves excluding demonstrations associated with inactive or outdated models, removing duplicates, and filtering out NSFW content. This careful curation refined the dataset to include the most effective demonstrations. Ultimately, this process results in a curated set of 44,881 high-quality human demonstrations across 6,807 unique models."}, {"title": "3.1.2 LLM-Driven Role-Play for Chatting Generation", "content": "While the demonstrations collected from communities include the essential procedural information needed for automation, they lack a critical element: freestyle chatting inputs. Such data is not available on open platforms, which has been a key limitation for previous methods [49]. To address this, we propose an LLM-Driven Role-Play strategy for Chatting Generation. As shown in Figure 2, we predefine over 100 roles from everyday life (e.g., students, doctors, professors) and prompt the LLM to simulate these roles, translating each demonstration into freestyle chatting input with tones and habits of the character. This approach significantly enhances data diversity and creates more lifelike inputs. To further enhance reverse synthesis diversity, we employed multiple versions of (M)LLMs to complete all generation tasks, displayed in Table 2. Additionally, we set the temperature parameter to 0.9 to make the model more stochastic. We also utilized BertScore [48] to filter out results with a similarity greater than 0.8.\nMoreover, we define three types of freestyle input formats: single-input, consisting of a single chatting-style sentence; multimodal-input, combining a sentence with an image; and history-input, comprising multiple rounds of dialogue history. These formats effectively simulate how users typically inquire about image generation needs, greatly expanding the practical value of automatic T2I."}, {"title": "3.1.3 Benchmark Construction", "content": "Following the above steps, we generate 330,970 freestyle inputs from 44,881 demonstrations. Considering the large-scale generated data, we don't randomly split a test set for benchmarking. Instead, we carefully select high-quality, non-overlapping samples as the benchmark for evaluation. The selection involves the following steps:\nTestSet Split: We split the data based on model origin. For data associated with the same model, we use BERTScore to assess the similarity between samples. From this, we select about 20% most semantically distinct data as initial TestSet in Table 3. This maximizes non-overlap with TrainSet.\nTestSet Filtering: Building on the initial TestSet, we perform multiple filtering rounds to ensure the final ChatGen-Bench's quality. This involves the following steps:\n\u2022 Length Filtering: Remove excessively long entries or those with too many dialogue turns for consistency.\n\u2022 Colloquialism Check: Utilize the Spacy [35] module to filter data for alignment with natural, everyday language.\n\u2022 LLM-Based Evaluation: Employ LLM to assess and select data that best matches the chatting tone.\n\u2022 Manual Verification: Manual verification is conducted to eliminate any inappropriate or irrelevant samples.\nThrough this rigorous process, we refine the initial 74,364 entries to a final set of 14,564 high-quality, freestyle chatting samples, constituting the novel benchmark.\nSetting Division: In practical scenarios, new models often have limited demonstration data, underscoring the importance of evaluations under constrained data. Therefore, we further divide ChatGenBench into two settings\u2014Supervised and Few-shot\u2014based on the availability of samples for each model in the TrainSet."}, {"title": "3.1.4 Benchmark Analysis", "content": "ChatGenBench offers distinct advantages over previous benchmarks, as summarized in Table 1. On the data level, ChatGenBench includes large-scale high-quality data and a broader range of T2I models. Additionally, human demonstrations provide relative ground truth across each step, allowing for step-wise evaluation that pinpoints potential challenges in automation models. ChatGenBench is also the novel benchmark to support multiple input types, making it more aligned with real-world scenarios."}, {"title": "3.2. ChatGen: Achieving Automatic T2I", "content": "Our goal is to train the model capable of processing freestyle user inputs and generating the necessary components for image generation (prompt, model, and argument), thereby achieving Automatic T2I. In this section, we first introduce a baseline method, ChatGen-Base. We then analyze the limitations of this approach and subsequently propose ChatGen-Evo, which leverages the multi-stage evolution strategy to enhance performance."}, {"title": "3.2.1 ChatGen-Base with SFT", "content": "We first apply supervised fine-tuning (SFT) to develop the ChatGen-Base model as an intuitive baseline. Given a set of freestyle chatting inputs c (which may include text, images, and historical context) and their corresponding outputs comprising prompt p, model m, and argument r, we use the auto-regressive objective to maximize the following loss:\n$I_{base}^{sft} = -\\sum_{t} log P_{\\Theta}(p, m, r \\mid c, *_{<t}).$ (1)\nChatGen-Base satisfies the essential requirements for"}, {"title": "3.2.2 ChatGen-Evo", "content": "To address the limitations of ChatGen-Base, we propose ChatGen-Evo, which trains MLLM M using a multi-stage evolution strategy. Instead of relying on final outcome supervision as in traditional fine-tuning, the multi-stage evolution strategy in ChatGen-Evo employs stage supervision. By providing more precise feedback at each stage, this approach gradually enables the MLLM to acquire the necessary capabilities for automated T2I. As shown in Figure 3, the training process includes three main stages:\nStage 1: Prompt Writing via SFT. We first train the MLLM using SFT with pairs of freestyle inputs and high-quality prompts. Different from the objective in ChatGen-base, the current stage focuses on a more specific and simplified task instead:\n$I_{stage1}^{sft} = -\\sum_{t} log P_{\\Theta}(p \\mid c', *_{<t}).$ (2)\nHere, c' represents the freestyle input with a prompt prefix that clarifies the task being performed. This prefix helps preserve the MLLM's original capabilities, minimizing catastrophic forgetting [22]. Through this stage, the model learns how to rewrite inputs into effective prompts.\nStage 2: Model Selection via ModelToken. We introduce the ModelToken strategy to equip the model with model selection capabilities without impacting the prompt-writing ability learned in Stage 1. Inspired by token learning [10, 15], ModelToken extends this approach by representing each candidate T2I model as a unique token within the MLLM's vocabulary. Specifically, the model tokens are parameterized as an embedding matrix $W_M \\in R^{|M| \\times d}$ and appended to the original word token matrix $W_v \\in R^{|V| \\times d}$.\nModelToken Training: during training, the user input c and prompt p are concatenated as a prefix, with the special model token <Model_i> appended as the ground truth for next-token prediction. The training objective is:\n$L(W_M) = - log P_{\\Theta}(<Model\\_i>\\mid c, p).$ (3)\nUnlike typical next-token prediction training, the embedding matrix $W_M$ represents the only tunable parameters, significantly enhancing training efficiency. With a small parameter size, fewer training samples are required, improving performance in scenarios with limited data.\nInference for Model Selection: Once the embedding matrix is trained, the inference process concatenates the model token and original word token, forming the new language modeling head of the MLLM. In this way, the MLLM predicts the next token with the following probability:\n$P(m|c, p) = softmax([W_v; W_M] \\cdot h_{i-1}),$ (4)\nwhere the operation [;] denotes concatenation, and $h_{i-1} \\in R^d$ represents the last hidden state. Once a model token is predicted, the MLLM stops decoding, and the corresponding model m is selected. Additionally, information such as the model's description and demonstrations is loaded for subsequent use. After Stage 2, the model not only retains its prompt-writing skills but also learns to select models.\nStage 3: Argument Configuration via In-Context Learning. After the above two stages, we have obtained the prompt p and model m from the original user input c. The"}, {"title": "4. Experiment", "content": "We conduct a comprehensive evaluation of various methods on the novel ChatGenBench. First, in section 4.2, we compare ChatGen-Evo with other baseline models, highlighting the efficacy and efficiency of our multi-stage evolution strategy. Next, in section 4.3, we perform extensive ablation studies, uncovering the impact of each step on the final results and providing valuable insights. Finally, we provide visualizations of the generated images in section 4.4."}, {"title": "4.1. Experimental Settings", "content": "Training Setups. We adopt InternVL2 [5] as the base MLLM, fully fine-tuning it for both ChatGen-Base and the first stage of ChatGen-Evo. In the second stage of ChatGen-Evo, all model parameters are frozen except for the Model-Token embeddings. We employ the AdamW optimizer with a learning rate of 4e-5 and a weight decay of 1.0 over 5 epochs, maintaining these settings consistently across training stages. All experiments are conducted on 8 A100 GPUs.\nMetrics for Step-wise Evaluation: Leveraging the comprehensive process data in ChatGenBench, we introduce the step-wise evaluation metrics to assess the distinct abilities of automatic T2I models in key stages:\n\u2022 Prompt BERTScore: To assess prompt rewriting ability, we use BERTScore [48] to compare predicted prompts with high-quality, human-validated prompts. BERTScore leverages pre-trained contextual embeddings to match words in candidate and reference sentences. The metric ranges from 0 to 1, with 1 indicating highly similar meanings and 0 signifying complete dissimilarity.\n\u2022 Selection Accuracy: We calculate the accuracy of model selection by comparing the predicted T2I model with the human-validated model.\n\u2022 Argument Accuracy: We evaluate argument configuration by calculating the exact match accuracy between the predicted arguments and the human-validated arguments. The overall argument accuracy is obtained by averaging the accuracy across all individual arguments.\nIt is important to note that obtaining the absolute ground truth across all stages is nearly impossible due to the infinite search space. Therefore, we use human-validated high-quality records as relative ground truth. While these are not absolute, the comprehensive evaluation of the large-scale benchmark is able to reflect the capabilities of automation models, as confirmed by experimental results.\nImage Quality Evaluation: We use HPS v2 [41] and ImageReward [42] metrics to assess the quality of generated images, reflecting alignment with human preferences. Additionally, we employ FID and CLIP Score to evaluate how well the generated images meet user requirements. FID [12] measures the distance between automatically generated images and human-validated high-quality images, while CLIP Score [11] calculates the similarity between the generated images and human-validated prompts. To provide an intuitive and comprehensive measure of image quality, we normalize and combine these four metrics into an aggregated score, Unified Metric [49]. Each of above scores are normalized to the range [0,1] and the final score is computed by averaging the value:\n$S_{unified} = \\frac{1}{4}((1-\\text{norm}(S_{fid})) + \\text{norm}(S_{clipscore}) + \\text{norm}(S_{hps}) + \\text{norm}(S_{reward})).$ (6)\nBy integrating these diverse metrics, the Unified Metric offers a more nuanced and comprehensive understanding of image quality, facilitating more informed comparisons and evaluations of generated images.\nBaseline Without Fine-Tuning. We further establish a baseline that uses the default in-context learning capabilities of the MLLM for prompt rewriting, along with a single Stable Diffusion model and a fixed set of default parameters. This baseline helps emphasize the significance of prompt rewriting and multi-model utilization."}, {"title": "4.2. Main Experiment", "content": ""}, {"title": "4.2.1 Quantitative Results", "content": "Table 4 presents the main quantitative results of ChatGen-Evo compared to different baselines. Overall, ChatGen-Evo significantly outperforms other methods across all metrics, including both step-wise and final image quality evaluations. Specifically, the low performance of the baseline highlights the importance of effective prompt rewriting and multi-model selection, underscoring the necessity of dedicated Automatic T2I methods. Additionally, fine-tuning MLLMs with progressively larger parameter scales-from"}, {"title": "4.2.2 Human Evaluation", "content": "We conduct a user study using pairwise comparisons to further evaluate ChatGen-Base(8B) and ChatGen-Evo(2B). Users are presented with two images generated from the same input: one by ChatGen-Base and the other by ChatGen-Evo. It is tasked with selecting the image that better matches the image quality and relevance to the given input. We sample 2,000 image pairs for supervised and 1,000 for few-shot."}, {"title": "4.2.3 Efficiency Comparison", "content": "Similar to existing multi-stage reasoning methods [25, 40, 46], ChatGen-Evo does not offer an efficiency advantage over direct prediction approaches like ChatGen-Base. However, the significant performance gains more than compensate for this drawback. As shown in Table 5, ChatGen-Evo achieves the performance level of ChatGen-Base at 8B parameters using only 2B parameters. Therefore, when comparing efficiency under equal performance, ChatGen-Evo maintains a relative advantage over ChatGen-Base."}, {"title": "4.3. Analysis", "content": ""}, {"title": "4.3.1 Capability Analysis.", "content": "We conduct ablation experiments on ChatGenBench to evaluate the contribution of individual steps to final performance by providing ground truth for the other steps."}, {"title": "4.3.2 Input Type Analysis.", "content": "Table 7 presents ChatGen-Evo performance across different input types. Multimodal inputs lead to better performance, as images may offer clearer prompt and model identification cues compared to text alone. Additionally, handling historical data remains a significant challenge, with the lowest performance across all metrics. These results point to valuable directions in enhancing history-based prompt generation."}, {"title": "4.4. Visualizations", "content": ""}, {"title": "4.4.1 Qualitative comparisons", "content": "Figure 5 presents examples of images generated by different methods. From the first row, it is evident that ChatGen-Evo understands user requirements and identifies suitable models to generate style-matching images. The second row demonstrates results based on multi-modal user inputs, where ChatGen-Evo shows a superior understanding of the reference image and preserves more details to generate refined outputs. The third row illustrates ChatGen-Evo's capability in handling historical data, ensuring that each step inherits the previous style while making appropriate modifications based on user requirements."}, {"title": "4.4.2 Qualitative results with step-wise outputs", "content": "In Figures 6 and 7, we present the step-wise outputs and final images of ChatGen-Evo. It can be observed that ChatGen-Evo effectively rewrites high-quality professional prompts based on the user's freestyle input. Furthermore, ChatGen-Evo selects suitable models to match the user's desired style or character. Finally, it generates appropriate argument configurations to ensure the high quality of the resulting images. These high-quality images, produced through well-designed step-wise outputs, demonstrate the value of Automatic T2I. It relieves users from tedious steps and automates the production of desired images directly from their freestyle input."}, {"title": "4.4.3 Comparisons with commercial models", "content": "In Figure 8, we compare the image quality of our method with the advanced commercial model DALL-E 3 [1]. While DALL-E is capable of generating high-quality images, its style is predominantly limited to a single type (anime-style). This limitation arises from its reliance on a single model, which cannot fully accommodate diverse and personalized styles. This highlights the value of our approach, which performs significantly better in scenarios requiring realistic"}, {"title": "5. Conclusion", "content": "Our research aims to automate tedious steps in T2I generation, allowing users to simply describe their needs in a freestyle chatting way. We introduce ChatGenBench for benchmarking Automatic T2I task. It includes high-quality paired data with diverse freestyle user inputs, enabling comprehensive evaluation across all steps. Furthermore, we argue that Automatic T2I should be regarded as a multi-step reasoning task. Consequently, we propose ChatGen-Evo, a multi-stage evolution strategy that progressively equips models with essential automation skills. Extensive evaluations not only demonstrate the superiority of ChatGen-Evo but also provide valuable insights for advancing Automatic T2I. We believe this represents a significant step toward the future of automated generative models."}]}