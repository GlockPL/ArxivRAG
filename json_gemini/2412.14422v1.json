{"title": "Enhancing Diffusion Models for High-Quality Image Generation", "authors": ["Jaineet Shah", "Michael Gromis", "Rickston Pinto"], "abstract": "This report presents the comprehensive implementation, evaluation, and optimization of Denoising Diffusion Probabilistic Models (DDPMs) and Denoising Diffusion Implicit Models (DDIMs), which are state-of-the-art generative models. During inference, these models take random noise as input and iteratively generate high-quality images as output. The study focuses on enhancing their generative capabilities by incorporating advanced techniques such as Classifier-Free Guidance (CFG), Latent Diffusion Models with Variational Autoencoders (VAE), and alternative noise scheduling strategies. The motivation behind this work is the growing demand for efficient and scalable generative Al models that can produce realistic images across diverse datasets, addressing challenges in applications such as art creation, image synthesis, and data augmentation. Evaluations were conducted on datasets including CIFAR-10 and ImageNet-100, with a focus on improving inference speed, computational efficiency, and image quality metrics like Fr\u00e9chet Inception Distance (FID). Results demonstrate that DDIM + CFG achieves faster inference and superior image quality. Challenges with VAE and noise scheduling are also highlighted, suggesting opportunities for future optimization. This work lays the groundwork for developing scalable, efficient, and high-quality generative Al systems to benefit industries ranging from entertainment to robotics.", "sections": [{"title": "I. INTRODUCTION", "content": "Diffusion models are a class of generative models that have revolutionized the \"Generative AI\" field by enabling high-quality data generation across various domains. These models operate by introducing random noise to datasets and subsequently learning to reverse this process to remove noise iteratively. With their noise prediction capability, diffusion models are capable of reconstructing original data from noisy inputs, making them versatile tools in image synthesis, data restoration, media content generation, and even robotic manipulation.\nThe primary goal of this project is to implement and enhance Denoising Diffusion Probabilistic Models (DDPMs) for high-quality image generation. During the training process, the inputs to these models consist of an RGB image per trading instance and Gaussian noise, progressively added to simulate the diffusion process. The model learns to predict the noise added to the image at each step, effectively learning how to reverse the noise-adding process and reconstruct the original image. During inference, the model starts with a random Gaussian noise input and iteratively denoises it step by step, guided by the learned noise predictions. This reverse process generates a high-quality image, starting from pure noise and refining it into a visually coherent output. Our implementation focuses on addressing computational inefficiencies inherent in diffusion models and optimizing their performance for practical deployment. This is achieved through integrating advanced methodologies, such as Denoising Diffusion Implicit Models (DDIMs), Latent Space Models/Variational Autoencoders (VAEs), and Classifier-Free Guidance (CFG), as outlined in the HW5 write-up.\nFurthermore, this project introduces an exploratory investigation into modifying the linear noise scheduler of DDPMs. By optimizing the noise scheduling process for specific tasks or datasets, the study aims to uncover potential improvements in inference speed, computational efficiency, and robustness. These enhancements are critical for real-world applications, such as scalable and controllable image generation in industries like gaming, design, and advertising, where performance and quality are paramount.\nThis work not only reinforces the foundational understanding of diffusion models but also contributes to the ongoing research in improving their scalability and applicability for diverse real-world use cases."}, {"title": "II. LITERATURE REVIEW", "content": "Several key works have laid the foundation and advanced this field significantly. The introduction of denoising diffusion probabilistic models (DDPMs) by Ho et al. [1] marked a breakthrough in generative modeling. By framing the generation process as the reverse of a noise corruption process, DDPMs demonstrated the ability to generate high-quality samples from complex distributions. This work highlighted the importance of optimizing the variational lower bound for improved training stability and image quality.\nExpanding upon this framework, Song et al. [2] proposed denoising diffusion implicit models (DDIMs), which introduced a deterministic sampling process. This approach retained the high fidelity of DDPMs while significantly reducing the number of sampling steps. Notably, DDIMs have been shown to achieve comparable results to DDPMs with improved computational efficiency, making them more practical for large-scale applications.\nTo address the computational challenges of pixel-space diffusion, Rombach et al. [3] introduced latent diffusion models (LDMs). This approach integrates diffusion processes into a compressed latent space, significantly reducing the memory and computational requirements while maintaining image fidelity. LDMs have enabled high-resolution image synthesis and are particularly effective in tasks requiring fine-grained control, such as text-to-image generation.\nThe concept of leveraging latent representations also finds strong parallels with the foundational work of Kingma et al. [4], which introduced the Variational Autoencoder (VAE) framework. VAEs use a probabilistic latent space to model data distributions, effectively encoding complex structures into compressed representations. This idea underpins many advancements in generative modeling, including the latent space optimization techniques seen in LDMs [3]. By combining latent representations with diffusion processes, LDMs extend the utility of VAEs to handle more intricate generative tasks, such as high-resolution image synthesis.\nNoise scheduling plays a critical role in the performance of diffusion models. The cosine noise scheduler, introduced by Nichol and Dhariwal [5], is a significant improvement over the original linear scheduling method proposed by Ho et al. [1]. This method uses a cosine function to control the variance schedule, enabling smoother transitions between noise levels and enhancing sample quality. The cosine scheduler has become a standard technique in state-of-the-art diffusion models, contributing to better performance with minimal computational overhead.\nIn our project, we are implementing these papers to build a robust image generation pipeline. Leveraging the foundational principles of DDPMs [1] and DDIMs [2], we aim to optimize our generative models for both quality and efficiency. By integrating the cosine noise scheduling strategy from Nichol and Dhariwal [5], we are focusing on achieving smoother noise transitions and improved sample quality. Additionally, the latent space optimization introduced in LDMs [3] provide promising avenues for handling high-resolution synthesis tasks efficiently.\nOur objective is to reproduce the high-quality results demonstrated in these papers while tailoring the approaches to our specific dataset and application needs. This involves experimenting with different noise scheduling strategies, sampling methods, and architectural choices to fine-tune the model for optimal performance. These state-of-the-art techniques form the backbone of our methodology, and by systematically implementing and testing them, we hope to achieve results that align with the high standards set by the research community."}, {"title": "III. DATASET", "content": "Our study utilizes two datasets: ImageNet-100 and CIFAR-10, to train various models which will be discussed later in this paper. The lighterweight CIFAR-10 model was used to train less efficient models, while a more efficient model was trained on the heavier ImageNet-100.\n1) ImageNet-100: ImageNet-100 is a subset of the larger ImageNet dataset, consisting of approximately 130,000 samples across 100 classes. Images are scaled to a resolution of 128 x 128. This dataset provides a wide range of categories, including animals, objects, and scenes, making it ideal for assessing the generalization capabilities of the model. ImageNet-100 was primarily used to train the final model and to evaluate its quality and robustness.\n2) CIFAR-10: In the intermediate stages of model development, the CIFAR-10 dataset was employed. CIFAR-10 is a lightweight dataset containing 60,000 32 \u00d7 32 color images evenly distributed across 10 balanced classes. Its simplicity and smaller scale made it suitable for training intermediate versions of the model before transitioning to ImageNet-100 for final evaluation and performance optimization."}, {"title": "B. Preprocessing and Collation", "content": "The two datasets were easy to download and unzip and begin working with. For preprocessing, we resized the images to 128x128 pixels. No padding was applied, as the images within each dataset are uniform in size. We used minor data augmentation techniques, such as random horizontal flipping, and normalized the data to have zero mean and unit variance. Future work could explore additional augmentation techniques to improve model robustness.\nWe adopt mini-batch sampling during training, a common practice in deep learning. We use a batch size of 128. The collation process involves shuffling the dataset at the start of each epoch to ensure randomness and prevent the model from learning patterns based on the data order."}, {"title": "IV. DDPM - BASELINE MODEL", "content": "We have decided to use our DDPM model as the baseline model. This choice is based on the fact that all the enhancements we described incorporate DDPM as a common architecture, making it a convenient and consistent reference point.\nDenoising Diffusion Probabilistic Models (DDPMs) generate images by gradually reversing a diffusion process that corrupts real data with Gaussian noise over multiple timesteps. During training, the model learns to predict and denoise the added noise, and during sampling, it starts from pure noise and iteratively reconstructs realistic images by reversing the learned process."}, {"title": "B. Evaluation Metrics: IS and FID in Diffusion Models", "content": "To measure model performance, we use Inception Score (IS) and Fr\u00e9chet Inception Distance (FID). These metrics are commonly used in the literature to evaluate generative models, particularly for assessing the quality and diversity of generated images.\n1) Inception Score (IS): The Inception Score (IS) is a widely used metric for evaluating the quality and diversity of generated images. It utilizes a pre-trained Inception network to compute the class probabilities of the generated images. The formula for IS is:\n$IS = exp (E_{x~p_g(x)} [D_{KL} (p(y|x) || p(y))])$, where:\n$p(y|x)$ is the conditional class distribution for an image x,\n$p(y) = E_{x~p_g(x)} [p(y|x)]$ is the marginal class distribution, and $D_{KL}$ denotes the Kullback-Leibler divergence.\n2) Fr\u00e9chet Inception Distance (FID): The Fr\u00e9chet Inception Distance (FID) is a robust metric for evaluating the similarity between real and generated image distributions. It compares the feature distributions of real and generated images extracted using a pre-trained Inception network. The formula for FID is:\n$FID = ||\u03bc_r - \u03bc_g||_2 + Tr(\u03a3_r + \u03a3_g \u2013 2(\u03a3_r \u03a3_g)^{1/2})$, where:\n$\u03bc_r, \u03a3_r$ are the mean and covariance of real image features,\n$\u03bc_g, \u03a3_g$ are the mean and covariance of generated image features\nIn the context of diffusion models, IS and FID are critical metrics for evaluating performance. Recall that Diffusion models aim to generate high-quality images that closely resemble real data. The Inception Score (IS) measures the confidence of class predictions for generated images, providing an assessment of image quality. Meanwhile, the Fr\u00e9chet Inception Distance (FID) compares the distributions of real and generated images, offering valuable insight into both diversity and fidelity. As standard metrics in the literature, IS and FID are widely used for benchmarking generative models, such as GANs, and for evaluating variations of diffusion models, including DDIM and LDM. However, both metrics have limitations: IS does not evaluate how well the generated data aligns with the real data distribution, while FID, though more robust, can be sensitive to the quality of the pre-trained Inception network used for feature extraction. By combining IS and FID, a comprehensive evaluation of diffusion models can be achieved, addressing both image quality and alignment with the real data distribution."}, {"title": "C. Prior Baseline Performance", "content": "Relevant State-of-the-Art is summarized below."}, {"title": "D. End-to-End Process", "content": "This section will explain the following components of the pipeline:\n1) Forward Diffusion Process: Gradually adds Gaussian noise to the image over a series of timesteps, transforming the clean image into pure noise. This process is governed by the noise schedule ($\u03b2_t$), which determines the variance of the added noise at each step.\n2) Reverse Diffusion Process: Iteratively removes the noise added during the forward process to reconstruct clean data from noisy inputs. This reverse process is learned by the model, subsequently discussed, which predicts the noise at each timestep.\n3) Noise Scheduler: Controls the distribution of noise added during the forward process and its corresponding removal during the reverse process.\n4) UNet Architecture: Serves as the backbone of the model, taking noisy data and timestep information as input and predicting the noise to be removed. It features an encoder-decoder structure with Residual Connection blocks to preserve spatial details while processing multi-scale features.\n5) Training Setup: The model is trained to minimize the mean squared error (MSE) between the true noise and the noise predicted by the UNet. This allows the model to accurately denoise samples during the reverse process.\n6) Inference Process: Starts from pure Gaussian noise and iteratively applies the reverse diffusion process using the trained model to generate higher-quality images."}, {"title": "1) Forward Diffusion Process", "content": "The forward diffusion process involves progressively corrupting the original data by adding Gaussian noise at each timestep t. This process can be mathematically represented as:\n$q(x_t|x_{t-1}) = N(x_t; \u221a(1 \u2013 \u03b2_t)x_{t-1}, \u03b2_tI)$,\nwhere:\n$x_t$ is the noisy sample at timestep t,\n$\u03b2_t$ is the variance of the noise added at timestep t,\n$N(\u00b7; \u03bc, \u03c3^2)$ represents a Gaussian distribution with mean \u03bc and variance \u03c3^2.\nThe cumulative effect of noise across timesteps is modeled as:\n$q(x_t|x_0) = N(x_t; \u221a\\bar{\u03b1_t}x_0, (1 \u2013 \\bar{\u03b1_t})I)$,\nwhere:\n$\\bar{\u03b1_t} = \u03a0_{i=1}^{t}(1 \u2212 \u03b2_i)$ is the cumulative product of (1 \u2013 \u03b2), representing the proportion of the original signal retained at timestep t."}, {"title": "2) Reverse Diffusion Process", "content": "The reverse diffusion process is the core of the denoising mechanism in DDPMs. It reconstructs the original data by removing noise step-by-step, starting from pure noise. This process can be represented as:\n$p_\u03b8(x_{t-1}|x_t) = N(x_{t-1}; \u03bc_\u03b8(x_t, t), \u03a3_\u03b8(x_t, t))$,\nwhere:\n$x_{t-1}$ is the reconstructed sample at timestep $t \u2013 1$,\n$\u03bc_\u03b8(x_t, t)$ is the predicted mean,"}, {"title": "3) Noise Scheduler", "content": "The Noise Scheduler is a critical component of the forward and reverse diffusion process in DDPMs. Its primary purpose is to control how noise is added to the data during training by defining the variance $\u03b2_t$ at each timestep $t$. The scheduler ensures that noise is added progressively, starting with little noise at the early stages and increasing gradually in later timesteps. This noise schedule helps the model learn how to effectively reverse the noise process during inference. The noise scheduler used here is a linear noise schedule where the variance increases linearly from a small value $\u03b2_{start}$ to a larger value $\u03b2_{end}$. In the reverse process, the Noise Scheduler determines the scaling factors for each timestep, enabling the model to effectively remove noise in a structured manner, transitioning smoothly from coarse denoising to fine-grained reconstruction.The schedule can be expressed as:\n$\u03b2_t = \u03b2_{start} + (\\frac{t}{T})(\u03b2_{end} \u2013 \u03b2_{start})$,\nwhere:\nt is the current timestep, ranging from 0 to T,\n$\u03b2_{start}$ and $\u03b2_{end}$ are the starting and ending values of the noise variance.\nThis linear schedule ensures that noise gradually increases as the training progresses, giving the model sufficient time to denoise the data during reverse sampling."}, {"title": "4) U-Net Architecture", "content": "The core neural network backbone of the DDPM is built upon a U-Net architecture, which is characterized by:\nEncoder (Downsampling Path): Extracts features while reducing spatial dimensions using layers of layers Res-Net blocks and downsampling operations. Captures both detailed and broad patterns and stores intermediate feature maps for later use in the decoder.\nBottleneck: Acts as a bridge between the encoder and decoder, processing the most compressed representation of the data. It captures global context and complex relationships.\nDecoder (Upsampling Path): Reconstructs the input to its original size by combining upsampled features with stored encoder feature maps through Res-Net blocks, promoting a balance of fine details and contextual coherence in the output."}, {"title": "5) Training Setup", "content": "In this section, we outline the training process, beginning with our objective function.\nWe use a formulation of Mean Squared Error (MSE) for our loss function during DDPM training:\n$L_{simple}(\u03b8) := E_{t,x_0,\u03b5} [||\u03b5 \u2013 \u03b5_\u03b8(\u221a{\u03b1_t}x_0 + \u221a{1 \u2013 \u03b1_t}\u03b5, t)||^2]$\nDescription of Components\n$L_{simple}(\u03b8)$: The loss function that minimizes the discrepancy between the true noise $\u03b5$ and the model's predicted noise $\u03b5_\u03b8$.\n$E_{t,x_0,\u03b5}$: The expectation is taken over:\nt: A timestep uniformly sampled from {1,...,T}, where T is the total number of timesteps.\n$x_0$: The original data sample drawn from the training dataset.\n$\u03b5$: The Gaussian noise sampled from a standard normal distribution, $\u03b5 \u223c N(0, I)$.\n$\u03b5$: The Gaussian noise added to the original data $x_0$ during the forward diffusion process.\n$\u03b5_\u03b8(x_t,t)$: The model's prediction of the noise $\u03b5$ at timestep $t$, conditioned on the noisy data $x_t$. The parameters $\u03b8$ are optimized during training.\n$\u221a{\u03b1_t}x_0 + \u221a{1 \u2013 \u03b1_t}\u03b5$: The noisy data $x_t$ at timestep $t$, constructed as a mixture of:\n$\u221a{\u03b1_t}x_0$: A scaled version of the original data $x_0$.\n$\u221a{1 \u2013 \u03b1_t}\u03b5$: A scaled version of the noise $\u03b5$.\nBy minimizing this loss, the model can learn to reverse the forward diffusion process step-by-step, ultimately reconstructing high-quality data from pure noise during inference."}, {"title": "6) Inference Process", "content": "The inference process in a Denoising Diffusion Probabilistic Model (DDPM) involves reversing the forward diffusion process to generate data from random noise. Below is a description of the key steps:\nInitialize with Random Noise: Start with a randomly sampled noise tensor $x_T \u223c N(0,I)$, representing the noisiest state at timestep T.\nIterative Denoising: For each timestep t (from T to 1):\nUse the trained model $\u03b5_\u03b8(x_t,t)$ to predict the noise $\u03b5$ present in $x_t$.\nEstimate the intermediate sample $x_{t-1}$ using:\n$x_{t-1} = \\frac{1}{\u221a{\u03b1_t}} (x_t - \\frac{1 - \u03b1_t}{\u221a{1 - \\bar{\u03b1_t}}}\u03b5_\u03b8(x_t,t)) + \u03c3_t \u03b6$,\nwhere:\n$\u03b1_t$: Noise scaling factor.\n$\\bar{\u03b1_t}$: Cumulative product of noise scalars.\n$\u03c3_t$: Optional noise term added for stochasticity (only for variational inference).\nFor t > 1, Gaussian noise $\u03b6 \u223c N(0, I)$ is added to preserve variance. For t = 1, no noise is added to ensure a clean output.\nGenerate Final Sample: The process concludes at timestep t = 1, yielding $x_0$, which approximates a sample from the learned data distribution."}, {"title": "V. IMPLEMENTED EXTENSIONS", "content": "Building on DDPM, DDIM introduced deterministic reverse diffusion to enhance inference speed and consistency. Unlike DDPM, which relies on a Markov chain for reverse diffusion, DDIM introduces a deterministic path leveraging information from earlier timesteps, enabling intermediate step skipping. Each timestep depends on the model's noise prediction and the deterministic reverse step formulation, avoiding randomness.\n1) Core Implementation Details: Building upon the foundation established by DDPM, our implementation of DDIM (Deterministic Denoising Diffusion Implicit Models) introduces a deterministic reverse diffusion process. This modification transitions the sampling procedure from a probabilistically-driven, chaotic mechanism to a more controlled, deterministic process.\nThe reverse diffusion process for generating $x_{t-1}$ from $x_t$ is defined in Figure 4, where:\n$x_t$ is the noisy data at timestep t.\n$\u03b5_\u03b8(x_t,t)$ is the noise predicted by the model at timestep t.\n$\u03b1_t$ is the cumulative noise schedule at timestep t.\n$\u03c3_t$ controls the variance of the noise added at each step.\n$z_t \u223c N(0, I)$ is sampled from a standard normal distribution, representing optional stochastic noise.\nThe variance for the DDIM reverse diffusion process also differs from DDPM and is defined as:\n$\u03a3_t = \u03b7 \u22c5 (\\frac{1 - \\bar{\u03b1_{t-1}}}{1 - \\bar{\u03b1_t}}) \u22c5 (1 \u2013 \u03b1_t)$,\nwhere:\n\u03b7 is a hyperparameter controlling the degree of stochasticity in the reverse process. For deterministic DDIM sampling, \u03b7 = 0."}, {"title": "B. Advanced Methodologies", "content": "To further enhance diffusion models, the following were integrated:\nLatent Diffusion Models (VAE): Reduced computational complexity by operating in latent space. Training leveraged a pre-trained VAE for ImageNet-100 that was provided to the team. The VAE works by taking an image and converting it into the latent space.\nClassifier-Free Guidance (CFG): Simplified conditional image generation by interpolating between conditional and unconditional scores.\nWith the VAE, we are finally able to train on ImageNet-100, as the VAE compresses the input of size 128x128 into a latent space of size 32\u00d732."}, {"title": "C. Exploration: Cosine Noise Scheduler", "content": "For the exploration portion of this project, we implemented an alternative to DDPM's linear noise scheduler. Following the methodology described by Nichol and Dhariwal [5], we allocated noise based on a cosine function, placing different emphasis on timesteps throughout the denoising process. Specifically, the cosine scheduler assigns more noise to earlier timesteps, encouraging the model to focus on learning how to handle noisier, more degraded inputs effectively. Conversely, it reduces the noise added during later timesteps, allowing the model to refine and recover fine-grained details as the denoising progresses. This non-linear noise allocation aims to strike a balance between learning global structure early in the process and focusing on high-quality reconstruction in the final stages, leading to improved sample quality and more efficient training dynamics.\nDefinition of $\u03b1_t$\nIn contrast to the linear schedule, the cumulative noise parameter $\u03b1_t$ is defined using a cosine function:\n$\u03b1_t = cos^2 (\\frac{t}{T} \\frac{\u03c0}{2})$\nwhere:\nt is the current timestep,\nT is the total number of timesteps.\nThis ensures a smooth decay of $\u03b1_t$ over time, leading to a more stable noise variance schedule.\nComputation of $\u03b1_t$ and $\u03b2_t$\nFrom $\u03b1_t$, the individual signal preservation parameter $\u03b1_t$ and noise variance parameter $\u03b2_t$ are computed as follows:\n$A_t = \\frac{\u03b1_t}{\u03b1_{t-1}}$\n$\u03b2_t = 1 - A_t$\nHere:\n$\u03b1_t$ represents the fraction of the signal preserved at timestep t,\n$\u03b2_t$ represents the noise variance added at timestep t."}, {"title": "Adjusted Formula for Numerical Stability", "content": "To avoid numerical instability during computation, a small constant $\u03b5$ was added. This modifies the definition of $\u03b1_t$ to:\n$\u03b1_t = \\frac{cos^2 (\\frac{t}{T} \\frac{\u03c0}{2})}{cos^2 (\\frac{t}{T} \\frac{\u03c0}{2}) + \u03b5}$\nwhere $\u03b5$ is a small positive constant added for numerical stability."}, {"title": "VI. SUMMARY OF EXPERIMENTS", "content": "Trained a Denoising Diffusion Probabilistic Model (DDPM) on the CIFAR-10 dataset.\nEvaluated the model's performance in terms of Inception Score (IS) and Fr\u00e9chet Inception Distance (FID).\nServed as the baseline diffusion approach for comparison with other methods."}, {"title": "B. DDIM on CIFAR-10", "content": "Utilized a Denoising Diffusion Implicit Model (DDIM) on CIFAR-10.\nExplored improved sampling efficiency compared to DDPM, leveraging non-Markovian steps.\nAnalyzed how well DDIM maintains generation quality with faster sampling."}, {"title": "C. DDIM + VAE + CFG on ImageNet", "content": "Applied DDIM with pre-trained VAE and Classifier-Free Guidance (CFG) on ImageNet.\nIncorporated CFG to condition the generation process on class labels, enhancing fidelity and alignment with the desired class.\nEvaluated the ability of the model to generate high-resolution, class-consistent images."}, {"title": "D. DDIM with Cosine Noise Schedule on CIFAR-10", "content": "Implemented DDIM with a cosine noise schedule on CIFAR-10 to improve noise distribution during the diffusion process.\nCompared the performance of the cosine noise schedule against standard noise schedules.\nMeasured improvements in sample quality and training stability."}, {"title": "VII. RESULTS", "content": "Given limited training time and resources, our models were able to converge for experiments B and C. For experiments A and D, we've only been able to train these models for about 10 epochs so far. Our results are too early to report, and so we are omitting our intermediate results. We urge our readers to focus on experiments B and C which successfully implement the diffusion process and show performance gains. We expand upon this in this section and the discussion."}, {"title": "E. DDIM Efficiency", "content": "The Denoising Diffusion Implicit Models (DDIM) demonstrate a significant improvement in inference efficiency compared to the Denoising Diffusion Probabilistic Models (DDPM). Specifically, DDIM achieves a 4x speedup in inference, meaning that for a given model, the inference process with DDIM requires only 25% of the time needed by DDPM. This improvement is largely attributed to a reduction in the number of inference steps, which decreases from 1000 steps in DDPM to just 250 steps in DDIM. Additionally, DDIM reduces peak memory usage by approximately 30%, further enhancing its practicality and efficiency in resource-constrained environments."}, {"title": "VIII. DISCUSSION", "content": "Our findings indicate that DDIM + CFG consistently outperformed the baseline DDIM with a linear noise schedule. This demonstrates the benefits of deterministic sampling and classifier-free guidance in achieving higher-quality and more diverse image generation. These results align with the performance improvements observed in similar studies on generative models.\n1) Baseline Comparison: DDPM on CIFAR-10: The original DDPM paper by Ho et al. [1] achieved an Inception Score (IS) of 9.46 and a Fr\u00e9chet Inception Distance (FID) of 3.17 on CIFAR-10. Given that our experiment is not complete, we are unable to compare against this baseline currently.\n2) Efficient Sampling with DDIM: Song et al. [2] reported significant improvements in inference speed and demonstrated comparable image quality with Denoising Diffusion Implicit Models (DDIM). Specifically, their FID (Fr\u00e9chet Inception Distance) values on the CIFAR-10 dataset ranged from 4.04 to 13.36, depending on the number of sampling steps used. In contrast, our implementation of DDIM achieved an FID of 47.92. While we successfully reduced the number of sampling steps, this quality gap highlights the critical role of sufficient training epochs in fine-tuning the deterministic sampling process. Additionally, another factor contributing to our underperformance relative to their results is the significant difference in training steps: the authors employed 800,000 steps, whereas our implementation utilized only 61,000 steps. These discrepancies underscore the importance of extended training and careful optimization to achieve state-of-the-art results with DDIM.\n3) High-Resolution Synthesis with Latent Diffusion Models: Rombach et al. [3] demonstrated the effectiveness of latent diffusion in handling high-resolution datasets like ImageNet, achieving an FID of 27.0 on the full ImageNet dataset. Our results on ImageNet-100, using a pre-trained VAE and CFG, yielded an FID of 323.51. The disparity is largely attributed to:\n4) Cosine Noise Scheduler Exploration: Nichol and Dhariwal [5] reported smoother training dynamics and enhanced sample quality using a cosine noise schedule. This approach mitigates abrupt transitions in noise levels, leading to more stable gradients and improved model convergence. Consequently, the enhanced sample quality highlights the effectiveness of this noise schedule in generating high-fidelity outputs."}, {"title": "C. Challenges with VAE", "content": "The integration of Variational Autoencoders (VAEs) presented notable challenges. Limited training epochs and model capacity constrained performance, highlighting the need for further optimization. These findings emphasize the potential of latent diffusion models, which, with adequate resources, can significantly enhance computational efficiency and output quality."}, {"title": "D. Noise Scheduling", "content": "The cosine noise scheduler exhibited promising results, particularly in enhancing perceptual smoothness during image generation. This observation aligns with the findings of Nichol and Dhariwal [5], where the cosine schedule demonstrated improved training dynamics and sample quality. However, achieving an optimal balance between diversity and fidelity requires additional tuning and experimentation."}, {"title": "E. Sensitivity and Risks", "content": "The results were particularly sensitive to training duration and dataset scale. Shortened training epochs limited the models' ability to fully adapt, leading to suboptimal noise prediction and image quality. Additionally, the reliance on pre-trained components, such as the VAE, introduced uncertainties regarding their compatibility with our specific tasks."}, {"title": "Potential risks include", "content": "Shorter training durations may cause the models to underperform on complex, diverse datasets like ImageNet-100.\nThe use of pre-trained VAEs could constrain model performance if they are not fine-tuned for the target dataset.\nVariations in noise scheduling parameters can significantly alter inference performance and require careful tuning."}, {"title": "IX. FUTURE WORK", "content": "Building upon the challenges and insights highlighted in this study, several directions for future research and development can further optimize the performance and applicability of diffusion models. One significant area of improvement is extending training durations. Prolonged training schedules are essential to enable models to fully converge, refine noise prediction capabilities, and achieve superior image quality metrics such as FID and IS. Leveraging distributed training across multiple GPUs or cloud-based computational resources could make this feasible for larger datasets like ImageNet-100.\nAnother promising direction involves fine-tuning pre-trained Variational Autoencoders (VAEs) on task-specific datasets. While pre-trained VAEs simplify model integration, they may introduce biases or constraints when not adapted to the target data. Transfer learning approaches, such as retraining the VAE encoder and decoder on datasets like ImageNet-100, can better align latent representations with the diffusion model.\nFurther investigation is warranted into the sensitivity of classifier-free guidance (CFG) parameters. CFG plays a critical role in balancing fidelity and diversity in generated images, and systematic exploration of its weights across different datasets can provide valuable insights into achieving optimal results. Additionally, alternative noise scheduling strategies, such as exponential or adaptive schedules, should be explored. Noise scheduling profoundly affects the model's ability to balance global and fine-grained features during training, and novel schedules could dynamically adjust noise addition based on model performance at each timestep.\nScaling models to generate higher-resolution images, such as 256x256 or 512x512, is another critical avenue. This advancement is crucial for applications in industries such as design, gaming, and advertising. Combining improved latent diffusion techniques with larger UNet architectures and hierarchical noise reduction frameworks can make high-resolution synthesis viable. To enhance image realism, experimenting with adaptive loss functions, such as perceptual losses based on pre-trained feature extractors like VGG, could improve the balance between global structure and local details.\nGeneralization and robustness could also benefit from expanded data augmentation techniques. While current augmentations, such as horizontal flipping, are effective, introducing advanced methods like CutMix, RandAugment, or adversarial perturbations can expose the model to greater data variability, improving its robustness. Additionally, efficient sampling strategies should be explored to reduce the number of sampling steps while maintaining image quality. Hybrid approaches that combine deterministic and stochastic steps or leverage knowledge distillation techniques are potential solutions for reducing inference latency, which is crucial for real-time applications.\nFinally, the evaluation of models on diverse datasets beyond CIFAR-10 and ImageNet-100 is imperative. Validating performance on datasets with higher complexity or domain-specific constraints, such as CelebA-HQ, LSUN, or others, can further benchmark the model's robustness and generalizability.\nTranslating these advancements into real-world applications, such as augmented reality, robotic vision, or creative content generation, is a critical step toward bridging the gap between theoretical progress and practical utility. Collaborating with industry partners to identify use cases and optimize models for deployment constraints, such as edge computing or cloud platforms, will be essential for the broader adoption of diffusion models.\nBy addressing these areas, future work can advance the state of the art in diffusion models, bridging theoretical advancements with practical utility, and pushing the boundaries of generative AI."}, {"title": "X. CONCLUSION", "content": "This project set out to address the computational inefficiencies and scalability challenges inherent in diffusion models for high-quality image generation. The primary goal was to enhance Denoising Diffusion Probabilistic Models (DDPMs) and Denoising Diffusion Implicit Models (DDIMs) to achieve faster inference, improved image quality, and broader applicability across datasets such as CIFAR-10 and ImageNet-100. These challenges are critical to enabling diffusion models to scale effectively for real-world applications in fields like media content creation, gaming, and robotics.\nOur findings demonstrate significant progress toward these objectives. The integration of Classifier-Free Guidance (CFG) with DDIM resulted in faster inference and improved image fidelity, addressing the limitations of baseline DDPMs. The use of Variational Autoencoders (VAEs) facilitated efficient latent space compression, enabling training on larger datasets such as ImageNet-100 without compromising computational efficiency. Furthermore, the implementation of a cosine noise scheduler introduced smoother transitions during the denoising process, leading to better sample quality and more efficient training dynamics. These enhancements highlight the potential of combining advanced techniques to overcome the shortcomings of traditional diffusion models.\nHowever, the project also revealed challenges and areas for improvement. The integration of VAEs, while effective, was limited by constraints in training epochs and model capacity, underscoring the need for further optimization. Similarly, the trade-off between image diversity and fidelity when using the cosine noise scheduler indicates that additional tuning is required to fully realize its benefits.\nIn light of these findings, the project achieved its original goal of demonstrating enhancements to diffusion models and addressing key inefficiencies. While not all challenges were fully resolved, the advancements made in inference speed, sample quality, and computational efficiency provide a"}]}