{"title": "Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions", "authors": ["Amirali Sajadi", "Binh Le Anh Nguyen", "Kostadin Damevski", "Preetha Chatterjee"], "abstract": "The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness.\nOur findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings and present a CLI-based prompting tool that can be used to generate significantly more secure LLM responses.", "sections": [{"title": "1 Introduction", "content": "Large language Models (LLMs) have become deeply integrated into software engineering workflows, performing tasks such as code generation, summarization, debugging, and addressing queries related to programming (Liu et al., 2023a; Hou et al., 2023; Zheng et al., 2023; Belzner et al., 2023). In particular, LLM chatbots or conversational LLMs, such as OpenAI's GPT (OpenAI,\n2023), Anthropic's Claude (Anthropic, 2024), and Meta's Llama (Meta, 2024), have significantly impacted problem-solving activities by enabling interactive Q&As (Suad Mohamed, 2024; Das et al., 2024; Da Silva et al., 2024). Developers use them to describe symptoms, provide contextual information, and seek guidance on solutions (Hou et al., 2023). According to a 2023 survey, 92% of U.S.-based developers are using various generative models to perform or to automate some of their daily tasks (Shani, 2024).\nHowever, the rapid adoption of LLMs by software developers has raised many concerns regarding the security implications of using LLMs. A recent study found that participants using AI assistants produced code with significantly more vulnerabilities (Perry et al., 2023). Alarmingly, these participants were also more confident in the security of their code, suggesting that AI code assistants can foster a false sense of security, increasing the risk of introducing vulnerabilities into real-world software. Another study found that 32.8% of Python and 24.5% of JavaScript code produced by GitHub Copilot are vulnerable (Fu et al., 2023). These vulnerabilities, if exploited, can lead to severe consequences, such as the Log4Shell vulnerability (Kosinski, 2023). In 2024 alone, over 34,000 vulnerabilities were reported (CVE Program, 2024), highlighting the increasing frequency and severity of cybersecurity threats that endanger the safety, security, and reliability of software systems.\nBeyond generating vulnerable code, using LLMs can impact software security in more intricate and subtle ways. For instance, novice developers may unknowingly input insecure code (copied from Q&A forums) and ask LLMs to refactor and adapt it to their problem context. Similarly, during debugging, a developer might provide a block of code containing vulnerabilities, such as unsanitized user input in an SQL query, without being aware of its potential security implications. If LLMs fail to identify and address these vulnerabilities, developers may integrate flawed code into their projects, relying on the model without recognizing the potential security risks themselves. To better understand this phenomenon, let us consider the following example. In a Stack Overflow question, a developer asks for help to resolve an issue with writing to a file:\nI have a text file with one URL per line, like:\nhttps://www.google.com"}, {"title": "Do LLMs Consider Security?", "content": "Here, the developer is focused on removing the extra %0A characters, which could be resolved using the strip() function i.e., requests.get(a.strip(),\ntimeout=(2,5), verify=False)). However, they fail to notice a significant\nsecurity risk posed by the verify=False parameter. Disabling SSL certificate verification is generally considered poor security practice and exposes the application to serious risks, including man-in-the-middle attacks, where an attacker could intercept or manipulate the data sent over HTTPS. When prompted with this question, GPT-4, Claude 3, and Llama 3 all correctly explained the issue with %0A and suggested using the strip() function to fix it. However, none of the LLMs mentioned the security implications of the verify=False parameter, leaving the developer unaware of the potential vulnerability. By failing to inform the user of this vulnerability, the LLM responses can indirectly reinforce the faulty implementation and, in many cases, further build upon it, perpetuating insecure coding practices.\nSeveral studies have explored the potential risks associated with the use of LLMs and examined concerns regarding the generation of insecure code (Pearce et al., 2022; Siddiq and Santos, 2022; Khoury et al., 2023; Siddiq et al., 2024b), inaccuracies in vulnerability detection (Ullah et al., 2024; Akuthota et al., 2023; Purba et al., 2023; Zhou et al., 2024b), and potential misuse for offensive applications, ranging from hardware to user-level attacks (Happe and Cito, 2023; Falade, 2023). However, most studies have focused on the security of LLM-generated code, often neglecting the natural language generated by LLMs that plays a critical role in interactive learning and problem-solving (Eastman, 2023; Xia and Zhang, 2023; Hao et al., 2024). Additionally, unlike prior research that focuses on detecting vulnerabilities in LLM-generated code or evaluating the capabilities of LLMs when explicitly tasked to detect vulnerabilities, our work investigates the ability of LLMs to proactively identify vulnerabilities in user-supplied code. This reflects real-world use cases where developers that rely on LLMs for various tasks inevitably prompt LLMs with vulnerable code. Our study is the first to assess the security awareness of LLMs using the textual information provided by the LLMs alongside or independent of code. We assess not only LLMs' ability to proactively detect"}, {"title": "2 Motivational Study", "content": "The 2023 JetBrains survey, based on responses from 26k developers across 196 countries, reveals that 77% (i.e., approximately three in four developers) use ChatGPT (JetBrains, 2023). Given its widespread adoption, we conducted a exploratory study to explore whether security considerations naturally emerge in developer conversations with ChatGPT. This motivational study serves as an initial gauge to determine if LLMs engage with security topics at all, helping us assess the feasibility of a larger, more comprehensive study. By examining"}, {"title": "3 Methodology", "content": "Building upon our findings from the motivational study, we designed an experimental study to systematically evaluate the security awareness of three prominent LLMs, Claude 3, GPT-4, and Llama 3. Our goal is to determine whether LLMs can proactively detect vulnerable code in prompt inputs, and how consistently they issue security warnings and relevant vulnerability-related information to the users. To this end, we first collected Stack Overflow (SO) questions containing vulnerable code snippets, which we then used as prompts for the LLMs. We qualitatively analyzed the LLM responses to address our research questions.\nBy examining whether an LLM issues a warning about the security of the code in the SO question, we evaluate the LLM's security awareness. When an LLM not only answers the question but also highlights the security issue in the code, it demonstrates a high level of security awareness. Conversely, if the LLM addresses the question without mentioning security, it demonstrates a low level of security awareness. This premise forms the basis for our assessment of LLMs' security awareness in answering developers' questions. Figure 2 provides an overview of our approach, which we discuss in detail next."}, {"title": "3.1 Dataset Collection and Refactoring", "content": "Data Collection. We collect a dataset of SO questions with insecure code but without any explicit mention of security within the questions themselves. Figure 3 shows an example of such a question, where the JavaScript code uses the eval function to dynamically create a variable based on user input, introducing a significant risk of code injection or a cross-site scripting (XSS) attack. The developer posting the question seems unaware of the related security issue; however, one of the respondents points this out in a comment \"You have a pretty nasty XSS vulnerability here\".\nHowever, not all SO questions containing vulnerable code receive responses that point out the security implications of the code. In fact, earlier studies highlight the concerns with developers copy-pasting insecure code from SO posts (Fischer et al., 2017). Therefore, we aim to create two datasets for our study:\n(a) Mentions-Dataset: a set of questions that received responses (either answers or comments) mentioning and/or addressing the security concerns in the question's code (as shown in Figure 3), (b) Transformed-Dataset: a set of questions that despite containing vulnerable code, do not receive such responses. The code in this dataset has been transformed (Later detailed in this section) to ensure that it is not easily recognizable for the LLMs.\nThe Mentions-Dataset represents a best-case scenario where the vulnerability has already been highlighted by the community, and the code appears exactly as it was originally posted on SO. This allows us to observe how LLMS respond to questions that both exist in their training data and include explicit security warnings. In contrast, the Transformed-Dataset intentionally excludes any mention of security in the responses and applies code refactoring to reduce the chance of data leakage from the LLM's training data. By comparing these two datasets, we can examine LLM behavior across a spectrum of possible real-world situations: from code that has been explicitly marked as vulnerable to code that has not been associated with any vulnerabilities by the SO responses. This contrast helps us assess not only how well LLMs leverage familiar content (Mentions-Dataset) but also their ability to general-"}, {"title": "Data Refactoring.", "content": "One concern when testing the performance of LLMs on these questions is the possibility that the LLM may have been trained on them (AI Stack Exchange, 2023). This is a deliberate attribute of the Mentions-Dataset, where prior exposure to the questions, their code, and associated responses is expected and intended. The purpose of this dataset is to examine LLM behavior in scenarios where data leakage is likely, allowing us to evaluate how well LLMs leverage familiar content and existing security discussions.\nIn contrast, to examine the LLMs' ability to generalize to new or unseen inputs, we applied code refactoring techniques to the Transformed-Dataset in line with prior research (Steenhoek et al., 2024). Even when explicit mentions of vulnerabilities are absent in SO responses, similar or identical vulnerable code could exist on other platforms, such as GitHub repositories or coding tutorials, and these resources can potentially be included in the LLM's training data. Therefore, to minimize the possibility of data leakage and maximize the generalizability of our evaluation, we applied meaning-preserving transformations to the vulnerable code snippets. These transformations help reduce input similarity and overlap with the LLM's training data. These transforma-"}, {"title": "3.2 Experimental Setup", "content": "After finalizing the datasets, we used the SO questions as prompts for the three LLMs. Each SO post served as a single prompt, and the responses generated by each LLM were subsequently collected for analysis. For this study, we utilized three prominent LLMs: Llama 3 with 70 billion parameters, Claude 3 Opus, and GPT-4 Turbo.\nLlama 3 was run locally on a server, while GPT-4 Turbo and Claude 3 Opus were accessed via their official APIs. To ensure more deterministic answers, all models were configured to generate content with a temperature of 0.1, in line with previous studies (Savelka et al., 2023; Siddiq et al., 2024a). Additionally, the top-p parameter was left at its default value, as recommended by the OpenAI (2024) documentation.\nRQ1: Given insecure code, do LLMs warn developers of the potential security flaws or required security modifications? To address RQ1, we conducted a qualitative analysis of the responses generated by each LLM for all 300 questions across our two datasets. Thus, this process involved analyzing a total of (300*3) = 900 LLM-generated responses.\nIn order to gain deeper insights into the LLM responses and behaviors, we analyzed and compared the performance of the three LLMs in terms of their ability to identify the vulnerabilities and subsequently inform the users about them. Three authors manually inspected all 900 LLM responses, and evaluated whether each response contains a warning about the security vulnerability or if it only answers the main question.\nAdditionally, we performed a comparative analysis of the LLMs' performances on questions from Mentions-Dataset against those from Transformed-Dataset. This comparison aimed to highlight any potential influence of the LLMs' training data on their performance. By examining these differences, we sought to understand the extent to which prior exposure to similar questions may affect the LLMs' ability to recognize vulnerabilities in new inputs and warn the users about them.\nRQ2: In instances where users are reminded of security concerns, are they informed about the causes, potential exploits, and possible fixes of the vulnerabilities? To answer RQ2, we qualitatively analyzed all 900 LLM-generated, and 150 user-provided responses that contained security warnings. Our goal was to evaluate how effectively each response can increase user's awareness of the"}, {"title": "4 Results", "content": "4.1 RQ1: Given insecure code, do LLMs warn developers of the potential security flaws or required security modifications?\nIn Table 3 presents the number and percentage of questions in which LLMs warned users about the security vulnerabilities of their code, across the two"}, {"title": "4.2 RQ2: In instances where users are reminded of security concerns, are they informed about the causes, potential exploits, and possible fixes of the vulnerabilities?", "content": "Figure 5 illustrates the responses of Llama 3, Claude 3, and GPT-4 Turbo, to the SO question depicted in Figure 3. All three LLMs mention the insecure nature of the question's code and its cause. Llama 3 and GPT-4 offer fixes to the vulnerability, while GPT-4 also explains the possible exploit.\nFigure 6 illustrates the extent of vulnerability-related information (causes, exploits, and fixes) provided by SO users and each LLM for all 300 questions. The results are divided into two sections corresponding to the two datasets. Results for Mentions-Dataset also include the analysis of SO responses that contain security warnings.\nFigures 7 and 8 present the co-occurrence of vulnerability-related informa-tion (causes, exploits, and fixes) in responses from SO users and each LLM"}, {"title": "5 Implications", "content": "Our findings demonstrate critical considerations for leveraging LLMs in SE context. Our study reveals both the potential and limitations of current LLMs in addressing vulnerabilities and informing developers about them. In this section, we discuss actionable insights for developers and model designers aiming to enhance the security awareness of LLMs. These insights not only highlight"}, {"title": "5.1 Implications for Software Engineers", "content": "Our study highlights the critical need for developers to be vigilant about the security of the programming solutions suggested by LLMs. Notably, in RQ1 we found that LLMs rarely issue security warnings autonomously, identifying vulnerabilities in fewer than 40% of cases under the more favorable experimental settings. This limited detection rate highlights a critical risk: engineers relying solely on LLMs may overlook key security flaws, leaving code vulnerable to exploitation.\nOne way to make LLM responses more secure could be by providing specific instructions in the prompt (Jensen et al., 2024). However, several studies show that developers struggle to create effective LLM prompts (Chopra et al., 2023; Nam et al., 2024). Therefore, we experimented with brief prompt additions that would be straight-forward for developers to implement.\nWe sampled 50 questions from our datasets where none of the LLMs issued any security warnings. To ensure representativeness, we selected 25 instances from the Mentions-Dataset and 25 instances from the Transformed-Dataset. Further, these questions were sampled proportionately to the distribution of vulnerabilities within the corresponding subsets of questions. Next, these prompts were fed to GPT-4, the best performing model in our experiments.\nWe experimented with multiple prompt structures, such as appending the phrase \"Be mindful of potential security implications.\" or \u201cMake the code secure.\", aiming to find a structure that not only maximized the likelihood of LLMs addressing security vulnerabilities but was also concise enough for practical use by developers. We found that the best performing technique included appending the phrase \u201cAddress security vulnerabilities\u201d to the end of each question. Consequently, we used all modified questions as prompts for three LLMs, resulting in 50 responses. Next, we analyzed the LLM responses to these questions with respect to the RQ1 and RQ2 metrics. The analysis of these 50 responses was performed by two of the authors and followed the same methodology used in answering RQ1 and RQ2 in the main study.\nOut of the 25 questions from the Mentions-Dataset, GPT-4 was able to provide security warnings for 19 questions. For the Transformed-Dataset, there was a much less notable enhancement in performance; GPT-4 issued security"}, {"title": "5.2 Implications for Tool Integration in Software Development", "content": "To address the limitations of LLMs in identifying and mitigating vulnerabilities, alternative approaches have been proposed. For instance, Pearce et al. demonstrated integrating outputs from static analysis tools, such as CodeQL and C sanitizers, into LLM prompts as a zero-shot strategy for fixing vulnerabilities Pearce et al. (2023). Building on the same idea, we explored using this technique to examine how well LLMs provide relevant security warnings and information when responding to general programming questions.\nWe use the same 50 questions used in Section 5.1 to experiment with this approach. Having analyzed each question's extracted code snippets with CodeQL, we add CodeQL's description of the detected vulnerabilities to the end of each question, using the phrase \"Address security vulnerability: <Vulnerability Description #1> Address security vulnerability < Vulnerability Description #2>...\". After prompting the GPT-4 with these prompts, the LLM responses were again analyzed with respect to RQ1 and RQ2. Here, similar to the previous section, the authors manually carried out the analysis.\nUpon providing GPT-4 with the output of CodeQL, it was able to identify and address vulnerabilities in 24/25 questions from the Mentions-Dataset."}, {"title": "5.3 Implications for LLM Designers", "content": "Overall, our findings highlight substantial gaps in the security awareness of LLMs, emphasizing the need for design and training improvements. Based on the results of RQ1, the best performance we observed in issuing security warnings was only 40%, which declines even further in the Transformed-dataset, dropping to as low as 12.6%. This significant disparity in performance across datasets suggests that LLMs require substantial modifications to improve the attention to security in their responses. Addressing these gaps might involve acquiring additional high-quality training data, or domain-specific fine-tuning for SE.\nMoreover, our study suggests that LLMs' lack of attention to security extends beyond code generation to the natural language they produce in software engineering-related discussions. This broader issue indicates the need to evaluate the security of LLM-generated content, particularly in the explanations, advice, and recommendations provided to developers. Future research could establish standardized methods for assessing LLM security awareness within software engineering contexts, focusing not only on the code produced but also on the security of LLM-provided guidance. This dual approach to security encompassing both code and conversational context could enable more robust, security-conscious LLM designs that better serve practitioners in high-stakes environments."}, {"title": "6 Threats to Validity", "content": "Internal Validity: An LLM may appear to address the security flaws within a given prompt; however, if the prompt was a part of the LLM's training data, the generated response could merely reflect that information, and not the model's generalizability. To counteract this threat, we created the Transformed-Dataset. This dataset includes transformed SO questions devoid of any security mentions, either in questions or responses. Therefore, we expect none of the three LLMs to have encountered these specific questions or related security concepts, allowing us to assess their ability to address security issues in new data.\nExternal Validity: The results of our study may not generalize to all LLMs or programming questions. For instance, the inclusion of other LLMs, such as code-specific models that place a heavier emphasis on code and provide more concise natural language output, is outside the scope of our study and"}, {"title": "7 Related Work", "content": "Researchers have studied the adoption of LLMs in security-related software engineering tasks. Multiple studies have explored the possibilities and limitations of LLMs in vulnerability detection (Zhou et al., 2024b; Noever, 2023; Bakhshandeh et al., 2023; Purba et al., 2023; Cheshkov et al., 2023; Wang et al., 2023; Liu et al., 2023b). Some of these studies have demonstrated the superior performance of LLMs compared to traditional methods such as static analysis tools (Zhou et al., 2024b; Noever, 2023; Bakhshandeh et al., 2023), while others have highlighted the shortcomings of LLMs, particularly their tendency to generate false positives (Purba et al., 2023; Cheshkov et al., 2023; Banerjee et al., 2023). Additionally, LLMs have been employed to detect malicious code (Henrik, 2023; Eli and Gil, 2023), generate test cases (Yao et al., 2023; Zhang et al., 2023), and to fix defective and/or vulnerable code (Jiang et al., 2023; Pearce et al., 2023; Xia et al., 2022; Jin et al., 2023). Beyond the use of LLMs in performing security-related tasks, researchers have also studied the security issues caused due to the use of LLMs. We discuss some of these studies in Sections 7.1 and 7.2."}, {"title": "7.1 Security Concerns with using LLMs", "content": "Numerous studies studied the security of the code generated by LLMs. Pearce et al. (2022) examined security of the code generated by GitHub's Copilot (GitHub and OpenAI, 2024). They found that around 40% of the code generated by Copilot contained vulnerabilities when analyzed with CodeQL (GitHub, 2022). Other studies have also evaluated the security of code from"}, {"title": "7.2 Stack Overflow vs. LLMs: Security Implications", "content": "Popularity of Stack Overflow has been significantly impacted by the emergence of LLMs(Da Silva et al., 2024). Despite the varying quality of LLM-generated answers, many developers prefer ChatGPT due to their well-articulated language (Kabir et al., 2024). While a rich body of literature on SO and the security concerns associated with its usage remains (Yang et al., 2016; Licorish and Nishatharan, 2021; Fischer et al., 2017; Zhang et al., 2018), researchers are still exploring the consequences of relying on LLM responses instead of user-provided SO responses.\nDa Silva et al. (2024) compared the reliability of LLM-generated responses to user-provided answers on SO. Their results indicate that despite a high degree of textual similarity, the LLM responses are not reliable. Furthermore, they report a considerable decline in user activity on SO since the introduction of ChatGPT. In another study, ChatGPT was prompted with 517 SO questions, revealing that 52% of the responses contained incorrect and fabricated information(Kabir et al., 2024). Delile et al. (2023) have shown that 74% of the user-provided responses and 71% of the ChatGPT-generated responses to privacy-related SO questions are accurate. In another study, Hamer et al. (2024) discovered multiple vulnerabilities in ChatGPT's code when responding to SO security questions. While they focused on whether LLMs generate"}, {"title": "8 Conclusions", "content": "This paper presents the first study to examine the security awareness of three popular LLMs when answering programming-related questions. In our motivational study, ChatGPT demonstrated potential in offering developers relevant security-related information and context-based solutions for writing secure code. These findings inspired us to conduct a deeper investigation into the capabilities of other popular LLMs. We prompted three popular LLMs, GPT-4, Llama 3 and Claude 3, with SO questions containing vulnerable code and evaluated their responses.\nOur results indicated the underwhelming performance of all three LLMs in pointing out the security vulnerabilities. We observed even lower performances when prompting LLMs with questions from Transformed-Dataset which, indicates the limitations of these models in generalizing their learned data. Additionally, we observed that LLMs are more likely to point out certain vulnerabilities, specially those related to the improper management and protection of sensitive information (e.g., CWE-532, CWE-321, CWE-798) compared to those involving external control of file names or paths (e.g., CWE-400).\nWhile some of our findings align with existing research on security flaws in LLM-generated content, our approach addresses a crucial gap by focusing on real-world scenarios where developers seek general coding assistance without explicitly considering security, which is common in interactions. By selecting prompts without explicit security mentions, we simulate typical developer interactions in which security concerns are often overlooked, highlighting a key risk: LLMs often overlook insecure practices and inadvertently reinforce them. This insight, when coupled with the prompting techniques we propose, could help developers elicit more security-aware responses from LLMs, thus promoting secure coding practices.\nIn general, our findings have the following key implications. For software engineering practitioners, prompt engineering can be a practical tool to promote security-aware LLM outputs, though limitations remain; integrating tools like CodeQL may further aid developers in outlining vulnerabilities. For researchers, the results point to the importance of evaluating both code and natural language guidance from LLMs and highlight opportunities for fine-tuning models to generate more security-aware responses. For LLM designers, there is a need"}]}