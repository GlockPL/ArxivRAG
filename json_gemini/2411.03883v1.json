{"title": "MEG: Medical Knowledge-Augmented Large Language Models\nfor Question Answering", "authors": ["Laura Cabello", "Carmen Martin-Turrero\u2020", "Uchenna Akujuobi\u2020", "Anders S\u00f8gaard", "Carlos Bobed"], "abstract": "Question answering is a natural language\nunderstanding task that involves reason-\ning over both explicit context and un-\nstated, relevant domain knowledge. Large\nlanguage models (LLMs), which underpin\nmost contemporary question answering sys-\ntems, struggle to induce how concepts relate\nin specialized domains such as medicine.\nExisting medical LLMs are also costly to\ntrain. In this work, we present MEG,\na parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses\na lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling\nit to leverage external knowledge in a cost-\neffective way. We evaluate our method\non four popular medical multiple-choice\ndatasets and show that LLMs greatly bene-\nfit from the factual grounding provided by\nknowledge graph embeddings. MEG at-\ntains an average of +10.2% accuracy over\nthe Mistral-Instruct baseline, and +6.7%\nover specialized models like BioMistral. We\nalso show results based on Llama-3. Finally,\nwe show that MEG's performance remains\nrobust to the choice of graph encoder.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) induce knowledge\nfrom vast text corpora. Through self-supervised\nlearning, these models capture deeply contextu-\nalized representations of input tokens that enable\nthem to generalize to new tasks with remarkable\nperformance. This, as well as their ability to\nwrite long coherent passages, has made LLMs\nincredibly popular, despite their considerable in-\nference costs (Cheng et al., 2023) and their con-\ncerning carbon footprint (Strubell et al., 2019).\nMoreover, current LLMs face significant chal-\nlenges with handling complex reasoning and en-\nsuring trustworthiness (Liu et al., 2023; Huang\net al., 2024) and factual consistency (Maynez\net al., 2020; Zhou et al., 2023; Tam et al., 2023;\nHager et al., 2024), essential to critical fields like\nhealthcare. While LLMs are poised to revolution-\nize our medical system, already performing well\non medical licensing exams (Jin et al., 2020; Pal\net al., 2022; Singhal et al., 2023a; Brin et al., 2023)\nand other tasks (Nazario-Johnson et al., 2023;\nVan Veen et al., 2023; Tu et al., 2023; Carl et al.,\n2024), there is still much room for improvement.\nTo improve reliability and reduce computational\ncosts, researchers have experimented with train-\ning from mixtures of corpora and knowledge bases\n(Pan et al., 2023, 2024). Knowledge Graphs\n(KGs), such as the Unified Medical Language Sys-\ntem (UMLS) (Bodenreider, 2004), are structured\nknowledge bases that explicitly store rich factual\nknowledge. KGs are good at capturing the nu-\nances of complex data and can provide comple-\nmentary information to LLMs, especially useful\nfor tasks requiring structured understanding. The\npotential of knowledge-augmented LLMs\u00b9 out-\nlines an interesting research paradigm that can al-\nleviate current challenges of LLMs, and reduce\nthe need of training ever-larger models (Hooker,\n2024). However, how to effectively model inter-\nactions between LLMs and KGs remains an open\nquestion.\nRecent efforts have focused on self-supervised\nmethods for jointly training graph neural networks\nand pretrained language models (Yang et al., 2021;\nChien et al., 2022; Brannon et al., 2024). Others"}, {"title": "Related Work", "content": "Medical Language Models Current state-of-\nthe-art (SOTA) in medical QA benchmarks like\nMedQA (Jin et al., 2020), PubMedQA (Jin et al.,\n2019) or MedMCQA (Pal et al., 2022) belongs to\nclose-sourced models of unknown size like Med-\nGemini (Saab et al., 2024), Med-PaLM2 (Sing-\nhal et al., 2023b) or GPT-4 (Nori et al., 2023).\nPopular open-source LLMs in biomedicine in-\nclude MedAlpaca (Han et al., 2023) and PMC-\nLLaMA (Wu et al., 2023) based on Llama (Tou-\nvron et al., 2023a), MediTron (Chen et al., 2023)\nbased on Llama-2 (Touvron et al., 2023b), or\nBioMistral (Labrak et al., 2024) based on Mistral-\nInstruct (Jiang et al., 2023). These models con-\ntinue pretraining the base general-purpose models\non curated medical corpora. More recently, Kim\net al. (2024) present the Meerkat models trained\nwith chain-of-thought (Wei et al., 2024) synthetic\ndata. Meerkat-7B outperforms the previous best\n7B models across several medical benchmarks.\nHowever, it takes eight 80G A100 GPUs and 1.5\ndays to complete training. In contrast, our ap-\nproach is the first to leverage pretrained medical\nKGEs and can be trained on four A10G GPUs\nwithin a few hours (see \u00a7 5 for details).\nKnowledge-Augmented\nLanguage Models\nBringing together LLMs and KGs is an active line\nof research that has gained increasing attention\nfrom both academia and industry (Pan et al.,\n2023, 2024). Among numerous efforts in this\narea, Zhang et al. (2019); Yasunaga et al. (2022);\nTang et al. (2024); Zhu et al. (2023), to name a\nfew, propose different methods for combining text\nand graphs during pretraining. Parallel to these\nlines of work, Sarmah et al. (2024); Edge et al.\n(2024); Hu et al. (2024); Mavromatis and Karypis\n(2024) approach the integration of LLMs and KGs\nthrough retrieval-augmented generation (RAG)\n(Lewis et al., 2020). However, the deployment of\nsuch knowledge-augmented LLMs for medical\nQA remains understudied. Our work fills this\ngap and presents a novel approach to medical\nknowledge-augmented LLMs based on KGES.\nWe note that MEG may resemble a sort of RAG\nsystem, where an LLM leverages knowledge from\nan external database of KGEs. In this case, the\ngrounding module would act as retrieval module,\nfetching appropriate KGEs that ground text\ninformation in KG entities as part of a prompt."}, {"title": "Problem Formulation", "content": "We augment an LLM with KG embeddings to an-\nswer medical questions drawing on factual knowl-\nedge from the KG. We rely on a large KG in\nthe target domain, namely UMLS (Bodenreider,\n2004). Our proposed approach, MEG, consists of\nfour key components:\ni) A KG encoder to represent knowledge graph\nentities in a continuous vector space, while\npreserving their semantic meaning.\nii) An instruction-tuned language decoder ca-\npable of generating textual answers.\niii) A mapping function fk that transforms the\noutput of the KG encoder into a represen-\ntation that can be used by the language de-\ncoder. fk is parameterized by a neural net-\nwork. Thus, we interchangeably use the term\nmapping network.\niv) A KG grounding module that detects textual\nentities and grounds them in graph entities.\nFigure 1 depicts the full pipeline of MEG. We\ncarefully investigate the design of these compo-\nnents and how they interact with each other (\u00a7 4.1).\nTo attain the best accuracy on downstream tasks,\nwe conduct a two-phase training (\u00a7 4.2).\nDefinitions. A generic dataset for multiple-\nchoice question answering (QA) consists of ex-\namples with a context paragraph, a question and a\ncandidate answer set, all expressed in text. Given\na QA example, each prompt W is the concatena-\ntion of context, question and candidate answer set.\nWe denote the sequence of tokens (words) in W as\n{W1,...,Ws}, where S is the maximum sequence\nlength. We denote the sequence of tokens (vec-\ntors) in the language model embedding space as\nWe = {We1,...,Wes}.\nWe define a knowledge graph (KG) as a directed\ngraph G = (V, E), where V is the set of entity\nnodes, and E\u2286V\u00d7R\u00d7V is the set of edges\n(triples) that connect nodes in V, with R being the\nset of relation types. Each triple (s, p, o) in a KG\nrepresents a knowledge fact, such as (HEADACHE,\nIS_A, CEPHALGIA). A KGE e is a mathematical\nrepresentation that maps each entity v \u2208 V and\neach relation r\u2208 R of a directed knowledge graph\nG to low-dimensional vectors in R9, preserving\nthe semantic relationships within the graph.\nFinally, we define a KGE-augmented language\nmodel to be a function $f\u2081(W_e\u2295 f_k(X))$, with fi \u2208\n$R^1$, where fk(X) is a set of KGEs, {$e_1,...,e_n$}\nwith $e_i$ \u2208 $R^9$, that has been mapped to the LLM's\nspace using a learned mapping function fk : $R^9$ \u2192"}, {"title": "Method", "content": "4.1 MEG\nMEG combines a pretrained KG encoder and a\npretrained LLM by means of an intermediate map-\nping network (see Figure 1). The KG encoder,\nwhich is trained separately on a large medical\nKG\u00b3, provides graph embeddings that are directly\nfed to the mapping network. Then, the LLM uses\nthe text content and mapped KG embeddings as\ninput to generate an answer.\nKnowledge Graph Encoder The KG encoder\nis trained up-front over the selected graph to gen-\nerate KGEs. We choose GraphSAGE (Hamilton\net al., 2017) as our preferred KG encoder. In \u00a7 6.1\nwe present an ablation study with random-walk-\nbased, energy-based translational, and message-\npassing encoders.\n= 4096 after a series of non-\nMapping Network The mapping function fk\ntransforms a sequence of graph features from the\nKG encoder into a sequence that can be consumed\nby the LLM. We parameterize fk as an MLP with\nfour hidden layers of size dh = 128. In particu-\nlar, a set of graph embeddings is transformed from\ndg = 256 to d\u2081\nlinear transformations through the hidden layers\nof fk. We denote the initial embedding sets as\nX = {$x_i$}=$_{i=1}^N$, Y = {$Y_j$}=$_{j=1}^N$, $Xi$ \u2208 $Rd^g$, $Yj$ \u2208 $Rd_1$,\nbeing $x_i$ the KGEs, yj the averaged token embed-\ndings of the entity in the LLM, and N the to-\ntal number of graph embeddings (entities). We\nfurther denote the set of mapped embeddings as\nfk(X) := {$fk(xi)$}=$_{i=1}^N$.\nThe goal is to learn the mapping fk that trans-\nforms X to the LLM's vector space, while preserv-\ning its semantic meaning and structural informa-\ntion. Rather than minimizing the sum of squared\ndifferences between fk(X) and Y, we aim at po-\nsitioning each xi in the neighborhood of its coun-\nterpart in Y. Pursuing an exact matching of space\ndistributions, such as through a Procrustes trans-\nformation (Sch\u00f6nemann, 1966; Gower, 1975),\nwould disregard the structural knowledge encoded\nin X.\nTo achieve this, we design an architecture sim-\nilar to Xu et al. (2018) with two mappings fk :\nX \u2192 Y and gk : Y \u2192 X, as illustrated in Fig-\nure 2. We construct an instruction dataset with la-\nbels from UMLS's entities to teach the LLM to in-\nterpret the transformed graph embedding fk(xi).\nFigure 2 shows an example of an instruction,\nwhere the placeholder {kg_embedding} is re-\nplaced by fk(xi). We train the full network jointly\nwith the LLM. Our loss function consists of three\nparts: a standard next-token prediction objective\n(cross-entropy loss Lce), and a sum of a con-\ntrastive loss and a back-translation loss to optimize\nthe mapping network. Specifically,\n\u2022 Given a batch X including a positive pair\nof examples xi and xj, a contrastive objec-\ntive (Hadsell et al., 2006) is a function whose\nvalue is low when xi is similar to xj and dis-\nsimilar to all others, which are considered\nnegative pairs for xi. We employ a popu-\nlar contrastive self-supervised learning objec-\ntive (Sohn, 2016; van den Oord et al., 2019;\nHe et al., 2019), dubbed as NT-Xent loss by\nChen et al. (2020). NT-Xent uses dot prod-\nuct as similarity measure, and computes a\nnormalized temperature-scaled cross-entropy\nloss for a positive pair as follows,\n$l_{i,j} = -log \\frac{exp(x_i \\cdot x_j / \\tau)}{\\sum_{k=1, k \\neq i}^B exp(x_i \\cdot x_k / \\tau)}$         (1)\nwhere B is the batch size and 7 is the temper-\nature. We set the hyper-parameter T = 1.05.\nThe final loss Le is computed across all\npositive pairs in a batch, summed across\nall batches. Intuitively, the contrastive loss\nserves as an unsupervised objective function\nfor training the network to bring similar enti-\nties closer together in Y and push dissimilar\nones apart."}, {"title": "Training", "content": "4.2\nWe aim to train MEG to achieve competent re-\nsults on medical question answering benchmarks\nwhile minimizing computational cost. To do this,\nwe conceive a two-phase training strategy with a\nminimal part of the model's parameters updated.\nPhase I: Embedding Transfer Learning We\nfirst learn the optimal transformation fk : X \u2192 Y\nso that the mapped KG embeddings retain rele-\nvant information from the KG and can be effec-\ntively used by the LLM. As explained in \u00a7 4.1, we\ncreate an instruction dataset to guide the LLM in\nlearning the relationship between its original rep-\nresentation of medical entities and their mapped\ngraph embeddings. The train set contains 297,927\nexamples, following the same template shown in\nFigure 2 for every entity label in UMLS.7 We train\nfor one epoch jointly the mapping network and the\nLLM to minimize the objective from Eq. 3.\nPhase II: Downstream task Given a medical\nmultiple choice QA dataset, we fine-tune MEG\nto answer the input question based on the tex-\ntual content and information leveraged from the\nmapped KG embeddings. We format the input\nprompts Was follows. For each example in\nthe dataset, we concatenate the context (if any),\nquestion and candidate answer set following the\npseudo-code shown in Figure 3. The placeholder\n{kg_embedding} is replaced with N KGES\ntransformed by the mapping network. We assume"}, {"title": "Experimental Details", "content": "5\nData Following previous research on medical\nLLMs, we evaluate MEG on four well-known\nmedical benchmarks that require extensive back-\nground knowledge. The first one is MedQA-\nUSMLE (MedQA) (Jin et al., 2020), which con-\nsists of 10,178 train questions and 1,273 test ques-\ntions, formatted with four choices each. The con-\ntent was originally curated by experts from the US\nMedical License Exam. The second benchmark,\nPubMedQA (Jin et al., 2019), was collected from\nPubMed abstracts and includes 1,000 expert la-\nbeled question-answer pairs. The task is to pro-\nduce a yes/no/maybe answer based on the question\nand an abstract as context. As previously done by\nothers (Singhal et al., 2023a; Chen et al., 2023;\nLabrak et al., 2024), we use 500 random sam-\nples for evaluation. The remaining 500 samples,\nthough limited in size, serve as our only source of\ntraining data. We exclude the 211k artificially la-\nbeled yes/no samples provided by Jin et al. (2019)\nto avoid bias towards these two options. The third\nbenchmark, MedMCQA (Pal et al., 2022), con-\ntains 179,72210 train questions from Indian med-\nical entrance exams. Due to the unavailability\nof answer keys for the test set, we follow oth-\ners (Wu et al., 2023; Tu et al., 2023; Labrak\net al., 2024) and report results on the validation\nset (4,183 questions). Lastly, MMLU-Medical\n(Singhal et al., 2023a) includes 1,089 questions,\neach with four options, across six medical and\nbiology-related categories drawn from Hendrycks\net al. (2021). Since this dataset only provides test\ndata, we evaluate the generalization performance\nof MEG fine-tuned on MedMCQA as in (Chen\net al., 2023). Thus, results on MMLU-Medical re-\nport out-of-distribution inference.\nTraining Details We initialize the KG node em-\nbeddings with token embeddings from SapBERT\n(Liu et al., 2020). SapBERT leverages contextu-\nalized embeddings from a pretrained BERT-based\nlanguage model for biomedical KGs like UMLS.\nThis initialization leads to improved performance\ncompared to random embedding initialization. We\ntrain GraphSAGE with same hyperparameters as\nin Hamilton et al. (2017).\nDuring phase I of training, described in \u00a7 4.2,\nwe randomly initialize the mapping network and\nload the pretrained weights of the LLM. We fully\ntrain the mapping network and perform low-rank\nadaptation (LoRA, Hu et al. (2022)) fine-tuning on\nevery linear layer of the LLM, while the remaining\nparameters are frozen. This parameter-efficient\ntuning approach allows to learn the equivalent of\n2% of the model's parameters. Our full archi-\ntecture results on 216M trainable parameters for\nMEG-MISTRAL. After fine-tuning, we merge the\nLLM's updated parameters with the base model.\nSince the input prompt has a fixed size (see Fig-\nure 2), we use a reduced sequence length (124)\nto optimize computational efficiency. We train for\none epoch with gradient accumulation over 8 steps\nto achieve an effective batch size of 128. We em-\nploy a cosine learning rate scheduler with learning\nrate of 1e - 5, warmup ratio of 3% and no weight\ndecay. We use mixed precision (bfloat16) and\nFlashAttention2 (Dao, 2023) to optimize memory\nusage and speed up computations on the LLM.\nTraining takes 4h on 4 NVIDIA A10G GPUS us-"}, {"title": "Results", "content": "6\nWe evaluate accuracy on four medical multiple-\nchoice question datasets in three variants of MEG:\nMEG-MISTRAL1 and MEG-MISTRAL3, based\non Mistral-7B-Instruct-v0.1 and -v0.3, respec-\ntively; and MEG-LLAMA, based on Llama-3-\nInstruct. We report average accuracy and standard\ndeviation across three random seeds. Our results\nin Tables 1 and 2 reveal consistent average im-\nprovement across datasets compared to baselines.\nIn-prompt graph triples provide useful infor-\nmation We investigate whether the inclusion\nof KG information can positively influence the\nLLM's answers. To establish a primary baseline,\nwe take Mistral-Instruct-v0.1 and MedQA as a\nrunning example. For each question, we select a\nmaximum of 10 named entities s and randomly re-\ntrieve 2 graph neighbors o for each, resulting in a\nmaximum of 20 graph triples (s, p, o). We include\nthem as part of the prompt, in natural language13."}, {"title": "Ablation study", "content": "6.1\nIn this section, we evaluate how the choice of\ngraph encoder and mapping network architec-\nture impact MEG-MISTRAL1's performance on a\ndownstream task (case study on MedQA).\nOn the impact of the graph encoder We train\nencoders based on random-walk (RDF2Vec (Ris-\ntoski and Paulheim, 2016)), energy (DistMult\n(Yang et al., 2015)) and message-passing (Graph-\nSAGE (Hamilton et al., 2017) and eGraphSAGE,\nan edge-type-aware variant inspired by Hu et al.\n(2020)'s adaptation). Along with their impact in\nMEG's performance, we include a link classifica-\ntion task as a proxy to evaluate their capabilities.\nSince these encoders are fundamentally distinct,\nthey capture diverse graph properties, as reflected\nin classification accuracy in Figure 4, plain (or-\nange) bars. eGraphSAGE stands out with a con-\nsiderably higher score (73.9), as it naturally inte-\ngrates edge-type information during training.\nHowever, higher accuracy in a graph-oriented\ntask such as link classification, does not lead to\nbetter performance in a language-oriented down-\nstream task in MEG. When we integrate these"}, {"title": "Qualitative Analysis", "content": "6.2\nThis section provides insights into the representa-\ntion spaces before and after the mapping network,\ncomparing them with the LLM's vector space.\nVisualizing the embeddings To track the\nmapped KGEs, we select three UMLS con-\ncepts (entities) representing different semantic and\nspecificity levels within the graph's hierarchy,\nmeasured by the number of descendants (IS_A\nand SUBCLASS_OF relations): 'Diabetes Melli-\ntus' (CUI: C0011849, a broad disease category),\n'Headache' (CUI: C0018681, a symptom), and\n'Atorvastatin Calcium' (CUI: C0286650, a spe-\ncific pharmacological substance).\nFigure 5 depicts t-SNE14 plots of the concepts\n(Level 0) and their hierarchies (Level 1 to 4) in the\nKGE space (top row) and the mapped KGE along\nwith the concept's contextualized embeddings in\nthe LLM space (bottom row). Such contextual-\nized embeddings are the LLM's token embeddings\ngenerated for the labels of the concepts when ver-\nbalizing their KG's triples, i.e., they represent the\nconcepts in all the contexts given by the KG.\nExamining the distribution of concepts in the\nupper and lower rows, we observe two effects of\nthe mapping. First, the relative structure of KGES"}, {"title": "Analysis and Discussion", "content": "7\nOur approach is efficient not only because it trains\na small fraction of the LLM's parameters but also\nbecause it seamlessly handles out-of-vocabulary\nterms. A new term's KGE can be initialized, for\ninstance, by averaging its one-hop neighbors in the\nKG, making MEG both lightweight and adaptable\nto new vocabulary. The efficacy of this method\nshould be evaluated in future work.\nGarikipati et al. (2024) demonstrate that prompt\nengineering can outperform fine-tuning in med-\nical QA for open-source LLMs. However, our\nfocus was to investigate the viability of integrat-\ning knowledge from KG embeddings into LLMs\nrather than optimizing for peak downstream per-\nformance. Our experiments show that supervised\nfine-tuning of KGE-augmented LLMs yields more\naccurate answers than other specialized baselines.\nChain-of-thought tuning, as shown by Kim et al.\n(2024), is another promising step forward to im-\nprove MEG's accuracy. MEG improves response\ngeneration by injecting KGEs in a single gener-\nation step. This suggests that MEG could also\nbenefit from chain-of-thought tuning, as each of\nthe reasoning steps would increase precision of the\nmodel's response.\nBesides, the sensitivity of LLMs to the infor-"}, {"title": "Conclusion", "content": "8\nWe introduce MEG, a novel medical knowledge-\naugmented LLM based on KGEs for question an-\nswering tasks. To the best of our knowledge,\nwe are the first to inject pretrained KGEs into an\nLLM via a lightweight mapping network, enabling"}]}