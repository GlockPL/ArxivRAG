{"title": "Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers", "authors": ["Siddharth Singh", "Prajwal Singhania", "Aditya Ranjan", "John Kirchenbauer", "Jonas Geiping", "Yuxin Wen", "Neel Jain", "Abhimanyu Hans", "Manli Shu", "Aditya Tomar", "Tom Goldstein", "Abhinav Bhatele"], "abstract": "Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s). While the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore \"catastrophic memorization,\" where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier.", "sections": [{"title": "I. JUSTIFICATION FOR ACM GORDON BELL PRIZE", "content": "A novel four-dimensional hybrid parallel approach to scale neural network training to tens of thousands of AMD and NVIDIA GPUs. Time-to-solution for 80-billion parameter GPT-style transformer models was reduced by 56.0x, due to kernel tuning, aggressive overlap, and optimization of collective communication. Unprecedented performance of 1.423 Exaflop/s on 6,144 NVIDIA H100 GPUs, 1.381 Exaflop/s on 32,768 AMD MI250X GCDs, and 620.1 Petaflop/s on 4,096 NVIDIA A100 GPUs in half-precision (bf16)."}, {"title": "II. PERFORMANCE ATTRIBUTES", "content": null}, {"title": "III. OVERVIEW OF THE PROBLEM", "content": "The field of generative artificial intelligence (AI) has taken the world by storm. In particular, large language models (LLMs) and their chatbot interfaces have become ubiquitous, employed by students, researchers, and professionals in various fields on a daily basis. Modern generative AI models are built by training extremely large neural networks, which have been shown to generalize extremely effectively with increases in model size. This unprecedented scaling of neural network training has been enabled by the emergence of highly efficient GPUs, and the availability of open source training frameworks such as PyTorch, and TensorFlow.\nTraining large neural networks that do not fit on a single GPU with 40-96 GB of RAM requires partitioning the model across multiple GPUs and parallelizing the matrix-matrix multiplication operations, which are a significant fraction of the overall computation. Scalability and parallel efficiency of DNN training is impacted by several factors \u2013 sustained flop/s and scalability of parallel matrix multiplication, performance of collective communication operations over sub-communicators, and the degree of overlap of computation with non-blocking collectives. While classical parallel algorithms for matrix multiplication such as SUMMA and Cannon's 2D Matrix Multiply exist, they lead to significant communication bottlenecks when training models with hundreds of billions of parameters on hundreds of GPUs. All of these factors make efficient parallelization of DNN training a formidable task.\nIn this work, we target the challenging research problem of training models with hundreds of billions of parameters on the fastest supercomputers with thousands of GPUs. Traditionally, training at such scales has been restricted to companies with large budgets and access to significant GPU resources. However, programs such as INCITE for access to DOE supercomputers, Frontier and Perlmutter, and recent access to Alps at CSCS have enabled our team to solve the research challenges in the area of parallel training, and innovate in the area of AI/ML, by training and fine-tuning LLMs.\nThere are several different challenges in ensuring scalability and a high fraction of peak flop/s for parallel training. First, we need to ensure that math libraries such as cuBLAS and"}, {"title": "IV. STATE OF THE ART IN PARALLEL TRAINING", "content": "In this section, we present the state of the art in scaling parallel training of deep neural networks to large-scale HPC systems and data centers."}, {"title": "A. Methods for Parallel DNN Training", "content": "Deep neural network training is typically parallelized using one of three approaches \u2013 data parallelism, tensor parallelism, or pipeline parallelism, or a hybrid approach that combines some of the above. In data parallelism, all GPUs are assigned a full copy of the neural network, and parallelism is achieved by sharding the input batch equally among the GPUs. The main drawback of data parallelism is that it requires the entire network to fit on a single GPU. To mitigate this, sharded data parallelism has been developed, which divides the network parameters across GPUs [11]\u2013[13] and performs extra communication to gather them when needed.\nModel parallelism is used to train DNNs that far exceed the memory capacity of a single GPU, and it can be further divided into two approaches \u2013 tensor parallelism [14] and pipeline parallelism [6], [15]. The former parallelizes the computation of each layer of the neural network across several GPUs, and is the focus of our work. In pipeline parallelism, entire layers are assigned to each GPU. A popular framework for parallel training is Shoeybi et al.'s Megatron-LM [14], which uses a tensor parallel algorithm to parallelize a pair of fully-connected layers.\nSeveral frameworks combine multiple approaches to develop hybrid methods. Narayanan et al. [6] extend Megatron-LM to support hybrid parallelism by combining tensor, pipeline, and data parallelism. Rajbhandari et al. introduce a sharded data parallelism approach called ZeRO [11], which is combined with pipeline and tensor parallelism in Microsoft's training framework, DeepSpeed [16], [17]. Megatron-DeepSpeed uses Megatron-LM's tensor parallelism.\nSeveral other frameworks that further optimize DNN training have been proposed in recent times. GPT-NeoX builds upon Megatron-LM and DeepSpeed for ease of usage [18]. Wahib et al. introduce KARMA, an out-of-core data parallelism framework, managing CPU-GPU data transfers to alleviate GPU memory constraints [2]. Zheng et al. propose Alpa for automating neural network parallelization, optimizing communication across GPUs [19]. Colossal-AI [20] offers a unified interface for distributed deep learning training."}, {"title": "B. Large-scale Studies of Training LLMs", "content": "We now present recent studies on training large language models on some of the largest GPU-based clusters. Meta trained Llama 2 [21] on 2000 NVIDIA A100 GPUs. Jain et al. [1] benchmark the training of a variant of T5-Large [22] on 1024 NVIDIA V100 GPUs using their proposed sub-graph parallelism technique within the LBANN framework [23]. Wahib et al. [2] use KARMA to benchmark a 17B parameter model on 2048 NVIDIA V100 GPUs and report a 1.35x training speedup compared to ZeRO [11]. Narayanan et al. present a weak scaling study of Megatron-LM's pipeline parallelism, achieving 52% of the peak NVIDIA A100 flop/s when benchmarking the training of a 1000B parameter model on 3072 GPUs [6]. Shaden et al. [5] use Megatron-LM and DeepSpeed [16] to train a 530B parameter language model on the Selene supercomputer [24] using 4480 NVIDIA A100 GPUs. They achieved 113 Tflop/s per GPU with 3360 GPUs, equivalent to 36% of the peak performance. Ziheng et al. [7] introduce MegaScale, a production system for training"}, {"title": "V. INNOVATIONS REALIZED", "content": "Training deep neural networks on a single GPU involves processing subsets of the data called batches through the layers of a DNN in the forward pass to compute a loss, computing the gradient of the loss in a backward pass via backpropagation, and updating the parameters (also called \"weights\") in the optimizer step. These three steps are repeated iteratively until all batches have been consumed, and this entire training process is referred to as an epoch. We now describe our novel approach to scaling the computation in the steps described above in the context of large multi-billion parameter neural networks on thousands of GPUs."}, {"title": "A. A Four-Dimensional Hybrid Parallel Approach", "content": "We have designed a hybrid parallel approach that combines data parallelism with three-dimensional (3D) parallelization of the matrix multiplication routines.\nData Parallelism: In order to use a hybrid approach that combines data with tensor parallelism, we organize the total number of GPUs, $G$, into a virtual 2D grid, $G_{data} \\times G_{tensor}$. This results in $G_{data}$ groups of $G_{tensor}$ GPUs each. We use data parallelism across the $G_{data}$ groups, and tensor parallelism within each group. Each $G_{data}$ group collectively has a full copy of the neural network and is tasked to process a unique shard of the input batch. At the end of an input batch, all groups have to synchronize their weights by issuing all-reduces on their gradients after every batch (this is also referred to as an iteration).\n3D Parallel Matrix Multiplication (3D PMM): Next, we use each GPU group, composed of $G_{tensor}$ GPUs to parallelize the work within their copy of the neural network. This requires distributing the matrices, and parallelizing the computation within every layer of the neural network across several GPUs. Note that most of the computation in transformers is comprised of large matrix multiplications within fully connected (FC) layers. Hence, in this section, we will focus on parallelizing FC layers with a 3D PMM algorithm.\nWe now describe how a single layer is parallelized, and the same method is applied to all the layers in the neural network. Each FC layer computes one half-precision (fp16 or bf16) matrix multiplication (input activation, $I$ multiplied by the layer's weight matrix, $W$) in the forward pass and two half-precision matrix multiplications (MMs) in the backward pass ($I \\times \\frac{\\partial L}{\\partial O} \\times W^{T}$ and $I^{T} \\times \\frac{\\partial L}{\\partial O}$, where $L$ is the training loss, and $O$ is the output activation.) Thus, parallelizing an FC layer requires parallelizing these three MM operations across multiple GPUs."}, {"title": "Parallelizing an entire network", "content": "The approach of parallelizing a single layer in a deep neural network can be applied to all the layers individually. Let us consider a 2-layer neural network. If we use Algorithm 1 to parallelize each layer, the output $O$ of the first layer would be the input to the other. However, notice in Figure 1 that $O$ is distributed across the 3D virtual grid differently than the input $I$. So to ensure that the second layer can work with $O$, we would need to transpose its weight matrix \u2013 essentially dividing its rows across the X-axis and columns across the Y-axis. This transpose needs to be done once at the beginning of training. Hence, to parallelize a multi-layer neural network, we simply 'transpose' the weights of every other layer by swapping the roles of the X- and Y-tensor parallel groups.\nNote that the 4D algorithm (data + 3D PMM) discussed in this section is a generalization of various state-of-the-art parallel deep learning algorithms. For example, if one were to employ only the Z axis of our PMM algorithm to parallelize training, it would reduce to Fully Sharded Data Parallelism (FSDP) [12] and ZeRO [11]. Similarly, if we employ the Z axis of 3D PMM and data parallelism simultaneously, then our algorithm reduces to Hybrid Sharded Data Parallelism [12] and ZeRO++ [27]. If we use the X axis of our 3D PMM algorithm along with the \u2018transpose\u2019 scheme discussed in the previous paragraph, our 4D algorithm reduces to Shoeybi et al.'s Megatron-LM [14]. Finally, when all four dimensions of our 4D algorithm are being used, this is similar to a hybrid scheme that combines data parallelism, FSDP, and two-dimensional tensor parallelism."}, {"title": "B. A Performance Model for Identifying Near-optimal Configurations", "content": "When assigned a job partition of $G$ GPUs, we have to decide how to organize these GPUs into a 4D virtual grid, and how many GPUs to use for data parallelism versus the different dimensions of 3D parallel martix multiplication. To automate the process of identifying the best performing configurations, we have developed a performance model that predicts the communication time of a configuration based on the neural network architecture, training hyperparameters, and network bandwidths. Using these predictions, we can create an ordered list of the best performing configurations as predicted by the model. We describe the inner-workings of this model below.\nWe primarily focus on modeling the performance of the collective operations in the code, namely all-reduces, reduce-scatters, and all-gathers. We first list the assumptions we make in our model:"}, {"title": "Assumption-1", "content": "The ring algorithm [28] is used for implementing the all-reduce, reduce-scatter, and all-gather collectives."}, {"title": "Assumption-2", "content": "For collectives spanning more than one compute node, the ring is formed such that the number of messages crossing node boundaries is minimized."}, {"title": "Assumption-3", "content": "The message sizes are large enough, and hence, message startup overheads can be ignored. In other words, if a process is sending a message of $n$ bytes, then we assumed that the transmission time is simply $\\frac{n}{\\beta}$, where $\\beta$ is the available bandwidth between the two processes."}, {"title": "Assumption-4", "content": "We only model the communication times and ignore the effects of any computation taking place on the GPUs."}, {"title": "Assumption-5", "content": "We assume the same peer-to-peer bidirectional bandwidth, $\\beta_{inter}$, between every pair of nodes."}, {"title": "", "content": "We use the analytical formulations in Thakur et al. [28] and Rabenseifner [29] for modeling the performance of ring algorithm based collectives. Let $t_{AG,z}$ denote the time spent in the all-gather across the Z-tensor parallel groups (line 2 of Algorithm 1). Similarly, we use $t_{RS,z}$, $t_{AR,y}$ and $t_{AR,x}$ to refer to the time spent in the collectives in lines 14, 4, and 12 respectively. Similarly, we use $t_{AR,data}$ for the time spent in the data parallel all-reduce. Then, we can model these times as follows,\n$t_{AG,z} = \\frac{1}{\\beta} \\times (G_z - 1) \\times \\frac{k \\times n}{G_x \\times G_y \\times G_z}$,\n$t_{RS,z} = \\frac{1}{\\beta} \\times \\frac{(G_z - 1)}{G_z} \\times \\frac{k \\times n}{G_x \\times G_y}$,\n$t_{AR,y} = \\frac{2}{\\beta} \\times \\frac{(G_y - 1)}{G_y} \\times \\frac{m \\times n}{G_z \\times G_x}$,\n$t_{AR,x} = \\frac{2}{\\beta} \\times \\frac{(G_x - 1)}{G_x} \\times \\frac{m \\times k}{G_z \\times G_y}$,\n$t_{AR,data} = \\frac{1}{\\beta} \\times \\frac{(G_{data} - 1)}{G_{data}} \\times \\frac{k \\times n}{G_x \\times G_y \\times G_z}$.\nThe total communication time for a single layer, $t_{comm}$ is simply the sum of Equations 1 through 5:\n$t_{comm} = t_{AG,z} + t_{RS,z} + t_{AR,y} + t_{AR,x} + t_{AR, data}$\nFor layers with 'transposed' weight matrices as discussed at the end of Section V-A, we need to swap the values of $G_x$ and $G_y$. And finally, to model the communication time for the entire network, we apply Equation 6 to all of its layers, and take a sum of the times.\nIn the equations derived above, we made a simplifying assumption that all collectives in our hybrid parallel method can achieve the highest peer-to-peer bandwidth, denoted by $\\beta$. However, since several collectives are often in operation"}, {"title": "", "content": "at once, the actual bandwidth achieved for a collective operation among a group of GPUs depends on the placement of processes in our 4D virtual grid to the underlying hardware topology (nodes and network) [30]\u2013[33]. For example, process groups that are contained entirely within a node can experience higher bandwidths than those containing GPUs on different nodes. Next, we model the specific bandwidths used in Equations 1 through 5.\nTo model the process group bandwidths, we begin by assuming a hierarchical organization of process groups: X-tensor parallelism (innermost), followed by Y-tensor parallelism, Z-tensor parallelism, and data parallelism (outermost). As a concrete example, if we have eight GPUs, and set $G_x = G_y = G_z = G_{data} = 2$, then the X-tensor parallel groups comprise of GPU pairs (0,1), (2,3), (4,5), and (6,7). Similarly, the Y-tensor parallel groups would comprise of GPU pairs (0,2), (1,3), (4,6), and (5,7), and so on.\nNow let $G = (G_x, G_y, G_z, G_{data})$ be the tuple of our configurable performance parameters, arranged in order of the assumed hierarchy. Let $B = (\\beta_x, \\beta_y, \\beta_z, \\beta_{data})$ be the effective peer-to-peer bandwidths for collectives issued within these process groups. We use $B_i$ and $G_i$ to represent the $i^{th}$ elements of these tuples ($0 \\leq i \\leq 3$). Also, let $G_{node}$ refer to the number of GPUs per node. Now let us model each $\\beta_i$ i.e. the bandwidth available to the GPUs in the process groups at the $i^{th}$ level of the hierarchy."}, {"title": "1", "content": "Case 1: GPUs in the process group lie within a node \u2013 in our notation, this is the scenario when $\\Pi_{i=0}^{2} G_i \\leq G_{node}$."}, {"title": "2", "content": "The bandwidth $\\beta_i$ is determined by two primary factors: (i) the size of the $i^{th}$ process group, $G_i$, and (ii) the cumulative product of the sizes of all preceding process groups, $\\Pi_{j=0}^{i-1} G_j$. Given that the number of GPUs per node is typically small, the number of possible scenarios is also small. Therefore, we can profile the bandwidths for all potential configurations in advance and store this information in a database. Specifically, we generate all possible two-dimensional hierarchies of process groups $(G_0, G_1)$ such that $G_0 \\times G_1 < G_{node}$, and then perform simultaneous collectives within the outer process groups of size $G_1$ with a large message size of 1 GB. We record the achieved bandwidths for this tuple in our database. Then, for a given model, when we need the predicted bandwidths for the $i^{th}$ process group, we retrieve the bandwidth recorded for the tuple $(G_0 = \\Pi_{j=0}^{i-1} G_j, G_1 = G_i)$."}, {"title": "", "content": "Case 2: GPUs in the process group are on different nodes \u2013 in our notation, this is the scenario when $\\Pi_{i=0}^{2} G_i > G_{node}$. For process groups spanning node boundaries, the approach of recording all possible configurations in a database is not feasible due to the vast number of potential scenarios, given the large number of possible sizes of these groups in a multi-GPU cluster. Therefore, we develop a simple analytical model for this scenario, which predicts the achieved bandwidths as a function of the inter-node bandwidths ($\\beta_{inter}$), process group sizes ($G_i$), and the number of GPUs per node ($G_{node}$).\nFirst, let's first explore two simple examples to build some intuition."}, {"title": "", "content": "We use this bandwidth term in Equations 1 through 5 of our model. We use the model to create an ordered list of configurations, and then we can pick the top few configurations for actual experiments."}, {"title": "C. Automated Tuning of BLAS Kernels", "content": "In deep neural networks, a significant portion of the computational workload is matrix multiplications kernels or \u201cmat-muls\u201d, particularly in transformer models. These matmuls can be performed in one of three main modes based on whether the operands are transposed: NN, NT, and TN. Prior research has highlighted that NT and TN kernels are often less optimized than NN kernels in most BLAS libraries [34]. In our experiments, we found this discrepancy to be more pronounced when running transformers with large hidden sizes on the AMD MI250X GPUs of Frontier. For example, in the GPT-320B model (described in Table II), we observed that a matrix multiply defaulting to the TN mode in PyTorch achieved only 6% of the theoretical peak performance, whereas other matmuls reached 55% of the peak.\nTo address this issue, we implemented an automated tuning strategy in which, during the first batch, each matmul operation in the model is executed in all three modes (NN, NT, and TN) and timed. We then select the most efficient configuration for each operation, which is subsequently used for the remaining iterations. This tuning approach ensures that our deep learning framework, AxoNN, avoids the pitfalls of using suboptimal matmuls that could significantly degrade performance. For the aforementioned 320B model, our BLAS kernel tuning approach successfully switches the poorly performing TN matmul with a nearly 8\u00d7 faster NN matmul, thereby reducing the total time spent in computation from 30.1 seconds to 13.19s! Note that for other models used in Table II, the speedups attained via tuning are relatively modest."}, {"title": "D. Overlapping Asynchronous Collectives with Computation", "content": "We use non-blocking collectives implemented in NCCL and RCCL on NVIDIA and AMD platforms respectively. This enables us to aggressively overlap the collective operations in AxoNN with computation, which can minimize communication overheads."}, {"title": "Overlapping All-reduces with Computation (OAR)", "content": "In this performance optimization, we overlap the all-reduce across the X-tensor parallel groups in the backward pass (Line 12 of Algorithm 1) with the computation in Line 13. Once this computation has completed, we wait on the asynchronous all-reduce. Note that for layers with 'transposed' weight matrices, this communication happens across the Y-tensor parallel groups."}, {"title": "Overlapping Reduce-scatters with Computation (ORS)", "content": "Next we overlap the reduce-scatters in the backward pass (line 14 of algorithm 1). The outputs of this reduce-scatter are the gradients of the loss w.r.t. the weights. These outputs are not needed until the backward pass is completed on all the layers of the neural network and we are ready to start issuing the all-reduces in the data parallel phase. Exploiting this, we issue these reduce-scatters asynchronously and only wait on them once all layers have finished their backward pass. This allows us to overlap the reduce-scatter of one layer with the backward pass computations of the layers before it."}, {"title": "Overlapping All-gathers with Computation (OAG)", "content": "Our next optimization overlaps the all-gather operations in the forward pass (line 2 of Algorithm 1) with computation. We observe that this all-gather operation does not depend on any intermediate outputs of the forward pass. Leveraging this, we preemptively enqueue the all-gather for the next layer while the computation for the current layer is ongoing. At the start of training, we generate a topological sort of the neural network computation graph to determine the sequence for performing the all-gathers. Subsequently, we execute them preemptively in this order."}, {"title": "VI. HOW PERFORMANCE WAS MEASURED", "content": "All of our innovations are implemented in an open-source framework called AxoNN [9], which can be integrated easily as a backend in existing serial training codebases. This section provides details of the experimental setup for benchmarking training performance using AxoNN."}, {"title": "A. Applications: Model Architecture Details", "content": "We evaluate the effectiveness of our implementation by conducting experiments on a well-known neural network architec-"}, {"title": "VII. PERFORMANCE RESULTS", "content": "We now discuss the results of our performance benchmarking experiments described in Section VI."}, {"title": "A. Weak Scaling Performance", "content": "We first present the weak scaling performance of AxoNN on Perlmutter, Frontier and Alps using GPT-style transformers as the application in Figure 6. We observe that on all three systems, AxoNN achieves near-ideal weak scaling up to 4096 GPUs/GCDs. This is particularly promising because most large-scale LLM training falls within this hardware range. When running the 60B model on 6144 H100 GPUs of Alps, we see a small reduction in efficiency \u2013 76.5% compared to the performance on 1024 GPUs."}, {"title": "", "content": "Since Frontier has a significantly large number of GPUs than the other two platforms, we scaled AxoNN on Frontier to 32,768 GCDs. We see near perfect weak scaling up to 8,192 GCDs with a significantly high efficiency of 88.3% (compared to the performance on 512 GCDs). Although our weak performance drops at 16,384 GCDs, we are still able to sustain an efficiency of 79.02%. However, with rising overheads of communication, there is a notable decline in our performance on 32,768 GCDs, and a corresponding drop in efficiency to 53.5%.\nWe used timers to gather breakdowns of the time per batch into computation and non-overlapped communication to better understand the impact of the performance optimizations described in Section V. We present these results in Figure 7, for some model sizes running on 512\u20138,192 GCDs of Frontier. As a baseline, we use a configuration of AxoNN that corresponds to a hybrid of 1D tensor parallelism within node (similar to Megatron-LM [14]) and hybrid sharded data parallelism across nodes (similar to FSDP [12], [27]).\nWe observe that using the 3D parallel matrix multiplication and performance model to select the best configuration results in significant performance improvements of 13-45% over the baseline. Most of the improvement comes from a significant reduction in communication times. For the models in the plot, the improvements in the batch times due to our BLAS kernel tuning are relatively modest (2\u20134%). Finally, the improvement from our overlap optimizations is largest for the largest model in this series i.e. 80B on 8192 GCDs. In this case, we observe a 22% reduction in the batch times! This is expected because the overheads of communication tend to increase with scale and subsequently the benefits of our overlap optimizations become more pronounced."}, {"title": "B. Sustained floating point operations per second (flop/s)", "content": "Next, we examine the floating-point operations per second (flop/s) achieved by AxoNN. In Figure 8, we present the total bf16 flop/s sustained by AxoNN in our weak scaling experiments on Perlmutter, Frontier and Alps. In Table III, we also show our sustained flop/s as a percentage of the vendor advertised and empirical obtained peak flop/s. As discussed in Section VI-B, we use 280 Tflop/s, 125 Tflop/s, and 813 Tflop/s as the empirical peak bf16 flop/s for an A100 GPU,"}, {"title": "C. Predicted Time-to-solution", "content": "The training of state-of-the-art large language models (LLMs) presents a significant computational challenge due"}, {"title": "VIII. IMPLICATIONS", "content": "A scalable training framework such as AxoNN and access to large supercomputers such as Frontier and Alps can enable studying properties of LLMs at scales that were impossible before. Below, we present a study on the behavior of memorization by large language models."}, {"title": "A. Memorization of Training Data by Large Language Models", "content": "A growing body of work has shown that language models memorize a portion of their training data and can reproduce this training data at inference time [44]. The ability of LLMs to reproduce training data has become a flashpoint for the AI community, as it poses major privacy and legal risks for commercial models [44]-[46].\nIt is thought that memorization is largely due to training data repetition, and it may be mitigated by dataset deduplication. Other factors such as data structure and model size may play a factor, but the issue is not well understood because public experiments have been constrained to smaller models (e.g. the popular Llama-2 7 billion parameter model [21]) with limited capacity and correspondingly small rates of memorization [44], [47]. As we observe below, the ability to memorize entire documents emerges only for large model sizes. Further, we hypothesize that models above a certain size threshold may exhibit catastrophic memorization, in which documents are memorized immediately in one single pass. When training a model above this size limit, even perfectly deduplicated datasets may still result in privacy and copyright leaks.\nBy creating scalable, user-friendly and portable access to model parallelism, AxoNN unlocks the potential for training and fine-tuning much larger models under commodity computing constraints using sequential LLM training codebases. This creates a scientific laboratory where large-model phenomena such as memorization can be publicly reproduced and studied. It also raises the ability of many practitioners to fine-tune large models on domain-specific data, expanding the need to understand memorization risks."}, {"title": "B. Experimental Setup: Training Llama models on Wikipedia", "content": "We design a targeted set of continued pre-training experiments to quantify the relationship between model size and memorization. We consider the Llama family of LLMs with publicly available pre-trained weights, and use the AxoNN infused LitGPT framework (introduced in Section VI-A) to parallelize the models. Our experiments start with pre-trained checkpoints for the TinyLlama-1B model [48], the 7B, 13B, and 70B parameter models in the Llama 2 family [21] and the 8B, 70B, and 405B parameter models from the recent Llama 3.1 release [49]. We train on English text data from Wikipedia with varying levels of repetition to quantify how memorization depends on model scale.\nWe train on English Wikipedia pages with 2048 tokens or more. The articles are randomly placed into one of four disjoint \"buckets,\u201d each with 200 articles. During training, the first three buckets are repeated for 1, 4, or 6 \"epochs\" (one pass over every page in the bucket) respectively. The fourth bucket is a control group to measure baseline preexisting memorization from pre-training, and we do not perform any further training on the pages in the fourth bucket. After training is complete, we prompt the model with the beginning of each training sequence, and let the model write the last 50 tokens. We consider a sequence memorized if the model perfectly reproduces the correct 50 tokens.\nWe train the 1B, 7B, and 8B models on eight GCDs of Frontier using 8-way Z-tensor parallelism (i.e. $G_z = 8$), the 13B model using 16 GCDs, the 70B models using 64 GCDs, and the 405B model using 128 GCDs, each with a corresponding level of Z-tensor parallelism. The total batch size is fixed at 128 samples for all model sizes. In the case of smaller models, lower level of tensor parallelism is needed, so data parallelism is used to utilize the remaining GPUs. We warm up each model for 50 steps, increasing the learning rate to $3 \\times 10^{-4}$ on the non-bucketed Wikipedia pages, and then inject the three buckets of target data over the next 50 steps of training while decaying the learning rate to $3 \\times 10^{-5}$. We report memorization for each bucket separately, and also for the held-out (\u201c0 Ep\u201d) control bucket."}, {"title": "C. Results: Catastrophic Memorization as a Function of Model Size", "content": "Figure 10 shows the impact of parameter count and number of epochs on exact memorization under otherwise identical conditions. At the 1B-13B scale (left plot), training for up to six epochs causes memorization of less than 1% of the 200 documents on average. However, we observe that the 70B models and the 405B model are capable of significant memorization (right plot). After just six passes over the data, the 70B Llama 2 and 70B Llama 3.1 models memorize 47% and 67% of documents on average respectively. Furthermore, we observe catastrophic memorization behavior starting at the 70B scale; roughly 5% of documents are memorized in just one single pass.\nMoving to the 405B scale, we make several surprising observations. This model had already memorized over 10% of the control documents (see the bars labeled \"0 Ep\") before our experiment even began, showing that the ability to memorize and retain documents during pre-training has emerged at this scale. While Wikipedia pages were certainly included in"}]}