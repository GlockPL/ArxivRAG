{"title": "MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation", "authors": ["Bo Li", "Shaolin Zhu", "Lijie Wen"], "abstract": "Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 840K images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority.", "sections": [{"title": "1 Introduction", "content": "Image Translation (IT), the task of translating embedded text within an image from a source language to a target language (Watanabe et al., 1998; Yang et al., 2002; Lan et al., 2023), holds significant promise for various applications. Its utility spans domains such as scene text translation, document image translation (Liang et al., 2024), and photo translation, enhancing accessibility and cross-lingual communication. This is in contrast to traditional neural machine translation (Chen et al., 2022; Zhu et al., 2024), which is based solely on textual information.\nTraditional IT approaches often employed cascade methods (Zhang et al., 2019; Zhao et al., 2020; Shekar et al., 2021; Hinami et al., 2021; Afli and Way, 2016). The emergence of end-to-end IT models (Zhu et al., 2023; Liang et al., 2024; Niu et al., 2024; Ma et al., 2023a) offered a more streamlined approach, utilizing a unified model to directly translate image text. Multimodal Large Language Models (MLLMs) (Bai et al., 2023; Chen et al., 2024b; Hong et al., 2024; Liu et al., 2024; Yao et al., 2024; Lu et al., 2024; Li et al., 2023) have further fueled progress.\nHowever, IT model development and evaluation lies in the scarcity of high-quality, large-scale datasets (Ma et al., 2022, 2023b; Zhu et al., 2023; Ma et al., 2023a; Liang et al., 2024). Models trained on limited real-world data often struggle to generalize to complex scenarios, while those trained on synthetic datasets (Chen et al., 2021; Su et al., 2021; Niu et al., 2024) may not adequately capture the nuances of real-world image characteristics. Furthermore, existing datasets frequently lack fine-grained splits and diversity in language representation and task difficulty, hindering comprehensive model assessment.\nTo address these issues, we introduce MIT-10M, a large-scale multilingual image translation corpus. MIT-10M is the largest real-world image translation dataset to date. As shown in Figure 1, it includes 840K images in 8 languages and 10M image-text pairs across 14 languages, and split into a train set a test set. Figure 2 shows some examples from the MIT-10M dataset. Each image contains the original text and the corresponding language. Additionally, we annotate the image category, the token length of the text and the number of bounding boxes and used them for the difficulty level. In addition to the original text, 13 further translations were annotated. Detailed examples can be found in Appendix B. The MIT-10M dataset construction process consists of three main stages: data collection and pre-processing, OCR annotation and cleaning, and multilingual translation and validation. We compared MIT-10M with several existing popular image translation datasets in terms of data scale, task difficulty level, diversity, and image quality. The comparison results demonstrate that MIT-10M significantly outperforms other datasets in these aspects. We conduct extensive experiments to evaluate the multilingual translation capabilities of 7 end-to-end IT models using the MIT-10M test set. The results show that using the MIT-10M test set is beneficial when evaluating challenging tasks of translating multi-line text into images with complex backgrounds. Furthermore, we fine-tune the Qwen2-VL (Bai et al., 2023) model using the MIT-10M training set, BLEU, chrF++, and METEOR scores have increased by 230%, 88%, and 130% respectively. The results demonstrate significant performance improvements in multilingual image translation tasks, further validating the advantages of MIT-10M dataset.\nThe main contributions are outlined below:\n\u2022 We present MIT-10M, a dataset comprising 10M image-text pairs and 840K high-resolution real-world images, representing the largest publicly available real-world high-quality dataset specifically designed for multilingual image translation tasks.\n\u2022 We propose a multi-dimensional evaluation framework for multilingual image-text translation datasets, considering aspects such as data scale, task difficulty level, diversity, and image quality. Our comparative analysis demonstrates the superiority of MIT-10M across these dimensions.\n\u2022 We conduct comprehensive experiments to validate the effectiveness of MIT-10M for training and evaluating multilingual image translation models."}, {"title": "2 Related Work", "content": "IT focuses on translating text embedded within images from a source language into a target language. This section reviews existing approaches and datasets relevant to IT."}, {"title": "2.1 IT Models", "content": "Two primary paradigms dominate the field of image translation:\nCascade Methods. These approaches decompose the task into sequential steps, typically employing Optical Character Recognition (OCR) followed by NMT (Zhao et al., 2020; Shekar et al., 2021; Hinami et al., 2021; Zhong et al., 2024; Chen et al., 2024a; Zhu et al., 2024). While conceptually straightforward, cascade methods suffer from error propagation between stages, increased latency due to separate model processing, and redundant parameterization (Ma et al., 2023b, 2022; Lan et al., 2023; Zhu et al., 2023).\nEnd-to-End Methods. These models aim to directly translate image text using a unified architecture (Mansimov et al., 2020; Jain et al., 2021; Zhu et al., 2023; Liang et al., 2024; Niu et al., 2024; Ma et al., 2023a), typically comprising a visual encoder for extracting image features and a text decoder for generating the target translation. The advent of Multimodal Large Language Models (MLLMs), such as (Bai et al., 2023; Chen et al., 2024b; Hong et al., 2024; Liu et al., 2024; Yao et al., 2024; Lu et al., 2024; Li et al., 2023), has significantly advanced end-to-end IT, enabling more effective cross-modal fusion and improved translation accuracy. However, the success of end-to-end models heavily relies on the availability of large-scale, high-quality parallel image-translation data, which remains a critical challenge in the field (Ma et al., 2022, 2023b; Zhu et al., 2023; Ma et al., 2023a; Liang et al., 2024)."}, {"title": "2.2 IT Datasets", "content": "Due to the scarcity of real-world image translation data, several studies have utilized synthetic datasets created by rendering text onto background images. These include datasets focused on single-line text (Mansimov et al., 2020), multi-line text (Jain et al., 2021), Chinese-English translation (Ma et al., 2022), and English-German translation (Niu et al., 2024). While valuable for initial model training, synthetic datasets often oversimplify real-world complexities and may not adequately reflect the diversity and challenges encountered in practical applications. Manually curated real-world datasets are crucial for advancing IT research, but their development is resource-intensive, resulting in limited availability. Existing datasets include OCRMT30K (Lan et al., 2024), a Chinese-English dataset based on OCR annotations; DoTA (Liang et al., 2024), focused on document image translation into Markdown format; and ECOIT (Zhu et al., 2023), targeting the e-commerce domain. Despite these efforts, real-world datasets remain limited in scale, language coverage, and diversity of image characteristics, hindering comprehensive model evaluation and generalization assessment. Our work addresses this gap by introducing MIT-10M, a novel large-scale, multilingual, and real-world image translation dataset designed to facilitate the development and evaluation of robust and generalizable IT models."}, {"title": "3 MIT-10M Construction", "content": "In this section, the construction pipeline of MIT-10M is described in detail, highlighting the crucial filtering steps to ensure data quality. Figure 3 illustrates the MIT-10M construction pipeline."}, {"title": "3.1 Data collection", "content": "Web Crawling. High-quality websites with extensive language coverage were selected for data collection, including google.com, baidu.com, amazon.com, jd.com, and amazon.jp.co. These websites offer a rich source of high-resolution images and support multiple languages. Crawling was conducted across eight languages: English, French, Chinese, Japanese, Portuguese, Italian, German, and Spanish. To ensure data uniqueness, SHA256 hashing was employed to identify and remove duplicate pages. Documents lacking images or containing an excessive number of images (over 50) were excluded. This process resulted in 405K unique HTML files, occupying 440 GB."}, {"title": "Image collection.", "content": "We use the BeautifulSoup (Leonard, 2004) library to parse the HTML documents into a tree structure and extract the image tags. To ensure the quality of the data, we filter the images according to their resolution and only keep the images with a resolution of more than 800x800 pixels that are in the main content area of the HTML document to exclude logos and advertisements. Duplicate images are then removed based on their MD5 hash values."}, {"title": "Filtering and cleaning data.", "content": "To ensure that the content is appropriate, we apply a NSFW detection tool (Laborde, 2024) to all images. If NSFW content is detected, we discard all images from the corresponding HTML document. The final set contains 6.3M images occupying 900 GB."}, {"title": "3.2 Data annotation", "content": "Initial text recognition. To quickly determine whether images contain text, we first apply easy-OCR for text recognition and remove images where no text is recognized or the recognized text is meaningless, which accounts for about 50% of the images. This results in 3M images being retained. Next, we use the tools langid and langdetect to identify the language of the recognized text and perform cross-validation. Only images for which both tools consistently identify the same language are retained. The result is 1 million images."}, {"title": "Precise text recognition.", "content": "We then use GPT-4 (OpenAI, 2024) for detailed text recognition to extract precise text information from the images and discard those images where the text cannot be recognized."}, {"title": "Filtering sensitive content and text cleaning.", "content": "To reduce the risk of sharing personal data, we remove images whose OCR-recognized text contains sensitive information such as email addresses or IP addresses. We also exclude images where the recognized text contains NSFW characters to exclude potentially inappropriate content. To further improve the quality of the dataset, we filter out images with advertising or meaningless content (e.g. excessive numbers or punctuation) and remove images with excessively long text (over 450 tokens or 60 words). After the cleanup, 957K images remain, taking up 150 GB."}, {"title": "3.3 Multilingual text translationn", "content": "Machine translation. To translate the text into other languages, we first use GPT-4 (Achiam et al., 2023) to translate the OCR-recognized text. We create a highly optimized translation prompt that translates the image text from 8 languages into 13 languages, with the additional languages being Korean, Thai, Arabic, Turkish, Hindi and Russian.\nValidation of the translation. Although GPT-4 performs well in multilingual translation, we cross-validate with Google Translate (via the Google Gemini 1.5 Pro API) (Gemini, 2024). We use the tool spacy to convert the results of GPT-4 and Google Translate into word vectors and calculate their semantic similarity. For text pairs with a similarity score below 0.8, we filter out those with significant differences and keep translations with a semantic similarity above 0.8. The final parallel corpus contains 840K images."}, {"title": "4 Analysis MIT-10M", "content": "This section presents a comprehensive analysis of the MIT-10M dataset. First, we compare MIT-10M with existing popular image translation datasets (see Table 1). Then, we analyze MIT-10M in detail in terms of data scale, difficulty, diversity, and image quality."}, {"title": "4.1 Data Scale Comparison", "content": "Table 1 shows a comparison of MIT-10M with other popular image translation datasets in terms of statistical data. MIT-10M clearly outperforms the other datasets in terms of the number of languages, images and image-text pairs.\nMIT-10M includes 14 languages and is therefore better suited for cross-language multimodal image translation research, especially for scenarios requiring the processing of multiple languages. In addition, MIT-10M is derived from real-world scenarios and contains 840K images and 10M image-text pairs. This is a significantly larger volume than data sets such as DoTA and ECOIT, which also consist of real images."}, {"title": "4.2 Difficulty Levels", "content": "We visualized the distribution of the number of bounding boxes in the images and the corresponding lengths of the English tokens in the MIT-10M dataset, as shown in Figure 4. Most of the images have a bounding box count between 1 and 3, with the proportion of images with a bounding box count of 2 being the highest. In addition, the length of text tokens in the images is mainly in the range of 10 to 25 tokens, which is consistent with the distributional characteristics of text length in natural language.\nBased on the above analysis, we categorize the MIT-10M dataset into three difficulty levels: easy, medium, and hard. Table 2 shows the number and proportion of corpora of different difficulty levels of the MIT-10M training and testing sets.\nEasy (number of bounding boxes < 2 and token length < 16) contain fewer bounding boxes and shorter texts, resulting in a relatively easy translation task.\nHard (number of bounding boxes > 5 or length of tokens > 25) contain more bounding boxes or longer texts, which places higher demands on the model's attention mechanism.\nMedium (other cases) have a wider spread in terms of the number of bounding boxes and length of tokens, indicating more realistic and complex"}, {"title": "Human evaluation.", "content": "We divide the translated image-text pairs and their corresponding text translations into 10,000 batches. We randomly select 10 batches and perform a human evaluation, which results in a translation accuracy of 99.4%."}, {"title": "4.3 Diversity", "content": "As shown in Figure 1, MIT-10M is divided into 28 categories based on image content, ranging from daily objects (e.g., Home & Kitchenware, Health & Personal Care) to specialized equipment (e.g., Industrial & Scientific, Jewelry) and digital goods (e.g., Games & Apps, Digital Music). The detailed categories and data can be found in Table 3. And categories such as \"Cell Phones & Accessories\" (162K images) and \"Clothing & Shoes\" (86K images) have a significant number of images. The products in these categories are frequently used in daily life, which means they are more common. This diversity provides a wealth of scenarios for image translation models and enables a comprehensive evaluation of their performance in different image types and text contexts. Consequently, training with such diverse data helps in developing models with superior generalization abilities. The English images dominate the dataset (about 49%), while Chinese is the second most common language and accounts for about 60% of the English data. This diversity of language and image data helps to improve the generalization ability of the model in different cultural and linguistic environments."}, {"title": "4.4 Image resolution", "content": "When translating images to text, the quality and resolution of the images have a direct impact on how the model extracts semantic features from the visual information, which in turn affects translation accuracy.\nUnlike previous works, which often only include images with a single resolution (e.g. the ECOIT dataset with an image resolution of only 64x600), MIT-10M selects images with a resolution of more than 1000x1000 pixels (width) when creating the dataset. It also provides images in three different sizes: the original size, a \"large\" version with a width of 768 pixels and a \"small\" version with a width of 500 pixels. These different image sizes allow the model to learn visual features under different conditions, which increases its robustness."}, {"title": "5 Experiments", "content": "In this section, we empirically demonstrate the efficacy of the MIT-10M dataset both as a pretraining dataset as well as an evaluation set for image-text translate task."}, {"title": "5.1 Experiment Details", "content": "Setup. The operating system which we use is CentOS Linux release 7.5, and the programming language is Python 3.9.12. Our experiments are conducted on NVIDIA TESLA A100-40G GPU, the CUDA version is 12.2, and the deep learning framework is torch with version 2.1.0, torchvision with version 0.16.0 and Transformers with 4.44.2. During the inference stage, we set the following hyperparameters: temperature to 0.2.\nModels. In this study, we perform a comparative analysis of our dataset using both cascaded and end-to-end models for image translation. The cascaded model first applies EasyOCR to extract text from images and then translates the extracted text using the NLLB model. This choice of established components makes our baseline representative of typical cascaded methods and facilitates reproducibility. As for the end-to-end model, we compare with mainstream MLLM. The detailed introduction is as follows:\n\u2022 LLaVA-NeXT is a large-scale multimodal model that can process a variety of data types, including text, images and video. It has been trained on large multimodal datasets and shows strong performance in understanding multiple images and videos. In addition, LLaVA-NeXT has multi-language support, enabling it to understand multilingual text in images, including most European languages, Japanese, Korean, Arabic, Vietnamese and more.\n\u2022 Qwen2-VL (Bai et al., 2023) can process images with different resolutions and aspect ratios and supports the understanding of multi-lingual texts. Qwen2-VL has achieved world-leading performance in several visual comprehension benchmarks and has open-source 2B and 7B scale models.\n\u2022 CogVLM2-LLaMA3 builds on LLaMA3 and shows exceptional performance on multimodal tasks involving images and text, especially when processing long texts and high-resolution images. It has a visual expert module that focuses on complex visual tasks and achieves deep integration of vision and speech through a sophisticated fusion strategy.\n\u2022 InternVL2 boosts the model's performance through visual encoder improvements, dynamic strategies for high resolutions and high-quality bilingual data sets. It performs admirably on tasks such as OCR, multimodal assessment, mathematical reasoning and dialog with multiple interlocutors.\n\u2022 DeepSeek-VL is an open-source multimodal model designed to improve performance in real-world scenarios. It accepts high-resolution images as input and has general multimodal comprehension capabilities that process logic diagrams, web pages, formula recognition, scientific literature and natural images.\n\u2022 MiniCPM-Llama3-V is designed for consumer devices and focuses on providing advanced AI capabilities on resource-constrained devices such as cell phones. It can process various types of data, including text and images, and is capable of describing images, answering text questions with one or more images, writing and debugging code, conducting dialogs with multiple images, conducting dialogs to understand videos, formatting JSON, and performing OCR in high resolution.\nIn order to make the model output the translated content stably, after several attempts, we use the prompt \"Translate the text in the image from {src_lang} into {tgt_lang}. Please output the translation directly without any explanation or other words:\" to obtain the inference result of the model.\nMetrics. We use the BLEU score to assess the similarity between predicted translations and reference translations. This method calculates n-gram precision and applies a brevity penalty for shorter translations. We compute the chrF++ score, which operates on both character and word-level n-grams. It effectively handles morphologically complex languages and is highly sensitive to spelling errors and minor mistakes. We employ the METEOR metric to evaluate translation quality. This metric incorporates stemming, synonym matching, and word reordering to improve its correlation with human evaluation."}, {"title": "5.2 Evaluate IT models with MIT-10M", "content": "In this paper, we systematically evaluate the performance of 7 state-of-the-art models on the MIT-10M, focusing on the comparison of cascaded models and multimodal end-to-end models on the image translation task. Table 4 shows the performance of all models on the MIT-10M. Among all models, EasyOCR_NLLB is the only cascaded model, while the others (e.g. LLaVA-NeXT, Qwen2-VL) are multimodal end-to-end models.\nThe Table 4 shows that DeepSeek-VL model perform poorly in IT task, with BLEU, chrF++,\nand METEOR of 5.7, 15.1, 10.5, well below average. Although the BLEU, chrF++ and METEOR values of EasyOCR_NLLB (6.3, 18.9 and 16.3, respectively) are close to the average, they lag behind the end-to-end models. In contrast, end-to-end models such as InternVL2 and Qwen2-VL perform better on IT task, with scores of 13.9, 16.8, 23.3 and 14.3, 29.0, 21.3 respectively, far outperforming those of the other models. In particular, Qwen2-VL achieved the best performance. The detail comparison of the BLEU of the individual models in multiple languages can be found in Appendix C.\nWe attribute this to the ability of the end-to-end models to generate text outputs directly from the image inputs and effectively utilize the correlations between images and text for deeper feature fusion and contextual understanding, resulting in superior performance in the fluency, consistency, and accuracy of the generated text. In contrast, cascaded models suffer from error propagation from the OCR phase, which limits their final translation quality. It is noteworthy that all models show relatively low evaluation results compared to their performance on other datasets. This highlights the realism of the dataset, which includes a wider range of challenges, including text with different fonts, colors and backgrounds, as well as instances of text occlusion and blurring. Consequently, the MIT-10M provides a more realistic and sophisticated benchmark for assessing the generalization ability of models."}, {"title": "5.3 Effect of Resolution and Difficulty", "content": "To further investigate the properties of the MIT-10M, we conducted finer analyzes at different image resolutions and task difficulties.\nTable 4 illustrates the performance differences of the models in the image resolution tasks. A comparison of the average score of all models in different sizes shows that almost all models perform better with larger images. The BLEU for the base, large and small resolutions are 9.9, 9.6 and 9.2 respectively, while the chrF++ and METEOR are 22.1, 21.6, 21.1 and 17.0, 16,6, 16.0 respectively. This emphasizes the importance of image resolution for the model's understanding of the image content. High-resolution images provide more detailed information that helps the model to recognize and translate the text in the image more accurately.\nFurthermore, MIT-10M contains tasks with three levels of difficulty: Easy, Medium and Difficult (see \u00a7Section 4.2). Table 5 illustrates the differences in model performance for the different tasks. When analyzing the performance of the models on the different difficulty levels, we observe a clear trend that the performance decreases with increasing difficulty. The average METEOR drops from 18.3 to 14.0, and this is particularly evident for Qwen2-VL, whose score decreases from 23.3 to 18.8.\nThis shows that the carefully designed difficulty distribution in the MIT-10M can effectively differentiate the performance of the models and provide the models with different difficulty levels.\nAs for the evaluation metrics, given the abundance of short sentences in MIT-10M, we recommend focusing on chrF++ and METEOR in addition to BLEU, as they are more suitable for evaluating the quality of short sentence translations.\nOverall, our experimental results show that MIT-10M is a comprehensive, high quality and realistic dataset for image to text translation. MIT-10M represents a challenging benchmark for image translation research and serves as a valuable resource to advance the development of robust and versatile multimodal models."}, {"title": "5.4 Fine-tune with MIT-10M", "content": "To evaluate the effectiveness of our proposed MIT-10M for training image-to-text translation models, we perform fine-tuning experiments using the Qwen2-VL model as the baseline. During training, we use the following important hyperparameter settings: per_device_train_batch_size, gradient_accumulation_steps, learning_rate, num_train_epochs, lr_scheduler_type and warmup_ratio are set to 4, 32, 1.0e-4, 1, cosine and 0.1, respectively. Additionally, we utilize mixed precision training to accelerate the training process and reduce memory consumption.\nWe fine-tune the model with different subsets of the MIT-10M to investigate the effects of data size on model performance. Specifically, we take 10%, 30%, 50%, 70%, and 100% of the training set from the MIT-10M and compare their performance to analyze the effects of data size.\nAs shown in Figure 6, the results demonstrate that increasing the size of the training data leads to significant improvements in all three metrics. For example, the BLEU increases from 14.6 for the base model (base@10%) to 35.9. Similarly, the chrF++ score increases continuously from 29.0 to 56.6 and the METEOR score improves from 21.2 to 52.0. It strongly emphasize the crucial role of data size in improving the performance of the image-to-text translation model: with richer training data, the model can capture finer correlations between images and text, improving translation accuracy and fluency.\nIt is noteworthy that even with only 10% of the training data, the performance of the model is still significantly better than that of the base model, indicating the high quality of the MIT-10M. This result emphasizes that our dataset can effectively improve the performance of the model even with relatively small amounts of data."}, {"title": "5.5 Comparison with Existing IT Datasets", "content": "To further demonstrate the advantages of MIT-10M over existing real-world datasets, we perform fine-tuning experiments with comparable subsets. We compare the fine-tuning performance of MIT-10M with the IIMT dataset. We select subsets of both datasets to control the data size. We use 150K image-text pairs from MIT-10M and the slightly larger 170K pairs from IIMT. This choice ensures that any performance differences are not simply attributable to the volume of training data, but rather to inherent dataset qualities. We focus on four language pairs: DE-EN, EN-DE, EN-FR, and FR-EN. The BLEU after fine-tuning can be found in Table 6. Even with less data, MIT-10M consistently outperforms IIMT in all four language pairs. The performance gains are particularly noticeable in the FR-EN pair, where MIT-10M shows a nearly 4 BLEU improvement.\nThe results strongly support that MIT-10M is a valuable resource for multilingual image translation, enabling the training of more robust and generalizable models. This comparative analysis provides compelling evidence that the quality and diversity of MIT-10M contribute significantly to improved fine-tuning results, even when controlling for data size."}, {"title": "6 Conclusion", "content": "In this paper, we propose MIT-10M, a large-scale, high-quality dataset designed for multilingual image translation. MIT-10M contains over 10M image-text pairs in 14 languages and 840K high-resolution real-world images. We described the dataset creation process in detail and conducted a comprehensive analysis of the dataset across multiple dimensions. The results of experiments with different end-to-end IT models and evaluation metrics show that MIT-10M significantly improves multilingual translation performance and has strong generalization capabilities, especially for complex tasks. In the future, we will focus on further improving translation accuracy, extending support for more languages and processing even more diverse and complex images."}, {"title": "Limitations", "content": "In this work, we introduce MIT-10M, a novel, large-scale multilingual image translation parallel corpus that significantly advances research in the field of cross-lingual image translation. However, our approach is not without limitations. First, it is a major challenge to achieve a balanced representation of language and domain within the dataset. Furthermore, despite careful annotation and translation, the inherent complexity of multilingual data may lead to inaccuracies that could affect the reliability of the dataset. Furthermore, while the dataset has been extensively cleaned and filtered to address ethical concerns, including the removal of privacy and sensitive content issues, unforeseen possibilities remain. We acknowledge these limitations transparently to promote ethical research and encourage the community to make improvements."}, {"title": "A Prompt", "content": null}, {"title": "A.1 OCR Prompt", "content": "To enhance the precision of text identification within images, we employ the GPT-40 model for Optical Character Recognition (OCR) across the dataset. We meticulously construct a prompt for this task, ensuring that the textual data extracted is both accurate and reliable for subsequent analysis and translation processes. It is important to note that we instruct the GPT-40 model to output both English and Chinese content. This bilingual output capability is used to identify and translate the text, which is essential for the subsequent calibration phase."}, {"title": "A.2 Translation Prompt", "content": "We use the GPT-4 model to translate the extracted text into 13 target languages. To improve the quality of the translations, we use carefully crafted prompts. We also call Google Translate with the same prompts for cross-validation. The translations are further refined by filtering out those with ambiguous meanings based on semantic similarity scores to ensure the accuracy and reliability of the translations."}, {"title": "B Dataset Examples", "content": "Figure 7 shows the detail fields in the dataset, including 6 labels and 14 languages text."}, {"title": "C Experiments", "content": "Figure 8 show the detail comparison of the BLEU of each model in multiple language pairs."}]}