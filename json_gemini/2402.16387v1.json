{"title": "On the Generalization Capability of Temporal Graph Learning\nAlgorithms: Theoretical Insights and a Simpler Method", "authors": ["Weilin Cong", "Jian Kang", "Hanghang Tong", "Mehrdad Mahdavi"], "abstract": "Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world\napplications, especially in domains where data can be represented as a graph and evolves over\ntime. Although TGL has recently seen notable progress in algorithmic solutions, its theoretical\nfoundations remain largely unexplored. This paper aims at bridging this gap by investigating the\ngeneralization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-\nbased methods) under the finite-wide over-parameterized regime. We establish the connection\nbetween the generalization error of TGL algorithms and \u2460 \"the number of layers/steps\" in the\nGNN-/RNN-based TGL methods and \u2461 \"the feature-label alignment (FLA) score\", where FLA\ncan be used as a proxy for the expressive power and explains the performance of memory-based\nmethods. Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network\n(SToNe), which enjoys a small generalization error, improved overall performance, and lower model\ncomplexity. Extensive experiments on real-world datasets demonstrate the effectiveness of SToNe.\nOur theoretical findings and proposed algorithm offer essential insights into TGL from a theoretical\nstandpoint, laying the groundwork for the designing practical TGL algorithms in future studies.", "sections": [{"title": "1 Introduction", "content": "Temporal graph learning (TGL) has emerged as an important machine learning problem and is widely\nused in a number of real-world applications, such as traffic prediction [Yuan and Li, 2021, Zhang et al.,\n2021], knowledge graphs [Cai et al., 2022, Leblay and Chekol, 2018], and recommender systems [Kumar\net al., 2019, Rossi et al., 2020, Xu et al., 2020a]. A typical downstream task of temporal graph learning\nis link prediction, which focuses on predicting future interactions among nodes. For example in an\nonline video recommender system, the user-video clicks can be modeled as a temporal graph whose\nnodes represent users and videos, and links are associated with timestamps indicating when users click\nvideos. Link prediction between nodes can be used to predict if and when a user is interested in a\nvideo. Therefore, designing graph learning models that can capture node evolutionary patterns and\naccurately predict future links is important.\nTGL is generally more challenging than static graph learning, thereby requiring more sophisticated\nalgorithms to model the temporal evolutionary patterns [Huang et al., 2023]. In recent years, many\nTGL algorithms [Kumar et al., 2019, Xu et al., 2020a, Rossi et al., 2020, Sankar et al., 2020, Wang\net al., 2021e] have been proposed that leverage memory blocks, self-attention, time-encoding func-\ntion, recurrent neural networks (RNNs), temporal walks, and message passing to better capture the\nmeaningful structural or temporal patterns. For instance, JODIE [Kumar et al., 2019] maintains a\nmemory block for each node and utilizes an RNN to update the memory blocks upon the occurance of\neach interaction; TGAT [Xu et al., 2020a] utilizes self-attention message passing to aggregate neighbor\ninformation on the temporal graph; TGN [Rossi et al., 2020] combines memory blocks with mes-\nsage passing to allow each node in the temporal graph to have a receptive field that is not limited\nby the number of message-passing layers; DySAT [Sankar et al., 2020] uses self-attention to capture\nstructural information and uses RNN to capture temporal dependencies; CAW [Wang et al., 2021e]\ncaptures temporal dependencies between nodes by performing multiple temporal walks from the root"}, {"title": "2 Related works and preliminaries", "content": "In this section, we briefly summarize related works and preliminaries. The more detailed discussions\non expressive power, generalization, and existing TGL algorithms are deferred to Appendix H.\nGeneralization and expressive power. Statistical learning theories have been used to study the\ngeneralization of GNNs, including uniform stability [Verma and Zhang, 2019, Zhou and Wang, 2021,\nCong et al., 2021], Rademacher complexity [Garg et al., 2020, Oono and Suzuki, 2020, Du et al., 2019],\nPAC-Bayesian [Liao et al., 2020], PAC-learning [Xu et al., 2020c], and uniform convergence [Maskey\net al., 2022]. Besides, [Souza et al., 2022, Gao and Ribeiro, 2022] study the expressive power of\ntemporal graph networks via graph isomorphism test. However, the aforementioned analyses are data-\nindependent and only dependent on the number of layers or hidden dimensions, which cannot fully\nexplain the performance difference between different algorithms. As an alternative, we study the\ngeneralization error of different TGL methods under the finite-wide over-parameterized regime [Xu\net al., 2021a, Arora et al., 2019, Arora et al., 2022], which not only is model-dependent but also could\ncapture the data dependency via feature-label alignment (FLA) in Definition 1. Most importantly,\nFLA could be empirically computed, reveals the impact of input data selection on model performance,\nand could be used as a proxy for the expressive power of different algorithms.\nTemporal graph learning. Figure 2 is an illus-\ntration of a temporal graph, where each node has\nnode feature $x_i$, each node pair could have mul-\ntiple temporal edges with different timestamps t\nand edge features $e_{ij}(t)$. We classify several cho-\nsen representative temporal graph learning meth-\nods into memory-based (e.g., JODIE), GNN-\nbased (e.g., TGAT, TGSRec), memory&GNN-\nbased (e.g., TGN, APAN, and PINT), RNN-\nbased (e.g., CAW), and GNN&RNN-based (e.g.,\nDySAT) methods. For example, JODIE [Kumar\net al., 2019] maintains a memory block for each"}, {"title": "3 Generalization of temporal graph learning methods", "content": "Firstly, we introduce the problem setting of theoretical analysis in Section 3.1. Then, we formally\ndefine the feature-label alignment (FLA) and derive the generalization bound in Section 3.2. Finally,\nwe provide discussion on the generalization bound and its connection to FLA in Section 3.3."}, {"title": "3.1 Problem setting for theoretical analysis", "content": "For theoretical analysis, let us suppose the TGL models receives a sequence of data points $(X_1,Y_1), ..., (X_N, Y_N)$\nthat is the realization of some non-stationary process, where $X_i$ and $y_i$ stand for a single input data\n(e.g., a root node and its induced subgraph or random-walk path) and its label at the i-th iteration.\nDuring training, stochastic gradient descent (SGD) first uses $(X_1,Y_1)$ and the initial model $f(\\theta_0)$ to\ngenerate the $\\theta_1$. Next, SGD uses the second example $(X_2, y_2)$ and the previously obtained model $f(\\theta_1)$\nto generate $\\theta_2$, and so on. The training process is outlined in Algorithm 1. We aim to develop a unified\ntheoretical framework to examine the generalization capability of the three most fundamental types of\nTGL methods, including GNN-based method, RNN-based method, and memory-based method on a node-\nlevel binary classification task. Our goal is to upper bound the expected 0-1 error $E[loss_i^{0-1}(\u0398)|D_{i-1}]$\nconditioned on a sequence of sampled data points $D_{i-1}$, where $loss_i^{0-1}(\u0398) = \\mathbb{1}{y_if_i(\u0398) <0}$ is the 0-1\nloss computed by model $f(\u0398)$ on data $(X_i, y_i)$ at i-th iteration, and $D_{i-1} = {(X_1, y_1), . . ., (X_{i-1}, Y_{i-1})}$\nis the set of all data points sampled before the i-th iteration. Our analysis of the fundamental TGL\nmethods can pave the way to understand more advanced algorithms, such as GNN&RNN-based and\nmemory&GNN-based methods.\nGNN-based method. We compute the representation of node $v_i$ at time $t$ by applying GNN on the\ntemporal graph that only considers the temporal edges with timestamp before time t. The GNN has\n$\\theta = {W^{(l)}}_{l=1}^L$ as the parameters to optimize and $\\alpha \\in {0,1}$ as a binary variable that controls whether\na residual connection is used. The final prediction on node $v_i$ is computed as $f_i(\u0398) = W^{(L)}h^{(L-1)}$,\nwhere the hidden representation $h^{(l-1)}$ is computed by\n$h^{(l)} = \u03c3(W^{(l)}(\\sum_{j \\in N(i)} P_{ij}h^{(l-1)})) + \u03b1 h^{(l-1)} \u2208 \u211d^m$,\n$h^{(1)} = \u03c3(W^{(1)}(\\sum_{j \\in N(i)} P_{ij}x_j)) \u2208 \u211d^m$.\nHere $\u03c3(\u00b7)$ is the activation function, $P_{ij}$ is the aggregation weight used for propagating information\nfrom node $v_j$ to node $v_i$, and $N(i)$ is the set of all neighbors of node $v_i$ in the temporal graph.\nFor parameter\ndimension, we have $W^{(1)} \u2208 \u211d^{m \\times d}, W^{(l)} \u2208 \u211d^{m \\times m}$ for $2 < l < L \u2212 1$, and $W^{(L)} \u2208 \u211d^{1 \\times m}$, where m is\nthe hidden dimension and d is the input dimension. TGAT [Xu et al., 2020a] can\nbe thought of as GNN-based method but uses self-attention neighbor aggregation.\nRNN-based method. We compute the representation of node $v_i$ at time t by applying a multi-step\nRNN onto a sequence of temporal events ${v_1, ..., v_{L-1}}$ that are constructed at the target node, where\neach temporal event feature $v_e$ is pre-computed on the temporal graph. We consider the time-encoding\nvector as part of event feature $v_e$. The RNN has trainable parameters $\u0398 = {W^{(1)}, W^{(2)}, W^{(3)}}$ and\n$\u03b1 \u2208 {0,1}$ is a binary variable that controls whether a residual connection is used. Then, the final\nprediction on node $v_i$ is computed as $f_i(\u0398) = W^{(3)}h_{L-1}$, where the hidden representation $h_{L-1}$ is\nrecursively compute by\n$h_l = \u03c3 (\u03ba(W^{(1)}h_{l-1} + W^{(2)}x_e)) + \u03b1 h_{l-1} \u2208 \u211d^m$,\n$\u03c3(\u00b7)$ is the activation function, $h_0 = 0^m$ is an all-zero vector, and $x_e = W^{(0)}v_e$. We normalize each\n$h_e$ by $\u03ba = 1/\u221a2$ so that $||h_e||_2$ does not grow exponentially with $L$. We have $W^{(1)}, W^{(2)} \u2208 \u211d^{m \\times m}$,\n$W^{(3)} \u2208 \u211d^{1 \\times m}$ as trainable parameters, but $W^{(0)} \u2208 \u211d^{m \\times d}$ is non-trainable. CAW [Wang et al., 2021e]\nis a special case of RNN-based method for edge classification tasks, where temporal events are sampled\nby temporal walks from both the source node and the destination node of an edge."}, {"title": "Memory-based method", "content": "We compute the representation of node $v_i$ at time t by applying weight\nparameters on the memory block $s_i(t)$. Let us define $\u0398 = {W^{(1)},...,W^{(4)}}$ as the parameters to\noptimize. Then, the final prediction of node $v_i$ is computed by $f_i(\u0398) = W^{(4)}s_i(t)$ and $s_i(t) \u2208 \u211d^m$ is\nupdated whenever node $v_i$ interacts with other nodes by\n$s_i(t) = \u03c3(\u03ba(W^{(1)}s_i(h_i) + W^{(2)}s_j(h_j) + W^{(3)}e_{ij}(t)))$,\nwhere $\u03c3(\u00b7)$ is the activation function, $h_i$ is the latest timestamp that node $v_i$ interacts with other\nnodes before time $t$, $s_i(0) = W^{(0)}x_i$, and $s_i^+(t) = StopGrad(s_i(t))$ is the memory block of node $v_i$ at\ntime $t$. We consider the time-encoding vector as part of edge feature $e_{ij}(t)$. We normalize $s_i(t)$ by\n$\u03ba = 1/\u221a3$ so that $||s_i(t)||_2$ does not grow exponentially with time $t$. We have trainable parameters\n$W^{(1)}, W^{(2)} \u2208 \u211d^{m \\times m}, W^{(3)} \u2208 \u211d^{m \\times d}, W^{(4)} \u2208 \u211d^m$ and fixed parameters $W^{(0)} \u2208 \u211d^{m \\times d}$. JODIE [Kumar\net al., 2019] can be thought of as memory-based method but using the attention mechanism for final\nprediction."}, {"title": "3.2 Assumptions and main theoretical results", "content": "For the purpose of rigorous analysis, we make the following standard assumptions on the feature\nnorms [Cao and Gu, 2019, Du et al., 2018], which could be satisfied via feature re-scaling.\nAssumption 1. All features has $l_2$-norm bounded by 1, i.e., we assume $||x_i||_2, ||e_{ij} (t) ||_2, ||v_e|| \u2264 1$.\nIn addition, we assume that the activation functions are Lipschitz continuous in TGL methods.\nThe following assumption holds for common activation functions such as ReLU and LeakyReLU (which\nare often used in GNNs), Sigmoid and Tanh (which are often used in RNNs).\nAssumption 2. The activation function has Lipschitz constant $\u03c1 \u2265 1$.\nFurthermore, we make the following assumptions on the propagation matrix of GNN models, which\nare previously used in [Liao et al., 2020, Cong et al., 2021]. In practice, we know that $\u03c4 = 1$ holds for\nrow normalized and $\u03c4 = \\sqrt{max_{i \u2208 v} d_i}$ holds for symmetrically normalized propagation matrix.\nAssumption 3. The row-wise sum is bounded by $\u03c4 = max_{i \u2208 v}(\\sum_{j \u2208 N(i)} P_{ij})$ where $\u03c4 > 1$.\nFinally, we adopt the following assumption regarding the non-stationary data generation process,\nwhich is standard assumption in time series prediction and online learning analysis [Kuznetsov and\nMohri, 2015, Kuznetsov and Mohri, 2016]. As the data generation process transitions to a stationary\nstate with an identical distribution at each step, this deviation $\u0394$ diminishes to zero.\nAssumption 4. We assume the discrepancy measure that quantifies the deviation between the inter-\nmediate steps (i.e., $i = 1, ..., N \u2013 1$) and the final step (i.e., $i = N$) data distribution as\n$\u0394 = sup_{f(\u0398)} \\frac{1}{N} \\sum_{i=1}^{N-1} |E [loss_i^{0-1} (\u0398_{i-1})|D_i] - \\frac{1}{N} \\sum_{i=1}^{N-1} E [loss_N^{0-1} (\u0398_{i-1})|D_i]|$,\nwhere the supremum is on any model, $D_{i\u22121} = {(X_1,Y_1), . . ., (X_{i\u22121}, Y_{i\u22121})}$ is the sequence of data\npoints before the i-th iteration, and $loss^{0-1}(\u0398) = \\mathbb{1}{y_if_i(\u0398) <0}$ is 0-1 loss.\nWe introduce the feature-label alignment (FLA) score, which measures how well the representations\nproduced by different algorithms align with the ground-truth labels.\nDefinition 1. FLA is defined as $y^T(JJ^T)^{-1}y$, where $J = [vec(\u2207_\u0398 f_i(\u0398_0))]_{i=1}^N$ is the gradient of different\ntemporal graph algorithms computed on each individual training example and $\u0398_0$ is the untrained weight\nparameters initialized from a Gaussian distribution.\nFLA has appeared in the convergence and generalization analysis of over-parameterized neural\nnetworks [Arora et al., 2019]. In practice, FLA quantifies the amount of perturbation we need on\n$\\Theta$ along the direction of $J = [vec(\u2207_\u0398 f_i(\u0398_0))]_{i=1}^N$ to minimize the logistic loss, which could be used to\ncapture the expressiveness of different TGL algorithms, i.e., the smaller the perturbation, the better the\nexpressiveness. Detailed discussion can be found in Appendix I.1. Computing FLA requires $O(N^2|\u0398|)$"}, {"title": "3.3 Discussion on the generalization bound: insights and limitations", "content": "Dependency on depth and steps. The generalization error of GNN- and RNN-based methods\ntends to increase as the number of layers/steps L increases. This partially explains why the hyper-\nparameter selection on L is usually small for those methods. For example, GNN-based method TGAT\nuses 2-layer GNN (i.e., L = 3), RNN-based method CAW selects 3-steps RNN (i.e., L = 4), and\nGNN&RNN-based method DySAT uses 2-layer GNN and 3-steps RNN to achieve outstanding perfor-\nmance. On the other hand, memory-based method JODIE alleviates the dependency issue by using\nmemory blocks and can leverage all historical interactions for prediction, which enjoys a generalization\nerror independent of the number of steps. However, since gradients cannot flow through the memory\nblocks due to \u201cstop gradient\", its expressive power may be lower than that of other methods, which\nwill be further elaborated when discussing the impact of feature-label alignment. Memory&GNN-based\nmethods TGN and APAN alleviate the lack of expressive power issue by applying a single-layer GNN\non top of the memory blocks.\nDependency on feature-label alignment (FLA). Although the dependency on the number of\nlayers/steps of GNN/RNN can partially explain the performance disparity between these methods,\nit is still not clear if using \"stop gradient\" in the memory-based method and the selection of input\ndata (e.g., using recent or uniformly sampled neighbors in a temporal graph) can affect the model\nperformance. In the following, we take a closer look at the FLA score, which is inversely correlated to\nthe generalization ability of the TGL models, i.e., the smaller the FLA, the better the generalization\nability. According to Figure 3, we observe that \u2460 JODIE (memory-based) has a relatively larger FLA\nscore than most of the other TGL methods. This is potentially due to \"stop gradient\u201d operation that\nprevents gradients from flowing through the memory blocks and could potentially hurt the expressive\npower. APAN and TGN (memory&GNN-based) alleviate the expressive power degradation issue\nby applying a single layer GNN on top of the memory blocks, resulting in a smaller FLA than the\npure memory-based method. \u2462 TGAT (GNN-based) has a relatively smaller FLA score than other\nmethods, which is expected since GNN has achieved outstanding performance on static graphs. 4\nDySAT (GNN&RNN-based) is originally designed for snapshot graphs, so its FLA score might be\nhighly dependent on the selection of time-span size when converting a temporal graph to snapshot\ngraphs. A non-optimal choice of time-span might cause information loss, which partially explains why\nDySAT'S FLA is large. Additionally, the selection of the input data also affects the FLA. We will\ndiscuss further in the experimental section with empirical validation and detailed analysis.\""}, {"title": "4 A simplified algorithm", "content": "Informed by our theoretical analysis, we introduce Simplified-Temporal-Graph-Network (SToNe) that\nnot only enjoys a small generalization error but also empirically works well. The design of SToNe is\nguided by the following key insights that are presented in Section 3.3:\n\u2022 Shallow and non-recursive network. This is because the generalization error increases with respect\nto the number of layers/steps in GNN-/RNN-based methods, which motivates us to consider a\nshallow and non-recursive neural architecture to alleviate such dependency.\n\u2022 Selecting proper input data instead of using memory blocks. Although memory blocks could alleviate\nthe dependency of generalization error on the number of layers/steps, it will also affect the FLA\nand hurt the generalization ability of the models. As an alternative, we propose to capture the\nimportant historical interactions by empirically selecting the proper input data.\nTo this end, we introduce the data preparation and the neural architecture in Section 4.1, then\nhighlight the key features of SToNe that can differentiate itself from existing methods in Section 4.2."}, {"title": "4.1 Simplified temporal graph network: input data and neural architecture", "content": "Input data preparation. To compute the representation of node $v_i$ at time $t$, we first identify\nthe K most recent nodes that have interacted with $v_i$ prior to time $t$ and denote them as temporal\nneighbors $N_k(v_i)$. Then, we sort all nodes inside $N_k(v_i)$ by the descending temporal order. If a\nnode $v_j$ interacts with node $v_i$ multiple times, each interaction is treated as a separate temporal\nneighbor. For example in Figure 2, for any time $t > t_6$ and large enough constant $K$, we have\n$N_k(v_4) = {(v_5, t_6), (v_2, t_4), (v_2, t_3)}$ in the descending temporal order. For each temporal neighbor\n$v_j \u2208 N_K (v_i, t)$, we represent its interaction with the target node $v_i$ at time $t'$ using a combination of edge\nfeatures $e_{ij}(t') \u2208 \u211d^{d_e}$, time-encoding $(t \u2212 t') \u2208 \u211d^{d_t}$, and node features $x_i, x_j \u2208 \u211d^{d_n}$, which is denoted\nas $u_{ij} (t') = [e_{ij} (t') || \u03c8(t \u2212 t') || x_i || x_j]$. Then, we define $H_i(t) = {u_{ij}(t') | (v_j, t') \u2208 N_k(v_i)}$ as the\nset of all features that represent the temporal neighbors of node $v_i$ at time $t$, where all vectors in $H_i(t)$\nare ordered by the descending temporal order. For example, the interactions between the temporal\nneighbors of node $v_4$ at time $t > t_6$ to its root node in Figure 2 are $H_4(t) = {u_{4,5} (t_6), u_{4,2}(t_4), u_{4,2}(t_3)}$,\nwhere $u_{4,5}(t_6) = [e_{4,5}(t_6) || \u03a8(t - t_6) || x_4 || x_5], u_{4,2}(t_4) = [e_{4,2}(t_4) || \u03a8(t - t_4) || x_4 || x_2]$, and\n$u_{4,2}(t_3) = [e_{4,2}(t_3) || \u03a8(t - t_3) || x_4 || x_2]$. The time-encoding function in SToNe is defined as\n$\u03c8(t \u2212 t') = cos((t \u2013 t')\u03c9)$, where $\u03c9 = [\u03b1^{-(i\u22121)/d_t}\u03b2]_{i=1}^{d_t}$ is a fixed $d_t$-dimensional vector and $\u03b1 = \u03b2 =\n\u221a{d_t}$. Notice that a similar time-encoding function is used in other temporal graph learning methods,\ne.g., [Kumar et al., 2019, Xu et al., 2020a, Rossi et al., 2020, Wang et al., 2021e, Cong et al., 2023].\nEncoding features via GNN. STONe is a GNN-based method with trainable parameters $\u0398 =$\n${\u03b1, W^{(1)}, W^{(2)}}$, where $\u03b1 \u2208 \u211d^K, W^{(1)} \u2208 \u211d^{d_{hidden} \\times d_{in}}, and W^{(2)} \u2208 \u211d^{d_{out} \\times d_{hidden}}$. Here $K$ is the maximum\ntemporal neighbor size we consider, $d_{in} = d_e + d_t + 2d_n$, and $d_{hidden}, d_{out}$ are the dimensions of hidden\nand output representations. The representation of node $v_i$ at time $t$ is computed by\n$h_i (t) = W^{(2)} LayerNorm (z_i(t)), z_i(t) = \u03c3 (\\sum_{k=1}^{|H_i(t)|} \u03b1_k W^{(1)}[H_i(t)]_k)$.\nIf the temporal neighbor size of node $v_i$ is less than K, i.e., $|H_i(t)| <K$, we only need to update the first\n$|H_i(t)|$ entries of the vector \u03b1. Notice that the number of parameters in SToNe is $(K + d_{hidden}d_{in} + d_{hidden}d_{out})$,\nwhich is usually fewer than other TGL algorithms given the same hidden dimension size. We will report\nthe number of parameters and its computational cost in the experiment section.\nLink prediction via MLP. When the downstream task is future link prediction, we predict\nwhether an interaction between nodes $v_i, v_j$ occurs at time $t$ by applying a 2-layer MLP model on"}, {"title": "4.2 Comparison to existing methods", "content": "Comparison to TGAT. GNN-based method TGAT uses 2-hop uniformly sampled neighbors and\naggregates the information using a 2-layer GAT [Veli\u010dkovi\u0107 et al., 2017]. The neighbor aggregation\nweights in TGAT are estimated by self-attention. In contrast, SToNe uses 1-hop most recent neighbors\nand directly learns the neighbor aggregation weights a as shown in Eq. 1. Moreover, self-attention in\nTGAT can be thought of as weighted average of neighbor information, while SToNe can be thought\nof as sum aggregation, which can better distinguish different temporal neighbors, and it is especially\nhelpful when node and edge features are lacking.\nComparison to TGN. Memory&GNN-based method TGN uses 1-hop most recent temporal neigh-\nbors and applies a self-attention module to the sampled temporal neighbors' features that are stored\ninside the memory blocks. In fact, SToNe can be thought of as a special case of TGN, where we use\nthe features in $H_i(t)$ instead of the memory blocks and directly learn the neighbor aggregation weight\na instead of using the self-attention aggregation as shown in Eq. 1.\nComparison to GraphMixer. SToNe could be think of as a simplified version of [Cong et al.,\n2023] that addresses the high computation cost associated with the MLP-mixer used for temporal ag-\ngregation [Tolstikhin et al., 2021]. Instead of relying on the MLP-mixer, we introduce the aggregation\nvector \u03b1 and employ linear functions parameterized by $W^{(1)}$ and $W^{(2)}$ for aggregation. Additionally,\nwe do not explicitly model the graph structure through [Cong et al., 2023]'s node-encoder. Instead,\nwe implicitly capture the node features within the temporal interactions present in $H(t)$. Our exper-\niments demonstrate that SToNe significantly reduces the model complexity (Table 2) while achieving\ncomparable performance (Figure 4 and Table 1).\nTemporal graph construction. Most of the TGL methods [Kumar et al., 2019, Xu et al., 2020a,\nRossi et al., 2020, Sankar et al., 2020, Wang et al., 2021e] implements temporal graphs as directed graph\ndata structure with information only flowing from source to destination nodes. However, we consider\nthe temporal graph as an bi-directed graph data structure by assuming that information also flow from\ndestination to source nodes for SToNe. This means that the \"most recent 1-hop neighbors\" sampled\nfor the two nodes on the \"bi-directed\u201d temporal graph should be similar if two nodes are frequently\nconnected in recent timestamps. Such similarity provides information on whether two nodes are\nfrequently connected in recent timestamps, which is essential for temporal graph link prediction [Cong\net al., 2023]. For example, let us assume nodes $v_i, v_j$ interacts at time $t_1, t_2$ in the temporal order.\nThen, given any timestamp $t > t_2$ and a large enough K, the 1-hop temporal neighbors on the bi-\ndirected graph is $N_k(v_i) = {(v_j, t_1), (v_j, t_2)}$ and $N_k(v_j) = {(v_i, t_1), (v_i, t_2)}$, while on directed graph\nis $N_k(v_i) = {(v_j, t_1), (v_j, t_2)}$ but $N_k(v_j) = \u2205$. Intuitively, if two nodes are frequently connected in\nrecent timestamps, they are also likely to be connected in the near future. In the experiment section,\nwe show that changing the temporal graph from bi-directed to directed can negatively impact the\nfeature-label alignment and model performance."}, {"title": "5 Experiments", "content": "We compare SToNe with several TGL algorithms under the transductive learning setting. We conduct\nexperiments on 6 real-world datasets, i.e., Reddit, Wiki, MOOC, LastFM, GDELT, and UCI. Similar\nto many existing works, we also use the 70%/15%/15% chronological splits for the train/validation/test\nsets. We re-run the official released implementations on benchmark datasets and repeat 6 times with\ndifferent random seeds. Please refer to Appendix A for experiment setup details."}, {"title": "5.1 Experiment results", "content": "Comparison on average precision. We compare the average precision score with baseline methods\nin Table 1. We observe that SToNe could achieve compatible or even better performance on most\ndatasets. In particular, the performance of SToNe outperforms most of the baselines with a large\nmargin on the LastFM and UCI dataset. This is potentially because these two datasets lack node/edge\nfeatures and have larger average time-gap (see dataset statistics in Table 3). Since baseline methods\nrely on RNN or self-attention to process the node/edge features, they implicitly assume that the\nfeatures exists and are \"smooth\" at adjacent timestamps, which could generalize poorly when the\nassumptions are violated. PINT addresses this issue by pre-processing the dataset and generating\nits own positional encoding as the augment features, therefore it achieves better performance than\nSToNe on UCI dataset. However, computing this positional encoding is time-consuming and does\nnot always perform well on other datasets. For example, PINT requires more than 400 hours to\ncompute the positional features on GDELT, which is infeasible. Additionally, the performance of\nSToNe closely matches that of GraphMixer, but with significantly lower model complexity, which\nwill be demonstrated in Table 2. SToNe exhibits improved and more consistent performance on UCI\ndataset, because SToNe is less susceptible to overfitting on small dataset.\nComparison on generalization performance. To validate the generalization performance of\nSToNe, we compare the average precision score and the generalization gap in Figure 4. The general-\nization gap is defined as the absolute difference between the training and validation average precision\nscores. Our results show that SToNe, similar to GraphMixer, consistently achieves a higher average\nprecision score faster than other baselines, has a smaller generalization gap, and has relatively stable\nperformance across different epochs. This suggests that SToNe has better generalization performance\ncompared to the baselines. In particular on the UCI dataset, SToNe is less prone to overfit and its\ngeneralization gap increases more slowly than all other baseline methods.\nComparison on model complexity. We compare the number of parameters (including trainable\nweight parameters and memory block size) and wall-clock time per epoch during the training phase in\nTable 2. Our results show that SToNe has fewer parameters than all the baselines, and its computation\ntime is also faster than most baseline methods. Note that some baselines also require significant time\nfor data preprocessing, which is not included in Table 2. For example, PINT takes more than 84 hours\nto pre-compute the positional encoding on the Reddit dataset, which is significantly longer than its"}, {"title": "6 Conclusion", "content": "In this paper, we study the generalization ability of various TGL algorithms. We reveal the re-\nlationship between the generalization error to \u201cthe number of layers/steps"}]}