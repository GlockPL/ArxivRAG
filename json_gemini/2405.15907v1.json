{"title": "Belief-State Query Policies for Planning With Preferences Under Partial Observability", "authors": ["Daniel Bramblett", "Siddharth Srivastava"], "abstract": "Planning in real-world settings often entails addressing partial observability while aligning with users' preferences. We present a novel framework for expressing users' preferences about agent behavior in a partially observable setting using parameterized belief-state query (BSQ) preferences in the setting of goal-oriented partially observable Markov decision processes (gPOMDPs). We present the first formal analysis of such preferences and prove that while the expected value of a BSQ preference is not a convex function w.r.t its parameters, it is piecewise constant and yields an implicit discrete parameter search space that is finite for finite horizons. This theoretical result leads to novel algorithms that optimize gPOMDP agent behavior while guaranteeing user preference compliance. Theoretical analysis proves that our algorithms converge to the optimal preference-compliant behavior in the limit. Empirical results show that BSQ preferences provide a computationally feasible approach for planning with preferences in partially observable settings.", "sections": [{"title": "1 Introduction", "content": "Users of sequential decision-making (SDM) agents in partially observable settings often have preferences on expected behavior, ranging from safety concerns to high-level knowledge of task completion requirements. However, users are ill-equipped to specify desired behaviors from such agents. For instance, although reward engineering can often encode fully observable preferences [Devidze et al., 2021, Gupta et al., 2023], it requires significant trial-and-error, and leads to unintended behavior even when done by experts working on simple domains [Booth et al., 2023]. These challenges are compounded in partially observable environments, where the agent will not know the full state on which the users' preferences are typically defined. Furthermore, expressing preferences in partially observable settings by defining reward functions on belief states can result in wireheading [Everitt and Hutter, 2016] (see Sec. 2 for further discussion on related work).\nConsider a simplified, minimal example designed to illustrate the key principles (Fig. 1(a)). A robot located on a spaceship experiences a communication error with the ship and needs to decide whether to attempt to repair itself or the ship. Importantly, while a robot error is harder to detect, the user would rather risk repairing the robot than repairing the ship, as each repair risks introducing additional failures. In other words, the user may expect the robot to work with the following goals and preferences: The objective is to fix the communication channel. If there is a \u201chigh\u201d likelihood that the robot is broken, it should try to repair itself; otherwise, if there is a \u201chigh\u201d likelihood that the ship is broken, it should try to repair that. Such preferences go beyond preferences in fully observable settings: they use queries on the current belief state for expressing users' preferences while using the conventional paradigm of stating objectives in terms of the true underlying state (fixing the communication channel). Such a formulation avoids wireheading, allowing users to express"}, {"title": "2 Related Work", "content": "Planning over preferences has been well studied in fully observable settings [Baier et al., 2007, Aguas et al., 2016]. Voloshin et al. [2022] present an approach for complying with an LTL specification while carrying out reinforcement learning. Other approaches for using LTL specifications use the grounded state to create a reward function to teach reinforcement learning agents [Toro Icarte et al., 2018, Vaezipoor et al., 2021]. These approaches do not extend to partially observable settings as they consider agents that can access the complete state.\nIn partially observable settings, existing approaches for using domain knowledge and preferences require extensive, error-prone reward design and/or do not guarantee compliance. LTL specifications have been incorporated either by designing a reward function that incentivizes actions more likely to adhere to these specifications [Liu et al., 2021, Tuli et al., 2022] or by imposing a compliance threshold [Ahmadi et al., 2020]. In both approaches, the user calibrates rewards for preference compliance with those for objective completion; it is difficult to ensure compliance with preferences. We focus on the problem of guaranteeing preference compliance without reward engineering.\nMazzi et al. [2021, 2023] proposed expressing domain control knowledge using belief state probabilities. Mazzi et al. [2021] used expert-provided rule templates and execution traces to construct a shield to prevent irregular actions. Mazzi et al. [2023] used execution traces and domain-specified belief-state queries to learn action preconditions over the belief state. Both approaches use input"}, {"title": "3 Formal Framework", "content": "This section formally defines the belief-state query (BSQ) preference framework. Our framework is built for relational goal-oriented partially observable Markov decision processes (gPOMDP)."}, {"title": "3.1 Goal-Oriented Partially Observable Markov Decision Process (gPOMDP)", "content": "Partially observable Markov decision processes (POMDPs) constitute a standard mathematical framework for modeling sequential decision-making problems in partially observable, stochastic settings [Kaelbling et al., 1998, Smallwood and Sondik, 1973]. State-of-the-art POMDP solvers often rely on approximate online approaches [Silver and Veness, 2010, Somani et al., 2013] where recent work addresses the problem of obtaining performance bounds [Barenboim and Indelman, 2023, Lim et al., 2023].\nWe use goal-oriented POMDPs (gPOMDPs), where the agent tries to complete one of the tasks/goals. This eliminates the burden of error-prone reward engineering by using a default cost function that associates a constant cost for each timestep before reaching the goal. E.g., the Spaceship Repair problem (Sec. 1) has two objects: the robot and the spaceship. A state is defined using a Boolean function $broken(o)$ representing whether object $o$ needs repair and an integer-valued function $rlocation()$ representing the robot's location. These functions are not observable. The agent has two types of actions: try to repair object $o$ ($repair(o)$) or wait ($wait()$). A transition function expresses the distribution of $rlocation()$ depending on the action taken and the robot's previous location. At each timestep, the robot receives a noisy observation $obs_err(o)$ regarding the status of object $o$. Thus the set of observations can be expressed as {$obs_err(robot), obs_err(ship)$}. Due to noisy perception, $obs_error(o)$ may not match $broken(o)$. An observation function denotes the probability of each observation conditioned on the (hidden) current state. E.g., $Pr(obs_err(robot) = 1|broken(robot) = 1) = 0.75$. The goal is to reach the repair station corresponding to the truly broken component. We define gPOMDPs formally as follows.\nDefinition 1. A goal-oriented partially observable Markov decision process (gPOMDP) P is defined as $\\langle C, F, A, O, \\mathcal{T}, \\Omega, G, Cost, H, b_o \\rangle$ where C is the finite set of constant symbols and F is the finite set of functions. The set of state variables for F, $V_F$, is defined as all instantiations of functions in F with objects in O. The set of states S is the set of all possible valuations for $V_F$; A is a finite set of actions, O is a subset of F of observation predicates, $\\mathcal{T} : S \\times A \\times S \\rightarrow [0, 1]$ is the transition function $\\mathcal{T}(s, a, s') = Pr(s'|a, s)$; G = S is the set of goal states that are also sink states, $\\Omega : S \\times A \\times O \\rightarrow [0,1]$ is the observation function; $\\Omega(s, a, o) = Pr(o|s,a)$, $Cost(s) = \\{0 if s \\in G;else 1\\}$ is the cost function, H is the horizon, and $b_o$ is the initial belief state. A solution for a gPOMDP is a policy that has a non-zero probability of reaching G in H \u2013 1 timesteps."}, {"title": "3.2 Belief-State Queries (BSQs) and Preferences", "content": "Computing a policy for any gPOMDP requires planning around state uncertainty. This is done using the concept of a belief state, which is a probability distribution over the currently possible states. Formally, the belief state constitutes a sufficient statistic for observation-action histories [Astrom et al., 1965]. We express user preferences using queries on the current belief state.\nFor any belief state b, when action a is taken and observation o is observed, the updated belief state is computed using $b'(s') = \\alpha \\Omega(s', a, o) \\sum_s \\mathcal{T}(s, a, s')b(s)$ where $\\alpha$ is the normalization factor. We refer to this belief propagation as $b' = bp(b, a, o)$. We extend the notation to refer to the sequential application of this equation to arbitrary bounded histories as $bp^* (b_o, a_1, o_1, ..., a_n, o_n) = bp(... bp(bp(b_o, a_1, o_1), a_2, o_2) ...)$.\nFor example, the user preference in the Spaceship Repair problem has the expression \u201ca high likelihood that the robot is broken\". This can be expressed as a query on a belief state b:\""}, {"title": "4 Formal Analysis", "content": "Our main theoretical result is that the continuous space of policy parameters is, in fact, partitioned into finitely many constant-valued convex sets. This insight allows the development of scalable algorithms for computing low-cost preference-compliant policies. We introduce formal concepts and key steps in proving this result here; complete proofs for all results are available in the appendix. We begin with the notion of strategy trees to conceptualize the search process for BSQ policies."}, {"title": "4.1 Strategy Trees", "content": "Every BSQ preference $\\pi(b, \\Theta)$ and a gPOMDP P defines a strategy tree (e.g., Fig. 2(a)) that captures the possible decisions at each execution step. Intuitively, the tree starts at a belief node representing"}, {"title": "4.2 BSQ Policies are Piecewise Constant", "content": "We now use the concept of braids to prove that the continuous, high-dimensional space of parameter values of a BSQ preference reduces to a finite set of contiguous, convex partitions with each partition having a constant expected cost. This surprising result implies that although the expected cost of BSQ policies is not a concave function of parameter assignments, optimizing a BSQ preference requires optimization over a finite set rather than over a continuous space. We first define a notion of similarity over assignments to BSQ preferences that define BSQ policies:"}, {"title": "5 Partition Refinement Search", "content": "In this section, we present a novel algorithm for optimizing the parameters for a BSQ preference using the theory of braids developed above. The Partition Refinement Search (PRS)(Algo. 1) constructs the set of partitions using hierarchical partition selection and refinement, where a partition is selected to be refined, a leaf that can occur in that partition is sampled and evaluated, and the partitions are refined to isolate the interval of the braid corresponding to the sample. It keeps track of the hypothesized optimal partition $X_{opt}$ with $X_{opt}$ being the final result returned after timeout.\nPRS constructs the first parameter space interval as the domain of all possible parameter values (line 3). This is set as the initial hypothesized optimal partition (line 4). In each iteration, a partition p is selected using exploration-exploitation approaches discussed in Sec. 5.1 (lines 6). A leaf l is sampled from p by uniformly sampling pa- rameter value v from p's parameter intervals and"}, {"title": "5.1 Partition Selection Approaches", "content": "We explored multiple partition selection approaches with a multiprocessing version of PRS in line 6. Each approach used the same dynamic exploration rate $\\epsilon_r$ that diminished over time. Each thread managed a subset of partitions $X' \\subseteq X$ and updated a global hypothetical optimal partition. Additionally, new partitions were selected for sampling if they were below five samples. In this paper, we focus on three selection approaches and discuss two others in the Appendix."}, {"title": "6 Empirical Results", "content": "We created an implementation of PRS and evaluated it on four challenging risk-averse problems. Complete source code is available in the supplementary material. We describe the problems and user preferences here; further details, including BSQ preference listings, can be found in the appendix."}, {"title": "6.1 Analysis of Results", "content": "We implemented a baseline approach, RCompliant, where we uniformly select random parameter values from the parameter space to produce preference-compliant policies. This baseline allowed us to measure the benefits of solving the BSQ preference's parameters. For each problem, we evaluated RCompliant and each PRS variant ten times, solving for a horizon of 100 with a 25-minute timeout."}, {"title": "7 Conclusion", "content": "We presented a formal framework for expressing users' preferences and for reliably computing policies compliant with them in partially observable settings. We performed a formal analysis of these policies and showed that the parameter value space introduced in the BSQ preferences can be partitioned, resulting in BSQ preferences being optimizable through a hierarchical optimization paradigm. We introduced the probabilistically complete Partition Refinement Search algorithm to perform this optimization. Our empirical results show that this solver produces well-performing preference-compliant policies. Results indicate that BSQ preferences provide a promising approach for solving a diverse set of real-world problems that require user preference compliance.\nOur future work is to enhance the capabilities of BSQ preferences by enabling them to manage approximate belief states, incorporate memory, and generate stochastic actions. Additionally, we plan to develop methods for constructing BSQ preferences, either through elicitation of user preferences or by computing the BSQ preference needed for solving a gPOMDP."}, {"title": "A Appendix: Lemmas and Proofs From Formal Analysis [Section 3]", "content": "In this section, we provide the formal proofs for Lemma 1, Theorem 1, Theorem 2, and Theorem 3 from Section 3, where we proved that braids partition the parameter space resulting in the expected cost function of a BSQ preference w.r.t its parameter being piecewise constant. We define and prove Lemmas 2, 3, 4, and 5 in this section for building these proofs.\nFirst, we prove that the similarity operator $=_H$ for braids (Def. 9) has the properties of being reflexive, symmetric, and transitive. As such, $=_H$ defines an equivalence relation over the n-dimensional parameter space $R^n$ meaning it defines a partition over $R^n$.\nTheorem 1. Let $\\pi(b, \\Theta)$ be a BSQ preference, P be a gPOMDP, $b_o$ be the initial belief state, and H be the horizon. The operator $=_H$ partitions $R^n$.\nProof. Let $\\vartheta \\in R^n$ be n-parameter values and H be the horizon. By way of contradiction, let's assume that $\\vartheta$ is not similar to itself, $\\vartheta \\neq_H \\vartheta$. This would mean that $braid_{H,1}(\\vartheta) \\neq braid_{H,2}(\\vartheta)$. As such, there must exist a leaf l, which is in one but not the other braid. Note that l represents a unique rule-observation trajectory $(r_1, o_1, ..., r_H, o_H\\}$. Additionally, for l to be in one of these braids it would need to be true that $\\forall i, r_i(\\vartheta)$ must be satisfied, where $b = bp^* (b_o, r_1, o_1, ..., r_i, o_i)$ (Def. 7). However, note that this would hold true for the other braid as well, making it a contradiction for l to be exclusive in either $braid_{H,1}(\\vartheta)$ or $braid_{H,2}(\\vartheta)$. As such, l must be similar to itself meaning the similarity property holds.\nLet $\\vartheta_1, \\vartheta_2, \\vartheta_3 \\in R^n$ where $\\vartheta_1 =_H \\vartheta_2$ and $\\vartheta_2 =_H \\vartheta_3$. Therefore, $braid_{\\pi,H}(\\vartheta_1) = braid_{\\pi,H}(\\vartheta_2)$ and $braid_{\\pi,H} (\\vartheta_2) = braid_{\\pi,H}(\\vartheta_3)$ (Def. 7). Using substitution, $braid_{\\pi,H}(\\vartheta_1) = braid_{\\pi,H}(\\vartheta_3)$ meaning $\\vartheta_1 =_H \\vartheta_3$. As such, the transitive property holds.\nDue to set equality being symmetric, the symmetric property holds. Thus, the operator $=_H$ is an equivalence relation over $R^n$ causing $=_H$ to define a partition over $R^n$.$\\square$\nFor compound BSQs $\\Psi$, we now prove that there exist unique intervals of the parameter space where $\\Psi$ is satisfied that we can calculate.\nLemma 1. Let $\\Psi(b; \\Theta)$ be an n-dimensional compound BSQ. There exists a set of intervals $I(\\Psi) \\subseteq R^n$ s.t. $\\Psi(b; \\Theta)$ evaluates to true iff $\\Theta \\in I(\\Psi)$.\nProof. Let P be a gPOMDP, b be a belief state, $\\Theta \\in R$ be a parameter, and $\\varphi$ be first-order logic formula composed of functions from P. There exist two possible forms for a BSQ (Def. 2). Let $A_{\\varphi}(b; \\varphi, \\Theta) = Pr[\\varphi]_b \\circ \\Theta$. Note that $Pr[\\varphi]_b$ evaluates to the probability of $\\varphi$ being satisfied in a belief state b. Therefore, we can simplify $A_{\\varphi}(b; \\varphi, \\Theta)$ to $p \\circ \\Theta$ where p $\\in R^1$, meaning this type of BSQ simplifies to an inequality. Now, let $A_{\\varphi}(b; \\varphi, \\Theta) = Pr[\\varphi]_b == 1$ where $\\varphi$ is composed of fully observable function in P and . We assume that cannot be used as function parameter meaning that it must be an operand of a relational operator in $\\varphi$. Since the functions are fully observable, they can be evaluated for b, leaving just the inequalities involving $\\Theta$ to dictate whether $\\varphi$ is satisfied. Thereby, BSQs evaluate to inequalities involving $\\Theta$.\nA compound BSQ $\\Psi$ is composed of conjunctions/disjunctions of BSQs by Definition 3. By substituting each BSQ with its inequalities we can calculate the interval of $\\Psi$, $I(\\Psi)$.\nLet us assume that $\\Theta \\in I(\\Psi)$. By way of contradiction, let us assume that $\\Theta$ does not satisfy $\\Psi$. If $\\Psi$ is a conjunction of BSQs, there exists at least one BSQ that is not satisfied by $\\Theta$. If $\\Psi$ is a disjunction, all the BSQs are not satisfied by $\\Theta$. However, this would mean that cannot satisfy the inequalities from these BSQs, so $\\Theta$ cannot be in I($\\Psi$), which is a contradiction. Going the other way, let us assume that $\\Theta$ satisfies $\\Psi$. This means one or all the BSQs are satisfied by $\\Theta$ depending on if I is a conjunction or disjunction. If was not in I($\\Psi$) there could not exist a set of BSQs satisfied for $\\Psi$ to be satisfied.\nThus, for a belief state b, a n-parameter compound BSQ $\\Psi$ has an interval in the parameter space I($\\Psi$) s.t. $\\forall \\Theta \\in R^n$, $\\Theta \\in I(\\Psi)$ iff $\\Psi(b; \\Theta)$ evaluates true. $\\square$"}, {"title": "B Appendix: Proofs For Partition Refinement Search [Section 5]", "content": "In this section, we provide the formal proof for Theorem 4 proving that the Partition Refinement Search (PRS) algorithm introduced in Section 5 is probabilistically complete. We define and prove Lemmas 6, 7, and 8 for building this proof."}, {"title": "C Appdenix: Spaceship Repair Partitions Closed Form", "content": "In this section, we calculate the braids that partition the parameter space for the Spaceship Repair problem with the BSQ preference from Fig. 1."}, {"title": "D Appendix: Evaluation Problem's Belief-State Query Preferences", "content": "In this section, we provide the BSQ preferences for the Lane Merger, Graph Rock Sample, and Store Visit problems discussed in Section 6. To do this, we first describe the functions that compose each problem's state and action. We use loops and quantifiers in the BSQ preferences for clarity that can be unrolled on a problem-by-problem basis."}, {"title": "D.1 Lane Merger", "content": "The Lane Merger problem is that there are two lanes, and the agent must merge into the other lane within a certain distance. In this other lane, there is another car whose exact location and speed are unknown. Therefore, there exist two objects in the environment: the agent (agent) and the other car (other). For either object o, the location and speed are tracked using the unary integer functions $loc(o)$ and $speed(o)$. For actions, the agent can increase their speed ($speed_up()$), decrease their speed ($slow_down()$), remain in their current lane at their current speed ($keep_speed()$), or attempt to merge lanes ($merge()$). Using these functions, the BSQ preference $\\pi_{lm}(b; \\Theta_1, \\Theta_2)$ is formally defined as follows."}, {"title": "D.2 Graph Rock Sample", "content": "The Graph Rock Sample problem is that there is a rover with pre-programmed waypoints, where some waypoints contain rocks. These rocks have been categorized into types, and whether it is safe for the rover to sample them is unknown. The objective of the rover is to sample each type with a safe rock before traversing to a dropoff location. The objects are the waypoints, including the rocks"}, {"title": "D.3 Store Visit", "content": "The Store Visit problem involves an agent in a city with a grid-based layout. Some locations are unsafe, while others contain a bank or a store. The objective is for the agent to visit a bank safely and then a store. The objects are the agent, the set of stores $\\{s_1, ..., s_n\\}$, and the set of banks $\\{b_1, ..., b_m\\}$. Labeling functions $bank(o)$ and $store(o)$ check whether object o is a bank or store, respectively. The ternary Boolean function keeps track of the current (x, y) location of the object o, $loc(o, x, y)$. Similarly, whether location (x, y) is safe is tracked by the binary Boolean function $is_safe(x, y)$. Lastly, the state keeps track of whether the agent has visited a bank using the nullary Boolean function $vbank()$. The agent can move left ($left()$), right ($right()$), up ($up()$), and down ($down()$) in the grid.\nThe agent can also visit a building in its current location ($visit()$) or scan its surroundings to figure out its location ($scan()$). Using these functions, the BSQ preference $\\pi_{sv}(b; \\Theta_1, \\Theta_2, \\Theta_3)$ is formally defined as follows."}, {"title": "E Appendix: Experimental Setup And Computational Cost", "content": "In this section, we go through the empirical setup of the experiments performed in Section 6 and include an estimate of the computation cost for running the experiments for this paper.\nAll experiments were performed on an Intel(R) Xeon(R) W-2102 CPU @ 2.90GHz without using a GPU. The Partition Refinement Search algorithm was implemented using a manager-worker design pattern where 8 workers were initialized when solving. The manager maintained the hypothesized optimal partition and current exploration rate. Every 12.5 seconds, the current hypothesized optimal partition was recorded. At exactly 25 minutes, the hypothesized optimal partition was recorded as the solution before closing the workers.\nBoth solutions and recorded hypothesized optimal partitions were evaluated using the same random seed to ensure that the same initial states were assessed. This evaluation process was carried out in parallel using a manager-worker design pattern with 16 workers. 25,000 independent runs were conducted for each solution to determine the expected cost and goal achievement rate. Additionally, for each recorded hypothesized optimal partition, 10,000 runs were performed. The average perfor- mance and standard deviation error were calculated by averaging the results of ten runs for each combination of problem and solver. A similar approach was used to evaluate the random-parameter user-compliant policy RCompliant. Instead of using solved policies, ten parameter value sets were uniformly at random selected from the parameter space, and each set was evaluated for 25,000 runs. These results are presented in Figure 3.\nFor constructing the Spaceship Repair heatmap (Figure 1), all combinations of parameters $\\Theta_1$ and $\\Theta_2$ were evaluated with parameter values sampled from 0 to 1 with increments of 0.002. This produced 251,001 equally-spaced parameter values. Parameter values were evaluated on 300 runs with a horizon of 12 to calculate the expected cost."}, {"title": "F Appendix: Additional Results", "content": "In this section, we provide additional results from the experiments performed. This includes intro- ducing two additional partition selection approaches we evaluated: Global Thompson Sampling (PRS_Global) and Maximum Confidence (PRS_Max). We also provide graphs of the performance of the hypothesized optimal partition while solving the Lane Merger, Spaceship Repair, and Store Visit problems. Finally, we provide a results table for all five partition selection approaches and the baseline RCompliant."}, {"title": "G Appendix: Broader Impacts", "content": "The primary positive impact of BSQ preferences is their accessibility to non-experts, allowing them to input their preferences directly into a solver that optimizes the completion of tasks while aligning with the user's intentions. This framework increases the usability of artificial intelligence to a more diverse user base. Moreover, BSQ preferences enable encoding safety constraints with enforceable guarantees over the belief state. Thus, this paper represents an important step in making AI more usable for non-experts, particularly in encoding preferences and addressing safety concerns in real-world applications.\nA potential negative impact of making AI more accessible through BSQ preferences is that it could also be exploited by bad actors who might encode harmful preferences. To mitigate this risk, one approach is to design goals such that negative outcomes inherently prevent goal completion, thereby teaching the agent to avoid these outcomes. Additionally, future work can explore methods for prioritizing certain preferences to ensure that the AI does not align with harmful intentions."}, {"title": "H Appendix: Limitations", "content": "The first main limitation of this paper is that BSQ preferences require computing the exact belief state; otherwise, the number of possible partitions nearly becomes infinite because each partition represents a probability distribution over what braids are reachable. Therefore, the Partition Refinement Search algorithm (Section 5) will not perform well, meaning a different algorithm is required. This significantly limits the usability of the current work to smaller problems where computing the exact belief state is possible.\nThe second limitation is that the current implementation of BSQ preferences produces quite rigid policies. For example, note that we had to order the objects arbitrarily when constructing BSQ preferences (e.g., higher-indexed rocks are always evaluated later, even if evaluating them first might be more optimal). Such orderings prevent BSQ policies from generalizing, and the user would unlikely know the ideal ordering of objects even if the ordering was static. Adding deterministic functions like sorting requires moving to BSQ programs, one direction we intend to explore.\nThe last main limitation is the reliance on users to construct the BSQ preferences. While high-level intuition might be easy for us to understand, converting that intuition into a BSQ preference is not trivial, with different BSQ preferences performing differently for the same problem. There are also risks of users' preferences not aligning with the goal/task, causing the agent to attempt to minimize the preference (e.g., if sampling rocks were optional in Graph Rock Sample, then the optimal strategy is to set the parameters too high for any rocks to be evaluated). Furthermore, like shielding, figuring out the else-case for BSQ preference is not trivial if there is not a clear safe action. Lastly, users often think in terms of high-level actions rather than the low-level actions used in the current BSQ preferences. One direction of future work is preference elicitation, where we can help the user answer these questions while constructing a BSQ preference."}]}