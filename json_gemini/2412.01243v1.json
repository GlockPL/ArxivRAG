{"title": "Schedule On the Fly: Diffusion Time Prediction for Faster and Better Image Generation", "authors": ["Zilyu Ye", "Zhiyang Chen", "Tiancheng Li", "Zemin Huang", "Weijian Luo", "Guo-Jun Qi"], "abstract": "Diffusion and flow models have achieved remarkable successes in various applications such as text-to-image generation. However, these models typically rely on the same predetermined denoising schedules during inference for each prompt, which potentially limits the inference efficiency as well as the flexibility when handling different prompts. In this paper, we argue that the optimal noise schedule should adapt to each inference instance, and introduce the Time Prediction Diffusion Model (TPDM) to accomplish this. TPDM employs a plug-and-play Time Prediction Module (TPM) that predicts the next noise level based on current latent features at each denoising step. We train the TPM using reinforcement learning, aiming to maximize a reward that discounts the final image quality by the number of denoising steps. With such an adaptive scheduler, TPDM not only generates high-quality images that are aligned closely with human preferences but also adjusts the number of denoising steps and time on the fly, enhancing both performance and efficiency. We train TPDMs on multiple diffusion model benchmarks. With Stable Diffusion 3 Medium architecture, TPDM achieves an aesthetic score of 5.44 and a human preference score (HPS) of 29.59, while using around 50% fewer denoising steps to achieve better performance. We will release our best model alongside this paper.", "sections": [{"title": "1. Introduction", "content": "In recent years, deep generative models, including diffusion models [10, 44, 46] have achieved extraordinary per-formance across a variety of tasks, including image synthesis [14, 15, 38, 39, 41], video generation [3, 11, 55], and others [18, 31, 35, 43]. As a multi-step denoising framework, diffusion models progressively refine random noise into coherent data through iterative sampling, which underlies their impressive capabilities in generating high-quality, diverse outputs.\nHowever, inference with a diffusion model always involves manually selecting a noise scheduler, e.g. how the noise level changes step by step when denoising a clean image from Gaussian noise. This requires the user to delicately adjust parameters like the number of steps, resulting in a high threshold for customers to use. Numerous works attempt to find the optimal scheduler to liberate the users from these tedious tasks.\nThe leading flow-based models, Stable Diffusion 3 and Flux, provide recommended schedulers that adjust noise levels only based on the targets' resolution. Sabour [40] and Xia [52] explore ways to fine-tune the schedule for a model, improving either its efficiency or overall performance. In addition, some new one-step generators [13, 28-30] also achieve impressive performance. Despite the excellent performance they achieve, most of these works hold the assumption that there exists a universally applicable schedule that is optimal for all prompts and images, which is doubtful.\nTake several images in Fig. 4 as an example. Images on the right are rich in content, requiring more denoising steps to capture finer details. In contrast, the images on the left are relatively simple and can be generated with fewer steps without compromising quality. Moreover, Karras [16] also proves that different noise schedules also greatly affect the generation quality. Thus, we may ask: Is it possible to adaptively adjust both the number of denoising steps and the noise level at each step during inference, without anymanual intervention from the user?\nIn this paper, we propose Time Prediction Diffusion Models(TPDMs) that can adaptively adjust both the number of steps and the denoising strength during inference. This is achieved by implementing a plug-and-play Time-Prediction Module (TPM) that can predict the next diffusion time conditioned on the latent features at the current step, so that the noise schedule can be adjusted on the fly.\nTPM is trained with reinforcement learning. We regard the multi-step denoising process as an entire trajectory, and the quality of the generated image discounted by the number of steps as the reward. The image quality is measured by a reward model aligned with human preference [54].\nTPM can be easily integrated into any diffusion models with marginal additional computation, and allow them to automatically adjust hyper-parameters like sample steps and the noise level at each step, reaching a balance between image quality and efficiency without human interventions.\nMoreover, during training, the model rolls out the diffusion process the same as in inference, directly optimizing the inference performance and reducing denoising steps.\nWe implement TPDM on several start-of-the-art models, including Stable Diffusion and Flux. With an adaptive noise schedule, our model can also generate 50% fewer steps on average with image quality on par or slightly better (0.322 CLIP-T, 5.445 Aesthetic Score, 22.33 Pick Score, 29.59 HPSv2.1) than Stable Diffusion 3. These results demonstrate that TPDM has the potential to either pursue high-quality image generation or improve model efficiency.\nOur contributions are summarized below:\n\u2022 We introduce the Time Prediction Diffusion Model (TPDM) that can adaptively adjust the noise schedule during inference, achieving a balance between image quality and model efficiency.\n\u2022 To train TPDM, we maximize the generated image quality discounted by the number of steps with reinforcement"}, {"title": "2. Related Works", "content": "2.1. Diffusion Models\nDiffusion Probability Models (DPMs) [10, 44] recover the original data from pure Gaussian noise by learning the data distribution across varying noise levels. With their strong adaptability to complex data distributions, diffusion models have achieved remarkable performance in diverse fields such as images [12, 17, 34, 35, 37], videos [3, 11, 55], and others [7, 50, 56, 57], significantly advancing the capabilities of Artificial Intelligence Generated Content.\n2.2. Noise Schedule\nIn order to generate an image, the model must determine the diffusion time for each step. This can be achieved using either discrete-time schedulers [10, 45] or continuous-time schedulers [22, 25] depending on the model. Typically, the diffusion time reflects the noise strength at each step, and most existing approaches rely on pre-determined schedules. Currently, the leading flow-based models, Stable Diffusion 3[8] and FLUX[19], provide recommended schedulers that adjust noise levels only based on the target resolution.\nThere exist some methods fine-tuning the scheduler to speed up sampling or improve image quality. Xia et al. [52] predict a new denoising time for each step to find a more accurate integration direction, Sabour et al. [40] uses stochastic calculus to find the optimal sampling plan for different schedulers and different models. Wang et al. [49] leverages reinforcement learning to automatically search for an optimal sampling scheduler. Some one-step generators[13, 29, 30] with diffusion distillation [26] also achieve impressive performance.\nAll of the above noise schedulers not only have high thresholds that require users to adjust many parameters, but also use the same denoising schedule for all prompts and images. On the contrary, TPDM can adaptively adjust the noise schedule during inference, and dynamically select the optimal sampling plan with decreased number of sampling steps for all prompts, achieving a balance between image quality and model efficiency. We will introduce the detailed definitions and practical algorithms of TPDM in Sec. 3.\n2.3. Reinforcement Learning and Learning from Human Feedback\nReinforcement Learning from Human Feedback (RLHF) has recently gained significant attention in the field of large language models (LLMs) [1, 42, 47] and is gradually expanding into other domains. Advances in diffusion models have increasingly incorporated reward models, trained on human preferences, to enhance alignment with human values [21, 51, 54]. Some recent works have also studied the RLHF for few-step generative models [27, 32]. However, most approaches only employ reward models as substitutes for diffusion loss or in conjunction with diffusion processes to align with human preferences, which significantly differs from the Proximal Policy Optimization (PPO) paradigm [42] commonly used in RLHF for LLMs. This divergence arises primarily because the multi-step denoising process in DPMs does not naturally yield likelihoods in the same manner as language models, posing challenges for the direct application of PPO. In this paper, however, we introduce an approach that, to the best of our knowledge, is the first to treat each denoising step as a distribution analogous to an action in reinforcement learning. This enables the use of well-established optimization techniques from LLM-based RLHF within the context of diffusion models."}, {"title": "3. The Proposed Approach", "content": "In this section, we first provide a brief review of the fundamental principles of diffusion models, followed by an introduction to the Time Prediction Module (TPM). Finally, we detail the training algorithm for the TPM.\n3.1. Preliminary\nDiffusion Models learn to generate images with a reverse process that gradually removes noise from a sample. The leading paradigm for implementing this reverse process is flow matching [8, 9, 23]. Thus we introduce how flow-matching models work, and the detailed structure inside current start-of-the-art models here.\nWe consider a generative model that establishes a mapping between samples x1 drawn from a noise distribution p\u2081 and samples xo from a data distribution po. The Flow Matching objective aims to directly regress a vector field vt, which generates a probability flow, enabling the transition from p\u2081 to po.\n$LFM(0) = Et,pt(x) ||Vo(xt, t) \u2013 u(x, t) ||\u00b2$\n(1)\nThe flow-matching model with parameters @ aims to predict the noise prediction function vo (xt, t), which approximates the true velocity field u(x, t) that guides the diffusion process from the noise distribution to the clean distribution of generated samples. Thus, we can get the diffusion ODE:\n$\\frac{dxt}{dt}$ = vo(xt,t)\n(2)\nDuring inference, suppose we generate an image with N steps, each step has a time tn corresponding to its noise"}, {"title": "3.2. Time Prediction Diffusion Model (TPDM)", "content": "As aforementioned, we need a series of denoising steps to generate an image with the trained diffusion model. Usually, a fixed noise schedule is applied in this process for all prompts, assigning a pre-determined noise level for each step.\nOn the contrary, to enable the model to adjust the noise schedule on the fly, the TPDM predicts how noise strength t decays between adjacent steps. This ensures the noise strength is monotonously decreasing to reflect the denoising progress, avoiding backward progression. To be compatible with training in Sec. 3.3, TPM estimates the distribution over the denoising schedule, rather than predicting an exact value. Suppose we are performing the n-th denoising step. Besides the denoising outputs at the current step, TPDM predicts the distribution of the decay rate rn as well. This distribution can be parameterized as a Beta Distribution over (0, 1), where the TPDM needs to estimate two parameters a and \u03b2. We notice that when a > 1 and \u1e9e > 1 it can ensure a unimodal distribution. Therefore, we reparameterize that TPDM predicts two real numbers a and b, and determines the distributions with Eq. 4. Thus the decay rate rn and the next noise level tn can be sampled as in Eq. 5-Eq. 6.\na = 1 + e, \u03b2 = 1 + e,\nr ~ Beta(\u03b1, \u03b2)\ntnrntn-1\n(4)\n(5)\n(6)\nTPDM requires only a minimal modification to the original diffusion model: adding a lightweight Time Prediction Module (TPM) as shown in Fig. 3. This module takes the latents before and after the transformer blocks as inputs, so that both the original noisy inputs and the predicted results at this step are taken into consideration. After several convolution layers, TPM pools the latents into a single feature vector and predicts a and b with two linear layers. We also use an adaptive normalization layer [36] in TPM so that the model is aware of the current time embedding.\nDuring training, we freeze the original diffusion model and only update the newly introduced TPM. Thus, the model learns to predict the next diffusion time while preserving the original capacity for image generation."}, {"title": "3.3. Training Algorithms", "content": "To train TPM, we need to roll out at least two denoising steps: predicting the next diffusion time with the first step, and denoising with this time at the next step. A naive method provides a noised image as input to the first step and trains the model with the reconstruction loss calculated at the second step. The gradients would backpropagate through the predicted tn to update TPM in the first step. However, we found that the trained model tends to complete the denoising process in very few steps during inference, leading to poor image quality. We hypothesize that by supervising the loss calculated after two steps, the model learns to generate a fully denoised image after two steps, and stop intermediately to minimize the loss function. However, what matters is the final image generated after the whole diffusion reverse process, which is ignored in this method.\nTherefore, we optimize TPM to maximize the image quality generated after the whole denoising process to achieve precise time prediction. The quality is measured by an image reward model. Considering the computation graph of whole inference is too deep for gradients to back-prop, we train the model with Proximal Policy Optimization (PPO) [42], whose loss function is formulated as\n$L(0) = \\frac{Et[\\frac{\u03c0\u03b8(y|s)}{\u03c0old(y|s)}\u00c2(8,y) \u2013 AKL[Tref(-18), g(-s)]]}{Told (ys)}$\n(7)\nUnder our problem definition, s = (c, \u20ac) denotes initial states including the input prompt c and gaussian noise \u2208 ~ \u039d(0,1); \u03c0\u03b8 denotes the policy network, e.g. our TPDMmodel. \u03c0old denotes the old policy for sampling trajectories; Tref denotes a reference policy for regularization; y denotes the action our policy takes, i.e., the scheduled time; and A(s, y) denotes the advantage the action y has given the state s.\nWe will specify the above elements in the PPO including the action and advantage we use in the following.\nTreat the whole schedule as an action Usually, when the model makes a sequence of predictions, PPO regards every single prediction as an action and optimizes with them as a batch. Recently, RLOO [1] claims that when the reward signal only appears at the end of the sequence and the environment dynamics is fully deterministic, we can regard the whole sequence as an action with no harm to the performance. Thus for simplicity, the entire generation, including all predicted time in the schedule, is considered as a single action to optimize.\nThus, when computing expectation in Eq. 7, we consider the whole trajectory as a training sample in optimization. However, TPDM only outputs the distribution of each single time prediction, which we denote by \u03c0(). By the chain rule, the probability of the entire generation can be calculated as the product of each prediction as in Eq. 8,\n$\u03c0\u03bf (t1, ..., tv|s) = \u03a0\u03c0\u03bf\u03b5) (tils, t1, ..., ti\u22121)$,\n(8)\nwhere N denotes the total number of the generation steps. In our model, each factor of the probabilistic policy network \u03c0\u03bb\u03b5) is computed through the TPM that outputs a distribution over the possible diffusion time ti for the next step.\nImage reward discounted by the number of steps When generating samples for policy gradient training, we obtain trajectories by generating images from Gaussian noise with the current TPDM policy. Since there exists no ground truth for these generated images, we choose ImageReward [54] to assign a reward score based solely on the final image in the last time. Considering we do not want a trivially large number of too many steps during generation, we apply a discount factor y < 1 to discount reward to intermediate diffusion steps and calculate the average over the total number of denoising steps N. The final reward for this trajectory is shown in Eq. 9 below,\n$R(s,y) = \\frac{1}{N}\u03a3\u03b3^{k-1}IR(y, s)$,\n(9)\nwhere IR denotes the image reward model.\nWe refer readers to RLOO [1] on how to compute the advantage A(s, y) directly from a batch R(s, y) without a value model for optimization. This reward function wouldencourage TPDM to generate images with high quality, and reduce the number of diffusion steps for model efficiency as well. By adjusting y, one can control how many diffusion steps tend to be applied to generate high-quality images. The smaller the value of y, the fewer steps would tend to be allowed. We will elaborate more in Sec. 4.2."}, {"title": "4. Experiments", "content": "4.1. Implementation details\nDataset We collect prompts to train our model. These prompts are generated with Florence-2 [53] and Llava-Next [24] to generate captions for the Laion-Art [48] and COYO-700M [4] datasets, and utilize these prompts to constitute our training set. We will elaborate on this in Appendix.\nTraining Configurations We use the AdamW optimizer with \u03b2\u2081 = 0.9, \u03b22 = 0.99, a constant learning rate of le-5, and a maximum gradient norm of 1.0. Our TPM module typically only requires around 200 training steps with a batch size of 256.\n4.2. Main Results\nDynamic Schedule for Different Images In Fig. 4, we present images generated with different prompts and their corresponding schedules predicted by TPDM. When prompting TPDM with shorter and simpler prompts, there exists fewer objects and details in the generated image, thus the diffusion time decreases faster and reaches 0 in relatively fewer steps. On the contrary, when longer and more complex prompts are provided, the model needs to generate more visual details. Therefore, the diffusion time decreases slower to generate delicate details. In this case TPDM requires more denoising steps in the generation process.\nAdjustingy for Different Number of Steps y in Eq. 9 controls how the image reward discounts with more generation steps, influencing how fast diffusion time decays during the denoising process, thereby affecting the average number of denoising steps of our model.\nAs shown in Fig. 6, as we decrease y from 0.97 to 0.85, TPDM tends to decrease the diffusion time more rapidly, leading to fewer sampling steps, from 15.0 to 7.5. Additionally, when compared to the baseline (yellow line), TPDM (purple line) consistently achieves a significantly higher aesthetic score with the same number of inference steps, achieving a good balance between model efficiency and generation performance.\nVisual Comparison Our method achieves superior ability in generating fine-grained details. Images generated by TPDM have a more realistic laptop keyboard than both images from SD3-Medium in Fig. 5C, and the results in"}, {"title": "4.4. Ablation on Module Architectures", "content": "In this section, we ablate on the choice of the inputs of TPM. As shown in Tab. 3, taking features from both the first and last layer into TPM performs better than only taking either of them."}, {"title": "5. Conclusions and Limitations", "content": "In this paper, we introduce the Time Prediction Diffusion Model (TPDM), a text-to-image diffusion model that has flexible denoising schedulers for different individual prompts. By introducing a Time Prediction Module, we effectively train TPDM by reinforcement learning guided by a reward model. Based on the current leading diffusion model architecture proposed in Stable Diffusion 3 Medium, we train a strong MM-DiT-based TPDM that shows competitive quantitative performances on multiple text-to-image generation benchmarks.\nDespite its promising performances, TPDM has some limitations. For instance, in this paper, we only design a relatively simple architecture for TPM. Whether and how to improve such a module to obtain better performances remains unexplored. Second, we froze the parameters of the original model and employed our training method to update the parameters of the diffusion model, leading to improved results. This approach warrants further exploration."}]}