{"title": "Norface: Improving Facial Expression Analysis by Identity Normalization", "authors": ["Hanwei Liu", "Rudong An", "Zhimeng Zhang", "Bowen Ma", "Wei Zhang", "Yan Song", "Yujing Hu", "Wei Chen", "Yu Ding"], "abstract": "Facial Expression Analysis remains a challenging task due to unexpected task-irrelevant noise, such as identity, head pose, and background. To address this issue, this paper proposes a novel framework, called Norface, that is unified for both Action Unit (AU) analysis and Facial Emotion Recognition (FER) tasks. Norface consists of a normalization network and a classification network. First, the carefully designed normalization network struggles to directly remove the above task-irrelevant noise, by maintaining facial expression consistency but normalizing all original images to a common identity with consistent pose, and background. Then, these additional normalized images are fed into the classification network. Due to consistent identity and other factors (e.g. head pose, background, etc.), the normalized images enable the classification network to extract useful expression information more effectively. Additionally, the classification network incorporates a Mixture of Experts to refine the latent representation, including handling the input of facial representations and the output of multiple (AU or emotion) labels. Extensive experiments validate the carefully designed framework with the insight of identity normalization. The proposed method outperforms existing SOTA methods in multiple facial expression analysis tasks, including AU detection, AU intensity estimation, and FER tasks, as well as their cross-dataset tasks. For the normalized datasets and code please visit project page.", "sections": [{"title": "1 Introduction", "content": "Facial Expression Analysis (FEA) is a complex task in affective computing that often involves facial Action Units (AU) analysis (AU detection and AU intensity estimation) and Facial Emotions Recognition (FER). Despite the progress made so far, both FEA tasks still face challenges due to the inherent entanglement of expressions with unexpected factors like identity, head pose, and background. Specifically, FEA suffers from severe identity bias, which exacerbates the intricacies of refinement and representation of facial expressions. FEA model struggles to generalize effectively to unseen identities, this bias also leads the model to overfit seen identities and undermines the overall performance. In addition, variations in pose and background compound the challenges in FEA tasks. Hence, a crucial objective of FEA is to mitigate interference arising from irrelevant factors, including identity, pose, and background.\nTo address these challenges, some studies attempt to construct identity-based expression pairs to obtain identity-invariant representations. Due to significant variations in identities, the issue of identity bias still persists. Alternatively, certain studies aim to incorporate generative techniques, such as generating 'neutral' emotions, 'average' identities, or diverse emotions, to disentangle expression from identity. Nonetheless, these methods either suffer from limited generation quality or heavily rely on controlled lab datasets, thereby restricting their generalizability in complex and diverse real-world scenarios. Moreover, these approaches often overlook non-identity noise factors, such as pose and background variations. Additionally, their frameworks are often tailored to either AU analysis or FER tasks, which cannot be unified for both tasks, despite task-irrelevant noise being common to both.\nTo address the above challenges, this paper introduces a novel framework called Norface, which is applicable to AU detection, AU intensity estimation, and FER tasks. The core concept behind Norface is Identity normalization (Idn), which normalizes all images to a common identity with consistent pose and background, as shown in Fig.1. By doing so, the resulting normalized data is intended to retain only the facial expression variations that are relevant to the task. Consequently, Idn mitigates the influence of identity bias, pose variations, and background changes, which are commonly encountered in both AU analysis and FER tasks. This process can also be interpreted as the removal of the above task-irrelevant noise, which is carried out by a normalization network in the first stage. Then, in the second stage, normalized images, as complementary to the original images, are used to improve expression analysis (AU detection, AU intensity estimation, or emotion recognition) by a classification network.\nSpecifically, in the first stage, the normalization network achieves Idn from all original faces to a target face. Thus, our normalization network is not only suitable for in-the-wild data but also capable of meeting the high-quality requirement for expression consistency. Specifically, we employ a pre-trained Masked AutoEncoder (MAE) to extract facial features, effectively capturing both expression and other attributes. Then, we develop an Expression Merging Module"}, {"title": "3 Method", "content": "As shown in Fig.2, Norface comprises two stages: the first stage employs a normalization network \\(F_{nor}\\) for Identity normalization (Idn), while the second stage utilizes a classification network \\(F_{cla}\\) for expression classification. Let I be the"}, {"title": "3.1 Normalization network", "content": "Normalization network \\(F_{nor}\\) performs Idn on the original face \\(I_o\\) and the target face \\(I_t\\). As shown in Fig.2, a shared face encoder \\(E_s\\) first maps \\(I_o\\) and \\(I_t\\) into patch embeddings \\(e_o\\) and \\(e_t\\). Then, an Expression Merging Module (EMM) is designed to adaptively fuse the expression information of \\(I_o\\) and other attribute information of \\(I_t\\). Finally, a facial decoder \\(D_f\\) outputs the normalized face \\(I_n\\).\nShared Face Encoder. We design a shared face encoder following MAE [19], to project original and target faces to a common latent representation. The MAE is pre-trained on a large-scale face dataset using a mask training strategy, which has been validated in many domains due to its superior feature extraction performance [3,53]. By forcing the model to learn the topological and semantic information of masked image patches, the latent space of the MAE can better capture facial expressions and attributes. That is, based on the pre-trained face encoder \\(E_s\\), we can project a original/target face \\(I_{o/t}\\) into a latent presentation:\n\\(e_{o/t} = E_s(I_{o/t})\\)   (1)\nwhere \\(e_{o/t} \\in R^{N\\times L}\\). N and L denote the number of patches and the dimension of each embedding, respectively.\nExpression Merging Module. We designed an Expression Merging Module (EMM) that adaptively integrates the expression information of the original face with the other attributes of the target face. As shown in Fig.2, EMM consists of a multi-head cross-attention block and two transformer blocks [15]. Given the original patch embeddings \\(e_o\\), and the target patch embeddings \\(e_t\\), we first compute Q, K, V for each patch embedding in \\(e_o\\) and \\(e_t\\). Then the cross-attention can be formulated as \\(CA(Q_t, K_o) = softmax(\\frac{Q_tK_o^T}{\\sqrt{d_k}})\\), where CA represents Cross Attention, Q, K, V are predicted by attention heads, and \\(d_k\\) is the dimension of K. Next, the expression information from the original face is aggregated based on the computed CA:\n\\(V_{fu} = CA * V_o + V_t\\)   (2)\nThen, \\(V_{fu}\\) are normalized by a layer normalization (LN) and processed by multi-layer perceptrons (MLP). The fused embeddings \\(e_{fu}\\) are further fed into two transformer blocks to obtain the output \\(e_n\\). Finally, we utilize a convolutional decoder \\(D_f\\) to generate the normalized face \\(I_n\\) from \\(e_n\\).\nTraining Loss. The use of multiple loss functions is a pervasive way to constrain the image generation, e.g. those works [40, 42, 69, 73, 86-89] on face manipulation, as these loss functions carry out supervision from perspectives. Inspired"}, {"title": "3.2 Classification Network", "content": "The classification network \\(F_{cla}\\) is designed to perform AU detection, AU intensity estimation, or emotion recognition. As shown in Fig.2, \\(F_{cla}\\) takes \\(I_n\\) as input, as well as \\(I_o\\). Specifically, \\(F_{cla}\\) consists of a facial feature extractor \\(E_f\\), an Input MoE module \\(M_i\\), and an Output MoE module \\(M_o\\). First, \\(E_f\\) maps \\(I_n\\) and \\(I_o\\) to their latent presentations \\(e_f\\), where \\(e_f \\in \\{e_n, e_o, \\}\\). Then, \\(e_f\\) are fed into \\(M_i\\), which consists of two identity-specific Mixture of Expert (MoE) [43] blocks, resulting in the identity-specific facial representation \\(e_n\\). Finally, all of \\(e_n\\) are input into \\(M_o\\), which refines an identity-fused representation from fixed and original identities for the final classification. \\(E_f\\), \\(M_i\\), and \\(M_o\\) will be further described below, as well as the MoE block.\n\\(E_f\\) aims to map \\(I_n\\) and \\(I_o\\) to a latent representation \\(e_f\\). The design of \\(E_f\\) is similar to \\(E_s\\) and it is also pre-trained on the same large-scale face datasets. That is, based on \\(E_f\\), we can project \\(I_{n/o}\\) into a latent presentation \\(e_f\\), that is \\(e_f = E_f(I_{n/o})\\).\n\\(M_i\\) consists of two identity-specific MoE blocks, with each MoE block receiving the individual latent representations \\(e_f\\) of \\(I_n\\) and \\(I_o\\), respectively, that is \\(e_n = M_i(e_f)\\)."}, {"title": "4 Experiment", "content": "Datasets. Previous works on collecting datasets (e.g., BP4D [95], BP4D+ [99], DISFA [47], AffectNet [48] and RAF-DB [34]) make significant contributions to academic communities. We evaluate the AU analysis task on BP4D, BP4D+, DISFA and the FER task on AffectNet and RAF-DB. BP4D consists of 328 video clips from 41 participants. About 140,000 frames are annotated with the occurrence or absence of 12 AUs for AU detection and the intensity of 5 AUs for AU intensity estimation. BP4D+ contains 1,400 video clips from 140 participants. About 198,000 frames are annotated with the same AUs in BP4D for AU detection. DISFA contains about 131,000 frames from 27 video clips. Each frame of DISFA is annotated with the occurrence or absence of 8 AUs for AU detection and the intensity of 12 AUs for AU intensity estimation. AffectNet, we use 7 basic emotions, selecting approximately 280,000 and 3,500 images in total for training and testing, respectively. RAF-DB, the experiment's training set and test set sizes are 12,271 and 3,068, respectively.\nFor fair comparisons, our experiments use the same training and testing datasets, as well as evaluation criteria as other methods. In the AU detection task, our evaluation follows the prior works [35,57], using three-fold cross-validation, and the report results are the F1 averages from three-fold experiments. For AU intensity estimation task, we evaluate the model performance with intra-class correlation (ICC) [60], mean squared error (MSE), and mean absolute error (MAE). We report the accuracy in the FER task.\nTo pick up the appropriate target face, we intentionally select 10 individuals from BP4D&BP4D+, ensuring diversity in age, gender, and ethnicity, and used the individual with the best AU detection performance as the fixed target face \\(I_{id}\\) that can be seen in Fig.1.\nImplementation Details. The normalization network \\(F_{nor}\\) is first pre-trained on CelebA-WebFace [41] and Emo135 [7] datasets. The face images are aligned and cropped to a size of 256x256. Afterward, we fine-tune the network on the target AU and emotion datasets. We adopt Adam [28] optimizer with \\(\\beta_1 = 0\\), \\(\\beta_2 = 0.99\\), a learning rate of 0.0001, and a batch size of 8. We set \\(\\lambda_{rec} = 10\\), \\(\\lambda_{perc} = 5\\), \\(\\lambda_{id} = 10\\), \\(\\lambda_{lm} = 5000\\), \\(\\lambda_{exp} = 5000\\), and \\(\\lambda_{eye} = 10\\). For the classification network \\(F_{cla}\\), we performed fine-tuning for 40 epochs, and set \\(\\lambda_{imp} = 0.001\\), \\(\\lambda_{g&l} = 0.001\\). The settings for each MoE block are m = 4 and k = 2. The base learning rates are set to 1e-4, 1e-3, and 2e-5 for AU detection, AU intensity estimation, and FER tasks, respectively."}, {"title": "4.1 Comparison with State-of-the-Art Methods", "content": "We verify the effectiveness of our method on AU detection, AU intensity estimation, and facial emotion recognition tasks, as well as their cross-dataset tasks.\nAU detection task. Tab.1 and 2 report comparison results with those previous methods. Our method outperforms those baselines on all datasets with the average of AUs detection. Specifically, ours exceeds the SOTA of GLEE-Net [94] for over 3% on both BP4D and BP4D+ datasets, and exceeds the SOTA of LGRNet [17] for over 5% on the DISFA dataset, which validates our method on the AU detection task.\nAU intensity estimation. Tab.3 report comparison results with those previous methods. Ours surpasses all existing approaches in terms of MSE and MAE, and exceeds the SOTA of APs [56] by 0.19 in terms of ICC on the DISFA dataset, which validates our method on the AU intensity estimation task.\nFacial emotion recognition task. Tab.4 reports comparison results with previous methods. Ours outperforms these baselines in terms of accuracy on both datasets. Specifically, ours exceeds the SOTA of LA-Net [74] for 1.41% and 1.09% on RAF-DB and AffectNet, respectively, validating our method on the FER task.\nCross-dataset task. Tab.5 and 6 report comparison results of AU detection and FER with previous methods on cross-dataset tasks, respectively. Ours achieves the best performance, surpassing the SOTA of GLEE-Net [94] by 5.1% on AU detection, and outperforming RANDA [24] by 6.09% and 11.2% on the 'R\u2192A' and 'A\u2192R' tasks of FER, respectively. Due to the normalization of task-irrelevant noise, domain barriers between datasets are reduced, which highlights the benefits of our method for cross-domain tasks.\nThe above experiments validate our framework being unified for those tasks, as identity normalization addresses the interference of task-irrelevant noise, such as identity, pose, and background, etc. More experiments on identity normalization are described below."}, {"title": "4.2 Analysis on Identity Normalization", "content": "We address the following Idn questions: A1. Is Idn effective? A2. How does the normalization network perform? A3. What is the difference between Idn and data augmentation? A4. Why use normalized images instead of expression features for the classification network?\nA1. Impact of Idn. Tab.7 reports the results of different FEA tasks w/ and w/o Idn. The results show that the performance w/ Idn consistently outperforms that w/o Idn in all tasks. For example, w/ Idn, there are improvements of 2.1%, 1.9%, and 2.5% in AU detection on BP4D, BP4D+, and DISFA, 3.73% and 1.4% increase in FER on RAF-DB and AffectNet, highlighting the benefits of Idn.\nIn addition, Fig.4 provides a detailed presentation of the normalization and AU detection results for different target identities. It is clear that using several target identities are validated to consistently yields better results than w/o Idn. Moreover, our study indicates that the selection of target identities contributes differently to performance enhancement, and certain identities are unsuitable as target faces. For example, w/ 'ID3' resulted in a performance decrease of 1.1% compared to w/ 'ID6', which we attribute to the confusion in facial expressions. In Fig.4, the non-candidate target faces in the blue box are labeled as having no AUs present, but these faces still exhibit raised eyebrows or lip corner depressors, leading to confusion in the expressions of their normalized faces. In contrast, as shown in the orange box in Fig.4, these faces have minimal confusion in facial"}, {"title": "5 Conclusion", "content": "This paper provides a novel insight and a unified framework called Norface for AU detection, AU intensity estimation, and FER tasks, aiming to remove the task-irrelevant noise by identity normalization that normalizes all images to a common identity with consistent pose and background, and improves expression classification. Extensive quantitative and qualitative experiments demonstrate the superior performance of Norface for AU analysis and FER tasks as well as their cross-dataset tasks."}]}