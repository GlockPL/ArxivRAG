{"title": "Does Current Deepfake Audio Detection Model Effectively Detect ALM-based Deepfake Audio?", "authors": ["Yuankun Xie", "Chenxu Xiong", "Xiaopeng Wang", "Zhiyong Wang", "Yi Lu", "Xin Qi", "Ruibo Fu", "Yukun Liu", "Zhengqi Wen", "Jianhua Tao", "Guanjun Li", "Long Ye"], "abstract": "Currently, Audio Language Models (ALMs) are rapidly advanc-\ning due to the developments in large language models and audio\nneural codecs. These ALMs have significantly lowered the bar-\nrier to creating deepfake audio, generating highly realistic and\ndiverse types of deepfake audio, which pose severe threats to\nsociety. Consequently, effective audio deepfake detection tech-\nnologies to detect ALM-based audio have become increasingly\ncritical. This paper investigate the effectiveness of current coun-\ntermeasure (CM) against ALM-based audio. Specifically, we\ncollect 12 types of the latest ALM-based deepfake audio and\nutilizing the latest CMs to evaluate. Our findings reveal that the\nlatest codec-trained CM can effectively detect ALM-based au-\ndio, achieving 0% equal error rate under most ALM test condi-\ntions, which exceeded our expectations. This indicates promis-\ning directions for future research in ALM-based deepfake audio\ndetection.", "sections": [{"title": "1. Introduction", "content": "Currently, due to the rapid development of large language model\nand audio neural codec, there have been significant advance-\nments in audio generation models. We typically refer to these\nnovel types of audio generation models as Audio Language\nModels (ALMs) [1-10]. These ALM-based audio generation\nmodels have lowered the barrier to creating deepfake audio,\nmaking the process significantly easier and more accessible. On\nthe other hand, ALM-based audio is characterized by its high\ndiversity type and remarkable realism. As shown in Fig. 1, a\nforger can easily choose any ALM model and generate different\ntypes of deepfake audio, such as speech, singing voice, music,\nand sound, for various forgery tasks. These high-fidelity deep-\nfake audio files are difficult for humans to discern, and the latest\nALM models [11] have even achieved human parity in zero-shot\ntext-to-speech (TTS) synthesis for the first time. This poses sig-\nnificant threats, including fraud, misleading public opinion, and\nprivacy violations. Therefore, the urgent development of effec-\ntive audio deepfake detection technologies is crucial.\nIn this paper, we aim to address the aforementioned ques-\ntion by collecting as many ALM-based audio samples as possi-\nble and using the latest CM for deepfake detection. Specifically,\nwe collected 12 types of ALM-based deepfake audio, which"}, {"title": "2. ALM-based deepfake audio", "content": "ALM has rapidly developed due to advancements in both lan-\nguage model (LM) and neural codec model. In the left part of\nFig. 3, it illustrates the pipeline used by most ALM models for\ngeneration. At first, the audio waveform is converted into dis-\ncrete code representations through the encoder part of the neu-\nral codec. Then, the LM decoder performs contextual learning,\nwhere the discrete quantized tokens contain information about\nthe speaker style. Finally, the non-autoregressive neural codec\ndecoder generates the audio waveform from the discrete codes."}, {"title": "2.2. Data collection", "content": "We collected 12 type of ALM-based audio, denote as A01-A12.\nAll collected audio samples can be found on the website\u00b9. The\nnumbering principle follows the publication date of the ALM\nworks. Since most ALMs are not open-sourced, the majority of\nALM-based audio samples come from demos.\nA01-AudioLM [1]. AudioLM is a framework for high-\nquality audio generation with long-term consistency that maps\nthe input audio to a sequence of discrete tokens and casts audio\ngeneration as a language modeling task in this representation\nspace. We collected 48 fake speech samples and 28 real speech\nsamples from the generation and continuation tasks on demo\npages2. Real speech comes from the ground truth or the speaker\nprompt, whereas AudioLM generated speech is categorized as\nfake speech.\nA02-AudioLM-music [1]. AudioLM is not limited to mod-\neling speech. It can also learn to generate coherent piano music\ncontinuations. 4 pairs of music segments were collected from\nthe piano continuation task on demo pages\u00b3. Real audio comes\nfrom the 4-second piano prompts, whereas AudioLM generated\nmusic is categorized as fake audio.\nA03-VALL-E [3]. VALL-E is a neural codec language\nmodel designed to generate discrete codes derived from En-\nCodec [20], utilizing either textual or acoustic inputs. We used\ncondition A1 from the Codecfake dataset 4 for testing, which in-\ncludes 4,451 real audio samples and 4,436 fake audio samples\nsynthesized by VALL-E.\nA04-VALL-E X [5]. VALL-E X can generate high-quality\naudio in the target language with just a single utterance of\nthe source language audio as a prompt. Similar to VALL-E, we\nused condition A2 from the Codecfake dataset for testing, which\nincludes 4,451 real audio samples and 4,436 fake audio sam-\nples.\nA05-SpeechX [8]. SpeechX is a versatile speech genera-\ntion model leveraging audio and text prompts, which can deal\nwith both clean and noisy speech inputs and perform zero-shot\nTTS and various tasks involving transforming the input speech.\nWe collected 16 fake speech samples and 16 real speech sam-\nples from the TTS, content editing and target speaker extraction\ntasks on demo pages. Real speech comes from the ground truth\nor the speaker prompt, whereas SpeechX generated speech sam-\nples are categorized as fake speech.\nA06-UniAudio [10]. UniAudio is a versatile audio gen-\neration model that conditions on multiple types of inputs and\nperforms a variety of audio generation tasks. It treats all modal-\nities as discrete tokens. We collected 30 fake speech sam-\nples and 18 real speech samples from the TTS tasks on demo\npages. Real speech comes from the ground truth or the speaker\nprompt, whereas Uniaudio generated speech is categorized as\nfake speech.\nA07-LauraGPT [9]. LauraGPT can take both audio and\ntext as input and output both modalities, and perform a broad\nspectrum of content-oriented and speech-signal-related tasks.\nWe collected 8 fake speech samples and 8 real speech sam-\nples from the TTS tasks on demo pages. Real speech samples\ncomes from the ground truth or the speaker prompt, whereas\nLauraGPT generated speech samples are categorized as fake\nspeech.\nA08-ELLA-V [21]. ELLA-V is a simple but efficient LM-\nbased zero-shot TTS framework, which enables fine-grained\ncontrol over synthesized audio at the phoneme level. We col-\nlected 14 real speech samples and 32 fake speech samples from\nthe generation and cloning tasks on demo pages. Real speech\ncomes from the ground truth or the speaker prompt (speaker\nprompt-encodec), whereas ground truth Encodec regenerated\nand ELLA-V generated speech samples are categorized as fake\nspeech.\nA09-HAM-TTS [22]. HAM-TTS is a novel TTS system\nthat leverages a hierarchical acoustic modeling approach. We"}, {"title": "collected 35 fake speech samples and 6 real speech samples", "content": "from the TTS task on demo pages. Real speech samples come\nfrom the ground truth or the speaker prompt, whereas HAM-\nTTS generated speech samples are categorized as fake speech.\nA10-RALL-E [23]. RALL-E is a robust language model-\ning method for text-to-speech synthesis that improves perfor-\nmance and reduces errors by using chain-of-thought prompting\nto decompose the task into simpler steps. We collected 5 fake\nspeech samples and 5 real speech samples from the TTS tasks\non demo pages10. Real speech comes from the ground truth,\nwhereas RALL-E generated speech samples are categorized as\nfake speech.\nA11-NaturalSpeech 3 [24]. NaturalSpeech 3 is a TTS sys-\ntem that enhances speech quality, similarity, and prosody by\nusing factorized diffusion models and factorized vector quanti-\nzation to disentangle and generate speech attributes in a zero-shot manner. We collected 32 fake speech samples and 24 real\nspeech samples from the TTS tasks on demo pages\u00b9\u00b9. Real\nspeech samples comes from the ground truth or the speaker\nprompt, whereas NaturalSpeech 3 generated speech samples are\ncategorized as fake speech.\nA12-VALL-E 2 [11]. VALL-E 2 is the latest advancement\nin neural codec language models that marks a milestone in zero-\nshot TTS, achieving human parity for the first time. We col-\nlected 91 fake speech samples and 33 real speech samples on\ndemo pages 12. Real speech samples come from the speaker\nprompt, whereas VALL-E 2 generated speech samples are cate-\ngorized as fake speech."}, {"title": "3. Countermeasure", "content": "For the countermeasure, we consider two aspects: the training\ndatasets and the audio deepfake detection (ADD) model. For\nthe dataset, we selected models trained on the classic vocoder-\nbased deepfake dataset ASVspoof2019LA and the latest codec-\nbased deepfake dataset Codecfake. This approach allows us to\nverify whether countermeasures trained on traditional vocoder\ndatasets can effectively detect ALM-based audio, as well as\nto evaluate the effectiveness of CM trained on the Codecfake\ndataset in practical ALM in the wild tests.\nAs the feature of ADD model, we selected handcrafted fea-\ntures Mel-spectrogram and pre-trained features wav2vec2-xls-r [25]. For the Mel-spectrogram, we used 80-dimensional Mel-\nspectrograms to match the features of most conventional audio\ngeneration tasks. For wav2vec2-xls-r, we froze the weights and\nused its fifth hidden layer as the feature, due to its proven su-\nperiority in previous research [26]. For the backbone networks,\nwe chose LCNN [27] and AASIST [28], which are currently the\nmost commonly used backbone networks in the filed of ADD.\nThe CM pipeline is shown in Fig. 3."}, {"title": "4. Experiments", "content": "In the experiments, we trained on two different datasets. The\nvocoder-trained CM was trained using the ASVspoof2019 LA\n(19LA) training set, which includes 25,380 training samples\nand 24,844 validation samples. There are six spoofing meth-\nods in total, and the validation set was used only to select the"}, {"title": "4.2. Implementation details", "content": "In the pre-processing stage for the countermeasure models\n(CMs), all audio samples were initially down-sampled to 16,000\nHz and adjusted to a uniform duration of 4 seconds through\ntrimming or padding. For the mel-spectrogram feature extrac-\ntion, we derived an 80-dimensional mel-spectrogram. For the\nself-supervised feature extraction, we utilized the Wav2Vec-\nXLS-R model13 with frozen parameters, extracting 1024-\ndimensional hidden states as feature representations.\nAll CMs were trained using the Adam optimizer with a\nlearning rate of 5\u00d710-4. The vocoder-trained CM underwent\n100 epochs of training using a weighted cross-entropy loss, as-\nsigning a weight of 10 to the real class and 1 to the fake class.\nThe learning rate was halved every 10 epochs. In contrast, the\ncodec-trained CM was trained for 10 epochs, with the learning\nrate halved every 2 epochs. The model showing the best perfor-\nmance on the validation set was chosen for evaluation.\nFor experimental evaluation, we used the official imple-\nmentation for calculating EER\u00b94, maintaining precision to three\ndecimal places. To compute the confusion matrix, we used a\nthreshold of 0.5 to distinguish between real and fake predic-\ntions, utilizing calculations from SKlearn."}, {"title": "5. Results and Discussion", "content": "To verify the generalization ability of the CM, we first test the\nperformance on traditional vocoder-based dataset as shown in\nthe left part (19LA, ITW) of Table 1 and Table 2. Specifically,\nthe vocoder-trained W2V2-AASIST achieve the best equal er-\nror rate (EER) of 0.122% in 19LA test set and 23.713% in ITW\ndataset. Especially in 19LA, to the best of our knowledge, our\nCM can achieve the lowest EER, indicating the generalizabil-\nity of the CM. Additionally, in cross-domain training scenarios,\nwhere only Codecfake is used for training and testing is con-\nducted on 19LA and ITW, W2V2-AASIST also achieved good\nresults, with an EER of 3.806% on 19LA and 9.606% on ITW."}, {"title": "5.2. Results on ALM-based dataset", "content": "We tested the collected and generated ALM data. For the 19LA-\ntrained CM, the overall average (AVG) result was not very\ngood, with the lowest AVG being 24.403% for W2V2-AASIST."}, {"title": "5.3. Discussion", "content": "Some codec-trained CMs still exhibit shortcomings, particu-\nlarly high false negative (FN) rates, which suggest several rec-"}, {"title": "ommendations for future research on countering ALM-based", "content": "audio. At first, the Codecfake dataset may lacks generaliza-\ntion to other audio types such as music and sound. Even though\ncodec-trained CMs can detect ALM-based audio, they can also\nmisclassify genuine audio. From dataset perspective, this may\nnecessitate enriching the codec with a variety of audio types.\nAs for CM, specialized features for different audio types rather\nthan relying solely on speech self-supervised feature such as\nW2V2 may be needed. Additionally, the high FN rates indicate\ndeficiencies in the classifier's ability to learn from real-world\naudio data. This is apparent due to the limited diversity of real-\nworld domains covered by a single dataset during training, and\nthe inadequate representation of real audio in pre-trained fea-\ntures. Therefore, strategies such as co-training with supplemen-\ntary real audio datasets or enhancing pre-trained features could\nsignificantly enhance performance."}, {"title": "6. Conclusions", "content": "In this paper, we attempt to address a novel question: does\nthe current deepfake audio detection model effectively detect\nALM-based deepfake audio? We evaluate this by collecting and\ngenerating the latest 12 types of ALM-based audio and assess-\ning them using SOTA performance CM. The surprising results\nindicate that codec-trained CMs can effectively detect these\nALM-based audios, with most EERs approaching 0%. This in-\ndicates that the current CM, specifically the codec-trained CM\ntrained with Codecfake dataset, can effectively detect ALM-\nbased audio."}, {"title": "7. Acknowledgements", "content": "This work is supported by the National Natural Science\nFoundation of China (NSFC) (No.62101553, No.62306316,\nNo.U21B20210, No. 62201571)."}]}