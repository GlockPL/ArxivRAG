{"title": "Does Current Deepfake Audio Detection Model Effectively Detect ALM-based Deepfake Audio?", "authors": ["Yuankun Xie", "Chenxu Xiong", "Xiaopeng Wang", "Zhiyong Wang", "Yi Lu", "Xin Qi", "Ruibo Fu", "Yukun Liu", "Zhengqi Wen", "Jianhua Tao", "Guanjun Li", "Long Ye"], "abstract": "Currently, Audio Language Models (ALMs) are rapidly advancing due to the developments in large language models and audio neural codecs. These ALMs have significantly lowered the barrier to creating deepfake audio, generating highly realistic and diverse types of deepfake audio, which pose severe threats to society. Consequently, effective audio deepfake detection technologies to detect ALM-based audio have become increasingly critical. This paper investigate the effectiveness of current countermeasure (CM) against ALM-based audio. Specifically, we collect 12 types of the latest ALM-based deepfake audio and utilizing the latest CMs to evaluate. Our findings reveal that the latest codec-trained CM can effectively detect ALM-based audio, achieving 0% equal error rate under most ALM test conditions, which exceeded our expectations. This indicates promising directions for future research in ALM-based deepfake audio detection.", "sections": [{"title": "1. Introduction", "content": "Currently, due to the rapid development of large language model and audio neural codec, there have been significant advancements in audio generation models. We typically refer to these novel types of audio generation models as Audio Language Models (ALMs) [1-10]. These ALM-based audio generation models have lowered the barrier to creating deepfake audio, making the process significantly easier and more accessible. On the other hand, ALM-based audio is characterized by its high diversity type and remarkable realism. As shown in Fig. 1, a forger can easily choose any ALM model and generate different types of deepfake audio, such as speech, singing voice, music, and sound, for various forgery tasks. These high-fidelity deep-fake audio files are difficult for humans to discern, and the latest ALM models [11] have even achieved human parity in zero-shot text-to-speech (TTS) synthesis for the first time. This poses significant threats, including fraud, misleading public opinion, and privacy violations. Therefore, the urgent development of effective audio deepfake detection technologies is crucial."}, {"title": "2. ALM-based deepfake audio", "content": "ALM has rapidly developed due to advancements in both language model (LM) and neural codec model. In the left part of Fig. 3, it illustrates the pipeline used by most ALM models for generation. At first, the audio waveform is converted into discrete code representations through the encoder part of the neural codec. Then, the LM decoder performs contextual learning, where the discrete quantized tokens contain information about the speaker style. Finally, the non-autoregressive neural codec decoder generates the audio waveform from the discrete codes."}, {"title": "2.2. Data collection", "content": "We collected 12 type of ALM-based audio, denote as A01-A12. All collected audio samples can be found on the website\u00b9. The numbering principle follows the publication date of the ALM works. Since most ALMs are not open-sourced, the majority of ALM-based audio samples come from demos.\nA01-AudioLM [1]. AudioLM is a framework for high-quality audio generation with long-term consistency that maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We collected 48 fake speech samples and 28 real speech samples from the generation and continuation tasks on demo pages2. Real speech comes from the ground truth or the speaker prompt, whereas AudioLM generated speech is categorized as fake speech.\nA02-AudioLM-music [1]. AudioLM is not limited to modeling speech. It can also learn to generate coherent piano music continuations. 4 pairs of music segments were collected from the piano continuation task on demo pages\u00b3. Real audio comes from the 4-second piano prompts, whereas AudioLM generated music is categorized as fake audio.\nA03-VALL-E [3]. VALL-E is a neural codec language model designed to generate discrete codes derived from En-Codec [20], utilizing either textual or acoustic inputs. We used condition A1 from the Codecfake dataset 4 for testing, which includes 4,451 real audio samples and 4,436 fake audio samples synthesized by VALL-E.\nA04-VALL-E X [5]. VALL-E X can generate high-quality audio in the target language with just a single utterance of the source language audio as a prompt. Similar to VALL-E, we used condition A2 from the Codecfake dataset for testing, which includes 4,451 real audio samples and 4,436 fake audio sam-ples.\nA05-SpeechX [8]. SpeechX is a versatile speech genera-tion model leveraging audio and text prompts, which can deal with both clean and noisy speech inputs and perform zero-shot TTS and various tasks involving transforming the input speech. We collected 16 fake speech samples and 16 real speech samples from the TTS, content editing and target speaker extraction tasks on demo pages. Real speech comes from the ground truth or the speaker prompt, whereas SpeechX generated speech sam-ples are categorized as fake speech.\nA06-UniAudio [10]. UniAudio is a versatile audio gen-eration model that conditions on multiple types of inputs and performs a variety of audio generation tasks. It treats all modal-ities as discrete tokens. We collected 30 fake speech sam-ples and 18 real speech samples from the TTS tasks on demo pages. Real speech comes from the ground truth or the speaker prompt, whereas Uniaudio generated speech is categorized as fake speech.\nA07-LauraGPT [9]. LauraGPT can take both audio and text as input and output both modalities, and perform a broad spectrum of content-oriented and speech-signal-related tasks. We collected 8 fake speech samples and 8 real speech sam-ples from the TTS tasks on demo pages. Real speech samples comes from the ground truth or the speaker prompt, whereas LauraGPT generated speech samples are categorized as fake speech.\nA08-ELLA-V [21]. ELLA-V is a simple but efficient LM-based zero-shot TTS framework, which enables fine-grained control over synthesized audio at the phoneme level. We col-lected 14 real speech samples and 32 fake speech samples from the generation and cloning tasks on demo pages. Real speech comes from the ground truth or the speaker prompt (speaker prompt-encodec), whereas ground truth Encodec regenerated and ELLA-V generated speech samples are categorized as fake speech.\nA09-HAM-TTS [22]. HAM-TTS is a novel TTS system that leverages a hierarchical acoustic modeling approach. We"}, {"title": "3. Countermeasure", "content": "For the countermeasure, we consider two aspects: the training datasets and the audio deepfake detection (ADD) model. For the dataset, we selected models trained on the classic vocoder-based deepfake dataset ASVspoof2019LA and the latest codec-based deepfake dataset Codecfake. This approach allows us to verify whether countermeasures trained on traditional vocoder datasets can effectively detect ALM-based audio, as well as to evaluate the effectiveness of CM trained on the Codecfake dataset in practical ALM in the wild tests.\nAs the feature of ADD model, we selected handcrafted fea-tures Mel-spectrogram and pre-trained features wav2vec2-xls-r [25]. For the Mel-spectrogram, we used 80-dimensional Mel-spectrograms to match the features of most conventional audio generation tasks. For wav2vec2-xls-r, we froze the weights and used its fifth hidden layer as the feature, due to its proven su-periority in previous research [26]. For the backbone networks, we chose LCNN [27] and AASIST [28], which are currently the most commonly used backbone networks in the filed of ADD."}, {"title": "4. Experiments", "content": "In the experiments, we trained on two different datasets. The vocoder-trained CM was trained using the ASVspoof2019 LA (19LA) training set, which includes 25,380 training samples and 24,844 validation samples. There are six spoofing meth-ods in total, and the validation set was used only to select the best-performing model without participating in the training.\nThe codec-trained CM was trained using the Codecfake training set and used the validation set for model selection. The Codecfake training set contains 740,747 samples, and the val-idation set contains 92,596 samples, with a total of six codec reconstruction methods.\nFor the test set, we first conducted preliminary performance tests on the CM using the 19LA test set and an in-the-wild (ITW) dataset. The 19LA test set includes 71,237 audio sam-ples with attack types (A07-A19) that are not seen in the train-ing set. ITW dataset includes 31,779 audio samples. Both real and fake audio samples are collected from publicly available sources, such as social networks and video streaming platforms, which may contain background noise. This dataset is intended to evaluate the generalizability of detection models, including cross-dataset evaluation."}, {"title": "4.2. Implementation details", "content": "In the pre-processing stage for the countermeasure models (CMs), all audio samples were initially down-sampled to 16,000 Hz and adjusted to a uniform duration of 4 seconds through trimming or padding. For the mel-spectrogram feature extrac-tion, we derived an 80-dimensional mel-spectrogram. For the self-supervised feature extraction, we utilized the Wav2Vec-XLS-R model13 with frozen parameters, extracting 1024-dimensional hidden states as feature representations.\nAll CMs were trained using the Adam optimizer with a learning rate of 5\u00d710-4. The vocoder-trained CM underwent 100 epochs of training using a weighted cross-entropy loss, as-signing a weight of 10 to the real class and 1 to the fake class. The learning rate was halved every 10 epochs. In contrast, the codec-trained CM was trained for 10 epochs, with the learning rate halved every 2 epochs. The model showing the best perfor-mance on the validation set was chosen for evaluation.\nFor experimental evaluation, we used the official imple-mentation for calculating EER\u00b94, maintaining precision to three decimal places. To compute the confusion matrix, we used a threshold of 0.5 to distinguish between real and fake predic-tions, utilizing calculations from SKlearn."}, {"title": "5. Results and Discussion", "content": "To verify the generalization ability of the CM, we first test the performance on traditional vocoder-based dataset as shown in the left part (19LA, ITW) of Table 1 and Table 2. Specifically, the vocoder-trained W2V2-AASIST achieve the best equal er-ror rate (EER) of 0.122% in 19LA test set and 23.713% in ITW dataset. Especially in 19LA, to the best of our knowledge, our CM can achieve the lowest EER, indicating the generalizabil-ity of the CM. Additionally, in cross-domain training scenarios, where only Codecfake is used for training and testing is con-ducted on 19LA and ITW, W2V2-AASIST also achieved good results, with an EER of 3.806% on 19LA and 9.606% on ITW.\nWe tested the collected and generated ALM data. For the 19LA-trained CM, the overall average (AVG) result was not very good, with the lowest AVG being 24.403% for W2V2-AASIST."}, {"title": "5.3. Discussion", "content": "Some codec-trained CMs still exhibit shortcomings, particu-larly high false negative (FN) rates, which suggest several rec-ommendations for future research on countering ALM-based audio. At first, the Codecfake dataset may lacks generaliza-tion to other audio types such as music and sound. Even though codec-trained CMs can detect ALM-based audio, they can also misclassify genuine audio. From dataset perspective, this may necessitate enriching the codec with a variety of audio types.\nAs for CM, specialized features for different audio types rather than relying solely on speech self-supervised feature such as W2V2 may be needed. Additionally, the high FN rates indicate deficiencies in the classifier's ability to learn from real-world audio data. This is apparent due to the limited diversity of real-world domains covered by a single dataset during training, and the inadequate representation of real audio in pre-trained fea-tures. Therefore, strategies such as co-training with supplemen-tary real audio datasets or enhancing pre-trained features could significantly enhance performance."}, {"title": "6. Conclusions", "content": "In this paper, we attempt to address a novel question: does the current deepfake audio detection model effectively detect ALM-based deepfake audio? We evaluate this by collecting and generating the latest 12 types of ALM-based audio and assess-ing them using SOTA performance CM. The surprising results indicate that codec-trained CMs can effectively detect these ALM-based audios, with most EERs approaching 0%. This indicates that the current CM, specifically the codec-trained CM trained with Codecfake dataset, can effectively detect ALM-based audio."}]}