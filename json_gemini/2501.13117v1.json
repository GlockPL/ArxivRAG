{"title": "MyGO Multiplex CoT: A Method for Self-Reflection in Large Language Models via Double Chain of Thought Thinking", "authors": ["Ji Shihao", "Song Zihui", "Zhong Fucheng", "Jia Jisen", "Wu Zhaobo", "Cao Zheyi", "Xu Tianhao", "Data Dream", "AI."], "abstract": "Recent advancements in large language models (LLMs) have demonstrated their impressive abilities in various reasoning and decision-making tasks. However, the quality and coherence of the reasoning process can still benefit from enhanced introspection and self-reflection. In this paper, we introduce Multiplex CoT (Chain of Thought), a method that enables LLMs to simulate a form of self-review while reasoning, by initiating double Chain of Thought (CoT) thinking. Multiplex CoT leverages the power of iterative reasoning, where the model generates an initial chain of thought and subsequently critiques and refines this reasoning with a second round of thought generation. This recursive approach allows for more coherent, logical, and robust answers, improving the overall decision-making process. We demonstrate how this method can be effectively implemented using simple prompt engineering in existing LLM architectures, achieving an effect similar to that of the Learning-Refinement Model (LRM) without the need for additional training. Additionally, we present a practical guide for implementing the method in Google Colab, enabling easy integration into real-world applications.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revolutionized natural language processing (NLP) by ex-celling in tasks ranging from translation to text generation. However, these models often struggle with producing coherent, logical reasoning when faced with complex decision-making scenarios. One of the key limitations of LLMs is their inability to critically reflect on their own thought process, which can lead to inconsistencies and errors in the final output. While recent research has explored methods for improving reasoning in LLMs, including Chain of Thought (CoT) rea-soning and fine-tuning approaches, there is still room for improvement in terms of the model's ability to refine and critique its own reasoning.\nIn this paper, we propose Multiplex CoT, a novel method for enhancing LLM reasoning by prompting the model to perform a self-reflection process. The technique involves generating an initial CoT and then initiating a second round of reasoning, which critiques and refines the initial chain of thought. By employing this iterative process, the model can simulate a form of self-review, leading to more coherent and logical outputs. Importantly, this method does not require additional training but instead utilizes a simple prompt engineering approach, making it easy to implement in existing LLM architectures."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Chain of Thought (CoT) Reasoning", "content": "Chain of Thought (CoT) reasoning has been proposed as a technique to improve the logical coherence of LLM outputs. The method involves prompting the model to produce a step-by-step sequence of thoughts, which guides the reasoning process and helps the model arrive at more accurate conclusions. CoT has been shown to significantly improve performance in tasks that require complex reasoning, such as mathematical problem-solving and commonsense reasoning."}, {"title": "2.2 Learning-Refinement Models (LRM)", "content": "Learning-Refinement Models (LRM) aim to improve model performance by iteratively refining the outputs through multiple training steps. These models typically involve a feedback loop where the initial predictions are revised based on some form of error analysis or critique. While LRM-based approaches have proven effective in certain contexts, they often require additional training and fine-tuning, which can be computationally expensive and time-consuming."}, {"title": "2.3 Self-Reflection in AI", "content": "Self-reflection is a cognitive process in which an agent reviews its own reasoning to identify errors or inconsistencies. While traditional LLMs are not equipped for self-reflection, recent work in meta-learning and reinforcement learning has explored ways to enable models to reflect on their actions. This line of research has shown promise in improving decision-making processes, particularly in scenarios where error correction or refinement is crucial."}, {"title": "3 Multiplex CoT: A Double Chain of Thought Approach", "content": "Multiplex CoT combines the benefits of CoT reasoning with a self-reflection mechanism. The process is outlined as follows:"}, {"title": "1. Initial CoT Generation:", "content": "The model generates a chain of reasoning, where each step of the thought process is articulated and used to reach a final conclusion."}, {"title": "2. Review and Refinement:", "content": "After generating the initial CoT, the model then initiates a second round of reasoning, which critiques the first CoT. This second chain of thought evaluates the logical consistency of the initial reasoning, identifying any potential flaws or inconsistencies."}, {"title": "3. Final Output:", "content": "Based on the critique, the model refines its reasoning, producing a more coherent and accurate final answer.\nThis two-phase process mimics human-like self-reflection, where the first phase involves gen-erating ideas, and the second phase focuses on evaluating and refining those ideas. The method is designed to work seamlessly with existing LLM architectures, without requiring any changes to the underlying model parameters."}, {"title": "3.1 Prompt Engineering", "content": "To implement Multiplex CoT, we utilize a simple prompt engineering technique. By structuring the input prompt to request both the initial reasoning and a follow-up critique, the model is able to generate and refine its reasoning within the same inference cycle. The prompt is designed to encourage the model to \"think twice\" about its initial response, leading to better overall performance."}, {"title": "3.2 Example", "content": "Consider the following example:\nPrompt:\nPlease solve the following problem: What is the capital of France?\nFirst, generate a Chain of Thought for how you would arrive at the answer. Then,\nreview your answer and critique it. If you find any inconsistencies or errors, correct\nthem and provide the final answer.\nThe model might respond with:\nStep 1 (Initial CoT):\n\u2022 France is a country in Europe.\n\u2022 The capital of France is well-known as the city of Paris.\n\u2022 Therefore, the capital of France is Paris.\nStep 2 (Review and Refinement):\n\u2022 Upon reviewing my initial reasoning, I realize that Paris is indeed the capital of France, and no errors or inconsistencies are present.\n\u2022 The final answer is Paris."}, {"title": "4 Mathematical Analysis of Multiplex CoT: Refining the Reasoning Process", "content": "To mathematically validate the effectiveness of Multiplex CoT in improving the reasoning quality of large language models (LLMs), we will introduce formal definitions for the concepts of logical consistency, coherence, and error correction rate. These metrics provide a quantitative way to assess the impact of self-reflection on reasoning quality."}, {"title": "4.1 Logical Consistency and Coherence", "content": "The key advantage of Multiplex CoT lies in its ability to iteratively improve the reasoning process by reviewing and refining the initial output. We define logical consistency as the number of valid logical connections between consecutive reasoning steps. If si represents the i-th step in the Chain of Thought, and \u2161(si, Si+1) is an indicator function that returns 1 if there is a logical connection between si and si+1, then the logical consistency C for a single Chain of Thought is:\n$C_{CoT} = \\sum_{i=1}^{n-1} I(s_i, s_{i+1})$\nwhere n is the total number of steps in the reasoning chain. A higher value of $C_{CoT}$ indicates that the reasoning steps are logically consistent and connected.\nWhen applying Multiplex CoT, a second round of reasoning is conducted, which critiques and refines the initial reasoning. We define the coherence of the reasoning process as the degree of alignment between the initial and refined reasoning steps. The coherence H can be quantified as:\n$H = \\frac{\\sum_{i=1}^{n} I(s_i, s_i^{refined})}{n}$\nwhere $s_i^{refined}$ is the corresponding statement in the refined reasoning chain, and \u2161(si, sefined) is 1 if the statement in the second round is consistent with the original reasoning. Coherence measures how well the second round of reasoning preserves the logic of the initial thought while refining it."}, {"title": "4.2 Error Correction Rate", "content": "One of the primary benefits of Multiplex CoT is its ability to correct errors in the initial reasoning chain during the second round of thought generation. We define the error correction rate Ecorr as the proportion of errors identified and corrected in the second round of reasoning. Let Einitial represent the number of errors in the initial chain of thought, and Ecorrected represent the number of errors corrected during the review. The error correction rate can be calculated as:\n$E_{corr} = \\frac{E_{corrected}}{E_{initial}} \\times 100$\nA higher value of Ecorr indicates that Multiplex CoT is effective at identifying and rectifying mistakes made during the first round of reasoning, leading to a more accurate final output."}, {"title": "4.3 Iterative Refinement and its Impact on Error Correction", "content": "To further analyze the impact of iterative refinement, we introduce a recursive function for reasoning quality across multiple rounds. Let C(k) denote the logical consistency score after the k-th round of reasoning. Initially, at k = 1, the model produces a chain of thought with consistency C(1) = CCOT. After the second round of reasoning, the model refines its output, and the consistency score improves to C(2) = CRefined. We can generalize the improvement in consistency after k rounds of reasoning as:\n$C^{(k)} = C^{(k-1)} + \\delta_k$\nwhere dk represents the change in consistency from the (k \u2212 1)-th to the k-th round. In the case of Multiplex CoT, the first two rounds provide significant improvements, with diminishing returns observed as additional rounds of reasoning are performed.\nThe total improvement after K rounds of reasoning can be expressed as the cumulative sum of consistency changes:\n$Total\\ Improvement = \\sum_{k=1}^{K} \\delta_k$\nIn practice, we observe that the most significant improvements occur in the first few rounds of reasoning. This behavior is consistent with the Multiplex CoT approach, where the second round of self-reflection provides substantial refinement to the reasoning process."}, {"title": "4.4 Quantitative Validation of Multiplex CoT", "content": "To validate the impact of Multiplex CoT, we conducted a series of experiments across various tasks. For each task, we measured both the logical consistency and error correction rate before and after applying Multiplex CoT. Below is a summary of the findings for the arithmetic problem-solving task.\nIn this example, the Multiplex CoT approach improved logical consistency by 7%, while the error correction rate was 15%, indicating that the model was able to identify and correct a significant proportion of mistakes during the self-reflection phase."}, {"title": "4.5 Extension to Other Tasks", "content": "We also evaluated the impact of Multiplex CoT on tasks beyond arithmetic, such as common-sense reasoning, ethical decision-making, and logical puzzles."}, {"title": "5 Conclusion", "content": "In this section, we provided a detailed mathematical analysis of Multiplex CoT, quantifying its impact on logical consistency, coherence, and error correction rates. The findings demonstrate that Multiplex CoT significantly enhances the reasoning process of large language models by improving both the quality of reasoning and the model's ability to self-correct. Through iterative refinement, Multiplex CoT outperforms traditional single-phase Chain of Thought reasoning, providing a more robust approach for tasks requiring logical rigor and critical reflection.\nThe combination of theoretical insights and experimental results confirms that Multiplex CoT offers a scalable and effective method for improving LLM performance, making it a valuable tool for applications requiring accurate, coherent, and consistent reasoning."}]}