{"title": "GeLLM30: Generalizing Large Language Models for Multi-property Molecule Optimization", "authors": ["Vishal Dey", "Xiao Hu", "Xia Ning"], "abstract": "Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMs' potential for molecule optimization, we introduce MuMOInstruct, the first high-quality instruction-tuning dataset specifically focused on complex multi-property molecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLM3Os, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that GeLLM\u00b3Os consistently outperform state-of-the-art baselines. GeLLM3Os also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of GeLLM3Os as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. MuMOInstruct, models, and code are accessible through https://github.com/ninglab/GeLLMO.", "sections": [{"title": "1 Introduction", "content": "Drug discovery is a costly and time-consuming process, costing over $2 billion and a decade (Sertkaya et al., 2024). One of the most critical stages (Hughes et al., 2011) in this process is lead optimization (Sun et al., 2022a), where a molecule with promising bioactivity against a drug target is optimized into a lead molecule by improving multiple molecular properties simultaneously. For example, a hit molecule to treat schizophrenia is optimized such that it can permeate the blood-brain barrier (Pollak et al., 2018) to reach the DRD2 target (Seeman, 2006) in the brain, while balancing lipophilicity, solubility and toxicity. Improving all these properties together requires balancing multiple trade-offs (Nicolaou and Brown, 2013) and conflicting objectives (Nicolotti et al., 2011), making multi-property optimization extremely challenging.\nMost computational methods (Gao et al., 2022) for molecule optimization focus on single- or double-property tasks, leaving multi-property optimization tasks largely unexplored. Existing approaches (Kim et al., 2024; Yang et al., 2021) use predefined fitness and reward functions (Luukkonen et al., 2023), respectively, to model property trade-offs. However, designing such functions for each task demands significant effort and domain expertise. In contrast, other methods (Chen et al., 2021; Wu et al., 2024) obviate the need for such functions, but depend on scarce task-specific data, limiting their scalability and adaptability. Additionally, existing methods lack generalization to unseen tasks, hindering their practical applicability to emerging therapeutic requirements.\nLarge language models (LLMs) (Naveed et al., 2023) have demonstrated remarkable generalization to unseen tasks across diverse domains (Chang et al., 2024) recently. However, their potential in challenging, multi-property molecule optimization tasks remains largely unidentified. To fully identify LLMs' potential, we introduce MuMOInstruct, the first high-quality instruction-tuning dataset specifically focused on complex, multi-property tasks, each aiming to improve at least 3 properties simultaneously. This is in stark contrast to existing instruction-tuning datasets (Ye et al., 2025) that are limited to single- and double-property tasks.\nLeveraging MuMOInstruct, we develop a series of Generalizable LLMs for Multi-property Molecule Optimization, denoted as GeLLM30s, by instruction-tuning general-purpose LLMs. Task-specific GeLLM30s are fine-tuned on individual"}, {"title": "2 Related Work", "content": "Various computational approaches have been developed for molecule optimization (You et al., 2018; Blaschke et al., 2020; Xie et al., 2021; Bung et al., 2022; Sun et al., 2022b). For example, Modof (Chen et al., 2021), MIMOSA (Fu et al., 2021), and f-RAG (Lee et al., 2024) perform substructure modifications over molecular graphs. Chemformer (Irwin et al., 2022) and Prompt-Mol0pt (Wu et al., 2024) treat optimization as a translation over SMILES (Weininger, 1988) strings, and learn the required modification from molecule pairs. GraphGA (Jensen, 2019) and MolLeo (Wang et al., 2025) leverage genetic algorithms to evolve molecules via genetic algorithm. These methods (Kim et al., 2024; Yang et al., 2021) often require designing non-trivial fitness or reward functions to capture nuanced trade-offs among multiple properties. Moreover, such methods tends to generate molecules with entirely new scaffolds, limiting their applicability in vitro optimization.\nRecently, LLMs (Chang et al., 2024) have emerged as a promising option for molecule optimization. For example, ChatDrug (Liu et al., 2024) and Re3DF (Le and Chawla, 2024) leverage LLMs to optimize a molecule iteratively through multi-turn dialogues. DrugAssist (Ye et al., 2025) instruction-tuned Llama2-7B-Chat (Touvron et al., 2023) on each optimization task. While these approaches offer flexible task formulation through natural language, they still face several limitations. ChatDrug incurs high costs due to multiple API calls, and instruction-tuning in DrugAssist relies"}, {"title": "3 MuMOInstruct Dataset", "content": "Comparison among MolOpt-Instructions and MuMOInstruct: To address these gaps, we introduce MuMOInstruct, the first instruction-tuning dataset specifically focusing on realistic multi-property optimization tasks. Different from MolOpt-Instructions (Ye et al., 2025), which focuses on single- and double-property tasks, MuMOInstruct emphasizes on tasks with at least 3 properties for evaluating LLMs in in-domain and out-of-domain settings. Table A1 highlights the notable differences between the two datasets.\nProblem Definition: A molecule optimization task is to transform a hit Mx a molecule exhibiting initial bio-activity against a therapeutic target into a lead molecule My an improved molecule for drug development, through structural modification over Mr, such that (1) My is structurally similar to Mx (similarity constraint), and (2) My is better than Mx in terms of all desired properties of interest (property constraint). The desirability of a property is determined by the therapeutic goal, where improved properties indicate more suitable toward a successful drug candidate. For example, for drug candidates targeting the central nervous system (CNS), higher blood-brain barrier permeability (BBBP) is desired to allow the molecules to reach the brain or spinal cord, whereas for those targeting the peripheral nervous system (PNS), lower BBBP is desired instead to prevent the drugs from damaging the CNS. Under the property constraint, the molecule pair (Mx, My) is represented as (Mx <\u2206p My)\u2200p\u2208P, indicating that My is better than Mx on each property p of all the desired properties P with a property-specific difference \u2206p.\nIn this paper, we introduce MuMOInstruct, the first high-quality instruction-tuning dataset to evaluate models in molecule optimization tasks. Our design is based on four key principles: (1) Pair-wise optimization: MuMOInstruct contains a comprehensive set of molecule pairs satisfying the similarity constraint (Tanimoto similarity > 0.6) and property constraint over multiple desired properties. Such molecule pairs enable opportunities for molecule optimization models to learn the association between the structural differences and the property improvement among the pairing molecules, and apply such associations for new lead optimization. (2) Comprehensive coverage: MuMOInstruct covers more molecular properties, and extends beyond single- and double-property tasks in existing molecular optimization benchmarks. It introduces multi-property optimization tasks that require simultaneous improvement of at least 3 properties, thereby representing complex pharmacological trade-offs in lead optimization. (3) Real-world relevance: The tasks in MuMOInstruct are carefully constructed to represent realistic challenges in lead optimization by combining molecular properties key to drug development. For instance, one of the tasks aims to improve intestinal adsorption, toxicity and BBBP \u2013 key properties for optimizing orally bioavailable CNS drugs. (4) Diverse instructions: MuMOInstruct provides diverse natural language instructions, each describing the optimization task using different phrasings. This prevents LLMs instruction-tuned on MuMOInstruct from overfitting to a specific phrasing and thus, enables them to generalize to unseen instructions, which is crucial in practice to allow different descriptions on optimization tasks."}, {"title": "3.1 Overview of MuMOInstruct Tasks", "content": "MuMOInstruct comprises 63 tasks, with 42 tasks aiming to improve at least 3 properties simultaneously, out of which 10 tasks are further divided into IND and OOD tasks (Section 3.5). All tasks in MuMOInstruct are systematically designed by considering combinations from 6 molecular properties: (i) Penalized LogP (plogP) representing lipophilicity, balancing permeability, solubility, and metabolic stability \u2013 higher plogP is generally desired in drug development; (ii) Quantitative Estimate of Drug-Likeness (QED) assessing overall drug-likeness, incorporating multiple molecular attributes related to molecular weight, lipophilicity, and solubility \u2013 higher QED indicates better drug-likeliness; (iii) Blood-Brain Barrier Permeability (BBBP) which refers to the ability of a drug to cross the blood-brain barrier \u2013 higher BBBP is desired for CNS drug candidates; (iv) Mutagenicity (Mutag) indicating the likelihood of a drug causing genetic mutations \u2013 lower Mutag scores are desired to reduce toxicity; (v) Human Intestinal Absorption (HIA) which reflects a drug's ability to be absorbed through the gastrointestinal tract \u2013 higher HIA is desired for orally administered drugs; and (vi) Dopamine Receptor D2 (DRD2) binding"}, {"title": "3.2 Creation of Task-Specific Training Pairs", "content": "We construct task-specific training pairs, where each pair (Mx, My) is sourced from the dataset provided by Chen et al. (2021), which consists of 255K molecule pairs dervied from 331K molecules. Each pair differs at only one disconnection site, meaning Mr can be transformed to My by modifying exactly one fragment. Among these molecule pairs, we select those that satisfy all P property constraints for a given task optimizing P properties (i.e., (Mx <\u2206p My)\u2200p\u2208P). This ensures that the hit molecule Mx in each pair requires substantial optimizations, making the selected pairs suitable to model realistic optimization tasks."}, {"title": "3.3 Creation of Task-Specific Test Set", "content": "We construct a test set by randomly sampling an initial pool of 250K molecules from the ZINC database (Sterling and Irwin, 2015) \u2013 a collection of commercially available drug-like molecules that are not included in the training set. Out of this pool, we select a molecule into the test set of a task which has a property worse than the median among all Mr in the training pairs (i.e., median property scores of Mx denoted as MPStrain) for each desired property. This provides a task-specific, data-driven selection criteria that is robust to outliers. Additional criteria to exclude outliers are detailed in Appendix A.2. After applying these steps to the initial pool of 250K molecules, we randomly select at most 500 molecules into the test set for each task, with possible overlap across tasks. Table A2 presents the task-specific data set characteristics."}, {"title": "3.4 Quality Control", "content": "We implement multiple quality control measures as detailed in Appendix A.3. We remove duplicate molecules based on canonicalized SMILES strings. For each molecule, we compute empirical property scores using well-established tools: ADMET-AI (Swanson et al., 2024) and the official implementation provided by You et al. (2018). Additionally, we provide 6 distinctly phrased (i.e., diverse) instructions for each task (Appendix A.4). To evaluate LLMs' instruction understanding and generalizability to unseen instructions, we hold out one instruction for each task during training."}, {"title": "3.5 IND and OOD Tasks", "content": "To distinctly assess the capabilities of instruction-tuned LLMs on both familiar and novel optimization tasks, we categorize our tasks into two groups:"}, {"title": "4 GELLM30 Models", "content": "We introduce GeLLM30, a series of general-purpose LLMs instruction-tuned over MuMOInstruct. Through instruction tuning, GeLLM30 implicitly learns chemical semantics, structure-property relationships (SPR) (Hansch, 1969) and associations between structural differences expressed in molecule pairs and the desired property improvement expressed via natural language instruction. GeLLM30 applies this knowledge to perform structural modifications on a given molecule and generate better molecules with improved properties.\nWe develop both task-specific and generalist GeLLM3Os. Task-specific models are trained on a single optimization task, and thus benefit from dedicated training tailored to that specific task. In contrast, generalist models are trained across multiple optimization tasks simultaneously. This multi-task training enables cross-task knowledge transfer, allowing the generalist GeLLM30 to leverage shared chemical knowledge on SPR and multi-property trade-offs across all possible property combinations. Thus, the generalist GeLLM30 represents a step toward a foundational model for molecule optimization, capable of handling diverse tasks without task-specific retraining.\nWe develop a series of generalist GeLLM3Os trained on the power sets of 3, 4, and 6 properties, denoted as GeLLM30-P(3), GeLLM30-P(4), and GeLLM30-P(6), respectively. To train these models, we fine-tune 2 general-purpose LLMs: Mistral-7B-Instruct-v0.3 (AI, 2023) and Llama3.1-8B-Instruct (Grattafiori et al., 2024) by applying LORA (Hu et al., 2022) adapters to all projection layers and the language modeling head. We per-"}, {"title": "5 Experimental Setup", "content": ""}, {"title": "5.1 Baselines", "content": "We compare GeLLM3Os against 3 categories of baseline models: (1) general-purpose LLMs: Mistral-7B Instruct-v0.3 (AI, 2023), Llama-3.1 8B-Instruct (Touvron et al., 2023), and Claude-3.5; (2) foundational LLMs for chemistry: LlaSMol tuned on Mistral-7B, denoted as LlasMolMistral (Yu et al., 2024), and (3) task-specific non-LLMs: Prompt-Mol0pt (Wu et al., 2024). Similarly to GeLLM30s, we generate 20 molecules for each input molecule for all baselines. For LLM baselines, we use the same generation strategy as for GeLLM30s. Experimental setups are detailed in Appendix B.2. Prompt templates for LLMs are in Appendix C. Discussion on Prompt-Mol0pt and DeepSeek-R1 are in Appendix E and F, respectively."}, {"title": "5.2 Evaluation Metrics", "content": "We employ multiple evaluation metrics (detailed in Appendix B.3) for a holistic comparison. For brevity and clarity, we present the results only in terms of: (1) Success Rate (SR) which is the proportion of input molecules that are successfully optimized with improvement in all desired properties; (2) Similarity with input (Sim) which denotes the average Tanimoto similarity (Bajusz et al., 2015) between the optimized and the corresponding input molecule; and (3) Relative Improvement (RI) representing the average improvement in each desired property relative to its initial value in the input molecule. Higher SR, Sim, and RI are desirable, indicating more successful optimizations."}, {"title": "6 Experimental Results", "content": "Main Findings: Our experiments reveal the following findings: (1) Both task-specific and generalist GeLLM3Os consistently outperform general-purpose LLMs, foundational LLMs for chemistry, and task-specific non-LLMs across all IND (Section 6.1) and OOD tasks (Section 6.2), significantly improving SR by as much as 186.6% over the best baseline. (2) Compared to task-specific GeLLM30s, generalist GeLLM30s excel on 3 out of 5 IND tasks and demonstrate competitive performance on the"}, {"title": "6.1 IND Evaluation", "content": "Table 2 shows the overall performance of GeLLM3Os and baselines across all 5 IND tasks. Detailed results for each task are in Appendix D.1.\nOverall Comparison: Both task-specific and generalist GeLLM\u00b3Os significantly outperform all baselines across all IND tasks. Specifically, the generalist GeLLM\u00b3OS, GeLLM3O-P(4)Mistral and GeLLM3O-P(6)Mistral, achieve an average SR of 76.8% and 76.1%, respectively, across all 5 tasks - outperforming the best baseline by 113.2% and 108.8% on average. This is due to the ability of generalist GeLLM30s to leverage knowledge syner-"}, {"title": "6.2 OOD Evaluation", "content": "Table 3 presents the overall performance of GeLLM3Os and baselines across all 5 OOD tasks (with detailed results in Appendix D.2). Note that OOD tasks involve novel property combinations excluded from training, making task-specific models and comparisons with GeLLM30-P(3) and GeLLM30-P(4) inapplicable. Generalist GeLLM\u00b3OS demonstrate robust 0-shot generalization to OOD tasks, significantly outperforming all baselines. For instance, both GeLLM30-P(6)Mistral and GeLLM30-P(6)Llama achieve very high SR of 88.7% and 90.8%, respectively, on average across all 5 tasks - outperforming strong baselines such as Claude-3.5 (5-shot) and LlaSMolMistral by as much as 159.9% on task BDMQ. By learning optimization strategies and property trade-offs across diverse tasks during training, generalist GeLLM\u00b3Os develop a flexible understanding of modification strategies that can generalize to novel optimization tasks. This generalizability is crucial in practice, where the dynamic nature of therapeutic requirements requires one unified foundational model capable of handling novel and diverse optimization tasks without task-specific retraining."}, {"title": "6.3 Generalizability to Unseen Instructions", "content": "Table 4 presents the performance of task-specific GeLLM3Os and generalist model, GeLLM30-P(6), when prompted with a hold-out instruction and unseen property names (Appendix A.4). Overall, task-specific GeLLM\u00b3Os retain their performance across all tasks, while, generalist GeLLM30s exhibit a slight drop of 7% in SR on average. This minor drop is expected, since generalist GeLLM3Os trained on more property combinations, encounter the same property names more frequently during instruction-tuning. This may lead to subtle overfitting to specific names. Importantly, even with this minor performance drop, generalist GeLLM3Os still outperform all baselines by a large margin, (Section 6.1), highlighting their overall superiority. Detailed results are provided in Appendix D.3."}, {"title": "6.4 Case Studies", "content": "Figure 2 shows a successful optimization for the OOD task BHMQ, where GeLLM30-P(6)Mistral improves all desired properties by replacing the sugar moiety in Mx with a nitrogen-containing heterocycle in My (highlighted fragments). The sugar moiety, with multiple hydroxyl (-OH) groups, increases polarity and hydrogen bonding, limiting passive permeability and leading to low BBBP and HIA (Goetz et al., 2017; Mullard, 2018). Replacing this fragment with a nitrogen heterocycle reduces polarity and hydrogen bonding, leading to improved BBBP (+0.74) and HIA (+0.56). More-"}, {"title": "7 Conclusion", "content": "In this work, we introduced MuMOInstruct, the first high-quality instruction-tuning dataset specifically focused on challenging multi-property optimization tasks. Leveraging MuMOInstruct, GeLLM30s achieve state-of-the-art performance across all IND and OOD tasks, notably outperforming strong general-purpose LLMs and founda-"}, {"title": "8 Limitations", "content": "Despite the strong performance of GeLLM30s as demonstrated in our work, we acknowledge several limitations. (1) We did not explore scenarios where users specify precise property-specific improvement thresholds during inference, which could enhance the applicability of GeLLM30s for highly customized therapeutic needs. (2) Our evaluations are limited to single-step optimization. We did not explore iterative refinement of generated molecules that could yield even better lead molecules over multiple steps. (3) MuMOInstruct leverages empirical property scores that are not experimentally validated, which may impact the accuracy of optimization outcomes. (4) MuMOInstruct encompasses 6 molecular properties that play a critical role in successful drug design. However, real-world lead optimization often involves additional, more specialized properties and complex trade-offs depending on specific therapeutic requirements. Addressing these limitations in future work could enhance GeLLM30s' applicability in practice."}, {"title": "9 Impact Statement", "content": "Our work introduces the first large-scale, high-quality instruction-tuning dataset, MuMOInstruct, specifically focused on molecule optimization tasks improving at least 3 properties simultaneously. By leveraging MuMOInstruct, we developed a series of instruction-tuned LLMs (GeLLM30s). These models significantly outperform strong closed-source LLMs such as Claude-3.5 as well as foundational LLMs for chemistry on complex multi-property optimization tasks. To the best of our knowledge, our work is the first to introduce a generalist model training framework and a foundational model for molecule optimization. Notably, the robust zero-shot performance of our generalist GeLLM30s demonstrates their potential as foundational models for molecule optimization, offering scalability and adaptability to diverse optimization scenarios."}, {"title": "Broader Impacts:", "content": "The introduction of foundational models capable of handling diverse optimization tasks holds tremendous potential to accelerate drug discovery pipelines. These models offer unparalleled flexibility and scalability, enabling practitioners to adapt them to a wide range of therapeutic requirements without requiring resource-intensive training. By relying solely on an efficient inference process, such models democratize access to advanced optimization capabilities to a broader range of practitioners. This advancement could streamline the identification of novel drug candidates, significantly reducing the cost and time required to develop a new drug."}, {"title": "10 Ethics Statement", "content": "While MuMOInstruct has been carefully curated to include drug-like, commercially accessible molecules, we cannot guarantee that the dataset is entirely free from inaccuracies or harmful content. We also cannot eliminate the potential of our tuned GeLLM\u00b3Os to generate undesirable or harmful content (e.g., lethal drugs). We should emphasize that our models are specifically tuned to improve widely used molecule properties aligned with general drug discovery goals, and are not intended for generating toxic or lethal molecules.\nThe only property in MuMOInstruct that is related to toxicity is mutagenicity, which measures the risk of DNA mutations. Importantly, our models are tuned explicitly to reduce mutagenicity, and not to increase it. Furthermore, GeLLM30 models are tuned exclusively on drug-like molecules and optimization objectives aimed at reducing mutagenicity. As a result, they are unlikely to generate molecules with increased toxicity or molecules that can be lethal under a normal dosage.\nHowever, if such molecules can be generated with adversarial prompts, this could potentially arise from the pretrained knowledge of the base models, which includes broader chemical information outside the scope of MuMOInstruct and our instruction-tuning. To mitigate such risks, safeguards such as usage monitoring, and integration with toxicity prediction pipelines should be implemented when deploying these models. Users of our dataset and models are expected to uphold the highest ethical standards and incorporate robust validation pipelines to prevent misuse."}, {"title": "A.1 Details on Evaluation Tasks", "content": "In this section, we provide descriptions of 10 tasks in MuMOInstruct used for evaluation."}, {"title": "A.1.1 IND tasks", "content": "Below are the 5 IND tasks:\n1. BDP: This task optimizes molecules to improve BBBP, DRD2 receptor inhibition, and lipophilicity (plogP). These properties are critical for central nervous system (CNS) drugs, where molecules must penetrate the blood-brain barrier, bind effectively to the DRD2 receptor (a common target for neurological disorders), and maintain sufficient lipophilicity for stability and membrane permeability.\n2. BDQ: This task optimizes molecules to increase BBBP, DRD2 binding affinity, and improve QED. By balancing brain permeability, receptor activity, and drug-likeness, this task captures realistic trade-offs required in CNS drug development.\n3. BPQ: This task aims to improve BBBP, plogP, and QED, prioritizing brain permeability and appropriate lipophilicity while ensuring the optimized molecules retain favorable drug-like properties.\n4. DPQ: This task focuses on improving DRD2, plogP, and QED. It targets receptor binding potency while optimizing lipophilicity and maintaining overall drug-likeness, representing key requirements for receptor-specific drug design.\n5. BDPQ: This task jointly optimizes BBBP, DRD2 activity, plogP, and QED, reflecting a challenging and comprehensive scenario for developing CNS drug candidates with high permeability, receptor activity, and drug-like characteristics."}, {"title": "A.1.2 OOD tasks", "content": "Below are the 5 tasks used for evaluating out-of-domain generalizability:\n1. MPQ: This task focuses on reducing mutagenicity, improving plogP, and enhancing drug-likeness (QED). This task represents an early-stage lead optimization scenario to reduce genotoxic risks while ensuring adequate lipophilicity and drug-like properties.\n2. BDMQ: This task optimizes BBBP, DRD2 inhibition, mutagenicity, and QED. It reflects CNS drug development by balancing domapine receptor activity, brain permeability, and safety while ensuring overall drug-likeness.\n3. BHMQ: This task focuses on increasing BBBP and HIA, reducing mutagenicity, and improving QED. It is particularly relevant for orally administered CNS drugs, where both brain and intestinal absorption are critical.\n4. BMPQ: This task optimizes BBBP, mutagenicity, plogP, and QED. It reflects CNS drug design by balancing adequate lipophilicity, reduced toxicity, and favorable drug-like properties, simulating realistic requirements for CNS-active drugs.\n5. HMPQ: This task enhances HIA, reduces mutagenicity, and improves plogP and QED. It represents optimization for orally administered drugs, focusing on absorption, genotoxic risk reduction, and overall drug-like quality."}, {"title": "A.2 Additional Filtering in Test Set", "content": "Out of the initial pool of 250K molecules sampled from ZINC, we select a molecule into the test set of a task which has a property worse than the median MPStrain. Additionally, for properties with highly skewed distributions, we exclude molecules falling below the 10th percentile of properties in training hit molecules, thereby eliminating extreme cases (e.g., a molecule with a plogP of -30) that are rarely encountered as hits. After applying these steps to the initial pool of 250K molecules, we randomly select at most 500 molecules into the test set for each task, with possible overlap across tasks."}, {"title": "A.3 Quality Control", "content": "We implement multiple quality control measures to ensure dataset integrity. In MuMOInstruct, molecules are represented as Simplified Molecular Input Line Entry System (SMILES) (Weininger, 1988) strings that are canonicalized and deduplicated. For each molecule, empirical property scores are computed using well-established tools: ADMET-AI (Swanson et al., 2024) for BBBP, HIA, Mutag and QED, and the official implementation provided by You et al. (2018) for DRD2 and plogP. While these property scores are not experimentally validated, they provide reliable and computationally efficient estimates, making them well-suited for large-scale dataset construction like ours.\nWe also ensure instruction diversity to enhance the generalizability of instruction-tuned LLMs (Xu et al., 2024). We provide a manually written, clear and concise seed instruction into GPT-4 (OpenAI, 2024) to construct multiple distinctly phrased (i.e., diverse) instructions. We select into MuMOInstruct 5 diverse instructions synonymous with the seed instruction. To evaluate LLMs' instruction understanding and generalizability to unseen instructions, we hold out one instruction for each task during training. Thus, each task in MuMOInstruct has 5 diverse instructions for instruction tuning, and 1 unseen instruction for testing. All instructions are presented in Appendix A.4."}, {"title": "A.4 Diverse Instructions", "content": "Figure Al presents the prompt template used for instruction-tuning.\nThe '{instruction}' will be replaced with one of 6 diverse instructions. 5 diverse instructions are used in training, and 1 is held out for testing in the unseen instruction setting. Below are the six diverse instructions, where the first one is manually written, and the rest are generated by GPT-40. The last one is the hold-out instruction.\n1. \"Your task is to modify the given molecule to adjust specific molecular properties while keeping structural changes as minimal as possible. Your response should only contain a valid SMILES representation of the modified molecule enclosed with  tags.\"\n2. \"Modify the given molecule to adjust the specified molecular properties by substituting functional groups while keeping changes to the core structure minimal. Output only the SMILES of the modified molecule, wrapped in  tags.\"\n3. \"Your goal is to fine-tune the specified molecular properties of the given compound with minimal structural changes. Make the necessary adjustments and return the modified molecule in a SMILES format enclosed in  tags.\"\n4. \"Adjust the structure of the given molecule to target the specified adjustments in molecular properties. Retain the core structure as much as possible. Respond with only the SMILES of the modified molecule enclosed in  tags.\"\n5. \"Alter the given molecule to meet the desired property changes with the least structural alteration possible. Output only the adjusted molecule in SMILES format, using  tags.\""}, {"title": "B Details on Experimental Setup", "content": ""}, {"title": "B.1 GeLLM3Os", "content": "We develop a series of generalist GeLLM30s which are trained on the power sets of 3, 4, and 6 properties, denoted as GeLLM30-P(3), GeLLM30-P(4), and GeLLM30-P(6), respectively. To train these models, we fine-tune 2 general-purpose LLMs: Mistral-7B-Instruct-v0.3 (AI, 2023) and Llama3.1-8B-Instruct (Grattafiori et al., 2024) using LoRA (Hu et al., 2022), leveraging the Huggingface Transformers library (Wolf et al., 2020). We fine-tune all models with a learning rate of 1e-4 and a batch size of 128, using a cosine learning rate scheduler with a 5% warm-up period. We fine-tune task-specific GeLLM3Os and generalist GeLLM30s for 10 and 3 epochs, respectively, to balance efficiency and overfitting. We set LoRA parameters with \u03b1 = 16, dropout of 0.05, and a rank of 16, and apply LoRA adapters to all projection layers and the language modeling head. We perform 0-shot evaluations (i.e., without in-context examples) for all GeLLM30 models in all tasks. For each test molecule, we generate 20 molecules using beam search decoding, with the number of beams set to 20.\nThe number of trainable parameters varies from 42 million for Mistral-7B-Instruct-v0.3 to 44 million for Llama3.1-8B-Instruct. Task-specific GeLLM3Os need up to 1 hour on average on a NVIDIA H100 (Hopper) GPU for 10 epochs. Generalist GeLLM3Os take from 8 to 24 hours on average on the same GPU for 3 epochs, depending on the number of tasks (property combinations). In total, we spent about 120 GPU hours on an NVIDIA H100 GPU with 96 GB HBM2e memory."}, {"title": "B.2 Baselines", "content": "In this section, we present the baselines considered and selected for our comparison. Table A3 details the licenses and sources for both the datasets and models (i.e., artifacts). We ensured that all artifacts used in this work were employed in a manner consistent with their intended use as specified by the original authors or licensors. For the models we developed, we identified ethical considerations which are discussed in Section 10.\nGeneral-purpose LLMs: We evaluate 3 general-purpose LLMs: Mistral-7B Instruct-v0.3 (\u0391\u0399, 2023), Llama-3.1 8B-Instruct (Touvron et al., 2023), and Claude-3.5 to assess the performance of such LLMs in molecule optimization. For Mistral-7B Instruct-v0.3 and Llama-3.1 8B-Instruct, we use the officially released checkpoints provided in"}, {"title": "B.3 Evaluation Metrics", "content": "We use the following evaluation metrics for a holistic comparison.\n1. Success rate (SR): Success rate is the proportion of test molecules for which at least one of 20 generated molecules has improvements in all desired properties. If multiple generated molecules have improved properties", "Val)": "Validity is the proportion of test hit molecules for which at least one of 20 generated molecules is chemically valid. A molecule is considered valid if it can be successfully parsed by RDKit. Higher validity indicates more test cases have valid generations.\n3. Similarity (Sim): Sim denotes the average Tanimoto similarity between successfully optimized molecules and the corresponding test molecules. The Tanimoto similarity is computed using binary Morgan fingerprints with a dimension of 2", "Nov)": "Novelty is defined as the percentage of optimized molecules that are unseen during training. Higher Nov indicates the models' ability to generate novel molecules", "SAS)": "SAS estimates how easily a molecule can be synthesized based on its structural complexity and the presence of uncommon fragments. SAS generally ranges from 1 (easy to synthesize) to 10 (challenging to synthesize (Ertl and Schuff"}]}