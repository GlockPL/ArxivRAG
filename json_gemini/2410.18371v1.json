{"title": "USMID: A Unimodal Speaker-Level Membership Inference Detector for Contrastive Pretraining", "authors": ["Ruoxi Cheng", "Yizhong Ding", "Shuirong Cao", "Shitong Shao", "Zhiqiang Wang"], "abstract": "Audio can disclose PII, particularly when combined with related text data. Therefore, it is essential to develop tools to detect privacy leakage in Contrastive Language-Audio Pretraining(CLAP). Existing MIAs need audio as input, risking exposure of voiceprint and requiring costly shadow models. To address these challenges, we propose USMID, a textual unimodal speaker-level membership inference detector for CLAP models, which queries the target model using only text data and does not require training shadow models. We randomly generate textual gibberish that are clearly not in training dataset. Then we extract feature vectors from these texts using the CLAP model and train a set of anomaly detectors on them. During inference, the feature vector of each test text is input into the anomaly detector to determine if the speaker is in the training set (anomalous) or not (normal). If available, USMID can further enhance detection by integrating real audio of the tested speaker. Extensive experiments on various CLAP model architectures and datasets demonstrate that USMID outperforms baseline methods using only text data.\nIndex Terms-membership inference, CLAP models, privacy", "sections": [{"title": "I. INTRODUCTION", "content": "Microphones in Internet of Things (IoT) devices [1] like phones can lead to unintended inferences from audio data [2]\u2013[5]. Vocal features and linguistic content can reveal personally identifiable information (PII) [6] like biometric identity and socioeconomic status. Combining audio with text data increases susceptibility to inference attacks. Thus, developing tools to detect privacy leakage in text-audio models like contrastive language-audio pre-training(CLAP) [7]\u2013[9] is essential.\nTraditional methods like membership inference attacks (MIAs) [10], have focused on determining whether a specific data sample was used for model training. Research on MIAs for multimodal contrastive learning (MCL) [11] like Contrastive Language-Audio Pretraining(CLAP) [12] is extensive [13]\u2013[15], but little attention is given to CLAP. Moreover, current MIAs for MCL often rely on dual-modal data inputs [16], which may lead to new leakage, as one modal of the pair might not have been exposed to the risky target model. Therefore, a detector that does not query CLAP with explicitly matched audio-text pair of speaker (see an example in Figure 1) is desirable. This concept is known as multimodal data protection [17]. Moreover, traditional MIAs train shadow models to simulate target model's behavior [18]\u2013[20], which requires high computational costs, particularly for multimodal models like CLAP.\nTo address these limitations, we propose USMID, a textual unimodal speaker-level membership inference [21] detector for CLAP models, which queries the target model with only text data. Specifically, we introduce a feature extractor that maps text data to feature vectors through CLAP-guided audio optimization. We then generate sufficient text gibberish that clearly does not match any text description in training dataset. As shown in Figure 2, we observe a distinct separation between the features of gibberish and members in the training set.\nBased on this observation, we train multiple anomaly detectors using the feature vectors of generated text gibberish, creating an anomaly detection voting system. During testing, USMID inputs the feature vectors of test text into the voting system to determine if the corresponding speaker is in(anomalous) or out(normal) of the training set.\nOur contributions are summarized as follows:\n\u2022 We constructed audio-text pair datasets and trained various architectures of CLAP models for further research.\n\u2022 We introduce USMID, the first speaker-level membership"}, {"title": "II. METHODOLOGY", "content": "Consider a CLAP model M trained on a dataset Dtrain. Each sample si = (ti,xi) in Dtrain captures the PII of a speaker, consisting of a textual description ti and its corresponding audio xi. For distinct indices i \u2260 j, it is possible for t\u2081 = tj while xi \u2260 xj, indicating that multiple non-identical audio samples may exist for the same speaker.\nA detector aims to probe potential leakage of a speaker's PII through the target CLAP model M by conducting a membership inference task. This task seeks to determine whether any PII samples of the speaker were included in the training set Dtrain.\nDetector's Goal. For a speaker with the textual description t, the detector aims to determine whether there exists a PII sample (ti, xi) \u2208 Dtrain such that t\u2081 = t.\nNote that our goal is not to detect a specific text-audio pair (t, x), but rather to identify the existence of any pair with textual description t. This is because that multiple audio samples of the same speaker may be used for training, any of which could contribute to potential PII leakage.\nDetector's Knowledge and Capability. The detector operates with black-box access to M, i.e., it can query M and observe the outputs, but does not know the model architecture of M, the parameter values, or the training algorithms. For the target textual description t, depending on the application scenarios, the detector may or may not have actual audios corresponding to t. However, if the detector does have the corresponding audio samples, it cannot include them in its queries to M due to privacy concerns. Additionally, the detector is unable to modify M or access its internal state."}, {"title": "B. Textual Speaker-Level Membership Inference Detector", "content": "We design a textual unimodal detector for membership inference (USMID), to determine whether the PII of a speaker is in the training set of the target CLAP model M, with the restriction that only the speaker's textual description is exposed to M. Firstly, for a textual description t, we develop a feature extractor to map t to a feature vector, through audio optimization guided by CLAP. Then, we make the key observation that textual gibberish like \"dv3*4l-XT0\"\u2014random combinations of numbers and symbols clearly do not match any textual descriptions in the training set, and hence the detector can generate large amount of textual gibberish that are known out of Dtrain. Using feature vectors extracted from these gibberish, the detector can train multiple anomaly detectors to form an anomaly detection voting system. Finally, during the inference phase, the features of the target textual description are fed into the system, and the inference result is determined through voting. Furthermore, when actual audio samples corresponding to the textual description are available, the detector can leverage them to perform clustering on the feature vectors of the test samples to enhance detection performance.\nAn overview of USMID is illustrated in Figure 3.\nFeature Extraction through CLAP-guided Audio Opti-mization. The feature extraction for a textual description t involves iterative optimization of an audio x, to maximize the correlation between the embeddings of t and x produced by the target CLAP model. The extraction process, described in Algorithm 1, iterates for n epochs; and within each epoch, an audio is optimized for m iterations, to maximize the cosine similarity between its embedding of CLAP and that of target textual description. The average optimized cosine similarity S and standard deviation of optimized audio embeddings D are extracted as the features of t from model M.\nGeneration of Textual Gibberish. USMID starts the detection process with generating a set of l gibberish strings G = {91, 92, ..., ge}, which are random combinations of digits and symbols with certain length. As these gibberish texts are randomly generated at the inference time, with overwhelming probability that they did not appear in the training set. Applying the proposed feature extraction algorithm on G, we obtain l feature vectors F = {f1, f2, ..., fe} of the gibberish texts.\nTraining Anomaly Detectors. Motivated by the observations in Figure3 that feature vectors of the texts in and out of the training set of M are well separated, we propose to train an anomaly detector using F, such that texts out of Dtrain are considered \"normal\", and the problem of membership inference on t is converted to anomaly detection on its feature vector. More specifically, t is detected to be in Dtrain, if its feature vector is detected \u201cabnormal\" by the trained anomaly detector. Specifically in USMID, we train several anomaly detection models on F, such as Isolation Forest [22], LocalOutlierFactor [23] and AutoEncoder [24]. These models"}, {"title": "III. EVALUATIONS", "content": "We evaluate the performance of USMID, for speaker-level membership inference using only text PII of the individual.\nDataset Construction. In addition to LibriSpeech [25], we built a speaker recognition dataset based on Common-Voice18.0 [26], which covers various social groups and has richer background information. Specifically, 3,000 speakers (1,500 for training and 1,500 for verification) were selected from CommonVoice, and their audio files were accompanied by unique user PII like ID, age, gender, and region information; then for each user ID, we used GPT-40 to generate detailed background description based on their PII; finally, these expanded background descriptions and audio files corresponding to each user ID constituted the training set of CLAP.\nBy doing this, we obtained basic facts about who is in the training set and who is not. For each type of content, we created two datasets: one with 1 audio clip per person and another with 50 audio clips per person.\nModels. In our CLAP model, audio encoder uses HTSAT [27], which is transformer with 4 groups of swin-transformer blocks [28]. We use the output of its penultimate layer (a 768-dimensional vector) as the output sent to the projection MLP layer. Text encoder uses ROBERTa [29], which converts input text into a 768-dimensional feature vector. We apply a 2-layer MLP with ReLU activation [30] to map the audio and text outputs to 512 dimensions for final representation.\nEvaluation Metrics. USMID's effectiveness is assessed using Precision, Recall, and Accuracy metrics, measuring anomaly prediction accuracy, correct anomaly identification, and overall prediction correctness, respectively.\nBaselines. Current speaker-level membership inference detection methods typically require detector to query target model with real audio. Most MIAs involve training shadow models, which can be particularly costly for large-scale multimodal models. We empirically compare the performance of USMID with the following SOTA inference methods. The audio encoders for Audio Auditor and SLMIA-SR are LSTM, for AuditMI they are Transformer, and for USMID, they are CLAP.\n\u2022 Audio Auditor [21] trains shadow models and extracts audio features for inference.\n\u2022 SLMIA-SR [19] employs a shadow speaker recognition system to train attack model.\n\u2022 AuditMI [31] trains shadow model using input utterances and features from model outputs."}, {"title": "A. Results", "content": "On training anomaly detectors, we randomly generated l = 120 textual gibberish (some of them are shown in Table IV).\nThe audio optimization was performed for n = 100 epochs; and in each epoch, m = 100 Gradient Descent (GD) iterations with a learning rate of 3 \u00d7 10\u22122. Four anomaly detection models, i.e., LocalOutlierFactor [23], IsolationForest [22], OneClassSVM [32], [33], and AutoEncoder [34] were trained, and N = 3 was chosen as the detection threshold.\nAs shown in Table I, USMID consistently outperforms all baselines even with only text PII, achieving a precision of 88.12% on LibriSpeech with 50 audio clips per person.\nWe also evaluate the effect of providing USMID with a real audio of the tested person. In this case, the embedding distances between the real and optimized audios of the test samples are used to perform a 2-means clustering, adding another vote to the inference. We accordingly raise the detection threshold N' to 4. As illustrated in Table II, the given audio helps to improve the performance of USMID across all tested CLAP models, showing an increase of 3.36% on CommonVoice with 1 audio clip per person."}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose USMID, the first method to conduct membership inference without exposing acutal audios to target CLAP models. USMID turns the inference problem into an anomaly detection problem, through randomly generating textual gibberish that are known to be out of training set, and exploting them to train anomaly detectors. Furthermore, the incorporation of real audios is shown to enhance detection performance. Through evaluations across various CLAP model architectures and datasets, we demonstrate the consistent superiority of USMID over baselines."}]}