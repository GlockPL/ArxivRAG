{"title": "UDC: A Unified Neural Divide-and-Conquer\nFramework for Large-Scale Combinatorial\nOptimization Problems", "authors": ["Zhi Zheng", "Changliang Zhou", "Xialiang Tong", "Mingxuan Yuan", "Zhenkun Wang"], "abstract": "Single-stage neural combinatorial optimization solvers have achieved near-optimal\nresults on various small-scale combinatorial optimization (CO) problems without\nneeding expert knowledge. However, these solvers exhibit significant performance\ndegradation when applied to large-scale CO problems. Recently, two-stage neural\nmethods with divide-and-conquer strategies have shown superiorities in addressing\nlarge-scale CO problems. Nevertheless, the efficiency of these methods highly\nrelies on problem-specific heuristics in either the divide or the conquer procedure,\nwhich limits their applicability to general CO problems. Moreover, these methods\nemploy separate training schemes and ignore the interdependencies between the\ndividing and conquering strategies, which often leads to sub-optimal solutions. To\ntackle these drawbacks, this article develops a unified neural divide-and-conquer\nframework (i.e., UDC) for solving general large-scale CO problems. UDC offers a\nDivide-Conquer-Reunion (DCR) training method to eliminate the negative impact\nof a sub-optimal dividing policy. Employing a high-efficiency Graph Neural Net-\nwork (GNN) for global dividing and a fixed-length sub-path solver for conquering\nsub-problems, the proposed UDC framework demonstrates extensive applicability,\nachieving superior performance in 10 representative large-scale CO problems.", "sections": [{"title": "1 Introduction", "content": "Combinatorial optimization (CO) [1] has numerous practical applications including route planning [2],\ncircuit design [3], biology [4], etc. Over the past few years, neural combinatorial optimization (NCO)\nmethods have been developed to efficiently solve typical CO problems such as the Travelling Salesman\nProblem (TSP) and Capacitated Vehicle Routing Problem (CVRP). Among them, reinforcement\nlearning (RL)-based constructive NCO methods [5, 6] have shown the performance of generating\nnear-optimal solutions for small-scale instances (e.g., TSP instances with no more than 200 nodes)\nwithout the requirement of expert knowledge [7]. These solvers mainly adopt a single-stage solution\ngeneration process, constructing the solution of CO problems step-by-step in an end-to-end manner.\nHowever, hindered by the disability of training on larger-size instances [8, 9, 10], these methods show\nlimited performance when generalizing to solve large-scale CO instances.\nTo meet the demands of larger-scale CO applications, researchers have paid increasing attention\nto extending NCO methods to larger-scale instances (i.e., 1,000-node ones) [11]. Existing NCO"}, {"title": "2 Preliminaries: Neural Divide-and-Conquer", "content": null}, {"title": "2.1 Divide-and-Conquer", "content": "A CO problem with N decision variables is defined to minimize the objective function f of a solution\nx \u2208 (X1,X2,...,xn) as follows:\nminimize f(x, G)                                                                (1)\nwhere G represents the problem instance and \u03a9 is a set consisting of all feasible solutions.\nLeveraging the divide-and-conquer idea in solving CO problems is promising in traditional (meta)\nheuristic algorithms [25] especially large-neighborhood-search-based algorithms [26, 27]. These\nmethods achieve gradual performance improvement from an initial solution through iterative sub-\nproblem selection and sub-problem repair. Recent neural divide-and-conquer methods conduct\na similar process and employ efficient deep learning techniques for initial solution, sub-problem\npartition (i.e., dividing policy), and sub-problem solving (i.e., conquering policy) [13]. The dividing\npolicy TD(G) decomposes the instance G to K mutually unaffected sub-problems {G1, G2, ...,GK}\nand the conquering policy \\(\u03c0\\)C generates the solutions sk, k \u2208 {1, ..., K} of corresponding sub-\nproblems by sk ~ \\(\u03c0\\)C(Gk). Finally, a total solution \u00e6 of G is merged as x = Concat(s1,..., sK)."}, {"title": "2.2 Constructive Neural Solvers", "content": "The (single-stage) constructive NCO solver has garnered attention from researchers for its ability\nto solve general CO problems with time efficiency. These constructive solvers typically employ\nan attention-based encoder-decoder network structure, where a multi-layer encoder processes CO\ninstances into embeddings in hidden spaces and a lightweight decoder handles dynamic constraints\nwhile constructing feasible solutions [5]. With being modeled as Markov Decision Processes (MDPs)\n[28], constructive solvers can be trained using deep reinforcement learning (DRL) techniques without\nrequiring expert experience. In the solution generation process, constructive solvers with parameter 0\nprocess the policy of a T-length solution x = {x1,...,xT} as follows:\n\u03c0(x|G, \u03a9, 0) = \u03a0P\u03b8(xt|x1:t\u22121, G, \u03a9),                                      (2)\nwhere x1:t-1 represents the partial solution before the selection of xt. DRL-based constructive NCO\nsolvers demonstrate outstanding solution qualities in small-scale CO problems. So existing neural\ndivide-and-conquer methods [13, 21] generally employ a pre-trained constructive neural solver (e.g.,\nAttention Model [28]) for their conquering policy \u03c0\u03b8."}, {"title": "2.3 Heatmap-based Solver", "content": "Advanced constructive NCO solvers rely on the self-attention encoder layers [29] for node repre-\nsentation, whose quadratic time and space complexity related to problem scales [30] hinders the\ndirect training on large-scale instances. So, heatmap-based solvers [31, 32] are proposed to solve\nlarge-scale CO problems efficiently by a lighter-weight GNN [33]. In heatmap-based solvers for\nVRPs, the GNN with parameter \u03b8 first generates a heatmap H\u2208 R\u00d1\u00d7N within linear time [34] and\na feasible solution is then constructed based on this heatmap with the policy [35] as follows:\n\u03c0(x|G,\u03a9, 0) = P\u03b8(H|G, \u03a9) P(x1) \u03a0 exp(Hxt-1,xt)\nHowever, the non-autoregressively generated heatmap [36] excludes the representations of the\nconstructed partial solution x1:t-1, so the greedy decoding policy in heatmap-based solvers becomes\nlow-quality and these solvers highly rely on search algorithms [37, 38] for higher-quality solutions."}, {"title": "3 Methodology: Unified Divide-and-Conquer", "content": "As shown in Figure 1, UDC follows the general framework of neural divide-and-conquer methods\n[13], solving CO problems through two distinct stages: the dividing stage and the conquering stage.\nThe dividing stage of UDC is expected to generate a feasible initial solution with a similar overall"}, {"title": "3.1 Pipeline: Dividing Stage & Conquering Stage", "content": "Dividing Stage: Heatmap-based Neural Dividing: Heatmap-based solvers require less time\nand space consumption compared to constructive solvers, so they are more suitable for learning\nglobal dividing policies on large-scale instances. The dividing stage first constructs a sparse graph\nGD = {V, E} (i.e., linking K-nearest neighbors (KNN) in TSP, or original edges in G for MIS) for\nthe original CO instance G. Then, we utilize Anisotropic Graph Neural Networks (AGNN) with\nparameter \u03a6 to generate the heatmap H. For N-node VRPs, the heatmap H \u2208 RN\u00d7N and a T-length\ninitial solutions xo = {xo,1,..., xo,T} is generated based on the policy \u03c0\u03b8 as follows:\n\u03c0\u03b8(xo|GD, \u03a9, \u03a6) = {p(H|GD, \u03a9, \u03a6) P(xo,1) \u03a0 exp(Hxo,t-1,xo,t) if xo \u2208 \u03a9\nConquering Stage: Sub-problem Preparation: The conquering stage first generates pending sub-\nproblems along with their specific constraints. For VRPs, the nodes in sub-VRPs and the constraints of\nsub-VRPs are built based on continuous sub-sequences in the initial solution xo. In generating n-node\nsub-problems, UDC divides the original N-node instance G into [] sub-problems {G1,...,G[]}\naccording to xo, temporarily excluding sub-problems with fewer than n nodes. The constraints\n{\u03a91,..., \u03a9[]} of sub-problems include not only the problem-specific constraints (e.g., no self-\nloop in sub-TSPs) but also additional constraints to ensure the legality of the merged solution after\nreplacing the original solution sub-sequence with the sub-problem solution (e.g., maintaining the first"}, {"title": "3.2 Training Method: Divide-Conquer-Reunion (DCR)", "content": "Both the dividing and conquering stages in neural divide-and-conquer methods can be modeled\nas MDP [12]. The reward for the conquering policy is derived from the objective function of the\nsub-solutions, while the reward for the dividing policies is calculated based on the objective function\nof the final merged solution. Existing methods face challenges of instability when training both\npolicies simultaneously with RL [20]. Therefore, they generally employ a separate training scheme,\npre-training a conquering policy on specific datasets before training the dividing policy [13]. However,\nthis separate training process not only requires additional problem-specific dataset designs [13] but\nalso undermines the collaborative optimization of the dividing and conquering policies (further\ndiscussed in Section 5.1).\nIn this paper, we imply that the instability of unified training is related to the sub-optimal sub-problems\ndecomposition. As shown in Figure 2, sub-problems generated by the initial solution will probably\nnot match any continuous segments in the optimal solution so there may be errors emerging at the\nconnection (i.e., linking region) of these sub-solutions. The naive solving procedure of neural divide-\nand-conquer methods ignores the negative impact of these solution errors, thus obtaining biased\nrewards for the dividing policy. To tackle this drawback, we propose the Divide-Conquer-Reunion\n(DCR) process to eliminate the impact as much as possible. DCR designs an additional Reunion step\nto treat the connection between the original two adjacent subproblems as a new sub-problem and\nperform another conquering stage on x1 with new sub-problem decompositions. The Reunion step"}, {"title": "3.3 Application: Applicability in General CO Problems", "content": "This subsection discusses the applicability of UDC to various CO problems. Most existing neural\ndivide-and-conquer methods rely on problem-specific designs, so they are only applicable to limited\nCO problems. UDC divides the original CO problems into their sub-problems with similar constraints,\nand uses no heuristics algorithms in the divide-and-conquer process, thus having the potential to be\napplied to general CO problems which satisfy the following three conditions:\n\u2022 The objective function contains only decomposable aggregate functions (i.e., No functions\nlike Rank or Top-k).\n\u2022 The legality of initial solutions and sub-solutions can be ensured with feasibility masks.\n\u2022 The solution of the divided sub-problem is not always unique.\nFor the second condition, the proposed UDC framework leverages autoregressive solution generation\nprocedures for both solutions and sub-solutions. Therefore, the proposed UDC is disabled on complex\nCO problems whose solution cannot be guaranteed to be legal through an autoregressive solution\nprocess (e.g., TSPTW). For the third condition, on certain CO problems such as the MIS problem on\ndense graphs or job scheduling problems, the solution of sub-problems has already been uniquely\ndetermined, so the conquering stages become ineffective."}, {"title": "4 Experiment", "content": "To verify the applicability of UDC in the general CO problem, we evaluate the UDC on 10 combi-\nnatorial optimization problems that satisfy the conditions proposed in Section 3.3, including TSP,\nCVRP, 0-1 Knapsack Problem (KP), Maximum Independent Set (MIS), ATSP, Orienteering Problem\n(OP), PCTSP, Stochastic PCTSP (SPCTSP), Open VRP (OVRP), and min-max multi-agent TSP\n(min-max mTSP). Detailed formulations of these problems are provided in the Appendix B.\nImplementation By implementing lightweight networks for dividing policies, UDC can be directly\ntrained on large-scale CO instances (e.g., TSP500 and TSP1,000). To learn more scale-independent"}, {"title": "5 Discussion", "content": "The experimental results have preliminarily proven that the proposed UDC method achieves excellent\nresults on a wide range of large-scale CO problems with outstanding efficiency. In this section, we\nconduct ablation experiments to verify the necessity of components in the training processes of UDC.\nAppendix E further includes the ablation experiments on the testing procedure of UDC, including the\nablation on the number of conquering stages r and the number of sampled solutions \u03b1."}, {"title": "5.1 Unified Training Scheme versus Separate Training Scheme", "content": "For neural divide-and-conquer methods, the unified training process is easier to implement than\nseparate training, which does not require a special design for pre-training conquering policies.\nMoreover, unified training can also avoid the antagonism between the dividing and conquering\npolicies. In this section, we pre-train a conquering policy (i.e., ICAM) on uniform TSP100 data and\nthen train two ablation variants of the original UDC (separate training scheme): one variant uses the\npre-trained conquering policy in the subsequent joint training (i.e., Pre-train + Joint Training, similar\nto H-TSP [20]), and the other variant only train the dividing policy afterward (i.e., Pre-train + Train\nDividing, similar to TAM [12] & GLOP [13]). According to the training curves of UDC and the two\nvariants in Figure 3, the unified training scheme in UDC demonstrates superiority, while the Pre-train\nstep leads the training into local optima, which significantly harms the convergence of UDC."}, {"title": "5.2 Ablation study", "content": "UDC adopts the state-of-the-art constructive solver\nICAM [16] for conquering sub-TSP. However, the se-\nlection of constructive solvers for the conquering policy\nis not limited. We employ POMO [33] for the conquer-\ning policy, with experiment results of this version (i.e.,\nUDC(POMO)) on TSP500 and TSP1,000 shown in Ta-\nble 5.1 (\u03b1 = 50 for all variants). The UDC framework\ndemonstrates conquering-policy-agnosticism, as it does\nnot show significant performance degradation when the\nconquering policy becomes POMO."}, {"title": "6 Conclusion, Limitation, and Future Work", "content": "This paper develops a training method DCR for neural divide-and-conquer methods by additionally\nconsidering the negative impact of sub-optimal dividing. By using DCR, this paper enables the\nsuperior unified training scheme and proposes a novel unified neural divide-and-conquer framework\nUDC. UDC exhibits not only outstanding performances but also extensive applicability, achieving\nsignificant advantages in 10 representative large-scale CO problems."}, {"title": "Limitation and Future Work.", "content": "Although UDC has achieved outstanding performance improvements,\nwe believe that UDC can achieve better results through designing better loss functions. In the future,\nwe will focus on a more suitable loss for the UDC framework to further enhance the solution quality."}]}