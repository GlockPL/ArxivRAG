{"title": "Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models", "authors": ["Akash Bonagiri", "Lucen Li", "Rajvardhan Oak", "Zeerak Babar", "Magdalena Wojcieszak", "Anshuman Chhabra"], "abstract": "The prevalence of harmful content on social media platforms poses significant risks to users and society, necessitating more effective and scalable content moderation strategies. Current approaches rely on human moderators, supervised classifiers, and large volumes of training data, and often struggle with scalability, subjectivity, and the dynamic nature of harmful content (e.g., violent content, dangerous challenge trends, etc.). To bridge these gaps, we utilize Large Language Models (LLMs) to undertake few-shot dynamic content moderation via in-context learning. Through extensive experiments on multiple LLMs, we demonstrate that our few-shot approaches can outperform existing proprietary baselines (Perspective and OpenAI Moderation) as well as prior state-of-the-art few-shot learning methods, in identifying harm. We also incorporate visual information (video thumbnails) and assess if different multimodal techniques improve model performance. Our results underscore the significant benefits of employing LLM based methods for scalable and dynamic harmful content moderation online.", "sections": [{"title": "Introduction", "content": "Social media platforms are an integral part of people's daily lives, influencing how individuals communicate, share in-formation, and connect with others. Platforms such as Face-book and YouTube are not only tools for interaction, but also play a crucial role in networking, marketing, education, sales, and political campaigns. However, platforms can also inadvertently disseminate a myriad of harmful content to their users. Over 30% of users have encountered hate speech or aggressive behavior online\u00b9, false stories are 70% more likely to be shared online than true ones (Vosoughi, Roy, and Aral 2018), vulnerable users are targeted with recommenda-tions to problematic and distressing content (Hilbert et al. 2023), and dangerous challenges are propagated through so-cial media (Haidt and Twenge 2023). Exposure to harm-ful content can have negative consequences for individuals, groups, and the society at large (Haidt and Twenge 2023).\nGiven these potential consequences, platforms take a number of steps to decrease exposure to harmful con-tent. These approaches range from removal (e.g., banning content that encourages violence, suicide, or eating disor-ders (YouTube 2021)), limiting amplification without re-moval (e.g., minimizing the sharing of controversial posts), or providing additional information alongside potentially harmful content (e.g., offering links to fact-checking pages to debunk false claims (Twitter 2023)). All these strategies depend on dynamic and scalable approaches for identifying ever-changing harmful content online.\nTo this end, platforms generally employ a combination of human moderators and automated classifiers (Facebook 2024). Human moderators review content flagged as harm-ful either by users or automated content moderation systems, and decide if it violates the platform's policies. In some cases, Machine Learning (ML) classifiers trained on large human-annotated datasets (Jigsaw 2023; OpenAI 2023) are used to identify misinformation, hate speech, sexually ex-plicit material, among other types of harm. However, both human and supervised content moderation classifiers pos-sess significant drawbacks. Human moderation is subjec-tive, not easily scalable, and prone to operator error (Gille-spie 2020). In turn, supervised ML classifiers are ineffi-cient, as they require large volumes of human-annotated data for training and struggle with nuances in language such as sarcasm and context-specific meanings (Baruah et al. 2020). These issues with supervised ML models are fur-ther compounded by the fact that harmful content classifi-cation is a dynamic temporal problem (e.g., new conspir-acies or new dangerous challenges become popular). As a result, content moderation classifiers are susceptible to con-cept drift (Qui\u00f1onero-Candela et al. 2022) and require hu-mans to annotate large amounts of data at frequent intervals, exacerbating the aforementioned scalability issues.\nIt is apparent then that an ideal solution to the harmful content moderation problem needs to be highly scalable (i.e. require minimal supervised signal and human effort) and highly effective (i.e. attain human moderator-level accuracy). Moreover, the approach should be easily extensible and"}, {"title": "LLMs for Harm Classification", "content": "We now describe our LLM based strategies for harmful con-tent moderation in the zero-shot and few-shot (in-context learning or ICL) unimodal (text-only) and multimodal (text + vision) settings. For undertaking multimodal classifica-tion, we describe two approaches: (1) converting the visual input into text for use with text-only LLMs (as in past work (Yang et al. 2024)) and (2) directly utilizing visual input with natively multimodal LLMs/VLMs. We first define the harm classification problem analytically and then formalize our zero-shot and few-shot/ICL approaches."}, {"title": "Problem Formulation", "content": "Let $X = \\{X_i | X_i =\\langle t_i, v_i \\rangle\\}_{i=1}^n$ be a sequence of n content instances, where each $x_i$ may contain both visual and textual components $v_i$ and $t_i$ respectively. Let L denote an LLM. Our goal is to learn a function $f : X \\rightarrow \\{0,1\\}$ using L that operates on a given prompt P to map every content instance $x \\in X$ to a binary label, such that:\n$f(x) = \\begin{cases}1 & \\text{if x is harmful,}\\\\0 & \\text{if x is harmless.}\\end{cases}$"}, {"title": "Zero-Shot Learning (ZSL)", "content": "ZSL (Yin, Hay, and Roth 2019) is the the ability of a model to classify data it has never encountered during training and perform tasks without explicit pretraining. Because LLMs are trained on vast amounts of data, we expect them to have an inherent understanding of what constitutes harmful con-tent. In the ZSL approach, we provide a sample as input to the model, and ask the LLM to classify it as harmful or harmless, without explicitly providing any demonstrations for what constitutes harmful/harmless. Now, the function f is defined using the LLM L and the content sample $x_i$, as:\n$f_{ZSL}(x_i):= L(t_i, P_{zSL}),$\nwhere $P_{zsL}$ is a prompt that guides L to classify $x_i$ as harm-ful or harmless without explicit examples of harmful con-tent. The text input $t_i$ for each content sample $x_i$ comprises of the YouTube video's title. The exact prompt we use can be found in Appendix C.1."}, {"title": "Few-Shot In-Context Learning (FS-ICL)", "content": "Few-shot learning (FSL) (Brown et al. 2020) refers to the ability of a model to learn new tasks from a very limited amount of supervised data, often restricted to 8-14 annotated samples. In this setting, we utilize LLMs to perform harm classification by providing a small number of task-specific exemplars within the input prompt, via in-context learning (Dong et al. 2022) (ICL). We opt for coverage-based ICL approaches as they have been shown to be highly successful at generalizing to a wide array of domains (Gupta, Gardner, and Singh 2023). These methods ensure maximally informa-tive demonstrations are selected by submodular optimiza-tion, which efficiently maximizes the coverage of salient as-pects of the content sample. Selectors such as BERTScore, cosine, and BM25 can guide the selection process. Thus, we utilize coverage-based approaches and augment the ZSL prompt to include k labeled examples contained in the ex-emplar set $E = \\{(x_j, Y_j)\\}_{j=1}^k$, where $y_j \\in \\{0,1\\}$ represents the true known label for $x_j$. The examples in $E$ are selected based on their ability to maximize coverage of salient as-pects and are ordered by relevance, with the most relevant examples placed closest to $x_i$. The function f takes the form:\n$f_{FS-ICL}(x_i) := L(t_i, E, P_{FS-ICL})$\nHere, $P_{FS-ICL}$ is an augmented prompt that instructs the LLM L to utilize the demonstration set $E$ while performing the classification based on the in-context exemplars utilizing BERTScore, Cosine, BM25 selectors. More details on se-lectors are provided in Appendix B."}, {"title": "Multimodal FS-ICL", "content": "We enhance the FSL approach by including features from another modality (vision) along with the textual input for each content sample. For YouTube videos, this constitutes using video thumbnails. To incorporate visual input, we use multimodal LLMs (also referred to as VLMs): GPT-40-Mini (OpenAI 2024), OpenFlamingo (Alayrac et al. 2022) and LLaVA (Liu et al. 2023). We employ models in two key ways: (1) Caption Generation (where we generate captions for the image input associated with the content sample and pass this in the text prompt) and (2) Direct Image Input (where we utilize a VLM that can operate on multimodal text + visual input directly). Note that we can also obtain the zero-shot setting as a special case for these multimodal approaches by simply discarding the exemplar set in few-shot learning.\nCaption Generation (CG) We pass the input image(s) (thumbnails) to BLIP pretrained (Bootstrapping Language-Image Pre-training) model on the captioning task (Shen et al. 2022; Li et al. 2022) and extract the generated caption. Then, we augment the textual content instance with this caption. Additionally, we did the same experiment with Qwen-VL-chat model (Bai et al. 2023). More details are mentioned in the Appendix G. We essentially follow the same procedure for the text-only FS-ICL approach but with the generated caption also provided as input to the LLM. Essentially, de-noting the captioning VLM as V, the function f incorporates a caption $c_i$ generated by V from the visual component, i.e. $c_i = V(v_i)$, as follows:\n$f_{FS-ICL-CG}(x_i) := L(t_i, c_i, E, P_{FS-ICL-CG}).$\nThe goal here is to extract additional signals from the im-age, which may not be present in the text (for example, a YouTube video may have a harmless caption, but a sexually explicit or clickbait thumbnail). However, errors in the cap-tion generation process might propagate further down the learning pipeline and reduce LLM performance.\nDirect Image Input (DII) Some LLMs/VLMs, such as GPT-40-Mini (OpenAI 2024) operate on multimodal input directly (Radford et al. 2021) (i.e., they can process and inte-grate both visual and textual data simultaneously). Now, by simply utilizing such an VLM in the FS-ICL approach and providing image data along with text input, we can augment task performance. The classification function is as follows:\n$f_{FS-ICL-DII}(x_i) := L(v_i, t_i, E, P_{FS-ICL-DII}).$\nThe Direct Image Input (DII) approach leverages mul-timodal data, enhancing the model's understanding by di-rectly incorporating visual cues. This approach can lead to more accurate and context-aware predictions, especially in cases where visual content carries critical information. However, it is relatively computationally intensive."}, {"title": "Experimental Setup", "content": "Datasets. We employ a curated dataset of YouTube videos (Jo, Weso\u0142owska, and Wojcieszak 2024) that encom-passes distinct types of harms identifiable in multimodal social media data. Each video in the dataset was labeled as harmful/harmless by crowdworkers, LLMs, and domain experts. The ground truth is taken as the majority label across these three labeling actors. We used a train-test split of 3,000 videos evenly divided between harmful and harmless videos. We chose this dataset because of the diverse nature of harms represented as well as features from multiple modalities (text, image). Additionally, to validate the generalizability of our approach, we evaluate it on two publicly available datasets; the Jigsaw Toxicity Classification Dataset (Google/Jigsaw 2019) which consists of comments from the Civil Comments platform, labeled for toxicity and various identity-based biases, and the Mea-suring Hate Speech dataset (D-Lab 2022) by UC Berkeley's D-Lab, which contains a large corpus of annotated social media posts specifically labeled for hate speech. More details on all datasets are provided in AppendixA.\nModels. For experiments, we use two open-source LLMs, Mistral-7B and Llama2-13B. Additionally, we employ pro-prietary closed-source OpenAI LLMs, GPT-3.5-Turbo and GPT-40-Mini. We use the BLIP (Li et al. 2022) model for generating captions and LLaVA (Liu et al. 2024), GPT-40-Mini (OpenAI 2024), OpenFlamingo (Alayrac et al. 2022) for vision-based ICL. All the prompts we design are pro-vided in Appendix C."}, {"title": "Results", "content": "Zero-Shot Harm Detection. In ZSL, we utilize LLMs to classify content as harmful/harmless without using labeled training examples. As shown in Figure 2, the LLMs significantly outperformed proprietary baselines in terms of accuracy. Specifically, the OpenAI Moderation API achieves 56% accuracy and Perspective API achieves 50% accuracy as shown in Table 1. Our ZSL approach, on the other hand, was able to achieve accuracies of 65% (Mistral-7B), 69% (Llama2-13B; GPT-40-Mini), and 70% (GPT-3.5-Turbo). We also compare performance across dif-ferent ZSL settings over various metrics, such as Precision, Recall, and F1 score as shown in Table 2\nFew-Shot Harm Detection. We employ few-shot ICL to leverage the full capacity of LLMs and aim to achieve performance comparable to the domain expert accuracy of 90.67%. The FS-ICL approaches demonstrate a significant improvement over the zero-shot configurations, with a no-table 5-10% increase in accuracy. Among the selectors, BSR consistently achieves the best performance.\nLLMs in the few-shot setting also significantly out-perform open-source FSL baselines such as Prototypical Networks and Matching Networks. As seen in Figure 3, GPT-40-Mini with 12 shots and the BERTScore selector"}, {"title": "Discussion", "content": "LLM Performance Across Other Harm Classification Datasets. To ensure that LLM performance at harm clas-sification is not localized to the YouTube videos dataset (Jo, Weso\u0142owska, and Wojcieszak 2024), we experiment on two additional harm-specific datasets: D-Lab Hate Speech (D-Lab 2022) and Jigsaw Toxicity (Google/Jigsaw 2019). These datasets do not consider diverse harm categories as the YouTube dataset (i.e. focus solely on hate speech and toxicity). As the GPT-40-Mini and GPT-3.5-Turbo models were the highest performers in prior experiments, we use these to undertake experiments on the additional datasets. These results are shown in Appendices D and E. We find that the LLMs outperform the other baselines on these bench-marks as well, highlighting their use as content moderators.\nExemplar Reordering and Balanced Selection. Since ICL and few-shot learning lead to greatly improved performance in harm classification, we undertake further ablations to analyze how exemplar order and balanced selection affect model performance. As exemplar order has been shown to influence performance (Lu et al. 2021), we first undertake two experiments for exemplar selection (using cosine sim-ilarity as the selector): (1) reordering exemplars based on their instance-level metric, which means the most similar demonstration lists closest to the content sample, and (2) listing exemplars in the prompt as selected by the coverage-based metric, without reordering. Figure 6 details the results. As seen, the GPT models benefit from reordering but this is"}, {"title": "Conclusion", "content": "We study online harm classification (or content modera-tion) problem where approaches need to be highly effective while requiring minimal supervision (i.e. annotated data) and should adapt to the ever-changing dynamic nature of"}, {"title": "Limitations", "content": "Despite the promising results, there may be several chal-lenges in using LLMs and VLMs for content moderation. One is the computational cost associated with processing multimodal data in approaches such as FS-ICL-DII. Addi-tionally, supplying multimodal inputs is generally slower compared to text-only inputs, with inference times for mul-timodal LLMs ranging between 10-30 seconds per test sam-ple. However, with the development of faster inference pipelines for multimodal input as well as improved multi-modal LLMs (and associated frameworks), we posit that this issue will be obviated in the future. We also restrict our anal-yses to social media content in English, but it is important to extend these efforts to other languages, especially low-resource ones with limited data available."}, {"title": "Ethics Statement", "content": "Our work on employing LLMs to detect harmful content in YouTube videos is a crucial step toward maintaining a healthy online environment. Through experiments on three diverse datasets and four LLMs, we demonstrate the effec-tiveness and scalability of few-shot learning for this task. The results from three different multimodal LLMs under-score the potential of multimodal models to enhance content moderation. We hope to inspire further research into more reliable and impactful content moderation strategies using LLMs to ensure safer and more positive social media plat-forms. We are also committed to ensuring the reproducibil-ity of our ideas and methods. Our code and implementation details are provided in Appendix I."}, {"title": "Appendix", "content": "A Datasets\nA.1 YouTube Harms Dataset\nWe leverage a multimodal dataset (Jo, Weso\u0142owska, and Wojcieszak 2024), comprising of YouTube videos from a variety of harm categories. The data is categorized online harm into six types: information harms, hate and harassment harms, clickbaitive harms, addictive harms, sexual harms, and physical harms. Videos were labeled by crowdworkers (from Amazon MTurk), fine-tuned LLMs and domain ex-perts trained in the social sciences, specifically with a focus on communication and digital media. Each video was clas-sified harmful or harmless, and assigned one or more harm categories if classified as harmful. The final dataset was formed by filtering for the videos where there was agree-ment between the crowdworker and domain expert, resulting in 19000 labeled videos.\nA.2 Measuring Hate Speech Corpus (D-Lab)\nThe Berkeley D-Lab Measuring Hate Speech Corpus (D-Lab 2022) is a labeled dataset curated to facilitate the study and measurement of hate speech in online platforms. The dataset is composed of 6, 954 social media posts, which have been manually annotated for the presence of hate speech. The corpus is derived from Twitter data collected from a period between January 2015 and June 2018. Each post has been labeled according to three key categories: (i) hate speech, (ii) offensive but not hate speech, and (iii) neither of-fensive nor hate speech. The labeling process was conducted by multiple annotators, ensuring inter-annotator agreement and consistent classification standards. The dataset aims to capture a wide range of hate speech expressions, including slurs, dehumanizing language, and threats, across various contexts. Each tweet was annotated by multiple annotators.\nA.3 Jigsaw Toxicity Dataset\nThe Jigsaw Toxicity Classification Dataset (Google/Jigsaw 2019) was developed as part of a Kaggle competition in 2017, with the goal of developing and evaluating machine learning models to detect toxic comments online. This dataset contains a large collection of Wikipedia comments, each labeled by human raters for various types of toxic be-havior (toxic, severe toxic, obscene, threat, insult, identity hate). In our experiments, we retrieve the comment text by the comment ID and convert the probability scores for the various toxicity types into a single binary label: Harmful or Harmless."}, {"title": "Selectors", "content": "B.1 Cosine\nCosine similarity is a similarity measure between two embedding vectors. We utilize the Sentence Transformers (SBERT) library (Reimers and Gurevych 2019) with the all-mpnet-base-v2 model from Hugging Face to generate"}, {"title": "BM25", "content": "BM25 (Best Matching 25) is a ranking function commonly used in information retrieval systems to rank documents based on their relevance to a given query. Developed as a part of the Okapi system, BM25 considered term fre-quency saturation and document length normalization, has been tested as an effective information retrieval method. We use the BM25Okapi method from the rank_bm25 library with the syntactic structure of size-4 n-grams. The BM25 score between each demonstration and the content sample is calculated as follows:\n$BM25(Q, D) = \\sum_{i=1}^{n} IDF(q_i).\\frac{f(q_i, D) (k_1+1)}{f(q_i,D)+k_1.(1-b+b.\\frac{D}{avgdl})}$\nHere, Q is the content sample and D is the demonstration. $f(q_i, D)$ is the frequency of the n-gram structural element $q_i$ of the content sample in the demonstration candidates. D is the length of the demonstration. $k_1$ and b are hyperparam-eters of the BM25 Okapi class that are set to 1.5 and 0.75 respectively. The avgdl parameter is the average demonstra-tion length, and IDF is the weight of the structural element $q_i$."}, {"title": "BERTScore", "content": "BERTScore measures the similarity between two sentences by computing the similarity of each token of the candidate sentence and each token of the reference sentence by co-sine similarity. Unlike cosine similarity which compares two sentences directly by the embedding vectors, BERTScore brings the comparison process to the token level. We use bert_score library with the deberta-large-mnli model to generate token embeddings. In our experiments, we use only recall as the BERTScore metric and do not apply IDF weighting. We cal-culate BERTScore-Recall (BSR) between a content sample and a demonstration by first computing the cosine similarity between the token embeddings of both and then maximizing the aggregated cosine similarity score. This can be repre-sented as:\n$BSR(Q, D) = \\sum_{i=1}^{|Q|} max_j cosine similarity(Q_i, D_j)$\nHere, Q is the length of the content sample, $Q_i$ is the em-bedding of the i-th token of the content sample, and $D_j$ is the embedding of the j-th token of the demonstration."}, {"title": "Prompts", "content": "C.1 ZSL Prompt\nIn the ZSL approach, each prompt consists of the task in-struction for harm detection and a single content sample to"}, {"title": "Additional Results for the Qwen-VL Multimodal LLM", "content": "To evaluate the effectiveness of using advanced caption-ing models for multimodal content moderation, we com-pared two state-of-the-art models: Qwen-VL and BLIP. As shown in Table below, the BertScore differences between the two models are minimal across multiple language mod-els. Specifically, Qwen-VL achieved slightly higher scores"}]}