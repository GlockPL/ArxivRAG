{"title": "ROBUST Q-LEARNING FOR FINITE AMBIGUITY SETS", "authors": ["C\u00c9CILE DECKER", "JULIAN SESTER"], "abstract": "In this paper we propose a novel Q-learning algorithm allowing to solve distributionally robust Markov decision problems for which the ambiguity set of probability measures can be chosen arbitrarily as long as it comprises only a finite amount of measures. Therefore, our approach goes beyond the well-studied cases involving ambiguity sets of balls around some reference measure with the distance to reference measure being measured with respect to the Wasserstein distance or the Kullback-Leibler divergence. Hence, our approach allows the applicant to create ambiguity sets better tailored to her needs and to solve the associated robust Markov decision problem via a Q-learning algorithm whose convergence is guaranteed by our main result. Moreover, we showcase in several numerical experiments the tractability of our approach.", "sections": [{"title": "1. INTRODUCTION", "content": "Markov decision problems are discrete time stochastic control problems involving an agent who can influence the environment by executing actions affecting the distribution of the realization of the subsequent state of the environment. In turn, depending on the state realization, the agent receives a feedback in form of a reward and decides on her next action. The objective of the agent is then to optimize the cumulative expected rewards obtained over an infinite time horizon by choosing the highest rewarding sequence of actions.\nBy construction, the described optimization problem depends crucially on the involved distributions for the state transitions. In practice, frequently in model-based approaches these distributions are estimated ([1], [32], [37]) from data or implicitly learned on-line in model-free approaches (see, e.g., [8], [39]). However chosen, the underlying model is highly prone to model misspecification, i.e., if the underlying distribution changes after training, the agent is trained to the wrong underlying distribution and successful execution of actions (i.e., receiving high rewards) typically will fail if the distribution change is too large. A solution to this problem is to take distributional uncertainty into account, i.e., to train the agent already by using an ambiguity set of probability distributions and train the agent according to the worst case measure contained in the ambiguity set, following the paradigm that during execution the distribution of the state transition might change but still is contained in a pre-defined ambiguity set. In this paper we follow this paradigm and propose a novel numerical algorithm to solve Markov decision problems while accounting for distributional uncertainty by considering arbitrary finite ambiguity sets."}, {"title": "1.1. Our Contribution.", "content": "One frequently used numerical method to solve (non-robust) Markov decision problems that has turned out to work efficiently in practice is the so-called Q-learning algorithm which initially was introduced by Watkins in his PhD thesis, see [46], as well as [47] for the accompanying paper. For a general introduction to Q-learning algorithms we refer to [11] and [19], and for more example of associated applications, see among many others [3], [4], [9], [10], [16], [21], and [26].\nOur contribution is to present a novel Q-learning algorithm that allows to take distributional uncertainty for the state transitions of Markov decision problems into account and which therefore"}, {"title": "1.2. Related Work.", "content": "While the study of distributionally robust optimization (DRO) problems and distributionally robust Markov decision problems and problems has been an active research topic already in the past decade (compare, e.g., [5], [6], [15], [23], [28], [29], [30], [34], [35], [41], [45], [48], [49], [50], and [51]), the discussion and construction of Q-learning algorithms to solve these sequential decision making problems numerically has only become an active research topic very recently.\nIn [6], [22], and [44] Q-learning algorithms for ambiguity sets that are constructed as balls around some reference measures with respect to the Kullback-Leibler divergence are discussed.\nMoreover, in [27], the authors present a novel Q-learning algorithm for the specific case that the ambiguity set is given by a Wasserstein-ball around a reference measure.\nTo the best of our knowledge going beyond the aforementioned cases imposes a novelty, and has not been studied yet."}, {"title": "1.3. Structure.", "content": "The remainder of this paper is as follows. In Section 2 we present the setting and the associated stochastic control problem. A numerical solution to this optimization problem via Q-learning is presented in Section 3 together with our main result guaranteeing convergence of the Q-learning algorithm. In Section 4 we provide numerical experiments showcasing the tractability of our Q-learning algorithm. The proofs are reported in Section 5."}, {"title": "2. SETTING AND SPECIFICATION OF THE PROBLEM", "content": "In this section we introduce the underlying framework which we use to establish a robust Q-learning algorithm for finite ambiguity sets."}, {"title": "2.1. Setting.", "content": "Optimal control problems, such as Markov decision problems, are formulated using a state space comprising all possible states accessible by an underlying stochastic process. We model this state space by a d-dimensional finite Euclidean subset \\(X \\subset \\mathbb{R}^d\\) and eventually aim at solving a robust control problem over an infinite time horizon. Hence, we define the state space over the entire time horizon via\n\\[\\Omega := X^{\\mathbb{N}} = X \\times X \\times ...\\]\nequipped with the corresponding \u03c3-algebra \\(\\mathcal{F} := 2^X \\otimes 2^X \\otimes ...\\). Next, let \\((X_t)_{t \\in \\mathbb{N}}\\) be the state process, i.e., the stochastic process on \u03a9 describing the states attained over time. We denote the finite set of possible actions by \\(A \\subset \\mathbb{R}^m\\), where \\(m \\in \\mathbb{N}\\) represents the dimension of the action space. The set of admissible policies is then given by\n\\[\\mathcal{A}:=\\{a = (a_t)_{t\\in \\mathbb{N}} \\, | \\, (a_t)_{t\\in \\mathbb{N}} : \\Omega \\rightarrow A; \\, a_t \\, \\text{is } \\sigma(X_t) \\text{ \u2013 measurable for all } t \\in \\mathbb{N}\\}\n=\\{(a_t(X_t))_{t\\in \\mathbb{N}} \\, | \\, a_t : X \\rightarrow A \\text{ Borel measurable for all } t \\in \\mathbb{N}\\}.\\]\nIn contrast to classical non-robust Markov decision problems we work under the paradigm that the precise state transition probability is not known but instead contained in ambiguity set of finitely"}, {"title": "2.2. Finite ambiguity sets and other ambiguity sets.", "content": "Note that the paradigm under which we work in this paper is fundamentally different from the approaches pursued, e.g., in [6], [22], and [27] where the corresponding ambiguity sets of probability measures are defined as balls around some reference measure.\nThe Q-learning approaches presented in [6], [22], and [27] provide solutions to account for a potential misspecification of a reference measure, by allowing deviations from it where the size of the deviations is measured by the respective distances (Kullback-Leibler distance and Wasserstein distance). However, these approaches allow not to control of which type the distributions in the ambiguity set are but simply consider all measures that are in a certain sense close to the reference measure. If an applicant is instead interested in controlling the distributions contained in the ambiguity set, for example, by allowing only for a specific type of parametric distributions with a finite set of possible parameters or by considering an asymmetric ambiguity set, the applicant can use the Q-learning approach presented in this paper. Another relevant situation is the consideration of multiple estimated models / probability measures obtained for exampled via different estimation methods. To account for all of these estimations, an applicant can simply construct an ambiguity set containing all estimated probability measures.\nBeyond these use cases, our approach also allows to combine different types of distributions, if desired.\nThe main difference therefore is that an applicant of the approach presented in this paper can control exactly what types of distributions are deemed possible whereas the approaches from [6], [22], and [27] account for a misspecification of one single reference measure."}, {"title": "2.3. Specification of the optimization problem.", "content": "The robust optimization problem consists, for every initial state \\(x \\in X\\), in maximizing the expected value of \\(\\sum_{t=0}^{\\infty} \\alpha^t r(X_t, a_t(X_t), X_{t+1})\\) under the worst case measure from \\(\\mathbb{P}_{x,a}\\) over all possible policies \\(a \\in \\mathcal{A}\\), where \\(\\alpha \\in \\mathbb{R}\\) is some discount factor accounting for time preferences of rewards. Therefore, the value function\n\\[X \\ni x \\mapsto V(x) := \\sup_{a \\in \\mathcal{A}} \\inf_{\\mathbb{P} \\in \\mathbb{P}_{x,a}} E_{\\mathbb{P}} \\bigg[ \\sum_{t=0}^{\\infty} \\alpha^t r(X_t, a_t(X_t), X_{t+1}) \\bigg]\\]\ndescribes the expected value of \\(\\sum_{t=0}^{\\infty} \\alpha^t r(X_t, a_t(X_t), X_{t+1})\\) under the worst case measure from \\(\\mathbb{P}_{x,a}\\) and when executing the optimal policy \\(a \\in \\mathcal{A}\\) after having started in initial state \\(x \\in X\\)."}, {"title": "2.4. Dynamic Programming.", "content": "We consider the one step optimization problem\n\\[T V(x) := \\max_{a \\in A} \\min_{\\mathbb{P} \\in \\mathcal{P}(x,a)} E_{\\mathbb{P}} [r(x, a, X_1) + \\alpha V(X_1)], \\quad x \\in X\\]\nwhere \\(X \\ni x \\rightarrow V(x)\\) is the value function defined in (2.3). Analogue to the definition of the non-robust Q-value function, see e.g. [46], we define the robust optimal Q-value function by\n\\[X \\times A \\ni (x, a) \\rightarrow Q^*(x, a) := \\inf_{\\mathbb{P} \\in \\mathcal{P}(x,a)} E_{\\mathbb{P}} [r(x, a, X_1) + \\alpha V(X_1)] .\\]\nWe can interpret \\(Q^*(x, a)\\) as the quality of executing action a when in state x (under the worst case measure) with the best possible action leading to V(x). By using the above definitions we indeed obtain the following dynamic programming equation, also famously known in the non-robust formulation as Bellman equation, compare e.g. [12, p.164] for a non-robust version."}, {"title": "3. ROBUST Q-LEARNING ALGORITHM", "content": "In this section we propose with Algorithm 1 a novel distributionally robust Q-learning algorithm and provide in Theorem 3.1 conditions for its convergence."}]}