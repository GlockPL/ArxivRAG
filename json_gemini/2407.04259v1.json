{"title": "ROBUST Q-LEARNING FOR FINITE AMBIGUITY SETS", "authors": ["C\u00c9CILE DECKER", "JULIAN SESTER"], "abstract": "In this paper we propose a novel Q-learning algorithm allowing to solve distributionally robust Markov decision problems for which the ambiguity set of probability measures can be chosen arbitrarily as long as it comprises only a finite amount of measures. Therefore, our approach goes beyond the well-studied cases involving ambiguity sets of balls around some reference measure with the distance to reference measure being measured with respect to the Wasserstein distance or the Kullback-Leibler divergence. Hence, our approach allows the applicant to create ambiguity sets better tailored to her needs and to solve the associated robust Markov decision problem via a Q-learning algorithm whose convergence is guaranteed by our main result. Moreover, we showcase in several numerical experiments the tractability of our approach.", "sections": [{"title": "1. INTRODUCTION", "content": "Markov decision problems are discrete time stochastic control problems involving an agent who can influence the environment by executing actions affecting the distribution of the realization of the subsequent state of the environment. In turn, depending on the state realization, the agent receives a feedback in form of a reward and decides on her next action. The objective of the agent is then to optimize the cumulative expected rewards obtained over an infinite time horizon by choosing the highest rewarding sequence of actions.\nBy construction, the described optimization problem depends crucially on the involved distributions for the state transitions. In practice, frequently in model-based approaches these distributions are estimated ([1], [32], [37]) from data or implicitly learned on-line in model-free approaches (see, e.g., [8], [39]). However chosen, the underlying model is highly prone to model misspecification, i.e., if the underlying distribution changes after training, the agent is trained to the wrong underlying distribution and successful execution of actions (i.e., receiving high rewards) typically will fail if the distribution change is too large. A solution to this problem is to take distributional uncertainty into account, i.e., to train the agent already by using an ambiguity set of probability distributions and train the agent according to the worst case measure contained in the ambiguity set, following the paradigm that during execution the distribution of the state transition might change but still is contained in a pre-defined ambiguity set. In this paper we follow this paradigm and propose a novel numerical algorithm to solve Markov decision problems while accounting for distributional uncertainty by considering arbitrary finite ambiguity sets."}, {"title": "1.1. Our Contribution.", "content": "One frequently used numerical method to solve (non-robust) Markov decision problems that has turned out to work efficiently in practice is the so-called Q-learning algorithm which initially was introduced by Watkins in his PhD thesis, see [46], as well as [47] for the accompanying paper. For a general introduction to Q-learning algorithms we refer to [11] and [19], and for more example of associated applications, see among many others [3], [4], [9], [10], [16], [21], and [26].\nOur contribution is to present a novel Q-learning algorithm that allows to take distributional uncertainty for the state transitions of Markov decision problems into account and which therefore respects the so called Knightian uncertainty ([20]), i.e., the uncertainty of having chosen the correct model.\nWe respect this model uncertainty by, instead of fixing a unique probability distribution for the state transitions, allowing the state transition between two subsequent states to be realized according to any distribution from a set of predetermined transition probabilities and then to consider the worst-case measure to compute rewards. Moreover, to account for the fact that transition probabilities may change over time, we consider a time-inhomogeneous formulation, i.e., we allow different transition probabilities in every instance of time as long as the transition probabilities are contained in the ambiguity set. To solve this robust formulation of a Markov decision process (as introduced in [28]) under the worst-case measure we develop a novel Q-learning algorithm.\nThe finite ambiguity set of transition probabilities can be designed by the applicant and its shape is, in particular, not restricted to measures that are in a certain sense close to a reference measure. This allows to consider different ambiguity sets as it were possible in the case of ambiguity sets that are defined as balls around reference measures. Put differently, using our approach, an applicant can simply include any possible scenarios in which she is interested and hence is able to control the shape and content of the ambiguity sets and hence the dynamics of the underlying Markov decision process that are deemed to be admissible. Moreover, in Section 5 we provide a proof guaranteeing convergence of our algorithm."}, {"title": "1.2. Related Work.", "content": "While the study of distributionally robust optimization (DRO) problems and distributionally robust Markov decision problems and problems has been an active research topic already in the past decade (compare, e.g., [5], [6], [15], [23], [28], [29], [30], [34], [35], [41], [45], [48], [49], [50], and [51]), the discussion and construction of Q-learning algorithms to solve these sequential decision making problems numerically has only become an active research topic very recently.\nIn [6], [22], and [44] Q-learning algorithms for ambiguity sets that are constructed as balls around some reference measures with respect to the Kullback-Leibler divergence are discussed.\nMoreover, in [27], the authors present a novel Q-learning algorithm for the specific case that the ambiguity set is given by a Wasserstein-ball around a reference measure.\nTo the best of our knowledge going beyond the aforementioned cases imposes a novelty, and has not been studied yet."}, {"title": "1.3. Structure.", "content": "The remainder of this paper is as follows. In Section 2 we present the setting and the associated stochastic control problem. A numerical solution to this optimization problem via Q-learning is presented in Section 3 together with our main result guaranteeing convergence of the Q-learning algorithm. In Section 4 we provide numerical experiments showcasing the tractability of our Q-learning algorithm. The proofs are reported in Section 5."}, {"title": "2. SETTING AND SPECIFICATION OF THE PROBLEM", "content": "In this section we introduce the underlying framework which we use to establish a robust Q-learning algorithm for finite ambiguity sets."}, {"title": "2.1. Setting.", "content": "Optimal control problems, such as Markov decision problems, are formulated using a state space comprising all possible states accessible by an underlying stochastic process. We model this state space by a d-dimensional finite Euclidean subset $X \\subset \\mathbb{R}^d$ and eventually aim at solving a robust control problem over an infinite time horizon. Hence, we define the state space over the entire time horizon via\n$\\Omega := X^{\\mathbb{N}} = X \\times X \\times X \\times ...$\nequipped with the corresponding $\\sigma$-algebra $\\mathcal{F} := 2^X \\otimes 2^X \\otimes ...$. Next, let $(X_t)_{t \\in \\mathbb{N}}$ be the state process, i.e., the stochastic process on $\\Omega$ describing the states attained over time. We denote the finite set of possible actions by $A \\subset \\mathbb{R}^m$, where $m \\in \\mathbb{N}$ represents the dimension of the action space. The set of admissible policies is then given by\n$\\mathcal{A}:=\\{a = (a_t)_{t\\in\\mathbb{N}} | (a_t)_{t\\in\\mathbb{N}} : \\Omega \\rightarrow A; \\ a_t \\text{ is } \\sigma(X_t) \\text{ \u2013 measurable for all } t \\in \\mathbb{N}\\} \\\\=\\{(a_t(X_t))_{t\\in\\mathbb{N}} | a_t : X \\rightarrow A \\ \\text{Borel measurable for all } t \\in \\mathbb{N}\\}.$\nIn contrast to classical non-robust Markov decision problems we work under the paradigm that the precise state transition probability is not known but instead contained in ambiguity set of finitely"}, {"title": "2.2. Finite ambiguity sets and other ambiguity sets.", "content": "Note that the paradigm under which we work in this paper is fundamentally different from the approaches pursued, e.g., in [6], [22], and [27] where the corresponding ambiguity sets of probability measures are defined as balls around some reference measure.\nThe Q-learning approaches presented in [6], [22], and [27] provide solutions to account for a potential misspecification of a reference measure, by allowing deviations from it where the size of the deviations is measured by the respective distances (Kullback-Leibler distance and Wasserstein distance). However, these approaches allow not to control of which type the distributions in the ambiguity set are but simply consider all measures that are in a certain sense close to the reference measure. If an applicant is instead interested in controlling the distributions contained in the ambiguity set, for example, by allowing only for a specific type of parametric distributions with a finite set of possible parameters or by considering an asymmetric ambiguity set, the applicant can use the Q-learning approach presented in this paper. Another relevant situation is the consideration of multiple estimated models / probability measures obtained for exampled via different estimation methods. To account for all of these estimations, an applicant can simply construct an ambiguity set containing all estimated probability measures.\nBeyond these use cases, our approach also allows to combine different types of distributions, if desired.\nThe main difference therefore is that an applicant of the approach presented in this paper can control exactly what types of distributions are deemed possible whereas the approaches from [6], [22], and [27] account for a misspecification of one single reference measure."}, {"title": "2.3. Specification of the optimization problem.", "content": "The robust optimization problem consists, for every initial state $x \\in X$, in maximizing the expected value of $\\sum_{t=0}^{\\infty} \\alpha^t r(X_t, a_t(X_t), X_{t+1})$ under the worst case measure from $\\mathbb{P}_{x,a}$ over all possible policies $a \\in \\mathcal{A}$, where $\\alpha \\in \\mathbb{R}$ is some discount factor accounting for time preferences of rewards. Therefore, the value function\n(2.3)\n$X \\ni x \\mapsto V(x) := \\sup_{a \\in \\mathcal{A}} \\inf_{\\mathbb{P} \\in \\mathbb{P}_{x,a}} \\mathbb{E}_{\\mathbb{P}} \\bigg[ \\sum_{t=0}^{\\infty} \\alpha^t r(X_t, a_t(X_t), X_{t+1}) \\bigg]$\ndescribes the expected value of $\\sum_{t=0}^{\\infty} \\alpha^t r(X_t, a_t(X_t), X_{t+1})$ under the worst case measure from $\\mathbb{P}_{x,a}$ and when executing the optimal policy $a \\in \\mathcal{A}$ after having started in initial state $x \\in X$."}, {"title": "2.4. Dynamic Programming.", "content": "We consider the one step optimization problem\n(2.4)\n$TV(x) := \\max_{a \\in A} \\min_{\\mathbb{P} \\in \\mathbb{P}(x,a)} \\mathbb{E}_{\\mathbb{P}} [r(x, a, X_1) + \\alpha V(X_1)], \\ \\ \\ x \\in X$\nwhere $X \\ni x \\rightarrow V(x)$ is the value function defined in (2.3). Analogue to the definition of the non-robust Q-value function, see e.g. [46], we define the robust optimal Q-value function by\n(2.5)\n$X \\times A \\ni (x, a) \\mapsto Q^*(x, a) := \\inf_{\\mathbb{P} \\in \\mathbb{P}(x,a)} \\mathbb{E}_{\\mathbb{P}} [r(x, a, X_1) + \\alpha V(X_1)]$.\nWe can interpret $Q^*(x, a)$ as the quality of executing action $a$ when in state $x$ (under the worst case measure) with the best possible action leading to $V(x)$. By using the above definitions we indeed obtain the following dynamic programming equation, also famously known in the non-robust formulation as Bellman equation, compare e.g. [12, p.164] for a non-robust version.\nProposition 2.1. Assume $0 < \\alpha < 1$. Then, for all $x \\in X$ we have\n$\\max_{a\\in A} Q^*(x, a) = TV(x) = V(x)$."}, {"title": "3. ROBUST Q-LEARNING ALGORITHM", "content": "In this section we propose with Algorithm 1 a novel distributionally robust Q-learning algorithm and provide in Theorem 3.1 conditions for its convergence."}, {"title": "4. EXAMPLES", "content": "In this section we provide two numerical examples comparing Algorithm 1 with other robust and non-robust Q-learning algorithms. To apply the numerical method from Algorithm 1, we use for all of the following examples a sequence of learning rates defined by $\\gamma_t = \\frac{1}{t+1}$ for $t \\in \\mathbb{N}$, a discount factor of $\\alpha = 0.95$, as well as an $\\epsilon$-greedy policy with $\\epsilon_{greedy} = 0.1$. Finally, we run\u00b2 Algorithm 1 with 1 000 000 iterations on a processor: 13th Gen Intel(R) Core(TM) i7-13700KF 3.40 GHz. Further details of the implementation can be found under https://github.com/CecileDecker/FiniteQLearning.git."}, {"title": "4.1. Coin Toss.", "content": "As in [28, Example 4.1] we consider an agent playing a coin toss game.\nAt each time step $t\\in \\mathbb{N}$ the agent observes the result of 10 coins where an outcome of heads corresponds to 1, and tails corresponds 0. The state $X_t$ at time $t \\in \\mathbb{N}$ is then given by the sum of the 10 coins value, i.e., we have $X := \\{0, ..., 10\\}.\nAt each time step $t$ the agent can make a bet whether the sum of the next throw strictly exceeds the previous sum (i.e. $X_{t+1} > X_t$), or whether it is strictly smaller (i.e. $X_{t+1} < X_t$). If the agent is correct, she gets 1$, however if the agent is wrong she has to pay 1$. The agent also has the possibility not to play. We model this by considering the following reward function:\n(4.1)\n$X \\times A \\times X \\ni (x, a, x') \\rightarrow r(x, a, x') := a \\cdot 1_{\\{x<x'\\}} - a \\cdot 1_{\\{x>x'\\}} - |a|\\cdot 1_{\\{x=x'\\}},$\nwhere the possible actions are given by $A := \\{-1,0,1\\}$ with $a = 1$ corresponding to betting $X_{t+1} > X_t$, $a = 0$ to not playing, and $a = -1$ to betting $X_{t+1} < X_t$. In a next step, we define two different ambiguity sets via\u00b3\n(4.2)\n$\\mathbb{P}^1(x, a) :={\\text{Bin}(10,0.5), \\text{Bin}(10,0.6)},$\n(4.3)\n$\\mathbb{P}^2(x, a) :={\\text{Bin}(10,0.5), \\text{Bin}(10,0.3)},$\nand we aim at comparing Algorithm 1 with the Algorithm from [28] which however does not allow to build the same assymmetric sets as in (4.2) and (4.3). To build comparative ambiguity sets with Wasserstein uncertainty we define ambiguity sets such that all probability measures from $\\mathbb{P}^1$ and $\\mathbb{P}^2$ are contained, respectively. This leads to\u2074\n(4.4)\n$\\mathbb{P}^3(x, a) := {\\mathbb{P} \\in \\mathcal{M}_1(X) | W_1(\\mathbb{P}, \\text{Bin}(10,0.5)) \\leq 1\\}$\n(4.5)\n$\\mathbb{P}^4(x, a) := {\\mathbb{P} \\in \\mathcal{M}_1(X) | W_1(\\mathbb{P}, \\text{Bin}(10, 0.5)) \\leq 2\\}$\nwhich are Wasserstein-balls around the reference probability $\\text{Bin}(10,0.5)$ with radii 1 and 2, respectively. As one can see, e.g., from [27, Equation (4.2)], $\\mathbb{P}^3$ is the smallest Wasserstein-ball around the reference measure that contains $\\text{Bin}(10,0.6)$, whereas $\\mathbb{P}^4$ is the smallest Wasserstein-ball containing $\\text{Bin}(10, 0.3)$."}, {"title": "4.2. Stock investing.", "content": "This example is a variant of [28, Example 4.3] showcasing the use of tailored ambiguity sets which are not possible to construct and use in the same manner with Wasserstein or Entropic ambiguity sets as they are considered in [6], [22], and [44].\nWe consider the problem of optimally investing in the stock market, and, to this end, encode the return of a stock by 2 numeric values: either the return is positive (1) or negative (-1). The space of numerically encoded returns is therefore given by: $T := \\{-1,1\\}$. At each time step t the agent can choose to buy the stock or not, or to short sell the stock. The action $a_t$ represent this investment decision, buying the stock is encoded by 1 and not buying it by 0, whereas short-selling corresponds to $-1$, i.e., we have $A := \\{-1,0,1\\}.\nThe agent's investment decision should depend not only on the most recent return but naturally will depend on the current market situation. Therefore the agent relies her investment decision on the last $h = 5$ returns. Hence, we consider the state space\n$X := T^h = \\{-1,1\\}^h.$\nThe reward is given by the function:\n(4.6)\n$X \\times A \\times X \\ni (x, a, x') \\rightarrow r(x, a, x') := a \\cdot x',$\ni.e., we reward a correct investment decision and penalize an incorrect investment decision.\nNow, to train the agent, we construct an estimated reference measure in the following way. For the state transition, we consider the historic evolution of the (numerically encoded) returns of the underlying stock. This time series is denoted by $(R_j)_{j=1,...,N} \\subset T^{\\mathbb{N}}$ for some $N \\in \\mathbb{N}$. We then define for some small $\\gamma > 0$ a map\u2076 $X \\times A \\ni (x, a) \\rightarrow \\mathbb{P}(x, a) := \\sum_{i \\in T} p_i(x) \\cdot \\delta_{\\{i\\}} \\in \\mathcal{M}_1(T)$ where for $x \\in X$, $i \\in T$ we define:\n(4.7)\n$p_i(x) := \\frac{\\tilde{p}_i(x) + 1}{\\gamma + \\sum_{i \\in T} \\tilde{p}_j(x)} \\in [0, 1],$\nwith\n(4.8)\n$\\tilde{p}_i(x) := \\sum_{j=1}^{N-h+1} 1_{\\{(\\pi(x),i)=(R_j,...,R_{j+h-1})\\}},$\nand where $R^{h-1}(x_1,...,x_h) \\mapsto \\pi(x_1,...,x_h) := (x_2,...,x_h) \\in R^{h-1}$ denotes the projection onto the last $h-1$ components. This means the data-driven construction of the measure $\\mathbb{P}(x, a)$ relies on the relative frequency of the sequence $(\\pi(x), i)$ in the time series of past realized returns $(R_j)_{j=1,...,N}$"}, {"title": "5. PROOFS AND AUXILIARY RESULTS", "content": ""}, {"title": "5.1. Auxiliary Results.", "content": "For any function $f : X \\times A \\rightarrow \\mathbb{R}$, we write\n(5.1)\n$|| f || := \\sup_{x \\in X} \\sup_{a \\in A} | f (x, a)|.$\nNext, we introduce the operator $\\mathcal{H}$ operating on a function $v : X \\times A \\rightarrow \\mathbb{R}$ being defined by\n(5.2)\n$X \\times A \\ni (x, a) \\rightarrow \\mathcal{H}v(x, a) := \\inf_{\\mathbb{P} \\in \\mathbb{P}(x,a)} \\mathbb{E}_{\\mathbb{P}} [r(x, a, X_1) + \\alpha \\max_{b \\in A} v(X_1, b)]$"}]}