{"title": "Doe-1: Closed-Loop Autonomous Driving with Large World Model", "authors": ["Wenzhao Zheng", "Zetian Xia", "Yuanhui Huang", "Sicheng Zuo", "Jie Zhou", "Jiwen Lu"], "abstract": "End-to-end autonomous driving has received increasing attention due to its potential to learn from large amounts of data. However, most existing methods are still open-loop and suffer from weak scalability, lack of high-order interactions, and inefficient decision-making. In this paper, we explore a closed-loop framework for autonomous driving and propose a large Driving wOrld modEl (Doe-1) for unified perception, prediction, and planning. We formulate autonomous driving as a next-token generation problem and use multi-modal tokens to accomplish differ-", "sections": [{"title": "1. Introduction", "content": "The emergence of GPT series [8, 9, 60] stimulates the rapid development of large models with various functions, including language modeling [2, 73, 74], visual undertanding [3, 46, 47, 57], and decision-making [7, 37, 53]. The key to the success of large models is scaling up model sizes and training data [34]. When designing a model, the scalability advocates large representation compacity (e.g., transformers [17, 49, 75]) over well-designed inductive biases (e.g., convolution neural networks [23, 29]) to improve the upper bound of performance.\nTo build large models for autonomous driving, some methods directly apply large language models (LLMs) [13, 53, 65, 67] or vision-language models (VLMs) [62, 66, 71, 80, 81, 89, 98] for motion planning [71] or scene question-answering [55, 79]. They usually align the inputs with texts (e.g., Q-Former [39]) and output language descriptions of the planning results [53]. However, LLMs are known to share the hallucination issue [21, 42, 48], hindering the interpretability and safety of autonomous driving. To avoid this, others follow the well-tested pipeline of perception, prediction, and planning for autonomous driving and explore a scalable end-to-end model [27, 28, 33, 72, 93, 100] to jointly accomplish them. Though promising, most existing methods are still open-loop and suffer from several issues. 1) Weak scalability. They use manually designed scene representation which cannot provide comprehensive information for downstream tasks. 2) Lack of high-order interactions. They predict future scenes without considering the planned ego trajectory. 3) Inefficient decision-making. They usually plan several steps ahead yet practically only use the first step to execute.\nTo address thesex, we propose a closed-loop large Driving world modEl (Doe-1) for unified perception, prediction, and planning without intermediate latent scene representations, as shown in Figure 2. We cast autonomous driving as a scene evolution problem and represent each scene with observation, description, and action tokens. We then formulate the conventional per-"}, {"title": "2. Related Work", "content": "Large Models for Autonomous Driving. The success of GPT series [8, 9, 60] confirms the power of model and data scaling and promotes the rapid development of large language models (LLMs) [2, 73, 74], which demonstrate impressive performance on a wide range of language tasks. Early methods combining LLMs with autonomous driving use ground-truth labels [13, 65, 67] or pre-trained perception models [53] to obtain detection and map results. They then translate them into text descriptions as inputs and leverage the reasoning ability of LLMs to output the planned trajectory in text. To enable LLMs to process images, large vision-language models (VLMs) [3, 46, 47, 57] usually pre-align the image representations with text space and then jointly finetune the overall model. This facilitates the emergence of end-to-end planning methods which take as inputs images and directly output the planned trajectory. Further methods explore the use of chain-of-thoughts [55, 80], instruction tuning [97], or additional visual question answering [79] to improve the interpretability. Still, existing methods struggle with the hallucination issue [21, 42, 48] commonly possessed by LLMs and VLMs and thus lack robustness and safety, which are critical for autonomous driving. To alleviate this issue, we follow the well-tested perception, prediction, and planning pipeline and propose a large driving world model (Doe-1) to unify all these tasks for more robust autonomous driving.\nWorld Models for Autonomous Driving. The conventional world model aims to predict the next observation of"}, {"title": "3. Proposed Approach", "content": "3.1. Closed-Loop Autonomous Driving\nAutonomous driving is a long-standing task to apply artificial intelligence to the real world, which aims to plan the future actions {a} for the ego vehicle from scene observations {0}. Several attempts [6, 12, 15, 16, 59] explore the use of deep neural networks to directly model the mapping from observations {0} to actions {a}:\n{ot-H,\u2026\u2026,ot} \u2192 {at,\u2026\u2026\u2026,at+F-1}, (1)\nwhere t is the current time stamp, and H and F is the number of concerned history and future frames, respectively. The black-box nature of deep neural networks makes the decision-making process less transparent and trustworthy.\nRecent methods utilize the reasoning ability of large language models (LLMs) or vision-language models (VLMs) to improve the interpretability of the planning results [13, 55, 65, 67, 97]. They align the visual features with the text space by direct transcription [65] or learnable projection [66]. Some methods further use additional text descriptions d as an auxiliary task [79] to further alleviate the hallucination issue [48] of large models:\n{ot-H,...,ot} LLM/VLM, dt, {at,..., at+F-1}. (2)\nStill, it is doubtful whether LLMs/VLMs truly understand the 4D dynamics and interactions among traffic agents without fine-grained understanding and prediction of the scene."}, {"title": "3.2. Doe-1: A Large Driving World Model", "content": "Overview. The overall framework of Doe-1 is depicted in Figure 5. We use Chameleon architecture [70] as a uni-"}, {"title": "3.3. Applications of Doe-1", "content": "As a unified multimodal world model, Doe-1 supports inputs from multiple modalities and automatically predicts the next modality, facilitating the applications to different tasks simply by altering the prompt. Figure 6 introduces the application of Doe-1 to different tasks of visual question-answering, motion planning, and action-conditioned video generations as examples. By designing prompt sequences, Doe-1 can be transferred to other multimodal tasks.\nVisual Question-Answering. Given the observation o, the model is required to generate a precise description do of the observation. Furthermore, to assess the model's understanding of the scene and the perception ability, we require the model to complete interactive question-answering tasks based on the given observations and the generated descriptions. These tasks include identifying objects in the scene that may impact driving, describing the environment, and checking the plausibility of a given driving trajectory, among others, which requires the model to have as comprehensive a description capability of the scene as possible.\nAction-Conditioned Video Generation. We hope that the world model, Doe-1, can enable closed-loop interactive simulation of driving scenarios without the need for feedback from real-world interactions. This requires Doe-1 to be able to predict the changes in observations after taking an action. Given a sequence of observations with descriptions {oj, dj}=0 and the actions history {aj}=0, the model is required to generate the observations ot+1.\nIn the action-conditioned video generation task, for the observations, we only provide the model with the image at time step 0, and the model is required to iteratively generate the next p frames of images based on the given actions.\nMotion Planning. Given a sequence of observations {0;}=0 and history motions {z}=0, the model gener-"}, {"title": "4. Experiments", "content": "In this section, we evaluate the performance of our model on a variety of driving-related tasks. Despite only using a single-view camera as input and high-level QAs as supervision, the model still exhibits promising performance.\n4.1. Dataset\nWe conducted experiments on the widely use nuScenes dataset [10] for autonomous driving. It includes 1,000 driving sequences captured in diverse scenarios, including daytime/nighttime and sunny/cloudy/rainy weather. Each video clip consists of 400 frames with a frequency of 20Hz, spanning across 20 seconds as a scene. They are downsampled to 2Hz to obtain keyframes and annotated with labels including scene descriptions, 3D object bounding boxes, and semantic maps. Though it provides point clouds scanned by a 32-beam LiDAR, we mainly focus on using the RGB images captured by six surrounding cameras as inputs. We follow existing methods [100] to use the training 700 scenes for training and the validation 150 scenes for testing.\n4.2. Task Descriptions\nWe evaluate our Doe-1 on various tasks including visual question-answering, motion planning, and action-conditioned video generation.\nVisual Question-Answering. We evaluate the perception ability of our method on the OmniDrive-nuScenes [79] benchmark, which supplements the original nuScenes data with high-quality visual question-answering (QA) text pairs generated by GPT4. The QA pairs include perception, reasoning, and planning in the 3D space. The goal of visual"}, {"title": "4.3. Implementation Details", "content": "We use the pre-trained Lumina-mGPT 7B [45] to initialize our model and fine-tune the model for 5 epochs on the BDD100k [94] dataset to improve the conditioned image generation capability for driving scene images. We take as input images with a resolution of 672 \u00d7 384 and only adopt the images from the camera. The resolution of the action tokenizer is set to 0.02m, where the scaling factor is set to 10000 for displacements and 1000 for yaw angles.\nFor training, we use the AdamW [50] optimizer with a cosine learning rate scheduler. The initial learning rate is set to 1 \u00d7 10-5 with a weight decay of 0.1. To emphasize the accuracy of action prediction, we increase the loss weight for action tokens in the input sequence by a factor of 5. Z-loss is applied with a weight of 1 \u00d7 10-5 to stabilize the training. We train our model for 16 epochs on 8 A100 GPUs with a total batch size of 24."}, {"title": "4.4. Close-Loop Autonomous Driving", "content": "Figure 1 shows the visualizations of Doe-1 for closed-loop autonomous driving. We model perception, planning, and prediction as the transitions of observation\u2192description, description action, and action\u2192observation, respectively. We observe that the proposed method can correctly generate scene descriptions, answer questions about the scene, plan ego trajectory, and predict future observations correctly with one trained model without finetuning."}, {"title": "4.5. Visual Question-Answering", "content": "We use visual question-answering to evaluate the perception ability of our Doe-1. We compare our method with OmniDrive [79] with 3D Q-Former (OmniDrive-3D), 2D Q-Former (OmniDrive-2D), and dense BEV (OmniDrive-BEV) on the OmniDrive-nuScenes [79] benchmark, as shown in Table 1. We use bold numbers to denote the best results. Note that OmniDrive uses surrounding cameras as inputs, while our Doe-1 only uses the front camera. Still, we see that our model achieves competitive results on both visual caption and counterfactual reasoning tasks.\nVisualizations. We provide a qualitative analysis of the visual question-answering results in Figure 7. We see that our Doe-1 correctly describes the scene and answers the questions about the input image."}, {"title": "4.6. Action-Conditioned Video Generation", "content": "We evaluate the prediction ability of Doe-1 on the action-conditioned video generation, where we adopt accurate actions (displacements in the BEV space) as the condition. We compare our model with existing real-world world models in Table 2. We see that Doe-1 achieves comparable performance with Drive-WM [83] and GenAD [90], yet underperforms Vista [20]. Still, our model is the first to use the autoregressive architecture instead of diffusion, facilitating the joint perception and planning of Doe-1.\nVisualizations. Figure 8 shows the generated sequences of images given an image and trajectory as conditions. We see that Doe-1 generates high-quality images following the prompt actions. They show consistency in the 3D structure and demonstrate the ability of Doe-1 to understand the evolutions of the 3D world."}, {"title": "4.7. End-to-End Motion Planning", "content": "We evaluate the action planning performance of our Doe-1 following existing end-to-end autonomous driving methods [28, 33, 100], as shown in Table 3. Additionally, we also compute the average performance across all previous frames for each time step at the bottom of the table following VAD [33]. Though our Doe-1 does not achieve the best results, it demonstrates competitive performance with existing methods using only question-answering pairs as the auxiliary supervision. Note that using more supervision signals generally results in better performance, with the cost of expensive annotations. Also, our model only takes the front camera as input, while the other vision-based methods use surrounding cameras. Still, our model plans the future trajectory with a satisfactory collision rate. In particular, Doe-1 delivers a small collision rate within 1 second, which is the most important factor in the practical close-loop scenario."}, {"title": "4.8. Analysis", "content": "Effect of Different Planning Strategies. Doe-1 leverages the perceived descriptions before generating the current action and masks the previous frames of the generated actions to avoid accumulation of error. Table 4 demonstrates the effectiveness of our design, which shows that the planning performance is influenced by the constraints of the textual modality. The mask mechanism also effectively prevents significant error accumulation."}, {"title": "5. Conclusion", "content": "In this paper, we have presented a large driving world model (Doe-1) for closed-loop autonomous driving. While existing end-to-end autonomous driving methods demonstrate strong planning performance, they are still open-loop and suffer from information loss with hand-crafted scene representations. We address this with a next-token prediction formulation and model perception, prediction, and planning with the transitions between multi-modal tokens. We have conducted extensive experiments on the widely used nuScenes dataset and demonstrated the effectiveness of Doe-1 on visual question-answering, action-conditioned video generation, and end-to-end motion planning.\nLimitations. Doe-1 only takes the front-view images as inputs due to the inefficiency of using multi-view inputs. However, surround-view information is critical for safe autonomous driving and is an interesting future direction."}]}