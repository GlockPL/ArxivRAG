{"title": "THERMALGAUSSIAN: THERMAL 3D GAUSSIAN SPLATTING", "authors": ["Rongfeng Lu", "Hangyu Chen", "Zunjie Zhu", "Yuhang Qin", "Ming Lu", "Le Zhang", "Chenggang Yan", "Anke Xue"], "abstract": "Thermography is especially valuable for the military and other users of surveillance cameras. Some recent methods based on Neural Radiance Fields (NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS) prevails due to its rapid training and real-time rendering. In this work, we propose ThermalGaussian, the first thermal 3DGS approach capable of rendering high-quality images in RGB and thermal modalities. We first calibrate the RGB camera and the thermal camera to ensure that both modalities are accurately aligned. Subsequently, we use the registered images to learn the multimodal 3D Gaussians. To prevent the overfitting of any single modality, we introduce several multimodal regularization constraints. We also develop smoothing constraints tailored to the physical characteristics of the thermal modality. Besides, we contribute a real-world dataset named RGBT-Scenes, captured by a hand-hold thermal-infrared camera, facilitating future research on thermal scene reconstruction. We conduct comprehensive experiments to show that ThermalGaussian achieves photorealistic rendering of thermal images and improves the rendering quality of RGB images. With the proposed multimodal regularization constraints, we also reduced the model's storage cost by 90%. The code and dataset will be released.", "sections": [{"title": "INTRODUCTION", "content": "Thermal imaging is widely used in fields such as military (He et al., 2021), healthcare (Lahiri et al., 2012), industry (Glowacz, 2021), agriculture (Zhou et al., 2021), building inspection (El Masri & Rakha, 2020), and search and rescue (Yeom, 2024) because it converts temperature information\u2014an important physical modality not visible to the human eye\u2014into interpretable images. 3D reconstruction technology, which involves lifting multi-view 2D images into 3D scenes, is foundational for key technologies such as the metaverse, digital twins, autonomous driving, and robotics. Any image with valuable 2D applications can be lifted into 3D to view the captured scene from a new view and in greater detail. Thermal images are no exception (Abreu de Souza et al., 2023; Liu et al., 2024)."}, {"title": "RELATED WORK", "content": ""}, {"title": "THERMAL IMAGING AND 3D RECONSTRUCTION", "content": "All objects with temperatures above absolute zero emit energy in the form of electromagnetic waves, a phenomenon known as thermal radiation. Through Planck's law, thermal imaging converts the wavelength and intensity of electromagnetic waves radiated from an object's surface into thermal information, which is then used to create images. Originally, thermal imaging was developed for military purposes to enable visualization under extreme lighting conditions, such as nighttime or smoke. As costs have decreased in recent years, it has also been widely applied in fields of healthcare, industry, agriculture, building inspection, and so on.\n3D reconstruction has significant applications in visualizing, surveying, and analyzing large, inaccessible, and important objects, in ways that 2D images cannot provide. The advent of the groundbreaking KinectFusion (Newcombe et al., 2011) marked the beginning of an era of high-precision, dense 3D reconstruction. Subsequent developments (Nie\u00dfner et al., 2013; K\u00e4hler et al., 2015; Dai et al., 2017; Gong et al., 2021; Zhang et al., 2021) have optimized 3D reconstruction in terms of accuracy, efficiency, and unconstrained camera movement. Some works (Rangel et al., 2014; Zhao et al., 2017; M\u00fcller, 2019; Li et al., 2023) have been made to integrate thermal imaging with the aforementioned 3D reconstruction methods, leading to the development of thermal 3D reconstruction. However, these traditional multi-view geometry-based methods do not perform as well in rendering new views as the more recent deep learning-based approaches.\nNeRF (Mildenhall et al., 2021) has emerged as a significant milestone in the field of 3D reconstruction due to its impressive ability to render highly realistic images from a new view. Recently, ThermoNeRF (Hassan et al., 2024) and Thermal-NeRF (Ye et al., 2024) have been proposed to reconstruct thermal scenes by combining a set of thermal images with NeRF. Although these approaches successfully generate images from new perspectives, they are limited by the slow rendering speed and implicit scene representation of NeRF, which hampers their practical application."}, {"title": "3DGS AND MULTIMODALITY", "content": "3DGS (Kerbl et al., 2023) represent a revolutionary technology in the fields of 3D reconstruction. Distinct from methods like NeRF, 3DGS employs millions of explicit Gaussians, fundamentally altering its approach. This technology merges the advantages of neural network-based optimization with structured data representation, enabling photorealistic rendering from new views, significantly enhancing real-time rendering capabilities, and introducing the ability to manipulate and edit 3D scenes. These features make 3DGS highly compatible with a broad range of downstream applications, establishing it as the baseline for next-generation 3D reconstruction technologies (Chen & Wang, 2024). Although 3DGS is constrained and trained using only RGB modality images, it ultimately generates millions of Gaussians, resembling a point cloud. This characteristic makes 3DGS particularly suitable for multimodal fusion with other devices that directly capture scene point clouds, such as depth cameras and LiDAR. Studies (Matsuki et al., 2024; Yan et al., 2024; Keetha et al., 2024) have effectively integrated depth cameras to implement 3DGS-based simultaneous localization and mapping. Studies (Li et al., 2024; Chung et al., 2024) have effectively combined depth images (Yan et al., 2020) estimated from a pre-trained Monocular Depth estimation model (Bhat et al., 2023) with the RGB modality, resulting in improved rendering quality and more accurate geometric structures. 3D scenes contain not only RGB and geometric modalities but also other"}, {"title": "METHOD", "content": "Fig. 3 shows the overview of the proposed ThermalGaussian, which is based on the 3DGS (Kerbl et al., 2023), aiming to extend its capability to simultaneously render images of color and temperature. In this section, we first briefly introduce the background of the 3DGS. Then, we provide a detailed description of our method's specific implementation details, including multimodal initialization, three types of multimodal thermal Gaussians, thermal loss, and multimodal regularization."}, {"title": "PRELIMINARY: 3D GAUSSIAN SPLATTING", "content": "3DGS (Kerbl et al., 2023) represents a 3D reconstruction scene using a large number of anisotropic 3D Gaussians. This representation not only provides differentiability, which offers advantages in learning-based methods, but also enables explicit spatial expression, enhancing the editability and controllability of 3D scenes. Furthermore, it allows for rapid and efficient rasterization rendering through splatting. Initially, a set of unordered images of objects to be reconstructed is processed using SfM to obtain the camera poses and sparse point clouds. 3DGS then initializes these sparse point clouds as the position \u03bc of a 3D Gaussian:\n$G(x) = e^{-(x-\\mu)^T \\Sigma^{-1}(x-\\mu)}$\nwhere \u2211 represents the covariance matrix of the 3D Gaussian, and x denotes any point in the 3D scene. \u2211 is defined using a scaling matrix S and a rotation matrix R:\n$\\Sigma = RSST RT$\nThe 3D Gaussian G(x) is projected onto the imaging plane using the camera's intrinsic parameters, transforming it into a 2D Gaussian. Subsequently, the image is rendered through alpha-blending:\n$C (x') = \\sum_{k \\in N}c_k\\alpha_k \\prod_{j=1}^{k-1} (1 \u2013 \\alpha_j)$", "1": true}, {"title": "MULTIMODAL INITIALIZATION", "content": "Previously, methods for calibration RGB and thermal images (Zhang et al., 2023) often involve designing specialized, non-standard metallic calibration boards with uniformly sized circular holes, as shown in Fig. 4a. The calibration relies on the temperature difference between the board and the background to compute thermal features, enabling calibration. However, the high complexity and stringent requirements for producing these calibration boards make them difficult to obtain and lack a universal standard. We find that a standard chessboard pattern, as shown in Fig 4b, commonly used for RGB camera calibration, can effectively be used for calibrating both thermal and color cameras, with a mean reprojection error of less than 0.5 pixels. Initially, we heat the calibration board using devices like an infrared heater; black regions, absorbing heat faster due to their material properties, exhibit relatively higher temperatures. We capture simultaneous color and thermal images before a thermal equilibrium with surroundings is achieved. Subsequently, conventional camera calibration (Zhang, 1999) is performed.\nUsing the calibrated intrinsic parameters KRGB for the color camera, KTh for the thermal camera, and the rotation R and translation t from the temperature camera to the color camera, we computed the corresponding positions (UTh, Th) on the thermal image mapped to the registered positions on the color image:\n$\\begin{bmatrix} U^{RGB} \\\\ V^{RGB} \\\\ 1 \\end{bmatrix} = K_{RGB} (R^{-1}(K_{Th}^{-1} \\begin{bmatrix} U^{Th} \\\\ V^{Th} \\\\ 1 \\end{bmatrix} + t))$\nAs shown in Fig.2(b), directly using thermal images, which exhibit low texture and ghosting characteristics, makes it difficult to successfully run SfM (Schonberger & Frahm, 2016). Therefore, to obtain the thermal camera poses, we tested three different multimodal SfM strategies. The first utilizes registered high-texture RGB images directly for camera pose estimation. These poses serve simultaneously for both the RGB and thermal cameras. However, practical scenarios that require thermal scene reconstruction often occur under dim lighting conditions or in scenes lacking distinct color features. Therefore, relying solely on color images may impede the precise camera pose estimation necessary for thermal scene reconstruction. The second approach, illustrated in Fig.2(c), involves blending registered color and thermal images using the following formula:\n$I_{mix} = \\beta I_{Th} + (1 \u2013 \\beta)I_{RGB}$", "4": true, "5": true}, {"title": "THERMAL GAUSSIAN", "content": "We utilize three different multimodal training strategies to construct the thermal Gaussian.\nMultimodal Fine-Tuning Gaussians (MFTG): Inspired by the fine-tuning approach used in large-scale models, our first multimodal training strategy is training a basic Gaussian with RGB images and then refining this Gaussian with thermal images to generate thermal Gaussian. This is a two-stage process. In the first stage, similar to 3DGS, we utilize multimodal camera poses and initial"}, {"title": "THERMAL LOSS AND MULTIMODAL REGULARIZATION", "content": "The loss function for RGB modality images is directly given by Equation 6. The same loss function can also be applied to thermal modality images. However, because thermal images exhibit unique low-texture and ghosting characteristics, we design a specific thermal loss function to better accommodate these features.\nThe RGB modality may exhibit abrupt changes. However, because all objects above absolute zero continuously engage in heat transfer and thermal radiation, eventually reaching thermal equilibrium with their surroundings, significant abrupt changes are typically not observed in thermal images. Additionally, most regions of objects in thermal equilibrium have similar temperatures, resulting in smoother thermal images. Therefore, we introduce a smoothness term for regularization:\n$L_{smooth} = \\frac{1}{4M}\\sum_{i,j} (|T_{i\\pm1,j} \u2013 T_{i,j}| + |T_{i,j\\pm1} \u2013 T_{i,j}|)$\nwhere $T_{ij}$ represents rendered thermal values at pixel position (i, j). M denotes the number of rendering pixels. Similarly to the color modality, we also incorporate L\u2081 and LD-SSIM Thus, our final temperature loss is:\n$L_{thermal} = (1 \u2013 \\lambda_1)L_1 + \\lambda_2L_{ssim} + \\lambda_3L_{smooth}$", "9": true, "10": true}, {"title": "SELF-COLLECTED THERAML DATASET", "content": "We introduce a new dataset, named RGBT-Scenes, which consists of aligned collections of thermal and RGB images captured from various viewpoints of a scene. The images are collected using the commercial-grade handheld thermal-infrared camera FLIR E6 PRO (Teledyne FLIR, 2024), which can simultaneously capture RGB and thermal images. The basic specifications of this camera include a resolution of 240\u00d7180, a field of view of 33\u00b0\u00d725\u00b0, a temperature range from -20\u00b0C to 550\u00b0C, and a temperature accuracy of \u00b12% of the reading. Our dataset includes over 1,000 RGB and thermal images from 10 different scenes. These scenes encompass both indoor and outdoor environments, various object sizes (from large structures to everyday items), different temperature variations (ranging from a 300\u00b0C difference to a 4\u00b0C difference), and include both 360-degree and forward-facing scenarios. We provide the raw images captured by the thermal camera, as well as the RGB images, thermal images, MSX images, and camera pose data. In Table 1, we compare our dataset with those from concurrent works, Thermal-NeRF (Ye et al., 2024) and ThermoNeRF (Hassan et al., 2024). Our dataset includes both RGB and thermal images and applies multimodal calibration methods to align these images. The images used for calibration will also be made available. Compared to ThermoNeRF, our dataset features improved multiview consistency of thermal information and encompasses a richer variety of scenes. Detailed descriptions of each scene are provided in the supplementary materials."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "IMPLEMENTATION DETAILS", "content": "Our method is an improvement upon the 3DGS framework, with all experimental settings consistent with the reference 3DGS. The specific hyperparameter Asmooth is set to 0.6. Each comparative experiment was trained for 30K iterations. All experiments are conducted on a single NVIDIA 3090 GPU. The resolution of the rendered RGB images and thermal images is 640\u00d7480."}, {"title": "THERMAL VIEW SYNTHESIS", "content": "Similar to 3DGS, we employ image quality assessment metrics including Peak Signal-to-Noise Ratio (PSNR) (Hore & Ziou, 2010), Structural Similarity Metric (SSIM) (Wang et al., 2004), and Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018) to evaluate the quality of reconstructed thermal and RGB images from new views.\nAs shown in Table 2, even in scenes with pronounced thermal variations, specifically targeting low-texture thermal characteristics, direct application of thermal data proves challenging for 3DGS. In very few successful cases, inadequate precision in thermal camera positioning has compromised the quality of thermal reconstructions. 3DGS+MI denotes training the original 3DGS using thermal images instead of RGB images after obtaining accurate thermal poses through our multimodal initialization. Compared to 3DGS, 3DGS+MI adapts to a wider range of scenarios and achieves higher reconstruction quality. Given the higher reconstruction quality of 3DGS (Kerbl et al., 2023) com-"}, {"title": "RGB VIEW SYNTHESIS", "content": "Our method not only achieves high-quality thermal image rendering but also significantly enhances RGB image rendering quality. As shown quantitatively in Table 4, our multimodal constraints improve RGB rendering quality in nearly all scenarios, with an average PSNR improvement of 1.1 dB compared to the original 3DGS (Kerbl et al., 2023). This improvement is particularly evident in scenarios where the RGB modality struggles to identify the environment, while the thermal modality can recognize it clearly. As shown in the top part of Fig. 6, where distinguishing between foreground and background is challenging in the RGB modality but straightforward in the thermal modality due to temperature differences, constraints from the thermal modality aid in the accurate learning of the RGB modality. Additionally, as depicted in the bottom part of Fig. 6, the assistance from thermal images enables accurate color rendering in low-light scenes for the RGB modality. Our results demonstrate that, under multimodal constraints, when one modality fails, our approach leverages accurate information from the other modality to enhance the model's understanding of the scene, thus facilitating the correct learning of the failing modality. This enables our method to advance 3D reconstruction in low-light scenes and enhances the robustness of 3D reconstruction techniques to some extent."}, {"title": "ABLATION STUDY", "content": "We separate different contributions and algorithm choices to test their effectiveness. As shown in Table 2 and Table 4, after incorporating multimodal initialization, allows 3DGS to achieve thermal reconstruction across various environments. Our designed multimodal thermal Gaussian models, MSMG and OMMG, not only render both thermal and RGB images simultaneously but also improve rendering quality for both modalities in all scenes, with an average increase of over 1.2 dB. We also observed that multimodal constraints mitigate the generation of excessive redundant Gaussians in certain modalities. Later, we introduced a regularization term to dynamically adjust the coefficients of both modalities. As shown in Table 4, directly training RGB modality Gaussians with 3DGS results in an average storage requirement of 159 MB. On the other hand, directly training thermal Gaussians with MI requires an average of 65 MB. The RGB Gaussians for Ours MSMG+MR average only 9 MB in storage, with thermal Gaussians averaging the same. Our method requires only $(9+9)/159 \\approx 0.08$ of the storage space compared to directly using 3DGS. Due to the reduction in the number of Gaussians, the rendering speed has also significantly increased. Additionally, the rendering quality for both modalities has also improved. In Fig. 7a, we compare our multimodal regularization y with manually adjusting the thermal constraint coefficients in a truck scene. The comparison shows that our multimodal regularization approach reduces storage space for both RGB and thermal modalities while maintaining high image quality. In Fig.7b, we visually present the Gaussian distributions of the original 3DGS method and our method with multimodal regularization."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We are the first to implement thermal reconstruction based on 3DGS. We not only achieve simultaneous rendering of thermal and RGB images but also significantly improve the rendering quality of both color and thermal images. Additionally, we greatly reduce the model's storage requirements. In the future, we plan to incorporate super-resolution for more detailed thermal reconstruction and to explore the reconstruction of dynamic thermal processes."}]}