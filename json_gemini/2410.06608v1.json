{"title": "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS.", "authors": ["Onkar Kishor Susladkar", "Vishesh Tripathi", "Biddwan Ahmed"], "abstract": "This research introduces a comprehensive Bahasa text-to-speech (TTS) dataset and a novel TTS model, EnGen-TTS, designed to enhance the quality and versatility of synthetic speech in the Bahasa language. The dataset, spanning ~55.0 hours and 52K audio recordings, integrates diverse textual sources, ensuring linguistic richness. A meticulous recording setup captures the nuances of Bahasa phonetics, employing professional equipment to ensure high-fidelity audio samples. Statistical analysis reveals the dataset's scale and diversity, laying the foundation for model training and evaluation. The proposed EnGen-TTS model performs better than established baselines, achieving a Mean Opinion Score (MOS) of 4.45 \u00b1 0.13. Additionally, our investigation on real-time factor and model size highlights EnGen-TTS as a compelling choice, with efficient performance. This research marks a significant advancement in Bahasa TTS technology, with implications for diverse language applications.", "sections": [{"title": "Introduction", "content": "The Bahasa language, spoken by a vibrant and diverse community, serves as a linguistic tapestry that encapsulates the rich cultural heritage of its speakers. In our increasingly digital world, the demand for advanced speech synthesis technologies tailored specifically to Bahasa becomes more pronounced. This necessity arises from the need for synthetic speech that authentically captures the nuances of Bahasa expressions, accommodating the linguistic diversity within the Bahasa-speaking population, including various dialects, registers, and cultural nuances.\nExisting text-to-speech (TTS) systems, while making strides in the broader landscape, often fall short in addressing the requirements of Bahasa. This gap underscores the motivation for our research, which introduces a meticulously curated Bahasa TTS dataset and an innovative TTS model, EnGen-TTS. While other models have attempted to address the synthesis challenges for diverse languages, including Bahasa, they may exhibit drawbacks such as limited adaptability, linguistic richness, or efficiency.\nOur proposed EnGen-TTS model not only fills these gaps but also showcases superior performance when compared to established baselines. The model achieves a remarkable Mean Opinion Score (MOS) of 4.45 \u00b1 0.13, outperforming existing models even without fine-tuning on additional Bahasa data. Our key strength is, positioning EnGen-TTS as a solution for high-quality, adaptive Text-to-Speech synthesis across various languages."}, {"title": "Contributions", "content": "1. Comprehensive Bahasa Dataset: Our research introduces a meticulously curated Bahasa text-to-speech (TTS) dataset, comprising ~55.0 hours sourced from diverse linguistic contexts. This dataset, enriched with contributions from skilled voice artists and varied textual sources, stands as a valuable resource for the research community and addresses the need for a comprehensive linguistic foundation for Bahasa TTS systems.\n2. Efficient Model Architecture: The proposed model architecture leverages a multi-lingual T5 (m-t5) encoder for con-fitioning text latents for decoding audio se-"}, {"title": "Related Work", "content": "In the domain of multilingual text-to-speech (TTS) datasets and models, several noteworthy contributions have been done for enhanced synthesis capabilities across diverse languages. The IndicSpeech: Text-to-Speech Corpus for Indian Languages project recognizes the critical need for TTS systems tailored to the linguistic diversity of India. Presenting a 24-hour corpus for Hindi, Malayalam, and Bengali, the authors not only contribute data but also train state-of-the-art TTS systems for each language.\nIn a similar vein, the paper titled Towards Building Text-to-Speech Systems for the Next Billion Users explores the landscape of deep learning-based TTS systems, specifically focusing on the challenges and opportunities within the context of Indian languages. Some models like SLBERT a speech and language processing framework, which uses multimodal attention mechanism to get the better transition between the speech and language features. Recognizing the computational"}, {"title": "Dataset", "content": "In creating our Bahasa text-to-speech (TTS) dataset, we curated a linguistically diverse textual foundation. This dataset is integral to our innovative TTS model, EnGen-TTS. Drawing from sources like Wikipedia and incorporating content from chat-GPT translation, our approach involved a strategic gathering of text samples. This fusion of varied linguistic contexts lays the groundwork for a robust Bahasa TTS dataset, capturing the language's nuanced breadth of expression."}, {"title": "Text collection", "content": "The textual foundation for our Bahasa text-to-speech (TTS) dataset was meticulously curated from diverse sources, enriching the dataset with varied linguistic contexts. We gathered text samples from prominent repositories such as Wikipedia, ensuring a broad representation of topics and language styles. Additionally, we incorporated content generated through chat-GPT translation, further diversifying the dataset with conversational and translated expressions. This eclectic mix of sources contributes to a comprehensive and linguistically diverse textual corpus, laying the groundwork for a robust Bahasa TTS dataset."}, {"title": "Speaker selection", "content": "To imbue the TTS dataset with authentic and expressive voices, we engaged two skilled voice artists-one male and one female. These artists were selected based on their proficiency in Bahasa and their ability to convey the nuances of the language with clarity and naturalness. Both of the speakers are from southern Indonesia. The inclusion of both male and female voices ensures a balanced representation, catering to the diverse preferences of users interacting with the TTS system. The careful selection of voice artists contributes to the overall quality and authenticity of the recorded audio samples."}, {"title": "Recording Setup", "content": "Ensuring optimal recording quality is paramount for the success of any TTS dataset. Our recording setup was designed to capture the richness of Bahasa phonetics and nuances. A controlled acoustic environment was maintained to minimize external interference, and high-quality recording equipment was employed to capture the nuances of the voice artists' performances accurately. The setup included professional microphones, soundproof-ing measures, and studio-grade audio interfaces, creating an environment conducive to the production of high-fidelity Bahasa TTS audio samples. All the data we have recorded is at a sample rate of 48 kHz."}, {"title": "Corpus Statistics", "content": "We have a report of few statistics of our Bahasa Corpus in Table 1. Upon collecting the text data and organizing it into coherent sentences, the resultant Bahasa TTS corpus exhibits notable statistics reflecting the dataset's scale and diversity. The corpus comprises a total of 55 hours of recorded voice across 52K recordings. This extensive dataset is a testament to the effort invested in capturing a comprehensive range of linguistic expressions, ensuring the TTS system's adaptability to various applications and user preferences. These corpus statistics lay the foundation for subsequent model training and evaluation, fostering advancements in Bahasa TTS technology."}, {"title": "Dataset Characterstics", "content": "The dataset encompasses 52K recordings, featuring a vocabulary size of 23,000 unique terms. Comprising a total of 55 hours, evenly distributed between male and female speakers, we strategically allocated 5 hours from each speaker for validation and an additional 5 hours from each speaker for testing purposes.So a total of 10 hours of testing & 10 hours of validation. This balanced selection ensures comprehensive coverage and representation in both the validation and test sets, fostering robust evaluation and training of our Bahasa TTS model, EnGen-TTS."}, {"title": "Architecture", "content": "Our research introduces the EnGen-TTS, a novel Bahasa text-to-speech synthesis system inspired by the state-of-the-art Encodec-based TTS Bark .The system leverages the architectural framework illustrated in Figure 1:\n1. m-T5 Encoder: A frozen multi-li T5 encoder is utilized for conditioning on text latents. This encoder is pre-trained and is kept frozen during training.\n2. Audio Encodec: Audio Encodec from meta  is pre-trained on an extensive 60K-hour audio dataset and kept frozen during training. This discretizes the audio into tokens, providing a robust audio representation without further training.\n3. Neural Codec Language Model: This module generates the audio sequence in an autoregressive manner. It conditions on both the text embeddings from the m-T5 encoder and the speaker embeddings, yielding a sequence that closely follows the linguistic and speaker-specific nuances of the input.\n4. Speaker Encoder: A frozen encoder  trained on the LibriSpeech dataset. This module produces speaker latent vectors that condition the TTS output on the speaker's unique voice characteristics.\n5. HiFi-Gan Vocoder: It is for converting the mel spectrogram into natural speech, this vocoder is fine-tuned to adapt to the specific frequency profiles of the Bahasa language. The system is designed to synthesize natural-sounding Bahasa speech by conditioning on both linguistic content and speaker identity."}, {"title": "Method", "content": "Let, $X_s$ be the input audio, $X_t$ is the text corresponding to $X_s$, and, $X_r$ be the reference audio of the same speaker. The methodology commences with byte pair encoding (BPE) to convert $X_t$ into input IDs. These are fed into the frozen m-T5 encoder to derive text embeddings $X_{te}$. Con-currently, $X_s$ is discritized by the frozen Audio-Encodec encoder $E_e$, producing discrete audio tokens in between (0 to 1023). The Speaker Encoder processes $X_r$ to generate a speaker latent vector for each frame. These vectors ($X_{se}$) are then used to condition the model on the speaker's voice, providing a direct sequence that correlates with the length of the $X_s$ sample. Then, Neural Codec Language Modeling $N_c$ predicts the audio sequence conditioned on $X_{te}$ and $X_{se}$. The Sequential cross-entropy loss ($L_{ce}$) between the generated audio sequence and the ground truth audio sequence is computed here to ensure the fidelity of the audio tokens $X_f$. The predicted audio sequence is then passed through the Encodec Decoder $E_d$ to produce an intermediate audio representation. We calculate the loss ($L_{mel}$) between the predicted mel-spectrogram $X_p$ and the GT-Mel ($Y_{mel}$) to ensure the model accurately captures the handcrafted audio features.\n$L_{ce} = -log(N_c(E_e(X_s), X_{te}, X_{se}))$\n$X_f = N_c(E_e(X_s), X_{te}, X_{se}))$\n$L_{mel} = | Y_{mel} - E_d(X_f) |$\nAt last, To achieve natural-sounding audio, we pass intermediate udio representation to the HiFi-Gan vocoder $V_c$ which is conditioned on the speaker embeddings $X_{se}$ to prevent mode collapse. The loss ($L_{gan}$) between the predicted speech and input speech is computed to align the output with the input audio distribution. To compute the Gan loss we use the temporal Discriminator module as a D. The loss follows:\n$L_{gan} = - log(D(V_c(E_d(X_f), X_{se}))) + |X_s - V_c(E_d(X_f), X_{se})|$\nWe adopt a composite loss function, taking a weighted average of $L_{ce}, L_{mel}, and L_{gan}$ to update the model's weights. This combination of losses ensures that the model learns not only the accurate prediction of audio tokens but also the refined generation of melspectrograms and the final audio output. From Figure 2 we can see that all the 3 losses are helping model to learn optimally. The overall loss ($L_t$) is expressed as follows:\n$L_t = \\alpha L_{ce} + \\beta L_{mel} + \\gamma L_{gan}$\nFrom our experiment, we determined the optimal values for the coefficients as follows: \u03b1 = 1.2, \u03b2 = 0.7, \u03b3 = 0.6."}, {"title": "Audio Codec language modeling", "content": "In this section, we delve into the core component of our model, namely the Audio Codec Lan-guage Modeling Module. This pivotal module encompasses a Masked Self-Attention mechanism, Layer Normalization, two Cross-Attention blocks, and FeedForward layers, all integrated with GeLU non-linearity. As depicted in Figure 3, the process begins with discrete audio tokens being right-shifted and passed through the embedding layer to obtain audio embeddings. These embeddings are then directed through the Masked Self-Attention block. This specific block is utilized to decode the audio sequence autoregressively, akin to the GPT model, where the full scope of tokens isn't available during decoding. Utilizing normal self-attention could lead to overfitting on particular sequences and a lack of generalization across different audio sequences. The output from the masked self-attention undergoes layer normalization, augmented with a residual connection from the input audio embeddings. Subsequently, the embeddings are processed through a cross-attention module, conditioned on the text embeddings ($X_{te}$) derived from the m-T5 encoder. The output from this cross-attention phase is then subject to another layer normalization, followed by a residual connection from the preceding text-conditioned cross-attention block.\nIn contrast to previous methods that concatenate speaker embeddings directly with audio embeddings, our approach employs an additional cross-attention step for conditioning on speaker embeddings ($X_{se}$), enhancing the model's generalizability across multiple speakers in TTS applications. This output is finally channeled to a feedforward layer, followed by GeLU non-linearity and layer normalization.\nThe architecture maintains a consistent embedding dimension of 1024 and 16 attention heads across all sub-modules. During inference, we also implement a KV-cache mechanism to enhance efficiency. Our larger model configuration comprises 26 such blocks, each meticulously designed to op-"}, {"title": "Inference", "content": "During inference, the Neural Codec Language Model ($N_c$) is initialized with the \u00a1SOS\u00bf (start of sequence) token as the input. The model then autoregressively generates the entire sequence, conditioned on the text embeddings produced by the pre-trained m-T5 model and the speaker embeddings derived from the specified reference audio. During training, audio codec inputs help the model learn the mapping between audio and text embeddings. However, during inference, the model relies on learned embeddings and the autoregressive mechanism to generate the sequence. Our current training does not use scheduled sampling or a weaning-off period typical of teacher forcing. The model smoothly transitions from training to inference due to robust text and speaker conditioning."}, {"title": "Experiments", "content": "We assessed our Text-to-Speech (TTS) models quality using Mean Opinion Score (MOS) and Comparative Mean Opinion Score (CMOS). MOS measures average listener preferences, indicating the models' naturalness, pleasantness, and intelligibility. CMOS compares models directly, identifying slight differences in perceived quality. Together, these metrics offer detailed insights into each model's performance and real-world applicability."}, {"title": "Real-Time Factor (RTF)", "content": "It is a measure of how quickly a TTS system can generate speech relative to the length of the input text. Specifically, the RTF is calculated as the ratio of the time taken to synthesize speech to the duration of the resulting audio. For example, an RTF of 1.0 means that the TTS system takes one second to generate one second of speech. An RTF of less than 1.0 indicates that the system is faster than real-time, whereas an RTF greater than 1.0 indicates that the system is slower than real-time. Lower RTF values are generally preferred as they indicate a more efficient and faster TTS system."}, {"title": "Quantitative Results", "content": "As the Bahasa language is written in Latin script, So there is always phonetic misalignment between speech and the input text. To learn those alignments we first trained our models and baselines on LJ-speech(Ito and Johnson, 2017) and VCTK dataset. After pertaining we use these learn weights for fine-tuning on our proposed bahasa dataset. We found that Our EnGen-TTS-L (Large) model outperforms other previous baselines at Bahasa text-to-speech synthesis. We Evaluate our model and baseline on MOS and CMOS metrics from Table 2 Our EnGen-TTS-L model achieves the highest MOS of 4.45 \u00b1 0.13, surpassing all other models in the comparison. This indicates that listeners rated the speech generated by EnGen-TTS-L as more natural and closer to human speech than that of the competing models. Notably, EnGen-TTS outperforms Natural-Speech and CLaM-TTS, which have MOS scores of 4.39 \u00b1 0.19 and 4.41 \u00b1 0.09, respectively.\nEven without pre-training on LJ-speech and VCTK dataset, our Engen-TTS-L model achieves results comparable to NaturalSpeech, CLaM-TTS, and XTTS, shows EnGen-TTS-L is good at adapting to new languages, which is important for creating high-quality speech synthesis for languages like Bahasa.\nIn terms of CMOS EnGen-TTS also exhibits superior performance. It achieves the lowest (best) score in Metric A with -0.0101, indicating a closer alignment with target speech characteristics than other models.\nThese results collectively affirm that EnGen-TTS not only advances the state-of-the-art in TTS by delivering the most natural-sounding speech but also maintains high performance across various evaluation metrics. Which highlights the strength of our model's architecture and training methodology."}, {"title": "Quntiative Results based on Multi-Lingual Dataset", "content": "We have trained the model in 7 languages, For Latin languages (Spanish, Portugeas, German, and Dutch) we used the CML tts-dataset and for indic languages (Hindi, Marathi, and Tamil), we used the indic-speech  dataset. For Fair comparison, We loaded the models that are pre-trained from LjSpeech and VCTK datasets. We evaluate each model with the MOS score. We with our EnGen-TTS-L, we used two more baselines to showcase our novelty.\nThe Table 3 presents a comparison of Mean Opinion Scores (MOS) across different languages for three models: VITS, NaturalSpeech, and EnGen-TTS-L. The languages evaluated include Spanish, Portuguese, German, Dutch, Hindi, Marathi, and Tamil, with varying amounts of training data for each language. EnGen-TTS-L consistently outperforms both VITS and Natural-Speech across all the languages evaluated. For example, in Spanish, EnGen-TTS-L achieves a MOS of 4.28, significantly higher than both VITS (3.39) and NaturalSpeech (3.72). Similarly, in Portuguese, EnGen-TTS-L scores 4.37, surpassing VITS (3.41) and NaturalSpeech (3.78).\nThe performance gap is especially pronounced for languages like Marathi and Tamil, where EnGen-TTS-L achieves the highest scores of 4.87 and 4.78, respectively. In contrast, NaturalSpeech performs noticeably worse with MOS scores of 4.01 and 3.96 for Marathi and Tamil, respectively,"}, {"title": "Model Performance", "content": "The model generates audio at a 24 kHz quality. When it comes to pronouncing acronyms, the task can be challenging. A helpful strategy is to articulate each letter separately, spacing them out to improve clarity. For numerical data, converting digits into their word equivalents often yields better results. An important observation is that the model might inadvertently replicate the reference speaker's audio in its output, particularly when the input text closely mirrors the reference material. The overall quality of the output is heavily influenced by the caliber of the reference audio. Ideally, the reference should be between 4 to 6 seconds long and exclusively contain clear speech, free from any background noises. It's worth noting that employing a cartoon-like voice in audio references might lead to model failure, as such in-puts are significantly different from the data used"}, {"title": "Implementation details", "content": "The EngenTTS-L's Audio Codec language modeling module utilizes transformer architecture with 26 blocks, 16 attention heads, a hidden dimension of 1024, a feed-forward layer dimension of 1024. The average length of the waveform in LJspeech and VCTK is 9.8 seconds, for our Bahasa dataset average length of the audio is around 7 seconds. During training, we randomly crop the waveform to a random length between 2 seconds and 6 seconds. Its corresponding phoneme alignments are used as the phoneme prompt. We remove the consecutive repetitions in the force-aligned phoneme sequence. While training we keep max sequence length of 500. The models are trained using 3 NVIDIA RTX 3090Ti 24 GPUs with a batch size of 4 with gradient accumulation steps of 24 per GPU for 800k steps. We optimize the models with the AdamW optimizer, warm up the learning rate for the first 32k updates to a peak of 5 \u00d7 10\u20134, and then linear decay it.\nFor the evaluation of inference performance, all models were tested under identical hardware conditions to ensure consistency and comparability. Specifically, we utilized an NVIDIA T4 GPU equipped with 16 GB of VRAM. Inference was performed with a batch size of 16, and each input had an average token length of 20. Under this setup, the inference speed was approximately 200 milliseconds per batch for all models. This uniform testing environment ensured that the performance metrics reported are directly comparable across all evaluated TTS systems."}, {"title": "Ablation", "content": "In our experimentation with different vocoders for generating high-quality audio from latent representations, Univnet emerged as the top performer, achieving a Mean Opinion Score (MOS) of 4.41 \u00b1 0.12. This slight edge over MelGan (4.39 \u00b1 0.11) and Wavgrad (4.36 \u00b1 0.13) suggests its superiority in preserving speech quality and naturalness during waveform reconstruction (see Table 5). While HiFi-GANs (4.35 \u00b1 0.13) exhibited comparable performance, its slightly lower MOS indicates room for further optimization. Overall, these results highlight the importance of vocoder selection in the Text-to-Speech pipeline, with Univnet demonstrating its potential for creating highly faithful and human-sounding synthetic speech.\nAdditionally, to explore the impact of model size on both perceptual quality and real-time efficiency, we conducted an ablation study as outlined in Table 4. The table presents results for three variants of our EnGen-TTS model, denoted as EnGen-TTS-S, EnGen-TTS-M, and EnGen-TTS-L, with varying parameters. As model size increases from 87M to 570M, we observe a corresponding improvement in Mean Opinion Score (MOS), indicating enhanced speech quality. Specifically, EnGen-TTS-L achieves a MOS of 4.42 \u00b1 0.08, outperforming the smaller variants. However, this comes at the cost of increased Real-Time Factor (RTF), with EnGen-TTS-L demonstrating a slightly longer synthesis time (0.021) compared to EnGen-TTS-S (0.014). This trade-off between model size, perceptual quality, and synthesis speed provides valuable insights into tailoring the EnGen-TTS architecture based on specific application requirements and resource constraints.\nWe also conducted an ablation study focusing on the impact of different loss components. Table 6 summarizes the Mean Opinion Scores (MOS) obtained from three model variants: Lgan, Lmel, and the combination of both (Lgan + Lmel). The results indicate that incorporating both the adversarial loss (Lgan) and the mel-spectrogram loss (Lmel) leads to a MOS of 4.35 \u00b1 0.12, showcasing a marginal improvement over individual losses. This nuanced exploration of loss components provides valuable insights into the synergy between adversarial and mel-spectrogram losses in our training pipeline, contributing to the optimization of our Bahasa TTS model for enhanced speech synthesis quality. Note: For every loss we are always computing Lee, without it model can't be trained.\nWe conducted an ablation study on three different model sizes-large, medium, and small-focusing on their performance with varying lengths of input text sequences. Our observations indicate that for text sequences ranging from 5 to 75 tokens, there is minimal variation in the MOS metrics. However, as the sequence length exceeds 75 tokens, we noticed a decline in MOS metrics. This decline correlates with deteriorations in pronunciation and timbre quality of generated speech, along with an increase in the Word Error Rate (WER). As depicted in Figure 5, extending the sequence length beyond 100 tokens results in a significant decrease in MOS metrics, likely due to the models' inability to manage longer contexts effectively, leading to catastrophic forgetting which is discuss in the paper ."}, {"title": "Conclusions", "content": "In conclusion, our study presents a pivotal advancement in Bahasa text-to-speech (TTS) synthesis, combining a richly curated dataset with a groundbreaking model design. Our comprehensive Bahasa TTS dataset, encompassing over 55 hours of audio and 52K recording, is a robust resource, crafted with inputs from proficient voice artists and varied textual content. The introduced model, EnGen-TTS, excels in performance, surpassing conventional benchmarks with its innovative architecture, which includes a multi-task T5 (m-T5) encoder and a neural codec language modeling module, without necessitating extra fine-tuning for Bahasa. This design not only enhances speech synthesis quality but also ensures computation efficiency, establishing a new standard in TTS technology. Our work not only pushes forward the boundaries of Bahasa TTS but also lays the groundwork for future developments in multilingual text-to-speech systems, promising high-quality and diverse linguistic applications."}, {"title": "Limitations", "content": "One limitation of our proposed method is its reliance on audio sampled at 22.05 KHz. This sampling rate is necessitated by the use of Meta's pre-trained Audio Encodec, which requires 22.05 kHz audio data. However, this presents a challenge for applications such as automatic voice calling, where telephony standards typically mandate an 8 kHz sampling rate. The required down-sampling from 22.05 KHz to 8 KHz results in a significant reduction in audio quality, manifesting as \"muffled speech\" due to the drastic decrease in sampling rate. Future work will focus on enabling high-quality audio generation directly at 8 kHz to better align with telephony requirements without compromising speech clarity.\nAnother limitation of our method lies in the maximum sequence length used during training, which is capped at 500 audio tokens. This constraint is well-suited for generating high-quality speech for shorter sentences or sentences containing up to 70-80 words. However, when the word count exceeds this limit, the generated speech may exhibit unnatural pauses or occasional missing words. This issue is likely due to catastrophic forgetting of longer contexts. Our future research will focus on increasing the context window up to 2048 audio tokens to better handle larger sentences or paragraphs, thereby improving the naturalness and continuity of generated speech."}]}