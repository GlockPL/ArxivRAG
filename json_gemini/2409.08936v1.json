{"title": "SynSUM \u2013 Synthetic Benchmark with Structured and Unstructured Medical Records", "authors": ["Paloma Rabaey", "Henri Arno", "Stefan Heytens", "Thomas Demeester"], "abstract": "We present the SynSUM benchmark, a synthetic dataset linking unstructured clinical notes to structured background variables. The dataset consists of 10,000 artificial patient records containing tabular variables (like symptoms, diagnoses and underlying conditions) and related notes describing the fictional patient encounter in the domain of respiratory diseases. The tabular portion of the data is generated through a Bayesian network, where both the causal structure between the variables and the conditional probabilities are proposed by an expert based on domain knowledge. We then prompt a large language model (GPT-40) to generate a clinical note related to this patient encounter, describing the patient symptoms and additional context. The SynSUM dataset is primarily designed to facilitate research on clinical information extraction in the presence of tabular background variables, which can be linked through domain knowledge to concepts of interest to be extracted from the text - the symptoms, in the case of SynSUM. Secondary uses include research on the automation of clinical reasoning over both tabular data and text, causal effect estimation in the presence of tabular and/or textual confounders, and multi-modal synthetic data generation. The dataset can be downloaded from https://github.com/prabaey/SynSUM.", "sections": [{"title": "1 Introduction", "content": "Electronic health records (EHRs) are a gold mine of information, containing a mix of structured tabular variables (medication, diagnosis codes, lab results...) and free unstructured text (detailed clinical notes from physicians, nurses...) (Ford et al., 2016). These EHRS form a valuable basis for training clinical decision support systems, (partially) automating essential processes in the clinical world, such as diagnosis, writing treatment plans, and more (Peiffer-Smadja et al., 2020; Mujtaba et al., 2019; Rasmy et al., 2021; Li et al., 2020; Xu et al., 2019). While large language models can help leverage the potential of the unstructured text portion of the EHR (Zhang et al., 2020; Liu et al., 2022; Huang, Altosaar, and Ranganath, 2019; Lehman and Johnson, 2023; Singhal et al., 2023; Labrak et al., 2024), these black box systems lack interpretability (Quinn et al., 2022; Zhao et al., 2024; Tian et al., 2024). In high-risk clinical applications, it can be argued that one should prefer more robust and transparent systems built on simpler, feature-based models, like regression models, decision trees, Bayesian networks, etc (Rudin, 2019; Sanchez et al., 2022; Lundberg et al., 2020). However, such models cannot directly deal with unstructured text and require tabular features as an input. For this reason, automated clinical information extraction (CIE) (Ford et al., 2016; Wang et al., 2018; Hahn and Oleynik, 2020) remains an essential tool for building large structured datasets that can serve as training data for such systems.\nHowever, CIE remains a challenging task due to the complex nature of clinical notes. These often leave out important contextual details that an inexperienced reader would not know about, and which an automated system would need in order to correctly extract concepts from the text. Existing systems do not fully exploit the available medical domain knowledge to fill in this gap. We propose that clinical information extraction could benefit from leveraging two additional sources of information, apart from the unstructured text itself. On the one hand, we have the tabular features already encoded in the EHR, containing information on the medical history of the patient, as well as partially encoded information relating to the current visit. On the other hand, we can connect this encoded background information with the concepts we are trying to extract from the text, using medical domain knowledge. Such domain knowledge could be structured in the form of a Bayesian network. This idea is presented conceptually in Figure 1.\nTo put this idea into practice, we need a clinical dataset which (i) contains a mix of tabular data and unstructured text, where (ii) the tabular concepts and the concepts we aim to extract from the text can be linked through domain knowledge. While open-source datasets like MIMIC-III and MIMIC-IV contain this mix, they are too complex for the preliminary research we envision. First, the area of intensive care in which the data was collected is very extensive, making it hard to isolate a specific small-scale use-case for which the domain knowledge could be listed. Second, the portion of the dataset which is encoded into tabular features is mostly driven by billing needs, rather than completeness or accuracy, and does not contain any encoded symptoms, which are the concepts we would like to extract from the text. Third, the link between the tabular features and the concepts mentioned in the text might be inconsistent due to system design or human errors, as shown by Kwon et al. (2024). Finally, the EHRs in MIMIC are time series, adding another layer of complexity.\nIn this work, we build a synthetic yet sufficiently realistic dataset that addresses some of these shortcomings and allows research on the idea of incorporating domain knowledge for improved CIE in the presence of encoded tabular variables. Our dataset, called SynSUM (Synthetic Structured and Unstructured Medical records) is a self-contained set of synthetic EHRs in a primary care setting, fulfilling the following requirements:\n\u2022 There is a mix of structured tabular data and unstructured text.\n\u2022 The use-case in which the EHR is constructed allows us to explicitly model the domain knowledge in the form of a Bayesian network, which links the background tabular variables with the concepts"}, {"title": "2 Data generation", "content": "Our general methodology for generating the artificial patient records is shown in Figure 2. First, we sample a tabular patient record from an expert-defined Bayesian network. Then, we create a prompt containing the background information and symptoms available in the tabular patient record. We use this to prompt a large language model (GPT-40) to generate a clinical note, as well as a more compact version of the same note. This process is repeated 10.000 times, leaving us with a dataset containing both the tabular patient record and two versions of the clinical note describing the patient encounter.\nWe now zoom in on the two major parts of this data generating process. First, Section 2.1 describes how we defined the generative process, including the Bayesian network modeling the structured tabular variables in the domain of respiratory disorders. Then, Section 2.2 dives into the clinical note generation by the large language model."}, {"title": "2.1 Modeling structured tabular variables with a Bayesian network", "content": "We asked an expert to define a Directed Acyclic Graph (DAG) which (partially) models the domain of respiratory diseases in primary care. In this DAG, a directed arrow between two variables models a causal relation between them. The full DAG is shown in the top portion of Figure 2.\nCentral to the model are the diagnoses of pneumonia and common cold (also known as upper respiratory tract infection), which may give rise to five symptoms. The expert also modeled some relevant underlying conditions which may render a patient more predisposed to certain diagnoses or symptoms. Based on the symptoms experienced by a patient, a primary care doctor decides whether to prescribe antibiotics or not. The presence and severity of the symptoms, as well as the prescription of antibiotics as a treatment, influence the total number of days that the patient eventually stays home as a result of illness (the outcome). Finally, there are some variables which are not clinical that exert an external influence on the diagnoses, the treatment and the outcome. Table 1 summarizes all variables and their meaning, as well as their possible values. While this model does not completely describe the real world, we do believe it to be sufficiently realistic for the purpose of generating an artificial dataset.\nTo define a data generating mechanism from which we can sample synthetic patients, we turn the DAG into a Bayesian network by defining a joint probability distribution. In a Bayesian network, this joint distribution factorizes into the product of conditional probability distributions for each variable, as shown in Equation (1). We parameterize these using four different approaches.\nWhen the variable is discrete and has a limited number of parents, we define a conditional probability table (CPT). Each entry of the table contains the probability for a particular value of the variable, conditional on the combination of values of the parent variables. If the variable has no parents, we just define a prior probability. The probabilities in the tables were filled in by the expert based on experience, as well as demographics in Belgium and the expert's local general practice. While we do not expect these probabilities to generalize to the global patient population as a whole, a realistic-looking distribution suffices for our use-case. We provide these tables for the variables asthma, smoking, hay fever, COPD, season, pneumonia, common cold, fever, policy and self-employed in Figure 3.\nFor categorical variables with many parents, it becomes infeasible to manually fill in the CPT in a clinically meaningful way, because of the large number of possible combinations of parent values. This is the case for the symptoms dyspnea, cough, pain and nasal in our Bayesian network. To circumvent this problem, one can use a Noisy-OR distribution (Koller and Fried-"}, {"title": "2.1", "content": "mann, 2009). The Noisy-OR model is commonly used to define the distribution of a variable Y which depends on a set of causes {X1,..., Xk}. It rests on the assumption that the combined influence of the possible causes {X1,..., Xk } on Y is a simple combination of the influence of each X\u2081 on Y in isolation. This is a reasonable assumption to make in the case of symptoms with multiple possible causes (parents in the Bayesian network): a symptom arises in a patient if any of its possible causes succeeds in activating the symptom through its own independent mechanism. As shown in Equation (2), the parameterization of the noisy-OR distribution rests on choosing the parameters pi, which is the probability that a possible cause X\u2081 activates symptom Y. As a special case, po, also known as the leak probability, is the probability that symptom Y is activated as the result of another unmodeled cause (ouside of all Xi's). Note that xi in the equation is 1 when the cause Xi is present in the patient, and 0 if not. Equations (3) through (6) define such a Noisy-OR distribution for the symptoms dyspnea, cough, pain and nasal. Note that the symptom fever is fully defined through a CPT, since the expert was able to provide intuition on all possible combinations of its two parent values, eliminating the need for a Noisy-OR distribution."}, {"title": "2.2 Generating unstructured text with a large language model", "content": "Starting from the tabular portion of the patient record, we aim to generate a text describing this fictional pa-"}, {"title": "3 Symptom predictor baselines", "content": "To measure the amount of information encoded in the data and set a baseline for subsequent information extraction tasks, we run various prediction models on both the tabular and textual part of the dataset. These models are trained to predict each of the five symptoms: dyspnea, cough, pain, fever and nasal.\nTwo of our baselines only get to see the tabular portion of the dataset at the input: Bayesian network (BN-tab) and XGBoost (XGBoost-tab). We use these models to predict each symptom in three settings, differing from one another in the set of tabular features that are taken as an input, which we call the evidence:\n\u2022 P(sympt all): Predict the symptom given all other tabular features as evidence. This set includes the background, diagnoses, external influence, treatment, outcome and other symptoms.\n\u2022 P(sympt | no-sympt): Predict the symptom given all other tabular features as evidence, except for the other symptoms. This mimics the setting where we have tabular features available in the patient record, but have not extracted any symptoms from the text yet. This set includes the background, diagnoses, external influence, treatment and outcome variables.\n\u2022 P(sympt | realistic): Predict the symptom given all tabular features which would be available as evidence in a realistic setting. We do not expect policy, self-employed and #days to be recorded in any kind of realistic patient record, and therefore leave them out of this evidence set. As in the no-sympt setting, we assume that we have not extracted any tabular symptoms from the text yet. Therefore, this set includes the background, diagnoses, season and treatment variables.\nApart from the tabular-only baselines, we also train some baselines that get to see the text at the input. Our neural-text classifier takes only the text as an input (in the form of a pretrained clinical sentence embedding) and outputs the probability that a symptom is mentioned in the text. Finally, we extend this text-only baseline by concatenating a numerical representation of the tabular features to the text embedding at the in-"}, {"title": "3.1 Models", "content": "We provide the causal structure in Figure 2 to the Bayesian network, and learn all parameters in the conditional probability tables (CPTs), Noisy-OR distributions, logistic regression model and Poisson regression model from the training data. In each case, we use maximum likelihood estimation to estimate the parameters and fill in a CPT. Where we don't directly learn a CPT (for the variables dyspnea, cough, pain, nasal, antibiotics and #days), we evaluate the learned distribution for each combination of child and parent values to obtain a CPT. For more details, we refer to Appendix B.1. We then use variable elimination over the full joint distribution to evaluate the capability of the learned Bayesian network to predict each of the symptoms, taking different variables as evidence according to the 3 settings described earlier (all, no-sympt and realistic).\nWe train an XGBoost classifier for each symptom in combination with each of the 3 settings, meaning each classifier has a different set of tabular features at the input. We optimize the hyperparameters separately for each combination (15 in total) using 5-fold cross-validation. For more details, we refer to Appendix B.2.\nWe train a neural classifier that takes only the text as an input and is trained to predict the probability a symptom is mentioned. We train separate classifiers for each symptom. We first split the text into sentences, and transform these into an embedding using the pretrained clinical representation model Bi-OLORD (Remy, Demuynck, and Demeester, 2024). We explore 4 settings for turning these sentence embeddings into a single note embedding:\n\u2022 hist: We average all sentence embeddings for the sentences in the \u201chistory\u201d portion of the note.\n\u2022 phys: We average all sentence embeddings for the sentences in the \"physical examination\" portion of the note.\n\u2022 mean: To get a single representation for the full note, we take the average of the hist and phys embeddings.\nall results in a text embedding of 2*768 dimensions.\nThese embeddings are fed into a linear layer with 256 neurons, which is then transformed into a single output neuron, followed by a Sigmoid activation. For the classifiers that predict fever, three output neurons followed by a Softmax activation are used instead, one for each class. While the embeddings remain fixed, we learn the parameters in the hidden and output layers using cross-entropy as a loss function over the training set. We train a separate classifier for each symptom, setting and difficulty of the text (normal vs. compact). For the binary symptoms, we train for 15 epochs using"}, {"title": "3.2 Results", "content": "We randomly split the dataset into a train and test set, using an 8000/2000 split. We use cross-validation on the train set to tune any hyperparameters, and report the final F1-score over the test set after training. We classify a symptom as positive if the predicted probability is larger than 0.5. Since fever has three possible categories, the class with the highest predicted probability is chosen. In that case, we report the macro F1-score over all three categories.\nTable 6 compares the results obtained over all baselines. The tabular-only baselines (BN-tab and XGBoost-tab) perform consistently worse than the baselines that include text (neural-text and neural-text-tab). The evidence setting where all other features are included as evidence usually performs best for the tabular-only baselines.\nThe neural-text-tab baseline does not perform better than the neural-text baseline when the normal notes are used. While there is little room for improvement in the dyspnea, cough and nasal classifiers, the symptoms pain and fever are harder to predict. We also note a consistent gap in performance between the normal notes and the compact notes, which can be attributed to the higher complexity of the compact notes. In that case, the neural-text-tab classifier manages to marginally improve over the neural-text classifier by including the tabular features.\nTable 7 further breaks down the results for the neural-text classifier over the different embedding types. There is a significant gap in performance between the hist and phys settings for the symptoms cough, pain and fever. This makes sense, as the \"history\" section of the note outlines the symptoms experienced by the patient more clearly. The performance difference between the mean and concat settings is usually small, with concat slightly outperforming mean. While the score for hist comes close to those for mean and concat, the latter usually still outperform the former for the normal notes, showing that there is some complementary information in the \u201chistory\u201d and \u201cphys-"}, {"title": "4 Discussion", "content": "Our analysis of simple tabular and textual baseline models revealed that the symptoms pain and fever are hardest to predict in both the tabular-only and the text-only setting. Combining both settings, i.e. integrating tabular background features in the extraction of the concepts from the text, linking them through domain knowledge, may have potential for improved information extraction. Future work will focus on realizing this hybrid approach to improve upon the baseline results presented in Table 6. The naive way of including tabular features by simply concatenating them to the text embedding at the input of the neural classifier already improves extraction performance on the more difficult compact version of the notes. This indeed suggests that symptoms that are mentioned less explicitly in the text might be recovered by including background tabular features in the prediction.\nWhile the dataset we constructed is meant to have realistic properties, we also intentionally simplify reality to make the design and generation process feasible. The dataset is purely meant as a research benchmark where the ground truth relations are known, and results obtained on it are not meant to transfer to real clinical notes or datasets. We therefore advise strongly against using the dataset for training prediction models which will be deployed in real settings."}, {"title": "Potential uses", "content": "The dataset is primarily designed to facilitate research on clinical information extraction in the presence of tabular background variables. Future work will focus on realizing the idea presented in Figure 1, where the tabular features aid in more accurately extracting concepts from the text by linking them through domain knowledge. Apart from this, we also foresee multiple secondary uses of the dataset. First, the dataset could facilitate research on the automation of"}, {"title": "A", "content": "Figures 5 and 6 show two additional example prompts. Figure 7 shows what would happen if we prompted the unrelated cases (where no symptoms are present in the patient) using our normal strategy. Figure 8 shows the prompt we used instead, in the case where the patient experiences no respiratory symptoms and does not have any underlying respiratory conditions either."}, {"title": "B Symptom predictor baselines", "content": "We learn a Bayesian network over the training data, providing the structure over all variables as in Figure 2. For the variables asthma, smoking, hay fever, COPD, season, pneumonia, common cold, fever and self-employed, we learn the conditional probability tables (CPTs) from the training data using maximum likelihood estimation (which comes down to counting co-occurrences of child and parent values for each entry in the CPT). We use the pgmpy library (Ankur Ankan and Abinash Panda, 2015) with a K2 prior as a smoothing strategy to initialize empty CPTs.\nAs support for learning Noisy-OR distributions is not provided in pgmpy, we learn these parameters with a custom training loop. We formulate the likelihood as in Equation (2), and learn the parameters pi in Equations (3) through (6) for the variables dyspnea, cough, pain and nasal through maximum likelihood estimation by iterating over the train set for 10 epochs, using an Adam optimizer with a batch size of 50, a learning rate of 0.01 and random initialization of each parameter. To integrate the learned Noisy-OR distributions in the Bayesian network, we turn them into fully specified CPTs. To obtain these, we simply evaluate Equation (2) for all possible combinations of child and parent values. While this results in large and inefficient CPTs, the automated inference engine bulit into pgmpy library does not support Noisy-OR distributions directly. Note that both versions of the conditional distribution are equivalent, so we do not incur a loss in precision.\nSimilarly, the coefficients in the logistic regression model for antibiotics and the Poisson regression model for #days are learned using maximum likelihood estimation over the training set. The likelihood is expressed as in Equation (7) and Equations (8) and (9) respectively, with learnable parameters in place of each coefficient. We iterate over the train set for 15 epochs, again using an Adam optimizer with a batch size of 50, a learning rate of 0.01 and random initialization of each parameter. Finally, we turn the logistic regression and Poisson regression models into CPTs by evaluating Equations (7), (8) and (9) for all combinations of parent and child values. For the variable #days, we needed to turn each discrete number of days into a category, because pgmpy only provides automated inference for"}, {"title": "B.1", "content": "Bayesian networks consisting of exclusively categorical variables. This results in a large CPT containing one row per possible number of days, which range from 0 to 15 in our training dataset, and one column for each combination of the 7 parent variables. To allow for a possible larger maximum number of days in the test set, we create a category \u2265 15 days, which is defined as one minus the summed probability of all other days.\nOnce we have learned all parameters in the joint distribution, we can evaluate the Bayesian network's ability to predict each of the symptoms. For each evidence setting (as defined in the main text), we apply variable elimination with each of the symptoms as a target variable. Looking at the causal structure in Figure 2, we note that the model never has to marginalize over the many rows in the learned #days CPT, since it is never a target variable. This makes automated inference feasible in our case."}, {"title": "B.2 XGBoost-tab", "content": "We use the xgboost library in combination with sklearn. We train separate classifiers per symptom, one for each setting, which means we train 15 classifiers total. We tune the hyperparameters separately for each classifier, using 5-fold cross validation with F1 as a scoring metric (macro-F1 for fever).\nThe classifiers for the symptoms dysp, cough, pain and nasal use a binary logistic objective and logloss as an evaluation metric within the XGBoost training procedure, while the classifiers for the symptom fever use the multi-softmax objective with multiclass logloss as an evaluation metric. The scale_pos_weight parameter is set to the ratio of negative over positive samples for the binary classifiers. For the fever classifier, we address class imbalance by setting class_weight = balanced, which ensures that samples from less frequent"}, {"title": "B.3 Neural-text", "content": "The pretrained BioLORD encoder (Remy, Demuynck, and Demeester, 2024) was obtained through the huggingface library. The encoder outputs 768-dimensional sentence embeddings. Since the full text did not fit into the context window, we embedded each sentence separately, and then combined them using our strategies outlined in the main text. To split the text into sentences, we used the nltk package. The settings hist, phys and mean all result in a text embedding of 768 dimensions, while the setting concat results in a text embedding of 2*768 dimensions.\nThese embeddings are fed into a linear layer with 256 neurons, which is then transformed into a single output neuron, followed by a Sigmoid activation. For the classifiers that predict fever, three output neurons followed by a Softmax activation are used instead, one for each class. While the embeddings remain fixed, we learn the parameters in the hidden and output layers using cross-entropy as a loss function over the training set. We train a separate classifier for each symptom, setting and difficulty of the text (normal vs. compact). For the binary symptoms, we train for 15 epochs using"}]}