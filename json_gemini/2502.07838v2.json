{"title": "NanoVLMs: How small can we go and still make coherent Vision Language Models?", "authors": ["Mukund Agarwalla", "Himanshu Kumar", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "abstract": "Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have garnered significant research attention for their ability to leverage Large Language Models (LLMs) in multimodal tasks. However, their potential is constrained by inherent challenges, including proprietary restrictions, substantial computational demands, and limited accessibility. Smaller models, such as GIT and BLIP, exhibit marked limitations, often failing to generate coherent and consistent text beyond a few tokens, even with extensive training. This underscores a pivotal inquiry: how small can a VLM be and still produce fluent and consistent text? Drawing inspiration from the exceptional learning process of 3-4 year old children, who rely heavily on visual cues for understanding and communication, we introduce two novel datasets: ShortDesc (featuring concise image descriptions) and LongDesc (containing more detailed image descriptions). These datasets consist of image-text pairs where the text is restricted to the simple vocabulary and syntax typically used by young children, generated with a scaled- down model, GPT-40. Using these datasets, we demonstrate that it is possible to train VLMs that are significantly smaller-up to 10 times smaller than state-of-the- art(SOTA) small VLMs while maintaining architectural simplicity. To evaluate the outputs, we leverage GPT-40 to grade the text, as if stories written by students, on creativity, meaningfulness, and consistency, assigning scores out of 10. This method addresses limitations of standard benchmarks by accommodating unstructured outputs and providing a multidimensional evaluation of the model's capabilities. Our findings contribute to the development of lightweight, accessible multimodal models for resource-constrained environments.", "sections": [{"title": "1. Introduction", "content": "LLMs(Zheng et al., 2023; Zhao et al., 2024; OpenAI, 2023a;b) have significantly advanced natural language processing (NLP), demonstrating strong capabilities in reasoning, long-form content generation and in-context learning (ICL). While models like GPT-3, LLaMA(AI, 2024), and Claude have achieved SOTA performance across text-based tasks, their unimodal nature limits their ability to process and interpret visual data. The rise of VLMs has bridged this gap by integrating pretrained vision encoders with large-scale language models, enabling multimodal reasoning across tasks such as Image Captioning (IC)(Vinyals et al., 2015), Visual Question Answering (VQA)(Agrawal et al., 2015), Optical Character Recognition (OCR)(Shi et al., 2017), and Visual Grounding(Plummer et al., 2015). Recent breakthroughs in multimodal learning(Akkus et al., 2023) have led to the development of high-performing VLMs such as PaLI-X(Luo et al., 2024), GPT-4V(Ghosh"}, {"title": "2. Methodology", "content": "This section outlines the methodologies employed to develop NanoVLMs, a highly efficient VLM that is more than 10 times smaller than SOTA VLMs while retaining competitive performance. To the best of our knowledge, NanoVLMs are the first to achieve such extreme compression without compromising on results. To achieve this, the data used for model training should be simple and easy to understand. aligning with the cognitive abilities of 3-4-year old children. Inspired by the learning processes of 3-4 year old children, we adopted an analogy based on how children in this age group acquire knowledge. A similar analogy was explored in (Eldan & Li, 2023), which focused exclusively on textual data. However, we found this approach inadequate, as 3-4 year old children are more reliant on visual stimuli than text for learning. Recognizing this, we designed and trained a minimalist VLM with a straightforward architecture, leveraging simple images and text that could be easily understood by children.\nThe high-level architectural design and training process draw inspiration from methodologies employed in SOTA VLMs while emphasizing efficiency and scalability. Our work is structured around several key components: a robust dataset preparation framework that ensures diverse and high-quality training data, the NanoVLMs architecture designed to optimize parameter efficiency without sacrificing performance, a comprehensive training and experimentation pipeline tailored for effective convergence, and a rigorous evaluation methodology that examines both quantitative and qualitative aspects of generated descriptions. We also implement techniques to enhance cross-modal interactions, ensuring that the model effectively captures fine-grained visual details."}, {"title": "2.1. Dataset", "content": null}, {"title": "2.1.1. DATASET OVERVIEW", "content": "To train NanoVLMs, we utilized the COCO (Common Objects in Context)(Lin et al., 2014; Chen et al., 2015) dataset, a widely recognized resource in computer vision tasks. This dataset is ideal for our study as it features high-resolution, richly annotated images from diverse domains,"}, {"title": "2.1.2. DATASET PREPARATION", "content": "To prepare the dataset, we used the COCO dataset's images and corresponding captions to generate image descriptions, constructing two datasets: ShortDesc and LongDesc. Specifically, ShortDesc comprises concise image descriptions of 20-25 words, while LongDesc features detailed image descriptions of 60\u201370 words. These datasets were designed to assess how the model handles shorter versus longer text inputs, reflecting its ability to process and generate meaningful and consistent outputs. This mirrors the developmental process of 3-4 year old children, who acquire intellectual abilities through exposure to diverse visuals along with various linguistic patterns.\nFor generating these descriptions, we employed OpenAI's GPT-40, a SOTA text generation model capable of producing high-quality synthetic content. Combined captions for each image (all captions) along with the respective prompt is passed to GPT-40(OpenAI, 2024), where Prompt1 shown in Figure 2 is used to generate ShortDesc dataset and Prompt2 shown in Figure 2 is used to generate LongDesc dataset. The model produced outputs based on the structure and constraints defined in the respective prompts. Figure 3 illustrates the process of prompt passing and dataset preparation."}, {"title": "2.2. Architecture", "content": "The primary objective of NanoVLMs is to complete partially provided textual descriptions by generating coherent and contextually appropriate outputs. To achieve this, we designed a VLM with a simple yet effective transformer-based architecture consisting of three key components: a visual encoder for processing images, a visual-textual connector to bridge visual and textual modalities, and a language decoder for generating text as shown in Figure 1.\nThe core of the NanoVLM architecture lies in its transformer blocks (shown in Figure 5), which form the foundation of both the visual encoder and the language decoder. Each transformer block comprises multi-head attention(Jalammar, 2019; Vaswani et al., 2023) for capturing relationships across input tokens\u2014whether image patches or text-and a multi-layer perceptron (MLP) for processing the outputs of the attention mechanism. To ensure stable training and faster convergence, layer normalization is applied prior to the attention and MLP layers. A key distinction in the decoder is the use of causal self-attention, where masking is employed to uphold the autoregressive nature of text generation. This mechanism is vital for maintaining coherence and contextual accuracy, ensuring that predictions are based solely on prior information, a critical requirement for generating fluent and logically consistent textual descriptions."}, {"title": "2.2.1. VISUAL ENCODER", "content": "The visual encoder in NanoVLM is a critical component responsible for extracting meaningful features from images, drawing inspiration from the Vision Transformer (ViT) architecture while being optimized for compactness. To maintain performance, we process images at a resolution of 224x224 pixels(Thapa et al., 2024), dividing them into 16x16 pixel patches to yield 196(Wen et al., 2024) patches per image. These patches undergo a series of transformations beginning with patch embedding, where the image is passed through two 2D convolutional layers(as shown in Figure 4) followed by layer normalization(Ba et al., 2016) and ReLU(Agarap, 2019) activation. This is succeeded by a fully connected neural network, which transforms the patches into 196 tokens. A [CLS] token is then prepended, making the sequence 197 tokens. Positional encoding is applied to retain spatial information, followed by normalization. These enriched embeddings are then processed through a series of transformer blocks, where multi-head attention"}, {"title": "2.2.2. VISUAL-TEXTUAL CONNECTOR", "content": "The visual-textual connector is a pivotal component in the NanoVLMs architecture, responsible for bridging the gap between the visual and textual modalities. The visual embeddings and the textual embeddings must be aligned in the same dimensional space to enable effective interaction between the two modalities. To achieve this, we employ a multimodal projector that consists of a single learnable layer followed by GELU that reduces the dimensionality of the visual embeddings. Once the visual embeddings are projected into the textual embedding space, both the visual and textual embeddings are concatenated to form a multimodal token embedding. This combined representation effectively encapsulates both the image's content and its corresponding textual description. The resulting multimodal token embedding is then passed as input to the decoder block, where it will guide the generation of coherent and contextually relevant textual descriptions."}, {"title": "2.2.3. DECODER BLOCK", "content": "The decoder block in NanoVLM transforms fused visual-textual embeddings into coherent text using a transformer-based architecture, ensuring text generation. It begins by passing the multimodal token embedding through a positional embedding layer, which encodes token order. The input then moves through transformer blocks with multi-head self-attention, but unlike the encoder, the decoder applies causal self-attention, masking(Liu et al., 2022; Yin et al., 2024) future tokens to prevent information leakage and enforce autoregressive generation. Finally, the processed output undergoes layer normalization and a linear projection, mapping it to a vocabulary space where logits determine the next token. This structured decoding mechanism enables NanoVLM to generate fluent, context-aware descriptions when provided with both an image and partial text as input.We employ cross-entropy loss to compute the error between the predicted and actual target text. This loss is used to guide the training of the model, optimizing the parameters to generate accurate and coherent textual descriptions."}, {"title": "2.3. Experiments", "content": "This section details the experimental setup and hyperparameter tuning for training all three versions of NanoVLM. The models were trained on a single A100 GPU, with key hyperparameters such as n_blks (number of transformer blocks in the visual encoder), n layer (number of transformer layers in decoder), n_head (number of attention heads), head_size (size of each head), n_embd (textual embedding dimension), and img_embd_dim (visual embedding dimension) gradually scaled up as we moved from the mini to the large version, as shown in Table 1. This progressive scaling allowed us to analyze how increasing model capacity influenced performance while maintaining computational efficiency. Certain hyperparameters remained fixed across all versions"}, {"title": "3. Evaluation", "content": "Traditional evaluation of VLMs typically relies on structured benchmark datasets where the model's output is compared against a predefined ground-truth answer. To comprehensively evaluate a VLM, we focus on five key benchmarks-grammatical correctness, consistency, creativity, meaningfulness, and plot each of which plays a crucial role in determining the model's ability to generate structured and engaging descriptions. Our primary objective is to investigate whether a VLM with as few as 6M-25M parameters can still generate coherent and contextually relevant text. Inspired by the evaluation framework of (Eldan & Li, 2023), we employ an LLM-based evaluation approach that leverages GPT-40 to assess generated text quality. Our evaluation setup consists of a manually curated dataset of 25 image descriptions, where each description's beginning along with its corresponding image, is provided as a prompt"}, {"title": "4. Results", "content": "This section evaluates the effectiveness of NanoVLMs in generating accurate and coherent textual descriptions. We first analyze the training and validation losses to assess the model's convergence and stability. Next, we evaluate the generated outputs on structured benchmarks, highlighting their relevance and fluency. Additionally, we perform a ROUGE score analysis, verifying the diversity and contextual alignment of the generated descriptions.\nThe training and validation loss trajectories, as illustrated in Figure 6, exhibit a consistent downward trend across all NanoVLM versions, affirming stable convergence and effective optimization. The loss curves reveal that the gap between training and validation losses remains minimal for NanoVLMs, with a maximum observed difference of only 0.08 to 0.1. However, NanoVLMs trained on LongDesc show slightly less stable convergence, primarily due to the increased complexity and length of textual descriptions. Generating a long and contextually rich description while maintaining consistency and meaningfulness is a challenging task for a compact model with limited parameters. Despite this, the loss curves across all versions eventually plateau in the final training epochs, demonstrating that the models successfully learn structured vision-language representations while mitigating overfitting.\nTo rigorously evaluate the performance of our NanoVLM models, we compare their generated outputs, conditioned on visual input and partial text against three significantly larger VLMs: BLIP-base (223M), GIT (350M), and Kosmos-2 (1.3B). These models contain approximately 10x, 14x, and 50x times more trainable parameters than NanoVLM-large, respectively. Figure 7 presents a qualitative comparison between NanoVLMs and these large SOTA VLMs, analyzing their performance in the image description generation"}, {"title": "5. Conclusion", "content": "Our introduction of NanoVLMs, a highly efficient family of VLMs built from scratch, places a strong emphasis on minimizing parameters while preserving performance. By systematically optimizing each component (encoder, decoder, and connector), we develop NanoVLM variants that are significantly smaller than conventional VLMs, raising fundamental questions about the training process and data requirements for building such models. To explore these aspects, we employ LongDesc and ShortDesc, highlighting the need for a more complex architecture to generate extended narratives compared to concise ones.\nWhile our findings demonstrate that even with a small dataset, a well-designed, small-scale VLM can achieve competitive results, certain limitations remain. Increasing the dataset size could further enhance the model's generalization capabilities, particularly for long-form descriptions. Additionally, the model's ability to generalize to more complex domains or handle fine-grained visual reasoning requires further exploration. Optimizing for compactness may also introduce trade-offs in multimodal alignment, potentially impacting performance on tasks requiring deep semantic understanding. Finally, although our evaluation provides structured insights into model performance, a more extensive human evaluation could help assess fluency, coherence, and real-world applicability. This work lays the foundation for developing ultra-compact yet effective VLMs, making them more practical, accessible, and adaptable to real-world applications."}]}