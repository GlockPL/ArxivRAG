{"title": "Language-specific Calibration for Pruning Multilingual Language Models", "authors": ["Simon Kurz", "Jian-Jia Chen", "Lucie Flek", "Zhixue Zhao"], "abstract": "Recent advances in large language model (LLM) pruning have shown state-of-the-art compression results in post-training and retraining-free settings while maintaining high predictive performance. However, such research mainly considers calibrating pruning using English text, despite the multilingual nature of modern LLMs and their frequent uses in non-English languages. In this paper, we set out to explore effective strategies for calibrating the pruning of multilingual language models. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse tasks, models, and state-of-the-art pruning techniques. Our results present practical suggestions, for example, calibrating in the target language can efficiently yield lower perplexity, but does not necessarily benefit downstream tasks. Our further analysis experiments unveil that calibration in the target language mainly contributes to preserving language-specific features related to fluency and coherence, but might not contribute to capturing language-agnostic features such as language understanding and reasoning. Last, we provide practical recommendations for future practitioners.", "sections": [{"title": "1 Introduction", "content": "State-of-the-art language models often rely on over-parameterization with millions or billions of parameters, resulting in significant memory and computational demands (Zhang et al., 2017; Allen-Zhu et al., 2019; Xu and Du, 2023). Pruning is a model compression technique that removes unimportant weights to reduce memory footprint and inference computation (Gholami et al., 2022; Hoefler et al., 2021; Kuzmin et al., 2023). Recent pruning methods for language models, such as SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2024), demonstrated state-of-the-art performance in a post-training and retraining-free setting (Zhu et al., 2023). The pruning process involves passing a small number of examples, i.e. calibration data, to the model to determine the importance of weights for subsequent pruning (Kuzmin et al., 2022; Frantar and Alistarh, 2023; Kuzmin et al., 2023).\nNotably, existing studies typically use calibration data in English and evaluate the post-pruning performance in English. On the other hand, most state-of-the-art LLMs are multilingual and frequently employed for tasks in non-English languages (Touvron et al., 2023; Achiam et al., 2023; Jiang et al., 2023). It remains unclear how to calibrate pruning to optimize the post-pruning performance of multilingual LLMs on tasks in non-English languages. In other words, the current literature provides little insight into efficient calibration strategies for multilingual LLMs targeting specific non-English languages. For example, if we aim to prune a multilingual LLM and use the pruned LLM for tasks in German, should we calibrate the compression process with text in English, German, or both?\nThis research, as the first, presents an extensive study investigating the impact of calibration data language on pruning LLMs for language-specific tasks. We recognize the significant value of our study, particularly in resource-constrained scenarios and for languages with limited resources. The main takeaways are the following:\n\u2022 Calibrating on the target language consistently yields the lowest perplexity, but will not guarantee optimal performance on downstream tasks.\n\u2022 Pruning re-orders the strength language of the multilingual model. That is, the model sacrifices performance in some strength languages for others after the pruning."}, {"title": "2 Related Work", "content": "2.1 Multilingual Language Models\nMost current state-of-the-art LMs, such as Llama-3 (Meta AI, 2024) and Phi 3(Abdin et al., 2024), are trained on multilingual data, enabling them to understand and generate text in multiple languages (Huang et al., 2023; Holmstr\u00f6m et al., 2023; Xu et al., 2024; Meta AI, 2024). Although these multilingual LMs follow the general training paradigm of training monolingual LMs, they are often found to behave differently or show unique features from monolingual models (Xu et al., 2024). For example, Deng et al. (2024) reveals that multilingual LMs are prone to generate unsafe outputs on malicious instructions in non-English languages, i.e. multilingual jailbreak. Wang et al. (2023) also identify that all LLMs tested produce significantly more unsafe responses for non-English queries than English ones. Furthermore, previous work on model explanation finds that multilingual models are different from counterpart monolingual models in terms of their internal processes, that is, different importance distributions on the same input (J\u00f8rgensen et al., 2022; Zhao and Aletras, 2024). A general conclusion drawn from these studies is that findings from monolingual settings, particularly those in English, are unlikely to hold in multilingual settings involving non-English languages.\n2.2 Calibration of Post-training Pruning\nUnlike sparse training (Yuan et al., 2021; Hoang and Liu, 2023; Zhang et al., 2023) or pruning-aware training (Liu et al., 2021), which require iterative training to achieve sparsity, post-training pruning (Frantar and Alistarh, 2023; Sun et al., 2024) does not require training but eliminates redundant weights based on the importance of weights that are calculated with calibration data. The retraining-free feature of post-training pruning makes it a more efficient approach for LLMs.\nPrior research has primarily focused on calculating weight importance to optimize the performance of pruned models, examining importance metrics and importance ranking ranges (Frantar and Alistarh, 2023; Sun et al., 2024; Zhang et al., 2024). Two studies have investigated the impact of calibration data, looking at the quantity of calibration data (Zhang et al., 2024) and the sources of the data (Williams and Aletras, 2023). These studies have been limited to English. The effects of pruning on the multilingual capabilities of language models, and the impact of the language of calibration data on performance in other target languages remain unknown."}, {"title": "3 Methodology", "content": "To investigate the impact of the language of calibration data on the pruned model, we compare the performance (perplexity, and downstream tasks) in a range of languages between pruned models and their counterpart full-sized models, where different calibration data are applied.\n3.1 Models\nOur experiments use two State-of-the-Art (SotA) LLM families: Llama-3 (Meta, 2024), the open-source SotA at the time of writing, and Aya-23 (Aryabumi et al., 2024), renowned for its multilingual pre-training. As our evaluation focuses on instructed generation tasks, also following Chrysostomou et al. (2023), we employ the instruction-tuned versions of both families. Consequently, our experimental setup includes Llama-3-instruction models in 8B and 70B parameter sizes, alongside Aya-instruction models in 8B and 35B parameter sizes.\n3.2 Languages\nWe include seven languages in our study: Arabic (AR), German (DE), English (EN), Spanish (ES), Russian (RU), Swahili (SW), and Chinese (ZH). This selection spans six language families, four writing systems, and encompasses both high and mid-resource languages. We include Swahili as an outlier calibration language, as neither Llama-3 or Aya-23 includes it in their pre-training corpora. A summary of languages used in our paper is given in Appendix A.\n3.3 Pruning\nCalibration We construct language-specific calibration sets from mC4 (Raffel et al., 2019) for"}, {"title": "3.4 Evaluation Tasks", "content": "We compare the performance of the pruned models calibrated on different languages using the perplexity on a subset of the mC4 validation set, and a selection of downstream tasks in different target languages: MMLU(Hendrycks et al., 2021), MKQA(Longpre et al., 2020), and Belebele(Bandarkar et al., 2023). To better isolate the impact of the calibration language, we also chose mARC and mHellaSwag (Dac Lai et al., 2023) to compare with their original versions ARC(Clark et al., 2018) and HellaSwag(Zellers et al., 2019) in English for their low sensitivity to the choice of calibration samples (Williams and Aletras, 2023). These tasks primarily assess commonsense reasoning, reading comprehension, and question answering using multiple choice questions. Unless stated otherwise, we evaluate in a zero-shot setting.\n3.5 Implementation Details\nWe adopt the code from Sun et al. (2024) for implementing model pruning. We use EleutherAI Evaluation Harness (Gao et al., 2024) for a robust and reproducible evaluation. We depend on Huggingface (Wolf et al., 2020) for loading datasets and models. All experiments are conducted with two NVIDIA A100 GPUs."}, {"title": "4 Results", "content": "4.1 Perplexity\nWhich calibration language would lead to lower perplexity?\nTable 1 presents the perplexity scores in different languages for pruned models. The column headers denote the evaluation languages, while the row headers indicate the calibration languages. Our findings reveal that there is no universal calibration language that consistently minimizes perplexity across all evaluation languages. In other words, the optimal calibration language to achieve lower perplexity varies depending on the target language being evaluated. Notably, as indicated by the darker diagonal pattern, calibrating on the target language itself consistently yields the lowest perplexity. The only exception occurs when evaluating Llama-3 pruned with Wanda on Chinese. In this case, the optimal calibration language is Russian, with Chinese as the second-best option. The perplexities of these two calibration languages are closely aligned, i.e., 24.4 for Chinese and 23.6 for Russian."}, {"title": "4.2 Downstream Task Performance", "content": "Which calibration language would lead to better downstream-task performance?\nTable 2 shows the models' performance on downstream tasks. The column headers denote the evaluation languages, while the row headers indicate the calibration languages. First, for both pruning methods, the choice of calibration language affects downstream task performance. This impact is particularly pronounced when pruning Llama-3 8B model with SparseGPT or Aya-23 models with either Wanda or SparseGPT, as evidenced by the greater color difference in the table, compared to pruning Llama-3 8B with Wanda (top quarter). For example, when evaluating Llama-3 8B model on Belebele in English, pruning with Wanda results in performance ranging from 58.5 with Spanish calibration to 65.0 with English calibration. In contrast, when pruning Llama-3 8B with SparseGPT the performance varies from 50.0 with Chinese calibration to 71.4 with English calibration.\nUnlike the perplexity evaluation in 1, calibration in the target language does not reliably result in good performance. For instance, on Belebele, the pruned Aya-23 8B model mainly achieves higher accuracy on evaluation languages other than on its calibration languages. Overall, of 50 evaluation groups, in 26 group comparison cases, calibration in the target language yields the best performance, 8 for Wanda and 18 for SparseGPT (e.g. Arabic on ARC and MMLU). For the remaining cases, calibration in the target language achieves the second best or comparable performance. Therefore, calibrating using the target language often results in acceptable, though not consistently the best, performance. Given the standard deviations shown in Table 6 in Appendix B, calibration in the target language is a reasonable choice in terms of downstream performance.\nIn terms of downstream language performance, the full-sized baseline models exhibit the strongest performance in English, followed by other Latin languages, with Russian next in line. Arabic and Chinese generally perform worse. Further, pruning can sometimes disrupt the original ranking of languages observed in the baseline models. For example, on the Belebele benchmark, the Llama-3 8B model achieves a baseline accuracy of 55.3 for Arabic and 44.8 for Chinese, but the pruned models reverse this trend and achieve an accuracy of 45.1 for Arabic and 57.0 for Chinese. That is, pruning can shift which languages the model performs best or worst on."}, {"title": "Calibrating in an outlier language could enhance generalization for downstream in non-English?", "content": "We include Swahili as an outlier language, which Llama-3 and Aya-23 do not include in the pre-training corpora. In column-wise comparison, the SW cell is consistently the darkest or relatively dark one. That is, in most cases, calibration in Swahili results in the worst or the second-worst performance across different calibration language strategies, given a downstream task.\nWould calibrating in a similar language be helpful?\nThere was no consistent pattern based on the similarity of calibration-evaluation language pairs. For example, Latin language pairs such as English-Spanish (calibrating in English and evaluating in Spanish) or pairs from the same language family like English-German did not always perform well, while pairs with different writing systems, such as Russian-English or Spanish-Arabic, did not consistently perform poorly. On the other hand, calibration in a dissimilar language, i.e. Chinese, often results in particularly low accuracy across many tasks and evaluation languages, as demonstrated by the darker row of \"ZH\".\nTo what extent, do the model and pruning method matter?\nBetween Llama-3 8B and Aya-23 8B, despite their identical decoder-only architecture, Table 2 highlights distinct performance patterns between the two models under investigation. With its larger vocabulary size in the six non-English languages, Aya-23 8B generally outperforms the Llama-3 8B model in most evaluation languages and tasks, both before and after pruning. Notably, Aya-23 8B experiences less performance drop after pruning but shows less stable results, often performing better on languages other than the one used for calibration. Overall, these results suggest that vocabulary size and pre-training data impact baseline performance and accuracy after pruning in a multilingual context.\nBetween Wanda and SparseGPT, Llama-3 8B's performance degrades less after pruning with SparseGPT pruning, while the performance of the pruned Aya-23 8B model varies, that is, excelling with Wanda or SparseGPT depends on the task. Specifically, SparseGPT produces a distinct diagonal pattern for Aya-23 8B on ARC and HellaSwag, while Wanda often yields superior performance for calibration in non-target languages on Belebele"}, {"title": "4.5 Multiple Calibration Languages", "content": "Will incorporating more languages in the calibration, i.e.bilingual and multilingual calibration set, benefit the performance of pruned multilingual models?\nWe repeat the experiments but include more than one language in the calibration set. As English is the dominant language in pre-training data, we first test the combination of English and the target language for a bilingual calibration setup. We further experiment with including all seven languages in the calibration, i.e. multilingual setup. For all setups, the total calibration sample number remains the same, i.e. 128.\nComparing Table 4 with Table 2, we observe that bilingual and multilingual calibration sets generally outperform monolingual calibration for non-English target languages. For example, for Chinese on ARC, the seven-lingual calibration set, AR-DE-EN-ES-RU-SW-ZH, often yields higher accuracy than calibration in Chinese, i.e. the target language, across models and pruning methods. That is, AR-DE-EN-ES-RU-SW-ZH achieves 28.3 (Wanda, Llama-3 8B), 30.0 (SparseGPT, Llama-3 8B), 32.0 (Wanda, Aya-23 8B) and 30.9 (SparseGPT, Aya-23 8B), consistently higher or comparable to monolingual calibration set, i.e. 27.0, 30.1, 30.8 and 31.7 respectively. On the other hand, multilingual calibration pruning often degrades the downstream performance in English, particularly on ARC when pruning Aya-23 with SparseGPT and on MMLU when pruning Llama-3 with Wanda. Interestingly, the best bilingual combination does not necessarily contain the target language but English. For example, for Russian on ARC when pruning with Wanda, calibration in EN-ES and AR-EN leads to the optimal performance for Llama-3 8B and Aya-23 8B."}, {"title": "5 Analysis", "content": "Inspired by Xie et al. (2022), we explore how pruning affects language-specific and langauge-agnostic features in multilingual LLMs. Utilizing Low-rank Subspace for language-Agnostic Representations (LSAR) by Xie et al. (2022), we disentangle language-specific syntactic features from semantic language-agnostic features to investigate pruning behavior across different calibration languages for the final layer's output embeddings. The dominance of language-specific elements within embeddings allows us to distinguish embeddings of various languages in low dimensional space through an SVD. Projecting back from this reduced space, using the most significant singular values, into the original embedding space provides the language-specific features $s$. By subtracting $s$ from the original embedding $e$, we isolate the language-agnostic features, $a = e - s$.\nFigure 1 compares the magnitude of the difference of the language-agnostic features (a) and language-specific features (b) of the final layer's embeddings before and after pruning with SparseGPT on Llama-3 8B. First, the language-agnostic features remain similar among pruned models of different calibration languages when evaluated in different downstream languages (x-axis), whereas language-specific features changed noticeably as different languages were used for calibration. Particularly, as shown in sub-figure (b), calibrating in Germany results in the smallest change of language-specific features when evaluating in German, compared to calibration in other languages. We also observe similar patterns in Spanish and Swahili. We hypothesize that calibration in the target language helps retain language-specific features, which might benefit coherent and fluent text generation. This might be the reason behind the diagonal pattern observed in Table 1 in Section 3.5 that calibration in the target language results in the lowest perplexity. On the other hand, while these language-specific features enhance fluency, they are unlikely to improve the model's understanding"}, {"title": "6 Recommendations", "content": "We summarize our findings into three recommendations for pruning multilingual language models to achieve optimal performance in specific languages.\nChoice of Calibration Language For a given target language, it is advisable to calibrate the pruning of the multilingual model specifically in that target language, even if the primary pre-training language of the model is English. However, such calibration in the target language may result only in optimal perplexity and reasonable downstream performance, particularly in tasks requiring deep language understanding and reasoning, not just superficial coherence and fluency.\nChoice of Pruning Method Between Wanda and SparseGPT, there is no better one across model families, tasks, and languages. In general, Wanda is recommended for Aya-23 models and SparseGPT for the Llama-3 models.\nEvaluation The perplexity score that is often used for evaluating pruning performance can be misleading for downstream task performance. For"}, {"title": "7 Conclusion", "content": "In this paper, we empirically demonstrate that the choice of calibration language influences the downstream performance of pruned models across various languages. Particularly, calibration in the target language mainly benefits preserving perplexity performance but does not necessarily help maintain the downstream task performance. Further investigation into this phenomenon by analyzing the pruning impact on hidden states indicates that calibrating in the test language does not reliably benefit layers associated with semantic understanding. Additionally, we recommend practitioners do not depend on perplexity for accessing the pruned model, or performance in English to estimate the performance on target languages."}, {"title": "8 Limitations", "content": "Generality of Findings. Due to resource constraints, we predominantly experimented with the small versions of Llama-3 and Aya-23, and validated our findings with fewer pruning runs on their counterpart large version. Since our results translate between model families, and to bigger model sizes, we assume a certain degree of generalization. Nonetheless, other models trained with different techniques or for other tasks might show different behavior. Given the pace of this research field, it is also unclear whether these results translate to future models.\nUnderrepresented Languages. Our experiments focused on languages with sufficient model and downstream task support. However, this selection does not encompass all languages of interest, particularly mid and low-resource languages that are underrepresented in the pretraining, and challenging to evaluate due to the lack of benchmark support. Future research could benefit from including more languages to explore the interplay between different language families or writing systems and performance after pruning."}]}