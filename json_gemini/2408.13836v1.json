{"title": "PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images", "authors": ["Zifan Chen", "Xinyu Nan", "Jiazheng Li", "Jie Zhao", "Haifeng Li", "Zilin Lin", "Haoshen Li", "Heyun Chen", "Yiting Liu", "Bin Dong", "Li Zhang", "Lei Tang"], "abstract": "Background: Volumetric segmentation is pivotal for medical imaging applications but is often hindered by the extensive manual annotations and model training specific to each medical scenario. Additionally, current general segmentation models underperform and suffer inefficiencies due to their architectural and inferential strategies. There is an urgent need in clinical practice for a segmentation model that is both high-performing and efficient for segmenting any 3D object across various modalities in medical images.\nMethods: We introduce PropSAM, a propagation-based segmentation model that leverages the continuous flow of information within 3D medical structures. PropSAM combines a CNN-based UNet architecture for intra-slice information processing with a Transformer-based attention module to facilitate inter-slice propagation. This methodology is designed to enhance segmentation effectiveness across various imaging modalities by focusing on structural and semantic continuities rather than isolating specific objects. Built upon this innovative framework, PropSAM is engineered to support a one-view prompt, such as a 2D bounding box or a 2D sketch mask, differentiating it from the conventional two-view prompts typically used.\nResults: PropSAM demonstrated superior performance on 44 diverse medical datasets, notable improving the dice similarity coefficient (DSC) for hundreds of segmentation object types and multiple medical imaging modalities. Compared to popular models like MedSAM and SegVol, PropSAM achieved an average DSC improvement of over 18.1%, while maintaining stable predictions despite prompt deviation (one-way ANOVA test, P > 0.5985) and varying propagation configurations (one-way ANOVA test, P > 0.6131). Due to its efficient architecture and inference strategy, PropSAM exhibited significantly faster inference speeds (Wilcoxon rank-sum test, P < 0.001) than existing models. The one-view prompt used by PropSAM also enhanced human prompt efficiency, reducing interaction time by about 37.8% compared to the two-view prompts required by existing methods. Moreover, thanks to its generalized learning focus on structural or semantic propagation relationships between slices, PropSAM displayed robust performance on unseen objects. It demonstrated a notable advantage in handling irregular and complex objects, exhibiting a linear relationship where DSC improvements are negatively correlated (r < -0.1249) with the degree of object irregularity. The source code and supplementary materials are available at Github\u00b9.\nConclusions: PropSAM represents a significant advancement in medical image segmentation, offering a tool that is both generalizable and versatile, with an efficient, user-friendly design. Its potential to quickly adapt to new tasks with minimal retraining underscores its promise for clinical applications, potentially paving the way for more automated and reliable medical imaging analyses.", "sections": [{"title": "1 Introduction", "content": "Volumetric segmentation is an essential task in the analysis of medical imaging [1], which entails the identification and delineation of regions of interest (ROIs) in various objects such as organs, lesions, and tissues within three-dimensional (3D) medical images. Accurately segmenting these objects across various medical imaging modalities\u2014including computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography-computed tomography (PET-CT), and micro-computed tomography (micro-CT)\u2014is crucial for a multitude of clinical applications. These include disease diagnosis [2, 3, 4], surgical and treatment planning [5, 6], monitoring disease progression [7, 8, 9], and therapy optimization [10, 11]. In most current clinical scenarios [12, 9, 8], manual segmentation within 3D medical images remains the predominant method for delineating anatomical structures and pathological areas. This process is not only time-consuming and labor-intensive but also requires precise segmentation across a diverse objects and imaging modalities [13]. Consequently, there is a pressing need to develop semi-automatic or fully automatic segmentation algorithms capable of handling any medical imaging modality and object. Such advancements are expected to greatly decrease the time and labor involved for segmentation tasks, while also improving the consistency of the delineations [14, 15].\nTo address these challenges, over the past decade, deep learning-based models have shown great promise in medical image segmentation, owing to their capacity to learn complex image features and achieve precise segmentation across a variety of tasks [16, 1, 17]. However, these successful models are often specifically designed for certain segmentation challenges, a necessity driven by the distinctive demands of various medical imaging modalities and the intricate anatomy of the target objects. Typically, these task-specific models require the assembly of large, meticulously annotated datasets for each new task, where medical experts carefully delineate the ROI for each specific object and modality [18, 19, 20, 21]. While these models frequently deliver high accuracy and robust performance within their designated settings, their reliance on extensive annotated datasets constrains their adaptability and scalability. For every new object or modality, a similar process must be repeated: data collection, manual annotation by medical experts, and model training [1], which is not only resource-intensive but also impractical for addressing emergent medical scenarios or rare pathologies. The considerable cost associated with data annotation and the scarcity of expert annotations further compound these challenges, hindering the broad deployment of these models across diverse clinical environments. Consequently, there is an urgent need for more generalized models that can overcome these limitations, offering the flexibility and rapid adaptability to new tasks without the necessity for repetitive, extensive training on narrowly defined datasets [22].\nRecently, the concept of foundation models has gained prominence in the field of natural image processing, where models like the segment anything model (SAM) [23, 24] have demonstrated remarkable generalization capabilities across diverse tasks, thanks to training on vast datasets. The SAM model, trained on extensive and varied image sets, enables precise segmentation with minimal user prompts such as points, bounding boxes, and masks, effectively segmenting any object within an image. Inspired by this success, researchers have begun adapting these versatile frameworks to the medical imaging domain through two primary types of models [13, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]. The type I model (Figure 1b), often exemplified by MedSAM [13, 35], directly applies the SAM approach to various two-dimensional (2D) medical images. By"}, {"title": "2 Results", "content": null}, {"title": "2.1 PropSAM: a propagation-based model for segmenting any 3D objects across multiple medical imaging modalities", "content": "The need for an effective volumetric segmentation model in medical imaging is paramount, typically initiated by a user interactive prompt that guides the workflow. Existing models can be categories into type I and type II models, as illustrated in Figure 1b-c. While these models have demonstrated commendable performance in many scenarios, they possess inherent limitations. Type I models fail to fully address the 3D complexities of medical imaging, whereas type II models, although promising for volumetric analysis, still confront challenges related computational efficiency and a heavy reliance on 3D annotations to fully leverage model capabilities. In contrast, the inherent continuity of information in 3D medical structures offers an opportunity to exploit 2D models to learn inter-slice propagation relationships, balancing the model's performance with computational efficiency. Based on this insight, we proposed PropSAM, a propagation-based segmentation model for any 3D objects, also referred to as the type III model in Figure 1d. PropSAM focuses on learning the propagation of information across 2D slices in 3D medical images rather than on specific segmentation objects. This approach not only broadens the model's general learning tasks but also enhances its general segmentation capabilities. It effectively disseminates knowledge based on the user's prompt on a slice, achieving efficient and precise 3D segmentation.\nAs depicted in Figure 2, the workflow of PropSAM starts with a physician reviewing a 3D medical image and providing prompts within a slice for the target objects. PropSAM supports two style of prompts: bounding boxes and sketch-based masks (refer to Figure 2a, Supplementary Text S1.1, and Supplementary Figure S2). When a bounding box is used, the Box2Mask module exectues a foreground segmentation within the box, standardizing the input prompt format as sketch-based masks for subsequent modules. Within PropSAM, the slice annotated by the physician is termed the 'guiding slice,' and the corresponding prompt is known as the 'guiding prompt.' The PropMask module then plays a crucial role in utilizing"}, {"title": "2.2 Data characteristic and preprocessing", "content": "In this study, we compiled 44 3D medical image datasets (Supplementary Table S1), consisting of 43 publicly available datasets and one micro-CT dataset from our clinical practice. These datasets encompass a range of medical imaging modalities, including CT, MRI, PET-CT, and micro-CT, covering 168 different target object types and totaling 1,645,871 3D objects for experimental analysis (see Figure 3a). The diversity of these 44 datasets is categorized across five dimensions (Supplementary Table S4): number of 3D scans, number of voxels, size anisotropy, spacing anisotropy, and variety of object types. This multidimensional diversity is essential for a comprehensive evaluation of PropSAM, as illustrated in Figure 3b. Size anisotropy is defined, following nnUNet [1], as the ratio of the smallest to the largest size in 3D scans, while spacing anisotropy is calculated as the ratio of the smallest to the largest spacing in 3D scans. In alignment with the protocol established in MedSAM [13], we partitioned these datasets into 34 internal datasets (D01\u2013D34) for training and validation, and ten external datasets (D35-D44) for independent testing (Supplementary Tables S2-S3).\nAs mentioned in the preceding Section 2.1, PropSAM comprises two main modules: the Box2Mask and PropMask modules. For training and evaluating the Box2Mask module, a 2D architecture model (detailed further in Sections 4.1-4.2), we processed 3D images and their 3D annotations through three steps: 1) simulating bounding boxes based on 3D masks to extract ROI images; 2) normalizing these ROI images; 3) applying random data augmentation to enhance the training data (details in Supplementary Text S1.3.2). Following this preprocessing, we obtained a total of 19,344,368 samples (2D medical image-mask pairs), comprising 14,974,620 training samples, 3,782,206 interval validation samples, and 587,542 exterval validation samples. Additionally, for training and evaluating the PropMask module, a 2D architecture model that receives both guiding slice and prompt and adjacent slices as inputs, we furhter preprocessed 3D images with their 3D annotations as follows: 1) determining the cropped size to extract both the guiding and adjacent slices, thereby constructing ROI tasks; 2) normalizing these ROI tasks; 3) employing random data augmentation to enhance the training of these ROI tasks (further details in Supplementary Text S1.4.2). Following these preprocessing steps, we amassed a total of 1,345,871 tasks (a guiding slice, a guiding prompt, and several adjacent slices), with 1,020,576 training tasks, 258,889 interval validation tasks, and"}, {"title": "2.3 PropSAM exceeds the segmentation performance of existing models", "content": "We evaluated two version of PropSAM, PropSAM-2DBox which accepts bounding-box-style (style 1) prompts, and PropSAM-2DMask which receives mask-style (style 2) prompts, against two popular existing models, MedSAM and SegVol, on both internal and external datasets. Unlike PropSAM-2DBox, which requires only a single-view prompt such as a 2D bounding box, both MedSAM and SegVol necessitate two-view prompts (typically one bounding box on the axial plane and another on the orthogonal plane) within volumetric medical images. These two-view prompts form the tightest possible 3D bounding boxes of the segmentation targets, restricting the inference to the given slice area (Supplementary Figure S2). Moreover, MedSAM, which is originally designed for 2D medical image segmentation, necessitates processing each 2D medical slice containing the segmentation targets individually. The results from these segmentations are then stacked to form a volumetric final 3D segmentation, following guidance from its offical Github \u00b2. We refer to this process as 'slice-by-slice prediction'.\nIn contrast, SegVol directly segments volumetric medical images and empolys a 'zoom-out-zoom-in' strategy using resized global image and cropped patches as inputs to balance the acquisition of both global and local image features. We refer to this process as 'patch-by-patch prediction'.\nAs illustrated in Figure 4a, our two proposed PropSAMs both exhibit superior segmentation performance (Supplementary Table S5), evaluated by the dice similarity coefficient (DSC), across various experimental datasets and achieve DSCs of 0.95 or 0.95+ on several segmentation objects (e.g. DSC=0.963 for livers, DSC=0.950 for kidneys, and DSC=0.950 for pancreas). Specifically, PropSAM-2DBox and PropSAM-2DMask both achieved higher DSCs on 31 of the 34 internal datasets and all ten external datasets compared to MedSAM and SegVol. Overall, PropSAM-2DBox achieves an average DSC that is 19.7% higher than MedSAM and 18.1% higher than SegVol across all datasets. Similarly, PropSAM-2DMask demonstrates an average DSC that is 25.1% higher than MedSAM and 23.6% higher than SegVol. These results indicates the proposed PropSAMs' robust performance, while demonstrating outstanding performance on external validation datasets. Furthermore, we observed that MedSAM does not demonstrate superior performance on any 3D segmentation tasks due to its 'slice-by-slice prediction'. Besides, SegVol, while showing good performance on organ-related segmentation objects (e.g. DSC=0.941 for livers, DSC=0.912 for kidneys, and DSC=0.842 for pancreas), exhibits a notable decrease in performance on lesion-related or tissue-related segmentation objects (e.g. DSC=0.189 for whole-body lesions, DSC=0.001 for white matter hyperintensities, and DSC=0.080 for glomeruli). These limitations of SegVol stem from the general challenges of 3D segmentation models in limited medical image data, and the 'patch-by-patch prediction' may cause fine information loss and discontinuity, posing challenges in predicting lesions with rare annotations and variable shapes. In contrast, both PropSAM-2DBox and PropSAM-2DMask can accurately segment organ-related, lesion-related and tissue-related segmentation objects. For example, their DSCs are 0.669 and 0.755 for whole-body lesion, 0.443 and 0.569 for white matter hyperintensities, and 0.769 and 0.874 for glomeruli. This indicates the exceptional generalization capabilities of PropSAM, stemming from its ability to learn"}, {"title": "2.4 PropSAM demonstrates superior inference and interaction efficiency compared to existing models", "content": "We conducted a comprehensive evaluation of the inference times of PropSAMs, MedSAM and SegVol across all datasets. As illustrated in Figure 4b, MedSAM exhibits the slowest inference speeds (the longest inference times), while SegVol shows a speed improvement over MedSAM. However, our proposed PropSAMs, in both the 2DBox and 2DMask versions, consistently achieve the fastest inference speeds (the least inference times). The right side of Figure 4b visually details the specific inference time comparisons across the 44 datasets, with PropSAMs outperforming in nearly all cases (Supplementary Tables S6-S7).\nThe superior inference speed of PropSAMs can primarily be attributed to two factors: model structure design and inference strategy design. Existing models are generally based directly or largely on the successful SAM models in natural imaging, which employ purely Transformer-based architectures adept at modeling long-distance relationships. However, we observed that in medical images, particularly in segmentation tasks, local information is equally important and contains many anatomical structures and details necessary for accurate segmentation. Long-distance relationships generally exist between different slices of the same anatomical object. Therefore, our PropSAM utilizes a CNN-based architecture, similar to UNet [16], for precise location segmentation while leveraging the attention mechanism in Transformer architectures [36] to model information propagation between slices. This approach not only allows PropSAM to utilize architectures that have proven successful in medical segmentation [16, 1] but also reduces the model's parameter count (a total of 32.48 M parameters and 53.1 M parameters when combined with Box2Mask module for supporting bounding-box prompts). In terms of inference strategy, MedSAM, a type I model, predicts each slice individually with a complex Transformer model in response to a prompt, requiring extensive computational resources (Figure 1b). SegVol, a type II model, adopts a inference process similar to the 3D-nnUNet model, predicting individual patches with dense overlapping strides that are later merged. This not only increases the computational cost due to the 3D model's inherent complexity but also leads to increased prediction overhead and potential artifacts during patch merging due to the small stride in sliding window patch prediction (Figure 1c). In contrast, PropSAM, belonging to type III models, requires only the parameter count and computational load of a 2D model and allows bidirectional parallel inference without the need for overlapping window slides (Figure 1d). Consequently, PropSAM offers a more efficient pipeline for segmenting any 3D objects, making it notable more efficient than existing models.\nFurthermore, we explored the interaction efficiency of different models. Both MedSAM and SegVol require two-view prompts, whereas PropSAM-2DBox only necessitates interaction in one view. In our extracted test subset (see Supplementary Text S2 for more experimental details), an experienced radiologist interacted with different datasets and objects using the various interaction prompts. We recorded the time taken for each interaction and compared the different prompt types. As demonstrated in Figure 4c, the one-view box prompt of PropSAM took significantly less times than the most common two-"}, {"title": "2.5 PropSAM exhibits predictive stability and consistency", "content": "As mentioned in Section 2.4, PropSAM operates with one-view prompts, typically seletected by physicians according to response evaluation criteria in solid tumors (RECIST). To assess the impact of variations in prompts provided by different physicians on PropSAM's performance, we conducted an ablation study. As depicted in Figure 4d-e, we simulated deviations from the RECIST-standard confirmed largest slice through five experimental groups, which included deviations of 0% (no deviation), \u00b15%, \u00b110%, \u00b115%, and \u00b120%. Both PropSAM-2DBox and PropSAM-2DMask demonstrated stable DSC across these variations, as confirmed by one-way ANOVA tests, with P-values of 0.6736 and 0.5985, respectively (see Supplementary Figure S9 and Supplementary Table S8 for further details). While performance slightly declined with increasing deviations, it is important to note that a deviation of \u00b120%, which corresponds to a total range of 40%, is uncommon in clinical practice. Even with such substantial deviations, PropSAMs maintained commendable performance.\nMoreover, during the inference process, PropSAMs iteratively selects the most marginal predicted slice as the next round's guiding slice and guiding prompt. The distance of this slice from the original guiding slice could potentially affect the accuracy of subsequent predictions, particularly when far apart, as the propagation relationship weakens with distance. This influence may be amplified through iterative propagation, impacting the overall 3D segmentation of the object. To evaluate the impact of propagation slice thickness, we conducted another ablation study with propagation thicknesses of 10 mm, 20 mm, 30 mm, and 40 mm. As shown in Figure 4f-g, PropSAMs maintained predictive stability and consistency across these different thicknesses, as assessed by one-way ANOVA tests with P-values of 0.7114 and 0.6131, respectively (refer to Supplementary Figure S10 and Supplementary Table S9 for more details). Based on these findings, we empirically selected 20 mm as the default propagation thickness for PropSAMs.\nIn summary, through ablation experiments involving varied prompt deviations and propagation thicknesses, PropSAMs have demonstrated notable predictive stability and consistency. This provides a reliable foundation for the clinical application of PropSAMs."}, {"title": "2.6 PropSAM's superior efficacy in segmenting complex and irregular objects", "content": "We visualized the qualitative segmentation results of different models as shown in Figure 5a. Our proposed PropSAMs effectively utilize propagation information between slices, resulting in visually complete and smooth segmentation outcomes. In contrast, MedSAM, which employs a 'slice-by-slice prediction' and merging strategy, and SegVol, which uses 'patch-"}, {"title": "2.7 PropSAM demonstrates strong generalization and adaptability across diverse segmentation tasks", "content": "We further explored PropSAM's generalization capabilities from two perspectives: model fine-tuning and training from scratch. Firstly, as depicted in Figure 6a, PropSAM, serving as a general segmentation model, outperforms MedSAM on ten external datasets, demonstrating its robust generalization ability to unfamiliar datasets and objects. We then partitioned these ten datasets into training and validation sets and conducted minimal fine-tuning of PropSAM on the training sets to create the PropSAM-2DMask-Finetuned model. Experiments indicate that with minimal data fine-tuning, PropSAMs can quickly adapt"}, {"title": "3 Discussion", "content": "Volumetric segmentation plays a pivotal role in medical imaging, serving essential downstream tasks such as disease diagnosis, surgical planning, and therapy monitoring. Traditional approaches typically involve extensive manual annotation by experts to train specialized segmentation models for specific medical scenarios. This process is costly, inefficient, and struggles to adapt to the expanding scale of clinical demands. With the advent of large model technologies, particularly the emergence of SAM, there is hope for achieving general segmentation capabilities. These models, trained on large-scale data, exhibit robust segmentation with minimal human interaction and maintain impressive generalization, especially for unseen objects. Such capabilities perfectly align with clinical needs, potentially accelerating the delineation process, rapidly accumulating high-quality annotated data, and enhancing the stability of downstream analysis tasks.\nCurrent general segmentation models primarily draw from the success of SAM in natural image processing, categorized into two types based on basic model architecture and inference strategy differences. The first type (type I models) adapts SAM's 2D model architecture directly to medical images through extensive training or fine-tuning in medical images, exem-"}, {"title": "4 Methods", "content": null}, {"title": "4.1 Data acquisition", "content": "We collected 44 public 3D medical segmentation datasets(see Supplementary Table S1) covering multiple modalities, such as CT, MR, PET-CT and micro-CT, to build a large-scale and comprehensive dataset for model training and validating(Figure 1a). Since those datasets we collected have been widely used as the training and validating data of universal and specialized medical images segmentation models, all the volumetric medical images present in our dataset possess high-quality annotations.\nAs Supplementary Table S4 and Figure 3b illustrated, we recorded these 44 in five dimensions: number of 3D scans, number of voxels, size anisotropy, spacing anisotropy, and variety of object types, which are essential for a comprehensive evaluation of PropSAMs. Following nnUNet [1], size anisotropy is defined as the ratio of the smallest to the largest size in 3D scans and spacing anisotropy is calculated as the ratio of the smallest to the largest spacing in 3D scans. Furthermore, in alignment with the protocol established in MedSAM [13], we partitioned these datasets into 34 internal datasets (D01\u2013D34) for training and validation, and ten external datasets (D35\u2013D44) for independent testing (Supplementary Tables S2-S3).\nAs Supplementary Table S2 and S4 shown, our dataset contains 168 different categories of objects and 1,645,871 3D"}, {"title": "4.2 Data pre-processing", "content": "Firstly, to obtain bounding boxes for Box2Mask module, we generated a tightest bounding box on the slice where the corresponding foreground mask annotation contains over 100 pixels. Then we randomly adjusted the width and height of the bounding box with a scaling ratio between 1.0 to 1.25 to account for potential deviation in actual usage and used the processed bounding box as training data for Box2Mask module. Correspondingly, we constructed ROI tasks for PropSAM. We generated the tightest bounding box around the mask of the guiding slice and then randomly adjusted its width and height with a scaling ratio between 1.0 to 2.0 to capture the context around the target object. This adjusted bounding box was then used to crop both the guiding slice and the adjacent slices sampled within the propagation thickness, forming the cropped ROI tasks as training data for PropMask module.\nThe training data for both Box2Seg and PropSAM requires two steps of data pre-processing: images normalization and enhancement by random data augmentation. After acquiring the ROI images/tasks, we applied normalization to clip the intensity values to the range between the 0.5th and 99.5th percentiles of pixel values within the annotated mask of the original slice images, which represent the minimum and maximum values, respectively. The context of foreground ROI images/tasks were enhanced and emphasized after normalization. Finally, to optimize training efficiency, we applied offline data augmentation five times for each sample for Box2Mask module. Specifically, each image had a 50% chance of being flipped horizontally and vertically. Additionally, we randomly adjusted the image's brightness and contrast, also with a 50% probability, setting the adjustment ranges to [-0.2, 0.2]. The images were also rotated randomly up to 45 degrees with a 50% probability, filling any areas outside the original boundaries with a constant value (typically black). Otherwise, since the fundamental training unit of PropMask is a task, each containing several images (typically 20 adjacent images and one guiding image). Specifically, each image in a task had a 50% chance of being flipped horizontally or vertically and being rotated up to 45 degrees. These samples were uniformly resized to a resolution of 224 for input into the Box2Mask module and PropMask module.\nFollowing these pre-processing steps, we obtained a total of 19,344,368 samples for Box2Mask module and 1,345,871 tasks across 43 datasets comprising 284 objects (Figure ??a). According to the data partitioning in MedSAM, these data were divided into internal and external validation datasets. The internal validation dataset was further split into training and validation sets at an 80:20 ratio. There were 14,974,620 training samples, 3,782,206 internal validation samples, and 587,542 external validation samples for Box2Mask module (Supplementary Table S2) and 1,020,576 training samples, 258,889 interval validation tasks, and 66,406 external validation tasks for PropMask module (Supplementary Table S3). We trained PropSAM on the training set, with the internal validation set used to evaluate model performance and select the final model checkpoint. The external validation dataset served to demonstrate the robustness of PropSAM and its zero-shot capability with unseen objects and datasets."}, {"title": "4.3 Network architecture", "content": "PropSAM is composed of the Box2Mask module and the PropMask module. Both are built based on convolutional neural network (CNN), which have been the predominant network architecture in the field of computer vision for a long time and are more efficient compared to the commonly used Transformer architecture nowadays, making them suitable for a broader range of clinical application settings. And we note that the UNet-based architecture[16], particularly the nnUNet model[1], has become the most widely adapted and effective approach for medical imaging segmentation in recent years.\nTo convert ROI images, which are cropped according to bounding box prompts, to binary foreground masks (Figure 2a), we employed a six-stage encoder-decoder UNet-based network[16] as the Box2Mask module. The input utilizes three-dimensional channels, appropriate for a grayscale image replicated three times. The initial stage features 32 channels, which doubles with each subsequent stage, capping at 512. Thus, the channel counts across the six stages are [32, 64, 128, 256, 512, 512]. Each stage includes two convolutional layers, followed by instance normalization[45] and the activate function LeakyReLU. All convolutional kernels are 3, with a stride of one within each stage and a stride of two in the last layer of each stage for down-sampling. Additionally, to maintain low-frequency features, skip connections were employed to bridge each encoder level with its corresponding decoder. We also produced binary foreground segmentation predictions at all six decoding stages for deep supervision.\nWe obtained a 2D mask from two styles of prompts via Box2Mask module. The 2D mask serves as the segmentation for the initial guiding slice, and the PropMask module utilizes it to generate segmentation for the adjacent slices by propagating the segmentation of the guiding slice (Figure 2b). As the core component of the network and its architecture is largely based on UNet[16]. PropMask consists of an image encoder, a mask encoder, a sequence of cross-attention modules and a decoder. The image encoder and mask encoder are also six-stage CNN encoders, which are the same as the encoder of Box2Mask module, but the input channel of the mask encoder is one to accept the 2D mask prompt directly. Both the guiding slice and its adjacent slices go through the image encoder to produce support features and query features of six resolution([224 \u00d7 224,112 \u00d7 112,56 \u00d7 56,28 \u00d7 28, 14 \u00d7 14,7 \u00d7 7]), respectively. Similarly, the 2D mask from the guiding slice go through the mask encoder to produce mask features of six resolutions. Subsequently, a sequence of cross-attention modules are employed. Given a set of query vectors Q, key vectors K, and value vectors V, the definition of cross-attention is as follows:\n$Attention(Q, K,V) = Softmax(\\frac{Q \\circ K^{T}}{\\sqrt{d_{k}}})V$\nwhere $Q \\circ K^{T}$ represent the dot product between queries and keys, which measures the similarity or alignment between the queries and keys and cross-attention is particularly used when the sets of queries, keys and values are derived from different input sources, enabling the model to integrate information across the sources.\nSupport features, query features and mask features of PropMask are respectively flattened into 1-dimensional vectors, serving as the support, query and value vectors for cross-attention. Considering the definition of cross-attention, the outputs of the cross-attention modules of PropMask can be regarded as the value vectors for the query features. The output value"}, {"title": "4.4 Training configuration and inference settings", "content": "We utilized PyTorch[46](version 2.0.0) to implement our models and executed them on a server equipped with the CUDA platform(version 11.8). The Box2Mask module and the PropSAM module was trained using four NVIDA A800-SXM4-80GB GPUs and 64 Intel(R) Xeom(R) Platinum 8358 P CPUs(2.60GHz). The AdamW optimizer was utilized with an initial learning rate of 1e-3 for Box2Mask module and 5e-4 for PropMask module as well as a weight decay of le-4. The learning rate was adjusted according to Cosine Annealing LR schedule with a maximum period of 100 epochs and a minimum eta of 1e-5.\nFor Box2Mask module, during each epoch, we randomly selected 10,000 samples for training and conducted evaluations every 20 epochs using a set of 5,000 randomly sampled validation samples. The training lasted for 4,100 epochs, with a batch size of 1,024, over a span of about six days. Supplementary Figure S5 illustrates the training and validation curves. We selected the latest checkpoint as the final weight configuration for our Box2Mask module. Once we trained the Box2Mask module, we salloc one GPU and 8 CPUs for inference evaluation, as well as the compared methods, to ensure that inference time and resource comparisons fair. For inference phase, we first cropped the ROI images from the promptable bounding boxes, then normalized them by a series of candidate minimum and maximum parameters. The minimum parameters are determined using the 5th to 40th percentiles (in steps of 1), while the maximum parameters are determined using the 90th to 95th percentiles (in steps of 0.5). These parameters are then combined to a standardize the ROI images, resulting in candidate normalized ROI images. Subsequently, the Box2Mask module is employed to predict the foreground. The final standardization parameters, $V_{min}$ and $v_{max}$, are determined based on the 0.5th and 99.5th percentile values of the pixel locations predicted as foreground. These parameters are then used to standardize the ROI images prediction.\nFor PropMask module, throughout the training process, we randomly selected 10,000 tasks per epoch. Each task consists of the guiding slice and four randomly sampled adjacent slices. Evaluations were conducted every 20 epochs using a set of 5,000 randomly selected validation tasks. The training extended 4,500 epochs, with a batch size of 160, lasting approximately seven days. Supplementary Figure S8 displays the training and validation loss curves. We chose the most recent checkpoint as the final weight configuration for our PropMask module."}, {"title": "4.5 Loss function", "content": "The Box2Mask module leverages deep supervision, enabling predictions at six distinct stages, denoted as {$P_{1},\u2026\u2026,P_{s}$}$_{s=6}$. Each prediction, $P_{s}$, activated by the Sigmoid function, outputs a 2D representation where values between [0.0,1.0] indicate the probability that each pixel is part of the foreground. The foreground ground truth is accordingly rescaled to align with the resolutions of these six stages, represented as {$M_{1},\u2026,M_{s}$}$_{s=6}$, where each $M$ is binary with 1 indicating the foreground. To calculate the loss, we apply soft dice loss at each stage and then compute the average of these losses to derive the overall loss function, which is expressed as:\n$L_{Box2Mask} = \\frac{1}{6} \\sum_{s=1}^{S} (1.0 - \\frac{2 \\times \\sum_{i,j}^{W_{i}H_{i}}P_{s,i,j}M_{s,i,j}}{\\sum_{i,j}^{W_{i}H_{i}}P_{s,i,j} + \\sum_{i,j}^{W_{i}H_{i}}M_{s,i,j}})$\nwhere $W_{s}$ and $H_{s}$ denote the resolution at the sth stage. The loss ranges from 0.0 to 1.0, ensuring that the module is trained effectively across all resolutions, thus enhancing its predictive accuracy and reliability."}, {"title": "4.6 Evaluation metrics", "content": "We evaluated the model from two aspects. For segmentation performance evaluation, we used the Dice Similarity Coefficient (DSC) to evaluate the segmentation results. DSC is a set similarity metric commonly used to calculate the similarity between two samples, with a value range [0.0,1.0"}]}