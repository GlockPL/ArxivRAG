{"title": "PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation", "authors": ["Yungang Yi", "Weihua Li", "Matthew Kuo", "Quan Bai"], "abstract": "Music generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains a significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in music generation, especially in audio generation models such as AudioLDM [1], MusicGen [2], and Jen-1 [3], have demonstrated significant progress, with these models capable of generating highly natural-sounding music. However, symbolic music generation, an area where models can generate and manipulate music in symbolic form, plays a crucial role in the field of music generation, particularly due to its editable nature. This allows for operations such as cutting and rearranging different sections or substituting instrument timbres, enabling human involvement in high-quality music production during the post-processing stage. Compared to audio generation, symbolic music offers a further level of abstraction, making it easier for machine learning models to capture deeper musical characteristics and understanding.\nHowever, the symbolic music generation still faces two key challenges. First, despite the advancements, many of the most expressive datasets, recorded from live performances and recording studios, are seldom used compared to manually created MIDI-file datasets. The primary reason is that they lack detailed annotations, making it harder for models to learn complex structures. Furthermore, due to computational limitations, these models cannot fully capture the context of an entire piece of music. Techniques, such as chunking and quantization, are often employed to reduce computational complexity, leading to the loss of crucial musical details and making it difficult for models to grasp the full structure of a composition. Second, the waterfall-like approaches that use abstract structural representations as conditions for music generation tasks have enabled the generation of structured music. However, such methods rely heavily on handcrafted feature engineering. Our objective is to design and develop a model that is capable of learning the long-range dependencies in music without relying on explicit structural annotations.\nThe emergence of Transformer Attention technologies, such as Perceiver AR [4], has made it possible to access much longer contextual dependencies. It allows for the simultaneous learning of musical structure and the generation of expressive performances. Perceiver AR has demonstrated the ability to attend to a context length of up to 32,768 tokens using the Maestro dataset [5], where the query in cross-attention attends to significantly longer key/value pairs [4]. However, this approach has also introduced challenges. Specifically, the causal mask, when applied with the default input sequence segmentation, does not fully conceal tokens that should not be visible during autoregressive training and generation, which ultimately degrades the quality of the generated music. Additionally, when using ultra-long context as a condition, the model tends to generate identical or similar repetitive segments as the sequence length increases due to issues with high similarity in the context of neighboring tokens, which leads to a high token autocorrelation [6] tendency.\nTo address the challenges mentioned above, in this paper, we propose PerceiverS, a novel model that addresses the causal masking issue by incorporating Effective Segmentation. Additionally, PerceiverS employs Multi-Scale attention to mitigate the high token autocorrelation problem that arises from relying solely on long-range dependencies. Specifically, by adjusting the input sequence segmentation to start from the head segment with an effective causal mask and aggressively increasing the segment length up to the maximum input sequence length, we resolve the learning limitations caused"}, {"title": "II. RELATED WORKS", "content": "The most significant early work in this area came from Google's team, which introduced Music Transformer [7], a model capable of generating expressive piano music using the Piano-e-Competition dataset, later known as the Maestro dataset [5]. Since this model was trained using MIDI files recorded from live piano performances, it utilized the attention mechanism to focus on the context of all previously generated tokens when predicting the next one, allowing it to generate highly detailed and expressive music.\nHowever, due to the quadratic complexity $O(n^2)$ of the Transformer attention mechanism, it could only generate music lasting for tens of seconds, but not for several minutes. Unlike Music Transformer [7], few other models utilize performance datasets, primarily due to the lack of annotations associated with these types of datasets and the computational limitations involved in processing them."}, {"title": "A. Capturing Music Expressiveness", "content": "1) Dataset Selection: A key factor in generating production-quality music lies in the selection of datasets. The Maestro dataset [5], consisting of real human performances, offers dynamic and expressive recording music. Datasets recorded from live performances are rare, but there are quite a few Automatic Music Transcription (ATM) datasets, including GiantMIDI [8], ATEPP [9], PIJAMA [10], and others. Advanced models have been developed, from Hawthorne [11] to Kong [8], that convert audio into MIDI files. These advances have made it possible to use a vast amount of recorded audio music to train models, as the development of symbolic music models has long been constrained by the limited availability of datasets.\nManually created MIDI datasets, like LAKH [12], provide valuable human-annotated information, e.g., beats, bars, and phrases, which allows for flexible segmentation and richer feature extraction but lack expressive nuances found in live performances, such as dynamics, tempo variations, and subtle timing shifts. Other attempts, such as ASAP [13] with human-assisted beat correction, and advancements in automatic beat tracking, such as Beat This! [14], aim to bring annotation to live-recorded datasets, though accuracy limitations still present challenges.\n2) Computational Limitations: Although using MIDI datasets recorded from live performances and music studios allows for the generation of music with rich and expressive details, generating such music over long durations remains a challenge. This is mainly due to the substantial increase in computational resources required for processing long-range contextual dependencies. Almost all current models, e.g., Music Transformer, employ strategies such as chunking and quantization to reduce token sequence length and vocabulary size [7]. While these approaches help to reduce computational burdens, they also limit the model's ability to capture ultra-long dependencies and compromise expressive performance details. As stated on the Music Transformer webpage,\u00b9 \"Some 'failure' modes include too much repetition, sparse sections, and jarring jumps.\" In our experiments, we also observed that chunking introduces similar issues. This trade-off prevents the effective generation of long-term coherent symbolic music."}, {"title": "B. Capturing Long-term Coherence", "content": "In the efforts to learn musical structures and generate symbolic music with long-distance coherence, the approaches can generally be categorized into two main types, i.e., those that utilize explicit structural features and those that encourage the model to learn implicit structural features. Each approach is introduced in the following subsections.\n1) Explicit Structural Features: The explicit use of structural features often relies on handcrafted feature engineering and external analyzing tools. A common method in these models is a waterfall-like approach. Typically, the process begins by generating a lead sheet and then using the lead sheet as a condition for the subsequent generation tasks.\nMuseMorphose [15] explicitly controls rhythmic intensity and polyphony density on a bar basis by training on datasets with bar annotations, specifically the LPD-17-cleansed and Pop1K7 datasets. Compose & Embellish [16] leverages third-party tools such as the skyline algorithm, edit similarity, and A* search measures to extract melody and identify structural patterns, enabling the model to produce music with enhanced structural organization. However, it still relies on bar-annotated datasets for training. Rule-Guided Diffusion [17] uses note density and chord to condition generation, resulting in structured musical segments. Its training does not rely on an annotated dataset but rather uses the performance dataset, Maestro. Still, it only produces short musical pieces instead of full-length segments. Whole-Song Hierarchical Generation [18] is capable of generating fully structured, complete pieces of music. It employs a multi-stage approach, using annotations from the POP909 dataset [19], including chord information and separate tracks for melody, bridge, and accompaniment, to produce structured elements such as form, lead sheet, and accompaniment, ultimately generating a complete, full-length piece."}, {"title": "2) Implicit Structural Features:", "content": "Another important approach involves enabling the model to learn the structural information of music implicitly. The method has the advantage of being more generalizable, as it does not rely on handcrafted feature engineering. However, its downside lies in the increased difficulty for the model to capture complex structural features of music.\nMusicVAE [20] uses a bidirectional RNN and a Conductor RNN to generate per-bar latent vectors that are decoded into individual notes, focusing on bar-level structure through training on datasets containing bar annotations, which may not be applicable to freely performed music. Museformer [21] applies sparse Transformer attention by fully attending to all tokens in selected bars and the summarized vectors of all preceding bars, allowing it to capture long-range context with limited computational resources. However, it still leverages the Lakh MIDI dataset [12]'s bar annotations, which cannot be used with unannotated performance datasets."}, {"title": "C. SOTA Solutions with Long-Term Dependency On Performance Datasets", "content": "The Perceiver AR model [4] from DeepMind has been a significant source of inspiration. It combines cross-attention and self-attention mechanisms, enabling the model to attend to sequences with up to tens of thousands of tokens. Like Perceiver [22] and Perceiver IO [23], Perceiver AR [4] uses a shorter query in its cross-attention mechanism to attend to much longer sequences, thereby minimizing computational costs. As noted in the paper, this approach allows the model to attend to up to 32,768 tokens when trained on the Maestro dataset [5], offering a significantly longer context compared to models like Transformer-XL [24]. This ability to efficiently process long-range contextual data is crucial for learning the structure of entire musical pieces.\nHowever, Perceiver AR leverages the last N tokens as the Query with a limited causal mask, and training with teacher-forcing on long sequences led to lower quality generation. Furthermore, we observed that relying solely on long, especially ultra-long, context resulted in repetitive segments in the latter part of generated content."}, {"title": "III. PRELIMINARIES", "content": "In this section, we introduce the fundamental concepts and key challenges necessary to understand our proposed model. We review the operational mechanism of cross-attention in Perceiver AR, the role of its causal mask, and the key considerations when using ultra-long sequences as context for token generation."}, {"title": "A. Input Sequence Preprocessing", "content": "Let the complete sequence be $X = {X_1, X_2, . . ., x_l}$, where $l$ is the total length of the entire music sequence, and $m$ is the maximum input length, representing the longest sequence that the model can attend to in one pass. The query length is denoted by $n$, which represents the number of tokens the model uses to query the context, and typically, $n \\leq m$."}, {"title": "1) Causal Masking in Typical Transformers:", "content": "In a typical transformer with causal masking, the goal is to ensure that when generating token i, the model does not attend to tokens j where j > i. The causal mask for this is typically represented by:\n$M_{ij} = \\begin{cases}\n0 & \\text{if } i \\geq j\\\\\n-\\infty & \\text{if } i < j\n\\end{cases}$\nThis matrix is added to the attention scores $QK^T$ so that all positions j > i (i.e., future tokens) are masked out by setting their attention scores to -$\\infty$, ensuring they don't influence the generation of the current token.\nFor example, consider a case where the query has a length of n = 5 and the key/value has a length of m = 5. The expected causal mask (Vanilla Transformer) is as follows:\n$\\begin{bmatrix}\n0 & -\\infty & -\\infty & -\\infty & -\\infty\\\\\n0 & 0 & -\\infty & -\\infty & -\\infty\\\\\n0 & 0 & 0 & -\\infty & -\\infty\\\\\n0 & 0 & 0 & 0 & -\\infty\\\\\n0 & 0 & 0 & 0 & 0\n\\end{bmatrix}$"}, {"title": "2) Perceiver AR's Causal Mask Issue:", "content": "In Perceiver AR [4], the situation is different because the query token length n is much smaller than the key and value lengths m. Specifically, the causal mask only works on the final part of the context sequence, equivalent to the length of the query n, but does not mask tokens that occur before that. This results in some tokens before the query length being visible during training, which is not an issue for generation except that the segment from $X_1$ to $X_{m-n}$ is not properly learned by the model.\nLet's denote the sequence of keys and values as K and V, respectively, and the query length as n, while the context length (keys/values) is m, where m > n. The attention mask matrix M in Perceiver AR can be represented as follows:\n$M_{ij} = \\begin{cases}\n0 & \\text{if } j \\leq n\\\\\n-\\infty & \\text{if } j > n \\text{ and } i\\leq n\\\\\n0 & \\text{if } i > n\n\\end{cases}$\nBelow shows an example of the causal mask used in Perceiver AR with n = 5 (query length) and m = 10 (context length):\n$\\begin{bmatrix}\n-\\infty & -\\infty & -\\infty & -\\infty & -\\infty & 0 & 0 & 0 & 0 & 0\\\\\n-\\infty & -\\infty & -\\infty & -\\infty & -\\infty & 0 & 0 & 0 & 0 & 0\\\\\n-\\infty & -\\infty & -\\infty & -\\infty & -\\infty & 0 & 0 & 0 & 0 & 0\\\\\n-\\infty & -\\infty & -\\infty & -\\infty & -\\infty & 0 & 0 & 0 & 0 & 0\\\\\n-\\infty & -\\infty & -\\infty & -\\infty & -\\infty & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}$"}, {"title": "3) Calculation for Attention with Mask:", "content": "The attention mechanism with a causal mask is computed as follows:\n$\\text{Attention}(Q, K, V) = \\text{softmax} (\\frac{QK^T + M}{\\sqrt{d_k}})V$"}, {"title": "B. Ultra-Long Context in Autoregressive Generation", "content": "We observed that relying only on the ultra-long context in autoregressive models can lead to generated sequences containing excessively long repetitive short segments, especially as the sequence length increases. Given that the probabilities of generating tokens $x_t$ and $x_{t-k}$ (where k is a small integer) are, respectively:\n$P(x_t|x_1,x_2,...,x_{t-1})$\n$P(x_{t-k}|x_1,x_2,...,x_{t-k-1})$\nAs t increases, the contexts $[x_1,x_2,...,x_{t-k-1}]$ and $[x_1,x_2,...,x_{t-1}]$ become nearly identical due to the ultra-long context. Consequently, the conditional distributions $P(x_t|x_1,x_2,...,x_{t-1})$ and $p(x_{t-k}|x_1,x_2,...,x_{t-k-1})$ are almost indistinguishable, resulting in a KL divergence close to zero:\n$D_{KL}(P(x_t|x_1,x_2,...,x_{t-1})||P(x_{t-k}|x_1,x_2,...,x_{t-k-1})) \\approx 0$\nThis similarity in conditional distributions means that the model is likely to generate similar tokens in nearby steps, as the probability distributions governing $x_t$ and $x_{t-k}$ become nearly identical. Thus, the probability of $x_t = x_{t-k}$ increases, leading to repetitive short segments.\nWhen such repetitive tokens are generated across multiple time steps, the sequence exhibits high token autocorrelation. Mathematically, the token autocorrelation at lag k for the sequence $X = [x_1,x_2,...,x_t]$ can be expressed as:\n$\\rho_k(X) = \\frac{\\sum_{t=k+1}^T (x_t - \\overline{x})(x_{t-k} - \\overline{x})}{\\sum_{t=1}^T (x_t - \\overline{x})^2}$\nwhere $\\overline{x}$ represents the mean of the sequence X. When values at nearby steps exhibit high similarity, as suggested by nearly identical conditional distributions, the term $(x_t - \\overline{x})(x_{t-k} - \\overline{x})$ becomes large, leading to high token autocorrelation at lag k.\nDuring generation, identical values are not produced at nearby steps to avoid training penalties. In fact, the similar context of neighboring tokens causes the generation process to produce identical or similar tokens at nearby steps, leading to a higher probability of generating repetitive short segments as the sequence grows longer."}, {"title": "IV. PROPOSED APPROACH", "content": "Our proposed model, PerceiverS, introduces a dual approach of Effective Segmentation and Multi-Scale attention to address the limitations in symbolic music generation. Building on the strengths of Perceiver AR [4] and introducing mechanisms to handle both short and ultra-long dependencies, PerceiverS (Segmentation and Scale) achieves greater coherence and expressiveness in generated music.\nSince Perceiver AR provides the possibility of accessing extremely long contexts, we attempt to use Perceiver AR as a baseline model to learn entire musical pieces and evaluate the quality of its generation. Our goal for symbolic music generation is to achieve long-term coherence while maintaining diversity within shorter segments. Furthermore, we expect the model to learn patterns similar to human composition, with repetition and development. The detailed steps of our approach and improvements are elaborated in the following subsections."}, {"title": "A. Improving the Model to Effectively Learn Ultra-Long Sequences", "content": "This section outlines an improved dataset preprocessing strategy designed to enhance token generation quality in autoregressive models. As discussed in the previous section, Perceiver AR's causal mask has limitations in its coverage of the entire input sequence, making it necessary to implement preprocessing adjustments before feeding data into the model.\nWe set the maximum context length to 32,768 tokens. Based on the Perceiver AR's original implementation\u00b2, we randomly cropped the dataset. In this approach, a random starting point is selected between 0 and (the sequence length - the maximum input length + 1), from which a segment of length equal to the maximum input length is taken.\nThe upper part of Figure 1 demonstrates the original input sequence pre-processing of the baseline model. Specifically, the baseline model segments the input sequence using the maximum input length as the window size, leaving the beginning of the sequence uncovered by the causal mask. This results in these initial tokens not being progressively used as validation tokens in teacher forcing, preventing the model from learning"}, {"title": "B. Further Improving the Model for Generating Music with Both Coherence and Diversity", "content": "After applying ultra-long contexts in the autoregressive generation, we aim to combine the strengths of both consistency and diversity, allowing the proposed PerceiverS to generate music without a tendency toward repeated segments caused by attending solely to long-distance contexts.\nThe Multi-Scale causal cross-attention mechanism adopted in the proposed PerceiverS, while somewhat similar to the concept of Museformer [21], is fundamentally different. It introduces multiple layers of attention with varying scales of attending length, designed to balance focus on both long and short contexts, thereby enhancing diversity and reducing repetitive tendencies."}, {"title": "V. TECHNICAL DETAILS", "content": "In this section, we provide the mathematical formulation and technical explanation of our proposed approach, PerceiverS, for symbolic music generation, based on improvements to Perceiver AR [4], including our proposed Effective Segmentation for input sequences and Multi-Scale cross-attention."}, {"title": "A. Improved Segmenting", "content": "To address the mismatch between training and autoregressive generation, we propose a segmenting method aligned with the causal mask mechanism, whether using random or sequential sampling, that emphasizes shorter context sequences, gradually building up to the maximum context length."}, {"title": "1. Attention mask without scale mask:", "content": "In this case, all tokens are visible to the model. The scale mask matrix M is defined as:\n$M_{ij} = 0, \\forall i, j$"}, {"title": "2. Attention mask with scale mask (masking the 1st to the m \u2212 nth tokens):", "content": "In this case, tokens from the 1st to the m \u2212 nth positions are masked by the scale mask, preventing the model from attending to these tokens. The scale mask matrix M is defined as:\n$M_{ij} = \\begin{cases}\n0 & \\text{if } j > m-n\\\\\n-\\infty & \\text{if } j \\leq m-n\n\\end{cases}$\nThis ensures that only tokens starting from the m-n+1th position are visible, while the model cannot attend to tokens before this position.\nThe combined causal mask M and scale mask M are added directly to the attention score calculation, modifying the softmax as follows:\n$\\text{Attention}(Q, K, V) = \\text{softmax} (\\frac{QK^T + M + M}{\\sqrt{d_k}})V$\nHere, M is the causal mask that ensures the model doesn't attend to future tokens, and M is the scale mask applied to limit attention to certain parts of the sequence. Both masks work together to control which tokens the model can attend to during training.\nIn the Cascade mode, the output from the first attention layer is fed as input to the next layer, enabling each layer to refine and build upon the previous layer's output."}, {"title": "VI. EXPERIMENT AND RESULTS", "content": "Our experimental setup included five key components:\n1) Dataset Selection: In our experiments, we used three datasets: Maestro [5], GiantMIDI [8], and ATEPP [9]. The Maestro dataset [5] was the primary dataset used, containing 1,251 sequences, with a validation set of 240 sequences. Additionally, we combined the three classical piano datasets (Maestro [5], GiantMIDI [8], and ATEPP [9]), resulting in a training set of 25,662 sequences and a validation set of 4,824 sequences. This combined dataset was only used for training the cascade Multi-Scale attention model to assess the model's generalization abilities.\n2) MIDI Preprocessing: For MIDI preprocessing, we set the Note On and Note Off events within the range of 0 to 127. Time Shift events were discretized into 100 time steps per second, where each step represents 10 milliseconds. Volume events were quantized into 32 bins, and pedal events were mapped to the duration of relevant notes, discarding the pedal events afterward. Each song ends with a token_end marker. No data augmentation, such as key or tempo changes, was applied, though this is planned for future experiments.\n3) Hyperparameter Selection: The hidden dimension was set to 1,024 with 24 self-attention layers. Each attention layer had 16 heads, and the head dimension was 64. Adam optimization was used, with an initial learning rate set to 0.03125. Each generated sequence length was set to 8,192 tokens, resulting in approximately 2-10 minutes of music. The music generation in this setup was unconditional, meaning that no external conditions or prompts were used to guide the generation process. The resulting MIDI files were rendered into audio using the Vintage Piano sound from Logic Pro.\n4) Hardware Setup: All experiments were conducted on an NVIDIA RTX 4080 GPU with 16GB of memory. The batch size was set to 1, and training on the Maestro dataset [5] took approximately 9 minutes and 30 seconds per epoch. Each experiment was run for 100 epochs.\n5) Evaluation Metrics: We adopted the evaluation method outlined in the paper \"On the Evaluation of Generative Models in Music\" [25]. For evaluation, we constructed a reference dataset by separating a set of pieces from the training dataset (Maestro [5]) before model training. Then, we generated an equal number of files using the model to form the generated dataset. We calculated the distances within each dataset and between the generated and reference datasets for the following metrics:\nTotal Used Pitch (PC): Measures the overall pitch diversity by counting distinct pitch classes used throughout the entire piece.\nTotal Used Note (NC): Counts the distinct notes (pitch and octave combinations) used across the entire piece, indicating the variety in pitch and register.\nTotal Pitch Class Histogram (PCH): A histogram representing the frequency distribution of pitch classes across the entire piece, offering insights into pitch class preference.\nPitch Class Transition Matrix (PCTM): A matrix representing the probabilities of transitioning from one pitch class to another. This metric captures melodic and harmonic movement patterns.\nPitch Range (PR): Measures the difference between the highest and lowest pitches used in the piece, indicating the overall range of pitches.\nAverage Pitch Interval (PI): The average interval between consecutive pitches, which reflects the tendency towards stepwise or leapwise motion in melodies.\nAverage Inter-Onset Interval (IOI): Measures the average time interval between note onsets, capturing the rhythmic density across the entire piece.\nNote Length Histogram (NLH): A histogram representing the distribution of note lengths, giving insights into note duration diversity.\nNote Length Transition Matrix (NLTM): A matrix representing the transition probabilities between different note lengths, capturing rhythmic patterns and variations in note duration.\nThe original paper [25] includes metrics analyzed on a bar-by-bar basis. We introduced four metrics based on time"}, {"title": "B. Experiments and Results", "content": "We conducted three sets of experiments to evaluate the models' performance in various scenarios.\n1) Input Sequence Segmentation: The purpose of this experiment is to demonstrate that Effective Segmentation is essential for the model to fully leverage ultra-long-distance context.\nWe compared two types of input sequence segmentations, with a maximum sequence length set to 32,786 tokens:\nBaseline Model: A random starting position is selected within the range [0, sequence length - max input length + 1], then a segment of max input length tokens is taken from this start.\nImproved Model: A random end position is selected within the range [query length + 1, sequence length + 1], then a segment of tokens, up to the max input length, is taken backward from this end.\nSegments shorter than the max input length are padded at the beginning.\nThe segmentation methods produced very different results in autoregressive training and generation, with the baseline model exhibiting poor quality and the improved model showing excellent quality, as shown in the data within the red dashed box in Table I. The music generated using the improved input sequence processing approach is closer to the ground truth in terms of lower KLD metrics, including PC, PC/seg, NC/seg, PCH/seg, PCTM, PR, PI, IOI, and IOI/seg, as well as higher OA metrics, including PC, PC/seg, NC, NC/seg, PCTM, PR, PI, IOI, IOI/seg, and NLTM. This result indicates that the improvement effectively enables the model to utilize ultra-long-distance context for music generation.\n2) Multi-Scale Attention: While an ultra-long context provides long-term consistency, it tends to generate repetitive segments in the latter part of the sequence (see Figure 3). To address this issue, we added different scale masks to the multi-layer cross-attention, allowing different layers to focus on distinct context lengths and merging these results. The aim of this experiment is to evaluate whether adding attention outputs for shorter-range dependencies can improve generation quality. Here, we define two scale masks for the maximum sequence length of 32,786 tokens:\nNo Scale Mask: All tokens are unmasked.\nMasked Scale: Only the last 1,024 tokens are visible, with all previous tokens masked.\nThe cross-attention layer without a mask is fed into the cross-attention layer with a context mask as its input. We refer to this approach as cascade Multi-Scale cross attention."}, {"title": "VII. DISCUSSION", "content": "Perceiver AR [4] represents a significant advancement in the realm of autoregressive (AR) Transformers, as it directly addresses the challenge of handling long context attention, a limitation that has constrained almost all previous Transformer architectures. By breaking free from traditional context length constraints, Perceiver AR [4] has made possible many tasks that were previously unachievable for AI models.\nA distinctive feature of Perceiver AR [4]'s design is its use of differing key-value (KV) and query (Q) lengths within the attention mechanism. This unique approach allows for an unrestricted context length, yet it requires a carefully considered application of causal masking. The causal mask only applies effectively to the query length, meaning that both training and generation must account for this query-specific length. To maximize learning, it is crucial for the model to begin training with query-length sequences starting from the beginning of each dataset sample, rather than focusing solely on the context-length segment. Without this adjustment, the model would fail to develop the capability for generating tokens beyond the masked region.\nOur proposed Effective Segmentation technique, which optimizes the input sequence processing for Perceiver AR [4], has shown that, with appropriate data segmentation, Perceiver AR [4] can effectively learn long contexts and produce coherent, contextually rich content. The ultra-long context attention indeed provides the model with remarkable benefits in maintaining content consistency over extended generation. However, a potential drawback of solely relying on long-range dependencies emerged: as sequences get longer, the model increasingly tends to generate repetitive short segments. This occurs because, in the later stages of generation, the long context windows are nearly identical, differing by only a few tokens. Consequently, the probability of generating identical or similar segments increases, further amplified by the high token autocorrelation [6] tendency due to the high similarity of context as the sequence lengthens..\nTo mitigate this issue, we introduce a Multi-Scale cross-attention mechanism that combines both long and short context windows. By integrating varying context lengths into multi-layer cross-attention, our approach successfully reduces the tendency for repetitive sequences while maintaining the model's long-term consistency. This combination strategy enables the generation of high-quality symbolic music over extended sequences.\nOur enhanced model, PerceiverS (Segmentation and Scale), leverages Effective Segmentation aligned with Perceiver AR [4]'s operation mode. By applying Multi-Scale masking strategies within multi-layer cross-attention, Per-ceiverS effectively generates cohesive, consistent music over extended temporal contexts, preserving intricate musical pat-terns and expressive details across long sequences. Additionally, through the use of performance music datasets, Per-ceiverS is capable of producing high-quality symbolic music that captures the nuances of human performance. Importantly, because the model does not rely on annotated datasets, Per-ceivers can be trained on MIDI data derived from audio using any automatic music transcription (AMT) technique. This capability suggests a future in which symbolic music generation can leverage vast historical recordings, unbounded by the limitations of manually annotated data.\nIn essence, Perceiver AR [4] and, by extension, PerceiverS, are general-purpose models adaptable to a wide range of AI tasks. The Effective Segmentation and Multi-Scale innovations introduced here open up avenues for future applications across domains such as text, image, and video. Future research could thus extend the potential of Perceivers, exploring its capabilities across diverse modalities and expanding its utility within the broader landscape of AI tasks."}, {"title": "VIII. CONCLUSION", "content": "In this work, we introduced a novel model, PerceiverS, which builds on the Perceiver AR [4] architecture by incor-porating Effective Segmentation and a Multi-Scale attention mechanism. The Effective Segmentation approach progressively expands the context segment during training, aligning more closely with autoregressive generation and enabling smooth, coherent generation across ultra-long symbolic music sequences. The Multi-Scale attention mechanism further enhances the model's ability to capture both long-term structural dependencies and short-term expressive details.\nBy addressing limitations in existing models, particularly the issue of causal masking in autoregressive gener-ation and the high token autocorrelation problem in ultra-long sequences, Perceivers enables the effective handling of ultra-long token sequences without compromising the quality of generated music. Through our proposed Effective Segmentation in dataset preprocessing and Multi-Scale atten-tion modifications, we demonstrated significant improvements in generating coherent and diverse musical pieces.\nOur approach to symbolic music generation provides a new balance between structural coherence and expressive diversity, setting a foundation for future advancements in symbolic music generation models."}]}