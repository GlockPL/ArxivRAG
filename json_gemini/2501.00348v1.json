{"title": "Temporal Information Reconstruction and Non-Aligned Residual in Spiking Neural Networks for Speech Classification", "authors": ["Qi Zhang", "Huamin Wang", "Hangchi Shen", "Shukai Duan", "Shiping Wen", "Tingwen Huang"], "abstract": "Recently, it can be noticed that most models based on spiking neural networks (SNNs) only use a same level temporal resolution to deal with speech classification problems, which makes these models cannot learn the information of input data at different temporal scales. Additionally, owing to the different time lengths of the data before and after the sub-modules of many models, the effective residual connections cannot be applied to optimize the training processes of these models. To solve these problems, on the one hand, we reconstruct the temporal dimension of the audio spectrum to propose a novel method named as Temporal Reconstruction (TR) by referring the hierarchical processing process of the human brain for understanding speech. Then, the reconstructed SNN model with TR can learn the information of input data at different temporal scales and model more comprehensive semantic information from audio data because it enables the networks to learn the information of input data at different temporal resolutions. On the other hand, we propose the Non-Aligned Residual (NAR) method by analyzing the audio data, which allows the residual connection can be used in two audio data with different time lengths. We have conducted plentiful experiments on the Spiking Speech Commands (SSC), the Spiking Heidelberg Digits (SHD), and the Google Speech Commands v0.02 (GSC) datasets. According to the experiment results, we have achieved the state-of-the-art (SOTA) result 81.02% on SSC for the test classification accuracy of all SNN models, and we have obtained the SOTA result 96.04% on SHD for the classification accuracy of all models. Furthermore, on the non-spiking dataset GSC, we can achieve a test classification accuracy of 95.63%, which surpasses the original baseline SNN-Delays. Meanwhile, the method proposed in this paper can speed up the model's processing speed and reduce its energy consumption.", "sections": [{"title": "I. INTRODUCTION", "content": "PROFOUNDLY inspired by biological systems [1] [2], Maass provided the spiking neural networks (SNNs), named as the third-generation artificial neural networks (ANNs) [3], which can meticulously mimic the operational mechanisms of biological neural systems [4]. Compared to ANNs, SNNs can achieve a transformation from static to dynamic in information representation, computing patterns, and learning mechanisms [5] by using the timing and the frequency of spike releases to encode information, which allows the state of neurons to evolve dynamically over time [6], [7]. Furthermore, spikes can be triggered only when the membrane potential of the neurons exceeds the threshold [8], [9]. This means that SNNs exhibit a dynamic discrete characteristic, and only have sparse accumulation (AC) instead of costly multiply-accumulate (MAC) operations [10], [11], thereby demonstrating higher energy efficiency [12]\u2013[14]. Therefore, SNNs have been applied in many fields to reduce the energy consumption in recent years, such as pattern recognition [15], [16], image classification [17], [18], speech classification [19]\u2013[21], and etc.\nDue to the dynamic discrete characteristics of SNNs, they can efficiently capture the dynamic changes and fully leverage the temporal information, making them particularly suitable for handling complex data with temporal dependencies, such as speech data. As a result, when SNNs are used for speech classification, they can demonstrate some significant advantages. Cramer et al. [22] introduced the Spiking Heidelberg Digits (SHD) dataset and trained a single-layer spiking recurrent neural network (SRNN) [23] on it for the first time. Base on SRNN, the authors in [24] provided adaptive spike neurons to dynamically adjust their firing rate and threshold according to the changes of the input current and achieve high performance. Then, Bittar et al. [25] introduced an additional adaptive variable w(t) that linearly coupling with the membrane potential u(t) to reproduce various special firing patterns observed in the biological neurons. Dampfhoffer et al. constructed the SpikGRU [26] model by adding an additional gate to the cuba-LIF [27] to calculate the current state through the optimal combination of the previous state and input current, which could achieve higher accuracy than other SRNNs and reduce a large number of operations compared to ANNs.\nIn recent years, with the proposition of attention mechanisms, they have gradually been incorporated into SNNs to process speech data, resulting in more significant effects [19] [28]. In [28], the attention concept was extended to the temporal dimension to discard the irrelevant frames, name as TA-SNN that can extract effective spatio-temporal features from event streams and consider the changes in frame signal-to-noise ratio. After that, STSC-SNN [19]incorporated attention mechanism and temporal convolution to achieve the filtering and gating functions of synapses, endowing the synapses with temporal dependence and enhancing their ability to classify speech information. As we all know, connection delay among spiking neurons may affect the performance of models, which makes some authors to investigate delay problem. Sun et al. [20] designed an adaptive training scheduling mechanism allowing the axon delays to be learned, which implied that it could be adapted to the learning of each layer in the SNNs to improve the performance of speech classification tasks. Based on dilated convolution with learnable spacings (DCLS), Hammouamri et al. [21] built the SNN-Delays to learn the connection delay between neurons, achieving the SOTA results on multiple datasets.\nAdditionally, the process of speech comprehension is generally viewed as a series of computational steps that are carried out by different processing modules with distinct functional role in the brain, each of which is used to extract information on different temporal scales [29] [30]. Therefore, learning multi-temporal information from speech signals has great value and significance in speech classification, and has attracted some researchers' attention [31]. Multiscale audio spectrogram transformer (MAST) [32] introduced the concept of multi-scale feature hierarchy into audio spectrogram transformer (AST), allowing MAST to learn information at different temporal resolutions and achieving excellent results in speech classification tasks. Moreover, according to the published research results, it has found that SNN models possess spatial-temporal characteristics, giving them an advantage in handling speech classification tasks. However, as far as we know, there have few works that use SNN models to deal with the information of multiple temporal scales for speech classification problems.\nMotivated by the above analysis, we have proposed the Temporal Reconstruction (TR) method to achieve multi-time-scale modeling of input speech information in speech classification tasks by referencing the hierarchical processing of speech information in the human brain. In addition, we have also proposed the Non-Aligned Residual (NAR) method to enable residual connections to be used on speech data with different lengths in the temporal dimension. We have conducted a great deal of experiments on the spiking datasets SSC and SHD to validate the effectiveness of our proposed methods. And we have also conducted some experiments on the non-spiking dataset GSC to further validate the effectiveness of our methods. The contributions of this paper can be summarized as follows:\n(1) In analogy with the hierarchical processing of speech information in the human brain, a novel TR method has been proposed to reconstruct the temporal dimension of the audio spectrum, which enables the network to learn information at different time scales of the input audio spectrogram.\n(2) A NAR method has been proposed to establish residual connections between speech data with different time lengths, which can solve the problem that the residual connections cannot be directly used to process speech data of different time lengths.\n(3) Promising experimental results have been obtained on the spiking datasets SSC and SHD, and the non-spiking dataset GSC by a great deal of experiments using our proposed methods. On the dataset SSC, a SOTA result of 81.02% has been achieved for the test classification accuracy among all SNN models, and a SOTA result of 96.04% has been obtained on the dataset SHD for the test classification accuracy across all models. At the same time, the methods proposed in this paper can also achieve a favorable energy efficiency ratio on the non-spiking dataset GSC.\nThe structure of this paper is as follows. In Section II, we introduce the spiking neuron model known as the leaky integrate-and-fire (LIF) model, the learning mechanism and components of the SNN-Delays model, related work on multi-scale information learning, and research on residual connections. In Section III, we describe the detailed implementation of our proposed TR and NAR. In Section IV, we present the experimental results of our methods and the results of ablation experiments. In Section V, we conclude our paper."}, {"title": "II. RELATED WORKS", "content": "A. Spiking Neuron Model\nThere are several neuron models available in SNNs, including the integrate-and-fire (IF) model [33], the LIF model [27], and others. The LIF model is widely used due to its simplicity and efficiency. In an LIF neuron, a spike is emitted only when the membrane potential exceeds the threshold. After the spike is fired, the membrane potential is reset, and there is a degree of attenuation of the membrane potential at every moment. In our work, we choose to employ the LIF model and implemented by spikingJelly [34]. The LIF model is described by the following equations:\n$\\begin{cases}\nI[t] = W \\cdot X[t] + b, \\\\\nU[t] = \\eta \\cdot H[t - 1] + I[t], \\\\\nS[t] = Heav(U[t] \u2013 V_{th}),\n\\end{cases}$ (1)\n$\\begin{cases}\nH[t] = (U[t] (1-S[t]) + V_{reset} \\cdot S[t], & \\text{hard reset}, \\\\\nU[t] - V_{th} S[t], & \\text{soft reset},\n\\end{cases}$ (2)\nwhere $X[t]$ represents the input to the LIF neuron model at time t, with W and b denoting the synaptic weights and bias, respectively. $I[t]$ denotes the input current at time t, while $U[t]$ is the voltage at that moment. The parameter $\\eta$ is the decay constant, and $H[t-1]$ represents the voltage value after reset at time t - 1. $S[t]$ indicates the firing activity at time t, while Heav(*) refers to the Heaviside function. $V_{th}$ is the firing threshold, and $V_{reset}$ is the reset voltage.\nB. Baseline Architecture\nIn [21], the author proved the mathematical equivalence between 1D temporal convolution and connection delay, built the SNN-Delays network using the dilated convolution with learnable spacings (1D version) proposed by Khalfaoui-Hassani [35], and achieved the SOTA on three audio classification datasets: SHD, SSC, and GSC. Its core innovation lies in learning both synaptic weights and connection delays between neurons. The network consists of multiple delay modules, each of which contains a DCLS1D, Batch Normalization, LIF Neuron, and Dropout in sequence, with no Batch Normalization and Dropout in the output layer. The connection delay refers to the varying time taken when a spike signal passes through different structural synapses between two neurons. Maass [36] has proved that the delayed learning is important in theory and the learning in the brain cannot be simplified to synaptic plasticity. Based on these, we have considered SNN-Delays of [21] as the baseline in this paper, and have successfully achieved SOTA on both the SSC and SHD datasets by combining our methods.\nC. Multi-scale Information Learning\nIn the field of natural language processing and visual fields, multi-scale information learning has been used in some published works [32], [37], [38]. In [32], Ghosh et al. built the MAST by introducing the concept of multi-scale feature hierarchy, and achieved significantly higher classification accuracy on LAPE's 8 tasks than AST. Meanwhile, MAST could also reduce the square attention complexity in AST due to the lower time resolution of the input data. As we know, MAST can process data with high temporal resolution at the shallow layers, which means that it is focused on local information of the data and is dedicated to modeling simple low-level acoustic attributes. While at the deep layers, it can process data with low temporal resolution, which means that it is focused on global information of the data and learns complex high-level acoustic attributes. Motivated by this method, we propose TR to enable the SNNs to learn information at different temporal scales from the input data for improving speech classification, which is shown in Section III.\nD. Residual Connections\nIn 2016, the concept of residual connections was first introduced by Kaiming He et al. [39]. They not only provided strong theoretical support but also validated its significant advantages in image recognition by leveraging the additive properties of signals. In the field of SNNs, some works have involved residual connections to improve the models' performance [16], [40], [41]. Given this, we believe that applying residual connections to our baseline will also yield promising results. However, due to the special padding method of SNN-Delays, the lengths of the two data that require residual connections differ in the temporal dimension. To address this, we propose NAR that is described in the section III."}, {"title": "III. METHODS", "content": "In the following two subchapters, we will provide a detailed description of our proposed TR and NAR methods, which is shown in Figure 2.\nA. Temporal Reconstruction (TR)\nThe core strategy of TR is to first group the data along the temporal dimension, and the number of time points in each group is named as within-group time length. Subsequently, within each group, we perform comparative analysis in the frequency dimension for each discrete time point, and reconstruct multiple time points into a single time point. Specifically, in a group, we can obtain the value of a frequency point at the new reconstruction time by logic OR operation along the temporal dimension at that frequency point. Then, we repeat the above operation along the frequency dimension to complete the TR. If a residual connection is used before TR, the Max function can be used instead of the logical OR operation.\nTR method can learn information at different temporal scales from the input audio spectrogram by reducing its temporal resolution. At shallow layers, it allows the model to learn local information and extract low-level acoustic attributes such as loudness, pitch, and timbre. While at deep layers, it can study global information and extract high-level acoustic attributes such as dynamic range, audio quality defects, and emotional attributes. Furthermore, as the length of the data in the temporal dimension decreases, the processing speed of the model will become faster. We have two versions of TR: TR with overlap and TR without overlap, based on whether the time points between groups overlap.\nBased on whether the chosen group-within time length can be evenly divided by the total time length of the input data, TR-o is divided into two cases: one that is evenly divided and the other that isn't. The implementation formula of TR-o is shown by the following equation:\n$\\begin{cases}\nx_t^{[0:T_1:1]} = Max(x_t^1,...,x_t^{i+len-1}), & \\text{if } T\\%T_1 = 0, \\\\\nx_t^{[0:T_1:1]} = Max(x_t^1,...,x_t^{i+len-1])\\oplus x_{tlast}, & \\text{else},\n\\end{cases}$ (3)\nwhere $\\hat{x}$ represents the data after TR, and $x$ represents the original data, $x$ and $\\hat{x}$ belong to $R^{(T,B,N)}$, T represents the size of the temporal dimension, B represents the batch size, N represents the size of the frequency dimension, stride is the step size relative to time points, len is the within-group time length, $T_1 = \\frac{T}{\\lceil\\frac{T}{\\text{stride}}\\rceil - 1}, T_2 = (\\lceil\\frac{T}{\\text{stride}}\\rceil - 1) * \\text{stride}$, and the symbol \"\" represents the concatenate operation in temporal dimension.\nB. Non-Aligned Residual (NAR)\nTo overcome the problem that residual connections cannot be directly used for SNN-Delays model, we have designed the NAR method to allow residual connections to be flexibly applied in this model, thereby effectively enhancing its information processing ability. Since the padding method of SNN-Delays results in different time lengths of the data before and after a delay module, the time lengths of the two data need to be processed to the same length in order to apply the residual connections to the model. After the residual connection, the local data will appear values 0, 1, and 2, where 2 can be considered as strong spike improving the expression ability\nThrough a lot of experiments, we found that using 0 for right padding is the best approach. On account of the property of learning delay for the SNN-Delays model, left padding will destroy the connection delay that the model has learned. Additionally, in the field of linguistics and phonetics, it is mentioned that the volume and pitch of speech may gradually decrease, especially at the end of a sentence, pause, or the end of a speech, which is called intonation decline or the diminution of the volume, so it is more reasonable to use 0 to padding the data in this method [42].\nThe specific operation process is shown in the Figure 5. First, the difference between the two data in the temporal dimension is calculated, and then the shorter data in the temporal dimension is padded with 0 on the right to supplement it. The length of the padding is the difference between the two data in the temporal dimension. After padding, the two data can be residual connected, as shown in the following formula:\ny = D(x) + NAR(x, D(x)), (5)\nwhere D(*) represents a module, NAR(*) represents NAR, and the return value of NAR(*) is x after padding, and the shape of NAR(x, D(x)) is exactly the same as that of D(x). When NAR is applied to the model, the performance of the model improves significantly, and the specific experimental data can be found in IV of this paper."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we have conducted numerous experiments on the datasets SSC, SHD, and GSC to verify the effectiveness and advancement of our proposed methods.\nA. Datasets\nSHD.The Spiking Heidelberg Digits (SHD) dataset is a classification dataset based on audio, which is derived from the Heidelberg Digits (HD) dataset through spike conversion. SHD contains approximately 10k high-quality English and German spoken digit recordings ranging from 0 to 9. There are 12 speakers, including 6 women and 6 men. Speakers ranged in age from 21 to 56, with an average age of 29.\nGSC.The Google Speech Commands (GSC) dataset is released under the Creative Commons BY 4.0 license and contains the words spoken by 1864 speakers. The GSC contains a total of 35 categories, which include 24 word commands such as Yes, No, Up, Down, Left, Right, On, and Off, as well as 10 auxiliary words such as Bed, Bird, Cat, Dog, and silence.\nSSC.The Spiking Speech Commands (SSC) dataset is converted from GSC using an artificial model of the inner ear and parts of the ascending auditory pathway [22].\nB. Experiments Setup\nFor the basic hyperparameters of the models, we generally follow the settings of the original baselines. We have applied TR and NAR behind the middle layer of baselines, and have achieved the best results by using SNN-Delays as the baseline. The middle layer of SNN-Delays consists of one or more delay modules, and its structure has been described in Section II and III. For the SHD dataset, since its middle layer only has one delay module, we first perform NAR between the input and output of the middle layer. Next, we apply TR-no on the residual connection result, selecting the within-group time length to be 3 and the stride to be 1, and then send the result to the output block. For the SSC and GSC datasets, since the middle layer of the model is composed of 2 delay modules, we use NAR behind each delay module in middle layer. Then, if we use TR-no, we set the within-group time length to 3 or 2 with a stride of 1. For TR-o, we set the within-group time length to 2 and the stride to 1.\nC. Throughput\nThroughput refers to the number of samples that the model can process in one second. We use the shape of the first batch of samples from each dataset as the standard batch size for calculating throughput, and then generate a tensor of the corresponding size with batch size using PyTorch. By inputting this tensor into the respective model and running it one thousand times, we can calculate the number of samples processed by the model in one second, which is the throughput of this paper.\nD. Energy Consumption\nThe energy consumption for synaptic computation mainly comes from the accumulator (0.9pJ) and the multiplier (4.6pJ) in the neuromorphic chip [43]. For pure SNN models, the energy consumption is calculated by first determining the number of activated synapses and then multiplying that number by the energy consumption of the accumulator per operation. If the model includes ANN components, it is necessary to calculate the FLOPS for those components, the energy consumption for the ANN part is then the corresponding FLOPS multiplied by the energy consumption of the multiplier per operation. Therefore, the formulas for calculating energy consumption are as follows:\n$EC_{pure}^{SNN} = Synapses_{activated} \\times 0.9$ (6)\n$EC_{non-pure}^{SNN} = Synapses_{activated} \\times 0.9 + FLOPS_{ANN} \\times 4.6$ (7)\nE. Result\nFirstly, by using the SNN-Delays model as the baseline, we have validated the advancement and effectiveness of our methods on the SSC dataset, as is shown in Table I. From Table I, it can be found that throughput, energy consumption, and classification accuracy have been improved by our methods. Especially, if the SNN-Delays (3L-2KC) is integrate with the TR and NAR method, the classification accuracy on the SSC dataset has been improved by 0.33% to reach 81.02%, establishing a SOTA result for all SNN models on the SSC dataset.\nFurthermore, we have also validated our methods on the SHD dataset by using STSC-SNN, RadLIF, and SNN-Delays as the baseline models. As is shown in Table II, our methods have achieved varying degrees of improvement over the baseline models in different aspects including throughput, energy consumption, and classification accuracy. It is worth mentioning that the best results can be obtained if SNN-Delays model is used as the baseline, where the classification accuracy has improved by 0.94% to reach 96.04%, establishing a SOTA result for all models on the SHD dataset."}, {"title": "V. CONCLUSION", "content": "In order to learn the information of input data at different temporal scales and apply the residual connection to data of varying time lengths, we propose TR and NAR methods in this paper. TR groups the input data along time dimensions and then uses logical OR or MAX operations to reconstruct the frequency information of multiple time points within each group into the frequency information of a new single time point, which is divided into TR with overlap and TR without overlap. By using TR, we enable the model to learn information about the input data on a smaller time scale at the shallow layers and learn it on a larger time scale at the deep layers. As a result, the models can process data faster than baselines and can learn information at different temporal scales of the speech data. NAR allows the different time lengths data to perform residual connection, enabling the models with data of unequal lengths in the temporal dimension to benefit from this structure. By using the SNN-Delays model as baseline, we have achieved the desired results on the spiking datasets SSC, SHD, and the non-spiking dataset GSC through numerous experiments. On the dataset SSC, we have achieved the SOTA result 81.02% for the test classification accuracy of all SNN models. And we have obtained the SOTA result 96.04% on the dataset SHD for the classification accuracy of all models. On the non-spiking dataset GSC, our methods also have brought improvements to the baseline model. Meanwhile, our methods have also increased the models' throughput and reduced their energy consumption."}]}