{"title": "Adam-mini: Use Fewer Learning Rates To Gain More", "authors": ["Yushun Zhang", "Congliang Chen", "Ziniu Li", "Tian Ding", "Chenwei Wu", "Yinyu Ye", "Zhi-Quan Luo", "Ruoyu Sun"], "abstract": "We propose Adam-mini, an optimizer that achieves on-par or better performance than AdamW with 45% to 50% less memory footprint. Adam-mini reduces memory by cutting down the number of learning rates in Adam: Instead of assigning an individual learning rate for each parameter using $1/\\sqrt{v}$, Adam-mini uses the average of $v$ within a pre-defined parameter block as the learning rate for that block. Such a design is inspired by two empirical findings. First, the Hessian of Transformers exhibits a near-block diagonal structure with different sizes of dense sub-blocks. Second, for each of these dense sub-blocks, there exists a single high-quality learning rate that can outperform Adam, provided that sufficient resources are available to search it out. Adam-mini provides one cost-effective way to find these good learning rates and manage to cut down \u2265 90% $v$ in Adam. Empirically, we verify that Adam-mini performs on par or better than AdamW on various language models sized from 125M to 7B for pre-training, supervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini also alleviates communication overheads among GPUs and CPUs, thereby increasing throughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamW when pre-training Llama2-7B on 2\u00d7 A800-80GB GPUs, which saves 33% wall-clock time for pre-training 1.", "sections": [{"title": "Introduction", "content": "Adam(W) [21, 27] has become the de-facto optimizer for training large language models (LLMs) (e.g., [49, 2, 48, 46]). Despite its superior performance, Adam is expensive to use. Specifically, Adam requires the memory for its optimizer states: the first-order momentum $m$, and second-order momentum $v$. These in total take at least 2\u00d7 the memory of the model size. This memory consumption has become a major burden in LLM training. For instance, to train a 7B model, Adam alone requires about 56 GB per card for $m$ and $v$, and with the gradients included, a total of 86 GB is needed. This is expensive even for cutting-edge graphical cards (e.g., A100-80GB). To support the high-memory algorithm, CPU-offload and sharding [40] must be used in practice, which unfortunately increases the latency and slows down the training [41]. The situation further deteriorates when training larger models like PaLM with 540B parameters [6]. In this case, Adam alone occupies more than 50 GPUs and has become a major overhead for pre-training.\nIt is intriguing to design effective optimizers that require less memory. First, the reduction in memory can ease the burden on CPU offloading, and alleviate the need to shard model parameters. All these reduce communication among GPUs and CPUs, which in turn, can enhance throughput and accelerate the training process. Second, it allows practitioners to use fewer GPUs to train a model with desired size, leading to substantial saving in both cost and energy. Third, it lowers the threshold for training LLMs and encourages participation from more researchers with limited GPU resources.\nIt is challenging to modify Adam without sacrificing performance. One primary reason is that we still lack understanding for the role of Adam's $m$ and $v$ [55, 23]. It remains uncertain which components in Adam are indispensable for superior performance, and consequently, which elements could be re-designed and improved. One popular attempt is Adafactor [44], which cuts down memory by low-rank factorization on $v$. However, Adafactor is widely reported to suffer degenerated performance for training large language models (e.g. [29]). There are two possible reasons. First, the current $v$ in Adam might be crucial for effectiveness and has no room to cut down. This is possible because most existing Adam variants proposed to modify $v$ to varying extents, but they are reported to perform worse than Adam [34]. Second, it is possible to cut down $v$, but Adafactor did not use the most suitable way: matrix factorization is a generic approach that could be applied broadly, but does not leverage much problem-specific structure, thus does not work well on specific neural-net tasks.\nIn this work, we find it is possible to significantly reduce the usage of $v$ by a simple trick. Currently, Adam's $v$ assigns an individual learning rate for each parameter, i.e., i-th parameter receives learning rate $\\frac{\\eta}{\\sqrt{v_i}}$, where $v_i$ is the i-th component of $v$. For a billion-sized model, Adam designs billions of learning rates. We argue that it is possible to achieve on-par or better performance with much fewer learning rate resources. We notice that Transformer's Hessian has a near-block diagonal structure consisting of different sizes of dense sub-blocks. For each of these dense sub-blocks, there exists a single high-quality learning rate that outperforms Adam, provided that we have enough resources to search them out. Since the number of dense sub-blocks is much fewer than the number of parameters, our findings imply that it is possible to achieve good performance with much fewer learning rates. The remaining question is how to find them efficiently.\nWe then propose a cheap and simple way to find good learning rates that are sufficient to performs on-par or better than Adam. We introduce the crucial design principle here: we first partition the gradient vector into $B$ sub-vectors according to the dense Hessian sub-block, and call it $g_b$ for $b = [B]$. For each $g_b$, we calculate the quantity below.\n$\\tilde{v}_b = \\left( \\frac{\\eta}{\\sqrt{\\tilde{v}_b}} \\right) * mean(g_b \\circ g_b) + \\beta_2 * \\tilde{v}_b, b = 1,... B$\nWe then use $\\eta/\\sqrt{\\tilde{v}_b}$ as the learning rate for the parameters associated with $g_b$. We call the corresponding method Adam-mini. We provide a simple illustration in Figure 2 and relegate the complete form later in Algorithm 1. As a result, Adam-mini changes almost all Adam's $v$ to a handful of scalars and thus significantly saves memory of Adam. We summarize our main contribution as follows.\n\u2022 New optimizer. We propose a new optimizer called Adam-mini. First, Adam-mini partitions the model parameters based on the principles we established upon the Hessian structure. Then,"}, {"title": "Method", "content": "Now we discuss our observations that motivate the design of Adam-mini. We start by investigating the role of Adam's $v$ and explore possibilities for improvement. In Adam, $v$ provides an individual learning rate for each parameter, i.e.,i-th parameter receives the learning rate $\\frac{\\eta}{\\sqrt{v_i}}$, where $v_i$ is the i-th component of $v$. Recently, Zhang et al. [57] point out that such design is crucial because Transformers need different learning rates to different blocks. They provide two evidence: First, the Hessian of Transformers and various neural nets are near block diagonal (restated in Figure 3). Second, for Transformers, each block has a dramatically different eigenvalue distribution (restated in Appendix A.2). Combining together, Transformer needs different learning rates for each block to handle the eigenvalue heterogeneity. This could be provided by Adam's $v$.\nThe findings in [57] suggest that it is necessary to use a different learning rate for each block. Nonetheless, Adam does much more than that: it assigns an individual learning rate not just for each block, but for each parameter. Note that the number of parameters (could be billions) is much larger than the number of blocks (usually hundreds). This begs the question:\nIs it necessary to use an individual learning rate for each parameter? If not, how much can we save?"}, {"title": "Proposed Method: Adam-mini", "content": "Based on the discussion above, we propose Adam-mini in Algorithm 1. Adam-mini aims to cut down the learning rate resources in Adam without laboriously grid-search the learning rates as in Section 2.1. The design of Adam-mini consists of two steps. Step 1 is only required at the initialization.\nStep 1-1. We partition the model parameters into blocks. For Transformers, we use the strategy of \"Partition for Transformers\", which partitions all Querys and Keys by heads and uses the default PyTorch partition for the rest. For other networks, we use the default PyTorch partition, and we call it \"Partition for non-Transformers\". We discuss its design principle later in Section 2.3.\nStep 1-2. We choose emb_block. For Transformers, it includes the embedding layer and output layer. For other networks, no parameters will be chosen.\nStep 2. For each parameter block that is outside the emb_block, we use a single learning rate. To efficiently choose a suitable learning rate in each block, Adam-mini simply replaces $g \\circ g$ in vanilla Adam by its mean value. We adopt the moving average on these mean values as in Adam.\nA simple example. We use a simple example to illustrate the key design of Adam-mini. For a problem with 5 parameters $w \\in R^5$, Adam and Adam-mini both perform $w = w - \\frac{\\eta}{\\sqrt{v}} \\circ m$, where $m$ is the 1st-order momentum and $u$ has different forms as follows:\n\u2022 For Adam: $u_{Adam} = (\\frac{\\eta}{\\sqrt{v_1}}, \\frac{\\eta}{\\sqrt{v_2}}, \\frac{\\eta}{\\sqrt{v_3}}, \\frac{\\eta}{\\sqrt{v_4}}, \\frac{\\eta}{\\sqrt{v_5}})$.\n\u2022 For Adam-mini: suppose the partition is (1, 2, 3) and (4, 5) then\n$u_{mini} = (\\frac{\\eta}{\\sqrt{(v_1 + v_2 + v_3)/3}}, \\frac{\\eta}{\\sqrt{(v_1 + v_2 + v_3)/3}}, \\frac{\\eta}{\\sqrt{(v_1 + v_2 + v_3)/3}}, \\frac{\\eta}{\\sqrt{(v_4 + v_5)/2}}, \\frac{\\eta}{\\sqrt{(v_4 + v_5)/2}})$.\nNote that the number of effective elements $u_{mini}$ equals # blocks, which could be significantly smaller than that of $u_{Adam}$, which equals # parameters. For LLMs, we will show that this would free \u2265 90% elements in $v$.\nRemark on the \"emb_block\". In Transformers, \u201cemb_block\u201d refers to the embedding layer and output layer. We explain why Step 2 does not apply to these blocks. This is because these blocks do not reconcile with the average operation in Step 2: By design of the embedding layer, many rows will be zero if the tokens corresponding to these rows do not appear in the current data minibatch. In other words, the only non-zero row is the row that represents the current input word [38]. As such, taking an average over the entire embedding layer will absorb a significant amount of zeros, which leads to highly biased learning rates. A similar argument also applies to the output layer. In Figure 8 (a), we show that removing the emb_block causes training instability."}, {"title": "Principle for the Partition Strategy", "content": "We now discuss how to choose the parameter partition for Adam-mini. Based on the analysis in Section 2.1, a general principle is shown in Principle 1. Building upon this principle, we now present the corresponding partitioning strategy in the context of neural networks.\nPrinciple 1: We should partition parameters into blocks, s.t., each parameter block is associated with the smallest dense sub-blocks in Hessian.\nBased on the block-diagonal structure reported in the literature (Figure 3), the default partition in PyTorch would be a reasonable candidate. In Section 3, we will show that this partition indeed works well widely on non-Transformer tasks such as ResNet, diffusion model, and graph models. We call this strategy \"Partition for non-Transformers\"."}, {"title": "Experiments", "content": "We now verify the efficacy of Adam-mini. We primarily focus on LLM tasks, including pre-training, supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). We will also evaluate Adam-mini on non-LLM tasks. All LLM experiments are conducted on four NVIDIA A800-80GB GPUs and the rest are conducted on four V100 GPUs. All the experimental details are explained in Appendix B.1."}, {"title": "Pre-training", "content": "Setups. We pre-train open-sourced LLMs including GPT2 series and Llama series. We train these models on mainstream English Corpus from scratch. In particular, We train GPT2 [39] series"}, {"title": "Supervised Fine-tuning and RLHF", "content": "In this section, we evaluate the effectiveness of Adam-mini for downstream fine-tuning tasks. Specifically, we consider two representative tasks: supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). We use the Llama-2-7b pretrained model (released by Meta [48]) for our study. We use the ultrafeedback dataset and implement the RLHF workflow from [35]."}, {"title": "Non-LLM Tasks", "content": "We now evaluate Adam-mini on non-LLM tasks. Table 4 shows the results for training ResNet18 on ImageNet; Diffusion model on CelebA; Graph Convolution Net (GCN), Graph Attention Net (GAT) on OGB-arxiv. We find that Adam-mini reaches comparable or better performance than AdamW, but with less memory."}, {"title": "Related works", "content": "Understanding of Adam. There is an active line of works trying to understand why Adam works well [54, 52, 56, 51, 36, 20, 3, 57]. In contrast to these works, we point out that Adam's $v$ might not function at its full potential as effectively as we expected: sometimes fewer learning rates can reach the same or better results (due to the dense Hessian sub-blocks). Our findings might motivate stronger optimizers that better fit the neural-net Hessian structure.\nLightweight optimizers for general tasks. There are several attempts to reduce the memory cost of Adam. Adafactor [44] and its variant CAME [29] conduct nonnegative low-rank factorization over Adam's $v$. SM3 [4] is a lightweight version of Adagrad [12]. SM3 chooses the learning rate of i-th parameter by taking the minimal value in a certain candidate set, and each element in the candidate set is related to the maximal squared gradient under a predetermined cover. These methods could release almost all memory for $v$ and save about 48% of Adam's memory. However, we find that their performance degenerate in various experiments, while Adam-mini maintains as effective as AdamW.\nLightweight methods for SFT. To fine-tune LLMs under limited resources, researchers proposed to use SGD [31] or its zeroth-order version [32]. Yet, these methods converge more slowly than AdamW"}, {"title": "Concluding Remarks", "content": "We proposed Adam-mini, an optimizer that saves 45% to 50% memory of Adam. One limitation is that Adam-mini does not change Adam's $m$, which is also a heavy overhead. We believe there is room to simplify $m$ and leave it as a future direction."}, {"title": "Broader Impacts", "content": "We propose a new method for training AI models. Our method can help save energy for large AI model training. However, it would be a potential threat if the AI models are used for illegal usage."}, {"title": "More Results", "content": "We introduce Adam and AdamW in the Pytorch style in Algorithm 2 and 3. These methods need to track $m$ and $v$ along the training. Both $m$ and $v$ are vectors of the same size as # model parameter."}, {"title": "Prelimiary results in [57]", "content": "We here restate [57, Figure 3]. This figure shows that: for Transformers, different parameter blocks have different Hessian eigenvalue distributions, while for CNNs, the eigenvalue distributions are similar among blocks. This suggests that Transformers need different learning rates for different blocks to handle the heterogeneity in eigenvalue distributions."}, {"title": "More Hessian sub-blocks in Figure 6", "content": "In Figure 6, we present the Hessian of Query, Key, Value and MLP_proj in the 2nd layer. We now comprehensively provide Hessian in all attention and MLP blocks in all layers in the 4-layer Transformer used in Figure 6. We plot the Hessian at initialization. Experimental details can be seen in Appendix B.2. As we can see below, almost all these Hessian sub-blocks belong to either Class 1 or Class 2. The only exception is the MLP_fc block in the 4th layer, which shows a diagonal pattern."}, {"title": "Embedding Proportion in LLMs", "content": "Table 5 shows the proportion of the embedding & output parameters in mainstream LLMs including GPT2 [39], Llama series [48], Phi-3 [1], and Gemma [47]. We find that the embedding & output layer takes a fairly small proportion of the total parameters, usually \u2264 10%."}, {"title": "Comparison with Popular Memory-Efficient Optimizers", "content": "We now compare Adam-mini to popular memory-efficient optimizers. Adam-mini can simultaneously achieves memory reduction, same (or higher) throughput, and same (or better) performance than AdamW."}, {"title": "Training configurations for Section 3", "content": "For all experiments, we choose the model configurations (e.g. context length) by their standard protocols. We choose the learning rates by the recommendation from open-source platforms if applicable. For instance, for GPT2 series, we use the recommended learning rates by [26], which are reported to be optimal by grid search. Unless mentioned otherwise, Adam-mini, Adafactor, SM3, and Galore use the same learning rate as the recommended ones of AdamW. We choose the learning rate of CAME following the suggestions from the authors 6. If there is no public recommended learning rate for AdamW, we tune the learning rate for all optimizers within the same computational budget and report the best performance. For other hyperparameters, we follow the recommendation from open-source platforms or by their default setting. For instance, for Galore, we used the recommended hyperparameters 7 including target_modules_list = [\"attn\", \"mlp\"], rank = 128, update_proj_gap = 200, scale = 0.25, proj_type= std. For SM3 and Adafactor, we incorporate momentum with \u03b2\u2081 = 0.9 to offer a fair comparison with other optimizers and the rest of the hyperparameters are set as default. The detailed configurations are explained as follows.\nWe use the nanoGPT codebase 8 to train GPT2 sized 125M (small), 330M (medium), 770M (large), and 1.5B (XL) on Openwebtext. For all models, we use context length = 1024, batch size = 480, weight decay coefficient x = 0.1, \u20ac = 1e-8, \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.95. We use cosine-decay learning rate schedule with 2000 iterations of warm-up. For GPT2-small, medium, and large, we use the recommended peak learning rate by [26], which are reported to be the optimal ones found by grid search. For GPT2-XL, we use the recommended peak learning rate by the Levanter 9. The chosen peak learning rates are 6e-4, 3e-4, 2e-4, 1e-4 for GPT2-small, medium, large, XL, respectively. The minimal learning rate is chosen as 3e-5, 6e-5, 1e-5, 1e-5 for these models."}, {"title": "Llama pre-training.", "content": "We use TinyLlama codebase 10 to train TinyLlama-1B and Llama2-7B on CommonCrawl 11. We use batch size = 40. We preprocess the directionary \u201c2019-30\u201d of CommonCrawl into 10850 bins, which in total gives 85GB of tokens. We leave the last 20 bins out as the validation set. For both models, we use weight decay coefficient X = 0.1, \u20ac = 1e-8, \u03b2\u2081 = 0.9, \u03b22 = 0.95. We use cosine-decay learning rate schedule with 2000 iterations of warm-up. For TinyLlam-1B, we use context length = 2048, batch size = 512, peak learning rate = 2e-4, and minimal learning rate = 2e-5. For Llama2-7B, we use context length = 4096, batch size = 256, peak learning rate = 8e-5, and minimal learning rate = 8e-6."}, {"title": "SFT and RLHF.", "content": "The implementation of SFT and RLHF code is based on the ReMax codebase12. We use DeepSpeed ZeRO-2 in our training. GPT-4 evaluation template in Table 3 is from the codebase13. In the reward optimization stage, We use ReMax, a memory-efficient alternative to PPO. We use UltraFeedback dataset [8] and use 40% data for SFT and 60% data for ReMax.\nWe use 80 samples in a batch and train the model for 3 epochs. For the full parameter tuning, we search the learning rate from {1e-6, 2e-6, 3e-6, 4e-6, 5e-6, 1e-5, 2e-5} based on validation loss, and we use 2e-6 with cosine annealing for both AdamW and Adam-mini. For LoRA, We apply LORA for all layers except the embedding layer. The rank of LoRA is set to 128. After selecting the learning rate from the same set as the full parameter tuning, we use 5e-6 for both AdamW and Adam-mini when LoRA is applied. The weight decay coefficient is set to 0 as recommended by LlamaFactory 14. The rest of the hyperparameters of AdamW and Adam-mini are \u20ac = 1e-8, \u03b2\u2081 = 0.9, \u03b22 = 0.95.\nWe use 48 samples in a batch and train the model for 1 epoch. By searching the peak learning rate from {5e-7, 1e-6, 2e-6} based on validation reward, AdamW uses le-6 while Adam-mini selects 5e-7 as the peak learning rate. The weight decay coefficient is set to 0. The rest of the hyperparameters of AdamW and Adam-mini are e = 1e-8, \u03b2\u2081 = 0.9, \u03b22 = 0.95."}, {"title": "ResNet.", "content": "We use the PyTorch official implementation codebase15 to train ResNet18 [16] on ImageNet [10]. We use cosine-decay learning rate, epoch =90, \u03b2\u2081 = 0.9, \u03b22 = 0.999, \u20ac =1e-8. For ResNet18, we use batch size = 256, peak learning rate = 0.005. For ViT-base, we use batch size = 128, peak learning rate = 0.0001. These configurations are used for both Adam-mini and AdamW."}, {"title": "Diffusion models.", "content": "We use the codebase16 to train diffusion models. The image size is 64 and the training objective is to predict the noise as in [17]. We use the default U-Net archiecture hyperparameters and the dimension multiply in U-Net is (1, 2, 4, 8). We use the CelebA dataset\u00b9 and train the diffusion model with a learning rate 5 \u00d7 10-5 with cosine decay. The batch size is 128 and the training epoch is 50."}, {"title": "Graph Neural Networks.", "content": "We use the DGL implementation 18 of Graph Convolution Networks (GCN) [22] and Graph Attention Networks (GAT) [50] for OGBN-arxiv 19 dataset. All configurations as default. For both Adam-mini and AdamW, we use the default learning rate = 0.005 for GCN and the default learning rate = 0.002 for GAT."}, {"title": "Detailed Setup for Other Experiments", "content": "For each dense sub-block $H_{i,l}$, $l = 1,2,3$, we use random positive definite matrices. We fix the choose the eigenvalues of each $H_l$ as follows: for $l = 1$, we independently sample from {1,2,3} for 30 times; for $l = 2$, we repeat this procedure for {99,100,101}; for $l = 3,"}]}