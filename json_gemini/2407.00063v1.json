{"title": "An Interpretable Alternative to Neural Representation Learning for Rating Prediction - Transparent Latent Class Modeling of User Reviews", "authors": ["Giuseppe Serra", "Peter Ti\u0148o", "Zhao Xu", "Xin Yao"], "abstract": "Nowadays, neural network (NN) and deep learning (DL) techniques are widely adopted in many applications, including recommender systems. Given the sparse and stochastic nature of collaborative filtering (CF) data, recent works have critically analyzed the effective improvement of neural-based approaches compared to simpler and often transparent algorithms for recommendation. Previous results showed that NN and DL models can be outperformed by traditional algorithms in many tasks. Moreover, given the largely black-box nature of neural-based methods, interpretable results are not naturally obtained. Following on this debate, we first present a transparent probabilistic model that topologically organizes user and product latent classes based on the review information. In contrast to popular neural techniques for representation learning, we readily obtain a statistical, visualization-friendly tool that can be easily inspected to understand user and product characteristics from a textual-based perspective. Then, given the limitations of common embedding techniques, we investigate the possibility of using the estimated interpretable quantities as model input for a rating prediction task. To contribute to the recent debates, we evaluate our results in terms of both capacity for interpretability and predictive performances in comparison with popular text-based neural approaches. The results demonstrate that the proposed latent class representations can yield competitive predictive performances, compared to popular, but difficult-to-interpret approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, with the advent of more powerful and capable machines, researchers have started focusing more and more on developing (deep) neural architectures for a wide range of applications. The success of neural-based approaches in different domains, as language modeling [1], [2] or computer vision [3], [4], led these models to also dominate the recommender systems research area [5], [6], [7]. However, recent works have raised some concerns about the relative performance improvements of deep learning approaches compared to simpler algorithms for recommendation tasks [8], [9], [10]. Indeed, as shown in previous works [11], [12], new recommendation methods do not significantly outperform existing approaches or even can be outperformed by very simple methods, e.g. nearest-neighbor-based techniques [13]. Previous investigations in this direction were mainly focused on pure collaborative filtering (CF) data, where the only available input is the rating matrix. Nevertheless, recent studies in this area tend to conclude that numerical rating data are not informative enough for discovering user preferences. Consequently, given the availability of large collections of textual data, such as product reviews or social media posts, many approaches have tried to extend and improve recommendation models by leveraging such textual information [14], [15], [16], [17], [18].\nIn fact, corpora of textual documents contain a wealth of information. They can be used to improve the predictive performances of recommendation systems, but more importantly, they provide human understandable explanation about user preference and product properties. To integrate the extra textual information into recommendation systems, most existing works employ embedding methods, i.e., representing items, words, and documents as vectors (also known as embedding) for better flexibility. Although the embedding-based methods often provide good predictions, the resulting embeddings are usually not explainable; if singularly evaluated, a 100D or 200D vectors can be hard to comprehend by humans.\nInspired by recent discussions about the \"neural hype\" [11], in this paper we investigate whether using a simpler, more transparent and principled way to learn user and product latent representations can lead to comparable results in the recommendation task, i.e. review-based rating prediction. We present a probabilistic framework for the topographic organization of review data. In contrast with previous neural-based works, we impose a double two-dimensional topological organization of user and product latent classes based on the textual information. As a result, the latent classes of users and products are organized on two different square grids that reflect the textual input space. The grid organization makes the investigation and interpretation of the results fast and intuitive. Additionally, the probabilistic assumptions of our system enable us to analyze the extracted information in a statistical manner.\nWith the interpretable topographically organized latent space representations obtained in our probabilistic framework, one can naturally solve many downstream learning tasks, such as product rating prediction. Motivated by the strong correla-"}, {"title": "II. RELATED WORKS", "content": "There are two lines of research related to the work.\nA. Topographic Organization in Latent Models\nKohonen's seminal work on self-organizing maps (SOM) [19] was introduced in the 1980s. Given the ability to produce low-dimensional representations of high-dimensional data while providing a good approximation of the input space, many extensions and advancements have been proposed in later years. Generative topographic mapping (GTM) [20] is one of the most popular probabilistic alternatives to SOM. GTM provides a generative extension of the SOM, assuming a specific discretised non-Gaussian latent prior. Applications and extensions of SOM span many different domains. For example, in [21], the author proposed a data-driven, statistical approach to visualize large collections of text documents using two-dimensional maps. In CF applications, [22] introduced a topographic organization of latent classes for rating preferences. Differently from their work, where the user preferences are organized using the numerical information, in this paper, we propose to induce a topographic organization of both user and product latent classes exploiting the associated textual information. As a result, we obtain two separate grids (one for users and one for products) reflecting the word patterns of the data. As reported in [21], the most nuanced and sophisticated medium to express our feelings is our language. We believe that it is important to understand and organize the review information in a structured and intuitive way.\nB. Text-based Recommendation Models\nDespite the statistical foundation and the nice visualization capabilities of the previous methods, with the advent of more powerful and capable machines, researchers started focusing more and more on developing deep neural architectures for recommendation. More generally, for this task, one of the most popular approaches is matrix factorization (MF) and, specifically, Singular Value Decomposition (SVD). This method maps users and items into a latent factor space and computes the rating as a dot product between the user and the product embeddings. Although such approaches are effective and simple, the results are poorly interpretable. Indeed, the embeddings of users and items are not explainable and, not knowing what each feature means, it is impossible to unveil user preferences and product characteristics [23].\nGiven the availability of large collections of product reviews, researchers have recently extended latent factor models to leverage the textual information for improving rating prediction performances. In fact, recent studies in this area"}, {"title": "III. PROPOSED FRAMEWORK", "content": "In this section, we present the key ingredients of our framework. Based on our observation that analysis of the textual latent patterns is often not fully covered, we propose an interpretable review-based probabilistic model for rating prediction. First, we describe the estimation of the model parameters. Then, to make our model able to integrate new user data after training, we propose an out-of-sample extension allowing us to compute the latent class assignments of new users that reviewed products available in our data set. Finally, we present how to exploit the estimated latent space representations as model inputs for a rating prediction task.\nA. Topological Organization of the Latent Model\nConsider a collection of users $U = \\{u_1, u_2, ..., u_n\\}$, products $P = \\{p_1, p_2,\\dots,p_M\\}$ and words $V = \\{w_1, w_2, ..., w_v \\}$. The data $D$ is a collection of $R$ triples $D = \\{(u^i, p^i, r^i)\\}_{i=1}^R$, each triple identifying the user $u^i \\in U$ writing a review $r^i$ on product $p^i \\in P$. The review $r_i$ is a multi-set of words from $V$, $r^i = (w_1, w_2, ..., w_s)$, $w_j \\in V$. The latent variables $Z_u \\in \\{1,...,K\\}$ and $Z_p \\in \\{1,...,L\\}$ represent abstract classes of users and products.\nGiven a review $i$, the probability of sampling a word $w_j \\in r^i$ is modeled as:\n$P(w_{u^i, p^i}) = \\sum_{k=1}^K \\sum_{l=1}^L (P(w | Z_u=k, Z_p=l) \\cdot P(Z_u = k | u^i) P(Z_p = l | p^i))$   (1)\nWe impose a grid topology on latent classes via the channel noise methodology [21], [22]. Let's assume $y$ and $z$ are two different latent classes in our grid. The channel noise for both the product and user latent class grids is defined using the neighborhood function:\n$P(y|z) = \\frac{exp(-\\frac{||z-y||^2}{2\\sigma^2})}{\\sum_{y'} exp(-\\frac{||z-y'||^2}{2\\sigma^2})}$  (2)\nwhere $\\sigma > 0$ controls the 'concentration' of the transition probabilities among the neighbors of the latent class $y$. Overall, when $y$ and $z$ are close to each other on the grid, the probability of being corrupted one into another is higher than when they are distant. Additionally, when $\\sigma$ is close to 0, then the transition probabilities are more concentrated around $y$ than for larger values of $\\sigma$.\nFor each user $u \\in U$, the generative process is as follows:\n1) the latent class assignment $z_u = k$ is randomly sampled from the user-conditional probability distribution $P(\\cdot | u)$ on $Z_u$;"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the performance of our proposed framework. First, we analyze the probabilistic latent model and the generated organization of user and product latent classes. Second, we investigate the generative extension of our approach. Last, we compare the performance of our model with state-of-the-art approaches for the rating prediction task."}, {"title": "V. CONCLUSION", "content": "Recently, several approaches for rating predictions of textual reviews in the framework of deep neural networks have appeared in the literature [14], [27], [15], [16], [17], [18]. Given the highly stochastic nature of the data and relative data sparsity, one can legitimately ask to what extent can the full predictive power of deep networks be utilized in this context. The question is even more relevant when one realises that clear interpretability of the deep network functionality is still an open problem [40]. To answer this question, we present an approach for product rating prediction using a relatively simple and interpretable latent class probabilistic model utilizing topographic organization of user and product latent classes based on the review information. In existing works, the review information is usually exploited to enhance the rating prediction performances, but is not fully inspected to understand user and product features. In contrast, we propose a deeper understanding of the data, presenting a two-step approach that starts from the interpretable organization of textual information to arrive at the rating prediction task. The organization of the latent classes on 2-dimensional grids provides a visualization tool that can be used to statistically investigate user and product features from a review-based perspective. Through this organization, we can arrange complicated and unstructured textual data in a simple way. The thorough analysis in the experimental section demonstrates the ease of analyzing the latent review patterns using tools from probabilistic theory. The visualization of the results, presented in sections IV-C and IV-D, shows that the lower-dimensional latent representations of users and products are a good approximation of the textual input space. Consequently, driven by the assumption that ratings and reviews are strongly correlated, we propose to use the resulting latent features as input for a rating prediction task. In this part, we contribute to the debate about the \"phantom progress\" of deep learning approaches for recommendation tasks. Our investigation, differently from the previous ones, is mainly focused on methods that take advantage of the textual information for rating prediction tasks, and includes an evaluation that considers the capacity of such models to represent users, products, and reviews utilizing interpretable latent vectors. The results suggest that, also in the textual-based case, the use of dense and complicated representations is not fully motivated. Indeed, even though our representations are not learned for a rating prediction task specifically, the results are comparable to models that learn ad-hoc representations. Nonetheless, being highly interpretable, the proposed latent representations overcome the limitations of the common embedding techniques used in most of the considered previous works. Finally, the prediction results of our linear regression model suggest that we do not need to implement deep architectures either. This simple model is able to outperform some of the baselines and get comparable results with the remaining ones, while being fully transparent. Naturally, there is always a trade-off between model capabilities and interpretability. The nice and explainable visualization properties of our constrained model may affect the modeling capabilities for rating prediction. On the other hand, better modeling capabilities through common embedding techniques and deep architectures provide representations that are created for performing well on a specific task, but at the price of losing the human interpretation of the results."}, {"title": "A. E-step", "content": "First, we estimate $P(z_u^i, z_p^i|w, u,p)$ as:\n$\\frac{P(w|u, p, z_u^i, z_p^i) P(z_u^i|u, p) P(z_p^i|u,p)}{\\sum_{k'', l''}P(w|u, p, z_u^{k''}, z_p^{l''})P(z_u^{k''}|u, p)P(z_p^{l''}|u,p)}$   (27)\nBy model assumptions, $z_u$ is independent from products and $z_p$ is independent from users, so we have $P(z_u^i|u, p) = P(z_u^i|u)$ and $P(z_p^i|u,p) = P(z_p^i|p)$. Consequently:\n$P(w|u, p,z_u^i, z_p^i) = \\sum_{k', l'} \\sum_k \\sum_l P(w, y_u^{k'}, y_p^{l'}|z_u^{k}, z_p^{l}, u, p)=\\sum_{k', l'} \\sum_k \\sum_l P(w| y_u^{k'}, y_p^{l'}) P(y_u^{k'}|z_u^{k}) P(y_p^{l'}|z_p^{l}) P(z_u^{k}|u) P(z_p^{l}|p)=\\sum_{k', l'} \\sum_k \\sum_l P(w| y_u^{k'}, y_p^{l'}) P(y|z)P(y|z)$,  (28)\nand so $P(z_u^i, z_p^i|w, u, p)$ is equal to\n$\\frac{P(z_u^{k}|u)P(z_p^{l}|p) \\sum_{k'} \\sum_{l'} S(k, l)}{\\sum_{k''} \\sum_{l''}P(z_u^{k''}|u)P(z_p^{l''}|p)\\sum_{k'} \\sum_{l'} S(k'', l'')}$   (29)\nwith\n$S(\\alpha, \\beta) = P(w|y_u^{k'}, y_p^{l'})P(y|z) P(y|z)$.\nAnalogously, to compute $P(y_u^{k'}, y_p^{l'}|w, u,p)$, we have:\n$\\frac{P(w|u, p, y_u^{k'}, y_p^{l'})P(y_u^{k'}|u, p)P(y_p^{l'}|u, p)}{\\sum_{k''} \\sum_{l'}P(w|u, p, y_u^{k''}, y_p^{l''})P(y_u^{k''}|u, p)P(y_p^{l''}|u, p)}$  (30)"}]}