{"title": "An Interpretable Alternative to Neural Representation Learning for Rating Prediction - Transparent Latent Class Modeling of User Reviews", "authors": ["Giuseppe Serra", "Peter Ti\u0148o", "Zhao Xu", "Xin Yao"], "abstract": "Nowadays, neural network (NN) and deep learning (DL) techniques are widely adopted in many applications, including recommender systems. Given the sparse and stochastic nature of collaborative filtering (CF) data, recent works have critically analyzed the effective improvement of neural-based approaches compared to simpler and often transparent algorithms for recommendation. Previous results showed that NN and DL models can be outperformed by traditional algorithms in many tasks. Moreover, given the largely black-box nature of neural-based methods, interpretable results are not naturally obtained. Following on this debate, we first present a transparent probabilistic model that topologically organizes user and product latent classes based on the review information. In contrast to popular neural techniques for representation learning, we readily obtain a statistical, visualization-friendly tool that can be easily inspected to understand user and product characteristics from a textual-based perspective. Then, given the limitations of common embedding techniques, we investigate the possibility of using the estimated interpretable quantities as model input for a rating prediction task. To contribute to the recent debates, we evaluate our results in terms of both capacity for interpretability and predictive performances in comparison with popular text-based neural approaches. The results demonstrate that the proposed latent class representations can yield competitive predictive performances, compared to popular, but difficult-to-interpret approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, with the advent of more powerful and capable machines, researchers have started focusing more and more on developing (deep) neural architectures for a wide range of applications. The success of neural-based approaches in different domains, as language modeling [1], [2] or computer vision [3], [4], led these models to also dominate the recommender systems research area [5], [6], [7]. However, recent works have raised some concerns about the relative performance improvements of deep learning approaches compared to simpler algorithms for recommendation tasks [8], [9], [10]. Indeed, as shown in previous works [11], [12], new recommendation methods do not significantly outperform existing approaches or even can be outperformed by very simple methods, e.g. nearest-neighbor-based techniques [13]. Previous investigations in this direction were mainly focused on pure collaborative filtering (CF) data, where the only available input is the rating matrix. Nevertheless, recent studies in this area tend to conclude that numerical rating data are not informative enough for discovering user preferences. Consequently, given the availability of large collections of textual data, such as product reviews or social media posts, many approaches have tried to extend and improve recommendation models by leveraging such textual information [14], [15], [16], [17], [18].\nIn fact, corpora of textual documents contain a wealth of information. They can be used to improve the predictive performances of recommendation systems, but more importantly, they provide human understandable explanation about user preference and product properties. To integrate the extra textual information into recommendation systems, most existing works employ embedding methods, i.e., representing items, words, and documents as vectors (also known as embedding) for better flexibility. Although the embedding-based methods often provide good predictions, the resulting embeddings are usually not explainable; if singularly evaluated, a 100D or 200D vectors can be hard to comprehend by humans.\nInspired by recent discussions about the \"neural hype\" [11], in this paper we investigate whether using a simpler, more transparent and principled way to learn user and product latent representations can lead to comparable results in the recommendation task, i.e. review-based rating prediction. We present a probabilistic framework for the topographic organization of review data. In contrast with previous neural-based works, we impose a double two-dimensional topological organization of user and product latent classes based on the textual information. As a result, the latent classes of users and products are organized on two different square grids that reflect the textual input space. The grid organization makes the investigation and interpretation of the results fast and intuitive. Additionally, the probabilistic assumptions of our system enable us to analyze the extracted information in a statistical manner.\nWith the interpretable topographically organized latent space representations obtained in our probabilistic framework, one can naturally solve many downstream learning tasks, such as product rating prediction. Motivated by the strong correlation between reviews and ratings, we believe that exploiting our simple, yet meaningful representations of textual review information will lead to competitive rating prediction results, while maintaining the explainability of the latent classes. We compare our results with state-of-the-art approaches, including both neural network (NN) based methods and non-NN ones. The main goal of this work is not to develop a new approach that outperforms the state-of-the-art with respect to a single predictive score (e.g. accuracy or AUC), but rather to propose a principled and interpretable method that focuses on learning a transparent latent structure of the review data, while having competitive rating prediction performances. Finally, inspired by the recent debates on the phantom progress [10] of NN-based methods for recommendation tasks, we compare our findings in terms of both interpretability and predictive performances with respect to the most popular text-based neural approaches for rating prediction.\nIn summary, the main contributions of this paper are three-fold:\n1) We present a topographic organization of user and product latent classes based on the latent structure of the review data. Common embedding techniques model each item by employing dense and complicated representations. In our case, we model classes of users and products resulting on latent vectors that are compact, interpretable and rather discrete. Also, different from existing works where the analysis of the user and product characteristics from the textual perspective is practically ignored, we propose a more complete investigation of the available information.\n2) In contrast with previous works, the probabilistic assumptions of the framework allow us to explicitly impose through model formulation the interpretability of the latent codes. Differently from neural-based models where explanations are typically generated through a modification of a difficult-to-interpret network (e.g., by integrating an attention mechanism), in our case, interpretability is at the core of the proposed approach. Rather than implementing an architecture capable of creating explanations without a true understanding of the data, we present a probabilistic model grounded on data understanding which, in turn, is inherently interpretable. In addition, the topographic organization can help us to understand the relationships among different latent classes. Indeed, classes close to each other in the topographic maps should exhibit similar patterns.\n3) Motivated by the strong correlation between reviews and ratings, we exploit our representations of the review information as input for a rating prediction task. We contribute to the debate about the effective performances of neural-based approaches in comparison with more principled and simpler methods. Differently from previous investigations, we present a comprehensive comparison mainly focused on text-based approaches for rating prediction, considering both interpretability capacity and predictive performances.\nThe rest of the paper is organized as follows. We start off with a brief review of related works. Afterwards we describe the proposed model. Before concluding, we present the experimental results on multiple data sets. In the experimental section, we perform both quantitative and qualitative evaluations of the proposed method."}, {"title": "II. RELATED WORKS", "content": "There are two lines of research related to the work.\nA. Topographic Organization in Latent Models\nKohonen's seminal work on self-organizing maps (SOM) [19] was introduced in the 1980s. Given the ability to produce low-dimensional representations of high-dimensional data while providing a good approximation of the input space, many extensions and advancements have been proposed in later years. Generative topographic mapping (GTM) [20] is one of the most popular probabilistic alternatives to SOM. GTM provides a generative extension of the SOM, assuming a specific discretised non-Gaussian latent prior. Applications and extensions of SOM span many different domains. For example, in [21], the author proposed a data-driven, statistical approach to visualize large collections of text documents using two-dimensional maps. In CF applications, [22] introduced a topographic organization of latent classes for rating preferences. Differently from their work, where the user preferences are organized using the numerical information, in this paper, we propose to induce a topographic organization of both user and product latent classes exploiting the associated textual information. As a result, we obtain two separate grids (one for users and one for products) reflecting the word patterns of the data. As reported in [21], the most nuanced and sophisticated medium to express our feelings is our language. We believe that it is important to understand and organize the review information in a structured and intuitive way.\nB. Text-based Recommendation Models\nDespite the statistical foundation and the nice visualization capabilities of the previous methods, with the advent of more powerful and capable machines, researchers started focusing more and more on developing deep neural architectures for recommendation. More generally, for this task, one of the most popular approaches is matrix factorization (MF) and, specifically, Singular Value Decomposition (SVD). This method maps users and items into a latent factor space and computes the rating as a dot product between the user and the product embeddings. Although such approaches are effective and simple, the results are poorly interpretable. Indeed, the embeddings of users and items are not explainable and, not knowing what each feature means, it is impossible to unveil user preferences and product characteristics [23].\nGiven the availability of large collections of product reviews, researchers have recently extended latent factor models to leverage the textual information for improving rating prediction performances. In fact, recent studies in this area tend to conclude that the numerical rating information is not powerful enough for discovering user preferences. One of the first attempts that demonstrates the usefulness of leveraging features extracted from reviews to improve the rating prediction accuracy was presented in [24]. Among other popular works in this direction, Hidden Factors as Topics (HFT) [14] learns topics using a Latent Dirichlet Allocation (LDA)-like model for each item and an MF for ratings. Ratings Meets Reviews (RMR) [25] uses the same LDA model for modeling the textual information, but it uses Gaussian mixtures for the rating prediction part. Similarly, TopicMF [26] learns topics from each review. In [27], instead of using an approach based on LDA, the explored methods are neural network-based. Most of the existing works combine two learning objectives; one (unsupervised) for the textual information, and one (supervised) for rating prediction. The unsupervised loss acts as a regularization term for the rating prediction loss while taking advantage of the review data. Consequently, the vector representations of the reviews are learned to work well for rating prediction tasks while preventing overfitting.\nAnother category of models uses deep learning approaches for learning latent representations of users and items. These methods mainly differ in the neural architecture they use. In [23], the authors propose an attention-based CNN (Attn+CNN) to build vector representations of users and products. In [28] they propose a hierarchical Bayesian model called Collaborative Deep Learning (CDL), which jointly performs deep representation learning and CF for the rating matrix. In [18], the model learns vector representations of users, items, and reviews. The review embedding is learned as a translation in the vector space between the user and the product embeddings.\nEven though many works have been published in this direction, as pointed out in [8] and [9], it is debatable whether deep-learning-based models are really making progress in this research area. Additionally, the majority of the existing works deal with embedding, but the resulting vector representations usually do not reflect any visualization-driven assumption of the data, making the output poorly explainable. Indeed, if singularly evaluated, common embeddings are not informative. Also, since the ultimate goal is to improve the rating prediction part, the textual information is solely used as an additional source to achieve this goal. Consequently, the latent information contained in the textual data is not fully exploited and investigated. Moreover, there have been related methods in the direction of explainable recommendations. These works address the problem of generating explanations through knowledge graph reasoning [29], neural attentive models [30], [31], [32], attraction modeling [33] or substitute recommendation systems [34]. However, they do not aim to explicitly generate interpretable vector representations. Instead, the main objective is to generate user-specific explainable recommendations using the complete textual information and highlighting words that are important to explain item-specific ratings. Differently, in our work, we propose a framework for deeper understanding of the data, presenting a two-step approach that starts from the interpretable organization of the textual information to arrive at the rating prediction task."}, {"title": "III. PROPOSED FRAMEWORK", "content": "In this section, we present the key ingredients of our framework. Based on our observation that analysis of the textual latent patterns is often not fully covered, we propose an interpretable review-based probabilistic model for rating prediction. First, we describe the estimation of the model parameters. Then, to make our model able to integrate new user data after training, we propose an out-of-sample extension allowing us to compute the latent class assignments of new users that reviewed products available in our data set. Finally, we present how to exploit the estimated latent space representations as model inputs for a rating prediction task.\nA. Topological Organization of the Latent Model\nConsider a collection of users $\\mathcal{U} = \\{u_1, u_2, ..., u_n\\}$, products $\\mathcal{P} = \\{p_1, p_2, \\dots, p_M\\}$ and words $\\mathcal{V} = \\{w_1, w_2, ..., w_v\\}$. The data $\\mathcal{D}$ is a collection of $R$ triples $\\mathcal{D} = \\{(u^i, p^i, r^i)\\}_{i=1}^R$, each triple identifying the user $u^i \\in \\mathcal{U}$ writing a review $r^i$ on product $p^i \\in \\mathcal{P}$. The review $r^i$ is a multi-set of words from $\\mathcal{V}$, $r^i = (w_1, w_2, ..., w_{S_i})$, $w \\in \\mathcal{V}$. The latent variables $Z_u \\in \\{1,...,K\\}$ and $Z_p \\in \\{1,...,L\\}$ represent abstract classes of users and products.\nGiven a review $i$, the probability of sampling a word $w_j \\in r^i$ is modeled as:\n$\\begin{equation}\nP(w|u^i, p^i) = \\sum_{k=1}^K \\sum_{l=1}^L (P(w|Z_u = k, Z_p = l)  P(Z_u = k|u^i)P(Z_p = l|p^i)).\n\\end{equation}$\nWe impose a grid topology on latent classes via the channel noise methodology [21], [22]. Let's assume $y$ and $z$ are two different latent classes in our grid. The channel noise for both the product and user latent class grids is defined using the neighborhood function:\n$\\begin{equation}\nP(y|z) = \\frac{\\exp(-\\frac{||z-y||^2}{2 \\sigma^2})}{\\sum_{y'\\in Y} \\exp(-\\frac{||z-y'||^2}{2 \\sigma^2})},\n\\end{equation}$\nwhere $\\sigma > 0$ controls the 'concentration' of the transition probabilities among the neighbors of the latent class $y$.\u00b9 Overall, when $y$ and $z$ are close to each other on the grid, the probability of being corrupted one into another is higher than when they are distant. Additionally, when $\\sigma$ is close to 0, then the transition probabilities are more concentrated around $y$ than for larger values of $\\sigma$.\nFor each user $u \\in \\mathcal{U}$, the generative process is as follows:\n1) the latent class assignment $z_u = k$ is randomly sampled from the user-conditional probability distribution $P(\\cdot|u)$ on $Z_u$;"}, {"title": "B. Inference and Learning", "content": "Assuming independent data items in $\\mathcal{D}$, the log-likelihood of the model is:\n$\\begin{equation}\n\\mathcal{L} = \\sum_{i=1}^R \\log P(r^i|u^i, p^i).\n\\end{equation}$\nWe will consider a simple review model assuming independence of the words appearing in the review $i$:\n$\\begin{equation}\nP(r^i|u^i, p^i) = \\prod_{j=1}^{S_i} P(w|u^i, p^i).\n\\end{equation}$\nHence, the log-likelihood reads:\n$\\begin{equation}\n\\mathcal{L} = \\sum_{i=1}^R \\sum_{j=1}^{S_i} \\log P(w|u^i, p^i).\n\\end{equation}$\nPlugging in eqs. (3-5), we obtain:\n$\\begin{equation}\n\\mathcal{L} = \\sum_{i=1}^R \\sum_{j=1}^{S_i} \\log \\sum_{k'=1}^K \\sum_{l'=1}^L P(w|y_u = k', y_p = l') \\left[ \\sum_{k=1}^K P(y_u = k'|z_u = k)P(z_u = k|u^i) \\sum_{l=1}^L P(y_p = l'|Z_p = l) P(Z_p = l|p^i) \\right].\n\\end{equation}$\nFor training, we use the Expectation-Maximization (EM) algorithm enabling maximum likelihood estimation (MLE) in latent variable models. It iterates the Expectation (E) and Maximization (M) steps until convergence. Detailed derivations of the following equations are presented in Appendix A.\nC. E-step\nIn the E-step, the algorithm evaluates the current estimates of the model parameters by computing the expected values of the latent variables. We will denote these quantities using $P(\\cdot)$. Note that we have two levels of hidden variables. First, given the user and the product they reviewed, we do not know which latent classes $z_u$ and $z_p$ represented the (user, product) couple when writing the review. Second, we know that the underlying latent classes $z_u$ and $z_p$ may have been disrupted to latent classes $y_u$ and $y_p$ before producing the review, but we do not know their identity. To simplify mathematical notation, we will denote $z_u = k$, $z_p = l$, $y_u = k'$ and $y_p = l'$ as $z_k$, $z_l$, $y_{k'}$ and $y_{l'}$, respectively. $P(z_k, z_l|w, u,p)$ is evaluated as:\n$\\begin{equation}\nP(z_k, z_l|w, u,p) = \\frac{P(w|u^i, p^i, z_k, z_l)P(z_k|u, p)P(z_l|u,p)}{\\sum_{k''=1}^K \\sum_{l''=1}^L P(w|u^i, p^i, z_{k''}, z_{l''})P(z_{k''}|u, p)P(z_{l''}|u,p)},\n\\end{equation}$\nwith\n$S(\\alpha, \\beta) = P(w|y_{k'}, y_{l'})P(y_{k'}|z_k)P(y_{l'}|z_l)$. Instead, $P(y_{k'}, y_{l'}|w, u,p)$ is computed as:\n$\\begin{equation}\nP(y_{k'}, y_{l'}|w, u,p) = \\frac{P(w|y_{k'}, y_{l'}) \\sum_{k''} G(k'') \\sum_{l''} W(l')}{\\sum_{k''} \\sum_{l''} P(w|y_{k''}, y_{l''}) \\sum_{k'''} G(k''') \\sum_{l''} W(l'')},\n\\end{equation}$\nwhere\n$G(\\alpha) = P(y_{k'}|z_k)P(z_k|u)$ W(\\beta) = P(y_{l'}|z_l)P(z_l|p)$.\nD. M-step\nIn the M-step, the algorithm maximizes the expectation computed in the E-step by re-estimating the model parameters. To do so, we need to specify functional forms of the distributions for $P(z_u|u)$, $P(z_l|p)$ and $P(w|y_{k'}, y_{l'})$. It is natural to model these distributions as multinomial distributions. Thus, we assume:\n$\\begin{equation}\nP(z_u|u) \\sim \\text{Multinomial}\n\\end{equation}$\n$\\begin{equation}\nP(z_l|p) \\sim \\text{Multinomial}\n\\end{equation}$\n$\\begin{equation}\nP(w|y_{k'}, y_{l'}) \\sim \\text{Multinomial}.\n\\end{equation}$\nThe update equation for $P(w|y_{k'}, y_{l'})$ is:\n$\\begin{equation}\nP(w|y_{k'}, y_{l'}) = \\frac{\\sum_{(u,p)\\in B(w)} P(y_{k'}, y_{l'}|u, p, w)}{\\sum_{w'} \\sum_{(u,p)\\in B(w')} P(y_{k'}, y_{l'}|u, p, w')},\n\\end{equation}$\nwhere $B(w)$ is the set of (user, product) tuples associated with the word $w$."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the performance of our proposed framework.\u00b2 First, we analyze the probabilistic latent model and the generated organization of user and product latent classes. Second, we investigate the generative extension of our approach. Last, we compare the performance of our model with state-of-the-art approaches for the rating prediction task.\nA. Data Sets Description\nWe evaluate our method with a benchmark dataset commonly used for product rating prediction: Amazon Product Data\u00b3. The dataset is a collection of product reviews and metadata, divided per category, retrieved from May 1999 to July 2014. We focus on the 5-core version of the data sets, where each user and item has at least 5 associated reviews. The ratings are integer values between 1 and 5. We use data from 16 categories for our experiments. Following [36], the review text has been normalized by: (a) setting the maximum length of a raw review to 300; (b) lowering case letters; (c) removing stopwords, numbers, and special characters; (d) removing non-existing words using an English vocabulary as a filter; (e) lemmatization; (f) removing reviews with just one word. Since the textual information shows strong diversity depending on the product category, we treat each dataset independently. Hence, we select a vocabulary for each category. The vocabulary size is $V = 2000$ for all product categories. The word selection is based on a modified version of the tf-idf index. After the vocabulary selection, we further filter the reviews to remove the ones without any word belonging to the corresponding vocabulary. Some statistics of the preprocessed data sets are summarized in Table I. From the table, we can observe that the sample size of the data may vary consistently depending on the category. For example, the largest category (Beauty) has around 30 times as many reviews as the smallest one (Musical instruments). Similarly, the number of users and products oscillates within a wide range of values among different categories. The data sets represent different and realistic conditions; this acts as a good framework for evaluating the quality of the results.\nB. Experimental Settings\nWe set the number of user latent classes $K = 25$ and product classes $L = 16$; we evaluated the robustness of our method to changes in the hyperparameters $K$ and $L$ but did not observe any significant difference in both qualitative and quantitative analysis. This is in line with the constraint imposed by the topological organization; using a larger number of latent classes did not lead to overfitting. The specificity parameters for users and products, defined in (2), are set respectively to $\\sigma_u = 3$ and $\\sigma_p = 2$. For the SOM initialization (Section III-E), we used the Python package MINISOM.4\nFor the probabilistic model part, following previous works (e.g. [22], [37]), we apply the so-called all but one protocol. From each user having at least 10 reviews, we randomly select one review to be assigned to the test set. The main focus of this part of the experiment is on discovering word patterns of users and products within large collections of review data, hence we do not need a generative formulation of the model. We evaluate the training procedure according to the normalized negative log-likelihood (NLL). Based on (6), we have:\n$\\begin{equation}\n\\text{NLL}_T = - \\frac{1}{T} \\sum_{(u,p,r) \\in T} \\log P(r|u,p),\n\\end{equation}$\nwhere $T$ represents either the training or test set.\nFor the rating prediction task, as in previous works, the data are randomly split by reviews into training (80%), validation (10%), and test (10%) sets. Additionally, we remove reviews from the validation and test sets if either the associated user or product does not belong to the training data set. The FCN block depicted in Fig. 2 is a neural network with four hidden fully-connected layers [128,64, 32, 16]. We evaluate different configurations of this block by changing the number of layers and the number of units for each layer. We select the best configuration based on the validation scores. The final experiment was run for 200 learning iterations and validated every iteration. A single epoch performs Adam optimizer [38] with a learning rate set to 0.02 and batch size of 256. The training metric is the MSE, as defined in (23). To prevent overfitting, we monitor the validation set score using early stopping with patience set to 10 epochs.\nC. Topographic Organization of Latent Classes\nGiven the model parameters estimated in Section III-D, we can analyze the organization of user and product latent classes by inspecting the associated word distributions. The word distribution for each user latent class, under the assumption of uninformative flat prior over latent classes, is computed as:\n$\\begin{equation}\nP(w|y_{k'}) = \\sum_{l'=1}^L P(w|y_{k'},y_{l'}) P(y_{l'}) = \\frac{1}{L} \\sum_{l'=1}^L P(w|y_{k'},y_{l'}).\n\\end{equation}$\nAnalogically, the word-distribution for each product latent class can be derived as follows:"}, {"title": "E. Rating Prediction Task", "content": "We compare our method with several baselines considering both factorization-based approaches and methods that take advantage of the textual information for improving the rating prediction performance.\nNon-negative Matrix Factorization (NMF) is a standard baseline for CF. The rating prediction is set as the dot product between item and user factors, i.e. $q_i^T p_u$. The latent factors are kept positive.\nSingular Value Decomposition (SVD) is very similar to NMF. The rating is computed as a dot product between the user and the product latent factors. In both cases, the results are poorly explainable. Indeed, it is not possible to understand user and product characteristics from the evaluation of the latent factors.\nMulti-Point Co-Attention Networks (MPCN) [17] proposes a pointer-based model to extract important reviews from user and item reviews, based on the intuition that not all the reviews are equally informative. Once the important reviews are selected, a co-attentive layer learns the most important words associated. In this way, the model can learn the most informative user and product reviews for each user-item pair.\nDeep Co-Operative Neural Networks (DeepCoNN) [16] learns convolutional user and item representations from the textual information. Even though is a text-based model, the analysis is focused only on the rating prediction performances. Additionally, given the deep nature of the representations, the results are poorly interpretable.\nTransNets (TNET) [15] is an extension of the DeepCoNN model. It introduces an additional transform layer that learns an approximation of the review corresponding to the target user-item pair and, during the training phase, enforces it to be similar to the embedding of the actual target review. The model can be used to suggest reviews that are more similar to the one potentially written by the target user. The qualitative evaluation just rely on the visualization of some sampled reviews by highlighting the most similar sentences between the original review and the predicted most helpful one. However, if singularly evaluated, the learned embeddings do not provide any interpretable information.\nTransRev [18] is similar to TransNets in that it learns review embeddings as a translation of the reviewer representation to the reviewed product embedding. Ratings are predicted based on an approximation of the review embedding at test time based on the difference between the embedding of the user and the item. As in TransNets, the approximation is also used to suggest a tentative review to users for further elaborations. The evaluation of the learned word embedding is performed using t-SNE. Items and users are related to reviews by means of non-interpretable latent representations.\nTLCM-CNN stands for Transparent Latent Class Model with CNN and represents the rating prediction model described in Section III-G. This comparison helps us to investigate if the latent representations of our probabilistic model, even though not learned ad-hoc, represent an informative input for the rating prediction task. For the reasons explained before, our input representations, if able to get competitive results, would be better in terms of interpretability capacity compared to the embedding techniques presented in the previous works.\nTLCM-LR performs a simple linear regression (LR) that, for each review, takes as input the concatenation of the estimates of the corresponding user and product learned by our probabilistic model. As stated in section III-G, the architecture for rating prediction can be changed as desired. In this case, we decided to use a linear regression model since, among other methods, it is known to be a transparent model. Following the direction of previous investigations on the neural hype, this experiment allows us to understand whether the use of complicated architectures is helping to make significant progress for recommendation tasks.\nAs anticipated, the ultimate goal of this work is to build a review-based model suitable for both visualizing and learning user and product characteristics. Then, driven by the limitations of existing approaches to represent users and products through explainable latent vectors, we propose to exploit our estimated quantities as input for rating prediction. This idea is fully motivated by the strong correlation between ratings and reviews. In addition, we also propose to use the same information as input for an interpretable model, i.e. a linear regression, to understand its predictive performances compared to neural approaches. This experimental part completes our analysis about the \"phantom progress\" of neural-based approaches for representation learning and recommendation tasks, considering also the interpretability capacity of the models taken into consideration.\nIn terms of interpretability, as already mentioned, all the baselines can provide meaningful representations in the latent space, but neither the single numbers contained in the vectors nor their dimensions have an interpretable meaning [39]. The main limitation of these techniques laid in the fact that they can capture relations among items by using vectors that are only meaningful to each other. For instance, if we try to evaluate user vectors without employing the review vectors, we would not be able to comprehend the latent textual information associated with them. Differently, our model can directly encode the textual information within the estimated quantities, providing vectors that are self-explainable. Additionally, our latent representations are more compact and sparse, making them easier to evaluate. Finally, the intuitive visualization properties provided by our square grids create a simple tool to investigate the results, further enhancing their interpretability.\nIn terms of rating prediction performance, we independently repeat each experiment on five different random splits closely following the experimental setup specified in the relevant studies, including reporting the averaged Mean Squared Error (MSE) as the performance measure. For a fair comparison, we conduct and compare the baselines on the same data splits, when possible, using the default configurations provided by the authors. If not, whether for difficulty in reusing the source code or its unavailability, we directly copy the results from the original papers. In detail, for SVD and NMF we used the Python package SURPRISE5 and we selected the best learning parameters through grid search. For DeepCoNN, since the original authors' source code has not been released, we used a third-party implementation6. In this case, we applied the default hyperparameter setting. For the remaining baselines, we copied the results from the corresponding original papers. Results in terms of MSE are reported in Table III. The asterisk indicates the macro MSE across all the product categories. From the results, in line with the conclusions of existing works, review-based methods outperform the ones based only on the numerical information. We can observe that our model is able to outperform most of the state-of-the-art approaches in the analyzed categories and has comparable performances with the best one, i.e. TransRev. Additionally, it is worth to mention that our simplest version of the architecture, i.e. TLCM-LR, is also able to get comparable results against most of the state-of-the-art approaches. This demonstrates that: 1) by carefully encoding the textual information into the latent representations, it is possible to get similar results in comparison with more complicated techniques that provide dense and high-dimensional vector representations of the items; b) in line with the conclusions of previous investigations on CF data, also in case the textual information is used, we do not need to employ complicated neural-based models to get competitive results. The latent representations proposed in our work, even though not learned to specifically perform well on a rating prediction task, are able to maintain competitiveness in comparison with more specialized architectures. This clearly suggests that one may potentially use the proposed model input with nice interpretable properties, without worsening the rating prediction task considerably."}, {"title": "V. CONCLUSION", "content": "Recently, several approaches for rating predictions of textual reviews in the framework of deep neural networks have appeared in the literature [14], [27], [15], [16], [17], [18]. Given the highly stochastic nature of the data and relative data sparsity, one can legitimately ask to what extent can the full predictive power of deep networks be utilized in this context. The question is even more relevant when one realises that clear interpretability of the deep network functionality is still an open problem [40]. To answer this question, we present an approach for product rating prediction using a relatively simple and interpretable latent class probabilistic model utilizing topographic organization of user and product latent classes based on the review information. In existing works, the review information is usually exploited to enhance the rating prediction performances, but is not fully inspected to understand user and product features. In contrast, we propose a deeper understanding of the data, presenting a two-step approach that starts from the interpretable organization of textual information to arrive at the rating prediction task. The organization of the latent classes on 2-dimensional grids provides a visualization tool that can be used to statistically investigate user and product features from a review-based perspective. Through this organization, we can arrange complicated and unstructured textual data in a simple way. The thorough analysis in the experimental section demonstrates the ease of analyzing the latent review patterns using tools from probabilistic theory. The visualization of the results, presented in sections IV-C and IV-D, shows that the lower-dimensional latent representations of users and products are a good approximation of the textual input space. Consequently, driven by the assumption that ratings and reviews are strongly correlated, we propose to use the resulting latent features as input for a rating prediction task. In this part, we contribute to the debate about the \"phantom progress\" of deep learning approaches for recommendation tasks. Our investigation, differently from the previous ones, is mainly focused on methods that take advantage of the textual information for rating prediction tasks, and includes an evaluation that considers the capacity of such models to represent users, products, and reviews utilizing interpretable latent vectors. The results suggest that, also in the textual-based case, the use of dense and complicated representations is not fully motivated. Indeed, even though our representations are not learned for a rating prediction task specifically, the results are comparable to models that learn ad-hoc representations. Nonetheless, being highly interpretable, the proposed latent representations overcome the limitations of the common embedding techniques used in most of the considered previous works. Finally, the prediction results of our linear regression model suggest that we do not need to implement deep architectures either. This simple model is able to outperform some of the baselines and get comparable results with the remaining ones, while being fully transparent. Naturally, there is always a trade-off between model capabilities and interpretability. The nice and explainable visualization properties of our constrained model may affect the modeling capabilities for rating prediction. On the other hand, better modeling capabilities through common embedding techniques and deep architectures provide representations that are created for performing well on a specific task, but at the price of losing the human interpretation of the results."}]}