{"title": "PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation", "authors": ["Liyao Jiang", "Negar Hassanpour", "Mohammad Salameh", "Mohammadreza Samadi", "Jiao He", "Fengyu Sun", "Di Niu"], "abstract": "Recent research explores the potential of Diffusion Models (DMs) for consistent object editing, which aims to modify object position, size, and composition, etc., while preserving the consistency of objects and background without changing their texture and attributes. Current inference-time methods often rely on DDIM inversion, which inherently compromises efficiency and the achievable consistency of edited images. Recent methods also utilize energy guidance which iteratively updates the predicted noise and can drive the latents away from the original image, resulting in distortions. In this paper, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation, where we directly create a duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring the edited image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Experimental evaluations based on benchmark datasets as well as extensive visual comparisons show that in as few as 16 inference steps, PixelMan outperforms a range of state-of-the-art training-based and training-free methods (usually requiring 50 steps) on multiple consistent object editing tasks.", "sections": [{"title": "Introduction", "content": "Diffusion Models (DMs) excel at generating stunning visuals from text prompts (Rombach et al. 2022; Saharia et al. 2022b; Chang et al. 2023), yet with potentials extending beyond text-to-image generation. A highly popular application is image editing, as evidenced by widespread tools such as Google Photos MagicEditor (Google 2023) and AI Editor in Adobe Photoshop (Adobe 2023). Many research efforts (Hertz et al. 2022; Tumanyan et al. 2023; Alaluf et al. 2023; Parmar et al. 2023) achieve promising results on text-prompt-guided rigid image editing involving tasks such as changing the color, texture, attributes, and style of the image. However, consistent object editing (Kawar et al. 2023; Cao et al. 2023; Duan et al. 2024) is a distinct type of image editing that aims to preserve the consistency of objects and background in the image without changing their texture and attributes, while modifying only certain non-rigid attributes of the objects (e.g., changing the position, size, and composition of objects). Typical consistent object editing tasks include object repositioning (Epstein et al. 2023; Mou et al. 2024b,a; Wang et al. 2024; Winter et al. 2024), object resizing (Epstein et al. 2023; Mou et al. 2024b,a), and object pasting (Chen et al. 2024b; Mou et al. 2024b,a). Consistent object editing tasks are complex and usually involve multiple sub-tasks such as: (i) generating a faithful reproduction of the source object at the target location, (ii) maintaining the background scene details, (iii) harmonizing the edited object into its surrounding target context, and (iv) inpainting the original vacated location with a cohesive background.\nTo solve this problem, training-based methods have been proposed (Rombach et al. 2022; Chen et al. 2024b; Wang et al. 2024; Winter et al. 2024), which however require a costly training process and usually also require collecting task-specific datasets. Alternatively, recent training-free methods (Epstein et al. 2023; Mou et al. 2024b,a) rely on DDIM inversion (Dhariwal and Nichol 2021) to estimate the initial noise corresponding to the source image. However, this process is inefficient as it often requires many (usually at least 50) inference steps. Reducing the number of steps to, e.g., 16, significantly compromises editing quality (see Fig. 2). Moreover, DDIM inversion struggles to produce a precise and consistent final reconstruction of the source image, often yielding a coarse approximation due to accumulation of errors at each timestep (Duan et al. 2024). As a result, training-free methods that rely on DDIM inversion are inherently limited in their ability to perform consistent edits.\nTo facilitate object generation at target location and reproduction of background, DragonDiffusion (Mou et al. 2024b) and DiffEditor (Mou et al. 2024a) utilize Energy Guidance (EG) to minimize the feature similarity between the source and target objects (backgrounds). While EG iteratively refines the predicted noise, this process can inadvertently drive the latent representation away from that of the original image during inference, causing distortions in object appearance and background. Additionally, seamlessly inpainting the vacated region (if any) with a coherent background remains a challenge, as existing methods often struggle to fully remove the original object or introduce unintended elements (see Fig. 2).\nIn this paper, we propose PixelMan, an inversion-free and training-free method to achieve consistent object editing with existing pretrained text-to-image diffusion models via Pixel Manipulation and generation in as few as 16 steps that outperform all competitive training-based and training-free methods (usually requiring 50 steps) on a range of consistent object editing tasks. Rather than performing DDIM inversion and edited denoising, we directly create a duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring output image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Our contributions are summarized as follows:\n\u2022 We propose to perform pixel manipulation for achieving consistent object editing, by creating a pixel-manipulated image where we copy the source object to the target location in the pixel space. At each step, we always anchor the target latents to the pixel-manipulated latents, which reproduces the object and background with high image consistency, while only focusing on generating the missing \"delta\" between the pixel-manipulated image and the target image to be generated.\n\u2022 We design an efficient three-branched inversion-free sampling approach, which finds the delta editing direction to be added on top of the anchor, i.e., the latents of the pixel-manipulated image, by computing the difference between the predicted latents of the target image and pixel-manipulated image in each step. This process also facilitates faster editing by reducing the required number of inference steps and number of Network Function Evaluations (NFEs).\n\u2022 To inpaint the manipulated object's source location, we identify a root cause of many incomplete or incoherent inpainting cases in practice, which is attributed to information leakage from similar objects through the Self-Attention (SA) mechanism. To address this issue, we propose a leak-proof self-attention technique to prevent attention to source, target, and similar objects in the image to mitigate leakage and enable cohesive inpainting.\n\u2022 Our method harmonizes the edited object with the target context, by leveraging editing guidance with latents optimization, and by using a source branch to preserve uncontaminated source K, V features as the context for generating appropriate harmonization effects (e.g. lighting, shadow, and edge blending) at the target location.\nWe provide extensive quantitative and/or qualitative visual comparisons to a range of state-of-the-art training-free and training-based approaches designed for object repositioning, object resizing and object pasting (some of which can be found in Appendix). Quantitative results on the COCOEE and ReS datasets as well as extensive visual comparisons suggest that PixelMan achieves superior performance in consistency metrics for object, background, and semantic consistency between the source and edited image, while achieving higher or comparable performance in IQA metrics. As a training-free method, PixelMan only requires 16 inference steps with lower average latency and a lower number of NFEs than current popular methods."}, {"title": "Related Works", "content": "Image editing with DMs. While standard text-to-image DMs are not directly designed for image editing, recent research is actively exploring their potential for this task. Training-based approaches (Saharia et al. 2022a; Brooks, Holynski, and Efros 2023; Zhang, Rao, and Agrawala 2023) optimize the UNet for certain editing scenarios. Wang et al. (2024) fine-tuned an inpainting model specifically for object repositioning task (by introducing and utilizing an ad-hoc dataset, namely ReS). However, these approaches may require high computational resources only to learn a specific task. As such, there is a high motivation to explore methods for augmenting pretrained UNets with different editing capabilities without additional training. In training-free methods (Hertz et al. 2022; Alaluf et al. 2023; Hertz et al. 2023; Tumanyan et al. 2023; Epstein et al. 2023), users can perform editing either by a descriptive text prompt (Hertz et al. 2022; Brooks, Holynski, and Efros 2023; Tumanyan et al. 2023; Epstein et al. 2023), or by specifying editing points within an image, called point-based editing (Endo 2022; Pan et al. 2023; Shi et al. 2023; Mou et al. 2024b,a). The main advantage of point-based editing is the granular control over the edit region. In this work, we propose a point-based training-free approach for consistent object editing using DM, which preserves the consistency between the source and edited image.\nTraining-free consistent object editing. Epstein et al. (2023) introduced Energy Guidance (EG) (see Appendix for details) and proposed SelfGuidance, a prompt-based editing method that guides the sampling process based on specific energy functions defined on attentions and activations. Mou et al. (2024b) proposed DragonDiffusion, a point-based editing approach that leverages EG to update the sampled noise. Building on this, DiffEditor (Mou et al. 2024a) improved the content consistency by introducing regional SDE sampling and score-based gradient guidance (Song et al. 2020). Despite their success, EG-based methods require computationally expensive tricks to propagate the guidance from e to zt. Different from EG-based methods that update the estimated noise \u03f5, our method directly updates the latents zt for consistent object editing, which reduces the latency as well as NFEs, while maintaining consistency and image quality.\nInverting real images. Preserving the consistency between the original and edited image is crucial for consistent image editing. Training-free methods often utilize the inversion techniques to convert the source image into a convertible initial noise (zr). DDIM inversion (Dhariwal and Nichol 2021) is a common but computationally expensive technique as it usually requires 50 inference steps. ReNoise (Garibi et al. 2024) is a recent inversion technique that can utilize few-steps models (Luo et al. 2023; AI 2023), but its repeated UNet calls in its refinement phase still leads to high computation costs. An alternative approach is Denoising Diffusion Consistent Model (DDCM) (Xu et al. 2024), which facilitates inversion-free prompt-guided rigid image editing for changing the texture and attribute of objects. In contrast to DDCM, our method does not use any prompt, and instead we propose an inversion-free approach for efficient consistent object editing which focuses on preserving the consistency of objects and background in the image without changing their texture and attributes while modifying only certain non-rigid attributes of the objects (e.g., changing the position, size, and composition of objects).\nAttention control for editing. Recent studies on training-free editing techniques (Cao et al. 2023; Hertz et al. 2023; Tumanyan et al. 2023; Hertz et al. 2022; Parmar et al. 2023) explore either integrating or manipulating Cross-Attentions (CAs) and Self-Attentions (SAs) to exert precise control over the editing process. Manipulating CAs has been demonstrated to offer control over object composition. Hertz et al. (2022) proposed an injection approach for swapping objects and changing the global style. Alternatively, since SAs incorporate information about pixel interactions in the spatial domain, manipulating them affects overall style, texture, and object interaction (Zhou et al. 2024; Alaluf et al. 2023; Hertz et al. 2023; Jeong et al. 2024; Cao et al. 2023). Building on this, Patashnik et al. (2023) presented a SA injection method to selectively preserve a set of objects while altering other regions. Following these insights, we propose a leakproof self-attention technique to ensure a complete and cohesive inpainting of the vacated area with the background, by preventing a root cause of failed inpainting which is information leakage from the source or similar objects."}, {"title": "Method", "content": "To enhance computational efficiency and preserve image consistency during object editing, we introduce PixelMan, an efficient inversion-free and training-free method that performs consistent object editing with DMs via Pixel Manipulation and generation in few inference steps. The following subsections describe our proposed three-branched inversion-free sampling approach, leakproof self-attention technique, and editing guidance with latents optimization method."}, {"title": "Three-Branched Inversion-Free Sampling", "content": "Our goal is to achieve the following three objectives with high efficiency: (i) consistent reproduction of the object and background; (ii) object-background harmonization; and (iii) cohesive inpainting of the vacated location. As the backbone of PixelMan, we propose a three-branched sampling paradigm that achieves these three objectives using a single pretrained DM, while also bypassing inversion to facilitate faster editing by reducing inference steps and NFEs.\nSpecifically, we utilize three separate branches: source branch, pixel-manipulated branch, and target branch. Each branch maintains its own nosiy latents that is initialized, denoised (using the same UNet), and updated in different manners throughout the T sampling time-steps t\u2208 [1,T]. We denote the noisy latents of the source branch, pixel-manipulated branch, and target branch at time-step t respectively with $z_{t}^{src}$, $z_{t}^{man}$, $z_{t}^{tgt}$.\nWe create a pixel-manipulated image $I_{man}$ by copying the source object to the target location in pixel-space. For the object resizing task, we interpolate the object pixels based on the resizing scale before making a copy at the target location. For object pasting, the source object comes from a separate reference image $I_{ref}$, and is copied into the source image $I_{src}$ at the target location to create $I_{man}$. Then, using the VAE encoder, we encode the pixel-manipulated image $I_{man}$ and the source image $I_{src}$ respectively into the pixel-manipulated latents $z^{man}$ and source latents $z^{src}$.\nPixel-manipulated latents as anchor. At each time-step t of our sampling process, our goal is to directly obtain a latent space estimation of the edited output image $I_{out}$ which we denote as output latents $z^{out}$. First, we ask the question of what would be a reasonable estimate of the output latents $z^{out}$. Intuitively, our estimation of output latents $z^{out}$ should be identical to the source latents $z^{src}$ so we can exactly reproduce the source image.\nHowever, we want to reproduce the source object at the new target location, so we set our estimation of the output latents $z^{out}$ to be identical to the pixel-manipulated latents $z^{man}$, which already have the source object reproduced at the target location through pixel manipulation. By using this naive estimation $z^{out} = z^{man}$, we can already effortlessly preserve the original background and consistently reproduce the object at the target location. Therefore, we refer to this pixel-manipulated latents $z^{man}$ as the anchor.\nIn addition to image consistency, we also want to achieve cohesive inpainting of the vacated location, and harmonize the object and background with realistic effects. So, there should be a delta editing direction \u0394z added on top of the anchor $z^{man}$ to achieve the inpainting and harmonization edits. More concretely, at each time-step t, we set the output latent $z^{out}$ as:\n$z^{out} = z^{man} + \u0394z$.\nWith our simple estimation of output latents $z^{out}$ using the sum of the anchor $z^{man}$ and the delta edit direction \u0394z, we can preserve the object and background consistency without any inversion, which improves both the efficiency and image consistency by avoiding the computation bottleneck and accumulated reconstruction error of the DDIM inversion (Dhariwal and Nichol 2021) process. Next, we introduce our method for obtaining the delta edit direction \u0394z.\nObtaining delta edit direction. We aim to obtain the delta editing direction \u0394z that can achieve cohesive inpainting of the vacated location, and harmonize the object and background with realistic effects (e.g., lighting, shadow, edge blending). To achieve this, we propose to apply several editing guidance techniques (introduced in the later sections) for generating the inpainting and harmonization edits in the target branch, including leak-proof self-attention, editing guidance with latent optimization, and injection of source K, V features into the target branch. Meanwhile, we keep the pixel-manipulated branch consistent with the anchor $z^{man}$, and obtain \u0394z by finding the difference in the output of the two branches.\nSpecifically, we calculate the difference between the predicted target latents $\\hat{z}_{t}^{tgt}$ from the target branch and predicted"}, {"title": "Three-Branched Inversion-Free Sampling", "content": "pixel-manipulated latents $\\hat{z}_{t}^{man}$ from the pixel-manipulated branch:\n$\u0394z = \\hat{z}_{t}^{tgt} - \\hat{z}_{t}^{man}$.\nTo obtain $\\hat{z}_{t}^{man}$ from the pixel-manipulated branch, we always analytically compute the noisy latents $z_{t}^{man}$ at each sampling time-step t from the pixel-manipulated latents $z^{man}$ (i.e., the anchor that ensures consistency to $I_{man}$) which has already reproduced object at the target location and the original background.\nSpecifically, at each time-step t, we first follow the FDP equation to obtain $z_{t}^{man}$ by adding random Gaussian noise \u03f5~N(0, I) to $z^{src}$:\n$z_{t}^{man} = \\sqrt{a_{t}} \u00d7 z^{man} + \\sqrt{1-a_{t}} \u00d7 \u03f5$.\nThen, we pass the noisy source latents $z_{t}^{man}$ to the denoising UNet (parameterized by \u03b8) to get the predicted noise $\\hat{\u03f5}_{t}^{man}$ at time-step t:\n$\\hat{\u03f5}_{t}^{man} = UNet(z_{t}^{man}, t)$.\nFinally, we obtain the predicted pixel-manipulated latents $\\hat{z}_{t}^{man}$ based on the noisy pixel-manipulated latents $z_{t}^{man}$ and the UNet predicted noise $\\hat{\u03f5}_{t}^{man}$ at time-step t, using the Reverse Generative Process (RGP) (more details in Eq. (10) in the Appendix):\n$\\hat{z}_{t}^{man} = RGP(z_{t}^{man}, \\hat{\u03f5}_{t}^{man}, t)$.\nNext, we obtain $\\hat{z}_{t}^{tgt}$ from the target branch. Before the initial timestep t = T, we initialize the target latents $z_{0}^{tgt}$ to be the same as $z^{man}$ which corresponds to the anchor $I_{man}$. In contrast, at each sampling time-step t, we instead utilize the FDP similar to Eq. (3) to analytically compute the noisy target latents $z_{t}^{tgt}$ from the estimated output latents $z_{out}$ of previous time-step.\nThen, $z_{t}^{tgt}$ is updated with latents optimization (detailed in \"Editing Guidance with Latents Optimization\"). Next, we pass $z_{t}^{tgt}$ along with the saved source branch $K^{src}$, $V^{src}$ (detailed in \u201cFeature-preserving source branch\") to the UNet to obtain the predicted noise $\\hat{\u03f5}_{t}^{rc}$, where $\\hat{\u03f5}_{t}^{rc} = UNet(z_{t}^{rc}, t; {K^{src}, V^{src}})$. Next, we obtain the predicted target latents $\\hat{z}_{t}^{tgt}$ using the RGP similar to Eq. (5).\nAfter calculating both $\\hat{z}_{t}^{tgt}$ and $\\hat{z}_{t}^{man}$, we finally obtain the delta editing direction \u0394z. To estimate the output image, we combine the anchor $z^{man}$ and the delta editing direction in Eq. (2), while applying a masked-blending approach with mask $(1 - m_{new})$ (i.e., the object target location):\n$z^{out} = z^{man} + (\\hat{z}_{t}^{tgt} - \\hat{z}_{t}^{man}) \u00d7 (1-m_{new})$.\nThe masked-blending is applied throughout the sampling time-steps to remove the delta editing direction \u0394z in the target location, and only use the anchor $z^{man}$ to achieve object consistency. While allowing \u0394z to change the background for inpainting and harmonization. For the last few time-steps no masking is applied, which encourages seamless object-background blending and allows the DM to refine the details of the output image."}, {"title": "Feature-preserving source branch.", "content": "At each time-step t, we always analytically compute the noisy source latents $z_{t}^{src}$ from $z^{src}$ (making $z_{t}^{src}$ consistent with $z^{src}$). Specifically, at each time-step t, we first follow the FDP equation similar to Eq. (3) to obtain $z_{t}^{src}$ by adding random Gaussian noise \u03f5~N(0, I) to $z^{src}$. Then, we pass the noisy source latents $z_{t}^{src}$ to the denoising UNet to get the predicted noise $\\hat{\u03f5}_{t}^{src}$ at time-step t: $\\hat{\u03f5}_{t}^{rc, {K^{src}, V^{src}}} = UNet(z_{t}^{rc}, t)$. Note that the $\\hat{\u03f5}_{t}^{rc}$ is discarded here, and we save the self-attention $K^{src}$ and $V^{src}$ matrices from the source branch and inject\u00b9 them back during the UNet call on $z_{t}^{tgt}$ in the target branch, which is inspired by the mutual self-attention technique proposed in (Cao et al. 2023). The saved $K^{src}$ and $V^{src}$ preserve the original visual details from $I_{src}$, and the injection into target branch serves as context for generating appropriate harmonization effects (e.g., lighting, shadow, and edge blending), and also for inpainting the vacated area."}, {"title": "Leak-Proof Self-Attention", "content": "Our objective is to achieve complete and cohesive inpainting of the vacated region after the edited object moves out. However, current methods often either struggle to remove all traces of the object (e.g., object is not entirely removed in columns (d), (e), and (f) of Fig. 2 by the SOTA method DiffEditor (Mou et al. 2024a)), or hallucinate new unwanted artifacts in the vacated region. We attribute these issues to information leakage from similar objects through the SA mechanism (Dahary et al. 2024), and propose a leak-proof self-attention technique that prevents the attention to source object, target object, and similar objects in the image. Leak-proof SA leverages and controls the inter-region dependencies captured by SA to alleviate information leakage.\nIntuitively, areas $m_{old}$, $m_{new}$, and $m_{sim}$ all contain information about the to-be-edited object, and this information can be leaked to area $m_{ipt}$ through the SA mechanism, where $m_{old}$ is mask of to-be-edit object at the source/old location; $m_{new}$ is $m_{old}$ shifted to the the target/new location; $m_{sim}$ is mask of other similar objects to the to-be-edited object (e.g., other apples in the multi-apple image in Fig. 1; see details on how to automatically obtain $m_{sim}$ in the Appendix); and $m_{ipt}$ equals the mask from $(m_{old} - m_{new})$ which represents the to-be-inpainted vacated region. To minimize the information leakage of the to-be-edited object and similar objects on the inpainted region, we strategically reset the corresponding elements (i.e., $m_{old}$ \u222a $m_{new}$ \u222a $m_{sim}$) in $QKT$ to a minimal value (i.e.,\u2212\u221e). This strategy is activated for the target branch UNet call in all SA layers and at all time-steps to mitigate leakage and enable cohesive inpainting."}, {"title": "Editing Guidance with Latents Optimization", "content": "Mou et al. (2024b) propose a set of energy functions, which enforce feature correspondence to provide editing guidance. We utilize the same energy functions from DragonDiffusion (Mou et al. 2024b) to obtain additional editing guidance for object generation, harmonization, inpainting, and background consistency in our target branch.\nMoreover, we aim to improve the efficiency of editing guidance. Mou et al. (2024a) showed that having a refinement loop that applies the editing guidance multiple times at a single time-step significantly enhances the performance. However, EG-based methods update the predicted noise \u03f5 while the loss function operates on the noisy latents zt. To bridge this gap and propagate the guidance from \u03f5 to zt, (Mou et al. 2024a) introduced \u201ctime travel\u201d that requires a repetitive second round of DDIM inversion (Dhariwal and"}, {"title": "Algorithm 1: Algorithm Overview of PixelMan", "content": "Require: VAE Encoder: $z_{t} = E(I)$; VAE Decoder: $I = D(z_{t})$\nRequire: $\\hat{\u03f5}_{t}, {K, V} = UNet(z_{t}, t)$\nRequire: $z_{t} = FDP(z_{0}, \u03f5); z_{0} = RGP(z_{t}, \\hat{\u03f5}_{t}, t)$\nRequire: $z^{src} = E(I_{src}); z^{man} = E(I_{man}); z^{out} = E(I_{man})$\nRequire: source, target, and inpaint mask: $m_{old}$, $m_{new}$, $m_{ipt}$\n1: for time-step t \u2208 {T, T \u2013 1, ..., 1} do\n2:  \u03f5 ~ N(0, I)\n3:  $z_{t}^{rc} = FDP(z^{orc}, \u03f5)$\n4:  $z_{t}^{man} = FDP(z^{man}, \u03f5)$\n5:  $z_{t}^{tot} = FDP(z^{out}, \u03f5)$\n6:  for repeat r do \u25b7 latents optimization\n7:   $z_{t}^{tot} = z_{t}^{tot} + \u2207_{zt}E(z_{t}^{tot}, z^{man})$\n8:  end for\n9:  $\\hat{\u03f5}_{t}^{man} = UNet(z_{t}^{man}, t)$\n10:  $\\hat{\u03f5}_{t}^{orc}, {K^{src}, V^{src}} = UNet(z_{t}^{src}, t)$ \u25b7 save K,V\n11:  $\\hat{\u03f5}_{t}^{tgt} = UNet(z_{t}^{tgt}, t; {K^{src}, V^{src}})$ \u25b7 apply leak-proof SA\n12:  $\\hat{z}_{t}^{man} = RGP(z_{t}^{man}, \\hat{\u03f5}_{t}^{man}, t)$\n13:  $\\hat{z}_{t}^{tot} = RGP(z_{t}^{tot}, \\hat{\u03f5}_{t}^{tot}, t)$\n14:  if t \u2208 {2, 1} then \u25b7 i.e., last few time-steps\n15:   $z^{out} = z^{man} + (\\hat{z}_{t}^{tgt} - \\hat{z}_{t}^{man})$ \u25b7 no masked-blending\n16:  else\n17:   $z^{out} = z^{man} + (\\hat{z}_{t}^{tgt} - \\hat{z}_{t}^{man}) \u00d7 (1-m_{new})$ \u25b7 with mask\n18:  end if\n19: end for\nOutput: $I_{out} = D(z^{out})$ \u25b7 the edited output image"}, {"title": "Experiments", "content": "First, we evaluate the effectiveness of PixelMan in the representative consistent object editing task, which is object repositioning. In addition, we also apply PixelMan on other consistent object editing tasks including object resizing, and object pasting to demonstrate the generalizability to different tasks (see Appendix). For the object repositioning task, we perform extensive quantitative evaluation and visual comparisons, both against the existing methods, as well as ablation studies on the various components of PixelMan (placed"}, {"title": "Overall performance.", "content": "In Figs. 3a, 3b, and 3c, we compare PixelMan against the four contenders on the COCOEE dataset at the same number of inference steps (8, 16, and 50 steps respectively). At 50 steps, PixelMan outperforms all other methods in 9 out of 9 metrics. At 16 steps, PixelMan outperforms other methods in 8 out of 9 metrics, while being second place on the MUSIQ IQA metric. At 8 steps, PixelMan scores the best in 8 out of 9 metrics, while being second place on the TOPIQ IQA metric. Overall, PixelMan outperforms the other methods at the same number of steps. In the Appendix, we provide the full quantitative results and visual comparisons of all methods at both 16 and 50 steps."}, {"title": "Efficiency.", "content": "More importantly, PixelMan achieves superior performance with fewer NFEs than existing methods. We attribute this to our three-branched inversion-free sampling approach that avoids quality degradation at 16 steps, seen in methods (Epstein et al. 2023; Mou et al. 2024b,a) that rely on DDIM inversion (e.g., row \"DiffEditor 16 steps\" of Fig. 2). As shown in Table 1, PixelMan at 16 steps requires 112 fewer computations and is 15 seconds faster than the SOTA DiffEditor on COCOEE. Despite being faster, PixelMan's quality at 16 steps surpasses DiffEditor's at 50 steps (additional examples in the Appendix). Therefore, hereafter, we directly compare PixelMan at 16 steps to other"}, {"title": "Image quality.", "content": "In Fig. 3d, PixelMan (16 steps) achieves significantly better image quality in all three IQA metrics than the other methods (50 steps) on COCOEE. In Fig. 3e, PixelMan has similar image quality to DragonDiffusion and DiffEditor on ReS dataset even when using significantly fewer steps. In visual comparisons, we observe PixelMan achieves overall better image quality than other methods while being more efficient. This includes less artifacts, more natural colors, well-blended objects and backgrounds, and natural lighting and shadow."}, {"title": "Object consistency.", "content": "PixelMan (16 steps) excels in object consistency on both COCOEE and ReS datasets (Figs. 3d, 3e), as measured by LPIPS (neg) and PSNR. Our three-branched inversion-free sampling approach helps the faithful reproduction of the object at the new location since we always anchor the output latents to the pixel-manipulated latents which ensures the moved object to be consistent with the original object. This is evident in Fig. 2 and Fig. 6 in Appendix, where PixelMan consistently preserves details like shape, color, and texture (e.g., clock, bird, airplane, orange)."}, {"title": "Background consistency.", "content": "On both COCOEE and ReS datasets (i.e., Fig. 3d and Fig. 3e), PixelMan outperforms all other methods in both background consistency metrics LPIPS (neg) and PSNR. In the visual examples in Fig. 2, we observe the background in PixelMan's edited images are more consistent with the source image (e.g., the grass texture and color in (b) and the water color in (e))."}, {"title": "Inpainting.", "content": "We provide abundant visual comparisons to assess the inpainting quality in Fig. 2 and in Figs. 6, 7, 8, 9, and 10 (in the Appendix). Here, we see PixelMan excels at removing objects (e.g., plane, pillow, orange in Fig. 2) while preserving the surrounding scene. Conversely, other methods either leave traces of the original object or introduce new artifacts in the inpainted area."}, {"title": "Semantic consistency.", "content": "PixelMan outperforms all methods on COCOEE and is best in CLIP-I2I on ReS and remains competitive in CLIP-T2T (see Fig. 3). PixelMan preserves the original semantics of the source image, while maintaining consistency in object, background and better inpainting quality (e.g., 2 instead of 3 oranges in Fig. 2 (h))."}, {"title": "Conclusion", "content": "In this work, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation. PixelMan maintains image consistency by directly creating a duplicate copy of the source object at target location in the pixel space, and we introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location. The key to ensuring image consistency is anchoring the output image to be generated to the pixel-manipulated image as well as introducing various consistency-preserving optimization techniques during inference. Moreover, we propose a leak-proof SA manipulation technique to enable cohesive inpainting by addressing the attention leakage issue which is a root cause of failed inpainting. Quantitative results on the COCOEE and ReS datasets as well as extensive visual comparisons show that PixelMan achieves superior performance in consistency metrics for object, background, and image semantics while achieving higher or comparable performance in IQA metrics. As a training-free method, PixelMan only requires 16 inference steps with lower average latency and a lower number of NFEs than current popular methods."}, {"title": "Technical Appendix", "content": "This technical appendix to our main paper has the following sections:\n\u2022 In Section \"Background and Related Works\u201d, we provide more details of the background and related works.\n\u2022 In Section \"Implementation and Evaluation Details\u201d, we provide the implementation details and evaluation settings including the datasets and metrics.\n\u2022 In Section \"Ablation Study"}]}