{"title": "XATTNMARK: Learning Robust Audio Watermarking with Cross-Attention", "authors": ["Yixin Liu", "Lie Lu", "Jihui Jin", "Lichao Sun", "Andrea Fanelli"], "abstract": "The rapid proliferation of generative audio synthesis and editing technologies has raised significant concerns about copyright infringement, data provenance, and the spread of misinformation through deepfake audio. Watermarking offers a proactive solution by embedding imperceptible, identifiable, and traceable marks into audio content. While recent neural network-based watermarking methods like WavMark and AudioSeal have improved robustness and quality, they struggle to achieve both robust detection and accurate attribution simultaneously. This paper introduces Cross-Attention Robust Audio Watermark (XATTNMARK), that bridges this gap by leveraging partial parameter sharing between the generator and the detector, a cross-attention mechanism for efficient message retrieval, and a temporal conditioning module for improved message distribution. Additionally, we propose a psychoacoustic-aligned temporal-frequency masking loss that captures fine-grained auditory masking effects, enhancing watermark imperceptibility. Our approach achieves state-of-the-art performance in both detection and attribution, demonstrating superior robustness against a wide range of audio transformations, including challenging generative editing with strong editing strength. Project webpage is available at https://liuyixin-louis.github.io/xattnmark/.", "sections": [{"title": "1. Introduction", "content": "With the rapid development of generative audio synthesis and editing techniques, anyone can now easily edit and re-synthesize audio content (OpenAI, 2024; Li et al., 2024;Copet et al., 2024). While it democratizes the creative process and enables new applications, it also brings serious concerns for copyrighted data misuse, data provenance and authenticity (Pan et al., 2023; Shoaib et al., 2023; Park et al., 2023). A notable example is the recent surge of deepfake audio and video, where actors have been using deepfake techniques to impersonate and create fake speech and video content of online politicians or public figures, with the malicious intent to spread misinformation and manipulate public opinion (Verma et al., 2024; Wenger et al., 2021; Buo, 2020; Bilika et al., 2023). Furthermore, beyond deepfake threats, the unauthorized exploitation of copyrighted content is also a growing concern in the AI industry (Singer, 2024; Qiwei et al., 2024; Brigham et al., 2024). These days, many content creators fall victim of copyright infringement due to the unauthorized use of their content for AI training and editing (Office, 2023; Abbott & Rothman, 2023). Original content is now being exploited and modified at scale in a way that makes it hard to track data provenance (Cho, 2024; Robinson, 2024; Vermillio, 2024). Among the various solutions to track audio provenance and guarantee artists' protection (Ren et al., 2024; Desai & Riedl, 2024; Liu et al., 2024c), watermarking stands out as one of the most effective proactive approaches. It involves embedding imperceptible perturbations into the audio that are both identifiable and traceable. Watermarking enables two key processes: detection, which verifies the presence of the watermark in an audio file, and attribution, which involves decoding a message that uniquely identifies the original creator.\nInitialized by WavMark (Chen et al., 2023) and the seminal work of AudioSeal (San Roman et al., 2024), using end-to-end deep neural networks for learning to watermark audio content has demonstrated stronger robustness with minor quality degradation compared to the state-of-the-art hand-crafted watermarking method (Westerfeld, 2020). This is especially true under more challenging audio editing transformations, like EnCodec (Defossez et al., 2022). WavMark proposes to use an invertible neural network architecture with a 16-bit synchronization code and a 16-bit message code to jointly conduct detection and attribution. However, the brute-force decoding approach in WavMark is inefficient, and the invertible architecture limits the watermarking capacity under more challenging transformations (Chen et al., 2023). Further work is represented by AudioSeal (San Roman et al., 2024), which utilizes a disjointed generator-detector framework with a detection head and a message decoding head split to improve robustness against advanced transformations. Nevertheless, AudioSeal improves detection at the cost of reduced accuracy in attribution. In a nutshell, whether the neural-network-based watermarking can achieve both robust detection and attribution is still an unsolved puzzle.\nThis paper identifies the following key factors to bridge the gap: first, motivated by the shared-parameter architecture advantage of WavMark for boosting learning efficiency, and the disjointed generator-detector architecture of AudioSeal for robust capability, we introduce a blended architecture of partial parameter sharing between generator and detector, which jointly achieve both efficient learning and robustness. Specifically, we propose a cross-attention module that leverages a shared embedding table to facilitate message decoding in the detector part. Secondly, we design a simple yet effective conditioning mechanism that distributes the message temporally before injection, which further improves the learning efficiency. With these two key components, we obverse significant boost in both robust detection and attribution performance. Furthermore, to enforce stronger constraints for watermark imperceptibility, we introduce a new per-tile temporal-frequency (TF) masking loss. Specifically, we first compute masking energy with an asymmetric 2D kernel, identify the masked regions and then use the masking energy as a weighting factor for computing a TF-weighted l2 loss in the mel-spectrogram domain. With these efforts, we demonstrate state-of-the-art robustness across a wide range of audio editing transformations, while preserving superior perceptual quality (see Figure 1). Furthermore, under the more challenging task of generative model editing, we demonstrate that XATTNMARK is the only watermarking approach that can conduct watermark detection even when edits of strong strength are applied. We summarize our main contributions as follows:"}, {"title": "\u2022 Blending the architecture advantages of previous works, we introduce partial parameter sharing between the neural generator and the detector, with an embedding table as the bridge and a cross-attention module in the detector as the core, to allow for more efficient learning and accurate message retrieval. Furthermore, we introduce a simple yet effective message conditioning module that distributes the latent message temporally, boosting the attribution learning efficiency.", "content": ""}, {"title": "\u2022 To increase the watermarking quality, we introduce a new psychoacoustic-inspired temporal-frequency (TF) masking loss that captures per-tile masking effects. We compute masking energy with an asymmetric 2D kernel, identify the masked TF tiles, and downweight these tiles with a TF-weighted l2 loss, achieving more imperceptible watermarking.", "content": ""}, {"title": "\u2022 We empirically show that our approach can achieve state-of-the-art performance in both detection and attribution with comparable perceptual quality and superior robustness. Furthermore, testing in a zero-shot manner on unseen generative editing transformations, we show that our approach is the only one that can still successfully conduct watermark detection even with strong editing strength.", "content": ""}, {"title": "2. Related Work", "content": "Audio Watermarking. Audio watermarking has evolved significantly from traditional signal processing to modern deep learning approaches. Early rule-based methods focused on embedding watermarks in time or frequency domains through hand-crafted techniques (Zhang, 2020; Hu et al., 2020; Zhang & Han, 2017; Qin et al., 2022). A notable example is AudiowMark (Westerfeld, 2020), which embeds a 128-bit message using convolutional coding and selective frequency band modifications. While sophisticated, these hand-crafted approaches often struggle with robustness against challenging transformations like neural audio codecs (Defossez et al., 2022). Deep neural networks (DNNs) have enabled more robust end-to-end watermarking systems that can generalize to unseen transformations (San Roman et al., 2024; Chen et al., 2023; Liu et al., 2023a). WavMark (Chen et al., 2023) introduced an invertible neural architecture for joint detection and attribution with 16-bit synchronization codes. While achieving strong performance, its brute-force decoding and architectural constraints limit scalability. AudioSeal (San Roman et al., 2024) addressed these limitations with a generator-detector design with separate detection and message decoding. However, this improved detection came at the cost of attribution accuracy. Our work focuses on enabling both robust detection and accurate attribution through a more efficient architecture design with psychoacoustic-inspired quality loss."}, {"title": "Source Attribution", "content": "A central objective in copyright protection is the ability to trace and verify the origin of creative works, which remains challenging in the realm of generative audio. Recent efforts have highlighted the necessity of robust source attribution mechanisms that work reliably across different transformations. For instance, Agnew et al. (Agnew et al., 2024) performed an extensive audit of popular audio datasets and revealed serious intellectual property infringements, underscoring the urgency for transparent dataset documentation and reliable authorship checks. In the music domain specifically, Barnett et al. (Barnett et al., 2024) advanced source attribution by leveraging audio embeddings to identify influential training data in generative music models, enabling a more transparent \u201cmusical roots\" analysis. Such embedding-based similarity checks align with the broader push for dataset auditing, as reflected in Du et al. (Du et al., 2024), who argue for holistic copyright auditing mechanisms throughout the existing machine learning processes. In this work, we propose a neural watermarking system that achieves SoTA message decoding performance, which is an essential step toward robust source attribution."}, {"title": "3. Preliminaries", "content": "Audio Watermarking. Audio watermarking systems typically comprise two primary components: a generator G to embed watermark, and a detector D for recovering them. Let $x \\in \\mathbb{R}^{T}$ be an audio signal of length T, and let $w \\in \\{0,1\\}^{K}$ be a binary watermark sequence. The generator $G : \\mathbb{R}^{T} \\times \\{0,1\\}^{K} \\rightarrow \\mathbb{R}^{T}$ outputs a watermarked signal $x_{w} = G(x, w)$, which should ideally preserve the audio's perceptual quality. The detector D is then responsible for two tasks. First, its detection head $D_{det} : \\mathbb{R}^{T} \\rightarrow [0,1]$ produces a probability indicating whether an input contains a valid watermark, which yields a final decision of presence or absence with a threshold. Second, its decoder head $D_{msg} : \\mathbb{R}^{T} \\rightarrow [0, 1]^{K}$ returns a vector indicating the probability of each bit being 1, which can be thresholded to reconstruct the embedded message. In practical settings, the watermarked audio may undergo various transformations $T(\\cdot)$ such as compression and cropping, resulting in a distorted signal $x_{t} = T(x_{w})$. Across different transformations, an ideal detector should be able to robustly detect and decode watermarks. In essence, the detection head should output probabilities close to 1 for watermarked signals (and close to 0 otherwise), while the decoder head should recover a bit sequence w matching w. In this paper, we focus on the per-sample level detection and attribution, which is a common and practical setting in real-world watermarking applications (Liu et al., 2024a).\nLearning to Watermark with Two-Headed Detector. We first present the general formulation of a neural watermarking system with detection and decoding separation. A neural watermarking system is parameterized by $\\Theta = (\\Theta_{G}, \\Theta_{D})$, where $\\Theta_{G}$ denotes the generator parameters and $\\Theta_{D}$ denotes the detector parameters. During training, the detection head typically first produces a logit for each audio input, which is then passed through a sigmoid function to yield a probability $p(x) = D_{det}(x) \\in [0,1]$. For a watermarked signal $x_w$ and an unwatermarked signal $x$, the detection head's objective is to correctly classify both, often formulated by maximizing the following expected log-likelihood:\n$\\max_{\\Theta_{D}} (\\mathbb{E}_{x,w,T} [\\log D_{det}(x_t)] + \\mathbb{E}_{x,T} [\\log(1-D_{det}(x_t))]),$\nwhere $x_{w} = G(x, w)$ and $x_{t} = T(x_{w})$. During the inference time, the threshold $\\tau$ is usually tuned on the validation set to control a certain level of false positive rate.\nMeanwhile, the decoder head produces K probabilities $D_{msg}(x) = [D_{msg}(x)_{1},..., D_{msg}(x)_{K}]$, where each entry indicates the likelihood of a particular bit being 1. Its training objective is to maximize the probability of correctly predicting each bit of w, which can be expressed as:\n$\\max_{\\Theta_{D_{msg}}} \\mathbb{E}_{x,w_k, T}[w_{k} \\log D_{msg}(x)_{k} +(1-w_{k})\\log(1-D_{msg}(x)_{k})]. \\quad (1)$\nBy thresholding each $D_{msg}(x)_{k}$ at around 0.5, the decoded bit string $\\hat{w}$ is obtained, with the learning objective seeking $\\hat{w} = w$ despite the distortion layer T over $x_{w}$."}, {"title": "3.1. Overview of AudioSeal (San Roman et al., 2024)", "content": "To contextualize our architectural improvements, we first analyze AudioSeal's framework, which pioneered the disjointed generator-detector paradigm for neural watermarking. While it achieves strong detection robustness, its attribution limitations motivate key aspects of XATTNMARK.\nDisjointed Generator\u2013Detector Architecture. AudioSeal employs two separate networks for watermark generation and detection: given audio x, the generator $G = \\{E_{gen}, I_{gen}\\}$ is composed of an encoder $E_{gen}$ that encodes the audio into compact latent $h_x$, and a decoder $I_{gen}$ that decodes the latent into the watermarked perturbation. The audio latent is obtained via $h_x = E_{gen}(x) \\in \\mathbb{R}^{t'\\times H}$, where $t' = [T/a]$, a is the temporal downsampling factor, and H is the latent dimension. The secret message w is injected into the latent space with a message encoder M that maps w to its latent $h_w = M(w)$. The modulated latent waveform is obtained via direct addition $h(x, w) = h_x + h_w$, following with the decoder network $I_{gen}$ to produce the predicted watermarked perturbation $\\delta_w = I_{gen}(h(x, w)) \\in \\mathbb{R}^{T\\times 1}$, which is later applied on the original audio to produce the watermarked audio $x_w = x + \\delta_w$. Note that the decoder model $I_{gen}$ shares a symmetric structure with the encoder $E_{gen}$, with layers of residual transposed convolution for temporal upsampling. The watermark detector $D = \\{E_{det}, \\Theta_{dec}\\}$ is composed of an encoder $E_{det}$ (sharing similar structure with $E_{gen}$ but with an additional padding to match original length T) and a unified decoding head $\\Theta_{dec} \\in \\mathbb{R}^{H\\times(1+K)}$ for both detection and message decoding. Given potential watermarked audio $x_t$, the encoder $E_{det}$ first processes the audio to obtain the latent $h_{t} = E_{det}(x) \\in \\mathbb{R}^{T \\times H}$. Then the decoding head $\\Theta_{dec}$ is applied on the latent to obtain the logits for both detection and message decoding, i.e., $\\Theta_{dec}(h_{t}) \\in \\mathbb{R}^{T\\times(1+K)}$."}, {"title": "4. Methodology", "content": "Despite AudioSeal achieving robust detection, the model struggles to perform accurate message decoding even without distortion (See Table 5). In this work, following the architecture backbone of AudioSeal, we identify and resolve two architectural limitations: disjoint generator-detector and the information bottleneck caused by embedding mean-pooling. Specifically, we first propose a cross-attention generator-detector watermarking system with a shared embedding table and temporal modulation to improve learning efficiency in message decoding. Then, to further improve watermark quality, we propose a psychoacoustic-aligned temporal-frequency masking l2 loss. The overall framework is shown in Figure 2."}, {"title": "4.1. Cross-Attention Generator-Detector Watermarking System with Shared Embedding Table", "content": "We observe that the fully disjointed architecture of AudioSeal ($\\Theta_{G} \\neq \\Theta_{D}$) often converges fast for the watermark detection learning but struggles to learn the message decoding part efficiently and accurately. On the other hand, the full parameter-sharing architecture of WavMark ($\\Theta_{G} = \\Theta_{D}$) can achieve superior efficiency in learning both detection and message decoding but lacks enough robustness capability against various distortions (See App. C.2). This motivates us to explore a blended architecture with partial parameter sharing between the generator and the detector, which has the potential to boost message decoding learning while preserving robust capability.\nOur key design is to share the message conditioning module M between the generator and the detector, which helps bridge the information flow between how the message is composed and how it can be reconstructed in the detector part. Specifically, the learnable part of the embedding table E in M, which serves as the fundamental vector set for composing the message latent in the generator part, is now utilized as a reference when decomposing the latent to retrieve the message bits in the detector. To achieve this, we propose leveraging a cross-attention block (Vaswani, 2017) to use the embedding table E as a (key-value) reference for message decoding, given the query audio latent $h_t$. We describe this module in detail in the following section.\nMessage Decoding with Cross-Attention. Ideally, for the given embedding table E with 2K entries and a watermarked audio $x_{w} = x + G(x, w)$, we want to reconstruct the original K embedding vectors $[E_{I_{1}}, ..., E_{I_{K}}]$, that were used to compose the ground-truth message latent $h_w$ and then feed them as context for the message decoding. To achieve this, we use an attention mechanism that transforms the embedding table into key and value using two different linear projections, and does attention-based vector merging with a query from the reconstructed latent representation $h_{t}$. Specifically, since the nearby two entries in E represent one position with different bit states, merging them into one 2H-dim vector represents the latent of each position. We transform this reshaped embedding matrix $E' \\in \\mathbb{R}^{K\\times 2H}$ into K, V with two linear projections $W_{K}, W_{V} \\in \\mathbb{R}^{2H\\times H}$, that is $K = E'W_{K}$ and $V = E'W_{V}$. Then, we \u201cdemodulate\u201d along the temporal axis the reconstructed waveform latent $h_{t} \\in \\mathbb{R}^{T\\times H}$, to obtain the first version of raw prediction $\\hat{V}_{w}$ for the original K components. To do this, we first use a linear projection $W_{dem} \\in \\mathbb{R}^{T\\times K}$ for obtaining the query sequence $h_{dem} = W^{T}_{dem}h_{t} \\in \\mathbb{R}^{K\\times H}$ and follow with a linear query projection $W_{Q} \\in \\mathbb{R}^{H\\times H}$. Then we can utilize the embedding table E to further refine the final component prediction $\\hat{V}_{w}$ with the following cross-attention mechanism:\n$Q = h_{dem} W_{Q} \\in \\mathbb{R}^{K\\times H},$\n$K = E' W_{K} \\in \\mathbb{R}^{K\\times H}$\n$V = E' W_{V} \\in \\mathbb{R}^{K\\times H}$\n$A = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{H}}) \\in \\mathbb{R}^{K\\times K}, \\quad (2)$\n$\\hat{V}_{w} = \\text{act}(AV) \\in \\mathbb{R}^{K\\times H},$\nwhere H is the latent dimension, K is the number of message bits, t is the temporal dimension, and act(\u00b7) can be any activation function. In this study, we use exponential linear units (ELU) (Clevert, 2015). With the predicted $\\hat{V}_{w}$, we further employ a linear projection layer $W_{dec} \\in \\mathbb{R}^{H\\times 1}$ with sigmoid activation function for the construction of the message decoding head. The final logit for message prediction bit is then obtained as $w_{t} = \\sigma(\\hat{V}_{w}W_{dec})$, where $\\sigma(\\cdot)$ is the sigmoid activation function.\nMessage Embedding via Temporal Modulation. In AudioSeal, the message latent is obtained by a mean-pooling operation with temporal-axis repetition, $h_w = \\text{Repeat} (\\frac{1}{K}\\sum_{i=1}^{K} E_{I_{i}}, t') \\in \\mathbb{R}^{t'\\times H}$.\nThis approach can be viewed as injecting the message information mostly into the frequency domain, which greatly limits the message hiding capabilities. To improve upon this, knowing the input audio length T, we introduce a temporal message conditioning mechanism that employs a linear modulation layer $W_{M} \\in \\mathbb{R}^{K\\times t'}$ to obtain the message latent:\n$h_w = W_M V(w) \\in \\mathbb{R}^{t' \\times H}. \\quad (3)$\nThis simple design not only prevents the sole reliance on the frequency domain for distributing the K-bit message, but also significantly facilitates the learning process for message decoding (see Figure 4)."}, {"title": "4.2. Psychoacoustic-Aligned Temporal-Frequency Masking Loss", "content": "Achieving imperceptibility is a key requirement of any watermarking system. Ideally, a watermark should exploit the perception characteristics of the human auditory system so that artifacts remain imperceptible. Following AudioSeal, we place a l\u2081 constraint on the watermark signal d to ensure waveform-domain smoothness, and we incorporate a multi-scale Mel spectrogram loss $L_{msspec}$ (e.g., as in (Defossez et al., 2022)) to manage frequency-domain fidelity.\nWe also adopt adversarial losses $L_{adv}$ on multi-scale STFT spectrograms for perceptual improvement. However, simply pushing the watermarked audio distribution to be close to the clean audio in an adversarial sense may penalize desirable watermark characteristics (e.g., certain musical \u201cremixes\u201d can produce satisfactory imperceptible watermarks, but still yield high adversarial loss). Thus, we choose to down-weight $L_{adv}$ and focus more on the psychoacoustic aspects to develop a human-centric, imperceptible watermark.\nPsychoacoustic Masking Principles. Our design is based on psychoacoustic masking principles (Gelfand, 2017; Holdsworth et al., 1988), which describe how listeners are less sensitive to low-level sounds occurring near stronger sounds, both in time (forward/backward masking) and in frequency (simultaneous masking). In other words, when a loud audio component is present at a certain time-frequency location, it can mask weaker signals in its temporal and frequency vicinity (Necciari et al., 2016). Inspired by this, we propose a novel l2 loss with per-tile TF-penalty.\nPer-Tile Penalty with Asymmetric Temporal-Frequency Decay. Let $S_{o}(m,t)$ denote the magnitude of the original audio's mel-spectrogram, and the watermarked Mel-spectrogram is $S_{w}(m, t)$. To identify those strong \u201cmasker\u201d tiles M, we apply a magnitude threshold as to different timestamps on each frequency band ($a_{s} = 0.8$):\n$M_{masker} = \\{(m, t) \\mid S_{o}(m, t) > a_{s} \\max_{t'} S_{o}(m,t')\\}.$\nEach index pair $(m_c, t_c) \\in M_{masker}$ acts as a masker, and we model its masking energy over the neighboring tiles as a linear decay in the decibel domain. Specifically, given mel-scale frequency radius $r_{m}^{+}$, $r_{m}^{-}$, and time-axis radius $r_{t}^{+}$, $r_{t}^{-}$, the local region of a masker is defined as:\n$R(m_c, t_c) = \\{(m, t) \\mid -r_m^{-} \\leq \\triangle m \\leq r_m^{+}, -r_t^{-} \\leq \\triangle t \\leq r_t^{+}\\},$\nwhere $\\triangle m = m - m_c$, and $\\triangle t = t -t_c$. Aligned with the empirical studies (Necciari et al., 2016) that the post-masking region (forward masking) is usually longer than the pre-masking region (backward masking) in the temporal axis, we set asymmetric radii $r_{t}^{+}$ and $r_{t}^{-}$, with forward masking region of 200ms and backward masking region of 20ms. For the frequency-bin radius, we first compute the empirical critical bandwidth for each mel-bin using the formula from Zwicker & Fastl (2013),\n$F(V[m_c]) = \\frac{1000}{(1+1.4(\\frac{V[m_c]}{1000})^{2})^{0.69}},$\n$W(m_c) = 25 + 75(1 + \\frac{F(V[m_c])}{1000})$, where\n$F(m_c)$ is the approximated center frequency obtained by converting each Mel bin value $V[m_c]$ back to Hz using $F(m_c) = 700 \\cdot (10^{\\frac{V[m_c]}{2595}} - 1)$. Then the radii for each mel-bin is set as $r_m^{+} = r_m^{-} = r = \\gamma$, where $\\gamma = W(m_c)/|F(m_{1}) - F(m_{0})|$ is an adaptive scaling term, $V[m_{i}]$ denote the i-th mel-bin value, $r_m$ is a base radius that is set to be 3. With the dynamic radius scaled by the critical bandwidth \u2013 where a higher frequency component has a wider masking range (Holdsworth et al., 1988) \u2013 we observe a better performance than using constant $\\gamma = 1$.\nThe physical structure of the cochlea determines that lower frequencies (located at the base of the cochlea) cause broader and stronger excitation patterns compared to the higher frequencies (located at the apex). To inject this bias of slower decay in the upward spread of masking (Zwicker & Fastl, 2013), we set the decaying term for the upward direction to be larger than the downward direction, i.e., $\\alpha_{f}^{+} \\le \\alpha_{f}^{-}$. For the temporal decay, following Necciari et al. (2016), we set the backward direction to be larger than the forward direction, i.e., $\\beta_{t}^{+} \\le \\beta_{t}^{-}$. Given the masker $(m_c, t_c)$, the threshold energy $E_{mask}(m,t)$ for the maskee tile (m, t) is computed as:\n$E_{mask}(m, t; (m_c, t_c)) = 20 \\log_{10} S_{o}(m_c, t_c) - \\triangle E,$\nwhere $\\triangle E = \\alpha_{f}^{-} \\max(0, \\triangle m) + \\alpha_{f}^{+} \\min(0, \\triangle m) \\quad (4)$\n$\\beta_{t}^{+} \\max(0, \\triangle t) + \\beta_{t}^{-} \\min(0, \\triangle t),$\nIn psychoacoustic modeling, particularly in audio compression (Bosi et al., 1997), the global masking threshold is determined by considering only the most dominant masker at each frequency component. Following this, the final masking threshold $E_{mask}(m, t)$ is computed as:\n$E_{mask}(m, t) = \\max_{(m_c,t_c) \\in M_{masker}, \\\\(m,t) \\in R(m_c,t_c)} E_{mask}(m, t; (m_c,t_c)).$\nThis ensures each maskee tile receives masking energy from its most perceptually dominant masker, while weaker maskers have negligible impact (Gelfand, 2017).\nPer-Tile-Weighted l\u2082 Loss Computation. Psychoacoustic masking effectively applies only when the masker's masking energy surpasses the tile's energy. We therefore apply the masking threshold on watermarked mel-spectrogram at $20 \\log_{10} S_{w} (m, t)$, filtering the masked tiles set as:\n$M_{maskee} = \\{(m, t) \\mid E_{mask} (m, t) > 20 \\log_{10} S_{w} (m,t)\\}$.\nTo encourage the model to embed the watermark in those maskee regions, we design per-tile penalty terms which weight the l2 difference (Chen et al., 2023) between the watermarked and original audio in Mel-spectrogram space:\n$\\chi(m,t) = 1 + 1_{(m,t) \\in M_{maskee}} 10^{E_{mask} (m,t)/20}, \\quad (5)$\n$L_{TF} = \\sum_{(m,t)} \\chi(m,t) \\cdot ||S_{w} (m,t) - S_{o}(m,t)||^2. \\quad (6)$\nwhere $10^{E_{mask}(m,t)/20}$ is the masking energy, and the weighting term $\\chi(m, t)$ is larger for tiles in the masking region $M_{maskee}$, effectively allowing more watermark signal to be embedded in those locations. For non-masked tiles, $\\chi(m, t) = 1$ enforces standard l2 loss."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Experimental Setup", "content": "Following prior works (San Roman et al., 2024; Chen et al., 2023), we use a sampling rate of 16 kHz and one-second mono samples for training (T = 16000) under 16 diverse audio editing transformations. We train the models on a mixed audio dataset of 4100 hours containing speech (3016 hours Vox Populi (Wang et al., 2021) and 100 hours LibriSpeech (Panayotov et al., 2015)), music (9 hours MusicCap (Agostinelli et al., 2023)) and sound effects (98 hours AudioSet (Gemmeke et al., 2017)). For evaluation, we use a held-out test set from MusicCap of size 100. The audio duration is set as 5s by default in evaluation. The loss weights are set as: $\\lambda_{LTF} = 1, \\lambda_{adv} = 1, \\lambda_{l1} = 0.1$, $\\lambda_{msspec} = 2, \\lambda_{detect} = \\lambda_{message} = 10$. We use the Adam optimizer (Kingma & Ba, 2014) with learning rate 1e-5, $\\beta_{1} = 0.4, \\beta_{2} = 0.9$, and Exponential Moving Average (EMA) with decay factor of 0.99 updated every step. The training uses batch size 16 for a total of 73k steps. The model's latent dimension is set to H = 32. For the attribution experiment, we follow the simulation protocol in San Roman et al. (2024) that defines a message pool of size \u039d (\u2208 {100, 1000, 10000}), where each message is uniquely associated to a different user. As N increases, the message length (in bits) also grows, making attribution more challenging due to the increased complexity of distinguishing individual messages. During decoding, the message is retrieved from the pool by selecting the one with the closest Hamming distance. We compare to four SoTA baselines: AudiowMark (Westerfeld, 2020), WavMark (Chen et al., 2023), TimbreWM (Liu et al., 2023a), AudioSeal (San Roman et al., 2024). Please refer to App. A for more details."}, {"title": "5.2. Detection and Attribution Effectiveness", "content": "Robustness to Standard Edits. We present the detection and attribution performance across a comprehensive suite of standard audio editing operations in Table 1. For detection, we report both the overall accuracy and the true/false positive rates (TPR/FPR). For attribution, we report the averaged performance across different user numbers (see Figure. 3 for decomposed result). Our method achieves new state-of-the-art performance on both tasks, maintaining high detection accuracy (99.19% average) and attribution accuracy (93%) on average across transformations. Notably, while AudioSeal demonstrates strong detection performance (97.1% average), it fails to perform effective attribution (39% average). The traditional approach AudiowMark exhibits more balanced detection-attribution trade-offs (around 88% attribution) but lower overall detection performance (92.9% average). On the speeding operation, our method achieves 99.5% detection accuracy while all other methods degrade to near random-guess levels (50-61%). Moreover, while all the models currently fail to attribute against speeding transformation, we demonstrate that our model can be further enhanced with a simple speed reversion layer to boost speed robustness while maintaining efficiency in the App. C.3.1. Finally, we validate the scalability of our watermarking system in Figure 3, which highlights that XATTNMARK can maintain high attribution accuracy even when the users pool becomes larger. In summary, we demonstrate that XATTNMARK is more robust under standard edits for both detection and attribution. See App. C.3 for more details.\nRobustness to Generative Edits. Beyond the standard audio transformations that are seen during training, audio generative editing is one particularly challenging type of editing that watermarking systems might have to endure at deployment (Liu et al., 2024b). To simulate this, we use two state-of-the-art audio generative models, AudioLDM2 (Liu et al., 2024b) and Stable Audio (Evans et al., 2024), with a text-guided DDIM inversion method proposed in ZETA (Manor & Michaeli, 2024). We test various editing strength t \u2208 {10, 70, 110}, which represents the diffusion forward step when using DDIM inversion. As shown in Table 2, AudiowMark and WavMark degrade to a random-guess level performance, while TimbreWM and AudioSeal show inferior performance across different editing strengths around 50-60%. In contrast, our method maintains consistently high detection accuracy, averaging 91-94% across all editing strengths, and consistent across both generative models. This demonstrates that XATTNMARK generalize better to unseen generative edits compared to existing methods."}, {"title": "5.3. Quality and Stealthiness Assessment", "content": "Following San Roman et al. (2024), we evaluate the following objective quality"}]}