{"title": "UU-Mamba: Uncertainty-aware U-Mamba for Cardiovascular Segmentation", "authors": ["Ting Yu Tsai", "Li Lin", "Shu Hu", "Connie W. Tsao", "Xin Li", "Ming-Ching Chang", "Hongtu Zhu", "Xin Wang"], "abstract": "Building on the success of deep learning models in cardiovascular structure segmentation, increasing attention has been focused on improving generalization and robustness, particularly in small, annotated datasets. Despite recent advancements, current approaches often face challenges such as overfitting and accuracy limitations, largely due to their reliance on large datasets and narrow optimization techniques. This paper introduces the UU-Mamba model, an extension of the U-Mamba architecture, designed to address these challenges in both cardiac and vascular segmentation. By incorporating Sharpness-Aware Minimization (SAM), the model enhances generalization by targeting flatter minima in the loss landscape. Additionally, we propose an uncertainty-aware loss function that combines region-based, distribution-based, and pixel-based components to improve segmentation accuracy by capturing both local and global features. While the UU-Mamba model has already demonstrated great performance, further testing is required to fully assess its generalization and robustness. We expand our evaluation by conducting new trials on the ImageCAS (coronary artery) and Aorta (aortic branches and zones) datasets, which present more complex segmentation challenges than the ACDC dataset (left and right ventricles) used in our previous work, showcasing the model's adaptability and resilience. We confirm UU-Mamba's superior performance over leading models such as TransUNet,", "sections": [{"title": "1. Introduction", "content": "Biomedical image segmentation is crucial for medical image analysis, enabling the precise identification and delineation of anatomical structures and abnormalities [1]. Segmentation of cardiovascular structures, such as the heart, aorta, and coronary arteries, from Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) is essential for diagnosing a wide range of cardiovascular conditions, developing treatment plans, and evaluating therapeutic outcomes [2, 3]. Both MRI and CT provide high-resolution images that offer detailed insights into the structure, function, and composition of these cardiovascular regions. However, manually segmenting these structures is time-consuming, labor-intensive, and prone to observer variability, underscoring the importance of automated segmentation techniques to ensure consistency and improve efficiency [4, 5].\nThe variability in cardiovascular anatomy, pathological changes, and the presence of imaging artifacts present significant challenges to segmenting both MRI and CT images [6]. Conventional techniques like thresholding and edge detection often fail to accurately capture the complex morphology of the heart, aorta, and coronary arteries. Recent advancements in machine learning, particularly through Convolutional Neural Networks (CNNs) [7] and other deep learning models, have shown potential in overcoming these challenges by learning intricate patterns from large datasets [8]. However, these models often require extensive computational resources and large annotated datasets, and their ability to generalize across diverse patient populations and imaging conditions may be limited [3].\nTo improve the generalizability and accuracy of segmentation across different cardiovascular structures, various specialized datasets have been developed. For example, the Automated Cardiac Diagnosis Challenge (ACDC) dataset [2] is designed for segmenting cardiac structures such as the left and right ventricles and myocardium from MRI. The ImageCAS dataset [9] focuses on the segmentation of coronary arteries, which is crucial for assessing"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Cardiovascular Segmentation", "content": "The development of deep learning techniques, particularly Convolutional Neural Networks (CNNs) [7], has significantly advanced cardiovascular segmentation [28, 29], driven by comprehensive datasets like ACDC [2], Image-CAS [9], and Aorta [10, 11]. These datasets address a range of cardiovascular structures, including heart chambers, coronary arteries, and aortic branches, and present challenges related to anatomical variability and disease-specific characteristics. Multi-modality cardiac imaging segmentation, which utilizes imaging modalities like Positron Emission Tomography (PET), Single Photon Emission Computed Tomography (SPECT), MRI, and CT, aims to precisely segment anatomical structures and pathological regions. However, inherent challenges such as phase alignment, resolution, and image quality imbalances complicate the process. Traditional methods, including registration-based segmentation with multi-atlas approaches, and fusion-based segmentation techniques, address these issues by combining information across modalities [30, 31, 32, 33], but these methods are computationally expensive and often require large datasets [34, 35]. Hybrid approaches, combining both traditional and deep learning methods, are emerging to improve the robustness and clinical applicability of cardiovascular segmentation [36].\nRecent advances in deep learning have improved segmentation accuracy by enabling complex representations of cardiovascular structures. For instance, CNNs like U-Net [37, 38] and its variants have been effective for cardiovascular segmentation tasks, particularly on the ACDC dataset [2]. However, these methods are prone to overfitting and issues such as class imbalance, which complicates their application across diverse datasets like ImageCAS [9] and Aorta [10, 11]. Zeng et al. [9] tackled coronary artery segmentation challenges in ImageCAS, utilizing multi-scale feature extraction to capture finer details, but the method still struggles with arteries exhibiting atypical morphology, requiring precise hyperparameter tuning."}, {"title": "2.2. Mamba for Medical Image Segmentation", "content": "The Mamba architecture [41] represents a substantial advancement in medical image segmentation by integrating the capabilities of Vision Transformers (ViTs) [42] and Convolutional Neural Networks (CNNs) [7]. It is essential to achieve precision in medical imaging duties by integrating global contextual information and managing long sequences [43]. U-Mamba [12] expands the conventional U-Net framework [37, 38] by integrating attention mechanisms and multi-scale processing, thereby improving the accuracy and robustness of segmentation. This is achieved by building upon this"}, {"title": "2.3. Sharpness-Aware Minimization for Medical Image Segmentation", "content": "Sharpness-Aware Minimization (SAM) [13] has gained substantial attention for its capacity to enhance the generalization of deep learning models by optimizing both the training loss and the sharpness of the loss landscape. This method has been particularly advantageous in the field of medical image segmentation, where the ability to accurately detect boundaries and generalize them across a variety of datasets is essential.\nSAM has exhibited significant enhancements in model robustness and accuracy within the context of medical image segmentation. For instance, Mariam et al. [45] implemented SAM in the RF-UNet model to segment retinal vessels. In addition to enhanced metrics such as accuracy, sensitivity, and specificity, their experiments on the DRIVE dataset demonstrated a substantial decrease in both training and validation losses. This emphasizes SAM's capacity to improve generalization and mitigate overfitting in retinal segmentation tasks.\nAdditional research has investigated sophisticated variations of SAM for the purpose of medical segmentation. In the context of breast ultrasound image segmentation, Hassan et al. [46] assessed a variety of sharpness-based optimizers, such as SAM. Their results suggested that SAM consistently enhanced generalization across various models, surpassing other sharpness-based optimizers such as Adaptive SAM.\nRandom Sharpness-Aware Minimization (RSAM) by Liu et al. [47] is another innovation in SAM that incorporates randomness to enhance the efficiency and stability of SAM optimization. RSAM's potential for improved generalization across domains suggests that it has the potential to be effective in medical imaging, despite the fact that it has not yet been explicitly applied to medical segmentation. Li et al. [48] also proposed Friendly SAM (FSAM), which further optimizes SAM for improved performance in diverse and complex data environments. This direction could be highly relevant to medical segmentation.\nCollectively, these studies demonstrate that SAM [13] and its variants are essential for the advancement of medical image segmentation, particularly in tasks that necessitate high precision and robustness, such as retinal"}, {"title": "3. Method", "content": "Figure 1 presents a comparison of our method against basic approaches, highlighting the advancements made by our UU-Mamba model. Figure 2 illustrates our proposed UU-Mamba architecture, showcasing improvements in the training process. This model builds upon the foundational U-Mamba structure, where input images are effectively encoded. A key innovation in our approach is the integration of a novel uncertainty-aware loss function, designed to better capture and manage the inherent uncertainties in the segmentation task. To further enhance model performance, we employ the Sharpness-Aware Minimization (SAM) optimizer [13]. This optimizer is particularly well-suited for our architecture as it operates within a flattened loss landscape, which helps in achieving more robust and generalized training outcomes. These enhancements make UU-Mamba a more effective and adaptable model for both cardiac and vascular segmentation tasks. Section \u00a7 3.1 discusses the Mamba block and the U-Mamba network, with a focus on the integration of state space models and their effectiveness in capturing long-range dependencies. In Section \u00a7 3.2, we introduce our uncertainty-aware loss, detailing how it combines multiple loss functions to boost model performance and robustness. Finally, Section\u00a7 3.3 covers the Sharpness Aware Minimization Optimization, emphasizing its advantages in achieving flat minima in the loss landscape, thereby enhancing generalization and mitigating overfitting."}, {"title": "3.1. Mamba Block and U-Mamba Network", "content": "The U-Mamba network is designed to improve the accuracy of medical image segmentation and improve global context comprehension by combining the assets of the Mamba block [41] and U-Net [37, 38]. The Mamba block, which is specifically engineered for Selective Structured State Space Sequence Models (S6), is particularly well-suited for medical imaging duties due to its exceptional ability to manage long-range dependencies and sequential information.\nState Space Models (SSM) [49] describe systems in terms of their internal states and observations over time, thereby facilitating effective sequence modeling through these underlying states. The fundamental form is denoted as follows: $x_t$ is the input state vector, $u_t$ is the control input, $w_t$ is the process noise, $A$ is the state transition matrix, and $B$ is the control input"}, {"title": null, "content": "matrix.\n$X_{t+1} = Ax_t + Bu_t + w_t$. (1)\nFor observation $y_t$, calculated using the observation matrix $C$, feedthrough matrix $D$, and observation noise $v_t$, the formula is:\n$y_t = Cx_t + Du_t + v_t$. (2)\nThe S6 architecture advances traditional state space models by integrating selective attention mechanisms and structured parameterization. The selective attention mechanism can be represented as:\n$a_t = \\text{softmax}(QK^T/\\sqrt{d_k})V$, (3)\nwhere $Q$, $K$, and $V$ are the query, key, and value matrices that are derived from the state vector $x_t$, and $d_k$ is the dimension of the key vectors. This mechanism enables the model to effectively capture intricate dependencies by concentrating on pertinent components of the input sequence.\nThe integration of S6 into the Mamba block is especially crucial for sequential medical image processing tasks, such as cardiac MRI segmentation, which require the capture of temporal dynamics and structure [41]. The method, on the other hand, is exclusively concerned with per-image segmentation, which involves the application of the state transition and observation matrices ($A$, $C$, etc.) to individual images. Each image is treated independently.\nU-Mamba capitalizes on Mamba's linear scaling advantage to improve CNNs' capacity to simulate long-range dependencies, all while circumventing the high computational costs associated with self-attention mechanisms employed in Transformers [50] such as ViT [42] and SwinTransformer [51].\nThe U-Mamba block, which is comprised of two sequential residual blocks followed by a Mamba block, is depicted in Figure 2.\nAdditionally, each block includes Leaky ReLU activation, Instance Normalization, and convolutional layers. Mamba blocks with two parallel branches: one with an SSM layer and one without, flatten, transpose, normalize, and process image features. The Hadamard product is then employed to merge these features, which are subsequently projected back to their original shape and transposed.\nAn encoder with these blocks is included in the complete U-Mamba network architecture to capture both local features and long-range dependencies,"}, {"title": "3.2. Uncertainty-aware Loss", "content": "Introducing uncertainty into loss functions entails allocating weights to distinct components of the loss according to the estimated uncertainty for each data point [52, 53]. This method allows the model to concentrate on learning from more dependable instances while simultaneously reducing the impact of potentially erroneous or ambiguous data. Kendall and Gal introduced the concept of adjusting loss functions by utilizing homoscedastic and heteroscedastic uncertainty [54]. Heteroscedastic uncertainty fluctuates between instances, while homoscedastic uncertainty remains constant across all data points. The model can improve its resilience and precision by focusing on confident predictions and reducing the impact of equivocal ones by adapting its learning process to capitalize on these uncertainties. This optimization enhances overall performance and improves the training process across diverse datasets [55, 56, 57].\nTo further boost segmentation accuracy, we employ an uncertainty-aware loss function that combines region-based, distribution-based, and pixel-based losses, capitalizing on their complementary strengths:\n1. Dice loss [15]: This region-based metric emphasizes the overlap between predicted and ground truth areas, ensuring accurate preservation of shape and boundary details in segmented regions.\n2. Cross-Entropy (CE) loss [16]: This distribution-based loss ensures precise categorization of individual pixels, thereby improving classification accuracy.\n3. Focal loss [17]: This pixel-level loss addresses class imbalance by assigning greater importance to challenging instances, enhancing the model's ability to manage complex scenarios [58, 59, 60].\nLet $p_i$ denote the predicted probability and $g_i$ the ground truth label, with the predicted segmentation and the corresponding ground truth mask. The Dice Similarity Coefficient (DSC) is a metric that quantifies the degree"}, {"title": null, "content": "of overlap between the predicted segmentation and the ground truth. It is defined as follows:\n$\\text{DSC} = \\frac{2 \\sum_i p_i g_i}{\\sum_i p_i + \\sum_i g_i}$ (4\nDSC values range from 0 to 1, with 1 signifying complete overlap between the prediction and the ground truth and 0 indicating no overlap.\nThe Dice loss is defined as: in order to integrate this metric into a loss function for training segmentation models.\n$L_{\\text{Dice}} = 1 - \\text{DSC} = 1 - \\frac{2 \\sum_i p_i g_i}{\\sum_i p_i + \\sum_i g_i}$ (5)\nThe Dice loss is designed to minimize the discrepancy between the predicted segmentation and the ground truth by optimizing the DSC. The model is trained to generate segmentations that exhibit a greater overlap with the ground truth by minimizing the Dice loss, thereby enhancing the accuracy of the segmentation.\nThe standard entropy formula is employed to determine the Cross-Entropy (CE) loss:\n$L_{\\text{CE}} = - \\sum_i g_i \\log(p_i)$ (6)\nTo address class imbalance, we utilize the Focal loss, which focuses more on difficult-to-classify samples:\n$L_{\\text{focal}} = - \\sum_i (1 - p_i)^{\\gamma} g_i \\log(p_i)$ (7)\nwhere $\\gamma$ is a focusing parameter default to 2.\nThe uncertainty-aware loss $L_{UA}$ is defined by combining these loss components within an uncertainty-aware framework:\n$L_{UA} = \\sum_{m=1}^{M} \\frac{1}{(2 \\sigma_m^2)} L_m + \\log(1 + \\sigma_m^2)$ (8)\nin which $M$ is the number of individual loss components, $L_m$ represents each loss component (such as Dice, CE, and Focal loss), and $\\sigma_m$ are learnable parameters that modify the contribution of each loss component based on the estimated uncertainty. To reduce the aggregate loss, these parameters are optimized during the training process."}, {"title": "3.3. Sharpness-Aware Minimization Optimization", "content": "To improve the U-Mamba model's generalizaiton in segmenting cardiovascular images, such as those in the ACDC [2], ImageCAS [9], and Aorta dataset [10, 11], our methodology employs Sharpness-Aware Minimization (SAM) optimization [13]. The model's generalizability is enhanced by the flattening of the loss landscape, implemented by SAM optimization. Despite the fact that conventional optimization techniques are designed to identify the lowest points in the loss landscape, these points are frequently precipitous, which results in inadequate generalization to new data. SAM, on the other hand, identifies gentler minima\u2500regions in the parameter space where the model's performance remains consistent and is less susceptible to perturbations.\nSAM is employed due to its effective reduction of overfitting, a common issue in medical image segmentation. Performance on unseen data may be impaired by the narrow valleys in the loss landscape that are a common consequence of conventional optimization methods. SAM, in contrast, concentrates on the identification of flattened minima, which are linked to enhanced generalization. When dealing with complex and diverse datasets, this is especially beneficial, as the variability in cardiac MRI images can exacerbate overfitting if not properly managed.\nOptimization of SAM is accomplished through a two-step iterative procedure. Parameters are initially adjusted to optimize loss for each mini-batch. After this, the model parameters are adjusted to reduce the maximum loss. This perturbation is designed to identify model parameters that are located in flatter regions of the loss landscape, which are typically associated with improved generalization and increased robustness to minor changes in the input data.\nThe model parameters shall be denoted by $\\theta$, the loss function by $L$, and the training dataset by $D$. To investigate the loss landscape within a neighborhood around $\\theta$ defined by the norm constraint $|\\epsilon||_2 \\leq \\rho$, the perturbation $\\epsilon$ is introduced, with $\\rho$ determining the size of this neighborhood."}, {"title": null, "content": "$\\theta^* = \\text{arg} \\min_{\\theta} \\max_{||\\epsilon||_2 \\leq \\rho} L(\\theta + \\epsilon; D)$. (9)\nThe perturbation $\\epsilon$ within the $|\\epsilon||_2 \\leq \\rho$ constraint is determined in the initial phase to maximize the loss. This method identifies the worst-case direction in the local neighborhood of $\\theta$. Furthermore, this guarantees that the model parameters are directed toward regions of the loss landscape that are not precipitous. In order to enhance the parameters' resilience to perturbations, the model parameters $\\theta$ are modified in the second phase to reduce the loss at the worst-case perturbed location.\nBy applying these two stages iteratively, SAM directs the optimizer toward parameter configurations that are resilient to perturbations, thereby improving generalization and overall performance. The model is able to identify flattened minima in the loss landscape as a result of this approach, which results in improved generalization [13]."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Settings", "content": ""}, {"title": "4.1.1. The ACDC Dataset", "content": "The Automated Cardiac Diagnosis Challenge (ACDC) dataset [2] is a widely recognized benchmark in medical image analysis, particularly for cardiac MRI segmentation. With a total of 300 images and 2,978 slices, this dataset comprises MRI scans from 150 patients, each divided into numerous slices. normal subjects, myocardial infarction, dilated cardiomyopathy, hypertrophic cardiomyopathy, and abnormal right ventricle are the five distinct categories in which the patients are evenly distributed. Each group is distinguished by specific cardiac pathologies.\nThe dataset includes short-axis cardiac MRI images that provide a thorough examination of the heart. Ground truth annotations are supplied for the left ventricle (LV), right ventricle (RV), and myocardium (MYO) in each image, facilitating the formulation and assessment of segmentation algorithms."}, {"title": "4.1.2. The Aorta Dataset", "content": "The Aorta dataset [10, 11] is a meticulously annotated compilation of 50 Computed Tomography Angiography (CTA) images that enables the multi-class segmentation of the aorta and its branches. The axial dimensions of these images range from 389 \u00d7 389 pixels to 516 \u00d7 516 pixels, with an average size of 450 \u00d7 450 pixels. The dataset is guaranteed to be consistent in measurement, as each image maintains an isotropic voxel resolution of 1mm x 1mm x 1mm. The average number of axial segments per scan is 695, with image numbers ranging from 578 to 801. This dataset is essential for the development and testing of sophisticated algorithms that are designed to accurately segment the complex vascular structures within the aorta and"}, {"title": "4.1.3. The ImageCAS Dataset", "content": "The ImageCAS dataset [9] focuses on the segmentation of coronary arteries using CTA images. This dataset contains approximately 1,000 3D CTA images, making it considerably larger than existing public datasets in this domain, which are crucial for diagnosing and assessing coronary artery disease. The dataset is particularly challenging due to the small size and complex branching patterns of the coronary arteries, as well as the motion artifacts introduced by cardiac and respiratory movements. Ground truth annotations include detailed segmentations of the coronary arteries, providing a comprehensive framework for evaluating the performance of segmentation algorithms in detecting and delineating these critical structures.\nThese datasets provide a diverse and comprehensive set of challenges for cardiac and vascular segmentation, allowing us to rigorously evaluate the effectiveness and generalizability of the proposed UU-Mamba model across different anatomical structures and imaging modalities."}, {"title": "4.1.4. Evaluation Metrics", "content": "We employ the Dice Similarity Coefficient (DSC) as our primary metric for evaluating segmentation performance, as per the evaluation protocol outlined in [26]. The DSC assesses the overlap between the predicted segmentation and the ground truth mask, thereby providing a reliable indication of the model's accuracy in delineating cardiac structures. The DSC is defined in Eq. (4).\nAdditionally, we employ the Mean Squared Error (MSE) to assess the average squared difference between the predicted probabilities and the ground truth labels, in addition to DSC. Let N denote the number of testing images, H and W denote the height and width of the images, $p_{i,j}^n$ be the predicted probability at pixel (i, j) for the n-th image, and $g_{i,j}^n$ the corresponding ground truth label. The MSE is determined by the following formula:\n$\\text{MSE} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{H W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (p_{i,j}^n - g_{i,j}^n)^2$ (10)\nMSE offers a complementary evaluation, providing insight into the model's pixel-wise precision."}, {"title": "4.1.5. Implementation Details", "content": "We conducted the experiments using the PyTorch framework and two NVIDIA A100 Tensor Core GPUs for training. During training, for the ACDC dataset [2], we used a patch size of [20, 256, 224] and a batch size of 4, with the number of pooling operations per axis configured to [2, 5, 5]. In the case of the ImageCAS dataset [9], a patch size of [96, 160, 160] and a batch size of 2 were selected, with pooling operations per axis set to [4, 5, 5]. For the Aorta dataset [10, 11], the patch size was [176, 112, 112], also with a batch size of 2, and the pooling operations per axis were set to [4, 4, 4]. The network configuration comprises 6 stages. An initial learning rate of 5\u00d710-3 was utilized, and the training proceeded for 500 epochs. In the SAM optimization, the hyperparameter $\\rho$ controlling the perturbation in Eq. (9) was set to 0.05. The focusing parameter $\\gamma$ in the focal loss in Eq. (7) was set to 2. The parameter M of the uncertainty-aware loss in Eq. (8) was set to 3, incorporating Dice loss, Cross-Entropy loss, and Focal loss."}, {"title": "4.2. Experimental Results", "content": "We conduct a comparison of UU-Mamba with five of the state-of-the-art segmentation models\u2014TransUNet [61], Swin-Unet [62], nnUNet [37], nn-Former [63], and U-Mamba [12] on the ACDC dataset [2]. Transformer-based networks are TransUNet and Swin-Unet, while nnUNet and nnFormer employ CNN-based architectures. U-Mamba is a hybrid architecture that"}, {"title": "4.3. Ablation Study", "content": "We perform an ablation study to investigate the impact of integrating Sharpness-Aware Minimization (SAM) optimization [13] into our UU-Mamba model, alongside traditional and uncertainty-aware loss functions. This study spans three datasets: ACDC [2], Aorta [10, 11], and ImageCAS [9], and employs 3D loss surface visualizations [27] using ParaView [64, 65, 66] to demonstrate the smoother loss landscapes indicative of model robustness and improved generalization."}, {"title": "4.3.1. Impact of Traditional and Uncertainty-Aware Loss Functions", "content": "To evaluate the impact of different loss functions on model performance, we first assess the baseline performance using the standard Cross-Entropy"}, {"title": null, "content": "(CE) loss. This traditional approach achieves satisfactory results but has limitations in handling complex segmentation challenges. To address these, we introduce an uncertainty-aware loss function, which combines Dice loss, CE loss, and Focal loss. This combination provides a more balanced approach by emphasizing confident predictions and reducing the negative impact of uncertain areas in the segmentation process.\nIn the ACDC dataset [2], the model trained with only CE loss achieves a Dice Similarity Coefficient (DSC) of 92.263%. When incorporating the uncertainty-aware loss, the DSC improves to 92.602%. This enhancement reflects the efficacy of the uncertainty-aware approach in improving segmentation resilience by carefully managing prediction uncertainty and refining the model's focus on areas of high confidence, as also shown in Table 2.\nFor the Aorta dataset [10, 11], training with only CE loss yields an average Dice Similarity Coefficient (DSC) of 73.761%. Incorporating the uncertainty-aware loss results in notable improvements across various anatomical zones and arteries, raising the average DSC to 75.053% and average NSD to 91.747%, also reducing the Mean Squared Error (MSE). These improvements, as shown in Table 3, highlight the uncertainty-aware loss's ability to handle complex segmentation tasks by balancing confident predictions with uncertain areas.\nFor the ImageCAS dataset [9], the model trained with only CE loss achieves an average Dice Similarity Coefficient (DSC) of 79.496%. By incorporating the uncertainty-aware loss, the average DSC improves to 81.146% and NSD improves to 88.045%, demonstrating the advantage of this comprehensive loss function in enhancing segmentation accuracy, particularly in challenging regions, as shown in Table 4."}, {"title": "4.3.2. Enhancements with Sharpness-Aware Minimization optimization", "content": "Building on the incorporation of uncertainty-aware loss, we further integrate Sharpness-Aware Minimization (SAM) optimization [13] to explore its additional benefits. SAM is designed to steer the training process toward flatter minima, which are associated with improved generalization in neural network models. Figure 6 illustrates a comparison of the loss landscapes between models trained with and without SAM optimization.\nIn Table 2, incorporating SAM with the uncertainty-aware loss increases the ACDC dataset's DSC to 92.787%, the highest among the tested methods, thus validating SAM's role in enhancing segmentation precision and generalization. In the Figure 6, the loss landscape on the ACDC dataset is shown for models with and without SAM optimization. The 3D loss surface plot of the model without SAM optimization exhibits a broader range of loss values, characterized by sharper and more erratic loss contours. In contrast, the model utilizing SAM optimization displays a flatter and smoother loss landscape, indicating improved stability and generalization.\nAs shown in Table 3, incorporating SAM into the model training process increases the average DSC to 77.084%, along with significant improvements in both DSC and NSD metrics. SAM optimization leads to smoother and more stable loss surfaces, as demonstrated in Figure 6. Without SAM, the 3D loss surface exhibits sharper and more rugged contours, particularly along the edges. In contrast, the model with SAM displays a flatter, more stable"}, {"title": null, "content": "loss landscape, indicating improved robustness and generalization.\nNotably, the UU-Mamba model achieves the highest DSC scores in 17 out of 24 anatomical regions and the highest NSD values in 13 out of 24 regions, as detailed in Table 5. These results underscore the superior generalizability and accuracy of the UU-Mamba model. The consistent top performance across most regions highlights the significant benefits of combining uncertainty-aware loss with SAM optimization to enhance segmentation outcomes.\nAs detailed in Table 4, SAM optimization pushes the ImageCAS dataset performance metrics to the highest levels observed in this study. In Figure 6, the 3D loss surface of the model without SAM shows greater variability, especially in the bottom right region. By contrast, with SAM optimization, the loss surface becomes much smoother, indicating improved model consistency, robustness, and generalization across diverse cardiovascular imaging scenarios.\nIncorporating Sharpness-Aware Minimization (SAM) optimization significantly improves performance across various cardiovascular imaging datasets. SAM promotes flatter minima in the training process, leading to better generalization and segmentation precision. In the ACDC dataset, SAM boosts the DSC to 92.787%, the highest among tested methods. For the Aorta and ImageCAS datasets, SAM enhances both DSC and NSD metrics, leading to smoother and more stable loss landscapes, reflecting improved model consistency and performance across diverse datasets."}, {"title": "4.4. Robustness Analysis", "content": "We perform experiments to evaluate each method on the Mean Squared Error (MSE) of the DSC scores to assess their robustness quantitatively. The MSE is calculated as shown in Eq. (10). Results are shown in the Tables 2, 3, and 4. These results show that the uncertainty-aware loss reduces the MSE compared to the standard CE loss, reflecting its ability to better address the variability and uncertainty in the data. The inclusion of SAM optimization significantly decreases the MSE, achieving the lowest error value. This reduction in MSE highlights SAM's role in minimizing errors and producing more accurate segmentation maps.\nFigures 3, 4, and 5 show the MSE between the output segmentation and the ground truth for each method, providing a visual comparison of the segmentation quality. These visualizations complement the quantitative results by illustrating the error distribution and highlighting areas where the"}, {"title": null, "content": "SAM optimization and uncertainty-aware loss contribute to more accurate and consistent segmentation outcomes."}, {"title": "5. Conclusion", "content": "We present a novel model, UU-Mamba, specifically developed for the purpose of segmenting cardiovascular MRI and CT data. This model combines the U-Mamba architecture with an uncertainty-aware loss function and the SAM optimizer, resulting in a substantial enhancement of biological picture segmentation. It achieves improved generalization and boundary accuracy. The uncertainty-aware loss function integrates region-based, distribution-based, and pixel-based losses to enhance segmentation performance by effectively managing jobs and prioritizing confident predictions. Simultaneously, the SAM optimizer directs the model towards flat minima in the loss landscape, improving its ability to withstand challenges and decreasing the likelihood of overfitting, ultimately resulting in more accurate segmentation. Aside from doing our main tests on the ACDC dataset [2], we also assessed the performance of UU-Mamba on two other datasets: ImageCAS [9] and Aorta [10, 11]. The model scored the greatest average DSC, NSD, and MSE on the ImageCAS dataset, demonstrating a considerable improvement compared to the baseline models. UU-Mamba demonstrated superior performance compared to other models on the Aorta dataset, earning the greatest average DSC in 17 out of 24 anatomical regions and the highest average NSD in 13 out of 24 anatomical regions. The data illustrate that the model is highly effective in different anatomical regions and segmentation tasks, highlighting its versatility and strength in numerous medical imaging situations. The comparative analysis conducted on five prominent models establishes the superiority of UU-Mamba. It achieves a DSC of 92.787% on the ACDC dataset, demonstrating high accuracy and robustness in segmenting various datasets, such as ImageCAS and Aorta.\nFuture work will involve examining supplementary data augmentation strategies, exploring other ways for modeling uncertainty, and validating the model on bigger and more varied datasets. Our main objective is to improve and expand the UU-Mamba technique in order to enhance automated medical imaging."}]}