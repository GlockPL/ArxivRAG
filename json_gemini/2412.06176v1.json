{"title": "AlphaVerus: Bootstrapping Formally Verified Code Generation through Self-Improving Translation and Treefinement", "authors": ["Pranjal Aggarwal", "Bryan Parno", "Sean Welleck"], "abstract": "Automated code generation with large language models has gained significant traction, but there remains no guarantee on the correctness of generated code. We aim to use formal verification to provide mathematical guarantees that the generated code is correct. However, generating formally verified code with LLMs is hindered by the scarcity of training data and the complexity of formal proofs. To tackle this challenge, we introduce AlphaVerus, a self-improving framework that bootstraps formally verified code generation by iteratively translating programs from a higher-resource language and leveraging feedback from a verifier. AlphaVerus operates in three phases: exploration of candidate translations, Treefinement a novel tree search algorithm for program refinement using verifier feedback, and filtering misaligned specifications and programs to prevent reward hacking. Through this iterative process, AlphaVerus enables a LLAMA-3.1-70B model to generate verified code without human intervention or model fine-tuning. AlphaVerus shows an ability to generate formally verified solutions for HumanEval and MBPP, laying the groundwork for truly trustworthy code-generation agents.", "sections": [{"title": "1. Introduction", "content": "There has been an enormous effort to train code-generating large language models (LLMs) (Chen et al., 2021; Austin et al., 2021; Li et al., 2023; Rozi\u00e8re et al., 2024; Team, 2024), leading to LLM-powered agents that can perform tasks ranging from fixing bugs in software repositories to solving Olympiad-level algorithmic problems (Jimenez et al., 2023; Li et al., 2022b). Despite these successes, multiple studies have identified disturbing mistakes in LLM-produced code, including subtle bugs and serious security vulnerabilities (Hendler, 2023; Pearce et al., 2021; Jesse et al., 2023; Zhong & Wang, 2023; Perry et al., 2023; Elgedawy et al., 2024). Ultimately these mistakes stem from a fundamental property of LLMs: language models can generate any string of code, without regard to correctness. As a result, automatically checking the correctness of LLM-generated code is one of the grand challenges facing the research community.\n\nThe generated code must be correct for all possible inputs it may receive. However, today's code generation methods select or filter generations with imperfect proxies of correctness, such as runtime testing or human inspection. Achieving perfect test coverage is typically infeasible (Li et al., 2022a; Liu et al., 2023), and incomplete coverage leads to an unreliable signal that can be exploited by a model (Pan et al., 2022; Liu et al., 2023; Denison et al., 2024). Relying on human review is equally problematic since it scales poorly and humans can struggle to tell whether LLM-generated code is correct (Perry et al., 2023). In turn, the difficulty of trusting generated code reduces the potential productivity gains from using LLMs and can lead to unexpected vulnerabilities or unreliable signals for improving models.\n\nIn contrast, generating code in a verification-aware programming language such as Dafny (Leino, 2010), F* (Swamy et al., 2016), or Verus (Lattuada et al., 2023) offers a promising approach to addressing these challenges by providing mathematical guarantees that a program obeys a specification for all possible inputs. In this paradigm, code is paired with a specification and proof written in a specialized language, and a mechanical verifier checks whether the code meets the specification. Doing so could dramatically improve the trustworthiness of the generated code: if the verifier passes, the LLM's generated program is mathematically guaranteed to meet the specification. However, writing formal specifications and proofs introduces additional layers of complexity. Furthermore, although LLMs have demonstrated success in automated theorem proving in mathematical domains (Lu et al., 2023; Li et al., 2024), their capability to generate verified code for even basic algorithms is limited (Sun et al., 2023; Lohn & Welleck, 2024).\n\nA significant barrier to automatically generating real-world,"}, {"title": "2. Formally Verified Code Generation", "content": "Our goal is to develop a model that generates formally verified code in a real-world programming language, which we refer to as formally verified code generation. Next, we provide background and then introduce AlphaVerus.\n\nFormal verification of code. Formal verification ensures that a program adheres to a formally defined specification of its intended behavior. As illustrated in Figure 2, formally verified code typically consists of three components: (1) formal specifications ys defining the expected input-output behavior; (2) a code implementation yr intended to satisfy the specifications; and (3) a proof yp demonstrating that the implementation conforms to the specifications. A verifier v(yS,YI,YP) \u2192 {0,1} uses the proof to statically check that the implementation meets the specification for all possible inputs, returning 1 when the program is correct with respect to ys and 0 when verification fails. Upon failure, the verifier additionally returns a set of messages {m\u2081,...,m\u043c} containing the number of verified statements, the number of errors, and localized error messages (e.g., see Figure 3).\n\nMisaligned specs and implementations. The specifications themselves are not verified, as they represent the developer's intended behavior. Therefore, it is critical that the specifications accurately reflect the desired input-output behavior for all possible inputs. We use the term misaligned to refer to situations in which the specification does not reflect the desired input-output behavior. This includes (i) a misalignment between the specification and the developer's intent, such as missing an edge case or allowing a trivial implementation, and (ii) a misalignment between the specification and an implementation, when the implementation passes the verifier but does not implement the functionality in the specification. The latter can occur due to language features in verification-aware languages that cause programs to pass the verifier (such as writing \"assume (false)\", which causes any program to pass).\n\nFormally verified code generation. Our goal is to develop a model that generates formally verified code given a specification. Specifically,\n\n(YI,YP) ~ G (ys; c, 0), (1)\n\nwhere G() is a generation algorithm such as sampling from a language model with parameters 0, and the model generates both an implementation y\u0131 and proofs yp given a specification ys and any additional context c. The goal is for the resulting code to verify, i.e., v(ys, YI, YP) = 1.\n\nBootstrapping formally verified code generation. A practical goal is to perform formally verified code generation in a mainstream language, such as Rust code verified with the Verus verifier (Lattuada et al., 2023). However, doing so raises a technical challenge: it is infeasible to train a model on (ys, YI, YP) examples since such examples do not exist. We refer to this as a bootstrapping problem, since we need to create an initial generation model (that we may subsequently improve) without any training data. Next, we describe AlphaVerus, a framework for bootstrapping a verified code generation model by translating from a more resource-rich language."}, {"title": "3. AlphaVerus for Bootstrapping Formally Verified Code Generation", "content": "To enable verified code generation in the absence of training data in our target language (Verus), we propose to iteratively translate programs from a higher-resource domain into Verus. Each iteration collects data by exploring candidate translations, refining them with a novel tree search, then filtering out misaligned programs. Finally, we use the data to enable a verified code generation model (via few-shot learning), and evaluate the model plus the tree search on the downstream task of verified code generation: generating verified code and proofs given a held-out test specification.\n\nAlphaVerus translates programs using a three-stage pipeline consisting of exploration, refinement, and critique. The exploration stage translates source programs into candidate Verus programs. The refinement stage repairs the programs using a novel tree search over program refinements. The critique stage uses a suite of models to discard flawed specifications and implementations that could degrade future iterations. The pipeline iterates, creating a self-reinforcing cycle where verified programs and refinement trajectories improve the models' capabilities, enabling translation of increasingly complex programs. The result is a growing synthetic dataset of progressively more complex and reliable Verus programs. The complete algorithm is listed in Algorithm 1 and visualized in Figure 1.\n\nExploration. Given a source program x (e.g., a Dafny implementation, specification, and proofs), exploration uses a model to generate candidate target (i.e., Verus) programs:\n\n{Y1,..., Yk} ~ Gexplore (x; y), (2)"}, {"title": "3.1. Translation", "content": "where G is a generation algorithm (e.g., LLM sampling) that is given the source and a set of (source, target) examples Dy. Initially, Dy has a few hand-written examples.\n\nAny generated (source, verified program) pairs are placed in a candidate set, C, that will be passed to the filtering stage. If no candidates verify for source x, candidates that are syntactically correct proceed to refinement. Intuitively, this stage serves as initial \u201cexploration\", in that it generates a set of candidates that may eventually be refined and filtered into verified programs in the later stages. Unlike other methods of bootstrapping (Zelikman et al., 2022; Lin et al., 2024) that discard anything but correct solutions, we use both syntactically correct programs and fully verified programs for further improvement, expanding the learning signal.\n\nRefinement with Treefinement. Having a verifier opens the possibility of refining candidate programs into verified ones by providing detailed feedback, including unverified functions and specific errors like overflows, unsatisfied conditions, and syntactic mistakes (e.g., Figure 3). While human programmers often use such feedback for iterative corrections, naively providing LLMs with incorrect solutions and feedback often fails to produce improvements. Our key insight is that verifier feedback induces an implicit ordering of solutions based on verified functions and error severity. This ordering lets us extend common refinement techniques by framing refinement as a tree search over the space of refined programs, which we call Treefinement.\n\nSpecifically, the refinement stage takes syntactically correct but unverified candidate translations {y1,..., Yk' } and performs a tree search to discover verified programs. Each node in the tree contains an imperfect program and its associated errors, (y, e(y)). Nodes are expanded by invoking a refinement model:\n\n{Y1,..., Yk} ~ Grefine (y, e(y); Dy'), (3)\n\nwhere Dy is a set of (program, error, correct program) examples, initially containing a few hand-written examples.\n\nGiven a node scoring function v(y) \u2192 R that is used to prioritize nodes, we can search over the space of program refinements with a tree search algorithm that selects and expands nodes, such as breadth-first or depth-first search.\n\nWe develop a symbolic scoring function based on the number of (un)verified functions, errors, and warnings:\n\ns(y) =\n\nnver(y) \u2013 \u03b1nerr(y) \u2013 \u03b2nwarn(y)\nnver(y) + nunver(y)\n\nwhere nver(y) is the number of verified functions in y, nerr(y) and nwarn(y) are the counts of errors and warnings from the verifier for the node's program y. \u03b1 and \u03b2 are hyperparameters controlling the penalties for errors and warnings, respectively. Figure 3 shows example verifier feedback, with nver(y) = 2, nunver(y) = 1, nerr(y) = 1, and nwarn(y) = 0, and its score is computed as (2 \u2013 \u03b1\u00b7 1 \u2013 \u03b2.0)/(2 + 1). Intuitively, programs that are closer to a verified program have higher scores, with proximity determined by the proportion of verified functions, resolved errors, and resolved warnings. Upon generating a verified program, the program's search trajectory is added to a candidate set C7, and the new (source, program) pair to the candidate set C that is passed to the critique stage.\n\nTreefinement extends two kinds of prior methods into a new search over program refinements. First, refining LLM outputs is a common technique (Madaan et al., 2023; Kamoi et al., 2024), but not within a tree search. On the other hand, tree search developed in step-by-step mathematical problem solving involves appending solution steps rather than refining a full program (Wu et al., 2024a). Our approach specifically addresses the non-local nature of error fixes.\n\nAlthough Treefinement can use any tree search algorithm, we use REBASE (REward Balanced SEarch) (Wu et al., 2024b). REBASE allocates an exploration budget by sampling nodes from a distribution determined by the node scores at the current depth, providing an effective balance of exploration and exploitation. The search continues until it finds a verified program or reaches a maximum depth.\n\nCritique. Synthesized specifications are the one part of the translation pipeline that lacks formal guarantees, which can result in a mismatch between the intended and actual functionality of generated programs. Furthermore, in a few degenerate cases, there can be a mismatch between the specification's intent and the program's implementation, since"}, {"title": "3.2. Downstream Evaluation", "content": "After generating high-quality synthetic data in the form of formally verified Verus programs and error-feedback-correction triples, we use the data to enable a model that performs formally verified code generation.\n\nWe adopt a two-stage approach comprising exploration and Treefinement. In the exploration stage, given a specification ys, we generate k candidate programs {y(1), ...,y(k)}. If any candidate passes verification, we consider the task solved. If none succeed, we initialize Treefinement with the candidates and run it until we obtain a fully verified solution or reach a maximum number of iterations. Conceptually, this can be viewed as a meta-generator that uses the collected data as a source of few-shot exemplars,\n\n(YI, YP) ~ G(ys; Dy, Dy\u2192y'), (7)\n\nwhich means generating an implementation and proofs using a language model prompted with a subset of the collected verified programs Dy and a test specification ys, followed by Treefinement with the collected refinement examples Dy\u2192y'. In practice, we randomly select a subset of exemplars before each call to the generator.\n\nThis generator is then evaluated using the verification success on a benchmark of held-out specifications, i.e.\n\npass@K(G, 7, {y...y}) = \n\n1\nN\nN\ni=1\nv(ys, G(ys)), (8)\n\nwhere K is the total number of sequences generated, and here v(,) returns 1 if any of the generated programs pass.\n\nUnlike prior work that requires LLMs to fill proof annotations in existing code and specifications (Loughridge et al., 2024; Yang et al., 2024; Chen et al., 2024), we evaluate our models on this challenging task of generating both the code and the corresponding proofs given only the specifications. This task is significantly more complex, as the LLM must structure the code to facilitate the completion of the proof."}, {"title": "4. Experimental Setup", "content": "Generators. We use LLAMA-3.1-70B for translation experiments and additionally evaluate LLAMA-3.1-8B, Qwen-32B, and GPT-40 for downstream tasks. The exploration phase uses k = 256 samples, while tree search uses breadth 32 and maximum depth 8. The exploit model generates 32 responses per specification. All generators use nucleus sampling with temperature 0.7.\n\nTranslation. We use DafnyBench as our source domain Dsre for our translation experiments. Starting with"}, {"title": "5. Results and Analysis", "content": "AlphaVerus translation success monotonically increases. Figure 4 shows the number of successful translations over each iteration. We see a steady increase in the number of translations as the iterations increase. The results indicate that AlphaVerus learns to translate and generate more complex programs over iterations. Altogether,\n\nAlphaVerus translates around 45% of DafnyBench into Verus programs that are verified by Verus and aligned according to the critique models. Listings 2, 3, and 4 in the Appendix show example translations.\n\nThe generated exemplars during the translation process are collected into our DAFNY2VERUS-COLLECTION, comprising 247 translated programs, 102 error trajectories, and 579 exploit pairs. These exemplars are used for downstream tasks.\n\nAlphaVerus enables verified code generation. shows the verified code generation performance for the AlphaVerus model obtained from the final translation iteration. AlphaVerus leads to a substantial increase over its underlying Llama 3.1 70B model and a prompted GPT-\n\nTreefinement leads to a jump in performance. We evaluate the effectiveness of tree search compared to further scaling the parallel sampling (exploration) budget without refinement. Figure 5 shows the percentage of solved problems versus the generation budget for both approaches. Treefinement leads to a substantial jump in performance over exploration. Notably, exploration plateaus while tree search continues improving as the generation budget is increased.\n\nCritique is crucial for preventing reward hacking. Next, we analyze the quality of translations without the critique phase. Figure 7 shows the effect of removing the critique models and continuing the self-improvement process on 100 examples from DafnyBench. The plot shows that without the critique phase, the model is able to translate a large fraction of programs, but it is primarily because of learning to use assume (false) which renders any implementation trivially verified. This is primarily used by human developers to debug their proofs; however, here AlphaVerus figures out how to game the system by generating trivial proofs.\n\nThere is also a snowballing effect, where while initially the model just generates a single program with assume (false), it soon learns to use it in all programs. This is evident from the leveling off of correct translations in the figure. While an obvious way is to disallow such statements (as done by our rule-based verifier), we see even more complicated cases of reward hacking, such as leaving small gaps in translated specifications or even generating degenerate translations, as illustrated in Figure 8. We conclude that the critique phase is critical for filtering out misaligned programs and preventing reward hacking.\n\nTreefinement outperforms linear refinement. We compare Treefinement against standard refinement that refines linearly, either by performing one step of refinement across multiple parallel branches, or performing several steps of refinement across branches. Using equivalent generation budgets, we adjust the breadth and depth parameters accordingly. We also evaluate the best-first search as a baseline. As seen in Table 2, all methods improve upon initial exploration, demonstrating Treefinement's compatibility with various search algorithms, and tree-search based refinement outperforms linear refinement. For the tree search, using REBASE outperforms the best-first search. Also note that the linear refinement variants are special cases of REBASE (depth = 1 with large breadth, and temperature = \u221e)."}, {"title": "6. Related Work", "content": "Automated Formal Verification. Automated formal verification has a long-standing history in interactive theorem provers (Redmon & Sanchez-Stern, 2016; Kaliszyk et al., 2018; Polu & Sutskever, 2020; First et al., 2020; Lu et al., 2023; Li et al., 2024), such as Coq (Coq Development Team, 2020), Lean (Lean FRO), and Isabelle (Isabelle). These approaches typically generate step-by-step proof statements for a given problem, with the theorem prover providing feedback on intermediate steps. While these methods have achieved significant success in proving complex mathematical theorems, their application to formal verification of code is typically limited to theorems from existing projects (e.g., First et al. (2023)) or simple program properties (Lohn & Welleck, 2024) rather than end-to-end verified code generation. An alternative paradigm integrates language models with languages that offload proving to automated reasoning tools (e.g., SMT), including Dafny (Leino, 2010; Sun et al., 2023; Loughridge et al., 2024) and F* (Swamy et al., 2016; Chakraborty et al., 2024). However, enabling verified code generation in these research languages may have limited applicability to real-world software and workflows.\n\nAutomated Formal Verification in Rust. In contrast, Verus (Lattuada et al., 2023) offers a verification framework for Rust, a widely adopted programming language. However, unlike in formal theorem proving or long-standing verification languages, there is a substantial lack of data for Verus. Two existing works, released during the development of Alpha Verus, attempt to overcome data scarcity. First, AutoVerus (Yang et al., 2024) prompts GPT-4 with a pipeline of hand-engineered prompts tailored to specific errors and programs. This allows for refining some errors but requires human expertise to support new strategies through additional prompts. In contrast, our Treefinement method learns new refinement strategies automatically. Second, the concurrent work SAFE++ (Chen et al., 2024) proposes translating an existing Rust dataset to Verus and training generation and refinement models on the collected data. However, the translation process in Chen et al. (2024) was initialized with over a month of continuous generation from GPT-4. In contrast, AlphaVerus relies only on a single openly available model, without an expensive GPT-4 initialization. AlphaVerus also incorporates a new tree-search refinement strategy that outperforms the linear strategy used in SAFE++, and a critique phase to ensure the generated specifications are high quality. These innovations contribute to better results, despite our method using open models and 100 times less data. Finally, these two existing works study the simplified task of proof generation, while we study the more general setting of verified code generation: generating the implementation and its proofs.\n\nInference-Time Strategies. Recent studies have shown that increasing inference-time compute can improve performance in reasoning, mathematics, and code generation via meta-generation strategies (Welleck et al., 2024) such as parallel sampling (Wang et al., 2022; Aggarwal et al., 2023; Sun et al., 2024), tree search (Yao et al., 2024; Wu et al., 2024a), and refinement (Welleck et al., 2023; Madaan et al., 2023; Snell et al., 2024). Our Treefinement algorithm can be viewed as a hybrid meta-generator that combines tree search and refinement, following initial parallel sampling (exploration). A variety of tree search methods generate one step of a mathematical solution at a time, with a verifier guiding the search process by assigning a score to the current state (Wu et al., 2024a). In contrast, Treefinement uses verifier feedback on the complete solution, modeling tree nodes as full programs and edges as refinement steps. Our strategy addresses the non-local nature of error fixes, and does not need an additional trained scoring model.\n\nVarious refinement strategies use external feedback from knowledge bases (Peng et al., 2023; Chern et al., 2023), code interpreters (Chen et al., 2023; Zhang et al., 2023), tool outputs (Gou et al., 2024; Schick et al., 2023), or separately trained reward models (Aky\u00fcrek et al., 2023). Our Treefinement algorithm uses a diverse set of feedback sources, including scalar and binary values, language feedback, and an exploit model. Moreover, whereas prior methods typically operate in a linear fashion-i.e., starting with an output and repeatedly refining it-our approach structures refinement as a tree search. This allows for prioritizing certain branches of refinement, which we find perform better.\n\nSelf-Improvement in LLMs. Various algorithms aim to improve a language model using data generated by the model along with an external feedback source (Zelikman et al., 2022; Wang et al., 2024; Hosseini et al., 2024), which is colloquially termed self-improvement. Common approaches rely on variants of expert iteration or rejection finetuning (Polu et al., 2022; Zelikman et al., 2022; Yuan et al., 2023; Lin et al., 2024), where multiple solutions are sampled, and an external signal selects the positive ones for model fine-tuning. Our approach, AlphaVerus, builds upon these concepts but moves beyond the simple sample-and-filter strategy. Our method additionally uses refinement and tree search to collect data, and the data is collected using multiple modules (e.g., outputs from Treefinement may be used to improve exploration). Additionally, AlphaVerus uses various forms of feedback-such as ternary, scalar, language, and verifier outputs-rather than just binary filtering. Conceptually, we can view AlphaVerus as a meta-generation algorithm (i.e., a combination of parallel sampling, refinement, and tree search) that improves over time, rather than a model trained on filtered outputs."}, {"title": "7. Conclusion", "content": "We introduced AlphaVerus, a novel self-improving framework for generating formally verified code in mainstream programming languages. By leveraging iterative translation from a higher-resource language (Dafny) to Verus and utilizing verifier feedback through our Exploration, Treefinement, and Critique stages, AlphaVerus overcomes the challenges of scarce training data and the complexity of formal proofs. We also address the issue of reward hacking, where models learn to exploit loopholes in the verification process to produce trivial or misaligned solutions. By incorporating a critique module that filters out such misaligned programs, we prevent the model from gaming the system an issue akin to reward hacking observed in reinforcement learning agents. We hope that the methods proposed in our work, such as the critique models, may evolve to handle more complex cases of reward hacking with LLMs. Our approach operates without human intervention, hand-engineered prompts, or extensive computational resources, yet achieves significant performance improvements on verified versions of the HumanEval and MBPP benchmarks where prior methods fail. We also contribute a new dataset of formally verified Verus programs, providing valuable resources for future research.\n\nAlphaVerus advances the state-of-the-art in formally verified code generation and establishes a scalable pathway for improving LLMs' capabilities in generating correct code. Intuitively, AlphaVerus distills inference-time computation into a meta-generator that improves over time, showing the potential for inference-time scaling in verified settings."}]}