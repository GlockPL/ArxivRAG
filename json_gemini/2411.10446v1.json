{"title": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning", "authors": ["Daniel Ekpo", "Mara Levy", "Saksham Suri", "Chuong Huynh", "Abhinav Shrivastava"], "abstract": "Recent advancements in vision-language models (VLMs) offer potential for robot task planning, but challenges remain due to VLMs' tendency to generate incorrect action sequences. To address these limitations, we propose VeriGraph, a novel framework that integrates VLMs for robotic planning while verifying action feasibility. VeriGraph employs scene graphs as an intermediate representation, capturing key objects and spatial relationships to improve plan verification and refinement. The system generates a scene graph from input images and uses it to iteratively check and correct action sequences generated by an LLM-based task planner, ensuring constraints are respected and actions are executable. Our approach significantly enhances task completion rates across diverse manipulation scenarios, outperforming baseline methods by 58% for language-based tasks and 30% for image-based tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "For robots to be able to solve complex manipulation problems in the real world, they need to understand the physical world around them, including object locations and relationships between objects in the scene. Humans intuitively understand spatial relationships between objects in the world and can use this understanding to develop efficient and executable plans to complete tasks. Consider the example of organizing a cluttered room. Humans can quickly understand which objects are out of place based on their understanding of how objects are supposed to relate to each other. For example, it seems intuitive that a book should be on a shelf, not on a cup. Robots struggle to perceive the world around them the way humans do.\nAdditionally, physical constraints in the real world restrict the order in which actions can be executed. For example, if a glass cup is on a book, which is on a desk, the robot must pick up the cup first and place it on the desk before picking up the book. Because of these constraints, robots need to understand the relationships between objects in the scene. If the robot does not understand that the cup is on the book, it might not factor that into its planning and may try to pick up the book first, which could result in the cup falling and breaking.\nRecent advances in large language models (LLMs) and vision-language models (VLMs) have opened up new pos- sibilities for robot task planning [1, 2]. These models demonstrate impressive reasoning capabilities and world knowledge. Prior work [3-5] used LLMs to generate Planning Domain Definition Language (PDDL), which can be used by classical planners to create a task plan. While the results have been promising, PDDL is inherently restrictive and does not generalize well [6, 7]. Other lines of work use VLMs"}, {"title": "II. RELATED WORKS", "content": "Scene graphs have been used in computer vision for symbolic representation. Scene graphs can represent object relationships in images [16]. These representations have been used for different tasks like image generation [17- 22], image/video captioning [23-25], and visual question answering [26-28]. Scene graphs can encode useful scene information without getting affected by pixel-level noise. This has increased the use of scene graphs in robotics. [29] propose a graph representation of the robot's environment for navigation by representing nodes as semantic places and edges as navigational behaviors. [30] use 3D scene graphs to train a reinforcement learning algorithm policy for navigation. They maintain a graph that encodes information about the observed scene and use a graph neural network (GNN) to encode graph features which are passed to the policy to predict a distribution over the action space. GRID [31] uses an existing scene graph generator to get an initial graph which is passed to a VLM encoder that acts as an 'instructor'. A separate robot graph is passed to the encoder which represents the robot's state. These are then combined through GNNS to predict actions. [32] generate a scene graph using an object detector and use the scene graph for cluttered scene exploration for question answering and planning. SARP [13] uses multi-view images to generate scene graphs for robot planning using a partially observable Markov decision process solver. [11] uses an object detector and pose estimator to create geometric and symbolic graphs and uses a graph neural network and symbolic reasoning for motion planning. SG- Bot [33] uses scene graphs to imagine the goal scene for a reconstruction task. [34] uses contact graphs and Graph Edit Distance for planning. Our work is similar to theirs in how we model actions as graph operations. However, we use an LLM as the task planner, which gives us much more flexibility and allows us to use natural language instructions to describe the tasks. An LLM planner also provides better generalization over multiple tasks, objects, and actions."}, {"title": "A. Scene Graph in Planning", "content": "Scene graphs have been used in computer vision for symbolic representation. Scene graphs can represent object relationships in images [16]. These representations have been used for different tasks like image generation [17- 22], image/video captioning [23-25], and visual question answering [26-28]. Scene graphs can encode useful scene information without getting affected by pixel-level noise. This has increased the use of scene graphs in robotics. [29] propose a graph representation of the robot's environment for navigation by representing nodes as semantic places and edges as navigational behaviors. [30] use 3D scene graphs to train a reinforcement learning algorithm policy for navigation. They maintain a graph that encodes information about the observed scene and use a graph neural network (GNN) to encode graph features which are passed to the policy to predict a distribution over the action space. GRID [31] uses an existing scene graph generator to get an initial graph which is passed to a VLM encoder that acts as an 'instructor'. A separate robot graph is passed to the encoder which represents the robot's state. These are then combined through GNNS to predict actions. [32] generate a scene graph using an object detector and use the scene graph for cluttered scene exploration for question answering and planning."}, {"title": "B. Planning with LLMs/VLMs", "content": "LLMs [35-37] and VLMs [38-40] have emerged as strong agents for open world reasoning and have shown good understanding of the real world. Due to their good reasoning and understanding of object relations, actions, and context have also been applied to robotics as agents to help solve tasks, especially planning.\nOn the side of LLMs, multiple works have explored them as planners [41, 42]. LLMs have also been used for embodied agents to generate plans that can be executed [8, 43, 44]. While [43] is similar to our work in the sense that we provide language feedback to the model, our work differs in the sense that our original input to the model is a scene graph, and we continue to use only the scene graph as feedback to the plan- ner, reducing the amount of data and computation required. SayCan [8] demonstrate good results using their approach. However, using a learned value function to verify the output requires training the value function and can require a lot of data and computation. ConceptGraphs [12] fuse the output of 2D foundation models to generate a 3D scene graph and use an LLM to generate plans based on the 3D scene graph.\nRecent VLM developments have made them an even better candidate for robotics since they can take visual data and raw pixels as input, unlike LLMs, which can only work with"}, {"title": "C. Execution-Verification", "content": "While correctness is important when designing a task execution plan, it is also essential for it to be plausible. SayPlan [14] assumes an existing 3D scene graph, which they use to interact with the LLM. They use a graph simulator to verify the LLM-generated task plans and show that their approach works for multi-room setups. CoPAL [47] proposes corrective planning by using different layers of encapsulation. Some works [48] modify an existing reinforcement learning algorithm to condition on natural language feedback from the environment. They automatically generate the language feedback based on the current goal and the agent's current actions. This is similar to our approach of providing feedback to the planner based on the current graph state and predicted action. REFLECT [49] introduces a framework to query the LLM planner to reason about failures based on the hierar- chical summary of the robot's past experiences generated from multisensory observations. They show that the failure explanation can help the LLM correct the failure and complete the task. Voyager [50] introduces an LLM learner that can learn executable skills as it interacts with the environment. It writes code to interact with the environment and correct itself with feedback received from the environment. ViLa [9] uses execution to verify the plan by feeding the current state of the environment to the model at every step using visual inputs.\nOur approach, on the other hand, can generate execution- verifiable plans by relying on the current scene graph for constraint checking. This makes verifying the affordances and plausibility of an action is especially quick and efficient."}, {"title": "III. OUR APPROACH: VERIGRAPH", "content": "VeriGraph takes in an image depicting an initial scene, along with either a target image portraying the desired goal scene state or instructions detailing modifications to the initial scene. It generates the initial and goal scene graphs and uses them to predict actions to transform the initial scene into the target scene. The scene graph generation method is discussed in Section III-A, and action constraints are discussed in Section III-B. Given a pair of scene graphs, the planner, discussed in Section III-C and Section III-D, generates a high-level plan instructing a robot to transform the initial scene into the goal scene."}, {"title": "A. Scene Graph Generation", "content": "Given an image I of a scene, the goal of scene graph generation is to create a graph that accurately represents the scene's structure. The scene graph comprises a set of vertices V, representing objects in the scene, and a set of edges E, describing the relationships between objects in V. R represents a set of possible relations between objects in the scene. An edge $e_{uv} \\in E$ between two vertices u and v in the scene is then defined as $e_{uv} = {u, v,r}$ where $r \\in R$ and $u, v \\in V$. The scene graph for image I is thus represented as $G = {V, E}$.\nIn VeriGraph, the set of relations R includes basic spatial relations such as {in, on}. However, this set is flexible and easily adapted for other tasks. To address the issue of varying object names (e.g., tabletop, table, and countertop for the same object), we maintain a global dictionary of unique object names D for scene graph generation. This dictionary encompasses all objects that could be present in any scene. The scene graph generator SGG takes the image I, the global dictionary D, the set of relations R and the task description T and returns a scene graph. We define the graph generator as\n$SGG(I, R, D,T) \\rightarrow G = {V, E}$    (1)\nWhen generating scene graphs, T is set to null except for the target scene graph for tasks where the target scene is described using natural language, in which case the task instruction/goal scene description T is used in the graph generation process. VeriGraph uses SGG to generate the initial and goal scene graphs. An example scene graph is shown in Figure 3."}, {"title": "B. Constraint Validation", "content": "Every action has a set of preconditions that must be met before execution. For instance, before performing the \"move\" action on a plate, any items on the plate must first be removed. Additionally, post-conditions must be satisfied for the action to be considered successful. In the example given, the plate must end up on the new supporting object. In VeriGraph, these conditions are represented as a set of constraints C.\nVeriGraph uses the current graph to validate constraints. The vertex v associated with the action must exist in the graph, and its in/out edges must satisfy specific conditions. For instance, if v is being moved, VeriGraph checks if v supports any other objects by ensuring no edges from v to any other nodes exist.\nAfter constraints are validated, VeriGraph updates the current graph state to reflect the execution of the action. The specific changes to the graph depend on the action taken. For a \"move\" operation, the edge representing the initial support relationship is removed, and a new edge is created for the new support relationship. The next action in the sequence is then validated, and the graph is modified accordingly. The final graph's nodes and edges are compared against the goal scene graph. If they match, the plan is considered successful."}, {"title": "C. Task Planning", "content": "Given the initial scene graph $G_{init}$ and the target scene graph $G_{tgt}$, the task planner P generates actions that can be executed on the initial scene to transform it into the target graph. Let A be the set of all high-level actions that a robot can perform, the sequence of actions $a = {a_1,a_2,\\ldots,a_n}$ such that $a \\in A$ predicted by the planner P must complete the given task while adhering to the constraints C. We define the task planner as\n$a = P(G_{init}, G_{tgt}, C, A)$.     (2)\nAn example of such a plan can be seen in Figure 4."}, {"title": "D. Iterative Planning", "content": "The planner described in Section III-C outputs the full action sequence without any feedback mechanism to refine the plan. Our experiments showed that this approach often fails in difficult tasks because LLMs tend to forget constraints. To address this issue, we designed an iterative planner $P_{iter}$ that receives feedback F about the proposed action sequences and corrects the plan accordingly.\nThe planner, $P_{iter}$, is given A, C, $G_{init}$, and $G_{tgt}$ and asked to output at most k high-level actions and an end token. VeriGraph attempts to perform the actions, and VeriGraph returns feedback, F, as well as the current graph state. If there is a constraint violation in the proposed actions, the error count is increased. The feedback and new graph state are then passed to $P_{iter}$, and the iteration continues until either the number of errors reaches the error threshold t or the end token is received. The iterative planning process is described in Algorithm 1."}, {"title": "IV. EXPERIMENTAL DETAILS", "content": "In this section, we outline the details of our experiments. The evaluation dataset is introduced in Section IV-A, followed by task description in Section IV-B. All baselines are mentioned in Section IV-C and finally Section IV-D discusses the results."}, {"title": "A. Dataset", "content": "We design three scenes-kitchen, tabletop, and block scenes to evaluate VeriGraph's performance and compare against baseline methods. Each scene has multiple configurations with varying numbers of objects and placements. We vary the number of objects between three and seven. Ground truth scene graphs for each scene were created using GPT-4V(ision) (GPT-4V) [51], and corrections were made manually when necessary. Figure 5 shows some example images from the dataset."}, {"title": "B. Tasks", "content": "We created three task groups - rearrange, language in- struction, and stacking - with varying difficulty levels. Each task has a ground truth goal scene graph for evaluating the planner. The predicted actions from the planner are executed on the initial scene graph, and the transformed graph is then compared against the ground truth goal scene graph. The different task groups are described below.\nStacking Task. We use block scenes of different configura- tions for the stacking task. The model is given an image of the initial scene and asked to stack all the blocks into one stack. The final order is arbitrary here, so there's no ground truth scene graph. Instead, the final scene graph is checked to ensure a single pile of blocks. Some block scenes already have multiple incomplete stacks, so the planner must unstack the other stacks and complete one.\nLanguage Instruction Task. This consists of an initial scene and a language instruction. The instruction is either direct commands, e.g., \"Move pan to the stovetop,\" or a description of the desired goal state of the scene, e.g., \"I need the positions of the pan and pot swapped.\" The model is asked to predict actions to execute the given instruction on the scene.\nReference Image Instruction Task. The model is given an initial scene and a structurally similar scene as the goal state and asked to predict a sequence of actions to transform the initial scene into the goal scene. The scene graph of the goal scene is used as the goal scene graph."}, {"title": "C. Baselines", "content": "We compare VeriGraph against the following baselines:\n(a) ViLa [9]. prompts the vision-language model (VLM) with an image and a language instruction or another image as the goal state. For a fair comparison, we use a prompt identical to ours and remove all references to scene graphs. We execute the proposed actions on the ground truth initial scene graph and compare the final graph against the ground truth graph. (b) SayCan [8] prompts the LLM with a textual representation of the scene. The text contains a list of all the objects in the scene. We implemented a similar setup using GPT-4 as the LLM. Since SayCan cannot understand the spatial relationships between objects in the scene because it only receives a list of all objects, we only evaluate it on image-language tasks. The objects from vertices in the ground truth scene graph are the scene observation for this baseline."}, {"title": "D. Experiments and Results", "content": "Scene graph generation: As mentioned earlier, VLMs are utilized in our scene graph generation model. Here, we evaluate some popular VLMs, such as Gemini 1.5 Pro [52], Mistral [53], and GPT-4V [51], without any in-context examples. For GPT-4V and Gemini 1.5 Pro, we use the official Python SDK. Ollama [54] is used to run Mistral locally. The same prompt is used for all three models. For our experiments, the set of relations R and global dictionary D are predefined and included in the prompt. We evaluate accuracy via an F1-score for the generated nodes and edges compared to the ground truth scene graph. Full accuracy is based on a perfect match of both nodes and edges. The three models are compared across the scenes in Table I. GPT-4V performs notably better than Gemini and Mistral across all scenes. We used GPT-4V as the scene graph generator SGG for other experiments based on these results.\nPlanning: We used GPT-4 [38] as the task planner P. We evaluate our approach on all three task groups presented in Section IV-B and show results for our approach in Table II. Compared to ViLa, we improve on language and image instruction planning tasks. For the image-based start and goal state, we see an average improvement of ~0.57. For the language instruction task, we outperform SayCan by ~0.56. This improvement is because of the efficient representation of the scene as a scene graph. Scene graph representations help planning by structuring the setup so the LLM does not need to interpret actions precisely from the image. Additionally, they allow for iterative correction during the actual planning stage. Most of the failure cases in our approach are caused by inaccurate scene graphs. This is addressable because as scene graph generation methods improve, our planning will improve with them. To test how much scene graphs affect planning, we passed the ground truth scene graphs to the planner and observed that the iterative planner proposed successful plans ~100% of the time.\nOur experiment results in Table II show that the iterative planner (last column) achieves higher accuracy than the baselines and our non-iterative planner. This improvement is expected since the iterative planner receives feedback about the plan and replans accordingly."}, {"title": "E. Ablation Study", "content": "Iterative vs. non-iterative planner. Our experiment results in Table II show that the iterative planner ('Ours') achieves higher accuracy than the non-iterative planner ('Ours (Direct)') for most tasks. This improvement is expected since the iterative planner receives feedback about the plan and can modify it accordingly. We are able to utilize such a correction and iterative planning model due to our scene graph representation.\nError thresholds. We observed that setting the error threshold to 2 resulted in the worst performance. While setting it to 10 yielded the best performance, it was more time-consuming. We found that setting the threshold to 5 provided a good balance between accuracy and speed.\nNumber of actions per iteration. We tested 1, 2, 5, and 10 actions per iteration. Although there was no significant difference in accuracy, we ultimately chose to use 3 actions per iteration for optimal performance."}, {"title": "V. CONCLUSION", "content": "In this paper, we present VeriGraph, a novel approach for generating high-level task plans for robot object manipulation using scene graphs. We demonstrate that scene graphs provide an efficient representation of scene information and show that our method outperforms methods that rely on raw pixel values for planning. We also introduced an iterative approach where the task planner's proposed actions are evaluated and corrected before execution on actual objects, enhancing the robustness of the planning process. Additionally, our method offers an efficient way to evaluate high-level robot task-planning algorithms. As scene graph generation algorithms continue to improve in accuracy, the effectiveness of our approach will correspondingly increase. This highlights the potential for further advancements in high-level robot task planning using scene graphs.\nWe note that current scene graph generation methods suffer from common problems in occlusion and poor performance in out-of-distribution settings. GPT-4v performed well for some scenes however, it failed to generate accurate scene graphs for some scenes. Further research is needed to improve the accuracy of scene graph generators. While this work has demonstrated promise in using scene graphs for planning, in the future, we want to investigate better ways to generate reliable scene graphs for more complex scenes."}]}