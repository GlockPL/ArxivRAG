{"title": "Derail Yourself: MULTI-Turn LLM JAILBREAK AT- TACK THROUGH SELF-DISCOVERED CLUES", "authors": ["Qibing Ren", "Hao Li", "Dongrui Liu", "Zhanxu Xie", "Xiaoya Lu", "Yu Qiao", "Lei Sha", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "abstract": "This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries. We introduce ActorAttack, a novel multi-turn attack method inspired by actor-network theory, which models a network of semantically linked actors as attack clues to generate diverse and effective attack paths toward harmful targets. ActorAttack addresses two main challenges in multi-turn attacks: (1) concealing harmful intents by creating an innocuous conversation topic about the actor, and (2) uncovering diverse attack paths towards the same harmful target by leveraging LLMs' knowledge to specify the correlated actors as various attack clues. In this way, ActorAttack outperforms existing single-turn and multi-turn attack methods across advanced aligned LLMs, even for GPT-01. We will publish a dataset called SafeMTData, which includes multi-turn adversarial prompts and safety alignment data, generated by ActorAttack. We demonstrate that models safety-tuned using our safety dataset are more robust to multi-turn attacks.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities, but they can be mis- used for both benefit and harm, like social engineering, developing biological weapons, and cy- berattacks (Bommasani et al., 2021; Weidinger et al., 2022). To thoroughly investigate the safety vulnerabilities of LLMs, it is critical to discover diverse attack ways that can elicit harmful or in-appropriate responses. Current attack methods are mainly single-turn based, which elicit harmful responses from the victim LLM within one turn of the conversation (Wei et al., 2024; Chao et al., 2023; Zeng et al., 2024; Zou et al., 2023b). However, in real-world scenarios, interactions between users and LLMs typically unfold over multiple conversational turns (ShareGPT, 2023).\nIdentifying and dealing with the potential harms and misuse risks in multi-turn interactions is an open research question. Unlike single-turn attacks, where the malicious intent is clear in the prompt, multi-turn interactions enable the malicious users to hide their intentions. For example, as shown in Fig. 1 (a), the user starts with a neutral query like \u201cWho is Ted Kaczynski?\" (a terrorist who has bomb-making activities). In each follow-up question, the user induces the victim model to provide more harmful details based on its previous response. Although all the follow-up questions are still innocuous, the user finally obtains the knowledge of bomb-making.\nThe above example reveals the safety risks in multi-turn conversations, while there are two main challenges in designing such attacks. First, attackers need to hide harmful intent to avoid detection. Second, multi-turn conversations give attackers more opportunities to act, allowing multiple possible attack paths for the same target. The challenge is how to discover these paths to reveal additional safety vulnerabilities. To resolve the first challenge, as shown in Fig. 1 (c), Crescendo implements its attack by gradually guiding benign initial queries towards more harmful topics, based on the fixed and human-crafted seed instances (Russinovich et al., 2024). The performance of Crescendo depends on the quality and relevance of the seed instances with the test cases. If the test cases differ from the seed examples, Crescendo may not generate effective attacks well. Moreover, Crescendo generates different attack paths via random trials, but these paths tend to be biased toward the seed instances and lack diversity, thus not effectively addressing the second challenge.\nIn this paper, we propose an effective and diverse multi-turn attack method, called ActorAttack. Inspired by Latour's actor-network theory (Latour, 1987), we explicitly model a network where each node (actor) is semantically linked with the harmful target (e.g., the actor, Ted Kaczynski, who builds bombs for terrorism, is correlated with the target of building a bomb.). These actors and their relationships with the harmful target constitute our attack clues, and we hide the harmful intent in the innocuous multi-turn conversation about the actor. Notably, as illustrated in Fig. 1 (b), we propose automating the discovery of attack clues by leveraging the knowledge of LLMs. Selecting an attack clue, ActorAttack then infers the attack chain, which describes how to achieve harmful targets step by step (Fig. 3). Following the attack chain, ActorAttack generates queries, which can guide LLMs' responses to become increasingly relevant to the harmful target until reaching it.\nOverall, our network design helps improve the diversity of our attack on two levels: (1) inter-network diversity: our attacker model generates target-specific networks for various harmful targets; (2) intra- network diversity: Inside the network, we categorize six distinct types of nodes (actors) based on their relationship to the harmful target and each type of nodes leads to different attack paths (Fig. 2). Experimental results show that ActorAttack finds higher-quality attacks from more diverse attack paths, and is effective over both single-turn and multi-turn attack baselines across various aligned LLMs, even for GPT-01 (OpenAI, 2024b) whose advanced reasoning improves safety. We find that though GPT-01 identifies our harmful intent and shows it should follow the safety policies in its chain of thought, it still outputs unsafe content, revealing the potential conflict between its helpfulness and safety goals against our attack.\""}, {"title": "RELATED WORK", "content": "Single-turn Attacks. The most common attacks applied to LLMs are single-turn attacks. One effective attack method is to transform the malicious query into semantically equivalent but out- of-distribution forms, such as ciphers (Yuan et al., 2024b; Wei et al., 2024), low-resource lan- guages (Wang et al., 2023; Yong et al., 2023; Deng et al., 2023), or code (Ren et al., 2024). Lever- aging insights from human-like communications to jailbreak LLMs has also achieved success, such as setting up a hypothesis scenario (Chao et al., 2024; Liu et al., 2023), applying persuasion (Zeng et al., 2024), or psychology strategies (Zhang et al., 2024a). Moreover, gradient-based optimization methods (Zou et al., 2023b; Wang et al., 2024; Paulus et al., 2024; Zhu et al., 2024) have proven to be highly effective. Some attacks exploit LLMs to mimic human red teaming for automated attacks (Casper et al., 2023; Mehrotra et al., 2023; Perez et al., 2022; Yu et al., 2023; Anil et al., 2024). Other attacks further consider the threat model, where the attacker can edit model internals via fine-tuning or representation engineering (Qi et al., 2023; Zou et al., 2023a; Yi et al., 2024).\nMulti-turn Attacks. Multi-turn attacks are less covered in the literature, though there have been several works to reveal the safety risks in the multi-turn dialogue scenario. One multi-turn attack strategy is the fine-grained task decomposition, which decomposes the original malicious query into several less harmful sub-questions (Yu et al., 2024; Zhou et al., 2024; Liu et al., 2024d). While this decomposition strategy successfully circumvents current safety mechanisms, it may be easily mitigated by including these finer-grained harmful queries in safety training data. Alternatively, researchers propose to use human red teamers to expose vulnerabilities of LLMs against multi-turn attacks (Li et al., 2024b). Moreover, Yang et al. (2024) depends on the heuristics from (Chao et al., 2024) and its seed examples to implement its attacks. The most relevant to our work is Crescendo (Russinovich et al., 2024), which gradually steers benign initial queries towards more harmful topics. The implementation of Crescendo is based on the fixed and human-crafted seed instances, making it challenging to generate diverse and effective attacks (Section 4.3, Fig. 5). By contrast, we propose to discover diverse attack clues inside the model's prior knowledge. We further model the attack clues via a network and classify these clues into different types, bringing a greater coverage of possible attack paths. Moreover, the inherent semantic correlation between our attack clues and our attack target ensures effectiveness.\nDefenses for LLMs. To ensure LLMs safely follow human intents, various defense measures have been developed, including prompt engineering (Xie et al., 2023; Zheng et al., 2024), aligning mod- els with human values (Ouyang et al., 2022; Bai et al., 2022; Rafailov et al., 2024; Meng et al., 2024; Yuan et al., 2024a), model unlearning (Li et al., 2024c; Zhang et al., 2024b), representation engineering(Zou et al., 2024a) and implementing input and output guardrails (Dubey et al., 2024; Inan et al., 2023; Zou et al., 2024b). Specifically, input and output guardrails involve input perturba- tion (Robey et al., 2023; Cao et al., 2023; Liu et al., 2024e), safety decoding (Xu et al., 2024), and jailbreak detection (Zhang et al., 2024c; Yuan et al., 2024c; Phute et al., 2023; Alon & Kamfonas, 2023; Jain et al., 2023; Hu et al., 2024). Priority training also shows its effectiveness by training LLMs to prioritize safe instructions (Lu et al., 2024; Wallace et al., 2024; Zhang et al., 2023)."}, {"title": "METHOD: GENERATE MULTI-TURN ATTACK THROUGH SELF-DISCOVERED CLUES", "content": "Overview. We propose a two-stage approach to automatically find attack clues and generate multi- turn attacks. The first stage consists of network construction around the harmful target, where every network node can be used as an attack clue (Fig. 2). The second stage includes the attack chain generation based on the attack clue and the multi-turn query generation (Fig. 3)."}, {"title": "PRE-ATTACK: FIND ATTACK CLUES", "content": "Inspired by Latour's actor-network theory, we propose a conceptual network $G_{concept}$ to categorize various types of actors correlated with the harmful target. These actors can be exploited as our attack clues, and we leverage the knowledge of LLMs to specify these clues. Latour (1987) claim that everything exists in networks of relationships, and is influenced by human and non-human actors in the network. For better coverage of possible attack clues, as illustrated in Fig. 2, we identify six types of actors in terms of how they influence the harmful target in a network, e.g., Distribution corresponds to actors who spread harmful behaviors or information across the network. Inside each type of actor, we further consider both human and non-human actors, e.g., human actors can include historical figures and influential people and non-human actors can be books, media, or social movements.\nTo be specific, we model our network into a two-layer tree, whose root node is the harmful target x, and the nodes in the first layer represent the six abstract types. The leaf nodes are specific actor names. Each leaf node $v_i$ and the edge between the leaf node and the parent node depict the relationship between the actor and the harmful target, which constitutes an attack clue $c_i$. Since LLMs are trained on vast amounts of text data, we view LLMs as \u201cknowledge base"}, {"title": "IN-ATTACK: FIRST REASON THEN ATTACK", "content": "Based on the identified attack clue, we perform our multi-turn attacks in three steps. The first step is about inferring the attack chain about how to gradually elicit the harmful responses from the victim model step by step. Secondly, the attacker LLM follows the attack chain to generate the initial multi-turn query set via self-talk, i.e., communicating with oneself. Finally, the attacker LLM dynamically modifies the initial attack path during the realistic interaction with the victim model. We present the concrete algorithm in Algorithm 1.\n1. Infer the attack chain. Given the selected attack clue $c_i$, and the harmful target x, our attacker LLM infers a chain of thoughts $z_1,..., z_n$ to build the attack path from $c_i$ to x. As illustrated in Fig. 3 (a), our attack chain specifies how the topics of our multi-turn queries evolve, guiding the victim model's responses more aligned with our attack target. In practice, each thought $z_i \\sim P(z_i |x, c_i, z_1,...,z_{i-1};\\theta)$ is sampled sequentially.\n2. Generate multi-turn attacks via self-talk. Following the attack chain, our attacker LLM generates multiple rounds of queries $[q_1,..., q_n]$ one by one. We refer to the context before generating the queries as s =[x, $c_i$, $z_{1...n}$]. Except the first query $q_1 \\sim p(q_1 |s; \\theta)$, each query $q_i$ is generated conditioned on the previous queries and responses $[q_1, r_1,..., q_{i-1}, r_{i-1}]$, i.e., $q_i \\sim P(q_i |s, q_1, r_1, ..., q_{i-1}, r_{i-1}; \\theta)$. As for the generation of the model response $r_i$, instead of directly interacting with the victim model, we propose a self-talk strategy to use the responses predicted by the attacker LLM as the proxy of responses from the unknown victim model, i.e., $r_i \\sim p(r_i |s, q_1, r_1, \u00b7 \u00b7 \u00b7, q_{i\u22121}, r_{i\u22121}, q_i; \\theta)$ (Fig. 3 (b)). We hypothesize that due to LLMs' using sim- ilar training data, different LLMs may have similar responses $r_i$ against the same query $q_i$, which indicates that our attacks have the potential of being effective against different models without spe- cific adaptation and enable us to discover common failure modes of these models.\n3. Dynamically modify the initial attack path for various victim models. During the interac- tions with the victim model, we propose to dynamically modify the initial attack paths to mitigate the possible misalignment between the predicted and realistic responses. We identify two typical misalignment cases and design a GPT4-Judge to assess every response from the victim model: (1) Unknown, where the victim model does not know the answer to the current query, (2) Rejective, where the victim model refuses to answer the current query. As for Unknown, we drop the attack clue, and sample another one to restart our attack again (Fig. 3 (c)), while for Rejective, we per- form the toxicity reduction by removing the harmful words and using ellipsis to bypass the safety guardrails of LLMs."}, {"title": "EXPERIMENTS", "content": "EXPERIMENTAL SETUP\nModels. We validate the efficacy of ActorAttack on 5 prevalent LLMs: GPT-3.5 (GPT-3.5 Turbo 1106) (OpenAI, 2023), GPT-40 (OpenAI, 2024a), Claude-3.5 (Claude-3.5-sonnet-20240620) (An- thropic, 2024), Llama-3-8B (Llama-3-8B-Instruct) (Dubey et al., 2024) and Llama-3-70B (Llama- 3-70B-Instruct) (Dubey et al., 2024)."}, {"title": "MAIN RESULTS: EFFECTIVE AND DIVERSE MULTI-TURN ATTACK", "content": "ActorAttack generates more effective prompts than single-turn baselines. Table 1 shows the baseline comparison results. Although our ActorAttack method does not use any special optimiza- tion, we find that ActorAttack is the only method that achieves a high attack success rate across all target LLMs, highlighting the common and significant safety risks in the multi-turn dialogue sce- nario. Among the baselines, CodeAttack achieves the best performance, while its jailbreak template is hand-crafted and contains identifiable malicious instructions, making it easy to defend.\nFor qualitative evaluation, we provide various examples of ActorAttack, showcasing different types of human and non-human actors such as regulation, facilitation, and execution across dif- ferent harmful categories, as shown in Fig. 10, Fig. 11, Fig. 12, Fig. 13 and Fig. 14. We truncate our examples to include only partial harmful information to prevent real-world harm.\nActorAttack dynamically modifies the attack path for various target models, enhancing its ef- fectiveness. We compare the performance of our method with and without dynamic modification. As shown in Table 1, we have two findings: (1) When our attack does not involve dynamic modi- fication and does not leverage information from the target model, our attack still exhibits good per- formance across different models. This indicates that our method is efficient at identifying common safety vulnerabilities of these models without requiring special adaptations. (2) The introduction of dynamic modification further improves the effectiveness of our attack by adaptively modifying the queries based on the responses from the target model, toward a more comprehensive evaluation of the safety mechanisms behind different models."}, {"title": "EMPIRICAL ANALYSIS", "content": "The diverse attack paths uncovered by ActorAttack are mostly effective. ActorAttack generates diverse attack paths for the same harmful target. We assess the effectiveness of every path using the score given by our judge model, and we calculate the proportion of different scores for our attack paths. As shown in Fig. 6 (a), we find that most of these paths are classified as most harmful with a top score of 5. This reveals that ActorAttack can effectively identify more safety vulnerabilities of models through its diverse attack paths.\nActorAttack finds higher-quality attacks from more diverse attack paths. One potential advan- tage of generating diverse attack prompts is that we can find more optimal attack paths, leading to answers of higher quality. To study this empirically, we sample different numbers of attack clues to generate diverse attacks for the same harmful target and record the best score of the attacks by our judge model. As shown in Fig. 5, we find that the proportion of attacks with a score of 5 increases with more actors (attack clues), which indicates that ActorAttack can discover more optimal attack paths by exploiting diverse attack clues.\nQueries generated by ActorAttack bypass the detection of LLM-based input safeguard. To assess the effectiveness of our method in hiding the harmful intent, we employ Llama Guard 2 (Team, 2024) and MD-Judge (Li et al., 2024a) to classify both the original plain harmful queries and the multi-turn queries generated by ActorAttack and Crescendo to be safe or unsafe. The classifier score represents the probability of being \u201cunsafe.\u201d We generate multi-turn queries based on Claude- 3.5-sonnet and GPT-40. As shown in Fig. 6 (b) and (c), the toxicity of our multi-turn queries is much lower than that of both the original harmful query and the queries generated by Crescendo, which verifies the effectiveness of our ActorAttack method."}, {"title": "SAFETY FINE-TUNING", "content": "SETUP\nEvaluation. For helpfulness evaluation, we use OpenCompass (Contributors, 2023), including the following benchmarks: GSM8K (Cobbe et al., 2021), MMLU (Hendrycks et al., 2020), Hu- maneval (Chen et al., 2021) and MTBench (Zheng et al., 2023). The detailed settings are shown in App. A.2. For safety evaluation, we use the default settings of ActorAttack and Crescendo and set the maximum number of conversation turns to 5.\nData. For helpfulness, we utilize UltraChat (Ding et al., 2023) as the instruction data. Following the practice of (Zou et al., 2024a), we maintain a 1:2 ratio between our safety alignment data and instruction data. To construct our safety alignment dataset, we sample 600 harmful instructions from Circuit Breaker training dataset (Zou et al., 2024a), which have been filtered to avoid data contamination with the Harmbench."}, {"title": "DATASET CONSTRUCTION", "content": "Generate refusal responses to the queries that first elicit harmful responses. To demonstrate that attack prompts generated by our methods can enhance the safety alignment of target LLMs in the multi-turn dialogue scenarios, we fine-tune LLMs with samples generated by ActorAttack. To construct the safety data, one critical problem is to decide where to insert the refusal response into the multi-turn conversations. As shown in Fig. 1, ActorAttack elicits harmful responses from the victim model during the intermediate queries. Though not directly fulfilling the user's intent, such responses can still be misused. Therefore, we propose to use the judge model to detect where the victim model first elicits harmful responses and insert the refusal responses here."}, {"title": "MAIN RESULTS", "content": "ActorAttack allows for robust safety-tuned LLMs against multi-turn attacks. We fine-tune Llama-3-8B-Instruct using our 500 and 1000 safety alignment samples respectively, combined with the instruction data. We assess the safety of models using prompts generated by ActorAttack and Crescendo based on Harmbench. Table 2 shows that our safety alignment data greatly improves the robustness of the target model against multi-turn attacks, especially for Crescendo, which is unseen during fine-tuning. We also find that performing multi-turn safety alignment compromises helpfulness, and we plan to explore better solutions to this trade-off in future work."}, {"title": "CONCLUSION", "content": "In this paper, we introduce ActorAttack to expose the significant safety vulnerabilities of LLMs in multi-turn interactions. Inspired by actor-network theory, we model the attack clues using a network and automate the discovery of these clues by leveraging LLMs' knowledge. Through our experiments, we showed that our approach is effective for jailbreaking a wide variety of aligned LLMs, even for GPT-01, whose advanced reasoning improves safety. We find that our diverse attack paths help find higher-quality attacks and identify additional safety vulnerabilities. To mitigate the safety risk, we construct a safety alignment dataset generated by ActorAttack and greatly improve the robustness of models safety-tuned using our safety dataset against multi-turn attacks.\nLimitation and future work. In this study, we focus on generating actors related to harmful tar- gets in English, without considering multilingual scenarios. Different languages come with distinct cultures and histories, which means that for the same harmful behavior, actors associated with differ- ent languages may differ. Since LLMs have demonstrated strong multilingual capabilities (Nguyen et al., 2023; Sengupta et al., 2023; Workshop et al., 2022), it would be valuable to study our at- tack methods across multiple languages for better coverage of the real-world distribution of actors. Future work can also explore the applicability of our method to jailbreak multi-modal models (Liu et al., 2024c;b). For defense, we use safety fine-tuning to generate refusal responses. However, we observe a trade-off between helpfulness and safety. Exploring reinforcement learning from human feedback (RLHF) in the multi-turn dialogue scenarios could be a valuable direction, e.g., designing a reward model that provides more granular scoring at each step of multi-turn dialogues.\nEthics Statement. We propose an automated method to generate jailbreak prompts for multi-turn dialogues, which could potentially be misused to attack commercial LLMs. However, since multi- turn dialogues are a typical interaction scenario between users and LLMs, we believe it is necessary to study the risks involved to better mitigate these vulnerabilities. We followed ethical guidelines throughout our study. To minimize real-world harm, we will disclose the results to major LLM developers before publication. Additionally, we explored using data generated by ActorAttack for safety fine-tuning to mitigate the risks. We commit to continuously monitoring and updating our research in line with technological advancements."}]}