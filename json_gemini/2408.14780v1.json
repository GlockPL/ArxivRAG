{"title": "GINN-KAN: Interpretability pipelining with applications in Physics Informed Neural Networks", "authors": ["Nisal Ranasinghe", "Yu Xia", "Sachith Seneviratne", "Saman Halgamuge"], "abstract": "Neural networks are powerful function approximators, yet their \"black-box\" nature often renders them opaque and difficult to interpret. While many post-hoc explanation methods exist, they typically fail to capture the underlying reasoning processes of the networks. A truly interpretable neural network would be trained similarly to conventional models using techniques such as backpropagation, but additionally provide insights into the learned input-output relationships. In this work, we introduce the concept of interpretability pipelining, to incorporate multiple interpretability techniques to outperform each individual technique. To this end, we first evaluate several architectures that promise such interpretability, with a particular focus on two recent models selected for their potential to incorporate interpretability into standard neural network architectures while still leveraging backpropagation: the Growing Interpretable Neural Network (GINN) and Kolmogorov Arnold Networks (KAN). We analyze the limitations and strengths of each and introduce a novel interpretable neural network GINN-KAN that synthesizes the advantages of both models. When tested on the Feynman symbolic regression benchmark datasets, GINN-KAN outperforms both GINN and KAN. To highlight the capabilities and the generalizability of this approach, we position GINN-KAN as an alternative to conventional black-box networks in Physics-Informed Neural Networks (PINNs), which we propose as a challenging testbed for backpropagation-friendly interpretable neural networks. By adding interpretability to PINNS, we allow far more transparent and trustworthy data-driven solutions to differential equations. We expect this to have far-reaching implications in the application of deep learning pipelines in the natural sciences. Our experiments with this interpretable PINN on 15 different partial differential equations demonstrate that GINN-KAN augmented PINNS outperform PINNs with black-box networks in solving differential equations and surpass the capabilities of both GINN and KAN.", "sections": [{"title": "Introduction", "content": "Neural networks have largely driven the advancement of artificial intelligence (AI) over the last decade. They are widely used in many domains including computer vision, natural language processing and speech processing for real-world applications. Such neural networks have already surpassed human performance in many applications. However, neural networks generally consist of a large number of neurons, which results in them learning a complex function that maps the inputs to the outputs. Though this function may be very accurate, their innate opaque nature prevents it from being adopted in many sensitive fields where critical decisions have to be made using the output of the model, and the ability to describe and justify the decision becomes paramount.\nThe interpretability of neural networks is crucial to increasing the trustworthiness of AI, thereby increasing its adoption in real-world applications. Though explainability of machine learning (ML) models including neural networks have been studied in the past (Ribeiro, Singh, and Guestrin 2016; Lundberg and Lee 2017; Zhou et al. 2016; Selvaraju et al. 2016), there is a subtle difference between explainability and interpretability. Explainability focuses on explaining the decisions made by a system and typically does not represent how the model actually made its decision (Ali et al. 2023). On the other hand, an interpretable model aims to provide insights into the reasoning of the model.\nIn this work, we focus on interpretable neural networks, which give insights into the learned representation of a model. In particular, we are interested in networks that can aid in scientific discovery, by learning functions which follow a concise mathematical equation while still being trained on input-output pairs similar to any other neural network. The growing interpretable neural network (GINN) (Ranasinghe et al. 2024) and the Kolmogorov Arnold Network (KAN) (Liu et al. 2024) are two such networks that have shown promise in this task, and have even been shown to be able to discover the ground truth mathematical equations while still being trained using regular backpropagation. The ability to learn equations is similar to symbolic regression (SR) but with a key difference. A SR algorithm searches the equation space in an efficient manner until the best equation describing the dataset is found. In contrast, these methods can be trained on input-output pairs like any other neural network, but in a way that allows the extraction of a mathematical equation upon inspection of the trained neural network.\nLeveraging this unique property exhibited by GINN and KANs, we introduce the concept of interpretability pipelining, which combines multiple interpretability methods to enhance model transparency and performance. We present GINN-KAN, the first such pipelined interpretable neural network, that combines the strengths of the GINN and KAN"}, {"title": "Neural Network Interpretability", "content": "Machine learning model explainability has garnered considerable academic interest. This has resulted in many explainability methods that can visually or quantitatively explain a model's output. Although these methods provide insights into the decisions of the model, they may not reflect the underlying decision-making process. Therefore, even with incorporated explainability methods, machine learning models, especially neural networks remain as black-boxes. On the other hand, a fully interpretable model can be considered a \"white-box\", since its inner workings are fully transparent and human-understandable. Some simple models like linear regressors and decision trees are inherently interpretable since the inner workings of the learned model can be simply interpreted using a mathematical equation (in linear regression) or a set of rules (in decision trees). However, these simple models cannot fit datasets with complex underlying functions.\nNeural networks such as GINN (Ranasinghe et al. 2024), KAN (Liu et al. 2024) and EQL (Sahoo, Lampert, and Martius 2018) can be considered interpretable since they provide insights into the learned function, once trained. They have also been shown to be able to discover mathematical equa-"}, {"title": "Physics-informed Neural Networks", "content": "PINNs are a deep learning method for solving PDEs. They ensure that the outputs comply with known physical laws by integrating domain-specific knowledge as soft constraints into the loss function. This is achieved by including \"physics\" loss terms which penalize the neural network from deviating from the underlying governing differential equation. The physics loss is defined as follows:\n$L_{physics} = \\frac{1}{N} \\sum_{i=1}^{N} (N[\\hat{u}(x_i, t_i)])^2,$\nwhere $\\hat{u}(\\cdot)$ is the estimated solution of the PDE and N represents the linear or nonlinear operator. x and t denote space and time respectively, where $x \\in \\Omega \\subset R^d$, $t \\in [0, T]$. T is the time horizon, and $\\Omega$ is the spatial domain. N is the number of collocation points within the spatiotemporal domain. This allows the network to be optimized using backpropagation techniques. PINNs generally do not require labelled datasets since the labels used for training the network can be generated using the initial or boundary conditions of the differential equation."}, {"title": "Methodology", "content": "We introduce GINN-KAN, an improved interpretable neural network that combines the strengths of GINNs and KANs. While GINNs excel when the underlying ground truth equation is a Laurent polynomial, their performance declines with non-LP equations. Conversely, KANs have the theoretical capability to learn non-LP equations, but in practice struggle with equations involving multiplications due to inherent assumptions.\nWe note many parallels between GINNs and KANs, enabling their seamless integration into a more robust network that leverages their strengths while mitigating their individual limitations. Both these networks can be trained using regular backpropagation algorithms on a dataset of input-output pairs. Once trained both GINNs and KANs are interpretable, providing insights into the learned function. Moreover, both of these networks can also use the trained weights of the network to discover a mathematical equation describing the learned function.\nWe make use of these parallels between GINNs and KANs, to create GINN-KAN, an end-to-end differentiable interpretable neural network, which can be trained using backpropagation. Similar to GINN and KAN, once trained, GINN-KAN can be inspected to gain insights into the network.\nThe GINN-KAN architecture is shown in Fig. 1. Both the GINN and KAN components of this network can be expressed in terms of a concise mathematical equation. For the GINNs part, this can be done by inspecting the weights and constructing the equation of the network. Each power-term approximator (PTA) block consists of logarithm activations, a single linear activated neuron and an exponential activation. The equation of a PTA block reduces to,\n$P_i = X_1^{W_{i1}} * X_2^{W_{i2}} * ... X_n^{W_{in}}$\nwhere $p_i$ is the output of the ith PTA block. Hence, the equation for a GINN with n power-term approximator (PTA) blocks is given by,\n$y = \\sum_{i=1}^{n} A_i * P_i$\nwhere ai is the ith coefficient of the linear activated neuron, and wij are the coefficients of the PTAs.\nThe KAN can be expressed using an equation by mapping each learned B-spline function to a symbolic function, by comparing them against a set of pre-defined symbolic functions from a library of univariate functions. Some example functions include sin(x), ex and ln(x)."}, {"title": "GINN-KAN Augmented PINNS", "content": "PINNs are black-box in nature due to the fully connected neural network neural network which is generally used as the function approximator. Though some work attempts to learn symbolic functions that approximate the learned function, this is done by training symbolic regression methods using data generated by black-box surrogate models (Podina, Eastman, and Kohandel 2023). Although this allows the discovery of an interpretable model that approximates the learned function, this may not accurately reflect the decision-making process of the trained PINN.\nIn this paper, we propose replacing the black-box neural networks within PINNs with GINN-KAN to create an interpretable PINN. Since GINN-KAN can be trained using backpropagation, it can be used within PINNs with minimal change to the training strategy. Once trained, the interpretable PINN will be able to provide insights into the learned solution to the differential equation. The architecture of GINN-KAN augmented PINNs is shown in Fig. 2."}, {"title": "Experiment Setup", "content": "Earlier, we noted that KANs cannot easily approximate multiplications, while GINN cannot easily approximate non-LP equations. To confirm these hypotheses, we construct datasets using eight ground truth equations. Two of these equations are LP equations, two are non-LP equations without any multiplications and the remaining four are non-LP equations with multiplications. We compare the performances of GINN, KAN and GINN-KAN on datasets constructed using these equations. We create these datasets by sampling 2000 points randomly for each input variable and generating the outputs using the ground truth equation.\nSymbolic regression benchmark. We perform the evaluation of GINN-KAN on the popular Feynman symbolic regression benchmark datasets using SRBench, a popular benchmark for symbolic regression methods (La Cava et al. 2021). This dataset contains 114 datasets with known ground truth equations and is often used for evaluating symbolic regression methods (Udrescu and Tegmark 2020).\nEvaluation metrics. After training the models on each SR dataset, we measure the performance using the Mean Squared Error (MSE) and the coefficient of determination R2 evaluated on the test set. The R\u00b2 is aggregated by calculating the percentage of datasets with R\u00b2 > 0.99. This metric is also used in SRBench, a widely used symbolic regression benchmark (La Cava et al. 2021). We calculate the MSE rank across GINN, KAN and GINN-KAN and average it to calculate the mean rank on the LP / Non-LP subsets of datasets, allowing us to compare performance within this subgroup of models."}, {"title": "GINN-KAN Augmented PINNS", "content": "The performance of GINN-KAN is evaluated on 15 PDEs, which are shown in the Appendix. The first 7 equations, include both linear and non-linear PDEs commonly used to model various physical phenomena. These equations vary in complexity and involve different operations (e.g., multiplication, division, addition) and basis functions (e.g., sin, exp, x\u00b2). The last 5 equations are designed with LP ground truth analytical equations."}, {"title": "GINN-KAN", "content": "We show the results on several synthetic datasets generated in Table 1. The first two equations are non-LP equations with no multiplicative terms. On these datasets, GINN performs poorly but KANs and GINN-KAN both perform well on these datasets. The next two equations are LP equa-"}, {"title": "Conclusion", "content": "In this work, we first evaluate interpretable neural networks, specifically focusing on the recently introduced GINN and KANs. While both methods have demonstrated interpretability, they exhibit limitations in learning certain types of functions. Our evaluation shows that GINNs do not perform well on datasets governed by non-LP equations, while KANs do not perform well on datasets governed by equations with multiplications. To address these limitations, we propose a novel interpretable neural network, GINN-KAN, which combines the strengths of both GINN and KANs. Experiments conducted with this novel network on the Feynman symbolic regression benchmark datasets show that GINN-KAN outperforms both GINN and KANs on datasets with known ground truth equations. We then apply GINN-KANs to physics-informed neural networks, showing that they can add interpretability to PINNs. By performing experiments on 15 differential equations, we demonstrate that this interpretable PINN not only adds interpretability but also improves the performance in solving differential equations when compared with traditional PINNs.\nInterpretability is a crucial aspect often overlooked in favor of performance in neural networks. However, methods like GINN-KAN have the potential to be as effective as black-box MLPs while also being interpretable. Since GINN-KAN can be trained using backpropagation, it can be seamlessly integrated into existing machine learning pipelines with minimal adjustments to the training strategy.\nDespite its advantages, GINN-KAN shares the limitation of GINN in being restricted to inputs with positive values. This can be mitigated by shifting all inputs to the positive range. Future research should explore more robust architectures that maintain interpretability while overcoming this limitation. Additionally, better regularization techniques are needed to enable GINN-KAN to accurately discover concise ground truth equations that describe the data.\nIn conclusion, GINN-KAN represents a significant step forward in developing interpretable neural networks, offering a promising balance between interpretability and performance. Its ability to integrate into existing machine learning frameworks and its potential applications in physics-informed neural networks highlight its importance in advancing the field."}]}