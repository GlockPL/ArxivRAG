{"title": "FAST GDRNPP: Improving the Speed of State-of-the-Art 6D Object Pose Estimation", "authors": ["T. P\u00f6llabauer", "A. Pramod", "V. Knauthe", "M. Wahl"], "abstract": "6D object pose estimation involves determining the three-dimensional translation and rotation of an object within a scene and relative to a chosen coordinate system. This problem is of particular interest for many practical applications in industrial tasks such as quality control, bin picking, and robotic manipulation, where both speed and accuracy are critical for real-world deployment. Current models, both classical and deep-learning-based, often struggle with the trade-off between accuracy and latency. Our research focuses on enhancing the speed of a prominent state-of-the-art deep learning model, GDRNPP, while keeping its high accuracy. We employ several techniques to reduce the model size and improve inference time. These techniques include using smaller and quicker backbones, pruning unnecessary parameters, and distillation to transfer knowledge from a large, high-performing model to a smaller, more efficient student model. Our findings demonstrate that the proposed configuration maintains accuracy comparable to the state-of-the-art while significantly improving inference time. This advancement could lead to more efficient and practical applications in various industrial scenarios, thereby enhancing the overall applicability of 6D Object Pose Estimation models in real-world settings.", "sections": [{"title": "1. Introduction", "content": "Object pose estimation is a fundamental computer vision problem that aims to determine the pose of an object in a given image relative to the camera. Depending on the application needs, the pose can be estimated in varying degrees of freedom (DoF): 3DoF includes only 3D rotation, 6DoF adds 3D translation, and 9DoF further includes the 3D size of the object. 6D object pose estimation, which involves determining both the orientation and translation of an object, is particularly significant in industrial applications. Accurate pose estimation gives an understanding of an object's spatial position, which is vital for tasks in virtual reality, industrial bin-picking, and autonomous driving. Recent state-of-the-art methods"}, {"title": "2. Related Work", "content": "We will present relevant work on the topic, introducing the BOP challenge and the test datasets (Section 2.1), discussing relevant algorithms (Section 2.2) and details on the selected GDRNPP (Section 2.3), before presenting the ideas of pruning (Section 2.4) and knowledge distillation (Section 2.5)."}, {"title": "2.1. BOP Challenge and Datasets", "content": "With the advent of consumer-grade sensors and cameras, the task of pose estimation has become more accessible. Research and development have progressed in various directions to tackle this complex problem. Meaningful evaluation and comparison of these solutions require unified guidelines. The BOP (Benchmark of Pose Estimation) project, introduced in 2019 [HSD*20] and repeated in 2022 and 2023 [SHL*23, HSL*24], addresses these needs by providing a comprehensive formulation of the 6D pose estimation task, standardized datasets for performance measurement, a set of pose error metrics accounting for different measurement ambiguities and use case requirements, and an online evaluation system with a leaderboard ranking various state-of-the-art methods. Over the years, new tasks have been added, along with updates to the datasets to better evaluate proposed solutions. Our research focuses on the latest 2023 version of the BOP project. We refer to this iteration to define the 6D pose estimation task on seen objects, compare relevant algorithms, and select the most promising as our candidate for further improvement.\nBOP provides a selection of relevant datasets, of which 7 are selected as \"core classic\". According to the task definition, a algorithm has to be benchmarked on all 7 core classic datasets. A short description of the datasets follows: Linemod Occlusion (LM-O). [BKM*14] An extension of Linemod (LM), designed for evaluating performance in occlusion scenarios. LM contains 15 RGBD sequences with annotated images, CAD models, 2D bounding boxes, and binary masks. It poses challenges with cluttered scenes, texture-less objects, and varying lighting conditions. LM-O includes 1214 RGBD images featuring 8 heavily occluded objects. IC-BIN. [HZL*15] Focuses on texture-less object pose estimation. IC-BIN addresses clutter and occlusion in robot bin picking scenarios. YCB-Video (YCB-V). [XSNF18] Contains 21 objects across 92 RGBD videos, totaling 133,827 frames. It is suitable for object pose estimation and tracking tasks. T-LESS. [HHO*17] Designed for texture-less industrial objects, featuring 30 items from the domain of electrical installation with varying backgrounds, lighting conditions, and occlusions. It is challenging due to the absence of texture and complex object placement. ITODD. [DUB*17] Comprises 28 industrial objects across more than 800 scenes with around 3,500 images. It utilizes industrial 3D sensors and high-resolution grayscale cameras for multi-angle observations. TUD-L. [HMB*18] Evaluates the robustness of pose estimation algorithms to lighting variations. TUD-L involves fixed cameras and manually moved objects for realistic movement representation. HB. [KZSI19] Covers various scenes with changes in occlusion and lighting conditions, including 33 objects (toys, household items, and industry-related objects) across 13 diverse scenes."}, {"title": "2.2. Algorithm Candidates for Speed up", "content": "To find a suitable algorithm to accelerate, we take a look at the comprehensive BOP leaderboard. Table 1a shows the top-performing models based on Average Recall (AR) across the seven core datasets of the BOP challenge on the task of localizing seen objects. From top to bottom we find places one to three taken up by GPose, an improved version of GDRNPP. GDRNPP [LZZ*22] is an improved version of GDR-Net [WMTJ21], which again, is an improved version of earlier algorithm CDPN [LWJ19a]. Different parameterizations of GDRNPP take up the places 4, 5, 7, and 10. ZebraPose [SSF*22] comes in on place 6, PFA [HFS22] at 8th place, and RDPN is the last entry in the top 10 at 9th place. Taking inference speed into account and taking another look at the leaderboard sorted by speed we aggregate a second table. Table 1b focuses on especially fast models according to the reported average processing time per image. To create the table, we sort by speed and only select models with an AR score greater than 0.60 and ignore any algorithm performing worse. We again find all of the above mentioned algorithms in the first positions, with the single additional entry MRPE.\nOur final list of candidates, therefore, consists of MRPE, GPose, ZebraPose, PFA, and GDRNPP. Looking through the list we find MRPE and GPose unpublished and neither does provide implementation details nor code. Comparing the remaining options, the ZebraPose algorithm [SSF*22] uses binary hierarchical encoding for the vertices of a 3D object. It groups and encodes these vertices, storing the mapping along with the 3D vertices offline. A detector identifies regions of interest in a 2D image, and a fully convolutional neural network (CNN) predicts a multi-layer code. This predicted code is then matched with the stored mapping to produce 2D-3D correspondences. Finally, a Perspective-n-Point (PnP) solver is used to estimate the 6D pose. RDPN [HHC24] regresses object coordinates per visible pixel and transforms the coordinates into a residual representation to predict pose. Third candidate PFA [HFS22] estimates an initial pose using a first network and matches this pose with offline-generated templates. The comparison between the retrieved template and the target view in the 2D image is performed by computing the displacement field, which is"}, {"title": "2.3. GDRNPP", "content": "GDRNPP is an enhanced version of GDR-Net [WMTJ21] illustrated in Figure 2, using deep neural networks to directly regress required features. It consists of three sub-modules: Object detection, feature prediction, and pose estimation. Also, there is a 4th optional step for pose refinement. To guarantee a fair comparison, we will use the official BOP detections and will not concern ourselves with the detection step at all. The pose refinement module includes two options: Coupled Iterative Refinement [LTGD22] and a fast refinement setup. For our purposes, we will use the fast refinement setup to evaluate the speed improvements introduced by our method. Our primary focus, however, is on both the feature prediction and the pose estimation modules (2nd and 3rd module).\nA forward pass through GDR-Net works as follows (and is functional-wise the same for GDRNPP): An RGB image is fed to the detector, which returns a set of detections. Next, GDR-Net applies dynamic zoom in [LWJ19b], to create a range of different object crops, making the algorithm more robust to variances in detection bounding box predictions. The object crops are resized to 256 \u00d7 256 pixels during the training phase. During testing, it uses inputs from the detector (we use the official YOLOX [GLW*21] detections). These inputs are then fed into the geometric feature module, a ResNet-34 [HZRS16] backbone, which extracts image feature maps of resolution 512 \u00d7 8 \u00d7 8 from the zoomed-in Rol, and a decoder head, which subsequently extracts intermediate geometric features from the image feature maps, namely surface region attention maps MSRA, visible/modal object mask Mvis, and 2D-3D correspondence maps M2D-3D. Finally, a Patch PnP module directly regresses the rotation (3DoF) and translationb(3DoF) from\nthe learned features MSRA and M2D-3D. The main architectural differences between GDR-Net and GDRNPP are the use of ConvNext-B [LMW*22] as a more advanced backbone for feature extraction, the addition of amodal mask prediction Mamo, in addition to modal mask Mvis, as well as more involved image augmentation."}, {"title": "2.4. Pruning", "content": "[LGW*21, CWZZ17] survey the use and effects of pruning and quantization. Pruning is the process of reducing the number of learnable parameters in a network by removing those that only negligibly add to the function approximation. Pruning might help to prevent overfitting and improve speed. Quantization is the process of reducing the expressiveness of parameters by switching to lower bit widths, such as 8-bit integers and even smaller. With techniques such as Nvidia's automatic mixed precision [NVI24] built right into TensorFlow and Pytorch, we focus on pruning. Pruning involves two main steps: first, selecting and removing parameters, and second, retraining the pruned model for a small number of epochs to regain performance, called fine-tuning. Pruning can be done in one-shot, where the network is pruned to the desired degree and then fine-tuned, or iteratively, where the model is partially pruned and retrained multiple times. A further distinction is made between structured [LKD*22, HX23] and unstructured [BGOFG20] pruning. Structured pruning removes entire channels or layers, while unstructured pruning removes individual weights. [VA22] categorizes common pruning techniques into, first, magnitude based, second, clustering based, and third, sensitivity analysis based methods."}, {"title": "2.5. Knowledge Distillation", "content": "Knowledge distillation (KD) in deep learning describes the process of extracting knowledge from one model (called the teacher) and transferring it to another model (the so called student) [CWZZ17, GYMT21]. The knowledge transfer can occur from the last layer, the entire teacher model, or specific parts of it, depending on the method used. KD can be applied to arbitrary domains of deep learning, but has been especially interesting for computer vision because of the large networks usually found within the domain. [WY21] provide an in-detail survey on the student-teacher framework applied to computer vision. Also, a very useful property of knowledge distillation, is the fact that models tend to learn faster from a teacher, than from ground truth data [PL19]."}, {"title": "3. Methodology", "content": "Next we discuss our approaches to reduce the run-time of GDRNPP. We start introducing a selection of relevant alternative backbones (Section 3.1), discuss the possibilities of shrinking the decoder head by reducing the number of predicted features (Section 3.2), applying pruning (Section 3.3), as well as knowledge distillation (Section 3.4)."}, {"title": "3.1. Backbone Selection", "content": "GDRNPP originally uses a ConvNext-B model [LMW*22] as backbone, featuring 89 million learnable parameters, a top-1 accuracy of 83.8% on the ImageNet-1K dataset [DDS*09], and 15.6 GFLOPS. For context, the original GDR-Net used ResNet-34 [HZRS16] achieved a top-1 accuracy of 73.31% with only 3.66 GFLOPS. It is fair to say that ConvNext-B demonstrates better feature extraction capabilities than ResNet-34 and, as a result, much of GDRNPPs outperformance can be credited to the more expressive backbone. The increase in performance between GDR-Net and GDRNPP due to the change in backbone indicates the backbone being a crucial factor for performance. Our goal therefore becomes to find a backbone that performs similar to ConvNext-B, all the while being noticeably faster."}, {"title": "3.2. Region Ablation", "content": "In the paper presenting GDR-Net, the authors test how the number of predicted regions for the attention maps MSRA influence overall accuracy evaluated on LM. They find that already without MSRA (0 regions) performance is good, while increasing the number of regions leads to slight performance gains. This behavior saturates at around 64 regions, which is the final value they settled with, arguing that this is a good trade-off between accuracy and memory requirements. We on the other hand are most interested in the effect on speed, wherefore we re-run the test to answer the question, how different numbers of regions influence run-time."}, {"title": "3.3. Pruning", "content": "We apply structured pruning with an L\u2081-norm based filter pruning method [LKD*22] to the estimation pipeline, removing weights with smaller magnitudes, assuming they have a negligible effect on the model's output. The pruning procedure works as follows: We select the weights of filters in a specific 2D convolutional layer and calculate the L\u2081 norm along the channel dimension. Filters are then ranked in ascending order of their norms, and the lowest D-ranking filters are removed. Consequently, the output channels from these removed filters and the kernels applied to these channels are also removed, as illustrated in Figure 3. Given an input feature map xi with shape wi \u00d7 hi \u00d7 ni and a convolutional layer with parameters Fi having shape ni+1 \u00d7 ni \u00d7 k \u00d7 k, the total operations are ni+1 \u00d7 ni \u00d7 k\u00b2 \u00d7 hi \u00d7 wi. Pruning a kernel Fi,j removes its corresponding feature map, reducing operations by ni \u00d7 k\u00b2 \u00d7 hi \u00d7 wi. This also affects the next convolutional layer Fi+1, further reducing operations by ni+2 \u00d7 k\u00b2 \u00d7 hi+2 \u00d7 wi+2. We apply pruning to both the geometric head, as well as the Patch PnP module. However, before starting the pruning, the group normalization layers need to be put into consideration, because stochastically selecting and removing layers could disrupt group normalization. Therefore, we remove groups of eight and four filters per parameter D for each module respectively, ensuring the grouping remains intact. Figures 4 and 5 show the parameterized representation of the layer architecture for both modules with respect to the hyperparameter D, controlling the amount of pruning with a higher value leading to more aggressive pruning."}, {"title": "3.4. Distillation", "content": "As with any learning problem, the choice of loss function is also very important for knowledge distillation. A typical loss function between the student and teacher is the Kullback-Leibler (KL) divergence loss [HVD15], as shown in Equation 1. This loss is calculated between the softened probability distributions of the teacher and student predictions given in Equation 2, controlled by the hyper-parameter \u03c4.\n$L_{KL}(p_s(t), p_t(t)) = \\sum_j p_{t, j}(t) \\log(\\frac{p_{t,j}(t)}{p_{s,j}(t)})$ (1)\n$p_{f,k}(t) = \\frac{exp(z_{f,k}/\\tau)}{\\sum_{j=1}^{K} exp(z_{f,j}/\\tau)}$ (2)\nKim et al. [KOK*21] compare the effectiveness of KL loss and mean squared error (MSE) loss, finding that MSE performs better for regression-based models. Since our solution space involves regression, we chose MSE as the distillation loss, as shown in Equation 3, where N is the length of the logits vector, zs,i is the student logits, and zt,i is the teacher logits.\n$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (z_{s,i} - z_{t,i})^2$ (3)\nFor our distillation process, we select the pre-trained GDRNPP model, specific to each dataset, as the teacher model. Student models are chosen based on the performance of selected backbones, as listed in Section 3.1 and evaluated in Section 4.3. Given that the output channel dimensions of the student model backbones differ from those of the teacher model, directly applying a loss function without aligning feature dimensions would be problematic. To address this, we initially up-sample the output of the student model's backbone to match the feature dimensions of the teacher model's backbone using a 1x1 convolutional layer. We then normalize the outputs of both the student and teacher models and apply the MSE loss, as described above. The computed loss is solely used to supervise the student backbone. The geometric head and Patch-PnP module are supervised based on their respective loss functions as found in the original GDRNPP [LZZ*22]."}, {"title": "4. Evaluation", "content": "We introduce the task and relevant evaluation metrics (Section 4.1), describe the implementation details (Section 4.2), evaluate the selected backbones integrated in GDRNPP (Section 4.3), and investigate the influence of reducing the number of regions in MSRA (Section 4.4), the influence of pruning both the geometric head, as well as the Patch PnP module (Section 4.5), and the influence of knowledge distillation (Section 4.6). Finally we conclude with combining our learnings into our final approach in Section 4.7."}, {"title": "4.1. Task Description and Metrics", "content": "For a fair comparison we follow the task and evaluation description of the BOP challenge for the task of localization of seen objects. Seen objects refers to the algorithm being allowed to see a set of images of the objects in question during training. The training and test sets of images are disjunct. We evaluate the following metrics: latency of the model, Average Recall following the 3 metrics VSD, MSSD, MSPD as defined by BOP, as well as the AR of ADD/ADD-S metric, which is another common metric in the field. The metrics are defined as follows:\nLatency. We measure latency as the forward pass time over all test images per dataset divided by the number of images in the dataset. All measurements, including those for the geometric head and Patch-PnP, are taken on the same GPU using the standard time module's performance counter, reported in milliseconds.\nAverage Recall. Recall is the ratio of truly positive predictions relative to all positive predictions as defined in Equation 4.\n$Recall = \\frac{True Positives}{True Positives + False Negatives}$ (4)\nAR defines thresholds for a given metric. A result is considered correct, if the the prediction falls within the defined threshold. Recall scores are calculated for multiple thresholds and then averaged. For BOP the three recall scores for VSD, MSSD, and MSPD are calculated and averaged to get the final ARBOP score.\nVSD. Equation 5 calculates VSD by comparing Euclidean distances between distance maps (ID, D) rendered from estimated and ground truth poses. The average number of pixels violating the distance threshold within the union of predicted and ground truth masks gives evsD, with \u03c4 varying from 5% to 50% of the object diameter and correctness thresholds ranging from 0.05 to 0.5.\n$e_{VSD}(D, D', V, V', \\tau) = avg_{p \\in U} \\begin{cases} 0 \\quad \\text{if } p \\in U \\cap V, |I(p) - D'(p)| < \\tau \\\\ 1 \\quad \\text{otherwise} \\end{cases}$ (5)\nMSSD and MSPD.\n$e_{MSSD} = min_{S \\in \\mathcal{S}} max_{X \\in M} || \\pi(X) - \\pi(PSX) ||$ (6)\n$e_{MSPD} = min_{S \\in \\mathcal{S}} max_{X \\in M} || proj(\\pi(X)) - proj(\\pi(PSX))||$ (7)\nEquations 6 and 7 define MSSD and MSPD respectively. Each vertex in the 3D model M is transformed using the ground truth P and estimated pose P, and the euclidean distances between the transformed 3D points and their 2D projections are computed separately. The minimum distances across all symmetries, eMSSD and eMSPD, are calculated, with eMSSD using the same threshold \u03c4 and eMSPD using a threshold of 5\u03c4 to 50\u03c4 in 5\u03c4 steps, where \u03c4 = w/640 and w is the image width.\nADD(-S).\n$\\text{ADD} = \\frac{1}{m} \\sum_{X \\in M} ||\\pi(X) - \\pi(PX)||$ (8)\nFor the ADD metric, the average distance between model points transformed using the estimated and ground truth poses is calculated using Equation 8. Recall is computed for correctness thresholds of 2%, 5%, and 10% of the object's diameter and averaged, which can be made symmetry-aware (ADD-S) by measuring the distance to the closest point on the ground truth model."}, {"title": "4.2. Implementation Details", "content": "Our experiments are conducted using a HPC cluster equipped with NVIDIA A100 SXM4 40GB GPUs. We use the BOP toolkit for evaluation, but measure latency ourselves due to instability in the toolkit's latency measurements. Among the 7 core datasets, we select LM-O for our ablation studies because it has fewer training instances but presents challenging scenarios, making it suitable for both computational and benchmarking purposes. We test all latency improvement methods on this dataset to inform our decision process on what to keep for our final model. Based on the performance improvements demonstrated by each method, we generalize to select the configuration that offers the best balance between speed and accuracy discussed in Section 4.7. As for data, we follow the approach of using PBR and REAL data as provided by the BOP challenge."}, {"title": "4.3. Backbone Selection", "content": "The backbones listed in Table 2 are used to estimate object poses in the LM-O dataset. All models are trained for 40 epochs with\nthe learning rate and hyper-parameters as reported for GDRNPP. The trained models are tested on the official BOP test data, which comprises 200 images with 1445 object instances across various scenes, all eight objects being visible in almost every image. Figure 6 shows AR results for different GDRNPP backbone variants.\nDespite FasterVIT variants having a much higher frame rate than ConvNext-B in ImageNet classification, their performance in pose estimation varies. The fastest variant, FasterVIT-0, is 7.39% faster than GDRNPP but suffers a 5.3% drop in AR. Other FasterVIT variants are slower and perform below the benchmark. EfficientNet variants show improved speed over ConvNext-B: EfficientNet-V2-BO is 40.53% faster, and EfficientNet-V2-S is 15.17% faster. However, their performance drops by 12.24% and 9.47%, respectively. ConvNext variants demonstrate promising performance, especially ConvNext-V2-P: The maximum recall score drop is a minor 5.24% while latency is reduce by 60%.\nSelection Criteria. Our goal is to achieve an inference rate as close as possible to 30 frames per second or above, giving us a time budget of approximately 33.33 ms per image. With GDRNPP requiring an off-the-shelf detector like YOLOX-X (for which we measured and inference time of 8 ms), we have 25.3 ms left for pose estimation. Therefore, we focus on backbones with latencies under 25 ms, narrowing our selection to ConvNext-V2-T, ConvNext-V2-N, and ConvNext-V2-P models. ConvNext-V2-N reduces latency by about 5 ms compared to ConvNext-V2-T, with only a 1.02% difference in AR scores. Given this trade-off, ConvNext-V2-N is chosen for its balanced performance and ConvNext-V2-P for its very low latency, allowing an extra 7-8 ms that might be invested in the optional pose refinement step. Also, keep in mind that inference time increases with the number of objects within a scene, so larger sets tend to require substantially more time."}, {"title": "4.4. Number of Regions", "content": "As described in Section 3.2, we evaluate the influence of the number of regions used with surface attention on run-time. The original GDRNPP achieves an AR of 68.6% when using 64 regions, with an inference time of 31.2ms. We test different numbers of regions and plot the results in Figure 7. As expected based on the GDR-Net paper, performance drops with lower number of regions (1.6% with only 8 regions). 16, 32, and 64 regions offer similar AR scores, while inference speed varies by 0.5ms between the choice of 16 and 64 regions. These numbers indicate that we can only expect limited gains, when decreasing the number of regions. Considering the ablation results of original GDR-Net, which show 64 regions to be the sweet spot, we decide not to alter from their choice and also use 64 regions with our faster versions."}, {"title": "4.5. Pruning", "content": "Having selected a faster backbone in Section 4.3, we now focus on the geometric head and the Patch PnP module. The experiments described in Section 3.3 yield the following observations: As expected and visualized in Figure 8, increasing the degree of pruning improves latency. The original model's geometric head has a latency of 3.8ms. Initial pruning improves latency by approximately 10%. For higher degrees of pruning, latency improves to about 1.89ms at D=28. The maximum degree of pruning in our setup is D=31, reducing latency by 57.36%. Interestingly, AR remains stable until D=28 as long as we fine-tune the pruned model. At the maximum pruning degree, the AR drops by only 6.7%, which is a small loss compared to the reduction in latency."}, {"title": "4.6. Distillation", "content": "We conduct the distillation process as described in Section 4.6 and compare the performance of the distilled student models to their non-distilled counterparts. The results from the experiment using different student backbones distilled from the GDRNPP model on the LM-O dataset are listed in Table 4. The four fastest backbones from Section 4.3 were chosen as the student backbones for this experiment. The original GDRNPP teacher model (ConvNext-B) achieves an Average Recall of 0.686. We see that every distilled model (w. Dist.) outperforms the model, which is trained directly\non the data (wo. Dist) and it is evident that distillation helps improve the model's score. For ConvNext-S and ConvNext-V2-T, the models perform on par with the original ConvNext-B model even without distillation. However, distillation helps these models match or slightly exceed the original model's performance. Overall distillation adds a slight performance increase to the smaller backbones, at the cost of requiring a trained base model."}, {"title": "4.7. Full Approach", "content": "Finally, we combine our learnings to create models for different run-time requirements. We choose two default configurations: The first opts to maximize inference speed and based on ConvNext-Pico (configuration P), the second tries to balance speed and the loss in accuracy and is based on ConvNext-Nano (configuration N).\nBased on our extensive analysis we have found the following methods to be the most promising: First, selection of a faster backbone, second backbone distillation, third pruning the geometric head and the Patch PnP module. We combine these into the following set of configurations: Models N and P are only replacing the backbone. 28 indicates that we use a pruned geometric head. -D shows that we use a distilled backbone. For all configurations, we additionally test the influence of fast depth refinement, indicated by -R. For comparison with the baseline, we add BM and BM-R representing the default configuration of GDRNPP. Results of the methods on LM-O are presented in Figure 11. Based on these re-"}, {"title": "4.7.1. Parameter Considerations", "content": "Table 5 shows the total number of learnable parameters for each configuration (without refinement) on the YCB-V dataset. The baseline model has 102.87M parameters. Switching to ConvNext-V2-Nano (N) and ConvNext-V2-Pico (P) reduces parameters by 71% and 78%, respectively. Pruning the geometric head by D=28 reduces approximately 5M parameters for both models. The pruned models N28 and P28 have a parameter reduction of 78.5% and 82.6%, respectively. Similar trends are observed across all datasets, with minor variations depending on the number of objects."}, {"title": "4.7.2. Real-time Considerations", "content": "As stated above, a typical camera framerate is 30 fps, giving a total of 33.33ms of processing time per frame. Looking at our results averaged across the seven datasets, all of different complexity, we see that only P28 and N28 stay below that budget. To illustrate the effect of different datasets, we report the results of both configurations per dataset in Table 6. We see vast differences in inference speed per dataset. Larger datasets such as T-LESS, ITODD, HB, and ICBIN tend to have more objects per image and much higher compute requirements. At the same time, our configurations move much closer towards real-time across all datasets, fullfilling the 30 fps target on LM-O, YCB-V, ITODD, and TUD-L. Our fastest configuration achieves 16.6 fps on the slowest dataset (ICBIN) compared to 2.35 ms with the original algorithm."}, {"title": "5. Conclusions and Future Work", "content": "We explored acceleration techniques for the 6D pose estimator GDRNPP. Surface region attention improved performance but not inference time, while smaller backbones enhanced speed with minimal performance loss. Knowledge distillation offered slight gains but requires the training of 2 individual models. The geometric head has a greater impact on inference time than the Patch PnP modules, leading us to reduce its parameters via pruning. We developed configurations that balance accuracy and speed, notably configuration P28, which has a 4.7% performance drop and an 82% latency improvement. Our findings move state-of-the-art 6D pose estimation a step closer towards real-time inference."}]}