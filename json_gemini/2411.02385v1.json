{"title": "HOW FAR IS VIDEO GENERATION FROM WORLD MODEL: A PHYSICAL LAW PERSPECTIVE", "authors": ["Bingyi Kang", "Yang Yue", "Rui Lu", "Zhijie Lin", "Yang Zhao", "Kaixin Wang", "Gao Huang", "Jiashi Feng"], "abstract": "OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give pre-dictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation mod-els to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit \"case-based\" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation models have emerged remarkable capabilities by scaling the model and data to an unprecedented scale. As an example, OpenAI's Sora not only generates high-fidelity and surreal videos, but also has sparked a new surge of interest in studying world models.\nWorld simulators are receiving broad attention from robotics and autonomous driving for the ability to generate realistic data and accurate simulations. These models are required to comprehend fundamental physical laws to produce data that extends beyond the training corpus and to guarantee precise simulation. However, it remains an open question whether video generation can discover such rules merely by observing videos, as Sora does. We aim to provide a systematic study to understand the critical role and limitation of scaling in physical law discovery.\nIt is challenging to determine whether a video model has learned a law instead of merely memorizing the data. Since the model's internal knowledge is inaccessible, we can only infer the model's understanding by examining its predictions on unseen scenarios, i.e., its generalization ability. We propose a categorization (Figure 1) for comprehensive evaluation based on the relationship between training and testing data in this paper. In-distribution (ID) generalization assumes that training and testing data are independent and identically distributed (i.i.d.). Out-of-distribution (OOD)"}, {"title": "2 DISCOVERING PHYSICS LAWS WITH VIDEO GENERATION", "content": ""}, {"title": "2.1 PROBLEM DEFINITION", "content": "In this section, we aim to establish the framework and define the concept of physical laws discovery in the context of video generation. In classical physics, laws are articulated through mathematical equations that predict future state and dynamics from initial conditions. In the realm of video-based observations, each frame represents a moment in time, and the prediction of physical laws corresponds to generating future frames conditioned on past states.\nConsider a physical procedure which involves several latent variables $z = (z_1, z_2, ..., z_k) \\in Z \\subset \\mathbb{R}^k$, each standing for a certain physical parameter such as velocity or position. By classical mechanics, these latent variables will evolve by differential equation $\\dot{z} = F(z)$. In discrete version, if time gap between two consecutive frames is $d$, then we have $z_{t+1} \\approx z_t + dF(z_t)$. Denote rendering function as $R(\\cdot) : Z \\rightarrow \\mathbb{R}^{3 \\times H \\times W}$ which render the state of the world into an image of shape $H \\times W$ with RGB channels. Consider a video $V = \\{I_1, I_2, ..., I_L \\}$ consisting of $L$ frames that follows the classical mechanics dynamics. The physical coherence requires that there exists a series of latent variables which satisfy following requirement: 1) $z_{t+1} = z_t + dF(z_t), t = 1,..., L-1$. 2) $I_t = R(z_t), t = 1,..., L$. We train a video generation model $p_\\theta$ parametried by $\\theta$, where $p_\\theta(I_1, I_2, ..., I_L)$ characterizes its understanding of video frames. We can predict the subsequent frames by sampling from $p_\\theta(I_{c+1},... I'_L | I_1, ..., I_c)$ based on initial frames' condition. The variable $c$ usually takes the value of 1 or 3 depends on tasks. Therefore, physical-coherence loss can be simply defined as $\\log p_\\theta(I_{c+1},..., I_L | I_1, ..., I_c)$. It measures how likely the predicted value will cater to the real world development. The model must understand the underlying physical process to accurately forecast subsequent frames, which we can quantatively evaluate whether video generation model correctly discover and simulate the physical laws."}, {"title": "2.2 VIDEO GENERATION MODEL", "content": "Following Sora, we adopt the Variational Auto-Encoder (VAE) and DiT architectures for video generation. The VAE compresses videos into latent representations both spatially and temporally, while the DiT models the denoising process. This approach demonstrates strong scalability and achieves promising results in generating high-quality videos.\nVAE Model. We employ a (2+1)D-VAE to project videos into a latent space. Starting with the SD1.5-VAE structure, we extend it into a spatiotemporal autoencoder using 3D blocks. All parameters of the (2+1)D-VAE are pretrained on high-quality image and video data to maintain strong appearance modeling while enabling motion modeling. More details are provided in Appendix A.3.1. In this paper, we fix the pretrained VAE encoder and use it as a video compressor. Results in Appendix A.3.2 confirm the VAE's ability to accurately encode and decode the physical event videos. This allows us to focus solely on training the diffusion model to learn the physical laws.\nDiffusion model. Given the compressed latent representation from the VAE model, we flatten it into a sequence of spacetime patches, as transformer tokens. Notably, self-attention is applied to the entire spatio-temporal sequence of video tokens, without distinguishing between spatial and temporal dimensions. For positional embedding, a 3D variant of ROPE is adopted. As stated in Sec.2.1, our video model is conditioned on the first $c$ frames. The $c$-frame video is zero-padded to the same length as the full physical video. We also introduce a binary mask \"video\" by setting the value of the first $c$ frames to 1, indicating those frames are the condition inputs. The noise, condition and mask videos are concatenated along the channel dimension to form the final input to the model."}, {"title": "2.3 ON THE VERIFICATION OF LEARNED LAWS", "content": "Suppose we have a video generation model learned based on the above formulation. How do we determine if the underlying physical law has been discovered? A well-established law describes the behavior of the natural world, e.g., how objects move and interact. Therefore, a video model incorporating true physical laws should be able to withstand experimental verification, producing reasonable predictions under any circumstances, which demonstrates the model's generalization ability. To comprehensively evaluate this, we consider the following categorization of generalization (see Figure 1) within the scope of this paper: 1) In-distribution (ID) generalization describes the setting where training data and testing data are from the same distribution. In our case, both training and testing data follow the same law and are located in the same domain. 2) A human who has learned a physical law can easily extrapolate to scenarios that have never been observed before. This ability is referred to as out-of-distribution (OOD) generalization. Although it sounds challenging, this evaluation is necessary as it indicates whether a model can learn principled rules from data. 3) Moreover, there is a situation between ID and OOD, which has more practical value. We call this combinatorial generalization, representing scenarios where every \"concept\" or object has been observed during training, but not their every combination. It examines a model's ability to effectively combine relevant information from past experiences in novel ways. A similar concept has been explored in LLMS, which demonstrated that models can excel at linguistic instructing tasks by recombining previously learned components, without task-specific experience."}, {"title": "3 IN-DISTRIBUTION AND OUT-OF-DISTRIBUTION GENERALIZATION", "content": "In this section, we study how in-distribution and out-of-distribution generalization is correlated with model or data scaling. We focus on deterministic tasks governed by basic kinematic equations, as they allow clear definitions of ID/OOD and straightforward quantitative error evaluation."}, {"title": "3.1 FUNDAMENTAL PHYSICAL SCENARIOS", "content": "Specifically, we consider three physical scenarios illustrated in Fig. 2. 1) Uniform Linear Motion: A colored ball moves horizontally with a constant velocity. This is used to illustrate the law of Intertia. 2) Perfectly Elastic Collision: Two balls with different sizes and speeds move horizontally toward each other and collide. The underlying physical law is the conservation of energy and momentum. 3) Parabolic Motion: A ball with a initial horizontal velocity falls due to gravity. This represents Newton's second law of motion. Each motion is determined by its initial frames.\nTraining data generation. We use Box2D to simulate kinematic states for various scenarios and render them as videos, with each scenario having 2-4 degrees of freedom (DoF), such as the balls' initial velocity and mass. An in-distribution range is defined for each DoF. We generate training datasets of 30K, 300K, and 3M videos by uniformly sampling a high-dimensional grid within these ranges. All balls have the same density, so their mass is inferred from their size. Gravitational acceleration is constant in parabolic motion for consistency. Initial ball positions are randomly initialized within the visible range. Further details are provided in Appendix A.4.\nTest data generation. We evaluate the trained model using both ID and OOD data. For ID evaluation, we sample from the same grid used during training, ensuring that no specific data point is part of the training set. OOD evaluation videos are generated with initial radius and velocity values outside the training range. There are various types of OOD setting, e.g. velocity/radius-only or both OOD. Details are provided in Appendix A.4.\nModels. For each scenario, we train models of varying sizes from scratch, as shown in Table 1. This ensures that the outcomes are not influenced by uncontrollable pretrain data. The first three frames are provided as conditioning, which is sufficient to infer the velocity of the balls and predict the subsequent frames. Diffusion model is trained for 100K steps using 32 Nvidia A100 GPUs with a batch size of 256, which was sufficient for convergence, as a model trained for 300K steps achieves a similar performance. We keep the pretrained VAE fixed. Each video consists of 32 frames with a resolution of 128x128. We also experimented with a 256x256 resolution, which yielded a similar generalization error but significantly slowed down the training process.\nEvaluation metrics. We observed that the learned models are able to generate balls with consistent shapes. To obtain the center positions of the i-th ball in the generated videos, $x_t$, we use a heuristic"}, {"title": "3.2 MAIN RESULT OF SCALING DATA AND MODEL", "content": "For in-distribution (ID) generalization in Figure 3, increasing the model size (DiT-S to DiT-L) or the data amount (30K to 3M) consistently decreases the velocity error across all three tasks, strongly evidencing the importance of scaling for ID generalization. Take the uniform motion task as an example: the DiT-S model has a velocity error of 0.022 with 30K data, while DiT-L achieves an error of 0.012 with 3M data, very close to the error of 0.010 obtained with ground truth video.\nHowever, the results differ significantly for out-of-distribution (OOD) predictions. First, OOD velocity errors are an order of magnitude higher than ID errors in all settings. For example, the OOD error for the DiT-L model on uniform motion with 3M data is 0.427, while the ID error is just 0.012. Second, scaling up the training data and model size has little or negative impact on reducing this prediction error. The variation in velocity error is higly random as data or model size changes, e.g., the error for DiT-B on uniform motion is 0.433, 0.328 and 0.358, with data amounts of 30K, 300K and 3M. We also trained DiT-XL on the uniform motion 3M dataset but observed no improvement in OOD generalization. As a result, we did not pursue training of DiT-XL on other scenarios or datasets constrained by resources. These findings suggest the inability of scaling to perform reasoning in OOD scenarios. The sharp difference between ID and OOD settings further motivates us to study the generalization mechanism of video generation in Section 5.2."}, {"title": "4 COMBINATORIAL GENERALIZATION", "content": "In Section 3, video generation models failed to reason in OOD scenarios. This is understand-able-deriving precise physical laws from data is difficult for both humans and models. For example, it took scientists centuries to formulate Newton's three laws of motion. However, even a child can in-tuitively predict outcomes in everyday situations by combining elements from past experiences. This ability to combine known information to predict new scenarios is called combinatorial generalization. In this section, we evaluate the combinatorial abilities of diffusion-based video models."}, {"title": "4.1 \u0421\u043e\u043cBINATORIAL PHYSICAL SCENARIOS", "content": "We selected the PHYRE simulator as our testbed\u2014a 2D environment involves multiple objects to free fall then collide with each other, forming complex physical interactions. It features diverse object types, including balls, jars, bars, and walls, which can be either fixed or dynamic. This enables complex interactions such as collisions, parabolic trajectories, rotations, and friction to occur simultaneously within a video. Despite this complexity, the underlying physical laws are deterministic, allowing the model to learn the laws and predict unseen scenarios.\nTraining Data. There are eight types of objects considered, including two dynamic gray balls, a group of fixed black balls, a fixed black bar, a dynamic bar, a group of dynamic standing bars, a dynamic jar, and a dynamic standing stick. Each task contains one red ball and four randomly chonsen objects from the eight types, resulting in $C_8^4 = 70$ unique templates. See Figure 4 for examples.\nEach template was initialized with random sizes and positions for four objects, generating 100K videos to cover a range of possible scenarios. To explore the model's combinatorial ability and scaling effects, we structured the training data at three levels: a minimal set of 6 templates (0.6M videos) that includes all types of two-object interactions among the eight object types, and larger sets with 30 and 60 templates (3M/6M videos), with the 60-template set nearly covering the entire template space. The mini-mal training set places the highest demand on the model's ability for compositional generalization.\nTest Data. For each training template, we reserve a small set of videos to create the in-template evaluation set. Additionally, 10 unused templates are reserved for the out-of-template evaluation set to assess the model's ability to generalize to new combinations not seen during training.\nModels. The first frame is used as the conditioning for video generation since the inital objects are static. We found that smaller models like DiT-S struggled with complex videos, so we primarily used DiT-B and DiT-XL. All models were trained for long 1000K gradient steps on 64 Nvidia A100 GPUs with a batch size of 256, ensuring near convergence. To better capture the complexity of physical events, we increased the resolution to 256x256 with 32 frames.\nEvaluation Metrics. We use several metrics to assess the fidelity of generated videos compared to the ground truth. Frechet Video Distance (FVD) calculates feature distances between generated and real videos using features from Inflated-3D ConvNets (I3D) pretrained on Kinetics-400. SSIM and PSNR are pixel-level metrics: SSIM evaluates brightness, contrast, and structural similarity, while PSNR measures the ratio between peak signal and mean squared error, both averaged across frames. LPIPS gauges perceptual similarity between image patches. We include human evaluations, reporting the abnormal ratio of generated videos that violate physical laws assessed by humans."}, {"title": "4.2 MAIN RESULTS", "content": "It requires higher resolution, much more training iterations, and larger model sizes to perform well on this task due to increased complexity. Consequently, we are unable to conduct a comprehensive sweep of all data and model size combinations as in Section 3. Therefore, we start with the largest model, DiT-XL, to study data scaling behavior for combinatorial generalization. As shown in Table 2, when the number of templates increases from 6 to 60, all metrics improve on the out-of-template testing sets. Notably, the abnormal rate for human evaluation significantly reduces from 67% to 10%. Conversely, the model trained with 6 templates achieves the best SSIM, PSNR, and LPIPS scores on the in-template testing set. This can be explained by the fact that each training example in the 6-template set is exposed ten times more frequently than those in the 60-template set, allowing it to better fit the in-template tasks associated with template 6. Furthermore, we conducted an additional experiment using a DiT-B model on the full 60 templates to verify the importance of model scaling. As expected, the abnormal rate increases to 24%. These results suggest that both model capacity and coverage of the combination space are crucial for combinatorial generalization. This insight implies that scaling laws for video generation should focus on increasing combination diversity, rather than merely scaling up data volume. The video generation visualizations of our models can be found in Figure 17 and Figure 18."}, {"title": "5 DEEPER ANALYSIS", "content": "In this section, we aim to investigate the generalization mechanism of a video generation model, through systemic experimental designs. Based on the findings, we try to identify certain patterns in combinatorial generalization that might be helpful in harnessing or prompting the models."}, {"title": "5.1 UNDERSTANDING GENERALIZATION FROM INTERPOLATION AND EXTRAPOLATION", "content": "The generalization ability of a model roots from its interpolation and extrapolation capability. In this section, we design experiments to explore the limits of these abilities for a video generation model. We design datasets which delibrately leave out some latent values, i.e. velocity. After training, we test model's prediction on both seen and unseen scenarios. We mainly focus on uniform motion and collision processes.\nUniform Motion. We create a series of training sets, where a certain range of velocity is absent. Each set contains 200K videos to ensure fairness. As shown in Figure 5 (1)-(2), with a large gap in the training set, the model tends to generate videos where the velocity is either high or low to resemble training data when initial frames show middle-range velocities. We find video generation model's OOD accuracy is closely related to the size of gap, as seen in Figure 5 (3), when the gap is reduced, the model correctly interpolates for most of OOD data. Moreover, as shown in Figure 5 (4) and (5), when a subset of the missing range is reintroduced (without increasing data amount), the model exhibits stronger interpolation abilities.\nCollision involves multiple variables, which is more challenging since the model has to learn a two-dimensional non-linear function. Specifically, we exclude one or more square regions from the training set of initial velocities for two balls and then assess the velocity prediction error after the collision. For each velocity point, we sample a grid of radius parameters to generate multiple video cases and compute the average error. As shown in Figure 6 (1)-(2), an interesting phenomenon happens. The video generation model's extrapolation error demonstate an intringuing discrepancy among OOD points: For the OOD velocity combinations that lie within the convex hull of the training set, i.e., the internal red squares in the yellow region, the model generalizes well. However, the model experiences large errors when the latent values lies in exterior space of training set's convex hull."}, {"title": "5.2 \u039c\u0395\u039cORIZATION OR GENERALIZATION", "content": "Previous work indicates that LLMs rely on memorization, reproducing training cases during inference instead of learning the underlying rules for tasks like addition arithmetic. In this section, we investigate whether video generation models display similar behavior, memorizing data rather than understanding physical laws, which limits their generalization to unseen data.\nWe train our model on uniform motion videos with velocities $v \\in [2.5, 4.0]$, using the first three frames as input conditions. Two training sets are used: Set-1 only contains balls moving from left to right, while Set-2 includes movement in both direction, by using horizontal flipping at training time. At evaluation, we focus on low-speed balls ($v \\in [1.0, 2.5]$), which were not present in the training data. As shown in Figure 7, the Set-1 model generates videos with only positive velocities, biased toward the high-speed range. In contrast, the Set-2 model occasionally produces videos with negative velocities, as highlighted by the green circle. For instance, a low-speed ball moving from left to right may suddenly reverse direction after its condition frames. This could occur since the model identifies reversed training videos as the closest match for low-speed balls. This distinction between the two models suggests that the video generation model is in-fluenced by \"deceptive\u201d examples in the training data. Rather than abstracting universal rules, the model ap-pears to rely on memorization, and case-based imitation for OOD generalization."}, {"title": "5.3 How DOES DIFFUSION MODEL RETRIEVE DATA?", "content": "We aim to investigate the ways a video model performs case matching\u2014identifying close training examples for a given test input. We use uniform linear motion for this study. Specifically, we compare four attributes, i.e., color, shape, size, and velocity, in a pairwise manner. Through comparisons, we seek to determine the model's preference for relying on specific attributes in case matching.\nEvery attribute has two disjoint sets of values. For each pair of attributes, there are four types of combinations. We use two combinations for training and the remaining two for testing. For example, we compare color and shape in Figure 8 (1). Videos of red balls and blue squares with the same range of size and velocity are used for training. At test time, a blue ball changes shape into a square immediately after the condition frames, while a red square transforms into a ball. We observed no exceptions on 1,400 test cases, showing that the model prioritizes color over shape for case matching. A similar trend is observed in the comparisons of size vs. shape and velocity vs. shape, as illustrated in Figure 8 (2)-(3), indicating that shape is the least prioritized attribute. This suggests that diffusion-based video models inherently favor other attributes over shape, which may explain why current open-set video generation models usually struggle with shape preservation.\nThe other three pairs are presented in Figure 9. For velocity vs. size, the combinatorial generalization performance is surprisingly good. The model effectively maintains the initial size and velocity for most test cases beyond the training distribution. However, a slight preference for size over velocity is noted, particularly with extreme radius and velocity values (top left and bottom right in Figure 9 (1))."}, {"title": "5.4 How DOES COMPLEX COMBINATORIAL GENERALIZATION HAPPEN?", "content": "In Section 4, we show that scaling the data coverage can boost combinatorial generalization. But what kind of data can actually enable conceptually-combinable video generation? In this section, we idenfify three foundamental combinatorial patterns through experimental design.\nAttribute composition. As shown in Figure 9 (1)-(2), certain attribute pairs\u2014such as velocity and size, or color and size-exhibit some degree of combinatorial generalization.\nSpatial composition. As given by Figure 11 (left side) in the appendix, the training data contains two distinct types of physical events. One type involves a blue square moving horizontally with a constant velocity while a red ball remains stationary. In contrast, the other type depicts a red ball moving toward and then bouncing off a wall while the blue square remains stationary. At test time, when the red ball and the blue square are moving simultaneously, the learned model is able to generate the scenario where the red ball bounces off the wall while the blue square continues its uniform motion.\nTemporal combination. As illustrated on the right side of Figure 11, when the training data includes distinct physical events\u2014half featuring two balls colliding without bouncing and the other half showing a red ball bouncing off a wall\u2014the model learns to combine these events temporally. Consequently, during evaluation, when the balls collide near the wall, the model accurately predicts the collision and then determines that the blue ball will rebound off the wall with unchanged velocity.\nWith these attribute, spatial, and temporal combinatorial patterns, the video generation model can identify basic physical events in the training set and combine them across attributes, time, and space to generate videos featuring complex chains of physical events."}, {"title": "5.5 IS VIDEO SUFFICIENT FOR COMPLETE PHYSICS MODELING?", "content": "For a video generation model to function as a world model, the visual representation must provide sufficient information for complete physics modeling. In our experiments, we found that visual ambi-guity leads to significant inaccuracies in fine-grained physics modeling. For example, in Figure 10, it is difficult to determine if a ball can pass through a gap based on vision alone when the size difference is at the pixel level, leading to visually plausible but incorrect results. Similarly, visual ambiguity in a ball's horizontal position relative to a block can result in different outcomes. These findings suggest that relying solely on visual representations, may be inadequate for accurate physics modeling."}, {"title": "6 RELATED WORKS", "content": "Video generation. Open-set video generation is mostly based on diffusion models or auto-regressive models. These models often require a pretrained image or video VAE for data compression to improve computational efficiency. Some approaches leverage pretrained Text-to-Image (T2I) models for zero-shot or few-shot video generation. Additionally, Image-to-Video(I2V) generation shows that video quality improves substantially when conditioned on an image. The Diffusion Transformer (DiT) demonstrates better scaling behavior than U-Net for T2I generation. Sora leverages the DiT architecture to directly operate on spacetime patches of video and image latent codes. Our model follows Sora's architecture and conceptually aligns with I2V generation, relying on image(s) for conditioning instead of text prompts.\nWorld model. World models aim to learn models that can accurately predict how an environment evolves after some actions are taken. Previously, they often operated in an abstracted space and were used in reinforcement learning to enable planning or facilitate policy learning through virtual interactions. With the advancement of generative models, world models can now directly work with visual observations by employing a general framework of conditioned video generation. For example, in autonomous driving, the condition is the driver's operations, while in robot world models , the condition is often the control signals. Genie instead recovers the conditions from video games in an unsupervised learning manner. In our physical law discovery setting, it does not require a per-step action/condition since the physical event is determined by the underlying laws once an initial state is specified. See more related works in Appendix A.1."}, {"title": "7 CONCLUSION", "content": "Video generation is believed as a promising way towards scalable world models. However, its capability to learn physical laws from visual observations has not yet been verified. We conducted the first systematic study in this area by examining its generalization performance across three typical scenarios: in-distribution, out-of-distribution (OOD), and combinatorial generalization. The findings indicate that scaling alone cannot address the OOD problem, although it does enhance performance in other scenarios. Our in-depth analysis suggests that video model generalization relies more on referencing similar training examples rather than learning universal rules. We observed a prioritization order of color > size > velocity > shape in this \"case-based\" behavior. In conclusion, our study suggests that naively scaling is insufficient for video generation models to discover fundamental physical laws."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 MORE RELATED WORKS", "content": "Physical reasoning. It refers to the ability to understand and predict the way objects will interact under certain conditions according to physical laws. Melnik et al. (2023) categorize physical reasoning tasks into passive and interactive tasks. Passive tasks often require the AI to predict certain properties of objects, e.g., materials, physical parameters, stability of a physical system or involve question-answering for the agent to recognize conceptual differences, or describe a physical scenario. For interactive tasks, AI is required to control some objects in the environment to complete certain tasks based on physical commonsense, e.g., solving classical mechanics puzzles, flying a bird to reach a target position, and using a tool. Two works closely related to ours are Girdhar et al. (2020) and de Silva et al. (2020). Girdhar et al. (2020) introduce a forward prediction model to aid physical reasoning but do not address what is learned in the prediction model. de Silva et al. (2020) attempt to discover universal physical laws from data with abstracted internal states and human expertise introduced in the dynamic model design. In contrast, we focus on recovering physical laws from raw observation without any human priors, akin to a newborn baby."}, {"title": "A.2 DIFFUSION PRELIMINARIES", "content": "Let $p(x)$ be the real data distribution. Diffusion models learn the data distribution by denoising samples from a noise distribution step-by-step. In this paper, we use the Gaussian diffusion models, where the video $V$ is progressively corrupted by gaussian noise $\\epsilon \\sim \\mathcal{N}(0, I)$ during the forward process, denoted by\n$V_t = \\alpha_tV + \\beta_t\\epsilon, \\qquad(1)$\nwhere $\\alpha_t, \\beta_t$ are the time-dependent noise scheduler. We use the original DDPM formulation , where $\\alpha_t = \\sqrt{\\bar{\\gamma}_t}, \\beta_t = \\sqrt{1 - \\bar{\\gamma}_t}, \\bar{\\gamma}_t$ is a monotonically decreasing scheduler from 1 to 0. The diffusion models are trained to reverse the forward corruptions, denoted by\n$\\mathbb{E}_{V \\sim p(x), t \\sim \\mathcal{U}(0, 1), \\epsilon \\sim \\mathcal{N}(0, 1)} [||y - p_\\theta(V_t, c, t)||^2], \\qquad(2)$"}, {"title": "A.3 VAE", "content": ""}, {"title": "A.3.1 VAE ARCHITECTURE AND PRETRAIN", "content": "We commence with the structure of the SD1.5-VAE, retaining the majority of the original 2D convolution, group normalization, and attention mechanisms on the spatial dimensions. To inflate this structure into a spatial-temporal auto-encoder, we convert the final few 2D downsample blocks of the encoder and the initial few 2D upsample blocks of the decoder into 3D ones, and employ multiple extra 1D layers to enhance temporal modeling. For all the downsample blocks where temporal downsampling is required, we replace all the original 2D downsample layers with re-initialized causal 3D downsample layers by adding causal paddings to the head of the frame sequence and introduce an additional causal 1D convolution layer after the original ResNetBlock. As for the decoder part, all the 2D Nearest-Interpolation operations are substituted by a 2D convolution layer and a channel-to-space transformation, which are specifically initialized to behave precisely the same as Nearest-Interpolation operations before the first training step. For the discriminator part, we inherit the structure of the original 2D PatchGAN discriminator used by SD1.5-VAE, and design a 3D PatchGAN discriminator based on the 2D version. Different from the generator module, we train the group of discriminators from scratch for the consideration of stability. Subsequently, all parameters of the (2+1)D-VAE are jointly trained with high-quality image and video data to preserve the capability of appearance modeling and to enable motion modeling. For the image dataset, we filter data samples from LAION-Aesthetics , COYO and DataComp with high aesthetics and clarity to form a high-quality subset. As for the video dataset, we collect a high-quality subset from Vimeo-90K, Panda-70M and HDVG. In the training process, we train the entire structure for 1M steps and only the random resized crop and random horizontal flip are applied in the data augmentation process."}, {"title": "A.3.2 VAE RECONSTRUCTION", "content": "In this paper, we fix the pretrained VAE encoder and use it as a video compressor. To verify the VAE's ability to accurately encode and decode the physical event videos, we evaluate its reconstruction performance. Specifically, we use the VAE to encode and decode (i.e., reconstruct) the ground truth videos and calculate the reconstruction error, $e_{recon}$. We then compare this error to the ground truth error, $e_{gt}$, as shown in Table 3. The results show that $e_{recon}$ is very close to $e_{gt}$, and both are an order of magnitude lower than the OOD error, as illustrated in Figure 3. It confirms the pretrained VAE's ability to accurately encode and decode the physical event videos used in this paper."}, {"title": "A.4 FUNDAMENTAL PHYSICAL SCENARIOS DATA", "content": "For the Box2D simulator, we initialize the world as a $10 \\times 10$ grid, with a timestep of 0.1 seconds, resulting in a total time span of 3.2 seconds (32 frames). For all scenarios, we set the radius $r \\in [0.7, 1.5]$ and velocity $v \\in [1, 4]$ as in-distribution (in-dist) ranges. Out-of-distribution (OOD) ranges are defined as $r \\in [0.3, 0.6] \\cup [1.5, 2.0]$ and $v \\in [0, 0.8] \\cup [4.5, 6.0]$."}, {"title": "A.5 MORE EXPERIMENTS AND DISCUSSIONS", "content": ""}, {"title": "A.5.1 CAN LANGUAGE AND NUMERICS AID IN LEARNING PHYSICAL LAWS?", "content": "As discussed in Section 3", "question": "can additional multimodal inputs", "variants": ""}]}