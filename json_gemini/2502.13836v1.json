{"title": "Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models", "authors": ["Peter Carragher", "Abhinand Jha", "R Raghav", "Kathleen M. Carley"], "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the-art on closed-domain tasks, general-purpose models like GPT-40 exhibit strong zero-shot performance. This raises questions about the trade-offs between memorization, generalization, and retrieval. In this work, we analyze the extent to which multimodal retrieval-augmented VLMs memorize training data compared to baseline VLMs. Using the WebQA benchmark, we contrast finetuned models with baseline VLMs on multihop retrieval and question answering, examining the impact of finetuning on data memorization. To quantify memorization in end-to-end retrieval and QA systems, we propose several proxy metrics by investigating instances where QA succeeds despite retrieval failing. Our results reveal the extent to which finetuned models rely on memorization. In contrast, retrieval-augmented VLMs have lower memorization scores, at the cost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a challenge for future work to reconcile memorization and generalization in both Open-Domain QA and joint Retrieval-QA tasks.", "sections": [{"title": "1 Introduction", "content": "The increasing reliance on LLMs for multimodal tasks across far-reaching sectors such as healthcare, finance, and manufacturing underscores the need to assess the accuracy and reliability of the information they generate. Vision-Language Models (VLM) have achieved state-of-the-art (SoTA) performance on Visual Question-Answering (VQA) benchmarks, and these models often utilize Retrieval-Augmented Generation (RAG) to maintain factual accuracy and relevance in a dynamic information environment. However, this has led to uncertainty in the information the LLM bases its answer on, as it may choose between parametric memory and retrieved sources. When models rely on memorized information instead of dynamically retrieving information, they may inadvertently propagate outdated or incorrect information, causing serious legal and ethical risks and undermining trust and reliability in AI systems (Huang et al., 2023).\nDespite these concerns, the way that Vision-Language models (VLMs) memorize and retrieve information, particularly in complex multimodal tasks, remains under-explored. Current research often focuses on either the general capabilities of large language models (LLMs) or the specialized retrieval mechanisms in retrieval augmented generation systems (RAG) (Ram et al., 2023; Chen et al., 2022b; Liu et al., 2023). Particularly in the context of multimodal retrieval and multihop reasoning, few studies analyze the tradeoff between finetuning for specialized tasks and zero-shot prompting for general-purpose vision-language capabilities. A lack of consensus on how to approach this tradeoff motivates the development of measures to quantify reliance on parametric memory, as well as metrics for quantifying the potential performance impact of extending LLMs with RAG systems.\nTo address this gap, we investigate how multimodal QA models balance accuracy with memorization on the WebQA benchmark. We compare finetuned multimodal systems against zero-shot VLMs, analyzing how retrieval performance influences QA accuracy. In particular, we focus on cases where retrieval fails, allowing us to measure reliance on parametric memory through two proposed metrics\u2014the Parametric Proxy Rate (PPR) which quantifies how much model accuracy is influenced by retrieval quality, contrasting performance in best-case versus worst-case retrieval"}, {"title": "2 Related Work", "content": "A large body of work on multimodal representations exists (Liu et al., 2022b; Chen et al., 2022b; Radford et al., 2021). CLIP enables the embeddings of text and images into aligned representations by supervised training over image-caption datasets (Radford et al., 2021). More sophisticated local alignment methods between captions and images using Graph Optimal Transport (GoT) have been proposed (Chen et al., 2020; Petric Maretic et al., 2019). The Universal Vision-Language Dense Retrieval model (UniVL-DR) showed SOTA performance on the WebQA retrieval task (Liu et al., 2022b) by using hard-negative sampling for constrastive learning. In this work, we compare UniVL-DR and CLIP embeddings as competing retrieval systems."}, {"title": "2.2 Multihop Language Models", "content": "A wealth of research exists on multimodal Vision-Language tasks and multihop language decoders. (Tanaka et al., 2023) propose a Fusion-in-Decoder (FiD) architecture for multihop reasoning over images. Utilizing advances in local alignment (Chen et al., 2020), VOLTA model combines graph representations of input questions and source images (Pramanick et al., 2022). For compatibility with retriever modules, we extend VoLTA with support for a variable number of input sequences.\nMore recently, the increasing context windows of VLMs enables them to demonstrate multihop reasoning abilities (Liu et al., 2024; Abdin et al., 2024; Wang et al., 2024). Recent work has found that not only are LLMs capable of determining when they should forgo their parametric memory and use a retriever module (Labruna et al., 2024), they are also capable of \"In-context Retrieval\" (Ram et al., 2023). Here, retrieved sources are used for grounded text generation by simply prepending the sources into the input prompt. We expand upon this idea, adapting it to a multimodal setting with VLMs, and report our findings."}, {"title": "2.3 The Parametric Effect", "content": "There is a wealth of research on reliance on parametric memory for unimodal retrieval and QA tasks (Galway et al., 2024; Xu et al., 2024; Longpre et al., 2022; Neeman et al., 2022; Hong et al., 2024; Chen et al., 2022a). Here, the entity replacement framework (Longpre et al., 2022; Neeman et al., 2022) is used to invalidate parametric memory by explicitly crafting knowledge conflicts between input sources and parametric memory (Xu et al., 2024; Hong et al., 2024; Chen et al., 2022a). As such, these studies guarantee that manipulated input sources no longer entail the expected labels. In contrast, we do not make the same guarantees, and our proxy measures are premised upon the key assumption that incorrectly retrieved sources do not entail the correct answer. Our focus is on developing proxy metrics for the parametric effect that do not require such involved source manipulation processes, so extending the entity replacement framework to the multimodal setting is left as the subject of future work."}, {"title": "3 Datasets", "content": "WebQA The WebQA dataset (Chang et al., 2022) is designed with a two-step approach in mind; retrieval followed by QA. First, given the question Q and all sources S, we retrieve the set of relevant sources, S'. Using these sources we then generate an answer A'. The following is passed to the QA classifier:\n < [CLS], s'%, [SEP], ..., s'n, [SEP], Q, [SEP] >\n(1)\nWe include only those questions that either one (n = 12,027) or two (n = 9,438) image sources. For a breakdown of question categories and their keywords, see subsection A.6 in the appendix.\nAs opposed to WebQA, open-domain VQA tasks such as OK-VQA (Marino et al., 2019) and HotpotQA (Yang et al., 2018) do not provide candidate sources S and source labels S*. As such, the memorization metrics proposed are incompatible with these tasks (see section 5). Moreover, while we do evaluate QA model performance on NLVR2 (Suhr et al., 2018) and VQAv2 (Goyal et al., 2017), these tasks do not have a retrieval step and so these results are instead presented in the appendix (see section A.1)."}, {"title": "4 Methodology", "content": "As WebQA is a joint retrieval and QA task, we develop several QA methods and retrieval methods separately. For the final end-to-end task, we evaluate the combination of the each retrieval method with the best-performing QA model, as determined during an initial QA model selection phase. This enables us to investigate the factors involved in both the model design and the model finetuning process that may influence how the parametric effect manifests itself."}, {"title": "4.1 Question Answering", "content": "Vision-Language Model For two-image questions, the WebQA finetuned VLP baseline (Zhou et al., 2020) takes as input the concatenation of both sources encodings with the query;\n< [CLS'], $1, $2, [SEP], Q, [SEP] >\n(2)\nAs such, it is an extension of VQA model trained on single-hop VQA-2 (Yu et al., 2023), which takes as input:\n< [CLS], s, [SEP], Q, [SEP] >\n(3)\nWe adopt this formulation for finetuning the Qwen2 VLM, using Low-Rank Adaptation (LoRA) to reduce trainable parameters (Hu et al., 2021). We use the same input formulation to evaluate zero-shot performance on GPT-40. In addition, we evaluate several baseline models from previous works, namely VLP (Zhou et al., 2020), GIT (Wang et al., 2022), GPT-3.5 (Brown et al., 2020), and BLIP-2 (Li et al., 2022). Details of these models are presented in appendix subsection A.7.\nMultihop Formulation We hypothesize that multihop tasks, such as WebQA, would benefit from a two-stage reasoning process. The first stage enables multimodal fusion between each input source and the question, and the second stage enables multihop fusion between the embedded multimodal representation of each source, conditioned on the question. Inspired by FiD architectures (Yu et al., 2022), this results in the following input construction:\nconcat(< [CLS], 81, [SEP], Q >,\n  < [CLS], S2, [SEP], Q >)\n(4)\nMultihop Classifier We select the VOLTA framework as the skeleton for encoding joint text and image representations (Pramanick et al., 2022). VOLTA uses Swin-Base (Liu et al., 2021) and ROBERTa-Base (Liu et al., 2019) as respective visual and textual encoders and we adopt the same encoder choices. Swin Transformer introduced a hierarchical Transformer design with a shifted windows scheme, which computes the self-attention layer over non-overlapping local windows and allows connection by shifting between windows. We jointly encode each image source returned by the retriever with the query and concatenate the resulting embeddings together before sending them to the MLP classifier to predict the keyword answer label. To handle variable input sequences during classification, we pad single image sources with blank images so that all inputs sent into the classifiers have two images. We call this model MultiHop-VOLTA (MH-VOLTA).\nWe finetune the models using the AdamW (Loshchilov and Hutter, 2019) optimizer with a learning rate of le-4 and a batch size of 32 samples. We use LoRA to reduce trainable parameters (Hu et al., 2021), and set r = 8 and a = 32 for the text encoder and r = 16 and a = 16 for the image encoder, updating only the attention weights. MH-VOLTA is trained until convergence (80 epochs, see Figure 5c in the appendix)."}, {"title": "4.2 Retrieval Methods", "content": "Dense Retrievers We adopt the pretrained UniVL-DR retriever for source retrieval in our finetuned experiments (Liu et al., 2023) and compare it with baseline CLIP (Radford et al., 2021) and WebQA finetuned CLIP (CLIP-DPR, (Liu et al., 2023)) embeddings. Specifically, we embed all text sources, image sources, and queries using UniVL-DR. For each query, we compute cosine similarity between the query and each of the sources, and use the top two ranked image sources and their captions as input to the QA model.\nGPT-40 Ranking We utilize GPT-40 to select sources from the set of distractor sources present in dataset using the prompt in the appendix subsection A.4. This is motivated by previous work in In-Context Retrieval Augmented Language Modeling (In-Context RALM) which demonstrated that LLMs are capable of reasoning over sources without finetuning (Ram et al., 2023).\nUpper and Lower Bounds In addition, to investigate the impact of the parametric effect on joint retrieval and QA performance, we also compare performance with a best and worst case retriever. The best case is the oracle retriever, using gold sources provided in the validation set, and the worst case is a random naive retriever, which returns random distractor sources (and so is always incorrect)."}, {"title": "5 Evaluation Metrics", "content": "We propose measures for evaluating the degree of memorization in QA models (Parametric Proxy Rate) and in end-to-end retrieval-QA systems (Unsupported Correctness Rate), as well as a metric for retriever-QA model compatibility (Retriever Potential Attainment)."}, {"title": "5.1 Unsupported Correctness Rate", "content": "We propose UCR, a metric to measure the parametric effect in the combined retrieval and QA model. It is formulated as a composition of QA accuracy and retrieval recall. Intuitively, it is the fraction of true positive predictions from the QA model for which there is no retrieval support (i.e. the retrieved sources were incorrect).\nRetrieval Recall and QA Accuracy The first stage of the joint task is retrieval, where the recall for retriever R is defined as the fraction of retrieved sources (positives) that are correct (true positives)\nRecallR = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n(5)\nAccuracy is the primary correctness metric for question answering in the WebQA task. Accuracy of model M is determined by comparing a restricted bag of words (bow) vector between the expected (E) and generated (G) answers;\n\u0410\u0441\u0441\u043c = \\frac{1}{n} \\sum [\\frac{|\\text{bow}_E \\cap \\text{bow}_G|}{|\\text{bow}_E|} = 1]\n(6)\nThe vocabulary of the vectors is restricted to a specific domain based on the question type; questions are labeled based on these domains which can be yes/no, color, shape, or number. Each category has a pre-defined vocabulary list, given in the appendix.\nUCR Using QA accuracy and retrieval recall, we construct UCR, a metric for measuring the parametric effect in a combined retrieval-QA model, which calculates the likelihood P(Q1|R0) that the QA model M returns a correct answer (Q1) given that the retrieval model R failed to return the correct sources (R0):\nUCR(R,M) = \\frac{\\text{Acc}_M == 1 \\cap \\text{Recall}_R = 0}{\\text{Recall}_R = 0} = P(Q_1 | R_0)\n(7)"}, {"title": "5.2 Oracle-Normalized Retrieval Scores", "content": "Using min-max scaling, we define two additional metrics to evaluate joint retrieval QA systems, by normalizing using the oracle retriever (upper bound) and random retriever (lower bound):\nX = \\frac{X - X_{min}}{X_{max} - X_{min}}\n(8)\nThese metrics are Retriever Potential Attainment (RPA, higher is better) and Parametric Proxy Rate (PPR, lower is better).\nRetriever Potential Attainment RPA quantifies the potential that a retriever has realized when used in a given end-to-end retrieval QA system. The upper bound (1) is given by same QA system's accuracy with oracle sources (Accm (oracle)). The lower bound (0) is given by the random negative source retriever (AccM(random)), which always retrieves incorrect sources. We apply random-oracle scaling, where AccM(R) denotes the accuracy of QA model M, given sources from retriever R:\nRPA(R,M) = \\frac{\\text{Acc}_M(R) \u2013 \\text{Acc}_M(random)}{\\text{Acc}_M(oracle) \u2013 \\text{Acc}_M(random)}\n(9)\nParametric Proxy Rate We postulate that the rate at which a model's performance increases when used in conjunction with increasingly accurate retrievers implies that it is using the retrieved sources effectively, instead of relying on parametric memory. To that end, we present PPR, an additional max scaling measure based off just the upper (oracle) and lower bound (randomized negatives) retrievers, which is simply their performance ratio with respect to QA model M:\nPPR(M) = \\frac{\\text{Acc}_M(random)}{\\text{Acc}_M(oracle)}\n(10)"}, {"title": "Question Complexity Analysis Metrics", "content": "To identify correlations between the complexity of the question, retrieval recall, and QA accuracy, we apply three separate measures to the input questions; Word Count, Flesch-Kincaid Grade Level (Flesch, 2007), and Gunning-Fog Index (Gunning, 1952)(subsection A.5)."}, {"title": "6 Results", "content": "First, we experiment with multiple QA models to determine which generalized LLMs and specialized, finetuned models should be selected for the joint retrieval and QA task. Then, we experiment with different combinations of retrieval systems and chosen QA models for the joint task. Finally, we analyze how performance on the retrieval task impacts QA accuracy and investigate the relationship between question complexity, performance, and model memorization."}, {"title": "6.1 Model Selection", "content": "We find that the MH-VOLTA model outperforms all baseline and zero-shot models on the WebQA validation set image questions, including BLIP-2, GIT, VLP, GPT-40, and GPT-3.5. We also find that MH-VOLTA performance is comparable to VoLTA on the (fixed input) VQA and NLVR2 tasks (section A.1 in the appendix). For a breakdown of model performance by question category on the WebQA dataset, see section A.3 in the appendix."}, {"title": "6.2 Impact of Retrieval on QA", "content": "To understand how retrieval and QA systems interact, we investigate the reliance of the QA task on retrieval correctness. We find that when GPT-40 correctly retrieves the relevant sources (retrieval recall is high), it has a 59% QA accuracy rate. Conversely, if GPT-40 does not retrieve the correct sources, the accuracy rate is reduced to 26% (Figure 1a). We also find that QA performance drops as the number of retrieved distractors increases and retrieval recall falls, showing that poor retrieval performance adversely affects QA (Figure 1b). This is to say that the QA task is heavily dependent on retrieval performance. However, there do exist correctly answered questions for which incorrect sources are retrieved, and these samples form the basis for the UCR measure of the parametric effect (Figure 1a)."}, {"title": "6.3 End-to-End Retrieval and QA", "content": "The PPR and RPA measures enable a quick comparison of joint retrieval and QA systems, where Figure 2 reveals some interesting trends. We find that of all QA models tested, GPT-40 benefits the most from the use of retrievers\u2014Retriever Potential"}, {"title": "6.4 Finetuning and UCR", "content": "We find that, while the finetuning process improves accuracy (and in part because of this fact), finetuning exacerbates the parametric effect. Qwen2-FT has a higher Parametric Proxy Rate than the baseline Qwen2 model (PPR, Figure 2), and it has a higher Unsupported Correctness Rate (UCR) than Qwen2 across all retrieval methods tested (Table 2). What's more, the act of finetuning Qwen2 has an outsized effect on UCR when compared with the effect that changing the retriever has. MH-VOLTA represents the extreme case; for each retriever R, UCR(R, MH-VoLTA) > 0.5, implying that MH-VOLTA is correctly answering the majority of questions for which the retrieval system fails to identify the 'correct' sources.\nHowever the effect of retrieval on UCR is not negligible, and we find that for a given QA model, UCR increases as retrieval recall increases; i.e. for each model M UCR(Rand, M) < UCR(Clip, M) < UCR(ClipDPR, M) (Table 2). This implies that as the retriever improves, the QA model is more successful on samples that retrieval fails on. This paradox is explained by inaccuracies in the source labels\u2014\u2018distractor' sources often provide enough evidence for the QA model to answer correctly. Rather than exposing memorization, this reveals an underlying issue with the source labels in the WebQA dataset, and as such, these measures can be adapted to evaluate the correctness of the joint retrieval-QA benchmarks."}, {"title": "6.5 Question Complexity", "content": "We observe and report interesting relationships between query complexity and retrieval and QA performance. We find that the accuracy of the in-context GPT-4o retriever is related to question complexity (Figure 3). The more complex the question in terms of word count, Flesch-Kincaid Grade, or Gunning Fog Index, the lower the QA performance (Figure 3). Conversely, increasing query complexity improves GPT-40's retrieval ability, where the additional complexity provides information on source relevancy 1. However, this relationship does not hold for the finetuned UniVL-DR retriever, where question complexity has little effect on retrieval recall or QA accuracy (Figure 4). As such, systems that rely on \"in-context\" retrieval using GPT-40 are limited by query complexity, but approaches that utilize finetuned retrievers are not."}, {"title": "7 Discussion", "content": "While QA performance is generally predicated upon retrieval success (Figure 1b), there are many cases where retrieval fails and QA succeeds (Figure 1a). These cases form the basis of our quantitative metrics, the Unsupported Correctness Rate (UCR; see Table 2) and Parametric Proxy Rate (PPR; see Figure 2), and with these measures we show that external retrievers significantly reduce the reliance of VLMs on parametric memory. This reduction in memorization not only preserves model flexibility but also mitigates the over-specialization common in finetuned systems. However, despite GPT-40 obtaining state-of-the-art performance on the WebQA benchmark using this approach (Table 1)2, for less powerful VLMs such as Qwen2 the decrease in memorization associated with not finetuning the model (PPR: 0.77 -> 0.53) comes at the cost of model accuracy (QA accuracy: 70% -> 52%).\nBuilding on previous research in retrieval-augmented generation and multimodal question answering, our study explicitly quantifies the balance between memorization and retrieval. Our findings reveal that in-context retrieval can effectively substitute for finetuning in multimodal QA tasks. However, this approach is currently limited by the complexity of the question and task (Figure 3). The strong performance of general-purpose VLMs when augmented with finetuned retrievers suggests that general-purpose VLMs are capable of reasoning over retrieved documents and images, without being affected by question complexity (Figure 4).\nThis analysis also reveals a paradoxical relationship between retriever recall and UCR that"}, {"title": "8 Conclusion", "content": "We demonstrate that retrieval-augmented VLMs have improved performance over general-purpose VLMs, with comparable memorization rates. However, there is still a substantial performance gap between finetuned and baseline QA models. By introducing UCR and PPR, we provide concrete measures of how retrieval mitigates memorization. This analysis outlines the interplay between parametric knowledge and external retrieval. Our measures are validated by the fact that they reveal this well-known tradeoff between memorization and generalization. Our work provides a foundation for future research aimed at refining retrieval mechanisms and ensuring that external sources effectively complement the parametric knowledge of VLMs."}, {"title": "9 Limitations", "content": "Our measures are based upon the assumption that incorrectly retrieved sources should result in incorrect answers from the VQA model. They are proxy measures premised upon the assumption that incorrectly retrieved sources do not entail the correct answer. Future work to build memorization metrics not subject to this assumption are warranted.\nGiven that there is a wealth of research on quantifying the parametric effect using unimodal QA benchmarks, we choose to look only at VQA. That being said, the evaluation metrics we present are general and are also relevant to the measurement"}, {"title": "A.1 Model Selection Results", "content": "We explore baseline methods for the QA task on the WebQA validation set. Table 3 gives results for the baseline models. The MH-VOLTA model outperforms all baseline and zero-shot models on the validation set image questions. However, the extension of the VOLTA model for variable input multi-hop tasks risks a regression in performance on traditional VQA tasks which have fixed-input where the number of input images is constant. To determine MH-VOLTA generalizes from fixed to variable input tasks, we compare performance between two variants of the original VOLTA model, finetuned on one and two image subsets of WebQA, with MH-VOLTA. We find that MH-VOLTA is capable of reasoning over both one and two-image image questions, and it's performance is on-par with VOLTA variants trained on one and two image sources separatelyTable 3. See subsection A.2 for more details on the one and two image VOLTA variants, as well as a breakdown of model performance by question category (Figure 5a). See subsection A.7 for a description of the baseline models used.\nVQAv2 and NLVR2 In addition to WebQA, we evaluate models on two fixed-input VQA datasets\u2014VQAv2 (Goyal et al., 2017), a multi-class, single-image VQA dataset, and (Suhr et al., 2018), a binary classification, two-image VQA dataset. These datasets are well-suited to VOLTA classifier architecture. In particular, question categories in VQAv2, along with the associated answer-domains, match well with WebQA, with a substantial portion of both datasets focusing on color, shape, number, and yes/no questions."}, {"title": "A.2 Multihop VoLTA on one vs two image sources", "content": "The results for finetuning VoLTA and MH-VOLTA on the WebQA dataset experiments are provided in Table 4. We explored the application of Multihop-VOLTA in addressing queries based on single images, questions involving two images, and a combination of both single and two-image queries (referred to as multiple images, Figure 5b).\nWe find that the variable Multihop-VoLTA model (Figure 5a) is en-par with the fixed-input one and two-image VoLTA model variants (Figure 5b). This underscores the stability of our finetuning approach"}, {"title": "A.3 Performance by Question Category", "content": "We report the mean accuracy per question category for Multihop-VoLTA in Figure 5a using source retrieval oracles. We find that performance is dependent upon the level of training data available, with the shape category having the least number of samples in the dataset. Question counts per category are as follows; Yes/No (n = 7,320), color (n = 1,830), number (n = 2,118), shape (n = 565). The similarity in results across different question categories reinforces the reliability and stability of our model's performance. For a breakdown of labels per question category, see subsection A.6."}, {"title": "A.4 GPT-40 Retrieval Prompt", "content": "system: Answer the question in one word. Then list the Fact_ID or Image_ID of all facts used to derive the answer in square brackets. human: Question: <query>"}, {"title": "A.5 Question Complexity Analysis Metrics", "content": "The Flesch-Kincaid Grade Level is a readability metric that evaluates the difficulty of a text based on the length of its words and sentences (Flesch, 2007), and is defined as;\nFKGL = 0.39(\\frac{\\text{Total Words}}{\\text{Total Sentences}}) +11.8 \u2013 15.59\n(\\frac{\\text{Total Syllables}}{\\text{Total Words}})\n(11)\nThe Gunning Fog Index is a readability test used in linguistics to assess the complexity of English writing (Gunning, 1952), and is defined as;\nGFI = 0.4 \\times \\frac{\\text{Total Words}}{\\text{Total Sentences}} + 40 \\times \\frac{\\text{Total Complex Words}}{\\text{Total Words}}\n(12)"}, {"title": "A.6 Question Category Domain Lists", "content": "yesno_set = {'yes', 'no' }\ncolor_set = {\n'orangebrown',\n'spot', 'yellow', 'blue', 'rainbow', 'ivory',\n'brown', 'gray',\n'teal', 'bluewhite', 'orangepurple', 'black',\n'white', 'gold',\n'redorange', 'pink', 'blonde', 'tan', 'turquoise',\n'grey', 'beige',\n'golden', 'orange', 'bronze', 'maroon', 'purple',\n'bluere', 'red', 'rust', 'violet', 'transparent', 'yes', 'silver',\n'chrome', 'green', 'aqua'\n}\nshape_set = {\n'globular', 'octogon', 'ring', 'hoop', 'octagon', 'concave', 'flat',\n'wavy', 'shamrock', 'cross', 'cylinder', 'cylindrical', 'pentagon',\n'point', 'pyramidal', 'crescent', 'rectangular', 'hook', 'tube',\n'cone', 'bell', 'spiral', 'ball', 'convex', 'square', 'arch', 'h',\n'cuboid', 'step', 'rectangle', 'dot', 'oval', 'circle', 'star',\n'crosse', 'crest', 'octagonal', 'cube', 'triangle', 'semicircle',\n'domeshape', 'obelisk', 'corkscrew', 'curve', 'circular', 'xs',\n'slope', 'pyramid', 'round', 'bow', 'straight', 'triangular',\n'heart', 'fork', 'teardrop', 'fold', 'curl', 'spherical',\n'diamond', 'keyhole', 'conical', 'dome', 'sphere', 'bellshaped',\n'rounded', 'hexagon', 'flower', 'globe', 'torus'\n}"}, {"title": "A.7 Baseline Models", "content": "VLP The VLP transformer model consists of a unified encoder and decoder (Zhou et al., 2020). The VLP architecture is made up of 12 layers of transformer blocks trained according to the BERT bidirectional and the seq2seq objectives where the self-attention module in the transformer block are defined as;\nA = \\text{softmax}(\\frac{Q^T K}{\\sqrt{d}} + M)V^T\n(13)\nwhere V = W_iH^{l-1}, Q = W_qH^{l-1}, K = W_kH^{l-1}. As in (Vaswani et al., 2017), a feed-forward layer (with residual) maps Al to Hl. The model is trained on image caption pairs, and then finetuned for the VQA task. Finetuning follows by taking the hidden states from the final layer and feeding them to a multi-layer perceptron. The model used has been finetuned twice, once on the VQA dataset (as described by (Yu et al., 2023)), and again on the WebQA dataset.\nGIT To contrast with VLP, a pretrained multihop VQA model, we use a pre-trained Generative Image-to-Text Transformer (GIT) (Wang et al., 2022). GIT employs a simplified VQA architecture with one encoder for images and one decoder for text. As such, the model is explicitly incapable for multihop VQA between text and images, so it serves as a baseline for pre-trained models that do not utilize image descriptions, and so we concatenate image sources if there are more than one.\nGIT is pre-trained using the language modeling task (as opposed to MLM which is used by VLP) where the model learns to predict captions in an auto-regressive manner. For VQA finetuning, the text input is swapped to the query, so that answers are predicted.\nBLIP-2 Similar to VLP, the Bootstrapping Language-Image Pre-training model (BLIP) is a unified vision language pre-trained model (Li et al., 2022). It relies on a visual transformer which is less"}]}