{"title": "Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models", "authors": ["Junfeng Tian", "Da Zheng", "Yang Chen", "Rui Wang", "Colin Zhang", "Debing Zhang"], "abstract": "Large language models (LLM) have prioritized expanding the context window from which models can incorporate more information. However, training models to handle long contexts presents significant challenges. These include the scarcity of high-quality natural long-context data, the potential for performance degradation on short-context tasks, and the reduced training efficiency associated with attention mechanisms. In this paper, we introduce Untie the Knots (UtK), a novel data augmentation strategy employed during the continue pre-training phase, designed to efficiently enable LLMs to gain long-context capabilities without the need to modify the existing data mixture. In particular, we chunk the documents, shuffle the chunks, and create a complex and knotted structure of long texts; LLMs are then trained to untie these knots and identify relevant segments within seemingly chaotic token sequences. This approach greatly improves the model's performance by accurately attending to relevant information in long context and the training efficiency is also largely increased. We conduct extensive experiments on models with 7B and 72B parameters, trained on 20 billion tokens, demonstrating that UtK achieves 75% and 84.5% accurracy on RULER at 128K context length, significantly outperforming other long context strategies. The trained models will open-source for further research.", "sections": [{"title": "1 Introduction", "content": "For the past few years, large language models (LLM) research has prioritized expanding the context window from which models can incorporate more information (Brown et al., 2020; Anthropic, 2023; OpenAI, 2023; Team et al., 2024). This emphasis stems from the recognition that a wider context window allows models to incorporate a larger amount of new, task-specific information not found in the training data at inference time, leading to improved performance in various natural language tasks (Caciularu et al., 2023; Bairi et al., 2023; Mazumder and Liu, 2024; Jiang et al., 2024; Gur et al., 2024).\nHowever, training transformer-based (Vaswani et al., 2017) models to handle long contexts effectively poses significant challenges due to the lower training efficiency and the quadratic computational cost of attention mechanisms in long-context models. As a result, many approaches treat long-context extension as a distinct stage. Training-free methods for length extropolation, such as those that modify Rotary Position Embedding (RoPE)(Su et al., 2021), often fail to deliver satisfactory performance. Continue pre-training approaches (Llama Team, 2024; ChatGLM, 2024; Gunter et al., 2024) aimed at improving long-context performance encounter a critical issue: the scarcity of sufficiently long training texts, which introduces a bias in training data. Texts ranging from 32K to 128K tokens are rare and typically consist of books and code. To miti-"}, {"title": "2 Related Work", "content": "2.1 Long Context in LLMs\nLong Document continue pre-training has become a crucial step in enhancing long-context capabilities in foundational models. Plenty of leading foundational models (Team et al., 2024; Llama Team, 2024; Yang et al., 2024; ChatGLM, 2024; Gunter et al., 2024) have emphasized the importance of ROPE's positional encoding and the upsampling of lengthy data. For example, models like LLaMA 3.1 (Llama Team, 2024) and Phi-3 (Abdin et al., 2024) leverage the Long RoPE method (Ding et al., 2024) to extend their context windows, while Qwen2 (Yang et al., 2024) utilizes the YARN and Dual Chunk Attention mechanisms (Peng et al., 2023; An et al., 2024) to increase the context length to 128k. Additionally, GLM Long (ChatGLM, 2024) and Apple's AFM (Gunter et al., 2024) scale the ROPE base frequency (Men et al., 2024) to improve generalization across varying sequence lengths.\nOne series of works manipulate the order of training tokens to achieve similar goals. For instance, UL2 (Tay et al., 2022) designs mixture of denoisers (MoD) objective to adapt the model to different tasks. FIM (Bavarian et al., 2022) applied data transformation by splitting documents into three random segments and rearranging them with sentinel tokens. FIM gives model ability to generate content conditioned on both prefix and suffix, which is essential on tasks like code editing. In-context Pretraining (Shi et al., 2024) proposed to train on a sequence of related documents to explicitly encourage the model to read and reason across document boundaries.\nAnother series of works, such as PoSE (Zhu et al., 2024), introduce large random gaps within"}, {"title": "2.2 Rotary Position Embedding", "content": "Rotary Position Embeddings (RoPE) (Su et al., 2021) have effectively encoded positional information in transformer-based models, yet they struggle to generalize beyond their trained sequence lengths. To overcome this limitation, various methods have been proposed to extend ROPE's context window for handling longer sequences.\nPosition Interpolation (PI) (Chen et al., 2023) extends RoPE by linearly interpolating the position index within the original context window. While effective, PI's uniform frequency scaling may limit the model's ability to capture high-frequency features. NTK-Aware and NTK-By-Parts (bloc97, 2023) introduce nonlinear strategies to address PI's limitations. NTK-Aware adjusts RoPE's base frequency, while NTK-By-Parts selectively scales different frequency components to better preserve local token relationships, enhancing the model's capacity to manage longer sequences. YaRN (Peng et al.,"}, {"title": "3 Method", "content": "In this section, we describe our Untie the Knots (UtK) augmentation strategy, aiming for effectively enhancing language models' long context abilities. The illustration of the method is shown in Figure 2. See Appendix B for more details.\n3.1 Tangling Phase\nChunking First, we chunk documents within target sequence length into several chunks, the split points are randomly chosen. Knot tokens are added before and after the split point. Chunk ID is prepended at each chunk\nTying Chunks are shuffled and randomly tied together. Considering the order of chunks of the same document may affect the result. We tried two strategies, preserve order and no preserver order."}, {"title": "3.2 Backtracing", "content": "After the final chunk of each document, we inserted the chunk IDs of this document, the model will enchant the ability to do backtracing in long range. A sentinel token is included to trigger the backtracing output. We masked the loss on both the knot tokens and sentinel token to keep the model from generating them."}, {"title": "3.3 Untying Phase", "content": "UtK turns the irrelevant documents into a complex myth. when the language model meet the \"head knot\", it's prompted to search its context for the unique matching \"tail knot.\" Only upon finding this correct match can the model reconstruct the fragmented document and proceed with its usual language processing. Furthermore, if a document is split into multiple chunks, the model must successfully identify and connect all related knots to fully restore the original context."}, {"title": "3.4 Longer than claimed", "content": "As indicated in the histogram in Figure 3, we realized that even when UtK is enabled, distances which is near the training sequence length are still rare in training data. So we purpose to use slightly longer sequence length than claimed max sequence"}, {"title": "4 Experimental Setting", "content": "4.1 Training Data\nFollowing Touvron et al. (2023a,b); Llama Team (2024); Yang et al. (2024), who emphasize the influence of data quality and diversity in training models, our curated dataset incorporates sources such as Common Crawl, books, Wikipedia, code, and academic papers. Additionally, our dataset is multilingual, with a significant portion of the data in English and Chinese. For continued training, we employ a quality classifier to filter high-quality data. After filtering, we randomly sample a total of 300 billion tokens for pre-training. Figure 4 illustrates the distribution of document lengths, where 70% of the data falls within the 0-32K token range."}, {"title": "4.2 Model Details", "content": "We continued pre-training the Qwen2 models with a sequence length of 128K tokens, as they were initially trained on sequences of only 32K tokens. The AdamW optimizer (Loshchilov and Hutter, 2017) is used for optimization, with parameters $\\beta_1 = 0.9$ and $\\beta_2 = 0.95$, alongside a cosine learning rate schedule starting at le-5 and decaying to 1e-6, with 200 warmup steps. Due to the models' long context windows, ring attention (cp=4) (Liu et al., 2023) and flash attention (Dao et al., 2022) are employed to reduce memory consumption. The training setup involves 128 H800 GPUs across 16 nodes, with a batch size of 4 million tokens. Training the 7B parameter models on 20B tokens takes 15 hours, while the 72B models require 5.5 days to complete training on the same amount of data. For each document, with a certain probability p, we"}, {"title": "4.3 Comparison Methods", "content": "We compare UtK against the following methods:\nCT In the naive continued pre-training experiment, we increased the training sequence length to 128K and trained on 20 billion tokens. Since the models were already pre-trained on 128K data, DCA was not applied during the inference stage.\nABF We increased the base frequency b of ROPE (Xiong et al., 2023) from 1e6 to 5e6, which is approximately the recommended base frequency as proposed by Men et al. (2024). Note that the 5e6 base frequency was used in all experiments except for the naive CT baseline in this paper.\nUpsampling Following Fu et al. (2024), we applied per-source length upsampling to maintain a fixed domain mixture ratio. Documents longer than 32K tokens were upsampled by a factor of five, without altering the overall domain mixture ratio.\nAttnMask As suggested by Llama Team (2024), an inter-document attention mask is essential during continued pre-training for long context. We applied this strategy in our experiment. Note that this strategy cannot be combined with UtK, as UtK requires the model to have full attention to locate the corresponding knots.\nSynthetic Xiong et al. (2024) demonstrated that fine-tuning LLMs using specially designed synthetic data can significantly enhance long-context understanding. Inspired by their approach, we constructed five types of synthetic datasets focused on specific tasks: sorting, multi-hop reasoning, state tracking, similarity retrieval, and attribute inclusion. Each dataset had a context length of 128K tokens. In this experiment, 30% of the original data mixture was replaced with synthetic data.\nCIP Following the optimal CIP-2 configuration from Zhao et al. (2024), each document was randomly split into two chunks, which were then interleaved in a pattern such as $D_1, D'_2, D_1, D_2, D'_2, D_3$."}, {"title": "5 Results", "content": "5.1 Main Results\n5.1.1 Long Tasks\nDatasets & Metrics To quantify the long context adaptation rate, we mainly focus on evaluate long-context language models on test sets with configurable sequence length. We use two widely recognized benchmarks: RULER (Hsieh et al., 2024) and LV-Eval (Yuan et al., 2024). RULER generates synthetic examples to assess long-context capabilities beyond simple in-context recall, comprising 13 tasks across 4 categories (i.e, NIAH, VT, CWE+FWE, and QA). We use the base model prompt template and report the average score across these 13 tasks.\nLV-Eval consists of two main tasks, single-hop QA and multi-hop QA, across 11 bilingual datasets. To reduce the influences from prompt engineering and minimize bias in automated evaluations, we assess 3-shot performance at 32K and 128K context lengths. We exclude the factrecall-en and factrecall-zh datasets, as factrecall-en and factrecall-zh are designed for pressure test of \"needle in haystack\", and they are not relevant to our work. Instead, we focus on evaluating real-world language tasks. We report the average F1 score or ROUGE score across the remaining 9 datasets. For all tasks except dureader-mixup and cmrc-mixup, we use a keyword-recall-based F1 metric, utilizing annotated answer keywords and a word blacklist. For cmrc-mixup, we apply the F1 metric with a word blacklist, and for dureader-mixup, we use the ROUGE-L metric with a word blacklist.\nResults. The results in highlight our model's effectiveness across various long-context evaluation benchmarks. Detailed values for different datasets are provided in Appendix C. On the RULER benchmark, our model, Qwen2-UtK-base (7B), consistently outperforms most other models at the 128K context length, achieving an average score of 75.0 significantly higher than Qwen2-base by 15.0% and Llama3.1-base by 13.5%. This demonstrates that Qwen2-UtK-base is particularly robust in handling extended contexts, maintaining strong performance as context length increases. We also applied UtK to Llama3.1-base, a model with a 128K context length, to evaluate its robustness. Llama3.1-UtK-base was trained using llama-rope, and UtK demonstrated an improvement of 11.6% in performance."}, {"title": "5.2 Ablation Analysis", "content": "We have conducted ablation analyses on two key design choices in the training strategy: (1) the optimal number of chunks for long-context training, and (2) the effects of each designed component. We have performed the ablation study on 7B models with 20B training tokens and evaluated them with the RULER benchmark. The results are illustrated in Figure 5 and Table 4.\nNumber of Chunks When evaluating the number of chunks, we find that using 2 or 3 chunks yields the best performance on the NIAH, VT, and CWE+FWE datasets. For the QA dataset, we observe that increasing the number of chunks improves the model's reasoning ability, suggesting that more complex training benefits QA tasks. We have also experimented with combining these approaches, which resulted in even better performance. We tried dividing the text into chunks of 1K tokens each, which resulted in an average score of 68.92. This indicates that a higher number of chunks can increase task complexity, potentially hindering the model's learning process."}, {"title": "5.3 Training Efficiency", "content": "As illustrated in Figure 6, we compare the baseline and UtK training methods by progressively increasing the number of training tokens to determine the required amount for effective long-context extension. We also include experiments with a longer sequence length of 192K to assess whether even longer context would enhance performance when still evaluated on the 128K tasks.\nOur findings indicate that: 1) Our approach UtK does have a higher training efficiency compared with the baseline regardless of how many training tokens are used, and the performance gains are steady. 2) Training on a 192K sequence length does increase the training efficiency at both the 1B and 5B token levels but the grains are diminishing when we reach 20B tokens. 3) Most significantly, with only 1B tokens, UtK-192K can already reach"}, {"title": "5.4 Attention Visualization", "content": "To visually represent the changes in attention of the model trained with UtK at a length of 128k, we have plotted the attention maps for the model trained on 128k lengths. We compared the original Qwen2 model, the ABF baseline, and the model trained with UtK on text lengths of 128k. Although the ABF-trained baseline can already accurately locate information within the same document, the model trained with UtK exhibits more attention on long-range dependencies within the same document, thereby reducing the loss of long-range relevant information. The detailed content and explanations of the plots can be found in the appendix."}, {"title": "6 Conclusion", "content": "In this paper, we proposed UtK, an augmentation recipe to adapt the model to longer context more efficiently and more effectively. We have trained and open sourced Qwen2-7B-UtK-128k and Qwen2-72B-UtK-128k base models with superior performance to the base models, as well as other long context enhancement strategies including upsam-"}, {"title": "7 Limitations", "content": "Although being efficient among continue training methods, due to the limitation of training tokens and practice patterns. As a result, it can only perform adaptation or transfer learning based on the model's original ability. Acquiring new abilities, such as solving complex problems within long context, is not feasible and may require further specialized training. Our experiments are also limited to the datasets we use. Our method applied to other datasets of different languages or genres might lead to different results."}, {"title": "BUtK Algorithm", "content": "Suppose n documents represent a sampled set of training data of length l (e.g., 128k), the ith document is represented as $D_i$, which contains $L_i$ tokens. $\\Sigma_{i=1}^n L_i >= l$.\nUtK rearranges the training data in the following procedures:\n1. For each document $D_i$ which $L_i >= min\\_split$, we split it into $h_i$ chunks, $D_i^1$ to $D_i^{h_i}$, $h_i \\sim P$, P is a custom discrete distribution, $2 * (h_i \u2013 1)$ split points are randomly chosen from (0, $L_i$).\n2. Prepend chunk label $CL_j$ for each chunk. Chunk labels are randomly generated characters, and are treated as normal words when doing tokenization. Each chunk label is surrounded by special tokens <CL> and </CL>.\n3. For $D_i^j$ with j > 1, prepend head knot token <h>\n4. For $D_i^j$ with j < $h_i$, add tail knot token <t_j> at the end of this chunk.\n5. Shuffle all $D_i^j$'s, when PreserveOrder constraint is enabled, we adjust the position of chunks of the same documents to preserve the order within each document."}]}