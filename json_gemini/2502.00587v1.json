{"title": "Robust Knowledge Distillation in Federated Learning: Counteracting Backdoor Attacks", "authors": ["Ebtisaam Alharbi", "Leandro Soriano Marcolino", "Qiang Ni", "Antonios Gouglidis"], "abstract": "Federated Learning (FL) enables collaborative model training across multiple devices while preserving data privacy. However, it remains susceptible to backdoor attacks, where malicious participants can compromise the global model. Existing defence methods are limited by strict assumptions on data heterogeneity (Non-Independent and Identically Distributed data) and the proportion of malicious clients, reducing their practicality and effectiveness. To overcome these limitations, we propose Robust Knowledge Distillation (RKD), a novel defence mechanism that enhances model integrity without relying on restrictive assumptions. RKD integrates clustering and model selection techniques to identify and filter out malicious updates, forming a reliable ensemble of models. It then employs knowledge distillation to transfer the collective insights from this ensemble to a global model. Extensive evaluations demonstrate that RKD effectively mitigates backdoor threats while maintaining high model performance, outperforming current state-of-the-art defence methods across various scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) has emerged as a powerful paradigm that enables multiple clients to collaboratively train a shared global model without exchanging their private local data, thereby enhancing data privacy and security [14]. In FL, each client trains a local model on its own dataset and shares only the model updates with a central server, which aggregates these updates to form the global model. Despite its advantages, FL faces significant challenges that threaten the effectiveness and integrity of the resulting global model.\nOne critical challenge is the vulnerability of FL to security threats, particularly backdoor attacks. In these attacks, malicious clients introduce harmful updates into the training process by embedding hidden triggers within their model updates [15]. These hidden triggers are specific patterns or inputs that, when encountered by the global model, cause it to produce incorrect or malicious outputs. Malicious clients achieve this by manipulating their local training data and altering their model parameters, ensuring that the backdoor behaviour is incorporated into the global model during aggregation without being detected. Since these triggers are often designed to be stealthy inputs, and malicious clients train on both clean input data and inputs containing the trigger, the attacks can go unnoticed during normal validation. This stealthiness makes backdoor attacks particularly effective and dangerous threats to FL systems [25]. As a result, the compromised global model behaves normally on standard inputs but can cause significant harm when the triggers are activated, posing serious risks to the integrity and reliability of FL deployments.\nAnother major challenge is handling Non-Independent and Identically Distributed (Non-IID) data across clients. In real-world scenarios, clients often possess data that are not representative of the overall data distribution, leading to inconsistent global model performance and reduced convergence rates. This heterogeneity can cause the global model to perform poorly on some clients' local data, undermining the benefits of collaborative learning [24]. The variations in local data distributions can result in conflicting model updates from different clients, making it difficult for the global model to generalize well across all participants.\nExisting defence methods against backdoor attacks employ various strategies but are often limited by strict assumptions regarding data heterogeneity. Specifically, they assume data are Independent and Identically Distributed (IID) across clients and the proportion of malicious clients is relatively low. Robust aggregation methods, such as RLR [17] and FoolsGold [8], adjust the aggregation process to mitigate the influence of malicious updates but may struggle against sophisticated attacks and are less effective under Non-IID data distributions. Root data validation approaches, like FLTrust [4] and Baffle [2], attempt to validate updates using trusted data but can be circumvented by stealthy attacks that mimic benign behaviour. Differential Privacy methods, including FLAME [16], introduce noise and apply weight clipping to protect privacy and integrity but require precise tuning and often involve trade-offs that degrade model performance. Knowledge distillation-based methods, such as FedDF [13] and FedBE [5], address issues of Non-IID data and noisy gradients but may lack mechanisms for outlier detection, potentially allowing backdoor triggers to be transferred to the global model.\nTo overcome these limitations, we propose Robust Knowledge Distillation (RKD), a novel defense mechanism that enhances model integrity without relying on restrictive assumptions. RKD integrates clustering and model selection techniques to identify and filter out malicious updates, forming a reliable ensemble of models for knowledge distillation. Specifically, RKD employs cosine similarity and Hierarchical"}, {"title": "II. BACKGROUND AND PROBLEM SETTING", "content": "FL is a distributed machine learning paradigm where multiple clients collaboratively train a shared global model $M_{global}$ while keeping their local data $D_i$ decentralized and private. Each client i trains a local model $M_i$ on its own dataset $D_i$ and then shares only the model parameters $\\theta_i$ with a central server. The central server aggregates these local parameters to update the global model. A common aggregation algorithm is Federated Averaging (FedAvg), which computes the global model parameters as: $\\theta_{global} = \\frac{1}{N} \\sum_{i=1}^{N} \\theta_i$, where N is the number of participating clients [14]. After aggregation, the server sends the updated global model $M_{global}$ back to all participating clients. Each client then updates its local model using the received global parameters, ensuring synchronization across the network. This collaborative process allows the construction of a global model that benefits all clients without exposing datasets, thus preserving data privacy [25].\nHowever, despite its advantages, FL is vulnerable to security threats, particularly backdoor attacks. In backdoor attacks, malicious clients aim to compromise the integrity of the global model $M_{global}$ by manipulating their local training data $D_i$ and/or altering their model updates $\\theta_i$ before sending them to the central server. These manipulations are designed to embed hidden malicious behaviours into the global model without being detected. Furthermore, the presence of Non-IID data distributions among clients exacerbates the difficulty in detecting and mitigating such attacks, as data heterogeneity can mask the malicious updates, making it challenging to distinguish between benign model variations and manipulations [7]."}, {"title": "A. Backdoor Attacks", "content": "1) Data Poisoning Attacks: In backdoor data poisoning attacks, malicious clients intentionally modify their local training data to embed hidden triggers that induce the global model to exhibit malicious behaviour when these triggers are encountered [15]. Specifically, a malicious client takes its original dataset $D_i = \\{(x_j, y_j)\\}$ and creates a poisoned dataset $D_{poison}$ by injecting a small number of modified data instances $\\{(x^*, y^*)\\}$. In these instances, $x^*\\in \\mathbb{R}^d$ contains a trigger pattern-such as a specific pixel arrangement in an image-and $y^*$ is an incorrect target label chosen by the attacker. The modified dataset $\\tilde{D}_i = D_i \\cup D_{poison}$ is then used to train the local model $M_i$ [20].\nThe attacker's objective is to train $M_i$ such that it performs well on normal data while misclassifying inputs containing the trigger pattern. This can be formalized by minimizing the loss function: $L(M_i(\\tilde{D}_i),y)$, where y represents the labels. By successfully training on $\\tilde{D}_i$, the model learns the backdoor association between the trigger and the target label y. When these malicious model updates $\\theta_i$ are aggregated by the server, the global model $M_{global}$ inherits the backdoor behaviour, causing it to misclassify inputs containing the trigger pattern while maintaining normal performance on clean data.\n2) Model Poisoning Attacks: In backdoor model poisoning attacks, malicious clients manipulate their local model parameters $\\theta_i$ to embed backdoors into the global model, often in combination with injecting triggers into their training data. Unlike data poisoning attacks that focus primarily on the training data, model poisoning attacks involve directly altering the model parameters to maximize the impact of the backdoor while minimizing the risk of detection [24].\nIn general, malicious clients may:\nManipulate Model Parameters: After training on the poisoned data, attackers further manipulate their model parameters $\\theta_i$ to enhance the backdoor effect or to avoid detection. This can involve:\nAdversarial Adaptation: Adapting the trigger and model parameters to remain effective against changes in the global model during training [22].\nScaling Weights: Amplifying the impact of the malicious update by scaling the model parameters [3]: $\\theta_{poison} = \\alpha\\theta_i$, where $\\alpha > 1$ increases the update's influence on the global model during aggregation.\nAdding Perturbations: Introducing subtle changes to the model parameters to maintain stealth [7]: $\\theta_{poison} = \\theta_i + \\delta$, where $\\delta$ is a small perturbation designed to embed the backdoor while avoiding detection by defence mechanisms."}, {"title": "B. Impact on the Global Model", "content": "When the central server aggregates the model updates, the presence of malicious updates from attackers influences the global model parameters. Specifically, the aggregated global model parameters become: $\\theta_{global} = \\frac{1}{N} \\left(\\sum_{i \\in H} \\theta_i + \\sum_{j \\in M} \\theta_{poison} \\right)$, where H is the set of honest clients and M is the set of malicious clients. The malicious updates $\\theta_{poison}$ are designed to inject the backdoor into the global model $M_{global}$. As a result, the compromised global model may behaves normally on standard (clean) inputs, maintaining high performance and thus not raising suspicion. However, when presented with inputs containing the backdoor trigger, the model exhibits incorrect or malicious behaviour, such as misclassifying the input or producing outputs desired by the attacker. This stealthy alteration poses significant risks to the integrity and reliability of the FL system, as the backdoor can remain undetected until the trigger is activated."}, {"title": "C. Characteristics of Backdoor Attacks", "content": "Backdoor attacks often exhibit distinctive characteristics in the model updates sent by malicious clients, which can be exploited for detection. Key characteristics include:\nAngular Deviation: Malicious model updates may have a different direction in the parameter space compared to updates from benign clients [3]. This directional difference can be quantified using the cosine similarity (or angular deviation) between the parameter vectors of malicious updates $\\theta_{attack}$ and benign updates $\\theta_{benign}$: $\\Delta_{angular} = cos^{-1} \\left( \\frac{\\theta_{attack} \\cdot \\theta_{benign}}{\\|\\theta_{attack}\\| \\cdot \\|\\theta_{benign}\\|} \\right)$. Angular deviations indicate that the malicious update is contributing in a different direction, which may be due to the attacker's attempt to embed the backdoor like F3BA attack [7].\nMagnitude Deviation: Malicious updates may have a significantly different norm magnitude compared to benign updates. This occurs when attackers scale their model updates to increase their influence on the aggregated global model [20]. The magnitude deviation can be observed when: $\\|\\theta_{attack}\\| \\gg \\|\\theta_{benign}\\|$. Subtle Deviations: Some attackers design their updates to closely resemble those of benign clients, keeping the deviation between the malicious and benign updates within a small threshold $\\epsilon$ to avoid detection: $\\|\\theta_{attack} - \\theta_{benign}\\| < \\epsilon$. Techniques like A3FL attack [22] adjust both the magnitude and direction of the malicious update to embed the backdoor while maintaining stealthiness."}, {"title": "D. Threat Model", "content": "Our work assumes the FL system in which a subset of clients, referred to as malicious clients, aim to compromise the global model through backdoor attacks. The goal of these attacks is to embed hidden triggers into the global model, causing targeted misclassifications on specific inputs while maintaining high accuracy on clean data to avoid detection. Below, we outline the adversary's knowledge, capabilities, and the defender's knowledge:\nAdversary's Knowledge: The malicious clients have no access to other clients' model updates or data. They operate under the assumption that they can only manipulate their local data and model updates.\nAdversary's Capabilities: The adversaries can collude and coordinate their attacks but cannot intercept or alter communications between other clients and the server. They can manipulate their local training data (data poisoning) and modify their model updates before sending them to the server (model poisoning).\nDefender's Knowledge: The server is aware that some clients may be malicious but does not know their identities. The server has access only to the submitted model updates and any auxiliary data (e.g., public unlabeled datasets) used for knowledge distillation."}, {"title": "III. RELATED WORK", "content": "Defending against backdoor attacks in FL is a critical area of research, with various strategies proposed to enhance the robustness of FL. These defence mechanisms can be broadly categorized into robust aggregation methods, clustering-based defences, and knowledge distillation approaches.\nRobust Aggregation Methods aim to mitigate the influence of malicious updates during the model aggregation process. The Robust Learning Rate (RLR) method [17] adjusts the learning rates of clients based on the alignment of their updates with the global model's direction. By assigning smaller learning rates to updates that deviate significantly, RLR reduces the impact of potentially malicious updates. However, RLR may not fully address attacks that manipulate the magnitude of updates, such as scaling attacks, and its effectiveness diminishes in heterogeneous (Non-IID) data environments where benign updates naturally vary in direction and magnitude.\nFoolsGold [8] is designed to counter backdoor attacks by analyzing the similarity of gradient updates among clients. It assigns lower aggregation weights to clients whose updates are overly similar, under the assumption that malicious clients will produce highly similar gradients due to coordinated attacks. While effective against certain types of attacks, FoolsGold may inadvertently penalize benign clients with similar data distributions, leading to unfairness and potential degradation of overall model performance.\nClustering-Based Defences have been employed to distinguish between benign and malicious updates by grouping similar updates and identifying outlier models. Methods like FLAME [16] leverage HDBSCAN for clustering and incorporate noise addition to bolster security, yet they require improved adaptability to evolving threats. RFCL [1] also uses HDBSCAN to cluster clients and defend against gradient poisoning attacks but is less effective in defending against adaptive backdoor attacks [22]. A significant challenge with clustering-based defences is their reliance on the precision of the clustering process, which can be compromised in high-dimensional parameter spaces typical of deep learning models. Traditional clustering algorithms often struggle with high-dimensional data due to the curse of dimensionality, leading to less efficient clustering results and potential misclassification of benign updates as malicious or vice versa.\nKnowledge Distillation Approaches have been recognized for their ability to improve learning efficiency and address Non-IID data distributions in a federated framework. Methods such as FedDF [13] and FedBE [5] leverage knowledge distillation to aggregate models. FedDF performs model fusion by distilling knowledge from client models into a global model using unlabeled public data, effectively aligning the models' outputs. FedBE builds upon this by employing Bayesian ensemble techniques to instruct a student model based on the ensemble's predictive distribution.\nFedRAD [18] enhances FedDF by assigning weights to client models according to their median scores, which measure how often a model's prediction corresponds to the median prediction among all clients. While these methods improve performance in Non-IID settings, they are vulnerable to backdoor attacks when transferring knowledge from ensemble models without analyzing outliers. Malicious clients can introduce backdoors into their models, and without mechanisms to detect and exclude these compromised models, the backdoor triggers can be propagated to the global model during distillation.\nRKD builds upon existing methods by innovatively integrating and enhancing foundational techniques such as HDBSCAN and knowledge distillation. This approach addresses the limitations of prior works, particularly in defending against recent backdoor attacks in Non-IID data distributions.\nFirst, by integrating cosine similarity with HDBSCAN, RKD improves clustering efficiency in high-dimensional parameter spaces, effectively identifying malicious updates even when they are subtle or adaptive. Focusing on the angular relationships between model updates captures essential differences without being overwhelmed by the volume of parameters, thereby ensuring accurate clustering despite Non-IID data.\nSecond, the RKD framework incorporates a robust model selection process that selects models near the median of the cluster. By identifying and using the most representative models, RKD further mitigates the influence of any remaining malicious updates. This selection forms a reliable and trustworthy ensemble for knowledge distillation.\nThird, RKD ensures secure knowledge transfer by distilling from the carefully selected ensemble models, preventing the propagation of backdoor triggers to the global model. By excluding outlier models identified during clustering, RKD reduces the risk of incorporating malicious behaviours into the model. Additionally, knowledge distillation aids in smoothing out variations caused by Non-IID data, leading to a more generalized and robust global model.\nBy addressing the challenges of high-dimensional data, improving clustering efficiency, and ensuring secure knowledge transfer, RKD provides a robust defence against backdoor attacks in federated learning while effectively handling Non-IID data distributions."}, {"title": "IV. METHODOLOGY", "content": "The RKD framework is designed to secure FL against backdoor attacks by identifying and mitigating malicious model updates. It consists of three core components: Automated Clustering, Model Selection, and a Knowledge Distillation Module. These components work together to detect and eliminate malicious influences while preserving the performance and integrity of the global model."}, {"title": "A. Framework Overview", "content": "The RKD framework employs a multi-tiered strategy to enhance model robustness within FL. This subsequent provides an overview of the RKD framework, outlining its key components and processes. Detailed explanations of each strategy method are presented in the subsequent subsections.\nInitially, the central server initializes the global model $M_{global}$ and broadcasts it to all participating clients. Each client i trains its local model $M_i^{(r)}$ on its private dataset $D_i$, starting from the current global model $M_{global}^{(r)}$. The result is updated model parameters $(\\theta_i^{(r)})$, which are then returned to the server.\nAt the server, the focus is on identifying potential malicious updates. The server computes the cosine similarity between each client model update $(\\theta_i^{(r)} - \\theta_{global}^{(r)})$ and the current global model $M_{global}^{(r)}$. This metric captures the angular similarity between local updates and the global model, enabling the detection of updates that significantly deviate in direction\u2014a characteristic of malicious behaviour known as Angular Deviation.\nUsing these similarity scores, the server employs the HDBSCAN algorithm to cluster the models based on their similarity to the global model. This clustering process distinguishes benign updates, which form dense clusters due to their similarity, from potentially malicious updates, which appear as outliers because of their dissimilarity. By clustering based on similarity scores rather than directly using high-dimensional parameter vectors, the computational complexity is significantly reduced, and the clustering becomes more scalable and efficient.\nWithin the benign cluster, the server computes the median of the model parameters by taking the median of each parameter across the models. This helps mitigate the impact of extreme values in the updates, addressing the Magnitude Deviation characteristic of malicious behaviour."}, {"title": "Automated Clustering", "content": "To identify alignment with the current global model, the server computes the cosine similarity $s_i$ between each client's model and the global model $M_{global}^{(r)}$ which serves as the reference point: $s_i = \\frac{(\\theta_i^{(r)} - \\theta_{global}^{(r)}) \\cdot M_{global}^{(r)}}{\\|(\\theta_i^{(r)} - \\theta_{global}^{(r)})\\| \\cdot \\|M_{global}^{(r)}\\|}$, for $i = 1,..., N$.\nHere, $M_{global}$ represents the current aggregated global model. Higher cosine similarity values indicate greater alignment with the global model, which is expected for benign clients, while malicious clients are more likely to deviate.\nThese similarity scores $\\{s_i\\}$ are then clustered using HDBSCAN. Clustering based on scalar similarity scores significantly reduces computational complexity compared to clustering in the high-dimensional parameter space.\nThe minimum cluster size Q for HDBSCAN is dynamically adjusted at each training round r using the formula: $Q = max\\left(2, \\left\\lceil 0.2N - r\\right\\rceil\\right)$, where N is the total number of participating clients, r is the current training round, and $\\left\\lceil \\cdot \\right\\rceil$ denotes the ceiling function to ensure Q is an integer.\nThis dynamic adjustment ensures that the clustering remains adaptable to changes in client participation and training dynamics. For example, as the training progresses and fewer clients actively participate, Q decreases, maintaining meaningful cluster sizes and computational efficiency.\nApplying HDBSCAN to the set of similarity scores yields cluster labels $\\{L_i\\}$: $\\{L_i\\} = HDBSCAN\\left( \\{s_i\\}, Q \\right)$.\nFor each identified cluster $C_k$, the mean cosine similarity $\\mu_k$ is computed: $\\mu_k = \\frac{1}{\\|C_k\\|} \\sum_{i \\in C_k} s_i$, where $\\|C_k\\|$ is the number of clients in cluster $C_k$.\nThe cluster with the highest mean cosine similarity $\\mu_{max}$ is considered the benign cluster: $\\mu_{max} = max\\left( \\{\\mu_k\\} \\right)$. Clusters are classified as follows:\nCluster $C_k$ is classified as $\\begin{cases} M_{benign}, & \\text{if } \\mu_k = \\mu_{max}, \\\\ M_{malicious}, & \\text{otherwise}. \\end{cases}$\nModels in the benign cluster are used for further training and receive the updated global model in the next iteration."}, {"title": "Model Selection", "content": "This component refines the set of benign models to select the most representative ones for aggregation. By focusing on models closest to the central tendency, it mitigates the impact of outliers and enhances the robustness of the aggregated model against backdoor attacks.\nFrom the set of benign models $M_{benign}^{(r)} = \\{\\theta_i^{(r)} | i \\in A_{benign}\\}$, we compute the median model parameter vector $\\theta_{median}^{(r)}$. This is done by taking the median of each parameter across all benign models: $\\theta_{median}^{(r)} = median\\left( \\{\\theta_i^{(r)} | i \\in A_{benign}\\}\\right)$, where the median is computed element-wise over the parameter vectors.\nNext, we calculate the distance between each benign model $\\theta_i^{(r)}$ and the median model $\\theta_{median}^{(r)}$ using the $L_1$ norm: $d_i = \\|\\theta_i^{(r)} - \\theta_{median}^{(r)}\\|_1$, for all $i \\in A_{benign}$.\nModels are then ranked based on their distances $d_i$. A subset of models with the smallest distances is selected to form the ensemble $E^{(r)} = \\{\\theta_i^{(r)} \\in M_{benign}^{(r)} | d_i \\leq \\epsilon\\}$, where $\\epsilon$ is a statistical measure of the standard deviation of distances from the median to inform the setting of epsilon, ensuring it adapts to the inherent variability in benign updates."}, {"title": "Knowledge Distillation Process", "content": "It refines the global model by distilling knowledge from an ensemble of selected benign models, denoted as $\\mathcal{E}^{(r)}$, which have been identified through automated clustering and model selection in previous steps. The server utilizes an unlabeled dataset $D_{val}$, comprising 16% of the total training data, to perform knowledge distillation. Pseudo-labels are generated from the ensemble model outputs to guide the training of the distilled model $M_{distill}$.\nFor each sample x in $D_{val}$, the server computes the logits using each model in the ensemble. Here, logits refer to the raw, unnormalized output scores from the models before applying the softmax function. These logits are averaged to produce the ensemble logits:\n$Ensemble\\_Logits(x) = \\frac{1}{\\|\\mathcal{E}^{(r)}\\|} \\sum_{M_i \\in \\mathcal{E}^{(r)}} f_{M_i}(x)$,\nwhere $f_{M_i}(x)$ denotes the logits produced by model $M_i$ for input x. The pseudo-labels are then generated by applying the softmax function with a temperature parameter T to the averaged logits:\n$\\tilde{y}(x) = softmax\\left(\\frac{Ensemble\\_Logits(x)}{T}\\right)$.\nThe temperature parameter T in the softmax function serves as a scaling factor that controls the smoothness of the resulting probability distribution.\nThe distilled model $M_{distill}$ is then trained to minimize the Kullback-Leibler (KL) divergence between its output probabilities and the pseudo-labels:\n$\\mathcal{L} = D_{KL}\\left( \\tilde{y}(x) \\|\\| softmax\\left(\\frac{f_{M_{distill}}(x)}{T}\\right) \\right)$.\nHere, $f_{M_{distill}}(x)$ represents the logits from the distilled model for input x. The KL divergence measures the difference between the two probability distributions, encouraging the distilled model to align closely with the ensemble's behaviour.\nTo further stabilize and enhance the generalization of $M_{distill}$, we employ Stochastic Weight Averaging (SWA) during training. SWA improves generalization by maintaining a running average of the model weights over training epochs, effectively capturing the trajectory of the model parameters as they converge.\nDuring each epoch of the knowledge distillation training, after updating the distilled model $M_{distill}$ using gradient descent, the SWA model $M_{SWA}$ is updated as follows:\n$M_{SWA} \\leftarrow \\frac{n_{SWA} M_{SWA} + M_{distill}}{n_{SWA} + 1}$\nwhere $n_{SWA}$ is the number of times the SWA model has been updated. Initially, $M_{SWA}$ is set to the weights of $M_{distill}$, and $n_{SWA} = 1$.\nFinally, the SWA model $M_{SWA}$ is set as the updated global model for the next iteration: $M_{global}^{(r+1)} \\leftarrow M_{SWA}$.\nThis updated global model integrates the distilled knowledge from the ensemble and the generalization properties provided by SWA. The process of knowledge distillation is in Algorithm 3."}, {"title": "V. EXPERIMENTS", "content": "We evaluate the effectiveness of the proposed RKD framework in the FL setting under backdoor attack scenarios. We simulate a standard FL environment where multiple clients collaboratively train a global model under the coordination of a central server. The training process is iterative and continues until convergence is achieved."}, {"title": "A. Datasets and Models", "content": "We conducted experiments on three widely used datasets: CIFAR-10, EMNIST, and Fashion-MNIST.\nCIFAR-10 [12] consists of 60,000 color images of size 32 \u00d7 32 pixels, categorized into 10 classes. It serves as a comprehensive benchmark for image classification tasks.\nEMNIST [6] is an extension of the MNIST dataset, containing 814, 255 handwritten character images across 62 classes, including digits and letters. The images are grayscale with a 28 \u00d7 28 pixels resolution.\nFashion-MNIST [23] comprises 70,000 grayscale images of fashion products from 10 categories, each of size 28 \u00d7 28 pixels, providing a challenging alternative to MNIST dataset.\nFor each dataset, we employed model architectures suitable for the complexity of the tasks:\nFor CIFAR-10, we used a ResNet-18 architecture [9], which is well-suited for handling the complexity of colour images and capturing hierarchical features. The model was trained with a batch size of 64 and an initial learning rate of 0.01.\nFor EMNIST, we utilized a Convolutional Neural Network (CNN) consisting of two convolutional layers, each followed by max pooling and dropout layers to prevent overfitting, and a fully connected layer for classification. The model was trained with a batch size of 64 and a learning rate of 0.001.\nFor Fashion-MNIST, we implemented a CNN with two convolutional layers, each followed by batch normalization and dropout layers, and a fully connected layer. This architecture aids in normalizing the input features and regularizing the model. The training was conducted with a batch size of 64 and a learning rate of 0.001."}, {"title": "B. Attack Setup", "content": "To evaluate the robustness of the RKD framework against backdoor attacks, we simulated the FL environment with 30 clients. We considered three scenarios where 20%, 40%, and 60% of the clients were compromised by an adversary.\nEach compromised client injected backdoor triggers into 50% of its local training data. The backdoor trigger was a pattern added to the images, and the labels of the poisoned samples were altered to a target class specified by the adversary. This simulates a realistic attack where malicious clients attempt to implant a backdoor into the global model while maintaining normal performance on clean data.\nThe compromised clients followed the federated learning protocol but aimed to influence the global model towards the backdoor task. The benign clients trained on their local data without any manipulation."}, {"title": "C. Attack Methods", "content": "To thoroughly evaluate the robustness of our RKD framework, we examined its efficacy against four recent and sophisticated backdoor attack methods. These attacks are designed to circumvent traditional defence mechanisms and present significant challenges in federated learning environments.\na) Adversarially Adaptive Backdoor Attack to Federated Learning (A3FL): The A3FL attack [22] enhances backdoor persistence by dynamically adapting the trigger pattern in response to the global training dynamics. Instead of using a static trigger, A3FL continuously optimizes the trigger to remain effective against the evolving global model. It employs an adversarial adaptation loss function and Projected Gradient Descent (PGD) to refine the backdoor trigger. By iteratively updating the trigger based on both the current global model and adversarially crafted models, A3FL ensures that the backdoor remains functional throughout multiple training rounds, making it particularly challenging to detect and mitigate.\nb) Focused-Flip Federated Backdoor Attack (F3BA): F3BA [7] targets the global model by manipulating a small subset of its parameters with minimal impact on overall performance. The attack calculates the importance of each parameter w[j] concerning the global loss $L_g$ using the following metric: $S[j] = \\left( \\frac{\\partial L_g}{\\partial w[j]} \\right) \\odot \\left( w[j] \\right)$. Here, $\\frac{\\partial L_g}{\\partial w[j]}$ is the gradient of the global loss with respect to parameter w[j], and $\\odot$ denotes element-wise multiplication.\nParameters with the highest importance scores are selected, and their signs are flipped to embed the backdoor. This selective manipulation allows the attacker to implant the backdoor with minimal deviation from normal training behaviour, thereby evading detection.\nc) Distributed Backdoor Attack (DBA): The DBA [20] spreads a trigger pattern across multiple adversarial clients, enhancing stealth and making detection more difficult. Each compromised client injects a portion of the full trigger into its local training data. When these local models are aggregated, the global model inadvertently learns to associate the combined trigger pattern with the target class. As a result, inputs containing the full trigger pattern are misclassified, effectively executing the backdoor attack without any single client contributing a suspiciously large modification.\nd) Train-and-Scale Backdoor Attack (TSBA): TSBA [3] aims to evade anomaly detection mechanisms by carefully scaling the model weights after training. The adversarial client first trains its local model with the backdoor trigger until convergence. Then, it scales down the model updates by a factor e that remains within acceptable bounds defined by the FL protocol. By doing so, the malicious updates appear similar to benign ones in magnitude, thereby bypassing defences that rely on detecting abnormal update sizes. The scaling factor e is chosen to balance stealth and backdoor effectiveness."}, {"title": "D. Heterogeneous Setting", "content": "In FL, data heterogeneity (Non-IID data distribution) among clients is a common and challenging issue that can significantly impact the learning process and model performance. To simulate realistic federated learning environments with varying degrees of data heterogeneity, we utilize the Dirichlet distribution to partition the datasets among the clients [22].\nThe Dirichlet distribution allows us to control the level of heterogeneity by adjusting the concentration parameter a. Each client i receives a proportion $p_{ik}$ of data from class k, where $p_i = [p_{i,1}, p_{i,2},..., p_{i,C}] \\sim Dirichlet(\\alpha)$ and C is the number of classes. A lower a value leads to more uneven class distributions among clients, simulating higher heterogeneity. Specifically, we set the a parameter to:\nExtreme Heterogeneity: a = 0.5, a = 0.3, and a = 0.1. These lower values of a result in clients having data predominantly from a few classes, leading to highly Non-IID data distributions scenarios.\nModerate Heterogeneity: \u03b1 = 0.9 and a = 0.7. Higher values of a produce more balanced class distributions across clients, representing a moderately heterogeneous setting.\nBy varying a, we create a spectrum of data heterogeneity scenarios to comprehensively evaluate the robustness of the RKD framework under different degrees of Non-IID data distributions. This approach allows us to assess how well the RKD framework can handle challenges posed by data heterogeneity, which is critical for practical FL applications.\nCompared Defence Baselines. We evaluate the RKD framework against seven recent FL defences methods: FedAvg [14], FLAME [16], FedDF [13], FedRAD [18], FedBE [5], RLR [17], and FoolsGold (FG) [8]."}, {"title": "E. Evaluation Metrics", "content": "We utilized two key evaluation metrics: Main Task Accuracy (MTA) and Attack Success Rate (ASR). These metrics provide a comprehensive understanding of the model's performance on legitimate tasks and its resistance to backdoor triggers."}, {"title": "a) Main Task Accuracy (MTA)", "content": "MTA measures the classification accuracy of the global model on a clean test dataset $T_m$, reflecting its ability to correctly predict the true labels of inputs without any backdoor influence. It is defined as: $MTA = \\frac{\\|\\{x \\in T_m | f(x) = y\\}\\|}{\\|T_m\\|}$, where x is an input sample from the clean test dataset $T_m$. f(x) denotes the prediction of the global model f for input x. y is the true label corresponding to x. $\\| \\cdot \\|$ represents the cardinality of the set.\nA higher MTA indicates that the model performs well on the primary classification task, correctly identifying inputs as per their true labels."}, {"title": "b) Attack Success Rate (ASR)", "content": "ASR evaluates the effectiveness of the backdoor attack by measuring the proportion of poisoned inputs that the global model misclassifies into the attacker's target class. It is calculated on a test dataset containing backdoor triggers $T_b$: $ASR = \\frac{\\|\\{x \\in T_b | f(x) = y_{target}\\}\\|}{\\|T_b\\|}$, where x is an input sample from the poisoned test dataset $T_b$ that contains backdoor triggers. f(x) is the prediction of the global model f for input x. $y_{target}$ is the target label that the attacker intends the model to output for backdoor inputs.\nA lower ASR signifies better robustness against backdoor attacks, as it indicates that the model is less likely to misclassify backdoor inputs into the attacker's target class.\nThe goal of an effective Defence mechanism like the RKD framework is to maintain a high MTA while minimizing the ASR. This balance ensures that the model retains its performance on legitimate data while being resilient to manipulation attempts by adversaries. In our experiments, we focus on achieving this balance to demonstrate the RKD framework's capability to defend against sophisticated backdoor attacks without degrading the overall model performance."}, {"title": "F. Experimental Results", "content": "We evaluated the robustness of the RKD framework against advanced backdoor attacks in FL. The models were trained under non-IID data distributions", "Attack": "Under highly heterogeneous non-IID conditions (a = 0.3), RKD demonstrated significant resilience against the A3FL attack on the CIFAR-10 and Fashion-MNIST datasets. As illustrated in Figures 1 and"}]}