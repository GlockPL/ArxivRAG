{"title": "Learning Model Agnostic Explanations via Constraint Programming", "authors": ["Frederic Koriche", "Jean-Marie Lagniez", "Stefan Mengel", "Chi Tran"], "abstract": "Interpretable Machine Learning faces a recurring challenge of explaining the predictions made by opaque classifiers such as ensemble models, kernel methods, or neural networks in terms that are understandable to humans. When the model is viewed as a black box, the objective is to identify a small set of features that jointly determine the black box response with minimal error. However, finding such model-agnostic explanations is computationally demanding, as the problem is intractable even for binary classifiers. In this paper, the task is framed as a Constraint Optimization Problem, where the constraint solver seeks an explanation of minimum error and bounded size for an input data instance and a set of samples generated by the black box. From a theoretical perspective, this constraint programming approach offers PAC-style guarantees for the output explanation. We evaluate the approach empirically on various datasets and show that it statistically outperforms the state-of-the-art heuristic ANCHORS method.", "sections": [{"title": "Introduction", "content": "With the increasing influence of machine learning systems in our daily lives, there is a pressing need to comprehend the reasoning behind their predictions. However, the current black-box nature of predictive models such as, for example, ensemble methods, kernel techniques or neural networks, often leaves users bewildered. In order to address this challenge, the field of Interpretable Machine Learning (IML) has emerged, which focuses on developing new learning and explanation techniques to make predictive models more transparent and comprehensible [6, 24].\nA common task in IML is explaining the predictions made by a classifier on data instances in terms that are easy for humans to understand. For example, when evaluating a personal loan application based on tabular data about the applicant, features such as a stable income, a low debt-to-income ratio, and the availability of a co-signer may have a collective impact on the loan approval."}, {"title": "Related Work", "content": "In the field of Interpretable Machine Learning, researchers have proposed various techniques to understand the behavior of machine learning models. Post hoc explanations are one such technique that aims to provide insights into how a model works without affecting its performance. These explanations give additional knowledge about the prediction $f(x)$ made by a model $f$ on a specific data instance $x$. Common types of post hoc explanations are feature-based explanations [21, 27], which interpret the prediction $f(x)$ by identifying the most relevant features of the instance $x$, and example-based explanations [18, 20], which improves the comprehension of $f(x)$ using counterfactual or influential examples. Feature-based explanations, examined in this study, can be categorized into two types: model-specific and model-agnostic explanations."}, {"title": "Model-Specific Explanations.", "content": "By exploiting the structure and parameters of the predictive model $f$, model-specific approaches aim to provide precise and mathematically verifiable explanations. Notably, when $f$ is a Boolean classifier, a common explanation for predicting the output $f(x)$ of an input instance $x$ is a subset-minimal collection of features $S$ such that the restriction $x_S$ of $x$ to $S$ determines $f(x)$. Such an abductive explanation [14], also called sufficient reason [8], is perfectly accurate since $x_S$ is a prime implicant of $f$ that covers $x$ [30]. Although finding an abductive explanation is generally an NP-hard problem, tractable cases have been identified for various model classes including, among others, decision trees [2, 11, 17], Naive Bayes classifiers [22], and monotone threshold functions [7]. Actually, even for intractable classes such as decision lists and random forests, empirical results indicate that abductive explanations can be quickly found in practice using constraint-based approaches [3, 13, 15].\nIn order to overcome the uncontrollable size of abductive explanations, model-specific approaches have recently focused on probabilistic explanations, which convey a natural trade-off between conciseness and precision [5, 16]. In simple terms, a $(k,\\epsilon)$-probabilistic explanation for a classifier $f$ and an instance $x$ is a subset $S$ of features such that $|S| \\leq k$ and $\\epsilon_{f,x}(S) \\leq \\epsilon$. Unfortunately, finding probabilistic explanations is generally more difficult than finding abductive explanations. Indeed, the problem of deciding whether a Boolean circuit representation of $f$ admits a $(k, \\epsilon)$-probabilistic explanation for an instance $x$ is complete for $N^{PP}$, a complexity class beyond the reach of modern solvers."}, {"title": "Model-Agnostic Explanations.", "content": "Unlike model-specific explanations, model-agnostic explanations do not rely on any assumptions about the inner workings of the predictive model $f$. Instead, $f$ is treated as a black box and accessed through membership queries. In post-hoc explanations, model-agnostic techniques seek a surrogate that approximates the model $f$ in some local neighborhood of the instance to be explained $x$. This neighborhood is explored using sampling and perturbation strategies. When the model is a classifier, the surrogate provided by the LIME approach [26] and its variants [9, 33] takes the form of a linear threshold function. Similarly, in the SHAP approach [21], the returned explanation is a linear function where each entry approximates the Shapley value of the corresponding feature that contributes to the outcome. Arguably, the ANCHORS approach [27] is the most relevant work to our study. It represents the surrogate of the model $f$ in the neighborhood of the instance $x$ as an easy-to-understand if-then rule. This rule can be described as a set $S$ of features which jointly determine the outcome $f(x)$.\nAlthough model-agnostic approaches are appealingly flexible, most of them are heuristic, meaning that they do not provide any theoretical guarantees regarding the quality or size of the explanations they generate. In practice, these approaches often return incorrect explanations [12], which can make them even less reliable. Such observations highlight the necessity of alternative techniques that preserve the flexibility of model agnosticism but provide precision and conciseness guarantees.\nTo the best of our knowledge, only Blanc et al. [4] have researched this area. Their objective is to find approximations of $(k, \\epsilon)$-probabilistic explanations for random data instances, using only query access to the model $f$. Based on some techniques in implicit learning, their result provides PAC-style guarantees on the quality and size of the approximate explanations. However, their approach is difficult to apply in practice since its runtime complexity is prohibitive. Our framework, in contrast, provides similar guarantees, using a restricted number of calls to an NP-oracle, implemented by a constraint solver."}, {"title": "Preliminaries", "content": "In model-agnostic approaches to Interpretable Machine Learning, the predictive model is treated as a black box. However, if we want to explain the model's response to input data in a way that humans can understand, we require a set of interpretable features [26, 27]. For example, when explaining a loan application, attributes such as \"stable income\", \"low debt-to-income ratio\", and \"proof of address\" can be considered as interpretable features. Any instance to be explained is a representation indicating the presence or absence of each of these features. In classification scenarios where the model's output is a label, the goal of the explainer is to pinpoint a few interpretable features that collectively determine this label with high precision.\nIn more formal terms, let $[d]$ denote the set ${1,\\dots,d}$ of interpretable features. The black-box models under consideration in this study are binary classifiers of the form $f : {0,1}^d \\rightarrow {0,1}$, Any input instance $x$ is a $d$-dimensional Boolean vector, where $x_j$ denotes the (Boolean) value of the $j$th interpretable"}, {"title": "The Framework", "content": "As we mentioned earlier, providing concise and precise explanations for arbitrary classifiers is a challenging task. Therefore, we cannot hope to offer polynomial-time algorithms for such a problem. So, in order to address this challenge, our framework relies on two key concepts. First, we approximate the explanation task by learning an IF-THEN rule where the body is a subset of literals occurring in the instance $x$ to explain, and the head is the predicted label $f(x)$. Next, we solve this learning task through constraint optimization. The low generalization"}, {"title": "From Explanations to Rules", "content": "For a set ${X_j}_{j=1}^d$ of Boolean variables, a literal is an expression of the form $X_j = v_j$, where $v_j$ is a Boolean value in ${0, 1}$, and a monomial is a conjunction of literals. An IF-THEN rule $R$ consists in a monomial, denoted $body(R)$, and a label in ${0,1}$, denoted $head(R)$. A $k$-monomial is a monomial of size at most $k$, and a $k$-rule is a rule whose body is a $k$-monomial. A rule $R$ is fired on an instance $z \\in {0,1}^d$ if its body is true for $z$, that is, $v_j = z_j$ for each literal $X_j = v_j$ occurring in $body(R)$. In that case, the label assigned to $z$ is $head(R)$. On the other hand, if the body of $R$ is false for $z$, then the corresponding label is $1 - head(R)$. A Boolean hypothesis $r : {0, 1}^d \\rightarrow {0,1}$ is representable by a rule $R$ if the following condition holds: for any $z \\in {0,1}^d$, $r(z) = head(R)$ if and only if $body(R)$ is true for $z$.\nBased on these standard notions, any explanation $S$ for a given data instance $x \\in {0,1}^d$ and a black-box model $f : {0,1}^d \\rightarrow {0,1}$ can be described by an IF-THEN rule where the body is the set of literals $X_j = x_j$ such that $j \\in S$, and the head is the prediction $f(x)$. By construction, this rule is fired on any instance $z \\in {0,1}^d$ if and only if $x_S$ covers $z$, that is, $x_S = z_S$. Thus, the IF-THEN rules of interest in this framework are those whose body is true for $x$ and whose head is $f(x)$.\nFrom this perspective, let $R_{f,x}$ denote the class of all Boolean hypotheses $r: {0,1} \\rightarrow {0,1}$ representable by a rule $R$, where $body(R)$ is a monomial over the set of literals ${X_j = x_j}_{j=1}^d$ and $head(R) = f(x)$. It is important to note that any hypothesis $r \\in R_{f,x}$ agrees with $f$ on $x$, that is, $r(x) = f(x)$. The (zero-one) loss of $r$ with respect to $f$ is defined as\n$L_f(r) = \\mathbb{P}_{z \\sim D}[r(z) \\neq f(z)]$\nwhere $z$ is again chosen randomly according to the distribution $D$.\nBy $R_{f,x,k}$ we denote the restriction of $R_{f,x}$ to the hypotheses representable by $k$-rules. The following result shows that algorithms for learning IF-THEN rules in $R_{f,x,k}$ can be used to explain predictions.\nLemma 1. Let $f : {0,1}^d \\rightarrow {0,1}$ be a black-box model, and $D$ be a probability distribution over ${0,1}^d$. Suppose there exists an algorithm with the following property: given $k \\in \\mathbb{N}$, and an instance $x \\in {0,1}^d$, the algorithm returns a rule representing a hypothesis $r \\in R_{f,x,k}$. Then, the rule representing $r$ determines a $k$-explanation $S$ for $x$ and $f$ satisfying\n$\\mathbb{E}_{x \\sim D} [\\epsilon_{f,x}(S)] \\leq L_f(r)$"}, {"title": "Learning Rules via Constraint Optimization", "content": "In light of Lemma 1, our framework's objective is to learn a hypothesis in $R_{f,x,k}$. Using the fact that each hypothesis in this class is representable by a rule whose body is included in the set of literals ${X_j = x_j}_{j=1}^d$, we will show that this objective is equivalent to the problem of (proper) learning monotone $k$-monomials. Monomials are known to be efficiently learnable in the realizable case where the target function $f$ can be represented by a monomial [31]. However, in the agnostic case, where $f$ is an arbitrary classifier, learning monotone monomials is NP-hard [19] and hard to approximate [10]. Nonetheless, monotone monomials are one of the simplest concept classes in machine learning, endowed with a low sample complexity. Hence, by framing the learning task as a constraint optimization problem, a constraint solver should be able to efficiently learn short rules in $R_{f,x,k}$ within a reasonable amount of time."}, {"title": "From Learning to Explanations", "content": "We can now combine the results obtained so far. Informally, Lemma 1 tells us that any classifier $r \\in R_{f,x,k}$ achieving a small loss $L_f(r)$ can provide a $k$-explanation $S$ for $x$ and $f$ with a small expected precision error. In addition, Lemma 2 shows us that the task of learning $r$ is equivalent to learning a monotone $k$-monomial, which can be achieved through constraint optimization. The next theorem establishes a connection between these results using standard tools in learning theory.\nTheorem 3. Let $D$ be a probability distribution over ${0,1}^d$ and $f : {0,1}^d \\rightarrow {0,1}$ be a black-box model accessed through an example oracle $EX(f,D)$. Then, for any data instance $x$ drawn at random from $D$, any $k \\in \\mathbb{N}$, and any $\\epsilon,\\delta \\in"}, {"title": "Experiments", "content": "In order to validate the effectiveness of our approach, we have considered various model-agnostic explanation tasks. The code was written using the Python language. Our experiments have been conducted on a Quad-core Intel XEON X5550 with 32GB of memory."}, {"title": "Experimental Setup", "content": "We conducted experiments on $B = 25$ tabular datasets (or benchmarks) from the OpenML repository. All datasets are classification tasks including both numerical and categorical attributes. To convert these raw attributes into interpretable binary features, we used a Scikit-Learn implementation of the K-bins discretization method with 3 bins per attribute. The resulting interpretable instances have an average dimension $d$ of 60, ranging from 12 to 352."}, {"title": "Experimental Results", "content": "An overview of our results on the 25 benchmarks, for $k = 5$, is presented in the performance of COP is not as good, which indicates that the black-box model $f$ is sometimes too complex to be approximated by a rule in $R_{f,x,k}$.\nWhen comparing the encodings COP and SAT, we can observe that COP generally produces more precise explanations than SAT. The reason behind this is that COP aims to minimize the loss $L_f(r)$ over $R_{f,x,k}$, which results in more accurate explanations. However, SAT is much faster than COP. Interestingly, SAT can achieve performances comparable to COP in just a few seconds for 11 of the 25 benchmarks, and sometimes even in less than one second.\nFigure 1 displays the precision errors of explanations produced by COP, SAT, and ANCHORS on four benchmarks. These precision errors are presented as a function of the number $m$ of samples used to learn the explanations. In particular, the plots illustrate how the performance of COP and SAT improves as the number of samples increases, while ANCHORS exhibits much less stability.\nOrthogonally, Figure 2 shows the precision errors as a function of the size $k$ of explanations for the explainers COP, SAT, and ANCHORS. The histograms indicate that both constraint encodings perform better than ANCHORS for all"}, {"title": "Conclusion", "content": "Interpretable Machine Learning faces the challenge of providing concise and precise explanations for black-box models. As most model-agnostic explainers are heuristic, we have proposed a learning framework that offers theoretical guarantees on the quality of explanations of a limited size. Our main result suggests that by learning an optimal monotone rule via constraint optimization, we can produce short explanations whose precision depends on the rule's ability to fit the black-box model. Evaluated on various benchmarks, our constraint encodings, COP and SAT, outperform the state-of-the-art ANCHORS method."}]}