{"title": "Croppable Knowledge Graph Embedding", "authors": ["Yushan Zhu", "Wen Zhang", "Zhiqiang Liu", "Mingyang Chen", "Lei Liang", "Huajun Chen"], "abstract": "Knowledge Graph Embedding (KGE) is a common method for Knowledge Graphs (KGs) to serve various artificial intelligence tasks. The suitable dimensions of the embeddings depend on the storage and computing conditions of the specific application scenarios. Once a new dimension is required, a new KGE model needs to be trained from scratch, which greatly increases the training cost and limits the efficiency and flexibility of KGE in serving various scenarios. In this work, we propose a novel KGE training framework MED, through which we could train once to get a croppable KGE model applicable to multiple scenarios with different dimensional requirements, sub-models of the required dimensions can be cropped out of it and used directly without any additional training. In MED, we propose a mutual learning mechanism to improve the low-dimensional sub-models performance and make the high-dimensional sub-models retain the capacity that low-dimensional sub-models have, an evolutionary improvement mechanism to promote the high-dimensional sub-models to master the knowledge that the low-dimensional sub-models can not learn, and a dynamic loss weight to balance the multiple losses adaptively. Experiments on 3 KGE models over 4 standard KG completion datasets, 3 real application scenarios over a real-world large-scale KG, and the experiments of extending MED to the language model BERT show the effectiveness, high efficiency, and flexible extensibility of MED.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KGs) are composed of triples representing facts in the form of (head entity, relation, tail entity), abbreviated as (h, r, t). KG has been widely used in recommendation systems [1, 2], information extraction [3, 4], question answering [5, 6] and other tasks. A common way to apply a knowledge graph is to represent the entities and relations in the knowledge graph into continuous vector spaces, called knowledge graph embedding (KGE) [7, 8], and then use the vector representation of entities and relations to serve a variety of tasks.\nKGEs with higher dimensions have greater expressive power and usually achieve better perfor- mance, but this also means a larger number of parameters and requires more storage space and computing resources [9, 10]. The appropriate dimensions of the KGE are different for different devices or scenarios. As shown in Fig. 1, large remote servers have large storage space and suffi- cient computing resources to support high-dimensional KGE with good performance, while small and medium-sized terminal devices, such as vehicle-mounted systems or smartphones, can only accept low-dimensional KGE due to limited computing power and storage capacity. Therefore, according to the conditions of different devices or scenes, people tend to train the KGE with ap- propriate dimensions and as high quality as possible. However, the challenge is that once a new dimension is required, a new KGE needs to be trained from scratch. Especially when only low- dimensional KGE can be applied, to ensure good performance, the additional model compression technology such as knowledge distillation [11, 9] is needed during training. This significantly increases training costs and limits KGE's efficiency and flexibility in serving different scenarios.\nThus a new concept \"croppable KGE\" is pro- posed and we are interested in the research ques- tion that is it possible to train a croppable KGE, with which KGEs of various required dimensions can be cropped out of it, directly be used without any additional training, and achieve promising performance?\nIn this work, our main idea of croppable KGE learning is to train an entire KGE that contains many sub-models of different dimensions in it. These sub-models share their embedding param- eters and are trained simultaneously. The goal is that the low-dimensional sub-models can benefit from the more expressive high-dimensional sub-models, while the high-dimensional sub-models retain the ability of the low-dimensional sub-models and master the knowledge that the low-dimensional sub- models cannot. Based on this idea, we propose a croppable KGE training framework MED, which consists of three main modules, the Mutual learning mechanism, the Evolutionary improvement mechanism, and the Dynamic loss weight to achieve the above purpose. Specifically, the mutual learning mechanism is based on knowledge distillation and it makes pairwise neighbor sub-models learn from each other, so that the performance of the lower-dimensional sub-model can be improved, and the higher-dimensional sub-model can retain the ability of the lower-dimensional sub-model. The evolutionary improvement mechanism helps the high-dimensional sub-model master more knowledge that the low-dimensional sub-model cannot by making the high-dimensional sub-model pay more attention to learn the triples that the low-dimensional sub-model can't correctly predict. The dynamic loss weight is designed to adaptively balance multiple losses of different sub-models according to their dimensions and further improve the overall performance.\nWe evaluate the effectiveness of our proposed MED by implementing it on three typical KGE methods and four standard KG datasets. We also prove its practical value by applying MED to a real-world large-scale KG and downstream tasks. Furthermore, we demonstrate the extensibility of MED by implementing it on language model BERT [12] and GLUE [13] benchmarks. The experimental results show that (1) MED successfully trains a croppable KGE model available for various dimensional requirements, which contains multiple parameter-shared sub-models of different dimensions that of high performance and can be used directly without additional training; (2) the training efficiency of MED is far higher than that of independently training multiple KGE models of different sizes or obtaining them by knowledge distillation. (3) MED can be flexibly extended to other neural network models besides KGE and achieve good performance; (4) our proposed mutual learning mechanism, evolutionary improvement mechanism, and dynamic loss weight are effective and necessary for MED to achieve overall optimal performance. In summary, our contributions are as follows:\n\u2022 We propose a new research question and task: training croppable KGE, from which KGEs of different dimensions can be cropped and used directly without any additional training.\n\u2022 We propose a novel framework MED, including a mutual learning mechanism, an evolution- ary improvement mechanism, and a dynamic loss weight, to ensure the overall performance of all sub-models during training the croppable KGE.\n\u2022 We experimentally prove that all sub-models of MED work well, especially the performance of the low-dimensional sub-models exceeding the KGE with the same dimension trained by the state-of-the-art distillation-based methods. MED also shows excellent performance in real-world applications and good extensibility on other types of neural networks."}, {"title": "2 Related Work", "content": "This work is to achieve a croppable KGE that meets different dimensional requirements. One of the most common methods to obtain a good-performance KGE of the target dimension is utilizing knowledge distillation with a high-dimensional powerful teacher KGE. Thus, we focus on two research fields most relevant to our work: knowledge graph embedding and knowledge distillation."}, {"title": "2.1 Knowledge Graph Embedding", "content": "Knowledge graph embedding (KGE) technology has been widely applied with the key idea of mapping entities and relations of a KG into continuous vector spaces as vector representations, which can further serve various KG downstream tasks. TransE [7] is the most representative translation- based KGE method by regarding the relation as a translation from the head to tail entity. Variants of TransE include TransH [14], TransR [15], TransD [16] and so on. RESCAL [17] is the first one based on vector decomposition, and then to improve it, DistMult [18], ComplEx [19], and SimplE [20] are proposed. RotatE [8] is a typical rotation-based method that regards the relation as the rotation between the head and tail entities. QuatE [21] and DihEdral [22] work with a similar idea. PairRE [23] uses two relation vectors to project the head and tail entities into an Euclidean space to encode complex relational patterns. With the development of neural networks, KGEs based on graph neural networks (GNNs) [24, 25, 26, 27] are also proposed. Although the KGEs are simple and effective, there is an obvious challenge: In different scenarios, the required KGE dimensions are different, which depends on the storage and computing resources of the device. It has to train a new KGE model from scratch for a new dimension requirement, which greatly increases the training cost and limits the flexibility for KGE to serve diversified scenarios."}, {"title": "2.2 Knowledge Distillation", "content": "High-dimensional KGEs have strong expression ability due to the large number of parameters, but require a lot of storage and computing resources, and are not suitable for all scenarios, especially small devices. To solve this problem, a common way is to compress a high-dimensional KGE to the target low-dimensional KGE by knowledge distillation [11, 28] and quantization [29, 30] technology.\nQuantization replaces continuous vector representations with lower-dimensional discrete codes. TS- CL [10] is the first work of KGE compression applying quantization. LightKG [31] uses a residual module to induce diversity among codebooks. However, quantization cannot improve the inference speed so it's still not suitable for devices with limited computing resources.\nKnowledge distillation (KD) has been widely used in Computer Vision [28] and Natural Language Processing [12, 32], helping reduce the model size and increase the inference speed. The core idea is to use the output of a large teacher model to guide the training of a small student model. DualDE [9] is a representative KD-based work to transfer the knowledge of high-dimensional KGE to low- dimensional KGE. It considers the mutual influences between the teacher and student and finetunes the teacher during training.MulDE [33] transfers the knowledge from multiple low-dimensional teacher models to a student model for hyperbolic KGE. ISD [34] improves low-dimensional KGE by making it play the teacher and student roles alternatively during training. Among these methods, DualDE [9] is more relevant to our work, both have the setting of high-dimensional teacher and low- dimensional student models. In this work, we propose a novel KD-based KGE training framework MED, one training can obtain a croppable KGE that meets multiple dimensional requirements."}, {"title": "3 Preliminary", "content": "Knowledge graph embedding (KGE) methods aim to express the relations between entities in a continuous vector space through a scoring function f. Specifically, given a knowledge graph G = (E, R, T) where E, R and T are the sets of entities, relations and all observed triples, we utilize the triple scoring function to measure the plausibility of triples in the embedding space for a triple (h, r, t) where h \u2208 E,r \u2208 R and t \u2208 E. The triple score function is denoted as S(h,r,t) = f(h, r, t) with embeddings of head entity h, relation r and tail entity t as input. Table 1 summarizes the scoring functions of some popular KGE methods, where o is the Hadamard product. The higher the triple score, the more likely the model is to judge the triples as true."}, {"title": "4 MED Framework", "content": "As shown in Fig. 2, our croppable KGE framework MED contains multiple (let's say n) sub-models of different dimensions in it, denoted as Mi(i = 1,2..., n) with dimension of di. Each sub-model Mi is composed of the first di dimensions of the whole embedding and the score of triple (h, r, t) output by Mi is $s(h,r,t) = f(h[0:d_i], r[0:d_i], t[0:d_i])$, where h[0:di] represents the first di elements of vector h. The param- eters of sub-model M\u2081 are shared by all sub-models Mj(i<j<n) that are higher-dimensional than it. The number of sub-models n and the specific dimension of each sub-model di can be set according to the actual ap- plication needs. For low-dimensional sub-models, we want to improve their performance as much as possible. For high-dimensional sub-models, we hope they cover the abilities that low-dimensional sub-models already have and master the knowledge that low-dimensional sub-models can not learn well, that is, they need to cor- rectly predict not only the triples that low-dimensional sub-models can predict correctly but also those low- dimensional sub-models predict wrongly.\nMED is based on knowledge distillation [11, 35, 12] technique that the student learns by fitting the hard (ground-truth) label and the soft label from the teacher simultaneously. In MED, we first propose a mutual learning mechanism that makes low-dimensional sub-models learn from high-dimensional sub-models to achieve better performance, and makes high-dimensional sub-models also learn from low-dimensional sub-models to retain the abilities that low-dimensional sub-models already have. Then, we propose an evolutionary improvement mechanism to enable high-dimensional sub-models to master the knowledge that the low-dimensional sub-models can not learn well. Finally, we train MED with dynamic loss weight to adaptively balance multiple optimization objectives of sub-models."}, {"title": "4.1 Mutual Learning Mechanism", "content": "We treat each sub-model Mi as the student of its higher-dimensional neighbor sub-model Mi+1 to achieve better performance, since high-dimensional KGEs usually have more expressive power than low-dimensional ones due to more parameters [10, 9]. We also treat sub-model Mi as the student of its lower-dimensional neighbor sub-model Mi-1, so the higher-dimensional sub-model can review what the lower-dimensional sub-model has learned and retain the low-dimensional one's existing abilities. Thus, pairwise neighbor sub-models serve as both teachers and students, learning from each other. The mutual learning loss between each pair of neighbor sub-models is\n$L_{ML}^{i-1,2} = \\sum_{(h,r,t) \\in T \\cup T^-} -ds(s(h,r,t), s(h,r,t)), 1 < i < n,$\nwhere $s(h,r,t)$ is the score of triple (h, r, t) output by sub-model M\u2081 and reflects the possibility that this triplet exists, T\u2212 = E \u00d7 R \u00d7 E \\ T is the negative triple set, n is the number of sub-models, and ds is Huber loss [36] with \u03b4 = 1 commonly used in knowledge distillation for KGE [9]. MED makes each sub-model only learn from its neighbor sub-models. The advantage is that this not only reduces the computational complexity of training but also makes every pair of teacher and student models have a relatively small dimension gap, which is important and effective because the large gap of dimensions between teacher and student will destroy the distillation effect [28, 9]."}, {"title": "4.2 Evolutionary Improvement Mechanism", "content": "The hard (ground-truth) label is the other important supervision signal during training in knowledge distillation [11]. High-dimensional sub-models need to master triples that low-dimensional sub- models can not learn well, that is, high-dimensional sub-models need to correctly predict those positive (negative) triples that are wrongly predicted to be negative (positive) by low-dimensional sub-models. In MED, for a given triple (h, r, t), the optimization weight in sub-model Mi for it depends on the triple score output by the previous sub-model Mi-1.\nFor a positive triple, the optimization weight of the model Mi for it is negatively correlated with its score by the model Mi-1. Specifically, the higher its score from the model Mi-1 (meaning that Mi-1 has been able to correctly judge it as a positive sample), the lower the optimization weight of the model M\u2081 for it, and the lower its score from the model Mi-1 (meaning that Mi\u22121 wrongly judges it as a negative sample), the higher the optimization weight of the model M\u2081 for it because Mi-1 cannot predict this triple well. The optimization weight of Mi for the positive triple is\n$pos_{h,r,t}^i = \\begin{cases}\n\\frac{exp \\omega_1/s_{h,r,t}^{i-1}}{\\sum_{(h,r,t) \\in T_{batch}} exp \\omega_1/s(h,r,t)^{i-1}}, & \\text{if } 1 < i < n;\\\\\n\\frac{1}{|T_{batch}|}, & \\text{if } i = 1,\\end{cases}$\nwhere $s(h,r,t)^{i-1}$ is the score for triple (h, r, t) output by the sub-model Mi\u22121, Tbatch is the set of positive triples within a batch, and \u03c9\u2081 is a learnable scaling parameter. Conversely, for a negative triple, the optimization weight of the model Mi for it is positively correlated with its score by the model Mi-1. The optimization weight of M\u00bf for the negative triple is\n$neg_{h,r,t}^i = \\begin{cases}\n\\frac{exp \\omega_2\\cdot s_{h,r,t}^{i-1}}{\\sum_{(h,r,t) \\in T_{batch}} exp \\omega_2\\cdot s(h,r,t)^{i-1}}, & \\text{if } 1 < i < n;\\\\\n\\frac{1}{|T_{batch}|}, & \\text{if } i = 1,\\end{cases}$\nwhere Tbatch is the set of negative triples within a batch, and \u03c9\u2082 is a learnable scaling parameter. Therefore, the evolutionary improvement loss of the sub-model Mi is\n$L_{EI}^i = - \\sum_{(h,r,t)\\in T\\cup T^-} pos_{h,r,t}^i \\cdot y \\log(\\sigma(s_{h,r,t})) + neg_{h,r,t}^i \\cdot (1-y) \\log(1 - \\sigma(s_{h,r,t})),$\nwhere \u03c3 is the Sigmoid activation function, y is the ground-truth label of the triple (h, r, t), and it is 1 for positive triples and 0 for negative ones. In each sub-model, different hard (ground-truth) label loss weights are set for different triples, and the high-dimensional sub-model will pay more attention to learn the triple that the low-dimensional sub-model can not learn well."}, {"title": "4.3 Dynamic Loss Weight", "content": "Since MED involves the optimization of multiple sub-models, we set dynamic loss weights during training. Initially, low-dimensional sub-models prioritize learning from high-dimensional sub-models to improve performance. This means low-dimensional sub-models rely more on soft label information, so for low-dimensional sub-models, evolutionary improvement loss should account for less than mutual learning loss. Conversely, high-dimensional sub-models should focus more on capturing knowledge that low-dimensional models lack, while mitigating the impact of low-quality outputs from low-dimensional models to maintain their good performance, that is, high-dimensional sub-models rely more on hard label information. So for high-dimensional sub-models, evolutionary improvement loss should account for more than mutual learning loss. For a teacher-student pair, their mutual learning loss acts on both teacher and student models simultaneously, so the effect of mutual learning loss for them is theoretically the same. We set different evolutionary improvement loss weights for different sub-models, and the final training loss function of MED is\n$L = \\sum_{i=2}^{n} L_{ML}^{i-1,i} + \\sum_{i=1}^{n} \\frac{L_{EI}^i}{\\frac{\\sum_{i=2}^{n} L_{ML}^{i-1,i}}{n}+exp(\\omega_3\\cdot d_i)} . $\nwhere w3 is a learnable scaling parameter, and di is the dimension of the ith sub-model."}, {"title": "5 Experiment", "content": "We evaluate MED on typical KGE and GLUE benchmarks and particularly answer the following research questions: (RQ1) Is it capable for MED to train a croppable KGE at once that multiple sub-models of different dimensions can be cropped from it and all achieve promising performance? (RQ2) Can MED finally achieve parameter-efficient KGE models? (RQ3) Does MED work in real-world applications? (RQ4) Can MED be extended to other neural networks besides KGE?"}, {"title": "5.1 Experiment Setting", "content": "MED is universal and can be applied to any KGE method with a triple score function, we select three commonly used KGE methods as examples: TransE [7], RotatE [8] and PairRE [23], the triple score functions are described in Table 1."}, {"title": "5.2 Performance Comparison", "content": "MED outperforms baselines in almost all settings, especially for the extremely low dimensions. On WN18RR with d=10, MED achieves an improvement of 14.9% and 15.1% on TransE, 8.4% and 6.6% on RotatE, 29.4% and 10.6% on PairRE compared with the best MRR and Hit@10 of baselines. We can observe a similar phenomenon on FB15K237. This benefits from the rich knowledge sources of low-dimensional models in MED: For sub-model Mi, Mi+1 is the teacher directly next to it, while Mi+2 can also indirectly affect M\u2081 by directly affecting Mi+1. Theoretically, all higher-dimensional sub-models can finally transfer their knowledge to low-dimensional sub-models through stepwise propagation. Although such stepwise propagation may have negative effects on high-dimensional models by bringing low-quality knowledge from low-dimensional sub-models, the evolutionary improvement mechanism in MED weakens the damage and makes high-dimensional ones still achieve competitive performance than directly trained KGEs as in Fig. 3. We also find that Ext- based methods perform extremely unstable: Ext, Ext-L, and Ext-V work worse than DT except on WN18RR with TransE, indicating that only considering the importance of each dimension is not enough to guarantee the performance of all sub-models. More results and ablation studies are in Appendix A and Appendix B."}, {"title": "5.3 Parameter efficiency of MED", "content": "In Table 4, we compare our sub-models of suitable low dimensions to parameter-efficient KGEs especially proposed for large-scale KGs including NodePiece [45] and EARL [40]. In the case that the number of model parameters is roughly equivalent, the performance of the sub-models of MED exceeds that of the specialized parameter-efficient KGE methods. This demonstrates sub-models of our method are parameter efficient. More importantly, it can provide parameter-efficient models of different size for applications."}, {"title": "5.4 MED in real applications", "content": "We apply the trained croppable KGE with TransE on SKG to three real applications: the user labeling task on servers and the product recommendation task on PCs and mobile phones. Table 5 shows that our croppable user embeddings substantially exceed all baselines including directly trained (DT), the best baseline DualDE, and a common dimension reduction method in industry principal components analysis (PCA) on MDT Imax. Notably, the excellent performance on the mobile phone task (which can only carry embeddings with a maximum di- mension of 10 limited by storage and computing resources) demonstrates the enormous practical value of our approach. More application details are in Appendix C."}, {"title": "5.5 Extend MED to Neural Networks", "content": "To verify the extensibility of our method to other neural networks, we take the language model BERT [12] as an example. To ensure the consistency of the experimental environment as much as possible, we uniformly adopt distillation methods implemented based on Hugging Face Transformers [46] as baselines. Following previous works [32, 35, 47, 48], we do not use pre-training distillation settings and only distill at the fine-tuning stage. More experimental details are in Appendix D."}, {"title": "5.6 Analysis of MED", "content": ""}, {"title": "5.6.1 Training efficiency", "content": "We report the training time of obtaining 64 models of all sizes (d=10, 20, ..., 640) by different methods in Table 7. For DT, the training time cost is the sum of the time of directly training 64 KGE models of all sizes in turn. For the Ext-based baselines, the training time cost is the same and is equal to the time of training a dn-dimensional KGE model since the time of arranging dimensions is very short and negligible. For the KD-based baselines, the training time cost is the sum of the time of training the dn-dimensional teacher model and distilling 63 student models (d=10, 20, ..., 630) in turn. All training is performed on a single NVIDIA Tesla A100 40GB GPU for fair comparison. For TA and DualDE on FB15K237, we don't train student models of all 63 sizes, which is estimated to take more than 400 hours on each KGE method. Compared with directly trained (DT) models of all sizes in turn, MED accelerates by up to 10x"}, {"title": "5.6.2 Whether high-dimensional sub-models cover the capabilities of low-dimensional ones", "content": "If a high-dimensional model retains the ability of lower-dimensional models, it should correctly pre- dict all triples that the lower-dimensional model can predict. We count the percentage of triples in test set that meet the condition that if the smallest sub-model that can correctly predict a given triple is Mi, all higher-dimensional sub-models (Mi+1, Mi+2, ..., Mn) also correctly predict it, and denote the re- sult as the ability retention ratio (ARR). We use Hit@ 10 to judge whether a triple is correctly predicted, that is, Mi correctly predicts a triple if Mi scores this triple in the top 10 among all candidate triples."}, {"title": "5.6.3 Visual analysis of embedding", "content": "We select four primary entity categories ('organization', 'sports', 'location', and 'music') that contain more than 300 enti- ties in FB15K237, and randomly select 250 entities for each. We cluster these en- tities' embeddings of 3 different dimen- sions (d=10, 100, 600) by the t-SNE al- gorithm, and the clustering results are visualized in Fig. 5. Under the same di- mension, the clustering result of MED is always the best, followed by DualDE, while the result of Ext-V is generally poor, which is consistent with the con- clusion in Section 5.2. We also find some special phenomenons for MED when dimension increases: 1) the nodes of the 'sports' gradually become two clusters meaning MED learns more fine-grained cat- egory information as dimension increases. and 2) the relative distribution among different categories hardly changes and shows a trend of \u201cinheritance\u201d and \u201cimprovement\u201d. This further proves MED achieves our expectation that high-dimensional sub-models retain the ability of low-dimensional sub-models, and can learn more knowledge than low-dimensional sub-models."}, {"title": "6 Conclusion", "content": "In this work, we propose a novel KGE training framework, MED, that trains a croppable KGE at once, and then sub-models of various required dimensions can be cropped out from it and used directly without additional training. In MED, we propose the mutual learning mechanism to improve low- dimensional sub-models performance and make the high-dimensional sub-models retain the ability of the low-dimensional ones, the evolutionary improvement mechanism to motivate high-dimensional sub-models to master more knowledge that low-dimensional ones cannot, and the dynamic loss weight to adaptively balance multiple losses. The experimental results show the effectiveness and high efficiency of our method, where all sub-models achieve promising performance, especially the performance of low-dimensional sub-models is greatly improved. In future work, we will further explore the more fine-grained information encoding ability of each sub-model."}, {"title": "B.1 Mutual Learning Mechanism (MLM)", "content": "We remove the mutual learning mechanism from MED and keep the other parts unchanged, where (5) is rewritten as\n$L = \\sum_{i=1}^{n} \\frac{exp (w_3 \\cdot d_i)}{d_n} L_{EI}^i.$\nFrom the result of \"MED w/o MLM\" in Table 12, we find that after removing the mutual learning mechanism, the performance of low-dimensional sub-models deteriorates seriously since the low- dimensional sub-models can not learn from the high-dimensional sub-models. For example, the MRR of the 10-dimensional sub-model decreased by 12.4%, and the MRR of the 20-dimensional sub-model decreased by 10%. While the performance degradation of the high-dimensional sub-model is not particularly obvious, and the MRR of the highest-dimensional sub-model (dim = 640) is not worse than that of MED, which is because to a certain degree, removing the mutual learning mechanism also avoids the negative influence to high-dimensional sub-models from low-dimensional sub-models. On the whole, this mechanism greatly improves the performance of low-dimensional sub-models."}, {"title": "B.2 Evolutionary Improvement Mechanism (EIM)", "content": "In this part, we replace evolutionary improvement loss $L_{EI}^i$ in (5) with the regular KGE loss $L_{KGE}$: \n$L_{KGE} = \\sum_{(h,r,t) \\in T \\cup T^-} y \\log \\sigma(s(h,r,t)) + (1-y) \\log(1 - \\sigma(s(h,r,t))).$\nFrom the result of \"MED w/o EIM\" in Table 12, we find that removing the evolutionary improvement mechanism mainly degrades the performance of high-dimensional sub-models. While due to the existence of the mutual learning mechanism, the low-dimensional sub-model can still learn from the high-dimensional sub-model, so as to ensure the certain performance of the low-dimensional sub- model. In addition, we also find that as the dimension increases to a certain extent, the performance of the sub-model does not improve, and even begins to decline. We guess that this is because the mutual learning mechanism makes every pair of neighbor sub-models learn from each other, resulting in some low-quality or wrong knowledge gradually transferring from the low-dimensional sub-models to the high-dimensional sub-models, and when the evolutionary improvement mechanism is removed, the high-dimensional sub-models can no longer correct the wrong information from the low-dimensional sub-models. The higher the dimension of the sub-model, the more the accumulated error, so the performance of the high-dimensional sub-models is seriously damaged. On the whole, this mechanism mainly helps to improve the effect of high-dimensional sub-models."}, {"title": "B.3 Dynamic Loss Weight (DLW)", "content": "To study the effect of the dynamic loss weight, we fix the ratio of all mutual learning losses to all evolutionary improvement losses as 1 : 1, and (5) is rewritten as\n$L = \\frac{\\sum_{i=2}^{n} L_{ML}^{i-1,i}}{n} + \\frac{\\sum_{i=1}^{n} L_{EI}^i}{n}$.\nAccording to the result of \"MED w/o DLW\" in Table 12, the overall results of \"MED w/o DLW\" are in the middle of the results of \"MED w/o MLM\" and \"MED w/o EIM\": the performance of the low-dimensional sub-model is better than that of \u201cMED w/o MLM\", and the performance of the high-dimensional sub-model is better than that of \"MED w/o EIM\u201d. On the whole, its results are more similar to \"MED w/o EIM\", that is, the performance of the low-dimensional sub-model does not change much, while the performance of the high-dimensional sub-model decreases more significantly. We believe that for the high-dimensional sub-model, the proportion of mutual learning loss is still too large, which makes it more negatively affected by the low-dimensional sub-model. This result indicates that the dynamic loss weight plays a role in adaptively balancing multiple losses and contributes to improving overall performance."}, {"title": "C Details of applying the trained KGE by MED to real applications", "content": "The SKG is used in many tasks related to users, and injecting user embeddings trained over SKG into downstream task models is a common and practical way.\nUser labeling is one of the common user management tasks that e-commerce platforms run on backend servers. We model user labeling as a multiclass classification task for user embeddings with a 2-layer MLP:\n$L = - \\frac{1}{U} \\sum_{i=1}^{U} \\sum_{j=1}^{C} Y_{ij} \\log(MLP(u_i)),$\nwhere ui is the i-th user's embedding, the label Yij = 1 if user u\u2081 belongs to class clsj, otherwise Yij = 0.\nThe product recommendation task is to properly recommend items to users that users will interact with a high probability and it often runs on terminal devices. Following PKGM [2], which recommends items to users using the neural collaborative filtering (NCF) [50] framework with the help of pre-trained user embeddings as service vectors, we add trained user embeddings over SKG as service vectors to NCF. In NCF, the MLP layer is used to learn item-user interactions based on the latent feature of the user and item, that is, for a given user-item pair useri itemj, the interaction function is\n$MLP (p_i, q_j) = MLP([p_i; q_j]),$\nwhere pi and q\u2081 are latent feature vectors of user and item learned in NCF. We add the trained user embedding ui to NCF's MLP layer and rewrite Equation (10) as\n$MLP (p_i, q_j, u_i) = MLP([p_i; q_j; u_i]),$\nand the other parts of NCF stay the same as in PKGM [2].\nWe train entity and relation embeddings for SKG based on TransE [7] and input the trained entity (user) embedding into Equation (9) and Equation (11)."}, {"title": "D Details of extending MED to language model BERT-base", "content": ""}, {"title": "D.1 Dataset and Evaluation Metric", "content": "For the experiments extending MED to BERT", "tasks": "Paraphrase Similarity Matching"}, {"title": "Croppable Knowledge Graph Embedding", "authors": ["Yushan Zhu", "Wen Zhang", "Zhiqiang Liu", "Mingyang Chen", "Lei Liang", "Huajun Chen"], "abstract": "Knowledge Graph Embedding (KGE) is a common method for Knowledge Graphs (KGs) to serve various artificial intelligence tasks. The suitable dimensions of the embeddings depend on the storage and computing conditions of the specific application scenarios. Once a new dimension is required, a new KGE model needs to be trained from scratch, which greatly increases the training cost and limits the efficiency and flexibility of KGE in serving various scenarios. In this work, we propose a novel KGE training framework MED, through which we could train once to get a croppable KGE model applicable to multiple scenarios with different dimensional requirements, sub-models of the required dimensions can be cropped out of it and used directly without any additional training. In MED, we propose a mutual learning mechanism to improve the low-dimensional sub-models performance and make the high-dimensional sub-models retain the capacity that low-dimensional sub-models have, an evolutionary improvement mechanism to promote the high-dimensional sub-models to master the knowledge that the low-dimensional sub-models can not learn, and a dynamic loss weight to balance the multiple losses adaptively. Experiments on 3 KGE models over 4 standard KG completion datasets, 3 real application scenarios over a real-world large-scale KG, and the experiments of extending MED to the language model BERT show the effectiveness, high efficiency, and flexible extensibility of MED.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KGs) are composed of triples representing facts in the form of (head entity, relation, tail entity), abbreviated as (h, r, t). KG has been widely used in recommendation systems [1, 2], information extraction [3, 4], question answering [5, 6] and other tasks. A common way to apply a knowledge graph is to represent the entities and relations in the knowledge graph into continuous vector spaces, called knowledge graph embedding (KGE) [7, 8], and then use the vector representation of entities and relations to serve a variety of tasks.\nKGEs with higher dimensions have greater expressive power and usually achieve better perfor- mance, but this also means a larger number of parameters and requires more storage space and computing resources [9, 10]. The appropriate dimensions of the KGE are different for different devices or scenarios. As shown in Fig. 1, large remote servers have large storage space and suffi- cient computing resources to support high-dimensional KGE with good performance, while small and medium-sized terminal devices, such as vehicle-mounted systems or smartphones, can only accept low-dimensional KGE due to limited computing power and storage capacity. Therefore, according to the conditions of different devices or scenes, people tend to train the KGE with ap- propriate dimensions and as high quality as possible. However, the challenge is that once a new dimension is required, a new KGE needs to be trained from scratch. Especially when only low- dimensional KGE can be applied, to ensure good performance, the additional model compression technology such as knowledge distillation [11, 9] is needed during training. This significantly increases training costs and limits KGE's efficiency and flexibility in serving different scenarios.\nThus a new concept \"croppable KGE\" is pro- posed and we are interested in the research ques- tion that is it possible to train a croppable KGE, with which KGEs of various required dimensions can be cropped out of it, directly be used without any additional training, and achieve promising performance?\nIn this work, our main idea of croppable KGE learning is to train an entire KGE that contains many sub-models of different dimensions in it. These sub-models share their embedding param- eters and are trained simultaneously. The goal is that the low-dimensional sub-models can benefit from the more expressive high-dimensional sub-models, while the high-dimensional sub-models retain the ability of the low-dimensional sub-models and master the knowledge that the low-dimensional sub- models cannot. Based on this idea, we propose a croppable KGE training framework MED, which consists of three main modules, the Mutual learning mechanism, the Evolutionary improvement mechanism, and the Dynamic loss weight to achieve the above purpose. Specifically, the mutual learning mechanism is based on knowledge distillation and it makes pairwise neighbor sub-models learn from each other, so that the performance of the lower-dimensional sub-model can be improved, and the higher-dimensional sub-model can retain the ability of the lower-dimensional sub-model. The evolutionary improvement mechanism helps the high-dimensional sub-model master more knowledge that the low-dimensional sub-model cannot by making the high-dimensional sub-model pay more attention to learn the triples that the low-dimensional sub-model can't correctly predict. The dynamic loss weight is designed to adaptively balance multiple losses of different sub-models according to their dimensions and further improve the overall performance.\nWe evaluate the effectiveness of our proposed MED by implementing it on three typical KGE methods and four standard KG datasets. We also prove its practical value by applying MED to a real-world large-scale KG and downstream tasks. Furthermore, we demonstrate the extensibility of MED by implementing it on language model BERT [12] and GLUE [13] benchmarks. The experimental results show that (1) MED successfully trains a croppable KGE model available for various dimensional requirements, which contains multiple parameter-shared sub-models of different dimensions that of high performance and can be used directly without additional training; (2) the training efficiency of MED is far higher than that of independently training multiple KGE models of different sizes or obtaining them by knowledge distillation. (3) MED can be flexibly extended to other neural network models besides KGE and achieve good performance; (4) our proposed mutual learning mechanism, evolutionary improvement mechanism, and dynamic loss weight are effective and necessary for MED to achieve overall optimal performance. In summary, our contributions are as follows:\n\u2022 We propose a new research question and task: training croppable KGE, from which KGEs of different dimensions can be cropped and used directly without any additional training.\n\u2022 We propose a novel framework MED, including a mutual learning mechanism, an evolution- ary improvement mechanism, and a dynamic loss weight, to ensure the overall performance of all sub-models during training the croppable KGE.\n\u2022 We experimentally prove that all sub-models of MED work well, especially the performance of the low-dimensional sub-models exceeding the KGE with the same dimension trained by the state-of-the-art distillation-based methods. MED also shows excellent performance in real-world applications and good extensibility on other types of neural networks."}, {"title": "2 Related Work", "content": "This work is to achieve a croppable KGE that meets different dimensional requirements. One of the most common methods to obtain a good-performance KGE of the target dimension is utilizing knowledge distillation with a high-dimensional powerful teacher KGE. Thus, we focus on two research fields most relevant to our work: knowledge graph embedding and knowledge distillation."}, {"title": "2.1 Knowledge Graph Embedding", "content": "Knowledge graph embedding (KGE) technology has been widely applied with the key idea of mapping entities and relations of a KG into continuous vector spaces as vector representations, which can further serve various KG downstream tasks. TransE [7] is the most representative translation- based KGE method by regarding the relation as a translation from the head to tail entity. Variants of TransE include TransH [14], TransR [15], TransD [16] and so on. RESCAL [17] is the first one based on vector decomposition, and then to improve it, DistMult [18], ComplEx [19], and SimplE [20] are proposed. RotatE [8] is a typical rotation-based method that regards the relation as the rotation between the head and tail entities. QuatE [21] and DihEdral [22] work with a similar idea. PairRE [23] uses two relation vectors to project the head and tail entities into an Euclidean space to encode complex relational patterns. With the development of neural networks, KGEs based on graph neural networks (GNNs) [24, 25, 26, 27] are also proposed. Although the KGEs are simple and effective, there is an obvious challenge: In different scenarios, the required KGE dimensions are different, which depends on the storage and computing resources of the device. It has to train a new KGE model from scratch for a new dimension requirement, which greatly increases the training cost and limits the flexibility for KGE to serve diversified scenarios."}, {"title": "2.2 Knowledge Distillation", "content": "High-dimensional KGEs have strong expression ability due to the large number of parameters, but require a lot of storage and computing resources, and are not suitable for all scenarios, especially small devices. To solve this problem, a common way is to compress a high-dimensional KGE to the target low-dimensional KGE by knowledge distillation [11, 28] and quantization [29, 30] technology.\nQuantization replaces continuous vector representations with lower-dimensional discrete codes. TS- CL [10] is the first work of KGE compression applying quantization. LightKG [31] uses a residual module to induce diversity among codebooks. However, quantization cannot improve the inference speed so it's still not suitable for devices with limited computing resources.\nKnowledge distillation (KD) has been widely used in Computer Vision [28] and Natural Language Processing [12, 32], helping reduce the model size and increase the inference speed. The core idea is to use the output of a large teacher model to guide the training of a small student model. DualDE [9] is a representative KD-based work to transfer the knowledge of high-dimensional KGE to low- dimensional KGE. It considers the mutual influences between the teacher and student and finetunes the teacher during training.MulDE [33] transfers the knowledge from multiple low-dimensional teacher models to a student model for hyperbolic KGE. ISD [34] improves low-dimensional KGE by making it play the teacher and student roles alternatively during training. Among these methods, DualDE [9] is more relevant to our work, both have the setting of high-dimensional teacher and low- dimensional student models. In this work, we propose a novel KD-based KGE training framework MED, one training can obtain a croppable KGE that meets multiple dimensional requirements."}, {"title": "3 Preliminary", "content": "Knowledge graph embedding (KGE) methods aim to express the relations between entities in a continuous vector space through a scoring function f. Specifically, given a knowledge graph G = (E, R, T) where E, R and T are the sets of entities, relations and all observed triples, we utilize the triple scoring function to measure the plausibility of triples in the embedding space for a triple (h, r, t) where h \u2208 E,r \u2208 R and t \u2208 E. The triple score function is denoted as S(h,r,t) = f(h, r, t) with embeddings of head entity h, relation r and tail entity t as input. Table 1 summarizes the scoring functions of some popular KGE methods, where o is the Hadamard product. The higher the triple score, the more likely the model is to judge the triples as true."}, {"title": "4 MED Framework", "content": "As shown in Fig. 2, our croppable KGE framework MED contains multiple (let's say n) sub-models of different dimensions in it, denoted as Mi(i = 1,2..., n) with dimension of di. Each sub-model Mi is composed of the first di dimensions of the whole embedding and the score of triple (h, r, t) output by Mi is $s(h,r,t) = f(h[0:d_i], r[0:d_i], t[0:d_i])$, where h[0:di] represents the first di elements of vector h. The param- eters of sub-model M\u2081 are shared by all sub-models Mj(i<j<n) that are higher-dimensional than it. The number of sub-models n and the specific dimension of each sub-model di can be set according to the actual ap- plication needs. For low-dimensional sub-models, we want to improve their performance as much as possible. For high-dimensional sub-models, we hope they cover the abilities that low-dimensional sub-models already have and master the knowledge that low-dimensional sub-models can not learn well, that is, they need to cor- rectly predict not only the triples that low-dimensional sub-models can predict correctly but also those low- dimensional sub-models predict wrongly.\nMED is based on knowledge distillation [11, 35, 12] technique that the student learns by fitting the hard (ground-truth) label and the soft label from the teacher simultaneously. In MED, we first propose a mutual learning mechanism that makes low-dimensional sub-models learn from high-dimensional sub-models to achieve better performance, and makes high-dimensional sub-models also learn from low-dimensional sub-models to retain the abilities that low-dimensional sub-models already have. Then, we propose an evolutionary improvement mechanism to enable high-dimensional sub-models to master the knowledge that the low-dimensional sub-models can not learn well. Finally, we train MED with dynamic loss weight to adaptively balance multiple optimization objectives of sub-models."}, {"title": "4.1 Mutual Learning Mechanism", "content": "We treat each sub-model Mi as the student of its higher-dimensional neighbor sub-model Mi+1 to achieve better performance, since high-dimensional KGEs usually have more expressive power than low-dimensional ones due to more parameters [10, 9]. We also treat sub-model Mi as the student of its lower-dimensional neighbor sub-model Mi-1, so the higher-dimensional sub-model can review what the lower-dimensional sub-model has learned and retain the low-dimensional one's existing abilities. Thus, pairwise neighbor sub-models serve as both teachers and students, learning from each other. The mutual learning loss between each pair of neighbor sub-models is\n$L_{ML}^{i-1,2} = \\sum_{(h,r,t) \\in T \\cup T^-} -ds(s(h,r,t), s(h,r,t)), 1 < i < n,$\nwhere $s(h,r,t)$ is the score of triple (h, r, t) output by sub-model M\u2081 and reflects the possibility that this triplet exists, T\u2212 = E \u00d7 R \u00d7 E \\ T is the negative triple set, n is the number of sub-models, and ds is Huber loss [36] with \u03b4 = 1 commonly used in knowledge distillation for KGE [9]. MED makes each sub-model only learn from its neighbor sub-models. The advantage is that this not only reduces the computational complexity of training but also makes every pair of teacher and student models have a relatively small dimension gap, which is important and effective because the large gap of dimensions between teacher and student will destroy the distillation effect [28, 9]."}, {"title": "4.2 Evolutionary Improvement Mechanism", "content": "The hard (ground-truth) label is the other important supervision signal during training in knowledge distillation [11]. High-dimensional sub-models need to master triples that low-dimensional sub- models can not learn well, that is, high-dimensional sub-models need to correctly predict those positive (negative) triples that are wrongly predicted to be negative (positive) by low-dimensional sub-models. In MED, for a given triple (h, r, t), the optimization weight in sub-model Mi for it depends on the triple score output by the previous sub-model Mi-1.\nFor a positive triple, the optimization weight of the model Mi for it is negatively correlated with its score by the model Mi-1. Specifically, the higher its score from the model Mi-1 (meaning that Mi-1 has been able to correctly judge it as a positive sample), the lower the optimization weight of the model M\u2081 for it, and the lower its score from the model Mi-1 (meaning that Mi\u22121 wrongly judges it as a negative sample), the higher the optimization weight of the model M\u2081 for it because Mi-1 cannot predict this triple well. The optimization weight of Mi for the positive triple is\n$pos_{h,r,t}^i = \\begin{cases}\n\\frac{exp \\omega_1/s_{h,r,t}^{i-1}}{\\sum_{(h,r,t) \\in T_{batch}} exp \\omega_1/s(h,r,t)^{i-1}}, & \\text{if } 1 < i < n;\\\\\n\\frac{1}{|T_{batch}|}, & \\text{if } i = 1,\\end{cases}$\nwhere $s(h,r,t)^{i-1}$ is the score for triple (h, r, t) output by the sub-model Mi\u22121, Tbatch is the set of positive triples within a batch, and \u03c9\u2081 is a learnable scaling parameter. Conversely, for a negative triple, the optimization weight of the model Mi for it is positively correlated with its score by the model Mi-1. The optimization weight of M\u00bf for the negative triple is\n$neg_{h,r,t}^i = \\begin{cases}\n\\frac{exp \\omega_2\\cdot s_{h,r,t}^{i-1}}{\\sum_{(h,r,t) \\in T_{batch}} exp \\omega_2\\cdot s(h,r,t)^{i-1}}, & \\text{if } 1 < i < n;\\\\\n\\frac{1}{|T_{batch}|}, & \\text{if } i = 1,\\end{cases}$\nwhere Tbatch is the set of negative triples within a batch, and \u03c9\u2082 is a learnable scaling parameter. Therefore, the evolutionary improvement loss of the sub-model Mi is\n$L_{EI}^i = - \\sum_{(h,r,t)\\in T\\cup T^-} pos_{h,r,t}^i \\cdot y \\log(\\sigma(s_{h,r,t})) + neg_{h,r,t}^i \\cdot (1-y) \\log(1 - \\sigma(s_{h,r,t})),$\nwhere \u03c3 is the Sigmoid activation function, y is the ground-truth label of the triple (h, r, t), and it is 1 for positive triples and 0 for negative ones. In each sub-model, different hard (ground-truth) label loss weights are set for different triples, and the high-dimensional sub-model will pay more attention to learn the triple that the low-dimensional sub-model can not learn well."}, {"title": "4.3 Dynamic Loss Weight", "content": "Since MED involves the optimization of multiple sub-models, we set dynamic loss weights during training. Initially, low-dimensional sub-models prioritize learning from high-dimensional sub-models to improve performance. This means low-dimensional sub-models rely more on soft label information, so for low-dimensional sub-models, evolutionary improvement loss should account for less than mutual learning loss. Conversely, high-dimensional sub-models should focus more on capturing knowledge that low-dimensional models lack, while mitigating the impact of low-quality outputs from low-dimensional models to maintain their good performance, that is, high-dimensional sub-models rely more on hard label information. So for high-dimensional sub-models, evolutionary improvement loss should account for more than mutual learning loss. For a teacher-student pair, their mutual learning loss acts on both teacher and student models simultaneously, so the effect of mutual learning loss for them is theoretically the same. We set different evolutionary improvement loss weights for different sub-models, and the final training loss function of MED is\n$L = \\sum_{i=2}^{n} L_{ML}^{i-1,i} + \\sum_{i=1}^{n} \\frac{L_{EI}^i}{\\frac{\\sum_{i=2}^{n} L_{ML}^{i-1,i}}{n}+exp(\\omega_3\\cdot d_i)} . $\nwhere w3 is a learnable scaling parameter, and di is the dimension of the ith sub-model."}, {"title": "5 Experiment", "content": "We evaluate MED on typical KGE and GLUE benchmarks and particularly answer the following research questions: (RQ1) Is it capable for MED to train a croppable KGE at once that multiple sub-models of different dimensions can be cropped from it and all achieve promising performance? (RQ2) Can MED finally achieve parameter-efficient KGE models? (RQ3) Does MED work in real-world applications? (RQ4) Can MED be extended to other neural networks besides KGE?"}, {"title": "5.1 Experiment Setting", "content": "MED is universal and can be applied to any KGE method with a triple score function, we select three commonly used KGE methods as examples: TransE [7], RotatE [8] and PairRE [23], the triple score functions are described in Table 1."}, {"title": "5.2 Performance Comparison", "content": "MED outperforms baselines in almost all settings, especially for the extremely low dimensions. On WN18RR with d=10, MED achieves an improvement of 14.9% and 15.1% on TransE, 8.4% and 6.6% on RotatE, 29.4% and 10.6% on PairRE compared with the best MRR and Hit@10 of baselines. We can observe a similar phenomenon on FB15K237. This benefits from the rich knowledge sources of low-dimensional models in MED: For sub-model Mi, Mi+1 is the teacher directly next to it, while Mi+2 can also indirectly affect M\u2081 by directly affecting Mi+1. Theoretically, all higher-dimensional sub-models can finally transfer their knowledge to low-dimensional sub-models through stepwise propagation. Although such stepwise propagation may have negative effects on high-dimensional models by bringing low-quality knowledge from low-dimensional sub-models, the evolutionary improvement mechanism in MED weakens the damage and makes high-dimensional ones still achieve competitive performance than directly trained KGEs as in Fig. 3. We also find that Ext- based methods perform extremely unstable: Ext, Ext-L, and Ext-V work worse than DT except on WN18RR with TransE, indicating that only considering the importance of each dimension is not enough to guarantee the performance of all sub-models. More results and ablation studies are in Appendix A and Appendix B."}, {"title": "5.3 Parameter efficiency of MED", "content": "In Table 4, we compare our sub-models of suitable low dimensions to parameter-efficient KGEs especially proposed for large-scale KGs including NodePiece [45] and EARL [40]. In the case that the number of model parameters is roughly equivalent, the performance of the sub-models of MED exceeds that of the specialized parameter-efficient KGE methods. This demonstrates sub-models of our method are parameter efficient. More importantly, it can provide parameter-efficient models of different size for applications."}, {"title": "5.4 MED in real applications", "content": "We apply the trained croppable KGE with TransE on SKG to three real applications: the user labeling task on servers and the product recommendation task on PCs and mobile phones. Table 5 shows that our croppable user embeddings substantially exceed all baselines including directly trained (DT), the best baseline DualDE, and a common dimension reduction method in industry principal components analysis (PCA) on MDT Imax. Notably, the excellent performance on the mobile phone task (which can only carry embeddings with a maximum di- mension of 10 limited by storage and computing resources) demonstrates the enormous practical value of our approach. More application details are in Appendix C."}, {"title": "5.5 Extend MED to Neural Networks", "content": "To verify the extensibility of our method to other neural networks, we take the language model BERT [12] as an example. To ensure the consistency of the experimental environment as much as possible, we uniformly adopt distillation methods implemented based on Hugging Face Transformers [46] as baselines. Following previous works [32, 35, 47, 48], we do not use pre-training distillation settings and only distill at the fine-tuning stage. More experimental details are in Appendix D."}, {"title": "5.6 Analysis of MED", "content": ""}, {"title": "5.6.1 Training efficiency", "content": "We report the training time of obtaining 64 models of all sizes (d=10, 20, ..., 640) by different methods in Table 7. For DT, the training time cost is the sum of the time of directly training 64 KGE models of all sizes in turn. For the Ext-based baselines, the training time cost is the same and is equal to the time of training a dn-dimensional KGE model since the time of arranging dimensions is very short and negligible. For the KD-based baselines, the training time cost is the sum of the time of training the dn-dimensional teacher model and distilling 63 student models (d=10, 20, ..., 630) in turn. All training is performed on a single NVIDIA Tesla A100 40GB GPU for fair comparison. For TA and DualDE on FB15K237, we don't train student models of all 63 sizes, which is estimated to take more than 400 hours on each KGE method. Compared with directly trained (DT) models of all sizes in turn, MED accelerates by up to 10x"}, {"title": "5.6.2 Whether high-dimensional sub-models cover the capabilities of low-dimensional ones", "content": "If a high-dimensional model retains the ability of lower-dimensional models, it should correctly pre- dict all triples that the lower-dimensional model can predict. We count the percentage of triples in test set that meet the condition that if the smallest sub-model that can correctly predict a given triple is Mi, all higher-dimensional sub-models (Mi+1, Mi+2, ..., Mn) also correctly predict it, and denote the re- sult as the ability retention ratio (ARR). We use Hit@ 10 to judge whether a triple is correctly predicted, that is, Mi correctly predicts a triple if Mi scores this triple in the top 10 among all candidate triples."}, {"title": "5.6.3 Visual analysis of embedding", "content": "We select four primary entity categories ('organization', 'sports', 'location', and 'music') that contain more than 300 enti- ties in FB15K237, and randomly select 250 entities for each. We cluster these en- tities' embeddings of 3 different dimen- sions (d=10, 100, 600) by the t-SNE al- gorithm, and the clustering results are visualized in Fig. 5. Under the same di- mension, the clustering result of MED is always the best, followed by DualDE, while the result of Ext-V is generally poor, which is consistent with the con- clusion in Section 5.2. We also find some special phenomenons for MED when dimension increases: 1) the nodes of the 'sports' gradually become two clusters meaning MED learns more fine-grained cat- egory information as dimension increases. and 2) the relative distribution among different categories hardly changes and shows a trend of \u201cinheritance\u201d and \u201cimprovement\u201d. This further proves MED achieves our expectation that high-dimensional sub-models retain the ability of low-dimensional sub-models, and can learn more knowledge than low-dimensional sub-models."}, {"title": "6 Conclusion", "content": "In this work, we propose a novel KGE training framework, MED, that trains a croppable KGE at once, and then sub-models of various required dimensions can be cropped out from it and used directly without additional training. In MED, we propose the mutual learning mechanism to improve low- dimensional sub-models performance and make the high-dimensional sub-models retain the ability of the low-dimensional ones, the evolutionary improvement mechanism to motivate high-dimensional sub-models to master more knowledge that low-dimensional ones cannot, and the dynamic loss weight to adaptively balance multiple losses. The experimental results show the effectiveness and high efficiency of our method, where all sub-models achieve promising performance, especially the performance of low-dimensional sub-models is greatly improved. In future work, we will further explore the more fine-grained information encoding ability of each sub-model."}, {"title": "B.1 Mutual Learning Mechanism (MLM)", "content": "We remove the mutual learning mechanism from MED and keep the other parts unchanged, where (5) is rewritten as\n$L = \\sum_{i=1}^{n} \\frac{exp (w_3 \\cdot d_i)}{d_n} L_{EI}^i.$\nFrom the result of \"MED w/o MLM\" in Table 12, we find that after removing the mutual learning mechanism, the performance of low-dimensional sub-models deteriorates seriously since the low- dimensional sub-models can not learn from the high-dimensional sub-models. For example, the MRR of the 10-dimensional sub-model decreased by 12.4%, and the MRR of the 20-dimensional sub-model decreased by 10%. While the performance degradation of the high-dimensional sub-model is not particularly obvious, and the MRR of the highest-dimensional sub-model (dim = 640) is not worse than that of MED, which is because to a certain degree, removing the mutual learning mechanism also avoids the negative influence to high-dimensional sub-models from low-dimensional sub-models. On the whole, this mechanism greatly improves the performance of low-dimensional sub-models."}, {"title": "B.2 Evolutionary Improvement Mechanism (EIM)", "content": "In this part, we replace evolutionary improvement loss $L_{EI}^i$ in (5) with the regular KGE loss $L_{KGE}$: \n$L_{KGE} = \\sum_{(h,r,t) \\in T \\cup T^-} y \\log \\sigma(s(h,r,t)) + (1-y) \\log(1 - \\sigma(s(h,r,t))).$\nFrom the result of \"MED w/o EIM\" in Table 12, we find that removing the evolutionary improvement mechanism mainly degrades the performance of high-dimensional sub-models. While due to the existence of the mutual learning mechanism, the low-dimensional sub-model can still learn from the high-dimensional sub-model, so as to ensure the certain performance of the low-dimensional sub- model. In addition, we also find that as the dimension increases to a certain extent, the performance of the sub-model does not improve, and even begins to decline. We guess that this is because the mutual learning mechanism makes every pair of neighbor sub-models learn from each other, resulting in some low-quality or wrong knowledge gradually transferring from the low-dimensional sub-models to the high-dimensional sub-models, and when the evolutionary improvement mechanism is removed, the high-dimensional sub-models can no longer correct the wrong information from the low-dimensional sub-models. The higher the dimension of the sub-model, the more the accumulated error, so the performance of the high-dimensional sub-models is seriously damaged. On the whole, this mechanism mainly helps to improve the effect of high-dimensional sub-models."}, {"title": "B.3 Dynamic Loss Weight (DLW)", "content": "To study the effect of the dynamic loss weight, we fix the ratio of all mutual learning losses to all evolutionary improvement losses as 1 : 1, and (5) is rewritten as\n$L = \\frac{\\sum_{i=2}^{n} L_{ML}^{i-1,i}}{n} + \\frac{\\sum_{i=1}^{n} L_{EI}^i}{n}$.\nAccording to the result of \"MED w/o DLW\" in Table 12, the overall results of \"MED w/o DLW\" are in the middle of the results of \"MED w/o MLM\" and \"MED w/o EIM\": the performance of the low-dimensional sub-model is better than that of \u201cMED w/o MLM\", and the performance of the high-dimensional sub-model is better than that of \"MED w/o EIM\u201d. On the whole, its results are more similar to \"MED w/o EIM\", that is, the performance of the low-dimensional sub-model does not change much, while the performance of the high-dimensional sub-model decreases more significantly. We believe that for the high-dimensional sub-model, the proportion of mutual learning loss is still too large, which makes it more negatively affected by the low-dimensional sub-model. This result indicates that the dynamic loss weight plays a role in adaptively balancing multiple losses and contributes to improving overall performance."}, {"title": "C Details of applying the trained KGE by MED to real applications", "content": "The SKG is used in many tasks related to users, and injecting user embeddings trained over SKG into downstream task models is a common and practical way.\nUser labeling is one of the common user management tasks that e-commerce platforms run on backend servers. We model user labeling as a multiclass classification task for user embeddings with a 2-layer MLP:\n$L = - \\frac{1}{U} \\sum_{i=1}^{U} \\sum_{j=1}^{C} Y_{ij} \\log(MLP(u_i)),$\nwhere ui is the i-th user's embedding, the label Yij = 1 if user u\u2081 belongs to class clsj, otherwise Yij = 0.\nThe product recommendation task is to properly recommend items to users that users will interact with a high probability and it often runs on terminal devices. Following PKGM [2], which recommends items to users using the neural collaborative filtering (NCF) [50] framework with the help of pre-trained user embeddings as service vectors, we add trained user embeddings over SKG as service vectors to NCF. In NCF, the MLP layer is used to learn item-user interactions based on the latent feature of the user and item, that is, for a given user-item pair useri itemj, the interaction function is\n$MLP (p_i, q_j) = MLP([p_i; q_j]),$\nwhere pi and q\u2081 are latent feature vectors of user and item learned in NCF. We add the trained user embedding ui to NCF's MLP layer and rewrite Equation (10) as\n$MLP (p_i, q_j, u_i) = MLP([p_i; q_j; u_i]),$\nand the other parts of NCF stay the same as in PKGM [2].\nWe train entity and relation embeddings for SKG based on TransE [7] and input the trained entity (user) embedding into Equation (9) and Equation (11)."}, {"title": "D Details of extending MED to language model BERT-base", "content": ""}, {"title": "D.1 Dataset and Evaluation Metric", "content": "For the experiments extending MED to BERT, we adopt the common GLUE [13] benchmark for evaluation. To be specific, we use the development set of the GLUE benchmark which includes four tasks: Paraphrase Similarity Matching, Sentiment Classification, Natural Language Inference, and Linguistic Acceptability. For Paraphrase Similarity Matching, we use MRPC [51], QQP and STS-B [52] for evaluation. For Sentiment Classification, we use SST-2 [53]. For Natural Language Inference, we use MNLI [54], QNLI [55], and RTE for evaluation. In terms of evaluation metrics, we follow previous work [12, 32]. For MRPC and QQP, we report F1 and accuracy. For STS-B, we consider Pearson and Spearman correlation as our metrics. The other tasks use accuracy as the metric. For MNLI, the results of MNLI-m and MNLI-"}]}]}