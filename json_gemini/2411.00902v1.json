{"title": "Differentiable architecture search with multi-dimensional attention for spiking neural networks", "authors": ["Yilei Man", "Linhai Xie", "Shushan Qiao", "Yumei Zhou", "Delong Shang"], "abstract": "Spiking Neural Networks (SNNs) have gained enormous popularity in the field of artificial intelligence\ndue to their low power consumption. However, the majority of SNN methods directly inherit the\nstructure of Artificial Neural Networks (ANN), usually leading to sub-optimal model performance\nin SNNs. To alleviate this problem, we integrate Neural Architecture Search (NAS) method and\npropose Multi-Attention Differentiable Architecture Search (MA-DARTS) to directly automate the\nsearch for the optimal network structure of SNNs. Initially, we defined a differentiable two-level\nsearch space and conducted experiments within micro architecture under a fixed layer. Then, we\nincorporated a multi-dimensional attention mechanism and implemented the MA-DARTS algorithm\nin this search space. Comprehensive experiments demonstrate our model achieves state-of-the-art\nperformance on classification compared to other methods under the same parameters with 94.40%\naccuracy on CIFAR10 dataset and 76.52% accuracy on CIFAR100 dataset. Additionally, we monitored\nand assessed the number of spikes (NoS) in each cell during the whole experiment. Notably, the\nnumber of spikes of the whole model stabilized at approximately 110K in validation and 100k in\ntraining on datasets.", "sections": [{"title": "1. Introduction", "content": "Spiking Neural Networks (SNNs) have been widely ex-\nplored in the artificial intelligence field in recent years due\nto their sparsity, low power consumption, and their ability\nto mimic dynamic neuronal properties, earning them the\ntitle of \"third-generation neural networks\" Maass (1997).\nHowever, the existence of binary spikes makes training\nSNNs a challenging task.\nExisting SNN training algorithms can be roughly di-\nvided into two categories: those based on neurobiologi-\ncal characteristic training algorithms, and those adapted\nfrom traditional neural network training methods for spik-\ning neural networks. Among the former, methods repre-\nsented by Spike Timing-Dependent Plasticity(STDP) Ca-\nporale and Dan (2008) have achieved remarkable results\nonly in shallow networks for lacking global information\nIakymchuk, Rosado-Mu\u00f1oz, Guerrero-Mart\u00ednez, Bataller-\nMompe\u00e1n and Franc\u00e9s-V\u00edllora (2015). Therefore, most al-\ngorithms have been enhanced based on the work of tradi-\ntional artificial neural networks(ANNs). However, directly\ninheriting network structures from ANN will inevitably lead\nto accuracy loss in deep spiking neural networks. Many\nresearchers Lee, Sarwar, Panda, Srinivasan and Roy (2020),\nSengupta, Ye, Wang, Liu and Roy (2019), Hu, Tang and Pan\n(2021), Han, Srinivasan and Roy (2020), Fang, Yu, Chen,\nHuang, Masquelier and Tian (2021a) have developed im-\nproved residual networks considering aspects like residual\nblock design, compensation mechanisms, and new neuron\nmodels, demonstrating decent performance in terms of ac-\ncuracy, parameters and time steps. However, these methods,\nwhich are artificially designed sub-optimal network struc-\ntures within a fixed framework, still lag behind traditional\nneural networks of the same structure in terms of perfor-\nmance.\nNeural Architecture Search (NAS) first proposed by\nZoph and Le (2016) is an automated approach for design-\ning effective neural networks, which often achieves more\nremarkable results than artificially designed networks in\nmany scenarios Xie, Chen, Bi, Wei, Xu, Wang, Chen, Xiao,\nChang, Zhang et al. (2021). Therefore, this method can also\nbe applied to exploit the potential of the network structure\nof SNN. In this work, we introduce a significant neural\narchitecture search method to automate the search for the\noptimal network structure of SNNs within a specific search\nspace.\nSpecifically, we initially established a search space con-\nsisting of two levels, including macro backbone architecture\nand micro candidate architecture. To effectively find the op-\ntimal network structure in this search space and consider the\ntemporal information of SNN when processing event-driven\ndata, we proposed a Multi-Attention Differentiable Archi-\ntecture Search (MA-DARTS) algorithm, incorporating an\nattention mechanism Bahdanau, Cho and Bengio (2014) at a\ncrucial stage based on the original algorithm Liu, Simonyan\nand Yang (2018). Unlike traditional types of attention mech-\nanisms, this attention mechanism not only includes channel\nattention Hu, Shen and Sun (2018) and spatial attention but\nalso temporal attention Yao, Zhao, Zhang, Hu, Deng, Tian,\nXu and Li (2022); Yao, Gao, Zhao, Wang, Lin, Yang and\nLi (2021). We conducted comprehensive experimental veri-\nfication of the proposed method on two datasets, CIFAR10\nand CIFAR100, evaluating and analyzing the network from\naccuracy, network parameters, and the number of spikes. The\nresult demonstrates that MA-DARTS not only outperforms\nmanually designed SNN network structures in terms of accu-\nracy but also achieves more lightweight network parameters.\nMoreover, the addition of an attention mechanism can also\nbring accuracy gains to the model with a minimal number\nof parameters. The contributions can be summarized as\nfollows:\n\u2022 To simplify the search process, we first established a\ndifferentiable two-level search space and then searched\nfor micro architecture within fixed layers. This method\nenables efficient search with limited computational re-\nsources.\n\u2022 We enhanced DARTS by incorporating multi-dimensional\nattention mechanisms within the defined search space\nand proposed MA-DARTS algorithm. Our algorithm\neffectively improved the model's classification accuracy\nwith the addition of a minimal number of parameters.\n\u2022 We conducted comprehensive experimental verification\non CIFAR10 and CIFAR100. The results show that our\nmethod can identify network structures with superior\naccuracy and parameter metrics at the same network size\ncompared to other methods."}, {"title": "2. Related work", "content": "There has been ongoing research in the field of SNN\nusing NAS method in recent years. Liquid State Machines\n(LSMs), which are a type of recurrent SNN, have the ability\nto emulate the structure and functionality of the human\nbrain, making them more biologically realistic. However,\nonly a few LSM models have demonstrated superior per-\nformance compared to traditional artificial neural networks\nwhen it comes to solving real-world classification or regres-\nsion problems. Zhou, Jin and Ding (2020) proposed a pow-\nerful method combining LSM with evolutionary algorithm,\nproving to be efficient and effective in optimizing the pa-\nrameters and architecture of the LSM. Furthermore, another\nstudy Tian, Qu, Wang, Hu, Li and Xu (2021) introduced a\nthree-step heuristic search technique to handle the enormous\nsearch space, and it demonstrates its effectiveness on the\nN-MNIST image dataset and FSDD voice dataset using the\nsimulated annealing algorithm.\nActually, some researchers explored the combination of\ngenetic algorithms in their studies. They designed their spe-\ncial fitness functions and achieved favorable SNN structures.\nGAQ-SNN(Nguyen, Tran and Iacopi (2022)) focuses on\nreducing the memory weight overhead due to the constrained\non-chip memory available in edge computing platforms.\nResults from simulations and hardware implementations\nindicate that GAQ-SNN can reduce memory storage by\nup to 12.5 times compared to the baseline network while\nmaintaining an accuracy loss of only 0.6%.\nA notable characteristic of spiking neural networks is\ntheir efficiency in processing spatio-temporal information\nthrough discrete and sparse spikes. However, some works\nfocus only on the accuracy of the algorithm, while ne-\nglecting the power consumption of the design. Auto-SNN\nNa, Mok, Park, Lee, Choe and Yoon (2022) analyzed the\npower consumption in terms of neural network structure,\nnetwork pooling layer, and downsampling layer and com-\nbined with evolutionary algorithms successfully designed\nnetwork structures with low spike firing rates.\nThere are also some significant NAS methods for de-\nsigning neural architecture in SNN. SpikeDHS Che, Leng,\nZhang, Zhang, Meng, Cheng, Guo and Liao (2022) used\nmultiple gradients to enhance Differentiable Architecture\nSearch (DARTS) Liu et al. (2018) and completed target\ndetection in event-driven scenarios, and SNASNet Kim, Li,\nPark, Venkatesha and Panda (2022) proposed a NAS algo-\nrithm that doesn't require training by incorporating Sparsity-\nAware Hamming Distance. These NAS methods can ef-\nfectively search for the optimal neural network structure,\namong which the family of differentiable NAS methods\nLiang, Zhang, Sun, He, Huang, Zhuang and Li (2019);\nChen, Xie, Wu and Tian (2019); Chu, Zhou, Zhang and Li\n(2020) are significantly faster than evolutionary algorithms\nand reinforcement learning Zoph and Le (2016) methods in\nsearch efficiency. Despite their excellent performance, they\noccupy more resources in terms of time step and model\nsize. Therefore we propose a lightweight method that almost\nminimizes the sacrifice of model performance to design the\nstructure of spiking neural networks."}, {"title": "3. Methodology", "content": "There are many neuron models for SNNs, and Leaky\nIntegrate and Fired (LIF) neuron model is the most widely\nused model Gerstner and Kistler (2002); Hunsberger and\nEliasmith (2015). Like other works Na et al. (2022); Che\net al. (2022); Wu, Deng, Li, Zhu, Xie and Shi (2019), we\nconvert the continuous dynamic properties into a discrete\niterative version, which can be described as:\n$u_i^{t,n} = u_i^{t-1,n} (1 \u2013 \u03c4) + x_i^{t,n},$\n$o_i^n = Spike(u_i^n - V_{th}),$\n$x_i^{t,n} = \\sum_{j=1}^{I(n-1)} w_{ij}^{n} * o_j^{t-1,n}.$\nIn the above equation, superscript t denotes the t-th time step,\nsuperscript n denotes the n-th layer, subscript i denotes the i-\nth neuron among a specific layer, 1() denotes the number of\nneurons. $u_i^{t,n}$ denotes the accumulated membrane potential\non the i-th neuron at the t-th time step and the n-th layer,\n$o_i^n \u2208 {0,1}$ denotes the output of the i-th neuron at the\nt-th time step and the n-th layer, where $o_i^1$ = 1 denotes a\nneuron firing a spike and $o_i^1$ = 0 denotes no neuronal activity,\nand $x_i^{t,n}$ denotes the total incoming neuronal activity from\nthe presynaptic layer received by the i-th neuron at the t-\nth time step and the n-th layer. $w_{ij}^n$ denotes the synaptic\nweight from the j-th neurons in the previous layer to the i-th\nneurons in the next layer, \u03c4 denotes the membrane potential\ndecay constant, $V_{th}$ denotes the threshold for neuron spike\ngeneration, and spike() is the neuron activation function,\ndefined as spike(x) = 1 or 0 for x > 0 or x < 0. In our\nsetting, \u03c4 = 0.2, $V_{th}$ = 0.5.\nEach neuron at the t-th time step and the n-th layer cor-\nresponds to 2 important parameters $u_i^{t,n}$ and $o_i^{t,n}$, where $u_i^{t,n}$ is\ncalculated by the membrane potential $u_i^{t-1,n}$ at the previous\nmoment that fades with time and the total incoming neuronal\nactivity from the presynaptic layer and $o_i^{t,n}$ is calculated by\nthe effective membrane potential value passing through the\nneuron activation function."}, {"title": "3.2. The DARTS Search Space", "content": "Search space Elsken, Metzen and Hutter (2019) deter-\nmines the scope of neural network structure, and formulating\nits scale is a meaningful and challenging task. A larger\nsearch space can allow the search scale to cover more neu-\nral network structures, thereby increasing the likelihood of\nfinding more effective ones. Correspondingly, a larger search\nspace also can make it difficult for the search algorithm\nto converge or require more time. Meanwhile, the scale of\nthe search space is typically related to model evaluation,\ntherefore determining search space is actually a complex\ntask.\nThere are various types of search spaces in the NAS field,\nand they generally consist of two parts: macro architecture\nand micro architecture (Cell). Macro architecture determines\nthe main backbone of the network while the micro architec-\nture determines the details of the network units. Most NAS\nalgorithms nowadays choose to fix the macro architecture\nand search for more remarkable micro architecture to better\ndetermine the scale of the search space. Similarly, we imple-\nmented MA-DARTS to search for the optimal spiking neural\nnetwork structure in the DARTS Search Space with fixed\nmacro architecture. The candidate operations in standard\nmicro architecture are defined as sep-conv-3x3, sep-conv-\n5x5, dil-conv-3x3, dil-conv-5x5, max-pool-3x3, avg-pool-\n3x3, None, and skip-connect. Considering that a cell unit\nhas 4 intermediate nodes, then both the number of possible\ntypes of reduction cell and normal cell are $C_8^2C_8^2C_8^2.78 \u2248\n1.037\u00d710^9$. Specifically, for a network with 5 layers, the size\nof the entire search space is approximately 5 \u00d7 109. In the\nsubsequent experiments, we specified the number of cells as\n5 or 6 at the stage of search, and the corresponding macro\narchitecture during the search process is shown in Figure 1."}, {"title": "3.3. Multi-dimensional Attention", "content": "The final step of the traditional DARTS searching cell\nis to obtain the output by directly concatenating four nodes\nwithout distinguishing the relationships between the nodes.\nA previous study Xu, Xie, Zhang, Chen, Qi, Tian and Xiong\n(2019) proposed to further set a channel merging parameter\nsimilar to the structural parameter, but this approach would\ngreatly increase the model complexity and make training\nmore complicated. Therefore, we propose to directly insert\nan attention mechanism into the concatenation step, which\ncan effectively distinguish the information of each node\nand only bring minimal model complexity according to our\nexperiments. In the traditional ANN field, attention mech-\nanisms are usually divided into channel attention, spatial\nattention, and channel-spatial mixed attention mechanisms.\nSimilarly, when dealing with input that contains temporal\ndimension information, the type pf attention mechanisms\ncould be extended, which leads to the emergence of temporal\nattention mechanisms. The principles of the three attention\nmechanisms are shown in Figure 2. Through different at-\ntention modules, attention weight can be obtained. Specif-\nically, the weight of spatial attention is two-dimensional"}, {"title": "3.4. Multi-Attention Differentiable Architecture Search", "content": "The family of differentiable NAS methods has demon-\nstrated significant results in classification and event-based\ndeep stereo tasks in the field of SNN. Nevertheless, most\nmethods neglect the consideration of model size and limited\nresources. Inspired by this idea, we propose a lightweight\nNAS method aiming to show exceptional results in SNN\nwhile maintaining a small model size.\nThe overview of MA-DARTS is shown as in Figure 5.\nThe macro architecture comprises 6 layers of arranged cells\nand the extra layers in the blue box are prepared for higher\nresolution datasets. Both types of cells share similarities,\nwith the main distinction in the preprocessing step: the\nreduction cell decreases the feature map size, while the\nnormal cell preserves it. Each type of cell consists of 4\ninternal nodes, where the later nodes rely on the previous\nnodes. The connection of internal nodes can be described as\nbelow:\n$b_i = \\sum_{j<i} O^{(i,j)} (b_j)$\n$O^{(i,j)} (b_j) = \\sum_{o \\in O} \\frac{e^{\u03b1_{o}^{(i,j)}}}{\\sum_{o' \\in O} e^{\u03b1_{o'}^{(i,j)}}} o(b_j)$\nHere, b\u2081 denotes the i-th internal node, the notation (i,j)(bj)\nindicates the mixed operation of the directed edge from i to\nj applied to b\u2081. The output of this operation applied to bj\nis denoted as o(bj), where the operation is chosen from the\ncandidate set O. And a denotes the architecture parameter\nthat can be learned on the validation dataset. To enhance\ngradient propagation, mitigate issues such as gradient van-\nishing or exploding, and expedite model convergence, we\nincorporate an auxiliary classifier Nekrasov, Chen, Shen and\nReid (2019) after the 5th cell. This auxiliary classifier aids\nin better gradient flow throughout the network.\nThe pseudo code of the algorithm is summarized in\nAlgorithm 1. In the beginning, we initialize the neuron\nthreshold $V_{th}$ and membrane potential decay constant \u03c4 as\nwe describe in subsection 3.1. We then specify the size of\nthe search space named the DARTS search space which is\nthe macro structure of the network and the time window\nused for processing temporal information. Meanwhile, we\nstipulate our candidate set as we describe in subsection 3.2.\nIn the process of forwarding the input, the dimensions of\nthe data are not properly distinguished in the step of con-\ncatenation. Therefore, we directly incorporate an attention\nmechanism into the concatenation step and use attention\nmechanism functions as we describe in subsection 3.3. Once\nwe calculate corresponding loss, such as cross-entropy (CE)\nloss, we then update architecture parameter a and weight\nw respectively based on the original DARTS algorithm.\nWe also evaluate the model on validation dataset and the\noptimal network structure is obtained according to accuracy.\nThe connections among the nodes still are mixed operations\nin the search stage, but the final structure comes from the\ninternal nodes connection path of the cell with the two\nmaximum architecture parameter a. After we derive the\narchitecture based on the corresponding a, we then need to\nretrain the final architecture with specified epochs."}, {"title": "4. Experimental results and analysis", "content": "The data in CIFAR10 and CIFAR100 datasets consists of\n50k/10k images respectively with a resolution of 32\u00d732. A\nstandardized normalization preprocessing step is applied to\nall datasets. In the experiments, each dataset is divided into\nthree parts for training, testing, and validating. During the\nfirst stage, we maintain a 1:1 ratio between the training and\ntesting sets. In the second stage, we adjust the ratio between\nthe training and validation sets to 5:1."}, {"title": "4.2. Implementation details", "content": "The training process of our experiment consists of two\nsteps. In the search stage, we adopt 5 layers of cells, each\ncontaining 4 intermediate nodes. We employ the same can-\ndidate operation set as described before and uniformly use\nLIF Spike() as the activation function. The search stage\nlasts for 40 epochs with a batch size of 64. Regarding weight\noptimization, we utilize SGD optimizer with momentum of\n0.9 and cosine annealing learning rate of 0.0050. As for opti-\nmizing the architecture parameter a, we use adam optimizer\nwith learning rate of 0.0003. With 16 initial channels and\n2 time windows, the search process takes approximately 2\nGPU days on 2 NVIDIA 3090 (24GB) GPUs.\nAfter completing the search, refinement is required. We\ntrain for 600 epochs with initial channel expansion and dif-\nferent batch sizes. We use SGD optimizer with momentum\nof 0.9 and Cosine Annealing learning rate of 0.0025. The\nnumber of cells we set varies between 5 or 6 depending on\nthe dataset. In this process, we deploy different attention\nmechanisms behind the final cell to obtain the weight of\ndifferent nodes. With 64 initial channels and 2 time windows,\nthe refinement process takes 2.08 GPU days on 2 NVIDIA\n3090 (24GB) GPUs."}, {"title": "4.3. Results", "content": "Works Lee et al. (2020); Wu et al. (2019); Fang et al.\n(2021b); Zheng et al. (2021) represent a type of direct\ntraining SNN method using spike-based approaches, while\nstudies Han et al. (2020); Chen et al. (2022); Kim et al.\n(2018) represent a type of ANN-to-SNN conversion method.\nTable 1 and Table 2 present a comparison of different\nmethods in terms of accuracy, time steps, and parameters\non the CIFAR10 and CIFAR100 datasets with various ini-\ntial channels. In CIFAR10, we observe that both spike-\nbased methods and conversion methods are closely related\nto the time steps and initial channels, but their performance\nis not satisfactory in terms of accuracy. Compared with\nNAS methods, our proposed method yields superior perfor-\nmance under identical parameter settings. Specifically, our\nmodel achieves an accuracy of 94.21% with CT-DARTS and\n94.40% with MA-DARTS in CIFAR10 and 75.82% with\nCT-DARTS and 76.52% with MA-DARTS in CIFAR100\nunder 64 initial channels and 2 timesteps. Additionally, our\nmodel not only provides high classification accuracy but\nalso has a significantly smaller parameter size compared to\nother methods. Among work until 2023, although SpikeDHS\npresents state-of-the-art accuracy performance in CIFAR10,\nour method can achieve similar results by extending the time\nsteps. Specifically, our method exceeds other methods and\nattains an accuracy of 95.21% on the CIFAR10 dataset with\n64 initial channels and 6 time steps. Compared to work in\n2023 Hu et al. (2023); Lian et al. (2023); Bu et al. (2023),\nthe performance gap between these methods is not obvious,\nbut inheriting directly from the deep classical network ar-\nchitecture will result in a large number of parameters. In a\nword, as the initial channels and time steps grow, our model's\naccuracy can be enhanced while maintaining a lighter model\nparameter size compared to other methods.\nWe provide the final architecture of normal cell and\nreduction cell our method automatically searches in the first\nstage on CIFAR10 and CIFAR100 as shown in Figure 6\n(a)~(d). For both kinds of cells, there is no jump connec-\ntion and average pooling operation among internal nodes.\nAnd most of the connections among internal nodes in the\nreduction cell are max pooling operation other than average\npooling operation. Actually, this corresponds to the fact that\nmax pooling operation can be better to preserve the asyn-\nchronous characteristic of neuron firing Fang et al. (2021b).\nIn previous studies, the types of down sampling layer are\nconvolutional block Zheng et al. (2021), average pooling Wu\net al. (2019) and max pooling Fang et al. (2021b). However,\nthese down sampling layers have different influences on\nthe number of spikes and model accuracy, where average\npooling operation is so hard to generate a spike that it is\ndiscouraged for the purpose of down sampling, but max\npooling operation is energy-efficient in transmitting spike\nwhile maintaining a remarkable model accuracy similar to\nconvolutional block Na et al. (2022).\nAnd we also provide the architecture of two types of cell\nour method search on the ANN search space as shown in\nFigure 6 (e)~(f). We first use our proposed method to search\nthe optimal ANN network, then we use this optimal archi-\ntecture to directly train on CIFAR10. And the final accuracy\non CIFAR10 is only 91.32%, which lags behind the majority\nof existing results. There are many distinctions between the\nANN network structure and SNN network structure, but the\nmost obvious distinction is that there are skip connections in\nthe ANN network structure, which may ensure the improve-\nment on the deep network. How to add skip connection to\nthe network structure of SNN is a promising direction worth\nstudying."}, {"title": "4.4. Ablation study", "content": "Table 3 show the results of our ablation study, where\nwe validated and compared the effects of two attention\nmechanism functions on CIFAR10 and CIFAR100 datasets.\nThe search space and experiment settings of the search\nprocess in all experiments were the same. Compared to the\ncommon DARTS algorithm, we find that both attention func-\ntions can further improve the performance of the original\nDARTS algorithm, and the CBAM-based attention function\nhas a more remarkable effect. Specifically, the ECA-based\nattention function has a 0.51% improvement and a 0.82%\nimprovement under 64 initial channels on two datasets re-\nspectively. And the CBAM-based attention function has a\n0.61% improvement and a 1.52% improvement under 64\ninitial channels on two datasets respectively. Additionally,\nour proposed method produces a negligible increase in terms\nof the network's parameter size. Notably, under 2 time\nsteps, the ECA-based model's parameter size with different\ninitial channels barely changes but comparable classification\nresults are obtained.\nWe also validated our proposed model on a more com-\nplicated dataset, like TinyImageNet-200 which has higher\nresolution and more categories. The micro architecture was\nmigrated from the CIFAR10 dataset and the number of\ninternal nodes was set to 3. The results show that MA-\nDARTS can achieve 59.39% accuracy while the original\nDARTS algorithm lags behind this result, demonstrating\nthe design of attention block can alleviate the difficulty of\nalgorithm convergence."}, {"title": "4.5. Neural architecture analysis", "content": "One of the salient features of spiking neural networks\nis low power consumption Roy, Jaiswal and Panda (2019).\nThe measurement and comparison of power consumption\ncan be done by calculating the number of neuron spikes\nTaherkhani, Belatreche, Li, Cosma, Maguire and McGinnity\n(2020). In CIFAR10 dataset, we observed the changes in\nthe number of neuron spikes (logarithmic) across different\ncells as the experiment progressed. Figure 7 (a) and (b)\nrecord the number of spikes on each cell with two attention\nfunctions, which reveals that the number of spikes in all cells\ndecreases rapidly in the first 100 epochs and stabilizes after\n200 epochs. Additionally, compared to the normal cell, the\nreduction cell yields significantly more spikes, suggesting it\nconsumes more power. However, after the data path passes\nthe reduction cell, the number of spikes on normal cells will\ncontinue to decrease. This may prevent the total number\nof spiking from increasing. Meanwhile, we computed the\nnumber of spikes for all cells, as shown in Figure 7 (c) and\n(d), and the final model stabilized at about 110K spikes on\nvalidation and 100k spikes on training."}, {"title": "5. Conclusion and future works", "content": "In this article, we initially introduced a differentiable\ntwo-level search space and defined the corresponding search\noperation set. Then we conducted the searching process\nwithin the cell under a fixed layer level and implemented\nthe MA-DARTS algorithm in the DARTS Search Space.\nThis method not only yields more efficient and lightweight\nnetwork structures than those designed artificially but also\nresults in outperforming other NAS algorithms under similar\nconditions by utilizing multidimensional attention mecha-\nnisms without significantly increasing the size of parame-\nters. Nonetheless, our approach also needs some improve-\nment. For example, power-aware methods can be integrated\ninto the search process and loss function to design more uni-\nversally low-power neural network structures. Based on our\nnetwork architecture analysis, we also find that our method\nhas a slight gap between validation and training, which could\nbe solved by modifying the connection relationship of nodes\nand redefining the set of operations. In addition, our method\nhas only been studied in tasks such as classification, and\nfurther research is needed in event-driven task scenarios."}]}