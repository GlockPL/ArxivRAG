{"title": "Multiple-Resolution Tokenization\nfor Time Series Forecasting\nwith an Application to Pricing", "authors": ["Egon Per\u0161ak", "Miguel F. Anjos", "Sebastian Lautz", "Aleksandar Kolev"], "abstract": "We propose a transformer architecture for time series forecasting with a focus on time series\ntokenisation and apply it to a real-world prediction problem from the pricing domain. Our\narchitecture aims to learn effective representations at many scales across all available data\nsimultaneously. The model contains a number of novel modules: a differentiated form of\ntime series patching which employs multiple resolutions, a multiple-resolution module for\ntime-varying known variables, a mixer-based module for capturing cross-series information,\nand a novel output head with favourable scaling to account for the increased number of\ntokens. We present an application of this model to a real world prediction problem faced\nby the markdown team at a very large retailer. On the experiments conducted our model\noutperforms in-house models and the selected existing deep learning architectures.", "sections": [{"title": "Introduction", "content": "Forecasting remains a frontier for deep learning models. Non-stationarity, highly dependent stochastic\nprocesses, limited predictability, and most importantly small datasets differentiate this prediction problem as\na particularly difficult one. The value of improved forecasting capabilities is nearly self-evident as it enables\nus to improve the contextualisation of decision making. The model we propose was developed specifically\nfor time series for which we have control over some of the auxiliary variables. We assume those auxiliary\nvariables have a significant effect on the time series. Pricing problems are a canonical example in which the\nauxiliary variable of price affects sales. The ubiquity of pricing problems and resulting large datasets make\nthem a natural candidate for deep learning methods. Markdowns are a subset of pricing problems concerned\nwith reducing prices to clear expiring stock. In this context a forecasting model which incorporates auxiliary\nvariables such as price can be utilised as a simulator to narrow down pricing strategies for empirical testing.\nThere is considerable operational and reputational risk in evaluating previously untrialled strategies. A\ngood simulator enables a virtual exploration of the pricing strategy hypothesis space which can help select\npotentially more profitable and sustainable pricing strategies for testing.\nThis work is about tokenisation strategies for time series and aligning a transformer architecture with these\nstrategies. We believe that the main way of advancing transformer capabilities for time series forecasting is\nby developing and validating specialised tokensiation strategies. Our contributions are:\n\u2022\n\u2022\n\u2022\nA set of new tokenisation modules to obtain: multiple-resolution past data tokens, multiple-\nresolution known time-varying tokens, and cross-series tokens. This creates a much broader context\nwindow.\nA reverse splitting output head with favourable scaling properties to deal with the increase in the\nnumber of tokens.\nAn application of the model to a real-world forecasting problem faced by the pricing team at a very\nlarge retailer. In the experiments we demonstrate that our method outperforms existing in-house\nmethods and two popular existing architectures on a real-world pricing problem."}, {"title": "Deep Learning for Time Series Forecasting", "content": "Denote the forecast horizon as $f$ and the lookback horizon as $l$. We define three forms of data associated\nwith each individual time series:\n\u2022\n\u2022\n\u2022\nStatic $s$ (Store information, product information),\nTime-varying known $X_{to-l+1:to+f}$ (global seasonal data, prices),\nObserved data/time-varying unknown $y_{to-l+1:to}$ (sales, weather)\nwhere $t_0$ is the point at which we want to make our forecast. The multivariate forecasting problem at\n$t_0$ with a forecast horizon of $f$ is the prediction of future values of the series for the time periods $t_0 +\n1$ to $t_0 + f$ across all channels $C_1,...,C_n \\in C$ which we denote as $\\hat{y}_{c1:cn,to+1:t0+f}$ to minimise some loss\nfunction $loss(Y_{c1:cn,t0+1:to+f},\\hat{Y}_{c1:cn,t0+1:to+f})$. Each channel respectively corresponds to each variate we are\ninterested in forecasting simultaneously. Channels are a choice reflecting our structural view of the time series.\nAggregation can be done by concatenating series and adding a static variable denoting some characteristic\nof the series or by stacking different series along a new dimension.\nTime series tend to be non-stationary, that is the joint probability distribution across the same patch reso-\nlution is not constant across time. The nature of the data-generating process is fundamentally inconsistent\nacross different time series meaning it is harder to aggregate large corpuses for training. Time series intrinsi-\ncally produce fewer observations. In most cases we simply do not have a sufficient number of samples for deep\nlearning to be viable. There are limited cases for which we have sufficient corpus sizes in time series: where\nthere is an enormity of related observed processes, a high resolution of measurement, or, a very short fore-\ncasting interest relative to the total length of the observed sequence. Given that deep learning requires large\ndatasets applying it to unsuitable forecasting problems is fruitless absent powerful foundation models. The\nforecasting problem in pricing markdowns is suitable as it benefits from being able to aggregate/concatenate\nmany individual pricing time-series across SKUs and stores."}, {"title": "Transformers", "content": "Components of the original transformer architecture (Vaswani et al., 2017) powers the state of the art for\nlarge language models and has seen more computational resource investment than any other deep learning\narchitecture. Empirically, the transformer is outstanding for language modelling and benefits from scaling\nlaws. Due to the success of transformers with the sequential task of language there has been considerable\ninterest in applying transformers to time series. Academic research has produced mixed results and tends to\nbe limited in the scope of study. In fact there are strong arguments to be made against using transformers\nfor time series with plenty of vocal opponents.\nTransformers have architectural limitations when applied to time series (Zeng et al., 2023). Transformers are\npermutation invariant and thus inherently do not create an inductive bias which reflects that observations\nhappen sequentially (typically in a highly auto-correlated manner). Transformers for language use two core\narchitectural adjustments for dealing with the lack of sequential inductive bias. Positional encoding adds a\nbias to the latent matrix representation of a language sequence which is designed or learnt to reflect sequen-\ntiality. Decoder architectures mask out the attention matrix so that for each unit of latent representation\n(token) can only observe the tokens that occurred before it in the sequence. The effectiveness of either of\nthese techniques for forecasting is still an open question. The evidence so far is mixed and simple models\nhave been shown to outperform very complex transformer models on academic benchmarks (Zeng et al.,\n2023).\nAt a higher level, the concept of a token in language\u00b9 does not trivially map to time series. Time series are\nprocesses which have regular or irregular observations, but single observations are not necessarily meaningful\nunits in terms of the dynamics of a time series. An analogous reasoning from classical time-series approaches\nis the concept of the order of integration which is the number of times a time series has to be differenced\n(how many adjacent points need to be considered) to become stationary. Time series are often decomposed\ninto trend and seasonality or frequencies, both of which are transformations of more than just a single\nobservation. A notable transformer time series architecture uses frequency decomposition to tokenize a time\nseries (Zhou et al., 2022). Alternatively, a popular and increasingly standard method (Das et al., 2023) for\ntokenisation is patching (Nie et al., 2023): it splits a series up into fixed length sequences which are then\nmapped into the embedding space using a shared learnable linear transformation. The original PatchTST\nuses patches of length 16 with a stride of 8 (allowing for overlapping patches). This can be interpreted as\nthe resolution at which a time series is processed. As opposed to previous by observation period encoder-\ndecoder designs (Lim et al., 2021) patching loses the 1-to-1 correspondence between tokens and forecast\nhorizon periods. Instead, the matrix output from the transformer blocks is flattened and passed to a large\nlinear layer (presenting a potential bottleneck) which outputs the forecast vector. The implicit assumption\nis that meaningful information about the time series occurs at resolutions denser than the base time series.\nA linear model over the whole series assumes that we need to look at the highest density; the whole series.\nOur work builds on this assumption by extending patching to multiple resolutions.\nThere have been many attempts to design transformer and transformer like architectures for time series,\nparticularly for long forecast horizons due to a set of popular testing benchmarks. The vast majority of\ninnovation appears either in the design of the tokenisation or the architecture of the token processing engine.\nThis repository is an excellent overview of recent developments across a range of modelling approaches."}, {"title": "Existing Multiple Resolution Patching Approaches", "content": "We would like to outline the differentiating contributions of our work in contrast with other recent approaches\nwhich employ the idea of utilising multiple resolution modelling in transformer architectures. We provide a\nbrief description of each model and point out the key difference in capabilities and architecture. Our model\nis unique in that it uses the tokens from multiple resolutions in the same attention mechanism allowing\nfor explicit pairwise modelling at multiple resolutions across multiple data types. In addition we provide\na scheme for how to tokenize time-varying known data at multiple resolutions including for the time steps\nwhich occur in the future.\nShabani et al. (2022) The Scaleformer was the first notable attempt at multi-scale modelling applied\nto time-series forecasting with transformers. It iteratively implements broader pooling which forms the\ninputs for an encoder and feeds the decoder linear interpolations of the prediction of the previous layer.\nThe outputs of all layers are combined with a \"normalising\"-weighted sum. It does not process multiple\nresolutions simultaneously, instead opting to do so sequentially. It only handles past/observed data and not\nany auxiliary variables.\n\u00b9 Note that tokenisation in LLMs is typically not trained end-to-end and is fixed, transferred into the model from a different\npretraining task. This is often a cause of problems."}, {"title": "The Model: Multiple-Resolution Tokenization (MRT)", "content": "Our model is based on two ideas. First, that time series carry relevant information at multiple resolutions and\nthat this should be represented in attention simultaneously. Second, that the resulting multiple resolution\ntokens can and should be joined with tokens obtained from auxiliary data to form the input matrix for\nthe transformer. This idea is a response to one of the key design questions in transformers for time series\nforecasting: what are meaningful units of information. Tokens in this context should just be understood\nas meaningful representations of some facet of the time series. In designing separate tokenisation for past\nand auxiliary data we implicitly assume that auxiliary data carries significant information. Designs which\ninclude auxiliary information often meld all information at a given token together (Lim et al., 2021; Chen\net al., 2023), whereas we process separately for each type of auxiliary information. The separation into more\ntokens enables explicit attention links to aspects of auxiliary information which increases interpretability. The\nmultitude of resolutions complemented by representations of auxiliary variables as a tokenization strategy\nenables the learning of complex pairwise relationships.\nTo add clarity we introduce a common notation to describe the view of the tensor and the operation performed\non it at a given point in the model. We will describe the dimensions of a tensor with a collection [ ] of\nletters which represent dimensions. The last k letters represent the dimensions the operation is taking place\nover. Most operations are performed over a single dimension, however there are exceptions for example\nself attention operates on matrices so the last two dimensions. If a tensor has been flattened this will be\nindicated in the new view as with a and if two tensors have been concatenated in a dimension we will\nuse +. The relevant dimensions in our case are B for batch or sample, C for channel, T for time and for\npatch which is typically a compression across the time dimension and auxiliary variable embeddings (can\nbe understood as the token dimension), V for the variable dimension, and L for the latent dimension. Note\nthat these dimensions will not describe unique states as they serve as illustrators as opposed to an exact\nfunctional description of the model which can be found in the code. We use lower case letters for fixed values\nto emphasise the size of the dimension, for example (unless stated otherwise) the size of the latent dimension\nis $d_m$ so [B, C,T, L = $d_m$]. To illustrate this notation consider the standard output head of PatchTST (Nie\net al., 2023), the operation takes the output from the transformer in the form [B, C,T, L], flattens it to\n[B,C,TL] applies a linear layer to the last dimension to produce an output [B, C, f]. In short we denote"}, {"title": "Multiple-Resolution Patching", "content": "The multiple resolution patching is defined by a resolution set $K = k_1,..., k_r$ takes an input of the form\n[B,C,T = $l$] and uses a set of |K| = r linear transformations to create an output of the form [B, C,T, L].\nDenote the size of the latent dimension of the model as $d_m$. We extend patching to multiple resolutions by\nusing several divider blocks. A divider block splits a time series into $k_i$ roughly equal parts. For a series\nof length $h$ the base patch length is set as $b_i = \\lfloor h/k_i \\rfloor$. If $k$ does not divide $h$ we increase the length of\nthe first $h - b_ik_i$ patches by 1. In our application $h$ is either $l$ (past data) or $l + h$ (time-varying known).\nFor each resolution $k_i$ we learn a linear projection into the latent space $R^{b_i+1 \\to R^{d_m}}$. Weights are shared\nfor all patches within a resolution. We left pad patches that are $b_i$ long with a 0. The core operation\nis [B,C,T = $b_i + 1$] $\\to$ [B,C, L = $d_m$] which is repeated $\\sum_{k_i \\in k} k_i$ times ($k_i$ times for each resolution)\nand stacked along a patch/token dimension into [B,C,T = $N_{MRP}$,L]. The created dimension T contains\n$\\sum_{k_i \\in k} k_i = N_{MRP}$ tokens obtained from multiple resolution patching. An illustration of multiple resolution\npatching is presented in figure 1."}, {"title": "Handling Auxiliary Information", "content": "A key advantage of deep learning models over classical models is that they can accommodate arbitrary\nforms of input enabling us to jointly model all available information for a time series. Auxiliary information\nis not trivially useful for time series forecasting. Most recent state of the art papers exclusively focus on\npast/observed data. The inclusion of auxiliary data often leads to issues with overfitting, explainable by\nthe noise in time series data and small datasets. In existing work which includes auxiliary information it is\ntypically mixed with past data to form tokens representing all information at each time step or resolution.\nWe take a different approach and design a separate tokenisation scheme for different types of auxiliary data.\nThis enables us to explicitly learn contextual representations of the time series that are purely auxiliary and\nnot directly dependent on the noise in the time series, which should help reduce overfitting.\nBase Tokenization Data for time series is either continuous or categorical. Categorical variables need\nadditional processing, in the case of transformers entity embeddings are standard practice and amount to\nlearning a vector embedding for each type within a category. To match the embedding size in the case\nof numerical auxiliary variables we employ numerical embeddings. We learn a vector for each numerical\nauxiliary variable which is then scaled by the value of that variable. We call this process base tokenisation.\nTo handle missing values, we assign a separate learnable vector embedding for each variable in cases the value\nhas not been observed. When there is variation in the length of the input horizon we left pad all temporal\nseries with a special category or numerical value that always yields a zero vector embedding, which then does"}, {"title": "Time-Varying Known Variables (TVK)", "content": "The TVK of a time series form a tensor [B,C,T = $l +$\n$f, V$]. We process global and specific variables separately. The base tokenisation first transforms both the\ncategorical and numerical variables in the tensor from [B, C,T,V] $\\to$ [B,C,T, V, L]. We then learn a linear\nprojection that compresses the variable dimension to one [B, C,T, L, V] $\\to$ [B, C,T, L, V = 1] = [B, C,T, L].\nThis base tokenisation plus mixing across variables creates one auxiliary token per time step.\nIn line with previous multiple resolution patching we apply a modified version built on basis combination.\nIn contrast with past-data we want to patch across tokens (collection of vectors) not individual observations\n(collection of points). We split the tensor $k_i$ roughly equal parts for each resolution i along the time\ndimension. We use the same resolutions, rules on length (denote the length of a patch $a_i$), and padding (so\neach patch is actually $a_i + 1$), but for a longer series as the size of T is $l + f$. The splitting leaves us with\na number of tensors of the shape [B, C, L, T = $a_i + 1$]. We treat the columns of each matrix in the last two\ndimensions of the from [L = $d_m$,T = $a_i + 1$] as a basis. For each resolution we learn a basis layer: vector of\nsize $a_i +1$ which linearly combines the columns of the matrix into a single vector. The basis layer transforms\n[B,C,L,T = $a_i + 1$] $\\to$ [B,C,L,T = 1] = [B, C, L] for each patch in each resolution. This operation is\nillustrated in figure 2. The basis combination results in $\\sum_{k_i \\in k} k_i = N_{MRP}$ such tokens. The tensors are\nstacked along a patch dimension into [B,C,T = $N_{MRP}$,L]. To reduce the relative influence of auxiliary\nvariables and reduce the size of the input matrix to the transformer we employ a linear compression layer\nto compress the patch dimension to a predefined number of tokens $n_{TVK}$. This layer mixes across patch\nrepresentation. The transformation is a linear layer [B, C, L,T = $N_{MRP}$] $\\to$ [B,C,L,T = $n_{TVK}$], which\nwe transpose to [B, C,T, L]. This process creates a a set of TVKT which are a mixture of representations\nextracted at multiple resolutions from joint base representations of the time-varying auxiliary data."}, {"title": "Static Variables", "content": "Static variables form a tensor [B, C, V]. They do not have a time component and are\nequal across channel C if they are global and can differ if specific. The base tokenisation transforms the\nstatic variable tensor into [B, C, V, L]. If V is low we append the base tokenisation to the MRP directly\nmeaning the number of static tokens (ST) $n_s = |V|$ and the variable dimension is just treated as the token\ndimension. Otherwise we employ an additional linear layer [B, C, L, V] $\\to$ [B,C, L,T = $n_s$] which operates\nover the V dimension and condenses the representation to a predefined number of mixed tokens $n_s$."}, {"title": "Cross-Series Information", "content": "The overfitting problem has also been empirically observed for the inclusion of cross-series information, which\nhas motivated the use of channel independence Han et al. (2024). We propose a channel mixer module which\nemploys a mixer architecture to generate a set of cross-series tokens (CST) from the tensor containing all\nother tokens.\nWe call the module responsible for extracting cross-series information the channel mixer. As input the\nchannel mixer takes the tensor of all existing tokens MRP, ST, and TVKT. Define the number of these\ntokens as $n_B =N_{MRP}+n_s+n_{TvK}$. The input tensor takes the form [B, C, T = $n_B$, L]. We employ a mixer\narchitecture inspired by Chen et al. (2023) to extract cross series tokens. Mixer architectures have been\nshown to perform well on time series forecasting while extracting cross-series representations in every block.\nWe roughly illustrate the module in figure 3. The operating tensor is normalised before each mixing step and\na skip connection is employed across each mixing step. The first mixing step is across the token dimension\nT, applying a linear transformation [B, C, L, T = $n_B$] $\\to$ [B, C, L, T = $n_B$] followed by an activation function\nand dropout. The number of tokens is then squeezed down to a predefined number $n_{CST}$ with another linear\nlayer [B, C, L, T = $n_B$] $\\to$ [B, C, L,T = $n_{CST}$] (we use A here to signal that the tokens have been mixed into\nan auxiliary information representation). The second mixing step is across the channel dimension C. Two\nlinear layers are employed, the first projects the channel dimension of size $d_c$ into the latent dimension of the\ncross series module $d_{cross}$ (adjustable hyperparameter), so [B, A, L, C = $d_c$] $\\to$ [B, A, L, C = $d_{cross}$]. After\nan activation function and a dropout layer the second linear layer projects back into the original channel\ndimension [B, A, L, C = $d_{cross}$] $\\to$ [B, A, L, C = $d_c$]. After the end of the second mixing another linear layer\nsqueezes the channel dimension to one leaving us with [B, C = 1,T, L]. The reason this is done is so that the\ncross-series tokens are the same across channels. To align it with the dimensions of our collection of tokens\nwe repeat the tensor |C| = $d_c$ times to obtain [B,C = $d_c$,T = $n_{CST}$, L]. We append it to all other existing\ntokens to obtain the final input matrix A with size $N_{MRT} = n_B + n_{CST}$ which is passed to the transformer\nmodule [B, C, T = $n_{MRT}$, L]."}, {"title": "Output Head", "content": "We propose a novel output head architecture which has favourable scaling properties. We call it reverse\nsplitting. The transformer blocks produce a matrix which is the same size as the input matrix [B, C, T, L].\nUsing the same resolution values as for the splitting, we reverse the process. For resolution $k_i$ we take the\nnext $k_i$ column vectors from the matrix and individually project them into patches that are composed to\nrepresent the forecast. The length of the projected patches ($p_i$ or $p_i + 1$) is determined the same way as\nthe splitting in MRP. We learn a projection matrix for each resolution $R^{d_m \\times p_i+1}$ and omit the final vector\nvalue for the reverse patching for which we have set the length to $p_i$. The core operation iterates over the\ndimension T so each input is [B, C, L] which is projected into [B,C,T = $p_i + 1$], this is repeated $k_i$ times\nand concatenated according to the length rule into [B, C, T = f]. This leaves us with |K| forecast vectors\nof the form [B,C,T = f] which we simply sum to obtain the final output. We call this reverse splitting."}, {"title": "Other Architectural Choices", "content": "The tensor of all tokens A is processed by a sequence of transformer blocks after which it is passed to\nthe reverse splitter output head. We use multi-head self-attention in the transformer block. The learning\nin self attention can be interpreted as learning four different mixing rules across the L dimension to infer\ngood pairwise comparisons. Since the focus of this work is on tokenisation strategies we avoid extensively\ninvestigating positional encoding or masking strategies.\nMasking We use an encoder design without masking. While a decoder architecture with masking could be\ndesigned to be time consistent with patching at multiple resolutios it is not trivial to extend it to TVKT and\nCST so we avoid masking. Any form of mixing such as compressing TVKT or CST across the T dimension\nobscures causality.\nPositional Encoding We use learnable positional encoding. This is equivalent to learning a fixed bias for\nthe input matrix to the transformer. We do this as it is difficult to design positional encoding for a complex\nset of tokenisation procedures. Certain patching approaches use aggregations over conventional positional\nencoding, but that is still limited to only past data.\nAfter Attention We follow the standard transformer architecture: self-attention is followed by a skip\nconnection, normalisation and then a feed-forward network. We use a standard two linear layer MLP with an\nactivation function and dropout. The latent dimension in between the two layers is another hyperparameter\n$d_{ff}$. The feed-forward network is applied to the latent dimension, so separately to each token. The operation\nis [\u0412,\u0421,\u0422 = $n_{MRT}$,L] $\\to$ [B,C,T = $d_{ff}$,L] $\\to$ [B,C,T = $n_{MRT}$, L]. This is followed by a skip connection\nfrom the input to the feed-forward network and another normalisation."}, {"title": "Architectural Limitations", "content": "Multiple resolution patching and associated auxiliary variable tokenisation adds many more hyperparameters\nin the form of the resolution set. The space of resolution sets is combinatorial making hyperparameter\noptimisation a challenging task. We somewhat constrain the space by not allowing overlaps, thus avoiding\nthe need for a stride hyperparameter associated with every resolution. Our tokenisation modules increase\nthe number of learnable parameters in the embedding especially for small $k_i$ which makes scaling to large\nlatent sizes more difficult. The CST are sequentially computed downstream of all other tokens limiting the\ndegree of parallelisation in tokenisation. Since all tokenisation is done end-to-end a degree of overfitting is\nexpected. Computationally the main difficulty is the increased number of tokens which scales computation\nquadratically. This constrains how high $n_{MRP}$ can get limiting dense resolution sets which may be useful for\nlonger context windows with long lookback horizons. Exploiting and encouraging sparsity in self-attention\nmay be one possibility to deal with the larger context window."}, {"title": "Application to a Forecasting Problem in Pricing", "content": "The data for the experiments is real markdown data from a very large European retailer. The markdown\npricing prediction problem is to estimate the sales of an item given a series of prices and other contextual\ninformation. This prediction problem is understood as particularly difficult. The theory is that each time\nthere is a price reduction we observe a spike in sales which then decays. Subsequent price reductions\nproduce a similar spike and decay effect which typically reduces in magnitude with each reduction. This\nhighly non-linear behaviour is difficult to model with simple approaches. This is further exacerbated by\nthe high levels of noise associated with consumer behaviour owing to stochastic factors like the weather or\ncompetitor behaviour and the human factor in actually delivering the pricing strategy in stores. Historical\ndata is limited to a narrow set of tried and tested pricing strategies such as 25% off then 50% then 75%\nadding to the epistemic uncertainty. The downstream decision problem of setting reduced prices for items\nthat are about to expire sees about a million decisions made every day.\nAuxiliary Information In the case of markdowns the inclusion of auxiliary data into the model is nec-\nessary as it is otherwise not possible to use it as a hypothesis generator for new pricing strategies. The\ndecision variable (price) must be included as an input in the prediction model in order to be able to com-\npare predictions under different markdown strategies. We try to predict both the full-price and the reduced\nprice-sales concurrently, this defines the channels in C. This reflects the operational characteristics of the\nretailer where some proportion items that are about to expire get moved to a discounted section.\nOne view of the utility of auxiliary information beyond the necessary decision variables is that it enables\nthe aggregation of a larger number of time series in a single training corpus with more learning potential.\nWe can learn tokenizations which capture the relatively specific context of each time series, which leads\nto a more general purpose prediction model for sales. In the case of markdowns individual time series are\nrelatively short and highly contextual. We take each series to be defined by as the unique combination of\nproduct, store, and expiration date. We want to aggregate all products of interest across all shops for all\ndates in a single model. We include auxiliary information because we do not believe that there is sufficient\ninformation in the starting course of a series to effectively differentiate amongst products and stores and\nexpiration dates. We use following auxiliary information:\n\u2022\n\u2022\nStatic:\nTVK:\nGlobal: product group, brand, stock.\nSpecific: dummy variable for channel (either full price or reduced price sales channel).\nGlobal: day of week, hour (half-hourly, continuous), month, which iteration of price reduction,\nproportion of stock marked down.\nSpecific: price (constant for full price prediction, changing for reduced price prediction)."}, {"title": "Normalisation", "content": "To further enable the aggregation of many potentially heterogeneous series and in line\nwith effective practice for deep learning we use a collection of normalisation procedures. Normalisation is\nprincipally utilised to deal with the problem of different scales. Neural network training can easily result in\nunsatisfactory outcomes without scaling due to issues with the absolute and relative magnitude of gradients.\nNormalisation leads to improved generalisation due to increased stability in training and being able to deal\nwith previously unseen scales. We employ two types of normalisation: instance normalisation for each\nseries, and residual connections with normalisation in the transformer and channel mixer blocks. Instance\nnormalisation subtracts the final value of the series from the entire series and optionally divides it by\nthe standard deviation of the series and adds a learnable bias (affine transformation). The normalisation\nis reversed after the pass through the network. This type of normalisation has been shown to perform\nempirically and was originally proposed to combat distribution-shift/non-stationarity (Kim et al., 2021).\nThe second type of normalisation is designed in a modular way and is applied after residual connections.\nIt can either take the form of layer or batch normalisation. LayerNorm standardises the entire layer per\ninstance, BatchNorm standardises each value in a layer across the batch. As opposed to language modelling,\nwhich typically uses LayerNorm, we use BatchNorm in experiments due to evidence that it is favourable\nin the time series context as it is better at handing outlier values (Zerveas et al., 2021). In preprocessing,\nwe scale all continuous variables in the dataset. We fit many scalers based on what store type-product\ngroup combination a series falls into. The intuition is that this heuristic clustering should provide for more\nconsistent scaling across different distributions of variables."}, {"title": "Data Setting", "content": "The main issue encountered when attempting to apply Multiple Resolution Tokenisation\nto pricing data is that the data has a distinct real-world flavour. Sales are aggregated within varying blocks\nof time that represent a duration of displayed price. The time series is irregularly spaced and each series\nhas very few observations (at most 4). The data does not contain time series of sales when markdowns are\nnot present. Each set of scan observations must be treated as its own series. Since only markdowns are\nobserved, there is a clear selection effect as the process to select markdowns is biased toward existing more\nprofitable strategies. This selection bias in the data generating process may result in poorer generalisation\nand overconfidence. The signal is also somewhat limited as there is only a narrow scope of markdown\nstrategies that are regularly implemented. The dataset features many missing values and heuristics to deal\nwith data acquisition problems."}, {"title": "Preprocessing", "content": "From a model architecture standpoint this dataset poses two key problems: irregularly spaced sampling, and\nthe varying length of the series.\nQuantisation The aggregation boundary times are rounded to the nearest 30 minute mark. The series\nis then quantised into 30 minute sections. We chose 30 minute intervals to create sequences with a non-\ntrivial length, to add more auxiliary data to the sequence, and to enable a prediction of more potential\npricing strategies. We quantise the entries in all columns which are aggregated (sales) over the given time\nperiod, the rest are simply copied. The quantisation is as simple as dividing the values by the number of\n30 minute periods (exclusive of closure times) that occur from one price reduction to the next. There are\ntwo simplifications here: the rounding to 30 minutes means that we do not perform any imputation on the\nboundaries of the aggregations, the second is assigning the average value to each of the quantised fields.\nMore disaggregated data would obviously work even better.\nNote that there are approaches that use irregular time series natively such as neural differential equations\n(Kidger et al., 2020) to model the latent state between two observations. Unfortunately, an initial value\nproblem has to be solved at every iteration which is computationally infeasible for large datasets."}, {"title": "Coping with Varying Length", "content": "To reduce the effect of changing sequence lengths we only keep the top five percent of sequences in terms\nof length. This also reduces the dataset to a much more manageable size. Since the sequences vary in the\nlength of time, so does the number of steps obtained after quantisation. We fix the forecast horizon to\nthe final 24 periods so f = 24. In its current design the model works for any fixed forecast horizon. We\ncontend with varying input lengths by setting the lookback horizon to the longest sequence minus the fixed\nforecast horizon and padding all shorter sequences with values that result in zero vector tokens effectively\ndeactivating that aspect of the network."}, {"title": "Experiments", "content": "The experiments were defined by the range of year-weeks they contained. The experiments are limited to\njust one week due to memory constraints. Each experiment contains the markdown data for one week across\nall shops and all items. For any experiment the series corresponding to the last 20% (rounded up) of the\ndue-dates were held out as the testing set, the 15% (rounded up) of due-dates immediately preceding were\nused as the validation set and the remaining as the training set.\nModel Hyper"}]}