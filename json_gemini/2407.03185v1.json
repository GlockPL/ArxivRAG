{"title": "Multiple-Resolution Tokenization\nfor Time Series Forecasting\nwith an Application to Pricing", "authors": ["Egon Per\u0161ak", "Miguel F. Anjos", "Sebastian Lautz", "Aleksandar Kolev"], "abstract": "We propose a transformer architecture for time series forecasting with a focus on time series\ntokenisation and apply it to a real-world prediction problem from the pricing domain. Our\narchitecture aims to learn effective representations at many scales across all available data\nsimultaneously. The model contains a number of novel modules: a differentiated form of\ntime series patching which employs multiple resolutions, a multiple-resolution module for\ntime-varying known variables, a mixer-based module for capturing cross-series information,\nand a novel output head with favourable scaling to account for the increased number of\ntokens. We present an application of this model to a real world prediction problem faced\nby the markdown team at a very large retailer. On the experiments conducted our model\noutperforms in-house models and the selected existing deep learning architectures.", "sections": [{"title": "Introduction", "content": "Forecasting remains a frontier for deep learning models. Non-stationarity, highly dependent stochastic\nprocesses, limited predictability, and most importantly small datasets differentiate this prediction problem as\na particularly difficult one. The value of improved forecasting capabilities is nearly self-evident as it enables\nus to improve the contextualisation of decision making. The model we propose was developed specifically\nfor time series for which we have control over some of the auxiliary variables. We assume those auxiliary\nvariables have a significant effect on the time series. Pricing problems are a canonical example in which the\nauxiliary variable of price affects sales. The ubiquity of pricing problems and resulting large datasets make\nthem a natural candidate for deep learning methods. Markdowns are a subset of pricing problems concerned\nwith reducing prices to clear expiring stock. In this context a forecasting model which incorporates auxiliary\nvariables such as price can be utilised as a simulator to narrow down pricing strategies for empirical testing.\nThere is considerable operational and reputational risk in evaluating previously untrialled strategies. A\ngood simulator enables a virtual exploration of the pricing strategy hypothesis space which can help select\npotentially more profitable and sustainable pricing strategies for testing.\nThis work is about tokenisation strategies for time series and aligning a transformer architecture with these\nstrategies. We believe that the main way of advancing transformer capabilities for time series forecasting is\nby developing and validating specialised tokensiation strategies. Our contributions are:\n\u2022\n\u2022\nA set of new tokenisation modules to obtain: multiple-resolution past data tokens, multiple-\nresolution known time-varying tokens, and cross-series tokens. This creates a much broader context\nwindow.\nA reverse splitting output head with favourable scaling properties to deal with the increase in the\nnumber of tokens.\n\u2022 An application of the model to a real-world forecasting problem faced by the pricing team at a very\nlarge retailer. In the experiments we demonstrate that our method outperforms existing in-house\nmethods and two popular existing architectures on a real-world pricing problem."}, {"title": "Deep Learning for Time Series Forecasting", "content": "Denote the forecast horizon as $f$ and the lookback horizon as $l$. We define three forms of data associated\nwith each individual time series:\n\u2022 Static $s$ (Store information, product information),\n\u2022 Time-varying known $X_{t_0-l+1:t_0+f}$ (global seasonal data, prices),\n\u2022 Observed data/time-varying unknown $y_{t_0-l+1:t_0}$ (sales, weather)\nwhere $t_0$ is the point at which we want to make our forecast. The multivariate forecasting problem at\n$t_0$ with a forecast horizon of $f$ is the prediction of future values of the series for the time periods $t_0 +\n1$ to $t_0 + f$ across all channels $C_1,...,C_n \\in C$ which we denote as $\\hat{y}_{c_1:c_n,t_0+1:t_0+f}$ to minimise some loss\nfunction $loss(Y_{c_1:c_n,t_0+1:t_0+f},\\hat{Y}_{c_1:c_n,t_0+1:t_0+f})$. Each channel respectively corresponds to each variate we are\ninterested in forecasting simultaneously. Channels are a choice reflecting our structural view of the time series.\nAggregation can be done by concatenating series and adding a static variable denoting some characteristic\nof the series or by stacking different series along a new dimension.\nTime series tend to be non-stationary, that is the joint probability distribution across the same patch reso-\nlution is not constant across time. The nature of the data-generating process is fundamentally inconsistent\nacross different time series meaning it is harder to aggregate large corpuses for training. Time series intrinsi-\ncally produce fewer observations. In most cases we simply do not have a sufficient number of samples for deep\nlearning to be viable. There are limited cases for which we have sufficient corpus sizes in time series: where\nthere is an enormity of related observed processes, a high resolution of measurement, or, a very short fore-\ncasting interest relative to the total length of the observed sequence. Given that deep learning requires large\ndatasets applying it to unsuitable forecasting problems is fruitless absent powerful foundation models. The\nforecasting problem in pricing markdowns is suitable as it benefits from being able to aggregate/concatenate\nmany individual pricing time-series across SKUs and stores."}, {"title": "Transformers", "content": "Components of the original transformer architecture (Vaswani et al., 2017) powers the state of the art for\nlarge language models and has seen more computational resource investment than any other deep learning\narchitecture. Empirically, the transformer is outstanding for language modelling and benefits from scaling\nlaws. Due to the success of transformers with the sequential task of language there has been considerable\ninterest in applying transformers to time series. Academic research has produced mixed results and tends to\nbe limited in the scope of study. In fact there are strong arguments to be made against using transformers\nfor time series with plenty of vocal opponents.\nTransformers have architectural limitations when applied to time series (Zeng et al., 2023). Transformers are\npermutation invariant and thus inherently do not create an inductive bias which reflects that observations\nhappen sequentially (typically in a highly auto-correlated manner). Transformers for language use two core\narchitectural adjustments for dealing with the lack of sequential inductive bias. Positional encoding adds a\nbias to the latent matrix representation of a language sequence which is designed or learnt to reflect sequen-\ntiality. Decoder architectures mask out the attention matrix so that for each unit of latent representation\n(token) can only observe the tokens that occurred before it in the sequence. The effectiveness of either of\nthese techniques for forecasting is still an open question. The evidence so far is mixed and simple models\nhave been shown to outperform very complex transformer models on academic benchmarks (Zeng et al.,\n2023).\nAt a higher level, the concept of a token in language\u00b9 does not trivially map to time series. Time series are\nprocesses which have regular or irregular observations, but single observations are not necessarily meaningful\nunits in terms of the dynamics of a time series. An analogous reasoning from classical time-series approaches\nis the concept of the order of integration which is the number of times a time series has to be differenced\n(how many adjacent points need to be considered) to become stationary. Time series are often decomposed\ninto trend and seasonality or frequencies, both of which are transformations of more than just a single\nobservation. A notable transformer time series architecture uses frequency decomposition to tokenize a time\nseries (Zhou et al., 2022). Alternatively, a popular and increasingly standard method (Das et al., 2023) for\ntokenisation is patching (Nie et al., 2023): it splits a series up into fixed length sequences which are then\nmapped into the embedding space using a shared learnable linear transformation. The original PatchTST\nuses patches of length 16 with a stride of 8 (allowing for overlapping patches). This can be interpreted as\nthe resolution at which a time series is processed. As opposed to previous by observation period encoder-\ndecoder designs (Lim et al., 2021) patching loses the 1-to-1 correspondence between tokens and forecast\nhorizon periods. Instead, the matrix output from the transformer blocks is flattened and passed to a large\nlinear layer (presenting a potential bottleneck) which outputs the forecast vector. The implicit assumption\nis that meaningful information about the time series occurs at resolutions denser than the base time series.\nA linear model over the whole series assumes that we need to look at the highest density; the whole series.\nOur work builds on this assumption by extending patching to multiple resolutions.\nThere have been many attempts to design transformer and transformer like architectures for time series,\nparticularly for long forecast horizons due to a set of popular testing benchmarks. The vast majority of\ninnovation appears either in the design of the tokenisation or the architecture of the token processing engine.\nThis repository is an excellent overview of recent developments across a range of modelling approaches."}, {"title": "Existing Multiple Resolution Patching Approaches", "content": "We would like to outline the differentiating contributions of our work in contrast with other recent approaches\nwhich employ the idea of utilising multiple resolution modelling in transformer architectures. We provide a\nbrief description of each model and point out the key difference in capabilities and architecture. Our model\nis unique in that it uses the tokens from multiple resolutions in the same attention mechanism allowing\nfor explicit pairwise modelling at multiple resolutions across multiple data types. In addition we provide\na scheme for how to tokenize time-varying known data at multiple resolutions including for the time steps\nwhich occur in the future.\nShabani et al. (2022) The Scaleformer was the first notable attempt at multi-scale modelling applied\nto time-series forecasting with transformers. It iteratively implements broader pooling which forms the\ninputs for an encoder and feeds the decoder linear interpolations of the prediction of the previous layer.\nThe outputs of all layers are combined with a \"normalising\"-weighted sum. It does not process multiple\nresolutions simultaneously, instead opting to do so sequentially. It only handles past/observed data and not\nany auxiliary variables.\nChen et al. (2024) The Pathformer is designed to work as a foundation model, amenable to working\nwith time series of differing scales. It is composed of many blocks which operate at different resolutions.\nEach block first uses QKV-attention to process by individual patch, allowing for cross-series modelling and\nthe inclusion of all data available at a given past time-stamp by compressing it into a single vector patch\nembedding. The next step is a layer of self-attention which models the pairwise relationships of all of the\nresulting patch embeddings. This is paired with an adaptive pathway which routes the input series into a\nweighted combination of the resolution blocks. The routing is based on a learned embedding obtained from\nthe trend and seasonality of a series. At no point does attention look at multiple resolutions simultaneously\nmeaning the routing is the only source of modelling such relationships which is sensible when trying to\naccommodate vastly different time-series.\nWoo et al. (2024) Another foundation model. The key motivation is being able to process any arbitrary\ntime series, this approach can model all auxiliary variables. All related inputs are padded, flattened, and\nconcatenated into a single series which is then patched to tokenize. Multiple scales are not used actively.\nInstead multiple resolutions are defined and contained in the model but only one is used for a given forecasting\ntask. The choice of resolution is determined heuristically.\nZhang et al. (2024) This work is the closest to ours in to what degree it models the relationships between\nembeddings at different resolutions. Each block uses separate branches to patch for different resolutions but\nat the end of the block it learns a linear projection which fuses all tokens to form a vector which then\ngets patched again in the next block. Every fusion models the relationships of representations at different\nresolutions. Interestingly the patching is done at each block. The approach does not extend to auxiliary\ndata or model cross-series relationships."}, {"title": "The Model: Multiple-Resolution Tokenization (MRT)", "content": "Our model is based on two ideas. First, that time series carry relevant information at multiple resolutions and\nthat this should be represented in attention simultaneously. Second, that the resulting multiple resolution\ntokens can and should be joined with tokens obtained from auxiliary data to form the input matrix for\nthe transformer. This idea is a response to one of the key design questions in transformers for time series\nforecasting: what are meaningful units of information. Tokens in this context should just be understood\nas meaningful representations of some facet of the time series. In designing separate tokenisation for past\nand auxiliary data we implicitly assume that auxiliary data carries significant information. Designs which\ninclude auxiliary information often meld all information at a given token together (Lim et al., 2021; Chen\net al., 2023), whereas we process separately for each type of auxiliary information. The separation into more\ntokens enables explicit attention links to aspects of auxiliary information which increases interpretability. The\nmultitude of resolutions complemented by representations of auxiliary variables as a tokenization strategy\nenables the learning of complex pairwise relationships.\nTo add clarity we introduce a common notation to describe the view of the tensor and the operation performed\non it at a given point in the model. We will describe the dimensions of a tensor with a collection [ ] of\nletters which represent dimensions. The last k letters represent the dimensions the operation is taking place\nover. Most operations are performed over a single dimension, however there are exceptions for example\nself attention operates on matrices so the last two dimensions. If a tensor has been flattened this will be\nindicated in the new view as with a and if two tensors have been concatenated in a dimension we will\nuse +. The relevant dimensions in our case are B for batch or sample, C for channel, T for time and for\npatch which is typically a compression across the time dimension and auxiliary variable embeddings (can\nbe understood as the token dimension), V for the variable dimension, and L for the latent dimension. Note\nthat these dimensions will not describe unique states as they serve as illustrators as opposed to an exact\nfunctional description of the model which can be found in the code. We use lower case letters for fixed values\nto emphasise the size of the dimension, for example (unless stated otherwise) the size of the latent dimension\nis $d_m$ so [B, C,T, L = $d_m$]. To illustrate this notation consider the standard output head of PatchTST (Nie\net al., 2023), the operation takes the output from the transformer in the form [B, C,T, L], flattens it to\n[B,C,TL] applies a linear layer to the last dimension to produce an output [B, C, f]. In short we denote"}, {"title": "Multiple-Resolution Patching", "content": "The multiple resolution patching is defined by a resolution set $K = k_1,..., k_r$ takes an input of the form\n[B,C,T = $l$] and uses a set of |K| = r linear transformations to create an output of the form [B, C,T, L].\nDenote the size of the latent dimension of the model as $d_m$. We extend patching to multiple resolutions by\nusing several divider blocks. A divider block splits a time series into $k_i$ roughly equal parts. For a series\nof length $h$ the base patch length is set as $b_i = \\lfloor h/k_i \\rfloor$. If $k$ does not divide $h$ we increase the length of\nthe first $h - b_ik_i$ patches by 1. In our application $h$ is either $l$ (past data) or $l + h$ (time-varying known).\nFor each resolution $k_i$ we learn a linear projection into the latent space $\\mathbb{R}^{b_i+1 \\to \\mathbb{R}^{d_m}}$. Weights are shared\nfor all patches within a resolution. We left pad patches that are $b$ long with a 0. The core operation\nis [B,C,T = $b_i + 1$] \u2192 [B,C, L = $d_m$] which is repeated $\\sum_{k_i \\in k} k_i$ times ($k_i$ times for each resolution)\nand stacked along a patch/token dimension into [B,C,T = $N_{MRP},L$]. The created dimension $T$ contains\n$\\sum_{k_i \\in k} k_i = N_{MRP}$ tokens obtained from multiple resolution patching. An illustration of multiple resolution\npatching is presented in figure 1."}, {"title": "Handling Auxiliary Information", "content": "A key advantage of deep learning models over classical models is that they can accommodate arbitrary\nforms of input enabling us to jointly model all available information for a time series. Auxiliary information\nis not trivially useful for time series forecasting. Most recent state of the art papers exclusively focus on\npast/observed data. The inclusion of auxiliary data often leads to issues with overfitting, explainable by\nthe noise in time series data and small datasets. In existing work which includes auxiliary information it is\ntypically mixed with past data to form tokens representing all information at each time step or resolution.\nWe take a different approach and design a separate tokenisation scheme for different types of auxiliary data.\nThis enables us to explicitly learn contextual representations of the time series that are purely auxiliary and\nnot directly dependent on the noise in the time series, which should help reduce overfitting.\nBase Tokenization Data for time series is either continuous or categorical. Categorical variables need\nadditional processing, in the case of transformers entity embeddings are standard practice and amount to\nlearning a vector embedding for each type within a category. To match the embedding size in the case\nof numerical auxiliary variables we employ numerical embeddings. We learn a vector for each numerical\nauxiliary variable which is then scaled by the value of that variable. We call this process base tokenisation.\nTo handle missing values, we assign a separate learnable vector embedding for each variable in cases the value\nhas not been observed. When there is variation in the length of the input horizon we left pad all temporal\nseries with a special category or numerical value that always yields a zero vector embedding, which then does"}, {"title": "Time-Varying Known Variables (TVK)", "content": "The TVK of a time series form a tensor [B,C,T = $l +$\n$f, V$]. We process global and specific variables separately. The base tokenisation first transforms both the\ncategorical and numerical variables in the tensor from [B, C,T,V] \u2192 [B,C,T, V, L]. We then learn a linear\nprojection that compresses the variable dimension to one [B, C,T, L, V] \u2192 [B, C,T, L, V = 1] = [B, C,T, L].\nThis base tokenisation plus mixing across variables creates one auxiliary token per time step.\nIn line with previous multiple resolution patching we apply a modified version built on basis combination.\nIn contrast with past-data we want to patch across tokens (collection of vectors) not individual observations\n(collection of points). We split the tensor $k_i$ roughly equal parts for each resolution $i$ along the time\ndimension. We use the same resolutions, rules on length (denote the length of a patch $a_i$), and padding (so\neach patch is actually $a_i + 1$), but for a longer series as the size of T is $l + f$. The splitting leaves us with\na number of tensors of the shape [B, C, L, T = $a_i + 1$]. We treat the columns of each matrix in the last two\ndimensions of the from [L = $d_m,T = a_i + 1$] as a basis. For each resolution we learn a basis layer: vector of\nsize $a_i +1$ which linearly combines the columns of the matrix into a single vector. The basis layer transforms\n[B,C,L,T = $a_i + 1$] \u2192 [B,C,L,T = 1] = [B, C, L] for each patch in each resolution. This operation is\nillustrated in figure 2. The basis combination results in $\\sum_{k_i \\in k} k_i = n_{MRP}$ such tokens. The tensors are\nstacked along a patch dimension into [B,C,T = $n_{MRP},L$]. To reduce the relative influence of auxiliary\nvariables and reduce the size of the input matrix to the transformer we employ a linear compression layer\nto compress the patch dimension to a predefined number of tokens $n_{TVK}$. This layer mixes across patch\nrepresentation. The transformation is a linear layer [B, C, L,T = $n_{MRP}$] \u2192 [B,C,L,T = $n_{TVK}$], which\nwe transpose to [B, C,T, L]. This process creates a a set of TVKT which are a mixture of representations\nextracted at multiple resolutions from joint base representations of the time-varying auxiliary data."}, {"title": "Static Variables", "content": "Static variables form a tensor [B, C, V]. They do not have a time component and are\nequal across channel C if they are global and can differ if specific. The base tokenisation transforms the\nstatic variable tensor into [B, C, V, L]. If V is low we append the base tokenisation to the MRP directly\nmeaning the number of static tokens (ST) $n_s = |V|$ and the variable dimension is just treated as the token\ndimension. Otherwise we employ an additional linear layer [B, C, L, V] \u2192 [B,C, L,T = $n_s$] which operates\nover the V dimension and condenses the representation to a predefined number of mixed tokens $n_s$."}, {"title": "Cross-Series Information", "content": "The overfitting problem has also been empirically observed for the inclusion of cross-series information, which\nhas motivated the use of channel independence Han et al. (2024). We propose a channel mixer module which\nemploys a mixer architecture to generate a set of cross-series tokens (CST) from the tensor containing all\nother tokens.\nWe call the module responsible for extracting cross-series information the channel mixer. As input the\nchannel mixer takes the tensor of all existing tokens MRP, ST, and TVKT. Define the number of these\ntokens as $n_B =N_{MRP}+n_s+n_{TvK}$. The input tensor takes the form [B, C, T = $n_B$, L]. We employ a mixer\narchitecture inspired by Chen et al. (2023) to extract cross series tokens. Mixer architectures have been\nshown to perform well on time series forecasting while extracting cross-series representations in every block.\nWe roughly illustrate the module in figure 3. The operating tensor is normalised before each mixing step and\na skip connection is employed across each mixing step. The first mixing step is across the token dimension\nT, applying a linear transformation [B, C, L, T = $n_B$] \u2192 [B, C, L, T = $n_B$] followed by an activation function\nand dropout. The number of tokens is then squeezed down to a predefined number $n_{CST}$ with another linear\nlayer [B, C, L, T = $n_B$] \u2192 [B, C, L,T = $n_{CST}$] (we use A here to signal that the tokens have been mixed into\nan auxiliary information representation). The second mixing step is across the channel dimension C. Two\nlinear layers are employed, the first projects the channel dimension of size $d_c$ into the latent dimension of the\ncross series module $d_{cross}$ (adjustable hyperparameter), so [B, A, L, C = $d_c$] \u2192 [B, A, L, C = $d_{cross}$]. After\nan activation function and a dropout layer the second linear layer projects back into the original channel\ndimension [B, A, L, C = $d_{cross}$] \u2192 [B, A, L, C = $d_c$]. After the end of the second mixing another linear layer\nsqueezes the channel dimension to one leaving us with [B, C = 1,T, L]. The reason this is done is so that the\ncross-series tokens are the same across channels. To align it with the dimensions of our collection of tokens\nwe repeat the tensor |C| = $d_c$ times to obtain [B,C = $d_c$,T = $n_{CST}$, L]. We append it to all other existing\ntokens to obtain the final input matrix A with size $n_{MRT} = n_B + n_{CST}$ which is passed to the transformer\nmodule [B, C, T = $n_{MRT}$, L]."}, {"title": "Output Head", "content": "We propose a novel output head architecture which has favourable scaling properties. We call it reverse\nsplitting. The transformer blocks produce a matrix which is the same size as the input matrix [B, C, T, L].\nUsing the same resolution values as for the splitting, we reverse the process. For resolution $k_i$ we take the\nnext $k_i$ column vectors from the matrix and individually project them into patches that are composed to\nrepresent the forecast. The length of the projected patches ($p_i$ or $p_i + 1$) is determined the same way as\nthe splitting in MRP. We learn a projection matrix for each resolution $\\mathbb{R}^{d_m \\times p_i+1}$ and omit the final vector\nvalue for the reverse patching for which we have set the length to $p$. The core operation iterates over the\ndimension T so each input is [B, C, L] which is projected into [B,C,T = $p_i + 1$], this is repeated $k_i$ times\nand concatenated according to the length rule into [B, C, T = f]. This leaves us with K\u2758 forecast vectors\nof the form [B,C,T = f] which we simply sum to obtain the final output. We call this reverse splitting."}, {"title": "Other Architectural Choices", "content": "The tensor of all tokens A is processed by a sequence of transformer blocks after which it is passed to\nthe reverse splitter output head. We use multi-head self-attention in the transformer block. The learning\nin self attention can be interpreted as learning four different mixing rules across the L dimension to infer\ngood pairwise comparisons. Since the focus of this work is on tokenisation strategies we avoid extensively\ninvestigating positional encoding or masking strategies.\nMasking We use an encoder design without masking. While a decoder architecture with masking could be\ndesigned to be time consistent with patching at multiple resolutios it is not trivial to extend it to TVKT and\nCST so we avoid masking. Any form of mixing such as compressing TVKT or CST across the T dimension\nobscures causality.\nPositional Encoding We use learnable positional encoding. This is equivalent to learning a fixed bias for\nthe input matrix to the transformer. We do this as it is difficult to design positional encoding for a complex\nset of tokenisation procedures. Certain patching approaches use aggregations over conventional positional\nencoding, but that is still limited to only past data.\nAfter Attention We follow the standard transformer architecture: self-attention is followed by a skip\nconnection, normalisation and then a feed-forward network. We use a standard two linear layer MLP with an\nactivation function and dropout. The latent dimension in between the two layers is another hyperparameter\n$d_{ff}$. The feed-forward network is applied to the latent dimension, so separately to each token. The operation\nis [\u0412,\u0421,\u0422 = $n_{MRT},L$] \u2192 [B,C,T = $d_{ff},L$] \u2192 [B,C,T = $n_{MRT}$, L]. This is followed by a skip connection\nfrom the input to the feed-forward network and another normalisation."}, {"title": "Architectural Limitations", "content": "Multiple resolution patching and associated auxiliary variable tokenisation adds many more hyperparameters\nin the form of the resolution set. The space of resolution sets is combinatorial making hyperparameter\noptimisation a challenging task. We somewhat constrain the space by not allowing overlaps, thus avoiding\nthe need for a stride hyperparameter associated with every resolution. Our tokenisation modules increase\nthe number of learnable parameters in the embedding especially for small $k_i$ which makes scaling to large\nlatent sizes more difficult. The CST are sequentially computed downstream of all other tokens limiting the\ndegree of parallelisation in tokenisation. Since all tokenisation is done end-to-end a degree of overfitting is\nexpected. Computationally the main difficulty is the increased number of tokens which scales computation\nquadratically. This constrains how high $n_{MRP}$ can get limiting dense resolution sets which may be useful for\nlonger context windows with long lookback horizons. Exploiting and encouraging sparsity in self-attention\nmay be one possibility to deal with the larger context window."}, {"title": "Application to a Forecasting Problem in Pricing", "content": "The data for the experiments is real markdown data from a very large European retailer. The markdown\npricing prediction problem is to estimate the sales of an item given a series of prices and other contextual\ninformation. This prediction problem is understood as particularly difficult. The theory is that each time\nthere is a price reduction we observe a spike in sales which then decays. Subsequent price reductions\nproduce a similar spike and decay effect which typically reduces in magnitude with each reduction. This\nhighly non-linear behaviour is difficult to model with simple approaches. This is further exacerbated by\nthe high levels of noise associated with consumer behaviour owing to stochastic factors like the weather or\ncompetitor behaviour and the human factor in actually delivering the pricing strategy in stores. Historical\ndata is limited to a narrow set of tried and tested pricing strategies such as 25% off then 50% then 75%\nadding to the epistemic uncertainty. The downstream decision problem of setting reduced prices for items\nthat are about to expire sees about a million decisions made every day.\nAuxiliary Information In the case of markdowns the inclusion of auxiliary data into the model is nec-\nessary as it is otherwise not possible to use it as a hypothesis generator for new pricing strategies. The\ndecision variable (price) must be included as an input in the prediction model in order to be able to com-\npare predictions under different markdown strategies. We try to predict both the full-price and the reduced\nprice-sales concurrently, this defines the channels in C. This reflects the operational characteristics of the\nretailer where some proportion items that are about to expire get moved to a discounted section.\nOne view of the utility of auxiliary information beyond the necessary decision variables is that it enables\nthe aggregation of a larger number of time series in a single training corpus with more learning potential.\nWe can learn tokenizations which capture the relatively specific context of each time series, which leads\nto a more general purpose prediction model for sales. In the case of markdowns individual time series are\nrelatively short and highly contextual. We take each series to be defined by as the unique combination of\nproduct, store, and expiration date. We want to aggregate all products of interest across all shops for all\ndates in a single model. We include auxiliary information because we do not believe that there is sufficient\ninformation in the starting course of a series to effectively differentiate amongst products and stores and\nexpiration dates. We use following auxiliary information:\n\u2022 Static:\n\u2022 TVK:\nGlobal: product group, brand, stock.\nSpecific: dummy variable for channel (either full price or reduced price sales channel).\nGlobal: day of week, hour (half-hourly, continuous), month, which iteration of price reduction,\nproportion of stock marked down.\nSpecific: price (constant for full price prediction, changing for reduced price prediction).\nNormalisation To further enable the aggregation of many potentially heterogeneous series and in line\nwith effective practice for deep learning we use a collection of normalisation procedures. Normalisation is\nprincipally utilised to deal with the problem of different scales. Neural network training can easily result in\nunsatisfactory outcomes without scaling due to issues with the absolute and relative magnitude of gradients.\nNormalisation leads to improved generalisation due to increased stability in training and being able to deal\nwith previously unseen scales. We employ two types of normalisation: instance normalisation for each"}, {"title": "Preprocessing", "content": "From a model architecture standpoint this dataset poses two key problems: irregularly spaced sampling, and\nthe varying length of the series.\nQuantisation The aggregation boundary times are rounded to the nearest 30 minute mark. The series\nis then quantised into 30 minute sections. We chose 30 minute intervals to create sequences with a non-\ntrivial length, to add more auxiliary data to the sequence, and to enable a prediction of more potential\npricing strategies. We quantise the entries in all columns which are aggregated (sales) over the given time\nperiod, the rest are simply copied. The quantisation is as simple as dividing the values by the number of\n30 minute periods (exclusive of closure times) that occur from one price reduction to the next. There are\ntwo simplifications here: the rounding to 30 minutes means that we do not perform any imputation on the\nboundaries of the aggregations, the second is assigning the average value to each of the quantised fields.\nMore disaggregated data would obviously work even better.\nNote that there are approaches that use irregular time series natively such as neural differential equations\n(Kidger et al., 2020) to model the latent state between two observations. Unfortunately, an initial value\nproblem has to be solved at every iteration which is computationally infeasible for large datasets.\nCoping with Varying Length\nTo reduce the effect of changing sequence lengths we only keep the top five percent of sequences in terms\nof length. This also reduces the dataset to a much more manageable size. Since the sequences vary in the\nlength of time, so does the number of steps obtained after quantisation. We fix the forecast horizon to\nthe final 24 periods so f = 24. In its current design the model works for any fixed forecast horizon. We\ncontend with varying input lengths by setting the lookback horizon to the longest sequence minus the fixed\nforecast horizon and padding all shorter sequences with values that result in zero vector tokens effectively\ndeactivating that aspect of the network."}, {"title": "Experiments", "content": "The experiments were defined by the range of year-weeks they contained. The experiments are limited to\njust one week due to memory constraints. Each experiment contains the markdown data for one week across\nall shops and all items. For any experiment the series corresponding to the last 20% (rounded up) of the\ndue-dates were held out as the testing set, the 15% (rounded up) of due-dates immediately preceding were\nused as the validation set and the remaining as the training set.\nModel Hyperparameters In this set of experiments we only used one set of model hyperparameters. The\nparameters picked reflect a set of informal observations on academic datasets and computational limits. We\nset the base latent dimension to $d_m = 64$, the dimension of the middle fully connected layer in the transformer\n$d_{ff} = 128$, the latent dimension in the channel mixer is set to $d_{cross} = 16$, we use the GeLU activation\nfunction, and set dropout to 0.0. We use 8 heads in the multi-head attention and stack 2 transformer layers.\nWe compress the number of TVKTs and CST to 8. We pick the following set of resolutions {1,```json\n, 2, 3, 4, 6, 8}.\nThis results in $n_{MRP} = 24$, $n_{TVK} = 2 * 8$ (specific and global), $n_s = 4$, and $n_{CST} = 8$ for a total of 52\ntokens. The resulting model had a parameter count of roughly 128k.\nTraining The batch size was set to 128, we used the Adam optimiser with a fixed learning rate of 0.0003,\nthe loss function was minimising the root mean squared error (summed across channels, time steps, and\nsamples). Note that MSE is not the optimal loss function for generalisation and calibration in this case, but\ninvestigating what is is beyond the scope of this work. We train the model for 20 epochs every experiment\nwith early stopping if no improvement in validation score is observed for 3 epochs.\nResults We display the results of our two main experiments in tables 1 and 2. The lookback horizon\nin the week 30 experiment is 28 and 37 in week 45, the forecast horizon is fixed at 24, so the next 12\nhours of operation. The week 45 experiment contains approximately 63 thousand series and training took\napproximately 2 hours per epoch run locally on a CPU. Early stopping in the week 30 experiment was\ntriggered after 7 epochs and after 19 epochs in the week 45 experiment. We only had access to internal\npredictions on full price sales, so we exhibit the mean squared error (MSE), and mean average error (MAE)\nfor that channel, comparing the internal result with our MRT result. The specifics of the internal predictions\nmethods are confidential but are built with statistical and classical machine learning techniques. We also train\nPatchTST (Nie et al., 2023) as the canonical transformer for forecasting (patch length 8 with stride 4, same\nlatent dimensions) and DLinear (Zeng et al., 2023) as an example of a simpler alternative to transformers\nunder the same training conditions."}, {"title": "Discussion", "content": "The model is competitive with internal predictions and an improvement on existing models in the week 30\nexperiment and significantly outperforms all other models on week 45. Note that the test scores have been\nreverted to their original scales. Given the complexity of the prediction task across a range of products and\nlocations and the necessary adjustments to fit a sequence-to-sequence model the result for week 45 provides\nstrong evidence that MRT is a powerful model which could deliver much with hyperparameter optimisation\nand scaling. The week ranges on these experiments are somewhat limited but we are confident that over\nlonger ranges these models would be even more competitive due to stronger auxiliary embeddings (especially\nfor global temporal information) and general scaling effects.\nOne contrarian explanation of week 45 results is that many values are 0 and our network learns to predict\nnear 0s in many cases. The many values being 0 case is somewhat credible given our selection of only\nthe longest series; the longer duration could reflect that getting rid of stock in those cases is difficult. An\nencouraging sign is that the predictions for reduced price sales are better than those for the full price sales\n(assuming similar scales across channels). Since we have shown that we are competitive or better than\ninternal models for full price sales, the reduced price sales predictions show even more promise.\nThe experiments are somewhat limited in scope. This is mainly due to computational constraints and the\ndelayed timing of the project. More experimentation is needed to weigh the evidence on this architecture.\nWith more compute the experiments could be extended to much larger dataset ranges and latent spaces.\nScaling properties are clearly of interest. Ablation would demonstrate what relative benefit there is for\neach of our tokenisation procedures. Hyperparameter exploration in the space of resolution sets would\nhelp determine heuristics on what resolution combinations are performant and uncover good heuristics like\nperhaps including multiple copies of the same resolution if it is highly involved in attention patterns.\nDue to commercial sensitivity the dataset cannot be shared\u00b2. We acknowledge that this is a serious limi-\ntation from a reproducibility standpoint, but believe that the experiments performed here are a substantial\ncontribution to research on transformers for forecasting. This is because most existing architectures have\nbeen empirically evaluated on a narrow set of datasets which are typically small scale such as ETT (Zhou\net al., 2021). What is most likely dataset bias has caused a shift of interest away from using auxiliary or\ncross-series information. In contrast we demonstrate that our transformer architecture outperforms cur-\nrently used in-house methods, DLinear, and PatchTST on a messy real-world problem. The work presents\nevidence that for certain forecasting problems modelling auxiliary data and cross series information adds to\ngeneralisation. More ablation is needed to understand what aspects of our tokenisation strategies are the\nmost important in supporting this claim."}, {"title": "Conclusion", "content": "The experiments provide clear evidence that our proposed MRT architecture works well on a noisy real-world\nproblem. This is achieved with an architecture which includes both auxiliary variables and models cross-\nseries information which otherwise is often avoided in transformer architectures for time series forecasting.\nThe modules designed for this purpose are novel as is the practice of treating all of their outputs as tokens\nin a single attention mechanism. While our experimentation is limited one of the experiments shows a\nsubstantial improvement in performance over existing architectures. Given the difficulty of the underlying\nprediction problem this is a significant observation in favour of testing and potentially deploying transformer\narchitectures on difficult real-world forecasting problems in the pricing domain and beyond."}]}