{"title": "GazeSearch: Radiology Findings Search Benchmark", "authors": ["Trong Thang Pham", "Tien-Phat Nguyen", "Yuki Ikebe", "Akash Awasthi", "Zhigang Deng", "Carol C. Wu", "Hien Nguyen", "Ngan Le"], "abstract": "Medical eye-tracking data is an important information source for understanding how radiologists visually interpret medical images. This information not only improves the accuracy of deep learning models for X-ray analysis but also their interpretability, enhancing transparency in decision-making. However, the current eye-tracking data is dispersed, unprocessed, and ambiguous, making it difficult to derive meaningful insights. Therefore, there is a need to create a new dataset with more focus and purposeful eye-tracking data, improving its utility for diagnostic applications. In this work, we propose a refinement method inspired by the target-present visual search challenge: there is a specific finding and fixations are guided to locate it. After refining the existing eye-tracking datasets, we transform them into a curated visual search dataset, called GazeSearch, specifically for radiology findings, where each fixation sequence is purposefully aligned to the task of locating a particular finding. Subsequently, we introduce a scan path prediction baseline, called ChestSearch, specifically tailored to GazeSearch. Finally, we employ the newly introduced GazeSearch as a benchmark to evaluate the performance of current state-of-the-art methods, offering a comprehensive assessment for visual search in the medical imaging domain.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) has been growing rapidly and become an important part of daily life [3, 27, 36, 49, 57, 64], including important workers like clinical experts and healthcare providers. Even though human experts remain the ultimate authority in decision-making, researchers are focusing on improving AI-assisted systems to reduce the burden for the experts. For example, we can use AI to produce preliminary results and the experts can either confirm or adjust [64]. As a result, the collaborative approach between AI and professionals has successfully improved radiological diagnosis in many cases compared to radiologists or the system alone [57]. However, a key challenge is building trust in AI, especially with black-box models in healthcare, such as CXR analysis. This has increased the demand for models that mimic radiologists' behavior to improve interpretability. For instance, aligning AI systems with radiologists' visual attention patterns is essential [47, 50]. This has opened a new domain of research focused on modeling the radiologists's eye movements to improve the transparency and reliability of AI systems in clinical practice [6].\nRecognizing the importance of understanding how radiologists' eye movements impact diagnosis, datasets like EGD [35] and REFLACX [4] have been introduced. But, these eye-tracking datasets present two major challenges: Challenge #1: Free-view format - Existing eye-tracking"}, {"title": "2. Related works", "content": "Visual Search Datasets. Search datasets have been rising recently due to the interest in understanding human behavior [8,21,22,25,32,46,59,65,66,70]. This is particularly evident in the general visual domain, where numerous datasets have been created across diverse settings. These datasets cover a wide range of scenarios, from searching for multiple targets simultaneously [25] to focusing on a single or two target categories [22, 70]. Some datasets, like COCO-Search18 [66], feature a large number of target objects, or adopt a Visual Question Answering approach [8]. In contrast, the medical domain has lagged behind in terms of dedicated visual search datasets. Existing medical datasets primarily focus on multi-target search tasks, as demonstrated by datasets like EGD [35] and REFLACX [4]. However, there is a significant lack of search datasets tailored for the medical domain. This paper makes a novel contribution by addressing this research gap. We introduce the first target-present visual search dataset specifically designed for the medical field. This dataset opens up new avenues for research and development in this critical area.\nVisual Search Baselines. Parallel to the growth of visual search datasets, significant advancements have been made in scan path prediction accuracy [1, 2, 7, 9-11, 15, 17, 23, 28, 31, 38, 43, 48, 53, 54, 61, 66-69, 71]. Early scanpath models mostly rely on sampling fixations from saliency maps [30, 42, 60, 62]. Recent advancements, including the integration of deep neural networks [10, 17,33,37,43,48,54, 66, 68, 69], reinforcement learning techniques [10, 66, 68], and transformer-based architectures [11, 43, 48, 67], have significantly deepened our understanding of the temporal dynamics of human attention. However, generic models are designed for broad application, so the performance of generic visual search models on CXR is uncertain and potentially subpar. This work introduces a transformer-based method that can work well without these restrictive assumptions. Additionally, we further conduct a comparative ex-"}, {"title": "3. GazeSearch Dataset", "content": "When studying free-view eye-tracking datasets from sources like REFLACX [4] and EGD [35], we notice that the eye-tracking data (including both gaze and fixations) is often ambiguous and lacks clarity. This ambiguity comes from the data collection settings, where radiologists look for multiple findings simultaneously. As a result, each fixation captures visual information relevant to multiple findings rather than a specific finding. Therefore, the fixations from these eye-tracking datasets are unsuitable for studying their relationship to specific findings, i.e. addressing the visual search problem. Additionally, when visualizing these gaze points or fixations over an image, they often cover more than 80% of the lung area, even though the actual anomaly area might be much smaller. We calculate the fixation coverage distribution in Supplementary. This raises a concern that using the free-view fixations from the given datasets may not be effective and could even pose risks in sensitive sectors like healthcare, particularly for tasks requiring precise localization of specific findings.\nTo solve this issue, one way is to collect eye-tracking data under the visual search setting directly. However, to"}, {"title": "3.1. Naive Finding Mapping", "content": "The first problem we must solve is the mismatch between the fixations and the corresponding radiologists' report sentences. The main reason is radiologists observe the images first and then describe their findings, meaning the fixations within the time frame of a sentence may not fully capture the findings reported. Inspired by I-AI [47], we start by completely removing fixations after the current spoken sentence. Let $S = \\{s_1, s_2, ..., s_{|S|}\\}$ be the sequence of sentences in the transcript. Let $C = \\{C_1,C_2, ...,C_m\\}$ be the set of possible findings (e.g., CheXpert labels). We define a function $\\phi : S \\rightarrow C$ where $c_j = \\phi(s_i)$ if sentence $s_i$ corresponds to finding $c_j$. In our implementation, $\\phi(\\cdot)$ is the Chexbert model [51]. For a target finding $c' \\in C$, let $u = max\\{i|\\phi(s_i) = c',1 \\leq i \\leq |S|\\}$. Then, the new finding-aware fixations $F'$ for $c'$ is\n$F' = \\{(x_i, y_i, t_i, d_i)|(X_i, Y_i, t_i, d_i) \\in F, 0 \\leq t_i \\leq e_u\\}$ (1)\nwhere $F = \\{(x_1,y_1,t_1,d_1), .., (X_{|F|}, Y_{|F|}, t_{|F|},d_{|F|})\\}$ is the free-view fixations, with $(x_i, y_i)$ as spatial coordinates, $t_i$ as captured timestamp, and $d_i$ as duration, and $e_u$ is the ending time of the sentence $s_u$. From this point onwards, we only use the triplet $(x_i, y_i, d_i)$ and ignore the captured timestamp $t_i$ for our fixation sequence: $F = \\{(x_1, y_1, d_1),..., (x_n, y_n, d_n)\\}$, where $n = |F|$ is the fixation sequence length."}, {"title": "3.2. Finding-Anatomy Relation Matrix", "content": "To address this, we leverage the Chest ImaGenome [63] dataset, which offers pairs of findings and their corresponding anatomies, along with anatomy bounding boxes linked to each finding. For precision, we rely on the gold subset of Chest ImaGenome to construct a relation matrix between findings and anatomies. As a final step, a radiologist with"}, {"title": "3.3. Visual Search Constraint Imposition", "content": "After Section 3.1, the maximum fixation sequence length can be over 340 fixations for a finding. Therefore, another task we must solve is reducing this length to an interpretable level for humans.\nUtilizing both properties (1) and (2) as our guidance for this process, we perform two main steps: radius-based filtering (to enforce property #1) and time-spent constraining (to enforce property #2). Besides property #1, we observe that the captured fixations from EGD and REFLACX cover one-degree visual angle [4, 35, 39]. Based on that fact, we use the Algorithm 1 to cluster the finding-aware fixations $F$ to create another fixation set $\\hat{F}$, with a larger radius $r$ of two-degree of visual angle and $M$ is the max length of fixation sequence. Property #1 is enforced by iterating backward from the end to the beginning of the fixation sequence $F$. Then, we use the Algorithm 2 to make sure the late fixations must spend the most time in the anatomies of interest, which satisfies property #2.\nIn Algorithms 1 and 2, we define a point $(x, y)$ to be in the bounding box sets $B$ for notation convenience:\n$(x, y) \\in B \\Leftrightarrow x_{left} < x < x_{right}, y_{top} \\leq y \\leq y_{bottom}$,\n$\\sqrt{(x_{left}, y_{top}, x_{right}, y_{bottom})} \\in B$ (2)\nTo align with the COCO-Search18 dataset, we set the maximum fixation length to $M = 7$ and add a default center as the start fixation. This choice is based on the observation that 95% of the samples in COCO-Search18 have fixation lengths under 7. For the first fixation's duration, we assign 0.3 seconds to it, which reflects the duration of 91% of first fixations in COCO-Search18. In total, GazeSearch has 2,081 images with 413 samples from EGD and 1,668 samples from REFLACX. There are a total of 13 findings. Each sample has fixations for 1 to 6 findings and has a max length of 7, including the default middle fixation. For training and evaluation, we split the dataset into 1,456 samples for training (70%), 208 samples for validation (10%), and 417 samples for testing (20%)."}, {"title": "3.4. Usage Validation", "content": "Filtering fixations requires discarding information, so it is essential to test and ensure that the new data remains valuable. To validate that GazeSearch's fixations can be as useful as the free-view fixation maps from EGD and RE-FLACX, we follow Karargyris et al. [35] to perform the"}, {"title": "4. ChestSearch", "content": "Given a CXR image $I$ of dimensions $H \\times W$ and a target finding $c'$, our objective is to generate a scan-path comprises of fixations $y = \\{f_i\\}_{i=1}^n$, where $n$ represents the number of fixations, and $f_i = (x_i, y_i, d_i)$ is the fixation at 2D coordinate $(x_i, y_i)$ with a duration of $d_i$.\nFigure 3 provides an overview of our method. The pro-"}, {"title": "4.1. Feature Extractor", "content": "Using features from only the last layer is inadequate for predicting scanpaths [68]. Therefore, we employ ResNet-50 FPN [40] as our Feature Extractor module (FE). Besides, using the ImageNet [18] checkpoint may not be optimal for the medical domain, so we train the FE using a self-supervised approach based on MGCA [58] with the MIMIC-CXR dataset [34]. From the CXR image $I$ with size $H \\times W$, FE produces four multi-scale feature maps $P = \\{P_1,..., P_4\\}$. Then we need to mimic how human see an image: at first we only see the image at a high level understanding, with no clear details, and then we look carefully to search for what we need [66]. So we use one feature map with low resolution $P_l = P^1 \\in R^{C \\times \\frac{H}{8} \\times \\frac{W}{8}}$, where $C$ is the channel dimension, to represent high-level visual feature, and one high-resolution feature map $P_h = P^4 \\in R^{C \\times \\frac{H}{4} \\times \\frac{W}{4}}$ to represent detailed visual information."}, {"title": "4.2. Spatiotemporal Embedding", "content": "Given the previous predicted fixations $\\{(x_i, y_i)\\}_{i=1}^{t-1}$, $P_l$, and $P_h$, we then embed the previous fixations to create the feature list as the input for the decoder in Section 4.3.\n2D Spatial Indexing. Every $(x_i, y_i)$, where $0 \\leq x \\leq W$ and $0 \\leq y \\leq H$, is scaled down to the same resolution as of $P_h$, which result in the new $0 \\leq x \\leq \\frac{H}{4}$ and $0 \\leq y \\leq \\frac{W}{4}$ in our case. Then, we index the feature cell at the coordinate $(\\hat{x}, \\hat{y})$ in $P_h$, called $\\hat{P_h}$. We will have the list of feature\n$\\mathcal{D}_{h}^{t-1}$"}, {"title": "4.3. Fixation Decoder", "content": "At this layer, we have the finding list $q = \\{q_c\\}_{c=1}^{|q|}$ which serves as the set of queries. The number of queries is the number of findings in our dataset $|q| = 13$ with $q_c \\in \\mathbb{R}^D$ is a learnable embedding for the current finding query $c$. The previous module (Section 4.2) gives us the embeddings of previous fixations $E$.\nThe Fixation Decoder module is the modified transformer decoder [13] including the blocks as shown in Figure 3. The cross-attention block uses the query embedding $q$ as the query input $Q$, with $E$ serving as both key $(K)$ and value $(V)$. This allows the model to capture the correlations among previous fixations and accurately predict the next fixation. The resulting feature then passes through self-attention layers, residual connections, normalization, and a feed-forward network. This process repeats for $L$ layers in the decoder. The final output $\\bar{E} \\in \\mathbb{R}^{|q| \\times D}$ is then processed by three different heads."}, {"title": "4.4. Termination Head", "content": "A fixation sequence's length can vary, so our model needs to learn when to stop. To achieve this, we use a head consisting of a fully connected (FC) layer followed by a sigmoid function that maps $\\bar{E}$ to termination value i.e., $\\hat{t}_f \\in \\mathbb{R} = sigmoid(FC_\\uparrow(\\bar{E}))."}, {"title": "4.5. Duration Head", "content": "The duration can be considered as a Gaussian distribution. We use $\\bar{E}$, then regress it into a mean value $\\mu_{d_t} = FC_\\mu(\\bar{E})$ and a log-variance $\\lambda_{d_t} = FC_\\lambda(\\bar{E})$:\n$\\hat{d}_t = \\mu_{d_t} + \\epsilon_{d_t} \\cdot exp(0.5 \\lambda_{d_t}), $\n$\\epsilon_{d_t} \\sim N(0,1)$ (3)\nwhere $\\epsilon_{d_t}$ noise gives our prediction a probabilistic characteristic, and $\\hat{d}_t \\in \\mathbb{R}^{|q|}$ is the duration prediction. The inspiration comes from using the reparameterization trick [20], which allows us to backpropagate from the label back to the normal distribution."}, {"title": "4.6. Distribution Head", "content": "Because fixation is random in nature, we predict a 2D distribution in the form of a heatmap $\\hat{h}_t \\in [0,1]^{|q| \\times (\\frac{H}{4} \\cdot \\frac{W}{4})}$. Formally, we compute:\n$\\bar{E}' = MLP(\\bar{E})$\n$\\hat{h}_t = sigmoid(Matmul(\\bar{E}', P_h))$ (4)\nwhere $Matmul(\\cdot, \\cdot)$ is the matrix multiplication between two input tensors, and $\\bar{E}' \\in \\mathbb{R}^{|q| \\times D}$ is the latent embedding prepared for heatmap generation. At inference, we sample the next 2D coordinate $\\hat{f}_t = (\\hat{x}_t, \\hat{y}_t)$ from the distribution map $\\hat{h}_t$ for every given timestamp $t$."}, {"title": "4.7. Objective Functions", "content": "ChestSearch has three objectives, each corresponding to one of its heads: the loss between the ground truth and predicted distributions, the loss for termination, and the loss for duration.\nThe termination loss is just a standard binary cross-entropy between the predicted termination value $\\hat{t}_t$ and the corresponding ground truth $T_t$.\n$L_t = -T_t log(\\hat{t}_t) - (1 - T_t) log(1 - \\hat{t}_t)$, (5)\nThe distribution loss is defined as focal pixel-wise loss:\n$L_h = - \\sum_{ij} \\begin{cases} (1 - h_{ij})^\\gamma log(\\hat{h}_{ij}) & \\text{if } h_{ij} = 1, \\\\ (1 - \\hat{h}_{ij})^\\alpha (\\hat{h}_{ij})^\\gamma log(1 - \\hat{h}_{ij}) & \\text{otherwise,} \\end{cases}$ (6)\nwhere $0 \\leq i \\leq \\frac{H}{4}$, $0 \\leq j \\leq \\frac{W}{4}$ are the 2D indexes, $N = \\frac{H}{4} \\cdot \\frac{W}{4}$ is the number of values, $\\alpha$ and $\\gamma$ are the hyperparameters indicating the importance of each pixel. The duration loss is defined as the L1 loss, i.e., $L_d = |d_t - \\hat{d}_t|$.\nFinally, we train all three losses jointly.\n$L = L_t + L_h + L_d$ (7)"}, {"title": "5. Experiments", "content": "5.1. Implementation and Metrics\nImplementation details. All images are scaled down to 224 x 224 for computing efficiency. The Fixation Decoder has $L = 6$ layers with a hidden dimension $D = 384$. The MLP of Fixation Distribution Head consists of 384 units with 3 layers and ReLU activation. Eq. (6) has $\\alpha = 4 \\gamma = 2$ based on the best validation results. The Feature Extractor's backbone is ResNet-50 [26], and we obtain the ResNet-50 checkpoint using MGCA [58] for 50 epochs with a batch size of 144. We then finetune this checkpoint jointly with the full pipeline. We train the full pipeline for 30,000 iterations with a learning rate of $1 \\times 10^{-5}$ and a batch size of 32. The entire training process was conducted using AdamW [41], on a single A6000 GPU with 48GB of RAM.\nMetrics. We evaluate fixation scanpath prediction accuracy using various metrics: ScanMatch [16, 52] applies the"}, {"title": "5.2. Quantitative results", "content": "Table 2 demonstrates the proposed method's superior performance, surpassing SOTA approaches. Note that IRL, FFMs, and HAT do not predict fixation duration, so their evaluation on this metric is excluded. IRL and FFMs face challenges with sample efficiency due to reinforcement learning pipelines, while ChenLSTM variants and ISP methods are limited by their specialized modules-ChenLSTM relies on pretrained object detectors and ISPs on Observer-Centric modules. HAT and Gazeformer overgeneralize and fail to fully leverage domain-specific information by design, with HAT ignoring duration data and Gazeformer relying heavily on CLIP for zero-shot visual"}, {"title": "5.3. Qualitative results", "content": "Figure 4 presents a qualitative comparison of scanpath patterns across different radiology findings and models, including radiologists and several state-of-the-art approaches. Generally, ChestSearch predicts more consistent and radiologist-like fixations than other methods. ChenLSTM-ISP often exhibits scattered, less focused patterns, while Gazeformer-ISP may overlook key areas or focus on fewer locations. Although Gazeformer aligns better with ground truth than its ISP variant, it occasionally misses critical regions, such as lung lesions. HAT performs reasonably well but frequently covers the entire lung, even when attention should be limited to smaller areas, such as in cardiomegaly. In contrast, our ChestSearch shows fixation patterns more closely resembling those of radiologists, outperforming other state-of-the-art methods. Overall, Figure 4 underscores the effectiveness of our approach in mimicking expert gaze patterns across different findings. Additional comparison will be included in the Supplementary."}, {"title": "5.4. Ablation study", "content": "To study the design choice of our proposed architecture, we ablate our method under several aspects. More ablation studies are provided in the Supplementary Materials. All experiments are performed on our dataset, with the same training settings. Here, we report the average of all aspects for the MultiMatch score\nThe importance of low- and high-resolution feature maps. In Section 4.2, guided by our intuition, we use two feature maps: a low-resolution map for high-level visual understanding and a high-resolution map for detailed visual understanding. These are concatenated into a single tensor for the Self-Attention layer, with the low-resolution feature serving as a reference and the high-resolution feature indexed using 2D Spatial Indexing to generate temporal features. Ablation results in Table 3 show that omitting 2D Spatial Indexing results in a significant performance drop due to the loss of temporal information. Conversely, not using the reference feature before Self-Attention has a lesser impact.\nInitial Feature Extractor's weight contribution. This ablation studies the effect of the initial weight for the Feature Extractor(Section 4.1), shown in Table 4. In conclusion, using ImageNet checkpoint can give a decent performance. But with a better checkpoint, the performance is higher. This shows the robustness of our architecture."}, {"title": "6. Conclusion", "content": "This paper addresses two key challenges: ambiguous fixations in existing eye-tracking datasets and the absence of a finding-aware radiologist's scanpath model. Drawing inspiration from visual search datasets in general domains, we align findings with fixations, manage fixation durations using a radius-based heuristic, and constrain fixations on duration to produce the first finding-aware visual search dataset, GazeSearch. Our dataset reflects two key properties of visual search behavior: #1 late fixations tend to converge on decisive regions of interest, and #2 fixations within objects of interest are typically longer in duration compared to those outside. We then propose ChestSearch that utilizes self-supervised training to obtain a medical pretrained feature extractor and a query mechanism to select relevant fixations for predicting subsequent ones. The extensive benchmark shows ChestSearch 's ability to generate radiologist-like scanpaths, serving as a strong baseline for future research.\nDiscussion: Our work impacts the behavioral vision literature in the medical domain, where (i) modeling and replicating radiologists' behavior has not been explored, (ii) understanding the understanding of finding-aware visual search and their integration with Deep Learning remains poorly understood [45]. These are critical for advancing diagnostics in radiology, enhancing decision-making processes with accuracy and trustworthiness, and enabling the future development of collaborative interactions between radiologists and AI systems. More broadly, our benchmark serves as the first dataset specifically designed for medical finding search prediction, setting the foundation for research across a wide range of medical applications, including human-computer interaction, error correction, workflow optimization, and expert analysis. Future work will focus on expanding the dataset to include target-absent samples, further enhancing the model's ability to handle complex diagnostic scenarios by identifying both the presence and absence of findings."}]}