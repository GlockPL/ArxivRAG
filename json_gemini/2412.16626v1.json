{"title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement", "authors": ["Junyu Wang", "Zizhen Lin", "Tianrui Wang", "Meng Ge", "Longbiao Wang", "Jianwu Dang"], "abstract": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech enhancement (SE) tasks aim to improve speech clarity by suppressing background noise, reverberation, and other acoustic interferences, thereby optimizing user experience and communication efficacy. In recent years, with the rapid development of deep learning, a variety of representative neural networks have emerged, especially those based on convolutional neural networks (CNN) [1]\u2013[3], transformers [4]\u2013[6], and U-Net architectures [7]\u2013[9]. Generally, depending on the processing method of the input signal, it can be broadly categorized into time-domain and time-frequency (T-F) domain approaches.\nTime-domain studies enhance noisy speech signals directly in the time domain, leveraging encoder-decoder frameworks to map noise waveforms to clean ones [10], [11]. While this approach retains all inherent information features of the time-domain signal, it often results in relatively coarse enhancements. In contrast, T-F domain SE methods demonstrate better capabilities in discerning the intricate structural details between speech and noise [12]\u2013[14].\nThe current mainstream SE models primarily employ two-stage (TS) architectures based on Transformer or its variant, Conformer [15], [16]. By learning input features in both time-domain and frequency-domain representations, these models achieve outstanding SE performance. However, TS methods typically perform dimensionality reduction only along the frequency dimension, which makes it difficult to capture the characteristics of the input information at different resolutions, thereby constraining their performance ceiling to some extent. Moreover, the quadratic complexity of the self-attention mechanism with respect to sequence length poses significant challenges for deploying these models in scenarios with limited computational resources. Recent research [9] introduced MUSE, a more efficient approach combining sub-quadratic complexity Taylor multi-head self-attention (T-MSA) [17] with U-Net framework [18]. By applying Taylor-Transformer to capture multi-scale information at different resolutions, MUSE achieves competitive performance with just 16 input channels and low computational complexity. However, the SE performance of T-MSA still falls short when compared to the original multi-head self-attention (MHSA) mechanism.\nIn parallel, developments in state-space models (SSM) [7], [19] present a promising alternative with linear complexity and high efficiency in handling long-sequence inputs. Mamba [20], as a novel structured SSM (S4), introduces a selective processing mechanism for input information and an efficient hardware-aware algorithm, achieving performance comparable to or exceeding Transformer-based methods across domains such as natural language, image, and audio [21]\u2013[23]. Particularly, a recent work [24] demonstrated improved performance with reduced FLOPs by simply replacing the conformer in MP-SENet with Mamba, further validating the effectiveness of Mamba in speech processing tasks.\nInspired by these studies, we propose Mamba-SEUNet, a model that integrates Mamba with U-Net. By leveraging TS-Mamba within the U-Net framework to learn coarse-grained and fine-grained information at different resolutions and performing multi-scale feature fusion, this network exhibits enhanced long-range dependency modeling and detail restoration capabilities. Extensive experiments on the VCTK+DEMAND dataset [25] demonstrate that Mamba-SEUNet achieves state-of-the-art (SOTA) performance, with a notable reduction in computational complexity measured in FLOPs."}, {"title": "II. MAMBA: SELECTIVE STATE SPACE MODEL", "content": "As an extension of S4, Mamba introduces a selection mechanism that enables model parameters to be dynamically adjusted according to the inputs. This mechanism allows for the selective mapping of the input $x(t)$ through a high-dimensional hidden state $h(t)$ to the output $y(t)$, which can be expressed as follows:\n$h'(t) = Ah(t) + Bx(t)$\n$y(t) = Ch(t)$\n(1)\nTo apply to discrete speech signals, it is necessary to discretize A and B in the Eq.1. Specifically, given a time-scale parameter \u2206, the discrete parameter can be approximated using a zero-order hold as follows:\n$\\overline{A} = e^{(\\Delta A)}$, $\\overline{B} = (\\Delta A)^{-1}(e^{(\\Delta A)} \u2013 I) \\cdot (\\Delta B)$\n(2)\nThus Eq.1 can be rewritten as:\n$h(t) = Ah(t-1) + Bx(t)$\n$y(t) = Ch(t)$\n(3)\nFurther expanding the computation in Eq.3 along the sequence, it can be seen that the output y is computed from the input x through a global convolution with kernel K:\n$K = (CB, AB, ..., CA^{L-1}B)$\n$y = x * K$\n(4)\nwhere L is the size of the input sequence.\nAdditionally, a significant contribution of Mamba is its hardware-aware algorithm, which enhances the efficiency of model execution on modern hardware. The core idea is to leverage the memory hierarchy of modern hardware, such as GPUs, to minimize I/O access between different memory levels. By executing discrete and recursive operations within the faster SRAM and returning final results to the slower HBM, the algorithm significantly reduces computations associated with (B, L, D, N), where B, L, D, and N represent the batch size, sequence length, number of channels, and state dimension respectively, thereby enhancing computational speed."}, {"title": "III. METHODOLOGY", "content": "A. Architecture Overview\nThe overall structure of the proposed Mamba-SEUNet is illustrated in Fig. 1(a). Given the noisy speech input y, we first apply the Short-Time Fourier Transform (STFT) to obtain its magnitude spectrum $Y_m \\in R^{T \\times F}$ and phase spectrum $Y_p \\in R^{T \\times F}$. These are then combined and transformed into an intermediate feature space $Y \\in R^{T \\times F \\times C_1}$ via a feature encoder. Subsequently, these patch features are processed through a series of TS-Mamba blocks with skip connections, patch embedding layers, and downsampling and upsampling operations for hierarchical processing at different resolutions. The patch embedding layers, come from [17], leverage a combination of depthwise separable and deformable convolutions, enabling the capture of fine-grained acoustic details and enriching the subsequent TS-Mamba blocks with enhanced feature representations. The TS-Mamba blocks focus on learning these representations and capturing time and frequency dependencies. Finally, the enhanced magnitude and phase spectra are decoded and reconstructed into the enhanced speech signal using the inverse STFT (ISTFT)."}, {"title": "B. TS-Mamba Block", "content": "We employ time and frequency Mamba blocks sequentially to learn the comprehensive feature representations, as illustrated in Fig. 1(b). To effectively capture both global and local information, each Mamba block incorporates the bidirectional SSM formulation proposed in [26], allowing the model to integrate both past and future information. This is similar to the forward and backward scanning mechanisms utilized in BLSTM [27]. Specifically, for a given input x, it is processed in parallel through both the forward Mamba and backward Mamba. Post-processing with RMSNorm [28] is applied to both the forward and backward outputs, after which they are concatenated and passed through a linear layer to obtain the output x':\n$x_f = RMS(FMamba(x)) + x$\n$x_b = RMS(BMamba(Flip(x))) + x$\n$x' = Linear(Concat(x_f,x_b))$\n(5)\nwhere $x_f$, $x_b$, RMS ,$FMamba$, $BMamba$, Flip, and Concat denote the forward output, backward output, RMS normalization, forward Mamba, backward Mamba, flip, and concatenation operations respectively.\nThe forward and backward Mamba blocks share an identical structure. Specifically, for an input sequence $X_{in} \\in R^{L \\times C}$, we first apply a linear layer to map it to $t \\in R^{L \\times 2C}$. This is followed by a convolution operation with a kernel size of 4, coupled with a Sigmoid Linear Unit (SiLU) activation function. The output on one side $x_1 \\in R^{L \\times 2C}$ is then obtained using the SSM described in Eq.4. To compensate for any information loss due to the sequential constraints of the SSM, we add a symmetric gated branch without convolution and SSM, consisting of an additional linear layer and SiLU activation. Finally, the outputs from both branches are concatenated and projected through the last linear layer to obtain the output $X_{out} \\in R^{L \\times C}$, as expressed in the following equation:\n$x_1 = SSM(\\delta(Conv(Linear(x_{in}))))$\n$x_2 = \\delta(Linear(x_{in}))$\n$X_{out} = Linear(Concat(x_1,x_2))$\n(6)\nwhere Conv denotes the 1-D convolution operation, $\\delta$ is the SiLU activation function, and Concat represents the concatenation operation."}, {"title": "C. Encoder and Decoder", "content": "The encoder-decoder framework of Mamba-SEUNet is inspired by the methodology employed in MP-SENet [16]. Its feature encoder consists of two convolutional layers and a Dilated DenseNet [29]. The first convolutional layer increases the input channels to $C_1$, producing an intermediate feature map, while the second halves the frequency dimension to optimize computational efficiency. In this study, the Dilated DenseNet is designed with a depth of 4 and a dilation factor of 2 to capture fundamental spectral features of speech. Both the magnitude and phase decoders incorporate the Dilated DenseNet structure from the encoder, followed by a 2-D transposed convolution and a 1 \u00d7 1 convolution. The primary distinction between the two is the activation function: the magnitude decoder utilizes a learnable sigmoid function (L-Sigmoid) [30], whereas the phase decoder employs a two-argument arctangent function (Arctan2) [16]."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets\nWe employ the extensively adopted VCTK+DEMAND dataset [25] to evaluate the effectiveness of the proposed method. The clean speech data comes from the VoiceBank corpus [32], while the mixed noise is sourced from the DEMAND dataset [33]. The generated mixed speech dataset includes 12,396 utterances from 30 speakers, with 28 speakers used for training and 2 for testing. The training and testing sets include 10 types of noise with signal-to-noise ratios (SNRs) ranging from 0 to 15 dB and 5 types of noise with SNRs from 2.5 to 17.5 dB. All samples are downsampled to 16 kHz.\nB. Experimental setup\nDuring training, the speech data is uniformly segmented into 30600 points. The FFT length is set to 510, with a window length of 510 and a hop size of 120. The channel numbers $C_2$ and $C_3$ are set to $\\frac{C_1}{2}$ and $\\frac{C_1}{4}$, respestively. All models are trained for 200 epochs using the AdamW optimization [34], with an initial learning rate of 0.0005, decaying by a factor of 0.99 per epoch. Table I provides the hyperparameters for the four models.\nC. Evaluation metrics\nThe efficacy of the proposed method is evaluated using various metrics. (1) Wide-band PESQ metric evaluates speech quality on a score scale from -0.5 to 4.5 [35]. (2) Speech intelligibility is measured by STOI [36], with a score range from 0 to 1. (3) Mean opinion score (MOS) ratings include CSIG for signal distortion, CBAK for background noise interference, and COVL for overall speech quality, all rated from 1 to 5. (4) FLOPs are calculated based on processing a 2-second, 16kHz audio sample on a GPU."}, {"title": "V. RESULTS AND ANALYSIS", "content": "A. Performance Comparison with Baselines\nTable II presents a performance comparison between the proposed Mamba-SEUNet and several baseline models on the VCTK+DEMAND dataset. This comparison includes classical models such as MetricGAN+ and TSTNN, as well as state-of-the-art (SOTA) models like MP-SENet and SEMamba. The results indicate that Mamba-SEUNet (XS) achieves performance comparable to MP-SENet with just 0.99M parameters and 4.16G FLOPs. As the number of TS-Mamba blocks N increases to 4, Mamba-SEUNet (S) surpasses all existing models with 1.88M parameters and 4.62G FLOPs. By increasing the number of channels from 16 to 24, an improvement across all evaluation metrics is observed. Expanding the output dimension of the feature encoder to 32 further enhances the PESQ score of Mamba-SEUNet (L) to 3.59."}, {"title": "B. Ablation Study", "content": "The experiments in Table IV investigate the impact of varying the number of TS-Mamba blocks N within Mamba-SEUNet (M) on SE performance. As N increases from 1 to 3, there is a consistent improvement in objective performance metrics, including PESQ, STOI, and three MOS scores, demonstrating the efficacy of adding TS-Mamba blocks in enhancing speech quality. However, with a further increase in N to 4, while some metrics such as PESQ and CBAK exhibit slight improvements, others remain unchanged, indicating a potential plateau in performance. Moreover, as N increases, the number of model parameters also rises, highlighting the trade-off between performance gains and model complexity."}, {"title": "C. Mamba/Transformer/Conformer Block", "content": "To ensure a fair comparison of Mamba with conformer and transformer, we employ the proposed U-Net architecture, replacing TS-Mamba with TS-Conformer as introduced in [15] and TS-Transformer as described in [31], as shown in Table V. The results indicate that Mamba outperforms conformer, achieving better performance with a slight increase in model parameters and a significant reduction in FLOPs, with a 0.12 improvement in the PESQ score. Compared to the transformer, which replaces the first linear layer in the feedforward network [38] with gated recurrent units (GRU) [39] to learn positional information, Mamba exhibits better performance, with a 0.05 improvement in the PESQ score, while requiring fewer parameters and lower FLOPs. These findings underscore the effectiveness of Mamba in SE tasks."}, {"title": "VI. CONCLUSIONS", "content": "In this study, we introduce Mamba-SEUNet, a U-Net style SE network based on TS-Mamba blocks. This architecture leverages bidirectional Mamba blocks to effectively capture both past and future information, addressing the limitations of existing transformer-based methods. By integrating TS-Mamba blocks into the U-Net framework, Mamba-SEUNet enhances multi-scale information capture while maintaining computational efficiency. Experimental results on the VCTK+DEMAND dataset demonstrate that Mamba-SEUNet achieves state-of-the-art (SOTA) SE performance with low computational complexity. In the future, we aim to extend this network to other SE tasks and further optimize its performance on more challenging datasets."}]}