{"title": "Proximity Matters: Local Proximity Preserved Balancing for Treatment Effect Estimation", "authors": ["Hao Wang", "Zhichao Chen", "Yuan Shen", "Jiajun Fan", "Zhaoran Liu", "Degui Yang", "Xinggao Liu", "Haoxuan Li"], "abstract": "Heterogeneous treatment effect (HTE) estimation from observational data poses significant challenges due to treatment selection bias. Existing methods address this bias by minimizing distribution discrepancies between treatment groups in latent space, focusing on global alignment. However, the fruitful aspect of local proximity, where similar units exhibit similar outcomes, is often overlooked. In this study, we propose Proximity-aware Counterfactual Regression (PCR) to exploit proximity for representation balancing within the HTE estimation context. Specifically, we introduce a local proximity preservation regularizer based on optimal transport to depict the local proximity in discrepancy calculation. Furthermore, to overcome the curse of dimensionality that renders the estimation of discrepancy ineffective\u2014exacerbated by limited data availability for HTE estimation\u2014we develop an informative subspace projector, which trades off minimal distance precision for improved sample complexity. Extensive experiments demonstrate that PCR accurately matches units across different treatment groups, effectively mitigates treatment selection bias, and significantly outperforms competitors. Code is available at https://anonymous.4open.science/status/ncr-B697.", "sections": [{"title": "1 Introduction", "content": "Estimating HTE through randomized controlled trials is fundamental in causal inference, widely applied in various domains such as healthcare [48], e-commerce [1, 30, 62], and education [8]. While randomized controlled trials are considered the gold standard in HTE estimation [26, 41], their practical deployment is often limited by significant financial and ethical constraints [25, 60]. Consequently, there is increasing reliance on observational data to estimate HTE, prompted by the broader accessibility of such data and the feasibility of conducting post-marketing surveillance instead of costly clinical trials [28, 61, 66].\nHTE estimation from observational data presents significant challenges primarily due to: (1) the absence of counterfactuals, where only one out of all potential outcomes is observable, and (2) the presence of treatment selection bias, where non-random treatment assignments lead to a covariate shift between treated and untreated groups, affecting the generalizability of outcome estimators across the population [59, 60]. Traditional meta-learners address the missing counterfactuals by segmenting the HTE estimation into more manageable tasks focused on factual outcomes [22]. Nevertheless, these approaches often struggle with the covariate shift, resulting in biased HTE estimations.\nRecent methods such as counterfactual regression have demonstrated promise by mitigating selection bias through minimizing distribution discrepancies in the representation space [6, 17, 49, 67, 68]."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Heterogeneous treatment effect estimation with observational data", "content": "This section outlines the HTE estimation task within the potential outcome framework [47] and the challenge of treatment selection bias. The fundamental encapsulated in Definition 2.12. Specifically, a unit characterized by covariates x possesses two potential outcomes: $Y_1$ if treated and $Y_0$ if untreated. The expected difference between these potential outcomes given covariates, represented as $r(x) = E[Y_1 \u2013 Y_0 | x]$, is termed as the conditional average treatment effect (CATE), and its expectation over all units is termed as the average treatment effect (ATE).\nDefinition 2.1. Suppose $X, R, Y$, and $T$ are random variables with probability density function p and support $S^*$. Typically, $X$ represents covariates with the probability density function $p_X$ and support $S_X = \\{0,1\\}$. $R$ represents induced representations, $Y$ denotes outcomes, and $T$ denotes treatment indicators.\nDefinition 2.2. Suppose $\\psi$: $S_X$ \u2192 $S_R$ is a representation mapping with $R$ = $\\psi(X)$. Define $\\phi_T$: $R$\u00d7$T$ \u2192 $Y$ as an outcome mapping that maps the representations and treatment to the corresponding factual outcome: $Y_1$ = $\\phi_1(R)$ and $Y_0$ = $\\phi_0(R)$."}, {"title": "2.2 Local proximity for HTE estimation", "content": "Local proximity, quantified as the mutual distance between units within a distribution, captures the geometric properties of distributions. The assumption that similar units have similar outcomes [67, 68] highlights the importance of local proximity for HTE estimation. This is evidenced by the success of HTE estimators based on matching (KNN [10], PSM [46]) and stratification [55].\nDespite the recognized significance of local proximity, state-of-the-art representation-based estima-tors, predominantly following the CFR [49, 57] paradigm, primarily focus on global alignment by minimizing global discrepancy Disc(\u00b7). These methods often neglects local proximity [50], which is crucial for accurate HTE estimation. A notable exception is the SITE model [67], which employs the PDDM and MPDM metrics [18] to depict local proximity and global discrepancy, respectively. However, both metrics are computed using only six selected anchor units [68], ignoring the role of other units and resulting in suboptimal performance due to insufficient granularity of the analysis.\nOur work builds upon the idea of exploiting local proximity in SITE, and proposes a more compre-hensive approach that moves beyond specific anchor units, to model both local proximity and global discrepancy in a unified framework, leveraging the optimal transport methodology."}, {"title": "2.3 Discrete optimal transport and Sinkhorn divergence", "content": "Optimal transport (OT) quantifies distribution discrepancy as the minimum transport cost, offering a tool to quantify the treatment selection bias in Figure 1(a). An applicable formulation proposed by Kantorovich [20] is present in Definition 2.4, which can be seen as a linear programming problem.\nDefinition 2.4. For empirical distributions \u03b1 and \u03b2 with n and m units, respectively, the Kantorovich problem aims to find a feasible plan $\\pi$\u2208 $R^{n\u00d7m}$ which transports \u03b1 to \u03b2 at the minimum cost:\n$W(\\alpha, \\beta) := \\min_{\\pi \\in \\Pi(\\alpha,\\beta)} <D, \\pi>, \\Pi(\\alpha, \\beta) := \\{\\pi \\in R^{n \\times m} : \\pi 1_m = a, \\pi^\\top 1_n = b\\},$ (4)\nwhere $W(\\alpha, \\beta)$ \u2208 R is the OT discrepancy between \u03b1 and \u03b2; $D$ \u2208 $R^{n\u00d7m}$ is the unit-wise distance between \u03b1 and \u03b2, which is implemented with the Euclidean distance; a and b are the mass of units in \u03b1 and \u03b2, and \u03a0 is the feasible plan set where the mass-preserving constraint holds."}, {"title": "3 Proposed method", "content": "In this section, we present the Proximity-aware Counterfactual Regression (PCR) approach, which leverages optimal transport to tackle the treatment selection bias. We first illustrate local proximity preservation regularizer (LPR) based on optimal transport framework for measuring and maintaining local similarities in different treatment groups, and demonstrate its efficacy for improving HTE estimation. Subsequently, we propose an informative subspace projector (ISP) to reduce the sampling complexity and handle the curse of dimensionality. We finally open a new thread to summarize the model architecture, learning objectives, and optimization algorithm."}, {"title": "3.1 Local proximity preservation regularizer for counterfactual regression", "content": "To mitigate treatment selection bias, representation-based methods align treated and untreated groups in the representation space, the core of which is the quantification of the distribution discrepancy Disc(\u00b7) between treatment groups. It is plausible to quantify the discrepancy with OT due to its numerical advantages and flexibility over competitors [57]. However, standard OT fails to preserve local proximity, a crucial aspect in HTE estimation. The treated and untreated units with similar neighbors for instance should have a higher probability of matching together since similar units have similar outcomes [46, 55]. An extension of OT that encodes local proximity is the Gromov-Wasserstein measure, primarily applied to matching objects with geometric structures [37, 63]. Resonating with [53], the LPR fuses the Gromov Wasserstein measure and restates the transport problem in the representation space as:\n$F(R^{T=0}, R^{T=1}):= \\min_{\\pi \\in \\Pi(\\alpha,\\beta)} \\kappa \\cdot (\\pi, D) + (1-\\kappa) \\cdot \\sum_{i,j,k,l} P_{i,j,k,l}\\pi_{i,j}\\pi_{k,l}$ (5)\nThe above optimization problem consists of two components with relative strengths controlled by $\\kappa$. The first term, adhering to the standard OT for-mulation in (4), measures the global discrepancy with $D_{i,j} = ||R_i - R_j ||_2$. The second term de-picts local proximity within each treatment group as $D_{i,k}^{T=t} = ||R_i^{T=t}-R_k^{T=t}||_2^2$, and preserve such local proximity via $P_{i,j,k,l} = ||D_{i,k}^{T=0}-D_{j,l}^{T=1}||_2^2$. Specif-ically, if the distance between $R_i^{T=0}$ and $R_k^{T=0}$ is close to that between $R_j^{T=1}$ and $R_l^{T=1}$ (i.e., $||D_{i,k}^{T=0}-D_{j,l}^{T=1}||_2^2 \u2192 0$), a higher volume of mass will be matched, indicated by a larger $\\pi_{i,j}\u03c0_{k,l}$. Conversely, if there is a significant disparity, less mass will be transported. The derived transport plan en-courages matching units with similar neighbors, pre-serving local proximity. Therefore, F quantifies the discrepancy between treatment groups while accom-modating the local similarities."}, {"title": "3.2 Informative subspace projector for the curse of dimensionality", "content": "The curse of dimensionality refers to the phenomenon where the Euclidean distance between data points tend to be identical [15]. It renders Euclidean distance difficult to model the proximity in high dimensional settings due to diminished discrimination.\nFrom a computational view, the diminishing discrimination necessitates more samples for estimating Euclidean-based discrepancy [36], which is a pivotal component in state-of-the-art HTE estimators such as MMD in [49], PDDM in [67], and EMD in [57]. A well-known result states, for instance, that the sample complexity of EMD can grow exponentially with dimension [7]. Similarly, the sample complexity of F reaches $O(N^{-2})$, forming the complexity term in Theorem 3.1. Such large sample complexity necessitates many treated and untreated units to faithfully estimate the true discrepancy. However, a large number of treated units is often difficult to acquire in real-world experiments, underscoring the adverse impact of the curse of dimensionality on HTE estimation.\nTo counteract the curse of dimensionality, reducing the computational space dimension is a common approach. Consider using a projector $U \u2208 R^{d\u00d7k}$ to transform the data from a high-dimensional space d to a lower-dimensional subspace k. The distance between two unit representations $R_1$ and $R_2$ in this reduced space is computed as $||R_1U \u2013 R_2U||$. This approach alleviates the curse of dimensionality but introduces the risk of losing significant information, potentially leading to overly optimistic discrepancy estimations.\nTo address the potential pitfalls of naive dimension reduction, it is crucial to choose a projection that maximizes the relevant discrepancy, mitigating the bias induced by dimension reduction. The projection robust optimization formulation is proposed as follows:\n$\\mathcal{I}_F^{(PR)}(R^{T=0}, R^{T=1}) := \\min_{\\pi \\in \\Pi(R^{T=0},R^{T=1})} \\max_{U,U^\\top=I} \\kappa \\cdot (\\pi, D^U) + (1-\\kappa) \\cdot \\sum_{i,j,k,l} P_{i,j,k,l}^U\\pi_{i,j}\\pi_{k,l},$ (7)\nwhere $D_{i,j}^U = ||R_iU-R_jU||_2$ is the distance in the reduced k-dimensional space, $P_{i,j,k,l}^U = ||D_{i,k}^{UT=0}-D_{j,l}^{UT=1}||_2^2$. However, this optimization problem proves difficult to solve [36]. An effective compromise involves projecting the data into a k-dimensional subspace through an ISP module that maximally preserves the information, and then compute discrepancy in the subspace. Built upon this idea, the transport problem modified with the ISP module is formulated in Definition 3.1.\nDefinition 3.1. Suppose $U^* \u2208 R^{d\u00d7k}$ an informative subspace projector, such that:\n$U^* = arg \\min_{U,U^\\top=I} \\|R - RUU^\\top\\|_2,$ (8)\nwhere $P = k/d$ denotes the ratio of dimensionality reduction. The distribution discrepancy equipped with LPR and ISP modules is formulated as\n$\\mathcal{I}_F^{(PCR)}(R^{T=0}, R^{T=1}) := \\min_{\\pi \\in \\Pi(R^{T=0},R^{T=1})} \\kappa \\cdot (\\pi, D^{U^*}) + (1-\\kappa) \\cdot \\sum_{i,j,k,l} P_{i,j,k,l}^{U^*}\\pi_{i,j}\\pi_{k,l}$ (9)"}, {"title": "3.3 Workflow of proximity-aware counterfactual regression", "content": "The architecture of PCR is presented in Figure 1(b), where the covariate X is first mapped to the representations R with \u03c8(\u00b7), and then to the potential outcomes with \u03c6(\u00b7). The learning objective is to minimize the risk of factual outcome estimation and the group discrepancy. Given mini-batch distributions $X^{T=1}$ and $X^{T=0}$, the risk of factual outcome estimation can be estimated as (2). Afterwards, the group discrepancy is calculated as $Disc_{\\kappa,P}(\\psi) = \\mathcal{I}_F^{(PCR)}(R^{T=0}, R^{T=1})$. Finally, the overall learning objective of PCR is\n$L^{(PCR)} := L^{(F)} (\\psi, \\phi) + \\lambda Disc_{\\kappa,P}(\\psi),$ (10)\nwhere \u03bb controls the strength of distribution alignment, \u03ba controls LPR in (7), and P controls the ratio of dimension reduction in Definition 3.1. The learning objective above mitigates the selection bias following Theorem 3.1 while handling the issues of local proximity and curse of dimensionality.\nThe optimization procedure of PCR consists of three steps. First, compute $U^*$ by solving the dimension reduction problem in (8), which can be solved via the well-established principal component analysis algorithm. Second, compute $\\mathcal{I}_F^{(PCR)}$ by solving the OT problem in Definition 3.1 with Algorithm 2, where the projection matrix is $U^*$. Finally, calculate the overall loss in (10) and update \u03c8 and \u03c6 with stochastic gradient methods."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental setup", "content": "Datasets. The evaluation of PEHE is challenged by the absence of counterfactuals in observational data. To address this, experiments are conducted using two semi-synthetic benchmarks [49, 67], namely the Infant Health and Development Program (IHDP) and the Atlantic Causal Inference Conference (ACIC) competition data. The IHDP dataset is constructed to evaluate the effect of specialist home visits on infants' cognitive development, containing 747 observations with 25 covariates. The ACIC data, derived from the collaborative perinatal project [40], comprises 4802 observations and 58 covariates.\nBaselines. The involved baseline methods can be categorized into three groups:\n\u2022 Statistical estimators: random forest with treatment as a covariate (R.Forest), a single neural network model treating the treatment indicator as a covariate (S.learner [22]), separate neural regressors for each treatment group (T.learner [22]), and TARNet [49].\n\u2022 Matching estimators: propensity score matching with logistic regression (PSM [46]), k-nearest neighbor matching (k-NN [10]), and orthogonal forest (O.Forest [55]).\n\u2022 Representation-based estimators: counterfactual regression with Maximum Mean Discrepancy (CFR-MMD), Wasserstein distance (CFR-WASS) [49], and generalized Wasserstein discrepancy (ESCFR) [57]; similarity-preserved individual treatment estimator (SITE) and the recent deep entire space cross network (DESCN) [74].\nTraining protocol. PCR is implemented with a fully connected neural network architecture com-prising two hidden layers with 16-16 and 32-32 nodes, respectively. It is trained using the Adam optimizer for a maximum of 400 epochs, with an early stopping patience set to 30 epochs. The learning rate and weight decay parameters are set at 1e-3 and 1e-4, respectively. Other optimization settings follow Kingma and Ba [21]. Hyper-parameters are tuned within the ranges in Section 4.4, with model performance validation conducted every epoch.\nEvaluation protocol. We select \u0420\u0415\u041d\u0415 (\u0454PEHE), absolute estimation error of ATE (\u0454ATE) and ATT (\u0454ATT) for evaluation. While PEHE is the primary metric for assessing performance [49, 67], it cannot be directly utilized during the model selection phase due to the absence of counterfactual outcomes. Instead, the area under the uplift curve (AUUC) [1] is employed to guide model selection. AUUC assesses the ranking performance of the HTE estimator and can be computed without access to counterfactuals. Although not a standard measure in HTE estimation, AUUC provides valuable insights as an auxiliary metric. Results are reported for both within-sample (training) and out-of-sample (test) datasets, consistent with common practices in the field [33, 34, 49, 67]."}, {"title": "4.2 Overall performance", "content": "Table 1 provides a comprehensive comparison of the PCR framework with various baseline method-ologies. Key observations from this comparative analysis are outlined below:\n\u2022 Statistical Estimators: This group exhibits strong performance on the PEHE metric. Neural network-based estimators particularly excel, surpassing linear models and random forests due to their enhanced ability to capture nonlinear relationships. Among them, TARNet, which integrates the strengths of both T-learner and S-learner, is noted for achieving the best overall performance within this group. However, its limitations in addressing treatment selection bias lead to its underperformance in certain scenarios.\n\u2022 Matching Methods: Techniques such as PSM and O.Forest show robust capabilities in estimating average treatment effects, which contributes to their widespread adoption in policy evaluation contexts. However, their efficacy diminishes on the PEHE metric, restricting their suitability for applications requiring personalized treatment approaches, such as healthcare and advertising.\n\u2022 Representation-Based Methods: These methods are particularly effective in mitigating treatment selection bias, thereby enhancing overall estimation accuracy. The SITE model, which emphasizes the preservation of local similarities, consistently outperforms the conventional TARNet approach. Furthermore, ESCFR, which achieves the best performance among the baselines with an out-of-sample PEHE of 2.678 on the ACIC dataset, utilizes an unbalanced Wasserstein discrepancy. However, its failure to handle the local proximity and the curse of dimensionality limits its effectiveness in fully overcoming treatment selection bias.\n\u2022 PCR: PCR surpasses other prevalent baselines with significant improvements across most metrics. This superiority is attributed to the innovative LPR and ISP modules. These components enable PCR to adeptly handle both local proximity preservation and dimensionality challenges, which facilitates more accurate alignment of treatment groups and thereby handling of selection bias."}, {"title": "4.3 Ablation study", "content": "In Table 2, we examine the contributions of individual components of PCR on the ACIC benchmark. Our study builds upon the CFR-Wass model [49], a canonical approach that employs the Wasserstein"}, {"title": "4.4 Analysis of the local proximity preservation regularizer", "content": "In this section, we analyze the efficacy of the proposed LPR component. There are two main hyperparameters in LPR: \u03bb that controls the strength of distribution alignment and \u03ba that controls the strength of LPR in (7). A sensitivity study for them is conducted in Figure 3, where the estimation error exhibits an reduction followed by an increase, with the increasing of both parameters. It reveals that both distribution alignment and LPR are effective to improve HTE estimation. However, overly emphasizing distribution balancing or local proximity preservation within a multi-task learning framework can result in compromised factual outcome estimation and, consequently, suboptimal treatment effect estimates."}, {"title": "4.5 Analysis of the informative subspace projector", "content": "In this section, we delve into the role of the ISP component. The characteristics of ISP are governed by the hyperparameter P, which dictates the extent of dimensionality reduction. We conduct a hyperparameter study for P in Figure 4, and the key observations are summarized below.\nDimensionality reduction effectively enhances model performance. Specifically, as P is decreased from 1 to 0.7, there is a notable improvement in the estimation accuracy, with the out-of-sample EATE diminishing from approximately 0.8 to about 0.5. This improvement is primarily due to effective handling of the curse of dimensionality, which in turn facilitates a more accurate estimation of discrepancies using minibatch samples. However, excessive reduction in dimensionality can lead to substantial information loss, and thereby suboptimal estimates.\nInterestingly, there is a relationship between the weighting parameter \u03ba and the optimal setting for P. As \u03ba increases, which shifts the discrepancy measure $\\mathcal{I}_F$ closer to the Gromov-Wasserstein discrepancy, the curse of dimensionality becomes more pronounced: the Gromov term relies heavily on unit-wise distances to compute local similarities, making dimensionality reduction increasingly"}, {"title": "5 Related works", "content": "Current research in causal inference focuses on alleviating treatment selection bias by balancing distributions between treated and untreated groups. This can be achieved through three main method-ological categories: reweighting-based, matching-based, and representation-based methods [32].\nReweighting-based methods primarily utilize propensity scores to achieve global balance between groups, involving the estimation of propensity scores and the construction of unbiased estimators [24, 32, 60]. Propensity scores are typically estimated using logistic regression [5, 12, 23, 70], with precision enhancements via feature selection [51, 58, 59], joint optimization [31, 70, 71], and alternative training techniques [75]. The inverse propensity score method exemplifies the unbiased estimator [45], although it suffers from high variance at low propensity scores and bias with incorrect estimates [25, 27, 56]. To mitigate these issues, doubly robust estimators and variance reduction techniques have been developed [16, 29, 44]. However, the reliance on propensity scores limits the practical application of these methods.\nMatching-based methods construct locally balanced distributions by matching comparable units from different groups, based on various similarity measures. Propensity score matching, which uses estimated propensity scores for calculating unit (dis)similarity, is one such technique [46]. Tree-based methods, employing adaptive similarity measures, are also considered under this category [55]. Despite their effectiveness, the computational intensity of these methods restricts their scalability in large-scale applications [3, 35, 60].\nRepresentation-based methods involve constructing a mapping to a feature space where distributional discrepancies are minimized, with initial approaches focusing on maximum mean discrepancy and vanilla Wasserstein discrepancy [19, 49]. Enhancements have been made by integrating local proximity preservation [67, 68], feature selection [6, 17], representation decomposition [17, 61], and adversarial training [69]. However, challenges such as outlier fluctuations [14] and unlabeled confounders [73] still undermine the reliability of these methods.\nThe recent advances of optimal transport (OT) in causality [52] has spawned innovative HTE estimators through sample reweighting [13, 64, 65] and matching [4]. A typically related line of works advocates building balanced representations with OT towhich significantly enhances representation-based HTE estimators [33, 34, 57]. Li et al. [34] for instance use OT to align factual and counterfactual distributions; Wang et al. [57] apply OT to achieve HTE estimation while minigating noise and unobserved confounding effects. Despite this progress, these approaches generally adhere to the traditional Kantorovich problem, similar to [49], focusing on global alignment while often neglecting both local proximity and the curse of dimensionality. Therefore, developing OT formulations to meet the unique property of HTE estimation remains a fruitful avenue for future research."}, {"title": "6 Conclusion", "content": "Representation learning has emerged as a pivotal approach for estimating individual treatment effects due to its efficacy in mitigating treatment selection bias. Despite the successes, current methods often overlook crucial aspects such as local proximity and the curse of dimensionality, which are essential for adequately addressing treatment selection bias. To bridge this gap, a principled approach known as PCR, based on a generalized OT problem, has been developed. Extensive experiments validate that PCR handles both problems effectively and outperforms prevalent baseline models.\nLooking forward, two promising research avenues are identified. The first involves the integration of normalizing flows for representation mapping, prized for their invertibility, a property that is consistent with the foundational assumptions of counterfactual regression as posited by Shalit et al. [49]. The second avenue focuses on the practical application of our methodology to industrial contexts, specifically for bias mitigation in recommendation systems [56]."}, {"title": "A Heterogeneous treatment effect estimation with observational data", "content": "In this section, we introduce essential preliminaries concerning HTE estimation, aimed at readers new to this field. We subsequently elucidate our theoretical contributions based on these foundational concepts."}, {"title": "A.1 Problem formulation", "content": "Here, we formalize the definitions, assumptions, and pertinent lemmas in the domain of HTE estimation from observational data. Building on the notations introduced in Section 2.1, consider an individual with covariates x exhibiting two potential outcomes: $Y_1(x)$ if treated and $Y_0(x)$ otherwise. The core of causal inference, the individual treatment effect (CATE), is defined as the difference between these outcomes.\nDefinition A.1. The individual treatment effect (CATE) for a unit with covariates x is defined as:\n$\\tau(x) := E [Y_1 - Y_0 | x],$ (11)\nwhere $Y_1(x)$ and $Y_0(x)$ are abbreviated to $Y_1$ and $Y_0$, respectively, for brevity, and the expectation is computed over the potential outcome space $\\mathcal{Y}$.\nEstimating CATE using observational data presents notable challenges:\n\u2022 Missing counterfactuals: Only the factual outcome is observable; e.g., if a patient is treated, the outcome had they not been treated remains unknown.\n\u2022 Treatment selection bias: Individuals might not be randomly assigned to treatments, leading to non-comparability between treated and untreated groups. For instance, doctors may choose different treatments based on a patient's health status, making the two groups inherently heterogeneous.\nPearl and Mackenzie [41] propose a two-step methodology to address these issues: identification and estimation. The identification step involves constructing an unbiased statistical estimand to identify the causal estimand (e.g., $\\tau(x)$), contingent on certain assumptions being met.\nAssumption A.1. (Unconfoundedness) For all covariates x in the population of interest, potential outcomes $(Y_0, Y_1)$ are conditionally independent of the treatment assignment given $X = x$.\nAssumption A.2. (Consistency) The observed outcome Y is consistent with the potential outcome corresponding to the assigned treatment, across all covariates x.\nAssumption A.3. (Positivity) Each individual has a nonzero probability of receiving each treatment, formally, 0 < p(T = 1 | X = x) < 1 for all x.\nAssumption A.4. (SUTVA) The potential outcomes for any individual are unaffected by the treatment assignments of others, and there are no varying versions of the treatment that could result in different outcomes.\nThe estimation step seeks to estimate the unbiased estimand using observational data. Lemma A.1 demonstrates how these assumptions facilitate CATE estimation.\nLemma A.1. The CATE $\\tau(x)$ can be identified by:\n$E [Y_1 \u2013 Y_0 | X = x] = E [Y_1 | X = x, T = 1] \u2013 E [Y_0 | X = x,T = 0]$\n$= E [Y | X = x, T = 1] \u2013 E [Y | X = x,T = 0],$ (12)\nleveraging the unconfoundedness and consistency assumptions. This statistical framework depends critically on the positivity assumption; violating it renders certain estimations infeasible, as detailed in the ensuing equations."}, {"title": "A.2 Meta-learners for CATE estimation with observational data", "content": "To address the issue of missing counterfactuals, existing meta-learner based methods decompose the challenge of CATE estimation into manageable subproblems, each solvable using standard supervised learning techniques [22, 39]. These methods capitalize on the structure of the data and the implicit relationships between treatment effects and covariates."}, {"title": "A.3 Representation-based methods for treatment selection bias", "content": "Treatment selection bias presents significant challenges in causal inference by causing covariate distributions to differ markedly between treated and untreated groups. This discrepancy can lead to models that overfit to characteristics specific to one group and fail to generalize across the population. For instance, a potential outcome estimator $\\phi_1$ trained exclusively on treated units may perform poorly when applied to untreated units, leading to biased estimates of \u03c4, as illustrated in Figure 1(a)."}, {"title": "A.4 Theoretical results", "content": "While Theorem A.1 provides a foundational framework for addressing biases in treatment effect estimation using representation-based methods, two significant challenges require further exploration.\nTractability of the IPM Metric: Despite the profound theoretical properties of the Integral Proba-bility Metric (IPM), its direct computation can be intractable for complex distributions. However, within certain function families $\\mathcal{F}$, such as the family of 1-Lipschitz functions, the IPM can be equivalently expressed as the Wasserstein distance through the Kantorovich-Rubinstein duality, a concept extensively documented in the literature [49, 54]. This equivalence allows for practical computation of the IPM as the Wasserstein discrepancy, detailed in the following lemma:\nLemma A.2. Given two distribution functions $p_1(x)$ and $p_2(x)$ supported over $\\mathcal{X}$, and letting $\\mathcal{F}$ be the family of 1-Lipschitz functions, the IPM induced by $\\mathcal{F}$ is equivalent to the Wasserstein distance $\\mathcal{W}$, as demonstrated by Villani [54]:\n$IPM_{\\mathcal{F}} (p_1, p_2) = \\mathcal{W} (p_1, p_2).$ (20)\nSampling Complexity of Discrepancy Measures: Theorem A.1 assumes access to the entire populations of treated and untreated groups to calculate the distribution discrepancy. However, in practical machine learning scenarios, especially in deep learning, parameters are often updated using stochastic gradient methods on mini-batches of data rather than the full dataset. This raises questions about the validity of Theorem A.1 when applied at the mini-batch level.\nRecent studies have explored the sample complexity of various discrepancy measures such as the Wasserstein distance and Gromov discrepancy, providing insights into their behavior with limited sample sizes [7, 72], as encapsulated in Lemma A.3 and A.4. Building on these insights, we innova-tively propose Theorem A.2, which extends Theorem A.1 to a specific Fused Gromov-Wasserstein (FGW) discrepancy employed in this work, examining its sample complexity when only a small minibatch sample is available."}, {"title": "B Discrete Optimal Transport", "content": "This section introduces the foundational concepts and algorithms necessary for computing optimal transport between discrete measures. We focus solely on discrete measures, as the general measures case extends beyond the scope of this paper [38]. For further exploration of this area, the reader is referred to seminal works in the literature [11, 43]."}, {"title": "B.1 Notations and problem formulation", "content": "We consider a scenario involving n warehouses and m factories, where the i-th warehouse holds ai units of material and the j-th factory requires b; units of material [43]. The objective is to establish a mapping from warehouses to factories that: (1) completely allocates all warehouse materials, (2) fulfills all factory demands, and (3) restricts the transportation from each warehouse to at most one factory. Each potential mapping is evaluated based on a global cost, which aggregates the local costs incurred from transporting a unit of material from warehouse i to factory j.\nDefinition B.1. The Monge problem for discrete measures, where $\\alpha = \\sum_{i=1}^{n} a_i \\delta_{x_i}$ and $\\beta = \\sum_{j=1}^{m} b_j \\delta_{x_j}$, seeks a mapping $T : \\{x_i\\}_{i=1}^{n} \\rightarrow \\{x_j\\}_{j=1}^{m}$ that optimally redistributes the mass from $\\alpha$ to $\\beta$. Specifically, for each j, it must hold that $b_j = \\sum_{i:T(x_i)=x_j} a_i$ and $T#\\alpha = \\beta$. The goal is to minimize the transportation cost, represented by c(x, y), leading to the following formulation:\n$\\min_{T} \\sum_{i=1}^{n} c(x_i, T(x_i))$ (28)\nThis problem setup also facilitates the comparison of two probability measures where $\\sum_i a_i = \\sum_j b_j = 1$. The original Monge formulation does not guarantee the existence or uniqueness of solutions [43]. Therefore, Kantorovich [20] extended this framework by relaxing the one-to-one mapping constraint, allowing transportation from a single warehouse to multiple factories, and reformulated the problem as a linear programming challenge.\nDefinition B.2. The Kantorovich problem for discrete measures $\\alpha$ and $\\beta$ defines a cost-minimization task over feasible transport plans $\\pi \u2208 R^{n\u00d7m}$. The objective is to find a plan that minimizes the overall transport cost:\n$\\mathcal{W}(\\alpha, \\beta) := \\min_{\\pi \\in \\Pi(\\alpha,\\beta)}  , \\Pi(\\alpha, \\beta) := \\{\\pi \\in R^{n \\times m} : \\pi 1_m = a, \\pi^\\top 1_n = b\\},$ (29)\nwhere $\\mathcal{W}(\\alpha, \\beta)$ represents the Wasserstein discrepancy between \u03b1 and \u03b2; D denotes the distance matrix computed using the squared Euclidean metric [9], and a, b are vectors describing the mass distribution in \u03b1 and \u03b2, respectively."}, {"title": "B.2 Entropy-regularized optimal transport and Sinkhorn algorithm", "content": "Computing exact solutions to the Kantorovich problem is computationally intensive, typically involving methods like the interior-point or network-simplex which exhibit a complexity of $O(n^3logn)$ [42]. A more efficient approach incorporates an entropic regularizer into the optimiza-tion, reducing computational complexity significantly. This regularization transforms```json\nproblem into an \u03f5-convex problem, which is amenable to solution by the Sinkhorn algorithm [11], reducing the complexity to $O(n^2/\u03f5^2)$. This approach is not only computationally advantageous but also conducive to acceleration using GPUs due to its reliance on matrix-vector operations.\n$\\mathcal{W}^\u03f5(\\alpha, \\beta) :=  - \u03f5H(\\pi), \\,\\\\ H(\\pi) := - \\sum_{i,j} \\pi_{ij} (log(\\pi_{ij}) - 1),$ (30)"}, {"title": "B.3 Fused-gromov optimal transport and Frank-wolfe algorithm", "content": "While the Kantorovich problem effectively computes the discrepancy between discrete measures as in (4), it does not consider the local similarities within each measure which include fruitful geometric information. Incorporating local similarities can produce plausible transport strategy. An exemplar approach is the Gromov-Wasserstein measure, which generates transport matrix merely using the local similarities within each distribution. It has been primarily applied to matching objects with geometric structures [37, 63].\nDefinition B.3. The Gromov-Wasserstein measure defines a cost-minimization task as follow\n$G(\\alpha, \\beta) := \\min_{\\pi \\in \\Pi(\\alpha,\\beta)} (\\kappa \\cdot \\sum_{i,j,k,l} P_{i,j,k,l}\u03c0_{i,j}\u03c0_{k,l}),$ (36)\nwhere \u03a0(\u03b1, \u03b2) := {\u03c0\u2208 $R^{n\u00d7m}$ : \u03c01m = a,\u03c0\u00b91n = b} represents the set of feasible transport ma-trices, G(\u03b1, \u03b2) represents the Gromov-Wasserstein discrepancy between \u03b1 and \u03b2; a, b are vec-tors describing the mass distribution in \u03b1 and \u03b2, respectively. Pi,j,k,l = $||D_{i,k}^{T=0}-D_{j,l}^{T=1}||$, where $D^{T=t}$ = $||R^{T=t}\u2212R^{T=t} ||$ denotes the distances between samples within each measure computed using the squared Euclidean metric [9].\nThe Gromov-Wasserstein measure, as stated in Definition B.3, depicts local similarity within each treatment group as Dj = $||R\u2212R ||$, and preserve such local similarity via Pi,j,k,l = $||D^{T=0} \u2212 D^{T=1} ||$. Specifically, if the distance between R and R is close to that between R and R (i.e., $||D^{T=0} \u2212 D^{T=1} ||$ \u2192 0), a higher volume of mass will be matched, indicated by a larger \u03c0,\u03c0. Conversely, if there is a significant disparity, less mass will be transported. The derived transport plan encourages matching units with similar neighbors, preserving local similarity.\nTo further enhance the model by integrating global distribution alignment with local similarity preservation, the Fused Gromov-Wasserstein (FGW) problem was constructed [53], combining the"}]}