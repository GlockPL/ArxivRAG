{"title": "Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding", "authors": ["Konstantin Berestizshevsky", "Renzo Andri", "Lukas Cavigelli"], "abstract": "The attention mechanism is essential for the impressive capabilities of transformer-based Large Language Models (LLMs). However, calculating attention is computationally intensive due to its quadratic dependency on the sequence length. We introduce a novel approach called Top-Theta Attention, or simply Top-0, which selectively prunes less essential attention elements by comparing them against carefully calibrated thresholds. This method greatly improves the efficiency of self-attention matrix multiplication while preserving model accuracy, reducing the number of required V cache rows by 3x during generative decoding and the number of attention elements by 10x during the prefill phase. Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated. Unlike top-k attention, Top- eliminates full-vector dependency, making it suitable for tiling and scale-out and avoiding costly top-k search. A key innovation of our approach is the development of efficient numerical compensation techniques, which help preserve model accuracy even under aggressive pruning of attention scores.", "sections": [{"title": "1. Introduction", "content": "The transformer architecture has revolutionized both natural language processing (Vaswani, 2017) and computer vision (Dosovitskiy, 2020) by enabling models to capture complex dependencies through self-attention mechanisms effectively. However, despite its advantages, the attention mechanism suffers from quadratic time and linear memory complexity (Keles et al., 2023). Furthermore, the commonly used key-value (KV) cache optimization increases the memory requirement linearly with sequence length during the generative decoding phase. As a result, cache size requirements of-"}, {"title": "2. Background", "content": ""}, {"title": "2.1. Transformer Models", "content": "Transformer-based models consist of layers that process token sequences. Most current large language models (LLMs) for text generation are autoregressive decoder-only transformer layers, known for their strong zero-shot generalization (Wang et al., 2022) and use in chatbots and productivity tools. These models first process the entire input sequence in a single forward pass (prefill phase) and then generate additional tokens one at a time, each requiring a separate forward computation (generative decoding phase). This research aims to enhance the decoder-only transformers."}, {"title": "2.2. Self-Attention and Sparsity", "content": "Multi-head self-attention (MHA) is the first computational step of the transformer layer. The MHA receives as an input a sequence of tokens $X \\in \\mathbb{R}^{n \\times D}$ where n is the sequence length and D is the hidden dimension. Each of the heads processes X in parallel by first multiplying it by 3 different trained matrices $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times d}$ obtaining 3 matrices Q, K, V \u2208 $\\mathbb{R}^{n \\times d}$ and adding a positional encoding (e.g., ROPE (Su et al., 2024)) to them.\n\n$Q = XW_Q, K = XW_K, V = XW_V$  (1)\n\nThen, a pre-softmax attention matrix A is computed from matrices Q and K (2). Although A matrix is often normalized by $\\sqrt{d}$ for numerical stability and masked by a causality mask, we omit these 2 steps for simplicity.\n\n$A = QK^T$ (2)\n\nAfter that, each row of the A matrix is normalized by the Softmax operation yielding the post-softmax attention matrix S (3) followed by a multiplication by V (4).\n\n$S = row\\_softmax(A)$ (3)\n\n$P = SV$ (4)"}, {"title": "Generative decoding phase and KV-cache", "content": "MHA employs a well-established performance optimization called KV-cache (Shi et al., 2024) which allows processing a single embedded token $x \\in \\mathbb{R}^D$, computing only the current token's $q, k, v \\in \\mathbb{R}^d$ vectors, while the complete K,V matrices are loaded from the cache (avoiding recomputation) and the new k,v vectors are appended to them. In this situation, the attention matrices simplify to vectors $a, s \\in \\mathbb{R}^n$ representing the attention of the currently generated token to all the previous tokens, and the P matrix becomes a single token embedding $p = sV \\in \\mathbb{R}^d$. Due to the large size of the KV-caches, especially as the sequence length grows longer, the computation of the self-attention during the generative decoding is heavily limited by memory bandwidth, dominated by loading all the n rows of the K and V matrices, while only performing 1 multiply-add per value read from memory. Due to the memory bottleneck, a reduction of memory reads directly leads to a corresponding speed up.\n\nAn established variant of multi-head self-attention is called grouped query attention (GQA). The GQA introduces sharing a single pair of K,V matrices for a group of g heads (queries), which reduces the amount of K,V data to load by a factor of g. Recent studies have shown that GQA has only a marginal impact on the accuracy of the downstream task, making it a favorable optimization (Ainslie et al., 2023)."}, {"title": "2.3. Top-k Attention", "content": "One widely adopted approach of sparsifying the attention row is finding its top k out of n elements and discarding the rest, where k is a hyperparameter (Gupta et al., 2021). However, since the attention rows are often partitioned (tiling across the sequence dimension) and processed in parallel, computing the exact top-k values in a vector imposes an undesired full-row dependency, thereby constraining the partitioning strategies. Moreover, the computation of the top"}, {"title": "3. Top-0 Method", "content": "Top- attention involves comparing each attention vector against a calibrated threshold. Attention elements that fall below the threshold are pruned away from subsequent computations, enhancing the efficiency and focus of the model. Our underlying assumption is that a particular distribution of values characterizes each row of the attention matrix, therefore using a static threshold should keep approximately the desired number of selected attention elements. The benefit of using a threshold instead of computing the top-k attention values is that thresholding is a simple elementwise operation, requiring only a constant time and involving no row dependency. In contrast, ranking the elements of a vector requires at least logarithmic time and depends on the full length of the vector. Top- can be seen as an approximation of Top-k attention, as it allows calibrating the thresholds for specific user-defined k.\n\nThe rest of this section provides a detailed discussion of (1) the calibration process for the thresholds and (2) their application for efficient inference."}, {"title": "3.1. Threshold Calibration", "content": "Given a transformer model, a set C of calibration data samples, and the desirable number k of elements to keep per attention row, we propose to calibrate a single scalar threshold $\\theta_{l,h,r}(k) \\in \\mathbb{R}$ for every transformer layer l, head h, and attention row id r. The objective of the threshold calibration is to determine a threshold value $\\theta_{l,h,r}(k)$ such that on average k elements are greater than $\\theta_{l,h,r}(k)$. The calibration procedure listed in Algorithm 1 calibrates the thresholds of a single attention head in a single layer, thus"}, {"title": "3.2. Top-0 Attention Inference", "content": "The inference procedure is depicted in Figure 1, focusing on a single transformer layer l, single head h, and single attention row n of length n (as in generative decoding). The attention vector is compared against the calibrated threshold value $\\theta_{l,h,n}$. Attention elements that do not pass the threshold are discarded. Instead of multiplying by the entire V matrix, only the selected k row indices are loaded to a compact matrix $\\tilde{V}$, which is used to compute the final product p. The calibrated thresholds can be stored alongside the model parameters. Since the calibration might not cover the entire range of sequence lengths $r \\in \\{k, k+1, ...\\}$, a threshold of the nearest calibrated sequence length (r) can be used.\n\nApplying a threshold calibrated towards a specific k allows selecting k attention row elements on average because the distribution of the attention values slightly varies across the processed inputs. To address the cases when thresholding results in more attention elements than desired (k > k), we experimented with capping the number of selected elements to at most k, prioritizing first and last tokens (we dubbed this technique CAPK). However, in generative tasks, CAPK caused a noticeable degradation in performance.\n\nWe propose two compensation methods that recover the accuracy degradation incurred by the Top- by providing a better mathematical approximation of the overall self-attention operation. These methods introduce a minor additional compute but may be crucial for accuracy-sensitive tasks. Moreover, these compensations apply to other attention"}, {"title": "3.2.1. SOFTMAX DENOMINATOR COMPENSATION", "content": "Since the post-softmax sparsification showed higher accuracies in our experiments compared to pre-softmax, we are interested in approximating post-softmax-sparsified attention (s) using pre-softmax-sparsified attention (a). Let I\u2286 {0,...,n-1} denote a set of indices that we intend to keep during the sparsification, and let us establish the relation between post-softmax sparsified vector s and the pre-softmax sparsified vector that underwent softmax (softmax(a)). Let R and E denote sums of exponents of the selected and discarded elements, respectively.\n\n$\\forall i \\in I, \\tilde{s_i} = softmax(a)|_I = \\frac{e^{a_i - max(a|_I)}}{ \\sum_{j \\in I} e^{a_j - max(a|_I)} }$ (5)\n\n$= \\frac{e^{a_i - max(a|_I)}}{ \\sum_{j \\in I} e^{a_j - max(a|_I)} + \\sum_{j \\notin I} e^{a_j - max(a|_I)} }$ (6)\n\n$= \\frac{e^{a_i - max(a|_I)}}{R + E}$ (7)\n\n$= softmax(a) \\cdot \\frac{R}{R+E}$ (8)\n\nIn other words, to achieve a post-softmax sparsification effect, one can perform pre-softmax sparsification, estimate $E \\approx \\tilde{E}$, and compensate by multiplying by a factor of $R/(R+E)$. The multiplication step can be applied after the softmax (3) or even after the SV product (4) similarly to the flash-attention (Dao et al., 2022). We consider 3 estimations:\n\n1.  offline-calibrated: calibrate the compensation value $E$ for every (layer, head, row) similarly to the method of threshold calibration.\n2.  exp-threshold: $E = \\gamma (n-k)e^{\\theta}$ where $\\theta$ is the calibrated threshold for the current sequence length n, and k is the number of selected attention elements. The intuition behind this approximation is that all the not-selected attention elements are less than $\\theta$. The $\\gamma$ is a small constant hyperparameter that we set to 0.05 to approximate the difference between the sum of exponentiated thresholds and actual exponentiated discarded attention elements.\n3.  exact: compute the exact $E = \\sum_{j \\notin I} e^{a_j - max(a|_I)}$ using the sum of exponents of the non-selected row elements.\n\nAssuming that $argmax a|_I$ is included in the selected elements I, we do not correct the maximum value subtracted from the exponents during softmax computation."}, {"title": "3.2.2. V-MEAN-COMPENSATION (VMC)", "content": "Applying Top- in the following two cases will result in attention vector rows not summing up to 1: (i) post-softmax, (ii) pre-softmax followed by SDC. As a result, any attention element dropped due to thresholding will cancel the effect of its product by the corresponding row of the V matrix in the final $p = sV$ product (4). In these situations, we propose compensating for the missing value by adding a mean row $\\mu$ (9) of the V matrix scaled by the sum of all the discarded"}, {"title": "4. Evaluations", "content": "We use the LM Evaluation Harness (Gao et al., 2024) to evaluate the normalized accuracy metric on multiple-choice Q&A datasets, including their standard few-shot settings. In addition, we assess the Human-eval dataset using the official library (Chen et al., 2021) to measure the pass@1 metric on all 164 code generation tasks. Our source code is available\u00b9.\n\nWe evaluate 3 main attention variants: (i) Baseline - full attention, without any sparsification; (ii) Top-k \u2013 keep k attention elements per row; (iii) Top- \u2013 keep k\u2248k attention elements by thresholding. We apply our compensation methods also to the Top-k baselines for a fair comparison."}, {"title": "4.1. Top-0 Overall Performance", "content": "This section evaluates the best configurations of Top- , comparing them to the non-sparsified baseline and to Top-k.\n\nIn Figure 4, LLaMA2 and LLaMA3 models are evaluated on Q&A tasks, where the k parameter of Top-k and of the Top- is swept from 32 to 512. The first two layers are kept at k=512. The calibration set used for Top- in each dataset is 10% of the training or validation sets (different from the test set). The x-axis shows the fraction of the attention elements that are involved in computation, normalized to an average number of attention elements in the entire model in a given forward pass. The y-axis shows the normalized accuracy. The main observation is that in all models, both Top-k and Top- have increased the accuracy (by 0.2% - 1%) compared to the baseline while pruning away a significant portion of attention elements (2\u00d7 5x fewer elements were active). Second, post-softmax sparsification performs"}, {"title": "4.2. Pre- vs Post-Softmax Thresholding", "content": "We evaluate the impact of attention matrix sparsification in its two main variants: on matrix A and on the post-softmax matrix (S), where each of these two variants requires individual calibration. Figure 6 depicts how the different thresholdings impact the accuracy of LLaMA2-7b on the Hellaswag dataset. For comparison, the Top-k approach is also evaluated alongside Top-0. We conclude that post-softmax preserves more accuracy compared to pre-softmax thresholding. This conclusion is confirmed on extended evaluations on other datasets (see Appendix F)."}, {"title": "4.3. Thresholding Different Layers", "content": "We explored the impact of thresholding different layers with a different target k. For the LLaMA models, we have observed that targeting a slightly higher k in the first layers is crucial. As a best practice, we found keeping 2 initial layers at k = 512, whereas the rest of the layers could be sparsified more aggressively. Figure 7 shows the LLaMA2-7b model accuracy on Hellaswag, comparing Top-k and Top- using higher k in first 2 layers against using equal k in all layers. All variants do not perform any compensations. The conclusion is that preserving denser attention matrices in the early layers is beneficial for the accuracy of a downstream task, which is aligned with some of the works on quantization (Tang et al., 2024; Huang et al., 2024)."}, {"title": "4.4. Thresholding Different Attention Rows", "content": "In Section 4.3, we have shown that Top- attention should use individually calibrated thresholds for every transformer layer in the model due to inherently different attention element distributions across the layers, and in Section 3.1, we saw that different heads require individual thresholds."}, {"title": "4.5. Numerical Compensations", "content": "We evaluate the proposed numerical compensation methods SDC, and VMC. Figure 9 shows that the more explicit SDC versions (exp-threshold, exact) reclaim more of the degraded accuracy on the hard task of Hellaswag. Moreover, when applying the VMC as well, the benefit is further increased.\n\nFigure 9 focuses on the Hellaswag task, LLaMA2-7b model with Top- attention pre-softmax, in which the non-compensated Top- attention incurred up to 1.5% accuracy degradation, whereas the compensations have effectively closed the gap to the baseline accuracy.\n\nIn similar experiments on ARC-C and ARC-E datasets, Top- has outperformed the baseline attention without the compensations thanks to increasing the focus of the model, or in other words, reducing the attention noise. In such cases, the compensations do not contribute much and may only bring back the noise, thereby degrading the accuracy down to the baseline accuracy levels."}, {"title": "4.6. Impact of Grouped Query Attention (GQA)", "content": "In GQA, multiple attention heads share the same V matrix. In LLaMA-3-8B models, the GQA group size is g = 4; therefore, when Top-k attention is applied, in the worst case of total disagreement between the heads, there could be 4\u00b7 k"}, {"title": "4.7. Distribution Shifts", "content": "To examine how resilient the calibrated threshold is, we evaluate on Human-eval the two following Top- variants: 1) calibrated on Q&A dataset of ARC-C, and 2) calibrated on the first 10% of Human-eval tasks. As seen in Figure 11, the Top- post-softmax is even more accurate when using thresholds calibrated on a different dataset. For pre-softmax, there is a benefit of calibrating on the same dataset. Overall, the thresholds show resilience towards distribution shift, which suggests that they are strongly associated with the model rather than with the data. This allows calibrating thresholds once per model."}, {"title": "5. Related Work", "content": "In the line of work on the content-based sparsity, the most fundamental approach is Top-k attention (Gupta et al., 2021), which selects the k highest values in each row of the A matrix. However, in practical kernel implementations, where the attention rows are tiled, it is highly difficult to implement"}, {"title": "6. Conclusion", "content": "We have presented Top-0, a new sparse attention algorithm based on fixed and calibrated thresholds. At inference time, it boils down to a simple elementwise operation, which overcomes the limitation of full-row dependent algorithms, such as Top-k, and thereby unconstraining tiling and distributed inference implementations. Moreover, only a small subset of calibration samples is required to calibrate the thresholds, which we showed to be resilient to distribution shifts. Therefore, a short calibration is needed once per every model. Furthermore, we show minor to no performance degradation using 10x less attention elements at the computationally-bound prefill phase and using 3x less V matrix rows at the memory bandwidth-bound generative decoding phase. These reductions unlock promising speedups for inference."}, {"title": "A. Impact Statement", "content": "This paper presents work whose goal is to advance the field of efficient Machine Learning. All potential societal consequences are unrelated to the specific work but are related to machine learning applications in general."}, {"title": "B. Threshold Calibration", "content": "In this appendix section, we show more calibrated threshold values (as a function of the sequence length) in more layers and heads. Figure 12 visualizes the thresholds as a function of a sequence length that was calibrated for The LLaMA2-7b model for the Hellaswag dataset. Different heads have evidently different threshold values, hence the importance of per-head individual calibrations"}, {"title": "C. Multi-k Cumulative Calibration", "content": "Algorithm 1, which we presented in the paper, calibrates thresholds of a single attention head for a single target k. We propose to generalize it to calibrate all thresholds for a broader range of target k and to do it in one pass over the calibration samples. We call such a calibration procedure \"Multi-k Cumulative\u201d (MKC).\n\nThe goal of MKC calibration is to construct a multi-k threshold function \u03b8(k) : N \u2192 R for every (layer l, head h, attention row id r). The user will be able to query such a function using their k of choice and obtain the threshold needed to accommodate this k. Such an ability allows a very flexible control over the sparsification of the model, which is a highly desirable tool for LLM inference service that should be able to dynamically trade some accuracy for speedup.\n\nMKC algorithm Algorithm 2 describes how a threshold function (k) can be calibrated. First, for every given calibration sample, let v represent the rth attention row. The v undergoes a sorting and then is treated as a sequence of half-open intervals. For each interval, we treat its smaller endpoint as its threshold, and we also associate with it an effective k (effective k of a threshold w.r.t the vector v is the number of elements that are greater than the threshold). For example, on line 2 we represent by (r-1,[v\u2080,v\u2081)) the interval between v\u2081 (inclusive) and v\u2082 (exclusive) that corresponds to the threshold v\u2081 and an effective k of r - 1. Second, per-calibration-sample interval sequences collected in the set \u0398r are merged to represent the function k\u1d63(\u03b8) : R\u2192R which maps each interval to a threshold and to an average effective k achievable by this threshold"}, {"title": "D. V-Mean Compensation", "content": "In this section, we provide a formal proof for our V-Mean Compensation (VMC) serving as a good approximation of the sparsified sV product to the full sV product.\n\nLemma D.1. Let s \u2208 [0,1]\u207f denote the post-softmax attention vector, and let \u0161 \u2208 [0,1]\u1d4f denote its thresholded variant, containing the k selected values of s, and let I \u2208 {0,...,n\u22121}\u1d4f denote the indices of s that surpassed the threshold such that \u2200i \u2208 {0,...,k\u22121} : \u0161\u1d62 = s\u1d62. Let V \u2208 \u211d\u207f\u02e3\u1d48 be the value matrix and let \u00d1 \u2208 \u211d\u1d4f\u02e3\u1d48 be consisting only of the selected V rows such that \u2200i \u2208 {0,...,k\u22121} :\u00d1\u1d62=V\u1d62. We claim that in the expectation the full product p=sV\u2208\u211d\u1d48 is equal to the thresholded attention plus the residual probability mass \u03b2 (10) multiplied by the mean-V row \u03bc (9). Namely,\n\n$\\forall 0 < j < d: E[p_j] = (sV)_j + \\beta \\mu_j$ (13)\nProof.\n\n$E[p_j] = E[\\sum_{i=0}^{n-1} s_i V_{ij}]$ \n\n$= E[\\sum_{i \\in I} s_i V_{ij}] + E[\\sum_{i \\notin I} s_i V_{ij}]$ \n\n$=(s\\tilde{V})_j+ E[\\sum_{i \\notin I} s_i V_{ij}] $\n\n$=(s\\tilde{V})_j+ E[\\sum_{i \\notin I} s_i E[V_{ij}] ]$\n\n$=(s\\tilde{V})_j+ \\sum_{i \\notin I} E[s_i] E[V_{ij}]$\n\n$=(s\\tilde{V})_j+(n-k)\\frac{\\beta}{n-k} \\mu_j$ \n\n$=(s\\tilde{V})_j+\\beta\\mu_j$ (14)\n\nUnder the following assumptions:\n\n1. s \u2568 V\u1d62, that is the attention vector s is statistically independent on the elements in the columns of matrix V. They are conditionally independent given the input X from which they were originally computed (1).\n2. The distribution of s\u1d62, \u2200i \u2208 I within the long tail of the non-selected indices is close to uniform and hence we can approximate its expectation by an average.\n3. The expectation of V\u1d62\u2c7c can be approximated by its average."}, {"title": "E. Evaluation statistics", "content": "In this section, we present again the experimental results from Section 4.1; however, to demonstrate statistical significance, we show the error bars. This is important since every data point is aggregated using averaging across layers, heads, and test examples, as we will describe below. Therefore standard deviation of such an averaged metric is of interest.\n\nFigure 15 focus on Q& tasks, showing the tradeoff between the model's accuracy (y-axis) and the number of attention elements as selected by the Top-k or Top-0 (x-axis). The standard deviation in the accuracy was provided by the LM-Eval evaluation harness, as a part of the standardized evaluation procedure. The average ratio presented on the x-axis and its standard deviation were computed over the following population: number of test samples \u00d7 number of model layers \u00d7 number of attention heads. To present the ratios, we first compute the absolute average and absolute standard deviation of the total count of attention elements and in the attention matrix, second - we normalize both the standard deviation and the average by the average number of attention elements when no sparsification took place.\n\nFigures 16 and 17 show the evaluation on Human-eval dataset. Figure 16 is similar to the Q&A plots as it shows how the reduction in the number of attention elements during prefill phase impacts the accuracy score (pass@1). Figure 17 focuses on the generative decoding phase and shows how the number of the needed V rows is affected by the elements that were selected"}, {"title": "G. Thresholding different layers", "content": "In this appendix, we present more results that support the decision to use denser initial layers. That is to use higher k for Top-k or calibrating towards a higher k in Top-0. Figure 19 shows LLaMA2-7b model accuracy on Q&A datasets, and LLaMA2-70b model accuracy for Hellaswag dataset. The figure compares Top-k and Top-0 using higher k in the first 2 layers against using equal k in all layers. All variants do not perform any compensations."}, {"title": "I. Impact of GQA", "content": "In Figure 21, we show how the different attention sparsification approaches (Top-k, Top-0 with and without CAPK) affect the number of required V rows. Top-k (Figure 21a) guarantees exactly 128 selected elements per row of every head. However, the unified set of 4 heads in the group reaches only about 250, which indicates a certain agreement between the heads, which is mostly found in the recent tokens (as seen on the heatmap). We observed very similar characteristics in other heads and layers. Top- approach with capping the number of selected elements to at most 128 per head yielded degraded quality of the generated text since it mainly focused attention on the most recent tokens. Finally, the ordinary Top-0 in Figure 21c, which provided good quality results, does seem to exhibit a certain variability in the number of selected elements per group, sometimes selecting more than 128 per group."}]}