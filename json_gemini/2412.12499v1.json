{"title": "LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for Low-Resource Language Tasks", "authors": ["Hongbin Zhang", "Kehai Chen", "Xuefeng Bai", "Yang Xiang", "Min Zhang"], "abstract": "Large language models (LLMs) have demonstrated impressive multilingual understanding and reasoning capabilities, driven by extensive pre-training multilingual corpora and fine-tuning instruction data. However, a performance gap persists between high-resource and low-resource language tasks due to language imbalance in the pre-training corpus, even using more low-resource data during fine-tuning. To alleviate this issue, we propose LinguaLIFT, a two-stage instruction tuning framework for advancing low-resource language tasks. An additional language alignment layer is first integrated into the LLM to adapt a pre-trained multilingual encoder, thereby enhancing multilingual alignment through code-switched fine-tuning. The second stage fine-tunes LLM with English-only instruction data while freezing the language alignment layer, allowing LLM to transfer task-specific capabilities from English to low-resource language tasks. Additionally, we introduce the Multilingual Math World Problem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and 10 high-resource languages, enabling comprehensive evaluation of multilingual reasoning. Experimental results show that LinguaLIFT outperforms several competitive baselines across MMWP and other widely used benchmarks.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive understanding and reasoning capabilities, opening up a new paradigm for artificial intelligence (Wei et al., 2022; Zhao et al., 2023). These capabilities arise from self-supervised pre-training on large-scale multilingual data, followed by supervised fine-tuning on multilingual instruction data (Ouyang et al., 2022; Muennighoff et al., 2023; Chung et al., 2024). As a result, LLMs have achieved remarkable performance across various natural language processing tasks (Chen et al., 2024a,b; Zhang et al., 2024b,a), especially high-resource language mathematical reasoning tasks (Shi et al., 2023; She et al., 2024) and natural language inference tasks (\u00dcst\u00fcn et al., 2024; Lu et al., 2024).\nHowever, a substantial performance gap still exists between high-resource and low-resource language tasks (Zhu et al., 2024c), primarily due to the language imbalance within the large-scale multilingual corpora during pre-training, where high-resource languages dominate (Touvron et al., 2023). This gap remains unaddressed, even using more low-resource language data during the following fine-tuning (Singh et al., 2024). As illustrated in Figure 1, while LLMs can generate the correct answer to the query in high-resource languages like English, they struggle with the same query in low-resource languages. These discrepancies emphasize a fundamental limitation in the understanding and reasoning capabilities of LLMs for low-resource languages.\nTo alleviate this issue, we propose a two-stage instruction tuning framework (LinguaLIFT) for enhancing low-resource language tasks. An additional language alignment layer is initially integrated into the LLM to adapt a pre-trained multilingual encoder. LinguaLIFT then improves multilingual alignment via code-switched fine-tuning, using the code-switched translation data generated from unsupervised multilingual alignment lexicons The second stage fine-tunes LLMs with English-only instruction data while freezing the language alignment layer, which enables LLMs to transfer the learned task-specific capabilities from English to low-resource languages. Additionally, we introduce a new benchmark named Multilingual Math World Problem (MMWP), spanning 21 low-resource, 17 medium-resource, and 10 high-resource languages to evaluate multilingual reasoning tasks comprehensively. Experiments demonstrate that LinguaLIFT significantly outperforms several strong competitive methods on the MMWP and other widely used benchmarks, such as MGSM (Shi et al., 2023), MSVAMP (Chen et al., 2023), XNLI (Conneau et al., 2018) and X-CSQA (Lin et al., 2021)."}, {"title": "Related Work", "content": "Leveraging Powerful Cross-lingual Transfer Capabilities of Multilingual Models. Recent studies have shown that multilingual models possess strong cross-lingual transfer capabilities (Conneau et al., 2020; Xue et al., 2021; FitzGerald et al., 2023; Chirkova and Nikoulina, 2024; Shaham et al., 2024; Chen et al., 2024c), benefiting the low-resource language community by enabling task transfer across multiple languages after fine-tuning on high-resource languages (Kew et al., 2023; Pfeiffer et al., 2020; Reimers and Gurevych, 2020; Pan et al., 2021; Feng et al., 2022).\nUnlike previous approaches focusing on multilingual instruction tuning (Singh et al., 2024; \u00dcst\u00fcn et al., 2024; K\u00f6pf et al., 2024; Zhu et al., 2024b,a; Li et al., 2023a; Wei et al., 2023; Ranaldi et al., 2023), LinguaLIFT incorporates a language alignment layer into LLMs to adapt a pre-trained multilingual encoder through code-switched fine-tuning. While freezing the language alignment layer and a pre-trained multilingual encoder, the LLM is fine-tuned with English-only instruction data to transfer task-specific capabilities from English to low-resource language tasks.\nImproving Multilingual Mathematical Reasoning Tasks. Recent efforts to improve multilingual mathematical reasoning for LLMs can be categorized into three ways: 1) Prompting close-source LLMs: Qin et al. (2023), Shi et al. (2023), and Huang et al. (2023) designed prompts for closed-source LLMs like ChatGPT, translating non-English contexts into English for reasoning. However, this approach is limited by translation quality and does not improve multilingual understanding or work well for open-source LLMs (Zhu et al., 2024b). 2) Instruction-tuning open-source LLMS: Chen et al. (2023), Chai et al. (2024), and Lai and Nissim (2024) adopted a translate-training method, translating English reasoning datasets into non-English and instruction-tuning LLMs. Zhu et al. (2024b) and She et al. (2024) proposed approaches to transfer mathematical reasoning capabilities from English to non-English. While these methods improve multilingual reasoning, they incur high translation costs and errors, making them impractical for low-resource languages. 3) Bridging existing skilled LLMs to multilingual-ism: Yoon et al. (2024) combines pre-trained multilingual models with skilled reasoning LLMs, but a performance gap persists between low-resource and high-resource languages. While Huang et al. (2024) shows significant improvement, it relies heavily on parallel and multilingual task data.\nDifferent from existing studies, this paper proposes a novel two-stage instruction fine-tuning framework to enhance the reasoning capabilities of low-resource language tasks without relying on multilingual instruction data."}, {"title": "Methodology", "content": "Figure 2 illustrates an overview of the proposed two-stage instruction tuning framework, LinguaLIFT. The core idea is to transfer task-specific capabilities learned from English to low-resource languages, leveraging the language alignment established by code-switched tuning. We then introduce the model architecture (\u00a73.1), the language alignment stage (\u00a73.2) and the task transfer stage (\u00a73.3).\nModel Architecture\nGiven a task-specific input x with lx tokens, we use a multilingual pre-trained model to encode it into a moderately language-agnostic representation X, facilitating more effective cross-lingual understanding and transfer:\n$X = Encoder(x),$\nwhere $Encoder(\u00b7)$ is the pre-trained multilingual encoder, and $X \u2208 R^{lx\u00d7d1}$ is the final hidden state. We introduce the language alignment layer, $Alignment()$, to bridge the gap between the multilingual encoder and the LLM representation, implemented as a multi-layer perceptron (MLP). This layer maps X to the input embedding space of the LLM:\n$X = Alignment(X),$\nwhere $X \u2208 R^{lx\u00d7d2}$ is the alignment representation of X on the input embedding space of the LLM. To leverage the built-in capabilities of the LLM for instruction following, we embed the instruction context q with lq tokens using the embedding layer $Embedding(\u00b7)$:\n$Q = Embedding(q),$\nwhere $Q \u2208 R^{lqxd2}$ presents the instruction context q in the input embedding space of the LLM. Then, we concatenate the instruction context representation with the multilingual input representation:\n$(Q, X) = [<bos>; Q; <enc_start>; X; <enc_end>],$\nwhere $<bos> \u2208 R^{d2}$ is the start token representation and $<enc_start>$ and $<enc_end>$ are trainable boundary parameters of X. Finally, (Q, X) is fed into the LLM to generate the response.\nLanguage Alignment\nIn this stage, we enhance the model's multilingual alignment through code-switched tuning. This involves two key parts: building multilingual alignment lexicons and enhancing alignment via code-switched translation instruction tuning.\nBuilding Multilingual Alignment Lexicons. Given the scarcity of data for low-resource languages, we adopt an unsupervised word translation method MUSE (Lample et al., 2018) to construct the multilingual alignment lexicons"}, {"title": "The MMWP Benchmark", "content": "Existing multilingual mathematical reasoning benchmarks (Shi et al., 2023; Chen et al., 2023) predominantly focus on 7 high-resource languages and 3 low-resource languages, leaving a significant gap in coverage for low-resource languages. This imbalance introduces evaluation bias, as models are optimized for high-resource languages while their performance on low-resource languages remains underexplored, hindering the comprehensive development of multilingual models. To fill this gap, we build a new Multilingual Math World Problem (MMWP) benchmark and describe its collection process in this section.\nSource data. We use AsDiV (Miao et al., 2020) and MAWPS (Koncel-Kedziorski et al., 2016) as base datasets. From the AsDiv official test set, we randomly select 500 examples and another 500 from MAWPS, where all the problems require multiple steps to solve, as described by Miao et al. (2020). We filter out duplicates and problems with non-numeric answers, resulting in 811 examples.\nTarget Language Selection. We select a typologically diverse set of 48 languages, spanning 12 language families and 12 writing systems, with a range of resource levels\u2014low, medium, and high. This includes 21 low-resource languages, 17 medium-resource languages, and 10 high-resource languages. Additional selection details are provided in Appendix B.1.\nTranslation process. To ensure the high quality of translations, we employed Google Translation System to translate the selected English questions into 47 other languages. Moreover, five annotators then post-edit and calibrate the translation, followed by quality estimation through both human and automatic evaluation, as detailed in Appendix B.2."}, {"title": "Experiments", "content": "Datasets\nEvaluation Dataset. We use the MMWP and latest multilingual benchmarks, MGSM (Shi et al., 2023) and MSVAMP (Chen et al., 2023) to evaluate the performance of LLMs in multilingual mathematical reasoning. We evaluate in zero-shot chain-of-thought reasoning (Wei et al., 2022) setting. To further assess the task generalization of LinguaLIFT, we incorporate several challenging multilingual datasets, including X-CSQA (Lin et al., 2021) for commonsense reasoning and XNLI (Conneau et al., 2018) for natural language inference.\nTraining Dataset. We utilize English-only instruction data following prior work (Lu et al., 2024; Huang et al., 2024), which include Meta-MathQA (Yu et al., 2024) for mathematical reasoning, MultiNLI (Williams et al., 2018) for natural language inference, and a set of unified commonsense reasoning tasks, comprising the X-CSQA, OpenBookQA (Mihaylov et al., 2018), ARC (Clark et al., 2018), and QASC (Khot et al., 2020) datasets. Additionally, to further explore the potential of the proposed method, we incorporate the recent advanced mathematical reasoning instruction dataset, OpenMathInstruct-2 (Toshniwal et al., 2024). Statistics of the datasets involved are presented in Table 5, and the prompts for each task are given in Appendix A.3.\nBaselines\nWe consider three categories of baselines: (1) Mono-SFT (Luo et al., 2023; Yue et al., 2024; Yu et al., 2024; Toshniwal et al., 2024; Zhu et al., 2024b), a vanilla method that fine-tunes the model on English task datasets. (2) Multi-SFT (Chen et al., 2023; Zhu et al., 2024b; She et al., 2024), a translation-based method that fine-tunes the model on multilingual task datasets. (3) Leveraging External Tools or Models (Shi et al., 2023; Yoon et al., 2024; Huang et al., 2024), a method that leverages external translation systems or models with existing powerful LLMs. More training details are presented in Appendix A.4.\nExperimental Results\nLinguaLIFT demonstrates significant improvements across low-resource languages on MMWP. Experimental results on the MMWP test set, grouped by language resource levels, are presented in Table 1. We elaborate on six key observations from the results: (1) Mono-SFT models exhibit substantial performance drops in low-resource languages and slight drops in medium-resource languages. (2) Multi-SFT models, while outperforming their monolingual counterparts in low-resource settings, still show a considerable performance gap between low-resource and high-resource languages. (3) Models leveraging external translation systems or pre-trained multilingual models achieve limited generalization to unseen languages, and they still lag behind in performance compared to high-resource languages. (4) Mind-Merger, retrained using additional open-source parallel data in low-resource languages for a fair comparison, performs well in high-resource languages while hardly improving low-resource tasks, demonstrating its limited applicability in low-resource scenarios. (5) LinguaLIFT significantly enhances low-resource reasoning performance, outperforming all competitive baselines. (6) Incorporating advanced English reasoning datasets (e.g., OpenMathInstruct-2) further enhances low-resource language reasoning, highlighting the importance of adapting to evolving, high-quality data for improving low-resource language reasoning.\nThese results highlight the importance of comprehensively evaluating multilingual reasoning models across diverse languages and further validate LinguaLIFT's effectiveness in enhancing reasoning performance for low-resource languages."}, {"title": "Analysis", "content": "Two-Stage Training Ablation Studies\nTraining Stage Ablation As shown in Figure 3a, removing the Language-Align stage leads to a noticeable performance drop in low-resource languages, demonstrating the essential of the Language-Align stage in enhancing low-resource language tasks. Removing the Task-Transfer stage causes substantial performance degradation across all languages. These results suggest the necessity of the two stages. Further details are provided in Appendix D.1.\nTrainable Modules Ablation As illustrated in Figure 3b, training the LLM during the Language-Align stage reduces performance on high-resource language reasoning. Freezing LLM training in the Task-Transfer stage significantly degrades reasoning performance across all languages. Additionally, training the language alignment layer during the Task-Transfer stage harms performance on low-resource language reasoning. These results indicate the benefit of first training the language alignment layer, followed by LLM fine-tuning. Further details are available in Appendix D.2.\nAnalysis of Code-Switched Tuning\nImpact of Code-Switched Ratio to Multilingual Alignment and Reasoning Tasks. We report top-1 retrieval accuracy and reasoning accuracy on Tatoeba (Artetxe and Schwenk, 2019) and MGSM (Shi et al., 2023) to indicate language alignment degree and reasoning performance, respectively, as shown in Figure 4. Multilingual alignment improves as the code-switch ratio increases, especially for low-resource languages, and reasoning performance also increases. Notably, at an 80% code-switch ratio, the model's reasoning performance reaches comparable performance to those trained with parallel corpora.\nImpact of the Part-of-Speech in Code-Switched to Reasoning tasks. As shown in Figure 6, among the individual part-of-speech (POS) groups, substituting nouns significantly affects the model's reasoning ability. Syntactic structures involving subject-verb and prepositional phrase combinations outperform other POS combinations, highlighting the importance of core arguments (subjects and verbs) and their relations (prepositions) for capturing key relationships in reasoning tasks. These structures are sufficient for the model to generalize reasoning across languages. In contrast, adjective-adverb and auxiliary-conjunction combinations perform the worst, suggesting that modifiers are less critical for reasoning tasks. More details are provided in Appendix D.3\nLinguaLIFT better aligns low-resource languages\nFor each low-resource language in the MMWP benchmark, we selected 100 texts with equivalent meaning from the Flores-101 dataset (Goyal et al., 2022). We obtained these input mean pooling representations from different methods and visualized them using T-SNE (van der Maaten and Hinton, 2008). As shown in Figure 5, the LLM embeddings of low-resource languages are distinct from English, indicating the challenges of understanding and transferring knowledge to these languages. In the case of MindMerger, some low-resource language representations closely overlap with English, while others remain isolated, demonstrating its limitations in scenarios involving a wide range of low-resource languages. In contrast, LinguaLIFT aligns low-resource language representations more closely with English, enabling better transfer of reasoning capabilities from English instruction data, thereby resulting in improved reasoning performance.\nSupplementary Experiments\nWe conducted several supplementary experiments, including the quantitative analysis of the correlation between multilingual alignment and reasoning performance (Appendix E.1), the analysis of language transferability in language families and writing systems (Appendix E.2), the adaptation of different types and scales of LLMs (Appendix E.3, the selection of the language alignment layer and pre-trained multilingual encoder (Appendix E.4, E.5), the incorporation of multilingual instruction data into LinguaLIFT (Appendix E.6), and zero-shot CoT examples in mathematical reasoning tasks (Appendix E.7)."}, {"title": "Conclusion", "content": "This paper introduced LinguaLIFT, a novel two-stage instruction tuning framework that enhances low-resource language tasks without relying on parallel corpora or multilingual instruction data. Additionally, we introduced MMWP, a multilingual benchmark spanning 21 low-resource, 17 medium-resource, and 10 high-resource languages, to comprehensively evaluate multilingual mathematical reasoning tasks. Experiments on the MMWP and other widely used benchmarks demonstrate its effectiveness in advancing low-resource language tasks and further alleviating the performance gap between high-resource and low-resource language tasks in LLMs.\nLimitations\nWhile our experimental results demonstrate that the proposed two-stage instruction tuning method significantly improves low-resource language reasoning and understanding tasks, it does require a certain level of computational resources. Specifically, the need for a moderately larger pre-trained multilingual encoder and full fine-tuning of the LLM may impose a computational burden. Additionally, we have not explored parameter-efficient fine-tuning (PEFT) methods, so the effectiveness of our approach in PEFT settings remains untested. Future work could focus on developing more efficient and lightweight tuning strategies for LinguaLIFT to reduce the computational limits."}, {"title": "Implementation Details", "content": "Collection of the Alignment Lexicons\nTo better construct multilingual alignment lexicons for low-resource language tasks, we leveraged the Unsupervised Bilingual Lexicon Induction (UBLI) (Zhang et al., 2017; Lample et al., 2018; Dou et al., 2018; Artetxe et al., 2019; Li et al., 2023b), which has been proven effective in inducing word translation pairs by aligning independently trained word embeddings in two languages. Initially, we tokenized the text using the Spacy library's tokenization function. The resultant word set comprised all words, barring specific named entities, numbers, and date tokens. This step is crucial in ensuring our focus on frequent and pertinent terms that are likely to hold significance in task-specific domains.\nFollowing prior outstanding work MUSE6 (Lample et al., 2018), we construct multilingual lexicons using adversarial training to establish a linear mapping between source and target spaces without relying on cross-lingual supervision. This process involves training the model to align the word embeddings of the source and target languages in a shared semantic space. For each word in the source language, we identify the most relevant translations in the target language by projecting the source word embeddings into the target space and retrieving the top nearest neighbor words based on cosine similarity. These translated words form the bilingual lexicons, which are essential for enhancing multilingual understanding.\nDataset Statistics\nInstruction Tuning Prompts\nThe prompt for code-switched tuning is adapted from Zhang et al. (2023), where the source language, source sentence, and target language are replaced with the relevant translations.\nThe prompts for mathematical reasoning tasks, natural language inference, and commonsense question answering are modified from Toshniwal et al. (2024) and Lu et al. (2024), with the general instruction being replaced by the specific problems from the training data.\nPrompt for Code-Switched Tuning\nTranslate the following code-switched sentence from {source_lang} to pure {target_lang}:\n{source_lang}: {source_sentence}\n{target_lang}:\nPrompt for Mathematical Reasoning\nSolve the following math problem. Make sure to put the answer (and only the answer) inside \\boxed{}.\n{instruction}\nPrompt for XNLI\nI will give you a premise and a hypothesis. Choose the most appropriate relationship from the following options: Entailment, Neutral, Contradiction.\n### Premise:\n{premise}\n### Hypothesis:\n{hypo}\n### Answer:\nPrompt for X-CSQA\nQuestion:\n{question}\nChoices:\nA. {choice_A}\nB. {choice_B}\nC. {choice_C}\nD. {choice_D}\nE. {choice_E}\nAnswer:\nTraining Details\nWe use LlamaFactory (Zheng et al., 2024) as the training codebase for our experiments. In the first stage, we train only the additional alignment modules (e.g., the language alignment layer and the boundary tokens) for 3 epochs, with a constant learning rate of 6e-4 and a batch size of 256. In the second stage, we fine-tune all parameters of the LLM for 3 epochs. The learning rate is set to 2e-5, with a warm-up ratio of 0.05 and a cosine learning rate scheduler. We also apply a weight decay of le-2 and use a batch size of 128. All experiments are conducted on eight NVIDIA A100 GPUs for a day."}, {"title": "Details of Constructing the MMWP Benchmark", "content": "Selection of Target Languages and Categorization of Resource-level.\nIn our study, we employ a unique categorization method for selecting target languages, which deviates somewhat from the conventional definitions based on the abundance or scarcity of linguistic resources. This deviation is primarily due to our focus on LLMs. Rather than adhering to traditional classifications, we opt to categorize languages based on their language distribution in the pre-training corpus used for the LLMs.\nGuided by the LLaMA2 technical report (Touvron et al., 2023), we define low-resource languages as those that constitute less than 0.005% of the available multilingual datasets. On the other hand, we categorize languages with a representation percentage between 0.005% and 0.1% as medium-resource languages. This categorization method allows us to incorporate a diverse set of languages into our study, thereby enabling a more effective and wide-ranging assessment of the multilingual reasoning capabilities of our model.\nFurthermore, we believe that the comprehensive evaluation of multilingual tasks necessitates the inclusion of languages from various families and scripts. This diversity is crucial in understanding the robustness and versatility of our model, as it allows us to evaluate its performance and generalization capabilities across different linguistic contexts and resource levels. By incorporating languages from various families, scripts, and resource categories, we can ensure more comprehensive coverage in our multilingual settings evaluation.\nThis unique approach to language selection and categorization provides a more nuanced understanding of language resource levels in LLMs while also ensuring a broader and more diverse evaluation of multilingual tasks for LLMs. It also underscores the importance of considering language families and scripts for the assessment of LLMs, thereby contributing to a more comprehensive and inclusive approach to language model development and evaluation.\nQuality Estimation proves the effectiveness of the proposed MMWP benchmark.\nTo ensure the quality of the proposed MMWP benchmark, we conducted both human and automatic quality estimation evaluations.\nFor human evaluation, we employed five annotators, each of whom was responsible for post-calibrating and assessing the quality of the translated dataset. To facilitate this, we utilized GPT-4 for back-translation of the problems into English, a process that allowed the annotators to compare the translated versions with their original counterparts. The evaluators were tasked to scrutinize the translations for grammatical correctness, fluency, and semantic accuracy. Any significant deviations in meaning between the original and translated versions were carefully addressed. The annotators were guided to revise such instances to better align with the original intent, thereby ensuring the preservation of the original linguistic integrity. Moreover, the post-editing process was carried out to maintain native language properties, such as syntactical flexibility and natural phrasing, thus ensuring the fluency and native-like quality of the back-translations. This rigorous human calibration process aimed to certify that the final MMWP dataset was linguistically accurate and culturally appropriate for all represented languages.\nThe prompt for back-translating the low-resource language texts and assisting the post-editing process are presented below.\nGPT-4 Prompt for Back-Translating\nYou are a translation assistant. Directly translate the mathematical problems from {source_lang} to English without additional explanations.\n{source_sentence}\nGPT-4 Prompt for Assisting Annotators Post-Editing\nYou are provided with the following:\n1. A low-resource source language text.\n2. The back-translated English text (BT).\n3. The reference English translation (REF).\nYour task is to evaluate the quality of the back-translated English text (BT) based on three criteria: grammatical correctness, fluency, and semantic accuracy. Then, propose three alternative revisions to improve the BT. For each revision, explain why it was made and how it improves the translation.\n1. Grammatical Correctness:\nDoes the back-translated text adhere to standard English grammar rules (e.g., subject-verb agreement, punctuation, tense consistency)?\n2. Fluency:\nIs the back-translated text natural and smooth? Does it sound like it was written by a native speaker?\n3. Semantic Accuracy:\nDoes the back-translated text accurately reflect the meaning of the source language text? Are there any discrepancies in the interpretation of the source?\nProvide three revision suggestions for improving the BT:\n1. Each revision should aim to enhance either grammatical correctness, fluency, or semantic accuracy.\n2. For each suggestion, explain:\n\u2022 Why the revision is necessary.\n\u2022 Which aspect of translation quality (grammar, fluency, or accuracy) it improves.\nComplementing the human evaluation, we conducted an automatic evaluation to assess the MMWP dataset's translation quality quantitatively. In this process, we adopted a back-translation strategy, translating the MMWP problems into the target languages and then back into English. Subsequently, we used widely-accepted automatic evaluation metrics such as BLEU (Papineni et al., 2002), chrF (Popovi\u0107, 2015), and TER (Olive et al., 2011) to compare the back-translated problems with the original MMWP dataset. These metrics provided a quantitative measure of the overlap between the original and back-translated versions, thereby offering a quantifiable estimation of the translation fidelity. The results of this quality estimation, presented in Table 6, attest to the reliable quality of the translations.\nThe results from both the human and automatic evaluations indicate that the dataset is properly constructed and adequately reflects the multilingual nature of the tasks. The human post-editing process ensured that the translated problems maintained their semantic integrity, while the automatic evaluation confirmed that the translations preserved the necessary linguistic structure for multilingual reasoning. Overall, the quality assurance measures we implemented guarantee that the MMWP benchmark is reliable and effective in evaluating multilingual mathematical reasoning across diverse languages and resource levels."}, {"title": "Complete Experimental Results", "content": "Evaluation Results on MMWP\nThe complete experimental results on MMWP are shown in Table 7, 8 and 9. These tables present Mono-SFT, Multi-SFT, and Leveraging External Tools or Models category comparison baselines.\nEvaluation Results on MGSM and MMWP\nThe complete experimental results on MGSM and MSVAMP are shown in Table 10, and 11.\nEvaluation Results on XNLI and X-CSQA\nThe complete experimental results on XNLI and X-CSQA are shown in Table 10, and 11."}, {"title": "Analysis Experiments Details", "content": "This section outlines the experimental implementation for the various analysis experiments presented in the main text.\nAblation of Two-Stage Training\nTo demonstrate the necessity of the proposed two-stage instruction tuning approach", "process": "the Language-Align stage and the Task-Transfer stage. This study is conducted on the mathematical reasoning datasets MGSM and MSVAMP to assess the impact of each stage's removal on reasoning performance across all languages. The detailed results are presented in Tables 14 and 15 for MGSM and MSVAMP", "w/o\" indicates the absence of the specific stage.\nWhen the Language-Align stage was removed, we fine-tuned both the language-alignment layer and the LLM using only English reasoning data. This resulted in a noticeable decline in performance in low-resource languages, such as a 10.2% performance drop in Bengali (Bn), 13.2% performance drop in Thai (Th), 21.2% performance drop in Telugu (Te), and 7.6% performance drop in Swahili (Sw) on the MGSM dataset and a 14.4% performance drop in Bengali (Bn), 15.7% performance drop in Swahili (Sw) and 19.8% performance drop in Thai (Th) on the MSVAMP dataset. This decline highlights the importance of using code-switched multilingual input as a warm-up strategy before directly applying English-only instruction data. By incorporating this stage, LinguaLIFT can better leverage the multilingual model's representation space, which contains valuable information from low-resource languages.\nWhen the Task-Transfer stage was removed, we fine-tuned the language alignment layer using only code-switched translation data without updating the LLM's parameters. After ablating the Task-Transfer stage, the models couldn't complete reasoning tasks across all languages, resulting in an average accuracy of just 8.49% on the MGSM dataset and 9.74% on the MSVAMP dataset. This demonstrates that task-specific instruction tuning is critical for enabling the LLM to perform task-solving operations effectively.\nThese results underscore the complementary nature of the two stages": "the Language-Align stage enhances the cross-lingual transferability within models", "w/o\" indicates the absence of a specific operation.\nThe results reveal that performance on high-resource languages suffers when the LLM is frozen during the Language-Align stage. We attribute this decline to catastrophic forgetting, where the initial training on low-resource languages interferes with the model's ability to retain its high-resource language capabilities. Specifically, the average performance on high-resource languages decreases by 3.5% on the MGSM dataset and 2.6% on the MSVAMP dataset.\nWhen the LLM training is ablated in the Task-Transfer stage, performance significantly deteriorates across all languages. This indicates that relying solely on the language alignment layer is insufficient for learning high-level reasoning tasks and effectively transferring knowledge to low-resource languages. The average performance across all languages drops to 35.4% on the MGSM dataset and 37.8% on the MSVAMP dataset.\nFurthermore, freezing the language alignment layer during the Task-Transfer stage results in a decline in performance on low-resource languages. This suggests that the alignment learned in the language alignment layer is disrupted during continuous training, as the average performance on low-resource languages drops by 9.1% on the MGSM dataset and 5.3% on the MSVAMP dataset.\nThese findings emphasize the benefit of training the language alignment layer first, followed by the LLM training, to improve performance on both low-resource and high-resource languages.\nAnalysis of Code-Switch Tuning\nIn our experiment, we aimed to scrutinize the influence of the part-of-speech (POS) of the substitution words on the multilingual reasoning performance in the context of code-switched tuning. This analysis is essential as it provides insights into the role of various word categories and their combinations in the effectiveness of language model tuning and reasoning capabilities.\nOur initial step involved an examination of the distribution of part-of-speech (POS) tags in the reasoning queries, the results of which are tabulated in Table 18. This analysis facilitated a preliminary understanding of the prevalence of different word classes within the reasoning queries, setting the stage for further investigation into their impact on multilingual reasoning.\nSubsequently, we categorized the POS combinations into three distinct groups for a more granular analysis. The first group comprised individual POS categories, specifically Verbs, Adpositions, and Pronouns. These categories were selected due to their fundamental role in sentence construction and their potential to affect the meaning and structure of statements significantly.\nThe second group consisted of syntactic function combinations, including Verb+Adverb, Adjective+Adverb, and Pronoun+Auxiliary+Conjunction combinations. These combinations were chosen based on their syntactic roles and capacity to influence sentence structure and meaning. They are integral to creating complex sentence structures and are often pivotal in conveying nuanced meanings.\nThe third group focused on key syntactic structures": "Subject-Verb (Noun+Verb) and Prepositional Phrases (Adposition+Noun). These structures were selected due to their central role in sentence construction and their potential to encapsulate core semantic information. They form the backbone of many sentence structures and play a crucial role in interpreting a sentence's meaning.\nBy analyzing the impact of these POS categories and combinations on the performance of code-switched tuning, we aim to provide a comprehensive understanding of the interplay between syntax and semantics in the context of multilingual reasoning. This analysis will shed light on the importance of different word classes and their combinations in improving the efficacy of multilingual language models.\nFurther experiments were conducted to investigate the effects of replacing different types of words (e.g., nouns, verbs, prepositions, etc.) on the multilingual reasoning capabilities of LinguaLIFT models. As indicated in Figure 6, we observed that replacing nouns significantly impacted the model's reasoning performance within the individual POS categories. This finding leads to the hypothesis that reasoning in query sentences may be closely tied to noun-based understanding, possibly due to the central role nouns play in representing key entities and concepts in the task.\\"}]}