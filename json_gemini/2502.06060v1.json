{"title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning", "authors": ["Bidipta Sarkar", "C. Karen Liu", "Warren Xia", "Dorsa Sadigh"], "abstract": "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on AMONG US, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/.", "sections": [{"title": "1 INTRODUCTION", "content": "A longstanding goal of multi-agent artificial intelligence is the development of independent agents that can communicate using a shared language. Communication is especially necessary in \u201cpartially observable\u201d settings, where each agent only has a limited view of the world and therefore benefits from sharing knowledge with other agents to achieve its goal. In particular, \u201csocial deduction\u201d games are settings where each agent's goal is to deduce information about the environment by communicating with other agents \u2013 requiring each player to learn how to parse messages from other players while effectively sharing important information needed for game completion.\nIn this work, we study the hidden-role game of AMONG US [18] as a specific instance of a challenging social deduction game to investigate the importance of communication, illustrated in Fig. 1. Hidden-role games [4, 19] are a class of environments where players are split into an uninformed majority and a smaller informed hidden subteam, which we refer to as crewmates and imposters respectively. These two teams are adversaries, resulting in a zero-sum game, where the goal of the crewmates is to deduce the identity of imposters to vote them out. Unlike other popular hidden role games such as the game of Mafia [2], where statements from players are unfalsifiable, AMONG US is based in a 2D embodied environment, allowing discussions and intuitions to be grounded in specific observations. In the game, crewmates try to complete an assigned set of tasks scattered across the environment while imposters try to kill all crewmates. If a player reports the corpse of an eliminated crewmate - killed by an imposter \u2013 the game moves to a discussion phase with a free-form chat followed by a voting period, where all players vote to eject a suspected imposter. For crewmates, success in the discussion phase would mean correctly voting out the imposter, while success for imposters means avoiding suspicion from the crewmates to continue staying in the game as long as possible. This highlights the importance of communication during the discussion phase as crewmates need to learn to effectively utilize the discussion phase to vote out imposters in an adversarial setting.\nFor the rest of this paper, we study the game of AMONG US from the perspective of crewmates attempting to perform tasks, identify imposters, and win the game.\nIn multi-agent environments, an effective technique for training strong cooperative and competitive agents is multi-agent reinforcement learning (MARL), which enables artificial agents to achieve superhuman levels of performance in competitive games such as StarCraft [36], and cooperative games such as Overcooked [5, 31] and Hanabi [13]. However, in settings where communication in natural language is necessary, existing MARL techniques often struggle as they require large datasets of task-specific human communication data to perform on-par with humans [8]. This fundamentally limits the agents' ability to communicate at human-level and is not practical for learning in settings where these datasets do not readily exist. The game of AMONG US falls into this category, where communication is necessary to reason and progress in the game. Therefore, we would like to find an approach that learns to communicate effectively and convincingly without requiring large amounts of task-specific human data. However, the major challenge in learning to communicate without access to large amounts of human data is that novice agents do not have a strong signal for understanding the helpfulness of the messages they send (speaking) or for learning the meaning of messages from other players (listening). In particular, the sparse reward signal the agents receive when winning the game is not informative enough to reinforce high-quality discussions between agents. Our key insight is that we can leverage the agents' instrumental goal of predicting useful information about the world \u2013 e.g., the identity of imposters as a dense reward to provide a higher-quality signal that can enable more informative communication during the discussion phase and potentially higher performance policies.\nWe propose an approach that rewards a message generated during the discussion phase based on how the other crewmates' beliefs on the identity of the imposter changes. Each crewmate wants to send messages that help other crewmates be more certain about the true identity of the imposter. However, this only explains how to learn to \"speak\" assuming that the other agents can appropriately update their belief about the world given a message. We also need to ensure the agents know how to \"listen\" and update beliefs appropriately. To encourage this, we additionally add an imposter prediction signal to guide the agent's learning to predict the true identity of the imposter after each message. By training agents to speak and listen effectively, we enable the agents to self-improve their discussion abilities. Further, to encourage listening and speaking in natural language during the discussion phase of the game, we tap into the power of large language models (LLMs), unspecialized models trained with large amounts of human language data. Specifically, we initialize crewmates as LLMs capable of communicating via natural language. Recent advances in foundation models have demonstrated some reasoning abilities [3, 27], including understanding social scenarios [20], but even the strongest language models today are weak at self-critiquing [34] or performing theory of mind reasoning [33], limiting their ability to improve their listening skills based on their own feedback. However, by training LLMs within our proposed framework of encouraging listening and speaking with auxiliary dense rewards for helping other crewmates vote out the correct imposter, we overcome this limitation, enabling the self-improvement of these models over time.\nTo evaluate our framework, we analyze the success rate of crewmates against both pretrained and adaptive imposters, and find that crewmates form a robust communication strategy. We find that our technique results in emergent behavior commonly found in real games of AMONG US between humans, such as directly accusing"}, {"title": "2 RELATED WORK", "content": "In this section, we review related work on emergent communication, prior works that use language models as agents in embodied settings, and past works integrating language models with RL.\nEmergent Communication. A major topic in MARL is emergent communication between agents, especially in the context of reference games and repeated reference games, where a speaker knows the ground-truth answer to a question (e.g., a specific image out of a set of images that needs to be referred to). Then, the speaker needs to communicate to the listener, who later needs to choose the item being referenced either over one or repeated interactions. Prior work has shown that humans tend to quickly adapt to such tasks [26], naturally using theory of mind reasoning to determine the intents of speakers [9]. Further, Hawkins et al. [12] showed that language models can also learn to adapt to human conventions via continual learning. Without using human natural language data, Lazaridou et al. [23] and Havrylov and Titov [11] use symbolic cheap-talk signals to solve referential games. Our framework of social deduction games, however, is more challenging as each agent does not know the ground truth answer, so teams must communicate to collectively learn the answer. Therefore, our domain does not have as clear of a distinction between \"speakers\" who have knowledge and \"listeners\u201d who need to gain answers as agents in social deduction games must play both roles.\nLanguage Models Agents. A large body of prior work use LLMs' access to internet scale data for task planning and decision making. In robotics, prior works explore how language models can be used to plan out a sequence of high-level primitives given an instruction in natural language [1, 17, 24]. In a virtual gaming setting, Park et al. [29] uses ChatGPT to simulate members of a small virtual town. Although there is no specific task or mechanism for \"training\" these agents, they demonstrate the use of a long-term memory stream to store memories beyond the context length of the language models, enabling the formation of social networks. This technique of having external memory has later been used to learn \"skills\" in a single-player environment [38] and for coordination in multi-agent environments [10]. These works demonstrate that language models are capable of controlling agents in a wide range of settings, which is key to our motivation to directly use language models as a strong starting point for agents operating in more challenging environments such as social deduction games.\nReinforcement Learning with Foundation Models. Some works also combine language models with reinforcement learning. Cicero [8] is an AI for the game of Diplomacy that uses a dialogue-conditional action model from human actions and trains a dialogue-free model using RL to choose actions. Cicero uses an \"intent\" embedding to connect the dialogue generation and strategic reasoning components. This allows Cicero to communicate with other agents in a way that feels natural to other players, but it prevents the RL model from directly controlling the generated messages, potentially limiting improvements in message quality. Another drawback is that this technique requires a large number of human demonstrations, which may be impractical in many settings.\nFoundation models have been effective in both providing rewards and as a base model for policies. Hu and Sadigh [14] and Kwon et al. [21] use language models as reward signals to train a separate network to follow a specific coordination strategy. We similarly use the LLM to provide denser rewards during the discussion phase, but we train the LLM itself instead of a separate policy.\nOutside of the embodied setting, reinforcement learning has also been key to improving the chat capabilities of LLMs. Ouyang et al. [28] demonstrates the effectiveness of reinforcement learning from human feedback (RLHF), where a reward model is trained using human feedback and an LLM is fine-tuned using a modification of the PPO algorithm to improve its performance. Yuan et al. [39] extends this by allowing the LLM to be its own reward model and generate its own data for self-improvement, similar to how we use the LLM's own change in beliefs as a reward signal. However, a crucial difference is that our reward model remains grounded in an environment by design due to the imposter prediction training signal. This means that we do not need to rely on the ability of pretrained LLMs to critique their own generations, enabling us to use smaller language models and correct logical errors over time."}, {"title": "3 PRELIMINARIES", "content": "We model social deduction games, such as AMONG US, as a variant of the partially observable Markov game (POMG) [25] that includes a question whose answer must be deduced through interacting with players and the rest of the environment. Our modified POMG can be described by a tuple (n, S, A, P, r, O, \u03b3, Q, q), where n is the number of players, S is the joint (hidden) state space and A is the joint action space. The transition function P : S \u00d7 A \u00d7 S \u2192 [0, 1], is the probability of reaching a state given the current state and joint action. The reward function r : S \u2192 Rn, gives a real value reward for each state transition to each player, and \u03b3 is the reward discount. The observation function, O : S \u2192 On, generates the player-specific observations from the state.\nOur POMG has additional terms for the task of social deduction, which are the set of all possible answers to the deduction problem QA and the correct answer q \u2208 Q. In social deduction games, agents will be given opportunities to answer the question as a literal action (i.e. voting in AMONG US or choosing the correct object in reference games), and at those steps the correct action to take is q.\nThe trajectory up to time t is defined as a sequence of joint observations and actions: \u03c4t = (o0, a0,..., at\u22121, ot). An individual player only experiences their own action-observation history (AOH), which is defined for player i as \u03c4i = (oi, a0i, ..., ati\u22121, oti), and they follow a stochastic policy \u03c0i (ai|\u03c4i). In the game of AMONG Us, the AOH consists of past observations supplied by the environment, past embodied actions, and all prior discussions with the other players.\nLanguage Models. Language models are trained to model the probability of a sequence of discrete tokens, where each token represents a string of natural language. For a sequence of tokens,\nW = {w0, w1,..., wk}, the probability of the sequence being generated is p(W) = \\prod_{j=0}^{k} P(w_j|w_{<j}), so causal language models predict the distribution of the next token conditioned on all prior tokens.\nOur AMONG US environment is designed such that each observation at time step t is a sequence of tokens ot = Wt = {wo, W1,..., wk} and each action at time step t is a single token at = wt, allowing us to use language models as the policy for each agent. The AOH is a sequence of tokens, so language models can sample the next action by predicting the next token in the sequence, constrained to the set of legal actions at that timestep.\nIn this work, we use the RWKV language model [30], a recurrent language model based on a linear attention mechanism, as the pretrained foundation model. We choose RWKV over more common transformer-based models [35], because the recurrent formulation allows us to generate RL trajectories with a constant time and space complexity per token, and RWKV enables unbounded-context training using truncated backpropagation through time. This is especially important since AMONG US trajectories often reach tens of thousands of tokens in length per player, which would require significantly more compute for classic attention-based models. Empirically, RWKV has also performed on-par with transformer-based models, especially in decision-making tasks [7] and long-context understanding [15], making it the ideal choice for this study."}, {"title": "4 THE GAME OF AMONG US", "content": "In this section, we describe the key design decisions of our implementation of the hidden-role game of AMONG US. Our goal is to create an environment where agents can ground their discussion based on evidence in the environment. A more complete description of the game is in Appendix A.\nRole Assignment. At the start of the game, each player is either assigned as an imposter or a crewmate. The crewmates are not informed of the identities of the other players, but all imposters are informed of the identities of the other players at the start.\nIn our setting, we assign one player to be the imposter and the other n \u2212 1 players as crewmates. The crewmates are assigned a set of N tasks, scattered across the environment. As an example, N = 3 in the example in Fig. 1.\nGameplay Phase. During the gameplay phase, players simultaneously move in an embodied environment, receiving observations from the environment and taking actions, as illustrated in Fig. 2. Players freely move around a W \u00d7 H grid of rooms during the gameplay phase, receiving new observations or at each time step. All agents can move between adjacent rooms by choosing ago to x, where x is a cardinal direction, or they can simply wait in the room by choosing await. Crewmates can complete tasks in their current room by choosing atask, but they are unable to observe the environment for Ntask time time steps, i.e., they will not be able to observe if a crewmate is being killed by an imposter while performing a task. Note that tasks are indistinguishable from one another, so we do not have different actions for different tasks. Imposters can kill crewmates by choosing akill, j where j is a crewmate in the same room as them, but they have to wait Ncooldown time steps between killing crewmates. Finally, crewmates can report dead bodies in their room by choosing areport, j, where j is the corpse of player j, which initiates the discussion phase."}, {"title": "5 TRAINING LLM CREWMATES IN AMONG US", "content": "By defining an environment that only interfaces with players through natural language, we can directly use a language model as the policy \u03c0i (ai|\u03c4i) of an agent i. The action-observation histories of our agents \u03c4i are just strings of natural language, and new observations and actions can simply be appended to the end of the strings. Furthermore, when taking actions ai, the outputs of the language model can be constrained to be one of the legal actions provided by the environment at each timestep. Following this procedure, we construct an agent using a pretrained RWKV language model, which we define as policy \u03c0RWKV.\nAlthough the environment is designed to interface nicely with language models, we find that \u03c0RWKV struggles to reason as crewmates in a zero-shot fashion in AMONG US, with models frequently voting to eject the wrong players. In this section, we describe our procedure for improving the performance of crewmates by enabling them to self-critique and use these scores to improve dialogue generation.\nThe first two subsections describe how to improve the performance of an individual learning crewmate, first describing a reinforcement learning procedure and then describing how to enhance communication by learning to listen and speak. The third subsection describes how to train the team of crewmates to be robust to adaptive imposters and different policies within the crewmate population.\nTo train a language model to take more effective actions without expert demonstrations, we can turn to reinforcement learning. Since AMONG US already provides rewards for winning, we can directly optimize this to produce a model \u03c0RL that minimizes the following loss:\n\\mathcal{L}_{RL} (\\pi) = - \\mathbb{E}_{\\tau} \\Big[ R \\frac{\\pi(a_t|\\tau_t)}{\\pi_{RWKV}(a_t)} - \\lambda_{NL} D_{KL} (\\pi || \\pi_{RWKV} ) \\Big] \nwhere \u03a0 represents the joint policy that has a controlling agent i, and \u03bbNL is a hyperparameter controlling the strength of a soft KL constraint regularizing trained models to the base LLM to prevent discussions from moving out of natural language [28]. Note that the only reward signal is the sparse reward received at the end of the game along with additional rewards for completing tasks. In particular, there is very little signal for the effectiveness of its messages during discussions, which makes utilizing communication very difficult with just RL in practice. This sparse signal also makes identifying the imposter difficult in the multi-agent setting, because voting correctly may still result in a loss and voting incorrectly could result in a win if a plurality of agents vote for the imposter.\nTo improve beyond the RL baseline, we can take advantage of the social deduction component of the game. In particular, each agent's belief in choosing the correct answer q \u2208 Q will provide a stronger signal for learning the core components of the game and the means of communication relative to the RL baseline.\nIn this subsection, we discuss the key contributions of this work. Specifically, we highlight new loss terms to enhance both listening and speaking abilities, enabling crewmates to better utilize discussions.\nTo effectively discuss the game of AMONG US, crewmates need to understand who the imposter is given their past observations and the past messages. This prediction task can act as an auxiliary task that can guide the discussion phase to be more grounded and meaningful.\nWe directly train crewmates to improve their reasoning over imposters using the environment's ground truth answer for the identity of the imposter. Specifically, we use the timesteps when the environment directly surveys the players for their beliefs over the imposters, which occurs between discussion messages, as the training signal. Note that this training signal does not specifically require human demonstration data; agents can learn to understand observations and messages from other players using any rollout buffer.\nFor every living crewmate, if they are asked to provide their beliefs regarding the identity of the imposter at timestep t, the listening loss for that timestep is\n\\mathcal{L}_{L} (\\pi, \\tau_t^i) = - log \\pi(q|\\tau_t^i)\nwhere q = avote, j is the action representing choosing the correct imposter j, and \u03c4ti is the AOH until timestep t, which may include prior discussions.\nAt the very start of the discussion phase, agents need to reflect on the probabilities of other agents being the imposter based on their observations during the gameplay phase. For instance, if a crewmate directly witnesses a murder, they should be very certain that the murderer is the imposter; our listening loss uses this signal to increase their certainty over the imposter.\nBy framing the task of identifying imposters using messages and observations as a supervised learning problem, agents learn to understand the meaning of messages, enabling them to vote out the correct imposter. Using this loss term, we can define two new policies. We can directly incorporate the listening loss into the RL policy, giving us the policy \u03c0RL+L that optimizes\n\\mathcal{L}_{RL+L} (\\pi) = \\mathcal{L}_{RL} (\\pi) + \\mathbb{E}_{\\tau} \\Big[ \\lambda_L \\mathcal{L}_{L} (\\pi, \\tau_t^i) \\Big]\nwhere \u03bbL is a hyperparameter controlling the strength of the listening loss and is only nonzero on timesteps when the crewmates are asked to predict the identity of the imposter. This enables the model to optimize actions while improving its ability to identify imposters.\nWe can also define a purely listening policy, \u03c0L that incorporates the listening loss without an RL component, therefore optimizing\n\\mathcal{L}_{L_{only}} (\\pi) = \\mathbb{E}_{\\tau_{rand}} \\Big[ \\lambda_L \\mathcal{L}_{L} (\\pi, \\tau_t^i) \\Big]\nwhere \u03a0rand is a joint policy that uses TRWKV for discussions and chooses gameplay actions uniformly at random.\nSo far, we have developed a policy that can learn to take effective actions in the environment with RL, and can update beliefs based on discussion messages. Now suppose that an agent is partnered with expert crewmates who already know how to parse messages from other players. How can this agent learn to construct helpful messages when it is their turn to speak?\nAlthough our use of a supervised imposter prediction loss allows agents to learn how to interpret messages from other agents in the previous subsection, we cannot directly apply the same idea to learning how to speak as there is no ground truth notion of effective messages. We instead improve the agents' discussion abilities using reinforcement learning. Specifically, we grant rewards to the speaking agent based on the change in living crewmates' beliefs on the true imposter after each message. Formally, let Bt be the sum of all living crewmates' beliefs,\nB_t = \\sum_{k \\in C_t} \\pi_k (q|\\tau_t^i)\nwhere the q represents voting out the correct imposter, and Ct is the set of all living crewmates at time t. If t' is the previous belief-querying timestep, then the reward for crewmate i, who just finished speaking, is r:\nr = B_t - B_{t'}.\nIntuitively, this reward models the causal effect of each message on the task of predicting the correct imposter. The most effective message that a crewmate could send would convince other crewmates to vote out the true imposter.\nUsing speaking and listening, we can train an agent \u03c0RL+S+L that minimizes the following loss:\n\\mathcal{L}_{RL+L+S} (\\pi) = \\mathcal{L}_{RL+L} (\\pi) - \\mathbb{E}_{\\tau} \\Big[ \\lambda_S \u03b3r \\Big]\nAs a team zero-sum game, we want our trained crewmates to work well against a wide range of imposters. To do so, we employ an iterated self-play algorithm, where crewmates and imposters train against earlier iterations of their adversary's policy. We train imposters to learn to mislead crewmates into voting out other agents, so we keep the RL loss and invert the speaking loss, minimizing the following:\n\\mathcal{L}_{imp} (\\pi) = \\mathcal{L}_{RL} (\\pi) + \\mathbb{E}_{\\tau} \\Big[ \\lambda_S \u03b3r \\Big]\nAs the inner optimization loop, we use independent PPO [16, 32] with shared networks for policy and value functions and the Schedule Free AdamW optimizer [6].\nWe also want our crewmates to be robust to different partners who also act reasonably. Therefore, we always set one crewmate to be frozen to the listening policy \u03c0L when forming the joint policy \u03a0, following the N-Agent Ad hoc teamwork setting [37] instead of assuming a homogeneous population. This change also ensures that crewmates cannot simply determine the identity of the imposter by forming an arbitrary convention and voting out any agent who violates that convention.\nFinally, we want our agents to be robust to different environment configurations. We randomize multiple environment parameters while training: choosing between three different layouts of the environment (1 \u00d7 3, 2 \u00d7 2, and 2 \u00d7 3 grids), and randomizing the number of tasks assigned to each crewmate to either 3, 4, or 5. We only train on configurations where there are 4 crewmates and 1 imposter, but we report generalization results when playing with different numbers of crewmates.\nTo stabilize training, we also include the following world modeling loss to each model's loss function:\n\\mathcal{L}_{WM} (\\pi) = - \\beta \\sum_{t} log \\pi(o_{t+1} / \\tau_t^i, a_t)\nwhere \u03b2\u03c9 is the relative strength of the world modeling loss. Although this loss does not directly contribute to improving task success, it subtly helps improve the model's performance. In particular, as a recurrent model, RWKV benefits from this world modeling loss as it ensures that features are remembered throughout training. Furthermore, the world modeling loss prevents the model from placing too much weight on action tokens, which would cause models to output action tokens even during regular discussion sections."}, {"title": "6 RESULTS", "content": "In this section, we analyze the quality of our trained crewmates. We inspect the importance of different design decisions regarding the training of crewmates by ablating over the components of the loss function in Eq. (7): RL, listening, and speaking. We determine the importance of discussions in the game by measuring the equilibrium win rates of crewmates against imposters and analyze emergent behaviors in discussions. Finally, we highlight some common failure modes we observed during training and how they are mitigated in our final training algorithms.\nFor this set of experiments, we analyze the performance of the crewmates from the first iteration of the self-play algorithm. We conduct all experiments using the 1.5B RWKV model, because we find diminishing returns at higher parameter counts from our experiments on base models (see Appendix B for more details). We report the win rates of each policy in the base environment when keeping the imposter and one crewmate fixed to \u03c0L in Fig. 3.\nModel Evaluations. The simplest baselines are direct evaluations of the base model. We find that the 1.5B RWKV model struggles to win in the game, with the larger 7B parameter model performing slightly better as both win less than 20% of the time in the base environment.\nJust training with RL significantly boosts the performance relative to the base models, even significantly outperforming the 7B parameter model. However, we still find that RL without the additional listening loss struggles to reason about the identity of imposters. Even when warm-starting the RL policy from \u03c0L, we find that it quickly loses the ability to identify imposters, instead voting for any agent with equal probability. When we instead only trained with listening \u2013 using the loss LL only \u2013 the model, \u03c0L, does not know which actions are effective or how to discuss details about the environment, but it is an effective baseline due to the fact that predicting the identity of the imposter is valuable in AMONG US.\nWhen combining RL and the listening loss, we find success rates again increase dramatically, with further improvements when adding our denser speaking rewards, as agents can now differentiate between helpful and unhelpful messages when training. We ultimately find that our full model achieves twice the win rate of the RL-only baseline in the base environment. Note that the difference in scores when adding the additional speaking term is relatively small. Even without the explicit speaking reward, the language model produces coherent messages, often sharing their current suspicions during discussion rounds, thus benefiting from discussion even without additional rewards. This is an interesting emergent behavior as it shows that speaking is indirectly improved by training the model to listen better.\nWe present the win rate of crewmates against imposters across different environment configurations in Fig. 4, and see that the trends between models observed in the base environment generally persist across configurations. We find that the shape of the environment has little effect on the win rates of crewmates, with smaller environments generally being easier since it is harder for imposters to kill crewmates without witnesses. We see a general decline in performance across all models when increasing the number of tasks, because this makes it harder to win the game by completing tasks instead of voting out the imposter. Finally, we see a significant increase in win rates as the number of crewmates increase, which we expect since the crewmates can still recover from incorrectly voting out a crewmate.\nWe do not observe significant deviations from the expected trend lines in settings that were out of the training distribution, demonstrating how the language models can extrapolate their behaviors to unseen deviations in the configuration.\nWe find a major difference between the message patterns of the base RWKV model and those from \u03c0RL+L+S. Most messages from the base RWKV model are often unfocused, hallucinating a wider context to the game and role-playing a crewmate. Meanwhile, crewmates using \u03c0RL+L+S often directly accuse the imposter or otherwise name the imposter in their messages. In general, we find that naming an agent makes it more likely for other agents to vote against them. Furthermore, crewmates share messages that resemble environment observations that helped them judge the identity of the imposter. For instance, a crewmate may say \"Player Green is leaving Room (0,1)\" when the body is in Room (0,1) to indicate that Player Green was running away from the dead body, which is often correlated with being the imposter. However, the crewmates sometimes tell lies in their messages - just like humans often do when playing AMONG US. In particular, they often simply make up evidence and state whatever is most convincing to other agents to get enough votes to eject the correct imposter. Representative behavior samples are provided in Appendix C.\nWhen training against frozen imposters, crewmates could come up with simple strategies that result in high win rates but are easy to overcome with a more intelligent imposter. We therefore run multiple iterations of self-play to investigate whether crewmates can use discussions even against imposters that can evolve to their policies, which we illustrate in Fig. 5. Note that in exploitability curves, we would like to see convergence upon a narrow interval for both the upper and lower bounds; a weak strategy can be easily exploited at each iteration and would therefore have both lines stay far apart and converge slowly.\nWe find that the crewmates' strategies are robust to imposters trained in an adversarial fashion. In particular, we see that crewmate scores converge after only a few iterations; depending on the seed, the win rate converges to between 0.51 and 0.56 on the base environment. In fact, even the crewmates that only trained on the base model imposter are relatively strong, as can be seen by the large jump in the lower bound between iterations 0 and 1. This result implies that policies discovered by \u03c0RL+L+S are very robust since even facing adversarially trained imposters does not cause a significant performance drop.\nQualitatively, we observe that imposters attempt to shift blame to other players by counter-accusing another crewmate. In particular, they mimic the discussion patterns of crewmates, and the crewmates sometimes fall for this deception. Crewmates who have not witnessed the murder tend to support claims made by other players, causing them to sometimes help the imposter. Interestingly, we still see similar behavior to a smaller level in the base model imposters when playing against strong crewmates. This emergent behavior can likely be attributed to the in-context learning capabilities of language models, which would allow the imposter to mimic the speech of crewmates who spoke beforehand."}, {"title": "6.3 Failure Modes", "content": "Throughout our experimentation, we encountered various failure modes that we tackled in our final training algorithms. Specifically, discussions tended to leave natural language and generally degenerate without careful consideration from the training algorithm.\nFirst, we observed that the soft KL constraint commonly used in RLHF [28] required careful tuning to keep language generations in English. When this constraint is weighted too low, all of our RL-trained models diverge from natural language after only a few iterations, causing it to output random tokens during discussions and stop improving in performance.\nWe also observed that allowing all crewmates to be trained simultaneously would lead to degenerate solutions. Sometimes the models learn a social convention where they simply do not speak during the discussion phase. Specifically, models would output newlines when it is their turn to speak instead of actually speaking. In this case, only the imposter would speak and all the agents would just vote the speaker out. The models would also learn to just wait in the starting room instead of moving around, allowing them to witness the murder or vote out the person who moves out of the room. These strategies are degenerate solutions since they would not work if the imposter was aware of their strategy or if not all the crewmates shared the same strategy, but these strategies would lead to nearly perfect win rates during the first iteration of self-play. The fix to this issue was to \"freeze\" one crewmate to not learn and therefore not follow changes in strategies.\nThe final failure mode we observed was using action tokens in discussions instead of natural language. Specifically, the RL-trained models would learn to take actions by explicitly choosing action tokens, but this gives action tokens a higher probability to be chosen overall, even during discussion phases. We observed that the best way to counteract this effect was to introduce the world modeling loss from Eq. (9). This loss ensured that the model preserved its language modeling abilities and had the side effect of helping the models match the patterns it experienced in observations within its own discussions, which would help independent agents understand the intentions of our models."}, {"title": "7 DISCUSSION", "content": "Summary. We introduce a technique to self-improve the discussion ability of an LLM in social deduction games and show how it enables agents to communicate effectively in the game of AMONG US. We demonstrate that, despite having weak base models, our agents learn to speak effectively and extract information from discussion messages. We also find that our agents are robust to adversarially trained imposters, who, despite attempting to sabotage the discussion, are unable to break the crewmates' coordination during discussions. Our technique ultimately shows that self-improving discussions in multi-agent settings does not require task-specific human data, unlocking the possibility for multi-agent communication with language models in novel tasks.\nA key limitation of our approach is that our scene prediction technique is task-dependent. In AMONG Us, there is a natural connection between the discussion and trying to predict the identity of the imposter, and a similar structure applies to a wide range of social deduction games and real-world settings. An interesting future direction would be to allow agents to identify which aspects of the scene are relevant to a specific task instead of manually specifying it. Please refer to Appendix D for more analysis on the broader impacts of our work.\nWe also note that crewmates are not always truthful in their discussions, opting instead to make the most convincing statements. We consider this behavior to be potentially dangerous outside of our sandboxed setting of Among Us, so we believe that optimizing for truthfulness is an important future direction."}, {"title": "A ENVIRONMENT DESIGN", "content": "Gameplay Phase. The main gameplay loop consists of players navigating a 2D environment, consisting of a W \u00d7 H grid of rooms. The location of the agent is just the room number; movement within the room takes no time. All agents can move between rooms based on a set speed of movement, Ntravel.\nCrewmates use this time to complete \"tasks\" around the map. In the original game of AMONG Us, tasks involve completing minigames that require a player's attention, such as solving a maze, preventing them from observing the environment around them. In our implementation, we simplify the notion of tasks, and require the crewmates complete tasks by only staying in the room containing the task for a specified amount of time. However, similar to the original game"}]}