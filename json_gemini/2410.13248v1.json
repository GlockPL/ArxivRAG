{"title": "Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation", "authors": ["Ryotaro Shimizu", "Takashi Wada", "Yu Wang", "Johannes Kruse", "Sean O'Brien", "Sai HtaungKham", "Linxin Song", "Yuya Yoshikawa", "Yuki Saito", "Fugee Tsung", "Masayuki Goto", "Julian McAuley"], "abstract": "Recent research on explainable recommendation generally frames the task as a standard text generation problem, and evaluates models simply based on the textual similarity between the predicted and ground-truth explanations. However, this approach fails to consider one crucial aspect of the systems: whether their outputs accurately reflect the users' (post-purchase) sentiments, i.e., whether and why they would like and/or dislike the recommended items. To shed light on this issue, we introduce new datasets and evaluation methods that focus on the users' sentiments. Specifically, we construct the datasets by explicitly extracting users' positive and negative opinions from their post-purchase reviews using an LLM, and propose to evaluate systems based on whether the generated explanations 1) align well with the users' sentiments, and 2) accurately identify both positive and negative opinions of users on the target items. We benchmark several recent models on our datasets and demonstrate that achieving strong performance on existing metrics does not ensure that the generated explanations align well with the users' sentiments. Lastly, we find that existing models can provide more sentiment-aware explanations when the users' (predicted) ratings for the target items are directly fed into the models as input. We will release our code and datasets upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Recently, there has been a growing interest in developing explainable recommendation systems, which not only recommend items to target users, but also provide explanations as to why they would like the recommended items [4, 5, 28, 41, 42, 60, 61]. To achieve this goal, most previous studies automatically extract users' main opinions about items from their post-purchase reviews, which they treat as ground-truth explanations, and train a model that generates the extracted texts given users and items as input [6, 19-22, 39, 53]. However, a majority of existing datasets are constructed using rudimentary algorithms, and they often discard users' important opinions and sentiments [3]. For instance, in the example presented in the first row of Table 1, only a positive opinion is extracted from the original review, which also describes the negative aspects of the item. Training models on such noisy data will result in poor performance, motivating the need to create a more reliable dataset.\nAdditionally, another limitation of previous studies is that they perform evaluation largely based on the string matching or textual similarity (e.g., as measured BERTScore [56]) between the model's outputs and the sentences or features (keywords) extracted from the reviews. However, this approach cannot take into account whether the model accurately predicts the sentiments (positive or negative) of the original reviews. That is, a model can achieve good scores as long as it generates a lot of keywords even if they are mentioned with the wrong sentiment. We argue that considering sentiments is vital in evaluation, since users can mention mixed feelings about one item in the review and hence predicting keywords alone does not suffice to provide reliable and convincing explanations.\nTo address the aforementioned limitations, we introduce new datasets that focus on whether and why users like and/or dislike the recommended items. To construct such datasets, we utilize a large language model (LLM) to: (1) summarize a user review; and (2) extract a list of positive and negative opinions (features) separately from the summary, i.e., what the user likes and dislikes about an item. Table 1 shows two examples of the generated summaries and extracted features we treat the summaries as ground-truth explanations, and use the features to perform fine-grained evaluation. Specifically, we propose to evaluate models from two perspectives: whether the model's output (1) aligns well with the user's sentiment; and (2) correctly identifies the positive and negative features.\nWe evaluate several recent models using our datasets and evaluation methods, and find that strong models in existing metrics such as BERTScore do not necessarily capture the users' sentiments very well. Additionally, we find that existing models can generate more sentiment-aware explanations when we use the users' (predicted) ratings for the target items as additional input of the models.\nIn summary, our contributions are as follows:\n\u2022 We introduce new datasets for explainable recommendations that focus on the users' sentiments. Using an LLM, we construct reliable datasets that explicitly present the users' positive and negative opinions about items.\n\u2022 Using our datasets, we propose to evaluate models based on whether they accurately reflect the users' sentiments. We show that existing evaluation metrics are limited in measuring the sentiment alignment.\n\u2022 We find that the users' predicted ratings about items help models to generate more sentiment-aware explanations."}, {"title": "2 Related Work", "content": "Previous work on explainable recommendation extracts ground-truth explanations from either item descriptions [10] or user reviews [13, 15, 19, 20, 23, 45, 54], and we use the latter source to build"}, {"title": "3 Our Datasets", "content": "We construct new datasets for explainable recommendation from existing user review datasets. Our datasets are built in two steps: review summarization and positive/negative feature extraction.\nIn the review summarization step, we extract users' main opinions from reviews (and use them as ground-truth explanations) by prompting an LLM to explain what the user likes or dislikes about the target item, using the prompt shown in Table 2. For the LLM, we use GPT-40-mini [36]. To reduce the risk of hallucinations and keep the explanations concise, we restrict the model's output to 15 words or less, which roughly aligns with the average lengths of the explanations in existing datasets.\nIn the feature extraction step, we further prompt GPT-40-mini to extract users' positive and/or negative opinions about items (denoted as features) from the explanations generated in the previous step; Table 3 shows the prompt used in this step. This feature extraction task is known as aspect-based sentiment analysis [33, 38, 59] in natural language processing, and recent studies demonstrate that LLMs perform well on this task even in zero-shot or few-shot settings [14, 17, 58]. Table 1 shows two examples of the generated explanations and extracted features under the OURS column. Compared to the existing dataset shown next to OURS, our dataset summarizes the reviews more accurately and also extracts the features along with the associated sentiments (either positive or negative). This new format makes it possible to perform more fine-grained evaluation based on whether a model generates explanations with the correct sentiment, as we will explain in Section 4.\nWe construct our datasets from three existing user review datasets in different domains, namely Amazon [34], Yelp [55], and RateBeer [32]. Amazon contains user reviews for movies; Yelp for restaurants; and RateBeer for alcoholic drinks. We discard very short reviews that contain less than 15 words. Following previous work [18, 19], we also exclude the users/items which interact with the other items/users less than 20 times in the entire dataset. Table 4 shows the statistics of our datasets generated from each source. We use the latest and second latest interactions of each user as test and validation data, respectively, and use the rest as training data."}, {"title": "3.2 Dataset Quality Evaluation", "content": "While LLMs generally perform well on summarization [1, 7, 49, 57] and feature extraction [14, 17, 58], there is always a risk of hallucinations [11, 16, 25, 46]. An ideal solution to this problem is to verify the datasets by hiring human annotators, which however comes with a considerable annotation cost. Therefore, inspired by Chen et al. [3], we verify the dataset quality by utilizing GPT-40 [35] as an automated evaluator. To ensure its reliability, we also ask human annotators to assess a small portion of the datasets and measure the agreement between the humans and GPT-40.\nWe evaluate the LLM's outputs generated at the \"review summarization\" and \"positive/negative feature extraction\u201d steps, respectively. We verify the summarizations (which we use as the ground-truth explanations) based on the following metrics:\n\u2022 Factual hallucination (denoted as Factual): the percentage of the instances that do not contain any information that is not described or implied in the original reviews.\n\u2022 Contextual hallucination for positive/negative features (denoted as context-p/n): the percentage of the instances where the positive/negative features are mentioned with the correct (not the opposite) sentiment.\nFor instance, given the user review: I was fascinated by the romantic scenes, a summary should be labeled as factual hallucination if it says the user enjoys the thriller aspects; and as contextual hallucination if it says the user hates the romantic scenes.\nThen, we also verify the extracted positive and negative features based on the following metrics:\n\u2022 Factual hallucination for positive/negative features (denoted as factual-p/n): the percentage of the instances that do not include any positive/negative features that are not present in the explanations.\n\u2022 Completeness of positive/negative features (denoted as complete-p/n): the percentage of the instances that contain all positive/negative features mentioned in the explanations.\nFor instance, given the explanation: the user enjoyed the thriller aspect and great action, the model should flag factual hallucination"}, {"title": "4 Evaluation Methods", "content": "In previous work, models are evaluated based on standard textual similarity metrics, such as BLEU [37], ROUGE [26], and BERTScore [56]. Several studies [18, 19, 21, 22, 39] also look at whether the model's output contains a single-word feature included in the ground-truth explanation (e.g., great and humor in the existing data in Table 1). However, these evaluation metrics cannot consider whether the model predicts the correct sentiments of the original review. For example, if the ground-truth explanation is the user loves the movie's storyline but is dissatisfied with the visual quality, and the generated explanation is the user loves the visual quality but is dissatisfied with the movie's storyline, previous metrics assign unreasonably high scores to the generated explanation due to the significant overlap of words and phrases between the two texts, including the key features visual quality and movie's storyline. However, the generated explanation does not accurately describe what"}, {"title": "5 Evaluation Experiment", "content": "Using our proposed datasets, we benchmark recent models for explainable recommendation. We evaluate them using our proposed"}, {"title": "5.1 Models", "content": "We evaluate various models listed in Table 7, which include CER [39], ERRA [6], PETER [21], and PEPLER/PEPLER-D [22]. All models are based on transformers [50] with or without pre-training on monolingual data, and are trained to generate explanations given user and item IDs as input. Additionally, the models except for PEPLER-D also perform multi-task learning by predicting the users' ratings about the target items, which is found effective in enhancing the generation performance. Among these models, CER is trained with an auxiliary loss that minimizes the difference between the ratings predicted from the user and item IDs, and those from the hidden states of the explanation. The authors show that including this loss enhances the sentiment coherence between the predicted rating and explanation; e.g., the coherence is high if the model predicts a very high rating and generates a positive explanation such as the movie is great.\nWhile the method used in CER is sensible, we hypothesize that directly feeding the predicted rating into the model as input would make it generate more coherent explanations with the rating, since this way the model can predict every word in the explanation conditioned directly on the rating information via self-attention. In fact, this approach was also adopted by earlier models [18, 24] based on Gated Recurrent Unit (GRU) [9]. To verify our hypothesis, we propose to slightly modify PETER and PEPLER and let them directly take the predicted ratings as input. Figure 3 shows an overview of the modified version of PETER. We remove the multi-tasking"}, {"title": "5.2 Evaluation Metrics", "content": "We evaluate models using our evaluation metrics proposed in Section 4 (i.e., the sentiment-matching score and content similarity of positive/negative features). We also report the scores in several established metrics used in previous work [6, 18, 19, 21, 22, 39]. They are categorized into two groups, referred to as the text quality metric and explainability metric, respectively. The former evaluates the quality of the generated explanations, while the latter focuses on the quality of the predicted features in the explanations.\nFor the text quality metrics, we use BLEU [37], ROUGE [26], Unique Sentence Ratio (USR) [19], and BERTScore (BERT) [56]. BLEU and ROUGE measure the n-gram overlaps between the generated and ground-truth explanations, with BLEU focusing on precision and ROUGE on recall. We calculate BLEU with $n \\in \\{1,4\\}$ (B1 and B4) and ROUGE with $n \\in \\{1,2\\}$ (R1 and R2), following previous work [6, 21, 22]. USR calculates the number of unique sentences generated by the model, divided by the total number of the generated sentences; the higher this score is, the more diverse the explanations are."}, {"title": "6 Results and Analysis", "content": "Table 8 shows the results for each dataset based on our proposed evaluation metrics. It demonstrates that the models with our proposed modification (i.e., *-c/d-emb) outperform the original models and achieve the best scores on all datasets. These results verify our hypothesis that incorporating the users' predicted ratings as input is more effective than predicting the ratings as a subtask. We also find that the models that treat the ratings as discrete variables (i.e., *-d-emb) generally perform better than those that treat them as continuous ones (i.e., *-c-emb). This is likely because there is a non-linear relationship between the users' sentiments and ratings about items, as we showed in Figure 2 in Section 4. When we look at the results on each dataset, PEPLER-d-emb achieves the best scores"}, {"title": "6.2 Performance on Rating Prediction", "content": "As we mentioned in Section 5.1, we propose to pre-train a rating prediction model and use its predictions as additional input of PETER and PEPLER. On the other hand, the original models of PETER and PEPLER predict ratings as a subtask. Intuitively, training a model specifically for rating prediction would lead to better performance on this task, and that could be part of the reasons why our proposed method works well. To investigate this, we compare the rating prediction performance among these models, and the results are presented in Table 10. We compare the performance in two metrics: mean absolute error (MAE) and root mean square error (RMSE), both of which measure the distance between the"}, {"title": "6.3 Case Studies", "content": "In Table 13, we present two examples of the ground-truth and generated explanations by CER, ERRA, PETER, PETER-d-emb, PEPLER and PEPLER-d-emb, respectively. In the first instance, PETER correctly identifies two features character development and villains, but wrongly predicts them both as negative features despite character development being mentioned positively in the ground-truth explanation. On the other hand, both PETER-d-emb and PEPLER-d-emb successfully generate these features with the correct sentiments. In the second example, only PETER-d-emb identifies the positive and negative features (service and wait time, resp.) with the correct sentiments. These examples highlight the importance of considering the users' sentiments when we evaluate the quality of explanations. Our proposed datasets and metrics shed light on this problem, and open up a new research direction for explainable recommendation systems."}, {"title": "7 Conclusion", "content": "This paper introduced new datasets for explainable recommendations that focus on the users' sentiments. Using an LLM, we built reliable datasets in a new format that separately presents the users' positive and negative opinions about items. Based on our datasets, we introduced evaluation methods that focus on how well a model captures the users' sentiments. We benchmark various models on our datasets and find that existing evaluation metrics are limited in measuring the sentiment alignment between the generated and ground-truth explanations. Lastly, we found that we can make existing models more sensitive to the sentiments by feeding the users' predicted ratings about the target items as additional input of the models, and also showed that the rating prediction accuracy has a large impact on the quality of the generated explanations."}, {"title": "A Appendix", "content": "Table 14 shows the statistics of existing datasets [19] that are widely used in previous work on explainable recommendation [6, 21, 22, 39, 53]. Based on the average lengths of the explanations on these datasets, we restricted the output length of GPT-40-mini to 15 or less words when summarizing user reviews."}, {"title": "A.2 Dataset Quality Evaluation", "content": "Tables 15-16 show the prompts with input and output examples used for the dataset quality evaluation in Section 3.2. We use GPT-40 as an auto-evaluator of the outputs of GPT-40-mini at the review summarization and positive/negative feature extraction steps, respectively. We design these prompts and the evaluation processes based on the methods proposed by Chen et al. [3]."}, {"title": "A.3 Generated Data Analysis", "content": "Figure 6 shows the users' rating distributions on Amazon, Yelp, and RateBeer. On Amazon and Yelp, users tend to assign high scores, while on RateBeer a majority of users give ratings between 10 and 20 and the distribution peaks at 15.\nFigure 7 shows the rating-sentiment distributions on the train and test datasets of Amazon and Yelp. The distributions are similar between the train and test sets on these datasets, unlike on RateBeer (as we showed in Figure 4 in Section 6.2)."}, {"title": "A.4 Details of Existing Evaluation Metrics", "content": "USR calculates the number of the unique sentences generated by a model, divided by the total number of the sentences, as follows:\n$USR = \\frac{|8|}{ND_t}$  (1)\nwhere & denotes the set of unique sentences generated by a model, and $ND_t$ is the total number of the instances on test data.\nFMR calculates the percentage of the explanations that include the ground-truth feature, as follows:\n$FMR = \\frac{1}{ND_t} \\sum_{u,i} 8(f_{u,i} \\in \\hat{E}_{ui})$,  (2)\nwhere $f_{u,i}$ denotes the ground-truth feature; $\\hat{E}_{u,i}$ denotes the generated explanation for the pair of the user u and item i; and $\\delta(x)$ is an indicator function which returns 1 if x is true and 0 otherwise."}, {"title": "A.5 Implementation Details", "content": "In PETER, CER, and ERRA, we employ Stochastic Gradient Descent (SGD) [40] as the optimizer, with a batch size of 128 and an initial"}, {"title": "follows:", "content": "$DIV = \\frac{2}{ND_t(ND_t - 1)} \\sum_{u,u',i,i'} |F_{u,i} \\cap F_{u',i'}|$,  (4)\nwhere $F_{u,i}$ denotes the feature set included in the generated explanation for the pair of the user u and item i, and $F_{u',i'}$ for the pair of the user u' and item i', respectively."}]}