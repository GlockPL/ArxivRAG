{"title": "Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices", "authors": ["Mohamed Aboelenien Ahmed", "Kilian Pfeiffer", "Ramin Khalili", "Heba Khdr", "J\u00f6rg Henkel"], "abstract": "Federated fine-tuning offers a promising approach for tuning Large Language Models (LLMs) on edge devices while preserving data privacy. How-ever, fine-tuning these models on edge devices remains challenging due to high memory, com-munication, and computational demands. Zero-order optimization with task alignment provides a potential solution, enabling fine-tuning with inference-level memory requirements but requires a longer convergence time. In this paper, we pro-pose Federated Split-Perturbation Zero-order Op-timization (FedSPZO) that divides the network into two blocks, applying a different number of perturbations per block in a computationally ef-fective way, achieving faster convergence. Our evaluation shows a 2.5 - 7\u00d7 reduction in compu-tation overhead compared to zero-order state of the art techniques in federated learning.", "sections": [{"title": "1. Introduction", "content": "LLMs are demonstrating high performance in various ma-chine learning tasks, including next-token prediction and text classification. Despite their popularity, these models are very costly to train in terms of required data, computa-tion, and memory resources. The current trend is thus to train such large models over the coarse of few months using highly specialized hardware and large set of \"public\" data and then finetune them over specific tasks in hand using transfer learning techniques. However, this demands access to the finetuning data, which might be only available at the edge devices/users due to privacy requirements. Finetuning in a federated (McMahan et al., 2017) manner on the edge offers adaptability to the data available on these devices while reducing the risk of private data exposure.\nAlthough it offers less training time compared to training from scratch, finetuning is still a computationally demand-ing task, especially in terms of memory usage. It is hence of most importance to reduce the resource footprint of fine-tuning, as many edge devices, e.g. IoT devices, have very limited resources to offer (Imteaj et al., 2022; Pfeiffer et al., 2023b). Parameter Efficient Finetuing (PEFT) techniques such as LoRA (Hu et al., 2022) reduce the memory require-ments for storing gradients. However, this still results in a large memory footprint for inputs with long context length. For example, finetuning RoBERTa-large (Liu, 2019) on a dataset with context length 256 requires memory about 4 GB. This is because the activations should still be stored in memory to perform the backpropagation.\nRecently, MeZO (Malladi et al., 2023) showed that it is possible to finetune language models using Zero-order Opti-mization (ZO) (Spall, 1992) with an inference-like memory footprint, utilizing pseudo-random seeds to generate pertur-bation vectors.\nIn this paper, we adopt ZO to finetune language models at edge devices. We consider a Federated Learning (FL) setting, where a group of edge devices are collectively fine-tuning the model using their local data. While ZO reduces the memory footprint, this reduction comes with a cost: noisy gradient updates that could affect the accuracy and convergence of the training. In FL, (Li et al., 2024; Fang et al., 2022) use a large number of perturbations to improve the quality of gradient estimation at the cost of high compu-tation.\nWe propose Federated Split-Perturbation Zero-order Opti-mization (FedSPZO), which applies a different number of perturbations to different layers of the model, reducing com-putational overhead and increasing the convergence speed. In addition, we adopt the seed trick proposed in (Malladi et al., 2023) to reduce communication overhead in our set-ting. Our work differs from DecomFL (Li et al., 2024), as we use a more accurate approach to merge updates on servers and apply a more efficient perturbation technique, significantly improving the finetuning process.\nOur main contributions are:\n\u2022 We propose FedSPZO, a novel approach that reduces"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Federated Learning", "content": "In FL, the significant computational and communication burden placed on participating devices remains a chal-lenge. Several techniques have been proposed to mit-igate this burden. To reduce communication overhead, methods such as compression (Thakker et al., 2019) and sketched updates (Shi et al., 2020) have been explored. Other approaches address computational overhead by train-ing only subsets of the neural network (NN), such as ran-dom subsets (Caldas et al., 2018) or a sliding window ap-proach (Alam et al., 2022). Progressive training has been introduced to alleviate communication and computational costs through model growth (Wang et al., 2022) and suc-cessive layer training with freezing, which also reduces memory requirements (Pfeiffer et al., 2023a). However, the majority of these works assume training from scratch.\nRecently, fine-tuning of large language models through FL has also been considered (Babakniya et al., 2023) using LORA (Hu et al., 2022), such techniques are not always feasible due to memory constraints. First-order forward gradients (Baydin et al., 2022) with Forward-mode auto-matic differentiation were proposed for finetuning LLMs (Panchal et al., 2024; Xu et al., 2024), however, forward gradients are computationally expensive, require a larger memory footprint compared to ZO, and the upload of train-able parameters."}, {"title": "2.2. Zero-order Optimization", "content": "Zero-order optimization has been adopted in black-box opti-mization due to the lack of accessible gradient information (Nikolakakis et al., 2022; Cai et al., 2021).\nMeZO (Malladi et al., 2023) builds on ZO-SGD (Spall, 1992) using central finite difference to estimate the gradi-ents without backpropagation. The paper proposes a random seed trick to reduce the memory footprint by generating per-turbation vectors on the fly, thus utilizing the same memory as inference and efficient storage of checkpoints. More im-portantly, it showed that the fine-tuning of LLMs is possible using zero-order optimization when utilizing prompt-based"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Background on Zero-order Optimization", "content": "Zero-order Optimization reduces the memory footprint com-pared to first-order backpropagation, as it does not require the storage of the activations in memory. Furthermore, uti-lizing the seed trick proposed by (Malladi et al., 2023), it is possible to maintain an inference-like memory footprint essential to edge devices participating in federated learn-ing. Given model parameters 0, loss L, and input batch B, we define the scalar projected gradient (difference between perturbed losses) g as follows:\n$g = \\frac{L(\\theta + \\epsilon z; B) \u2013 L(\\theta \u2013 \\epsilon z; B)}{2\\epsilon}$ (1)\nwhere $\\epsilon$ is the perturbation scale, and z is sampled from N(0, Ia), where Ia is the identity matrix and d is the number of parameters. By resetting the pseudo-random generator to seed s, each element of the perturbation vector z can be generated to perturb the corresponding parameter in place. This perturbation is applied to the model before a forward pass and is reused to compute the gradient by multiplying the scalar g and update parameters using the SGD rule. This approach eliminates the need to store pseudo-random perturbations in memory, as they can be regenerated on demand. The previous gradient estimation presents a single perturbation P = 1."}, {"title": "3.2. Federated Split-Perturbation Zero-order Optimization (FedSPZO)", "content": "We propose FedSPZO, a federated learning framework that utilizes zero-order optimization with a huge reduction in upload communication cost, as discussed in Section 3.2.1."}, {"title": "3.2.1. FEDSPZO SERVER", "content": "We first discuss the proposed FL design. In each round r, the server starts by broadcasting the model parameters \u03b8 and the number of perturbations for each block (i.e., P\u2081 and P2). Each client trains the model for the K steps on its local data, as we discuss in detail in Section 3.2.2. After a client finishes its steps, it uploads only the vectors of projected gradients G and seeds S used in the training for each block, which are independent of the model parameters.\nThe server is responsible for reconstructing the exact model for each client after K steps by regenerating the perturbation vectors, using the seeds Sc provided by the client c. It performs the same parameter updates using the learning rate \u03bc and the gradients computing the same parameter updates using \u00b5, P and G as illustrated in Algorithm 1. Importantly, this reconstruction does not require carrying out any forward pass or access to the training data on the device. Finally, the server averages all the clients models similar to FedAvg and starts another round. The proposed round design is provided in Figure 1."}, {"title": "3.2.2. FEDSPZO CLIENT", "content": "Zero-order Optimization requires a small learning rate u to convergence to a good solution. However, training with a small learning rate requires many rounds compared to its first-order counterpart, as the gradient norm of zero-order scales with the Hessian effective rank of the loss (Malladi et al., 2023).\nIncreasing the number of perturbations P reduces the noise in the gradient approximation, enabling a larger learning rate. However, this requires P times an increase in compu-tation cost per step. To break this down, the computational cost in Floating-Point Operations (FLOPs) of a zero-order step with central difference (when z is regenerated given seed s) can be expressed as the sum of the ZO forward (zo-fWFLOPs), perturbation computation (ZO-PFLOPs), and up-date (zo-UFLOPs) FLOPs. Let fWFLOPs represent FLOPs for doing one forward pass (inference) on the input, zo-fWFLOPS can be defined as follows:\n$Zo-fWFLOPS = 2 \\times fWFLOPS \\times P$ (3)\nwhile given that the cost of one perturbation of the parameter is PFLOPS and update UFLOPs, the costs for zo-PFLOPs and ZO-UFLOPs are:\n$ZO-PFLOPS = PFLOPS \\times 3 \\times P$ (4)\n$ZO-UFLOPS = UFLOPS \\times P$ (5)\nTo reduce this high computational cost, we propose Fed-SPZO that inherit the fast convergence characteristics when using a large P, with fewer computations. Specifically, Fed-SPZO divides the network into two blocks of subsequent"}, {"title": "5. Conclusion", "content": "In this work, we study the federated fine-tuning of language models. We propose FedSPZO, a novel approach that splits the model into two blocks while applying a different number of perturbations for each block, reducing the computational overhead of applying multiple perturbations while improv-ing convergence. Furthermore, in FedSPZO, the client sends only scalar gradients, reducing the communication cost.\nOur experimental results demonstrate substantial computa-tional efficiency, achieving reductions of up to 7\u00d7 compared to existing state-of-the-art zeroth-order FL methods, while also achieving higher accuracy. When compared to first-order FL with backpropagation, FedSPZO offers advantages in memory efficiency and reduced upload bandwidth but exhibits lower computational efficiency and accuracy. A limitation of our method is that it introduces an additional hyperparameter, requiring the tuning of (P1, P2) instead of a single P, warranting further investigation into optimal tuning strategies.\nFor future work, we plan to explore zero-order on inference-only accelerators with lower precision. This direction aims to bridge the computational efficiency gap with backpropa-gation methods, further enhancing the practicality of Fed-SPZO in resource-constrained environments."}, {"title": "A. Implementation Details", "content": "We use Pytorch for our implementation. We used ROBERTA-Large CasualLLM weights from the Hugging-Face library. For the experiments, we train for a maximum of 5000-15000 rounds. We use the same train/test split of the datasets, where only the train split is divided across clients and test set used for evaluation.\nFor the zero-order methods over SST2, we experimented with learning rates of {1e \u2013 5, 8e \u2013 6, 6e \u2013 6, 5\u0435 \u2013 6, 4e \u2013 6, 2-6, 1-6} and {5e - 6, 2.5e \u2013 6, 1e - 6, . . ., le - 7} for the rest of the dataset. We set e to le 3 for all the methods (same as the papers) and found that adjusting it does not lead improvement and may lead to divergence. For all ZO, an effective batch size of 16 is used.\nFor LORA on SST2, we use rank r = 8 and a = 16 and learning rate {5e \u2013 4, 1e \u2013 4}. We apply LoRA to the query and value of attention layers.\nWe measure FLOPs for training (including zero-order and first-order) using the Pytorch profiler."}]}