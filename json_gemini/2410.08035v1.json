{"title": "INTRINSICVOICE: EMPOWERING LLMS WITH INTRINSIC REAL-TIME VOICE INTERACTION ABILITIES", "authors": ["Xin Zhang", "Xiang Lyu", "Zhihao Du", "Qian Chen", "Dong Zhang", "Hangrui Hu", "Chaohong Tan", "Tianyu Zhao", "Yuxuan Wang", "Bin Zhang", "Heng Lu", "Yaqian Zhou", "Xipeng Qiu"], "abstract": "Current methods of building LLMs with voice interaction capabilities rely heavily on explicit text autoregressive generation before or during speech response generation to maintain content quality, which unfortunately brings computational overhead and increases latency in multi-turn interactions. To address this, we introduce IntrinsicVoice, an LLM designed with intrinsic real-time voice interaction capabilities. IntrinsicVoice aims to facilitate the transfer of textual capabilities of pre-trained LLMs to the speech modality by mitigating the modality gap between text and speech. Our novelty architecture, GroupFormer, can reduce speech sequences to lengths comparable to text sequences while generating high-quality audio, significantly reducing the length difference between speech and text, speeding up inference, and alleviating long-text modeling issues. Additionally, we construct a multi-turn speech-to-speech dialogue dataset named IntrinsicVoice-500k which includes nearly 500k turns of speech-to-speech dialogues, and a cross-modality training strategy to enhance the semantic alignment between speech and text. Experimental results demonstrate that IntrinsicVoice can generate high-quality speech response with latency lower than 100ms in multi-turn dialogue scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) (Yang et al., 2024; Dubey et al., 2024; OpenAI, 2023) and multi-modal large language models (MLLMs) (Tang et al., 2023; Chu et al., 2024; Liu et al., 2024) have exhibited exceptional performance across a variety of natural language processing tasks and multimodal comprehension tasks, allowing them to become powerful solvers for general tasks. These models assist humans in solving diverse problems through text-based multi-turn dialogue interactions. However, this single mode of interaction limits the applicability of LLMs, hindering their deeper integration into everyday life. Recently, GPT-40 (OpenAI, 2024) has showcased real-time, efficient voice interaction capabilities, responding to users' speech commands with low-latency and generating high-quality speech responses. This enhancement significantly improves user experience and broadens the application scenarios for LLMs. Yet, the open-source community still lacks successful explorations in constructing such LLMs with multi-turn efficient and low-latency voice interaction capability. How to empower LLMs with low-latency, high-quality voice interaction abilities in multi-turn dialogue scenarios remains a critical challenge to be solved.\nTraditionally, LLMs with voice interaction capability mainly adopt a cascading paradigm (Huang et al., 2024), i.e. the LLM is connected with an automatic speech recognition (ASR) model and a text-to-speech (TTS) model in tandem. The ASR model transcribes the user's speech instruction into text and the TTS model synthesizes response from LLM into speech. However, this cascaded\napproach suffers from error accumulation and the loss of paralinguistic information such as emotion and prosody, and the overall system tends to have higher latency.\nZhang et al. (2023a) proposed SpeechGPT, an end-to-end method that discretizes speech waveforms into discrete tokens and extends LLM's vocabulary to support speech input and output. Theoretically, models built in this approach can generate speech responses directly from speech instructions. However, the modality gap between speech and text poses significant challenges in transferring textual reasoning abilities to the speech domain. Existing work circumvents the modality alignment challenge by introducing text autoregressive generation before or when generating speech responses. This approach allows text to guide the generation of speech responses, thereby enhancing the quality of response content. Zhang et al. (2023a) proposed a prompting approach named Chain-of-Modality (CoM), which generates intermediate text before producing a speech response to improve the quality of the response content, but at the cost of increased response latency. Mitsui et al. (2024) introduced PSLM, which generates text and speech tokens in parallel through multiple input-output sequences to reduce latency while maintaining the quality of speech response content. Nevertheless, the need for the ASR model on the input side introduces additional latency. Xie & Wu (2024) and Fang et al. (2024) integrate the Whisper (Radford et al., 2023) encoder as their speech encoder addressing the issue of requiring text input. During the generation phase, they simultaneously generate text tokens and discrete speech tokens extracted by either SNAC (Siuzdak, 2024) or HuBERT (Hsu et al., 2021). Yet, since these approaches employ different speech representations for input and output, in multi-turn dialogue scenarios, the Whisper encoder must re-encode the generated speech responses before the next dialogue turn, which introduces additional computational overhead and increases latency.\nIn this paper, we propose IntrinsicVoice, a large language model with intrinsic real-time voice interaction ability, capable of generating speech responses directly from speech instructions. Distinct from previous work, to avoid the additional computational overhead and latency in multi-turn dialogue scenarios, IntrinsicVoice eliminates the need for explicit text autoregressive generation either before or during the generation of speech responses. It aims to achieve high-quality speech response by bridging the modality gap between speech and text and facilitating the transfer of textual capabilities to the speech domain. This is accomplished by narrowing the sequence length difference between text and speech tokens and enhancing semantic alignment between the two modalities.\nTypically, speech token sequences span several times the length of text token sequences, leading to a substantial disparity in semantic information density between the two modalities. This difference significantly affects how models compute attention when processing these sequences, making the capabilities of textual Large Language Models transfer to the speech domain more difficult. Moreover, the excessive length of speech sequence also reduces inference speed and leads to issues with long-context modeling. To address this challenge, inspired by Chen et al. (2024a), we partition the speech token sequence into specified-sized groups, merging all speech tokens within each group into a single frame. This approach circumvents the frame rate limitations inherent to the speech encoder, making reducing the speech frame sequence to a length comparable to that of the text token sequence possible. To maintain the generated speech quality at such meager frame rates, we propose GroupFormer, which augments an LLM with a smaller transformer-encoder-based Group Model. At each time step, the Group Model non-autoregressively predicts a group of speech tokens based on the hidden states output by the LLM.\nPrevious work (Zhang et al., 2023a; Rubenstein et al., 2023; Fang et al., 2024; Xie & Wu, 2024) has primarily utilized text-speech paired data from TTS and ASR tasks to train LLMs modeling $P(speech \\vert transcription)$ and $P(transcription \\vert speech)$, to align the two modalities in semantic. To further enhance the semantic alignment between text and speech modalities, we propose a new training strategy that constructs multiple cross-modal tasks from a speech-to-speech dataset. By training the model on these tasks simultaneously, we aim to deepen its understanding of the semantic consistency between speech and text, thereby improving overall semantic alignment.\nThe contribution of this work can be summarized in the following:\n\u2022 We introduce IntrinsicVoice, a speech-text Large Language Model equipped with intrinsic multimodal understanding and generation capabilities, capable of engaging in multi-turn real-time voice interactions.\n\u2022 We propose GroupFormer, a model that effectively reduces speech token sequences to lengths comparable to text sequences while generating high-quality audio. This approach mitigates the"}, {"title": "2 RELATED WORK", "content": "Multi-modal Large Language Model A typical approach to constructing multimodal LLMs involves utilizing a well-pretrained vision or audio encoder to obtain visual or audio embeddings, which are then aligned with the text input of LLM through a lightweight adaptor (Liu et al., 2024; Chen et al., 2024b; Li et al., 2023; Chu et al., 2024; Tang et al., 2023). While this method enables LLMs to develop multimodal understanding abilities, their output remains constrained to the text modality. On the other hand, works like SpeechGPT (Zhang et al., 2023a), LauraGPT (Chen et al., 2023), and AnyGPT Zhan et al. (2024) proposed converting continuous vision/audio representations into discrete tokens and incorporating these discrete tokens into the LLMs' vocabulary, thereby empowering LLMs with multimodal generation capabilities. Yet, how to transfer the knowledge of pretrained LLM to other modalities seamlessly, consequently obtaining better generalization and human-instruction following ability, remains an unresolved challenge.\nAudio Language Modeling With the success of language models in natural language processing and the development of discrete audio representations for audio understanding and generation (Hsu et al., 2021; Chung et al., 2021; Zeghidour et al., 2021; D\u00e9fossez et al., 2022; Zhang et al., 2024b), language models have been widely applied to various speech generation tasks. Lee et al. (2021); Zhang et al. (2023b) utilized HuBERT units (Hsu et al., 2021) for speech translation, while the VALL-E series (Wang et al., 2023; Zhang et al., 2023c; Chen et al., 2024a) employed audio codecs for zero-shot text-to-speech (TTS) synthesis. AudioLM (Borsos et al., 2023) proposed a hierarchical approach that first models semantic tokens and subsequently audio codecs from the generated semantic tokens, enabling consistent speech synthesis in a textless setting. SPEAR-TTS(Kharitonov et al., 2023) and CosyVoice (Du et al., 2024) further adopted this hierarchical method in zero-shot TTS. Zhang et al. (2024a) observed that disentangled semantic modeling and perceptual modeling can enhance performance in both dimensions. The excessive length of audio token sequences is a challenge of audio language modeling, which not only results in slow inference speed but also leads to issues of long-context modeling. Lee et al. (2021) proposed collapsing consecutive sequences of identical units into a single unit to reduce the length of audio token sequences, which led to performance improvements in speech translation tasks. However, the resulting sequence length still differs significantly from text sequences. Chen et al. (2024a) introduced a grouping strategy that partitions the speech token sequence into specified-size groups and models each group as a single frame, reducing the frame rate by integer multiples. Nevertheless, this method suffers from noticeable performance degradation at high compression rates.\nReal-Time Voice Interaction with LLMs Early efforts to enable voice interaction with LLMs primarily realied a cascading paradigm, where ASR and TTS models were connected in tandem with the LLM. However, this approach suffers from issues such as error accumulation, loss of paralinguistic information, and increased latency. SpeechGPT (Zhang et al., 2023a), AudioPalm (Rubenstein et al., 2023), Spectron (Nachmani et al., 2023), and SpeechGPT-Gen (Zhang et al., 2024a) have adopted an end-to-end method using Chain-of-Modality (CoM) prompting, which generates intermediate text before producing a speech response. While this approach improves the quality of the generated speech content, it comes at the cost of increased latency due to the text autoregressive generation before generating speech. To address this problem, PSLM (Mitsui et al., 2024) and Moshi (D\u00e9fossez et al.) generate text and speech in parallel, but the reliance on ASR models fro input still introduces additional latency. Mini-Omni (Xie & Wu, 2024) and Llama-Omni (Fang et al., 2024) tackle this issue by feeding continuous speech embeddings from Whisper (Radford et al., 2023) encoder into the LLM, enhancing model's ability to understand speech instructions without requiring text input. However the inconsistency between speech input and output introduces additional computational overhead, increasesing latency in multi-turn scenarios."}, {"title": "3 INTRINSICVOICE", "content": "In this section, we introduce the architecture of IntrinsicVoice. As depicted in Figure1, our model consists of the following main components: a speech encoder and a streaming vocoder for the tokenization and detokenization of speech; a speech embedding layer and a speech adaptor for mapping grouped discrete speech tokens into embeddings of LLM; and a GroupFormer for modeling speech and text sequence, which consists of a large language model and a group model."}, {"title": "3.1 SPEECH TOKENIZATION", "content": "Speech Tokenization Speech tokenization involves transforming continuous speech signals into a sequence of discrete speech tokens, which enables language models to handle speech like text tokens. Semantic tokens, typically from self-supervised pre-trained models with masked language modeling as training objective, have been widely used for speech tokenization (Zhang et al., 2023a; Rubenstein et al., 2023; Hassid et al., 2024) due to their strong alignment with text (Zhang et al., 2024b). Following Zhang et al. (2023a) and Mitsui et al. (2024), we employ HuBERT (Hsu et al., 2021) as speech encoder for speech tokenization, converting speech waveform A into speech token sequence $S = [s_0, s_1,\uff65\uff65\uff65, s_{T-1}]$, where T is the speech tokens length.\nSpeech Detokenization Speech detokenization refers to the process of converting discrete speech tokens S back into speech waveform A. We use HiFi-GAN (Kong et al., 2020), a non-autoregressive neural vocoder that efficiently generates high-fidelity waveforms, for speech detokenization to reduce overall latency. Following the approach in Mitsui et al. (2024), we provide partial speech tokens to generate waveform fragments. Specifically, we utilize non-causal convolution to guarantee high speech quality, and the first speech fragment can be generated once $N_{offset} = [R/2] + 1$ tokens are decoded, where R denotes the receptive field of HiFi-GAN."}, {"title": "3.2 EMBEDDING GROUPED SPEECH TOKENS", "content": "To reduce the length of speech token sequence S, we partition it into a grouped token sequence $S^G = [S_{0:G}, S_{G:2G},\u2026\u2026,S_{(T-G):T}]$, where G denotes group size and $S_{0:G}$ stands for the group $[s_0, s_1,\u2026\u2026, s_{G-1}]$. Following Chen et al. (2024a), we clip a few tokens from the start of the speech token sequence to ensure the token sequence length T is the integer multiple of group size.\nAs shown in the right part of Figure 1, a speech embedding layer transforms the discrete grouped speech token sequence $S^G$ into a dense embedding sequence $[e_{0:G}, e_{G:2G},\u2026\u2026e_{(T-G):T}]$, where $e_{0:G} = [e_0, e_1,\u2026, e_{G-1}]$. Each group token embedding is concatenated in the hidden dimension"}, {"title": "3.3 GROUPFORMER", "content": "Lee et al. (2021) observed that the \"stacked\" strategy, which is similar to our group strategy, achieved worse performance than the \"reduced\" strategy, which represents collapsing a consecutive sequence of the same units into a single unit, resulting in a sequence of unique discrete units. We attribute this to the high compression rate causing a group of speech tokens to probably contain distinct HuBERT units, which have temporal dependencies among them. The approach of using multiple linear heads to parallelly predict a group of tokens exhibits weaker context modeling capabilities, struggling to handle such temporal dependencies within the group. To mitigate this issue without significantly increasing latency, as illustrated in Figure 1, we introduced a GroupFormer, which augments the LLM with a smaller non-autoregressive transformer encoder model (dubbed \"GroupModel\") designed to enhance the model's ability to model sequences within groups.\nSpecifically, for a given sequence step 0 < t < L, the LLM maps $E_{<t} = [e_0, e_1, \u2026\u2026\u2026, e_t]$ to a global context embeddings $z_{t+1} \u2208 R^d$, where d denotes the dimension of the LLM's hidden states. The prediction head of the LLM maps $z_{t+1}$ to logits estimates $p_{t+1} \u2208 R^N$, where N denotes the size of the LLM's vocabulary. This vocabulary is an extension of the original pretrained LLM vocabulary, enriched with <sosp>, <eosp> and <speech>. The predicted token $y_{t+1}$ is then sampled from the distribution obtained by applying softmax function to $P_{t+1}$\u00b7\nIf the currently predicted token $y_{t+1}$ is <speech>, $z_{t+1}$ is transformed through a projection layer and then concatenated with a G learnable queries, forming embedding sequence $E^1_{GM} = [proj (z_{t+1}), q_0, q_1, \u2026\u2026\u2026, q_{G-1}]$. Taking $E^1_{GM}$ as input, the GroupModel predicts a group of speech tokens in one step, denoted as $S_{(t+1)G:(t+2)G} = [s_{(t+1)G}, s_{(t+1)G+1},***, s_{(t+2)G-1}]$. These tokens are subsequently processed as outlined in Section 3.2, producing the input embedding $e_{t+1}$ for the"}, {"title": "3.4 TRAINING", "content": "Speech-to-speech data is structured as quadruples of the form (SI, IT, SR, RT), where SI denotes speech instruction, IT denotes instruction transcription, SR denotes speech response, and RT denotes response transcription. We reformulate these tuples into four distinct tasks: (1) SI\u2192SR, (2) SI\u2192RT, (3) IT\u2192SR, and (4) IT\u2192RT. By this approach, we construct multiple cross-modal tasks from a single dataset, enabling the model to better learn the semantic consistency between speech and text.\nTo fully leverage the speech data, the losses $L_G$ is computed on both the input and output speech, whereas $L_{LLM}$ is computed solely on the response portion. Consequently, the overall training objective L can be formulated as follows:\n$L = L_{LLM} + L_G$."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUPS\nDatasets We utilize CosyVoice-300M-SFT model (Du et al., 2024) to synthesize 89k multi-turn speech QA pairs (comprising 345k turns in total) from the Moss-002-sft-data\u00b9 (Sun et al., 2024) and CoQA2 (Reddy et al., 2019) datasets. Additionally, we use the SQUAD3 dataset (Rajpurkar, 2016) to synthesize 87k single-turn speech QA pairs. By incorporating 54k samples from the Spoken-Alpaca-GPT4 dataset, we construct our multi-turn speech-to-speech QA dataset, Intrinsic_500k, which consists of approximately 500k QA turns. We employ around 20k hours from Multilingual Librispeech (Pratap et al., 2020) for ASR and TTS tasks to further enhance the model's speech understanding and synthesis capabilities. To maintain the LLM's textual abilities, we sample 350k samples from the Guanaco_Belle_Merge_v1.05 Chenghao Fan & Tian (2023) text QA dataset for training. Datasets are detailed in Tabel 1.\nModel Configuration For the speech tokenizer, we utilize mhubert-base-25hz and its corresponding KMeans quantizer from Hassid et al. (2024), operating at 25Hz with 500 clusters. Our backbone\nLLM is Qwen2-7B-Instruct (Yang et al., 2024), and the GroupModel is an 8-layer, 16-head, 512-hidden-size non-causal transformer encoder with learnable position embeddings. We partition the speech tokens into group sizes of 5, resulting in a 5 tokens-per-second (TPS) speech sequence, closely matching the 4.2 TPS we observed from text token sequences in LibriSpeech dev-clean and dev-other (Panayotov et al., 2015).\nTraining IntrinsicVoice follows a two-stage training process and is trained on 8 NVIDIA A100 GPUs with DeepSpeed ZeRO-2 (Rasley et al., 2020). We utilize a cosine annealing learning rate scheduler with a peak learning rate of 1.5e-4, training the model with a batch size of 256 for 4 epochs. The maximum sequence length during training is 1200. For decoding, we use a temperature of 0.7, Top-k sampling with k = 10, and Top-p sampling with p = 0.8."}, {"title": "4.2 BASELINES", "content": "We employed two baseline models based on SpeechGPT (Zhang et al., 2023a), which share the same model weights but differ in their decoding prompts: (1) SpeechGPT, which processes only speech instructions, and (2) SpeechGPT-ASR, which processes both speech instructions and their corresponding transcriptions generated by Whisper en-medium (Radford et al., 2023). In addition to the baselines above, we trained a model under the same training settings as IntrinsicVoice, but employing the \"reduced\" strategies outlined in Section 3.3. This model, named IntrinsicVoice (Reduce), collapses consecutive sequences of identical units into a single unit, producing a sequence of unique discrete units."}, {"title": "4.3 EVALUATION", "content": "We evaluate the cross-modal instruction-following in multi-turn dialogue scenarios across four tasks:\nspeech-to-speech instruction-following (S2SIF), text-to-speech instruction-following (T2SIF), text-to-text instruction-following (T2TIF), speech-to-text instruction-following (S2TIF).\nData We construct our evaluation set, IntrinsicVoice_Eval, by randomly selecting 100 samples from the AlpacaEval dataset.We then use CosyVoice to convert the text into speech. Each sample forms a quadruplet consisting of a speech instruction, text instruction, text response, and speech response, which we designate as ground truth.\nChatGPT Score Following Zhang et al. (2023a), we utilize GPT-40 to assess the content quality of response. For tasks involving speech, we employ Whisper-large-v3 (Radford et al., 2023) to transcribe the speech into text, which is subsequently evaluated. The detailed prompt used for evaluation is available in the Appendix A.1.\nSpeech Quality Following Fang et al. (2024), we utilize a Mean Opinion Score (MOS) prediction model, UTMOS7 (Saeki et al., 2022), to evaluate the quality and naturalness of the generated speech, referring to this metric as the UTMOS score."}, {"title": "4.4 MAIN RESULTS", "content": "Content As shown in Table 2, IntrinsicVoice (Group) with TPS of 5 significantly outperformed IntrinsicVoice (Reduce), which averaged 19.16 TPS, in both the S2SIF and T2SIF tasks regarding content quality. This result underscores the significant advantages of minimizing the gap between text token sequence length and speech token sequence length, thereby enhancing the transfer of textual capabilities to the speech domain. Furthermore, SpeechGPT+ASR demonstrated a notable performance advantage over SpeechGPT, indicating that the response content quality of SpeechGPT is highly dependent on the accuracy of the transcription provided for the speech instructions. In contrast, IntrinsicVoice outperformed both SpeechGPT on the S2SIF and S2TIF tasks, showcasing its superior intrinsic speech understanding and modeling capabilities.\nSpeech Quality As shown in Table 2, IntrinsicVoice (Group) achieved UTMOS scores comparable to those of IntrinsicVoice (Reduce), SpeechGPT, and SpeechGPT+ASR, indicating that the GroupFormer architecture enables the model to maintain high audio quality even at very low TPS.\nLatency IntrinsicVoice achieved a latency of less than 100 ms, which is only one-tenth that of the SpeechGPT-based model, enabling real-time voice interaction. This experimental result highlights the significant advantage of generating speech responses directly from speech instructions, as opposed to relying on text autoregressive generation, in terms of real-time performance."}, {"title": "5 CASE STUDY", "content": "Figure 3 illustrates a multi-turn speech QA example using Intrinsic Voice. The results demonstrate that IntrinsicVoice can understand speech instructions and respond appropriately during multi-turn interactions with humans."}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed IntrinsicVoice, an LLM with intrinsic real-time voice interaction capability. We introduced the GroupFormer architecture to reduce speech token sequences to lengths comparable to text token sequences while generating high-quality audio. This approach significantly mitigates the length disparity between speech and text, accelerates inference, and alleviates challenges associated with long-text modeling. Additionally, we constructed a multi-turn speech-to-speech dialogue dataset named IntrinsicVoice-500k, which includes nearly 500,000 turns of speech-to-speech"}, {"title": "A APPENDIX", "content": "A.1 CHATGPT SCORE EVALUATION PROMPT"}]}