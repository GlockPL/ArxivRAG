{"title": "TO COT OR NOT TO COT? CHAIN-OF-THOUGHT HELPS\nMAINLY ON MATH AND SYMBOLIC REASONING", "authors": ["Zayne Sprague", "Fangcong Yin", "Juan Diego Rodriguez", "Dongwei Jiang", "Manya Wadhwa", "Prasann Singhal", "Xinyu Zhao", "Xi Ye", "Kyle Mahowald", "Greg Durrett"], "abstract": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting reason-\ning capabilities from large language models (LLMs). But for what kinds of tasks\nis this extra \"thinking\u201d really helpful? To analyze this, we conducted a quantitative\nmeta-analysis covering over 100 papers using CoT and ran our own evaluations of\n20 datasets across 14 models. Our results show that CoT gives strong performance\nbenefits primarily on tasks involving math or logic, with much smaller gains on\nother types of tasks. On MMLU, directly generating the answer without CoT leads\nto almost identical accuracy as CoT unless the question or model's response con-\ntains an equals sign, indicating symbolic operations and reasoning. Following this\nfinding, we analyze the behavior of CoT on these problems by separating plan-\nning and execution and comparing against tool-augmented LLMs. Much of CoT's\ngain comes from improving symbolic execution, but it underperforms relative to\nusing a symbolic solver. Our results indicate that CoT can be applied selectively,\nmaintaining performance while saving inference costs. Furthermore, they suggest\na need to move beyond prompt-based CoT to new paradigms that better leverage\nintermediate computation across the whole range of LLM applications.", "sections": [{"title": "INTRODUCTION", "content": "Chain-of-thought (CoT) (Nye et al., 2022; Wei et al., 2022) has become a widely used prompting\ntechnique for eliciting reasoning from language models. CoT can provide human-readable expla-\nnations of how problems are solved (Joshi et al., 2023; Lanham et al., 2023), but most frequently\nit is invoked to improve an LLM's ability to answer complex questions via intermediate computa-\ntion (Madaan & Yazdanbakhsh, 2022; Wang et al., 2023a; Dziri et al., 2023). Current post-training\nschemes for LLMs heavily infuse CoT capabilities into models: systems like ChatGPT or Llama 3.1\ndefault to CoT when given reasoning problems (OpenAI, 2023; Dubey et al., 2024).\nCoT has seen widespread usage, but it is most heavily explored in the domain of mathematical rea-\nsoning (Zhou et al., 2023a; Fu et al., 2023; Chae et al., 2024; Xu et al., 2024b; Qi et al., 2024).\nIn fact, many \"reasoning\" methods for LLMs are evaluated only in the math domain; for instance,\nLightman et al. (2024) frame their paper as \u201ccomplex multi-step reasoning\u201d and Mixtral-Large2's re-\nlease\u00b9 cited effort \u201cenhancing the model's reasoning capabilities\u201d, but performance is only reported\non GSM8K and MATH. CoT is reported to be effective across a wide range of studies, but many of\nthese studies focus on a narrow slice of the task space. In areas beyond math, results show that CoT\nis not as useful (Kambhampati et al., 2024a) or can even hurt performance (Wang et al., 2024).\nIn this work, we aim to evaluate where prompt-based CoT helps and why. We begin with a sys-\ntematic meta-analysis of recent literature that reports performance of CoT versus direct answering\n(DA). We then augment this picture by conducting experiments on 20 datasets and 14 contemporary\nLLMs across zero-shot and few-shot prompt settings. Finding 1: CoT only helps substantially\non problems requiring mathematical, logical, or algorithmic reasoning. Figure 1 shows this\nholds both across the literature and our own experiments. We find only a few cases of large gain\nin other kinds of tasks, and many of these outliers feature some component of symbolic reasoning.\nFor instance, on MMLU (Hendrycks et al., 2021a) and MMLU Pro (Wang et al., 2024), we analyze\nthe improvements from CoT and find that CoT only gives benefit on math slices of the dataset. As\nmuch as 95% of the total performance gain from CoT on MMLU is attributed to questions\ncontaining \"=\" in the question or generated output. For non-math questions, we find no features\nto indicate when CoT will help.\nHow can we better understand why CoT improves on these questions and only these questions?\nThe math and formal logical reasoning datasets we consider can be broken down into two stages of\nprocessing: a planning step (e.g., parsing a problem into equations) and an execution step (building\nintermediate outputs and working towards a solution) (Ye et al., 2023; Wang et al., 2023b; Sun et al.,\n2024). Finding 2: CoT primarily helps with the execution step that performs computation and\nsymbolic manipulation, but falls short of what LLMs with tool augmentation can do. We find\nthat LMs prompted with CoT can generate executable formal solution plans and execute those plans\nbetter than direct answering. But using LMs to generate a solution plan and then using an external\nsymbolic solver to solve the plan outperforms using CoT for both stages for these tasks.\nThese results paint a picture that CoT's utility is often circumscribed by tool augmentation: on\nproblems where CoT helps, we already have more powerful tools than CoT that we can employ,\nand on \"soft reasoning\" problems like commonsense where no tools exist, we see limited benefit\nfrom CoT. This characterization has two major implications. First, CoT is unnecessary for many\nproblems where it is widely employed: there exist more efficient prompting strategies that yield\nsimilar performance for much lower inference cost. Second, we see a critical need to move beyond\nprompt-based CoT to more sophisticated approaches based on search, interacting agents, or models\nmore heavily fine-tuned for CoT. Future work can explore how intermediate computation can be\nbetter used to solve challenging problems outside of the math and symbolic reasoning domains."}, {"title": "BACKGROUND: CHAIN-OF-THOUGHT", "content": "The tasks we consider in this work consist of a question $q \\in \\Sigma^*$ for a vocabulary $\\Sigma$ and an answer\n$a \\in L(q)$ for a label set $L(q)$. $L(q)$ can consist of a data type like boolean or integer, classification\nlabels, or problem-dependent labels like names of entities from $q$. One exception that we still"}, {"title": "RESULTS FROM THE LITERATURE", "content": "We first perform a meta-analysis of recent papers comparing the performance of prompts $I_{CoT}$ and\n$I_{direct}$ to identify the types of tasks where CoT has been reported to help."}, {"title": "CRITERIA AND PROCESS", "content": "Automatic Selection and Paper Filtering We investigate all papers from ICLR 2024, a repre-\nsentative ML venue, and two representative NLP venues, EACL 2024 and NAACL 2024 (including\nFindings and Workshop papers). We filtered all 4,642 papers (2,259 from ICLR 2024 and 2,382\nfrom the two ACL-affiliated conferences) for those with at least two occurrences of \u201cCoT\", \"chain-\nof-thought\", or \"chain of thought\", resulting in 516 papers. There are conceivably papers using CoT\ncalled by another name (e.g., Scratchpads), but we believe these 516 give a representative sample\nappropriate for systematic analysis.\nManual Paper Filtering and Results Extraction We then filter down to papers that perform a\ncomparison of CoT prompting vs. direct prompting, whether or not this is core to the paper's research\nquestion. We manually filtered the 516 papers in question and extracted the key results from those\nthat remained. We excluded multimodal models, CoT-fine-tuned models, any experiments where\nthe \"CoT\" method involves multiple forward passes (e.g., self-consistency (Wang et al., 2023c) and\ntree-of-thought (Yao et al., 2023)), and systems that augment LLMs with external tools (discussed\nmore in Section 5).\nFor each paper passing through these criteria, we manually extracted the results from key tables com-\nparing CoT and direct answer prompts. We only include results where the CoT and direct prompts\nare run on the same model and same dataset while being on a scale of 0 to 100 (excluding Likert scale\nevaluations, for example) for a more direct comparison. When papers include various CoT or direct\nanswer prompts (including zero/few-shot variants), we always take the best-performing prompt for\nboth. We focus on key test results where applicable, excluding dev sets if they are reported alongside\ntest and also excluding numbers from ablations or nonstandard subsets of datasets.\nThis resulted in a total of 1,218 experimental comparisons across 110 papers (35 from ICLR and 75\nfrom NAACL and EACL) covering 264 datasets. Details and more information can be found in our\nGitHub Repo: https://github.com/Zayne-sprague/To-CoT-or-not-to-CoT."}, {"title": "RESULTS", "content": "Figure 2 shows the distribution of CoT deltas (CoT prompt minus the direct answer prompt per-\nformance) across our categorization of different task types found in the literature. Compared to"}, {"title": "RESULTS FROM EXPERIMENTS", "content": "Our analysis of the literature sheds light on the behavior of CoT, but still leaves open questions about\nthe behavior of the newest models and apples-to-apples comparisons across model classes, datasets,\nand prompting techniques. To further our characterization, we perform a series of experiments on\n20 datasets across 14 models in both the zero-shot and few-shot setting to compare performance."}, {"title": "EXPERIMENTAL SETUP", "content": "Table 2 lists the models, datasets, and prompting techniques we con-\nsider for our experiments (more details, including the dataset composition of each reasoning cat-\negory, in Table 4 and Table 5 of Appendix A). We restricted our experiments to English models\ncommonly used and benchmarked on general reasoning datasets. This excludes math-specific lan-\nguage models like DeepSeekMath-Instruct models (Shao et al., 2024) as well as non-English models.\nWe also focus on instruction-tuned language models. Our datasets include those which are widely\nused in CoT and reasoning literature, including a mix of non-symbolic, semisymbolic, and symbolic\nreasoning. They span different formats, including multiple-choice, short-answer, and free-response;\nhowever, most of these datasets are multiple choice or short answer, as CoT is not typically used\nin long-form response settings. We also categorize each dataset into a larger category of reasoning\nrequired to solve it: Commonsense, Knowledge, Symbolic, Mathematical, and Soft Reasoning. We\ndefine Soft Reasoning as questions relying on commonsense and natural language but going beyond\nsimple inferences about these statements. Finally, we explore several prompting strategies for elic-\niting reasoning from language models, as past work has emphasized the importance of the prompt\n(Yang et al., 2024). However, we generally found slight performance differences; see Appendix G\nfor details. We therefore focus on prompts similar to Kojima et al. (2022) and Wei et al. (2022)\nfor zero-shot and few-shot settings, respectively, with alterations to improve the model's ability to\nproduce desired behavior (i.e., formats that allow for easily parsed answers).\nWe use a high-throughput inference package, vLLM (Kwon et al., 2023),\nfor the model inference process. We use greedy decoding on all models. Our prompts are taken from\nthe Llama 3.1 evaluations when available (Dubey et al., 2024), and minor adjustments are made to\nunify prompting strategies. For other datasets, we either use the standard prompt for the dataset\nfrom the corresponding original paper or implement our own prompt. Our answer parser (extract)\nis tailored to each dataset and model. Specific details about each dataset, its prompts, and answer\nextractor can be found in Appendix A."}, {"title": "RESULTS", "content": "Where does zero-shot CoT improve over direct prompts? On datasets that require math\n(MATH, GSM8K) or formal logic (ContextHub, MuSR to a lesser degree) to answer the problem.\nFigure 3 on the left shows the average CoT performance improvement for each reasoning category\nfrom Figure 1 (right); raw numbers can be found in Table 6 of the Appendix. On the right, Figure 3\nshows the performance gain from using CoT for each dataset, averaged across all models and for\na selection of individual models. On non-symbolic reasoning categories and datasets, specifically\nthose that contain questions primarily involving commonsense (CSQA, PIQA, SiQA), language un-\nderstanding (WinoGrande), and reading comprehension (AGI LSAT, ARC-Easy, ARC-Challenge),\nthere is little to no separation between the performance of zero-shot CoT and zero-shot direct answer.\nDespite these datasets involving reasoning, CoT does not yield improvement.\nBy contrast, the mathematical and symbolic categories get larger boosts in improvements alongside\nsymbolic and many semi-symbolic datasets. MATH and GSM8k show gains as large as 41.6%\nand 66.9%, respectively. The semi-symbolic datasets like ContextHub and MuSR Murder Mysteries\nshow moderate gains. These datasets require the application of logical rules to reach the answer, e.g.,\nfirst-order logic parsed from simple natural language (ContextHub) or more complex commonsense\nstatements (MuSR Murder Mysteries). All results are shown in the Appendix C.1 as well as a full\nlist of numeric results for both CoT and direct answer prompting in Table 7. We also explored the\nfew-shot setting and found it had little impact on when CoT will help; see Appendix B.\nDoes the answer format impact where CoT will help? Not much. Free response capabilities\nmay be hindered by pre-planning or reasoning about the correct response.\nMany of the commonly-used datasets for problems other than math are multiple choice. We highlight\nhere that CoT has similar performance to direct answer across models for two datasets that are not\nmultiple-choice and contain varying levels of non-symbolic reasoning to answer the question. First,\nMuSiQue (Trivedi et al., 2022) is a short-form QA task requiring multi-hop reasoning. We consider\nthis a semi-symbolic dataset as the questions have an explicit multi-hop structure. Because answer\nspans in MuSiQue can be paraphrased in many different ways, we use GPT-4o to judge if two answer\nspans are equivalent. Despite being semi-symbolic, we see no overall improvement from CoT.\nSecond, BiGGen Bench (Kim et al., 2024) uses free-form responses as the answer to a question, and\nan LLM-as-a-judge is used to evaluate these responses on a scale of 1 to 5. The free-form nature\nof the answers blurs the lines between CoT and direct answer. However, we devised a CoT prompt\nfor this setting where we ask the language model to generate a plan for the free-form response\n(the reasoning part), and then we ask it to generate the full response (the answer part) all in one\ngeneration. We then only give the response to the judge. We use GPT-40 mini as the judge with\nthe prompt from Kim et al. (2024). We also exclude slices from BiGGen Bench that ask the LLM\nto \"Think step by step\" within the question, as comparing it to direct answer is difficult with these\nprompts. We plot the performance of BiGGen Bench as the number of times a prompt receives a\nscore of 4 or better on each question. CoT leads to mild overall improvement here, which we expect\nto see given the benchmark's inclusion of reasoning questions (including several categories of math)\nand other categories such as planning.\nAre the gains in Knowledge, Soft Reasoning, and Commonsense significant? Mostly no, except\nfor MMLU, StrategyQA, and MuSR.\nWe tested the significance of the improvements from CoT on the 13 datasets in the Knowledge, Soft\nReasoning, and Commonsense reasoning categories using paired bootstrapping to assess whether\nCoT gives a significant improvement. To account for multiple comparisons, we applied a Bonferroni\ncorrection, setting the p-value to 0.00027 to account for the 14 models and 13 datasets. About\n38% (58) of the datasets that show a benefit in these three reasoning categories were considered\nsignificant. Nearly half of these comparisons (26) are on MMLU and MMLU Pro, which we study\nmore closely in the next section. StrategyQA and MuSR also received a consistent performance\nboost across 9 and 6 models respectively. StrategyQA is often used to benchmark reasoning methods\nand is built specifically to get a benefit from methods that decompose the question into steps, so a\ngain in performance is not unprecedented. MuSR, similarly, was built to have multiple steps of"}, {"title": "ZOOM-IN: MMLU AND MMLU PRO", "content": "MMLU and MMLU Pro show gains from adding CoT, but because these datasets are so broad, they\ndefy simple characterization. We explore the performance of CoT on each category of MMLU to\nunderstand divergences in CoT performance between these domains. We list the top three categories\nwhere CoT gives the largest error reduction for Llama 3.1 8B and 70B on MMLU and MMLU Pro\nin Table 3. Some of these categories are explicitly mathematical in nature, as we might expect from\nFigure 8. We can also see that CoT is helping on categories like \"business\"; upon closer inspection,\nwe found that these categories frequently involve math as well (e.g., business questions may involve\ncomputations surrounding wealth). We need to more carefully characterize MMLU at the instance\nlevel. In doing so, we can test our hypotheses with much finer granularity than possible by relying\non subjective groupings into tasks and categories.\nBreakdown by the presence of equations We aim to design an instance-level classifier to deter-\nmine if CoT is expected to help on a question or not. That is, we want a function $g : q \\rightarrow \\{0,1\\}$\nwhere $g(q)$ returns 1 if $extract(\\tilde{y}_{cot}) = y^*$ and $extract(\\tilde{y}_{da}) \\neq y^*$ where $y^*$ is the gold answer\nto $q$. We explored different forms of $g$; however, we ultimately found it most effective to use a\nclassifier $g: (q, \\tilde{y}_{cot}) \\rightarrow \\{0,1\\}$ which also consults the chain-of-thought produced by the model.\nThis allows us to featurize how the LM solves the problem, particularly whether it uses symbolic\nreasoning or not."}, {"title": "STRENGTHS AND WEAKNESSES OF COT AT FORMAL REASONING", "content": "Previous sections establish that CoT primarily helps with symbolic reasoning tasks, but not why.\nMany symbolic and semi-symbolic tasks be broken down into two stages (Ye et al., 2023; Pan et al.,\n2023; Jiang et al., 2024): planning, either via a formal or informal specification via prompting (Sun\net al., 2024; Wang et al., 2023b), and execution, using the same LM or external solvers. In this\nsection, we attribute the performance gains from CoT on symbolic tasks to these two stages.\nGiven a question that requires symbolic reasoning, we define the planning stage as extracting all\nvariables from the context into a formal specification and defining their relations. The execution\nstage uses a solver that takes as input a plan and can be run in an orderly fashion to derive the final\nanswer. Using our notation from Section 2, let $f(q) = I_{planning}(q)$ be a mapping of the question\n$q$ to a symbolic plan $S_{plan}$ that can be executed by the language model or by an external symbolic\nsolver, $a = solve(S_{plan})$, where $\\hat{a}$ is the final answer for $q$.\nBy separating planning and execution in this way, we can test how much a language model can gain\nfrom just knowing how to solve a problem (only having a plan), to having a plan and being able to\nreason about its output (having a plan and solving it with CoT), or to having a plan and then solve it\nwith an external symbolic solver. Given a plan $S_{plan} \\sim I_{planning}(q)$, we compare the performance\nof the settings below to evaluate at which stage LM is most effective and falls short."}, {"title": "SETTINGS EVALUATED", "content": "Settings 1 and 2: Few-shot direct answer and CoT: We use the few-shot direct answer and CoT\nprompts from Section 4.1 as baselines. Figure 5 includes an example of each setting on GSM8K.\nSettings 3 and 4: Plan + Direct Solver and Plan + CoT Solver: Here we use inspiration from\nXu et al. (2024a) and generate a symbolic plan using the same strategy as Ye et al. (2023). Specif-\nically, we use a few-shot prompt $I_{planning}$ that is meant to generate a formal specification $S_{plan}$\nthat should be executable by a symbolic solver. In the same prompt LMs are asked to solve their\ngenerated specification $S_{plan}$ and derive the final answer $\\tilde{y} \\sim p(y | I_{da}(S_{plan}))$, either directly\ngiving the answer after generating the specification (Plan + Direct Solver) or providing a trace of\nthe plan (step-by-step explanations and tracking of intermediate steps) for the derivation (Plan +\nCoT Solver). Particularly, $S_{plan}$ is a Python program for the math datasets, and is a set of formal\nspecifications in first-order logic for the logical reasoning datasets.\nSetting 5: Plan + Tool Solver We then evaluate how effective CoT can be at performing symbolic\ncomputations compared with external symbolic solvers. Following prior work on augmenting LMs\nwith tools for math and logic questions (Ye et al., 2023; Pan et al., 2023; Gao et al., 2023; Chen\net al., 2023), we generate $S_{plan}$ the same way as in CoT Solver, but now feed in the plan into a\nsymbolic solver (Python interpreter or a SMT Solver), such that $a = solve(S_{plan})$."}, {"title": "EVALUATION RESULTS", "content": "Figure 6 shows the results across a representative selection of models. Detailed numerical results,\nincluding the unparseable rates of model-generated plans, can be found in Appendix F.\nWhen comparing direct answer with Plan + Direct solver and Plan + CoT solver, we note that for\nmany datasets and models, only having a plan does not account for most of the performance gain.\nCompared with direct answer, CoT or Plan + CoT solver is needed for strong performance.\nTracking the execution with one of these methods gives the strongest accuracy benefit, espe-\ncially for math-heavy datasets.\nDespite their strength over direct answer and Plan + Direct solver, CoT and Plan + CoT solver are\ndominated by Plan + Tool solver in most settings. LLMs are limited by their ability to execute\nand track steps compared with symbolic solvers.\nWe argue that these results provide an explanation of why CoT helps on symbolic tasks. While all\ntasks could feasibly benefit from a detailed description of how to solve each individual question (e.g.,\na plan in the context of this section), CoT only outperforms direct answer when these steps require\na substantial amount of tracing and computation. In these settings, we can see clear performance\nbenefit from using symbolic solvers; CoT appears to be a poor (but universal) approximation to\nsuch solvers. When possible, LLMs should be paired with symbolic solvers when solving symbolic\ntasks to achieve consistently better performance over direct answer and CoT."}, {"title": "DISCUSSION AND RELATED WORK", "content": "Where is CoT helping and why? Our results showing CoT improvement for math and logic\naligns well with early work on CoT for LLMs such as Scratchpads (Nye et al., 2022). As CoT\ngained popularity, its application has broadened to tasks that canonically do not require multiple\nsteps. It can often yield small improvements over direct answering. We believe this led to the cur-\nrent prevailing sentiment that deliberation should improve performance on any task requiring some\ntype of reasoning (our original claim from Section 2). However, our results show a clear separation\nbetween performance on non-symbolic and symbolic tasks. If, in theory, any question could benefit\nfrom deliberation, why is CoT only benefiting the questions that can be solved through symbolic\nmanipulation? Our results from Section 5 suggest that the primary benefit of CoT comes in the\nability to execute symbolic steps and track their output. Not all tasks have this feature: for example,\nquestions from CommonsenseQA can hardly be translated into formally grounded and executable\nsolution plans. Datasets like StrategyQA may feature multiple steps of reasoning, but executing\nthose steps is not complex, so the benefits of CoT are small. It is unclear whether explicitly in-\nstilling models with particular modes of deliberation, like process of elimination for multiple choice\nquestions, might make them more effective for non-symbolic tasks, or whether there's a fundamental\nlimitation imposed by their pre-training data. We leave this distinction for future work.\nLong Horizon Planning One set of tasks where symbolic reasoning helps substantially that our\nexperiments haven't covered as thoroughly (with the exception of BiGGen-Bench) is long-horizon\nplanning (Valmeekam et al., 2023; Xie et al., 2024; Gundawar et al., 2024; Valmeekam et al., 2024).\nThere are two reasons we don't treat it here. First, we are primarily interested in tasks that are\nconveyed in language, and we see less complex planning in language-only tasks. Second, there has\nalready been a large debate on the effectiveness of CoT, both pro (Huang et al., 2022; Hu et al., 2023)\nand against (Valmeekam et al., 2023; Kambhampati, 2024; Kambhampati et al., 2024b; Stechly et al.,\n2024a; Guan et al., 2024; Verma et al., 2024; Gundawar et al., 2024; Stechly et al., 2024b) using\nCoT and its derivatives like tree-of-thought (Yao et al., 2023; Kang et al., 2024), that has resulted in\ncomplex systems to help solve planning problems better. While story generation and interpretation\ninvolve elements of planning with natural language (Peng et al., 2022; Karpinska et al., 2024), such\ntasks are not conventionally formalized and benchmarked as planning and reasoning."}, {"title": "CONCLUSION", "content": "In this work, we characterize the performance of prompt-based CoT through a meta-analysis of\nthe literature and experiments across different models, datasets, and prompts. We find that CoT\npredominantly helps on math and formal logic tasks regardless of including examples in the prompt,\nusing different question formats, or running on stronger models. We analyze CoT's behavior further\nand find that a majority of the performance gain is consistently attributed to tracing the intermediate\nsteps of a problem, which symbolic solvers are better suited for and thus CoT rarely outperforms\nthem. We believe that CoT remains a powerful technique, but to give improvement across a wider\nrange of NLP tasks, research should move beyond prompt-based CoT to new paradigms like search,\ninteracting agents, or better fine-tuned models."}, {"title": "REPRODUCIBILITY", "content": "For our experiments, we provide in-depth details of how we evaluated models on each dataset in\nSection 4.1 and Appendix A. Furthermore, we release all prompts for every dataset on Huggingface,\nincluding per model output and sampling parameters. For our meta-analysis of the literature, we\ndescribe our filtering criteria and process of annotating experiments into high-level categories in\nSection 3 and Appendix D. We also release the full list of papers in our meta-analysis together with\nextracted experimental comparisons and task category annotations."}, {"title": "EXPANDED EXPERIMENTAL DETAILS", "content": "A full list of the datasets can be found in Table 4. Each model can be seen in Table 5. We use\none answer parser for all datasets of the same answer response format (one for multiple choice,\nshort answer, etc.); however, some datasets require special handling and have edge cases that we\nhandle separately from the rest of the datasets. Similarly, for each model, we use the exact same\nprompt across them, except when closed source models require different prompts because they do\nnot allow for partial completions (i.e., when we cannot put \u201clet's think step by step\u201d to warm-start\nthe assistant's response). All prompts are given in our Huggingface repo, including the model output\nand what our answer parser extracted as the answer."}, {"title": "FEW-SHOT EXPERIMENTS", "content": "Compared to a zero-shot prompt, a few-shot prompt additionally contains demonstrations of the\nrelevant reasoning mode on different problem instances $\\{(q_i), y\\}$. Few-shot prompts for direct\nanswer simply encode the answer $a_i$ as $y^*$, whereas few-shot prompts for chain-of-thought include\na reasoning trace ending in the correct answer. Now we can define the $m$-shot direct prompt as\n$I_{da_m}(q) = \\upsilon_{da}(q_1)a_1\\upsilon_{da}(q_2)a_2 ... \\upsilon_{da}(q_m)a_m\\upsilon_{da}(q)$ and the $m$-shot cot prompt as $I_{cot_m}(q) =$\n$\\upsilon_{cot}(q_1)y_1 \\upsilon_{cot}(q_2)y_2 ... \\upsilon_{cot}(q_m)y_m \\upsilon_{cot}(q)$.\nFigure 7 shows the difference between few-shot prompting and the zero-shot setting discussed in\nthe main text of the paper. We see that using CoT in the few-shot setting largely does not change\nthe datasets that benefit from it. Only one dataset, MuSR Team Allocation, starts to improve with\nfew-shot; however, we believe this to be an exception because the final step to derive the answer\nis complex in the prompt and clearer in the examples. The magnitude of improvement over direct\nanswer prompting when using CoT is also similar to the zero-shot setting."}, {"title": "EXPANDED COT VS DIRECT EXPERIMENTAL RESULTS", "content": null}, {"title": "FULL ZERO-SHOT RESULTS", "content": null}, {"title": "EXAMPLE PROMPTS", "content": "All prompts can be found on Huggingface at https://huggingface.co/collections/TAUR-Lab/\ncot-analysis-project-66bbb9e5e0156e65059895f5. We list a few prompt response pairs here\nin this section.\nAGIEval LSAT AR zero-shot CoT prompt for Llama 3.1 70B\nExplain your reasoning step-by-step for each question before answering.\nGive your final answer in the format \"The answer is therefore <A", "following\nconditions": "Tuesday is the only day on which George can give a report.\nNeither Olivia nor Robert can give an afternoon report. If Nina gives a\nreport", "Wednesday.Q": "Which one of the following\ncould be the schedule of the students' reports? Answer Choices: (A) Mon.\nmorning: Helen; Mon. afternoon: Robert Tues. morning: Olivia; Tues.\nafternoon: Irving Wed. morning: Lenore; Wed. afternoon: Kyle (B)Mon.\nmorning: Irving; Mon. afternoon: 0Olivia Tues. morning: Helen; Tues.\nafternoon: Kyle Wed. morning: Nina; Wed. afternoon: Lenore (C)Mon.\nmorning: Lenore; Mon. afternoon: Helen Tues. morning: George; Tues.\nafternoon: Kyle Wed. morning: Robert; Wed. afternoon: Irving (D)Mon.\nmorning: Nina; Mon. afternoon: Helen Tues. morning: Robert; Tues.\nafternoon: Irving Wed. morning: Olivia; Wed. afternoon: Lenore (E)Mon.\nmorning: Olivia; Mon. afternoon: Nina Tues. morning: Irving; Tues.\nafternoon: Helen Wed. morning: Kyle; Wed. afternoon: George\nLet's think step by\nstep.\nAGIEval LSAT AR zero-shot CoT response for Llama 3.1 70B\nTo determine which one of the following could be the schedule of the\nstudents' reports", "conditions": 1.0}]}