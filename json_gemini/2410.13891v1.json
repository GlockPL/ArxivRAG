{"title": "SST: A Strong, Self-transferable, faSt, and Simple Scale Transformation for Transferable Targeted Attack", "authors": ["Yongxiang Liu", "Bowen Peng", "Li Liu", "Xiang Li"], "abstract": "Transferable targeted adversarial attacks against deep neural networks have been proven significantly more challenging than untargeted ones, yet they remain relatively underexplored in current research. Existing solutions either suffer from the requirements for extensive additional data and prolonged training for each target label (generative and universal attacks) or risk severe overfitting to the surrogate model (gradient-based attacks). These limitations significantly impede their practicality and may lead to underestimating the potential risks of targeted attacks. This paper sheds new light on performing highly efficient yet transferable targeted attacks leveraging the simple gradient-based baseline. Our research underscores the critical importance of image transformations within gradient calculations, marking a shift from the prevalent emphasis on loss functions to address the gradient vanishing problem. Moreover, we have developed two effective blind estimators that facilitate the investigation and design of transformation strategies to enhance targeted transferability under black-box conditions. The adversarial examples' self-transferability to geometric transformations such as rotation, scaling, and shear has been identified as strongly correlated with their black-box transferability, featuring these basic operations as potent yet overlapped proxies for facilitating targeted transferability. The surrogate self-alignment assessments further highlight the simple scaling transformation's exceptional efficacy, which rivals that of most existing advanced methods. Building on these insights, we introduce a scaling-centered transformation strategy termed Strong, Self-transferable, fast, and Simple Scale Transformation (SST) to enhance transferable targeted attacks. In experiments conducted on the ImageNet-Compatible benchmark dataset, our proposed SST attains a state-of-the-art (SOTA) average targeted transfer success rate across various challenging black-box models, outperforming the previous leading method by over 14% while requiring only 25.7% of the execution time. Additionally, our approach eclipses SOTA generative attacks by a considerable margin and exhibits remarkable effectiveness against real-world APIs. This work marks a significant leap forward in the field of transferable targeted adversarial attacks, revealing the realistic threats they pose and providing an effective and efficient generation method for future research.", "sections": [{"title": "INTRODUCTION", "content": "Deep Neural Networks (DNNs) have been shown to be Vulnerable to adversarial examples (AEs) [1], which refer to images with small, intentionally crafted perturbations that are usually imperceptible to the human visual system but can fool DNNs. The presence of AEs raises significant concerns when deploying DNNs in security-critical domains, including autonomous driving, face recognition, and medical image analysis. Therefore, finding, understanding, and defending AEs have received significant attention [2]\u2013[5]. AEs exhibit an intriguing property known as transferability [6], which means that AEs designed for one DNN can successfully deceive other DNNs with distinct architectures. The transferability of AEs facilitates the implementation of strict black-box attacks where queries to the victims are forbidden. As a result, significant attention has been paid to enhancing these examples' transferability in the last decade [7].\nWhile transferable untargeted attacks have been extensively investigated, yielding a substantial body of research [8]\u2013[13], Targeted Transferable Attacks (TTAs) are significantly more challenging and consequently less explored. This is primarily due to the increased complexity of directing a model to a specific target class rather than merely causing a misclassification to any class other than the original. More specifically, targeted transferability hinges on the generation of AEs that subtly align with the visual semantics of the targeted label [14], [15], as opposed to creating untargeted perturbations that just disrupt the existing visual cues. Achieving effective TTAs often entails optimizing a universal perturbation [14], [16] across tens of thousands of additional data points or indirectly through the generative learning [15], [17], [18]. Such approaches, known as resource-intensive TTAs [19], are significantly hampered in their practicality due to their reliance on extensive datasets (often a subset of the victims' training dataset), time-consuming training processes and their inherent limitation to single-target labels. In contrast, we focus on simple TTAs, which involve executing gradient descent on any (sample, label) pair, offering both efficiency and flexibility. However, these are plagued by severe overfitting and poor transferability [8], [20].\nThe limitations of simple TTAs have been attributed to the gradient vanishing issue associated with the cross- entropy loss function in targeted optimizations, prompting the proposal of a series of alternative loss functions aimed at circumventing or mitigating this issue [19], [21]\u2013[23]. In this paper, however, we shift our focus to the role of input transformations, which, despite being inherently critical within simple TTAs, have yet to be underexplored. An illustrative example of a transformation-involved attack is"}, {"title": "BACKGROUND AND PRELIMINARY", "content": "2.1 Problem Setting\nConsider an image classifier F that predicts a label among K categories, $y = \\{y_i \\mid y_i \\in Z, 0  i  K\\}$, for an input with RGB channels, $x \\in R^{3\\times H\\times W}$, where H and W denote the image height and width, respectively. Without loss of generality, we denote F = C\u00b0f, where $f(x) = p \\in [0, 1]^K$ represents the prediction distribution and $C(p) = Y_{argmax(p)}$ indexes the label with the highest confidence. TTA aims to manipulate black-box classifiers, $G = C \\circ g$, to produce an arbitrary targeted prediction, $y_t$, for any input:\n$G(x^{adv}) = y_t, s.t. ||x^{adv} - x||_p \\le \\epsilon$,\nwhere $x^{adv}$ is the AE crafted to be within a p-norm ball centered at x with radius $\\epsilon$ (the perturbation budget). In the context of transfer attacks, $x^{adv}$ is generated using the surrogate model f:\n$x^{adv} = \\underset{x': ||x' - x||_p \\le \\epsilon}{\\text{arg max }} L(f(x'), y_t)$,\nwhere L is the loss function designed to induce the targeted misclassification.\n2.2 Transferable Targeted Attacks\nWe adopt the categorization from Zhao et al. [19], which divides TTAs into simple and resource-intensive solutions. A schematic overview of these methods is presented in Fig. 1, with detailed descriptions provided below.\n2.2.1 Simple TTAs\nSimple TTAs employ gradient-based iterative optimization on a target (sample, label) pair using a surrogate model. The methodology for untargeted transferable attacks can be adapted for targeted scenarios. For instance, I-FGSM [35], which is indicative of this approach, addresses (2) through multi-step (approximately 10) gradient descent under the $l_\\infty$ constraint,, employing the cross-entropy loss function $L_{CE}$:\n$x_0^{adv} = x, x_{i+1}^{adv} = \\Pi_{\\epsilon}(x^{adv} - \\alpha \\cdot \\text{sign}(\\nabla_x L_{CE}(f(x^{adv}), y_t)))$,\nwhere $\\alpha$ represents the step size and $\\Pi_{\\epsilon}$ denotes the projection operation. While effective in untargeted attacks, the cross- entropy loss faces challenges in TTAs due to gradient vanishing issues [19], [21]. Let $\\phi$ denote the representation produced by f, such as features preceding the final linear layer. The logit values are computed as $l_i = w_i^T \\phi + b_i$, with $w_i$ and $b_i$ being the class-specific weight and bias, respectively. The normalized probability with respect to each category is given by the SoftMax function: $p_i = \\frac{e^{l_i}}{\\sum_{i=1}^Ke^{l_i}}$. The gradient\n$\\frac{\\partial L_{CE}}{\\partial x} = \\frac{\\partial L_{CE}}{\\partial \\phi} \\frac{\\partial \\phi}{\\partial x} = [(p_t - 1)w_t + \\sum_{i \\neq t}^T p_i w_i] \\frac{\\partial \\phi}{\\partial x}$\ntends to vanish as the model becomes confident ($p_{i\\neq t} \\rightarrow 0$). To address this, Li et al. introduced a Poincar\u00e9 distance-based triplet loss, $L_{Po+Trip}$, to facilitate effective optimization [21]. The Logit loss, proposed by Zhao et al. [19], $L_{Logit}$, directly maximizes the target logit and provides a direct solution to the gradient vanishing dilemma. Subsequent studies have explored ways to soften $L_{CE}$ to stabilize the optimization process, such as through margin-based logit calibration [22], $L_{MEC}$, or an adversarial optimization scheme [23], among others.\nBeyond the design of loss functions, it is the image transformations, as elaborated upon below and in Sec. 2.3, that have proven crucial for achieving effective simple TTAs and have so far been underexplored. Zhao et al. [19] demonstrated that the Diverse Inputs (DI) transformation [10] significantly enhances the transferability of targeted attacks, noting that AEs rarely transfer successfully without it. The optimization process was observed to require substantially more iterations (approximately 300) to converge. Since then, the DI method has become a standard benchmark in subsequent research. However, efforts to understand which transformations are essential for targeted"}, {"title": "A CLOSER LOOK AT TRANSFORMATIONS IN TTA", "content": "In this section, we delve into the impact of image transformations on targeted transferability. We also conduct experimental evaluations to determine if the existing consensuses can serve as universal metrics to understand the role of transformations and to guide method design.\n3.1 Analysis\nWe begin with a comprehensive evaluation to assess the current state of simple TTAs. This evaluation encompasses 30 different combinations involving six transformation scenarios-{No transformation, DI [10], ODI [34], SIA [11], BSR [12], and T- Aug [26]} and five loss functions\u2014{$L_{CE}$, $L_{MEC}$ [22], $L_{Logit}$ [19], $L_{Po+Trip}$ [21], and $L_{Logit}$ with Self-Universality (SU) strategy [27]}. In line with the established protocol [19], we employ the TMI attack as the baseline:\n$x_{i+1}^{adv} = x, g_0 = 0,$\n$g_{i+1} = \\mu \\cdot g_i + \\frac{W*\\nabla_xL(f(T(x^{adv}), y_t))}{\\|W *\\nabla_xL(f(T(x^{adv}), y_t))\\|_1},$\n$x_{i+1}^{adv} = \\Pi_{\\epsilon}(x^{adv} + \\alpha \\text{sign}(g_{i+1})).$\nHere, $\\mu$ denotes the momentum decay factor [8], and W represents a pre-defined kernel for gradient smoothing [9]. For our study, we extend the number of attack iterations to T = 900, in keeping with the increased diversity afforded by the most"}, {"title": "Failures of Existing Consensuses", "content": "In this subsection, we scrutinize the prevailing consensuses commonly referenced in the development of transformation methods for transfer attacks. Our objective is to ascertain their robustness as metrics reflecting targeted transferability and to determine their utility in guiding the design of transformations for TTAs. To achieve this, we assess the black-box transferability of various transformation techniques under both default and expanded hyperparameter configurations. We then examine the presence of a positive (or linear) correlation between their transferability and the quantitative measures derived from the established consensus, both across different methods and within individual methods. The consensuses under investigation and their corresponding measures are introduced as follows:\nDiversity. Initially, researchers emphasized the need for transformations that preserve loss [9], [10], [24], [25], ensuring that the surrogate model's output remains relatively constant post-transformation. The intensity of transformations was thus limited to a narrow scope to maintain this characteristic. However, more recent advancements have shifted towards utilizing gradient averaging over multiple transformed samples (20\u00d7) to achieve stable gradient updates [11]\u2013[13], [44]. This shift implies that increased diversity might lead to improved transferability. We quantify diversity by calculating the average cross-entropy loss values of the surrogate model across 50,000 images from the ImageNet-1k validation set [50].\nAttention Deviation. Dong et al. first pointed out that discrepancies in discriminative regions between models and defensive strategies can hinder adversarial transferability [9]. They advocated for the use of translation transformations to extend the reach of adversarial perturbations to wider attention regions. Following this, other studies have also utilized attention as a universal objective for crafting transferable attacks [51], [52]. More recently, BSR [12] and H-Aug [26] have set the new SOTAs for untargeted and targeted attacks by disrupting attention through image block shuffling and random attention-deviation transformations, respectively. We adopt the attention deviation metric proposed by Wei et al. [26], which computes the Intersection over Union (IoU) of Grad-CAM [53] activations concerning the target label between original and transformed images. The results are averaged over 1,000 images with distinct target labels.\nGradient Magnitude. A significant portion of research on simple TTAs has been devoted to addressing the issue of gradient vanishing, particularly in studies focusing on loss functions [19], [22], [23], [27]. The magnitude of gradients has often been used as an indicator to gauge the efficacy of different approaches. It is hypothesized that larger gradient magnitudes could lead to more efficient updates of perturbations, aligning with our analysis of the outcome from the dual effects of transformations. To test this hypothesis, we examine the gradient magnitudes produced by various transformations under an identical loss function. Specifically, the gradient magnitude is measured by the mean $l_2$-norm of gradients over 900 T- TMI iterations and is averaged across 1,000 images with distinct target labels.\nAll attacks are executed using the $L_{MLC}^{CE}$ loss function,"}, {"title": "THE PROPOSED METHOD", "content": "This section delineates the proposed method in a systematic manner. It integrates the motivation derived from previous analyses, delineates two plausible strategies for gauging the impact of transformation methods on targeted transferability, and culminates in the introduction of a powerful yet efficient transformation technique to augment simple TTAs.\n4.1 Motivation\nIn our research, we fervently promote the investigation into robust blind estimators for transformation-induced targeted transferability. To this end, we aspire to establish design principles for formulating transformation techniques. Our in- quiry specifically targets questions such as which transformations prove most effective and how to determine the synergistic effects of combining various transformations. We anticipate that our findings will identify transformations that facilitate highly efficient simple TTAs. Put succinctly, our goal is to surpass the efficacy of more resource-intensive attacks while preserving efficiency and the ability to handle arbitrary (sample, label) pairs. This is accomplished without resorting to additional data or retraining models, thereby striving for an optimal balance between performance and practicality. Moreover, the estimators in question should conform to the prerequisites of black-box transfer attacks, thereby ensuring their applicability to a broader spectrum of data beyond natural imagery. To begin, we focus on basic transformations and subsequently investigate their potential synergistic enhancements. The specifics of the basic transformations evaluated and their respective intensities S are meticulously defined and expounded upon in Appendix A.\nReflect on the trends elucidated in Figs. 4 and 5, which indicate that employing more powerful transformations or an increased scope of transformed images (by amplifying the hyperparameters of the given method) leads to stronger targeted transferability. We juxtapose these findings with those of resource-intensive attacks, recognizing that both are predominantly influenced by data diversity. To illustrate this, we replicate the SOTA generative method, M3D [18], with varying quantities of training data. The effects of training data volume on resource-intensive attacks (as shown in Fig. 1a) bear a striking parallel to the influence of transformation strength (in both cross- method and intra-method scenarios) on simple attacks\u2014both a paucity of transformation and insufficient training samples culminate in negligible transferability. Conversely, more robust transformations and a greater variety of transformed images, as well as an increased volume of training data, lead to marked enhancements. These insights propel us to initiate our exploration with an examination of AEs' self-transferability in relation to basic transformations.\n4.2 Mirroring Black-box Transferability via Self- Transferability\nTo elucidate, we conjecture that various transformed versions of a single image may serve as surrogates for multi-class data, thereby aiding the optimization of multi-to-single perturbations that better capture the invariant features. In certain studies, this approach has been construed as integration of transformation layers with the surrogate model, effectively emulating a range of black-box models [41], [54]. To substantiate our hypothesis, we define self-transferability as the mean efficacy of targeted adversarial perturbations across varying transformation intensities:\n$ST = E_{x\\in \\chi, s\\in S} (f(\\Tau_s(x^{adv}), t) - f(\\Tau_s(x), t))$,\nwhere s represents the intensity level of a basic transformation \\Tau. Hereafter, we omit the loss function from the discussion, as the $L_{MEC}^{CE}$ loss [22] is adopted by default. We generate AEs employing extant transformations with the TMI attack and conduct a PCC test to compare their black-box transferability, i.e., Eq. (6), with self-transferability, i.e., Eq. (7). It is important to note that methods with high correlation are deliberately excluded in the"}, {"title": "Assessing Effectiveness via Surrogate Self-Alignment", "content": "In this section, we probe deeper into comparing the ef- fectiveness of attacking basic transformations on the black- box transferability of AEs. This investigation is challenging due to the prohibition of direct evaluations on black-box models. To circumvent this, we suggest an approach rooted in the concept of model alignment. The underlying principle is straightforward: models trained on identical distributions exhibit a degree of inherent alignment. AEs that capitalize on this alignment between the surrogate and black-box models are more likely to transfer. However, the persisting discrepancies [55] between models, along with the overfitting of gradient- based optimization, sharply misalign the surrogate model with the black-box model on the distribution of AEs. In Fig. 8, we illustrate the average alignment metric between the surrogate and black-box models (defined subsequently in Eq. (8)) throughout the attack iterations. It becomes apparent that direct utilization of the surrogate model's gradients can lead to substantial misalignment with the black-box model, culminating in poor transferability. Although transformations can mitigate this misalignment, it prompts the question: how can one blindly assess the efficacy of these basic transformations in facilitating this mitigation?\nOur solution leverages what we term surrogate self- alignment induced by basic transformations. To simplify our analysis, we first examine the similarity between representations of individual samples. An illustration is depicted in Fig. 9. Given the inherent alignment between models f and g, we can reasonably postulate that the cosine similarity of their representations, $CosSim(\\phi(x), \\psi(x))$, is significantly greater than zero. More informally, we may suggest that $\\phi(x) \\approx \\psi(x)$.\nGiven an AE $x^{adv}$, crafted using a weakly-transferable algorithm like TMI that has a limited impact on the black-box model g, we hypothesize that $\\psi(x^{adv}) \\approx \\psi(x) \\approx \\phi(x)$, with all three representations being markedly different from $\\phi(x^{adv})$. This difference becomes more pronounced with each iteration of the attack. Image transformations can enhance attack effectiveness by harnessing the mutual alignment between f and g, with the aim of making $\\psi(x^{adv})$ closer to $\\phi(\\Tau(x^{adv}))$. In line with the principles of black-box attacks, we suggest an extension of this strategy such that $\\phi(\\Tau(x^{adv})) \\approx \\phi(x)$ (self-alignment).\nWe then expand the concept of similarity between individual representations to encompass the mutual alignment across the entire dataset and multiple black-box models. Let $\\Phi = \\{\\phi(x)\\}_{i=1}^b$, $\\Psi = \\{\\psi(x)\\}_{i=1}^b$, and $\\Psi^{adv} = \\{\\psi(x)\\}_{i=1}^b$ denote the sets of representations produced by models f, fo\\Tau, and g for a set of samples $\\chi = \\{x\\}_{i=1}^b$, respectively. The superscript (.)$^{adv}$ indicates the corresponding representations for the adversarial counterparts of $\\chi$, which are crafted using the TMI algorithm and model f. To quantify the mutual alignment between two sets of representations, such as $\\Phi$ and $\\Psi$, we measure the overlap of their respective k-nearest neighbors [56] throughout the dataset. This is formalized as:\n$Alignment (\\Phi, \\Psi) = \\frac{1}{k} E_{g\\in G, (\\Phi, \\psi) \\in (\\Phi, \\Psi)} \\mid dknn(\\Phi, \\Phi \\backslash \\phi) \\cap dknn(\\psi, \\Psi \\backslash \\psi)\\mid$,\nwhere dknn returns the set of indices corresponding to its k- nearest neighbors, with k set to 100 for a dataset comprising b = 1,000 images.\nTable 1 presents the alignment metrics between models for various combinations. The results are consistent with those illustrated in Fig. 9, demonstrating that the alignment scores for Alignment($\\Phi, \\Psi$) and Alignment($\\Psi, \\Psi^{adv}$) are not only comparable but also significantly exceed those of the other two scenarios. This substantiates the efficacy of our approach, leveraging the self-alignment, Alignment($\\Phi, \\Phi^{adv}$), as a proxy for desired alignment, Alignment($\\Psi, \\Psi^{adv}$), to blindly gauge the impact of different transformations on the transferability of attacks. Fig. 10 further delineates the desired alignment and self-alignment metrics for the basic transformations under"}, {"title": "S4ST", "content": "Drawing on the insights and findings from our research, we introduce a novel transformation methodology focused on scaling, named Strong, Self-transferable, Fast, and Simple Scale Transformation (S\u2074ST), designed to enhance simple TTAs. Figure 11 illustrates the schematic overview of S4ST, with detailed explanations to follow.\n4.4.1 Overview\nThe essence of the S\u2074ST framework is founded on the basic scaling transformation, informed by our preceding analysis, and seeks synergistic benefits with complementary basic operations. In alignment with recent advancements, our framework also integrates strategies for image block manipulation. To accommodate block-wise scaling and alleviate the computational demands associated with upscaled inputs, we have refined the scaling transformation to preserve dimensional consistency. As a result, the S\u2074ST framework is segmented into three distinct components: S\u2074ST-Base, S\u2074ST-Aug, and S\u2074ST-Block, each reflecting one of the three primary motivations identified.\n4.4.2 SST-Base\nWe define the intensity of the S\u2074ST-Base with the variable r, which is distinct from the definition of s in Appendix A. Consider a sample x with dimensions H \u00d7 W and an intensity r > 1. With a probability $p_r$, S\u2074ST-Base selects a relative scale factor r' from a uniform distribution $U(\\frac{1}{r}, r)$. If $r'  1, the sample x is downscaled to dimensions r'H \u00d7 r'W, followed by random zero-padding to restore its original size. In contrast, if r' > 1, a square patch of dimensions H \u00d7 W is randomly cropped from the image and then upscaled to the initial dimensions.\nScaling transformations are also utilized in existing ap- proaches, such as DI [10] and RDI [43]. The innovations of our work compared to these methods are threefold: 1) Our black-box condition analysis singles out scaling as the foremost"}, {"title": "SST-Aug", "content": "Guided by the insights gained from Fig. 7, we aimed to integrate a suite of complementary basic transformations into the S\u2074ST-Base to further boost its efficacy. With a probability $p_{Aug}$, S\u2074ST-Aug initially applies a randomly chosen transformation from an equitable selection pool of basic transformations to the entire input image. To compile this pool, we execute S4ST-Base across a spectrum of intensities $r\\in [1.02, 1.04, 1.06, 1.08, 1.1, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2]$. We then evaluate the linear correlation of self-transferability for this array of AEs over a range of basic transformations, with the results displayed in Fig. 12. We select five basic trans- formations that demonstrate the lowest PCCs with $ST_{Scaling}$. Additionally, we omit the translate and crop transformations due to their significant overlap with S4ST-Base. Following the parameterization in torchvision\u00b9, the transformation pool for S4ST-Aug includes the following basic transformations with their empirically determined parameters:\nFlip randomly flips the image vertically or horizontally.\nBrightness/Contrast/Saturation randomly alters the brightness/contrast/saturation factor from 1 to s' ~U(0,2).\nHue randomly adjusts the hue factor from 0 to s' ~U(-0.5, 0.5)."}, {"title": "SST-Block", "content": "Recent research has underscored the value of block-wise transformations in enhancing adversarial transferability. With S\u2074ST-Block, we advance this concept by randomly dividing the image into m blocks and applying S\u2074ST-Base to each block, where $m = m_h \\times m_w$ indicates partitioning the image into $m_h$ and $m_w$ segments along the coordinate axes (not specifying horizontal or vertical orientation in order).\nWe acknowledge that the design of the block-wise transfor- mation is heavily inspired by prior works such as SIA [11] and BSR [12]. It is important to reiterate that our design is rooted in the analysis conducted within this study, as opposed to being purely based on intuition or empirical choice, in accordance with the shortcomings of existing paradigms discussed in Section 3.2. This analytical approach yields notable differences between our proposed method and pre-existing ones: 1) Our method transforms"}, {"title": "EXPERIMENTS", "content": "In this section, we detail the experimental setup and conditions. We scrutinize the parameter configurations and the ablation of components for our proposed method, showcasing its novel aspects in contrast to existing techniques. This is followed by extensive experiments designed to compare our method with the SOTA transformation-based simple TTAs and the resource- intensive approaches.\n5.1 Experimental Settings\nDataset. Our experiments are conducted on the ImageNet- Compatible dataset\u00b2, a benchmark commonly utilized for evaluating TTA performance [19], [27], [34]. Released for the NIPS 2017 adversarial competition, it encompasses 1,000 images with dimensions of 299\u00d7299, each paired with a targeted label. For our evaluated models, we resize the images to 224x224.\nModels. In line with prior studies, we predominantly generate AEs using ResNet-50 [45] and transfer them to a variety of dissimilar victim models. These include CNN architectures-{MobileNet-v2 (MNv2) [57], EfficientNet-b0 (EN) [58], ConvNeXt-small (CNX) [59], Inception-v3 (INv3) [60], Inception-v4 (INv4) [61], Inception ResNet-v2 (IRv2), and Xception (Xcep) [62]} and vision transformers\u2014{ViT [63], SwinT [64], MaxViT [65], Twins [66], PiT [67], TNT [68], and DeiT [69]}. We source their pre-trained weights from the torchvision\u00b3 and timm [70] libraries. Supplemental black-box transfer experiments are performed on defense models and real-world APIs, with further details provided in subsequent sections.\nBaseline. The TMI attack is employed as the baseline. The step size and perturbation budget are configured to 2/255 and 16/255, respectively. The momentum accumulation decay factor is set at 1 [8], and a 5\u00d75 Gaussian kernel [9] is used for TI. We adopt $L_{MLC}^{CE}$ [22] with T = 900 as the attack criterion.\nCompetitors. Among simple TTAs, we compare our S\u2074ST transformation with established methods like DI [10], RDI [43], SI [25], Admix [24], SSA [44], ODI [34], SIA [11], BSR [12], T- Aug [26], and H-Aug [26]. For resource-intensive TTAs, we select Dominant Feature Attack (DFA) [14], TTP [15], and M3D [18] as competitors. We adhere to the implementations detailed in their respective publications and the TransferAttack\u2074 code base.\nMetrics. We gauge the efficacy of different methods by their average targeted success transfer rate (tSuc), i.e., the proportion of successful transfers across the dataset from the surrogate to the victim models. Additionally, we evaluate the time required by different transformations to craft AEs. All experiments are conducted on an NVIDIA GeForce RTX 4090 GPU using the PyTorch [71] framework. For consistency, the batch size is set to 10 across all experiments, ensuring that no method surpasses"}, {"title": "Analysis on S\u2074ST", "content": "5.2.1 Scaling versus Other Basic Transformations\nOur evaluation commences with the selection rationale for the scaling transformation, evaluating the judiciousness of transformation choices through the lens of the proposed self- alignment evaluation. To this end, we generate adversarial sam- ples employing each basic transformation under consideration. With a probability of 0.9, given a transformation strengths, each transformation is executed with an intensity s' \u2208 U(0, s), selecting a random variant for the surrogate model to process and compute gradients. The outcomes illustrated in Fig. 13 reveal that scaling transformation stands out, achieving an unparalleled average tSuc of 58.0% at s = 0.8. This performance is nearly 10% superior to that of the shear transformation (48.8%) and the perspective transformation (48.4%). Notably, the shear transformation shows substantial effectiveness at lower intensities; however, its efficiency markedly wanes at higher intensities. Furthermore, it is observed that employing color transformations in isolation offers marginal benefits for the attack. Yet, subsequent experimentation will demonstrate that when synergized as auxiliary augmentations to scaling, color transformations can significantly bolster targeted transferability.\n5.2.2 Comparison among Scaling's Variations\nWe perform comparative evaluations of the (R)DI methods under both unidirectional (i.e., limited to either enlargement or reduction) and bidirectional conditions (i.e., integrating both enlargement and reduction), with the findings presented in Fig. 14. Specifically, under the hyperparameter configurations from prior TTA research, the DI and RDI methods attain an average tSuc of only 22.4% and 27.3%, respectively. Our research reveals that minor adjustments to their parameters can elevate these figures to 36.6% and 41.7%. By synergistically applying these methods in both the upscale and downscale directions, we obtain a significantly enhanced average tSuc of 56.8% at r = 2.4, which, while slightly lower than the 57.2% achieved by the simple scaling transformation, is substantially surpassed by our S\u2074ST-Base approach, which further boosts the outcome to 61.6%. This highlights the superiority of our method in both unlocking and augmenting the effectiveness of the basic scaling transformation. Fig. 15 delineates the time consumption of these methods, highlighting our approach's computational efficiency. The incremental time investment required by S\u2074ST- Base compared to the baseline method (at r = 1.0) is minimal, and it markedly surpasses the competitors.\n5.2.3 Effectiveness of S\u2074ST Components\nBuilding upon the S\u2074ST-Base framework, we delve into the benefits conferred by the proposed complementary augmen- tations (S\u2074ST-Aug) and block-wise scaling (S\u2074ST-Block). We maintain the probability of scaling transformations at $p_r$ = 0.9 and progressively increase the number of image blocks m and the probability of applying complementary augmentations $p_{Aug}$. The outcomes are depicted in Fig. 16. For S\u2074ST-Block, an escalation in the number of image blocks correlates with a marked enhancement in attack efficacy, attaining its zenith at m = 6 (2 \u00d7 3). A further increase in blocks does not yield improved convergence within the configured iterations for the attack. In the case of S\u2074ST-Aug, a higher frequency of applying complementary augmentations is linked with a steady amplification in performance. Additionally, the integration of five distinct types of complementary transformations, particularly when paired with scaling (for r > 1.2), results in substantially improved outcomes compared to their solitary application (at r = 1.0). This emphasizes the soundness of our approach, which synergizes a primary transformation with complementary enhancements.\n5.2.4 Hyperparameter Setting and Component Ablation\nTo ascertain the most effective hyperparameter configuration for the given number of attack iterations, we utilize Bayesian opti-"}, {"title": "Comparison Against Existing Transformations", "content": "This subsection presents comparisons of the proposed S\u2074ST with existing transformation methods across various settings.\n5.3.1 Single Surrogate Attacks\nTable 4 reports the single surrogate model (ResNet-50) eval- uation. Herein the transformed copies per iteration for SSA, SIA, BSR, and DeCoWa are limited to 1. We also set $m_1$ = 1, $m_2$ = 5 for Admix to ensure a fair comparison with SI. The results demonstrate that the proposed S\u2074ST exhibits superior performance, not only achieving significantly higher average Suc than the previous SOTA in simple TTAs, surpassing H- Aug by 14.2% and ODI by 24.9%, but also leading in terms of efficiency, with computational time only 25.7% of H-Aug and 28.4% of ODI. In fact, as can be seen in conjunction with Fig. 14, even the simple scaling transformation and our S\u2074ST- Base are sufficient to achieve transferability on par with all methods except for T/H-Aug. And the further enhancements"}, {"title": "Ensemble-based Attacks and Against Robust Victims", "content": "We further extend our comparison to scenarios involving the generation of AEs based on an ensemble of surrogate models and evaluations using robust victim models. For the selection of ensemble surrogate models, we distinguish between the commonly used self-ensemble in untargeted transfer attacks [11", "13": [44], "8": "which involves calculating the loss function using the average logits from different models to obtain gradients. In line with the evaluation protocols of previous research [15", "18": "we incorporate robust victim models for evaluation, including Augmix [74", "75": "adversarial training (AT) [2", "76": "and Ensemble AT [3"}]}