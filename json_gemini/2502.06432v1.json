{"title": "Prompt-SID: Learning Structural Representation Prompt via Latent Diffusion for Single-Image Denoising", "authors": ["Huaqiu Li", "Wang Zhang", "Xiaowan Hu", "Tao Jiang", "Zikang Chen", "Haoqian Wang"], "abstract": "Many studies have concentrated on constructing supervised models utilizing paired datasets for image denoising, which proves to be expensive and time-consuming. Current self-supervised and unsupervised approaches typically rely on blind-spot networks or sub-image pairs sampling, resulting in pixel information loss and destruction of detailed structural information, thereby significantly constraining the efficacy of such methods. In this paper, we introduce Prompt-SID, a prompt-learning-based single image denoising framework that emphasizes preserving of structural details. This approach is trained in a self-supervised manner using down-sampled image pairs. It captures original-scale image information through structural encoding and integrates this prompt into the denoiser. To achieve this, we propose a structural representation generation model based on the latent diffusion process and design a structural attention module within the transformer-based denoiser architecture to decode the prompt. Additionally, we introduce a scale replay training mechanism, which effectively mitigates the scale gap from images of different resolutions. We conduct comprehensive experiments on synthetic, real-world, and fluorescence imaging datasets, showcasing the remarkable effectiveness of Prompt-SID.", "sections": [{"title": "Introduction", "content": "Image noise arises from diverse sources, including sensor noise and environmental factors, alongside potential introduction during quantization and image processing procedures, thereby exerting adverse impacts on downstream tasks such as classification (Wang et al. 2017), detection (Shijila, Tom, and George 2019), and segmentation (Liu et al. 2020). Consequently, the quest for efficacious image denoising methodologies assumes critical significance within the domain of computer vision research.\nIn recent years, there has been a proliferation of learning-based supervised denoising methodologies (Zhang, Zuo, and"}, {"title": "Related Works", "content": "Self-Supervised Image Denoising\nSelf-supervised image denoising methods have evolved primarily along two paths. The first path, exemplified by methods like noise2void (N2V) (Krull, Buchholz, and Jug 2019), employs blind spot to introduce invisible pixels within the central region of convolutional kernels, thereby circumventing the issue of identity mapping. Recent advancements such as AP-BSN (Lee, Son, and Lee 2022) extend blind spot networks by introducing a shuffling mechanism to disrupt the spatial continuity of noise in natural images. Additionally, some studies (Wang et al. 2023) modify the blind spot areas within convolutional kernels. The second path, as Fig. 2 shows, represented by noise2noise (N2N) (Lehtinen et al. 2018), positing that training with L2 loss tends to converge towards the mean of observed values. This suggests the feasibility of replacing desired training targets with distribu-"}, {"title": "Method", "content": "To address the issues of low pixel utilization and structural damage, we made the following improvements in Prompt-SID. Firstly, We applied a spatial redundancy sampling strategy to minimize pixel wastage. Secondly, during the training phase at the downscaled image, we introduced RG-Diff for extracting structural representations via latent diffusion. Leveraging the generative capacity of the diffusion model,"}, {"title": "Spatial Redundancy Sampling Strategy", "content": "Following the principles of noise2noise, targets that adhere to a zero-mean noise while similar to the ground truth can serve as a supervisory signal. So we sample the input and target for network training within a single noisy image.\nBy employing spatial redundancy sampling strategy $m$, we can obtain sub-images $m_1(x)$, $m_2(x)$, $m_3(x)$ from the original-scale noisy image $x$. First, we divide the image $x$ into $h/2 \\times w/2$ small blocks, with each block containing four pixels. The small block located in the $i$-th row and $j$-th column is named $b(i, j)$. From each block, we randomly sample three adjacent pixels $p_1(b(i,j))$, $p_2(b(i, j))$, $p_3(b(i,j))$, where $p_1(b(i, j))$ is adjacent to the other two pixels. The selection of $p_2(b(i,j))$ and $p_3(b(i,j))$ is random among the remaining two pixels. Subsequently, we obtained three sub-images that are one-fourth the size of the original image.\nThe process can be written as follows:\n$\\qquad m_n(x) = \\sum_{i=1}^{h/2} \\sum_{j=1}^{w/2} p_n (b(i,j)), \\quad n = 1, 2, 3 \\qquad(1)$"}, {"title": "Structural Representation Generation Diffusion", "content": "We propose structural representation generation diffusion (RG-Diff), performing the diffusion process within a $1 \\times N$ dimensional vector space. To minimize the randomness in the generation process, we design a joint training framework using the $L_1$ loss in vector space, and integrating the generated representations into the feature map processing branch, rather than directly decoding them into output results. The operational principle of RG-Diff is illustrated in Fig. 4.\nFirst, we designed a pixel structure encoder (PSE) to compress image information into the implicit space and extract structural representations. The PSE comprises several residual blocks, a global average pooling layer, and two linear layers. We encode the downscaled image $m_1(x)$ and the original scale image $x$, resulting in the structural representations of the downsampled image $c_{sub}$ and the original scale image $\\mathbf{c_{org}(0)}$, respectively. The process can be represented by the following equation:\n$\\qquad c_{sub} = PSE(m_1(x))\\qquad(2)$\n$\\qquad c_{org}(0) = PSE(x)\\qquad(3)$\nSubsequently, we perform the forward diffusion process based on $c_{org}(0)$. At a sampled time step $t$, the forward diffusion is carried out using the following equation, where $c_{org}(0)$ serves as the initial state. We introduce noise to this representation according to the Markov process.\n$\\qquad q(c_{org}(t)|c_{org}(0)) = N(c_{org}(t); \\sqrt{\\bar{a}_t}c_{org}(0); (1-\\bar{a}_t)I) \\qquad(4)$\nHere, $c_{org}(t)$ represents the state with noise obtained after $t$ steps of sampling on $c_{org}(0)$. $\\bar{a}_t$ is a manually designed hyperparameter. $\\beta_t$ is the predefined scale factor, which increases linearly with the time steps. The relationship between $\\bar{a}_t$ and $\\beta_t$ satisfies: $a_t = 1 - \\beta_t$, $\\bar{a}_t = \\prod_{t=1}^{t} a_t$.\nIn the reverse process, we incorporate $c_{sub}$ as a conditional control input through concatenation during the $t$-step denoising procedure. Given that the features are one-dimensional vectors, we employ MLP for this task. Unlike"}, {"title": "Structural Prompt Integrative Transformer", "content": "We employ the vision transformer (ViT) (Dosovitskiy et al. 2020) module as the branch of image reconstruction. Similar to prior research (Zamir et al. 2022a; Zhang et al. 2023;"}, {"title": "Scale Replay Mechanism and Loss", "content": "After passing through SPIformer, we derive $f_\\theta(m_1(x))$, where $f_\\theta$ denotes the network parameters requiring optimization in RG-Diff and SPIformer. The reconstruction loss is computed by evaluating the $L_2$ loss between $f_\\theta(m_1(x))$ and $m_2(x)$, as well as between $f_\\theta(m_1(x))$ and $m_3(x)$. The specific formula is outlined as follows:\n$\\qquad L_{rec}=||f_\\theta(m_1(x))-m_2(x)||_2+|| f_\\theta(m_1(x))-m_3(x)||_2 \\qquad(9)$\nIn the preceding discussion, we emphasized the necessity of addressing the generalization between downscaled and original-scale images. Our objective is to train a model capable of alleviating the domain gap between them. Therefore, in each iteration, we conduct an additional inference process"}, {"title": "Experiment", "content": "Implementation Details\nTraining Details. We select supervised method (Ronneberger, Fischer, and Brox 2015), CBM3D (Dabov et al. 2007a), BM3D (Dabov et al. 2007b), anscombe (Makitalo and Foi 2010), noise2void(N2V) (Krull, Buchholz, and Jug 2019), NBR2NBR (Huang et al. 2021), blind2unblind(B2U) (Wang et al. 2022), zero shot noise2noise(ZS-N2N) (Mansour and Heckel 2023) for writing. More comparative experimental results can be found in the supplementary material. We obtain quantitative and qualitative results from other methods by adopting official pre-trained models and running their public codes.\nFor training, we fixed the decay rate for the exponential moving average at 0.999 and initialized the learning rate to 0.0002. Parameter optimization and computation were performed with Adam optimizer, setting $\\beta_1$ to 0.9 and $\\beta_2$ to 0.99. All training was executed on one Nvidia RTX3090.\nDatasets. In synthetic denoising, we curated a training set comprising 44,328 images from the ILSVRC2012 dataset (Deng et al. 2009), with testsets named kodak, BSD300 (Martin et al. 2001), and set14 (Zeyde, Elad, and"}, {"title": "Conclusion", "content": "We present Prompt-SID, a prompt-learning-based self-supervised image denoising framework that primarily addresses the semantic degradation and structural damage caused by the sampling processes of previous self-supervised methods. Our approach demonstrates the immense potential of the diffusion model and prompt-learning in image denoising tasks. We design a structural representation generation diffusion(RG-Diff) based on a latent diffusion model, using the degraded structural representations as conditional information to guide the recovery of undamaged ones. Additionally, through the scale replay mechanism, our method effectively reduces the scale gap between subsampled and original scale images. Extensive experiments demonstrate that our method consistently achieves state-of-the-art performance across synthetic, real-world, and fluorescence imaging datasets."}]}