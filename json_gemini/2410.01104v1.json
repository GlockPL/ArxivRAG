{"title": "softmax is not enough (for sharp out-of-distribution)", "authors": ["Petar Veli\u010dkovi\u0107", "Christos Perivolaropoulos", "Federico Barbero", "Razvan Pascanu"], "abstract": "A key property of reasoning systems is the ability to make sharp decisions on their\ninput data. For contemporary Al systems, a key carrier of sharp behaviour is the\nsoftmax function, with its capability to perform differentiable query-key lookups.\nIt is a common belief that the predictive power of networks leveraging softmax\narises from \"circuits\u201d which sharply perform certain kinds of computations consis-\ntently across many diverse inputs. However, for these circuits to be robust, they\nwould need to generalise well to arbitrary valid inputs. In this paper, we dispel this\nmyth: even for tasks as simple as finding the maximum key, any learned circuitry\nmust disperse as the number of items grows at test time. We attribute this to a\nfundamental limitation of the softmax function to robustly approximate sharp\nfunctions, prove this phenomenon theoretically, and propose adaptive temperature\nas an ad-hoc technique for improving the sharpness of softmax at inference time.", "sections": [{"title": "Motivation", "content": "It is no understatement to say that the $\\text{softmax}_{\\theta} : \\mathbb{R}^n \\rightarrow [0,1]^n$ function\u00b9:\n$\\text{softmax}(e) = \\begin{bmatrix} \\frac{\\exp(e_1/\\theta)}{\\sum_k \\exp(e_k/\\theta)} \\\\ ... \\\\  \\frac{\\exp(e_n/\\theta)}{\\sum_k \\exp(e_k/\\theta)} \\end{bmatrix}$\n(1)\nis one of the most fundamental functions in contemporary artificial intelligence systems.\nThe role of softmax in deep learning is to convert any vector of logits, $e \\in \\mathbb{R}^n$, into a probability\ndistribution, in a form that is part of the exponential family. Further, softmax allows for application\nof a temperature parameter, $\\theta\\in \\mathbb{R}$, to adjust the amount of probability mass attached to the highest\nlogit-a concept borrowed from the Boltzmann distribution in statistical mechanics.\nInitially, the primary utilisation of softmax in deep learning was within the final layer of classifiers.\nIts influence in this domain vastly expanded after it saw use in the internal layers\u2014as a differen-\ntiable key-value store [GWD14] or a mechanism for attending over the most relevant parts of the\ninput [BCB15]. This attentional framing of softmax was critical in defining important models for\nsequences [VSP+17, Transformers], images [DBK+21, ViTs] and graphs [VCC+18, GATs].\nSeveral efforts attribute the success of softmax to its capability of modelling computations relevant to\nreasoning. This can be related to the concept of circuits in theoretical computer science [AB09]. Sev-\neral interpretable pieces of \u201ccircuitry\" [OCS+20] have already been discovered in large Transformers,\nprimarily under the umbrella of mechanistic interpretability [ENO+21, OEN+22, WVC+22].\nHere we study the robustness of such circuitry, especially when going beyond the distribution the\nmodels are trained on\u2014a critical regime for reasoning engines. We find that, in spite of its many\nsuccesses, softmax does not have a chance to robustly generalise such circuits out of distribution,\nespecially because it provably cannot approximate sharpness with increasing problem size (Figure 1).\nHere we call a function sharp if its output only depends on a constant number of its inputs (e.g. max)."}, {"title": "Demonstrating and proving the dispersion in softmax and Transformers", "content": "To motivate our theory, we train a simple architecture including a single dot-product attention head\nto predict a feature of the maximum item in a set. Each item's features are processed with a deep\nMLP before attending, and the output vector of the attention is passed to a deep MLP predictor (see\nAppendix A for experimental details). We train this model using sets of < 16 items, and in Figure 2\nwe visualise the head's attentional coefficients, computed over sets of varying size at inference time."}, {"title": "Adaptive temperature", "content": "Since we now know dispersion is inevitable, are there any ways we can leverage our theory's\nfindings to make softmax sharper? One obvious constraint our theory rests on is the assumption that\n$\\theta > 0$, i.e. that our temperature is nonzero. While zero temperature also known as hard attention\n[DBLdF12, Ran14, MHG+14, XBK+15]\u2014guarantees sharpness, training large-scale Transformers\nwith it tends to not work well in practice [BIB+24].\nWhat about applying zero temperature to an already-trained Transformer? We can show that this is\nalso problematic since, for any attention head where the Transformer has learnt to induce sharpness,\nit necessarily did so by increasing magnitude of its weights (see Appendix B for a proof):\nProposition 3.1 (Sharpness in Transformers necessitates large weights). Let $e^{(n)} \\in \\mathbb{R}^n$ be a collec-\ntion of n logits, computed using a dot product attention mechanism; i.e. $e_k^{(n)} = (Qy, Kx_k)$, where\ny \u2208 Rm is a query vector and Q, K \u2208 $R^{m'\u00d7m}$ are parameters. Let $\\delta^{(n)} = \\max_{1 \\leq i \\leq n} e_i^{(n)} - \\min_{1 \\leq j \\leq n} e_j^{(n)}$ be\ntheir maximum difference. Then \u03b4 is upper bounded as:\n$\\delta < 2 \\sigma_{max}^{(Q)} \\sigma_{max}^{(K)} \\max \\|y\\| \\max_{1 \\leq i \\leq n} \\|x_i\\|$\nwhere $\\sigma_{max}^{(Q)}, \\sigma_{max}^{(K)} \\in \\mathbb{R}$ are the largest singular values of Q and K. That is, the sharpness of the\nsoftmax in Transformers depends on the norm of its parameters.\nNote that there is a common practice of leveraging operators such as layer normalisation [BKH16]\nextensively within Transformer architectures, which clamps $||x_i||$ and $||y||$ if applied right before the\nquery-key mechanism, more directly accentuating the impact of the singular values of Q and K.\nHowever, forcing large parameters promotes overfitting, and the likelihood that the incorrect item\ngets the largest logit-see Figure 2. Setting temperature to zero will then degrade accuracy-we\nmight prefer to make the coefficients sharper while making sure that the chosen item is not left behind.\nThis motivates our use of adaptive temperature, where we vary $\\theta$ depending on the entropy in the\ninput coefficients. Adaptive temperature can be elegantly motivated by the fact that decreasing the\ntemperature must monotonically decrease the entropy, which is well-known in thermodynamics:\nProposition 3.2 (Decreasing temperature decreases entropy). Let $e^{(n)} \\in \\mathbb{R}^n$ be a collection of\nn logits. Consider the Boltzmann distribution over these n items, $p_i \\propto \\exp(-\\beta e_i^{(n)})$ for $\\beta \\in \\mathbb{R}$,\nand let $H = \\sum_i p_i \\log p_i$ be its Shannon entropy. Then, as $\\beta$'s magnitude increases, H must\nmonotonically decrease. Thus, since $\\beta \\propto \\frac{1}{\\theta}$ where $\\theta$ is the temperature in softmaxe, decreasing the\ntemperature must monotonically decrease the entropy."}, {"title": "Experimental results", "content": "To validate the utility of our proposed adaptive temperature scheme, we evaluate it on both our\npreviously-mentioned max retrieval task\u2014which allows us a pristine environment for evaluating\nwhether adaptive temperature leads to more useful attention heads\u2014as well and the CLRS-Text\nalgorithmic reasoning benchmark for language models [MMI+24], which represents a challenging\nreasoning task for decoder-only Transformers, and is hence likely to require low-entropy behaviour."}, {"title": "max retrieval", "content": "For this task, we first train our single attention head architecture as described in Appendix A; then,\nwe evaluate it at various numbers of input items, with and without applying adaptive temperature to\nits sole softmax function call. Note that this is a \"pure\" inference time adjustment-no modifications\nto the learned parameters are performed!\nThe results-averaged over ten seeds and with statistical significance tests applied\u2014are summarised\nin Table 1. As is evident, applying adaptive temperature leads to a more performant retrieval head on\nout-of-distribution inputs, with statistical significance ascertained via a paired t-test.\nThese results are further supplemented by a qualitative comparison of the softmax coefficients before\nand after applying the temperature adaptation. As can be seen in Figure 6, our proposed adaptive\ntemperature adaptation indeed leads to sharper coefficients out-of-distribution and higher attention\nbeing directed to the desired item, even in situations where it did not receive the largest logit.\nWe have now successfully validated the predictions of our theory in a controlled environment. What\nabout a more challenging benchmark with a baseline model comprising many attentional heads?"}, {"title": "CLRS-Text", "content": "In this benchmark, we follow the protocol established by [MMI+24] and fine-tune Gemma 2B models\n[GMH+24] on the thirty algorithmic execution tasks in CLRS-Text, plotting their performance profiles\nin- and out-of-distribution at various problem sizes.\nWhile it may be tempting to directly re-apply our learned adaptive temperature function from Figure\n5 solely at inference time-the same way we did in the max retrieval experiments\u2014this approach\ndoes not empirically work well in the CLRS-Text regime. This is due to the fact that CLRS-Text\ninputs are often textual representations of floating-point numbers and therefore individual numbers\noften span multiple tokens. It is therefore insufficient and inappropriate to aim for entropy levels\nwhere all the focus would be on one token only, as was desirable in the max retrieval task.\nOne follow-up on this could be to perform exactly the same polynomial fit exercise leading up\nto Figure 5, only this time focussing on \u201coptimal\" values of temperature for Gemma's attentional\nheads. However, in this regime, we argue this exercise is substantially less trivial to do-as we are\nnow dealing with a system spanning many attentional heads across many layers, it is not easy to\neven discover relevant attentional heads' behaviours, and even less so to ascertain that the model's\nrobustness depends on those specific heads in those ways. As briefly discussed before, any such\nindividual endeavour typically leads to a brand-new research project in mechanistic interpretability,\nand we do not find this to be in-scope of our paper."}, {"title": "Conclusions", "content": "\"Energy continuously flows from being concentrated\nTo becoming dispersed, spread out, wasted and useless.\u201d\u2014The 2nd Law: Unsustainable, by Muse\nIn this paper, we have provided extensive theoretical and empirical evidence that the softmax-a key\nfunction in the design of modern frontier architectures-is fundamentally unable to sustain robust\nreasoning behaviours across all possible inputs, as its output coefficients are necessarily dispersing\nprovided sufficient input elements.\nBeyond illustrating and proving these dispersion effects, we also attempted to use our theoretical\nframework to propose an adaptive temperature approach that is able\u2014at least to a certain extent-to\nhold the dispersion effect at bay. It is our opinion that the favourable results we observe with adaptive\ntemperature warrant further investigation, and indicate that such adaptive layers are a strategy worth\ndedicating further attention to in future work.\nWe conclude by remarking, once again, that adaptive temperature is merely an ad-hoc method and it\ndoes not escape the conclusions of our theory! The key takeaway of our paper is not the adaptive\ntemperature proposal; it is the fact that we find it worthwhile to more seriously invest in research of\nhybrid architectures that will not fully rely on the softmax function, at least within the confines of\nthe assumptions of our theory. To name a few possibilities:\n\u2022 Any kind of unnormalised attention, such as linear [Sch92] or sigmoid attention [RDD+24]\ndoes not have the dispersion issues presented here. That being said, it becomes substantially\nharder to meaningfully rank items using them, see e.g. the GATv2 paper [BAY22].\n\u2022 Similarly, forcing the attention to be hard or local would also escape the confines of\nour theory. We already briefly discussed the challenges of learning with hard attention-\nlocal attention provides a very interesting alternative, but it must be stressed that \"out-of-\ndistribution\" behaviours for certain heads may appear even at highly \u201clocal\u201d scales; OOD\nhere refers to going outside the largest problem size the head saw at training time, not the\nlargest context deployed at training time.\n\u2022 Lastly, our key Theorem relies on the model being built out of continuous building blocks. In-\nserting discontinuities in the feedforward layers\u2014perhaps using approaches like [DvGPV24]\nas inspiration-would also break the assumptions of our theory, though it comes with obvi-\nous challenges to learning at scale.\nWhile such approaches haven't seen as much success at scale as the \u201cvanilla\u201d Transformer, we hope\nour results inspire future work into making them stable, especially for constructing reasoning systems."}]}