{"title": "A Comprehensive Survey of Datasets, Theories, Variants, and Applications in Direct Preference Optimization", "authors": ["Wenyi Xiao", "Zechuan Wang", "Leilei Gan", "Shuai Zhao", "Wanggui He", "Luu Anh Tuan", "Long Chen", "Hao Jiang", "Zhou Zhao", "Fei Wu"], "abstract": "With the rapid advancement of large language models (LLMs), aligning policy models with human preferences has become increasingly critical. Direct Preference Optimization (DPO) has emerged as a promising approach for alignment, acting as an RL-free alternative to Reinforcement Learning from Human Feedback (RLHF). Despite DPO's various advancements and inherent limitations, an in-depth review of these aspects is currently lacking in the literature. In this work, we present a comprehensive review of the challenges and opportunities in DPO, covering theoretical analyses, variants, relevant preference datasets, and applications. Specifically, we categorize recent studies on DPO based on key research questions to provide a thorough understanding of DPO's current landscape. Additionally, we propose several future research directions to offer insights on model alignment for the research community.", "sections": [{"title": "Introduction", "content": "Through pre-training on extensive, high-quality corpora using the next token prediction objective with a huge amount of computational costs, Large Language Models (LLMs) [OpenAI, 2022, Touvron et al., 2023a, OpenAI, 2024a, Jiang et al., 2023] assimilate comprehensive world knowledge into their internal parameters, demonstrating impressive language understanding and generation abilities. Further, LLMs have been extended to accommodate multi-modality inputs, including both language and vision, thereby giving rise to Large Vision Language Models (LVLMs) [OpenAI, 2023a, Liu et al., 2024a, Team et al., 2023, Bai et al., 2023]. These foundation models serve as a versatile solution and have achieved exceptional performance across a broad spectrum of both language and visio-language tasks, marking a significant milestone in the advancement toward artificial general intelligence.\nAs these foundation models grow larger and more powerful, they are still found grappling with following the user's instruction (explicit objective) and fulfilling the goals of being Helpful, Honest and Harmless (implicit objective), which attribute to the misaligned next token prediction task used in the pre-training stage [Leike et al., 2018, Askell et al., 2021, OpenAI, 2023b]. Therefore, a typical post-training stage, known as preference optimization (e.g., Reinforcement Learning from Human Feedback, RLHF), is additionally performed at the response level to align pre-trained language models with the user's intentions and ensure they remain helpful, honest, and harmless [Ouyang et al., 2022a, Dai et al., 2024, Sun et al., 2023]. RLHF first trains an explicit reward model on collected human preferences. Subsequently, RLHF fine-tunes the policy model (i.e., the LLM targeted for fine-tuning) with reinforcement learning (RL) algorithms (e.g., Proximal Policy Optimization(PPO; [Schulman et al., 2017a])) to output responses which can maximize the response reward rated by the reward model but not deviate too far from a reference model constrained by KL-divergence. Nevertheless, RLHF requires meticulous hyper-parameters tuning and extensive computational resource to maintain the RL training stability. Moreover, some research has identified several challenges associated with this explicit reward modeling, such as reward hacking [Casper et al., 2023], reward misspecification [Pan et al., 2022] and out-of-distribution generalization [Tien et al., 2023]."}, {"title": "Preliminary", "content": "In this section, we will revisit the foundational concepts of RLHF [Ouyang et al., 2022a] (Reinforcement Learning from Human Feedback) and DPO [Rafailov et al., 2023] (Direct Preference Optimization) to highlight the necessity of DPO fine-tuning as a solution to RLHF's shortcomings."}, {"title": "Reinforcement Learning from Human Feedback", "content": "The typical RLHF pipeline for large language models (LLMs) generally consists of three key phases: (1) supervised fine-tuning (SFT), (2) preference sampling and reward model training, and (3) reinforcement learning (RL) optimization.\nIn the SFT stage, RLHF typically starts with a pre-trained language model, which is further refined using supervised fine-tuning on a curated dataset of high-quality human-generated responses. This supervised fine-tuning stage produces an initial model, denoted as \u03c0SFT, which has improved but still not fully optimized alignment with human preferences. This model serves as the baseline for the subsequent RLHF stages. The preference learning stage involves collecting human feedback in the form of preference data. Given pairs of responses generated by SFT, human evaluators express a preference for one response over the other. These preferences serve as the foundational feedback for reward model training. The reward model's objective is to quantify these preferences by assigning a numerical score to each output, effectively converting human feedback into a scalar reward.\nOne widely used formula in this stage is the Bradley-Terry [Bradley and Terry, 1952b] (BT), which proposes a mechanism for modeling pairwise comparisons and assigning concrete reward values to the outputs. In the context of RLHF, BT is used to predict human preferences between pairs of responses. The resulting reward function is then used to guide the concrete model's decision-making by providing feedback on how well each generated response aligns with human preferences.The BT formula proposes that the human preference distribution p\u2217 for a given response pair can be expressed as the following formula:\n$p^* (y_1 \\succ y_2 | x) = \\frac{\\exp (r^* (x, y_1))}{\\exp (r^* (x, y_1)) + \\exp (r^* (x, y_2))}$\nReward Modelling. Given the dataset of prompts and pairs of D = {x(t), yw(t),y\u0131(t)}, a reward model r\u00f8(y, x) can be parameterized to predict the alignment of a given response y with human preferences. The parameters of the reward model can be optimized using the maximum likelihood estimation. The loss function used for this estimation can be expressed as the following formula:\n$L_R (r_\\phi, D) = -E_{(x, y_w, y_l) \\sim D} [\\log \\sigma (r_\\phi (x, y_w) - r_\\phi (x, y_l))]$\nIn EQ.(2), \u03c3 represents the sigmoid function, which transforms the difference between the reward values of the winning and losing responses into a probability that reflects the model's confidence in the preference. The difference in rewards, r\u00f8(x, yw) - r\u00f8(x, yl), is passed through the sigmoid function to map this difference to a probability in the range [0,1], corresponding to the model's confidence that yw is the preferred response. The negative sign in the loss function ensures"}, {"title": "Direct Preference Optimization", "content": "In RLHF, the process is intricate, involving the training of a reward model and the iterative sampling from the language model's policy during the training loop. This complexity arises from the need to continuously evaluate and refine the model based on feedback from the reward model, resulting in significant computational demands. Direct Preference Optimization (DPO) offers a streamlined alternative by optimizing the same objective as RLHF but bypasses the explicit need for a separate reward model, thereby reduceing the computational costs associated with aligning the LLM.\nDPO Objective. Deriving from the KL-constrained reward maximization objective in EQ.(3), DPO has the mathematically equivalent form as the following equation:\n$\\pi_r(y | x) = \\frac{1}{Z(x)} \\pi_{ref}(y | x) \\exp (\\frac{1}{\\beta} r(x, y))$\nwhere Z(x) = \u03a3y \u03c0ref(y | x) exp (\u03b2r(x, y)) is the partition function. The partition function Z(x) normalizes the policy distribution \u03c0(y | x). It is calculated by summing over the exponential terms of the reward function weighted by the reference model's distribution for all possible responses y.\nThis summation ensures that \u03c0r(y | x) is a valid probability distribution, but it is unpratical to enumerate all possible responses to traverse of all possible input x, which requires significant computational resources, especially when handling a large response space. Actually, DPO algorithm mitigates the need for explicit reward model training and sampling from the LM policy. By directly optimizing preferences without the intermediate step of training a reward model, DPO simplifies and reduces the overall computational burden, making it a more efficient approach for language model alignment. To be specific, the objective function in (4) can be rearranged to isolate r(x, y), yielding:\n$r(x, y) = \\beta \\log \\frac{\\pi_r(y | x)}{\\pi_{ref}(y | x)} + \\beta \\log Z(x)$\nIn this form, r(x, y) is expressed in terms of the optimized policy \u03c0, the reference model tref, and the partition function Z(x). By substituting this reparameterization into the original reward function r\u2217, we obtain:\n$r^* (x,y) = \\beta \\log \\frac{\\pi^* (y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x)$"}, {"title": "Research Questions and Variants", "content": "In this section, we discuss key research questions on DPO to provide a thorough understanding of its current landscape. We start by comparing DPO with RLHF to highlight the advantages and limitations of DPO. We then investigate the effects of implicit versus explicit reward modeling, focusing particularly on generalization challenges. Additionally, we discuss the effects of different feedback and analyze the roles of the KL penalty coefficient and reference model. Lastly, we review advancements in Online DPO and discuss issues like reward hacking and alignment tax.\nRQ0: why DPO? Large Language Models have recently developed rapidly with the supervised fine-tuning (SFT) training [Chung et al., 2022]. However, due to the next token prediction misaligned with the objective \"follow the user's instructions helpfully and safely\" [Brown et al., 2020, Fedus et al., 2022], SFT models often exhibit unintended behaviors. For example, they might generate toxic, harmful, hallucinatory, and biased responses [Chung et al., 2022]. Furthermore, SFT solely instructs models to learn from correct responses while neglecting to address incorrect ones, thus lacking the consideration of human values or preferences [Zhao et al., 2023].\nA potential solution to this issue is Reinforcement Learning from Human Feedback (RLHF), using human preferences as a reward signal to fine-tune models by reinforcement learning [Chung et al., 2022, Bai et al., 2022b, Lee et al., 2023], such as PPO [Schulman et al., 2017b] and actor-critic [Mnih et al., 2016, Haarnoja et al., 2018]. We follow the mainstream work [Ziegler et al., 2019, Stiennon et al., 2020] and focus on PPO [Schulman et al., 2017b] in this paper.\nNevertheless, RLHF requires loading four models (policy, reference, reward, and critic) and involves training sensitive hyperparameters along with substantial and costly human oversight to develop a well-trained reward model. Furthermore, some research has identified several challenges associated with reward modeling. For instance, reward misspecification [Pan et al., 2022] occurs when a scalar score from the reward model fails to comprehensively represent human preferences. Additionally, misgeneralization [Tien et al., 2023] arises when reward models compute rewards using unexpected or contingent features of the environment [Michaud et al., 2020], leading to causal confusion and poor out-of-distribution generalization. Another issue is reward hacking [Casper et al., 2023], for example, without regularization to penalize the KL divergence between a base model and the fine-tuned model, large language models (LLMs) undergoing RL often learn to output nonsensical text [Stiennon et al., 2020].\nA powerful alternative that does not require an explicit reward model or RL to RLHF is Direct Preference Optimization (DPO) [Rafailov et al., 2023]. DPO derived the closed-form solution of the PPO optimization objective, revealing the relationship between the reward and the optimal policy model. It then reparameterizes the reward modeling loss with"}]}