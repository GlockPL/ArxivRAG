{"title": "Communication-Aware Consistent Edge Selection for Mobile Users and Autonomous Vehicles", "authors": ["Nazish Tahir", "Ramviyas Parasuraman", "Haijian Sun"], "abstract": "Offloading time-sensitive, computationally intensive tasks\u2014such as advanced learning algorithms for autonomous driving\u2014from vehicles to nearby edge servers, vehicle-to-infrastructure (V2I) systems, or other collaborating vehicles via vehicle-to-vehicle (V2V) communication enhances service efficiency. However, whence traversing the path to the destination, the vehicle's mobility necessitates frequent handovers among the access points (APs) to maintain continuous and uninterrupted wireless connections to maintain the network's Quality of Service (QoS). These frequent handovers subsequently lead to task migrations among the edge servers associated with the respective APs. This paper addresses the joint problem of task migration and access-point handover by proposing a deep reinforcement learning framework based on the Deep Deterministic Policy Gradient (DDPG) algorithm. A joint allocation method of communication and computation of APs is proposed to minimize computational load, service latency, and interruptions with the overarching goal of maximizing QoS. We implement and evaluate our proposed framework on simulated experiments to achieve smooth and seamless task switching among edge servers, ultimately reducing latency.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of 5G ultra-communication has exponentially expanded the possibilities and services surrounding autonomous driving. Autonomous vehicles must process vast amounts of data in real time while maintaining high mobility. In the context of Industry 4.0, the importance of computation offloading is driven largely by Internet of Things (IoT) applications. To address these demands, autonomous vehicle systems must utilize a combination of local on-board processing and supplementary remote processing power [1]. To enable these remote processing capabilities, the 5G architecture incorporates multi-access edge computing (MEC). This integration allows for advanced remote processing services that augment autonomous driving with functionalities such as automated traffic management. Here, real-time traffic analysis across an entire area aids vehicles in efficiently navigating to their destinations. Within the literature, use cases that merge vehicular networks with MEC are referred to as vehicular edge computing (VEC) [2], [3]. It offers an appealing alternative to Cloud Computing by facilitating low-latency, computationally efficient operations while concurrently maximizing system performance.\nThe MEC (Multi-access Edge Computing) concept involves replicating traditional cloud data centers' processing and storage capabilities to the network's edge. The base stations in a typical 5G network infrastructure come equipped with edge servers, thus providing additional processing resources as users (or vehicles) need. The computation at the edge ensures there remains ultra-low latency in transferring and processing data at the edge servers due to their close proximity to the users. Leveraging this architecture, vehicular services can operate without resource limitations.\nIn the context of offloading time-sensitive tasks to edge devices, the primary challenge in the vehicular environment is the frequent handovers or changes of base stations or access points (APs) due to the vehicle's mobility [4]. Additionally, selecting and maintaining the optimal communication link for data offloading in fast-moving vehicles presents a significant challenge. When a vehicle exits the coverage area of an AP with co-located edge servers hosting its services, it must re-establish network connections through renewed handshakes and migrate tasks to the appropriate, available, and resourceful computing entity. In a highly dynamic environment, a vehicle's mobility can lead to unnecessary handovers due to slight signal drops, leading to *ping-pong effect*. This process complicates the maintenance of low latency, making it problematic in vehicular edge computing. Thus, efficient resource management is essential to resolve this issue, which involves strategically selecting access points that offer stable and good communication links to the edge servers and continuously migrating these services as the vehicle moves between access points [5]. This must be done within the constraints of the finite edge resources available at each location. Prioritizing link stability over high link quality ensures less frequent handovers among access points, thereby maintaining service continuity and reducing latency [6].\nIn the context of offloading time-sensitive tasks to edge devices [7], both computing and communication research communities share a common concern: minimizing AP handovers while ensuring that the vehicle selects the optimal link for data offloading. From a communication standpoint, frequent AP handovers may result in mobile agents always choosing the best quality link, but they can introduce latency due to handshaking and AP re-associations. These delays incur additional computational overhead and contribute to network congestion due to the frequent handover requests. Conversely, the edge/cloud computing community also grapples with significant challenges related to task migrations among the edge servers, which can result in uneven workloads on distributed servers and increased network congestion [8].\nThis paper investigates the integration of learning methodologies to perform offloading decisions, keeping optimal link quality and AP handovers to minimize task migrations and latency. We present the communication-aware consistent edge selection as a Markovian decision process and propose an approach based on deep reinforcement learning (DRL) off-policy deep deterministic policy gradient (DDPG). Our methodology is tailored to handle continuous task scenarios with a discrete action space. At each time step, an action is chosen, and the resulting outcome is utilized to train the DDPG model to maximize rewards efficiently. The effectiveness of the proposed communication-aware AP-edge selection is demonstrated through extensive simulations and experiments. Results showcase improved task execution times, reduced AP handovers, and enhanced resource utilization, validating the approach's efficacy in dynamic and mobile environments. This research contributes to advancing the design of efficient, intelligent vehicular systems by harnessing edge computing and mobility-awareness for optimized task execution."}, {"title": "II. RELATED WORK", "content": "Recently, there has been a shift in addressing resource management challenges in vehicular technology by adopting proactive methods that utilize predictive techniques. These approaches use predictive techniques to reserve edge server resources, ensuring continuous service with minimal delay [9], [10]. They involve preemptive service placement and migration based on learned mobility patterns and anticipated changes in vehicle locations, optimizing resource usage for seamless service delivery.\nPredictive signal modeling [11], [12] has been of recent interest in communication-aware robotic and vehicular coordination [13], [14]. A predictive approach to solve offloading and migration of computing services is proposed in [15] by forecasting user locations and times, balancing resource optimization and latency in multi-user scenarios. However, it overlooks service migrations from frequent handovers and the associated latency. Another work [16] optimized computation offloading and resource allocation in collaborative cloud and edge environments but neglected network factors crucial for maintaining connections, thus focusing more on computation than comprehensive resource management.\nTransitioning from prediction-based approaches to proactive methods, some research has explored the use of AI algorithms to address optimization and resource allocation challenges in vehicular networks. For example, in [17], a deep reinforcement learning scheme manages resources. Additionally, [18] proposes a double deep Q network for joint optimization in mobile edge computing (MEC), enhancing computing capabilities while minimizing energy, latency, and communication costs. Similarly, [19] explores spectrum, computing, and storage resource allocation using reinforcement learning in MEC-based vehicular networks to meet quality-of-service demands efficiently.\nResearchers have explored multi-dimensional resource management for UAV-assisted vehicular networks. In [20], a multi-agent deep reinforcement learning framework is proposed to optimize channel allocation and power control in heterogeneous vehicular networks, enhancing convergence and system performance for diverse QoS needs. Similarly, authors in [21] used a multi-agent DDPG-based method optimizes MEC server resource allocation for efficient task offloading and QoS requirements. Similarly, authors\nIn [22], researchers used a modified Genetic Algorithm for optimizing offloading, path-planning, and AP-selection under time and energy constraints. [23] addressed dynamic split computing between mobile devices and edge servers, optimizing task offloading based on network conditions and device mobility. Hayat et al. [24] studied edge computing in 5G for autonomous drone navigation, comparing image processing offloading modes: onboard, fully offloaded, and partially offloaded.\nIn the work of Saboia et al. [25], a multi-layer networking solution enhances scalability and bandwidth efficiency for multi-robot systems through bandwidth-aware prioritization. Another work by Ghiasi et al. [26] address AP selection in cell-free massive MIMO systems, optimizing association parameters while considering practical constraints like training errors and access to channel state information.\nThe above literature survey underscores a significant gap in research concerning minimizing handovers and optimizing resource allocation in mobile vehicle contexts. Our framework addresses this by introducing a resource allocation model that optimizes AP selection based on signal quality and load balancing, aiming to reduce frequent AP handovers and minimize service migrations.\nAccordingly, the key contributions in this paper include:\n\u2022 Establishing a collaborative vehicular network with access points linking edge servers and vehicles for parallel computing of delay-sensitive tasks.\n\u2022 Proposing a joint optimization approach using a DDPG-based framework to enhance network stability, AP handover, and load balancing while meeting QoS requirements of uninterrupted services.\n\u2022 Establishing optimal vehicle-AP associations through offline training to meet time-sensitive QoS needs.\n\u2022 Evaluating the algorithm through simulations, showing superior performance compared to baseline methods."}, {"title": "III. METHODOLOGY", "content": "Deep Reinforcement learning is a type of machine learning paradigm for automated decision-making through interac-"}, {"title": "A. Overview of Deep Reinforcement Learning", "content": "tions with the environment [27]. The vehicular community has extensively utilized Deep Reinforcement Learning due to its efficacy in computing service latency and improving service reliability for vehicular network [28].\nIn a typical reinforcement learning problem, the objective of an agent is to effectively determine a strategy (policy) that maximizes the long-term reward, where the learning problem can be modeled as a Markovian Decision Problem (MDP) [29]. MDP is a tuple (S, A, p, f, \u03b3), explained below.\n1) S represents the state space, i.e., a set of all observable states.\n2) A represents the action space, i.e., a set of all valid actions. The actions can be discrete or continuous.\n3) \u03c0: S \u2192 A represents the probability distribution with which the agent samples the next action, a if the state changes from s to the next state s'.\n4) \u03c1:S\u00d7A \u2192 R is the immediate scalar reward that the agent observes after taking the action a, transitioning the environment from s to s'.\n5) f:S\u00d7A\u00d7 A \u2192 [0, 1] represents the state transition function. f(s, a, s') = P(s'|s, a) is the probability that state s transits to s' after action a is performed.\n6) \u03b3\u2208 [0,1] is a discount factor that reduces the impact of future rewards on the present [30].\nThe reward Rt of MDP is formulated as\n$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1},$ (1)\nwhere Rt+k+1 represents the immediate reward obtained at time step t + k + 1, and y is the discount factor.\nThe value function determines how good it is for the agent to be in a given state, and a state-value function is quantified by the cumulative reward in a particular state following a policy \u03c0.\n$V^*(s) = E_{\\pi} [G_t | S_t = s] = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s]$ (2)\nSimilarly, an action-value function (Q-function) determines the cumulative reward of taking an action a in state s following a policy \u03c0.\n$Q^{\\pi} (s, a) = \\sum_{S' \\in S} P(s'|s, a) [R(s, a, s') + \\gamma V^{\\pi} (s')]$ (3)\nThe agent selects an action based on the policy, which could be stochastic, yielding a probability distribution of an action on a given state space \u03c0(s, a) or deterministic, leading to an action with certainty. The ultimate goal is to find the optimal policy that maximizes the total cumulative reward and, for that, the optimal state-action value function that indicates the maximum possible reward for each action given a specific state given as maxa Q*(s, a) = V*(s). Thus, the optimal state-action value function that the agent aims to find can be defined as:\n$Q^* (s, a) = \\sum_{S' \\in S} P(s'|s, a)[R(s, a, s') + \\gamma \\max_{a'} Q^*(s', a')]$ (4)\nDeep Deterministic Policy Gradient (DDPG): DDPG [31] is a model-free off-policy actor-critic algorithm designed for continuous environments that employ neural networks to approximate the Q-value for each state and action pair through a critic network (parameterized by \u03b8c) and an actor-network (parameterized by \u03b8a) to estimate optimal actions. This actor-critic architecture is well-suited for continuous action spaces. The critic network is trained similarly to Deep Q-Network, while the actor-network is updated using policy gradient by applying the chain rule."}, {"title": "B. SNR calculation", "content": "Traditional approaches for AP selection and handover are typically based on the measurement of Signal-to-Noise Ratio (SNR). SNR is used to identify candidate APs and ensure the availability of wireless links. An ideal AP selection algorithm should maintain continuous service and balance the network's overall load. Conventional methods select the AP with the strongest SNR without considering the traffic loads among APs, which often results in significantly uneven traffic distribution. Instead of solely selecting an AP with the maximum SNR, our proposed learning model incorporates the traffic load on each access point into the state of the environment during the learning process.\nThe SNR calculation is performed by initializing the transmission power (txPw) to 25 mW and the operational bandwidth (opBW) to 2 \u00d7 107 MHz. The channel parameters are specified in the Table I.\nThe noise power (nPw) is calculated using the formula:\nnPw = 10 \u00d7 1.3803 \u00d7 10-23 \u00d7 290 \u00d7 opBW (5)\nFor each AP, the path loss (Ploss) is computed based on the distance between the AP and the vehicle. If the distance is less than the breakpoint distance (dBP), the path loss is determined using the parameters before the breakpoint; otherwise, it is computed using the parameters after the breakpoint. Additionally, random noise is added to account for shadow fading. The total path loss is then adjusted by adding penetration losses and fixed shadow fading.\nFinally, the signal-to-noise ratio (SNR) is calculated as\nsnr_v = txPw - Ploss \u2013 30 \u2013 10 \u00d7 log10(nPw) (6)\nThese SNR values are then normalized by scaling the SNR vector between minimum and maximum SNR values."}, {"title": "IV. IMPLEMENTATION DETAILS", "content": "System Model: Suppose there are N vehicles denoted as V = {V1, V2, V3, ....VN}, and each vehicle has a computationally intensive task needing to be offloaded. There are multiple distributed Wi-Fi Access Points (APs), and these APs form the edge nodes, which serve as the remote computational resources available to the vehicles at the edge network. While autonomously navigating towards the target, each vehicle V must determine the optimal access point (AP) with the most suitable network resources to offload its computationally intensive service. Typically, due to mobility and overlapping coverage of the access point, the vehicles need to make handovers.\nOur proposed RL module uses signal strength measured as SNR, and each access point loads as input to perform intelligent AP selection. The RL module learns, based on the vehicle's location, AP-load information, and network control information (signal-to-noise ratio or SNR), the best AP for the vehicle to ensure reliable data transfer for maintaining the QoS for time-sensitive tasks. In this paper, the RL agent's co-optimization framework can balance communication quality and stability, ensuring the least handovers while maintaining performance guarantees.\nGiven the constraints of the proposed scenario, we formulate a DRL network resource orchestrator that aims to learn the optimal network resource orchestration over time. The respective orchestrator ensures the vehicle reaches its intended target location by connecting to the right AP with maximum SNR and optimized AP-to-vehicle load while ensuring minimum handovers possible, which would ensure correct and timely delivery of the vehicle's sensor data to the edge for offloading services to satisfy the application QoS.\nProblem 1 - The objective is to maximize the cumulative utility Ui for each vehicle Vi by selecting optimal access points while satisfying SNR thresholds and access point load constraints. The objective function is formulated as follows:\n$\\max \\sum_{i=1}^{N}U_i$ (7)\ns.t. $SNR_{ij} (t) \\geq SNR_{th}, \\forall i, j,t$\n$Load_j (t) < MaxLoad_j, \\forall i,j,t$\nwhere Ui is the cumulative utility of vehicle Vi, N is the total number of vehicles, SNRij(t) is the Signal-to-Noise Ratio (SNR) between vehicle Vi and access point APj at time t, SNRth represents the minimum acceptable SNR threshold, Loadj(t) is the load on access point APj at time t, indicating the number of connected vehicles, and MaxLoadj is the maximum load that access point APj can handle. Additionally, U\u1d62 can be further defined as presented in Eq. (8)\n$U_i = \\sum_{t=1}^{T} \\gamma^{t-1} (\\alpha \\cdot u_{SNR} (i, t) + \\beta \\cdot u_{Load} (i, t)-\\gamma \\cdot u_{Handover} (i, t) + \\delta \\cdot u_{Target} (i, t)),$ (8)\nwhere (\u03b3) represents the discount factor, with 0 \u2264 y \u2264 1, employed to discount future utilities, aligning with the principle that immediate utilities typically hold greater value than those in the distant future. The term yt-1 ensures a weighted consideration of utilities closer to the present time.\nThe SNR utility (usnr(i, t)) characterizes the utility associated with the Signal-to-Noise Ratio (SNR) for vehicle Vi at time t. A higher SNR typically indicates superior connection quality, thereby positively contributing to the overall utility."}, {"title": "", "content": "The weight \u03b1 underscores the significance of SNR in the aggregate utility.\nThe load utility (uLoad(i,t)) gauges the utility contingent upon the load on access points. This utility is influenced by the number of vehicles connected to a given access point, aiming to equitably distribute the load across access points. The weight \u03b2 denotes the importance assigned to this factor.\nThe handover utility (uHandover(i, t)) penalizes instances of handovers, occurring when a vehicle transitions from one access point to another. Frequent handovers can diminish service quality and amplify operational costs within the network. Thus, the weight -\u03b3 (with \u03b3 serving as the penalty coefficient) reflects the adverse impact of handovers.\nThe target utility (uTarget(i, t)) increases as vehicles reach or progress toward their designated locations. Reaching the destination holds paramount importance in applications such as navigation and logistics. The weight \u03b4 underscores the significance of this utility in the overall objective.\nSolution The overall framework of the proposed DDPG RL agent for solving problem P is illustrated in Fig. 2. The RL agent employs a Markovian Decision Problem (MDP) for its goal of selecting a policy that maximizes the total reward it receives when interacting with an environment. The objective of the RL agent is to tune its policy with the ultimate goal of making the vehicle reach the target by selecting the best AP with respect to SNR and AP load metrics. The policy updates \u03c0\u03b8 occur iteratively at each time slot t dynamically adjusting the actor-critic network, based on the collected reward rt and the observed state St of the vehicle generated by the environment \u03b5 following the execution of the action At on it. Below, we detail the state, action, and reward spaces.\nState Space: Our state space is defined as the combination of the associated SNR specified in a matrix St of each vehicle and the available APs and nt, AP's Load specifying the computational utilization of the current AP by the numbers of vehicles connected to each AP. We also include the AP selections of the vehicles APt-1 from the previous time step into the state vector. We formulate the state space as St = {nt, \u03b4t, APt-1}.\nAction Space: Action space comprises a vector that defines the AP-vehicle associations at each time step. Therefore, the action of the network orchestration is defined as At = {AP}.\nReward Space: The reward function is defined as\n$r_t = f(S_t, A_t) = r_{SNR} + r_{APload} + r_{Handover}.$ (9)\nHere, rSNR represents the positive reward for the least difference between the selected AP's SNR to the greedy-SNR-based AP selection (Eq. (10)), rAPload is the positive reward for low loads for the selected APs (Eq. (11)), and rHandover an exponentially negative reward for increasing handovers (Eq. (12)).\n$r_{SNR} = 2 (\\frac{e^{curr\\_snr}}{e^1} - \\frac{e^0}{e^1 -e^0}) \n$ (10)\n$\\tau_{APload} = \\begin{cases}\npenalty + exp \\left(10 \\frac{curr - load - max - load}{max-load} \\right) & if curr\\_load < max\\_load \\\\\n-1 & if curr\\_load = max\\_load\n\\end{cases}$ (11)\n$\\tau_{Handover} = -e^{-k \\cdot handovers}$ (12)\nLastly, a termination reward is also applied based on the outcomes.\n$\\tau_{termination} = \\begin{cases}\n2500 & if outcome = SUCCESS \\\\\n-2000 & if outcome \\neq SUCCESS \\\\\n0 & otherwise\n\\end{cases}$"}, {"title": "V. SIMULATION RESULTS", "content": "We now present the simulation results to demonstrate the performance of the proposed DDPG-based AP selection."}, {"title": "A. Simulation Settings", "content": "To validate the proposed algorithm, we conducted our simulation experiments in the Gazebo simulator. The learning module was developed using the ROS2 framework, which leverages services and clients to establish a communication network between all the modules. Vehicles are represented as robots in the simulation space, with a specified number of vehicles and access points (APs). The vehicles are free to move within a 20x20 meter area, and in each trial, they are given a target point to navigate towards. Each vehicle maintains a linear velocity of 0.9 m/s.\nThroughout the simulation, the vehicle density remains constant. The APs are also mobile, but their speed is relatively slow to capture a highly dynamic scenario in the simulation setting. The minimum SNR threshold for all the AP associations is set to be 22 dBm, which represents the low SNR condition, and the threshold for maximum AP-load is set to be equal to the total number of vehicles in the environment, which represents the High Load condition.\nIn the proposed DDPG-based AP-selection model, we design the actor-network with 3 fully connected layers with hidden layers of 512, each with ReLU and sigmoid activations. The critic network has 4 fully connected layers with ReLU activation. We assume that there are 100 steps in each episode and the environment is reset at the beginning of each episode. We update the actor and the critic network with a learning rate of 0.003 each. The detailed training parameters are summarized in Table II."}, {"title": "B. Convergence performance", "content": "Fig 4. demonstrates the learning performance of cumulative reward for all the vehicles over the course of 5000"}, {"title": "C. Average SNR", "content": "Fig. 7 represents the average SNR of the links maintained by each vehicle. As observed in Scenario 1, our proposed DDPG-based solution maintains the highest average SNR, as expected. The SSF is a close second, as it follows a greedy approach that only connects the APs with the best signal quality. This approach does lead to establishing good connections with the APs but results in significant performance degradation due to overloading edge servers, where the congestion of fast data transfer leads to low computing performance. Both SSF and DDPG ensure stable communication links for all vehicles at all times, compared to the LLF and RA approaches, which fall behind in maintaining a good average SNR. In Scenario 2, DDPG maintains a good SNR for all vehicles, while SSF maintains an uneven SNR, with some vehicles being connected to very low-quality signals and others to high-quality signals. The RA method follows a similar pattern to SSF but with a lower SNR."}, {"title": "D. Average AP-load", "content": "Fig. 8 illustrates the average AP-load across all APs during the trials. It is evident that the DDPG algorithm maintains an optimal load distribution on the most frequently selected APs for both scenarios. In contrast, the SSF method concentrates the maximum load on the single AP with the highest signal strength value. Conversely, the LLF scheme appears to manage load balancing effectively by considering APs with the least load, thereby avoiding both over-utilization and underutilization. However, this approach may compel a vehicle to connect to a lower-quality AP, potentially resulting in dropped packets and data loss."}, {"title": "E. Average handovers", "content": "The proposed DDPG algorithm demonstrates superior handover performance compared to all baselines, as shown in Fig. 9 for both scenarios. By minimizing the number of handovers over 100 trials, it maintains a well-balanced approach. On average, our methodology performs either no handovers or just one, ensuring consistent edge connections and reducing the need for task migrations while vehicles navigate longer distances, all while maintaining good link quality. In comparison, the SSF method averages three handovers in Scenario 1 and between zero and two in Scenario 2. The LLF scheme, however, performs frequent handovers, resulting in high transfer and service latency."}, {"title": "F. Average time taken", "content": "Additionally, the average time, as shown in Fig. 10, required to complete the trials, indicates that random allocation takes the longest to complete. Both SSF and LLF exhibit similar performance in terms of trial duration, but the DDPG algorithm shows a shorter timeframe. This efficiency is attributed to DDPG's ability to select the most optimal APs based on network metrics, resulting in fewer handovers and smoother transitions between APs. The results indicate that the selection of the optimal AP based on network and load dynamics reduces the latency of time-sensitive tasks, thus reducing the transfer latency associated with the edges."}, {"title": "VI. CONCLUSION", "content": "This study addresses a notable gap in the existing literature on minimizing handovers and optimizing resource allocation in vehicular edge networks. We present a DDPG-based learning framework for continuous AP-selection and a joint optimization approach that enhances network stability and load management while meeting QoS requirements. Simulation results demonstrate the superior performance of our proposed policy compared to baseline algorithms, highlighting its potential to significantly improve network efficiency and service quality in vehicular environments."}]}