{"title": "Communication-Aware Consistent Edge Selection for Mobile Users and\nAutonomous Vehicles", "authors": ["Nazish Tahir", "Ramviyas Parasuraman", "Haijian Sun"], "abstract": "Offloading time-sensitive, computationally inten-\nsive tasks such as advanced learning algorithms for au-\ntonomous driving-from vehicles to nearby edge servers,\nvehicle-to-infrastructure (V2I) systems, or other collaborating\nvehicles via vehicle-to-vehicle (V2V) communication enhances\nservice efficiency. However, whence traversing the path to\nthe destination, the vehicle's mobility necessitates frequent\nhandovers among the access points (APs) to maintain contin-\nuous and uninterrupted wireless connections to maintain the\nnetwork's Quality of Service (QoS). These frequent handovers\nsubsequently lead to task migrations among the edge servers\nassociated with the respective APs. This paper addresses the\njoint problem of task migration and access-point handover by\nproposing a deep reinforcement learning framework based on\nthe Deep Deterministic Policy Gradient (DDPG) algorithm. A\njoint allocation method of communication and computation of\nAPs is proposed to minimize computational load, service la-\ntency, and interruptions with the overarching goal of maximiz-\ning QoS. We implement and evaluate our proposed framework\non simulated experiments to achieve smooth and seamless task\nswitching among edge servers, ultimately reducing latency.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of 5G ultra-communication has exponen-\ntially expanded the possibilities and services surrounding\nautonomous driving. Autonomous vehicles must process vast\namounts of data in real time while maintaining high mobility.\nIn the context of Industry 4.0, the importance of compu-\ntation offloading is driven largely by Internet of Things\n(IoT) applications. To address these demands, autonomous\nvehicle systems must utilize a combination of local on-board\nprocessing and supplementary remote processing power [1].\nTo enable these remote processing capabilities, the 5G ar-\nchitecture incorporates multi-access edge computing (MEC).\nThis integration allows for advanced remote processing ser-\nvices that augment autonomous driving with functionalities\nsuch as automated traffic management. Here, real-time traffic\nanalysis across an entire area aids vehicles in efficiently\nnavigating to their destinations. Within the literature, use\ncases that merge vehicular networks with MEC are referred\nto as vehicular edge computing (VEC) [2], [3]. It offers an\nappealing alternative to Cloud Computing by facilitating low-\nlatency, computationally efficient operations while concur-\nrently maximizing system performance.\nThe MEC (Multi-access Edge Computing) concept in-\nvolves replicating traditional cloud data centers' processing\nand storage capabilities to the network's edge. The base\nstations in a typical 5G network infrastructure come equipped\nwith edge servers, thus providing additional processing re-\nsources as users (or vehicles) need. The computation at the\nedge ensures there remains ultra-low latency in transferring\nand processing data at the edge servers due to their close\nproximity to the users. Leveraging this architecture, vehicular\nservices can operate without resource limitations.\nIn the context of offloading time-sensitive tasks to edge de-\nvices, the primary challenge in the vehicular environment is\nthe frequent handovers or changes of base stations or access\npoints (APs) due to the vehicle's mobility [4]. Additionally,\nselecting and maintaining the optimal communication link\nfor data offloading in fast-moving vehicles presents a sig-\nnificant challenge. When a vehicle exits the coverage area\nof an AP with co-located edge servers hosting its services,\nit must re-establish network connections through renewed\nhandshakes and migrate tasks to the appropriate, available,\nand resourceful computing entity. In a highly dynamic en-\nvironment, a vehicle's mobility can lead to unnecessary\nhandovers due to slight signal drops, leading to ping-pong\neffect. This process complicates the maintenance of low\nlatency, making it problematic in vehicular edge computing.\nThus, efficient resource management is essential to resolve\nthis issue, which involves strategically selecting access points\nthat offer stable and good communication links to the edge\nservers and continuously migrating these services as the\nvehicle moves between access points [5]. This must be done\nwithin the constraints of the finite edge resources available at\neach location. Prioritizing link stability over high link quality\nensures less frequent handovers among access points, thereby\nmaintaining service continuity and reducing latency [6].\nIn the context of offloading time-sensitive tasks to edge de-\nvices [7], both computing and communication research com-\nmunities share a common concern: minimizing AP handovers\nwhile ensuring that the vehicle selects the optimal link for\ndata offloading. From a communication standpoint, frequent\nAP handovers may result in mobile agents always choosing\nthe best quality link, but they can introduce latency due\nto handshaking and AP re-associations. These delays incur\nadditional computational overhead and contribute to network\ncongestion due to the frequent handover requests. Con-\nversely, the edge/cloud computing community also grapples\nwith significant challenges related to task migrations among\nthe edge servers, which can result in uneven workloads on\ndistributed servers and increased network congestion [8].\nThis paper investigates the integration of learning method-\nologies to perform offloading decisions, keeping optimal\nlink quality and AP handovers to minimize task migrations\nand latency. We present the communication-aware consistent\nedge selection as a Markovian decision process and propose\nan approach based on deep reinforcement learning (DRL)\noff-policy deep deterministic policy gradient (DDPG). Our\nmethodology is tailored to handle continuous task scenar-\nios with a discrete action space. At each time step, an\naction is chosen, and the resulting outcome is utilized to\ntrain the DDPG model to maximize rewards efficiently.\nThe effectiveness of the proposed communication-aware AP-\nedge selection is demonstrated through extensive simulations\nand experiments. Results showcase improved task execu-\ntion times, reduced AP handovers, and enhanced resource\nutilization, validating the approach's efficacy in dynamic\nand mobile environments. This research contributes to ad-\nvancing the design of efficient, intelligent vehicular systems\nby harnessing edge computing and mobility-awareness for\noptimized task execution."}, {"title": "II. RELATED WORK", "content": "Recently, there has been a shift in addressing resource\nmanagement challenges in vehicular technology by adopting\nproactive methods that utilize predictive techniques. These\napproaches use predictive techniques to reserve edge server\nresources, ensuring continuous service with minimal delay\n[9], [10]. They involve preemptive service placement and\nmigration based on learned mobility patterns and anticipated\nchanges in vehicle locations, optimizing resource usage for\nseamless service delivery.\nPredictive signal modeling [11], [12] has been of recent\ninterest in communication-aware robotic and vehicular coor-\ndination [13], [14]. A predictive approach to solve offloading\nand migration of computing services is proposed in [15]\nby forecasting user locations and times, balancing resource\noptimization and latency in multi-user scenarios. However,\nit overlooks service migrations from frequent handovers and\nthe associated latency. Another work [16] optimized com-\nputation offloading and resource allocation in collaborative\ncloud and edge environments but neglected network factors\ncrucial for maintaining connections, thus focusing more on\ncomputation than comprehensive resource management.\nTransitioning from prediction-based approaches to proac-\ntive methods, some research has explored the use of AI\nalgorithms to address optimization and resource allocation\nchallenges in vehicular networks. For example, in [17],\na deep reinforcement learning scheme manages resources.\nAdditionally, [18] proposes a double deep Q network for\njoint optimization in mobile edge computing (MEC), en-\nhancing computing capabilities while minimizing energy,\nlatency, and communication costs. Similarly, [19] explores\nspectrum, computing, and storage resource allocation using\nreinforcement learning in MEC-based vehicular networks to\nmeet quality-of-service demands efficiently.\nResearchers have explored multi-dimensional resource\nmanagement for UAV-assisted vehicular networks. In [20],\na multi-agent deep reinforcement learning framework is\nproposed to optimize channel allocation and power control\nin heterogeneous vehicular networks, enhancing convergence\nand system performance for diverse QoS needs. Similarly,\nauthors in [21] used a multi-agent DDPG-based method\noptimizes MEC server resource allocation for efficient task\noffloading and QoS requirements. Similarly, authors\nIn [22], researchers used a modified Genetic Algorithm\nfor optimizing offloading, path-planning, and AP-selection\nunder time and energy constraints. [23] addressed dynamic\nsplit computing between mobile devices and edge servers,\noptimizing task offloading based on network conditions and\ndevice mobility. Hayat et al. [24] studied edge computing\nin 5G for autonomous drone navigation, comparing image\nprocessing offloading modes: onboard, fully offloaded, and\npartially offloaded.\nIn the work of Saboia et al. [25], a multi-layer networking\nsolution enhances scalability and bandwidth efficiency for\nmulti-robot systems through bandwidth-aware prioritization.\nAnother work by Ghiasi et al. [26] address AP selection in\ncell-free massive MIMO systems, optimizing association pa-\nrameters while considering practical constraints like training\nerrors and access to channel state information.\nThe above literature survey underscores a significant gap\nin research concerning minimizing handovers and optimizing\nresource allocation in mobile vehicle contexts. Our frame-\nwork addresses this by introducing a resource allocation\nmodel that optimizes AP selection based on signal quality\nand load balancing, aiming to reduce frequent AP handovers\nand minimize service migrations.\nAccordingly, the key contributions in this paper include:\n\u2022 Establishing a collaborative vehicular network with ac-\ncess points linking edge servers and vehicles for parallel\ncomputing of delay-sensitive tasks.\n\u2022 Proposing a joint optimization approach using a DDPG-\nbased framework to enhance network stability, AP han-\ndover, and load balancing while meeting QoS require-ments of uninterrupted services.\n\u2022 Establishing optimal vehicle-AP associations through\noffline training to meet time-sensitive QoS needs.\n\u2022 Evaluating the algorithm through simulations, showing\nsuperior performance compared to baseline methods."}, {"title": "III. METHODOLOGY", "content": "Deep Reinforcement learning is a type of machine learning\nparadigm for automated decision-making through interac-"}, {"title": "B. SNR calculation", "content": "Traditional approaches for AP selection and handover are\ntypically based on the measurement of Signal-to-Noise Ratio\n(SNR). SNR is used to identify candidate APs and ensure\nthe availability of wireless links. An ideal AP selection\nalgorithm should maintain continuous service and balance\nthe network's overall load. Conventional methods select the\nAP with the strongest SNR without considering the traffic\nloads among APs, which often results in significantly uneven\ntraffic distribution. Instead of solely selecting an AP with the\nmaximum SNR, our proposed learning model incorporates\nthe traffic load on each access point into the state of the\nenvironment during the learning process.\nThe SNR calculation is performed by initializing the\ntransmission power (txPw) to 25 mW and the operational\nbandwidth (opBW) to 2 \u00d7 107 MHz. The channel parameters\nare specified in the Table I.\nThe noise power (nPw) is calculated using the formula:\n$\\nPw = 10 \\times 1.3803 \\times 10^{-23} \\times 290 \\times opBW\n$\nFor each AP, the path loss (Ploss) is computed based on\nthe distance between the AP and the vehicle. If the distance\nis less than the breakpoint distance (dBP), the path loss\nis determined using the parameters before the breakpoint;\notherwise, it is computed using the parameters after the\nbreakpoint. Additionally, random noise is added to account\nfor shadow fading. The total path loss is then adjusted by\nadding penetration losses and fixed shadow fading.\nFinally, the signal-to-noise ratio (SNR) is calculated as\n$\\snr_v = txPw - Ploss \u2013 30 \u2013 10 \\times log_{10}(nPw)\n$\nThese SNR values are then normalized by scaling the SNR\nvector between minimum and maximum SNR values."}, {"title": "IV. IMPLEMENTATION DETAILS", "content": "System Model: Suppose there are N vehicles denoted\nas V = {V1, V2, V3, ....VN}, and each vehicle has a com-\nputationally intensive task needing to be offloaded. There\nare multiple distributed Wi-Fi Access Points (APs), and\nthese APs form the edge nodes, which serve as the remote\ncomputational resources available to the vehicles at the edge\nnetwork. While autonomously navigating towards the target,\neach vehicle V must determine the optimal access point\n(AP) with the most suitable network resources to offload its\ncomputationally intensive service. Typically, due to mobility\nand overlapping coverage of the access point, the vehicles\nneed to make handovers.\nOur proposed RL module uses signal strength measured\nas SNR, and each access point loads as input to perform\nintelligent AP selection. The RL module learns, based on the\nvehicle's location, AP-load information, and network control\ninformation (signal-to-noise ratio or SNR), the best AP for\nthe vehicle to ensure reliable data transfer for maintaining the\nQoS for time-sensitive tasks. In this paper, the RL agent's co-\noptimization framework can balance communication quality\nand stability, ensuring the least handovers while maintaining\nperformance guarantees.\nGiven the constraints of the proposed scenario, we for-\nmulate a DRL network resource orchestrator that aims to\nlearn the optimal network resource orchestration over time.\nThe respective orchestrator ensures the vehicle reaches its\nintended target location by connecting to the right AP with\nmaximum SNR and optimized AP-to-vehicle load while\nensuring minimum handovers possible, which would ensure\ncorrect and timely delivery of the vehicle's sensor data to the\nedge for offloading services to satisfy the application QoS.\nProblem 1 - The objective is to maximize the cumulative\nutility Ui for each vehicle Vi by selecting optimal access\npoints while satisfying SNR thresholds and access point load\nconstraints. The objective function is formulated as follows:\n$\\max\\ \u03a3U_i $\ns.t. SNRij (t) \u2265 SNRth, i, j,t\nLoadj (t) < MaxLoadj,\nwhere Ui is the cumulative utility of vehicle Vi, N is the\ntotal number of vehicles, SNRij(t) is the Signal-to-Noise\nRatio (SNR) between vehicle Vi and access point AP;\nat time t, SNRth represents the minimum acceptable SNR\nthreshold, Loadj(t) is the load on access point APj at time t,\nindicating the number of connected vehicles, and MaxLoadj\nis the maximum load that access point APj can handle.\nAdditionally, U\u2081 can be further defined as presented in Eq. (8)\n$\\U_i = \u03a3rt-1 (\u03b1 \u00b7 USNR (i, t) + \u03b2 \u00b7 ULoad(i, t)\n-\u03b3\u00b7 UHandover (i, t) + \u03b4\u00b7 uTarget(i, t)), $\nwhere (\u03b3) represents the discount factor, with 0 \u2264 y \u2264\n1, employed to discount future utilities, aligning with the\nprinciple that immediate utilities typically hold greater value\nthan those in the distant future. The term yt-1 ensures a\nweighted consideration of utilities closer to the present time.\nThe SNR utility (usnr(i, t)) characterizes the utility asso-\nciated with the Signal-to-Noise Ratio (SNR) for vehicle Vi at\ntime t. A higher SNR typically indicates superior connection\nquality, thereby positively contributing to the overall utility."}, {"title": "and", "content": "The weight \u03b1 underscores the significance of SNR in the\naggregate utility.\nThe load utility (uLoad(i,t)) gauges the utility contingent\nupon the load on access points. This utility is influenced by\nthe number of vehicles connected to a given access point,"}]}