{"title": "RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed Autonomous Driving", "authors": ["Haisheng Su", "Feixiang Song", "Cong Ma", "Panpan Cai", "Wei Wu", "Cewu Lu"], "abstract": "Robust object detection and tracking under arbitrary sight of view is challenging yet essential for the development of Autonomous Vehicle technology. With the growing demand of unmanned function vehicles, near-field scene understanding becomes an important research topic in the areas of low-speed autonomous driving. Due to the complexity of driving conditions and diversity of near obstacles such as blind spots and high occlusion, the perception capability of near-field environment is still inferior than its farther counterpart. To further enhance the intelligent ability of unmanned vehicles, in this paper, we construct a multimodal data collection platform based on 3 main types of sensors (Camera, LiDAR and Fisheye), which supports flexible sensor configurations to enable dynamic sight of view for ego vehicle, either global view or local view. Meanwhile, a large-scale multi-sensor dataset is built, named RoboSense, to facilitate near-field scene understanding. RoboSense contains more than 133K synchronized data with 1.4M 3D bounding box and IDs annotated in the full 360\u00b0 view, forming 216K trajectories across 7.6K temporal sequences. It has 270\u00d7 and 18\u00d7 as many annotations of near-field obstacles within 5m as the previous single-vehicle datasets such as KITTI and nuScenes. Moreover, we define a novel matching criterion for near-field 3D perception and prediction metrics. Based on RoboSense, we formulate 6 popular tasks to facilitate the future development of related research, where the detailed data analysis as well as benchmarks are also provided accordingly.", "sections": [{"title": "1. Introduction", "content": "Recent years have witnessed significant progress achieved in the field of autonomous driving, enabling numerous intelligent vehicles running on highway or urban areas. In addition to passenger cars and commercial vehicles, unmanned function vehicles have emerged as a new industry focus in low-speed autonomous driving. Differently, low-speed unmanned vehicles have clear application scenarios and controllable costs, which are mostly carried out in enclosed and semi-enclosed regions, with relatively fixed but more diverse scenarios, such as campuses, scenic spots, airports and parks, etc. The typical functions of such unmanned vehicles include tractor, sweeper, retail and delivery, which are closer to the landing goal of L4 autonomous driving. Hence, it is essential to push the limit of intelligent driving technology on low-speed driving scenarios.\nSingle-vehicle datasets related to autonomous driving have been released in recent years [4, 8, 15, 27, 33, 40], aiming at promoting the academic research on key technologies such as object perception and prediction. KITTI [8] is a pioneering dataset providing multi-modal sensor data including front-view LiDAR pointclouds as well as corresponding stereo images and GPS/IMU data. nuScenes [4] constructs a multi-sensor dataset collected in two cities travelling at an average of 16 km/h, where rich collections of 3D boxes and IDs are annotated in the full 360\u00b0 view, covering 1K scenes and 23 classes. Waymo Open dataset [33] significantly increases the amount of annotations with higher annotation frequency. However, these datasets are all constructed for passenger vehicles, where the sensor data are mostly captured from highway or urban roads. Due to the high-speed driving condition and structural driving constrains of traffic rules and static road elements (i.e., lanes, boundary and crossings), collected objects are distributed in a farther distance from the ego vehicle, lacking surrounding obstacles with extremely near distance. Therefore, AV models trained on these datasets are prone to concentrate on objects locating on farther areas, rather than near-field obstacles with high occlusion and truncation owing to the blind spots that are beyond the sight of Camera and LiDAR sensors. Besides, obstacle diversity on motorway is also limited, which usually only includes vehicle and VRU (Vunerable Road User). Obviously, AV models developed upon current datasets can not satisfy the performance requirement of low-speed vehicles, which are surrounded with various kinds and shapes of near obstacles in complex scenarios.\nTo fill the gap between high-speed and low-speed autonomous driving, meanwhile further accelerate research of near-field AV algorithms, in this paper, we construct a large-scale multimodal dataset and benchmark, named as RoboSense, which is collected from diverse scenarios with various kinds of obstacles. The data collection vehicle is equipped with 3 main types of sensors (C: Camera, L: LiDAR, F: Fisheye). To ensure the data capture under full 360\u00b0 view, each type of sensor consists of 4 devices installed on different sides of the data collection vehicle respectively, thus the captured areas of multiple sensors can overlap with each other. Benefiting from the well time-synced multi-sensor collected data pairs, our RoboSense can allow researchers to take flexible sensor combinations as input, thus achieving dynamic sight of view for scene understanding.\nRoboSense consists of a total of 133K+ frames of synchronized data, spanning over 7.6K temporal sequences of 6 main scene classes (i.e, scenic spots, parks, squares, campuses, streets and non-motorized lanes). Moreover, 1.4M 3D bounding boxes together with track IDs are annotated based on 3 different types of sensors, with more than 30% of targets locating within 10m. Then we form global trajectories for each agent separately through associating the same IDs across consecutive frames and different sensors from a Bird's-Eye View (BEV) perspective. Based on the constructed dataset, we formulate six popular autonomous driving tasks and benchmarks as follows: 1. Multi-view 3D Detection; 2. LiDAR 3D Detection; 3. Multi-modal 3D Detection; 4. Multiple 3D Object Tracking (3D MOT); 5. Motion Prediction; 6. Occupancy Prediction. Meanwhile, multi-task end-to-end training scheme is also supported in our RoboSense for evaluation of joint optimization. In sum, the main contributions of our work are four folds:\n\u2022 To the best of our knowledge, our RoboSense is the first dataset constructed for outdoor low-speed autonomous driving research of unmanned vehicles, which especially concentrates on near-field scene understanding.\n\u2022 We annotate 1.4M 3D bounding boxes on 133K+ synchronized frames of multi-sensor data. Over 30% of the targets are located within the near field around the ego vehicle. Each target is associated with a unique ID, thus forming a total of 216K trajectories, which spread over 7.6K temporal sequences, covering 6 main scene classes.\n\u2022 We construct several sensor layouts with flexible sensor configurations of Cameras, Fisheyes and LiDARs, along with multi-sensor synchronized data collected for scene understanding from a robotic sight of view.\n\u2022 We formulate 6 popular tasks as well as benchmarks to facilitate the research development of near-field environmental perception and prediction."}, {"title": "2. Related Work", "content": "The development of autonomous driving has been significantly accelerated by the availability of several prominent datasets. We summarize the compositions and comparisons of some influential datasets as shown in Tab. 1, including both perception datasets and prediction datasets.\nPerception Datasets. Current released perception datasets can be divided into image-only datasets [6, 42] and multimodal datasets [4, 8, 15, 16, 33]. BDD100k [42] and Cityscapes [6] focus on 2D perception which provide large amount of 2D annotations (boxes, masks) for driving scene understanding under various weather and illumination conditions. KITTI [8] is known as the pioneering multimodal dataset which has been widely used for academic research. It records 6 hours of driving data using a LiDAR sensor, GPS/IMU sensors and a front-facing stereo camera to provide dense pointclouds and images with annotated 3D boxes. H3D dataset [25] collects a total of 1.3M 3D objects over 27K frames from 160 crowded scenes of the full 360\u00b0 view. nuScenes [4] and Waymo Open Dataset [33] are two similar datasets with same structure, while the latter one providing more annotations owing to higher annotation frequency (2Hz vs. 10Hz). Differently, our proposed RoboSense dataset is annotated in 1Hz due to the relative lower driving speed (less than 1m/s), which is constructed on low-speed driving scenarios, aiming to facilitate near-field scene understanding for L4 unmanned vehicles working on unstructured roads.\nPrediction Datasets. nuScenes [4] and Waymo Open Dataset [33] can be also used for prediction task which release lane graphs as well. Lyft [16] introduces traffic/speed control data, and Waymo Open Dataset [33] adds more signals to the map such as crosswalk, lane boundaries, stop signs and speed limits. Recently, Shifts dataset [24] becomes the largest forecasting dataset with the most scenario hours to date. Meanwhile, Argoverse [5] is also a large-scale dataset with high data frequency (10Hz) and high sce-"}, {"title": "3. RoboSense Open Dataset", "content": "To facilitate the near-field obstacle perception and prediction, we introduce RoboSense, a real-world, large-scale and multi-modal dataset annotated with abundant of 3D bounding boxes and corresponding trajectories. We commence with the sensor setup as well as data acquisition details, delineate the coordinate systems and label generation process, and present data statistics respectively."}, {"title": "3.1. Sensor Setup and Data Acquisition", "content": "Sensor setup. We use an electric sweeper as data collection platform, which is equipped with 5 main types of sensors, including LiDAR, Camera, Fisheye, GPS/IMU and Ultrasonic, which are installed in top, front, rear and sides of the ego vehicle respectively to ensure data collection in 360\u00b0 horizontal view. Refer to Fig. 2 for sensor layouts and Tab. 3 for detailed sensor specifications.\nData acquisition. We utilize the equipped sweeper to collect multi-sensor data along the Dishui Lake in Shanghai, China, lasting 42h in total at an average speed of less than 1m/s through manually remote control. 22 different places are travelled, which can be categorized into 6 kinds of outdoor or semi-closed scenarios (i.e., office parks, tourist attractions, plazas, schools, streets and roads). After collecting the raw sensor data, we manually select and process 7619 representative scenes of 20s duration respectively for further annotation, which include various densities of traffic and human crowds, diverse weather and light conditions and abundant classes and number of obstacles."}, {"title": "3.2. Coordinate Systems", "content": "Ego-Vehicle Coordinate. The Ego-Vehicle Coordinate System is centered at the rear axle of the vehicle. The positive directions of the X, Y, and Z axes correspond to the forward, leftward, and upward directions of the vehicle, respectively. Ego-Vehicle Coordinate System is the most frequently used in tasks such as perception, tracking, prediction, and planning, where dynamic and static targets as well as trajectories are transformed into this coordinate system.\nGlobal Coordinate. To transform the dynamic and static elements from historical and future frames into the current frame coordinate system, we need to establish a global coordinate system to record the position and orientation of the ego vehicle in each frame. The origin of the Global Coordinate System is an arbitrarily defined point in Shanghai Lingang, China, and the positive directions of the X, Y, and Z"}, {"title": "3.3. Ground Truth Labels", "content": "After integrating, synchronizing and calibrating the multi-sensor raw data, we annotate keyframes (LiDAR, image) at the frequency of 1Hz, owing to such applications of unmanned functional vehicles generally running on low-speed scenarios (less than 1 m/s).\n3D object. With the selected scenes of collected RoboSense dataset, we annotate 3D object boxes of 3 movable classes (i.e., \"Vehicle\", \"Cyclist\" and \"Pedestrian\") for each sampled keyframe in both the LiDAR coordinate of point-clouds and the Camera coordinate of multi-view images respectively. Each annotated 3D box can be represented as $[x, y, z, w, l, h, \\theta, cls]$, where $x, y, z$ indicate the 3D position of a regular object, and $w, l, h$ represent the scale information including width, length and height. $\\theta$ and $cls$ correspond to the orientation (especially yaw angle) and the object class respectively. A three-stage auto-labelling pipeline is detailed in the supplementary material (see Sec. B.2).\nTrajectory. To facilitate the temporal tasks such as multi-object tracking and motion forecasting described in Sec. 4, we assign a unique Track ID $T$ to each agent across a temporal sequence on Bird-Eye-View (BEV) of the Ego-Vehicle coordinate. Furthermore, agents with the same within a sequence are linked together to form object trajectories.\nOccupancy label. In addition to 3 typical classes of moving objects on roads which are annotated temporally as above, there also exists a rich collection of static obstacles with irregular shapes especially in the complex scenarios (i.e., parks, schools and plazas, etc.) of RoboSense. To detailly describe the environment in surrounding camera views for driving safety, we voxelize the 3D space and gen-"}, {"title": "4. Tasks & Metrics", "content": "Various tasks related to autonomous driving are supported on RoboSense dataset, which can be divided into two main categories, namely perception and prediction. The perception task mainly includes 3D object detection and tracking based on image or pointcloud sequences. The prediction task aims to predict the occupancy and motion trajectory of each agent surrounding the ego-vehicle."}, {"title": "4.1. Perception", "content": null}, {"title": "4.1.1 3D Object Detection", "content": "The RoboSense 3D detection task includes multi-view Image 3D and LiDAR 3D detection, which requires to detect 3 object classes with 3D bounding boxes, i.e. \u201cVehicle\", \"Pedestrian\" and \"Cyclist\u201d. Formally, given multi-view image sequences or point cloud sequences, a set of 3D bounding boxes in the form of $[x, y, z, w, l, h, \\theta, cls]$ are detected, indicating target position, size, orientation and category respectively. Following the conventions in [4, 9, 23, 33], we adopt mAP (mean Average Precision), mAOS (mean Average Orientation Similarity) and mASE (mean Average Scale Error) to measure the performance of different detectors.\nAverage Precision (AP) metric is commonly utilized to evaluate the accuracy of 2D or 3D detectors. However, there are several matching criteria to define the true positive. For example, [9] adopts 3D IOU (Intersection-Over-Union) to match each prediction with a ground truth box, while [4] define a match through thresholding the 2D center distance on"}, {"title": "4.1.2 Multi-Object Tracking", "content": "The tracking task is designed to associate all detected 3D boxes of movable object classes across input multi-view temporal sequences (i.e. videos or point cloud sequences). Each object is assigned a unique and consistent track ID T from first appearance until complete vanishing. As for performance evaluation, we refer to [4, 9, 22, 32], and mainly adopt SAMOTA (Scaled Average Multi-Object Tracking Accuracy), AMOTP (Average Multi-Object Tracking Precision) to measure the 3D tracking performance.\nFormally, SAMOTA is defined as the mean value of SMOTA over all recalls:\n$SAMOTA = \\frac{1}{|R|}\\sum_{r \\in R}SMOTAr$\n$SMOTAr = \\max (0, 1 - \\frac{FPr + FNr + IDS - (1 - r) \\cdot GT}{r \\cdot GT})$, where $FPr$, $FNr$ and $IDS$ represent the number of false positives (wrongly detection), false negatives (missing detection) and identity switches at the corresponding recall $r$, respectively. Similarly, AMOTP is the average results of MOTP among different recall rates, which can defined as follows:\n$AMOTP = \\frac{1}{|R|}\\sum_{r \\in R} \\frac{\\sum d_{i,t}}{TPr}$, where $TPr$ is the number of true positives at the recall $r$, and $d_{i,t}$ denotes the position error of matched track $i$ at timestamp $t$. Besides, additional metrics such as MT (Most Tracked) and ML (Most Lost) from [3] are together evaluated for reference."}, {"title": "4.2. Prediction", "content": null}, {"title": "4.2.1 Motion Forecasting", "content": "Based on perception results, the motion forecasting task requires to predict agents' future trajectories. Specifically, K plausible trajectories in future T = 3s timesteps for each agent are forecasted as offsets to the current agent's position. Following the standard protocols [10, 20, 21, 26], we adopt minADE (minimum Average Displacement Error), minFDE (minimum Final Displacement Error), MR (Miss Rate) and EPA (End-to-end Prediction Accuracy) as metrics to measure the precision of motion prediction. In order to decouple the accuracy of perception and prediction, these metrics are only caculated for matched TPs (True Positives), where the matching threshold is set to $p_{match} = 5\\%$ of ground truth distance of the closest collision-point from the ego-vehicle. And the miss threshold of minFDE is set to $p_{miss} = 20\\%$ for calculating the MR metric."}, {"title": "4.2.2 Occupancy Prediction", "content": "The goal of occupancy prediction task is to estimate the state of each voxel in the 3D space. Formally, a sequence of T historical frames with N surround-view camera images ${I_{it} \\in R^{H \\times W \\times 3}}$ are served as input, where $i = 1, ..., N$ and $t = 1, ..., T$. Besides, sensor intrinsic parameters ${K_i}$ together with extrinsic parameters ${R_{i|t_i}}$ for each frame are also provided. Then the ground truth labels describe the voxel states separately, including occupancy state and semantic label. Three states are considered on the RoboSense"}, {"title": "5. Experiments", "content": "In this section, we present the benchmark setup with sensor specifications, and then describe the analytical details of multiple baselines with different modalities in terms of perception and prediction experiments conducted on the RoboSense dataset, respectively."}, {"title": "5.1. Benchmark Setup", "content": "Our RoboSense dataset contains 7.6K sequences (including 130K annotated frames) of synchronized multi-sensor data, covering 6 main categories (including 22 different locations) of outdoor or semi-closed scenarios (i.e., S1-office parks, S2-tourist attractions, S3-plazas, S4-schools and S5-unstructured streets or S6-roads). To protect the data privacy, we conduct a series of data desensitization measures through masking the human faces and car plates as well as road signs from all sensor data. The details of RoboSense dataset composition and partitioning are listed in the Tab. 2. The RoboSense dataset is collected under various illumination, traffic flow and weather conditions, to ensure the diversity of static background and movable obstacles, thus meeting the demand of different realistic applications.\nRoboSense dataset is divided into three parts with a ratio of 50%, 40% and 10%, for the purpose of training, testing and validation respectively. As for the scene partition, one of the 6 collected scenes (i.e. S-6) is assigned to the testing set exclusively, while the remaining scenes are shared among all splits. Ground truth labels of training and validation sets for corresponding task are provided, together with the synchronized multi-sensor raw data. However, the testing set only provides data. Hence algorithms can merely be submitted to our online benchmark for corresponding task evaluation of testing set."}, {"title": "5.2. Sensor Specifications", "content": "The detailed specifications of all devices are shown in Tab. 3. To cover the areas from local to global view, we select Cameras with different focal lengths and Field of View (FOV). Besides, 5 LiDAR sensors are installed in our data collection platform, where the top Hesai Pandar40M is served as autolabeller to provide initial annotations for the splicing points of target LiDARs. 11 Ultrasonics sensors are also installed for freespace detection to ensure safety. All devices are synchronized in time via Network Time Protocol (NTP) before data collection, we utilize a time interval of 100ms as the global timestamp for intersections, and match the frame from each device with the nearest timestamp adjacent to the global timestamp. This process ultimately yields synchronized multi-sensor data at a frame rate of 10 FPS."}, {"title": "5.3. Implementation Details", "content": "For LiDAR detection task, we set the point range to x\u2208[-45m, 45m], y\u2208[-45m, 45m], z\u2208[-1m, 4m], with a fixed voxel size of 0.16m and 0.05m for pillar-based and voxel-based methods respectively. For Image detection tasks, we use ResNet18 [11] as backbone network and the input image is resized to 640 \u00d7 352. For practical usages, we report performance using our proposed Closest-Collision Distance Proportion (CCDP) as matching criterion. Comparisons of different matching functions on average precision are shown in Fig. 4. As expected, when using Center Distance (CD) or IOU, objects without distance differentiation can not reflect the model capability of locating closest collision points of nearby obstacles, which is more challenging and essential for low-speed driving scenarios."}, {"title": "5.4. Baselines: Perception", "content": null}, {"title": "5.4.1 LiDAR 3D Detection", "content": "To demonstrate the performance of advanced 3D detectors on LiDAR-only detection track of our RoboSense benchmark, we implement several popular CNN-based methods with different fashions, including Pointpillar [17] (Pillar-based), SECOND [41] (Voxel-based), and PV-RCNN [31] (Two-stage Point-Voxel based). Besides, Transformer-based method such as Transfusion-L [2] is also implemented for architecture comparison. Pointpillar as the most efficient method above is adopted as our baseline for LiDAR 3D detection task."}, {"title": "5.4.2 Multi-View 3D Detection", "content": "Current works of multi-view 3D detection can be divided into two mainstreams, namely LSS [28] based and Transformer based. To examine the effectiveness of image-only multi-view 3D detection models, we select the widely-used method BEVDet [14] as our LSS-based baseline on image 3D detection track of RoboSense, and re-implement several extended versions such as BEVDet4D [13] which takes advantage of history temporal clues, and BEVDepth [18] which adopts an additional branch for depth prediction"}, {"title": "5.4.3 Multiple Object Tracking", "content": "We follow the \"Tracking-by-Detection\" paradigm using 3D detection results from Camera or LiDAR data as input respectively, and present several baselines for multiple 3D object tracking task. Specifically, 3D boxes detected from surround-view images by BEVDepth [18] and splicing pointclouds by Pointpillar [17] are provided separately. And the tracking approach AB3DMOT described in [39] is picked to serve as the baseline of multiple object tracker in the 3D space. Then the same objects across different sensors are associated with unique track IDs to form global trajectories in the past."}, {"title": "5.5. Baselines: Prediction", "content": null}, {"title": "5.5.1 Motion Prediction", "content": "Traditional motion prediction methods utilize perception ground truth (i.e., history trajectories of agents and HDmap) as input, which lacks of uncertainty modeling in practical applications. In this paper, we implement several vision-based end-to-end methods for joint perception and motion prediction on RoboSense benchmark, including ViP3D [10] and PnPNet [20]. For comparisons, we also report the motion prediction results of assuming agents surrounding the ego-vehicle with constant positions or velocities respectively, thus to reflect the diversity and difficulty of our dataset on prediction task."}, {"title": "5.5.2 Occupancy Prediction", "content": "We extend a BEV 3D detection model - BEVDepth [18] to the 3D occupancy prediction task, which is then adopted as our baseline for the visual occupancy prediction task. Concretely, we replace the original detection decoders with the occupancy reconstruction layers while maintaining the BEV feature encoders. ResNet18 [11] pretrained on FCOS3D [38] is employed as image backbone for visual feature extraction."}, {"title": "5.6. Results and Analysis", "content": "Benefiting from the multi-sensor collected data with various kinds of high-quality annotations, we conduct extensive baselines of different tasks. As for perception-level tasks, we evaluate the 3D object detection as well as multi-object tracking respectively using different kinds of sensor data as input, including vision-based paradigm and LiDAR-based paradigm. While for the prediction-level tasks, motion prediction and occupancy prediction are two crucial tasks for final ego-vehicle planning as proven in [1, 12, 37]. The main metric for each task is marked with gray background color in corresponding tables for intuitive comparison."}, {"title": "5.6.1 Perception Results", "content": "The 3D detection results based on multi-view images and splicing point clouds are shown in Tab. 4, where 3D AP, AOS and ASE are evaluated for three different object classes respectively. As for LiDAR 3D detection, Transfusion-L [2] achieves the leading performance owing to the advanced transformer architecture. In terms of multi-view 3D detection, BEVDet4D [13] obtains slight improvement than BEVDet [14] with temporal clues involved. And BEVDepth [18] yields a significant improvement of 2.0% and 2.2% 3D AP compared to BEVDet [14] on Pedestrian and Cyclist classes respectively, through adopting an additional depth branch supervised by LiDAR points. Besides, BEVFormer [19] introduced a query-based attention mechanism to encode BEV features, also achieving competitive results. Generally, LiDAR-based 3D detector can generate high-quality detection results than vision-based methods. However, vision-based methods are capable of detecting various ranges of objects with more sensors owing to the controllable cost. Note that two different matching criteria are both considered for TP calculation, namely Center-Point (CP) distance and Closest Collision-Point (CCP) distance. It can be observed that the CCP localization performance is obviously lower than the CP localization, i.e. 18.5% 3D AP drop of Transfusion-L for Vehicle class and 29.5% 3D AP drop for Pedestrian class. For security consideration, the CCP localization is more important for near-field obstacle perception under low-speed driving conditions.\nTo evaluate the performance of different sensor layouts under various ranges, we conduct extensive comparisons of multi-view 3D perception, LiDAR 3D perception and multi-modal 3D perception respectively as shown in Tab. 5. As for multi-view 3D perception, 4C layout achieves better 3D detection AP than 4F layout in farther areas (i.e., 10-30m), while 4F layout is good at detecting near-field objects within 10m. Through combining these two types of sensors, better performance can be obtained across different ranges with 8 camera views as input. LiDAR 3D detector exhibits an obvious advantage over vision-based detectors especially in CCP and farther object localization. For example, 4L layout obtains 26.8% 3D AP improvement over 4C+4F layout, while the performance of near-field objects within 5m is inferior (19.3% vs. 20.5%). Moreover, we implement multi-modal 3D perception (8V+4L) through late-fusion strategy. Specifically, 3D detection results from multi-view 3D detector and LiDAR 3D detector are adopted for post-processing. And we can observe that the CCP-based 3D AP of objects within 5m is remarkably boosted from 20.5% to 36.9%, and the AOS performance is also increased accordingly by a great margin.\nRegarding to the MOT task in Tab. 5, multi-view (4C or 4F) 3D based tracking exhibits poorer performance than LiDAR-based paradigm due to the weaker detection performance. LiDAR 3D tracker performs multi-object tracking in 3D space, which mitigates the impact of object occlusions existing in 2D image, especially for crowded scenarios. However, through introducing more sensors (4C+4F), vision-based methods can also achieve competitive tracking performance with LiDAR-based methods, even better in"}, {"title": "5.6.2 Prediction Results", "content": "Motion forecasting of surrounding agents as well as occupancy state descriptions around ego-vehicle are two crucial prediction tasks in the research field of autonomous driving, which have been extensively explored in urban and highway scenarios for L2 Autonomous Vehicles. Motion forecasting results on our RoboSense validation sets are shown in Tab. 6. Either visual end-to-end methods [10] or LiDAR-based end-to-end methods [20] are all supported for validation. PnPNet [20] with LiDAR points as input can produce less prediction errors and better EPA than ViP3D [10], both of which remarkably outperform two baseline settings of modeling agents with constant positions or velocities. In terms of occupancy prediction reported in Tab. 7, we use 4F sensor data as input and report the performance of mIOU metric in both 3D and BEV space under various ranges respectively. It should be noted that the metric is calculated without considering states of the ground voxels, leading to lower performance in either 3D or BEV space."}, {"title": "6. Conclusion", "content": "To facilitate the autonomous driving in low-speed scenarios, RoboSense, a real-world, large-scale and multi-sensor dataset is constructed with 1.4M 3D Boxes and 216K trajectories annotated in total on 133K synchronous frames, which is designed for specialized research on near-field obstacle perception and prediction models, either modular training or joint optimization. Our dataset consists of 7.6K sequences manually selected from different locations, covering various weather conditions and traffic densities. In the future works, more tasks together with corresponding benchmarks, such as motion planning, will be expanded for end-to-end autonomous driving application based on our RoboSense dataset, and explore the additional benefits that joint optimization can bring to the modular training."}, {"title": "A. Coordinates Transformation", "content": null}, {"title": "A.1. LiDAR Ego-Vehicle", "content": "LiDAR to Ego-Vehicle: $(X_v, Y_v, Z_v)$ represents a three-dimensional coordinate point in Ego-Vehicle Coordinate System. The transformation from the coordinates $(x_v, y_v, z_v)$ in the Ego-Vehicle Coordinate System to $(x_l, y_l, z_l)$ in the LiDAR Coordinate System is calculated as follows:\n$\\begin{bmatrix} x_l\\\\ y_l\\\\ z_l\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} R^{3 \\times 3}_L & T^{3 \\times 1}_L\\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_v\\\\ y_v\\\\ z_v\\\\ 1 \\end{bmatrix}$\nwhere $R_L \\in R^{3 \\times 3}$ and $T_L \\in R^{3 \\times 1}$ represent the rotation and translation from the Ego-Vehicle Coordinate System to the LiDAR Coordinate System, respectively.\nEgo-Vehicle to LiDAR: The transformation from Ego-Vehicle Coordinate System to LiDAR Coordinate System is the inverse transformation of Eq.(1)."}, {"title": "A.2. LiDAR Camera", "content": "LiDAR to Camera: Regardless of whether it is a fisheye or a pinhole camera, the coordinate transformation formula from the LiDAR Coordinate System to the Camera Coordinate System is the same and is given as follows:\n$\\begin{bmatrix} x_c\\\\ y_c\\\\ z_c\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} R^{3 \\times 3}_C & T^{3 \\times 1}_C\\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_l\\\\ y_l\\\\ z_l\\\\ 1 \\end{bmatrix}$\nwhere $(x_c, y_c, z_c)$ represents a three-dimensional coordinate point in the Camera Coordinate System. $R_C \\in R^{3 \\times 3}$ and $T_C \\in R^{3 \\times 1}$ represent the rotation and translation from the LiDAR Coordinate System to the Camera Coordinate System, respectively.\nCamera to LiDAR: The transformation from Camera Coordinate System to LiDAR Coordinate System is the inverse transformation of Eq.(2)."}, {"title": "A.3. Camera Pixel", "content": "Camera to Pixel: The projection formulas of different types of cameras are different in the RoboSense dataset, the projection formula of a pinhole camera is as follows:\n$\\begin{bmatrix} u\\\\ v\\\\ 1 \\end{bmatrix} = K_{3 \\times 3} \\frac{1}{z_c} \\begin{bmatrix} x_c\\\\ y_c\\\\ z_c \\end{bmatrix} \\quad K_{3 \\times 3} = \\begin{bmatrix} f_x & 0 & u_0\\\\ 0 & f_y & v_0\\\\ 0 & 0 & 1 \\end{bmatrix}$\n$(u, v)$ is pixel coordinate, $K \\in R^{3 \\times 1}$ represents the camera intrinsic parameters, $(f_x, f_y)$ represents the focal lengths of the camera, and $(u_0, v_0)$ indicates the displacement of the camera's optical center from the origin of the Pixel Coordinate System. The projection formula from camera coordinate to pixel coordinate of the fisheye camera is very different, the camera projection process refers to the projection formula of Omnidirectional Camera (OCam) in [29].\nPixel to Camera: The transformation from Pixel Coordinate System to Camera Coordinate System in a pinhole camera model requires the inverse of Eq.(3). Since this is a 2D to 3D transformation, it is necessary to first determine the magnitude of $z_c$. The projection formula from pixel coordinate to camera coordinate of the fisheye camera refers to the projection formula of Omnidirectional Camera (OCam) in [29]."}, {"title": "A.4. Ego-Vehicle Global", "content": "Ego-Vehicle to Global: $R_G \\in R^{3 \\times 3}$ and $T_G \\in R^{3 \\times 1}$ represent the transformation matrices of the vehicle's orientation and position in the Global Coordinate System, respectively. The transformation formula for converting the coordinates $(x_v, y_v, z_v)$ in the"}]}