{"title": "RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed Autonomous Driving", "authors": ["Haisheng Su", "Feixiang Song", "Cong Ma", "Panpan Cai", "Wei Wu", "Cewu Lu"], "abstract": "Robust object detection and tracking under arbitrary sight of view is challenging yet essential for the development of Autonomous Vehicle technology. With the growing demand of unmanned function vehicles, near-field scene understanding becomes an important research topic in the areas of low-speed autonomous driving. Due to the complexity of driving conditions and diversity of near obstacles such as blind spots and high occlusion, the perception capability of near-field environment is still inferior than its farther counterpart. To further enhance the intelligent ability of unmanned vehicles, in this paper, we construct a multimodal data collection platform based on 3 main types of sensors (Camera, LiDAR and Fisheye), which supports flexible sensor configurations to enable dynamic sight of view for ego vehicle, either global view or local view. Meanwhile, a large-scale multi-sensor dataset is built, named RoboSense, to facilitate near-field scene understanding. RoboSense contains more than 133K synchronized data with 1.4M 3D bounding box and IDs annotated in the full 360\u00b0 view, forming 216K trajectories across 7.6K temporal sequences. It has 270\u00d7 and 18\u00d7 as many annotations of near-field obstacles within 5m as the previous single-vehicle datasets such as KITTI and nuScenes. Moreover, we define a novel matching criterion for near-field 3D perception and prediction metrics. Based on RoboSense, we formulate 6 popular tasks to facilitate the future development of related research, where the detailed data analysis as well as benchmarks are also provided accordingly.", "sections": [{"title": "1. Introduction", "content": "Recent years have witnessed significant progress achieved in the field of autonomous driving, enabling numerous intelligent vehicles running on highway or urban areas. In addition to passenger cars and commercial vehicles, unmanned function vehicles have emerged as a new industry focus in low-speed autonomous driving. Differently, low-speed unmanned vehicles have clear application scenarios and controllable costs, which are mostly carried out in enclosed and semi-enclosed regions, with relatively fixed but more diverse scenarios, such as campuses, scenic spots, airports and parks, etc. The typical functions of such unmanned vehicles include tractor, sweeper, retail and delivery, which are closer to the landing goal of L4 autonomous driving. Hence, it is essential to push the limit of intelligent driving technology on low-speed driving scenarios.\nSingle-vehicle datasets related to autonomous driving have been released in recent years [4, 8, 15, 27, 33, 40], aiming at promoting the academic research on key technologies such as object perception and prediction. KITTI [8] is a pioneering dataset providing multi-modal sensor data including front-view LiDAR pointclouds as well as corresponding stereo images and GPS/IMU data. nuScenes [4] constructs a multi-sensor dataset collected in two cities travelling at an average of 16 km/h, where rich collections of 3D boxes and IDs are annotated in the full 360\u00b0 view, covering 1K scenes and 23 classes. Waymo Open dataset [33] significantly increases the amount of annotations with higher annotation frequency. However, these datasets are all constructed for passenger vehicles, where the sensor data are mostly captured from highway or urban roads. Due to the high-speed driving condition and structural driving constrains of traffic rules and static road elements (i.e., lanes, boundary and crossings), collected objects are distributed in a farther distance from the ego vehicle, lacking surrounding obstacles with extremely near distance. Therefore, AV models trained on these datasets are prone to concentrate on objects locating on farther areas, rather than near-field obstacles with high occlusion and truncation owing to the blind spots that are beyond the sight of Camera and LiDAR sensors. Besides, obstacle diversity on motorway is also limited, which usually only includes vehicle and VRU (Vunerable Road User). Obviously, AV models developed upon current datasets can not satisfy the performance requirement of low-speed vehicles, which are surrounded with various kinds and shapes of near obstacles in complex scenarios.\nTo fill the gap between high-speed and low-speed autonomous driving, meanwhile further accelerate research of near-field AV algorithms, in this paper, we construct a large-scale multimodal dataset and benchmark, named as RoboSense, which is collected from diverse scenarios with various kinds of obstacles. The data collection vehicle is equipped with 3 main types of sensors (C: Camera, L: LiDAR, F: Fisheye). To ensure the data capture under full 360\u00b0 view, each type of sensor consists of 4 devices installed on different sides of the data collection vehicle respectively, thus the captured areas of multiple sensors can overlap with each other. Benefiting from the well time-synced multi-sensor collected data pairs, our RoboSense can allow researchers to take flexible sensor combinations as input, thus achieving dynamic sight of view for scene understanding.\nRoboSense consists of a total of 133K+ frames of synchronized data, spanning over 7.6K temporal sequences of 6 main scene classes (i.e, scenic spots, parks, squares, campuses, streets and non-motorized lanes). Moreover, 1.4M 3D bounding boxes together with track IDs are annotated based on 3 different types of sensors, with more than 30% of targets locating within 10m. Then we form global trajectories for each agent separately through associating the same IDs across consecutive frames and different sensors from a Bird's-Eye View (BEV) perspective. Based on the constructed dataset, we formulate six popular autonomous driving tasks and benchmarks as follows: 1. Multi-view 3D Detection; 2. LiDAR 3D Detection; 3. Multi-modal 3D Detection; 4. Multiple 3D Object Tracking (3D MOT); 5. Motion Prediction; 6. Occupancy Prediction. Meanwhile, multi-task end-to-end training scheme is also supported in our RoboSense for evaluation of joint optimization. In sum, the main contributions of our work are four folds:\n\u2022 To the best of our knowledge, our RoboSense is the first dataset constructed for outdoor low-speed autonomous driving research of unmanned vehicles, which especially concentrates on near-field scene understanding.\n\u2022 We annotate 1.4M 3D bounding boxes on 133K+ synchronized frames of multi-sensor data. Over 30% of the targets are located within the near field around the ego vehicle. Each target is associated with a unique ID, thus forming a total of 216K trajectories, which spread over 7.6K temporal sequences, covering 6 main scene classes.\n\u2022 We construct several sensor layouts with flexible sensor configurations of Cameras, Fisheyes and LiDARs, along with multi-sensor synchronized data collected for scene understanding from a robotic sight of view.\n\u2022 We formulate 6 popular tasks as well as benchmarks to facilitate the research development of near-field environmental perception and prediction."}, {"title": "2. Related Work", "content": "The development of autonomous driving has been significantly accelerated by the availability of several prominent datasets. We summarize the compositions and comparisons of some influential datasets as shown in Tab. 1, including both perception datasets and prediction datasets.\nPerception Datasets. Current released perception datasets can be divided into image-only datasets [6, 42] and multimodal datasets [4, 8, 15, 16, 33]. BDD100k [42] and Cityscapes [6] focus on 2D perception which provide large amount of 2D annotations (boxes, masks) for driving scene understanding under various weather and illumination conditions. KITTI [8] is known as the pioneering multimodal dataset which has been widely used for academic research. It records 6 hours of driving data using a LiDAR sensor, GPS/IMU sensors and a front-facing stereo camera to provide dense pointclouds and images with annotated 3D boxes. H3D dataset [25] collects a total of 1.3M 3D objects over 27K frames from 160 crowded scenes of the full 360\u00b0 view. nuScenes [4] and Waymo Open Dataset [33] are two similar datasets with same structure, while the latter one providing more annotations owing to higher annotation frequency (2Hz vs. 10Hz). Differently, our proposed RoboSense dataset is annotated in 1Hz due to the relative lower driving speed (less than 1m/s), which is constructed on low-speed driving scenarios, aiming to facilitate near-field scene understanding for L4 unmanned vehicles working on unstructured roads.\nPrediction Datasets. nuScenes [4] and Waymo Open Dataset [33] can be also used for prediction task which release lane graphs as well. Lyft [16] introduces traffic/speed control data, and Waymo Open Dataset [33] adds more signals to the map such as crosswalk, lane boundaries, stop signs and speed limits. Recently, Shifts dataset [24] becomes the largest forecasting dataset with the most scenario hours to date. Meanwhile, Argoverse [5] is also a large-scale dataset with high data frequency (10Hz) and high scenario quality for motion forecasting (>2000km across 6 cities). Together, these datasets have enabled exploration of multi-actor, long-range motion forecasting leveraging both static and dynamic maps.\nOur dataset differs in three substantial ways: 1) more crowded objects (270\u00d7 and 18\u00d7 than KITTI and nuScenes) are collected within a near range in diverse scenarios, which is challenging for perception due to frequent occlusions and truncations, let alone motion forecasting of surrounding agents. 2) In addition to motion forecasting, our dataset also provide high-quality occupancy descriptions for each keyframe, allowing dense occupancy prediction of open-set obstacles. 3) Our dataset is mostly collected on unstructured/semi-enclosed roads/regions, without the explicit signals such as lane boundaries and traffic signs as provided in previous datasets, thus is more challenging and practical for unmanned function vehicles."}, {"title": "3. RoboSense Open Dataset", "content": "To facilitate the near-field obstacle perception and prediction, we introduce RoboSense, a real-world, large-scale and multi-modal dataset annotated with abundant of 3D bounding boxes and corresponding trajectories. We commence with the sensor setup as well as data acquisition details, delineate the coordinate systems and label generation process, and present data statistics respectively."}, {"title": "3.1. Sensor Setup and Data Acquisition", "content": "Sensor setup. We use an electric sweeper as data collection platform, which is equipped with 5 main types of sensors, including LiDAR, Camera, Fisheye, GPS/IMU and Ultrasonic, which are installed in top, front, rear and sides of the ego vehicle respectively to ensure data collection in 360\u00b0 horizontal view. Refer to Fig. 2 for sensor layouts and Tab. 3 for detailed sensor specifications.\nData acquisition. We utilize the equipped sweeper to collect multi-sensor data along the Dishui Lake in Shanghai, China, lasting 42h in total at an average speed of less than 1m/s through manually remote control. 22 different places are travelled, which can be categorized into 6 kinds of outdoor or semi-closed scenarios (i.e., office parks, tourist attractions, plazas, schools, streets and roads). After collecting the raw sensor data, we manually select and process 7619 representative scenes of 20s duration respectively for further annotation, which include various densities of traffic and human crowds, diverse weather and light conditions and abundant classes and number of obstacles."}, {"title": "3.2. Coordinate Systems", "content": "Ego-Vehicle Coordinate. The Ego-Vehicle Coordinate System is centered at the rear axle of the vehicle. The positive directions of the X, Y, and Z axes correspond to the forward, leftward, and upward directions of the vehicle, respectively. Ego-Vehicle Coordinate System is the most frequently used in tasks such as perception, tracking, prediction, and planning, where dynamic and static targets as well as trajectories are transformed into this coordinate system.\nGlobal Coordinate. To transform the dynamic and static elements from historical and future frames into the current frame coordinate system, we need to establish a global coordinate system to record the position and orientation of the ego vehicle in each frame. The origin of the Global Coordinate System is an arbitrarily defined point in Shanghai Lingang, China, and the positive directions of the X, Y, and Z axes follow the definition of the North-East-Up coordinate.\nLiDAR Coordinate. The LiDAR Coordinate System is defined based on the Hesai lidar installed directly above the vehicle, the positive directions of the X, Y, and Z axes follow the definition of the Ego-Vehicle Coordinate System.\nCamera Coordinate. The RoboSweeper is equipped with four fisheye cameras and four pinhole cameras. The origin of the Camera Coordinate System for both types of cameras is the optical center. However, the positive directions of the coordinate axes are defined differently in the RoboSense dataset. In the fisheye coordinate system, the X, Y, and Z axes correspond to directly below, right, and behind the optical center, respectively. In contrast, in the pinhole coordinate system, these axes correspond to directly right, below, and front of the optical center, respectively.\nPixel Coordinate. The image is presented in the form of pixels, each pixel corresponds to a 2D pixel coordinate. The origin of the Pixel Coordinate System is the upper left corner of the image. Points in the 3D Camera Coordinate System can obtain coordinates in the Pixel Coordinate System through the camera projection."}, {"title": "3.3. Ground Truth Labels", "content": "After integrating, synchronizing and calibrating the multi-sensor raw data, we annotate keyframes (LiDAR, image) at the frequency of 1Hz, owing to such applications of unmanned functional vehicles generally running on low-speed scenarios (less than 1 m/s).\n3D object. With the selected scenes of collected RoboSense dataset, we annotate 3D object boxes of 3 movable classes (i.e., \"Vehicle\", \"Cyclist\" and \"Pedestrian\") for each sampled keyframe in both the LiDAR coordinate of point-clouds and the Camera coordinate of multi-view images respectively. Each annotated 3D box can be represented as $[x, y, z, w, l, h, \\theta, cls]$, where x, y, z indicate the 3D position of a regular object, and w, l, h represent the scale information including width, length and height. $\\theta$ and cls correspond to the orientation (especially yaw angle) and the object class respectively. A three-stage auto-labelling pipeline is detailed in the supplementary material (see Sec. B.2).\nTrajectory. To facilitate the temporal tasks such as multi-object tracking and motion forecasting described in Sec. 4, we assign a unique Track ID $\\tau$ to each agent across a temporal sequence on Bird-Eye-View (BEV) of the Ego-Vehicle coordinate. Furthermore, agents with the same $\\tau$ within a sequence are linked together to form object trajectories.\nOccupancy label. In addition to 3 typical classes of moving objects on roads which are annotated temporally as above, there also exists a rich collection of static obstacles with irregular shapes especially in the complex scenarios (i.e., parks, schools and plazas, etc.) of RoboSense. To detailly describe the environment in surrounding camera views for driving safety, we voxelize the 3D space and generate high-quality yet dense occupancy labels to represent the voxel states. Similar with previous occupancy benchmarks [35, 36] built upon public datasets [4, 33], we conduct dynamic objects and static scenes segmentation along the temporal dimension based on annotated 3D boxes and trajectories. Then sparse LiDAR points inside each box are extracted from $T-k$ to $T+k$ frames respectively, where T indicates the index of current keyframe, and k is set to 10 empirically. Refer to the supplementary material for more details of occupancy label generation process (see Sec. B.3)."}, {"title": "4. Tasks & Metrics", "content": "Various tasks related to autonomous driving are supported on RoboSense dataset, which can be divided into two main categories, namely perception and prediction. The perception task mainly includes 3D object detection and tracking based on image or pointcloud sequences. The prediction task aims to predict the occupancy and motion trajectory of each agent surrounding the ego-vehicle."}, {"title": "4.1. Perception", "content": ""}, {"title": "4.1.1 3D Object Detection", "content": "The RoboSense 3D detection task includes multi-view Image 3D and LiDAR 3D detection, which requires to detect 3 object classes with 3D bounding boxes, i.e. \u201cVehicle\", \"Pedestrian\" and \"Cyclist\u201d. Formally, given multi-view image sequences or point cloud sequences, a set of 3D bounding boxes in the form of $[x, y, z, w, l, h, \\theta, cls]$ are detected, indicating target position, size, orientation and category respectively. Following the conventions in [4, 9, 23, 33], we adopt mAP (mean Average Precision), mAOS (mean Average Orientation Similarity) and mASE (mean Average Scale Error) to measure the performance of different detectors.\nAverage Precision (AP) metric is commonly utilized to evaluate the accuracy of 2D or 3D detectors. However, there are several matching criteria to define the true positive. For example, [9] adopts 3D IOU (Intersection-Over-Union) to match each prediction with a ground truth box, while [4] define a match through thresholding the 2D center distance on the Bird-Eye-View ground plane. As for RoboSense detection task, we also adopt a similar distance measure. Differently, we define the threshold as a relative proportion $p$ of ground truth closest collision-point distance from the ego-vehicle, rather than an absolute center distance $d$ adopted in [4]. We claim that the localization accuracy of near obstacles' closest collision-point is more important in low-speed driving scenarios. Then AP is calculated as the normalized area under precision-recall curve [7]. Finally, mAP is obtained by averaging over all classes C and matching thresholds P = {5%, 10%, 20%}:\n$\\text{mAP} = \\frac{1}{|C|P} \\sum_{C \\in C} \\sum_{p \\in P} \\text{AP}_{C,P}$ (1)\nIn addition to AP, we further measure AOS as well as ASE for each matched true positive prediction, which represent the precision of predicted yaw angle and object scale respectively as follows:\n$\\text{mAOS} = \\frac{1}{|C|} \\sum_{C \\in C} \\text{AOS}_{C}$, (2)\nwhere C is the set of categories and AOS (Average Orientation Similarity) is formulated as:\n$\\begin{equation}\\text{AOS} = \\frac{\\sum_{r \\in R} \\frac{1}{\\chi(r)} \\sum_{i \\in D(r)} \\text{max } \\text{s}(\\tau)}{\\sum_{r \\in R} \\frac{1}{\\chi(r)}}, (3)\\end{equation}\n$\\begin{equation}\\text{s}(\\tau) = \\frac{1+\\text{cos}(\\Delta \\theta)}{2}, (4)\\end{equation}\nwhere R is in the recall range [0.1, 1] obtained by performing 40-point interpolation. D(r) indicates the set of matched true positive samples at recall $r$. And $\\Delta \\theta$ denotes the angle difference between sample $i$ and ground truth. Different from [33], we only consider true positive samples under each recall level, rather than all predicted positives.\nASE is defined as 1-IoU, which aims to measure the scale error through calculating the 3D IoU after aligning orientation and translation of predictions with ground truth. And mASE is obtained similar to former metrics by averaging the accumulative mean scale errors of true positive samples under various recall levels over detected object classes.\""}, {"title": "4.1.2 Multi-Object Tracking", "content": "The tracking task is designed to associate all detected 3D boxes of movable object classes across input multi-view temporal sequences (i.e. videos or point cloud sequences). Each object is assigned a unique and consistent track ID $\\tau$ from first appearance until complete vanishing. As for performance evaluation, we refer to [4, 9, 22, 32], and mainly adopt SAMOTA (Scaled Average Multi-Object Tracking Accuracy), AMOTP (Average Multi-Object Tracking Precision) to measure the 3D tracking performance.\nFormally, SAMOTA is defined as the mean value of SMOTA over all recalls:\n$\\text{SAMOTA} = \\frac{1}{|R|}\\sum_{r \\in R} \\text{SMOT}_{Ar}$ (5)\n$\\text{SMOTA} = \u0442\u0430\u0445(0,1 - \\frac{FP_{r} + FN_{r} + IDS - (1 - r) \\cdot GT}{r \\cdot GT}), (6)\nwhere $FP_{r}$, $FN_{r}$ and $IDS$ represent the number of false positives (wrongly detection), false negatives (missing detection) and identity switches at the corresponding recall $r$, respectively. Similarly, AMOTP is the average results of MOTP among different recall rates, which can defined as follows:\n$\\text{AMOTP} = \\frac{1}{|R|}\\sum_{r \\in R} \\frac{\\sum_{i} d_{i,t}}{\\text{TP}}$ (7)\nwhere $\\text{TP}$ is the number of true positives at the recall $r$, and $d_{i,t}$ denotes the position error of matched track $i$ at timestamp $t$. Besides, additional metrics such as MT (Most Tracked) and ML (Most Lost) from [3] are together evaluated for reference."}, {"title": "4.2. Prediction", "content": ""}, {"title": "4.2.1 Motion Forecasting", "content": "Based on perception results, the motion forecasting task requires to predict agents' future trajectories. Specifically, K plausible trajectories in future T = 3s timesteps for each agent are forecasted as offsets to the current agent's position. Following the standard protocols [10, 20, 21, 26], we adopt minADE (minimum Average Displacement Error), minFDE (minimum Final Displacement Error), MR (Miss Rate) and EPA (End-to-end Prediction Accuracy) as metrics to measure the precision of motion prediction. In order to decouple the accuracy of perception and prediction, these metrics are only caculated for matched TPs (True Positives), where the matching threshold is set to $p_{match}$ = 5% of ground truth distance of the closest collision-point from the ego-vehicle. And the miss threshold of minFDE is set to $p_{miss}$ = 20% for calculating the MR metric."}, {"title": "4.2.2 Occupancy Prediction", "content": "The goal of occupancy prediction task is to estimate the state of each voxel in the 3D space. Formally, a sequence of T historical frames with N surround-view camera images $\\{I_{it} \\in R^{H\\times W\\times 3}\\}$ are served as input, where i = 1, ..., N and t = 1, ..., T. Besides, sensor intrinsic parameters $\\{K_{i}\\}$ together with extrinsic parameters $\\{R_{i|t_{i}}\\}$ for each frame are also provided. Then the ground truth labels describe the voxel states separately, including occupancy state and semantic label. Three states are considered on the RoboSense dataset, including \u201coccupied\u201d, \u201cfree\u201d and \u201cunknown\". And the semantic label of each voxel can be one of the 3 pre-defined object categories or an \"unknown\" class to indicate general objects. Furthermore, each voxel can be also equipped with extra attributes as outputs, such as instance IDs and motion vectors, which are left as our future work.\nTo evaluate the quality of predicted occupancy, we measure the whole-scene level voxel segmentation results using IoU metric for each class. Considering the low-speed driving scenarios, we evaluate the metric under different ranges around the ego vehicle in both 3D and BEV space. Finally, mIoU is obtained through averaging over 4 classes. Moreover, evaluation is only performed on the visible voxels from the camera view."}, {"title": "5. Experiments", "content": "In this section, we present the benchmark setup with sensor specifications, and then describe the analytical details of multiple baselines with different modalities in terms of perception and prediction experiments conducted on the RoboSense dataset, respectively."}, {"title": "5.1. Benchmark Setup", "content": "Our RoboSense dataset contains 7.6K sequences (including 130K annotated frames) of synchronized multi-sensor data, covering 6 main categories (including 22 different locations) of outdoor or semi-closed scenarios (i.e., S1-office parks, S2-tourist attractions, S3-plazas, S4-schools and S5-unstructured streets or S6-roads). To protect the data privacy, we conduct a series of data desensitization measures through masking the human faces and car plates as well as road signs from all sensor data. The details of RoboSense dataset composition and partitioning are listed in the Tab. 2. The RoboSense dataset is collected under various illumination, traffic flow and weather conditions, to ensure the diversity of static background and movable obstacles, thus meeting the demand of different realistic applications.\nRoboSense dataset is divided into three parts with a ratio of 50%, 40% and 10%, for the purpose of training, testing and validation respectively. As for the scene partition, one of the 6 collected scenes (i.e. S-6) is assigned to the testing set exclusively, while the remaining scenes are shared among all splits. Ground truth labels of training and validation sets for corresponding task are provided, together with the synchronized multi-sensor raw data. However, the testing set only provides data. Hence algorithms can merely be submitted to our online benchmark for corresponding task evaluation of testing set."}, {"title": "5.2. Sensor Specifications", "content": "The detailed specifications of all devices are shown in Tab. 3. To cover the areas from local to global view, we select Cameras with different focal lengths and Field of View (FOV). Besides, 5 LiDAR sensors are installed in our data collection platform, where the top Hesai Pandar40M is served as autolabeller to provide initial annotations for the splicing points of target LiDARs. 11 Ultrasonics sensors are also installed for freespace detection to ensure safety. All devices are synchronized in time via Network Time Protocol (NTP) before data collection, we utilize a time interval of 100ms as the global timestamp for intersections, and match the frame from each device with the nearest timestamp adjacent to the global timestamp. This process ultimately yields synchronized multi-sensor data at a frame rate of 10 FPS."}, {"title": "5.3. Implementation Details", "content": "For LiDAR detection task, we set the point range to x\u2208[-45m, 45m], y\u2208[-45m, 45m], z\u2208[-1m, 4m], with a fixed voxel size of 0.16m and 0.05m for pillar-based and voxel-based methods respectively. For Image detection tasks, we use ResNet18 [11] as backbone network and the input image is resized to 640 \u00d7 352. For practical usages, we report performance using our proposed Closest-Collision Distance Proportion (CCDP) as matching criterion. Comparisons of different matching functions on average precision are shown in Fig. 4. As expected, when using Center Distance (CD) or IOU, objects without distance differentiation can not reflect the model capability of locating closest collision points of nearby obstacles, which is more challenging and essential for low-speed driving scenarios."}, {"title": "5.4. Baselines: Perception", "content": ""}, {"title": "5.4.1 LiDAR 3D Detection", "content": "To demonstrate the performance of advanced 3D detectors on LiDAR-only detection track of our RoboSense benchmark, we implement several popular CNN-based methods with different fashions, including Pointpillar [17] (Pillar-based), SECOND [41] (Voxel-based), and PV-RCNN [31] (Two-stage Point-Voxel based). Besides, Transformer-based method such as Transfusion-L [2] is also implemented for architecture comparison. Pointpillar as the most efficient method above is adopted as our baseline for LiDAR 3D detection task."}, {"title": "5.4.2 Multi-View 3D Detection", "content": "Current works of multi-view 3D detection can be divided into two mainstreams, namely LSS [28] based and Transformer based. To examine the effectiveness of image-only multi-view 3D detection models, we select the widely-used method BEVDet [14] as our LSS-based baseline on image 3D detection track of RoboSense, and re-implement several extended versions such as BEVDet4D [13] which takes advantage of history temporal clues, and BEVDepth [18] which adopts an additional branch for depth prediction under point supervision. Besides, BEVFormer [19] as a Transformer-based representative work is also included."}, {"title": "5.4.3 Multiple Object Tracking", "content": "We follow the \"Tracking-by-Detection\" paradigm using 3D detection results from Camera or LiDAR data as input respectively, and present several baselines for multiple 3D object tracking task. Specifically, 3D boxes detected from surround-view images by BEVDepth [18] and splicing pointclouds by Pointpillar [17] are provided separately. And the tracking approach AB3DMOT described in [39] is picked to serve as the baseline of multiple object tracker in the 3D space. Then the same objects across different sensors are associated with unique track IDs to form global trajectories in the past."}, {"title": "5.5. Baselines: Prediction", "content": ""}, {"title": "5.5.1 Motion Prediction", "content": "Traditional motion prediction methods utilize perception ground truth (i.e., history trajectories of agents and HDmap) as input, which lacks of uncertainty modeling in practical applications. In this paper, we implement several vision-based end-to-end methods for joint perception and motion prediction on RoboSense benchmark, including ViP3D [10] and PnPNet [20]. For comparisons, we also report the motion prediction results of assuming agents surrounding the ego-vehicle with constant positions or velocities respectively, thus to reflect the diversity and difficulty of our dataset on prediction task."}, {"title": "5.5.2 Occupancy Prediction", "content": "We extend a BEV 3D detection model - BEVDepth [18] to the 3D occupancy prediction task, which is then adopted as our baseline for the visual occupancy prediction task. Concretely, we replace the original detection decoders with the occupancy reconstruction layers while maintaining the BEV feature encoders. ResNet18 [11] pretrained on FCOS3D [38] is employed as image backbone for visual feature extraction."}, {"title": "5.6. Results and Analysis", "content": "Benefiting from the multi-sensor collected data with various kinds of high-quality annotations, we conduct extensive baselines of different tasks. As for perception-level tasks, we evaluate the 3D object detection as well as multi-object tracking respectively using different kinds of sensor data as input, including vision-based paradigm and LiDAR-based paradigm. While for the prediction-level tasks, motion prediction and occupancy prediction are two crucial tasks for final ego-vehicle planning as proven in [1, 12, 37]. The main metric for each task is marked with gray background color in corresponding tables for intuitive comparison."}, {"title": "5.6.1 Perception Results", "content": "The 3D detection results based on multi-view images and splicing point clouds are shown in Tab. 4, where 3D AP, AOS and ASE are evaluated for three different object classes respectively. As for LiDAR 3D detection, Transfusion-L [2", "13": "obtains slight improvement than BEVDet [14", "18": "yields a significant improvement of 2.0% and 2.2% 3D AP compared to BEVDet [14", "19": "introduced a query-based attention mechanism to encode BEV features, also achieving competitive results. Generally, LiDAR-based 3D detector can generate high-quality detection results than vision-based methods. However, vision-based methods are capable of detecting various ranges of objects with more sensors owing to the controllable cost. Note that two different matching criteria are both considered for TP calculation, namely Center-Point (CP) distance and Closest Collision-Point (CCP) distance. It can be observed that the CCP localization performance is obviously lower than the CP localization, i.e. 18.5% 3D AP drop of Transfusion-L for Vehicle class and 29.5% 3D AP drop for Pedestrian class. For security consideration, the CCP localization is more important for near-field obstacle perception under low-speed driving conditions.\nTo evaluate the performance of different sensor layouts under various ranges, we conduct extensive comparisons of multi-view 3D perception, LiDAR 3D perception and multi-modal 3D perception respectively as shown in Tab. 5. As for multi-view 3D perception, 4C layout achieves better 3D detection AP than 4F layout in farther areas (i.e., 10-30m), while 4F layout is good at detecting near-field objects within 10m. Through combining these two types of sensors, better performance can be obtained across different ranges with 8 camera views as input. LiDAR 3D detector exhibits an obvious advantage over vision-based detectors especially in CCP and farther object localization. For example, 4L layout obtains 26.8% 3D AP improvement over 4C+4F layout, while the performance of near-field objects within 5m is inferior (19.3% vs. 20.5%). Moreover, we implement multi-modal 3D perception (8V+4L) through late-fusion strategy. Specifically, 3D detection results from multi-view 3D detector and LiDAR 3D detector are adopted for post-processing. And we can observe that the CCP-based 3D AP of objects within 5m is remarkably boosted from 20.5% to 36.9%, and the AOS performance is also increased accordingly by a great margin.\nRegarding to the MOT task in Tab. 5, multi-view (4C or 4F) 3D based tracking exhibits poorer performance than LiDAR-based paradigm due to the weaker detection performance. LiDAR 3D tracker performs multi-object tracking in 3D space, which mitigates the impact of object occlusions existing in 2D image, especially for crowded scenarios. However, through introducing more sensors (4C+4F), vision-based methods can also achieve competitive tracking performance with LiDAR-based methods, even better in SAMOTA metric (51.16 vs. 44.77). Similarly, with multi-modal 3D perception, AMOTP, MT and ML performance can be further improved as expected. As for 3D tracker method, AB3DMOT [39"}]}