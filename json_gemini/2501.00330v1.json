{"title": "Exploring the Implicit Semantic Ability of\nMultimodal Large Language Models: A Pilot Study\non Entity Set Expansion", "authors": ["Hebin Wang", "Yangning Li", "Yinghui Li", "Hai-Tao Zheng", "Wenhao Jiang*", "Hong-Gee Kim"], "abstract": "The rapid development of multimodal large lan-\nguage models (MLLMs) has brought significant improvements to\na wide range of tasks in real-world applications. However, LLMs\nstill exhibit certain limitations in extracting implicit semantic\ninformation. In this paper, we applies MLLMs to the Multi-\nmodal Entity Set Expansion (MESE) task, which aims to expand\na handful of seed entities with new entities belonging to the same\nsemantic class, and multi-modal information is provided with\neach entity. We explore the capabilities of MLLMs to understand\nimplicit semantic information at the entity-level granularity\nthrough the MESE task, introducing a listwise ranking method\nLUSAR that maps local scores to global rankings. Our LUSAR\ndemonstrates significant improvements in MLLM's performance\non the MESE task, marking the first use of generative MLLM\nfor ESE tasks and extending the applicability of listwise ranking.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have demonstrated out-\nstanding performance across a wide range of NLP tasks [1]\u2013\n[6] since their inception [7]\u2013[14]. The Multi-modal Language\nModels (MLLMs) merge the advanced reasoning capabilities\nof traditional LLMs with the processing of image, video, and\naudio data, garnering widespread attention for its ability to\nunderstand and generate visual language [15]\u2013[18].\nDespite the remarkable capabilities of MLLMs, they exhibit\nnotable limitations when it comes to implicit semantic infor-\nmation extraction and reasoning. One of the challenges for\nMLLMs lies in their reliance on explicit contextual informa-\ntion. Without well-defined prompts or rich context, MLLMS\nstruggle to infer hidden semantic relationships. For MLLMs,\nthe fusion of different modalities can lead to an overemphasis\non primary features, such as dominant visual cues in image-\ntext tasks, while neglecting subtler yet important contextual\nsignals. This issue limits their ability to infer complex rela-\ntionships between entities across modalities.\nIn our work, we explore the implicit semantic information\nextraction capabilities of MLLMs by applying the Multimodal\nEntity Set Expansion (MESE) task as probing task, which is a\nrepresentative task for evaluating their ability to infer hidden\nsemantics. The MESE task [18]-[20] seeks to expand the\ngiven set of seed entities by identifying additional entities\nthat belong to the same semantic class, utilizing a predefined\ncandidate entity vocabulary and corpus. For instance, given\nthe multimodal seed entities New York, Chicago, Los Angeles,\nthe goal of MESE is to retrieve other entities that implicitly\nshare the semantic category US Cities, such as NYC, and\nBoston.\nThe challenge of the MESE task lies in the implicit semantic\ninduction of entity set, which aligns with the implicit semantic\ninformation extraction challenges of MLLMs. Therefore the\nperformance on the MESE task can serves as an indicator of\nthe model's effectiveness in uncovering semantic information\nat the entity level. Taking the challenge of negative entities\nwith fine-grained semantic differences in the MESE task\nas an example, without an explicit prompt specifying the\nexact semantic class of the three seed entities, MLLM may\nencounter difficulties in inferring the hidden common features\nfrom a very small number of instances, in which they tend\nto generalize broad and imprecise semantic classes, rather\nthan extracting more fine-grained features. For instance, when\nprovided with a few plant entities (such as oak and maple)"}, {"title": "II. METHODOLOGY", "content": "We propose a novel method LUSAR, which is divided into\ntwo stages. As illustrated in Figure 1, in the first stage, a prefix\ntree constraint is used to restrict the large model to generate\na large number of candidate entities within a specified entity\ndataset. In the second stage, we designed a listwise approach.\nThis approach scientifically performs multiple sampling and\nranking of each candidate entity to obtain a ranking score for\neach entity. The sorted scores are then used as the final order\nfor evaluation."}, {"title": "A. Candidates Entities Sample", "content": "We employ a generative framework with constrained de-\ncoding strategy to the large language model, generating a\ncertain number of candidate entities for the initial step of\nentity filtering. Instead of relying on the intermediate semantic\nfeatures of entities as traditional models do, we adopt a\ngenerative method to accomplish entity set expansion. To\nensure that the LLM generates only valid entities within the\nentity vocabulary, we build a prefix tree based on the entity\nvocabulary and employ prefix-constrained Beam Search for\ndecoding [23], [24]. During the generation, at each timestep\nthe model extend a patial entity in the beam words with\nevery possible word in the vocabulary. Beam search will only\nmaintain the top n most likely tokens based on the model's\nlog probability, where n represents for the number of beams.\nOnce the \u00a1EOS\u00bf token is appended to a entity, it is removed\nfrom the beam and added to the set of completed candidate\nentities set."}, {"title": "B. Listwise Ranking prompting", "content": "1) prompting design: We utilize natural language prompt\ntemplates to transform the ranking problem into a generation\nproblem. The large language model will process the generation\ntask and only output the sequence of entities ranked by their\nrelevance to the semantic class represented by the seed entity.\nWe use $R_m = r(s, E_m)$ to denote the the output ranked\nsequence, where s represents the seed entity set {A,B,C} used\nto query, $E_m$ is the canditate entities $e_1$, $C_2$, ...$e_n$. n is is\ntypically set to 5, to balance the increasing entity relevance\nwith the number of entities involved in the ranking, and the\nmodel's ability to process image information. $R_m$ is the ranked\norder of canditate entities, {$R_m$} = {$E_m$}.\n2) List Comparisons and Scoring: While pairwise ap-\nproaches require to enumerate all pairs and perform a global\naggregation to give each item a score, it is impractical for\nthe listwise approach to generate and rank every possible\ncombination of all candidate entities using LLM.\nWe developed an algorithm that samples an appropriate\nnumber of lists from the entire set of candidate entities,\nensuring that each entity appears with balanced frequency.\nWe set the sample size n to 5, which means LLM ranks five\ncandidate entities in each query. And the occurrences each"}, {"title": "III. EXPERIMENTS", "content": "For our experiments, we use the MESED [18] dataset as\nthe benchmark, which is a multi-modal ESE dataset with\nmeticulous manual calibration. The MESED comprises 70\nfine-grained semantic classes, with each fine-grained class\ncontains 5 queries with three seed entities and 5 queries with\nfive seed entities."}, {"title": "B. Baseline Models", "content": "We compare two categories of baseline model, the first\nis the (multimodal) LM-based model tailored for ESE, in-\ncluding SetExpan [27], CGExpan [28], ProbExpan [29],\nCLIP [30], and ALBEF [31]. Of the above models, the\nformer three are the are based on pre-trained language model\nBERT. CLIP and ALBEF are multimodal ESE models with\ntext and images as inputs. The other category of models\nare generative MLLMs with instruction-following capabilities,\nwhich is Qwen-VL-Chat [32], Deepseek-7b-VL-Chat [33],\nQwen2-VL-7b-Instruct [34]. To accommodate the number of\nimages associated with multiple seed entities and candidate\nentities in the MESE task, only multimodal large models that\nsupport multiple images were selected."}, {"title": "Evaluation Metrics", "content": "The objective of MESE is to expand the\nranked entity list based on their similarity to given seed entities\nin descending order. Two widely used evaluation metrics,\nMAP@K and P@K, are employed, also utilized in previous\nresearch [18], [28]. The MAP@K metric is computed as\nfollows:\n$\\frac{1}{Q} \\sum_{q \\in Q} AP_K (R_q, G_q)$"}, {"title": "C. Backbone MLLM for LUSAR", "content": "Our baseline use Qwen-VL-Chat model [32]. To contrast\nwith our listwise approach, we use single MLLM to complete\nthe task of scoring each entity, eliminating the comparison\nbetween multiple candidate entities. The relative nature of the"}, {"title": "IV. CONCLUSION", "content": "To explore the implicit semantic ability of the multimodal\nlarge language models, we conduct a pilot study on MESE\ntask. We propose LUSAR a novel listwise paradigm on\nLLM to MESE task. . Extensive experiments demonstrate the\nsignificance of our method in enhancing MLLMs' ability to\ninfer implicit semantic information, resulting in substantial\nperformance improvements on MESE task metrics. Moreover,\nour method is the first to apply LLMs to the ESE task. Our\nLUSAR framework innovatively introduces a new paradigm\nfor utilizing listwise ranking in large models, with broader\napplicability across various domains."}]}