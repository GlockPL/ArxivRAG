{"title": "A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene Graphs with Large-Language-Models (LLMs)", "authors": ["Yiye Chen", "Harpreet Sawhney", "Nicholas Gyd\u00e9", "Yanan Jian", "Jack Saunders", "Patricio Vela", "Ben Lundell"], "abstract": "Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason framework for reasoning and planning with scene graphs. Our approach employs two cooperative, code-writing LLM agents: a (1) Reasoner for task planning and information queries generation, and a (2) Retriever for extracting corresponding graph information following the queries. Two agents collaborate iteratively, enabling sequential reasoning and adaptive attention to graph information. Unlike prior works, both agents are prompted only with the scene graph schema rather than the full graph data, which reduces the hallucination by limiting input tokens, and drives the Reasoner to generate reasoning trace abstractly. Following the trace, the Retriever programmatically query the scene graph data based on the schema understanding, allowing dynamic and global attention on the graph that enhances alignment between reasoning and retrieval. Through experiments in multiple simulation environments, we show that our framework surpasses existing LLM-based approaches in numerical Q&A and planning tasks, and can benefit from task-level few-shot examples, even in the absence of agent-level demonstrations. Project code will be released.", "sections": [{"title": "1. Introduction", "content": "With the remarkable prowess in language interpretation and reasoning (Achiam et al., 2023; Touvron et al., 2023), Large Language Models (LLMs) have been increasingly adopted in the embodied planning tasks (Huang et al., 2023a; 2022; Zeng et al., 2022), including plan generation (Song et al., 2023), interaction (Joublin et al., 2024), and action selection (Rana et al., 2023). Despite the progress, the challenge of grounding the LLMs reasoning to situated environments remains unsolved, primarily due to the absence of a environmental representation that LLMs can effectively process (Huang et al., 2023c). While LLMs can interface with external tools to directlyx process perceptual data such as images (Liang et al., 2023; Huang et al., 2023b), they are unable to comprehend the intermediate, non-textual outputs from those tools, which prohibits generating the grounded reasoning trace. In contrast, scene graphs represent environments as hierarchical graphs that encapsulate spatial relationships and semantic attributes in a structured and serializable format (Zhu et al., 2021; Hughes et al., 2022). As a result, scene graphs have emerged as a scalable, high-level environment representation for LLM-based spatial reasoning and planning, showing effectiveness in both simulation-based (Yang et al., 2025) and real-world applications (Rana et al., 2023; Gu et al., 2024; Ni et al., 2023; Cheng et al., 2024).\nLeveraging Large Language Models (LLMs) for reasoning with scene graphs remains a challenging and under-explored problem. Recent research explores graphs-as-text as the input for the single generation (Fatemi et al., 2024; Gu et al., 2024), categorized as \"Reason-only\" methods in Fig. 1, showcasing LLMs' preliminary capacity to interpret graph topology. Yet, they are prone to hallucinations or exceed token limits when handling large graphs (Wang et al., 2023). To address the issues, the \"Retrieve-then-Reason\" strategy is proposed (Luo et al., 2024; Sun et al., 2023; Rana et al., 2023), wherein the LLM first identifies the sub-graph pertinent to a given task before reasoning on the retrieved part. While this strategy is adept at information collection, it struggles with complex tasks requiring comprehensive graph understanding and dynamically shifting focus based on the reasoning process, restricting the utility of LLMs in understanding complex scenes from textualized graphs. The aforementioned limitations restrict the utility of LLMs in understanding complex scenes from textualized graphs.\nRecent advances on iterative retrieve-augmented generation (Yao et al., 2022; Jiang et al., 2023; Press et al., 2022; Shao et al., 2023) provides promising solutions to this challenge. By interleaving generation with retrieval, these methods aggregate relevant information throughout the reasoning"}, {"title": "2. Method", "content": ""}, {"title": "2.1. Problem Statement", "content": "Our problem setting involves a natural language task instruction I and a scene graph G = (V, E), where V and E denote vertices and edges, respectively. Each node Vi represents an object along with its attributes, such as coordinates or colors, while each edge indicates a type of spatial relationship, such as inside or on top of. Additionally, we assume access to the scene graph schema S, which is a textual description of types, formats, and the semantics of the graph vertices and edges. Our objective is to generate the solution of I using LLMs, based on the available information above, expressed as A = f(I,G, S; LLMs)."}, {"title": "2.2. Overview of SG-RwR", "content": "We explore grounding the reasoning process to scene graphs based on the scene graph schema S and the code-writing ability of LLMs. We develop SG-RwR, a two-agent framework that iteratively reasons through the next steps and retrieves necessary information from the graph. As shown in Figure 2, our method contains two LLM agents: a Reasoner and a Retriever. The system initializes with the Scene Graph Schema, the Environment Description, general Guidance to direct the cooperation process, and task-dependent information such as the description of Agent Actions and Reasoning Tools. Given a task, the Reasoner determines the next substep to approach the task and identifies necessary scene graph information. It then raises a natural language query to the Retriever for this information. Upon receiving the query, the Retriever processes the scene graph through code-writing and sends the data back to Reasoner. By iteratively performing these steps, both agents collaborate to solve the task. Formally, at each time step t:\n\n$a_t, q_t = Reasoner({a_0, q_0, G'_0}, {a_1, q_1, G'_1},...... ; S)$ \n$h_t = Retriever (q_t; S)$ \n$G'_t = h_t(G)$\n\nwhere a denotes the current analysis by the Reasoner; q denotes natural language query for the graph information; h denotes the retrieval code following the query; and G' denotes the retrieved information by executing the code on the scene graph G.\nImportantly, unlike previous iterative methods (Yao et al., 2022; Shao et al., 2023) that uses a single LLM to process the entire history, the two agents in SG-RwR only exchange the query q and the corresponding graph data G', excluding the underlying thought process, such as a and h. As we will show, this agent-level context filtering, enabled by our two-agent design, is critical for eliminating the interference from irrelevant conversation history, thereby ensuring a seamless"}, {"title": "2.3. Reasoner", "content": "Reasoner is the central agent steering the task-solving iterations. We prompt it with the schema S, environment and task information (such as action description for the planning task), annotations of reasoning tools, general guidance to ensure automated task-solving conversation, and optionally, few-shot task-level examples. Reasoner then initiates the conversation with Retriever to solve a given task.\nConcretely, without any knowledge about the graph data initially, the Reasoner analyzes only the task I and graph schema S, generates the first analysis ao, and sends out the first associated query qo to the Retriever. At the tth round of conversation, the Reasoner consumes past analyses, queries, and retrieved information: {(ao, qo, Go),\u2026\u2026, (at\u22121, qt\u22121, G1-1)}. It then generates the next corresponding analysis at and query qt, where at involves intermediate conclusions and the next subtask to be solved, which informs and justifies qt. For example, in the 2nd round of conversation shown in Figure 2, Reasoner processes previously retrieved agent and red box room and location ({(ao, qo, G\u00f3), (a1, 91, G\u2081)), identifies that the next subtask is to find \"the path between two rooms\" (a2), and then query for the \"door IDs and attributes\" that connect two rooms (q2) for solving the subtask. In this way, each reasoning step is grounded to the environment by factoring in the retrieved information.\nThe analysis at might involve solving complex spatial sub-problems, such as navigation and object search. Past literature shows that LLMs give unreliable solutions to quantitative problems (Ahn et al., 2024). To circumvent the deficiency, we follow prior work (Schick et al., 2024; Paranjape et al., 2023) to enable code-writing and tool-use for the Reasoner. We provide programmatic functions to address atomic problems critical to the given task family. As shown in Figure 2, at the tth round of conversation, the Reasoner uses the provided pathfinding tool traverse_room to identify obstacles that need to be removed to traverse to the key, a problem beyond the capacity of LLMs. We include tool annotations in the prompt to guide the Reasoner in querying for the information necessary. The introduction of tools prevents hallucination on complex problems and reduces the burden of LLMs by leveraging known algorithms."}, {"title": "2.4. Retriever", "content": "The Retriever assists the Reasoner by processing its free-form queries and returning the requested information from the graph. Specifically, given a query q, the Retriever generates code h that executes on the scene graph to retrieve the required information G'. Here, V' and E' denote subsets of graph nodes and edges, respectively. While the Reasoner"}, {"title": "2.5. Self-debugging and Error prevention in code-writing", "content": "Even with adequate context, LLMs are not guaranteed to write executable code in a single attempt. Therefore, we introduce a self-debugging mechanism to both the Retriever and the Reasoner to ensure the successful code execution (Chen et al., 2024). Specifically, we establish an inner itera-"}, {"title": "3. Experimental Settings", "content": "We evaluate our methods on a series of numerical Q&A (NumQ&A) and planning tasks in the BabyAI (Chevalier-Boisvert et al., 2018; Chevalier-Boisvert et al., 2023) and VirtualHome (VH) (Puig et al., 2018) environments. For each environment, we provide an unified scene graph"}, {"title": "3.1. 2D Grid World Numerical Q&A", "content": "Our first experiment is on a numerical Q&A task in a customized 9-room 2D BabyAI (Chevalier-Boisvert et al., 2018) environment, as shown in Figure 3(c). We generate scene graph representation of the environment following the hierachical graph design from 3DSG (Armeni et al., 2019), illustrated in Figure 4. Specifically, the graph represents the"}, {"title": "3.2. 2D Grid World Traversal Planning", "content": "We also test on the traversal planning in BabyAI, where the task is to generate a sequence of node-centric actions to pick up a target item. We design three atomic actions, including (1) pickup (nodeID): Walk to and pickup an object by the node ID; (2) remove (nodeID): Walk to and remove an object by the node ID; (3) open(nodeID): Walk to and open a door by the node ID.\nAs shown in Figure 3(a)(b), the traversal planning task is tested in two related double-room environments, both of which require the agent to pick up the key of the correct color to unlock the door, remove any obstacle that blocks the door, open the door, and pick up the target. The difference is that the first environment, dubbed Trv1, contains only the agent-side obstacle, whereas the second environment, dubbed Trv2, contains another target-side obstacle. We generate the in-context examples only in Trv1, and test if the methods can extrapolate to Trv2. As before, we evaluate each method in 100 times in different instance of both types of the environment. For SG-RwR, we provide the reasoning function traversal_room programmed based on the A* algorithm, which identifies the item to remove in order to reach from an initial to a desired location within the same room. As we will show, SG-RWR is able to leverage this external tool to compensate for the limited mathematical problem solving ability of LLMs."}, {"title": "3.3. Household Task Planning", "content": "The last evaluation is in two VirtualHome (VH) (Puig et al., 2018) environments shown in Figure 3(d), denoted as VH-1 and VH-2, respectively. We use the built-in environmental graph as the scene graph. Compared to BabyAI, VH environments have larger state space and action space, containing 115 object instances, 8 relationship types, and multiple object properties and states. Hence, VH environments are more challenging with richer information in the graphs. For each"}, {"title": "4. Results and Analysis", "content": ""}, {"title": "4.1. Experiment Results", "content": "Numerical Q&A Resutls The results are collected in Table 1. Zero-shot SG-RwR outperforms the best baseline by 30 percentage points (pp), even without taking advantage of the few-shot examples. Few-shot methods do not show significant advantage over zero-shot methods, as the reasoning trace for this task is simple. However, they all tend to make mistakes when addressing the substeps such as counting the item or locating the neighboring rooms. The retrieval mechanisms in both ReAct and SayPlan further degrades the performance. SayPlan cannot effectively retrieve information, due to its retrieve-then-reason framework that does not condition the retrieval on intermediate reasoning. ReAct employs the iterative reasoning, but API-calling is less effective for large scene graphs. In contrast, SG-RWR retrieves information that better facilitate reasoning.\n2D Traversal Results Table 1 also reports the success rate in the traversal task. In the seen Trv1 environment, our method achieves 38pp and 3pp higher success rate against the best performing baselines under zero-shot and few-shot settings, respectively. While few-shot baselines perform more than 10pp better compared to zero-shot baselines, they perform even worse in the unseen settings, achieving < 1% success rate. This indicates that although few-shot examples is helpful in the seen tasks, LLMs do not learn the reasoning process to extrapolate to similar unseen tasks. Rather, LLMs might only memorize the heuristic mechanism in the solution, such as always removing the item on the left of the door. On the other hand, by not directly processing scene graphs, the Reasoner in SG-RWR better learns the reasoning process essential for the task, and can thus extrapolate well to similar problems. SayPlan and ReAct achieve inferior results SG-RWR, indicating that their heuristic or API-based retrieval methods are less suitable for complex tasks concerning global information.\nHousehold Task Planning Results The planning success rate on the 8 tasks in the 2 VH environments are shown in Table 2. We observe that all baselines con-"}, {"title": "4.2. Ablation", "content": "Setup To further validate the designs of SG-RWR framework, we conduct an ablation study for the key component of our method. To this end, we compare against the following variants of SG-RWR:\n\u2022 SingleCoder: Single-shot code writing with LLMs to address a given task, without iterative retrieval and reasoning or multiple generation. This variant verifies the benefit of the iterative retrieval-generation. The SingleCoder is prompted with the combination of the information for both the Reasoner and the Retriever in SG-RwR, including the environment and action space"}, {"title": "5. Related Literature", "content": "Language models for Task and Motion Planning Many existing efforts harness the power of large language models for decision making (Xi et al., 2023; Chen et al., 2023; Liu et al., 2023) and robotic control (Dalal et al., 2024; Zhang et al., 2023; Lin et al., 2023; Chen et al., 2021; Hatori et al., 2018). With rich built-in knowledge and in-context learning ability, language models are used for generating task-level plans (Raman et al., 2022; Gao et al., 2024), action selection (Ahn et al., 2022; Nasiriany et al., 2024), processing environmental or human feedback (Skreta et al., 2023), training or finetuning language-conditioned policy models (Team et al., 2024; Padalkar et al., 2023; Szot et al., 2023), and more. To factor in the environment during planning, recent studies have explored using LLMs for programmatic plan generation (Singh et al., 2023), combining knowledge from external perception tools (Liang et al., 2023; Huang et al., 2023b) or grounded decoding (Huang et al., 2023c), and value function generation (Yu et al., 2023). While proven effective, those methods are limited to small scale environments, and rely on expert perception models to extract task-related states from the scene representation with implicit spatial structure. In this work, we study using pretrained LLMs to process the the global representation of large environments with explicit structure.\nGraph as the Scene Representation The scope of the solvable task is largely determined by the state representation. Compare to sensory representation such as images or point clouds, scene graphs are compact thus scalable to large environments (Greve et al., 2024), structured to represent spatial layout explicitly (Hughes et al., 2022; Wu et al., 2021), and efficient in representing diverse states of the environment (Armeni et al., 2019). Therefore, they have been used in various manipulation or navigation tasks (Ravichandran et al., 2022; Zhu et al., 2021). In this paper, we exploit these favorable features of the scene graph representation to ground the reasoning process of LLMs to the environment.\nLLMs for Reasoning on Graph Leveraging language models to reason with graphs is a growing area. While prior works trains to integrates graph and language knowledge (Ye et al., 2023; Ni et al., 2023), recent study explores se-"}, {"title": "6. Conclusion and Future Work", "content": "In this work, we have proposed SG-RWR: an iterative, two-agent framework that grounds LLMs in a physical environment through scene graphs, and enables them to reason using both natural and, crucially, programming languages. Specifically, SG-RWR facilitates reasoning on large scene graphs by enabling LLMs to write code that retrieves task-related information during the reasoning process.\nOur ablation study shows that all the core designs, involving the iterative reason-retrieval, the two-agent cooperation, and the code-writing are crucial to the framework's enhanced performance. Iterative reason-retrieval ensures that the environment information enters the planning process in a just-in-time manner, two-agent framework reduces interference across the planning, and code-writing enables prompting with a data schema rather than the data itself. In short, all of these limit \"information overload\" in the Reasoner.\nFuture work could explore the flexibility of SG-RwR framework to seamlessly integrate additional agents with new specialties. Potential new agents involve a verifier agent to correct the solution using graph information and new modality agent to process richer information. Reasoning trace optimization could also be explored, as the conversation rounds scale with task difficulty and agent numbers."}, {"title": "Impact Statement", "content": "The paper presents a framework that enables Large Language Models to solve spatial tasks with scene graphs. Due to the wide usage of scene graphs as the environmental"}, {"title": "A. Prompt Templates for SG-RwR", "content": "SG-RWR adopts template-based prompt generation for both the Reasoner and Retriever. The templates for them are shown in Table 5 and Table 4. The prompt is generated by populating the red contents in the template with the specific graph information."}, {"title": "B. Environment Details", "content": ""}, {"title": "B.1. BabyAI Environment and Scene Graph Details", "content": "Node attributes The node attributes in BabyAI scene graph involve:\n\u2022 \"type\": String. The type of the element type. Choices:\nroot, room, agent, key, door, box, ball\n\u2022 \"color\": String. For doors and items. The color of the element.\n\u2022 \"coordinate\": List of integer. Exist for all types of nodes except for the root node. For room nodes, the top left corner coordinate. For other nodes, the 2D coordinate in the grid.\n\u2022 \"is_locked\": Binary. For door. State indicating if a door is locked or not.\n\u2022 \"size\": List of integer. For room. The size of a room."}, {"title": "B.2. VirtualHome Environment and Scene Graph Details", "content": "Node attributes The node attributes in VirtualHome involve:\n\u2022 'id': Int. Node id.\n\u2022 'category': Str. Meta category. E.g. \"Room\".\n\u2022 'class_name': Str. Specific class name. E.g. \"bathroom\".\n\u2022 'prefab_name': Str. Instance name.\n\u2022 'obj_transform': Dict. 'position': 3D vector, 'rotation': Quaternion form as 4D vector, 'scale': 3D vector\n\u2022 'bounding_box': Dict. 'center': 3D vector, \"size\": 3D vector\n\u2022 'properties': List. Object properties. Determine the action that can act upon it.\n\u2022 'states': List. Object states. Full list of available states: ['CLOSED', 'OPEN', 'ON', 'OFF', 'SITTING', 'DIRTY', 'CLEAN', 'LYING', 'PLUGGED_IN', 'PLUGGED_OUT', 'HEATED', 'WASHED']\nEdge attributes The edge attributes in VirtualHome involve:\n\u2022 'from_id': Int. Id of node in the from relationship.\n\u2022 'to_id': Int. Id of node in the to relationship.\n\u2022 'relationships': Str. Relationship between the 2 objects. Available relationships:\n'ON': Object from_id is on top of object to_id.\n'INSIDE': Object from_id is inside of object to_id.\n'BETWEEN': Used for doors. Door connects with room to_id.\n'CLOSE': Object from_id is close to object to_id (< 1.5 metres).\n'FACING': Object to_id is visible from objects from_id and distance is < 5 metres. If object1 is a sofa or a chair it should also be turned towards object2.\n'HOLDS_RH': Character from_id holds object to_id with the right hand.\n'HOLD_LH': Character from_id holds object to_id with the left hand.\n'SITTING': Character from_id is sitting in object to_id."}, {"title": "Action Space", "content": "\u2022 [walk] <class_name> (id): Walk to an object.\n\u2022 [grab] <class_name> (id): Grab an object. Requires that the agent has walked to that object first.\n\u2022 [open] <class_name> (id): Open an object. Requires that the agent has walked to that object first.\n\u2022 [close] <class_name> (id): Close an object. Requires that the agent has walked to that object first.\n\u2022 [switchon] <class_name> (id): Turn an object on. Requires that the agent has walked to that object first.\n\u2022 [switchoff] <class_name> (id): Turn an object off. Requires that the agent has walked to that object first.\n\u2022 [sit] <class_name> (id): Sit on an object. Requires that the agent has walked to that object first.\n\u2022 [putin] <class_name1> (id1) <class_name2> (id1): Put object 1 inside object 2. Requires that the agent is holding object 1 and has walked to the object 2.\n\u2022 [putback] <class_name1> (id1) <class_name2> (id1): Put object 1 on object 2. Requires that the agent is holding object 1 and has walked to the object 2."}, {"title": "C. Baseline Details", "content": ""}, {"title": "C.1. ReAct", "content": "For ReAct, we create the following graph information retrieval APIs in the list below. Each of them is a wrapper of a basic NetworkX (Hagberg et al., 2008) operation:\n\u2022 get_nodes (): Get all node IDs in the scene graph.\n\u2022 get_links () : Get all links in the scene graph.\n\u2022 get_attrs (node_id): Get the all attributes of a target node;\n\u2022 get_neighbors(node_id): get all neighbor node IDs of a target node."}, {"title": "C.2. SayPlan", "content": "SayPlan (Rana et al., 2023) is tested in BabyAI tasks. We follow the original work to create the following APIs for the room-level graph traversal purpose:\n\u2022 collapse (G) for retaining only room and root nodes;\n\u2022 expand (node_id) for revealing all nodes rooted from a given room node;"}, {"title": "D. Qualitative results", "content": ""}, {"title": "D.1. Qualitative Results in BabyAI Traversal Task", "content": "We qualitatively demonstrate how SG-RwR addresses a challenging BabyAI traversal task in Figure 5. It shows the task solving process from the Reasoner's perspective, including the information queried from the Retriever as well as the intermediate solution obtained through its own code writing. It clearly demonstrates that SG-RWR is able to ground the plan to the environment by iteratively retrieving graph information based on the task solving process and establishing the next step towards solution based on the past retrieved information."}, {"title": "D.2. Qualitative results in VirtualHome", "content": "The exemplar result in VirtualHome is shown in Figure 6. Due to the prolonged output from the Retriever, we only show the core outputs from the Reasoner-side. The result shows that the Reasoner is able to generate the correct reasoning trace solely based on the graph schema, raising corresponding queries, and use the returned information to generate the correct plan for a given task."}, {"title": "E. Results in Partially Observable Environments with Dynamic Scene Graphs.", "content": "We show that our iterative framework can naturally extend to partially observable environments with dynamically changing scene graphs. To show this, we modify the BabyAI scene graph settings, where the scene graph visible to the agent only contains visited rooms in the past and the newly revealed room by the door opening actions. After the execution of each action, the scene graph is updated to reflect the revised visibility. The environment also provides action feedback, including the execution result (successful or failed) and the agent's updated location.\nWe update the prompt of SG-RwR to allow generating individual BabyAI action to interact with the environment, as opposed to only generate the action sequence at the end without interaction in the main manuscript. Specifically, we remove the solution summary message from the Reasoner prompt, and add an additional BabyAI action command message type, as shown in Table 7. The dynamic version of SG-RwR is tested on the BabyAI traversal tasks under the zero-shot settings. The results are shown in Table. 8, together with the performance in the static settings as a reference. We do not compare with any baselines as designing the optimal strategy for the dyanmic scene graphs is not the focus of this paper. Nonetheless, even by simple updating the prompts, SG-RwR performs well in the dynamic settings, even better than that in the static settings due to the opportunity of exploring the environments and processing the feedback. This shows that our method can be adopted for dynamic scene graphs."}, {"title": "F. Analysis on the Computational Cost", "content": "We show the number of the token processed by our method by iterations and average conversation rounds required to solve a query for the BabyAI tasks in Figure 7. We also plot the token counts of the scene graph and the CoT baseline input. As a direct whole-graph prompting method, the compute required by CoT is determined by the graph size. So the processed token for NumQ&A is 4 times larger than that for the Trv-1, despite that the former is a simpler task requiring less reasoning steps.\nOn the other hand, SG-RwR processed token number monotonically increase along the iteration, as it processes the cumulative conversation history. Hence, the compute required by SG-RwR also depends on the task difficulty. However, thanks to the code-writing-based retrieval design, SG-RwR only processes limited tokens in early iterations. Thus, for simpler task such"}, {"title": "G. Exemplar Baseline Hallucinations and how SG-RwR Avoids Them via Code-writing", "content": "In Figure 8, we qualitative show how SG-RwR avoids hallucination problems happened on baselines under several scenarios from our tasks. We use the zero-shot 0-CoT and the few-shot CoT as comparison. To focus on the key difference, we only show snippets of reasoning processes for each referent subtask. We show that when reasoning in language, baselines have the tendency to hallucinate in the interpretation of the spatial layout from the scene graph structure, and in address simple quantitative reasoning (e.g. counting) tasks. On the other hand, based on the scene graph schema understanding, SG-RwR is able to solve these subtasks more reliably via code-writing."}]}