{"title": "Large Language Models for Market Research: A Data-augmentation Approach", "authors": ["Mengxin Wang", "Dennis J. Zhang", "Heng Zhang"], "abstract": "Large Language Models (LLMs) have transformed artificial intelligence by excelling in complex natural lan-\nguage processing tasks. Their ability to generate human-like text has opened new possibilities for market\nresearch, particularly in conjoint analysis, where understanding consumer preferences is essential but often\nresource-intensive. Traditional survey-based methods face limitations in scalability and cost, making LLM-\ngenerated data a promising alternative. However, while LLMs have the potential to simulate real consumer\nbehavior, recent studies highlight a significant gap between LLM-generated and human data, with biases\nintroduced when substituting between the two. In this paper, we address this gap by proposing a novel statis-\ntical data augmentation approach that efficiently integrates LLM-generated data with real data in conjoint\nanalysis. Our method leverages transfer learning principles to debias the LLM-generated data using a small\namount of human data. This results in statistically robust estimators with consistent and asymptotically\nnormal properties, in contrast to naive approaches that simply substitute human data with LLM-generated\ndata, which can exacerbate bias. We validate our framework through an empirical study on COVID-19 vac-\ncine preferences, demonstrating its superior ability to reduce estimation error and save data and costs by\n24.  9% to 79.8%. In contrast, naive approaches fail to save data due to the inherent biases in LLM-generated\ndata compared to human data. Another empirical study on sports car choices validates the robustness of our\nresults. Our findings suggest that while LLM-generated data is not a direct substitute for human responses,\nit can serve as a valuable complement when used within a robust statistical framework.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by delivering\nunprecedented capabilities in natural language processing. These models are built on advanced\ndeep learning architectures known as transformer networks, which excel at handling sequential\ndata and understanding context (Vaswani 2017, Radford 2018). Trained on extensive and diverse\ndatasets including vast amounts of text from books, articles, and websites-LLMs are capable\nof generating human-like responses and performing complex language tasks. For example, Llama\n3 was trained on 15 trillion tokens of data (HuggingFace 2024). This training enables LLMs to"}, {"title": "2. Literature", "content": "In this section, we review several key streams of literature relevant to our research.\nTechnical Development of natural language processing (NLP) and LLM. The development of\nLLMs represents a culmination of decades of advances in NLP and deep learning techniques. The\nfoundational work of Chomsky (1956) introduced formal models for language description, paving"}, {"title": "3. Model and the Data-augmentation Approach", "content": "In this section, we establish the theoretical framework. Although our approach can be extended to\nvarious market research settings, we focus on conjoint analysis as the primary use case to present\nthe theory and conduct the empirical analysis detailed in Section 5."}, {"title": "3.1. Setup", "content": "In this section, we provide a detailed explanation of the setup, starting with an in-depth dis-\ncussion of the theoretical framework underlying the data generation process. We will outline the\nassumptions, structure, and specific parameters that guide this process, building a foundation for\nunderstanding how the data is conceptualized and modeled within this framework."}, {"title": "3.1.1. Data Generation Process and the Best-in-class Estimation", "content": "Consider a setting\nwhere we observe two datasets. The first, referred to as the primary data, consists of m data\npoints, {$(x_i,y_i,z_i)_{i=1}^m$}. For a fixed positive integer k, we denote the set {1,...,k} by K and define\n$K^+ = K \\cup {0}$. These data points are independently and identically distributed (i.i.d.) according to\nthe same distribution as the random vector (x, y, z), where $y, z \\in K^+$ and $x \\in \\mathbb{R}^{dk}$. To clarify, in the\ncontext of conjoint analysis with AI-generated labels, x represents the features or context of a choice\nsetting, and y denotes the respondent's choice. Here, $K^+$ corresponds to the k+1 options available\nto the respondent, where 0 represents the outside option. Additionally, $x = (x^{(1)}, x^{(2)},...,x^{(k)})$, \nwhere $x^{(j)}$ corresponds to the features of option j for each $j\\in K$. Throughout this paper, we refer\nto the random variable y as the real label. The variable z represents a predicted label generated by\nan AI model based on x, and we refer to it as the AI-generated label. The second dataset, called the\nauxiliary data, consists of {$(x_i, z_i)_{i=1}^n$}, which are independent of the primary data but follow\nthe same distribution. However, in the auxiliary data, the true label $y_i$ is missing for each data point.\nIn Figure 2, we illustrate this theoretical setup for conjoint analysis using an example simplified\nfrom the empirical setting in Section 5.\nHow is this data generation process implemented in practice? A manager facing a conjoint study\nproblem can begin by collecting a small sample of human response data. Using the same questions"}, {"title": "3.2. Primary, Auxillary and Naive Estimators", "content": "With no access to auxiliary data such as AI-generated labels, the default approach is to fit the\nparameter $\u03b2$ using only the primary data with its real labels {$(x_i, y_i)_{i=1}^m$} using the standard\nMaximum Likelihood Estimation (MLE). We use $\u03b2^P$ to denote the estimator obtained using only\nthe primary data. When the primary data size is sufficiently large, $\u03b2^P$ can be close to $\u03b2^*$. However,\nthe size of the primary data is often restricted due to the costs of recruiting real subjects. A small\nprimary set yields a inaccurate estimator that is far away from $\u03b2^*$. On the flipping side, we may\ndirectly perform MLE on the auxiliary data {$(x_i, z_i)_{i=1}^n$}, using $z_i$ as labels. Let us define this"}, {"title": "3.3. Estimation with AI-augmented Data", "content": "These challenges lead to our research question: How do we extract value from the AI-augmented\ndata to facilitate the estimation of $\u03b2^*$? To answer this question, we propose the following data\naugmentation approach that allows us to use the AI-generated data to fit the model.\nESTIMATION WITH AI-AUGMENTED DATA:\nStep 1. Obtain an estimator $\u03b8$ to $\u03b8^*$, where $P(y=j|x, z) = g_j(x, z; \u03b8^*)$, using the primary data.\nStep 2. With the auxiliary data, we construct the estimator $\u03b2^{AAE}$ as\n$\u03b2^{AAE} = \\arg \\max_{\\beta \\in \\mathbb{R}^d} \\hat{Q}(\\theta; \\beta) =  \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j \\in K^+} g_j(x_i, z_i; \\theta) \\log \\sigma_j(x; \\beta)$\nWe refer to $\u03b2^{AAE}$ as the AI-augmented estimator (AAE). In the next section, we demonstrate\nthat our estimator $\u03b2^{AAE}$ successfully recovers $\u03b2^*$ and exhibits the desired asymptotic properties.\nThe key idea in our approach is to use the primary dataset\u2014which contains both LLM-generated\nand human-produced data to learn an efficient mapping between huamn and LLM-generated\ndata. We then leverage this mapping when combining the primary and auxiliary datasets. This\nstrategy closely resembles a transfer learning approach, where knowledge from one domain (in\nour case, the LLM-generated data) is transferred to another domain (the human-produced data).\nThe fundamental assumption underpinning such transfer learning techniques is that learning the\nmapping function between two domains, denoted by g(\u00b7) in our context, is inherently simpler and\nrequires fewer data points than carrying out the individual learning tasks within either domain on\nits own. In other words, in our context, the AI model undertakes the \u201cheavy lifting\" of capturing\nhuman choice behavior in a fuzzing way, and the first-stage in our estimation is trying to capture\nhow fuzzy this AI model is compared to the human model. Moreover, the mapping will become\neasier to learn with lower LLM costs and higher LLM quality. This is because, as the accuracy\nand the size of AI-generated labels improves, we anticipate a high-quality estimation of $\u03b8$ due to a\ndecrease in the asymptotic variance of \u03b8 estimation; see Section 4.2 for further discussion."}, {"title": "4. Theoretical Properties of Our Estimator", "content": "In this section we provide a theoretical analysis of our estimator, $\u03b2^{AAE}$. In Section 4.1, we present\nthe main theoretical results, i.e., the consistency and asymptotic normality of the estimator. We\nthen discuss the efficiency gain by using our estimator compared to only using human data."}, {"title": "4.1. Main Theoretical Results", "content": "To deliver the analysis, a set of common regularity assumptions are needed. Note that these assump-\ntions are typically assumed in asymptotic statistical analysis (Newey and McFadden 1994, Van der\nVaart 2000). The first set of assumptions are used in the analysis of the consistency of $\u03b2^{AAE}$"}, {"title": "5. Empirical Analysis I: COVID-19 Vaccination", "content": "In this section, we present empirical studies to validate our approach. The purpose of our empirical\nstudy is twofold. First, we demonstrate that even with state-of-the-art (SoTA) models, significant\nmisalignment between AI-generated data and human data in conjoint analysis persists. Second, we\ndemonstrate that our proposed approach can significantly regulate such misalignment, which in"}, {"title": "5.1. Empirical Setup", "content": "We examine the performance of the AAE based on a high-impact real choice-based conjoint dataset\nfor COVID-19 vaccines (Kreps et al. 2020). The study was conducted on July 9, 2020, where 2,000\nUS adults were recruited to take a 15-minute survey through the Lucid platform. A quota-based\nsampling was employed to approximate nationally representative samples in terms of demographic\ncharacteristics. A total of 1,971 US adults responded to the survey. This survey is important\nbecause it provides evidence of factors associated with individual preferences toward COVID-19\nvaccination. The results may help inform public health campaigns to address vaccine hesitancy.\nThe dataset consists of responses from 1,971 participants, each expressing preferences for a series\nof hypothetical vaccines. Every respondent was shown five comparisons between two hypothetical\nvaccines, described by seven attributes with multiple levels, as outlined in Table 1. Participants\nwere asked to choose one of the two vaccines or opt for neither. We excluded respondents who did\nnot select any vaccines in this setting, as many public LLMs, such as ChatGPT and Gemini, do\nnot permit opting out of vaccines due to safety requirements. Since this data was excluded from\nboth ground truth calculations and LLM data augmentation, this sample selection should not bias\nour comparisons between estimators.\nData leakage is a critical concern when selecting datasets for LLM-related empirical studies. It\noccurs when an LLM's training data overlaps with the testing data used to evaluate its performance,\nthereby compromising the validity of the evaluation. In our empirical studies, we use OpenAI's\nGPT models. Since OpenAI does not disclose its training data, we cannot confirm whether the\nconjoint dataset is part of the GPT models' training data. However, we do not consider this a sig-\nnificant issue for evaluating our approach. If data leakage were present that is, if the GPT models\nhad already seen the conjoint dataset the naive augmentation method would perform substan-\ntially better. Nevertheless, as we demonstrate later, the naive augmentation method still produces\nsignificant biases, which our method effectively corrects. This highlights the critical importance of\nour approach.\nUsing standard maximum likelihood estimation, we estimated the best-in-class parameters $\u03b2^*$,\nas shown in (3.2). For each experiment, we randomly selected 240 respondents from the training\nset, yielding a dataset of 1,200 samples. The vaccine attributes in these samples were converted to\ntext, and various versions of GPT were used to generate labels $z_i$. Details of the label generation\nprocess are provided in Section 5.2. This produced a dataset D = {$(x_i, y_i, z_i)_{i=1}^{1,200}$}."}, {"title": "5.2. Conjoint Data Generation using LLMs", "content": "In this section, we present our procedure for generating conjoint data using LLMs. We propose\na general framework for LLM-based conjoint data generation, where the input structure follows\nthree key components to produce conjoint choice data:\nLLM-Based Conjoint Data Generation Framework\n(I) Choice Task Instruction: Defining the context of the conjoint analysis, specifying the item to choose\nfrom, the task for the LLM (selecting from multiple options), and the persona the LLM should emulate."}, {"title": "5.2.1. Basic Prompting", "content": "The basic prompting follows the three-part framework: it starts\nwith an instruction specifying the choice task for GPT. In particular, we ask GPT to act like\na random person to simulate the choice from a general population. More detailed demographic\ninformation can be added in this part to simulate choices from a more specific population. In\nthe second part of the prompt, we parse the choice task options to text description. We use\nthe minimalist representation of vaccine features, which was shown to be an effective prompting\ntechnique. In the last part, we ask which option GPT would choose and require it to return the\nanswer as a single letter. An example of the basic prompt for GPT is shown below."}, {"title": "5.2.2. Chain-of-Thought Prompting", "content": "Recent advancements in prompting techniques, such\nas Chain-of-Thought (CoT) prompting, Tree-of-Thought (ToT), and others, have significantly\nenhanced reasoning in LLMs. CoT, which structures intermediate reasoning steps, has become\nthe most widely utilized due to its simplicity, interpretability, and robust performance across\ndiverse tasks (Wei et al. 2022). While methods like ToT, which explores branching reasoning paths\n(Yao et al. 2024), and other approaches can offer advantages for specific tasks, they often require\nadditional computational resources or fail to consistently outperform CoT across a wide range\nof domains. Thus, we experiment mainly with CoT in our empirical studies for its balance of\neffectiveness and broad applicability.\nWe apply this CoT prompting technique with the aim of improving the quality of the choice data\ngenerated. Specifically, we introduce conversations that guide GPT to compare the two vaccines\nfeature by feature before arriving at a final choice in part (III) of the data generation framework.\nAn example of the CoT-based conversation is shown below."}, {"title": "5.2.3. Few-shot Prompting", "content": "Few-shot prompting is a technique in natural language process-\ning where a model is guided to perform a specific task by providing a small number of examples\ndirectly in the input prompt (Brown 2020). This approach is particularly useful in zero-shot and\nlow-resource scenarios, demonstrating strong performance across a variety of tasks such as text"}, {"title": "5.2.4. Fine-tuning", "content": "Fine-tuning is another major approach in transfer learning that enhances\na model's performance on a specific task with a few examples. In this process, a pre-trained model\nis adapted to a specific task by further training it on new, task-specific data. Fine-tuned models\nhave the potential to achieve higher accuracy and better performance for the specific task. In our\nexperiments, we fine-tuned GPT-4o by training it further upon 50 samples of real data.4 Basic\nprompting was employed to generate outputs from the fine-tuned GPT-4o model. Details of the\nfine-tuning implementation can be found at Peng et al. (2024)."}, {"title": "5.3. Our Approach v.s. Other Approaches of Using LLM-generated Data", "content": "In this section, we present a detailed discussion of the empirical results comparing our estimators to\nother methods of utilizing LLM-generated data. Table 2 summarizes the total change in the MAPE\nwith the naive augmentation estimator and the AAE, using $\u03b2^P$ as the benchmark. The first column\nof the table indicates the version of the GPT model. The second column indicates the prompting\ntechnique. The rest of the columns show the change in the MAPE after augmented n = 1,000 GPT-\ngenerated samples to a real set with m samples with $\u03b2^A$, $\u03b2^{Naive}$ and $\u03b2^{AAE}$, respectively. Therefore,\na negative number indicates error reduction, while a positive number means the error becomes\nlarger after adding GPT data. Also see the discussion in Section 5.1. Below, we highlight several\nkey observations from the results."}, {"title": "5.3.1. Pure AI Estimation or Naive Augmentation Incurs More Error", "content": "Our results\nshow that purely using AI-generated data (yielding estimator $\u03b2^A$) or naively combining the data\n(yielding estimator $\u03b2^{Naive}$) can introduce significant errors. As shown in Table 2, even with the\nSoTA models and prompting techniques, $\u03b2^A$ and $\u03b2^{Naive}$ can lead to an increase in estimation errors.\nGPT-Turbo-3.5-0613 and GPT-Turbo-3.5-0125 with basic prompting show moderate error reduc-\ntion in certain experimental setups. This outcome is somewhat expected, as the LLM-generated\ndataset may coincidentally exhibit patterns similar to the human data. This aligns with recent\nfindings showing mixed evidence of LLMs' ability to generate human-like data\u2014closely resembling\nhumans in some applications while producing significant errors in others (Goli and Singh 2024).\nMoreover, the performance of pure AI estimation or naive augmentation is irregular \u2013 it does not\nimprove with more advanced GPT models or sophisticated prompts, and in many setups, it even"}, {"title": "5.3.2. AAE Regulates Error Consistently", "content": "Comparing Naive with AAE in Table 2, we\nobserve that AAE consistently outperforms Naive and significantly reduces estimation error regard-\nless of GPT models and prompts. This underscores the key advantage of our method: regularizing\nestimation error, irrespective of the model or the quality of inference techniques. More impor-\ntantly, AAE's performance improves consistently with more advanced versions of GPTs and better\nprompting strategies: It achieves the most substantial error reduction with the fine-tuned GPT-4o\nwith all data. We find that CoT, few-shot prompting, and fine-tuning all significantly reduce error,\nwith fine-tuning being the most effective.\nIndeed, more advanced GPT versions and careful prompt designs result in more informative AI-\ngenerated labels, which enhances AAE's effectiveness. This supports our discussion in Section 4.2.\nHowever, despite the improved informativeness of these labels, they do not necessarily align more\nclosely with human-generated data. As a result, the performance of $\u03b2^{Naive}$ does not necessarily\nimprove. This highlights the potential of AAE to achieve better outcomes with future iterations\nof LLMs or other forms of AI data generators. As a result, AAE not only regulates estimation\nerror in AI-generated data but also has the potential to resolve the wild goose chase by providing\na structured direction for improving estimation results.\nWe conduct statistical tests on the results in Table 2. Specifically, pairwise t-tests show that $\u03b2^{AAE}$\noutperforms $\u03b2^P$, $\u03b2^A$, and $\u03b2^{Naive}$ at the 99% significance level for all instances, except for the few\ncases marked with a star. In addition to the average comparison in Table 2, Figure 3 illustrates the\ndiscrepancy between Naive and AAE for each feature. The figure compares the estimated parameter\nvalues for the eleven features between Naive, AAE, and the ground truth parameters, using m = 100\nprimary data points and n = 1,000 auxiliary data points. As shown, naive augmentation results\nin significantly larger errors, especially for the most influential vaccine features. In contrast, AAE\nproduces estimates that align more closely with the ground truth."}, {"title": "5.4. Our Approach v.s. Human-only Data  Financial Value of Our Approach", "content": "So far, we have focused on evaluating our method based on estimation error against other ways of\nutilizing AI data. In practice, the primary goal of using AI as a data augmenter in market research\nis to reduce the costs of hiring real survey participants. Thus, we evaluated the percentage of data\nsaved using AAE with various GPT models, as summarized in Table 3. Specifically, for a given\nerror reduction achieved by applying AAE to a real dataset of size $n_1$, we calculated the amount\nof real data samples, $n_2$, required to achieve the same error reduction without AI augmentation.\nThe percentage of data saved is then estimated as (($n_2$ - $n_1$)/$n_2$) \u00d7 100%.\nOur results show that with fine-tuned GPT-4o, data savings range from 24.9% to 79.8%. When the\nprimary set is small (m = 50), AAE saves between 71.64% and 79.8% of data samples, regardless of\nthe GPT model or prompt design. With a moderate primary set size (m = 100, 150), savings range\nfrom 27.7% to 58.0%, again consistent across GPT models and prompts. Compared to the costs\nof recruiting real survey participants, the costs of generating AI-based data are negligible and will\ncontinue to decrease as generative AI technology advances. In practice, conjoint surveys must be\nregularly re-administered across different product categories and customer segments to account for\nevolving consumer preferences. This can result in a substantial number of surveys being required\nover time. Thus, based on our analysis, we conclude that AAE offers significant cost savings in the\nlong run.\nFurthermore, we calculate the efficient frontier of estimation accuracy and recruitment costs\nusing our methods compared to human-only data. For this analysis, we assume that recruiting\neach human subject costs 50 USD, while querying the LLM incurs no cost. Fig. 4 illustrates how\nAAE extends the efficient frontier for conjoint market research. The yellow line represents the"}, {"title": "6. Empirical Analysis II: Sports Car", "content": "In this section, we present an alternative empirical study on a real choice-based conjoint dataset\nfor sports cars (Spencer 2019) as a robustness check for the results in Section 5. The empirical\nsetup is outlined below."}, {"title": "6.1. Empirical Setup", "content": "This dataset consists of responses from 200 participants, each expressing preferences for a series of\nhypothetical sports cars. Every respondent was shown ten sets of three sports cars, described by\nfive attributes with multiple levels, as outlined in Table 4. Participants were asked to choose one\nof the three cars. We randomly selected 120 respondents from the training set for each experiment,\nresulting in a dataset of 1,200 samples. The car attributes in these samples were converted to text\nand used to generate GPT datasets, following the same procedure in Section 5.2. The remainder\nof the experimental setup is consistent with the approach described in Section 5.1."}, {"title": "6.2. Conjoint Data Generation using GPT", "content": "In this section, we illustrate the conjoint data generation procedure with the sports car conjoint\nsetting, in particular, to show how the previously established framework can be applied in other\nconjoint settings. The models and prompting follow the same setup as in the previous section."}, {"title": "6.2.1. Basic Prompting", "content": "In this part, we present an example of basic prompting following\nthe three-step framework for the sports car data:\nInput (I): You should act like a random person purchasing a sports car.\nInput (II): There are three options for you:\nA: Type of car model: Basic. Number of seats: 2. Type of transmission: Manual. Convertible roofs: Yes. Price:\n$30k\nB: Type of car model: Basic. Number of seats: 4. Type of transmission: Manual. Convertible roofs: Yes. Price:\n$40k\nC: Type of car model: Basic. Number of seats: 4. Type of transmission: Auto. Convertible roofs: Yes. Price: $35k\nInput (III): Which option would you choose as a random person? Your response should be a single letter A, B,\nor C.\nOutput: B"}, {"title": "6.2.2. CoT Prompting", "content": "In CoT prompting, we allow GPT to compare the features of cars\nsystematically before reaching a final decision in part (III). An example of the CoT-based conver-\nsation is provided below. As illustrated, GPT demonstrates more personal insight when selecting\nsports cars compared to the vaccine context, with less clearly defined preference rankings involved\nin comparing each feature."}, {"title": "6.3. Empirical Results", "content": "Table 5 summarizes the change in MAPE of $\u03b2^A$, $\u03b2^{Naive}$, and $\u03b2^{AAE}$ using $\u03b2^P$ as the benchmark. Similar\nto the findings in Section 5, AAE significantly reduces MAPE across all scenarios, achieving in\ngeneral the best performance using GPT-4 with CoT and GPT-4o with few-shot prompting, while the\nnaive method tends to elicit errors. Interestingly, AAE's performance shows less correlation with\nhigher versions of GPT and CoT in comparison to the vaccine setting, which is expected given that\nsports car preferences are highly subjective. Despite being an older version, GPT-3.5-Turbo-0613\nperforms well with AAE in this context. One can notice that GPT-3.5-Turbo-0613 incurs a small\nestimation error with naive augmentation, suggesting it may happen to have a close alignment with\nreal human data, which in turn enhances AAE's effectiveness. On the other hand, Naive does not\nexhibit a consistent correlation with higher versions of GPT and performs significantly worse after\nimplementing CoT. This type of result is common in choice data heavily influenced by personal\ntastes rather than rational decision-making, as is the case with sports car selection. Moreover, fine-\ntuning GPT-4o does not improve the performance of naive augmentation. This underscores that\nfine-tuning is not a universal solution to all problems."}, {"title": "7. Conclusion", "content": "This paper presents a new approach for incorporating LLM-generated data into conjoint analysis,\naddressing the growing need for scalable and cost-effective methods in market research. While\nLLMs can mimic human-like responses, our study underscores the persistent errors and limitations\ninherent in directly using these AI-generated responses. Through our proposed data augmentation\nframework, which combines LLM-generated labels with real data, we demonstrate that it is possible\nto extract valuable insights from AI-generated data while mitigating errors that can distort market\nresearch outcomes. Our theoretical framework, inspired by knowledge distillation and transfer\nlearning, establishes a method to transfer the valuable but imperfect knowledge embedded in\nLLMs into a simpler, aligned model. This approach is validated empirically, where we show that\nour estimator not only reduces errors but also achieves significant data savings. Importantly, our\nfindings highlight that while state-of-the-art LLMs, such as GPT-4, can improve the quality of\nAI-generated labels, their usefulness ultimately depends on how well they are integrated with real\ndata.\nLooking ahead, our method opens the door for more effective use of LLMs in market research\nand beyond. As LLM technology advances, this framework can serve as a foundation for further\ninnovations in data augmentation, helping researchers and practitioners balance the trade-offs\nbetween AI-generated and human-generated data. Future work may explore how this approach\ncan be extended to other domains where real data is scarce, unlocking new applications for AI\nin understanding human preferences and behaviors. Future work could explore additional ways\nto enhance the usability of LLMs in combination with our methods. For instance, a promising\nresearch direction is examining the impact of incorporating different personas into LLM queries to\ngenerate data that more closely resembles a specific set of users. Finally, while our paper focuses\non conjoint analysis, the underlying theory applies more broadly to other knowledge distillation"}]}