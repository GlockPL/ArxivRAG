{"title": "MPLUG-OWL3: Towards Long ImAGE-SEQUENCE Un- DERSTANDING IN MULTI-MODAL LARGE LANGUAGE MOD- ELS", "authors": ["Jiabo Ye", "Haiyang Xu", "Haowei Liu", "Anwen Hu", "Ming Yan", "Qi Qian", "Ji Zhang", "Fei Huang", "Jingren Zhou"], "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable capabilities in executing instructions for a variety of single-image tasks. Despite this progress, significant challenges remain in modeling long image sequences. In this work, we introduce the versatile multi-modal large language model, mPLUG- Owl3, which enhances the capability for long image-sequence understanding in scenarios that incorporate retrieved image-text knowledge, interleaved image-text, and lengthy videos. Specifically, we propose novel hyper attention blocks to ef- ficiently integrate vision and language into a common language-guided semantic space, thereby facilitating the processing of extended multi-image scenarios. Ex- tensive experimental results suggest that mPLUG-Owl3 achieves state-of-the-art performance among models with a similar size on single-image, multi-image, and video benchmarks. Moreover, we propose a challenging long visual sequence evaluation named Distractor Resistance to assess the ability of models to maintain focus amidst distractions. Finally, with the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance on ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to the development of more efficient and powerful multimodal large language models.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, Multimodal Large Languages Models (MLLMs) (Liu et al., 2023a; Ye et al., 2023b; Liu et al., 2024a; Ye et al., 2024; Chen et al., 2024d) have achieved rapid advancements, demonstrating strong single-image understanding capabilities. The current approaches primarily rely on vast amounts of image and text data to align Large Language Models (LLMs) (Zheng et al., 2023; Touvron et al., 2023a;b) with visual encoders, thereby extending multimodal capabilities.\nMore advanced image-sequence understanding capabilities are required in practical applications, such as Multi-Image Reasoning (Suhr et al., 2018; Lu et al., 2021; Jiang et al., 2024), Multimodal RAG (Chen et al., 2022; Lin et al., 2024), Video Understanding (Xiao et al., 2021; Li et al., 2023c; Fu et al., 2024a; Wu et al., 2024), Multi-modal Agents (Wang et al., 2024a; Zhang et al., 2024a), and Multi-Doc QA (Tito et al., 2023; Van Landeghem et al., 2023). The existing methods are primarily based on interleaved image-text web data for pre-training (Lauren\u00e7on et al., 2023; Lauren\u00e7on et al., 2024) to extend multi-image capabilities or focused on the in-context abilities (Alayrac et al., 2022; Awadalla et al., 2023; Zhao et al., 2023) within multi-image scenarios. However, these methods have not explored the in-depth comprehension or the efficiency of multi-images sufficiently, which makes it hard to support long image sequences.\nFor example, LLAVA-Next-Interleave (Li et al., 2024a) and Mantis (Jiang et al., 2024) directly insert visual features into textual sequences. As shown in Figure 1, the inference latency and memory usage is dramatically increase. Flamingo (Alayrac et al., 2022) simply uses a Perceiver and cross-attention layers to reduce computational overhead. This results in the loss of visual fine-grained information and leads to poor performance in both single and multi-image scenarios.\nTo address this challenge, we introduce mPLUG-Owl3, a new general-purpose multi-modal foun- dation model. mPLUG-Owl3 is designed to handle long image sequences both effectively and efficiently. mPLUG-Owl3 integrates innovative hyper attention blocks in the language model to achieve efficient interleaved vision-language semantic alignment. Specifically, Hyper Attention in- troduces cross-attention parallel to the self-attention in the transformer block. The language query is reused to select and extract visual features from a lengthy visual sequence, allowing for adap- tively obtaining complementary visual information that the language model lacks, based on textual semantics.\nWe evaluate mPLUG-Owl3 with a total of twenty benchmarks, which include single-image, multi- image, and video. Specifically, experiments encompass five visual question answering tasks, five multimodal large language model tasks, four video tasks, and six multi-image tasks. Among models of the same size, mPLUG-Owl3 achieves state-of-the-art results in 14 out of 20 benchmarks. Besides existing benchmarks, we also propose a challenging long visual sequence evaluation named Distractor Resistance. It is designed to assess the ability of models to maintain focus amidst distractions."}, {"title": "2 MPLUG-OWL3", "content": "As illustrated in Figure 2, mPLUG-Owl3 comprises a visual encoder, a linear projection layer, and a decoder-only language model. This architecture is commonly employed in recently proposed Multi-modal Large Language Models. Unless specified otherwise, we use Siglip-400m (Zhai et al., 2023) as the visual encoder and Qwen2 (Yang et al., 2024) as the language model. First, we provide detailed information about our efficient architecture and its handling of various lengths of visual inputs in Section 2.1. Additionally, we introduce the Hyper Attention module in Section 2.2. It is a lightweight extension designed to enhance the transformer blocks of the language model by enabling cross-attention capabilities for adaptive visual sequence utilization."}, {"title": "2.1 CROSS-ATTENTION BASED ARCHITECTURE", "content": "Popular MLLMs (e.g., LLAVA-Interleave (Li et al., 2024a), InternVL (Chen et al., 2024d)) insert visual features into the sequence of embeddings, which can easily exhaust the language model's context window, resulting in significant memory and computational overhead. This kind of disad- vantage hinders these MLLMs to modeling the long vision input such as multiple images, videos and multiple pieces high-resolution images. Moreover, visual details can be lost, when going through the language model.\nTherefore, mPLUG-Owl3 consider use cross-attention for feeding the visual information into the language model. Specifically, given a interleaved multimodal input $S = [T_1, I_1, T_2, I_2, T_3]$ (the format can be adapted to various text-image organizational structures), mPLUG-Owl3 first extract visual features of the input images and use a linear projection to align the dimensions of visual features to be the same of the language model. The projected visual features are denoted by $H_{img} = [I_1', I_2'] \\in [R^{L \\times D_t}$. The text sequence are $S_{text} = [T_1, T_{img}, T_2, T_{img}, T_3] \\in R^{L \\times D_t}$, where $T_{img}$ is a plain text <|image|> to indicate the original place of the image. We feed the sequence into the word embedding to obtain text feature $H_{text}$.\nIn the language model, we fuse the visual features $H_{img}$ into the text features $H_{ext} \\in R^{L \\times D_t}$ of the $i^{th}$ layer through cross-attention operator. Different from Flamingo (Alayrac et al., 2022) and EVLM (Chen et al., 2024b) that insert an additional layer into each layer of transformer layer, we sparsely extend a small number of transformer blocks in the network to perform cross attention parallel with self-attention. We name the Hyper Attention Transformer Block (HATB). We discuss the design of HATB in detail in Section 2.2. HABT can significantly reduces the number of additional training parameters and facilitates model convergence. Besides, we observe that having fewer HATBs does not degrade the model's performance; instead, it offers the advantages of low memory consumption and high inference efficiency during inference. For a language model consisting of $N$ layers, we start from layer 0 and uniformly extend $K$ layers to HATB. Specifically, for Qwen2, we select layers [0, 9, 17, 25]."}, {"title": "2.2 HYPER ATTENTION TRANSFORMER BLOCK", "content": "In this section, we specifically introduce the Hyper Attention Transformer Block used in mPLUG- Owl3. The cross-attention structure employed in Flamingo, as shown in Figure 3 (a), has been widely utilized in constructing MLLMs (e.g., IDEFICS (Lauren\u00e7on et al., 2023), EVLM (Chen et al., 2024b)). However, this structure presents three main drawbacks: it introduces a large number of additional parameters, which results in significant memory and computational overhead; the knowledge learned by the language model cannot benefit the understanding of visual inputs; the cross attention does not fully take into account the original positions of images in the interleaved sequence, which limits the performance of these models in multi-image scenarios. In response to these issues, we propose a lightweight Hyper Attention Transformer Block, illustrated in Figure 3 (b). This block introduces a small number of parameters and extends self-attention capabilities to perform both intra-text self-attention and inter-modal cross-attention between text and images"}, {"title": "Shared Input Layernorm", "content": "The visual feature $H_{img}$ and the $i^{th}$ layer's text features $H_{ext}$, although sharing the same dimensionality, originate from different distributions. Hence, both sets of features are initially normalized using a LayerNorm module. Our findings indicate that employing the LayerNorm module already integrated within the transformer block results in better convergence compared to training a separate layer normalization module specifically for the visual features. This improvement is attributed to the compatibility of the mean and variance of the outputs from the integrated LayerNorm module with the distribution characteristics of the pre-trained language model."}, {"title": "Modality-Specific Key-Value Projection", "content": "In cross-attention, the Query is derived from textual data, while the Key and Value are extracted from visual features. Inspired by Ye et al. (2024), we construct a weight matrix $W_{K&V} \\in R^{2D \\times D}$ to generate the Key and Value for the visual features. This matrix is initialized using the weights from the language model's KV (Key-Value) projection. Furthermore, the query vector from the self-attention mechanism is repurposed as the Query in the"}, {"title": "Visual Position Modeling in Attention", "content": "For models that process multiple images, positional en- coding is essential to correctly understanding interleaved image-text input. Existing cross-attention models, such as Flamingo (Alayrac et al., 2022) and IDEFICS (Lauren\u00e7on et al., 2023), do not assign position embeddings to visual inputs, leading to suboptimal performance in scenarios involving mul- tiple images. To accurately represent the original positions of images in interleaved sequences, we develope a Multimodal-Interleaved Rotary Position Embedding, which we name MI-Rope. Specifi- cally, for each visual feature $I_n$ of image $n$, we pre-record the position index of its placeholder $T_{img}$ in the interleaved sequence $S_{text}$. All patches of $I_n$ share $T_n$'s positional encoding to obtain the rotary embedding. This ensures that the positional encoding of the image not only reflects the order among images but also reveals its position in the textual context. We also use a causal attention mask in cross attention. As shown in Figure 3 (c), for a text sequence $S = [T_1, T_{img}, T_2, T_{img}, T_3]$, each text token can only attend the visual features that precede it. Then, HATB simultaneously performs cross-attention and self-attention, denoting the resulting hidden states as $\\hat{H_i}$ and $\\hat{H}$."}, {"title": "Adaptive Gating", "content": "Existing implementations of cross-attention utilize a learnable scale to regulate the extent of information transfer from the image to the language model. However, the semantics of language are ignored. Consequently, we introduce an adaptive gate that obtains the gate value based on the textual features:\n$g = Sigmoid(\\hat{H}^i)$\n$\\mathbb{H}_{rused} = \\mathbb{H}_{text} * g + \\hat{\\mathbb{H}_{text}} * (1 - g)$\nThe $\\mathbb{H}_{rused}$ is passed to the FFN and fed to the next layer of transformer."}, {"title": "3 IMPLEMENT DETAILS", "content": "We adopt a three-stage training approach for mPLUG-Owl3. Initially, we pre-train mPLUG-Owl3 using image-text pairs to achieve robust multimodal alignment. In the second stage, we leverage diverse datasets that include image and video captions to enhance the model's ability to understand"}, {"title": "3.1 TRAINING PARADIGM", "content": "We collect image-text pairs from public datasets, including Conceptual Captions (CC3M/CC12M) (Changpinyo et al., 2021), COCO (Lin et al., 2014), Laion (Schuhmann et al., 2022), COYO (Byeon et al., 2022), DataComp (Gadre et al., 2023), Wukong (Gu et al., 2022), ImageNet (Deng et al., 2009), OCR-CC (Yang et al., 2021) and SBU (Ordonez et al., 2011). We randomly sample a subset consists of 41 million image-text pairs for pre-training. During pre-training, only the newly introduced modules are trainable, which include the linear layer following the vision encoder, the visual KV projection, and the Adaptive Gate in the Hyper Attention Transformer Block."}, {"title": "3.1.2 MULTI-IMAGE PRE-TRAINING", "content": "In the multi-image pre-training stage, we collected three types of data to enhance the model's multi-image understanding capabilities:\n\u2022 Interleaved data: We utilize sources such as MMDU (Liu et al., 2024d) and LLaVA- Interleave (Li et al., 2024a) for multi-image data. Additionally, from LLaVA-Recap 558K, we randomly sample 3 to 6 images and combine their image-caption pairs into an interleaved format to create Interleaved Captions. We also consider sampling 4 images and requiring a description of one among them to form Selective Captions.\n\u2022 Text-rich data: We use text reading and key point generation data proposed by UReader (Ye et al., 2023a), enabling the model to reconstruct the text contained within text-rich images"}, {"title": "3.1.3 SUPERVISED-FINETUNING", "content": "In Supervised-Finetuning stage, mPLUG-Owl3 is trained with an extensive and diverse assembly of instruction tuning datasets aimed at enhancing its instruction-following capability. The datasets include LLaVA-SFT-665K (Liu et al., 2024a), The Cauldron (Lauren\u00e7on et al., 2024), Mantis (Jiang et al., 2024), LLaVA-Interleave (Li et al., 2024a), ALLaVA (Chen et al., 2024a), ShareGPTVideo-QA 240K (Zhang et al., 2024c), Video Instruct 100K (Maaz et al., 2023), MSR-VTT (Xu et al., 2016) and MSVD Caption (Chen & Dolan, 2011). We keep the same training setting as the Multi-image Pre-training stage."}, {"title": "3.2 HIGH-RESOLUTION IMAGE PROCESSING", "content": "Inspired by UReader (Ye et al., 2023a), we introduce a similar adaptive method for image cropping. For a given image, we select from the cropping grids (2,2), (1,3), (1,4), (3,1), (4,1), (2,3), and (3,2) that most closely matches the shape of the input image. Additionally, we retain a global version of the original image. During the Supervised-Finetuning stage, for datasets rich in text, we perform cropping with a probability of 100%. For datasets containing a single image without text, we apply cropping with a probability of 20%. For datasets containing multiple images or videos, we do not perform cropping. During evaluation, cropping is enabled only for single-image tasks."}, {"title": "3.3 VIDEO PROCESSING", "content": "For videos, we sample 8 frames per video by default. Meanwhile, we replace the video markers in the text with multiple <|image|> placeholders corresponding to the number of sampled frames."}, {"title": "4 EXPERIMENTS", "content": "We conduct experiments on a diverse set of visual question answering benchmarks, including VQAv2 (Goyal et al., 2016), OK-VQA (Marino et al., 2019), GQA (Hudson & Manning, 2019), VizWizQA (Bigham et al., 2010), and TextVQA (Singh et al., 2019). The VQAv2 dataset is currently the largest visual question answering dataset available. OK-VQA involves questions that require external knowledge beyond multimodal inputs. GQA is designed to validate the model's reasoning capabilities. VizWizQA is constructed from question-answer pairs sourced from visually impaired users. TextVQA focuses more on evaluating the model's ability to understand text in natural scenes. These datasets are strategically selected to thoroughly evaluate our model's ability to understand and reason across various visual contexts and knowledge domains."}, {"title": "4.1 VISUAL QUESTION ANSWERING BENCHMARKS", "content": "mPLUG-Owl3 outperforms 8B-level language models in VQAv2, OK-VQA, GQA, and VizWizQA. Furthermore, it surpasses the 32B-parameter EVLM3 in GQA and VizWizQA. In TextVQA, al- though mPLUG-Owl3's performance is slightly lower than that of Idefics2, it still exceeds that of other 8B models. It is noteworthy that, despite having 8B parameters, mPLUG-Owl3 exhibits su- perior inference speed and memory efficiency compared to models of the same scale, thanks to the introduction of Hyper Attention."}, {"title": "4.2 GENERAL MLLM BENCHMARKS", "content": "We evaluate mPLUG-Owl3 on various single-image general multimodal large language model bench- marks including MMBench-EN/CN (Liu et al., 2023b), MM-Vet (Yu et al., 2023), POPE (Li et al., 2023d) and AI2D (Kembhavi et al., 2016). MMBench provides a comprehensive evaluation of a model's multimodal capabilities in both Chinese and English contexts. MM-Vet assesses the mul- timodal conversational abilities of a model using GPT-4 evaluation. POPE can evaluate the extent of multimodal hallucinations in a model. AI2D assesses a model's ability to understand science diagrams inputs."}, {"title": "4.3 MULTI-IMAGE AND VIDEO BENCHMARK", "content": "We also evaluate the performance of mPLUG-Owl3 on video and multi-image benchmarks, as it is capable of processing multiple images with an interleaved format. we include VideoChat2 (Li et al., 2023c), Video-LLaMA2 (Cheng et al., 2024), Video-ChatGPT (Maaz et al., 2023), ShareGPT4Video (Chen et al., 2024c), PLLaVA (Xu et al., 2024), Idefics2 (Laurenccon et al., 2024), Mantis-SigLIP (Jiang et al., 2024) and LLAVA-Interleave (Li et al., 2024a).\nThe results of video evaluation is shown in Table 5. The NextQA (Xiao et al., 2021) and MVBench (Li et al., 2023c) are short video benchmarks, with video durations all less than one\n3EVLM does not provide the number of parameters for its cross module. The parameter count in this table is estimated based on its model architecture."}, {"title": "4.4 ABLATION STUDIES", "content": "We adopt the training methods of LLaVA-1.5 (Liu et al., 2024a) using the same datasets to conduct our ablation study. Additionally, we employ the Qwen1.5 7B as our language model. To validate the single-image understanding capabilities of our structures, we use datasets such as GQA and TextVQA (with OCR). Furthermore, we examine the generalization capabilities of our structures in multi-image understanding and video comprehension by conducting zero-shot evaluations on benchmarks including MvBench, VideoMME, NLVR2, and Mantis-Eval."}, {"title": "4.4.1 CROSS ATTENTION INTEGRATION", "content": "There are two primary methods to integrate Cross-Attention into the transformer block: one method positions it prior to the self-attention (referred to as Pre-Cross-Attention), while the other places it subsequent to the self-attention (referred to as Post-Cross-Attention). We analyze both configurations and compare them to the concatenate-based method and our novel Hyper Attention in mPLUG-Owl3. Specifically, for Pre-Cross-Attention, it is positioned before the layer normalization at the input stage of the Transformer block. Conversely, for Post-Cross-Attention, it is positioned after the layer normalization that follows the self-attention stage. Both attention mechanisms employ a gating mechanism to fuse the multimodal representations effectively.\nTable 8 shows that the concatenate-based model which directly embeds image features into the input sequence of the language model, has the best performance in single-image understanding. On the other hand, utilizing Post-Cross-Attention results in the worst performance. Comparatively, Pre- Cross-Attention performs better but still incurs some performance loss. Hyper Attention, however, achieves comparable performance with concatenate-based model.\nIn evaluations involving videos and multiple images, we observe that the concatenate-based model may not follow textual instructions as accurately, leading to a significant performance degradation in multi-image scenarios. This is attributed to the inadequate training of inter-image attention, which"}, {"title": "4.4.2 DESIGN OF HYPER ATTENTION", "content": "To further investigate the impact of the structural design of Hyper Attention on model performance, we start with a basic hyper attention model and gradually introduce adaptive gating, shared layer- norm, and MI-Rope. The Table 10 shows that, when incorporate adaptive gating, the single-image"}, {"title": "4.5 DISTRACTOR RESISTANCE IN LONG VISUAL CONTEXTS", "content": "Recent works adopt the multimodal needle in a haystack (Wang et al., 2024b) approach to evaluate the understanding of long sequences. However, we notice that multimodal models, when understanding multiple images, are susceptible to interference from surrounding images, leading to visual illusions. The multimodal needle in a haystack evaluation cannot detect such errors. Therefore, we develop a challenge evaluation method to assess the distractor resistance of multimodal models in long visual contexts.\nSpecifically, we take samples from the MMBench dev set. For each test sample, we randomly select $N-1$ images from the original MMBench dev set as distractor and construct the model input in the format of Image 1: <|image|> Image 2: <|image|> ... Image N: <|image|>. In Image X, {question}, where N = 1,5, 10, 20, 50, 100, 200, 400 and X denotes the index of the image corresponding to the question. We use the CircularEval to measure the accuracy scores. For each question, we construct test samples with different orders of options and varying distractor images. The model needs to answer all test samples for a given question correctly for it to be counted as correct. Consequently, as the number of distractor images increases, the evaluation becomes significantly more challenging.\nWe compare mPLUG-Owl3 with LLaVA-Next-Interleave 7B (Li et al., 2024a), Mantis-Idefics2 (Jiang et al., 2024), Qwen-VL (Bai et al., 2023) and mPLUG-Owl2 (Ye et al., 2024). LLaVA-Interleave-7B can handle approximately 20 images given 80GB of VRAM. By utilizing model parallelism, we extend its capacity for images to 50 images. However, LLaVA-Next-Interleave is unable to handle settings with more images. Mantis-Idefics2 can handle up to 100 images but costs 9 hours to finish the evaluation.\nThe results are shown in Figure 4. It can be observed that the introduction of distractor images results in a certain degree of performance loss for all the models. When the number of images reaches 20 and 50, the performance of LLaVA-Next-Interleave dramatically drops to 43.18% and 12.52%, respectively. We observe that when the number of images reaches 50, LLaVA struggles to consistently answer the questions accurately when different distractor images are present, resulting in a low accuracy rate. And when the number of images reaches 100, Mantis-Idefics2 fails to solve most of the problems correctly. In contrast, mPLUG-Owl3 only drops to a performance level of 43.09% when processing 50 images. As the number of images increases to 400, the performance of mPLUG-Owl3 decreases to 28.58%. Since our multi-image training data consists of only about 6-8 images, this also presents a challenge for our model. Nonetheless, mPLUG-Owl3 can serve as a baseline for future research."}, {"title": "4.6 QUALITATIVE RESULTS", "content": "mPLUG-Owl3 can handle various number of images and videos as inputs. In this section, we further investigate the ability of mPLUG-Owl3 in real-world dialogue scenarios."}, {"title": "4.6.1 MULTI-IMAGE UNDERSTANDING", "content": "mPLUG-Owl3 demonstrate state-of-the-art performance on multi-image understanding benchmarks. In this section, we present multi-image dialogue examples in real-world. In the first example shown in Figure 5, it can be observed that mPLUG-Owl3 can activate the knowledge it learned based on the content of the images and perform cross-image reasoning. The second example demonstrates that the model can accurately distinguish the content of multiple images and respond appropriately based on cultural knowledge.\nFigure 6 shows a multi-turn dialogue example. mPLUG-Owl3 can find the differences between two images in various views. Besides, it can describe the correlations between images."}, {"title": "4.6.2 VIDEO UNDERSTANDING", "content": "We showcase the video understanding capabilities of mPLUG-Owl3. First, we compare it with LLaVA-Next-Interleave in Short Video Question Answering, Long Video Fine-grained Question Answering, and Long Video Comprehensive Understanding. For LLaVA-Next-Interleave, we input 8 frames, while for mPLUG-Owl3, we input 128 frames, which are the maximum numbers of images that can be accommodated by the two models on a V100-32G. The samples are shown in Figure 7.\nIn the short video tests, both LLaVA and mPLUG-Owl3 can provide correct answers. mPLUG-Owl3 tends to describe the attributes of objects based on the actual content seen. In long video lasting more than 40 minutes, when we ask about a specific detail, LLaVA fails to handle the long sequence and loses fine-grained information, rendering it unable to provide accurate information. On the other hand, mPLUG-Owl3 accurately captures key segment information within a long video. Additional, we have both models summarize the content of a longer video. mPLUG-Owl3's response is very detailed, not only providing an overall summary but also introducing the process in order. LLaVA- Next-Interleave's response, however, is more general and lacks detail. The comparative results indicate that mPLUG-Owl3 not only efficiently encodes long visual sequences but also captures and effectively utilizes both global and local information.\nWe also test mPLUG-Owl3 in multiple rounds using a long video that featuring many scenes. For clarity, we place the relevant segments beside the dialogue in the figure. During the test, we input only the complete video to the model. The dialogue is shown in Figure 8.\nFirst, we ask a question with a temporal constraint, and mPLUG-Owl3 accurately understands the concept of \"at first\" and correctly describes the detail of \"sitting in a room and discussing something on their laptops.\" However, the response incorrectly counts the number of people. The segment has only two people. We find that the model is confused by a later scene involving more people. We also notice that the visual content of this segment does not involve Australia as a destination, but the model can infer this from some diagrams later in the video, which makes the response more detailed.\nThen, we ask about the camera brand in a frame that briefly appears, and mPLUG-Owl3 accurately"}, {"title": "5 RELATED WORK", "content": "With the development of large language models (LLMs), researchers are exploring the integration of vision and other modalities into LLMs. These multimodal large language models (MLLMs) can perceive visual contents, conduct visual reasoning, and engage in multimodal dialogue with humans.\nBased on the way visual features are integrated into language models, MLLMs can be divided into three categories:\n\u2022 Models like LLaVA (Liu et al., 2023a) and CogVLM (Wang et al., 2023) use an MLP to map visual features into the representation space of the language model, and directly concatenate them with the text sequence. DeepSeek-VL (Lu et al., 2024) employs multiple visual encoders to obtain richer visual representations. While these methods can preserve fine-grained visual information, they consume a large number of tokens which slows down both training and inference.\n\u2022 To reduce the number of tokens, Mini-GPT4 (Zhu et al., 2023), mPLUG-Owl (Ye et al., 2023b), and Qwen-VL (Bai et al., 2023) adopt a structure similar to Q-Former (Li et al., 2023a), compressing the token count to a fixed size through learnable queries and cross-attention with visual features. InternLM-XComposer (Zhang et al., 2023) and IDEFICS2 (Lauren\u00e7on et al., 2024) also use the similar method. Models like In- ternVL (Chen et al., 2024d) and InternLM-XComposer-2.5 (Zhang et al., 2024b) use patch merge to compress visual tokens by several times. MiniGemini (Li et al., 2024b) uses a low-resolution visual representation as a query to compress and aggregate high-resolution"}, {"title": "5.1 MULTIMODAL LARGE LANGUAGE MODELS", "content": "\u2022 Flamingo (Alayrac et al., 2022) first proposed embedding cross-attention layers into the language model, integrating visual features into the intermediate representations of the language model. IDEFICS (Lauren\u00e7on et al., 2023) and EVLM (Chen et al., 2024b) have also trained MLLMs based on this structure. This method avoids occupying the"}, {"title": "5.2 MULTIMODAL MODELS WITH INTERLEAVED SUPPORT", "content": "Early-stage models, trained exclusively on single-image inputs, exhibit limitations in image-text interleaved scenario. Recent research are expanding the capabilities of multimodal models to process multiple images inputs.\n\u2022 Video is a special form of multi-image existence, and MLLMs related to video understanding treat frames as multiple images with temporal correlation as input. VideoChat2 (Li et al., 2023b) propose a Global Multi-Head Relation Aggregator to perform temporal message passing and use a Q-former to adapt the feature of video frames into language model. VideoLLaMA2 (Cheng et al., 2024) not only reads images but also expands the model's audio comprehension capabilities, ensuring that the information in the video is fully utilized. ShareGPT4Video (Chen et al., 2024c) propose to improve the video understanding by introducing GPT-4 annotated video caption as pretrain data.\n\u2022 In general multimodal dialogue, the model needs to have a more general multi-image understanding capability, including in-context learning, cross image reference, comparison, and reasoning. Flamingo (Alayrac et al., 2022) demonstrates limited in-context learning capabilities, while Idefics2 (Lauren\u00e7on et al., 2024) has acquired a broader multi-image understanding ability through multi-image training data. Mantis (Jiang et al., 2024) and LLAVA-Interleave (Li et al., 2024a) further enhance the model's multi-image understanding capabilities by constructing more refined multi-image understanding datasets.\nmPLUG-Owl3 abandons the approach of concatenate visual features to text sequences and instead employs efficient Hyper Attention for multimodal interaction. This not only enhances its capability for understanding multiple images and videos, but also enables it to handle very long visual sequence inputs with low resource overhead."}, {"title": "6 CONCLUSION", "content": "In this paper, we present mPLUG-Owl3, a multi-modal large language model that significantly advances the state-of-the-art in handling both single-image, multi-image and video tasks. The intro- duction of novel Hyper Attention enables the mPLUG-Owl3 to maintain the fine-grained visual input and effectively fuse visual and textual information, leading to superior performance across various benchmarks. We also propose a challenging long visual sequence evaluation named Distractor Re- sistance. Notably, mPLUG-Owl3 excels in managing ultra-long visual sequences and demonstrates a strong performance in evaluation. We believe that mPLUG-Owl3 reveals a direction for building an efficient and effective multi-modal large language model. We hope it can become the foundation for future research."}]}