{"title": "Infant CryNet: A Data-driven Framework for Intelligent Analysis of Infant Cries", "authors": ["Mengze Hong", "Chen Jason Zhang", "Lingxiao Yang", "Yuanfeng Song", "Di Jiang"], "abstract": "Understanding the meaning of infant cries is a significant challenge for young parents in caring for their newborns. The presence of background noise and the lack of labeled data present practical challenges in developing systems that can detect crying and analyze its underlying reasons. In this paper, we present a novel data-driven framework, \"Infant CryNet,\" for accomplishing these tasks. To address the issue of data scarcity, we employ pre-trained audio models to incorporate prior knowledge into our model. We propose the use of statistical pooling and multi-head attention pooling techniques to extract features more effectively. Additionally, knowledge distillation and model quantization are applied to enhance model efficiency and reduce the model size, better supporting industrial deployment in mobile devices. Experiments on real-life datasets demonstrate the superior performance of the proposed framework, outperforming state-of-the-art baselines by 4.4% in classification accuracy. The model compression effectively reduces the model size by 7% without compromising performance and by up to 28% with only an 8% decrease in accuracy, offering practical insights for model selection and system design.", "sections": [{"title": "1. Introduction", "content": "It is reported that more than 380,000 babies are born each day globally. For these newborns, the cry is their fundamental mode of communication with the outside world, serving as a critical indicator of their biological and psychological needs (Lockhart-Bouron et al., 2023). Experienced medical practitioners have developed the ability to interpret various cry sounds, enabling them to discern an infant's physical condition and, in some instances, identify potential diseases solely based on the characteristics of the cry. However, acquiring such a nuanced understanding quickly is impractical for new parents due to the lack of specific knowledge and experience (Jiang et al., 2023).\nThere is a pressing need for the development of accessible, user-friendly tools that can assist parents in comprehending their newborns' cries and supporting the overall well-being"}, {"title": "2. Related Work", "content": "This study has close ties to three areas of research: audio classification, model pre-training, and model compression."}, {"title": "2.1. Audio Classification with CNNs", "content": "The task of infant cry classification has seen significant advancements in recent years, evolving from traditional machine learning techniques to deep learning approaches. Among these methods, Convolutional Neural Networks (CNNs) have gained increasing popularity due to their effectiveness in extracting local patterns. Numerous applications have been developed based on CNNs, including acoustic event detection (Bae et al., 2016), music start detection (Schl\u00fcter and B\u00f6ck, 2014), automatic speech recognition (Abdel-Hamid et al., 2014; Song"}, {"title": "2.2. Pretraining Model for Audio Tasks", "content": "In computer vision, models commonly undergo pre-training on labeled datasets such as ImageNet (Deng et al., 2009), which has a massive sample size. In natural language processing, pretraining methods based on Transformers (Vaswani et al., 2017) have been proposed, leading towards the trending Large Language Models (LLMs) (Lin et al., 2024). Motivated by these advances, audio pretraining has become a popular research topic. For instance, Wav2vec (Schneider et al., 2019) can be used to transfer audio signals into vector representations, which can improve the training of acoustic models. Similar to Imagenet, AudioSet (Gemmeke et al., 2017) is a massive audio dataset comprising more than 2 million audio samples, each tagged with 527 sound event labels. By pretraining on AudioSet, researchers (Kong et al., 2018, 2020) have proposed a variety of deep neural networks for audio classification and achieved promising performance.\nIn infant cry classification, labeled datasets are scarce due to the sensitivity of data collection and the high cost of annotation by pediatricians. Extracting features from pre-trained models can effectively incorporate prior knowledge. The transfer learning technique leverages the rich feature representations learned by deep neural networks to train classifiers (Pan and Yang, 2010), thereby enhancing the performance for more specific tasks."}, {"title": "2.3. Model Compression", "content": "In the evolving landscape of machine learning techniques, there has been a notable shift in research focus from the development of \"best\" performing models to those that offer better efficiency and practicality for industrial applications (Choudhary et al., 2020). The advent of novel compression techniques such as knowledge distillation, network pruning, and quantization have enabled the deployment of sophisticated models in many practical use cases (Hinton et al., 2015; Zhou et al., 2017). Moreover, with the emergence of large pre-trained models, Li et al. (2020) suggests the strategy of training very large models and then compressing them to obtain a relatively smaller model, which has higher accuracy than directing training a smaller model. Similar work has also shown that model compression techniques can effectively reduce the size of neural networks without significantly compromising accuracy (Polino et al., 2018), which is particularly beneficial for scenarios where computational resources are at a premium, such as in edge computing environments.\nOur work diverges from existing infant cry systems by addressing data scarcity with pre-trained models, enhancing CNN pooling methods for better feature extraction, and providing a lightweight, mobile-deployable solution through model compression."}, {"title": "3. Framework Architecture", "content": "In this section, we first introduce the feature extraction methods for infant cry audio in Section 3.1. The two targeted tasks, namely crying detection and crying analysis, are"}, {"title": "3.1. Feature Extraction", "content": "In audio processing, the feature extraction can be divided into time and frequency domains. Time-domain features, such as amplitude and zero-crossing, provide straightforward insights but limited information for complex tasks due to their simplicity. In contrast, frequency domain features, including MFCCs, LPCCs, and LFCCs, have been proven to achieve superior results with more discriminative features to be learned by the model (Hertel et al., 2016). These features can be represented by two approaches: the waveform that reflects the pattern of sound pressure amplitude in the time domain, and the spectrogram which visually depicts a signal's frequency spectrum.\nThe comparison between infant cry sounds and adult voices is illustrated in Figure 1, using both waveform and spectrogram representations. Unlike adult speech, which tends to be more irregular and has lower amplitude, infant cries typically exhibit a more prosodic waveform and larger amplitude. This periodic nature makes infant cries particularly well-suited for combined analysis of prosodic and time-frequency domain features. The spectrogram, as a comprehensive visual representation, effectively displays both acoustic and prosodic characteristics, providing clear insights into how the signal's frequency content varies over time and highlighting the unique properties of infant cries.\nWith this feature representation, the problem of classifying audio has been transformed into an image classification problem, motivating the following discussions on constructing convolutional neural networks. Determining the right network architecture is crucial for developing the infant cry system. Thus, we present two distinct model architecture options in the subsequent sections, specifically focusing on the tasks of infant cry detection and classification. These proposed architectures aim to effectively capture relevant information and address the aforementioned challenges in the infant cry system."}, {"title": "3.2. Infant Cry Detection", "content": "For infant cry detection, as shown in Figure 2(a), we utilize the 10-layer CNNs (CNN-10) with four convolutional blocks, each consisting of two 3 \u00d7 3 kernel-sized convolutional layers, separated by batch normalization to improve training efficiency and stability. The ReLU activation function is utilized for this purpose. After each block, the spatial dimensions of the feature maps are reduced via a 2 \u00d7 2 average pooling operation. The final feature maps are summarized into a fixed-length vector through another pooling operation and fed into a softmax activation function to generate class probabilities for the binary classification of whether an audio clip contains an infant cry."}, {"title": "3.3. Infant Cry Analysis", "content": "Based on the available dataset, the infant cries can be classified into six distinct reasons: awake, hug, sleepy, uncomfortable, diaper, and hungry. Here, we utilize the 14-layer CNNs (CNN-14), as shown in Figure 2(b), which has two more convolutional blocks in comparison with CNN-10 for enhanced feature extraction and improved classification accuracy in identifying the distinct reasons behind infant cries."}, {"title": "3.4. Model Compression", "content": "For challenging tasks, models with complex network architectures typically perform better. However, the computation for such networks can be slow, posing the challenge of achieving good performance with a lightweight network. This paper addresses this problem by employing knowledge distillation (Hinton et al., 2015), which transfers knowledge from a complex \"teacher\" model to a simpler \"student\" model without sacrificing accuracy. It also considers model quantization technique (Jacob et al., 2018), which performs some or all operations on tensors using integers instead of floating-point values.\nFor knowledge distillation, we utilize ResNet22 (He et al., 2016) as the student model and CNN14 as the teacher model, with the distillation process illustrated in Figure 2(c). The loss function for the student model is carefully crafted to address two main discrepancies. First, it measures the divergence between the predictions made by the student model and the actual ground truth data, ensuring that the student learns to approximate the correct outputs effectively. Second, it accounts for the variance between the softened predictions of the student model and the softened labels provided by the teacher model. These softened labels are generated by applying a temperature scaling factor to the teacher's logits, which allows the student model to capture more nuanced information about the output distribution.\nOn the other hand, we implement dynamic quantization, which involves converting the weights and activation of the model from floating-point to integer values with scale factors determined dynamically at runtime, leading to a smaller model size and faster inference."}, {"title": "4. Experiments", "content": "In this section, we outline the experiments conducted to evaluate the performance of the proposed methods. We detail the datasets used, the baselines for comparison, and the implementation specifics. Additionally, we conduct several ablation studies to analyze the effectiveness of different components, suggesting best practices for implementing the system."}, {"title": "4.1. Experimental Setup", "content": ""}, {"title": "4.1.1. DATASET", "content": "In the task of infant cry detection, a dataset consisting of 6,600 audio clips is used, with 6,000 clips for training and 600 clips for validation (see Table 1). Each audio clip has a 15-second duration and a sampling rate of 16,000 Hz. Similarly, the dataset for the infant cry classification task contains 835 audio clips, each labeled by one of six reasons for crying (i.e., awake, hug, sleepy, uncomfortable, diaper, and hungry)."}, {"title": "4.1.2. BASELINES", "content": "The performance of the proposed model is compared with various baselines and reported in terms of classification accuracy. The baseline models include:\n\u2022 CNN10 The 10-layer CNN, which features only four convolutional layers, is a simpler architecture compared to CNN14, which has six convolutional layers.\n\u2022 Resnet22 Each block in the ResNet comprises two convolutional layers with 3 \u00d7 3 kernel sizes and a shortcut connection between convolutional layers. Here, a 22-layer deep ResNet (Resnet22) with eight basic blocks is considered.\n\u2022 Wavegram-Logmel-CNN In comparison to one-dimensional CNN that cannot capture frequency information, Kong et al. (2020) proposed Wavegram-Logmel-CNN, which combined wavegram (time-domain) and Log-Mel spectrogram (frequency-domain) as input to a CNN."}, {"title": "4.1.3. IMPLEMENTATION DETAILS", "content": "The infant cry detection task involves identifying the presence of crying activity in an audio clip. To accomplish this, we first extracted the Log-Mel spectrogram, which serves as the input features for our model, and then fine-tuned a 10-layer CNN from PANNs for this purpose. The infant cry classification involves categorizing audio clips according to the reasons for crying. Similarly, we begin by extracting Log-Mel spectrograms, followed by fine-tuning a 14-layer CNN from PANNs to build an effective classification system. The Adam optimizer is used for both models, and the training was conducted on eight Nvidia Tesla V100 32GB GPUs with a batch size of 256."}, {"title": "4.2. Accuracy Analysis", "content": "For infant cry detection, the results of our proposed method and a baseline model are shown in Table 2, where both models exhibit similar accuracy. This can be attributed to the fact that crying sounds are relatively distinct and easy to detect compared to other sounds, enabling even a simple network to perform effectively. The high amplitude observed in Figure 1 contributes to the mitigation of the effect of background noise in the audio. Hence, it is advisable to choose computationally efficient models that can still achieve good performance in detecting infant cries.\nFor the classification task, the comparison between our method (i.e., CNN14 with pre-training) and various baselines is presented in Table 3. The proposed model outperforms all baselines, achieving an encouraging 4.4% improvement in accuracy over the best-performing"}, {"title": "4.3. Ablation Study", "content": "By focusing on the accuracy comparison of the same model (i.e., CNN14 with pretraining) under different pooling methods (see Table 4), our findings revealed that average pooling consistently outperformed max pooling among the baseline methods, underscoring the advantage of incorporating all instances in the calculation. While the combination of max pooling and average pooling provided only a small improvement, our proposed methods demonstrated superior performance. Notably, statistic pooling achieved the highest accuracy, which can be attributed to the periodic nature of infant cries, where varying attention to instances is not always necessary. Attention pooling ranked as the second-best option. Overall, our proposed methods outperformed the baselines, achieving significant improvements in accuracy over the best-performing baseline method."}, {"title": "4.4. Model Compression", "content": "The results in Table 5 depict the effectiveness of the model compression techniques. By applying model quantization, the model size is reduced by 7% without any loss of accuracy. Knowledge distillation reduces the model size by 21% but also lowers the accuracy by 7%. By combining knowledge distillation with model quantization, the model size is reduced by 28%, and the accuracy is only reduced by 8%.\nBased on the observations, we conclude that model compression techniques can effectively reduce model size with a manageable compromise on accuracy, depending on the method chosen. Model quantization is ideal if the model size requirement is not strict, as"}, {"title": "5. Conclusion", "content": "This paper presents \"InfantCryNet,\" an innovative framework designed to detect and comprehend the meanings behind infant cries, addressing the challenges of background noise and limited labeled data. By utilizing pre-trained audio models, along with statistical and multi-head attention pooling techniques, we enhanced feature extraction capabilities and achieved significant improvement in classification accuracy. To facilitate the deployment on mobile devices, our approach incorporates knowledge distillation and model quantization to compress model size, offering practical solutions to real-world applications.\nWhile the experiments present promising results, this study extensively focused on comparing neural network solutions with varying complexity levels. Although traditional machine learning methods generally lag in accuracy, their lightweight nature and training efficiency suggest the potential for exploring hybrid models, such as CNN-SVM and GMM-CNN, for developing more efficient infant cry classification systems. Furthermore, the challenge of data scarcity can be mitigated by adopting a federated learning approach, which enables the collection of more training data while protecting end-user privacy (Jiang et al.,"}, {"title": "Equations", "content": "The common pooling methods include max pooling and average pooling. Max pooling selects the maximum value in each patch, with the limitation that only one instance represents the whole audio while other instances are ignored. This can be described as follows:\nhmax = max(h1, h2, ..., hN)\nAverage pooling calculates the average value of each patch, which considers all instances instead of the maximum. However, since each instance contributes equally, it fails to reflect the variance within the patch:\nhavg = $\\frac{\\sum_{i=1}^{N} h_i}{N}$\nIn order to combine the advantages of max pooling and average pooling, the pre-trained audio neural networks (PANNs) calculated a fixed-length vector by adding the averaged and maximized vectors (Kong et al., 2020). However, the difference between instances is still ignored, and the method is formulated as follows:\nhadd = max(h1, h2, ..., hN) + $\\frac{\\sum_{i=1}^{N} h_i}{N}$\nStatistic pooling calculates the mean and variance, then uses a linear layer to reduce two dimensions to one. Both the average and variability of values are leveraged to represent the feature. This can be summarized as:\nS$2 = $\\frac{\\sum_{i=1}^{N}(h_i - h_{avg})^2}{N}$\nhstat =fc(havg, s\u00b2)\nMulti-head attention pooling determines the attention distribution of each instance, allowing instances to be aggregated based on their significance rather than equally contributing to the feature representation, as shown below:\nhattn = $\\frac{\\sum_{i=1}^{N}(a_i h_i)}{\\sum_{i=1}^{N}(\u03b1_i)}$"}]}