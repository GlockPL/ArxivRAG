{"title": "Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval", "authors": ["Le Huang", "Hengzhi Lan", "Zijun Sun", "Chuan Shi", "Ting Bai"], "abstract": "As LLMs exhibit a high degree of human-like capability, increasing attention has been paid to role-playing research areas in which responses generated by LLMs are expected to mimic human replies. This has promoted the exploration of role-playing agents in various applications, such as chatbots that can engage in natural conversations with users and virtual assistants that can provide personalized support and guidance. The crucial factor in the role-playing task is the effective utilization of character memory, which stores characters' profiles, experiences, and historical dialogues. Retrieval Augmented Generation (RAG) technology is used to access the related memory to enhance the response generation of role-playing agents. Most existing studies retrieve related information based on the semantic similarity of memory to maintain characters' personalized traits, and few attempts have been made to incorporate the emotional factor in the retrieval argument generation (RAG) of LLMs. Inspired by the Mood-Dependent Memory theory, which indicates that people recall an event better if they somehow reinstate during recall the original emotion they experienced during learning, we propose a novel emotion-aware memory retrieval framework, termed Emotional RAG, which recalls the related memory with consideration of emotional state in role-playing agents. Specifically, we design two kinds of retrieval strategies, i.e., combination strategy and sequential strategy, to incorporate both memory semantic and emotional states during the retrieval process. Extensive experiments on three representative role-playing datasets demonstrate that our Emotional RAG framework outperforms the method without considering the emotional factor in maintaining the personalities of role-playing agents. This provides evidence to further reinforce the Mood-Dependent Memory theory in psychology.", "sections": [{"title": "I. INTRODUCTION", "content": "As artificial intelligence increasingly emerges in the large language models (LLMs), LLMs exhibit a high degree of human-like capability. Recent studies [1]\u2013[9] use LLMs as role-playing agents to mimic human replies, showing powerful abilities in maintaining the personalized traits of characters in their response generation process. Role-playing agents have been applied to various fields, such as customer service agents and tourist guide agents. They show great potential in commercial applications and attract increasing attention in the LLMs research area.\nTo maintain characters' personalized traits and abilities, the most important factor is their memory. Character agents make retrieval in their memory unit to access its historical data, such as user profiles, event experience, recent dialogues, and so on, providing rich personalized information for LLMs in the role-playing task. Retrieval Augmented Generation (RAG) technology is used to access the related memory to enhance the response generation of role-playing agents, termed Memory RAG. Different attempts have been made in existing studies [10]\u2013[24] by using different memory mechanisms in various LLM applications. For example, the Ebbinghaus forgetting curve has inspired the development of MemoryBank [10], facilitating the implementation of a more anthropomorphic memory scheme. Furthermore, drawing on Kahneman's Dual-process theory [25], the MaLP framework [11] introduces an innovative Dual-Process enhanced Memory mechanism that effectively fuses long-term and short-term memory.\nDespite research demonstrating the effects of using memory in the above LLM applications, achieving greater human-like response of role-playing agents is still an open and largely unexplored research area. Inspired by cognitive research in psychology, we make an initial attempt to emulate human cognitive processes in the memory-recalling process. According to the Mood-Dependent Memory theory, which was proposed by psychologist Gordon H. Bower in 1981 [26]: people recall an event better if they somehow reinstate and recall the original emotion they experienced during learning. Through the experiments in which happy or sad moods were induced"}, {"title": "II. METHOD", "content": "In this section, we first introduce the overview architecture of our Emotional RAG role-playing framework and then give a detailed introduction to each component.\nA. Overview Architecture of Emotional RAG\nThe aim of role-playing agents is to mimic human responses in conversation generations. Agents are powered by LLMs, which have the ability to generate responses according to the context of the conversation. As shown in Figure 1, given the query that the agents need to respond to, the framework of our proposed Emotional RAG role-playing agent framework contains four components, i.e., query encoding component, memory construction component, the emotional retrieval component, and the response generation component. The utilization of each component is introduced as follows:\n\u2022 Query encoding component: both the semantic and emotional state of the query are encoded as vectors in this component.\n\u2022 Memory encoding component: the memory unit stores conversation information about characters. Similar to query encoding, both the semantic and emotional state of the memory are encoded.\n\u2022 Emotional retrieval component: it mimics human memory recalls in the memory unit and then provides mood-"}, {"title": "B. Emotional RAG Framework", "content": "The definition of role-playing agents: given a specific role R and a user query q, the agents expect to be able to generate the answer to the question based on the role's knowledge background (contained in R) and the query-related memory fragments m. In the role-playing agent R, among all possible generated responses a', the one with the highest probability is selected as the response a for q:\n$a = \\operatorname{argmax}_{a'} P(a'|R, m,q,\\theta),$\nwhere $\\theta$ is the parameters of the LLMs during the generation process. Our goal is to optimize the retrieval of m to generate the most human-like response a. The aim of role-playing agents is to generate answers that are most consistent with the characters' personality traits by retrieving the most related memories.\n1) Query Encoding Component: In this component, both the semantic and emotional state of the query are needed to be encoded. The semantic vector $semantic_q$ of query is defined as:\n$semantic_q = F(q),$\nwhere $F$ is the embedding function. In this paper, the widely used embedding model bge-base-zh-v1.5 developed by BAAI [27] is used to capture the latent vector of the query, which is a 768-dimensional vector for each query.\nFor the emotional vector of query q, the emotional state $emotion_q$ of query q can be formalized as follows:\n$emotion_q = G(q),$\nwhere G represents the emotion modeling function, which takes query q as input and outputs its emotional vector. This process is accomplished through GPT-3.5, a large model with powerful language understanding capabilities. As shown in Figure 2, we carefully design an emotional prompt, including the task description, scores on defined emotional dimensions, scoring criteria, and output format. The output is an emotional vector of the query, which is an 8-dimensional vector containing 8 different emotional states, i.e., joy, acceptance, fear, surprise, sadness, disgust, anger, and anticipation. The 8 emotional states are defined according to the emotion circle in [28]. The value of each dimension is an integer between 1 and 10, which measures the intensity of the emotional state.\n2) The Memory Encoding Component: The memory unit stores conversation information of question-answer pairs. Given the memory unit M consisting of n memory fragments, denoted as M = {$m_1, m_2,...,m_n$}, we can compute the"}, {"title": null, "content": "sentiment vector $semantic_m^k$ and emotional vector $emotion_m^k$ of a specific fragment $m_k$ as follows:\n$semantic_m^k = F(m_k),$\nwhere F is the semantic embedding function introduced in Eq. 2.\n$emotion_m^k = G(m_k),$\nwhere G is the emotion embedding function introduced in Eq. 3.\nAfter encoding the semantic and emotional vectors of query and memory, we conduct emotional retrieval in the next component.\n3) Emotional Retrieval Component: We retrieve the memory fragments that are most similar to the user query from the memory unit of characters based on semantic similarity and emotional similarity.\nTo retrieve the memory fragments that are most semantically similar to the query, we utilize the Euclidean distance between their semantic embeddings. This metric effectively quantifies the semantic similarity of the query and the memory fragment, simulating humans' cognitive recall process. The similarity between the query q and the memory fragment $m_k$ can be calculated as follows:\n$score_\\text{semantic}^k = E(semantic_q, semantic_m^k),$\nwhere E is the similarity score function, which can be the Euclidean distance function or cosine distance function.\nAccording to Bower's Mood-Dependent Memory theory [26]: events that are consistent with the character's current emotion are easier to retrieve, we use the cosine distance between two emotion vectors to find emotionally consistent memory fragments, defined as:\n$score_\\text{emotional}^k = 1 \u2013 C(emotion_q, emotion_m^k),$"}, {"title": null, "content": "where C is a function of the cosine similarity of two vectors. The smaller the distance $score_\\text{emotional}^k$ is, the more similar the emotions contained in the query and the memory fragment. After obtaining the distant scores of memory fragments, the final similar distant score of memory fragments is defined as:\n$score_\\text{final}^k = M(score_\\text{semantic}^k, score_\\text{emotional}^k),$\nwhere M is the function that computes the final retrieval score. Two kinds of flexible retrieval strategies, i.e., combination strategy and sequential strategy, are proposed to fuse memory semantic and emotional states during the retrieval process.\n\u2022 Combination strategy: this strategy considers the two similarities at the same time. We adopt two functions, i.e., add function (C-A) and multiple function (C-M), to compute the retrieval scores of memory fragments.\n\u2022 Sequential strategy: it contains semantic first strategy (S-S) and emotional first strategy (S-E). In the semantic first strategy, the most similar memory fragments are retrieved based on their semantic scores and then re-ranked according to their emotional scores. Different order is conducted in the emotional first strategy.\nFinally, the top 10 memory fragments with the smallest distant scores (i.e., the highest similarity) are used for retrieval augmentation. The retrieved memory is not only semantically related to the query but also consistent with the emotional state in the query.\n4) Response Generation Component: After obtaining the retrieved memory, we design a prompt template for LLMS to generate responses in role-playing agents. The prompt template is shown in Figure 3. The query, role information, re-trieved memory fragments, and task description are formatted in the template that is sent to LLMs.\nIn summary, by incorporating the emotional factor into the RAG process in role-playing agents, the memory fragments retrieved in our framework are more aligned with the emotional state. This enables the role-playing agents to generate more human-like responses, thus enhancing the interaction quality."}, {"title": "III. EXPERIMENT", "content": "we conduct experiments on three public datasets to evaluate the role-playing capabilities of LLMs augmented with emotional memory.\nA. Experimental Settings\n1) Datasets: We conducted experiments on three public role-playing datasets, namely InCharacter, CharacterEval, and Character-LLM. Their statistics are summarized in the Table I.\n\u2022 InCharacter Dataset [29]: this dataset contains 32 characters. The characters are sourced from ChatHaruhi [3], RoleLLM [5] and C.AI\u00b9. Each character is associated with a memory unit that includes dialogues from notable scenes, with an average length of 337 entries.\n\u2022 CharacterEval Dataset [30]: the dataset consists of 77 distinct characters with 4,564 question-answer pairs. These characters are collected from well-known Chinese films and television series, and the dialogue data is compiled from their respective scripts. We selected the top 31 popular characters. For each character, we extracted all the question-answer pairs to establish a memory unit, with an average size of 113 entries.\n\u2022 Character-LLM Dataset [1]: the Character-LLM dataset contains 9 famous English characters, e.g., Beethoven, Hermione, etc. Their memory units come from scene-based dialogue completion (completed by GPT). We use 1,000 QA dialogues for each character.\n2) Evaluation Metrics: We conducted evaluations using the Big Five Inventory (BFI) and MBTI evaluation to ascertain the accuracy of the character agent's personality traits. Details of each evaluation metric are introduced as follows:\n\u2022 Big Five Inventory (BFI) [31]: The Big Five, also known as the Big Five personality trait theory, is a widely used psychological model that divides personality into five main dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.\n\u2022 MBTI\u00b2: is a popular personality test based on the Myers-Briggs Type Indicator (MBTI) theory. It categorizes people's personality types into 16 different combinations. Each type is represented by four letters, corresponding to the following four dimensions: Extroversion (E) vs. Introversion (I), Sensing (S) vs. iNtuition (N), Thinking (T) vs. Feeling (F), Judging (J) vs. Perceiving (P).\nThe evaluation of MBTI is a classification task of 16 types, while BFI predicts the values of five personality dimensions."}, {"title": "B. Main Results", "content": "We evaluate the performance of Emotional RAG and Ordinary RAG on three datasets, including InCharacter, CharacterEval, and Character-LLM datasets. Ordinary RAG uses semantic similarity as the only retrieval criterion. The experimental results are shown in Table II and Table III, we have the following observations:\n(1) In most cases, Emotional RAG achieves better results than the RAG method without considering the emotion factor. This indicates that incorporating emotional states helps the maintaining of personality traits in role-playing agents."}, {"title": "C. Experimental Analysis", "content": "1) RAG Strategy Analysis: we analyze the impact of different retrieval strategies in incorporating the emotional factor. As introduced in the Emotion Retrieval Component, four retrieval strategies, i.e., combination strategy, i.e., add function (C-A), multiple function (C-M), and sequential strategy, i.e., semantic first (S-S), emotional first (S-E), are proposed to fuse the semantic and emotional states of memory during the retrieval process. We present the experimental results on Qwen-72B in Figure 5, we can see that (1) Emotional RAG variants with all retrieval strategies (except the C-A strategy in BFI evaluation) achieve better results in MBTI and BFI evaluations, showing the effectiveness of incorporating emotional states in role-playing agents; (2) Different retrieval strategies are applicable to different evaluations. For example, in the BFI personality evaluation, the sequential strategy (S-S) performs the best, while in the MBTI task, the combination strategy (C-A) exhibits the best performance.\n2) Case studies: to provide an intuitive demonstration of the influence by incorporating the emotional factor in role-playing agents, we show two examples to illustrate the superiority of our Emotional RAG. Figure 6 shows memory fragments in the memory units with different emotional states. In the first case, two memory fragments are all related to the input query. our Emotional RAG retrieved more appropriate content when it came to mentioning being dumped by a girlfriend, so its responses showed empathy and understanding of the situation compared to Ordinary RAG, making the conversation more vivid and natural. In the second case, Emotional RAG retrieves a memory fragment that is consistent with the query's emotion, so the reply expresses excitement and anticipation about seeing the sea. Only considering the semantic similarity will lead to emotional inconsistency and make the response content somewhat unreasonable."}, {"title": "IV. RELATED WORK", "content": "A. Role-Playing Agents\nRole-playing agents, also termed Role-Playing Conversational Agents (RPCAs), aim to emulate the conversation behaviors and patterns of specific characters via LLMs. Role-playing agents show considerable promise and are poised to substantially advance the areas of gaming, literature, and creative industries [1]\u2013[6]. Currently, the implementation of role-playing agents can be categorized into two primary methodologies. The first strategy enhances the role-playing capabilities of LLMs through prompt engineering and generative enhancement techniques. This approach equips LLMs with character-specific data within the context, capitalizing on the advanced in-context learning capabilities of modern LLMs. For instance, ChatHaruhi [3] developed a RAG (Retrieval-Augmented Generation) system that leverages historical dialogues from iconic scenes to facilitate learning from a limited number of examples, thus capturing the personality traits and linguistic styles of characters. Conversely, RoleLLM [5] introduced RoleGPT, which uses role-based prompts for GPT models.\nThe other type of role-playing approach involves pre-training or fine-tuning LLMs with collected character data, thereby customizing LLMs for specific role-playing scenarios. In [4], dialogue and character data from the Harry Potter novels were utilized to train agents capable of generating responses that align accurately with the context of the scene and the inter-character relationships. Character-LLM [1] developed scenarios using ChatGPT to create conversational data, subsequently training a language model with meta-prompts and these conversations. This project implemented strategies to mitigate the creation of character discrepancies in the model training dataset, such as memory uploads and protective memory enhancements. RoleLLM [5] employed GPT to formulate question-answer pairs based on scripts, presenting them in a triplet format consisting of the question, answer, and confidence level. Incorporating a confidence metric significantly enhanced the quality of the generated data. CharacterGLM [2] trained an open-source character model using data from multiple characters. This approach embeds role-specific knowledge directly into the model's parameters.\nWhile existing studies of role-playing agents consider the character profile, relationships, and attributes relevant to the dialogue, they often overlook a critical element\u2014the emotional factor of the characters. Our emotional RAG framework is designed on the prompt engineering technique, in which the LLMs are not required to be pre-trained or fine-tuned in role-playing agents.\nB. Memory RAG in LLM Applications\nIn role-playing agents, memory is an important factor for characters to maintain their personality traits. Retrieval Augmented Generation (RAG) technology is widely used [35] to access the related memory to enhance the generation of role-playing agents, termed Memory RAG. For example, an"}, {"title": null, "content": "LLM-based automatic agent architecture proposed in [36] contains four components: a profiling module, a memory module, a planning module, and an action module. Among these, the memory module is crucial for the design of the agent architecture. It takes charge of obtaining information from the environment and utilizes these recorded memories to enhance future actions. The memory module enables the agent to accumulate experiences, evolve autonomously, and act in a manner that is more consistent, rational, and efficient [14].\nThe research on memory design in various LLM applications can be summarized into two categories. The first is capturing and storing intermediate states from past model reasoning as memory content. These memories are then retrieved as needed to support the generation of current responses. For instance, MemTRM [37] maintains past key-value pairs and employs the query vector of the current input to conduct K-nearest neighbor searches, applying mixed attention to both the current input and the past memories. However, MemTRM encounters challenges with memory obsolescence during training. To address this, LongMEM [38] separates the processes of memory storage and retrieval. This strategy is particularly tailored for open-source models and might necessitate adaptive training to effectively integrate the contents of the memory library. The second type of memory design scheme involves providing memory support via an external memory library. This external memory can take various forms, enhancing the system's ability to manage and retrieve information efficiently. One such implementation is MemoryBank [10], which stores past conversations, event summaries, and user characteristics in a vector library format. The use of vector similarity calculations significantly accelerates the memory retrieval process, allowing for rapid access to relevant past experiences and data. AI-town [12] uses a linguistic approach by preserving memory in natural language. It introduces a reflection mechanism that under specific conditions, transforms straightforward observations into more abstract and higher-order reflections. This system considers three critical factors during the retrieval process: the relevance, recency, and importance of memory, ensuring that the most pertinent and contextual information is retrieved for use in ongoing interactions.\nIn LLM-based role-playing agents, the memory unit typically operates via the second method, incorporating external memory libraries to enhance character authenticity. For example, in ChatHaruhi, the character agent retrieves dialogue from iconic scenes to enrich character development and interactions. Despite a large amount of research on memory RAG technique, achieving greater human-like response is still an open and unexplored area. Inspired by cognitive research in psychology, we make an initial attempt to incorporate the emotional factor to emulate human cognitive processes in the memory-recalling process, making the response of LLMs more emotionally resonant and human-like."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we make an initial attempt to incorporate emotional memory to enhance the performance of role-playing agents. A novel emotional RAG framework with four retrieval strategies is proposed to make role-playing agents more emotional and human-like in conversations. Extensive experiments on various characters on three public datasets demonstrate the effectiveness of our method in maintaining the personality traits of characters. We believe that imbuing emotions into role-playing agents is a pivotal research direction. In our current study, we conduct emotional RAG on an intuitive memory mechanism. In future work, we will attempt to incorporate the emotional factor into more advanced memory organization and retrieval schemes."}]}