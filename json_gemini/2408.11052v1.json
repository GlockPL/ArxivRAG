{"title": "Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research", "authors": ["Micha\u0142 Bortkiewicz", "W\u0142adek Pa\u0142ucki", "Vivek Myers", "Tadeusz Dziarmaga", "Tomasz Arczewski", "\u0141ukasz Kuci\u0144ski", "Benjamin Eysenbach"], "abstract": "Self-supervision has the potential to transform reinforcement learning (RL), paral- leling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover new behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environments as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (JaxGCRL) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. The key to this performance is a combination of GPU-accelerated environments and a stable, batched version of the contrastive reinforcement learn- ing algorithm, based on an infoNCE objective, that effectively makes use of this increased data throughput. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in a diverse set of challenging environments.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning has had a large impact on machine learning over the last decade, transforming how research is done in domains such as natural language processing and computer vision. In the context of reinforcement learning (RL), most self-supervised prior methods apply the same recipe that has been successful in other domains: learning representations or models from a large, fixed dataset. However, the RL setting also enables a fundamentally different type of self-supervised learning: rather than learning from a fixed dataset (as done in NLP and computer vision), a self-supervised reinforcement learner can collect its own dataset. Thus, rather than learning a representation of a dataset, the self-supervised reinforcement learner acquires a representation of an environment or of behaviors and optimal policies therein. Self-supervised reinforcement learners may address many of the challenges that stymie today's foundations models: reasoning about the consequences of actions [10, 58, 83] (i.e., counterfactuals [51, 103]) and long horizon planning [25, 42, 103]. In this paper, we study self-supervised reinforcement learning in an online setting: an agent interacts in an environment without a reward to learn representations or models, which are later used to quickly solve downstream tasks. We refer to such interactions as \"self-supervised\" or \"unlabeled\" because the agent does not receive any reward feedback. We will focus on goal-conditioned algorithms,"}, {"title": "2 Related Work", "content": "Our new benchmark and implementation build upon recent advances in goal-conditioned reinforce- ment learning (GCRL), showing that scalable contrastive learning algorithms can be combined with hardware-accelerated environments to enable fast and stable training of self-supervised RL agents across a suite of benchmarks."}, {"title": "2.1 Goal-Conditioned Reinforcement Learning", "content": "Goal-conditioned reinforcement learning is a special case of the general multi-task reinforcement learning setting, in which the potential tasks in an environment defined by goal states in the environ- ment that the agent is trying to reach [38, 54, 77]. Formally, the GCRL setting augments a Markov decision process (MDP) with a potential reward function (the indicator function) for each possible goal state in the MDP [32]. Being able to reach any possible goal is a particularly attractive as an ob- jective for learning generalist agents: arbitrary goals can express diverse behaviors in the environment and do not require a hand-specified reward function for each task (every state specifies a task when viewed as a goal) [89]. GCRL techniques have seen success in domains such as robotic manipulation [2, 24, 31, 39, 106] and navigation [47, 60, 66, 92]. Recent work has shown that representations of goals can be imbued with addtional structure to enable capabilities such as language grounding [64, 74], compositionality [63, 75, 107], and planning [30, 80]. In this paper, we show how GCRL techniques based on contrastive learning [32] can be scaled with GPU-acceleration to enable fast and stable training."}, {"title": "2.2 Accelerating Deep Reinforcement Learning", "content": "Deep RL has only recently become practical for many tasks, in part due to improvements in hardware support for these algorithms. Distributed training has enabled RL algorithms to scale across hundreds of GPUs [27, 28, 48, 70]. To resolve the bottleneck of environment interaction with CPU-bound environments, various GPU accelerated environments have been proposed [12, 21, 36, 61, 65, 67, 86]. In addition to reducing CPU-GPU data-transfer overhead, these environments can easily be vectorized with frameworks like Jax [14, 44, 46] to collect data from hundreds of rollouts in parallel. In this paper, we leverage these advances with a modified contrastive RL algorithm to scale self-supervised GCRL across orders of magnitude more data than previously possible."}, {"title": "2.3 Self-Supervised RL and Intrinsic Motivation", "content": "Self-supervised training has enabled key breakthroughs in language modeling and computer vision [23, 43, 68, 91, 112]. In the context of RL, we will use the term self-supervised to mean techniques that can learn through interaction with an environment without a reward signal. Perhaps the most successful form of self-supervision here have been in multi-agent games that can be rapidly simulated, such as Go and Chess, where self-play has enabled the creation of superhuman agents [94, 95, 97, 109]. When learning goal-reaching agents, another basic form of self-supervision is to relabel trajectories as successful demonstrations of the goal that was reached, even if it differs from the original commanded goal [54, 105]. This technique has seen recent adoption under the term \u201chindsight experience\" replay for various deep RL algorithms [2, 17, 32, 39, 84]."}, {"title": "2.4 RL Benchmarks", "content": "The RL community has recently started to pay greater attention to how RL research is conducted, reported and evaluated [1, 45, 52]. A key issue is a lack of reliable and efficient benchmarks\u2014 it becomes hard to rigorously compare novel methods when the number of trials needed to see statistically significant results across diverse settings ranges in the thousands of training hours [53]. Some benchmarks that have seen adoption include OpenAI gym [15], the DeepMind Control Suite [98], D4RL [37]. More recently, hardware-accelerated versions of some of these benchmarks have been proposed [41, 57, 65, 67]. Our contribution is to propose a stable contrastive algorithm and suite of goal-reaching tasks that can enable meaningful RL research on a single GPU by significantly reducing the cost of running experiments."}, {"title": "3 Self-Supervision through Goal-Conditioned Reinforcement Learning", "content": "In the goal-conditioned reinforcement learning setting, an agent interacts with a controlled Markov process (CMP) M = (S, A, p, po, \u03b3) to reach arbitrary goals [2, 11, 54]. At any time t the agent will observe a state st and select a corresponding action at \u2208 A. The dynamics of this interaction are defined by the distribution p(st+1 | St, at), with an initial distribution po(so) over the state at the start of a trajectory, for st \u2208 S and at \u2208 A. For any goal g \u2208 S, we cast optimal goal-reaching as a problem of inference [5, 11, 13, 32]: given the current state and desired goal, what is the most likely action that will bring us toward that goal? As we will see, this is equivalent to solving the Markov decision process (MDP) M\u1ef9 obtained by augmenting M with the goal-conditioned reward rg:\n\nrg(st, at) = p(g | st, at).\n\nFormally, a goal-conditioned policy \u03c0(\u03b1 | s, g) receives both the current observation of the environ- ment as well as a goal observation g \u2208 S. We inductively define the k-step (action-conditioned) policy visitation distribution:\n\npT($1 | so, ao) = p(s1 | So, ao),\nPk+1(Sk+1, ak+1 | So, ao) = \\int_{A} \\int_{S} p(Sk+1 | Sk, ak) dp_k(Sk | So, ao) d\\pi(ak | S).\n\nWe define the discounted state visitation distribution as $p^{\\pi}(s_{+} | s, a) \\equiv \\sum_{k=0}^{\\infty} \\gamma^{k} p_{k}^{\\pi}(s_{+} | s, a)$, which we interpret as the distribution of the state K steps in the future for K ~ Geom(1 \u2013 \u03b3). This last expression is precisely the Q-function of the policy \u03c0(\u00b7 | \u00b7, g) for the reward rg: $Q^{\\pi}(s, a) \\approx p^{\\pi}(g | s, a)$ [32]. For a given distribution over goals g ~ pg, we can now write the overall objective as\n\nmax_{\u03c0(s,g)} E_{po(so)pg (g)\u03c0(\u03b1\u03bf|80,9)} [P(g | 80, \u03b1\u03bf)]. (1)"}, {"title": "3.1 Contrastive Representations for Goal-Reaching", "content": "Equation (1) tells us that to optimally reach goals g \u2208 S, we should learn a policy\n\n\u03c0*(a | s, g) = arg max p (g | s, a).\n\u03c0(als,g)\n\nThis suggests that we can learn an optimal goal-reaching policy by learning and maximizing a critic f(s,a, g) xp (g | s, a) with respect to the actions a.\nWe extend a long line of work that views learning this critic as a classification problem [31-33, 75, 110, 111], training this critic on batches of (s, a, g) to classify whether g is the goal state of the trajectory (s, a), or whether it is a negative sample. In Section 5.4, we will discuss variants of this contrastive loss. The base contrastive loss we study will be the infoNCE objective [96], modified to use a symmetrized [82] critic parameterized with l2-distances [30]. To learn the critic, we will first define two additional state-marginal measures:\n\nps(s) = \\int_{A} p_S(s' | s, a) d\\pi(a | s) dp_0(s)\n\\ Ps (s+) = S/A p(s+ | s, a) d\u03c0(\u03b1 | | s) dps(s).\n\nAs we train \u03c0, we will sample batches B from a replay buffer D. Assuming \u03c0 is fixed and that resets occur with probability (1 \u2013 \u03b3), we can write the distribution over (s, a, g) values sampled in the batch as\n\nB\u2190 {(si, ai, gi)}=1 ~ ps(s)\u03c0(\u03b1 | s)p (g|s,a).\n\nThe family of contrastive RL algorithms Eysenbach et al. [29, 30], consists of the following compo- nents: (a) the state-action pair and goal state representations, $(s, a) and \u03c8(g)$, respectively; (b) the critic, which is defined as an energy function fp,4(s, a, g) and the loss function; and (c) a contrastive loss function, which is a function of the matrix containing the critic values {fp,y(Si, Ai, 9j)i,j}, related to the elements of the batch B. Section 5.1 contain the formulas for the choices considered in this paper."}, {"title": "3.2 Policy Learning", "content": "We use a DDPG-style policy extraction loss to learn a goal-conditioned policy with this critic [62]. Parameterizing the policy as some \u03c0\u03bf(\u03b1 | s, g), we get\n\nL_{policy} (B; 0) = \\sum_{i,j=1}^{K} \\int_{A} f\u00f8,\u03c8(Si, ai, 9i) d\u03c0\u03bf(ai | Si, gi).\n\nBreaking with prior contrastive RL approaches [32], we sample goals from the same trajectories as the states and actions, which we find improves performance (see Appendix C for more discussion). The overall objectives then become\n\nmin EB~D [LinfoNCE (B; 6,4)] and min EB-D [Lpolicy (B; 0)].\n\u03a6,\u03c8"}, {"title": "4 New Implementation and Benchmark", "content": "The key contribution of this paper is the extremely fast implementation of state-based self-supervised reinforcement learning algorithms and a new benchmark of BRAX-based [36] environments. Our implementation leverages the power of GPU-accelerated simulators to reduce the time required for data collection and training, allowing researchers to run extensive experiments in a fraction of the time previously needed. The bottleneck of former self-supervised RL implementations was twofold. Firstly, data collection was executed on many CPU threads, as a single thread was often used for a single actor. This reduced the number of possible parallel workers, as only high-compute servers could run hundreds of parallel actors. Secondly, the necessity of data migration between CPU (data collection) and GPU (training) posed additional overhead. These two problems are mitigated by fully jittable algorithm implementation and environments running directly on GPU. We provide a robust framework that showcases the potential of self-supervised RL and offers a practical tool for the community to develop and evaluate new algorithms efficiently. For instance, testing design choices such as new contrastive loss or energy function could be completed in less than 30 minutes, with ten of those dedicated to implementation. We tested several recent contrastive learning ideas during the framework's development, such as Eysenbach et al. [32]. Having results ready within half an hour after reading a paper or brainstorming is a radically different workflow from how RL research has been done over the last few years."}, {"title": "4.1 Training speedup from environment vectorization on a single GPU", "content": "To evaluate the speedup of the proposed implementation, we execute the basic goal-conditioned contrastive RL method [32] using original and novel repositories. We conducted experiments using a 16:1 ratio of environment steps to neural network updates, as the initially suggested 1:1 ratio led to lower performance, perhaps due to excessive updates per environment step. In Fig. 1, we report the performance of the CRL agent in Ant environment and the wall-clock time of the experiment. In particular, we report results for CRL agent that uses the NCE-binary objective with a dot-product critic, the main configuration from [32]. The experiments were carried out on an NVIDIA V100 GPU. The speedup in this configuration is 22-fold. Further speedup results are presented in Appendix A.1. Notably, our implementation uses only one CPU thread and has low RAM usage as all the operations, including those on the replay buffer, are computed on GPU."}, {"title": "4.2 Benchmark", "content": "To evaluate the performance of our methods, we propose a new goal-conditioned task benchmark consisting of eight diverse continuous control environments. These environments range from very easy ones, suitable for sanity checks, to complex, exploration-oriented tasks. All environments are implemented in JAX and can be run on accelerated hardware, enabling the collection of trajectories from thousands of environment instances in parallel. When selecting environments, we aimed to include tasks that require not only spatial navigation but also an understanding of the spatial layout and object manipulation through agent interaction. The list below includes a short description of each environment, with technical details for each environment in Table 1:\nReacher [15] This is a simple 2d manipulation task, the goal is to have end part of a 2-segment robotic arm in a specific place. The goal location is sampled uniformly from a disk around an agent.\nHalf Cheetah [108] In this 2d task, a 2-legged agent has to get to a goal that is sampled from one of the 2 possible places, one on each side of the starting position.\nPusher [15] This is a 3d robotic task, that consists of a robotic arm and a movable circular object resting on the ground. The goal is to use the arm to push the object into the goal position. With each reset both position of the goal and movable object are selected randomly. We prepared two variants of this task, an easy version, where object and goal start closer to each other, and a hard version, where they are further apart."}, {"title": "5 Empirical Experiments with JaxGCRL", "content": "We ablated the key components of the CRL algorithm across the environments in our benchmark to stabilize and improve its performance. These experiments were made possible by our hardware- accelerated environments and batched contrasting learning implementation, which allowed us to train hundreds of agents over millions of environment steps in a few hours."}, {"title": "5.1 Contrastive RL design choices", "content": "Energy functions Measuring the similarity between samples could be achieved in various ways, including cosine similarity [19], dot product [82], and negative L\u2081 and L2 distance [50]. Namely,\n\nfp,4,cos(s, a, g) = (\u03c6(s,\u03b1), \u03c8(g))/(||\u03c6(s,a)||2||\u03c8(9)||2), (3)\n\u0192\u00a2,\u03c8,dot(s, a, g) = ($(s, a), \u03c8(g)), (4)\n\u0192\u00a2,\u03c8,L\u2081 (s, a, g) = \u2212||$(s, a) \u2013 \u03c8(g) ||1, (5)\n\u0192\u00a2,\u03c8,L2(s, a, g) = \u2212||$(s, a) \u2013 \u03c8(g) ||2, (6)\nfp,4,L2 w\\0 sqrt(s, a, g) = \u2212||$(s, a) \u2013 \u03c8(g) ||2. (7)"}, {"title": "Contrastive losses", "content": "In addition to different parameterizations for the energy function, we also can vary the loss function with which the energy function is trained. In particular, we evalu- ate InfoNCE [32, 104]-type losses, FlatNCE [18]-type losses, Monte Carlo version of Forward- Backward unsupervised loss [101], and losses inspired by preference optimization for Large Language Models [16]. More precisely, we tested the following losses:\n\nL_{InfoNCE_forward} (B; \u03c6, \u03c8) = - \\sum_{i=1}^{K} log \\bigg(\\frac{e^{f\u03c6,\u03c8(Si,ai,gi)}}{ \\sum_{j=1}^{K} e^{f\u03c6,\u03c8(Si,ai,gj)}} \\bigg)\n\\ L_{InfoNCE_backward}(B; $, \u03c8) = -\\sum_{i=1}^{K} log \\bigg(\\frac{e^{f\u03c6,\u03c8 (Si,ai,gi)}}{ \\sum_{j=1}^{K} e^{f\u03c6,\u03c8 (Sj,aj,gi)}} \\bigg). (9)\n\\L_{InfoNCE_symmetric} (B; \u03c6, \u03c8) = L_{InfoNCE_forward}(B; $, \u03c8) + L_{InfoNCE_backward}(B; \u03c6, \u03c8), (10)\n\\LDPO (B; \u03c6, \u03c8) =-\\sum_{i=1}^{K} \\sum_{j=1}^{K} logo [fo, (8i, ai, g\u00ec) \u2013 \u00a3\u00f8,p(Si, ai, 9j)],\n\\LIPO (B; \u03c6, \u03c8) = \\sum_{i=1}^{K} \\sum_{j=1}^{K} [(fo,\u03c8(Si, ai, 9i) \u2013 \u00a3\u03c6,\u03c8(Si, ai, 9j)) \u2013 1]^2, (12)\n\\LSPPO (B; \u03c6, \u03c8) =\\sum_{i=1}^{K} \u03a3_ [fo, (Si, ai, 9i) - 1]^2 + [f\u00f8,y(Si, ai, 9j) + 1, (13)\n\u03a3K\u2081 ef, (Si,ai,95)-fp,\u03c8(Si,&i,gi)\n\\ L_{FlatNCE_forward}(B; \u03c6, \u03c8) =- log (14)\nK detach [\u03a3=1 ef($i,&i,9j)-\u00a3\u00f8,\u03c8(8i,&i,gi)]\n ef, (8j, aj,gi) -fp,\u03c8(Si,&i,gi)\n\\ L_{FlatNCE_backward}(B; \u03c6, \u03c8) = log 15)\n\\LFB (B; \u03c6, \u03c8) = \u2212 \u2211(ef\u03c6,\u03c8 (Si,&i,gi)) + \u2212 1)\u03a3\u03a3 (efo(8,049)2 (16)\n\nwhere we have highlighted the indices corresponding to positive and negative samples for clarity. Such a wide array of diverse losses not only allows us to test how well each particular loss performs but also shows the robustness of Contrastive Reinforcement Learning as a general method."}, {"title": "Architecture scaling.", "content": "While scaling architectures is commonplace in other areas of ML, it is still unclear the right way to scale RL models, mainly because RL objectives are primarily defined as regression rather than easier to train with deep architectures classification [33]. Recently, Zheng et al. [110] showed that CRL might benefit from deeper and wider architectures for offline CRL in pixel-based environments; we want to examine whether this also holds for online state-based settings."}, {"title": "Layer Normalization.", "content": "Recently, several design choices were thoroughly evaluated and incorporated into neural networks used for function approximation and policy in RL [78, 110]. Interestingly, one of the most potent design choices is layer normalization [3] for every hidden layer. The current understanding of layer normalization [4] is that it mitigates catastrophic overestimation by bounding the Q-values for actions even outside the offline dataset.\nWe evaluate these design choices in the following sections using the proposed codebase and bench- mark. We note that the primary goal of experiments is to establish a strong baseline on the proposed environments through a solid examination of well-known methods."}, {"title": "5.2 Experimental Setup", "content": "Our experiments use a suite of simulated environments described in Section 4.2. We evaluate CRL in an online setting, with a ratio of network updates to environment steps equal to 1:16, using a batch size of 256 and a discount factor of 0.99. For every environment, we sample evaluation goals from the same distribution as training ones and use a replay buffer of size 10M. We use 1024"}, {"title": "5.3 Energy Function Comparison", "content": "The performance of contrastive learning is sensitive to energy function choice [96]. This section aims to understand how different energy functions impact CRL performance. In particular, we evaluate five energy functions: L1, L2, L2 w/o sqrt, dot product and cosine. For every energy function, we use symmetric InfoNCE as a contrastive objective, with a 0.1 logsumexp penalty coefficient."}, {"title": "5.4 Contrastive losses comparison", "content": "Next, we study the efficacy of contrastive losses on the same set of environments as energy functions. The baseline contrastive loss used in the original CRL implementation was the NCE-binary objective. We compare it with five other objectives: InfoNCE, InfoNCE Backward, Symmetric InfoNCE, FlatNCE and FlatNCE Backward. We use a logsumexp penalty equal to 0.1 for every objective, as without this auxiliary objective, InfoNCE collapses.\nIn Fig. 5, we observe that the originally used NCE-binary objective, forward-backward, IPO and SPPO are the worst-performing ones among tested objectives. However, among the other InfoNCE-derived objectives, it is difficult to determine the best one, as their performance is similar. Interestingly, contrastive RL seems fairly robust to the choice of loss objective."}, {"title": "5.5 Scaling the Architecture", "content": "In this section, we examine how simply scaling up actor and critic neural networks in terms of depth and width impacts CRL performance. We evaluate the performance in four environments: Ant, Ant Soccer, and Ant U-Maze. We use the L2 energy function and Symmetric InfoNCE contrastive loss in every agent, with a logsumexp penalty equal to 0.1.\nIn Fig. 6, we report success rate averaged met- rics of the last 10M steps in runs of length 50M environment steps. There is a clear trend of improving performance with depth for hid- den layers with 256 and 512 neurons. How- ever, this trend breaks for 4 hidden layers with 1024 neurons. This indicates training destabi- lization while naively scaling architecture. In Section 5.6, we show that simply adding Layer Normalization [3] before every activation allows better scaling, even for deep and wide architec- tures."}, {"title": "5.6 Effect of layer normalization", "content": "As shown in Fig. 7, for state-based CRL, we also observe the positive effects of this intervention. The probability of method performance improve- ment increases with the width and depth of the neural network. Additionally, in Fig. 8, we plot the aggregated performance of the neural net- work with 4 hidden layers and 1024 neurons in each, which obtained low results while naively scaling in Section 5.5. We obtain the best-performing architecture from the tested ones by adding Layer Normalization."}, {"title": "5.7 Scaling the data", "content": "In Fig. 9, we report performance for different combina- tions of energy functions and contrastive objectives in a big data setup. In particular, we train the best-performing architecture from the previous section for 300 million envi- ronment steps in Ant and Ant U-Maze environments. We observe that the L2 energy function with InfoNCE objec- tive and 0.1 logsumexp penalty configuration outperforms all others by a substantial margin, resulting in a much higher success rate and time near the goal in both tasks. This indicates that only a subset of a wide array of design choices performs well in effectively scaling Contrastive RL."}, {"title": "5.8 Benchmark baselines", "content": "After establishing the best-performing vari- ant for key hyperparameters, we tested the performance of our method with configu- ration, including all of the improvements from previous sections. Final hyperparam- eter configuration for benchmark included Symmetric InfoNCE loss, L2 energy function with 0.1 logsumexp_penalty, full configura- tion can be found in Table 2. Each environ- ment was tested on 10 seeds, and the values reported in Fig. 3 are the Interquartile Mean (IQM) and standard error. Humanoid environ- ment was trained on goals sampled from the distance in range [1.0, 5.0] meters and evaluated on goals sampled from distance 5.0 meters. All other environments were trained and evaluated on identical environments."}, {"title": "6 Conclusion", "content": "In this paper, we proposed very fast bench- mark and baselines for goal-conditioned RL. The speed of the new benchmark enables us to rapidly study design choices for state-based CRL,"}, {"title": "Limitations.", "content": "Our benchmark environments and methods assume full observability and that goals can be sampled from a goal distribution during training rollouts. Future work should relax these assumptions, which are common in practical settings where self-supervised RL agents might be useful. We also only investigate the online setting. Future work could examine hardware-accelerated environments and algorithms in the offline setting."}, {"title": "A Experimental Details", "content": "A.1 Speedup comparison across various numbers of parallel environments\nWe present an extended version of the plot from Fig. 1, with additional experiments, as depicted on the left side of Fig. 10. Each experiment was run for 10M environment steps, and for the original repository, we varied the number of parallel actors for data collection, testing configurations with 4, 8, 16, and 32 actors, each running on separate CPU threads. Each configuration was tested with three different random seeds, and we present the results along with the corresponding standard deviations. The novel repository used 1024 actors for data collection.\nA notable observation from these experiments is the variation in success rates associated with different numbers of parallel actors. We hypothesize that this discrepancy arises due to the increased diversity of data supplied to the replay buffer as the number of independent parallel environments increases, leading to more varied experiences for each policy update. To further investigate, we conducted similar experiments using our method with varying numbers of parallel environments. The results are presented on the right side of Fig. 10. This observation, while interesting, is beyond the scope of the current work and is proposed as an area for further investigation.\nAdditionally, we repeated these experiments for the Ant U-Maze environment. The results are shown in Fig. 11. The speedup provided by our novel repository is clearly evident from the figure. Although the asymptotic performance of proposed and original CRL repositories differ, it is important to consider that the physical backends used for Ant-Umaze differ between the repositories, and Brax is still under active development. These discrepancies likely contribute to the observed differences in success rates. We anticipate that future versions will exhibit reduced performance discrepancies."}, {"title": "A.2 Energy functions", "content": "Figure 12 shows the results of different energy functions per environment, success rate and time near goal. Clearly, no single energy function performs well in all the tested environments, as, for instance, L1 and L2, which perform well in Ant environments, work poorly in Pusher. In addition, we observed high variability in every configuration performance, as indicated by relatively wide standard errors."}, {"title": "B Technical details", "content": "B.1 Environment details\nIn each of the environments there are a number of parameters that can change the learning process. A non-exhaustive list of such details for each environment is presented below."}, {"title": "B.2 Benchmark parameters", "content": "The parameters used for benchmarking experiments can be found in Table 2:\n\u00b9in 3 dimensions"}, {"title": "C Random Goals", "content": "Our loss in Eq. (2) differs from the original CRL algorithm [32] by sampling goals from the same trajectories as states during policy extraction, rather than random goals from the replay buffer. Mathematically, we can generalize Eq. (2) to account for either of these strategies by adding a hyperparameter a to the loss controlling the degree of random goal sampling during training.\nL_{policy} (B; 0) = \\sum_{i,j=1}^{K}(1 \u2212 a) \u222bAf\u00f8,\u03c8(Si, ai, gi) d\u03c0\u03b8(ai | Si, gi) \u222bAaf\u00f8,\u03c8(Si, ai, gj) d\u03c0\u03b8(ai | Si, 9j). (17)\n(1 \u2212 a)\u0192\u00a2,\u03c8(Si, Ai, gi) +af\u00a2,\u2084(Si, ai, 9j). (18)\nThe hyperparameter a controls the rate of counterfactual goal learning, where the policy is updated based on the critic's evaluation of goals that did not actually occur in the trajectory. We find that taking, a = 0 (i.e., no random goal sampling) leads to better performance, and suggest using the policy loss in Eq. (2) for training contrastive RL methods.\nWe use a DDPG-style policy extraction loss to learn a goal-conditioned policy with this critic [62]. Parameterizing the policy as some \u03c0\u03bf(\u03b1 | s, g), we get"}]}