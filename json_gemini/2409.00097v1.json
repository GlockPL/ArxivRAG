{"title": "Large Language Models for Disease Diagnosis: A Scoping Review", "authors": ["Shuang Zhou", "Zidu Xu", "Mian Zhang", "Chunpu Xu", "Yawen Guo", "Zaifu Zhan", "Sirui Ding", "Jiashuo Wang", "Kaishuai Xu", "Yi Fang", "Liqiao Xia", "Jeremy Yeung", "Daochen Zha", "Mingquan Lin", "Rui Zhang"], "abstract": "Automatic disease diagnosis has become increasingly valuable in clinical practice. The advent of large language models (LLMs) has catalyzed a paradigm shift in artificial intelligence, with growing evidence supporting the efficacy of LLMs in diagnostic tasks. Despite the growing attention in this field, many critical research questions remain under-explored. For instance, what diseases and LLM techniques have been investigated for diagnostic tasks? How can suitable LLM techniques and evaluation methods be selected for clinical decision-making? To answer these questions, we performed a comprehensive analysis of LLM-based methods for disease diagnosis. This scoping review examined the types of diseases, associated organ systems, relevant clinical data, LLM techniques, and evaluation methods reported in existing studies. Furthermore, we offered guidelines for data preprocessing and the selection of appropriate LLM techniques and evaluation strategies for diagnostic tasks. We also assessed the limitations of current research and delineated the challenges and future directions in this research field. In summary, our review outlined a blueprint for LLM-based disease diagnosis, helping to streamline and guide future research endeavors.", "sections": [{"title": "Introduction", "content": "Automatic disease diagnosis involves inputting clinical data into algorithms that analyze patterns and generate diagnoses with minimal or no human intervention. Its significance in clinical scenar- ios is multifaceted. First, it enhances diagnostic accuracy, supports physicians in clinical decision- making, and addresses disparities in healthcare access by providing more high-quality diagnostic services. Second, it increases diagnostic efficiency, particularly in contexts of population aging and clinician shortages, where the complexity of diagnosis demands significant time even from experi- enced clinicians. Finally, it offers patients greater convenience through online diagnostic services, facilitating early diagnosis and reducing the delays associated with traditional clinical visits.\nAdvancements in artificial intelligence have driven the development of automated diagnostic systems through two stages. Initially, machine learning techniques such as SVM and decision trees were employed for disease classification1;2, which typically involved four steps: data pro- cessing, feature extraction, model optimization, and disease prediction. With larger datasets and sufficient computational power, deep learning methods later dominated the development of diag- nostic tasks 3;4. These approaches leveraged deep neural networks (DNNs), including convolutional neural networks, recurrent neural networks, and generative adversarial networks7, enabling end- to-end feature extraction and model training. For example, a convolutional DNN with 34 layers achieved cardiologist-level performance in arrhythmia diagnosis. However, these models require extensive labeled data for training and are typically task-specific, limiting their adaptability to other tasks 5;8.\nIn recent years, the paradigm of artificial intelligence has shifted from traditional deep learning to the emergence of large language models (LLMs). Unlike supervised learning, LLMs, such as generative pre-trained transformers (GPT) and LLaMA, are generative models pre-trained on vast amounts of unlabeled data through self-supervised learning. These models, typically comprising billions of parameters, excel in language processing and adapt to various tasks. To date, LLMs have demonstrated superior performance in clinical scenarios, including question answering 10,"}, {"title": "", "content": "information retrieval\u00b9\u00b9, and clinical report generation 12;13.\nRecently, increasing numbers of studies have verified the effectiveness of LLMs for diagnostic tasks. For instance, PathChat14, a vision-language generalist LLM fine-tuned on hundreds of thou- sands of instructions, achieved state-of-the-art performance in human pathology. Med-MLLM13, a multimodal LLM pre-trained and fine-tuned on extensive medical data, including chest X-rays, CT scans, and clinical notes, demonstrated notable accuracy in COVID-19 diagnosis. Addition- ally, Kim et al. 15 employed GPT-4 with prompt engineering and found it surpassed mental health professionals in identifying obsessive-compulsive disorder.\nAlthough this research field has drawn wide attention, many key questions remain under- explored. For instance, what diseases and LLM techniques have been investigated for diagnostic tasks? How do researchers use LLMs to analyze various types of medical data for disease di- agnosis? What evaluation methods are appropriate for assessing performance? While numerous review papers on LLMs in medicine have been presented 16;17;18;19;20, they typically provide a broad overview of various clinical applications without underscoring disease diagnosis. For instance, Pressman et al. 21 offered a comprehensive summary of potential clinical applications of LLMs, in- cluding pre-consultation, treatment, postoperative management, discharge, and patient education. None of these surveys address the nuances and challenges of applying LLMs to disease diagnosis or answer the aforementioned questions, highlighting a critical gap in the research.\nThe primary aim of this review is to provide a comprehensive analysis of using LLMs in disease diagnosis. This involved examining various disease types, associated organ systems, clinical data, LLM techniques, and evaluation methods from existing works. Besides, we provided guidelines for data pre-processing, selecting appropriate LLM techniques, and employing suitable evaluation strategies for diagnostic tasks. Additionally, we analyzed the limitations of current studies and"}, {"title": "Results", "content": "Overview of the Scope\nThis section presents the overview of the diseases, clinical data, and LLMs covered in this study.\nFigure 2 illustrates the disease-associated organ systems, clinical data, data modalities of the in- vestigated LLMs, and the relevant LLM techniques. Notably, LLMs encompass a range of data modalities, including text, images, video, audio, tabular data, and time series. Figure 3 shows the publication trend over time and the statistics of data privacy and evaluation methods in this review."}, {"title": "Prompt-based Disease Diagnosis", "content": "Prompt engineering involves the creation of a prompting function designed to efficiently execute specific downstream tasks using LLMs71. A tailored prompt typically comprises four components: instruction (specifying the task), context (defining the scenario or domain), input data (identifying the data to be processed), and output indicators (directing the model on the desired style or role).\nWe identified five distinct prompt engineering techniques, which can be categorized into two main types: hard prompts and soft prompts. Hard prompts further involved zero-shot, few-shot, Chain-of-Thought (CoT), and self-consistency prompting. These are static, interpretable instruc- tions written in human language, making them particularly effective when both inputs and outputs are well-structured and clearly defined 72. Soft prompts are continuous vector embeddings gener- ated by a small, trainable model before being fed into a LLM. This approach, known as prompt tuning, encodes input data into task-specific embeddings, providing the LLM with context tailored to the task 73.\nAmong the 232 reviewed studies employing prompt engineering techniques, zero-shot prompt- ing was the most frequently employed technique (N=176), significantly surpassing the use of few- shot prompting (N=20). CoT prompting (N=25) was occasionally integrated into zero-shot or few-shot scenarios 32;34;35. Two studies implemented soft prompting to encode multimodal EHR data (e.g., medical images, clinical notes, and lab results) by integrating external domain knowl-"}, {"title": "", "content": "edge from pre-trained language models 36;37. Only one study utilized self-consistency prompting to enhance the reliability and robustness of LLM outputs 27.\nAmong reviewed studies, prompt engineering techniques were predominantly applied to a sin- gle modality of text data (N=173), while 59 studies involved multimodal data, incorporating struc- tured data, images, time series data, audio, and video. In studies involving multimodal data, most focused on generating differential diagnosis lists, accompanied by explanations or justifications in- formed by medical imaging data, such as X-rays 30;74;75, magnetic resonance imaging (MRIs) 76;77, pathology images 14, and optical coherence tomography (OCT) images78, along with relevant text- based contextual information. Visual Question Answering was another prominent application, pri- marily aiming to enhance the accuracy and robustness of medical image diagnoses by responding to questions related to these images79;80. Less common tasks driven by text prompts included im- age classification 81;82, segmentation 83, and captioning 36;84. For instance, Ono et al. 31 demonstrated that the few-shot prompting achieved performance comparable to the convolutional neural network model YOLOv8 in classifying histopathology images to identify specific lesion types. Time se- ries data from wearable sensors were analyzed either through direct abnormality reporting 25, or by integrating these data with contextual information in tabular format (e.g., user demographics, health-related definitions) for downstream tasks such as sleep disorder prediction27. Only one study employed audio data, in combination with video data, for recognizing autism-related behav- iors 26\nAlthough the unimodal studies solely focused on the text data, significant variability was ob- served in the ML tasks investigated through prompt engineering techniques. The primary ap- plications included: 1) text-based question answering, such as providing detailed explanations of clinical profiles and radiology reports to enhance diagnostic confidence 10;85; 2) text classifi-"}, {"title": "Retrieval-Augmented LLMs for Diagnosis", "content": "To address hallucination issues in LLMs and update their stored medical knowledge without need- ing re-training, recent studies 38;39;40;41;42;43;44;45;46;47;48;49;58;88;89;90;91;92;93;94 have incorporated exter- nal medical sources such as corpus ;41;42;43;44;45;58;88;89;90;91;92;93;94, databases 46;47;48;49, and knowledge graph 38;39;40 to enhance the LLMs' diagnostic capabilities. Based on the type of diagnostic data,\nRetrieval-Augmented Generation (RAG) for diagnosis can be categorized into text-based, image- based, time-series-based, and multimodal-based augmentations.\nFor text-based RAG, most research41;42;46;47;58;88;89;90;91;92;93;94 employs a fundamental retrieval approach where externally sourced knowledge is encoded using sentence Transformers (e.g., Ope- nAI text-embedding-ada-002) into vector representations used as retrieval sources. Queries are similarly encoded, allowing the system to identify and fetch the most relevant knowledge by calcu- lating the similarity between query vectors and source vectors. This combined information is then fed into LLMs using specially designed prompts to generate diagnostic results. However, some approaches like identifying key query entities to retrieve relevant knowledge graphs 38 or assigning importance to entities in medical records 39 have shown promising results in refining the process by accurately pinpointing potential diseases. In image-based data, the common practice43;44;48 in- volves extracting features from input images, describing them in text, and then applying text-based enhancement methods. Ranjit et al.49 directly computes similarities between image and text fea- tures using multimodal models to retrieve similar documents. For time-series RAG, Zhu et al. 40"}, {"title": "Fine-tuning LLMs for Diagnosis", "content": "Fine-tuning a LLM typically involves two key stages: Supervised Fine-Tuning (SFT) and Rein- forcement Learning from Human Feedback (RLHF). In the SFT stage, the model is trained using task-specific instruction-response pairs. During this phase, the LLM learns to process instructions and information from various modalities, producing the desired response as output. The RLHF stage focuses on aligning the LLM with human preferences. Here, the model is refined to be more helpful, honest, and harmless 95, ensuring it aligns better with human values and expectations.\nMedical SFT enhances the in-context learning, reasoning, planning, and role-playing capa- bilities of LLMs, leading to improved diagnostic performance. During this process, inputs from various data modalities are integrated into the LLM's word embedding space. Following the ap- proach outlined in LLaVA%, visual information is first converted into visual token embeddings using an image encoder and a projector. These embeddings, which match the dimensionality of language token embeddings, are then fed into the LLM for end-to-end training. In this review, approximately half of the studies focused on conducting SFT using medical texts for diagnostic purposes 50;60;61;67;97;98;99;100;101, while the other half combined both medical texts and images to enhance disease diagnosis 54;68;69;102;103;104;105;106;107. A few studies also explored the detection of diseases from medical videos 56;57;58, where video frames were sampled and transformed into vi- sual token embeddings. To perform SFT effectively, it is crucial to collect high-quality responses"}, {"title": "", "content": "to task-specific instructions. These instructions should be well-defined and diverse, covering a wide range of scenarios to ensure comprehensive training.\nRLHF methods could be divided into two categories: online-based methods and offline-based methods. PPO-based RLHF, a key process for the success of ChatGPT 108, is the representative and most popular online alignment method, where we first fit a reward model to datasets of prompts and human preferences over responses, then use PPO109 to update the LLM to maximize the learned reward model. Some explorations show PPO-based RLHF could effectively improve the diag- nostic ability of medical LLMs60;61;62. However, the overall performance of PPO-based RLHF highly relies on the quality of the reward model, which is expected to give accurate rewards to LLM responses, and several works demonstrated that the reward model could suffer from issues like over-optimization110 and shifting form initial data distribution111. Meanwhile, the training of RL is unstable and not easy to control112. Offline RLHF methods like DPO113 cast RLHF as optimizing a simple classification loss, eliminating the need for a reward model. These methods are also more stable and computationally lightweight and have proven useful in medical LLMs alignment 59;64;98. To conduct RLHF, a high-quality dataset of prompts and responses with human preferences is crucial to train a well-calibrated114 reward model for online RLHF or ensure the better convergence of DPO like offline RLHF algorithms115, whether from human experts 108 or powerful AI models 116, like GPT-4.\nAs the size of large models increases, their capabilities also become stronger. Typically, this leads us to choose larger models to ensure that they have a good foundational ability to be adapted to downstream tasks. However, as the model size increases, full training becomes increasingly impractical, requiring a lot of GPUs. Parameter-Efficient Fine-Tuning (PEFT) alleviates this issue by reducing the number of parameters that need to be fine-tuned in the model. PEFT can be divided"}, {"title": "", "content": "into two categories: one involves adding additional adapters117 (changes on model structure), and the other is Low-Rank Adaptation (LoRA)118, injecting trainable rank decomposition matrices into each layer of the model (no model structural changes). LoRA is more popular because it does not introduce additional inference latency. This is also true in the medical field: many researchers opt to use LoRA for fine-tuning due to limited computational resources 50;63;64;65;119;120;121.\nPre-training LLMs for Diagnosis\nLLMs are initially pre-trained on extensive text corpora to perform next-token prediction. During this phase, the model learns the structure of language and acquires a vast amount of knowledge about the world. When pre-trained on medical texts, LLMs gain foundational medical knowl- edge, which proves valuable when adapting them for various downstream medical tasks, including medical diagnosis. In this review, five studies perform text-only pretraining on the LLMs from different sources 66;67;122;123;124, such as clinical notes, medical QA texts, dialogues, and Wikipedia. Moreover, eight studies injected medical visual knowledge into multimodal LLMs via pretrain- ing 54;68;69;70;124;125;126. Chen et al. 68 and Wang et al. 124 pretrained the model on VQA data, where Chen et al. 68 used an out-of-shelf multi-model LLM to reformat image-text pairs from PubMed as VQA data points to train their LLM. To improve the quality of the image encoder, pretraining tasks like reconstructing images at tile-level or slide-level, and aligning similar images or image-text pairs 54\nare common choices."}, {"title": "Evaluation Strategy", "content": "Evaluating diagnostic performance is of crucial importance. Existing evaluation methods for di- agnosis generally fall into three categories: automatic evaluation, human evaluation, and LLM evaluation (shown in Table 2). An overview of the pros and cons of the evaluation strategies is depicted in Figure 6.\nMost studies assessed diagnostic effectiveness using automatic metrics, which can be broadly categorized into three types. The first type includes classification-based metrics such as accuracy, precision, and recall, which are suitable for single-disease prediction. For example, Liu et al. 13 adopted AUC, accuracy, and F1 score to evaluate COVID-19 diagnosis effectiveness. The sec- ond type is generally used in multi-label scenarios, where predictions involve multiple potential diagnoses, including top-k accuracy and top-k precision. For instance, Tu et al. 131 utilized top- k accuracy to measure the percentage of correct diagnoses appearing within the top-k positions of the diagnosis list. The third type applies to risk prediction tasks, where mean absolute error (MAE) or mean squared error (MSE) measures the deviation between predicted values and the ac-"}, {"title": "Discussion", "content": "This section presents notable findings from the included studies, highlighting key challenges and potential future research directions. Our review revealed that most studies utilized LLMs for dis- ease diagnosis through prompt learning. The phenomenon might be explained as follows. Firstly, it requires minimal data; zero-shot and few-shot prompts enable the development of diagnostic systems with just a few dozen examples 22;156. Secondly, prompt-based methods are user-friendly and require minimal setup, making them accessible to researchers with limited machine-learning expertise. Additionally, they significantly reduce computational overhead, making implementa- tion feasible on standard hardware. Furthermore, when used appropriately, large-scale LLMs like GPT-4 or GPT-3.5, which own extensive medical knowledge, demonstrate fair performance across"}, {"title": "", "content": "ious diagnostic tasks 150;156.\nWe have summarized the advantages and limitations of mainstream LLM techniques in Fig- ure 5. The selection of LLM techniques for developing diagnostic systems depends on the quantity and quality of available data. Specifically, prompt engineering is highly flexible and effective when annotated data is limited. For instance, zero-shot prompting allows models to perform di- agnosis without annotated data, while few-shot prompting requires only a few instructions and examples 150;156. To effectively use RAG for diagnosis, a comprehensive and high-quality external knowledge base is essential. This knowledge base can consist of databases41, corpora91, clinical guidelines 157, or knowledge graphs 158 from which LLMs can retrieve information during infer- ence. Effective fine-tuning necessitates a well-annotated, domain-specific dataset that includes labeled examples reflecting the target diagnostic tasks, such as annotated clinical notes or medical images, and a substantial number of samples 13. Pre-training requires extensive and diverse datasets that cover a broad range of medical knowledge, incorporating unstructured text (e.g., clinical notes, medical literature) or structured data (e.g., lab test results) 159;160. The quality and diversity of the datasets are crucial for establishing the model's foundational knowledge and its ability to general-"}, {"title": "", "content": "ize across various medical contexts.\nWhile pre-training and fine-tuning would achieve promising performance and reliability 13;125, they demand significant resources, including computational power and data collection. In con- trast, not all applications require high-accuracy disease diagnosis, such as large-scale screening 161, mobile health alerts 162. or diagnostic education163. Balancing the trade-off between accuracy and cost-effectiveness varies by scenario. In summary, the analyses presented in Figure 5 guide users in selecting appropriate LLM techniques for disease diagnosis based on available resources.\nDespite the progress in LLM-based methods for clinical diagnosis, this scoping review iden- tifies several barriers that impede their clinical utility. A notable limitation is the lack of studies integrating diverse multi-modal data for diagnosis 164, such as text, medical images, time series, and other modalities. For example, Deng et al. 165 developed a multi-modal LLM incorporating text, images, video, and speech for autism spectrum disorder screening. However, only a small subset of studies combined diverse modalities. This discrepancy contrasts with real-world diagnostic sce- narios, where comprehensive patient information spans multiple data modalities 166, particularly for complex conditions affecting multiple organs. Therefore, future research should emphasize fusing multi-modal data to simulate real-world scenarios.\nAnother limitation is that most studies implicitly assume the collected patient information is sufficient for accurate disease diagnosis. Nevertheless, this assumption usually hardly holds, par- ticularly in initial consultations or with complicated diseases, and using incomplete data would likely cause misdiagnosis 167;168. In practice, clinical information gathering is an iterative process, beginning with the collection of initial patient data (e.g., subjective symptoms), narrowing down potential diagnoses, and then conducting medical examinations for further data collection and dis- ease screening 169. This process typically requires extensive domain expertise from experienced"}, {"title": "", "content": "clinicians. To address these challenges, an increasing number of studies are exploring diagnostic conversations that collect relevant patient information through multi-round dialogues 170;171. For ex- ample, AIME utilized LLMs for clinical history-taking and diagnostic dialogue131, while MEDIQ asked follow-up questions to gather essential information for clinical reasoning 155. Therefore, fu- ture research should integrate the recognition of incomplete data into diagnostic models or develop advanced methods for automatic diagnostic queries.\nSome barriers lie in the decision-making step. While many studies emphasize diagnostic accu- racy, they usually ignore human-centric perspectives such as model interpretability, patient privacy, safety, and fairness 16;172;173. Specifically, providing diagnostic predictions alone is insufficient in clinical scenarios, as the black-box nature of LLMs often undermines trust 144;149 Accordingly, it is essential to provide interpretative insights into the diagnoses 149. For example, Dual-Inf is a prompt-based framework that not only provides potential diagnoses but also explains the rationale behind them 151. Regarding privacy, adherence to regulations like the Health Insurance Portability and Accountability Act (HIPAA) and the General Data Protection Regulation (GDPR) is crucial, such as the de-identification of sensitive information 11;174. To date, only a few works have in- vestigated the issue93;175. For instance, SkinGPT-4 is a dermatology diagnostic system designed for local deployment to protect user privacy 175. Fairness is another concern, ensuring patients are not discriminated against based on gender, age, or race 172. Research addressing the fairness issue in LLM-based diagnosis remains limited176;177. In short, future research should integrate these human-centric perspectives into diagnostic systems to address these critical issues.\nIn terms of technical aspects, integrating multi-modal data for disease diagnosis has garnered considerable attention178. However, several challenges remain, including the need to eliminate data noise 179, fuse heterogeneous data from various modalities 180, and perform efficient learning."}, {"title": "", "content": "Additionally, many domain-specific LLMs are constrained by smaller parameter scales compared to general-domain models 145;181 This phenomenon might arise from the substantial corpus and computational resources required for training large medical specialists, which are often unavail- able131. Nevertheless, pre-training on extensive medical corpora can enrich LLMs with more med- ical knowledge, thereby boosting their accuracy for rare diseases and enhancing medical reason- ing 182;183. Therefore, it is promising to investigate integrating more clinical knowledge either with pre-training or fine-tuning, thus alleviating the hallucination issues. Additionally, employing mul- tiple specialist models to handle complex diseases offers a practical simulation of interdisciplinary clinical discussions involving multiple organ systems93;184 and is worth further exploration.\nAnother critical area is the development of diagnostic systems. Many studies utilized private datasets, which are often inaccessible due to privacy concerns157;185. However, the advancement of diagnostic systems necessitates a greater availability of public data. Additionally, the scarcity of annotated data poses a significant challenge to the development of this field. This is because well-annotated datasets enable exploiting automatic metrics for evaluation, reducing the need for extensive human effort in performance assessment151. Therefore, constructing and releasing anno- tated datasets would significantly contribute to the research community. Moreover, performance evaluation should also be highlighted. Currently, there is no standardized guideline for evaluating diagnostic model performance, particularly regarding human-centric metrics 28;148;186. A generic principle is that metrics from different aspects, such as effectiveness, robustness, reliability, and explainability, should be considered.\nExploring the interaction between clinicians, patients, and diagnostic systems presents a promis-"}, {"title": "", "content": "cians 85;188. Future studies could explore how the effective application of diagnostic algorithms can further enhance clinical accuracy. Additionally, diagnostic systems should be designed to incorpo- rate feedback from medical experts, facilitating continuous refinement and adaptation.\nIn conclusion, our study provides a comprehensive review of LLM-based methods for disease diagnosis. Our contributions are multifaceted. First, we have summarized the application scenar- ios, associated human organ systems, the relevant clinical data, the employed LLM techniques, and evaluation methods within this research domain. Second, we compared the advantages and limitations of mainstream LLM techniques and evaluation methods, offering recommendations for developing diagnostic systems based on varying user demands. Third, we identified intriguing phenomena from the current studies and provided insights into their underlying causes. Lastly, we analyzed the current challenges and outlined the future directions of this research field. In summary, our review presented an in-depth analysis of LLM-based disease diagnosis, outlined its blueprint, inspired future research, and helped streamline efforts in developing diagnostic systems."}, {"title": "Method", "content": "Search Strategy and Selection Criteria\nThis scoping review is reported in accordance with the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines, as shown in Figure 1. We performed the liter- ature search from various resources to find relevant articles published between 1 Jan 2019 and 18 July 2024. We searched seven electronic databases, including PubMed, CINAHL, Scopus, Web of Science, Google Scholar, ACM digital library, and IEEE Xplore. The search terms were selected based on consensus expert opinion and used for each database (see Supplementary Data 1)."}, {"title": "", "content": "We performed a two-stage screening process to focus on LLMs for human disease diagno- sis. The first stage involved using the title and abstract for paper exclusion. The criterion was as follows: (a) articles were not published in English; (b) articles irrelevant to LLMs or foundation models; and (c) articles irrelevant to the health domain. The second stage was full-text screen- ing, emphasizing using language models for diagnosis-related tasks. We excluded review papers, papers without explicitly used for disease diagnosis, and foundation models not involving text modality (e.g., visual foundation models).\nData Extraction\nInformation garnered from the articles consists of four categories. (1) Basic information, including title, published venue, published time (year and month), and research task (e.g., diagnosis, report generation, or question-answering). (2) Data-related information, including data sources (conti- nents), dataset name, modality (e.g., text, image, video, or text-image), clinical specialty, disease name, data availability (i.e., private or public data), and data size. (3) Model-related information, comprising model name, base LLM type, parameter size, LLM technique type, model availability, and preferred language (e.g., English or multilingual). (4) Evaluation, which includes evaluation schema (e.g., automatic or human evaluation) and evaluation metrics (e.g., accuracy, F1, AUC).\nSee Supplementary Table 1 for a detailed description of the extracted data.\nData synthesis\nWe synthesized insights from the data extraction to highlight the principal themes in LLM-based disease diagnosis. Firstly, we explored the scope of our review, covering disease types, associated organ systems, data modalities, and LLM techniques. We then summarized the various LLM-"}, {"title": "", "content": "based methods and evaluation strategies, analyzing their strengths and weaknesses, and offering targeted recommendations. Diving deeper into technical aspects, we detailed modeling approaches such as prompt-based methods, RAG, fine-tuning, and pre-training. We also examined the chal- lenges faced by current research and outlined potential future directions. In summary, our syn- thesis encompassed a broad range of perspectives, assessing studies across data, LLM techniques, performance evaluation, and application scenarios, which are in line with established reporting standards."}]}