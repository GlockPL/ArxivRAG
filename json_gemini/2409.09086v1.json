{"title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language Models on a Single GPU", "authors": ["Zhenyu Ning", "Jieru Zhao", "Qihao Jin", "Wenchao Ding", "Minyi Guo"], "abstract": "Multimodal Large Language Models (MLLMs) are distinguished by their multimodal comprehensive ability and widely used in many real-world applications including GPT-4O, autonomous driving and robotics. Despite their impressive performance, the multimodal inputs always incur long context. The inference under long context requires caching massive Key and Value states (KV cache) of previous tokens, which introduces high latency and excessive memory consumption. Due to this reason, it is challenging to deploy streaming inference of MLLMs on edge devices, which largely constrains the power and usage of MLLMs in real-world applications. In this paper, we introduce Inf-MLLM, an efficient inference framework for MLLMs, which enable streaming inference of MLLM on a single GPU with infinite context. Inf-MLLM is based on our key observation of the attention pattern in both LLMs and MLLMs called \"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM maintains a size-constrained KV cache by dynamically caching recent tokens and relevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel approach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM enables multiple LLMS and MLLMs to achieve stable performance over 4M-token long texts and multi-round conversations with 1-hour-long videos on a single GPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than existing methods such as StreamingLLM and 2x speedup than H2O.", "sections": [{"title": "Introduction", "content": "Multimodal Large Language Models (MLLMs) (Gao et al. 2024; Alayrac et al. 2022; Li et al. 2022; Team et al. 2023) have been introduced to empower Large Language Models (LLMs) with new capabilities to process information of different modalities such as image, video, audio, etc (Liu et al. 2024). Video applications, which typically involve lengthy sequence lengths, exemplify the remarkable multimodal reasoning capabilities of MLLMs. However, they also result in significant memory consumption and a decline in model performance when the context length exceeds a certain threshold. These issues are exacerbated in scenarios of streaming inference, as shown in Fig. 1, where multimodal inputs are streamed in and MLLMs have to deal with long context or multi-round conversions continuously.\nEfficient streaming inference is crucial for many real-world applications. For instance, OpenAI's new flagship model, GPT-40 (OpenAI 2024), demonstrates efficient inference for video, audio, and text streams. However, it is not open-source and does not facilitate streaming inference on a local device without cloud access. Accessing a cloud-scale model through APIs can raise privacy concerns and incur additional costs. For other edge applications like robotics, cloud-scale model is not always accessible, making streaming inference on edge important. However, it is challenging to deploy MLLM in such real-world edge applications due to limited memory budget and high efficiency requirement.\nIn this paper, we focus on efficient streaming inference of MLLMs on a single GPU and summarize the challenges in four different aspects as follows.\nC1: Quadratic computation complexity: The computation complexity of attention is quadratic to the KV cache size, and retrieving KV states incurs additional memory accesses (Dao et al. 2022; Sukhbaatar et al. 2019; Choromanski et al. 2020). As the sequence length grows, the decoding speed"}, {"title": "", "content": "will decrease to an intolerable extent, especially for multi-round conversation and long video understanding.\nC2: Memory consumption: For MLLMs, a large KV cache is maintained to avoid re-computation during inference, which scales linearly with the sequence length. This can result in high memory consumption (Pope et al. 2023). The problem is even more severe for multimodal inputs which are transformed into a large number of tokens. For example, a several-minute-long video can be converted into thousands of tokens (Jin et al. 2024; Li, Wang, and Jia 2023).\nC3: Context length limitation: Since most MLLMs are fine-tuned with pre-trained LLMs, they are constrained by the context window. When sequence length exceeds the length of the training text, the performance degrades soon, which is unacceptable in real-world applications. Therefore, the techniques of length extrapolation are required to deal with over-long inputs (Press, Smith, and Lewis 2022; Su et al. 2024).\nC4: Long-term memory: The ability to capture long-term dependency is critical for streaming inference of MLLMs. However, it is hard to achieve due to the lack of high-quality multimodal datasets (Hudson and Manning 2019; Maaz et al. 2023; Li et al. 2023b) and cost of fine-tuning (Yu et al. 2024). Existing video QA datasets (Xu et al. 2017a,b; Li et al. 2024a, 2023a) contain several-second-long videos and short conversations, which cannot enhance the long-term reasoning capability of MLLMs during finetuning.\nPrior studies, such as window attention (Beltagy, Peters, and Cohan 2020; Jiang et al. 2023; Liu et al. 2022; Dong et al. 2022), H2O (Zhang et al. 2024b), Keyformer (Adnan et al. 2024) and StreamingLLM (Xiao et al. 2024), improve the inference performance of LLMs, but none of them can handle all the challenges simultaneously, especially for the streaming inference of MLLMs. Although H2O and StreamingLLM enable LLMs to work on super long texts, they either achieve unstable perplexity on long texts or fail on tasks that demand long-term memory. Details can be seen in Section 2. Moreover, existing methods focus on pure text inputs and cannot be applied to MLLMs with multimodal inputs directly.\nTo this end, we propose Inf-MLLM, an innovative in-ference framework that enables efficient and high-quality streaming inference of MLLMs on a single GPU with infinite text and video streams as input. We propose an effective KV cache eviction mechanism based on our key observation that there exist critical tokens with high attention scores, like a series of saddle points in non-linear curves. Borrowing the concept of saddle points in mathematics, we call these tokens as attention saddles. By caching the most relevant tokens and evicting less important KV states of irrelevant tokens, Inf-MLLM improves decoding speed (C1), reduces memory usage (C2), and enables existing MLLMs to support much longer sequence length than its original maximum context length without re-training and fine-tuning (C3). We observe that simply aggregating attention scores for each token causes the summation of scores leaning towards earlier tokens in the sequence, making it hard to select real relevant tokens. To solve this issue, we further introduce attention bias to ensure that the KV cache continuously evicts earlier tokens and accommodates new attention saddles. In this"}, {"title": "", "content": "way, Inf-MLLM can preserve the most relevant tokens dynamically and capture long-term dependency during streaming inference (C4). Our contributions are listed as follows.\n\u2022 We discover the phenomenon of attention saddles and summarize features of attention patterns on MLLMs. Based on it, we propose an effective KV cache eviction mechanism to reduce memory usage and enable efficient streaming inference of MLLMs on a single GPU.\n\u2022 We introduce attention bias to update KV cache for long context reasoning. It helps Inf-MLLM to handle streams of texts and videos and capture long-term dependency.\n\u2022 Experiments show that Inf-MLLM facilitates efficient and high-quality streaming inference for multi-round conversations and video clips on edge devices."}, {"title": "Related Works", "content": "KV Cache Eviction Previous works maintain a size-contrained KV cache by evicting KV states of unimpor-tant tokens. Window attention (Beltagy, Peters, and Cohan 2020; Jiang et al. 2023; Liu et al. 2022; Dong et al. 2022) caches recent tokens to reduce computation complexity and memory consumption. However, the model performance degrades once the sequence length exceeds the cache size. H2O (Zhang et al. 2024b), Keyformer (Adnan et al. 2024) and SnapKV (Li et al. 2024b) reduce memory us-age with their KV eviction strategy, and H2O enables LLMs to handle texts with infinite length. However, The perplexity is not satisfying on some long text benchmarks due to the improper eviction of important tokens. StreamingLLM (Xiao et al. 2024) enables LLMs to deal with infinite length by caching the KV states of initial and recent tokens. Al-though StreamingLLM maintains stable perplexity as the se-quence increases in multi-round conversation, it is restricted by its attention window and fails on tasks that demand long-term memory and extensive data dependency, such as long document question-answering and long video question-answering. All these methods deal with pure text inputs.\nKV Cache Compression There exist methods focusing on compressing KV cache. For instance, Transformer-XL (Dai et al. 2019) splits the entire context into shorter segments with manageable sizes and introduces a recurrence mech-anism from RNN to connect adjacent segments. Compres-sive transformer (Rae et al. 2019) compresses past mem-ories for long-range sequence learning through pooling or convolution. Gear (Kang et al. 2024) applies dimensional-ity reduction and quantization to compress the KV cache. These methods providing another interesting direction to re-lieve the large memory consumption while achieving good model performance and efficient inference. However, the maximum context length is constrained by the context win-dow determined during pre-training. The compression tech-niques are orthogonal with KV eviction methods.\nRelative Position Encoding Relative position encoding enables LLMs to process longer context during inference while training on shorter texts. Two representative methods are Rotary Position Embeddings (RoPE) (Su et al. 2024) and"}, {"title": "", "content": "ALiBi (Press, Smith, and Lewis 2022). RoPE introduces a rotational encoding method to capture relative token posi-tions. ALiBi adds negative values to weaken the relevance between distant tokens, thus introducing relative position in-formation. Despite the improvement, their performance de-clines when the context length exceeds the context window constraint (Press, Smith, and Lewis 2022; Chen et al. 2023). Although recent works show better performance (Peng et al. 2024; Chen et al. 2023), this technique cannot relieve the high memory usage caused by increasing KV states."}, {"title": "Methodology", "content": "We visualize the attention maps of different layers and discover their specific patterns which can benefit the KV cache selection and eviction mechanism. Take the Chat-UniVi-7B (Jin et al. 2024) as an example, as shown in Fig. 2. The attention maps of MLLMs exhibit several features.\nPattern 1: recent tokens have high attention scores. Recent tokens located at the end of the sequence receive much attention. This is obvious since they are mostly related to the new generated tokens in both position and semantics.\nPattern 2: tokens converted from videos typically receive high attention scores. We observe an interesting phenomenon that a large number of attention scores are allocated to the region of tokens converted from input videos. For some Vision Language Models (VLMs), the initial tokens of the video even share over 40% of attention scores. We attribute the feature to the pre-training process, which requires the model to focus on the video content for question answering. However, since the position of videos is unknown beforehand in the multi-round conversation, an effective method is required to identify important visual tokens dynamically.\nPattern 3: positions with high attention scores appear as vertical lines. Besides recent tokens and key visual tokens, we find that high attention scores are also distributed among tokens scattered in the sequence. These tokens are attended to for dozens or hundreds of decoding steps, resulting in short or long vertical lines on the attention map. A special case is the attention sink named by StreamingLLM (Xiao et al. 2024), which refers to the initial tokens because they are endowed with huge attention score by SoftMax. Unlike StreamingLLM that only caches static initial tokens, Inf-"}, {"title": "", "content": "MLLM can dynamically identify the influential scattered tokens, including the initial tokens.\nPattern 4: high attention scores shift forward as the multi-round inference progresses. During streaming inference, we observe that high attention scores shift forward across conversation rounds. When a new prompt comes, the distribution of attention scores changes significantly, indicating that the attention window containing attended tokens should be updated correspondingly, especially when a new conversation round starts. Existing methods cannot capture the shifting feature and simply accumulate attention scores for KV selection, making large scores aggregate at earlier tokens while ignoring important newer tokens."}, {"title": "KV cache eviction and updating", "content": "Inf-MLLM employs an efficient KV cache eviction and up-dating mechanism, as shown in Fig. 3. During the streaming inference, when a new prompt comes, the QKV states of each token in the prompt are computed and stored in a re-trieval window with a length of L. Suppose there exist KV states of t earlier tokens from previous rounds of inference. Attention scores are computed by multiplying queries of the L new tokens with the KV states of t tokens in the cache and the KV states of the new tokens themselves, generating a  L * n matrix as shown in Fig. 3.\nTo identify attention saddles and evict KV states of irrelevant tokens, the attentions scores for each token are accumulated. Due to the continuity of vertical lines exhibited in attention patterns, we conduct a local sum within the retrieval window length (from the (t+1)-th to the n-th row) to improve computation efficiency, rather than aggregate along the complete attention matrix. The summation results are then normalized to avoid the accumulation of attention scores in earlier tokens. This is adaptive to the shifting feature of the attention pattern. After that, Inf-MLLM select the KV states of tokens with top-t highest normalized attention scores and evict less important KV states from the cache. The updated KV cache will be utilized for the following decoding process at the current round. Inf-MLLM invokes the KV cache eviction and updating mechanism at the beginning of each conversation round when a new prompt arrives, and does not evict tokens during decoding steps. Therefore, the cost of KV cache eviction and updating is negligible, and the inference speed of models is increased since fewer tokens are involved in computations after eviction."}, {"title": "Attention Bias", "content": "To further strengthen the ability of KV cache eviction, especially in long context processing and multi-round conver-sations, some issues need to be solved. Firstly, because of the SoftMax operation, the total sum of attention scores or weights maintains as one, despite the increasing sequence length and the growing number of tokens. This means that the weight of each token degrades gradually as the inference progresses, and the difficulty of identification for high-score tokens is exacerbated. Secondly, after several rounds of KV eviction, the distribution of attention scores becomes uneven among the remaining tokens and the attention score of some tokens can be enhanced due to multiple rounds of accumulation, as shown in Fig. 4. This phenomenon can prevent the identification of new attention saddles which are more relevant to the current conversation round, leading to improper KV eviction and even model collapse when the cache is almost not updated after rounds of inference on long context.\nTo update the KV cache continuously in streaming inference, we introduce attention bias to shift the attention focus to the newest context. We demonstrate its effects in Fig. 4, where attention bias can adjust the distribution of attention scores and enables the multi-round video conversation to continue. The attention bias is employed when identifying the attention saddles. After calculating the average attention scores in retrieval window, we add the attention bias to them to impel the KV cache to discard tokens retained long ago. With the higher attention bias, the KV cache tends to involve more new tokens and the model focuses more on the incoming tokens to adapt to streaming scenarios. With relatively lower attention bias, the KV cache can retain prior tokens longer and the model is able to capture longer-term dependency. Therefore, properly adjusting the attention bias can preserve long-term dependency while ensuring long context streaming inference."}, {"title": "Inf-MLLM Algorithm", "content": "In this section, we present the overview of our Inf-MLLM algorithm. We highlight its core idea in maintaining a size-restrained KV cache consisting of recent tokens and relevant tokens based on the attention patterns we have observed and"}, {"title": "Experiments", "content": "We evaluate Inf-MLLM on both LLMs and MLLMs with pure texts and texts/videos as input. We test on three prominent LLMs, namely Vicuna-7B (Chiang et al. 2023), Pythia-2.8B (Biderman et al. 2023) and LLaMA-2-7B-32K (To-gether 2023), and two state-of-the-art MLLMs for videos, namely Chat-UniVi-7B (Jin et al. 2024) and Flash-VStream-7B (Zhang et al. 2024a). All of these models are employed with relative position encoding such as RoPE (Su et al. 2024). For pure text inputs, we compare Inf-MLLM with typical baselines including window attention (Beltagy, Peters, and Cohan 2020), H2O (Zhang et al. 2024b) and StreamingLLM (Xiao et al. 2024). For video and text in-puts, we evaluate the streaming inference performance of MLLMs empowered with and without Inf-MLLM. All experiments are conducted on a single NVIDIA 4090D GPU or NVIDIA ORIN GPU, demonstrating the powerful capa-bility of Inf-MLLM on resource-constrained devices."}, {"title": "LLM Perplexity on Super Long Texts", "content": "We first compare Inf-MLLM with previous methods in LLM perplexity on long text inputs, as shown in Fig. 5. The maximum context lengths of the tested LLMs, Vicuna-7B, Pythia-2.8B, and LLaMA-2-7B-32K, are 2K, 2K and 32K, respectively. After applying KV cache eviction strategies, the context length can be extended. We can see that for context length up to 20K, Inf-MLLM reaches better perplexity than window attention, H2O and StreamingLLM. Note that H2O only supports Vicuna-7B and the perplexity of window attention increases rapidly when exceeding the 2K limit on LLaMA-2-7B-32K. We also evaluate Inf-MLLM on texts"}, {"title": "", "content": "with up to 4 million tokens, as shown in Fig. 5. The results show that the LLMs empowered with Inf-MLLM presents stable perplexity on super long text inputs, which largely surpass the maximum context length constraint."}, {"title": "Long-term Memory Capability", "content": "To evaluate the capability of long-term memory, we de-sign a multi-round question-answering benchmark based on the LongEval-LineRetrieval dataset (Li* et al. 2023). The dataset involves 300 prompts each of which con-tains multiple lines of texts in the format of \"The REGIS-TER_CONTENT in line index is number\", and requires models to answer the number given index at the end of the prompt. We vary the distance between the final question and the corresponding answer line to evaluate the ability of long-term memory.\nWe select StreamingLLM (abbreviated to StrLLM) as the baseline since it outperforms other previous methods on long text inputs. As shown in Table 2, Inf-MLLM reaches higher accuracy across all token distances and LLMs. The superiority is particularly significant on LLaMA-2-7B-32K, where we set the attention bias to 0.0001. Inf-MLLM main-tains close to 100% accuracy while StreamingLLM drops to less than 50% at different token distances. The improve-ment can be attributed to (i) the relevant tokens broaden the span of attention window and (ii) the attention bias compensates the reshaped attention scores. Therefore, Inf-MLLM presents stable streaming performance with longer-term memory compared to existing methods."}, {"title": "Multi-round Video Question-answering", "content": "Inf-MLLM enables efficient streaming inference for MLLMs on overlong multimodal inputs such as videos. We test Inf-MLLM on two state-of-the-art Vision Lan-guage Models (VLMs), Chat-UniVi and Flash-VStream, us-ing three popular video question-answering datasets includ-ing MSVD-QA (Xu et al. 2017b), MSRVTT-QA (Xu et al. 2017a) and TGIF-QA (Jang et al. 2017). We formulate three multi-round video question-answering benchmarks by con-catenating each sample in three datasets.\nAs shown in Table 1, Inf-MLLM improves the model performance for most cases and extensively enables models to continuously process new video clips and maintain high-quality answering up to 300 rounds of conversations. The original models fail at long contexts due to out-of-memory (OOM). Although these VLMs compress and truncate patch tokens based on similarity between video frames, the mem-ory usage issue will still be severe due to the increasing KV states in the streaming inference. Inf-MLLM successfully solves this issue due to its effective KV eviction mechanism which maintains a small size of KV cache (2K). Note that despite the slight decrease of the score metric in some cases, models with Inf-MLLM can still provide correct answers while incurring minor issues like description redundancy."}, {"title": "Question-answering for Long Video Streams", "content": "We also test Inf-MLLM on a recently released benchmark, VStream-QA (Zhang et al. 2024a), which focuses on on-"}, {"title": "", "content": "line video stream understanding. VStream-QA includes ex-tremely long videos that last from 30 minutes to over 1 hour. Each sample contains video clips of around 20 seconds to 5 minutes. Similarly, we test Inf-MLLM using Chat-UniVi-7B and Flash-VStream-7B. Table 3 shows that Inf-MLLM enables models to deal with long video streams and continuously generate high-quality answers, even as the video length grows to over 1 hour and the length of context com-prised of both video clips and texts increases to up to 220K."}, {"title": "Efficiency Evaluation", "content": "We evaluate the efficiency of different methods in terms of the decoding latency and memory usage on a NVIDIA 4090D GPU using the Vicuna-7B model, as shown in Fig."}, {"title": "", "content": "7. Compared to other methods, Inf-MLLM achieves stably smaller per-token-latency as the context length exceeds 40K. Moreover, when increasing the KV cache size, the average memory usage of Inf-MLLM is always lower (around 13.5GB) than that of H2O (around 13.7GB) and StreamingLLM (13.7GB)."}, {"title": "Demo of Streaming Inference On Edge", "content": "We deploy Inf-MLLM on a Nvidia Orin GPU. Our method conducts long-term video stream understanding and multi-round QA continuously. As shown in Figure 6, without our method, the vanilla Chat-Univi quickly reaches 25GB mem-ory usage at Round 11 which keeps blowing up. On the other hand, with our method, the memory usage can be"}, {"title": "Effects of Attention Bias", "content": "We evaluate the effects of attention bias in Table 4. The experimental setup is similar to Section 4.2. To capture longer-term dependency, smaller attention bias is required to re-serve more former tokens and maintain long-term information. Table 4 shows that as token distance scales up, the best value of attention bias decreases. However, when attention"}, {"title": "Conclusion", "content": "Streaming inference of MLLMs encounters many challenges involving the under-performance on extended context and extensive memory consumption. The problem is more severe to deploy MLLMs on resource-constrained hardware like edge devices. In this paper, we observe attention saddles existing in attention maps of MLLMs, and introduce Inf-MLLM, an efficient framework to facilitate MLLMs to continuously handle long text and video streams on a single GPU without fine-tuning. Inf-MLLM contains an effective KV cache eviction mechanism to remove KV states of irrelevant tokens while maintaining a small size of KV cache during streaming inference. An adjustment strategy based on attention bias is proposed to further adjust the distribu-"}, {"title": "", "content": "tion of attention scores and avoid the accumulation in earlier tokens. Experiments show that Inf-MLLM extensively extend the context length of MLLMs with texts up to 4 million tokens and 1-hour-long videos."}]}