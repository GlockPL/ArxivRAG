{"title": "SLIC: Secure Learned Image Codec through Compressed Domain Watermarking to Defend Image Manipulation", "authors": ["Chen-Hsiu Huang", "Ja-Ling Wu"], "abstract": "The digital image manipulation and advancements in Generative AI, such as Deepfake, has raised significant concerns regarding the authenticity of images shared on social media. Traditional image forensic techniques, while helpful, are often passive and insufficient against sophisticated tampering methods. This paper introduces the Secure Learned Image Codec (SLIC), a novel active approach to ensuring image authenticity through watermark embedding in the compressed domain. SLIC leverages neural network-based compression to embed watermarks as adversarial perturbations in the latent space, creating images that degrade in quality upon re-compression if tampered with. This degradation acts as a defense mechanism against unauthorized modifications. Our method involves fine-tuning a neural encoder/decoder to balance watermark invisibility with robustness, ensuring minimal quality loss for non-watermarked images. Experimental results demonstrate SLIC's effectiveness in generating visible artifacts in tampered images, thereby preventing their redistribution. This work represents a significant step toward developing secure image codecs that can be widely adopted to safeguard digital image integrity.", "sections": [{"title": "1 INTRODUCTION", "content": "Digital image manipulation has long posed a security threat to images distributed on social media. Nowadays, people no longer trust sensitive images spreading on social networks and often need to re-confirm the image source. Detecting forged images from simple editing operations such as splicing and copy-moving are challenging tasks. The advent of Deepfake [40] and text-to-image diffusion models [46] in the Generative Al era has further exacerbated the credibility issue of social media images, as tampering and counterfeiting images are no longer restricted to experts. These altered images are re-encoded in either lossless or lossy formats. As the most popular lossy image codec, JPEG exhibits specific characteristics in its DCT coefficients, and there are methods [30, 34] to detect double JPEG compression to identify manipulated images. For lossless encoded images, pixel-based characteristics such as noise [31], patterns [3], and camera properties [12, 36] are analyzed to detect local inconsistencies and identify image forgery.\nImage forensics techniques [35, 44] are considered passive approaches to proving content authenticity because they detect tampering after it has occurred. Active approaches such as trustworthy cameras [5, 11] or fragile digital watermarking [20, 24] attempt to authenticate the image after its creation and detect any later modification by checking the embedded signature or watermark. We propose a novel active approach, the Secure Learned Image Codec (SLIC), to validate that contents in the SLIC format do not contain any watermarked source. Figure 1 shows our proposed application scenario, which forces a malicious party to obtain severely"}, {"title": "2 RELATED WORKS", "content": null}, {"title": "2.1 Adversarial Attacks", "content": "Szegedy et al. [38] first introduced the notion of adversarial examples and demonstrated that neural networks are vulnerable to small perturbations in input data. The goal of an adversarial attack is to find an adversarial example that is indistinguishable from the human eye but can lead to significant misclassifications. Adversarial attacks can be either targeted or untargeted.\nSearching for adversarial examples was once considered computationally expensive because it is an optimization problem. Szegedy"}, {"title": "2.2 Learned Image Compression", "content": "The learned image compression technique is an end-to-end approach that automatically learns a pair of image encoders and decoders from a collection of images without the need for hand-crafted design of coding tools. The field of learned image compression has witnessed significant advancements [2, 9, 14, 32] in the past few years, surpassing traditional expert-designed image codecs. Several comprehensive surveys and introductory papers [16, 28, 42] have summarized these achievements.\nHowever, neural network-based codecs are no exception to adversarial attacks. Kim et al. [18] first identified the instability issue of successive deep image compression and proposed including a feature identity loss to mitigate it. Liu et al. [22] investigated the robustness of learned image compression, where imperceptibly manipulated inputs can significantly increase the compressed bitrate. Such attacks can potentially exhaust the storage or network bandwidth of computing systems. Chen and Ma [8] examined the robustness of learned image codecs and investigated various defense strategies against adversarial attacks to improve robustness."}, {"title": "2.3 DNN-based Data Hiding", "content": "Most DNN-based data hiding methods [17, 25, 26, 39, 45, 49] use their own custom-trained image encoder/decoder for message embedding and hide the invisible perturbations in the spatial domain. Huang and Wu [15] first proposed reusing the standard neural codec's encoder/decoder to embed/extract messages in the compressed latent representation. Operating on the compressed latents, their method offers superior image secrecy in steganography and watermarking scenarios compared to existing techniques. Additionally, processing messages in the compressed domain has much lower complexity because additional encoding/decoding is not required.\nOnce a neural codec's compressed latent representation is jointly trained to allow additional perturbations as hidden data, we can effectively apply perturbations to the latent space as a universal adversarial attack. This compressed domain data hiding method inspires us to develop SLIC as a secure codec."}, {"title": "2.4 Watermarking to Defend Deepfake", "content": "Adversarial attacks via watermarking are one of the active defense strategies against Deepfakes. Lv [27] proposed an adversarial attack-based smart watermark model to exploit Deepfake models. When Deepfake manipulates their watermarked images, the output images become blurry and easily recognized by humans. Yu et al. [43] took a different approach by embedding watermarks as fingerprints in the training data for a generative model. They discovered the transferability of such fingerprints from training data to generative models, thereby enabling Deepfake attribution through model fingerprinting.\nWang et al. [41] proposed an invisible adversarial watermarking framework to enhance the copyright protection efficacy of watermarked images by misleading classifiers and disabling watermark removers. Zhang et al. [47] developed a recoverable generative adversarial network to generate self-recoverable adversarial examples for privacy protection in social networks."}, {"title": "3 PROPOSED METHOD", "content": "We extend the compressed domain data hiding technique [15] to learn a secure image codec and reuse its message encoder/decoder architecture. In our SLIC, the latent representation y transformed by the image encoder allows additive small residuals to form the embedded latent code $y_e$. The residuals added to the latent representation are considered a type of adversarial perturbation in the latent space, generating invisible perturbations in the stego image $\\hat{s}$ as an adversarial example. If the watermarked stego image is re-compressed, it exploits the image encoder $g_e$ and produces a severely damaged image. We show the proposed SLIC framework in Figure 2, which contains four subflows described as follows.\nImage encoding/decoding flow: We fine-tune a neural encoder/decoder pair $g_e$ and $g_d$ to stabilize reconstruction quality in the second compression. It is essential that the SLIC must not degrade image quality in the re-compression when an image is not watermarked. Given an input cover image c, the first compression by the neural compressor generates an encoded cover image $\\hat{c} = g_d(g_e(c))$. Since the cover image is not embedded with a watermark, we expect the re-compressed image $\\hat{c}' = g_d(g_e(\\hat{c}))$ to be perceptually similar to c. That is, we aim to minimize:\n$\\arg \\min_{\\theta_e,\\theta_d} LPIPS(c, \\hat{c}) + LPIPS(\\hat{c}, \\hat{c}')$,                                       (1)\nwhere $\\theta_e$, $\\theta_d$ are the parameters of the neural compressor $g_e$,$g_d$ and LPIPS is the DNN-based perceptual loss [48].\nStego image generating flow: We embed a message $m \\epsilon \\{0, 1\\}^n$ into a cover image c as a watermark to generate a stego image $\\hat{s}$. The compressed latent vector of the cover image is obtained as $y = g_e (c)$, and the embedded latent vector $y_e$ is obtained as follows:\n$y_e = y \\oplus h_e(m)$,                                                      (2)\nwhere $h_e$ is the message encoder that transforms the message to the same dimension as y and $\\oplus$ is the element-wise addition. The stego image $\\hat{s} = g_d(y_e)$ is obtained by decoding the watermarked latent vector $y_e$ using decoder $g_d$. Here, we optimize the perceptual loss with:\n$\\arg \\min_{\\theta_e,\\theta_d} LPIPS(\\hat{c}, \\hat{s}) + ReLU(\\tau - LPIPS(\\hat{s}, \\hat{s}'))$,                              (3)\nwhere $\\hat{s}' = g_d(g_e(\\hat{s}))$ is the re-compressed stego image, and $\\tau$ is a constant to control the degree of perceptual distance divergence between $\\hat{s}$ and $\\hat{s}'$. In other words, we expect the stego image to remain visually similar to the encoded cover image without a watermark, while the re-compressed stego image should be perceptually distorted.\nNoise attack simulation flow: In practical scenarios, the watermarked stego image may undergo various editing operations such as cropping, rotation, and lighting adjustment. These edits are considered noise attacks on the watermarking system. To simulate noise attacks, we use an attacker u similar to [15] and derive the noised stego image as $\\bar{s} = u(\\hat{s})$, then compress it as $\\hat{\\bar{s}} = g_d(g_e(\\bar{s}))$. Since our SLIC should be robust to noise attacks and damage the visual quality in the re-compression, we optimize the perceptual divergence between $\\hat{s}$ and $\\hat{\\bar{s}}'$:\n$\\arg \\min_{\\theta_e,\\theta_d} ReLU(\\tau - LPIPS(\\hat{\\bar{s}}, \\hat{\\bar{s}}'))$.                                  (4)\nwhere $\\hat{\\bar{s}}' = g_d(g_e(\\bar{s}))$ is the re-compressed noised stego image.\nMessage embedding/extraction flow: The extracted message $m' = h_d(y_e)$ is obtained with the message decoder $h_d$ on the compressed latent vector $y_e$. The noised embedded latent vector $\\bar{y_e} = g_e(\\bar{s})$ is then used to extract noised secrets with $m' = h_d(\\bar{y_e})$. We optimize for the message extraction accuracy by:\n$\\arg \\min_{\\phi_e,\\phi_d} BCE(m, m') + \\gamma BCE(m, m')$,                               (5)\nwhere $\\phi_e$ and $\\phi_d$ are the parameters of the message encoder/decoder $h_e,h_d$ and BCE is the binary cross-entropy function."}, {"title": "3.1 Training and Loss Functions", "content": "The subflows described in the prior section illustrate the fundamental building blocks of our framework. To learn a secure image codec, we jointly train the image codec and the message encoder/decoder in an interleaved manner, as presented in Algorithm 1. In each training epoch, we first freeze the image codec parameters and update the message network parameters $f_e$ and $d_d$ with the data hiding loss function $L_H$. We then freeze the message network parameters and fine-tune the image codec with the codec loss function $L_C$.\nAll the optimization objectives in each subflow are reformulated using different loss functions during training. We design the data hiding loss function as a combination of perceptual loss $L_P$, message loss $L_M$, and adversarial loss $L_A$:\n$L_H = L_P + L_M + \\beta L_A$,                                                       (6)\nwhere $\\alpha$ and $\\beta$ are hyper-parameters used to control the relative weights. We measure the perceptual loss with the DNN-based perceptual metric LPIPS [48]:\n$L_P = LPIPS(\\hat{c}, \\hat{s}) + LPIPS(\\hat{c}, \\hat{c}')$.                                    (7)\nThe second term of $L_P$ is designed to ensure an image with no watermark will remain perceptually stable after re-compression. We avoid using MSE (mean square error) to minimize image distortion because we observed that the LPIPS metric significantly impacts our standard neural codec more than a custom-trained image encoder/decoder. The ablation study is provided in Appendix A.\nTo measure the decoded message error, we define the message loss function as:\n$L_M = BCE(m, m') + \\gamma BCE(m, m')$,                                       (8)\nwhere $\\gamma$ is a hyper-parameter that controls the weight of the noised message error term. The adversarial loss is designed to make the stego image $\\hat{s}$ and its noised version $\\hat{\\bar{s}}$ become adversarial examples to the neural image codec in the re-compression:\n$L_A = ReLU(\\tau - LPIPS(\\hat{s}, \\hat{s}')) + ReLU(\\tau - LPIPS(\\hat{\\bar{s}}, \\hat{\\bar{s}}'))$,                               (9)\nwhere $\\tau$ is a constant threshold to enforce perceptual distance divergence between the original and re-compressed stego images. The codec loss $L_C$ optimizes the rate-distortion efficiency by minimizing:\n$L_C = R + \\lambda D$,                                                              (10)\nwhere $\\lambda$ is a hyper-parameter that controls the trade-off between rate and distortion.\nFor any image $x \\epsilon X$, the neural encoder $g_e$ transforms x into a latent representation $y = g_e(x)$, which is later quantized to a discrete-valued vector $\\hat{y}$. To reduce the notational burden, we refer to $\\hat{y}$ simply as y in what follows. The discrete probability distribution $P_y$ is estimated using a neural network and then encoded into a bitstream using an entropy coder. The rate of this discrete code, R, is lower-bounded by the entropy of the discrete probability distribution H($P_y$). Because the perturbations (i.e., hidden message) added to the latent code break the original entropy coding optimality, we revise the rate function to include both y and $y_e$ when we freeze the message network and fine-tune the image codec. That is:\n$R = H(P_y) + H(P_{ye})$.                                                             (11)\nOn the decoder side, we decode y from the bitstream and reconstruct the image $x = g_d (y)$ using the neural decoder. The distortion, D, is measured by a distance metric d(x, x), where mean square error is commonly used."}, {"title": "3.2 Noise Attacks", "content": "We define eight types of editing operations commonly used in creating spliced fake images, including copy, Gaussian blur, median filtering, lightening, sharpening, histogram equalization, affine transform, and JPEG compression. We then test the resilience of our stego image against these image manipulations and evaluate the effectiveness of SLIC upon re-compression.\nSurprisingly, our SLIC demonstrates robustness against non-filtering editing operations such as lightening, sharpening, and histogram equalization, even without noise attack simulation. These attacks tend to increase rather than decrease the magnitude of the adversarial perturbation. We classify Gaussian blur, median filtering, affine transform, and JPEG compression as filtering editing operations because the adversarial perturbations added to the stego image through latent space perturbations are partially filtered due to resampling. Therefore, to enhance robustness, we simulate two types of noise attacks during training:\nAffine transform: We randomly rotate from -10 to 10 degrees, translate from 0% to 10% in both axes and scale from 90% to 110% on the stego samples.\nJPEG compression: We re-compress the stego image with random JPEG quality from 70 to 95.\nWe do not simulate the blurring attack because the affine transform has a similar low-pass filtering effect from rotation and scaling. Further discussion on this is provided in Section 4.2."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "We implemented our SLIC using the CompressAI [4] release of neural codecs [2, 9, 32], denoted as Balle2018, Minnen2018, and Cheng2020. For training, we randomly selected 90% of the images"}, {"title": "4.1 Watermarked Image Quality", "content": "Quantitatively, we present the SLIC's watermarked image quality in PSNR metric and the bit error rate (BER) in Table 1. The PSNR of watermarked images (5) compared to images without watermark (c) is around 40 for all three neural codecs, which remains suitable for visual communication. The bit error rate of SLIC is unable to achieve zero, but there are established error correction techniques [6, 10] to mitigate.\nWe report the re-compression effects in PSNR in Table 1, including the cover, stego, and stego images with eight pre-defined editing operations. In the third column of Table 1, the PSNR of the re-compressed cover images stays close to that of the watermarked image, as expected because the re-compression should not cause any quality degradation on images without watermarks. The fourth column shows that the watermarked image's quality drops significantly after re-compression, where the PSNR is around 6 on average. Even with image editing such as Gaussian blur and median filtering, the watermarked SLIC images still cause severe quality degradation in a second compression.\nSince adversarial stego images are the result of invisible perturbations added to the pixels, the affine transform and JPEG compression could effectively reduce those perturbations. Table 1 shows that the re-compressed stego images after affine and JPEG attacks have a PSNR of around 25.\nAccording to [15], a codec with lower coding efficiency, such as Balle2018, tends to have higher stego image quality because its latent space has more room for data hiding. However, the relation between coding efficiency and re-compression quality degradation needs to be clarified, as indicated in Table 1. Another interesting finding is that Cheng2020 seems to have different patterns of adversarial effects in Gaussian blur, median filter, and JPEG compression. We think the cause may be the attention module in Cheng2020, which requires further investigation."}, {"title": "4.2 Adversarial Effects", "content": "Qualitatively, we present the re-compressed results of altered stego images in Figure 3. More adversarial effects regarding codecs Minnen2018 and Cheng2020 are provided in Appendix B."}, {"title": "4.3 Watermark Robustness", "content": "We evaluated the watermark robustness of our SLIC against noise attacks cropout, dropout, affine, and JPEG on the DIV2K dataset, as shown in Fig. 4. We varied the attack strength by increasing the noise parameter, which degrades image quality along the horizontal axis. We do not add the cropout and dropout attack simulations during training; we merely use affine and JPEG as proxies. From the watermark extraction point of view, the SLIC maintains robustness against selected attacks except for affine transform, which is generally considered challenging in the watermarking field. However, the content owner can re-compress the protected image with SLIC to spot the quality degradation as another piece of evidence of infringement."}, {"title": "4.4 Coding Efficiency Impact", "content": "Conceptually, watermarking techniques convert secret messages to noise-like signals and add them onto cover images as invisible perturbations. The embedding operation will increase the entropy"}, {"title": "5 CONCLUSION", "content": "This paper presents the Secure Learned Image Codec (SLIC), a novel active approach to ensuring image authenticity. SLIC effectively combats unauthorized image manipulation by embedding watermarks in the compressed domain, generating adversarial examples that degrade visually upon re-compression. Our method introduces an adversarial loss to train the SLIC, ensuring that watermarked images suffer noticeable quality degradation after re-compression if tampered with.\nExperimental results demonstrate SLIC's robustness against various noise attacks and common image editing operations, with significant artifacts appearing in counterfeit images, thus preventing their redistribution. This work represents an initial attempt to develop a secure image codec with the potential for widespread adoption once standardized and trusted by the public. Future research should explore the adversarial effects of different neural codecs and enhance noise attack simulations to further deteriorate image quality after editing and re-compression. By refining these techniques, we can advance towards more resilient defenses against digital image counterfeiting, ultimately protecting the integrity of digital media from the outset."}, {"title": "A PERCEPTUAL LOSS STUDY", "content": "The success of message extraction is highly dependent on the design of the message encoder/decoder when jointly trained with a standard codec that minimizes image distortion. Experimental results show that the widely used MSE loss performs poorly. The recovered bit accuracy is good, but the stego image has been visually deteriorated, as shown in preset a of Table 3.\nTo address this issue, we added LPIPS in the perceptual loss function in preset b, demonstrating a significant improvement in image quality. Compared to a custom-trained image encoder/decoder, the LPIPS metric significantly impacts the reconstruction quality of our standard neural codec. This observation could be due to the high compactness of our neural codec, which is more strongly connected to certain perceptual features in the latent space. Consequently, adding LPIPS as a distortion loss function can effectively optimize the message encoder to modify the compressed latents. Finally, we propose using the preset c, which omits the MSE loss, as our proposed perceptual loss function."}, {"title": "B MORE ADVERSARIAL EFFECTS", "content": "We present more qualitative results of adversarial stego images in Figures 5 and 6 based on neural image codec Minnen2018 and Cheng2020, respectively."}]}