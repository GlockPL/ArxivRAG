[{"title": "Mobile Video Diffusion", "authors": ["Haitam Ben Yahia", "Denis Korzhenkov", "Ioannis Lelekas", "Amir Ghodrati", "Amirhossein Habibian"], "abstract": "Video diffusion models have achieved impressive realism and controllability but are limited by high computational demands, restricting their use on mobile devices. This paper introduces the first mobile-optimized video diffusion model. Starting from a spatio-temporal UNet from Stable Video Diffusion (SVD), we reduce memory and computational cost by reducing the frame resolution, incorporating multi-scale temporal representations, and introducing two novel pruning schema to reduce the number of channels and temporal blocks. Furthermore, we employ adversarial finetuning to reduce the denoising to a single step. Our model, coined as MobileVD, is 523\u00d7 more efficient (1817.2 vs. 4.34 TFLOPs) with a slight quality drop (FVD 149 vs. 171), generating latents for a 14 \u00d7 512 \u00d7 256 px clip in 1.7 seconds on a Xiaomi-14 Pro. Our results are available at", "sections": [{"title": "1. Introduction", "content": "Video diffusion models are making significant progress in terms of realism, controllability, resolution, and duration of the generated videos. Starting from zero-shot video models [12, 25, 29, 35], which deploy pretrained image diffusion models to generate consistent frames, e.g., through cross-frame attention, modern video diffusion models rely on spatio-temporal denoising architectures, i.e., 3D UNets [2, 3, 18] or 3D DiTs [32, 67, 73]. This involves inflating image denoising models by adding temporal transformers and convolutions to denoise multiple frames simultaneously. Despite their impressive generation qualities, spatio-temporal denoising architectures demand high memory and computational power, which limits their usage to clouds with high-end GPUs. This hinders the wide adoption of video generation technology for many applications that require generating content locally on mobile devices.\nPrior work on accelerating video diffusion models has mostly focused on reducing the number of sampling steps [62, 71]. By extending the consistency models [58] and adversarial distillation [53] to video diffusion models, they managed to reduce the number of denoising steps from 25 to only 4 [62] and 1 step [71], which tremendously accelerates video generation. However, step distillation alone does not reduce the memory usage of the model, which is the key setback in deploying video diffusion models on mobile devices.\nThis paper is the first attempt to build video diffusion models for mobile. Starting from the spatio-temporal UNet from Stable Video Diffusion (SVD) [2, 3], as a representative for a variety of video diffusion models, we conduct a series of optimizations to reduce the size of activations to build a mobile-friendly UNet: driven by the lower resolution needs for user-generated content on phones, we opt for using a smaller latent space sufficient for generating 512 \u00d7 256 px frames. Instead of preserving the number of frames throughout the denoising UNet, we introduce additional temporal down- and up-sampling operations to extend the multi-scale representation both in space and time, which reduces the memory and computational cost with minimal loss in quality. Moreover, we discuss how naive visual conditioning through cross-attention leads to significant computational overhead that can be avoided without damaging visual quality.\nWe further accelerate the mobile-friendly UNet by reducing its parameters using a novel channel compression schema, coined channel funneling, and a novel technique to prune the temporal transformers and temporal residual blocks from the UNet. Finally, following Zhang et al. [71], we reduce the number of denoising steps to a single step using adversarial finetuning. This results in the first mobile video diffusion model called MobileVD, which is 523\u00d7 more efficient (1817.2 vs. 4.34 TFLOPs) at a slightly worse quality in terms of FVD (149 vs. 171) as reported in Fig. 1. MobileVD generates the latents for a 14 \u00d7 512 \u00d7 256 px clip in 1.7 seconds on a Xiaomi 14-Pro smartphone which uses a Qualcomm Snapdragon\u00ae 8 Gen 3 Mobile Platform."}, {"title": "2. Related work", "content": "Fueled by advancements in generative image modeling using diffusion models, there has been notable progress in the development of generative video models [2, 3, 15, 16, 23, 24, 31, 46, 51, 74]. These video models generally evolve from image models by incorporating additional temporal layers atop spatial blocks or by transforming existing 2D blocks into 3D blocks to effectively capture motion dynamics within videos. Although these advancements have paved the way for the generation of high-resolution videos, the significant computational demands make them impractical for use on low-end devices. In this work, we address this by optimizing a representative of video generative model, SVD [2], to make it accessible to a broader range of consumer-graded devices.\nThe problem of making diffusion models efficient naturally consists of the following two parts: (i) reducing the number of denoising steps and (ii) decreasing the latency and memory footprint of each of those steps. Reducing number of steps is achieved by using higher-order solvers [39, 40, 68], distilling steps to a reduced set using progressive step distillation [33, 42, 52], straightening the ODE trajectories using Rectified Flows [34, 36, 75], mapping noise directly to data with consistency models [38, 57, 58], and using adversarial training [53, 54, 63, 71]. To decrease computational cost of each step, research has been done in weight quantization [21, 47, 55] and pruning [9, 33] as well as architectural optimization of the denoiser [10, 19, 30, 65]. In this work, following [71] we reduce number of steps to one using adversarial training and optimize the UNet denoiser using multiple novel techniques.\nOn-device generation has attracted interest due to its ability to address privacy concerns associated with cloud-based approaches. There have been advancements in running text-to-image generation on mobile devices and NPUs [6, 8, 9, 33, 72]. While there has been progress in the video domain with fast zero-shot video editing models [26, 64, 70], there have been no attempts to implement spatiotemporal video generative models on device due to their high computational and memory requirements, which are challenging to meet within on-device constraints. In this work, we propose the first on-device model based on SVD [2] image-to-video model."}, {"title": "3. Mobile Video Diffusion", "content": "In this section, we propose a series of optimizations to obtain a fast and lightweight version of an off-the-shelf image-to-video diffusion model [2], suitable for on-device deployment."}, {"title": "3.1. Preliminaries", "content": "We adopt Stable Video Diffusion (SVD) [2] as the base model for optimization. The SVD released checkpoint\u00b9 is an image-to-video model that by default generates 14 frames at the resolution 1024 \u00d7 576 with 25 sampling steps. To generate a video from an image, the input image is first mapped into a latent code of resolution 128 \u00d7 72 using a Variational Auto-Encoder (VAE). Then it is duplicated 14 times and concatenated with a noise latent of spatiotemporal resolution 14 x 128 x 72. The combined latent is then denoised by a conditional UNet through an iterative process. Additionally, the input image is encoded with a CLIP image embedding for use in cross-attention layers [49]. The UNet denoiser consists of four downsampling blocks, one middle block and four upsampling blocks. To handle the dynamics of video sequences, the model employs temporal blocks after spatial blocks. Notably, up- and downsampling is conducted across spatial axes only, and temporal resolution of the latent is kept constant to 14 throughout the UNet. The UNet denoiser as-is is too resource-intensive for on-device use, requiring 45.43 TFLOPs and 376 ms per denoising step on a high-end A100 GPU. Using FLOPs and latency as proxy metrics, we propose a series of optimizations to make the model suitable for on-device deployment.\nIn addition to the high cost of the UNet, the iterative sampling process in video diffusion models further slows them down. For example, with the conventional 25-step sampling it takes 11 seconds to generate a video on a high-end A100 GPU. To reduce the cost, we follow SF-V framework [71] and use adversarial finetuning, enabling our models to generate videos in a single forward"}, {"title": "3.2. Mobile-friendly UNet", "content": "Our first modifications to SVD architecture regard optimizations along the latent and feature resolution that affect both GPU and mobile latency. Then we highlight some lossless optimizations that have big effect on mobile latency. These are the first optimizations that allow us to run the UNet on device.\nTo satisfy the memory constraints of mobile devices, we decrease the resolution of denoising UNet input to 64 \u00d7 32 by resizing the conditioning image to 512 \u00d7 256. While the released SVD checkpoint supports multiple resolutions, we found out that the original model demonstrates deteriorated quality for our target spatial resolution as reported in Tab. 1. Therefore, we finetuned the diffusion denoiser at our target spatial size. With this optimization, computational cost and GPU latency is reduced to 8.60 TFLOPS and 82 ms respectively, see Tab. 2.\nTo further reduce computational burden, one might lower the input resolution more heavily. However, this significantly degrades visual quality. Instead, we can additionally downscale the feature maps by a factor of 2 along either spatial or temporal axis after the first downsampling block of the UNet. To maintain the same output"}, {"title": "3.3. Channel funnels", "content": "Channel size, which refers to the width of neural network layers, is crucial for scaling models. Increasing channel size generally enhances model capacity but also increases the number of parameters and computational cost. Research has focused on compressing the number of channels by either discarding less informative channels through channel pruning [13, 45] or representing weight matrices as low-rank products of matrices using truncated singular decomposition [69]. However, these could have sub-optimal tradeoffs in quality and efficiency when deployed on device. While low-rank decomposition is relatively straightforward to implement, it only reduces the number of parameters and computational complexity if the rank is reduced by more than half for feed forward layers. Additionally, not all layers of neural network types benefit equally from low-rank factorization. Moreover, this method does not reduce the size of output feature maps, which can cause significant memory overhead on mobile devices. In this part, we propose channel funnels, a straightforward method to reduce the number of channels at inference time, with negligible quality degradation. Intuitively, a channel funnel is placed between two affine layers, reducing the intermediate channel dimensionality to save computation. Consider two consecutive linear layers \\(y = W_2\\sigma(W_1x)\\) and the non-linear function \\(\u03c3\\) in between, where \\(W_1 \\in \\mathbb{R}^{C_{inner} \\times C_{in}}\\), \\(W_2 \\in \\mathbb{R}^{C_{out} \\times C_{inner}}\\). We introduce two funnel matrices, \\(F_1 \\in \\mathbb{R}^{C' \\times C_{inner}}\\) and \\(F_2 \\in \\mathbb{R}^{C_{inner} \\times c'}\\), where \\(c' < C_{inner}\\), and rewrite our network as \\(y' = W_2F_2\\sigma(F_1W_1x)\\). The F-weights, having fewer channels, can be merged during inference with their associated W-weights, resulting a weight matrix with smaller inner dimension \\(c'\\), see Fig. 3. We refer to the reducing factor of the inner rank of the layers, i.e. \\(c'/C_{inner}\\), as the funnel-factor.\nWe propose to use coupled singular initialization (CSI) for funnel matrices \\(F_1\\) and \\(F_2\\) that improves the model results, as demonstrated below. In this method we ignore the non-linearity and consider the effective weight matrix \\(W_2F_2F_1W_1\\) which in practice has rank of \\(c'\\). For that reason, we aim to use such an initialization which mimics the best \\(c'\\)-rank approximation of the original effective matrix. As Eckart-Young-Mirsky theorem implies, this can be achieved by means of truncated singular decomposition [11]. Let \\(W_2W_1 = USV^T\\) be the singular vector decomposition, and \\(UDV^T\\) to be its truncated \\(c'\\)-rank version. Then it suffices to set \\(F_2 = W_2^{\\dagger}U{\\Sigma}^{1/2}\\) and \\(F_1 = {\\Sigma}^{1/2}V^{\\dagger}W_1\\) to obtain \\(W_2F_2F_1W_1 \\approx UD{\\Sigma}V^T\\), where \\(^{\\dagger}\\) means the Moore-Penrose pseudoinverse.\nWe apply channel funnels in attention layers where query and key embedding \\(W_q\\) and \\(W_k\\) are used to compute the similarity map of \\(XW_q(XW_k)^T\\). With funnel matrices \\(F_q\\) and \\(F_k\\) of size \\(C_{inner} \\times c'\\), we modify the aforementioned bilinear map as \\(XW_qF_q (XW_kF_k)^T = XW_qF_qF_k^T W_k^T X^T\\). Similarly, funnels are applied to the pair of value and output projection matrices of a self-attention layer. In our ablations we also show the impact of channel funnel on convolutions in residual blocks. Unless specified otherwise, we use fun-factor of 50%."}, {"title": "3.4. Temporal block pruning", "content": "The original UNet in the SVD model does not contain 3D convolutions or full spatiotemporal attention layers. Instead, to model motion dynamics, SVD incorporates temporal blocks after each spatial block. The output of such group is a linear combination of the spatial and temporal block outputs, \\(x_s\\) and \\(x_t\\) respectively, \\(\\alpha x_s + (1 - \\alpha) x_t\\), where \\(\u03b1\\) is a weight scalar that emphasizes spatial features when higher and temporal features when lower, see Fig. 4a. While this approach leverages image model priors when extending the model to videos, it adds computational cost. Moreover, not all of these blocks are equally important for maintaining quality. Here, we propose a learnable pruning technique to remove less important temporal blocks while minimizing quality degradation. To this end, for each temporal block we define an importance value \\(q_i\\), \\(0 \\leq q_i \\leq 1\\) where \\(i = 1,..., N\\) and \\(N\\) is number of temporal blocks. The values \\(\\{q_i\\}_{i}\\) are trained to identify the blocks that are the most crucial for model performance. At inference time,"}, {"title": "4. Experiments", "content": "In this section, we describe our experimental setup, followed by a qualitative and quantitative evaluation of our model. Finally, we present ablations to justify our choices for the final model deployed on the device."}, {"title": "4.1. Implementation details", "content": "For our experiments, we use a collected video dataset. We follow the data curation pipeline from Open-Sora [73, V.1.1], selecting videos with motion scores between 4 and 40, and aesthetic scores of at least 4.2. This results in a curated dataset of approximately 31k video files for finetuning.\nUNet used by SVD, has two conditioning parameters called FPS and Motion bucket id. To obtain videos with different FPS, we chose each k-th frame from the video with randomly sampled k, 1 \u2264 k \u2264 4, and adjusted the native FPS value of the video accordingly. The notion of motion bucket has not been properly documented at time of the model release. While it is connected with the speed of motion in the video, as described in the original SVD paper, the motion estimator has not been open-sourced. For that reason, we implemented our own definition of the motion bucket using a simple heuristic. For the sampled chunk of 14 frames, we converted them to gray color, spatially downsampled to the resolution of 14 \u00d7 128 \u00d7 64, and reshaped to the matrix of size 14 \u00d7 (128\u00b764). After that, we computed the singular values of that matrix. Note that for a completely static video this matrix has a rank of 1, and therefore the only non-zero singular value. And the less similar frames are, the more singular components are needed to faithfully reconstruct the full video. Based on that observation, we re-defined the motion bucket as the area under the normalized cumulative sum of singular values."}, {"title": "4.2. Results", "content": "In Tab. 2 we compare MobileVD to our base model. As the results indicate, each optimization reduces speed of inference on a mobile phone. Decreased spatial resolution and optimized cross-attention together unlock on-device execution with a latency of 3.6 seconds. Temporal downsampling layers in UNet make inference 29% faster. Additionally, temporal blocks pruning reduces phone latency by 13%, and channel funneling further decreases it by 9%. Empirically, we found that a difference of up to 20 FVD units does not significantly affect visual quality and typically falls within the standard deviation when using different random seeds. Based on that, we see that our optimizations have minimal impact on FVD."}, {"title": "4.3. Ablations", "content": "In this section, we evaluate our design choices through ablation experiments. Unless otherwise specified, we use the SVD checkpoint with low-resolution input, optimized cross-attention, and adversarial finetuning as the reference model, cf. Tab. 2."}, {"title": "4.3.1. Resolution impact in UNet", "content": "In Tab. 3 we compare different latent multiscaling optimizations proposed in Sec. 3.2. Specifically, we investigate the impact of inserting spatial or temporal multiscaling layers after the first UNet block in terms of FLOPs, latency, and"}, {"title": "B.1. Training", "content": "For diffusion training, we used AdamW optimizer [37] with learning rate of \\(1 \\times 10^{-6}\\) and weight decay \\(1 \\times 10^{-3}\\), while other hyperparameters were default. During adversarial finetuning, we also used AdamW. For generator the learning rate was equal to \\(1.25 \\times 10^{-6}\\), and for discriminator we set it 10 times higher. For logits of importance values, used for pruning of temporal blocks, learning rate was equal to \\(1 \\times 10^{-3}\\). Momentum weights for both optimizers we set as follows \\(\\beta_1 = 0.5, \\beta_2 = 0.999\\). For generator, the weights for adversarial and pseudo-Huber loss were equal to 1 and 0.1 respectively. For discriminator, weight of \\(R_1\\) penalty was \\(1 \\times 10^{-6}\\), and we applied it once per 5 iterations, as recommended in [27].\nFor high-resolution training, the first (diffusion) stage lasted for 10k iterations with total batch size of 4. The second (adversarial) stage comprised 30k iterations with batch size of 2. The learning rates for adversarial finetuning were twice as lower as for low-resolution case, except for the learning rate of importance values. \\(R_1\\) penalty was applied at each step. Other training aspects were the same both for MobileVD and MobileVD-HD."}, {"title": "B.2. Decoding latents", "content": "As our approach stems from SVD, the presented MobileVD is also a latent model. Therefore, each generated latent code must be decoded to raw RGB pixels. For all the experiments reported in the main text, we used the native autoencoder weights released alongside the SVD model itself. The decoding was conducted independently for each frame. Notably, this model is relatively slow on device: it takes 91.5 ms per frame, resulting in 1,280 ms for decoding the full generated video. This timing is comparable with the latency of MobileVD (1,780 ms). As an alternative, we used the decoder from TAESD autoencoder. It is significantly faster on a smartphone: 6.4 ms per frame, or 90 ms for the full video. At the same time, the difference in quality metrics is negligible, see Tab. 9."}, {"title": "B.3. Channel funnels", "content": "Consider a query and key projection matrices in a self-attention similarity map computation, \\(XW_q (XW_k)^T\\) with layer input X having a shape of \\(L \\times C_{in}\\), and \\(W_q\\) and \\(W_k\\) of \\(C_{in} \\times C_{inner}\\). With funnel matrices \\(F_q\\) and \\(F_k\\) of size \\(C_{inner} \\times c'\\), we modify the aforementioned bilinear map as \\(XW_qF_q (XW_kF_k)^T = XW_qF_qF_k^T W_k^T X^T\\). We apply our coupled singular initialization (CSI) by setting \\(F_q = W_q^{\\dagger}U{\\Sigma}^{1/2}\\) and \\(F_k = {\\Sigma}^{1/2}V^{\\dagger}W_k\\). For value and output projections matrices the initialization is applied in the way discussed in the main text.\nApplying the same initialization to a pair of convolutional layers is not straightforward. Weight tensors of 2D convolutional layers are 4-dimensional, and therefore computation of the effective weight tensor is not obvious. However, we make use of the following observation. Consider input tensor X with shape h \u00d7 W \u00d7 Cin. We refer to its 2-dimensional pixel coordinate as p, and therefore Xp is a vector with Cin entries. Let W be a convolutional kernel with size \\(k_h \\times k_w \\times C_{out} \\times C_{in}\\), and we refer to its spatial 2-dimensional coordinate as \u1fb7, while q = 0 is a center of a convolutional filter. For j-th output channel, \\(\\textbf{W}_{\\bar{q},j}\\) is also a vector with Cin entries. The layer output \\(Y \\in \\mathbb{R}^{h\\times w \\times C_{out}}\\) can be computed as\n\\[Y_{p,j} = \\sum_{\\bar{q}} (\\textbf{W}_{\\bar{q},j}, X_{p+\\bar{q}}),\\]\nwhere \\(\\langle , \\rangle\\) denotes inner product. Simply speaking, this means that convolution can be treated as a linear layer with weight matrix of shape \\(C_{out} \\times (k_h\\cdot k_w\\cdot C_{in})\\) applied to each flattened input patch.\nAt the same time, another way of reshaping the kernel is also possible. Consider a tensor E of shape \\(k_h \\times k_w \\times C_{out} \\times h \\times w\\) defined as\n\\[\\textbf{E}_{\\bar{q},j,p} = (\\textbf{W}_{\\bar{q},j}, X_{p}).\\]\nIn other words, the convolution kernel reshaped as \\((k_hk_wC_{out}) \\times C_{in}\\) is multiplied by features of each input pixel. Then Eq. (1) can be rewritten as\n\\[\\Upsilon_{p} = \\sum_{\\bar{q}}\\textbf{E}_{\\bar{q},.,p} = \\sum_{\\bar{q}} (\\textbf{E}_{\\bar{q},.,p},\\delta),\\\\]\nwhere \\(\u03b4\\) is a 4-dimensional identity tensor, i.e. \\(\\delta_{\\bar{u},\\bar{v}} = 1\\) if \\(\\bar{u} = \\bar{v}\\) and 0 otherwise.\nHaving said that, a sequence of two convolutions can be presented as (i) flattening the input patches; (ii) matrix multiplication by the first kernel reshaped according to Eq. (1); followed by (iii) matrix multiplication by the second kernel reshaped as in Eq. (2); and with (iv) independent of kernels operation described by Eq. (3) afterwards. Therefore, coupled singular initialization can be applied to the product of matrices used in steps (ii) and (iii). We follow this approach and introduce funnels to the pairs of convolutions within the same ResNet block of denoising UNet."}, {"title": "B.4. Pruning of temporal adaptors", "content": "In the main text we described that we transform the update rule of the temporal block as follows \\(x_s + z_i (1 - \\alpha_i) r_t\\)", "problem": "n\\[\\min_{c", "q_i)^2,\\": "n\\[s.t.  \\sum_{i"}, "p_i = n,\\"], "i": "gamma_i - \\delta_i = \\beta = const\\). If \\(\\beta > 0\\)", "true": "n\\[2 (p_j - c q_j) + \\beta - \\gamma_j + \\delta_j = 0\\]\n\\[= 2 (p_j - c q_j) + 2 c q_k - \\gamma_k - \\gamma_j + \\delta_j = 0\\]\n\\[= 2 p_j + 2 c (q_k - q_j) - (\\gamma_k + \\gamma_j) + \\delta_j = 0.\\]\nAll the terms in the last sum are known to be non-negative except for \\(\\gamma_k\\). Therefore", "sum_{i": "i < s"}, {"i": "i \\geq s"}, ["beta = \\frac{1}{\\sum_i q_i} \\Big( \\sum_{i: i < s} q_i \\delta_i + \\sum_{i: i \\geq s} q_i  (\\beta - 2c q_i) \\Big) \\quad (24)\\"], ["frac{1}{\\sum_i q_i} \\Big(- \\sum_{i: i < s} q_i \\delta_i - 2c \\sum_{i: i \\geq s} q_i^2  \\Big) + \\frac{\\beta \\sum_{i: i \\geq s} q_i  }{\\sum_i q_i}  \\quad (25)\\"], ["frac{\\beta \\sum_{i: i < s} q_i }{\\sum_i q_i} = \\frac{1}{\\sum_i q_i} \\Big(\\sum_{i: i < s} q_i \\delta_i - 2c \\sum_{i: i \\geq s} q_i^2  \\Big) \\quad (26)\\"], ["beta = -\\frac{\\sum q_i \\delta_i}{\\sum q_i} \\quad (27)\\"], [2, "p_j - c q_j) + \\beta + \\delta_j \\quad (28)\\"], [2, "p_j - c q_j) + 2 (c q_k - 1) - \\delta_k + \\delta_j  \\quad (29)\\"], [2, "p_j + 2 c (q_k - q_j) + (\\delta_j - \\delta_k) - 2 = 0. \\quad (3"]]