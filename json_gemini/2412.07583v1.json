{"title": "Mobile Video Diffusion", "authors": ["Haitam Ben Yahia", "Denis Korzhenkov", "Ioannis Lelekas", "Amir Ghodrati", "Amirhossein Habibian"], "abstract": "Video diffusion models have achieved impressive realism and controllability but are limited by high computational demands, restricting their use on mobile devices. This paper introduces the first mobile-optimized video diffusion model. Starting from a spatio-temporal UNet from Stable Video Diffusion (SVD), we reduce memory and computational cost by reducing the frame resolution, incorporating multi-scale temporal representations, and introducing two novel pruning schema to reduce the number of channels and temporal blocks. Furthermore, we employ adversarial finetuning to reduce the denoising to a single step. Our model, coined as MobileVD, is 523\u00d7 more efficient (1817.2 vs. 4.34 TFLOPs) with a slight quality drop (FVD 149 vs. 171), generating latents for a 14 \u00d7 512 \u00d7 256 px clip in 1.7 seconds on a Xiaomi-14 Pro. Our results are available at https://qualcomm-ai-research.github.io/mobile-video-diffusion/", "sections": [{"title": "1. Introduction", "content": "Video diffusion models are making significant progress in terms of realism, controllability, resolution, and duration of the generated videos. Starting from zero-shot video models [12, 25, 29, 35], which deploy pretrained image diffusion models to generate consistent frames, e.g., through cross-frame attention, modern video diffusion models rely on spatio-temporal denoising architectures, i.e., 3D UNets [2, 3, 18] or 3D DiTs [32, 67, 73]. This involves inflating image denoising models by adding temporal transformers and convolutions to denoise multiple frames simultaneously. Despite their impressive generation qualities, spatio-temporal denoising architectures demand high memory and computational power, which limits their usage to clouds with high-end GPUs. This hinders the wide adoption of video generation technology for many applications that require generating content locally on mobile devices.\nPrior work on accelerating video diffusion models has mostly focused on reducing the number of sampling steps [62, 71]. By extending the consistency models [58] and adversarial distillation [53] to video diffusion models, they managed to reduce the number of denoising steps from 25 to only 4 [62] and 1 step [71], which tremendously accelerates video generation. However, step distillation alone does not reduce the memory usage of the model, which is the key setback in deploying video diffusion models on mobile devices.\nThis paper is the first attempt to build video diffusion models for mobile. Starting from the spatio-temporal UNet from Stable Video Diffusion (SVD) [2, 3], as a representative for a variety of video diffusion models, we conduct a series of optimizations to reduce the size of activations to build a mobile-friendly UNet: driven by the lower resolution needs for user-generated content on phones, we opt for using a smaller latent space sufficient for generating 512 \u00d7 256 px frames. Instead of preserving the number of frames throughout the denoising UNet, we introduce additional temporal down- and up-sampling operations to extend the multi-scale representation both in space and time, which reduces the memory and computational cost with minimal loss in quality. Moreover, we discuss how naive visual conditioning through cross-attention leads to significant computational overhead that can be avoided without damaging visual quality.\nWe further accelerate the mobile-friendly UNet by reducing its parameters using a novel channel compression schema, coined channel funneling, and a novel technique to prune the temporal transformers and temporal residual blocks from the UNet. Finally, following Zhang et al. [71], we reduce the number of denoising steps to a single step using adversarial finetuning. This results in the first mobile video diffusion model called MobileVD, which is 523\u00d7 more efficient (1817.2 vs. 4.34 TFLOPs) at a slightly worse quality in terms of FVD (149 vs. 171) as reported in Fig. 1. MobileVD generates the latents for a 14 \u00d7 512 \u00d7 256 px clip in 1.7 seconds on a Xiaomi 14-Pro smartphone which uses a Qualcomm Snapdragon\u00ae 8 Gen 3 Mobile Platform."}, {"title": "2. Related work", "content": "Video generation. Fueled by advancements in generative image modeling using diffusion models, there has been notable progress in the development of generative video models [2, 3, 15, 16, 23, 24, 31, 46, 51, 74]. These video models generally evolve from image models by incorporating additional temporal layers atop spatial blocks or by transforming existing 2D blocks into 3D blocks to effectively capture motion dynamics within videos. Although these advancements have paved the way for the generation of high-resolution videos, the significant computational demands make them impractical for use on low-end devices. In this work, we address this by optimizing a representative of video generative model, SVD [2], to make it accessible to a broader range of consumer-graded devices.\nDiffusion optimization. The problem of making diffusion models efficient naturally consists of the following two parts: (i) reducing the number of denoising steps and (ii) decreasing the latency and memory footprint of each of those steps. Reducing number of steps is achieved by using higher-order solvers [39, 40, 68], distilling steps to a reduced set using progressive step distillation [33, 42, 52], straightening the ODE trajectories using Rectified Flows [34, 36, 75], mapping noise directly to data with consistency models [38, 57, 58], and using adversarial training [53, 54, 63, 71]. To decrease computational cost of each step, research has been done in weight quantization [21, 47, 55] and pruning [9, 33] as well as architectural optimization of the denoiser [10, 19, 30, 65]. In this work, following [71] we reduce number of steps to one using adversarial training and optimize the UNet denoiser using multiple novel techniques.\nOn-device generation. On-device generation has attracted interest due to its ability to address privacy concerns associated with cloud-based approaches. There have been advancements in running text-to-image generation on mobile devices and NPUs [6, 8, 9, 33, 72]. While there has been progress in the video domain with fast zero-shot video editing models [26, 64, 70], there have been no attempts to implement spatiotemporal video generative models on device due to their high computational and memory requirements, which are challenging to meet within on-device constraints. In this work, we propose the first on-device model based on SVD [2] image-to-video model."}, {"title": "3. Mobile Video Diffusion", "content": "In this section, we propose a series of optimizations to obtain a fast and lightweight version of an off-the-shelf image-to-video diffusion model [2], suitable for on-device deployment."}, {"title": "3.1. Preliminaries", "content": "Base model. We adopt Stable Video Diffusion (SVD) [2] as the base model for optimization. The SVD released checkpoint\u00b9 is an image-to-video model that by default generates 14 frames at the resolution 1024 \u00d7 576 with 25 sampling steps. To generate a video from an image, the input image is first mapped into a latent code of resolution 128 \u00d7 72 using a Variational Auto-Encoder (VAE). Then it is duplicated 14 times and concatenated with a noise latent of spatiotemporal resolution 14 x 128 x 72. The combined latent is then denoised by a conditional UNet through an iterative process. Additionally, the input image is encoded with a CLIP image embedding for use in cross-attention layers [49]. The UNet denoiser consists of four downsampling blocks, one middle block and four upsampling blocks. To handle the dynamics of video sequences, the model employs temporal blocks after spatial blocks. Notably, up- and downsampling is conducted across spatial axes only, and temporal resolution of the latent is kept constant to 14 throughout the UNet. The UNet denoiser as-is is too resource-intensive for on-device use, requiring 45.43 TFLOPs and 376 ms per denoising step on a high-end A100 GPU. Using FLOPs and latency as proxy metrics, we propose a series of optimizations to make the model suitable for on-device deployment.\nAdversarial finetuning. In addition to the high cost of the UNet, the iterative sampling process in video diffusion models further slows them down. For example, with the conventional 25-step sampling it takes 11 seconds to generate a video on a high-end A100 GPU. To reduce the cost, we follow SF-V framework [71] and use adversarial finetuning, enabling our models to generate videos in a single forward"}, {"title": "3.2. Mobile-friendly UNet", "content": "Our first modifications to SVD architecture regard optimizations along the latent and feature resolution that affect both GPU and mobile latency. Then we highlight some lossless optimizations that have big effect on mobile latency. These are the first optimizations that allow us to run the UNet on device.\nLow resolution finetuning. To satisfy the memory constraints of mobile devices, we decrease the resolution of denoising UNet input to 64 \u00d7 32 by resizing the conditioning image to 512 \u00d7 256. While the released SVD checkpoint supports multiple resolutions, we found out that the original model demonstrates deteriorated quality for our target spatial resolution as reported in Tab. 1. Therefore, we finetuned the diffusion denoiser at our target spatial size. With this optimization, computational cost and GPU latency is reduced to 8.60 TFLOPS and 82 ms respectively, see Tab. 2.\nTemporal multiscaling. To further reduce computational burden, one might lower the input resolution more heavily. However, this significantly degrades visual quality. Instead, we can additionally downscale the feature maps by a factor of 2 along either spatial or temporal axis after the first downsampling block of the UNet. To maintain the same output"}, {"title": "3.3. Channel funnels", "content": "Channel size, which refers to the width of neural network layers, is crucial for scaling models. Increasing channel size generally enhances model capacity but also increases the number of parameters and computational cost. Research"}, {"title": "3.4. Temporal block pruning", "content": "Motivation. The original UNet in the SVD model does not contain 3D convolutions or full spatiotemporal attention layers. Instead, to model motion dynamics, SVD incorporates temporal blocks after each spatial block. The output of such group is a linear combination of the spatial and temporal block outputs, $x_s$ and $r_t$ respectively, $\\alpha x_s + (1 - \\alpha) x_t$, where $\\alpha$ is a weight scalar that emphasizes spatial features when higher and temporal features when lower, see Fig. 4a. While this approach leverages image model priors when extending the model to videos, it adds computational cost. Moreover, not all of these blocks are equally important for maintaining quality. Here, we propose a learnable pruning technique to remove less important temporal blocks while minimizing quality degradation. To this end, for each temporal block we define an importance value $q_i$, $0 \\leq q_i \\leq 1$ where $i = 1,..., N$ and $N$ is number of temporal blocks. The values ${q_i}$ are trained to identify the blocks that are the most crucial for model performance. At inference time,"}, {"title": "4. Experiments", "content": "In this section, we describe our experimental setup, followed by a qualitative and quantitative evaluation of our model. Finally, we present ablations to justify our choices for the final model deployed on the device."}, {"title": "4.1. Implementation details", "content": "Dataset. For our experiments, we use a collected video dataset. We follow the data curation pipeline from Open-Sora [73, V.1.1], selecting videos with motion scores between 4 and 40, and aesthetic scores of at least 4.2. This results in a curated dataset of approximately 31k video files for finetuning.\nMicro-conditioning. UNet used by SVD, has two conditioning parameters called FPS and Motion bucket id. To obtain videos with different FPS, we chose each k-th frame from the video with randomly sampled k, 1 \u2264 k \u2264 4, and adjusted the native FPS value of the video accordingly. The notion of motion bucket has not been properly documented at time of the model release. While it is connected with the speed of motion in the video, as described in the original SVD paper, the motion estimator has not been open-sourced. For that reason, we implemented our own definition of the motion bucket using a simple heuristic. For the sampled chunk of 14 frames, we converted them to gray color, spatially downsampled to the resolution of 14 \u00d7 128 \u00d7 64, and reshaped to the matrix of size 14 \u00d7 (128\u00b764). After that,"}, {"title": "4.2. Results", "content": "MobileVD. In Tab. 2 we compare MobileVD to our base model. As the results indicate, each optimization reduces speed of inference on a mobile phone. Decreased spatial resolution and optimized cross-attention together unlock on-device execution with a latency of 3.6 seconds. Temporal downsampling layers in UNet make inference 29% faster. Additionally, temporal blocks pruning reduces phone latency by 13%, and channel funneling further decreases it by 9%. Empirically, we found that a difference of up to 20 FVD units does not significantly affect visual quality and typically falls within the standard deviation when using different random seeds. Based on that, we see that our optimizations have minimal impact on FVD.\nSOTA comparison. In Tab. 1 and Fig. 1 we compare to the"}, {"title": "4.3. Ablations", "content": "In this section, we evaluate our design choices through ablation experiments. Unless otherwise specified, we use the SVD checkpoint with low-resolution input, optimized cross-attention, and adversarial finetuning as the reference model, cf. Tab. 2."}, {"title": "4.3.1. Resolution impact in UNet", "content": "In Tab. 3 we compare different latent multiscaling optimizations proposed in Sec. 3.2. Specifically, we investigate the impact of inserting spatial or temporal multiscaling layers after the first UNet block in terms of FLOPs, latency, and"}, {"title": "4.3.2. Funnel finetuning", "content": "Fun-factor and funnel initialization. Reducing the width of affine layers in the model is a form of lossy compression, and overly aggressive fun-factor values will hurt the model performance. In Tab. 4, we observe the impact of the fun-factor. Reducing the fun-factor to 0.25 results in a performance loss of 22 FVD units compared to fun-factor of 1 (i.e., no compression). To avoid performance degradation from stacking multiple optimizations described in Sec. 3, we set the fun-factor to 0.5 for the deployed model. Additionally, the results highlight that the proposed coupled singular initialization (CSI) is essential for optimal funnel behavior, whereas the standard He initialization [20] is suboptimal.\nFunnel merging and low-rank layers. We compare channel funnels with multiple baselines in terms of FLOPs, on-device latency and FVD. All baselines are applied on the same attention layers, unless specified otherwise. The first baseline uses channel funnels but merges funnel and weight matrices during training, hence mimicking behavior at inference time as shown in Fig. 3. We report in Tab. 5 that keeping funnel and weight matrices separate at training performs equally well. The second baseline is applying funnels to convolutions in ResNet blocks instead of attention layers. While we obtain favourable FVD and even greater gain in"}, {"title": "4.3.3. Temporal blocks pruning", "content": "As mentioned in Sec. 3.4, in our experiments we found it possible to reduce up to 70% of all temporal blocks in the UNet. Notably, even with such a high pruning rate the quality is comparable to the original model: it achieves FVD of 127 and requires 14% less FLOPs, while the original model has FVD of 139. However, pruning even further seems far from straightforward. FVD degrades to the values above 200 with"}, {"title": "5. Conclusion", "content": "This paper introduced the first mobile-optimized video diffusion model, addressing the high computational demands that have limited their use on mobile devices. By optimizing the spatio-temporal UNet from Stable Video Diffusion and employing novel pruning techniques, we significantly reduced memory and computational requirements. Our model, MobileVD, achieves substantial efficiency improvements with minimal quality loss, making video diffusion technology feasible for mobile platforms.\nLimitations. Despite the impressive acceleration achieved as a steppingstone towards video generation on phones, the output is currently limited to 14 frames at a low resolution of 256 \u00d7 512 pixels. The next step involves leveraging more efficient autoencoders to achieve higher spatial and temporal compression rates, enabling the generation of larger and longer videos at the same diffusion latent generation cost.\nPotential negative impacts. Our work is a step towards making video generation technology accessible to a broader audience, allowing users to create content on their phones with fewer access controls and regulations compared to cloud-based solutions."}, {"title": "A. Additional results", "content": null}, {"title": "A.1. High-resolution generation", "content": "In addition to our main effort to port the SVD model on a mobile phone, we tested if our set of optimizations can be applied to a high-resolution model. For this purpose, we trained a model called MobileVD-HD which is capable of generating 14-frame videos with spatial size of 1024 \u00d7 576 px. Architecture hyperparameters used to finetune MobileVD-HD, i.e. temporal multiscaling factor, fun-factor and number of pruned temporal blocks, were the same as for our low-resolution MobileVD. We report the results of that model in Tab. 7. As shown, it achieves visual quality on par with SF-V while decreasing its GPU latency by 40%."}, {"title": "A.2. Visual quality metrics", "content": "Although FVD is a widely used metric for visual quality in the community, its ability to assess the faithfulness of motion as opposed to the appearance of individual frames is sometimes argued [5, 14, 56]. For that reason, we also evaluate the quality of different models using a recently proposed JEDI (JEPA Embedding Distance) metric [41, v.0.1.3]. JEDi reportedly demonstrates much better correlation with the human preference than FVD. Since Luo et al. [41] have not recommended guidelines for comparison between generative models using their metric, we opted for the setup similar to FVD computation [2, 71]. In detail, we used the same set of clips from UCF-101, and similarly downsampled the videos to the resolution closest to 320 \u00d7 240, preserving the original aspect ratio, with the central crop afterwards. In Tab. 8 we provide the extended quantitative results. We observe that in general the new metric is better aligned with architecture optimizations that we apply."}, {"title": "B. Additional details", "content": null}, {"title": "B.1. Training", "content": "For diffusion training, we used AdamW optimizer [37] with learning rate of 1 \u00d7 10-6 and weight decay 1 \u00d7 10-3, while other hyperparameters were default. During adversarial finetuning, we also used AdamW. For generator the learning rate was equal to 1.25 \u00d7 10\u22126, and for discriminator we set it 10 times higher. For logits of importance values, used for pruning of temporal blocks, learning rate was equal to 1 \u00d7 10-3. Momentum weights for both optimizers we set as follows \u03b2\u2081 = 0.5, \u03b22 = 0.999. For generator, the weights for adversarial and pseudo-Huber loss were equal to 1 and 0.1 respectively. For discriminator, weight of R\u2081 penalty was 1 \u00d7 10-6, and we applied it once per 5 iterations, as recommended in [27].\nFor high-resolution training, the first (diffusion) stage lasted for 10k iterations with total batch size of 4. The second (adversarial) stage comprised 30k iterations with batch size of 2. The learning rates for adversarial finetuning were twice as lower as for low-resolution case, except for the learning rate of importance values. R\u2081 penalty was applied at each step. Other training aspects were the same both for MobileVD and MobileVD-HD."}, {"title": "B.2. Decoding latents", "content": "As our approach stems from SVD, the presented MobileVD is also a latent model. Therefore, each generated latent code must be decoded to raw RGB pixels. For all the experiments reported in the main text, we used the native autoencoder weights released alongside the SVD model itself. The decoding was conducted independently for each frame. Notably, this model is relatively slow on device: it takes 91.5 ms per frame, resulting in 1,280 ms for decoding the full generated video. This timing is comparable with the latency of MobileVD (1,780 ms). As an alternative, we used the decoder from TAESD autoencoder\u00b3. It is significantly faster on a smartphone: 6.4 ms per frame, or 90 ms for the full video. At the same time, the difference in quality metrics is negligible, see Tab. 9."}, {"title": "B.3. Channel funnels", "content": "Attention layers. Consider a query and key projection matrices in a self-attention similarity map computation, $XW_q (XW_k)^T$ with layer input X having a shape of L \u00d7 Cin, and $W_q$ and $W_k$ of $C_{in} \u00d7 C_{inner}$. With funnel matrices $F_q$ and $F_k$ of size $C_{inner} \u00d7 c'$, we modify the aforementioned bilinear map as $XW_qF_q (XW_kF_k)^T = XW_qF_q \\mathbb{F} W_k^T X^T$. We apply our coupled singular initialization (CSI) by setting $F_q = W_q^T U_D \\Sigma^{1/2}$ and $F_k = \\Sigma^{1/2} V W_k^T$. For value and output projections matrices the initialization is applied in the way discussed in the main text.\nConvolutional layers. Applying the same initialization to a pair of convolutional layers is not straightforward. Weight tensors of 2D convolutional layers are 4-dimensional, and therefore computation of the effective weight tensor is not obvious. However, we make use of the following observation. Consider input tensor X with shape h \u00d7 W \u00d7 Cin. We refer to its 2-dimensional pixel coordinate as p, and therefore Xp is a vector with Cin entries. Let W be a convolutional kernel with size $k_h \u00d7 k_w \u00d7 C_{out} \u00d7 C_{in}$, and we refer to its spatial 2-dimensional coordinate as $\\bar{q}$, while $\\bar{q} = 0$ is a center of a convolutional filter. For j-th output channel, $W_{\\bar{q},j}$ is also a vector with Cin entries. The layer output Y \u2208 $R^{h\u00d7w\u00d7 C_{out}}$ can be computed as\n$Y_{p,j} = \\sum_{\\bar{q}} (W_{\\bar{q},j}, X_{p+\\bar{q}})$,  (1)\nwhere (,) denotes inner product. Simply speaking, this means that convolution can be treated as a linear layer with weight matrix of shape $C_{out} \u00d7 (k_h.k_w. C_{in})$ applied to each flattened input patch.\nAt the same time, another way of reshaping the kernel is also possible. Consider a tensor E of shape $k_h \u00d7 k_w \u00d7 C_{out}xhxw$ defined as\n$E_{\\bar{q},j,p} = (W_{\\bar{q},j}, X_{p})$.  (2)\nIn other words, the convolution kernel reshaped as $(k_h k_wC_{out}) \u00d7 C_{in}$ is multiplied by features of each input pixel. Then Eq. (1) can be rewritten as\n$Y_{p} = \\sum_{\\bar{q}} E_{\\bar{q}, p+\\bar{q}} = \\sum_{\\bar{q}} (E_{\\bar{q}}, \\delta_{\\bar{q}, p})$,  (3)\nwhere d is a 4-dimensional identity tensor, i.e. $\\delta_{\\bar{q}, \\bar{p}} = 1$ if $\\bar{q} = \\bar{p}$ and 0 otherwise.\nHaving said that, a sequence of two convolutions can be presented as (i) flattening the input patches; (ii) matrix multiplication by the first kernel reshaped according to Eq. (1); followed by (iii) matrix multiplication by the second kernel reshaped as in Eq. (2); and with (iv) independent of kernels operation described by Eq. (3) afterwards. Therefore, coupled singular initialization can be applied to the product of matrices used in steps (ii) and (iii). We follow this approach and introduce funnels to the pairs of convolutions within the same ResNet block of denoising UNet."}, {"title": "B.4. Pruning of temporal adaptors", "content": "Practical considerations. In the main text we described that we transform the update rule of the temporal block as"}, {"title": "inequalities", "content": "follows $x_s + \\mathbb{Z} (1 - \\alpha) r_t$, where $x_s$ and $r_t$ are outputs of the spatial and temporal layers respectively, $\\alpha$ is the learnable weight, and 2 is a zero-one gate multiplier. Note that if the temporal block is pruned, i.e. $\\mathbb{Z}_i = 0$, then the gradient of the loss function w.r.t. the temporal block's learnable parameters equals zero. This affects the gradient momentum buffers used by optimizers. For that reason, we do no update the momentum of the temporal block's parameters in case it has been pruned by all the devices at current iteration of multi-GPU training.\nIn the network, we parametrize the importance values qi with the sigmoid function with fixed temperature value of 0.1. In the same way weight coefficients $\\alpha_i$ were reparametrized. For faster convergence, each value qi is initialized with the weight of the corresponding temporal block, i.e. $1 - \\alpha_i$. We also found necessary to set the learning rate for the logits of importance values qi significantly higher than for the other parameters of the denoising UNet.\nConstrained optimization. As discussed in the main text, we relate the importance values of temporal blocks ${q_i}_{i=1}^{N}$ to their inclusion probabilities for sampling without replacement ${p_i}_{i=1}^{N}$ by solving the following constrained optimization problem:\n$\\min_{c,\\{P_i\\}i} \\sum_i (P_i - cQ_i)^2$,  (4)\ns.t. $\\sum P_i = n$,\n$0 \\leq P_i \\leq 1$.\nTo solve it, we employ the common method of Lagrange multipliers assuming that all q-values are strictly positive. W.l.o.g. we consider the case of sorted values ${q_i}_i$, i.e. $q_1 \u2265 q_2 \u2265 \\dots \u2265 q_N > 0$. In detail, we define a Lagrangian\n$\\mathbb{L}(c, \\{p_i\\}, \\lambda, \\beta, \\{ \\gamma_i \\}, \\{ \\delta_i \\})$\n$\\ = \\lambda \\sum_i (P_i - cQ_i)^2$\n$\\ + \\beta (\\sum P_i -n)$\n$\\ + \\sum \\gamma_i (-P_i)$\n$\\ + \\sum \\delta_i (P_i - 1)$  (5)\nand aim to solve the following system of equalities and inequalities\n$\\frac{\\partial \\mathbb{L}}{\\partial P_i} = 2\\lambda (P_i - cQ_i) + \\beta - \\gamma_i + \\delta_i = 0$ Vi,\n$\\frac{\\partial \\mathbb{L}}{\\partial c}= 2\\lambda \\sum_i (cQ_i - P_i) Q_i = 0$,  (7)\n$\\sum P_i = n$,  (8)\n$\\gamma_i P_i = 0$ Vi,  (9)\n$\\delta_i (P_i - 1) = 0$ Vi,  (10)\n$\\gamma_i, \\delta_i \u2265 0$ Vi,  (11)\n$\\lambda^2 + \\beta^2 + \\sum_i (\\gamma_i^2 + \\delta_i^2) > 0$.  (12)\nCase $\\lambda = 0$. In this case Vi $\\gamma_i - \\delta_i = \\beta = const$. If $\\beta > 0$, then Vi $\\gamma_i > \\delta_i \u2265 0 \u21d2 Vi > 0 \u21d2 P_i = 0, which leads to a contradiction. Cases $\\beta < 0$ and $\\beta = 0$ also trivially lead to contradictions.\nCase $\\lambda = 1$. First, we derive that $c\\sum_i q_i = \\sum_i P_iq_i$, and thus\n$c = \\frac{\\sum_i P_iq_i}{\\sum q_i} > 0$.  (13)\nSince Vi $\\frac{\\partial \\mathbb{L}}{\\partial P_i} = 0$, then $\\sum \\frac{\\partial \\mathbb{L}}{\\partial P_i}q_i = 0$.\n$\\sum \\frac{\\partial \\mathbb{L}}{\\partial P_i}q_i = \\sum [2 (P_i - cQ_i) + \\beta - \\gamma_i + \\delta_i]q_i$  (14)\n$ = 2\\sum Q_i (P_i - cQ_i) + \\beta \\sum Q_i - \\sum Q_i (\\gamma_i - \\delta_i)$  (15)\n$\\sum Q_i = \\beta \\sum Q_i - \\sum Q_i (\\gamma_i - \\delta_i)$  (16)\n$ = 0$,  (17)\nand therefore,\n$\\beta = \\frac{\\sum Q_i (\\gamma_i - \\delta_i)}{\\sum Q_i}$.  (18)\nLemma B.1. For all indices i it holds true that $\\gamma_i = 0$.\nProof. Proof is given by contradiction. Let us assume that \u2203k $\\gamma_k > 0 \u21d2 P_k = 0 \u21d2 \\delta_k = 0 \u21d2 -2cQ_k + \\beta + \\gamma_k = 0 \u21d2 \\beta = 2cQ_k + \\gamma_k > 0$. Then for any index j such that j > k and, consequently, $q_j \u2264 q_k$, the following equality holds true:\n$2 (P_j - cQ_j) + \\beta - \\gamma_j + \\delta_j$  (19)\n$ = 2 (P_j - cQ_j) + 2cQ_k + \\gamma_k - \\gamma_j + \\delta_j$  (20)\n$ = 2P_j + 2c (Q_k - Q_j) + (\\gamma_k - \\gamma_j) + \\delta_j$  (21)\n$ = 0$.  (22)\nAll the terms in the last sum are known to be non-negative except for $ \\gamma_kj$. Therefore, $ \\gamma_k \u2264 \\gamma_jY_j > 0 \u21d2 P_j = 0$. We define index s as the largest index for which $\\gamma_s = 0$, i.e. $\\gamma_1 = \\dots = \\gamma_s= 0, \\gamma_{s+1} > 0$. Note that s > n, since"}, {"title": "otherwise the equality $\\sum_i p_i = n$ cannot be satisfied", "content": "Also, $P_j = 0$ for j > s. Now we can rewrite Eq. (18) as follows\n$\\beta = \\frac{1}{\\sum Q_i} \\sum Q_i (\\gamma_i - \\delta_i)$  (23)\n$ = \\frac{1}{\\sum Q_i} \\sum Q_i(\\gamma_i - \\delta_i) + \\sum Q_i (0 - \\delta_i)$  (24)\n$ = \\frac{1}{\\sum Q_i} [ \\sum Q_i(\\gamma_i - \\delta_i) - 2c\\sum Q_i^2", "Q_i^2": "frac{\\beta \\sum Q_i}{\\sum_i Q_i}}{1} = \\frac{\\sum Q_i^2}{1 + \\sum Q_i}$.  (26)\nNote that LHS is obviously strictly positive, while RHS is non-positive.\nSince Vi $\\gamma_i = 0$, we rewrite Eq. (18),\n$\\beta = \\frac{\\sum Q_i \\delta_i}{\\sum_i Q_i}$.  (27)\nIf Vi $\\delta_i = 0$, then it is also true that $\\beta = 0$, leading to Vi $P_i = cQ_i$. This is the case when inclusion probabilities are exactly proportional to the importance values. However, this is possible if and only if the maximum value q1 is not too large in comparison with other values, since otherwise P1 > 1.\nIn this last case \u2203k $\\delta_k > 0 \u21d2 P_k = 1 \u21d2 2 (1 \u2212 cQ_k) + \\beta + \\delta_k = 0 2 (cQ_k \u2212 1) \u2212 \\delta_k. For any index j such that j < k we have\n$2 ("}]}