{"title": "A Systematic Review of Intermediate Fusion in Multimodal Deep Learning for Biomedical Applications", "authors": ["Valerio Guarrasi", "Fatih Aksu", "Camillo Maria Caruso", "Francesco Di Feola", "Aurora Rofena", "Filippo Ruffini", "Paolo Soda"], "abstract": "Deep learning has revolutionized biomedical research by providing sophisticated methods to handle complex, high-dimensional data. Multimodal deep learning (MDL) further enhances this capability by integrating diverse data types such as imaging, textual data, and genetic information, leading to more robust and accurate predictive models. In MDL, differently from early and late fusion methods, intermediate fusion stands out for its ability to effectively combine modality-specific features during the learning process. This systematic review aims to comprehensively analyze and formalize current intermediate fusion methods in biomedical applications. We investigate the techniques employed, the challenges faced, and potential future directions for advancing intermediate fusion methods. Additionally, we introduce a structured notation to enhance the understanding and application of these methods beyond the biomedical domain. Our findings are intended to support researchers, healthcare professionals, and the broader deep learning community in developing more sophisticated and insightful multimodal models.", "sections": [{"title": "1. Introduction", "content": "Deep learning has transformed the landscape of computational approaches in biomedical research, offering unique capabilities in handling complex, high-dimensional data [1]. Multimodal deep learning (MDL), which integrates multiple data types, has emerged as an innovative approach, leveraging the power of deep learning algorithms to interpret and integrate diverse data types, enhancing the robustness and accuracy of predictive models [2]. Unlike traditional unimodal methods, MDL utilizes a variety of data sources such as imaging, textual data, and genetic information, to name a few, providing a more comprehensive understanding of complex biomedical phenomena. This approach is particularly relevant in a field where data diversity and volume are rapidly expanding, offering unprecedented opportunities for advancements in diagnosis, treatment, and patient care.\nIn the domain of multimodal learning, the integration of data from multiple sources can be achieved through various fusion techniques, i.e., early, late and intermediate fusion. Early fusion combines features at the data level, potentially losing unique modality-specific characteristics. Formally, let $x_1, x_2, ..., x_n$ represent feature vectors from n different modalities. The fusion function $F$ combines these features into a single vector:\n$x = F(x_1, x_2, ..., x_n)$  (1)\nwhich is then fed into a learning model $f$, resulting in the output $y$:\n$y = f(x)$  (2)\nLate fusion, on the other hand, occurs at the decision level, often missing opportunities for deeper interaction between modalities. Formally, the feature vectors $x_1, x_2, ..., x_n$ are fed into independent models $f_1, f_2, ..., f_n$, producing separate predictions $y_1, y_2, ..., y_n$. The fusion function $F$ then combines these predictions:\n$y = F(y_1, y_2, ..., y_n)$  (3)"}, {"title": "", "content": "Intermediate fusion, striking a balance, integrates data at the feature extraction stage, allowing for a more effective combination of modality-specific features. Formally, the feature vectors $x_1, x_2, ..., x_n$ are processed by separate components of the model $f_1, f_2, ..., f_n$, yielding intermediate representations $h_1, h_2, ..., h_n$. These intermediate representations are fused using $F$\n$h = F(h_1, h_2, ..., h_n)$  (4)\nand then a final component $f$ processes $h$ to produce the output\n$y = f(h)$  (5)\nFurther details on each component can be found in Section 3.1. Among the various fusion techniques employed in MDL, intermediate fusion stands out for its ability to effectively integrate information at essential stages of the learning process, potentially leading to more accurate and robust models. This method is especially beneficial in biomedical applications where different data types, like imaging and genomic information, need to interact closely to produce meaningful insights. The main feature of intermediate fusion lies in its ability to preserve and utilize the distinct qualities of each data type, enhancing the model's capability to handle the complexities inherent in biomedical data.\nThe utility of deep learning in the context of intermediate fusion in multimodal settings, particularly within biomedical research, is marked by its ability to process and fuse distinct data modalities at a more abstract level [3]. In intermediate fusion, deep learning models are particularly capable at entangling the complex, nonlinear relationships that often exist between different modalities in biomedical data. This understanding is critical for accurately interpreting the multimodal nature of such data, where each modality carries its own information. By leveraging deep learning's ability in feature extraction and representation learning, intermediate fusion methods can effectively bridge the gap between these diverse data sources, leading the way for more insightful biomedical analysis. This approach not only may enhance the accuracy of predictive models but also may unveil patterns and interactions that might be missed by more conventional methods.\nThe application of MDL in the biomedical domain faces unique challenges, notably due to the heterogeneity and high dimensionality of the data [4]. Biomedical datasets often comprise varied data modalities, each with distinct characteristics and scales, making it difficult to achieve effective integration"}, {"title": "1.1. Objective of the Review", "content": "The primary objective of this systematic review is to comprehensively analyze and formalize the current intermediate fusion methods of MDL utilized in biomedical applications. The field of intermediate fusion in MDL, particularly within biomedical contexts, is still in its early stages. As such, there is a significant need for a structured analysis and for introducing a formal notation that not only categorizes and evaluates these methods, but also provides a framework that can be extended beyond the biomedical domain."}, {"title": "2. Methodology", "content": "2.1. Inclusion/exclusion criteria\nOur systematic review adheres to specific inclusion and exclusion criteria to ensure a focused and relevant analysis of intermediate fusion in MDL for biomedical applications:\n\u2022 Date: We include studies published up to the present day to capture the most recent advancements in the field.\n\u2022 Language: Only articles published in English are considered to ensure accessibility and standardization in analysis.\n\u2022 Study Design: We exclude review articles, non-peer-reviewed sources such as opinion pieces, editorials, letters, and conference abstracts, as they do not provide original research or detailed methodology.\n\u2022 Target Application: The focus is on articles that explore at least one biomedical application. Articles centered on non-biomedical applications or unrelated fields are excluded."}, {"title": "", "content": "\u2022 Modalities: We include studies utilizing multiple modalities, considering both homogenous (e.g., imaging) and heterogeneous types (e.g., imaging combined with tabular data). Studies focusing solely on a single modality or not involving the fusion of multiple modalities are excluded.\n\u2022 Fusion: Articles employing deep learning architectures for fusion, particularly those using intermediate fusion in an end-to-end manner, are included. Articles primarily using early or late fusion methods or non-deep learning approaches are excluded.\n2.2. Search Strategy\nOur systematic review implemented a comprehensive search strategy to identify relevant literature focusing on intermediate fusion methods in MDL within biomedical applications. The search strategy was meticulously crafted to encompass a wide range of pertinent studies while maintaining specificity to the review's objectives.\nThe literature search was conducted across multiple electronic databases, ensuring a thorough and diverse collection of articles. The databases included are: PubMed, IEEE Xplore, Scopus and Google Scholar.\nTo capture the breadth of literature relevant to our review, we utilized a set of synonyms and keywords across three main categories:\n\u2022 Multimodal Deep Learning:\n$A = \\{\\text{\\text{Multimodal Deep Learning\"}},\\atop \\text{\\text{Multimodal Fusion\"}}, \\atop \\text{\\text{Multimodal Representation Learning\"}}\\}^\n \\text{\\textMultimodal Representation Learning}\"}\\}^\n\u2022 Biomedical Context:\n$\nB = \\{\\text{\\text{Biomedical}\"}},\\atop \\text{\\text{Medical}\"}}, \\atop \\text{\\text{Healthcare}\"}}, \\atop \\text{\\text{Clinical}\"}}, \\atop \\text{\\text{Biomedicine}\"}}, \\atop \\text{\\text{Medicine}\"}\n$\n\u2022 Intermediate Fusion:\n$C = \\{\\text{\\text{Joint fusion\"}},\\atop \\text{\\text{Intermediate fusion\"}}, \\atop \\text{\\text{Mid-level fusion\"}}, \\atop \\text{\\text{Feature-level fusion\"}}\n$\nThe search query was formulated using a combination of these synonyms, structured as follows:\nQuery = (A1 OR A2 OR ...) AND (B1 OR B2 OR ...) AND (C1 OR C2 OR ...)\nThis structure allowed for a comprehensive yet focused search, ensuring that the results were pertinent to the scope of our review.\nTo understand the current state and trajectory of research in this area, we conducted a comprehensive search analysis by examining the publication numbers across various subdomains of multimodal learning,. We can observe that the significant drop of publications from \u201cMultimodal Learning\u201d to \u201cMultimodal Deep Learning\u201d highlights that, while multimodal learning is a broad field, the incorporation of deep learning techniques is a more focused area. This reflects the growing interest in leveraging deep learning's feature extraction and representation learning capabilities within multimodal settings. The slight narrowing for \u201cMultimodal Learning in Biomedical Applications\u201d and for \u201cMultimodal Deep Learning in Biomedical Applications\u201d illustrates the increasing application of these methodologies in this field. Furthermore, we notice from the progression of publications in \u201cMultimodal Deep Learning\u201d to \u201cMultimodal Deep Learning via Intermediate Fusion\u201d suggests that intermediate fusion is an emerging and specialized area within the broader field of MDL. The fact that only 2% publications are categorized under\u201cMultimodal Deep Learning in Biomedical Applications via Intermediate Fusion\u201d indicates that this area, despite its promising applications, remains relatively underdeveloped. This gap underscores the need for further research and exploration. These results justify the necessity of your systematic review, which aims to lay the foundational groundwork by defining further research and development in this promising field.\n2.3. Study Selection\nThe study selection process for this systematic review was meticulously designed to ensure the inclusion of the most relevant and high-quality studies"}, {"title": "3. Intermediate Fusion in Biomedical Applications", "content": "In this section, we present the various components that constitute a model employing intermediate fusion. These components are presented in the order they appear in the model: beginning with the modalities as inputs (Section 3.2), followed by the unimodal module that extracts unimodal features (Section 3.3), the fusion module that combines these features (Section 3.4), the multimodal module that processes the fused features further (Section 3.5), and finally the target as the output of the"}, {"title": "3.1. Definitions", "content": "Intermediate fusion, as already introduced in Section 1, within the domain of MDL, is an approach that involves extracting features from different modalities using specialized unimodal neural networks, and then merging these features into a fused multimodal representation. This fused representation is subsequently fed into another neural network to yield the final outcome. A distinctive feature of intermediate fusion is that, via the back-propagation of the loss, both the unimodal and multimodal modules are trained. This allows for continuous improvement in capturing the intra- and inter-relationships inherent in multimodal data. This methodology is particularly useful in complex fields like biomedicine.\nThis method, can be broken down into several key components: modalities, unimodal modules, fusion module, multimodal module, and the target. Each of these plays a critical role in the overall functionality and efficacy of the fusion process:\n\u2022 Modalities: Modalities, denoted as $x_1, x_2, ..., x_n$ refer to the different types of data or sources of information used in the model. In the context of biomedical applications, these could include imaging data, textual or tabular clinical notes, genetic information, or other relevant medical measurements.\n\u2022 Unimodal Module: The unimodal modules, denoted as $f_1, f_2, ..., f_n$ refer to the parts of the network that processes each modality independently. Here, deep learning models are used to extract features, denoted as $h_1, h_2, ..., h_n$, specific to each modality.\n\u2022 Fusion Module: The fusion module $F$ is the core of intermediate fusion, which takes the features extracted by unimodal modules and combines them into a unified multimodal representation, denoted as $h$. This fusion can be performed using various techniques like concatenation, averaging, or more complex operations that account for interactions between modalities."}, {"title": "", "content": "\u2022 Multimodal Module: After fusion, the combined features are fed into the multimodal module $f$. This part of the network processes the fused representation to extract insights that are only apparent when considering all modalities together.\n\u2022 Target: The target $y$ refers to the output of the network. In biomedical applications, this could be a diagnosis, prognosis, or any other relevant medical outcome. The target guides the choice of the loss function, which in turn influences the back-propagation to iteratively improve the layers part of the unimodal, fusion and multimodal modules.\nThe subsequent sections will delve into the specifics of each component, analyzing their functions and significance in the context of intermediate fusion for biomedical applications."}, {"title": "3.2. Modalities", "content": "MDL with intermediate fusion represents a rapidly growing field in Artificial Intelligence (AI), where information from multiple modalities is integrated at intermediate stages of a deep neural network. Each modality provides unique information, enriching the model's comprehension of context and thereby enhancing its performance. The different types of data offer complementary perspectives on complex medical scenarios, and their fusion provides a deeper understanding. For instance, medical image analysis can be integrated with textual data extracted from medical reports or laboratory data to obtain a more comprehensive view of a medical condition. Consequently, integrating multiple biomedical modalities strengthens predictive accuracy and fortifies models against input data fluctuations, amplifying their reliability and utility in practical applications.\n3.2.1. Type of Modalities\nFrom our analysis of the 54 articles included in our survey, we identified 6 macro-modalities:\n\u2022 Imaging: this modality encompasses various medical images that serve crucial roles in both diagnosis and ongoing monitoring of medical conditions. X-rays, Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET), Ultrasounds (US) are included in this category.\n\u2022 Tabular: under this modality fall tabular clinical data (spanning from demographic information to laboratory measurements), genomics data (which are used to analyze disease risk, genetic predisposition, or treatment response) and data related to drugs chemical features to analyze their functionality.\n\u2022 Text: textual data comprises free-text reports and transcripts documenting question-answer dialogues between healthcare professionals and patients.\n\u2022 Time Series: this category encompasses various types of signals. Specifically, we distinguish between electrical signals, clinical time series, and tactile signals. Electrical signals are derived from electrocardiograms (ECG), electroencephalograms (EEG), electroglottographs (EGG), electromyography (EMG), and electrodermal activity (EDA)."}, {"title": "3.2.2. Data Origin", "content": "Given the importance of modalities for MDL in biomedicine, we analyzed the origin of the datasets used in each of the 54 articles included in our survey. We specifically distinguished whether the data was real or synthetic and this analysis reveals a significant trend: a substantial majority of the studies rely on real data, while only a marginal proportion, specifically 2 out of 54 articles [34, 35], also incorporate synthetic data generated by generative adversarial networks for models' training. This observation aligns with the expectation that real data are typically reliable and representative of reality, especially in the biomedical domain. At the same time, it underscores that synthesizing data using generative adversarial networks still poses an open technical challenge that researchers may choose to address to bridge the gap in big data in the biomedical domain."}, {"title": "3.2.3. Data Source", "content": "The aforementioned real data sources mostly come from publicly available datasets. Indeed, only about 30% of the articles make use of private datasets. The presence of a significant number of articles utilizing public datasets suggests that accessible and usable data resources exist for research in this field, enabling researchers to conduct studies and develop models without necessarily having to create or acquire private data. Acquiring, annotating, and managing private datasets can be costly and time-consuming, and their use may sometimes be limited by privacy concerns. In contrast, utilizing existing public datasets can be a convenient and practical solution for many researchers, further promoting comparability and reproducibility in research.\nA total of 52 distinct public datasets are used in the analyzed articles, with Table 1 presenting the ones used in at least two articles. Detailed information on public datasets used in only one article can be found in Supplementary Material A.\nThe datasets listed in Table 1 primarily include modalities within the macro-categories of imaging, e.g., CT scans, X-rays, and MRI, and tabular data, e.g., clinical and genomics data. As we explored in Section 3.2.1, these are the most commonly utilized modalities in the analyzed articles, given their prevalence in the medical domain. The table shows that the most commonly used datasets, e.g., COBRE and MIMIC-CXR, appear in only 4 articles. This clearly indicates the absence of a benchmark dataset utilized in the majority of studies, making it difficult to establish the overall effectiveness of the models."}, {"title": "3.2.4. Sample Size", "content": "A crucial factor to consider when training MDL models with intermediate fusion techniques is the sample size of the dataset being used, i.e., the number of instances in a dataset. Intermediate fusion involves combining different input modalities into a shared layer within a neural network to enhance the overall performance of the model. This process is particularly complex and preferably requires large datasets so that the model has enough examples from each modality to learn the relationships between them effectively. Additionally, since intermediate fusion utilizes large networks with numerous weights, it is unsuitable for datasets with limited samples. The sample sizes of the datasets involved in the articles vary as follows: less than 100 samples for 7 datasets, 101 to 500 samples for 24 datasets, 501 to 1000 samples for 14 datasets, 1001 to 1500 samples for 6 datasets, and more than 1501 samples for 27 datasets.\nThese findings underscore the prevalence of moderate-sized datasets (with"}, {"title": "3.2.5. Class Distribution", "content": "In addition to the necessity of employing large datasets, if one intends to tackle a classification problem with intermediate fusion techniques, it is equally important to have balanced datasets. Balancing the diverse classes within the dataset is essential to prevent the model from being biased towards a particular class at the expense of others. However, real-world data, especially in the medical field, often exhibit imbalanced distributions. Since intermediate fusion methods are particularly sensitive to class imbalance, we further analyze the articles that tackle classification tasks to determine whether they use balanced datasets. To this end, we employed the Gini-index [70], denoted as G, computed as:\n$G = 1 - \\sum_{i=1}^{|C|}P(C_i)^2$  (6)\nwhere C is the set of classes, |C| is the number of classes, and $P(C_i)$ is the class prior probability. As a consequence, datasets can be defined as:\n\u2022 Balanced: if 0.48 < G \u2264 0.5;\n\u2022 Imbalanced: if 0.42 < G < 0.48;\n\u2022 Highly Imbalanced: if 0 < G < 0.42.\nAs an example, in a binary classification problem, these ranges of Gini-index correspond to having a balanced dataset when the class priors range from 50-50 to 41-59, an imbalanced dataset when the class priors range from 40-60 to 31-69, and a highly imbalanced dataset when the class priors range from 30-70 to 0-100."}, {"title": "3.2.6. Missing Modalities", "content": "A common challenge in biomedical multimodal data is the presence of one or more missing modalities, an issue emerging when certain expected data modalities for a given sample are unavailable. This can happen due to various factors, such as registration errors, technical limitations in capturing specific"}, {"title": "3.2.7. Modalities Combination", "content": "We define the multimodal dimension characterizing the studies exploiting the number of modalities provided as input to deep learning models. We note that a significant proportion of studies (74% of articles) utilize bimodal datasets, indicating a prevalent trend in research. Additionally, 20% of the articles employ trimodal datasets, while a smaller fraction, only 6%, engage with high-modal datasets, characterized by the incorporation of more than three modalities. The numbers on the edges represent the frequency of articles in which two modalities, indicated as nodes, are employed together. The width of the edges is drawn proportional to the corresponding number of articles. The sizes of the nodes are also arranged according to the number of articles that the modalities are used for. We observe that the most prevalent combination is the imaging modality with the tabular modality, appearing in 16 articles, which accounts for 40% of the bimodal articles. Among these, the most common pairings involve CT scans and clinical data [61, 62, 75, 77, 81], followed by MRI scans with clinical data [35, 47, 48, 79], and then CT scans with genomics data [57, 58, 59]. These findings align with expectations because, as outlined in Section 3.2.1, the imaging and the tabular are the most common macro-modalities and, within them, CT, MRI, clinical data, and genomics data are prevalent. The integration of imaging and tabular data offers a more comprehensive view of patients, with images providing detailed visual information on anatomy and"}, {"title": "3.2.8. Feature Extraction", "content": "As discussed in Section 3.2.7, understanding how modalities are combined is fundamental. Equally crucial, however, is considering the application of feature extraction techniques to data before they are fed into the models. Feature extraction is not part of the training process and should not be confused with the unimodal module. Feature extraction can play a pivotal role in preparing data for training deep learning models, involving the transformation of raw data into a set of representative features. These features serve as the actual input for the unimodal models and may belong to a different category of modalities than the orig-"}, {"title": "3.3. Unimodal Module", "content": "The power of intermediate fusion lies in its ability to capture both intra- and inter-modal relationships inherent in multimodal data. In contrast to"}, {"title": "3.3.1. Architecture Types", "content": "The analysis of the literature suggests categorizing the reviewed papers into two classes: homogeneous and heterogeneous. We say that a multimodal model is homogeneous when all constituent unimodal networks share a uniform type of architecture; conversely, we say it is heterogeneous when at least one network diverges from the others,"}, {"title": "3.3.2. Modality Distributions", "content": "We now turn our attention to the specific modalities employed by these unimodal architectures. These modalities represent the original data type which, in some instances, undergoes feature extraction through either handcrafted methods or deep learning algorithms as described in Section 3.2.8. Regarding the utilization of architecture types across different modalities, the distribution is as follows: CNNs are utilized in 63 modules, with the imaging modality accounting for 38 of these. Time series data follows with 13 modules, tabular data is used in 5 modules, and audio, video, and text data are employed in 3, 3, and 1 modules respectively. Among these, 8 modules use handcrafted features for time series data, while 1 module each for text and video modalities employs learned features, with the remaining modules processing raw data. FCNNs are utilized across 41 modules, with 32 dedicated to tabular and 7 to the imaging modality. Time series and video modalities each account for one module. Given the inefficiency of processing raw images with FCNNs, a feature extraction step is typically employed, with handcrafted methods used in 5 instances and learned features in 2. For time series data, handcrafted methods are employed, whereas video modality features are extracted using deep learning methods. RNNs serve 11 modules, 5 of which process time series data, and the remaining 6 are equally divided between tabular and text modalities. Ten of these modules use raw data, with only one, pertaining to text data, employing learned features. GNNs are favored for 6 modules, including 2 each for imaging and time series data, and 1 each for audio and video modalities. While imaging modalities are processed in their raw form, the others use extracted features, with one time series modality using handcrafted methods and the other using learned methods. Audio features are extracted via deep learning, whereas video features employ handcrafted methods. TFs are utilized in 4 modules: 2 for tabular data and 2 for text data, with learned features employed for one of the text modalities and the rest using raw data. AEs are employed in 2 modules, both for the imaging modality; one uses raw data and the other handcrafted features. Lastly, in all 5 instances where raw data is processed"}, {"title": "3.3.3. Architectural Combinations", "content": "Another analysis that we have conducted involved determining the types of architectures used in conjunction as unimodal modules.. The numbers on the edges represent the frequency of articles in which two architectures, indi-"}, {"title": "3.3.4. Dimensionality Transformation", "content": "Unimodal modules are responsible for extracting features, often referred to as marginal representations, from individual modalities which are subsequently utilized by the fusion block. A primary advantage of this approach is the ability to represent high-dimensional inputs through more compact unimodal features, thereby condensing the information into a vector of reduced dimensionality. Conversely, it can also expand the dimension of the infor-"}, {"title": "3.4. Fusion Module", "content": "Intermediate multimodal fusion involves integrating data from multiple modalities (e.g., text, image, audio, video) at an intermediate data representation of the modalities rather than at the data or the output level, as in early and late fusion approaches respectively. This approach enables an interaction between the modalities that are inherently unattainable in early fusion methods, where the early integration of modalities hinders the disentanglement of contributions for each data type. Likewise, it overcomes the limitations of late fusion techniques, which often rely on independently learned disjoint spaces for each modality.\nAt the core of the intermediate fusion approach there is the fusion module: it fuses the hidden representations extracted from the n unimodal modules $h_1, ..., h_n$ into the unified multimodal representation $h$. In other words, the fusion module aims to map the features of different modalities into a common embedding space, which can be obtained"}, {"title": "3.4.1. Systematic Notation for Multimodal Fusion", "content": "We now introduce a comprehensive mathematical notation system to facilitate a rigorous analysis of the multimodal intermediate fusion strategies employed in the revised studies. It enables the detailed description and precise representation of the nature, localization and timing of the fusion mechanisms designed in the architectures. A fusion is defined as the operation that integrates input data, as modalities and/or previous fusions, producing a unified representation.\nWe denote a generic single fusion as:\n$\\mathcal{F} = (\\alpha_j^l, \\alpha_k^m, ...) \\rightarrow$  (7)\nwhere the components of the notation are:\n\u2022 Fusion function ($\\mathcal{F}$): identifies the fusion function that takes as input multiple feature representations, e.g., modalities or outputs of other fusions, and returns a single fused representation."}, {"title": "3.4.2. Fusion Module Taxonomy", "content": "The systematical classification of the reviewed fusion methodologies by using the notation introduced so far not only offers to the readers a systematic analysis and categorization of the literature, but it also permits us to highlight what is fused, how many times the fusion is performed, when the fusion occurs, and how the fusion is performed.\nThese categorizations are intended to group the fusion methods according to their fundamental characteristics and principles, thus seeking an answer to some fundamental questions about intermediate fusion strategies for multimodal inputs:\n\u2022 What elements are involved in the fusion?\n\u2022 How many times does the fusion occur between the modalities?\n\u2022 When is the fusion performed between the modalities?\n\u2022 How is the fusion among the modalities performed?\nWe aim to provide a coherent and generalized understanding of the main fusion approaches applied in the field, facilitating a deeper comprehension of their implications and applications in multimodal research and proposing suitable and coherent taxonomies for the classification and categorization of any work in this field. However, the proposed taxonomies may not cover all the possible multimodal intermediate fusion methodologies, but it is applicable to the works considered in this review. Nevertheless, the introduction of our notation system aims at simplifying the analysis of any multimodal fusion method, thus making more straightforward the extension of the taxonomies presented in this review.\nIn the subsequent sections, we present our proposed taxonomies for multimodal fusion strategies. To illustrate these taxonomies, we draw upon select articles from the literature as examples. However, it is important to note that"}, {"title": "3.4.3. What?", "content": "Regarding what is being fused, in section 3.2.1, we identified the main categories of modalities, including imaging, tabular, text, time-series, video, and audio. Furthermore, our proposed notation allows for clear visualization of the fusions performed between different modalities and other multimodal representations before getting to the final output of the proposed model.\nFor a more comprehensive understanding, readers can refer to Supplementary Material B, where we have specified the modalities used by each fusion"}, {"title": "3.4.4. How Many?", "content": "Going forward, we quantitatively assess the fusion strategies adopted within the corpus of the reviewed literature, by categorizing the architectures based on whether they opt for a single fusion mechanism or multiple fusions throughout the model's structure. The predominance of single studies demonstrates a preference for this simpler approach over those employing multiple fusions. This observation underscores a preference towards intermediate fusion strategies that favor simpler and straightforward modalities integration, relying on robust unimodal representations. This trend is related to the vast typologies of data adopted in multimodal fusion in the biomedical field, where data curation and processing play a central role, and the fusion techniques are not fully investigated.\nFurthermore, these single fusion approaches often investigate more sophisticated multimodal modules able to analyze complex multimodal representations; for an in-depth discussion on the characteristics and implications of these multimodal modules, refer to Section 3.5."}, {"title": "3.4.5. When?", "content": "Upon evaluating when the fusion occurs among different modalities within multimodal architectures, two primary categorizations emerged from our analysis.\nThe first categorization focuses on the timing of the fusions that integrate the different modalities: whether all modalities are fused simultaneously in a single operation, gradually across multiple ones, or if there are multiple components of the network elaborating the same inputs. Firstly, we distinguish sudden architectures, characterized by the simultaneous fusion of all modalities in a single fusion, from gradual architectures, where modalities are fused progressively and incorporated in subsequent and distinct fusions. Secondly, we identify the multi-flow categorization as a specific group of methods that employ multiple sequences of fusion operations that process information from a set of considered modalities, defined as fusion flow. Therefore, the multi-flow structure is characterized by the generation of multiple distinct multimodal representations, each reflecting a unique integration and elaboration of all the input modalities, which are then fused into a unique fused representation.\nExamples of synchronous and asynchronous methods are found in in particular. Despite their use of the same modalities and fusion function, the distinction lies in the superscript numbers for each modality before fusion:\n$\\mathcal{F}_1 = (x_1^1, x_2^1) \\rightarrow$  (12)\n$\\mathcal{F}_1 = (x_1^3, x_2^2) \\rightarrow$  (13)\nThe synchronous method integrates modalities $x_1$ and $x_2$ after they have both been processed through a singular trainable layer, ensuring uniform abstraction levels at the fusion point. Conversely, the asynchronous method processes the modalities $x_1$ and $x_2$ through different unimodal modules, composed of 3 layers for $x_1$ and 2 for $x_2$ before their fusion.\nTo summarize the considerations reported so far,"}, {"title": "3.4.6. How?", "content": "We identify five types of approaches that can be used to fuse the data,. They are: Concatenation, Attention,"}, {"title": "3.4.7. Unimodal Module Influence on the Fusion Module", "content": "One of the main aspects that emerges from the analysis is the importance of the dimensionality of the fused space, a crucial factor for the effective integration of unimodal information, because it directly impacts the model's ability to capture and leverage multimodal interactions. Considering the proportions of the unimodal representations in the fused feature space, only 18 out of the 54 articles reported unimodal feature sizes for all the modalities involved in the fusion. Among these 18 articles, 13 of them employ an equal dimension for each unimodal representation, whereas the rest do not adopt this approach and have different dimensions for each unimodal feature vector. The choice of homogeneity of the unimodal"}, {"title": "3.5. Multimodal Module", "content": "The next component of the intermediate fusion framework, after the fusion operations that generated a multimodal representation, is the multimodal module, denoted with the $f$ block. This module leverages the representation $h$ derived from the fusion module to obtain the desired target $y$. The role of the multimodal module strongly depends on the type of architecture employed and the task for which it is used. Following our definition, no fusion operation takes place in this module, but only the final"}, {"title": "3.6. Target", "content": "In MDL models, the target represents the output , as outlined in Figure 3, which is functional for the task\u2019s execution. In the context of biomedical applications, this could be a diagnostic classification, a prognostic regression, or any other relevant task that requires the definition of an outcome. All selected articles perform supervised tasks, involving the use of annotated datasets in which all data instances are labeled. We categorized each article"}, {"title": "3.7. Learning", "content": "As we described the key components of an MDL model in the previous sections, we now turn our attention to pivotal techniques for training and deploying MDL models in real-world scenarios.\n3.7.1. Transfer Learning\nAlthough deep learning has achieved remarkable success, the hunger for data hinders its full potential in the medical field where data scarcity is fueled by high costs, privacy, and ethical issues [106]. To this end, transfer learning"}, {"title": "3.7.2. Multimodal optimization", "content": "In a MDL model, the optimization strategy can significantly influence the multimodal representation $h$, returned by the fusion module $\\mathcal{F}$, either through indirect or direct effects. Indirect effects occur when the loss function operates on the output $y$, thus indirectly influencing the update of the multimodal representation $h$. Conversely, direct effects involve loss functions that act directly on the feature representations, promoting their interaction and complementarity. Based on our analysis, 51 articles fall into indirect effects, with the majority of them using standard loss functions (e.g., cross-entropy, mean-squared error, focal loss). Among the indirect effects, it is notable"}, {"title": "3.7.3. Missing Modalities", "content": "The term missing modalities denotes the partial or complete absence of one or more modalities within a multimodal instance. In the context of multimodal learning, the presence of missing modalities is a major concern that can undermine the applicability of a model, especially in data-in-the-wild scenarios where incomplete instances are prevalent. In the medical field, this problem is even more pronounced as the combination of missing modalities and data scarcity severely limits the availability of large amounts of data.\nLooking at Figure 23, the majority of the articles (45 out of 54) showcase models that lack robustness to missing"}]}