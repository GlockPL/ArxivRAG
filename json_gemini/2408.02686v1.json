{"title": "A Systematic Review of Intermediate Fusion in Multimodal Deep Learning for Biomedical Applications", "authors": ["Valerio Guarrasi", "Fatih Aksu", "Camillo Maria Caruso", "Francesco Di Feola", "Aurora Rofena", "Filippo Ruffini", "Paolo Soda"], "abstract": "Deep learning has revolutionized biomedical research by providing sophisticated methods to handle complex, high-dimensional data. Multimodal deep learning (MDL) further enhances this capability by integrating diverse data types such as imaging, textual data, and genetic information, leading to more robust and accurate predictive models. In MDL, differently from early and late fusion methods, intermediate fusion stands out for its ability to effectively combine modality-specific features during the learning process. This systematic review aims to comprehensively analyze and formalize current intermediate fusion methods in biomedical applications. We investigate the techniques employed, the challenges faced, and potential future directions for advancing intermediate fusion methods. Additionally, we introduce a structured notation to enhance the understanding and application of these methods beyond the biomedical domain. Our findings are intended to support researchers, healthcare professionals, and the broader deep learning community in developing more sophisticated and insightful multimodal models.", "sections": [{"title": "1. Introduction", "content": "Deep learning has transformed the landscape of computational approaches in biomedical research, offering unique capabilities in handling complex, high-dimensional data [1]. Multimodal deep learning (MDL), which integrates multiple data types, has emerged as an innovative approach, leveraging the power of deep learning algorithms to interpret and integrate diverse data types, enhancing the robustness and accuracy of predictive models [2]. Unlike traditional unimodal methods, MDL utilizes a variety of data sources such as imaging, textual data, and genetic information, to name a few, providing a more comprehensive understanding of complex biomedical phenomena. This approach is particularly relevant in a field where data diversity and volume are rapidly expanding, offering unprecedented opportunities for advancements in diagnosis, treatment, and patient care.\nIn the domain of multimodal learning, the integration of data from multiple sources can be achieved through various fusion techniques, i.e., early, late and intermediate fusion. Early fusion combines features at the data level, potentially losing unique modality-specific characteristics. Formally, let $x_1, x_2, ..., x_n$ represent feature vectors from n different modalities. The fusion function $\\mathcal{F}$ combines these features into a single vector:\n$x = \\mathcal{F} (x_1, x_2, ..., x_n)$ (1)\nwhich is then fed into a learning model f, resulting in the output y:\ny = f(x) (2)\nLate fusion, on the other hand, occurs at the decision level, often missing opportunities for deeper interaction between modalities. Formally, the feature vectors $x_1, x_2, ..., x_n$ are fed into independent models $f_1, f_2, ..., f_n$, producing separate predictions $y_1, y_2,..., y_n$. The fusion function $\\mathcal{F}$ then combines these predictions:\ny = \\mathcal{F} (y_1, y_2, ..., y_n) (3)"}, {"title": "3.1. Definitions", "content": "Intermediate fusion, striking a balance, integrates data at the feature extraction stage, allowing for a more effective combination of modality-specific features. Formally, the feature vectors $x_1, x_2,..., x_n$ are processed by separate components of the model $f_1, f_2, ..., f_n$, yielding intermediate representations $h_1, h_2,..., h_n$. These intermediate representations are fused using $\\mathcal{F}$\n$\\bar{h} = \\mathcal{F} (h_1, h_2, ..., h_n)$ (4)\nand then a final component f processes $\\bar{h}$ to produce the output\ny = f(\\bar{h}) (5)\nFurther details on each component can be found in Section 3.1. Among the various fusion techniques employed in MDL, intermediate fusion stands out for its ability to effectively integrate information at essential stages of the learning process, potentially leading to more accurate and robust models. This method is especially beneficial in biomedical applications where different data types, like imaging and genomic information, need to interact closely to produce meaningful insights. The main feature of intermediate fusion lies in its ability to preserve and utilize the distinct qualities of each data type, enhancing the model's capability to handle the complexities inherent in biomedical data.\nThe utility of deep learning in the context of intermediate fusion in multimodal settings, particularly within biomedical research, is marked by its ability to process and fuse distinct data modalities at a more abstract level [3]. In intermediate fusion, deep learning models are particularly capable at entangling the complex, nonlinear relationships that often exist between different modalities in biomedical data. This understanding is critical for accurately interpreting the multimodal nature of such data, where each modality carries its own information. By leveraging deep learning's ability in feature extraction and representation learning, intermediate fusion methods can effectively bridge the gap between these diverse data sources, leading the way for more insightful biomedical analysis. This approach not only may enhance the accuracy of predictive models but also may unveil patterns and interactions that might be missed by more conventional methods.\nThe application of MDL in the biomedical domain faces unique challenges, notably due to the heterogeneity and high dimensionality of the data [4]. Biomedical datasets often comprise varied data modalities, each with distinct characteristics and scales, making it difficult to achieve effective integrationIntermediate fusion, as already introduced in Section 1, within the domain of MDL, is an approach that involves extracting features from different modalities using specialized unimodal neural networks, and then merging these features into a fused multimodal representation. This fused representation is subsequently fed into another neural network to yield the final outcome. A distinctive feature of intermediate fusion is that, via the back-propagation of the loss, both the unimodal and multimodal modules are trained. This allows for continuous improvement in capturing the intra- and inter-relationships inherent in multimodal data. This methodology is particularly useful in complex fields like biomedicine.\nThis method, as shown in Figure 3, can be broken down into several key components: modalities, unimodal modules, fusion module, multimodal module, and the target. Each of these plays a critical role in the overall functionality and efficacy of the fusion process:\n\u2022 Modalities: Modalities, denoted as $x_1, x_2, ..., x_n$ refer to the different types of data or sources of information used in the model. In the context of biomedical applications, these could include imaging data, textual or tabular clinical notes, genetic information, or other relevant medical measurements.\n\u2022 Unimodal Module: The unimodal modules, denoted as $f_1, f_2, ..., f_n$ refer to the parts of the network that processes each modality independently. Here, deep learning models are used to extract features, denoted as $h_1, h_2, ..., h_n$, specific to each modality.\n\u2022 Fusion Module: The fusion module $\\mathcal{F}$ is the core of intermediate fusion, which takes the features extracted by unimodal modules and combines them into a unified multimodal representation, denoted as $\\bar{h}$. This fusion can be performed using various techniques like concatenation, averaging, or more complex operations that account for interactions between modalities."}, {"title": "3.2. Modalities", "content": "MDL with intermediate fusion represents a rapidly growing field in Artificial Intelligence (AI), where information from multiple modalities is integrated at intermediate stages of a deep neural network. Each modality provides unique information, enriching the model's comprehension of context and thereby enhancing its performance. The different types of data offer complementary perspectives on complex medical scenarios, and their fusion provides a deeper understanding. For instance, medical image analysis can be integrated with textual data extracted from medical reports or laboratory data to obtain a more comprehensive view of a medical condition. Consequently, integrating multiple biomedical modalities strengthens predictive accuracy and fortifies models against input data fluctuations, amplifying their reliability and utility in practical applications.\nFrom our analysis of the 54 articles included in our survey, we identified 6 macro-modalities:\n\u2022 Imaging: this modality encompasses various medical images that serve crucial roles in both diagnosis and ongoing monitoring of medical conditions. X-rays, Magnetic Resonance Imaging (MRI), Computed Tomography (CT), Positron Emission Tomography (PET), Ultrasounds (US) are included in this category.\n\u2022 Tabular: under this modality fall tabular clinical data (spanning from demographic information to laboratory measurements), genomics data (which are used to analyze disease risk, genetic predisposition, or treatment response) and data related to drugs chemical features to analyze their functionality.\n\u2022 Text: textual data comprises free-text reports and transcripts documenting question-answer dialogues between healthcare professionals and patients.\n\u2022 Time Series: this category encompasses various types of signals. Specifically, we distinguish between electrical signals, clinical time series, and tactile signals. Electrical signals are derived from electrocardiograms (ECG), electroencephalograms (EEG), electroglottographs (EGG), electromyography (EMG), and electrodermal activity (EDA)."}, {"title": "3.2.5. Class Distribution", "content": "Clinical time series typically consists of measurements recorded during patients' hospital stays, which may include vital signs, laboratory results, and other metrics. Tactile signals, utilized in a single article [33], are recorded using a specialized tactile glove equipped with sensors.\n\u2022 Video: video data can be related to imaging techniques over time, incorporating techniques like Temporal Enhanced Ultrasounds (TeUS) and Dynamic Contrast-Enhanced Ultrasonography (DCE-US), but it can also include diverse RGB videos capturing movements of hands or the entire body.\n\u2022 Audio: it includes speech but also audio of coughing and breathing.In addition to the necessity of employing large datasets, if one intends to tackle a classification problem with intermediate fusion techniques, it is equally important to have balanced datasets. Balancing the diverse classes within the dataset is essential to prevent the model from being biased towards a particular class at the expense of others. However, real-world data, especially in the medical field, often exhibit imbalanced distributions. Since intermediate fusion methods are particularly sensitive to class imbalance, we further analyze the articles that tackle classification tasks to determine whether they use balanced datasets. To this end, we employed the Gini-index [70], denoted as G, computed as:\nG = 1 - $\\sum_{i=1}^{|C|}P(C_i)^2$ (6)\nwhere C is the set of classes, |C| is the number of classes, and P(Ci) is the class prior probability. As a consequence, datasets can be defined as:\n\u2022 Balanced: if 0.48 < G \u2264 0.5;\n\u2022 Imbalanced: if 0.42 < G < 0.48;\n\u2022 Highly Imbalanced: if 0 < G < 0.42.\nAs an example, in a binary classification problem, these ranges of Gini-index correspond to having a balanced dataset when the class priors range from 50-50 to 41-59, an imbalanced dataset when the class priors range from 40-60 to 31-69, and a highly imbalanced dataset when the class priors range from 30-70 to 0-100."}, {"title": "3.3. Unimodal Module", "content": "The power of intermediate fusion lies in its ability to capture both intra- and inter-modal relationships inherent in multimodal data. In contrast toearly fusion, each modality is processed through a dedicated branch denoted as fi in Figure 3, extracting modality-specific features, i.e., the marginal representations denoted as hi in the same figure. Extracting these marginal representations, facilitated by a global loss derived from fused features, allows for adaptability in relation to other modalities. This process empowers neural networks to derive representations that optimize the overarching goal. The rest of this section reports on the analysis of the unimodal networks adopted in the literature, shedding light on the preferences and trends observed in the field.\nThe analysis of the literature suggests categorizing the reviewed papers into two classes: homogeneous and heterogeneous. We say that a multi-modal model is homogeneous when all constituent unimodal networks share a uniform type of architecture; conversely, we say it is heterogeneous when at least one network diverges from the others, as schematically depicted in"}, {"title": "3.4. Fusion Module", "content": "Intermediate multimodal fusion involves integrating data from multiple modalities (e.g., text, image, audio, video) at an intermediate data representation of the modalities rather than at the data or the output level, as in early and late fusion approaches respectively. This approach enables an interaction between the modalities that are inherently unattainable in early fusion methods, where the early integration of modalities hinders the disentanglement of contributions for each data type. Likewise, it overcomes the limitations of late fusion techniques, which often rely on independently learned disjoint spaces for each modality.\nAt the core of the intermediate fusion approach there is the fusion module, denoted as $\\mathcal{F}$ in Figure 3: it fuses the hidden representations extracted from the n unimodal modules $h_1,..., h_n$ into the unified multimodal representation $\\bar{h}$. In other words, the fusion module aims to map the features of different modalities into a common embedding space, which can be obtainedwhere the components of the notation are:\n\u2022 Fusion function ($\\mathcal{F}$): identifies the fusion function that takes as input multiple feature representations, e.g., modalities or outputs of other fusions, and returns a single fused representation.\n\u2022 Fusion ordering (i): Denotes the position of the fusion within the architecture, following a sequential progression from input to output. Therefore, the highest fusion ordering index denotes the number of fusions performed in the fusion module.\n\u2022 Inputs ($\u03b1_j$, $\u03b1_k$, etc.): Represent the input modalities that can be the sample x represented in the original feature space, or the output of previous fusions $\\mathcal{F}$. For example, two different inputs j and k could be represented as $x_j$ and $x_k$, $\\mathcal{F}_j$ and $\\mathcal{F}_k$, or a combination such as $x_j$ and $\\mathcal{F}_k$\n\u2022 Layer depth (l and m): Indicate the number of trainable layers through which inputs $\u03b1_j$ and $\u03b1_k$ have been processed before their integration. This indicates when the fusion of information occurs. An example could be a fusion function that takes as input $\u03b1_j^l$, $\u03b1_k^m$, implying that the two input representations passed through two and three layers, respectively, before the fusion.\n\u2022 Fusion operation ($\\bullet$): Symbolizes the type of operation performed between inputs $\u03b1_j$, $\u03b1_k$, etc. To standardize the analysis, we identified 5 fusion categories: Concatenation, Attention, Tensor-operation, Calibration, and Knowledge-sharing, each represented by specific symbols listed in Table 2 and further detailed in Section 3.4.6. The analysis of these operations provides insight into how each fusion is performed.\n\u2022 Final Fusion ($\\rightarrow$): if present, it specifies that the considered fusion function is the last in the architecture and its output coincides with the latent fused representation $\\bar{h}$, given as input to the multimodal module f. Conversely, if not present, it denotes that the fusion under consideration is at the beginning or in the middle of the fusion structure."}, {"title": "3.4.2. Fusion Module Taxonomy", "content": "After identifying all the components used in the model, it is possible to draw a directional bipartite graph G(N, E) representing how the modalities and fusions are combined in each fusion approach, where N is composed of 2 types of nodes, modalities and fusions, respectively, whilst E is composed of directed edges which connect the inputs to a fusion.\nTo better illustrate the use of the proposed notation in the analysis of a multimodal fusion approach, we examine the intermediate fusion strategyThe systematical classification of the reviewed fusion methodologies by using the notation introduced so far not only offers to the readers a systematic analysis and categorization of the literature, but it also permits us to highlight what is fused, how many times the fusion is performed, when the fusion occurs, and how the fusion is performed.\nThese categorizations are intended to group the fusion methods according to their fundamental characteristics and principles, thus seeking an answer to some fundamental questions about intermediate fusion strategies for multimodal inputs:\n\u2022 What elements are involved in the fusion?\n\u2022 How many times does the fusion occur between the modalities?\n\u2022 When is the fusion performed between the modalities?\n\u2022 How is the fusion among the modalities performed?\nWe aim to provide a coherent and generalized understanding of the main fusion approaches applied in the field, facilitating a deeper comprehension of their implications and applications in multimodal research and proposing suitable and coherent taxonomies for the classification and categorization of any work in this field. However, the proposed taxonomies may not cover all the possible multimodal intermediate fusion methodologies, but it is applicable to the works considered in this review. Nevertheless, the introduction of our notation system aims at simplifying the analysis of any multimodal fusion method, thus making more straightforward the extension of the taxonomies presented in this review."}, {"title": "3.5. Multimodal Module", "content": "The next component of the intermediate fusion framework, after the fusion operations that generated a multimodal representation, is the multimodal module, denoted with the f block in Figure 3. This module leverages the representation $\\bar{h}$ derived from the fusion module $\\mathcal{F}$, to obtain the desired target y. The role of the multimodal module strongly depends on the type of architecture employed and the task for which it is used. Following our definition, no fusion operation takes place in this module, but only the final"}, {"title": "4. Discussion and Conclusion", "content": "As we draw this systematic review to a close, it is essential to step back and contemplate the current limitations, the trajectory for future research as well as the practical impact of our findings. This section synthesizes the insights garnered from our in-depth analysis of intermediate fusion methodsfor more rigorous reporting standards in MDL research, and also highlights an opportunity for future work to address these gaps, enhancing the quality and transparency of research within the domain.\nMoving forward, we hereby outline the primary research gaps identified:\n\u2022 Limited modalities combination: As discussed in Section 3.2.7, most studies utilizing intermediate fusion techniques in MDL within the medical field tend to focus on integrating only two types of modalities, typically imaging and tabular data [113]. More complex combinations that involve integrating more than two modalities, including text, video, time series, and audio, are relatively uncommon. This limited bimodal approach, centered on imaging and tabular modalities, may be due to the complexity and challenges associated with merging data from diverse modalities, as well as the need for effective data structuring and preprocessing. However, a more comprehensive multimodalanalysis could offer a more detailed view of the patient, enhancing the performance of the models.\n\u2022 Lack of Multi-scale Data: In the literature on MDL using intermediate fusion techniques within the biomedical field, there is a notable gap in integrating data of different dimensional scales. Combining macroscopic data (e.g., radiological images) with microscopic data (e.g., histopathology) can offer a more comprehensive understanding of the health status of patients. Such integration would enable the direct correlation of observable macroscopic changes with underlying molecular alterations, thereby improving the understanding of various pathologies.\n\u2022 Lack of benchmark datasets: As shown in Section 3.2, a notable research gap in the field of intermediate MDL for biomedical applications is the absence of standardized, publicly available benchmark datasets. Similar to the role of ImageNet [114] in evaluating novel models for computer vision classification tasks, the availability of such datasets would allow researchers to compare and validate their deep learning models across different biomedical domains.\n\u2022 Suboptimal marginal representation dimensions: A significant research gap exists regarding the dimensionality of marginal representations. Our analysis reveals no clear trend in how these dimensions are chosen, with many studies arbitrarily defaulting to powers of two, likely influenced by binary representations. More critically, as mentioned in Section 3.3.4 the majority of studies employ equal-dimensional marginal representations across modalities, an approach that often lacks justification and may be suboptimal, considering that different modalities inherently contain varying amounts and types of information. The lack of a systematic approach to determining appropriate dimensions for each modality potentially leads to suboptimal fusion performance, loss of critical modality-specific information, and inefficient use of computational resources.\n\u2022 Insufficient consideration of multimodal fusion strategies: As shown in Section 3.4, our analysis reveals that there is a persistent tendency in the field to prioritize unimodal feature extraction approaches over addressing the complexities of multimodal fusion. This bias hasted to the development of oversimplified architectures for intermediate fusion, leaving critical aspects unexplored, as shown in section 3.4.6 where we analyzed the fusion operations performed in the fusion module. Key challenges remain unresolved, such as determining optimal methods for combining network branches that process different modalities, managing varying input sizes and dimensionalities, and ensuring efficient information flow across the fused network. Furthermore, our investigation indicates that the majority of fusion operations are performed without strong theoretical foundations or assumptions regarding the relationships between modality features. This lack of a consistent approach results in methodologies that do not take full advantage of the complementarity and correlation of the characteristics of the different modalities. Consequently, the potential for integrating information from different sources remains largely unrealized, limiting the effectiveness of current multimodal fusion strategies in biomedical applications. Addressing these limitations requires a paradigm shift in research focus, emphasizing the development of more sophisticated, theoretically grounded fusion techniques that can effectively capture and utilize inter-modal relationships."}]}