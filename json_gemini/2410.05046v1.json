{"title": "Named Clinical Entity Recognition Benchmark", "authors": ["Wadood M Abdul", "Marco AF Pimentel", "Muhammad Umar Salman", "Tathagata Raha", "Cl\u00e9ment Christophe", "Praveen K Kanithi", "Nasir Hayat", "Ronnie Rajan", "Shadab Khan"], "abstract": "This technical report introduces a Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support.\n\nThe leaderboard provides a standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. A curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and a comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes a brief analysis of models evaluated to date, highlighting observed trends and limitations.\n\nBy establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP.\n\nLeaderboard available at https://huggingface.co/m42-health/clinical_ner_leaderboard.", "sections": [{"title": "1 INTRODUCTION", "content": "Named Entity Recognition (NER) in the clinical domain is a fundamental task in medical natural language processing (NLP), playing a crucial role in extracting structured information from unstructured clinical narratives. The ability to identify and classify entities such as diseases, symptoms, medications, and procedures within clinical texts is essential for a wide range of downstream applications (Pradhan et al., 2015; Stubbs et al., 2015). These applications include clinical decision support systems, where identified entities can trigger relevant alerts and/or recommendations; automated coding for billing and administrative purposes; and cohort identification for clinical trials, enabling rapid patient recruitment based on specific clinical criteria (Savova et al., 2010).\n\nAdditionally, as the volume of electronic health records (EHRs) continues to grow, efficient and accurate extraction of clinically relevant information becomes increasingly vital for both patient care and medical research (Hossain et al., 2023). Accurate NER systems can significantly improve"}, {"title": "2 RELATED WORK", "content": "Unlike general domains, where benchmarks like GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) are well-established, the biomedical field lacks equivalent resources (Kanithi et al., 2024). Over the years, the field of biomedical NLP has seen the development and release of numerous datasets, often stemming from shared tasks such as BioCreative (Li et al., 2016b), BioNLP (Demner-Fushman et al., 2024), and SemEval (Ojha et al., 2024). While the focus of these datasets has evolved from simple tasks like NER to other tasks such as relation extraction and question answering, there remains a significant gap in the availability of benchmarks and leaderboards for medical and clinical NLP."}, {"title": "3 THE CLINICAL NER BENCHMARK", "content": "To address the challenges in evaluating clinical NER models, we have developed a benchmark that provides a standardized platform for assessing performance. This benchmark consists of the following key components: it contains a common evaluation methodology that employs well-established evaluation metrics, primarily focusing on the F1-score; it employs terminology standardization of the clinical entities included in our evaluation, which ensures consistency and interoperability; and it includes a curated collection of openly available medical benchmark datasets, encompassing a broad spectrum of medical entities. In the subsections below, we first elucidate the problem and then elaborate on the components in the following subsections."}, {"title": "3.1 \u039d\u0391\u039cED-ENTITY RECOGNITION TASK", "content": "NER is a crucial task in biomedical NLP that aims to identify and classify medical entities in unstructured clinical text. Mathematically, we can formulate the NER task as follows. Given an input sequence of tokens $X = (x_1,x_2,...,x_n)$, where each $x_i$ represents a token (a word or sub-word) in clinical text, the goal is to assign a corresponding sequence of labels $Y = (y_1, y_2,..., y_n)$, where each $y_i$ belongs to a predefined set of clinical entity types $E \\cup {O}$, with O representing the \u201cOutside\" label for tokens that are not part of any medical entity.\n\nFormally, we can express this as a function $f : X \\rightarrow Y$, where X is the space of all possible input sequences of text, and Y is the space of all possible clinical label sequences.\n\nThe set of clinical entity types $E$ typically includes categories such as $E = {DIS, PROC, DRUG, . . .}$, where, for example:\n\n\u2022 DIS corresponds to medical conditions or disorders,\n\u2022 PROC includes medical procedures or interventions,"}, {"title": "3.2 EVALUATION METRICS", "content": "The performance of clinical NER models, which aim to optimize P(Y|X) as shown in equation (3), is evaluated using two types of metrics: token-based and span-based. Both types utilize precision, recall, and F1-score, but they differ in how they define true positives (TP), false positives (FP), and false negatives (FN)."}, {"title": "3.2.1 \u03a4\u039fKEN-BASED METRICS", "content": "Token-based metrics evaluate the model's performance at the individual token level. For each token $x_i$ in the input sequence X, we compare the predicted label $\\hat{y}_i$ with the true label $y_i$. Let $TP_t, FP_t,$ and $FN_t$ represent token-level true positives, false positives, and false negatives, respectively. Then:\n\nPrecision$_t = \\frac{TP_t}{TP_t + FP_t}$ (1)\n\nRecall$_t = \\frac{TP_t}{TP_t + FN_t}$ (2)\n\nF1-score$_t = 2 \\cdot \\frac{Precision_t \\cdot Recall_t}{Precision_t + Recall_t}$ (3)\n\nThe above metrics can be calculated either globally or on a per entity type basis, thus giving us two possible metrics:\n\n\u2022 Micro Average: The $TP_t, FP_t$, and $FN_t$ values are calculated globally to get the final precision, recall and F1 values.\n\n\u2022 Macro Average: The precision, recall and F1 are calculated for each entity type and then averaged without any weightage.\n\nWith this token-based approach, we have a broad idea of the performance of the model at the token level. However, it may misrepresent the performance at the entity level when the entity includes"}, {"title": "3.2.2 SPAN-BASED METRICS", "content": "Span-based metrics evaluate the model's performance at the entity level, considering full or partial matches. These metrics are particularly important in clinical NER, as they reflect the model's ability to identify complete medical entities. Let $TP_S, FP_S$, and $FN_S$ represent span-level true positives, false positives, and false negatives, respectively. We define:\n\n\u2022 Exact Match: The predicted entity spans exactly match the true entity span's boundary and label.\n\n\u2022 Partial Match: The predicted entity spans overlap with the true entity span's boundary and exactly matches the label.\n\nBased on the criteria above, each predicted or true span can be classified as Correct, Incorrect, Missed, Spurious (see Table 1).\n\nUsing the above classifications, we have\n\n$FP_S = Incorrect + Spurious$ (4)\n\n$FN_S = Incorrect + Missed$ (5)\n\nThen, we calculate:\n\nPrecision$_S = \\frac{TP_S}{TP_S + FP_S}$ (6)\n\nRecall$_S = \\frac{TP_S}{TP_S + FN_S}$ (7)\n\nF1-score$_S = 2 \\cdot \\frac{Precision_S \\cdot Recall_S}{Precision_S + Recall_S}$ (8)\n\nStrict span based evaluation may be more applicable in applications like de-identifying PII, where as partial span based evaluation is desirable when we have leading/following words that do not change the entity's meaning."}, {"title": "3.2.3 WORKING EXAMPLE", "content": "Consider the following example, with the following entities (i.e., true labels):\n\nThe patient's chest X-ray [PROC] showed pneumonia [DIS], and blood cultures [LAB] were ordered to rule out sepsis [DIS]. Patient has no diabetes [DIS] . Levofloxacin [DRUG] was prescribed for treatment.\"\n\nAssume the predicted labels are as follows:\n\nThe patient's chest X-ray [PROC] showed pneumonia [DIS], and blood cultures [LAB] were ordered to rule out sepsis. Patient has no diabetes [pis] . Levofloxacin [DRUG] was prescribed for treatment [PROC].\"\n\nToken-based evaluation (Micro Average):\n\n\u2022 TPt = 6 (X-ray, pneumonia, blood, cultures, diabetes, Levofloxacin)\n\n\u2022 FPt = 1 (treatment)\n\n\u2022 FNt = 2 (chest, sepsis)\n\n\u2022 F1-scoret = 0.80\n\nToken-based evaluation (Macro Average):\n\n\u2022 TPt = PROC: 1, DIS: 2, DRUG: 1, LAB: 2\n\n\u2022 FPt = PROC: 1, DIS: 0, DRUG: 0, LAB: 0\n\n\u2022 FNt = PROC: 1, DIS: 1, DRUG: 0, LAB: 0\n\n\u2022 Precisiont = PROC: 0.5, DIS: 1, DRUG: 1, LAB: 1\n\n\u2022 Recallt = PROC: 0.5, DIS: 0.66, DRUG: 1, LAB: 1\n\n\u2022 F1t = PROC: 0.5, DIS: 0.8, DRUG: 1, LAB: 1\n\n\u2022 Final F1-scoret = 0.82\n\nSpan-based evaluation (Exact Match):\n\n\u2022 TP = 4 (pneumonia, blood cultures, diabetes, Levofloxacin)\n\n\u2022 FP = 2 (chest X-ray, treatment)\n\n\u2022 FN = 2 (chest X-ray, sepsis)\n\n\u2022 F1-scores = 0.66\n\nSpan-based evaluation (Partial Match):\n\n\u2022 TP = 5 (chest X-ray, pneumonia, blood cultures, diabetes, Levofloxacin)\n\n\u2022 FP = 1 (treatment)\n\n\u2022 FN = 1 (sepsis)\n\n\u2022 F1-scores = 0.83\n\nThis example demonstrates how token-based and span-based metrics can provide different perspectives on model performance. Span-based metrics, in particular, reveal issues with entity boundary detection, particularly for the procedure entity. The partial match evaluation shows better performance than the exact match, indicating that the model is generally identifying the correct entities but sometimes struggles with precise boundaries.\n\nFor our evaluation framework we consider the Macro Average token-based metrics and the Partial Match for our span-based metrics.\n\nThe variety of entity types demonstrated in this example (procedure, disease, lab test, drug) highlights the complexity of clinical NER tasks. To ensure consistency across different NER systems and to facilitate interoperability in clinical applications, it is crucial to establish a standardized terminol-ogy for entity types. This standardization not only aids in the accurate evaluation of NER models"}, {"title": "3.3 \u0421\u043e\u043cMON TERMINOLOGY", "content": "Standardization of medical terminology is a critical requirement for the effective development and deployment of clinical NLP systems. In the medical field, the proliferation of institution-specific vocabularies, coding systems, and ontologies has long posed a significant challenge for data integration, interoperability, and the generalization of NLP models across different healthcare settings (Iroju et al., 2015).\n\nTo address this issue, the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) has emerged as a widely adopted standard for harmonizing clinical data (Observational Health Data Sciences & Informatics, 2021). The OMOP CDM provides a standardized framework for organizing and representing a wide range of medical concepts, including diagnoses, procedures, medications, laboratory tests, and demographic information. By mapping diverse source terminologies to the common OMOP concepts and vocabularies, the model enables seamless integration and analysis of data from multiple institutions and data sources.\n\nThe importance of terminology standardization is particularly evident in the context of clinical NER, where the accurate identification and classification of medical entities are crucial for downstream applications such as clinical decision support, automated coding, and cohort identification. Inconsistent or ambiguous representations of these entities can lead to significant errors and performance degradation in NER models (Kundeti et al., 2016; Klug et al., 2024).\n\nIn the development of our Clinical NER Benchmark, we have leveraged the OMOP Common Data Model to standardize the medical entities included in the evaluation datasets. By aligning the entities to the OMOP standard vocabularies, we ensure that the benchmark provides a consistent and interoperable representation of clinical concepts, facilitating fair comparisons of NER model performance across diverse datasets and healthcare settings. Furthermore, we propose two additional domains - genes and gene variants - to cover genomic data, aligning with the OMOP CDM extension for storing genetic information, thus enhancing the benchmark's applicability to precision medicine and genomics research (Shin et al., 2019). Table 2 provides an overview of these domains, including brief descriptions and examples for each entity type.\n\nBy incorporating these OMOP domains, our Clinical NER Benchmark provides a comprehensive framework for evaluating NER models across a diverse range of clinical entities. This approach not only ensures broad coverage of medically relevant concepts but also facilitates the benchmark's applicability to various clinical specialties and research areas, including oncology, pharmacogenomics, and rare genetic disorders. Importantly, the use of the OMOP CDM as our standardization framework ensures the scalability and future-proofing of our benchmark. Additional entity types or domains can be seamlessly integrated into the benchmark in the future, following a careful mapping"}, {"title": "3.4 DATASETS", "content": "Four publicly-available datasets have been included in our benchmark. They are summarized in Table 3.\n\nNCBI The NCBI Disease corpus includes mention and concept level annotations on 100 PubMed abstracts (Dogan et al., 2014). It covers annotations of diseases.\n\nCHIA This is large, annotated corpus of patient eligibility criteria extracted from 194 registered clinical trials (Kury et al., 2020). Annotations cover 15 entity types (according to OMOP domains), including conditions, drugs, procedures, and measurements.\n\nBC5CDR The BC5CDR corpus contains PubMed articles with human annotations of all chemicals and diseases (Li et al., 2016a).\n\nBIORED The BIORED corpus includes a set of PubMed abstracts with annotations of multiple entity types, including genes/proteins, diseases, and chemicals (Luo et al., 2022).\n\nThe above datasets were adapted to align with our evaluation framework by mapping the annotations to clinically relevant entity types, as defined by the OMOP CDM. Entity types not included in the framework were omitted due to the limited availability of datasets with sufficient annotations for those entities. To ensure consistency, the retained clinical entity types were standardized across all datasets, resulting in a final set of six clinical entity types, as detailed in Table 2."}, {"title": "4 RESULTS AND ANALYSIS", "content": "We performed an analysis of the performance of various models evaluated on the proposed benchmarks and included on our leaderboard, showcasing the outcomes of the models assessed to date, with additional models planned to be incorporated in future iterations."}, {"title": "4.1 MODEL DIVERSITY", "content": "The analysis encompassed a diverse range of model architectures, including encoder-only, decoder-only, and the recently proposed GLiNER models (Zaratiana et al., 2023). These models varied in size, pre-training data, and whether they underwent fine-tuning for the NER task. Table 4 provides a summary of the models evaluated in this study, highlighting their architectural differences and key characteristics.\n\nThe different model architectures included in the leaderboard are:\n\n\u2022 Encoder: The standard token classification model built on top of transformer encoder architecture.\n\n\u2022 Decoder: Autoregressive token generation models based on the transformer decoder architecture."}, {"title": "4.2 ENTITY-SPECIFIC PERFORMANCE", "content": "Figure 1 shows the overall performance of all models for each entity type using both span-based and token-based metrics.\n\nA notable observation from this analysis is the higher performance (F1-score) for condition and drug entities compared to other entity types, which is observed for both span-based and token-based approaches. This trend may be attributed to the prevalence and consistency of these entity types in clinical texts, as well as their potentially more standardized representation in medical terminology. This is also reflected in figure 6 that shows the span counts for each entity type present on the leaderboard.\n\nInterestingly, when examining the performance for a single entity type (condition) across different datasets (Figure 8), we observe relatively consistent performance. This suggests that the models' ability to recognize Condition entity type (for example) may be generalizable across various clinical contexts and data sources."}, {"title": "4.3 IMPACT OF MODEL SIZE AND ARCHITECTURE", "content": "Figure 2 illustrates the performance of models according to their size and architecture.\n\nA key finding from this analysis is that LLMs models (i.e., decoder-only architectures) generally do not perform as well as the specialized encoder-based GLiNER architecture for the clinical NER task. This disparity in performance may be attributed to the inherent strengths of encoder-based architec-"}, {"title": "4.3.1 IMPACT OF FINETUNING", "content": "Figure 3 depicts the performance across clinical entities of fine-tuned and zero-shot models. Only the decoder architecture subset is used for this comparison as architectures like GLiNER do not have a supervised variant at the time of writing the paper.\n\nWe note that the best performance is obtained by supervised models, which is an expected result. Among the zero-shot models, in lead are Meta-Llama-3-70B-Instruct which is much larger in size and UniNER-7B-type which has been trained on task specific synthetically generated data."}, {"title": "4.4 \u03a4\u039f\u039a\u0395N-BASED VS. SPAN-BASED EVALUATION", "content": "We have also compared token-based and span-based performance metrics for the evaluated models. While the core messages and trends derived from both evaluation approaches remain consistent, we observed differences in the absolute performance values and relative rankings of models between the two metrics (as shown in Figure 4).\n\nToken-based and span-based F1-scores reveal clear ranking distinctions between models. The figure compares the overall (average) token-based and span-based F1-scores for each model, highlighting the ranking of models according to each metric and providing insight into model performance across different evaluation approaches.\n\nThese differences highlight the importance of considering both evaluation methodologies in clinical NER tasks. Token-based metrics provide insights into the models' ability to correctly classify individual tokens, while span-based metrics offer a more holistic view of entity recognition. The disparity between these metrics underscores the complexity of clinical NER and the need for com-prehensive evaluation approaches to fully understand model performance."}, {"title": "5 DISCUSSION AND CONCLUSIONS", "content": "In this work, we introduce a Clinical NER Benchmark, providing a standardized framework for evaluating language models for NER tasks. Our work addresses some critical challenges in clinical NLP and offers valuable insights into model performance across various clinical domains.\n\nA key strength of this work lies in its comprehensive approach to addressing persistent challenges in clinical NLP. First, our leaderboard tackles the issue of non-standardized medical data formats through terminology standardization. By leveraging the OMOP CDM for entity standardization, we promote consistency and interoperability across diverse healthcare systems and datasets. This standardization not only facilitates more meaningful comparisons between models but also enhances the potential for collaborative research and development in clinical NLP. Second, we have processed a set of benchmark datasets that cover various entity types and clinical domains. This diverse collection ensures a robust evaluation of model performance across different aspects of clinical narratives, providing a more comprehensive assessment of a model's capabilities in real-world healthcare scenarios. Third, our methodology for evaluation includes different criteria for computing standard metrics such as precision, recall, and F1-score, this allows for a direct comparisons with existing lit-"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DECODER MODEL EVALUATION", "content": "Evaluating encoder models, such as BERT, for token classification tasks (e.g., NER) is straightforward given that these models process the entire input sequence simultaneously. This allows them to output token-level classifications by leveraging bidirectional context, facilitating a direct comparison of predicted tags against the gold standard labels for each token in the input sequence.\n\nIn contrast, decoder-only models, like GPT models, generate responses sequentially, predicting one token at a time based on the preceding context. Evaluating the performance of these models for token classification tasks requires a different approach. First, we prompt the decoder-only LLM with a specific task of tagging the different entity types within a given text. This task is clearly defined to the model, ensuring it understands which types of entities to identify (i.e., conditions, drugs, procedures, etc). An example of the task prompt is shown below."}]}