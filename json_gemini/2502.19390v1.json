{"title": "Multi-modal Contrastive Learning for Tumor-specific Missing Modality Synthesis", "authors": ["Minjoo Lim", "Bogyeong Kang", "Tae-Eui Kam"], "abstract": "Multi-modal magnetic resonance imaging (MRI) is essential for providing complementary information about brain anatomy and pathology, leading to more accurate diagnoses. However, obtaining high-quality multi-modal MRI in a clinical setting is difficult due to factors such as time constraints, high costs, and patient movement artifacts. To overcome this difficulty, there is increasing interest in developing generative models that can synthesize missing target modality images from the available source ones. Therefore, we design a generative model for missing MRI that integrates multi-modal contrastive learning with a focus on critical tumor regions. Specifically, we integrate multi-modal contrastive learning, tailored for multiple source modalities, and enhance its effectiveness by selecting features based on entropy during the contrastive learning process. Additionally, our network not only generates the missing target modality images but also predicts segmentation outputs, simultaneously. This approach improves the generator's capability to precisely generate tumor regions, ultimately improving performance in downstream segmentation tasks. By leveraging a combination of contrastive, segmentation, and additional self-representation losses, our model effectively reflects target-specific information and generate high-quality target images. Consequently, our results in the Brain MR Image Synthesis challenge demonstrate that the proposed model excelled in generating the missing modality.", "sections": [{"title": "1 Introduction", "content": "Reliable brain tumor segmentation in magnetic resonance imaging (MRI) plays a crucial role in diagnosis and treatment [6,10]. Since performing this task manually is tedious and highly variable, the various advantages of deep learning-based automated tumor segmentation methods have been studied recently [10]. Most of these recent methods utilize all four commonly used MRI modalities in clinical scenarios, including T1-weighted (T1), contrast enhanced T1-weighted (T1-ce) image, T2-weighted (T2), and Fluid Attenuated Inversion Recovery (FLAIR)"}, {"title": "2 Methods", "content": "The goal of missing modality synthesis is to generate images for a missing target modality by utilizing multiple available source images. To achieve this, our model is designed by incorporating multi-modal contrastive learning and a segmentation decoder, enhancing the ability to learn from multi-source modalities. Our model builds upon the multi-modal translation network T and self-representation autoencoder network SR of the previously studied ACA-GAN [5]. Specifically, the multi-modal translation network T comprises a generator G and a discriminator D. The G comprises a multi-branch encoder Ge with an attached single attention module, multi-modal fusion module, a decoder GD for generating missing modality, and an additional segmentation decoder GSD. The overall framework of our model is shown in Fig.1."}, {"title": "2.1 Multi-modal contrastive learning", "content": "Unlike conventional contrastive learning methods [12,8], focusing on single-source modality, our task requires a distinct approach for multi-source modalities. To address this, we integrate multi-modal contrastive learning into our multi-modal translation network. By utilizing fused features from all modalities, this approach enables contrastive learning on more robust and generalized features that incorporate both intra- and inter-modal complementary information.\nWhen the source images are X = {X1,X2,X13}, and the missing target real image is y, each of xi (where i \u2208 1,2,3) is processed through its respective multi-branch encoder designed for its specific modality. These modality-specific features are then processed through the multi-modal fusion module to produce the final fused features ffusion. The generated image G(X) follows the same processing steps, resulting in its corresponding feature fG(X) Ifusion. We then compute the multi-modal contrastive loss between ffusion and f ffusion. The anchor feature q is extracted from the flon, serving as the query. The positive k+ is obtained from the feature at the same location in ffusion, and the negative features k\u00af are selected from the remaining locations. The contrastive loss is defined as follows, ensuring that the query is close to the positive k+ while being pushed away from the negative k\u00af [12]:\n$L_{con} = -log \\frac{exp(q k^+ / \\tau)}{\\sum_{n=1}^N exp(q k_n / \\tau)}$ (1)\nwhere k denotes n-th negative, and \u03c4 is a temperature parameter that controls the scale of the similarity score [12].\nMoreover, the effectiveness of contrastive learning depends on the significance of the information contained in the query [8]. Therefore, rather than using features from randomly selected locations, we employ the Query-selected Attention (QS-Attn) [8] module to select queries based on their measured significance. In our study, the QS-Attn module selects features based on the entropy of fused multi-modal features (ffusion), thereby enabling the extraction of the most informative features from multi-source modalities. The detailed process begins by converting the ffusion into a 2D matrix Q. This matrix is then multiplied by its transpose K, resulting in a matrix that undergoes a softmax operation to generate the global attention matrix Ag. For each row in Ag, the entropy Hg is calculated using the following formula [8]:\n$H_g(i) = - \\sum_{j=1}^W A_g(i, j) log A_g(i, j)$. (2)\nBy measuring feature similarity, this module helps identify the most distinct feature for contrastive learning, ensuring that the network focuses on the most informative features during training [8]."}, {"title": "2.2 Segmentation Decoder", "content": "To enhance our multi-modal translation network, we incorporate an additional segmentation decoder (GSD) into the framework. This enhancement allows the"}, {"title": "2.3 Overall loss function", "content": "We employ multiple self-representation losses to ensure that target-specific information is effectively conveyed to the generator at all stages. Similar to AE-GAN [4], we apply the self-representation losses within the decoder. This loss [4] is particularly crucial and effective as it imposes a constraint ensuring the fused features from multiple source modalities remain similar to the target image features during the upsampling process. Consequently, this self-representa-tion loss aids the multi-modal translation network in better matching the distribution of the target images, enabling the network to generate pseudo-target images that more closely resemble the target modality images. This self-representat-ion loss, defined as LSR decoder, is calculated as the KL Divergence loss between the decoder features of GD and SRD [4]:\n$L_{SR\\_decoder} = \\sum_{i=1}^N KL (log ((G_p(f_i))) || (SR_p(f_i)))$. (4)\nMoreover, following ACA-GAN [5], we employ two self-representation losses at different stages: LSMR and LMMR. The LSMR is the L2 loss between the multi-branch encoder features and the encoder features of the self-representation network [5]. The LMMR is the L2 loss between the features passing through the multi-modal fusion module and the final encoder features of the self-representation network [5]. Therefore, the total objective loss is defined as follows:\n$L_G = L_{adv} + \\alpha \\cdot L_{con} + \\beta \\cdot L_{seg} + \\gamma \\cdot L_{SR\\_decoder} + \\delta \\cdot L_{SMR} + \\eta \\cdot L_{MMR}$,(5)\nwhere Ladv represents the adversarial loss [7], \u03b1, \u03b2, \u03b3, \u03b4 and \u03b7 are hyperparameters used to balance the overall losses. \u1e9e is set to 0.05, while the others are set to 0.1."}, {"title": "3 Experimental settings", "content": ""}, {"title": "3.1 Dataset", "content": "The BraSyn-2023 dataset [2,9], sourced from the RSNA-ASNR-MICCAI BraTS 2021 dataset, includes multi-parametric MRI (mpMRI) scans of brain tumors collected retrospectively across various institutions. The dataset comprises four modalities: pre-contrast T1-weighted (T1), post-contrast T1-weighted (T1-ce), T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (FLAIR). These scans were acquired under standard clinical conditions, albeit with different equipment and protocols, leading to variations in image quality. Expert neuroradiologists reviewed and annotated tumor subregions, including the Gd-enhancing tumor (ET), peritumoral edematous/infiltrated tissue (ED), and necrotic tumor core (NCR). The dataset consists of 1,251 scans for training, 219 scans for validation, and 570 scans for the private testing set. For training, participants receive complete sets of all four modalities with segmentation labels, while in the validation and test sets, one modality is omitted to assess image synthesis methods. Standardized preprocessing, including DICOM to NIfTI conversion, co-registration, resampling, and skull-stripping, has been applied, ensuring high-quality data for the challenge [10]."}, {"title": "3.2 Evaluation metric", "content": "The inference task in the BraSyn challenge requires algorithms to predict a missing MRI modality from a test set where one of the four modalities (T1, T1-ce, T2, FLAIR) is absent. The generated images will be evaluated on two aspects: (i) Image Quality using the Structural Similarity Index Measure (SSIM) [15] to compare the synthesized images with real clinical images in both tumor and healthy brain regions, providing two SSIM scores per test subject; (ii) Segmentation Performance using Dice scores for three tumor structures to evaluate the efficacy of the synthesized images in improving segmentation results. The segmentation will be performed using the final FeTS\u00b9 algorithm [13], pre-trained on the FETS brain tumor segmentation dataset. Due to time constraints, we were unable to include segmentation results within the validation phase. However, we believe that our method has the potential to significantly enhance segmentation performance."}, {"title": "3.3 Implementation Details", "content": "We implement our proposed models using Pytorch\u00b2. To train the proposed model, Adam optimizer is applied with momentum parameters \u03b21 and 2 set to 0.5 and 0.999, respectively. The batch size is set to 4, and the initial learning rate is set to 0.0001. For the training stability [5], the self-representation network is pre-trained for 200 epochs. We then loaded the pre-trained parameters"}, {"title": "4 Results and Discussion", "content": "Table 1 presents the performance of our model based on the validation set from the BraSyn2024 challenge. Since the validation set does not contain segmentation masks, we evaluate our model on the SSIM scores for the entire MRI images, without separately analyzing tumor and non-tumor regions. Shown in the table 1, our model achieves promising performance in generating missing FLAIR and T1-ce scenarios, achieving SSIM scores of 0.9071 and 0.9046, respectively. For missing T2 and T1 scenarios, the model also demonstrates exceptional performance, achieving notably higher SSIM scores of 0.9327 and 0.9258, respectively. Moreover, the average SSIM score of 0.9182 indicates that our model maintains high image quality across various missing modality situations."}, {"title": "5 Conclusion", "content": "In this work, we propose missing MRI generative model that utilizes multimodal contrastive learning and an additional segmentation decoder. Our model effectively leverages crucial information from integrated multi-source modalities through the multi-modal contrastive learning. By incorporating the segmentation decoder, we enhance the precision of tumor region generation. Through high SSIM scores, we have demonstrated that our model generated high-quality images. Furthermore, the reported visual results of the generated images further confirms the effectiveness of our model. Consequently, our method achieved superior performance in the BraSyn challenge, highlighting its efficacy in generating accurate and reliable target modality images."}]}