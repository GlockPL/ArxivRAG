{"title": "EditScribe: Non-Visual Image Editing with Natural Language Verification Loops", "authors": ["Ruei-Che Chang", "Yuxuan Liu", "Lotus Zhang", "Anhong Guo"], "abstract": "Image editing is an iterative process that requires precise visual evaluation and manipulation for the output to match the editing intent. However, current image editing tools do not provide accessible interaction nor sufficient feedback for blind and low vision individuals to achieve this level of control. To address this, we developed EditScribe, a prototype system that makes object-level image editing actions accessible using natural language verification loops powered by large multimodal models. Using EditScribe, the user first comprehends the image content through initial general and object descriptions, then specifies edit actions using open-ended natural language prompts. EditScribe performs the image edit, and provides four types of verification feedback for the user to verify the performed edit, including a summary of visual changes, AI judgement, and updated general and object descriptions. The user can ask follow-up questions to clarify and probe into the edits or verification feedback, before performing another edit. In a study with ten blind or low-vision users, we found that EditScribe supported participants to perform and verify image edit actions non-visually. We observed different prompting strategies from participants, and their perceptions on the various types of verification feedback. Finally, we discuss the implications of leveraging natural language verification loops to make visual authoring non-visually accessible.", "sections": [{"title": "1 INTRODUCTION", "content": "Images are crucial visual media used to convey information and serve a variety of critical purposes in our everyday lives (e.g., work, social, entertainment). Blind and low-vision (BLV) individuals often need to create, edit, and share images with sighted peers, for social engagement, visual information access, and many other personal pursuits [18, 40, 63, 65, 85]. Crucial to producing a satisfactory image is reviewing and editing its visual details. For BLV individuals, the necessity for image editing arises in various contexts, such as enhancing photo quality (e.g., clarity, lighting, and composition) in blind photography [22, 29] and obfuscating private content [19, 20, 30, 84, 86]. However, the BLV community commonly faces access barriers with the image editing process, as existing editing tools do not support comprehensive non-visual understanding of the image content nor accessible visual manipulation control for them to iteratively perform edits and evaluate the visual changes [18, 29, 32, 73, 84]. Recently, HCI and accessibility researchers have increasingly focused on developing accessible tools for the BLV community in visual content creation and editing, including enabling the perception of elemental changes on presentation slide editing platforms [44, 54, 87], making video editing accessible through alternative textual representations for sounds and visuals [38], modifying lay- outs with tactile supports [47], and allowing users to verify content created by generative AI models with preset verification prompts [37]. Despite these efforts, the domain of image editing remains under-explored, which is visually challenging as it requires users to precisely understand image content, perform desired edit effects, and evaluate changes [20, 84]. Thus in this work, we aim to address the following questions: How to enable BLV people to perform image editing non-visually? And how to support the evaluation of visual changes after edits? To achieve this, we propose EditScribe, a prototype system that utilizes large multimodal models (LMMs) to make object-level image editing actions non-visually accessible to BLV people. The core of EditScribe is natural language verification loops. Using EditScribe, the user first comprehends the image content through initial general and object descriptions, then specifies edit actions using open-ended natural language prompts. EditScribe performs the image edit, and provides four types of verification feedback for the user to verify the performed edit, including SUMMARY OF VISUAL CHANGES, AI JUDGEMENT, and updated GENERAL and OBJECT DESCRIPTIONS. The user can ask follow-up questions to clarify and probe into the edits or verification feedback before performing another edit. We demonstrated the utility of natural language verification loops with five object-level edit actions in EditScribe, including blurring an object, removing an object, changing the color of an object, adjusting the brightness of the object, and adding text to the image. We focus on object-level actions as they require precise understanding and manipulation of image details, and are critical to tasks commonly desired by BLV people [20, 22, 73], e.g., image obfuscation for privacy [20, 84] and background removal/blurring to focus on specific objects [22, 36]. We evaluated EditScribe with 10 BLV participants to understand RQ1: How does EditScribe support non-visual image editing? RQ2: How do BLV people prompt EditScribe? RQ3: How do BLV people per- ceive EditScribe's verification feedback? and RQ4: How do BLV people perceive the final edited images? We found that BLV participants were able to perform most of the editing tasks using EditScribe, and they had different strategies for prompting, such as creating prompts ranging from detailed to succinct to adapt to discrepant verification feedback, using varying tones or wordings to facilitate the system's understanding, or asking follow-up questions to gauge the reliability and build their trust in EditScribe. Also, participants had different preferences for each verification feedback, depending on the contexts of use. Other factors, such as inconsistent or dis- crepant information, visual experiences and expectations, and tones of verification feedback, also affected their trust and confidence in EditScribe. Furthermore, participants felt confident about the edited images by EditScribe and were willing to publish based on the context, but preferred a second check on the final edited images using sighted assistance or other AI services. Finally, we discussed how EditScribe could be extended to support additional edit actions, provide enhanced verification feedback, and implications for future systems to leverage natural language verification loops for content creation."}, {"title": "2 RELATED WORK", "content": "Our research is informed by prior work in visual content authoring accessibility, image accessibility, and AI-assisted image editing."}, {"title": "2.1 Visual Content Authoring Accessibility", "content": "Research around BLV individuals' digital creative activities has recently gained momentum in the field of accessibility. This body of research noted substantial interest from the community in a wide range of digital content creation [18, 40, 64, 65, 85]. Notably, BLV individuals commonly feel the need to engage in visual con- tent authoring, including but not limited to photography, videos, presentation slides, data visualization, websites, and visual-heavy documents [18, 40, 65, 85]. These visual creative activities bring BLV individuals opportunities for information access, socializing, employment, self-expression, entertainment and more [85]. For example, BLV people often share photos with sighted peers or re- mote agents (e.g., Aira [6], Be My Eyes [7]) for visual interpretation assistance [63], while many also desire access to authoring visual content to better engage with popular social media [22, 61, 85]. However, in authoring visual content, BLV individuals face im- mense access barriers [18, 22, 29, 32, 52]. Past surveys revealed that digital creative activities that involve visual elements are gener- ally considered more challenging to BLV creators [85]. While basic photography and document formatting tasks have been increas- ingly supported by technology [11, 12, 17], editing visual elements remains particularly challenging [52, 85]-most often, BLV individ- uals shy away from visual editing tasks or rely on sighted support. Specific visual editing challenges to BLV individuals were also noted, including limited understanding of visual editing standards, insufficient information about the visuals, difficulties perceiving the effect of edits, and inaccessible editing controls [47, 56, 57, 65, 85]. Still, additional challenges arise across different editing contexts. For example, when editing out private or sensitive information, BLV individuals experience heightened uncertainty about how obscured the result is [20, 84]. In authoring content in collaboration, there are challenges with reviewing others' visual changes [54]. For edit- ing visuals in motion-videos, blind creators also have difficulties skimming through visual information [39]."}, {"title": "2.2 BLV Individuals' Access to Images", "content": "Key to successful image editing is the access to relevant visual infor- mation in the image, which most BLV screen reader users perceive through image descriptions [2, 68, 74]. A high-quality image de- scription should provide a comparable experience to encountering the image visually [68] (e.g., providing the purpose of the image, object or people present, etc. [1, 3, 55, 74]). Also, it is critical to create image descriptions that adapt to individuals' information- seeking goals [1, 3, 76] but not \"one size fits all\u201d [68]. For example, for visual elements on social media images, information related to specific persons, location, photo quality and others' responses are all important to describe [50, 73, 76, 88], while for data visual- ization, granular and objective descriptions (e.g., shape, context) could support users' interpretation freedom [27]. To increase the availability of alternative texts online, Al is considered as a scalable solution [23, 49, 76, 88]. Automated image descriptions and object detection tools have become increasingly available (e.g., [12, 15]). However, as state-of-the-art Al models still produce inaccurate re- sults, these systems need to provide enough information for users to notice potential undesired outcomes, such as by clearly com- municating what the system is able to do and how well it could perform the intended task [21, 49, 76, 88]. In addition to the above guidelines, providing image description in the context of visual content sharing and editing involves addi- tional considerations. In authoring visual content, BLV individuals tend to desire more information about their images and description accuracy [41, 65, 88]. In particular, spatial layout, appearances of objects, and timely feedback to visual changes critically influence BLV creators' judgement of necessary edits [37, 54, 65, 84]. Image descriptions also need to include information specific to the edit- ing needs in different content authoring contexts (as mentioned in Section 2.1), e.g., focused descriptions on potentially private objects for managing the visual privacy [20, 84], visual changes for collaborative authoring [54]. Image descriptions for content authoring purposes thus tend to involve an abundance of visual details, a key challenge that needs to be designed around [47, 65]. Still, investigation on image descriptions in the context of visual editing is limited, especially around supporting BLV creators' understanding of visual changes on images [84]. In this paper, we draw on prior work and design a set of verification feedback on an image edit. We use it as a probe to gain more insights into how BLV users perceived it to inform potential improvements."}, {"title": "2.3 AI-assisted Image Editing", "content": "Recent advances on computer vision, large language and multi- modal models [8, 31, 42, 58, 70, 75, 81] have introduced vast op- portunity for easing image content authoring. Image editing au- tomation now can perform not only basic photo adjustment, e.g., color adjustment [4, 5], but also advanced feature modifications and creative generation [16, 28, 53, 66]. For example, development in object detection and segmentation algorithms [42, 75] now sup- ports precise image editing, such as foreground extraction, object removal, and inpainting [62, 80]. Generative models [31, 48, 60] opened up possibilities for realistic generation of new images as well as manipulation of image attributes through the latent space [78]. The emergence of text-to-image diffusion models, such as DALL-E 2 [59], Stable Diffusion [60], Promp-to-Prompt [35], and InstructPix2Pix [24] supports not only text-to-image generation but also text-instructed edits on the image [53, 66, 71]. So far, most AI-based image editing tools target at and have been tested with sighted image editors, primarily for efficiency- and inspiration-aid purposes. Only a limited number of accessibility research has explored how these tools may or may not support BLV individuals, a group whose content creation needs could critically benefit from this type of technology [20, 85]. Huh et al. prototyped a screen reader accessible text-to-image generation tool and evalu- ated with BLV creators [37]. Their study revealed design insights for accessible text-to-image generation (e.g., support for prompt generation and result image understanding). In terms of accessible image editing support, research studies have focused primarily on how object detection, image caption, as well as segmentation mod- els may be applied to support the BLV community' visual privacy preservation-specifically, the detection and obfuscation of private and sensitive information in their photos [20, 85]. So far, this re- search revealed promising benefits these methods could bring to BLV individuals (e.g., independence, sense of control, efficiency), but also challenges (e.g., algorithm inaccuracies, difficulties with envisioning and evaluating obfuscation results, heavy cognitive load). Natural language interaction has the potential to mitigate these challenges by facilitating mutual understanding between the BLV users and the AI models. However, little insight exists around how BLV individuals would leverage this method to elicit feedback from Al models and how Al models could elaborate visual effects in the context of image editing. To understand this loop of communi- cation, we build a prototype system, EditScribe, to explore how BLV individuals strategize prompts for the system to understand their editing needs, and how they perceive a set of verification feedback regarding the effects of their edits."}, {"title": "3 EDITSCRIBE", "content": "EditScribe leverages natural language verification loops as its core to support BLV people to understand and edit images non-visually. In this section, we first present how BLV users could use EditScribe to edit images in an example scenario, then present the inspiration to design natural language verification loops from prior work, and detail its implementation."}, {"title": "3.1 Scenario Walkthrough", "content": "Here, we illustrate EditScribe in an example scenario, taking Amanda as the main character, a person who is blind. Amanda just lost her short-haired British white cat, Elsa, and urgently wants to make a public post asking people to help find it. She navigates her album on the phone and found the latest photo of herself with her cats, Elsa and Rosa. She uploads this photo to EditScribe and obtains a general description and a list of detailed object descriptions to confirm that the image content matches what she remembered. With these descriptions, she has developed a plan in mind to make a post with this photo. First, Amanda decides to remove Rosa, her other orange tabby cat next to Elsa, to avoid confusion. She prompts \"remove the orange cat\" and received EditScribe feedback after the edit is completed, including SUMMARY OF VISUAL CHANGES, AI JUDGEMENT, UPDATED GENERAL DESCRIPTIONS, and UPDATED OBJECT DESCRIPTIONS. Amanda confirms Rosa has been removed by cross-checking EditScribe's feedback, e.g., SUMMARY OF VISUAL CHANGES confirms the edit and AI JUDGEMENT provides detailed reasons, only one cat is described in the UPDATED GENERAL DESCRIPTIONS and the removal of Rosa's description in UPDATED OBJECT DESCRIPTIONS. She further confirms with a question \"How many cats are in the image?\" with answer \"One.\" Amanda then blurs herself to maintain her privacy by prompting \"Blur out the woman in the image\", and also confirms the success of the edit through the consistent feedback from EditScribe. To help people better distinguish Elsa, she wants to highlight it and make the photo similar to when Elsa was lost. First, Amanda aims to increase the focus of Elsa and specifies \"Make the cat brighter to increase its focus.\" The feedback from EditScribe suggests \"the intensity of the sunlight on the cat has been increased, giving the cat a more illuminated appearance\". She wants to ensure the color of Elsa is still maintained and asks \"What is the color of the cat?\" with the answer \"Cream or white\", which aligns with how her friends describe Elsa before. Next, she changes the bow tie's color from red to blue, which is what Elsa wore when lost \"Change the color of the bow tie to blue.\" She confirms with EditScribe's feedback, especially the UPDATED OBJECT DESCRIPTION indicates \"blue bow tie with a bell on the cat's neck\". Finally, Amanda wants to add her phone number to the post for others' reference. She first asks EditScribe \"Dividing the image to nine squares, where is a good spot to add text that does not overlap the cat?\" with EditScribe answering \"Center Right.\" She then prompts Ed- itScribe \"add text \"Please call 12345 if you find Elsa\" in the center-right of the image\". Despite the EditScribe feedback confirm- ing the addition of text, she performs another check \"Does the text overlap with the cat?\" with answer \"No\", and another check \"Does the cat with blue bow tie stand out in the image?\" with answer \"Yes.\" Finally, she confidently posts it on her social media and waits for responses."}, {"title": "3.2 Natural Language Verification Loops", "content": "When sighted people perform image editing, they utilize visual feedback to verify their actions and make necessary adjustments. However, such visual verification loops are inaccessible to BLV people. Therefore, at its core, EditScribe is enabled by the idea of natural language verification loops to support non-visual image editing. Here, we introduce its high-level concept. LMMs (e.g., GPT-4v [9]) are leveraged in EditScribe as a proxy to communicate between the user and the image. EditScribe inter- prets the user's natural language prompt into the action of certain visual edits, and then converts the resulting visual changes back into textual feedback for the user to review and confirm. This loop of cross-modal communication can be complex and requires ex- tensive textual exchanges. First, from applying textual prompts to visual changes, EditScribe interprets the object or region of the user's interest and desired editing effect by grounding on the user's prompt and the image content. Second, from interpreting visual changes to textual feedback, EditScribe grounds on cross-modal sources to provide a set of verification feedback, which we describe their rationale: (1) SUMMARY OF VISUAL CHANGES is designed to resemble and simulate a sighted person editing an image, who can visually discern the main differences between the images before and after edits. (2) AI JUDGEMENT draws inspiration from a human judge, who, from a second-person perspective, assesses both visual and textual modifications to evaluate and rationalize the success of the edit. (3) UPDATED GENERAL DESCRIPTIONS is designed to offer an independent perspective on how a sighted person would perceive the new image after an edit [1, 3, 76]. (4) UPDATED OBJECT DESCRIPTIONS is designed as if a sighted person is taking detailed visual inspections of each object to discern the nuances. The updated general and object descriptions are also inspired by previous research on presenting image descriptions with varying levels of granularity to minimize cognitive load and enhance effi- ciency [38, 45, 68, 72]. The provision of four types of feedback, each grounded in different sources, aims to help mitigate the potential Al errors (e.g., misinterpreting user intent or generating hallucina- tions.) by enabling users to compare the feedback and judge their accuracy. Below, we detail the cross-modal grounding pipeline for understanding user prompts and generating verification feedback."}, {"title": "3.3 Cross-modal Grounding Pipeline for Understanding User Prompts and Generating Verification Feedback", "content": "To facilitate image content understanding, EditScribe first generates initial general and object descriptions for the user. Then, the user can perform edits with natural language prompts, and get verifica- tion feedback after each edit. The user can ask follow-up questions to clarify the verification feedback and verify the edits. They can repeat this process until the edited image satisfies their goal. Generating General and Object Descriptions. Given an in- put image, EditScribe generates general and object descriptions to support the initial understanding of image content. Specifically, Ed- itScribe provides general descriptions for the image by prompting GPT-4v [9]. Next, EditScribe generates object descriptions by Set- of-Mark Prompting [79] with input image overlaid with a visible bounding mask (by SEEM [89]) and index on each object: Mapping User Prompt to Edit Action or Followup Question. When receiving a user prompt, EditScribe classifies it as either a question or an edit instruction that corresponds to one of the EditScribe edit functions by prompting GPT-4. If the prompt is classified as a question, EditScribe prompts the user's prompt to GPT-4v [9] to answer the question. On the other hand, if the prompt is classified as an edit instruction, EditScribe extracts the intended edit action and the object of interest from the prompt, for which the user can specify either the object's name or index for flexible referencing. Having all this information, EditScribe performs the edit to the referenced object (See Section 3.4 for the supported edit actions). Verification Feedback. After an edit is completed, EditScribe generates four types of verification feedback for the user to verify the outcome of the edit, including SUMMARY OF VISUAL CHANGES, AI JUDGEMENT, UPDATED GENERAL DESCRIPTIONS, and UPDATED OB- JECT DESCRIPTIONS. These verification information are structured and presented to the user from high to low level as follows: (1) SUMMARY OF VISUAL CHANGES illustrates an overview of visual changes by comparing the images before and after the edit. It grounds only new and previous images for the prompt. (2) AI JUDGEMENT explicitly points out if the AI determines the edit is successful or not by grounding and comparing the images before and after the edit, new and previous general and object descriptions, and the edit actions altogether. It also details its reasoning process and identified evidence. (3) UPDATED GENERAL DESCRIPTIONS are new and indepen- dent general descriptions on only the edited image using the same prompt for generating the initial descriptions. It only takes the new image as input for the prompt. (4) UPDATED OBJECT DESCRIPTIONS are object descriptions on the edited image, using SoM prompting [79]. It takes the new image with labeled masks and indexes for the prompt."}, {"title": "3.4 Image Edit Actions", "content": "EditScribe supports five editing functions: (i) blur an object, (ii) remove an object, (iii) change color of an object, (iv) adjust brightness of an object, and (v) add text to the image. We focus on object-level actions as they are critical to tasks commonly desired by BLV peo- ple [20, 22, 73], e.g., image obfuscation for privacy [20, 84] and background removal/blurring to focus on specific objects [22, 36]. These actions require precise understanding and manipulation of image details, and we selected a subset of them that are represen- tative and could inform other tasks. For instance, object removal could inform replacing with new objects, or cropping an image. Blurring and changing color and brightness could inform other actions involving pixel modifications. Inserting texts could inform adding other visual elements such as icons or arrows. We can fur- ther augment editing functions in EditScribe by prompting LLMs to classify users' prompts to corresponding edit functions. Below, we exemplify how users could specify prompts for each edit action using natural language, but not limited to these phrasing: (1) Blur an object: Blurring is a common technique to preserve per- sonal image content privacy [20, 84]. Users can specify prompts relevant to blurring, such as \"blur the person out\", or \"make #2 vague.\" Based on the prompt, EditScribe will apply the blurry effect on the specified object, using OpenCV's built-in func- tion 'GaussianBlur' [14]. Users can perform this edit action repeatedly on an object until its level of blurriness meets their expectations. (2) Remove an object: Removing unwanted or personal image con- tent is also common in image editing tasks. With this action, the user can remove an object while preserving the background. Users can specify prompts such as \"remove the right person\", or \"remove the orange cat.\" EditScribe will then remove the specified object using LaMa [69]. (3) Change color of an object: Changing color is common in basic image editing apps for improving aesthetics (e.g., [4]). It is also a privacy-preserving technique for users to black out or fill an object with a color significantly different from its original appearance [20, 84]. Users can specify prompts, such as \"change the cat's collar to blue.\" EditScribe will then change the color of the specified object to the specified one. This is achieved by modifying the 'Hue' value of the object in the HSV color space (e.g., Hue, Saturation, Value), which represents the color type and is expressed as a degree on the color wheel, ranging from 0 to 360. (4) Adjust the brightness of an object: Adjusting the brightness is another common editing function. Users can input prompts such as \"increase the brightness of the #6 person\", or \"make the left cat brighter.\" EditScribe will then adjust the brightness by increasing or decreasing each RGB channel of a pixel with the same increments. Users can gradually adjust an object's brightness until its brightness level meets their expectations. (5) Add text to the image: Adding text is common for adding an explanation to an image or making any purpose explicit. In our current implementation, users can assign text to one of the nine locations on an image (e.g., center, bottom right) or onto a specific object. Users can specify \"add words 'Hello world' on upper third\", \"place sentence 'Hello world' to center\", or \"generate text 'Hello' to the top left corner.\" EditScribe will then place the text to the specified location using OpenCV's built-in function 'addText' [14] with pre-defined colors based on the contrast to the image."}, {"title": "3.5 EditScribe Web Interface", "content": "EditScribe was implemented using Gradio [10], an open-source Python library for fast prototyping front-end web interfaces. The left side of the interface shows the images before and after the most recent edit, and an image labeled with masks and in- dexes for debugging purposes. On the right side, there is an accessible chat with verification feedback indexed with different heading levels for BLV users to navigate using a screen reader. For instance, verification feedback of each edit starts with \"Verification Output of Edit #4 starts from here\u201d, which is labeled as heading level 1 to help BVI users navigate the edits. On the other hand, each type of feedback title, such as \"[#4] Summary of Visual Changes\", is labeled as heading level 2, which allows users to navigate each type of feedback. On the bottom right of the interface, the user can enter their prompts and questions using natural language, or \"undo\" or \"redo\" their edits."}, {"title": "4 USER STUDY", "content": "We conducted a user study with BLV individuals to understand how natural language verification loops, including user prompts and system verification feedback, may (or may not) support their image editing needs. Specifically, we focus on the following research questions: RQ1: How does EditScribe support non-visual image editing? RQ2: How do BLV people prompt EditScribe? RQ3: How do BLV people perceive EditScribe's verification feedback? RQ4: How do BLV people perceive the final edited images?"}, {"title": "4.1 Participants", "content": "We recruited ten BLV participants (8 male and 2 female) using mailing lists for local accessibility organizations and prior contacts. Participants aged from 26 to 43 (avg. 35.4) and described their visual impairment as total blindness (N=8), having only light perception (N=1) or low vision (N=1). All participants regularly used a screen reader as the primary means to accessing their devices. Some par- ticipants had prior experiences in taking photos or editing images, such as having attempted to crop images, adding text or graphics, creating images with Generative AI, or consuming images through tactile displays."}, {"title": "4.2 Study Procedure and Sessions", "content": "The study was conducted remotely over Zoom, and participants were provided with a link to our EditScribe site and used a screen reader to access. The study was approved by our Institutional Re- view Board (IRB). With participants' consent, the study was video recorded and took about two hours, and each participant was com- pensated $50 for their participation. Participants were asked to optionally provide one of their own images to edit in the study. The study had five sessions, including a tutorial session to familiarize participants with EditScribe, three sessions to edit our provided images, and a session to edit their provided images . We developed several tasks in each session, which we describe next. 4.2.1 Session 1: Tutorial. In this session, we aimed to familiarize our participants with EditScribe. Participants were guided through each element on the EditScribe UI and were asked to perform editing tasks of their interests on an example image. Participants were instructed on the EditScribe feedback after an edit, including the four types of outputs, and noted that they could ask follow-up questions to verify the image content as well as undo or redo their edits. 4.2.2 Session 2: Performing individual edit actions. In this session, we aimed to understand if participants could successfully perform each edit action of EditScribe. For each edit action, participants were asked to use natural language prompts to edit or verify edits until they confirmed the task was completed. We then asked participants to provide their agreement on the statement \"I am confident that the edit is successful\" on a scale from 1 to 7, where 1 is strongly disagree and 7 is strongly agree. We randomized the order of the five editing tasks across participants. 4.2.3 Session 3: Making a flyer to find a missing cat. In this session, we aimed to explore how participants would use EditScribe in a practical scenario. Participants engaged in the scenario detailed in Section 3.1. Participants were asked to perform the tasks in their preferred order until they confirmed or were satisfied with the results, or if they decided to move on. 4.2.4 Session 4: Making a flyer for recruiting a craftsman to make a bathroom shelf. Next, instead of providing individual specific tasks, we provided a high-level goal for participants to develop their editing tasks based on their needs. The image was more complex, with multiple objects and uneven lighting conditions . The instruction was: You plan to create a flyer and post it on your social media to find someone to custom-make a shelf to fit everything on your sink. You thus first took a photo of the sink in your bathroom. However, it includes many personal items, such as towels, containers, medication bottles, toothbrushes. You want to check the overall image content and quality, such as the lighting condition, visual aesthetics, or any private content you don't want to expose. You should develop three to five edit actions. 4.2.5 Session 5: Editing participants' own images. In this session, we aimed to encourage participants to brainstorm potential use cases and workflows to apply EditScribe on their own images. Five participants provided their images before the study, for which they were familiar with the content and context when the photos were taken. In this open-ended session, participants performed their goals without any restrictions."}, {"title": "4.3 Data Analysis", "content": "Besides reporting their perceived confidence in each editing task in session 2, participants were asked to think aloud during each session and interviewed afterwards to provide qualitative feedback on their strategies for creating prompts or perceiving EditScribe outputs. All sessions were recorded and transcribed for analysis. The first author independently performed open coding on all transcripts to identify initial codes. The initial coding focused on participants' prompt formation and decision making, feedback toward the system output, and any friction they experienced. Two authors reviewed all coded excerpts and iterated on the set of codes through discussing with the first author. They then conducted affinity diagramming [33] on the initial codes to extract and organize high-level themes."}, {"title": "5 FINDINGS", "content": "5.1 RQ1: How does EditScribe support non-visual image editing? Participants were able to successfully complete most of the tasks in the study sessions and found EditScribe promising in supporting everyday scenarios. On the other hand", "I would be lower on this, so I gave two. It says the face was blurred, but I expected it to blur out the entire body.\u201d For removing the bowl (\u03bc=5.8, \u03c3=1.0), P2 was initially not confident in the system and scored 4 for this task, but developed his trust in the system later after asking several followup questions (Section 5.2.3) to verify the changes, stating \"I was uncertain whether the system was reliable so I gave you a neutral rating, but it seemed correct after I tested its reliability, so I could have gave a higher rating later.\" Lastly, though participants were able to increase the brightness of the dog (\u03bc=5.3, \u03c3=1.1), Ed- itScribe occasionally described the white dog as a 'cream-colored' or 'light-colored' dog in UPDATED OBJECT DESCRIPTIONS, even though SUMMARY OF VISUAL CHANGES and AI JUDGEMENT both confirmed the changes of brightness. For instance, P3 specified \"Make the dog brighter color\" and scored five for his confidence after obtaining the updated object descriptions \u201cFluffy cream-colored dog sitting\u201d, which made him confused": "I said to make it brighter color, which is maybe a little bit different, it's no longer white but cream color. I am not sure if cream is brighter than white.\u201d In session 3, most participants were able to perform and confirm the completion of the tasks, while a few (P2, P3, P6) encountered issues and had to skip certain tasks. For example, P2 was unable to change the color of the bow tie to blue using his prompts, such as \"change the color of the cat to white cat with blue bow tie\" or \"change the color of the cat's bow tie from red to blue\". It was because EditScribe misclassified the object of interest as the cat rather than the bow tie and thus changed the color of the cat instead. In another case, P3 encountered discrepant verification feedback or hallucinations after changing the bow tie to blue. For instance, in his two attempts, the bow tie became blue, but with object descriptions changed from \u201cred bow tie with a bell...\u201d, to \u201cblue bow tie with a button or ornament...\u201d, and \u201cblue bow tie with polka dots...\u201d In session 4, participants developed a number of editing tasks (e.g., increasing the brightness of the sink or towel, blurring out the plastic jars) and were able to complete most of them, with a few failure cases encountered (P4, P8, P10). P4 specified her prompt \"Remove the pill bottle from this image\", in which the image had mul- tiple bottles, but none of them was recognized as a 'pill bottle' in the object descriptions, which made EditScribe fail to ground on a specific object. In another example, after P8 increased the bright-"}]}