{"title": "JUST SAY WHAT YOU WANT: ONLY-PROMPTING SELF-REWARDING ONLINE PREFERENCE OPTIMIZATION", "authors": ["Ruijie Xu", "Zhihan Liu", "Yongfei Liu", "Shipeng Yan", "Zhaoran Wang", "Zhi Zhang", "Xuming He"], "abstract": "We address the challenge of online Reinforcement Learning from Human Feedback (RLHF) with a focus on self-rewarding alignment methods. In online RLHF, obtaining feedback requires interaction with the environment, which can be costly when using additional reward models or the GPT-4 API. Current self-rewarding approaches rely heavily on the discriminator's judgment capabilities, which are effective for large-scale models but challenging to transfer to smaller ones. To address these limitations, we propose a novel, only-prompting self-rewarding online algorithm that generates preference datasets without relying on judgment capabilities. Additionally, we employ fine-grained arithmetic control over the optimality gap between positive and negative examples, generating more hard negatives in the later stages of training to help the model better capture subtle human preferences. Finally, we conduct extensive experiments on two base models, Mistral-7B and Mistral-Instruct-7B, which significantly bootstrap the performance of the reference model, achieving 34.5% in the Length-controlled Win Rates of AlpacaEval 2.0.", "sections": [{"title": "INTRODUCTION", "content": "Reinforcement Learning from Human Feedback (RLHF) is a prevalent technique for Large Language Model (LLM) alignment, ensuring models adhere to human preferences, produce useful and truthful responses, and prevent harmful ones. Current RLHF methods are classified into online and offline approaches. The primary distinction is that online methods involve real-time interaction with the environment for feedback, while offline methods rely on pre-existing datasets without environmental interaction. Due to distribution shift challenges in offline settings, our focus is on online approaches.\nHowever, many existing online methods necessitate an auxiliary reward model or a potent LLM, like GPT-4, for evaluating responses produced by the current policy. This evaluation entails assessing numerous samples, resulting in a substantial cost when using the GPT-4 API. Moreover, the considerable expense of human annotation makes training an efficient reward model a costly endeavor.\nBased on this, researchers have investigated LLM alignment using self-rewarding methods, such as those presented in Yuan et al. However, most self-rewarding approaches still depend on discriminators. For instance, Yuan et al. propose a policy that serves as both an actor and a judge, using a prompt to score its responses. Despite this, discriminators continue to rely on large-scale models like LLaMA-70B, which is inefficient in terms of both memory usage and inference speed. Moreover, smaller language models lack the necessary discriminative power, making it challenging to effectively evaluate preference datasets. We adopt the approach presented in Yuan et al. and apply it to Mistral-"}, {"title": "RELATED WORK", "content": ""}, {"title": "REINFORCEMANT LEARNING FROM HUMAN FEEDBACK", "content": "RLHF has recently proven crucial in developing state-of-the-art LLMs like ChatGPT, Gemini, and Claude. The standard RLHF framework for LLM alignment is proposed by Ouyang et al.. They train a reward model on a dataset of human preferences and then fine-tuned a pretrained LLM to maximize the reward from this reward model using the Proximal Policy Optimization (PPO) algorithm. However, PPO-style algorithms are characterized by instability, sample inefficiency, and a high demand for careful hyperparameter tuning. Fitting a high-quality reward model also requires substantial human-labeled data. Consequently, these factors lead to prohibitively high computational costs for PPO-based RLHF methods."}, {"title": "PREFERENCE OPTIMIZATION", "content": ""}, {"title": "OFFLINE PREFERENCE OPTIMIZATION", "content": "Therefore, recent research on RLHF has explored several alternatives to PPO-based methods, with direct preference optimization (DPO) emerging as the most widely used option. DPO is an RL-free algorithm for training language models from preferences. By optimizing a policy using a straightforward binary cross-entropy objective, DPO eliminates the need for reward model training and directly fits an implicit reward model to the preference data. Compared to PPO in RLHF, DPO offers greater stability and reduced computational demands. Several variants of the direct preference learning approach have been proposed, each aiming to address additional challenges of direct preference learning from different perspectives."}, {"title": "ITERATIVE PREFERENCE OPTIMIZATION", "content": "Although DPO-style methods have made significant progress, the preference datasets used in DPO typically consist of responses generated by different LLMs. As a result, the policy model does not receive feedback on its own generations during training, leading to a substantial distribution shift between the policy that generated the dataset and the aligned policy. To address this, further studies have extended the approach to an iterative training setup, continuously updating the reference model for optimization and sampling the current policy to generate preference datasets. However, many of these methods still require an additional reward model or a powerful LLM, such as GPT-4, to label preference data."}, {"title": "REWARD-FREE ALIGNMENT", "content": "Due to the high cost of human annotation, training an effective reward model is very expensive. Some methods attempt to generate preference datasets without external knowledge, such as Yuan et al., which use a single model for both policy and judgment. However, these approaches often rely on powerful models like LLaMA-70B, making it challenging for smaller models to achieve the necessary judgment capabilities. Jiang et al. argue that discrimination is not reliably better than generation, and in fact, performs worse. This suggests that discrimination is a higher-order ability compared to generation. Our approach bypasses discrimination by directly allowing the model to generate preference datasets. Liu et al. propose designing contrastive prompts to generate preference datasets, but their approach is limited to offline settings, and the quality of the generated preference data cannot be guaranteed. Self-play finetuning (SPIN) relies solely on seed SFT data by pairing a ground truth response as a positive sample with a model-generated response as a negative. However, this method assumes that the currently generated responses are always inferior to the original data, which may not accurately reflect reality. Our method is an online, only-prompting self-rewarding preference optimization approach that does not require an additional discriminator for judgment. Instead, it efficiently samples high-quality preference datasets."}, {"title": "METHOD", "content": "In this section, we first introduce an overview of our method in Section 3.1. We then describe the initialization of our method in Section 3.2, followed by the process of generating the preference dataset in Section 3.3. Subsequently, we mathematically prove the existence of a quality gap between the chosen and rejected response, which can be utilized for DPO training in Section 3.3.1. Finally, we present our iterative training strategy in Section 3.4."}, {"title": "OVERVIEW", "content": "In self-rewarding RLHF tasks, the policy does not interact with an external environment to receive feedback. Current methods rely heavily on a discriminator; however, smaller models have weaker judgment abilities, making them less suitable for such tasks. Moreover, repeatedly sampling for a given prompt leads to inefficient sampling. To address the reliance on discriminators and enhance effectiveness on small models, we propose a novel online only-prompting self-rewarding alignment framework. The key ideas of our method include two aspects: (1) directly leveraging the generation capability to create preference datasets without the need for a discriminator; (2) generating fine-grained control over the optimality gap between positive and negative examples, which allows for the creation of more challenging negative cases in later training stages, thereby better aligning the model with complex human preferences. An overview of our framework is depicted in Figure 2."}, {"title": "INITIALIZATION", "content": "In self-rewarding LLM alignment, we essentially align it without relying on any external preference feedback. Similar to Yuan et al., our method first assumes access to a base pretrained language model and a amount of human-annotated seed data {xsft, Xdpo}. Our seed data consists of two parts: one part is instruction-following data used for SFT, and the other part is preference data used for offline preference optimization. We find that applying our method in a bootstrap manner on a model trained with a seed preference dataset can lead to greater performance improvement. This is likely because a model that undergoes preference learning gains a better understanding of human preferences, resulting in stronger generative capabilities. Our initialization process aligns with the offline preference optimization procedure, as detailed below:"}, {"title": "GENERATE PREFERENCE DATASET", "content": "To obtain the preference dataset, we explicitly define a response score for each response, with higher scores indicating better quality. We design two different prompts to leverage the generative ability for eliciting both positive and negative examples. The current policy is directly instructed to generate both high-scoring and low-scoring responses, with the high-scoring responses labeled as chosen and the low-scoring responses labeled as rejected. Specifically, given an input instruction {x}-1, the input for generating a chosen response is {(pc, xi)}=1, while the input for generating a rejected response is {(pr, xi)}1. The prefixs pc and pr are defined as follows:\nChoosen prefix pc\nPlease produce a top-notch response that merits a perfect score of 10 out of 10. [prompt]\nRejected prefix pr\nPlease produce a good response that merits a perfect score of [rejected score] out of 10. [prompt]\nBy designing such pair-wise prompts, the model generates two responses, {y}1 and {y}1.\nWe observe that this straightforward design enables the language model to interpret distinct scoring prompts effectively, thereby producing finely differentiated chosen and rejected responses that align with human preferences."}, {"title": "THEORETICAL DEMONSTRATION", "content": "In this section, we provide a theoretical proof that the chosen responses generated in Section 3.3 are of higher quality than the rejected responses, making them suitable for constructing preference datasets for subsequent rounds of preference optimization. For simplicity, we denote the input instruction as x, the generated response as y \u2208 Y, where Y is response space, and the reward function fas f : XXY \u2192 R, which evaluates the quality of response y given instruction x. The policy distribution of the language model, \u03c0(y | x), determines the probability of generating a specific response y based on instruction x. We consider the policies that share the same support as the reference policy Tref, we take a policy class II as\n\u03a0 = { \u03c0: \u03a7\u2194 \u2206(V) | Supp(\u03c0(\u00b7|x)) \u2286 Supp(\u03c0ref(x)), \u2200x \u2208 X}, (3)\nwher Supp(p) denotes the support of a probability density function p.\nAssumption 1 (The range of f(x, y) is bounded). For all x and y, the function f(x, y) satisfies the bound 0 \u2264 f(x, y) \u2264 Rmax, where Rmax is a constant.\nAssumption 2 (The reward score r given instruction x and response y follows a unimodal distribution). Given any x and y, the conditional probability p(r | x, y) x exp (-y||f(x, y) \u2013 r||\u00b2), where ||\u00b7 ||a denotes the a-norm. For example, setting a = 2 corresponds to a Gaussian distribution. The parameter y controls the steepness of the corresponding distribution. As y increases, the peak of corresponding distribution becomes steeper, leading to a more concentrated distribution."}, {"title": "ITERATIVE TRAINING STRATEGY", "content": "We apply an iterative framework to optimize the current policy, dividing the optimization process into M iterations. For the first iteration, the reference model is the model \u03c0\u03bf obtained after initialization. For subsequent iterations, the reference model is the model from the end of the previous iteration.\nWe believe that in the early stages of training, when the policy has not yet aligned with human preferences, pair-wise datasets with a large gap between the chosen and rejected responses are still beneficial. However, as training progresses and the policy increasingly learns human preferences, it becomes easier to distinguish between chosen and rejected responses with a large gap. Feeding the model such simple data at this stage no longer contributes to further learning improvements. To ensure that the model continues to learn complex human preferences rather than merely becoming proficient with basic distinctions, we reduce the gap between pair-wise data in the later stages of training. Specifically, we progressively increase the rejected score of the rejected prefix pr. For instance, in the first iteration, the score is set to 3, in the second iteration to 5, and in the third iteration to 9. This encourages the generation of more challenging pairwise examples (hard negatives) in the later stages, forcing the policy to detect subtle differences between negatives and positives, thereby aligning with complex human preferences."}, {"title": "EXPERIMENT", "content": ""}, {"title": "EXPREIMENT SETUP", "content": "Models and training settings. We conduct our experiments on Mistral-7B \u00b9 and Mistral-Instruct-7B \u00b2. Our training code is based on the alignment-handbook. We use vLLM to sample responses. Following SimPO, we adopt the training pipeline outlined in their work, utilizing their SFT model. For the base version, we align with Zephyr, and for the instruct version, we use mistralai/Mistral-7B-Instruct-v0.2 as the SFT model. For the initialization dataset, we remain consistent with SimPO. For the base setup, we use the UltraFeedback dataset. For the instruct version, we use the datasets\u00b3 published by SimPO. Motivated by SimPO, for the second phase of initialization, we apply two tricks from their work: length-normalized reward formulation and target reward margin.\nWe find that these techniques achieve better results compared to naive DPO. Our training parameters are largely consistent with SimPO. However, we observed that as the number of iterations increases, it is necessary to reduce the learning rate. For iterations 2 and 3, we use learning rates of le-7 and 1e-8, respectively. In our experimental setup, M is set to 3.\nEvaluation benchmarks. We apply the most widely recognized public instruction-following benchmarks, MT-Bench and AlpacaEval 2.0, to evaluate our method. These benchmarks assess the models' capabilities in handling a diverse range of conversational queries and are well-regarded within the community. Both benchmarks necessitate a judgment model (GPT-4) for scoring. To ensure a fair comparison with previous baselines, we employ GPT-1104-preview 4, a high-performance variant, for the judgment.\nBaseline Our baseline consists of two components: offline preference optimization methods and self-rewarding online optimization methods. For the offline methods, we compare RRHF and SLiC-HF, which apply ranking loss; IPO , which avoids the pair-wise reward assumption; CPO and ORPO , which incorporate the SFT objective into the loss; KTO, which does not require training on paired datasets; and R-DPO and SimPO, which use a length-normalized reward formulation. For self-rewarding online optimization methods, we compare self-rewarding(SR) , which use a single model for both policy and judgment. The prompts used for judgment and scoring are provided in the Appendix."}, {"title": "RESULT", "content": "As presented in Table 1, our method shows significant improvements on AlpacaEval 2.0 and comparable performance on MT-Bench. It is evident that by the first iteration, our method significantly outperforms the current state-of-the-art offline method, SimPO. On AlpacaEval 2.0, our method achieves nearly a 2% improvement over SimPO for both Mistral-7B and Mistral-Instruct-7B in the first iteration. For self-rewarding methods, the results are suboptimal due to the limited judgment capability of smaller models, which hinders their ability to rank responses effectively, resulting in noisy preference data. As the number of iterations increases, AlpacaEval 2.0 scores gradually improve, ultimately surpassing SimPO by 4%. For Mistral-7B and Mistral-Instruct-7B, this yields scores of 25.9% and 34.5%, respectively. For MT-Bench, we find that we achieve results comparable to SimPO. As Meng et al. mentions, minor differences between methods on MT-Bench are possibly a result of randomness, influenced by the limited scale of its evaluation data and the use of a single-instance scoring protocol."}, {"title": "ABLATION STUDY", "content": "Arithmetic control of the optimality gap To analyze the effectiveness of each component, we conduct extensive experiments on Mistral-7B. We examine the impact of arithmetic control of the optimality gap by fixing the rejected response scores across all iterations and performing a set of experiments, as shown in Table 2. From the table, it is evident that removing arithmetic control results in no improvement in performance for iter2 and iter3, with AlpacaEval 2.0 LC remaining around 23% and MT-Bench at 5.9. However, with arithmetic control, the final AlpacaEval 2.0 score increases to 25.9% and MT-Bench to 6.0. This demonstrates that arithmetic control of the optimality gap is crucial. In the later stages of training, the model requires more challenging hard negatives. Exposure to easier cases with large gaps is not effective for learning. Human preferences are complex and difficult to model with simple methods. Gradually reducing the gap during training helps the model focus on hard negatives, allowing it to better capture the nuances of human preferences."}, {"title": "Inference on chosen prompt", "content": "To verify whether the gain brought by the chosen prefix can be directly obtained from inference, we apply the chosen prompt directly to the reference model during the first round of inference. The results show a slight improvement in AlpacaEval 2.0, but it still falls short of the results obtained after the first round of training. For MT-Bench, the results decrease. Upon examining the model outputs, we find that adding the chosen prefix during inference leads to outputs that include additional information beyond the response, such as \u201cOkay, here is a 10-score answer\" or \"The 10-score answer is as follows.\u201d This extra information results in GPT-4 assigning lower scores during evaluation. When generating the preference dataset, we specifically use regular expressions to remove responses containing redundant information, thereby avoiding issues related to extra information."}, {"title": "Analysis experiment", "content": "We evaluate the chosen response and rejected response for each round using an open reward model to see if the results aligned with our expectations. We analyze this from two perspectives: first, whether the same model could generate fine-grained quality assessments based on the responses. From Figure 3, it can be observed that the same model generates responses of varying quality with different prefixes. Higher prefix scores lead to higher quality responses, and lower prefix scores result in lower quality responses. Such post hoc experiments demonstrate that we can design different prefix scores to generate preference datasets of varying quality. Additionally, we observe that as iterations increase, both the reward scores for chosen and rejected responses rise, with the gap between them gradually narrowing, as shown in Figure 4. This could be due to two possible reasons: firstly, the model's capability improves over time, becoming more aligned with human preferences, and thus generating higher quality responses; secondly, we reduce the prefix score difference between chosen and rejected responses, leading to more negatives being generated in the later stages of training. We discuss the advantages of this design in Section 3.4."}, {"title": "CONCLUSION", "content": "In this paper, we propose a novel only-prompting self-rewarding online preference optimization method for LLM alignment. Our method relies solely on generation capability to create preference datasets without needing a discriminator, addressing the issue of poor judgment capability in small models and significantly saving on inference and API costs. Additionally, we provide a mathematical proof that preference datasets generated with different prefix scores exhibit quality differences, which can be used for DPO training. Furthermore, by applying fine-grained score control to the responses, we generate preference datasets of varying quality. In the later stages of training, we gradually reduce the quality gap between chosen and rejected responses, which forces the model to better align with complex human preferences. Extensive experiments on two widely used benchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of our method."}]}