{"title": "KN-LIO: Geometric Kinematics and Neural Field Coupled LiDAR-Inertial Odometry", "authors": ["Zhong Wang", "Lele Ren", "Yue Wen", "Hesheng Wang"], "abstract": "Recent advancements in LiDAR-Inertial Odometry (LIO) have boosted a large amount of applications. However, traditional LIO systems tend to focus more on localization rather than mapping, with maps consisting mostly of sparse geometric elements, which is not ideal for downstream tasks. Recent emerging neural field technology has great potential in dense mapping, but pure LiDAR mapping is difficult to work on high-dynamic vehicles. To mitigate this challenge, we present a new solution that tightly couples geometric kinematics with neural fields to enhance simultaneous state estimation and dense mapping capabilities. We propose both semi-coupled and tightly coupled Kinematic-Neural LIO (KN-LIO) systems that leverage online SDF decoding and iterated error-state Kalman filtering to fuse laser and inertial data. Our KN-LIO minimizes information loss and improves accuracy in state estimation, while also accommodating asynchronous multi-LiDAR inputs. Evaluations on diverse high-dynamic datasets demonstrate that our KN-LIO achieves performance on par with or superior to existing state-of-the-art solutions in pose estimation and offers improved dense mapping accuracy over pure LiDAR-based methods. The relevant code and datasets will be made available at https://**.", "sections": [{"title": "I. INTRODUCTION", "content": "ROBOTICS is gradually becoming integrated into daily human life. Their interaction with the environment and humans relies on a comprehensive perception of both intro- state and extro-state, which is facilitated by SLAM (Simultaneous Localization and Mapping) technologies. Recent rapid advancements in such techniques based on the fusion of LiDAR (Light Detection and Ranging) and IMU (Inertial Measurement Unit), namely the LiDAR-Inertial Odometry (LIO), have significantly propelled the implementation of related applications.\nEarly LIO systems focused on a loosely coupled integration of these two information sources [1]. While this way is straightforward, it does not fully leverage the complementary information between them. Recent research on LIO systems indicates that a tightly coupled fusion is the key to maximizing the complementary characteristics of the two types of sensors [2]-[6]. On one hand, real-time estimation of IMU biases enhances the accuracy of state prediction and helps eliminate motion distortion in point clouds. On the other hand, the registration of the undistorted point clouds with the environmental map can effectively suppress system state drift, enabling the system to work stably through the precise collaboration of LiDAR and IMU. However, existing LIO solutions tend to prioritize localization over mapping. In other words, their map representations often consist of geometric elements that facilitate pose estimation, such as points, lines, surfels, or voxels. Yet, such geometric elements are not what downstream tasks like navigation, augmented reality, and virtual reality aspire to. In practice, such applications prefer a dense three-dimensional scene representation rather than sparse geometric elements.\nTraditional dense reconstruction techniques can be categorized into offline reconstruction based on full maps and online incremental reconstruction. The former includes methods such as Poisson reconstruction [7], [8], which aims to fit surface equations, and graph-cut-based surface reconstruction [9], which focuses on identifying segmentation surfaces. Online reconstruction methods first emerged in RGBD-SLAM [10], where they partition the scene into regular grids and establish a distance field from spatial points to the nearest surfaces, employing the Marching Cubes algorithm for dense scene reconstruction [11]. In addition to the aforementioned traditional methods, dense reconstruction techniques for point clouds have recently achieved promising results driven by advancements in machine learning, especially with the rise of neural radiance fields (NeRF) [12]. In offline reconstruction, some approaches represent the scene as a compact octree and learn features attached to the vertices of spatial grids to fit dense neural fields [13], [14]. For online reconstruction, NeRF-LOAM [15], which is based on octree grid representation, and PIN-SLAM [16], which utilizes sparse neural point representation, both learn SDF (Signed Distance Function) decoders from scene features, track LiDAR poses, and jointly learn map neural features on the fly. Compared to traditional methods, the online solutions based on neural field representations not only enjoy the advantage of constructing continuous fields at arbitrary resolutions but also more naturally leverage data-driven approaches to fully integrate both introspective and extrospective perception information, thereby producing more accurate scene maps.\nHowever, despite the commendable performance of such pure LiDAR-focused neural dense reconstruction techniques in relatively stable near-2D autonomous driving scenarios, their effectiveness in more challenging cases, such as handheld"}, {"title": "II. RELATED WORK", "content": "A. LiDAR-Inertial Odometry\nIn recent years, the effective fusion of LiDAR and IMU has significantly improved the performance of LiDAR-Inertial Odometry/SLAM systems for localization and mapping. Existing approaches can be divided into feature-based ones and direct ones depending on whether geometric features are extracted. Feature-based approaches often rely on LOAM [17] features. Initially, these methods drew inspiration from valuable experiences in visual-inertial SLAM. For example, based on VINS-Mono [18], LIO-Mapping [19] utilizes graph optimization to construct geometric constraints using the line and plane features defined by LOAM [17], making it the first tightly coupled LiDAR-Inertial odometry solution. Subsequently, Qin et al. proposed LINS [2], which achieves effectively tightly coupling of feature-based ICP registration [20] and Kalman filter [21]. Shan et al. introduced LIO-SAM [3], which abandons inter-frame registration, aligns point clouds to local maps using LeGoLOAM [22] features, and jointly registers pose and IMU pre-integration based on smooth and mapping techniques [23], [24] to optimize system state. The reasonable utilization of line/plane features and effective estimation of IMU biases enabled LIO-SAM to achieve impressive results at the time, but its design requires high performance from the IMU and performs moderately in complex and unstructured scenarios. Additionally, CLINS [25] fuses LiDAR-Inertial data in a continuous-time framework, transforming the state estimation problem into a joint optimization problem of continuous-time trajectories and observed features, effectively fusing information from multiple frames but with higher computational complexity.\nIn the domain of direct methods, early works were based on probabilistic grid maps, loosely integrating LiDAR and IMU information [1]. Building upon this representation, Wang et al. drew inspiration from LIO-SAM and proposed the tightly coupled direct approach D-LIOM [4]. Furthermore, to combine laser-visual-inertial information, Wang et al. introduced Ct-LVI [26] within a continuous-time SLAM framework, constructing a tightly coupled LiDAR-Inertial subsystem Ct-LIO through direct registration. In a similar way, Chen et al. [5] improved upon the direct LiDAR odometry system (DLO) by tightly coupling state estimation based on GICP in a coarse-to-fine manner, achieving satisfactory performance in various complex scenarios. Building on their earlier work Fast-LIO [27], Xu et al. no longer extract point cloud line features, but directly construct point-to-plane constraints by fitting surfaces to neighboring points queried in an ikdtree, proposing Fast-LIO2 [6], which has achieved state-of-the-art performance in terms of localization accuracy and robustness.\nB. Dense Mapping\nTraditional dense mapping methods can be classified into Poisson-based, tetrahedron-based, voxel-based, and facet-based. Poisson-based reconstruction [7], [8] represents the scene surface as an indicator function and fits the target object/scene surface gradients (in the normal vectors of the point cloud) through optimization techniques to reconstruct"}, {"title": "III. METHOD", "content": "A. Overview\nThe system workflow is illustrated in Fig. 1. We represent the scene as a set of learnable neural points, where the SDF value at each spatial position is encoded by the weighted distance from its surrounding neural points' features and estimated by a simple MLP decoder. For the asynchronous heterogeneous LiDAR input, we first generate an integrated point cloud based on spatiotemporal parameters. For the high-frequency IMU data, we recursively estimate the nominal state and error state, as well as the corresponding error state covariance, using an error-state Kalman filter, and remove point cloud distortions based on the propagated state. Subsequently, within the semi-coupled KN-LIO, we register the de-skewed point cloud to the current neural map by minimizing the SDF residuals of sampled points to obtain the registration pose, and then update the ESKF with this pose. In the tightly coupled KN-LIO, starting from the nominal state, we iteratively update the Kalman filter by computing the SDF residuals of sampled points, until the ESKF converges. Finally, based on the optimized pose, we jointly optimize and update the local neural map by minimizing the BCE loss between sampled points and the neural field, and generate a dense mesh from the estimated neural field resorting to marching cubes.\nB. Notation\nWe define the state of KN-LIO as:\n$x = \\{p, v, R, b_g, b_a, g\\} \\in M = SO(3) \\times \\mathbb{R}^{15}$,\nwhere the elements the system's position (p), velocity (\u03c5), gyroscope bias ($b_g$), accelerometer bias ($b_a$), and gravity g are in $\\mathbb{R}^3$, while rotation (R) is in SO(3). p, v, R, and g are in the world frame by default, while $b_g$ and $b_a$ are in the body frame (IMU frame).\nWe define the corresponding error state to be estimated as:\n$\\delta x = [\\delta p^T, \\delta v^T, \\delta \\theta^T, \\delta b_g^T, \\delta b_a^T, \\delta g^T] \\in \\mathbb{R}^{18}$,\nwhere $\\delta \\theta$ is the axis-angle of the rotation error state $\\delta R$, which can be transformed from/to $SO(3)$ by logarithmic/exponential mapping.\nC. Error-State Kalman Filter\nThe Kalman filter for error states considers the true state of the system as the sum of the nominal state and the error state. It recursively calculates the covariance of the nominal"}, {"title": "1) Continuous-time Kinematics Model:", "content": "In continuous time, the motion of a rigid body system in an inertial frame follows Newton's laws of motion. The nominal kinematic model is:\n$\\dot{p} = v$,\n$\\dot{v} = R(\\tilde{a} \u2013 b_a \u2013 n_a) + g$,\n$\\dot{R} = R(\\tilde{\\omega} \u2013 b_g \u2013 n_g)^$,\n$\\dot{b_g} = n_{b_g}$,\n$\\dot{b_a} = n_{b_a}$,\n$\\dot{g} = 0$,\nwhere $\\tilde{a}$ and $\\tilde{\\omega}$ denote the IMU readings of acceleration and angular velocity, $n_{b_a}$ ($n_{b_g}$) is the random inpulse of accelerometer (gyroscope), $(\\cdot)^$ means the skew-symmetric matrix of the given vector. The corresponding error-state kinematics model is formulated as:\n$\\delta \\dot{p} = \\delta v$,\n$\\delta \\dot{v} = -R(\\tilde{a} \u2013 b_a)^\\delta \\theta \u2013 R\\delta b_a \u2013 n_{\\nu} + \\delta g$,\n$\\delta \\dot{\\theta} = -(\\tilde{\\omega} \u2013 b_g)^\\delta \\theta \u2013 \\delta b_g \u2013 n_g$,\n$\\delta \\dot{b_g} = n_{b_g}$,\n$\\delta \\dot{b_a} = n_{b_a}$,\n$\\delta \\dot{g} = 0$.\n2) Discrete-time Kinematics Model: By performing kinematic integration on the time dimension of the above equation, the nominal state kinematic equation in discrete time can be obtained as follows:\n$p(t + \\Delta t) = p(t) + v \\Delta t + \\frac{1}{2} R(\\tilde{a}-b_a) \\Delta t^2 + \\frac{1}{2} g \\Delta t^2$,\n$v(t + \\Delta t) = v(t) + R(\\tilde{a} \u2013 b_a)\\Delta t + g\\Delta t$,\n$R(t + \\Delta t) = R(t)Exp((\\tilde{\\omega} \u2013 b_g)\\Delta t)$,\n$b_g(t + \\Delta t) = b_g(t)$,\n$b_a(t + \\Delta t) = b_a(t)$,\n$g(t + \\Delta t) = g(t)$,\nwhere $\\Delta t$ is the integration interval and $Exp(\\cdot)$ denotes the compound exponential map from $\\mathbb{R}^3$ to SO(3). Correspondingly, the error state kinematic equation for discrete time is:\n$\\delta p(t + \\Delta t) = \\delta p + \\delta v \\Delta t$,\n$\\delta v(t + \\Delta t) = \\delta v(t) + (-R(\\tilde{a} \u2013 b_a)^\\delta \\theta \u2013 R\\delta b_a + \\delta g)\\Delta t \u2013 n_{\\nu}$,\n$\\delta R(t + \\Delta t) = R(t)Exp((\\tilde{\\omega} \u2013 b_g)\\Delta t)$,\n$\\delta b_g (t + \\Delta t) = \\delta b_g + n_g$,\n$\\delta b_a (t + \\Delta t) = \\delta b_a + n_a$,\n$\\delta g (t + \\Delta t) = \\delta g$.\n3) Propagation, Update, and Reset: According to the traditional form of the extended Kalman filter, the error state"}, {"title": "transition equation in matrix form can be obtained by reorganizing the above equations:", "content": "$\\delta x \\approx F \\delta x + w, w \\sim N(0, Q)$,\n$Q = \\Lambda(0, Q_{\\nu}, Q_{n_{\\theta}}, Q_{n_{b_g}}, Q_{n_{b_a}}, O)$,\nwhere F is the transition matrix of the error state which can be obtained from Eq. 6, Q is the covariance matrix of the error-state noise w, and $\\Lambda$ denotes the diagonal matrix. For each step of error state propagation, the covariance of the error state (P) needs to be mapped to a new tangent space by the transition matrix F and incorporate new noise:\n$P \\leftarrow FPFT + Q$.\nIf there are no new observations except the IMU readings, the uncertainty of the system will continue to increase with the recurrence of covariance. When a new sensor observes input, we can reduce this uncertainty by correcting the error state. The update at this time can be carried out according to the standard form of the extended Kalman filter:\n$K = PHT (HPHT + V)^{-1}$,\n$\\delta x = K(z - h(x))$,\n$x \\leftarrow x + \\delta x$,\n$P \\leftarrow (I \u2013 KH)P$,\nwhere H is the Jacobian matrix of the measurement h(\u00b7) with respect to the error-state $\\delta x$, which can be obtained by the chain rule:\n$H = \\frac{\\partial h}{\\partial x} = \\frac{\\partial h}{\\partial \\tilde{x}} \\frac{\\partial \\tilde{x}}{\\partial \\delta x}$,\nwhere the first term depends on the specific observation equation which will be discussed in subsequent sections, while the second term is known:\n$\\frac{\\partial \\tilde{x}}{\\partial \\delta x} = diag(I_{3 \\times 3}, I_{3 \\times 3}, J^{-1}(R), I_{3 \\times 3}, I_{3 \\times 3}, I_{3 \\times 3})$,\n$J^{-1}(R) = I_{3\\times 3} + \\frac{1}{2}\\delta \\theta^\\wedge + \\frac{1-cos ||\\theta||}{||\\theta||^2} (\\delta \\theta^\\wedge)^2 - \\frac{||\\theta|| - sin||\\theta||}{2||\\theta||^3} (\\delta \\theta^\\wedge)^3$.\nAfter completing an update of the error state and covariance, we need to reset the error state to prepare for the next update. Among them, the reset of the error state is trivial, resetting all variables to the zero vector is sufficient, while the covariance needs to be reset to the new state reference point. That is, the following reset operation is performed:\n$\\delta x = 0$,\n$P = JPJT$,\n$J = diag(I_{6 \\times 6}, J_\\theta, I_{9 \\times 9})$,\n$J_\\theta = I_{3 \\times 3} - \\frac{1}{2} \\delta \\theta^\\wedge$.\nD. Neural Map Representation\nCurrently, in neural SDF map representations for LiDAR, there are typically two organizational forms: octree-based and point-based. The former divides space into fixed-size voxels at a certain resolution, storing voxels hit by point clouds in an octree, with each vertex of the voxel attached with"}, {"title": "E. System Initialization", "content": "Like most LiDAR-Inertial systems, we initialize the system in a stationary state. We define the robot's starting point as the origin of the world frame, with the world frame's z-axis perpendicular to the gravity level, and the direction of gravity aligned with the negative z-axis. With this definition, we can accumulate multiple frames of IMU readings to estimate the initial orientation of the system. Let the accelerometer and gyroscope readings (A, W) during the initialization phase be:\n$A = \\{\\tilde{a_0},..., \\tilde{a_n}\\}$,\n$W = \\{\\tilde{\\omega_0},..., \\tilde{\\omega_n}\\}$.\nWe believe that the bias of the accelerometer is a very small amount relative to the measured acceleration value, so the initial orientation of the carrier can be obtained by aligning the direction of the acceleration mean vector with the opposite direction of gravity:\n$R_0 = R(e, e_g)$,\nwhere e is a unit vector and A is the mean of A. On the basis of obtaining the initial rotation, if the local gravity acceleration value is known, we can also estimate the initial bias of the accelerometer:\n$b_{a_0} = \\bar{A} + R_0g$.\nAs usual, the initial position and velocity are set as zero vectors, while the mean gyroscope reading is regarded as the initial guess of $b_g$.\nF. Sensor Data Preprocessing\nThe rotating mechanical LiDAR has low point cloud resolution in the vertical direction, while the existing solid-state LiDAR can only present point clouds within the forward view. To enhance the robustness of LIO systems in various complex"}, {"title": "G. Semi-Coupled KN-LIO", "content": "In the semi-coupled mode, we consider treating point cloud and IMU observations relatively independently. That is, we first utilize an IMU readings to propagate the error-state Kalman filter and then correct the filter using the registration results. The advantage of this approach is that it decouples the data processing for the two types of sensors through result-level fusion, while simultaneously achieving timely correction of the system state and IMU biases by propagating external LiDAR observation information. By linking the chain of state propagation prediction, point cloud distortion correction, point cloud registration with the neural map, and filter update, the system can achieve long-term stable running.\nWe perform direct registration from point clouds to neural maps to obtain LiDAR poses. Assuming the pose of point p is T = {R,t}, the purpose of registration is to adjust the pose so that the SDF value at its spatial position approaches 0, that is:\n${R, t}^* = arg \\underset{R,t}{min} \\sum f(p_i)^2$\n=${R, t}^* = arg \\underset{R,t}{min} \\sum D(Rp_i + t)^2$.\nTo use traditional optimization algorithms such as the Levenberg-Marquardt algorithm to solve the optimal pose, we need to obtain the derivatives of f(pi) for rotation and translation parameters:\n$\\frac{\\partial D(Rp_i +t)}{\\partial t} = \\frac{\\partial D(p')}{\\partial p'} n_i$,\n$\\frac{\\partial D(Rp_i +t)}{\\partial R} = \\frac{\\partial D(p')}{\\partial p'} \\frac{\\partial p}{\\partial \\theta} = n_i \\times (p_i \\times n_i)$,\n$J_{f(p_i)} = [n_i^T, (-p_i \\times n_i)^T]$,\nwhere $n_i$ denotes the normal vector of the neural field at pi and $\\theta$ is the axis-angle of the rotation."}, {"title": "H. Tightly Coupled KN-LIO", "content": "Although the semi-coupled mode can leverage LiDAR registration results to correct IMU biases and offers the advantage of relatively decoupling heterogeneous observations, the approach of first registering and then optimizing biases may entail certain risks of information loss. Therefore, this paper also tightly couples the registration of point clouds to the neural field with the estimation of IMU biases. With the goal of finding the optimal registration, we combine the estimation of the error-state Kalman filter with the iterative registration process, known as the iterative Kalman filter.\nIn each iteration, we solve the following least squares problem:\n$\\delta x^* = arg \\underset{\\delta x}{min} ||z \u2013 H(x + \\delta x)||^2 + ||\\delta x||^2, \\forall \\delta x \\in M$,\nwhere \\oplus indicates the plus operation on the manifold M. In the Bayesian framework, this objective can be derived to yield analytical solutions as shown in Eq. 10~Eq. 13. As shown in these equations, to perform tightly coupled iterations of the Kalman filter, we need to determine the observation equation at this time and the corresponding matrix H. Our registration objective remains as shown in Eq. 26, thus the observation model can be defined as:\n$z_t = h_t(p_x) + v_t = \\begin{bmatrix} h(p_{1x})\\\\  :\\\\ h(p_{nx}) \\end{bmatrix} + v_t$,\n$\\begin{bmatrix} D_0(Rp_0 + t)\\\\  :\\\\ D_n(Rp_n + t) \\end{bmatrix}$,\nTo update the Kalman filter using this observation model, we need to calculate the partial derivative of h(\u00b7) with respect to x:\n$\\frac{\\partial h(p_{ix})}{\\partial x} = \\frac{\\partial D_i}{\\partial p'} \\frac{\\partial p'}{\\partial x}$,\n$\\frac{\\partial p'}{\\partial x} = n_i^T[I_{3\\times 3} \\  0_{3\\times 3} \\  R_p \\  0_{3\\times 9}]$,\nwhere $O_{3\\times 3} / 0_{3\\times 9}$ is a 3 \u00d7 3/3 \u00d7 9 zero matrix. Thus, the i-th row of the matrix H is determined by the chain rule combining Eq. 14 to Eq. 16:\n$H_i = \\frac{\\partial h_i}{\\partial \\tilde{x}} \\frac{\\partial \\tilde{x}}{\\partial \\delta x} = [n_i \\ 0 \\ n_iR_pJ(R) \\ 0]$,\nwhere $0_{3/9}$ denotes a zero vector with 3/9 elements. We can estimate the error state and update the covariance according to Eq. 10~Eq. 13, however, a single point cloud"}, {"title": "typically contains thousands of points, which would make the inversion part in Eq. 10 computationally expensive. To address this issue, we resort to the Sherman-Morrison-Woodbury identity transformation and calculate the matrix K in an equivalent way as follows:", "content": "$K = (P^{-1} + HTV^{-1}H)^{-1}HTV^{-1}$.\nAs a result, the dimension of the inversion in each iteration is reduced from a variable N (number of points) to a constant 18 (the dimension of error state), making the system more efficient and stable. In each iteration, we calculate the error state according to Eq. 37 and Eq. 11 and update the state by Eq. 13. If the value of the current error state is below a certain threshold or reaches the maximum number of iterations, we consider the iteration to be complete, and transform the covariance matrix to the position of the last iteration using Eq. 13."}, {"title": "IV. EXPERIMENTS", "content": "A. Setup\n1) Dataset: We evaluated our KN-LIO and semi-KN-LIO on three widely-used datasets. One of them is the VIRAL dataset collected from a drone, consisting of 18 sequences from various indoor and outdoor courtyard scenes, including both structured and unstructured environments. Among these, the first 9 sequences exhibit relatively smooth motion, while the subsequent 9 sequences involve more aggressive drone movements, such as rapid ascents and fast rotations. Most existing methods have only been tested on the former 9 sequences. In this paper, to conduct a more comprehensive evaluation, we experiment with relevant competing approaches on all sequences and report detailed results accordingly.\nOne dataset is HILTI2022 collected from a construction site, where each sequence comprises featureless and varying lighting environments, posing challenges for an LIO system. This dataset is captured using a handheld device equipped with a Hesai PandarXT-32 LiDAR and Sevensense Alphasense Core. We evaluated the relevant algorithms on sequences with publicly available 6-DoF millimeter-level ground truth poses, specifically including \u201cExp04 Construction Upper Level 1\u201d (Exp04), \u201cExp05 Construction Upper Level 2\u201d (Exp05), \"Exp06 Construction Upper Level 3\" (Exp06), \u201cExp14 Basement 2\" (Exp14), \"Exp16 Attic to Upper Gallery 2\" (Exp16), and \"Exp18 Corridor Lower Gallery 2\" (Exp18).\nWe conducted quantitative testing of the algorithm for mapping on Newer College. The dataset provides scenes reconstructed using survey-grade laser scanners. Typically, pure LiDAR mapping methods are tested on segments extracted from the sequence \"02_long_experiment\". For comparison purposes, we also extracted the corresponding IMU data from the raw dataset, while retaining IMU readings corresponding to 10 frames of point clouds for system initialization. It should be clarified that these 10 frames of point clouds within the corresponding time period were not involved in map reconstruction.\n2) Metrics: Regarding localization, following the common practice in related methods [6], [16], we evaluated the absolute positioning error of the approaches. For each estimated pose sequence of a method, we utilized the Umeyama algorithm [44] to estimate its transformation relative to the ground truth, and subsequently calculated its Root Mean Square Error (RMSE). In terms of mapping, following the method proposed in [45], we assessed the accuracy (Acc.), completeness (Comp.), chamfer distance (C-L1), and F-Score of the reconstructed maps generated by the algorithm. Lower values for the first three metrics indicate better reconstruction quality, while a higher value for the last metric signifies a higher recall rate and precision.\nB. Traits Comparison\nFirstly, we compare the characteristics of our approach with competing methods. In terms of map representation, apart from our KN-LIO and semi-KN-LIO, only PIN-SLAM and SLAMesh support incremental dense reconstruction, with only PIN-SLAM and our approach being based on neural field representation, enabling continuous Signed Distance Function (SDF) field estimation. Regarding pose estimation, while several methods are feature-free, except for our approach, only D-LIOM and Ct-LIO support multiple LiDAR inputs. Furthermore, among all methods, our approach simultaneously supports dense reconstruction, direct registration, inertial information fusion, neural representation, and multi-LiDAR information fusion.\nC. Results of Localization\nFrom Table II, first analysing the results on VIRAL, it is evident that the successful integration of the IMU in our approach enables KN-LIO to achieve higher accuracy and better robustness compared to pure LiDAR-based dense SLAM methods (such as the neural field-based PIN-SLAM and the geometric mesh-based SLAMesh). Our KN-LIO outperforms the excellent-performing LIO-SAM in geometric feature-based methods. Compared to traditional direct approaches (D-LIOM, Ct-LIO, and dlio), our neural field-based pose estimation achieves higher accuracy, whether in semi-coupled or tightly coupled way. Compared to the state-of-the-art direct method Fast-LIO2, our performance is still comparable, with Fast-LIO2 achieving the best results on 7 sequences and our approach achieving the best on 8 sequences. Comparing the results of Semi-KN-LIO and KN-LIO, although semi-coupling significantly improves positioning accuracy compared to pure"}, {"title": "D. Results of Mapping", "content": "1) Qualitative: For the 6 scenes and 18 sequences in the VIRAL dataset, we conducted separate tests using PIN-SLAM, ImMesh, and our KN-LIO. We plotted the mapping results of each algorithm achieving the best pose tracking in each scene in Fig. 3. It can be observed that in terms of completeness, KN-LIO demonstrates the best capability, for example, in the grounds in \"rtp\" and \"spms\". In terms of detailed reconstruction, KN-LIO also exhibited higher accuracy, such as in the trees in \"eee\", the streetlights in \"sbs\", and the spiral staircase in \"tnp\". Furthermore, we found that the geometry-based ImMesh method tends to produce more noise compared to the neural field-based PIN-SLAM and KN-LIO, which is detrimental to detailed reconstruction.\n2) Quantitative: In terms of quantitative mapping results, Table III presents the relevant results of different mapping algorithms at Newer College. VDB-Fusion [47], SHINE-Mapping [13], and NKSR [48] are pure mapping algorithms, with their poses provided by the state-of-the-art LiDAR Odometry method KissICP [49], while PUMA [8], SLAMesh [46], NeRF-LOAM [15], S2KAN-SLAM, ImMesh [34], PIN-SLAM [16], and our KN-LIO estimate poses online within their algorithms. The data in Table III shows that our KN-LIO performed well across all metrics. Compared to the state-of-the-art LiDAR-based mapping method PIN-SLAM, KN-LIO significantly improves reconstruction accuracy and completeness through effective integration of the IMU. In comparison to ImMesh, which estimates poses based on Fast-LIO2, the superior mapping performance of KN-LIO indicates that the learning-based approach is more conducive to the effective fusion of multi-frame observation data, leading to noise reduction and accuracy improvement.\nE. Multi-LiDAR Support\nTo validate (Semi-)KN-LIO's support for multiple LiDAR inputs, we simultaneously utilized the horizontal and vertical LiDAR data from the VIRAL dataset to analyze their impact on the system's localization and mapping performance. As shown in Table IV, with the fusion of multi-view data, our approach exhibited higher pose estimation accuracy and better robustness compared to traditional probabilistic voxel-based methods. Compared to the results of single LiDAR scenarios in the table, it can be observed that the improvement in"}, {"title": "V. CONCLUSION", "content": "To achieve accurate simultaneous pose estimation and dense mapping, we propose a coupled LiDAR-Inertial Odometry system that integrates geometric kinematics with neural fields, (Semi-)KN-LIO. We fuse laser and inertial sensor information through an error-state Kalman filter. In the semi-coupled mode, the point cloud is first registered with the neural field, and then the error-state Kalman filter is updated with the composite registration result. In the tightly coupled mode, the system iteratively updates the error-state filter with the Kalman gain using residuals between the point cloud and the neural field until convergence. By coupling geometric kinematics with neural fields, our KN-LIO achieves comparable localization results to existing state-of-the-art LIO solutions on multiple public datasets and generates higher-quality dense mapping results than those of pure LiDAR solutions. Furthermore, support for multiple LiDARs enables our (Semi-)KN-LIO to easily fuse point cloud data from different perspectives."}]}