{"title": "Theoretical limitations of multi-layer Transformer", "authors": ["Lijie Chen", "Binghui Peng", "Hongxun Wu"], "abstract": "Transformers, especially the decoder-only variants, are the backbone of most modern large language models; yet we do not have much understanding of their expressive power except for the simple 1-layer case.\nDue to the difficulty of analyzing multi-layer models, all previous work relies on unproven complexity conjectures to show limitations for multi-layer Transformers. In this work, we prove the first unconditional lower bound against multi-layer decoder-only transformers. For any constant L, we prove that any L-layer decoder-only transformer needs a polynomial model dimension ($n^{\\Omega(1)}$) to perform sequential composition of L functions over an input of n tokens.\nAs a consequence, our results give: (1) the first depth-width trade-off for multi-layer trans-formers, exhibiting that the L-step composition task is exponentially harder for L-layer models compared to (L + 1)-layer ones; (2) an unconditional separation between encoder and decoder, exhibiting a hard task for decoders that can be solved by an exponentially shallower and smaller encoder; (3) a provable advantage of chain-of-thought, exhibiting a task that becomes exponen-tially easier with chain-of-thought.\nOn the technical side, we propose the multi-party autoregressive communication model that captures the computation of a decoder-only Transformer. We also introduce a new proof tech-nique that finds a certain indistinguishable decomposition of all possible inputs iteratively for proving lower bounds in this model. We believe our new communication model and proof technique will be helpful to further understand the computational power of transformers.", "sections": [{"title": "1 Introduction", "content": "The Transformer architecture [VSP+17] forms the backbone of modern large language models (LLMs). When pre-trained on vast corpora and fine-tuned on expert datasets, Transformers achieve impressive performance across a range of natural language tasks [AAA+23] and demonstrate emergent intelligence [BCE+23].\nThere is no doubt that the Transformer is an ingenious and powerful architecture, as evidenced by many of its substantial empirical successes. Nevertheless, does the architecture have any \u0440\u043e-tential limitations or weaknesses? We believe this is a fundamental question, especially given the widespread deployment of LLMs. In this paper, we investigate this question from a computational (representational) perspective, viewing neural networks as parameterized functions and studying what they can compute efficiently.\nThe literature on the representation power of Transformers is extensive (see Section 1.4 for a detailed discussion). Prior to our work, we only have a solid understanding of one-layer Transformers unconditional lower bounds have been established for various basic tasks, such as the composition of two functions [PNP24], 3SUM [SHT23], and induction heads [SHT24a]. However, for the arguably more interesting case of multi-layer Transformers, unconditional lower bounds remain elusive even for two-layer models. There is even speculation in the literature that multi-layer Transformers face circuit lower bound barriers, meaning that proving lower bounds for multi-layer Transformer may require first resolving long-standing open qestion on circuit lower bounds, see [Hah20] for a detailed discussion. As a result, researchers have turned to characterizing the limitations of Transformers through computational conjectures, proving that the computational power of Transformers is con-tained within constant depth threshold circuit $TC^{0}$ [MS23b], log-space computation L [PNP24] and Massively Parallel Computation (MPC) [SHT24b].\n\"Small\" vs. \"Large\" transformers. To study the transformer as a computational model, we often consider the context length (prompt length) n as a growing parameter and study the required size of the transformer (in terms of parameters such as head embedding dimension d and number of attention heads H) for solving particular problems. Note that the total number of parameters of a transformer is roughly L\u00b7 poly(dHp), which is independent of the context length n, where L is the number of layer, p is the number of bit precision for each entry in the embedding and one can often think p as O(log n). Following the convention of [SHT23], we consider the transformer small if $dHp \\le n^{o(1)}$, and large if $dHp > n^{\\epsilon}$ for some constant $\\epsilon > 0."}, {"title": "1.1 Our result", "content": "In this paper, we prove the first unconditional lower bounds for any constant layer (decoder-only) Transformer. Indeed, we prove that no small Transformer can not solve sequential composition tasks over long context; see Section 2.1 for a formal definition of transformers.\nTheorem 1.1 (Lower bound for multi-layer Transformer). Let H be the number attention heads, d be the head dimension, p be the precision, L be the number of layers, n be the prompt length. For any $L \\le \\tilde{O}(\\log \\log(n))$, an L-layer decoder-only Transformer could not solve L-sequential function composition whenever $Hdp < n^{2^{-4L}}$.\nA formal definition of the L-sequential function composition can be found at Definition 2.1. Roughly speaking, given L functions $f_1,..., f_L$ and a query $w = (w_1,..., w_L)$, it asks to compute $i_1 = f_1(w_1), i_2 = f_2(w_2, i_1), ..., i_L = f_L(w_L, i_{L-1})$."}, {"title": "Encoder vs. decoder", "content": "The focus of our paper is on decoder-only Transformer, which is the most popular architecture among all LLMs. The original Transformer paper [VSP+17] consists of two main components: the encoder and the decoder. The encoder processes input tokens with pairwise attention mechanisms, capturing contextual relationships across the input. In contrast, the decoder employs causal masking to attend only to previous tokens, enabling autoregressive sequence generation.\nIndeed, the history of LLMs reflects a shift from encoder-based to decoder-only architectures. Early models like BERT [JCKT19], built on the Transformer's encoder, excelled in understanding tasks by learning contextual relationships through masked language modeling. However, encoder-based models have been proved limited for text generation, as they lacked autoregressive capabilities. Decoder-only models are trained through next-token prediction, which not only aligns well with text generation tasks but also improves computational efficiency during generation. Following the success of GPTs [RWC+19, MRS+20], all prominent large language models, including Claude [Ant24], Gemini [TGL+24] and LLaMA [Met24], have adopted the decoder-only approach."}, {"title": "Composition", "content": "The ability to perform compositional tasks has been a central focus of empirical research [WTB+22, PZM+23, DLS+23, AG23, YGK+24, WSF+24, PSD+24, JMB+24, YXLAZ24], as compositionality is essential for reasoning and handling complex tasks. [DLS+23] demonstrated through extensive experiments that Transformers struggle with tasks requiring the sequential com-position of elementary steps, such as multiplying multi-digit integers and solving logical puzzles, with performance rapidly declining as the depth of composition increases. Our main theorem, Theorem 1.1, provides theoretical justification for the limitations of Transformers in executing sequential composition.\nOn the other hand, Theorem 1.1 highlights the importance of depth in Transformers. As we elaborate in Section 1.2, Theorem 1.1 implies a depth-size tradeoff for Transformers in compositional tasks. This aligns with the empirical findings of [YXLAZ24], which demonstrate that depth plays a more critical role than width in reasoning and composition tasks."}, {"title": "On the circuit lower bound barrier", "content": "It was suggested in [Hah20] that unconditional lower bounds against encoder-only transformer would imply breakthrough circuit lower bounds against linear threshold circuits.\\u00b9 Our result avoids this barrier by exploiting the information bottleneck and autoregressive nature of decoder-only models. Namely, our proof crucially depends on the fact that in decoder-only models, each token cannot attend to any token after it."}, {"title": "1.2 Applications", "content": "Application 1: Depth-size tradeoff for Transformer\nDepth-size (or depth-width) tradeoff has been extensively studied for neural networks [MCPZ13, ES16, Tel16, LS17, Dan17, SS17, LPW+17, Yar17, SES19, VS20, BN20, VRPS21, LWS+20]. Most work has focused on the feed-forward ReLU network. However, as pointed out by [VS20, VRPS21], proving depth-width tradeoff for L > 4 layer ReLU network faces circuit lower bound and natural proof barriers for benign functions (i.e., functions that can be computed in polynomial time and has polynomial-bounded Lipschitz constant). Hence, existing work [ES16, Dan17] either focus on"}, {"title": "Application 2: Separation between Transformer encoder/decoder", "content": "Theorem 1.1 also implies a separation between the encoder and the decoder architecture, since the L-sequential function composition task can be easily solved by an $O(\\log(L))$-layer Transformer encoder.\nCorollary 1.3 (Separation between encoder and decoder). For any constant L > 1, there exists a task (a.k.a. L-sequential function composition) such that (1) an $O(\\log(L))$-layer Transformer en-coder could solve the task with polylogarithmic number of parameters while (2) any L-layer Trans-former decoder needs polynomial number of parameters to solve the task.\nThere has been a lot of work [FLY+23, AZL23a, AZL23b, NESK24, QMN24] comparing the empirical performance of encoder and decoder architecture, see [Tay24] for a detailed coverage. A recent work [ECZ+24] compares the expressive power between encoder and decoder, showing a strong separation by assuming a conjecture of the hardness of a certain triplet counting problem. In contrast, our work gives the first unconditional separation without any unproven assumptions."}, {"title": "Application 3: Provable benefits of chain of thought", "content": "The chain of thought (CoT) [WWS+22] is_known to help with the reasoning by inducing the LLM to perform step by step reasoning and eventually leading to the correct answer. From a theoretical view, CoT provides Transformer with extra computation space, and previous work [PBM21, FGZ+23, MS23a, LLZM24] proved that log-precision Transformer with CoT could simu-late any polynomial-time algorithm. Therefore, by further assuming certain complexity conjecture (e.g. PTC), their results imply that constant depth Transformer with CoT could simulate poly-time algorithm, while constant depth Transform ($\\subseteq TC^{0}$) itself can not solve P-complete task.\nTheorem 1.1 implies the first provable benefits of CoT, without relying on any computational complexity conjecture. This is because L-sequential function composition can be easily solved using L-steps of CoT with only polylogarithmic number of parameters.\nCorollary 1.4 (Provably benefits of CoT). For any constant $L \\ge 1$, there exists a task (a.k.a. L-sequential function composition) such that (1) an one-layer Transformer with CoT could solve the task with polylogarithmic number of parameters while (2) any L-layer Transformer decoder needs polynomial number of parameters to solve the task."}, {"title": "1.3 Technique overview", "content": "Below, we provide an overview of Theorem 1.1. In Section 1.3.1, we introduce the autoregressive communication model, which captures the computation of decoder-only transformers. Then, in Section 1.3.2, we define the L-sequential function composition task, for which we will show hardness"}, {"title": "1.3.1 Autoregressive communication model", "content": "In order to capture the computational power of autoregressive models such as decoder-only Trans-formers, we introduce the autoregressive communication model, which is the key conceptual con-tribution of this paper.\nSetting. A protocol in the autoregressive communication model proceeds in L epochs (which is also the number of layers in a corresponding transformer). There are N players, each player $i \\in [N]$ receives $z_i$ as input. Their goal is to let player 1 compute an intended function $f (z_1,..., z_N)$ at the end of L-th epoch.\nIt is also helpful to imagine players are arranged on a line as player N, N \u2212 1, . . ., 1; so autore-gressively, player i can only attend to (i.e., send message to) player j such that $j > i$.\nCommunication. For $l\\in [0 : L]$, let $X_{i}^{(l)}$ be the message collected by player i ($i \\in [N]$) after the l-th epoch of communication. Initially, when $l = 0$, $X_{i}^{(0)}$ is just the input of player i. For $l = 1, 2, . . ., L, the l-th epoch of communication proceeds as follows. For player $i \\in [N]$:\n\u2022 The player i sends a message $\\Gamma_{j,i}^{(l)}$ to all players $j \\in [i + 1 : N]$.\n\u2022 The player $j \\in [i + 1 : N]$, based on its own information $X_{j}^{(l-1)}$ and player i's message $\\Gamma_{i,j}^{(l)}$, it sends a message $\\Pi_{i,j}^{(l)}$ to player i.\n\u2022 Finally, the player i updates its collection of information as\n$X_{i}^{(l)} := X_{i}^{(l-1)} \\cup \\cup_{j>i} \\Pi_{j,i}^{(l)}$\nThat is, the information state of player i is updated to include all messages received from players $j \\in [i + 1 : N]$.\nFinally, the player 1 returns an output based on its information state $X_{1}^{(L)}$.\nThe most salient feature of the autoregressive communication model is that the players are forgetful. That is, the player j does not remember anything sent from player $i \\in [1 : j \u2212 1]$; see Section 3 for a formal definition of the autoregressive communication model.\nTransformer as autoregressive communication. The Transformer architecture can be cap-tured as by the autoregressive communication model. In particular, if we partition the input prompt as $(z_N, ..., z_1)$ (where each $z_i$ can contain multiple tokens), the Transformer can be seen as a special autoregressive communication protocol, where each attention layer implements one epoch of communication and the MLP layers between the attention layers are used to perform local computation. The message $\\Gamma_{i,j}^{(l)}$ contains the queries from positions corresponding to tokens of $z_i$ and the returned message $\\Pi_{j,i}^{(l)}$ contains the partial sum of key/value. Moreover the size of $\\Gamma_{i,j}^{(l)}$ and"}, {"title": "1.3.2 Sequential function composition", "content": "Now we elaborate on the L-sequential function composition task and explain why an autoregressive communication protocol with small message bits $B = 2Hdp$ fails to solve it.\nIntuition. We first provide intuitions on what makes a task hard for autoregressive communica-tion models. Intuitively, the player i has stronger communication power than player $j > i$, since player i could communicate to all players $[i + 1 : N]$ and it remembers all their returned message in the information state. Therefore, the failure of an autoregressive communication protocol happens in the regime of $|z_N| \\gg |z_{N-1}| \\gg \\dots \\gg |z_1|$. On the other hand, in this regime, player j possess much more information than player i regard the entire sequence $(z_N, ..., z_1)$ since its input has larger size and its communication capacity is larger (i.e., $|z_j|\\cdot2Hdp \\gg |z_i|\\cdot2Hdp$). In order to avoid shortcut in the communication, we must make sure that player 1 holds important \u201csecrets\" that are crucial for all players $j\\in [2 : N]$.\nTo this end, consider the L-sequential function composition task. Let $m, n_1,...,n_{L-1}$ be pa-rameters and $N_l = m \\cdot \\prod_{l'=1}^{l} n_l$ for $l \\in [0 : L]$.\nL-sequential function composition. Our task, L-FuncComp(w, z0, z1, . . ., zL), takes a sequence of functions z0, z1 ..., zL as input, where z0 \u2208 [m] and zl : [Nl\u22121] \u2192 [Nl\u22121] for l \u2208 [L] and a query w = (w1,..., wL\u22121) \u2208 [n1] \u00d7 \u00b7\u00b7\u00b7 \u00d7 [nL\u22121]. The output is defined inductively as follows: First, one computes\ni0 = z0 \u2208 [m], i1 = z1(i0) \u2208 [N0]\nand one inductively computes, for each l = 1, 2, . . ., L \u2212 1:\ni2 = z2(w1, i1) \u2208 [N1], .... il+1 = zl+1(wl, il) \u2208 [Nl].\nThe final output is then defined as\nL-FuncComp(w, z0, z1, ..., zL) = iL.\nIn the autoregressive communication model, for the L-sequential function composition task, we have $N = L + 2$ parties. For the sake of exposition, we rename them as player L, L \u2212 1, . . ., 0, -1, where the player l receives zl ($l \u2208 [0 : L]$) and the player -1 receives the query z\u22121 := w. For simplicity, we can assume that the message $\\Gamma_{i,j}^{(l)}$ is just the whole information state $X_{i}^{(l-1)}$ of player i at the end of epoch $l - 1$."}, {"title": "1.3.3 Communication lower bound", "content": "Next, we elaborate on the communication lower bound for the sequential function composition task, which is the main technical part of this paper. In this overview, we hide the precise choice of parameters and highlight the key ideas behind the proof.\nFor convenience, we set $A_l = [N_{l-1}]^{N_{l-1}}$ be the input domain of player l ($l \u2208 [1 : L]$). We also set $A_0 = [m]$ and $A_{-1} = [n_1] \\times \\cdots \\times [n_{L-1}]$ be the input domain of player 0 and player -1, respectively."}, {"title": "4 Autoregressive Communication Lower Bound", "content": "Our main result (Theorem 1.1) can be obtained from Lemma 3.1 and the following lower bound for the communication problem.\nTheorem 4.1. There is no deterministic autoregressive communication protocol solving L-FuncComp with L epochs and $B = Hdp$ message bits.\nNotation. For notational convenience, we use z\u22121 and w interchangeably to denote player -1's input. In the following, we elaborate on several key definitions that will be crucial to our proof.\n\u2022 (The transcript $\\Pi_{j,i}^{(l)}$) For any $i \\in [-1 : L \u2212 1]$, $j \\in [i + 1 : L]$, $l \\in [L]$, recall $\\Pi_{j,i}^{(l)}$ is the transcript sent from the player j to the player i at the l-th epoch of communication. Its value is determined by the input of players [i : L], i.e., $z_L, ..., z_i$, and its value is independent of the choice of $z_{i\u22121},..., z_0, w$.\nFor any fixed value $z_L \\in [N_{L-1}]^{N_{L-1}},..., z_i \\in [N_{i-1}]^{N_{i-1}}$, let $\\Pi^{(l)}_{j,i}(z_L,..., z_i)$ be the transcript when the players t receives input $z_t = \\tilde{z}_t$ ($t \\in [i : L]$).\n\u2022 (The partial composition value $i_l(\\tilde{w}, \\tilde{z}_0,..., \\tilde{z}_l)$) For any $l \\in [0 : L]$, the value of il is deter-mined by w, z0,..., zl. We write $i_l(\\tilde{w}, \\tilde{z}_0, ..., \\tilde{z}_l)$ to denote the value of il when $w = \\tilde{w}, z_0 = \\tilde{z}_0,..., z_l = \\tilde{z}_l$.\nParameters. We use the following parameters\nXl = K^{8L-l\u22121} (\\forall l \u2208 [0 : L - 1]), Al = [Nl\u22121]^{Nl\u22121} (\\forall l \u2208 [L])\nand\n\\Delta_l = 2^{4\\sqrt{K}(x_0...x_{l-2})\\cdot(n_1...n_{L-1})} (\\forall l \u2208 [2 : L]), \\Theta_l = 8^{-Ll}(x_0... x_l)\\cdot (n_1... n_{l-1}) (\\forall l \u2208 [L - 1]).\nFor notational convenience, we also set $A_{-1} = \\prod_{i=1}^{L-1}[n_i]$ and $A_0 = [m]$. Note that with our convention of denoting w by z-1, player i takes an input from Ai for every $i \\in [-1 : L]$.\nThe meaning of parameters will be clearer after we state our main technical centerpiece Lemma 4.5.\nIndistinguishable decomposition. Our key idea is the following concept of indistinguishable decomposition, which is two sets $R_{>l}$ and $Z_{<l}$, where $R_{>l}$ is a set of input assignments to players [l : L] and $Z_{<l}$ is a set of input assignments to players [\u22121 : l \u2212 1]), such that for every possible inputs $z_{<l} \\in Z_{<l}$, all assignments to $R_{>l}$ are indistinguishable to players [\u22121 : l \u2212 1] on inputs $z_{<l}$ after l epochs (because they lead to the same transcripts).\nFormally, we define:"}, {"title": "Definition 4.2 (Indistinguishable decomposition)", "content": "Let $l \\in [2 : L]$,\nR>l \u2286 AL \u00d7 AL\u22121 \u00d7 \uff65\uff65\uff65 \u00d7 Al\nand\nZ<l = Z\u22121 \u00d7 \u00b7\u00b7\u00b7 \u00d7 Zl\u22121 \u2286 A\u22121 \u00d7 \u30fb\u30fb\u30fb \u00d7 Al\u22121 where Z\u22121 = A\u22121, Z0 \u2286 A0,..., Zl\u22121 \u2286 Al\u22121.\nWe say R>l and Z<l is an indistinguishable decomposition, if for every $\\tilde{z}_{<l} \\in Z_{<l}$, and for every $\\tilde{a}_{\\ge l}, \\beta_{\\ge l} \\in R_{>l}$, it satisfies:\n$\\Pi_{j,i}^{(l')}(\\tilde{z}_{<l}, \\tilde{a}_{\\ge l}) = \\Pi_{j,i}^{(l')}(\\tilde{z}_{<l}, \\beta_{\\ge l})$\nfor every $j \\in [l : L]$, $i \\in [-1 : l \u2212 1]$, and $l' \\in [l]$.\nIndistinguishable configuration is helpful because when l = L, for every input assignment from Z<L to players [\u22121 : L \u2212 1], the player -1 after L epochs (i.e., at the end of the protocol) sees the same transcript when player L receives inputs from R>L. In particular, it means for every $\\tilde{z}_{<L} \\in Z_{<L}$, the answer L-FuncComp($\\tilde{z}_{<L}, z_L$) must be the same for every $z_L \\in R_{>L}$. This is a strong requirement and we will carefully define properties of R>l and Z<l such that this requirement would lead to contradiction, and therefore obtaining our lower bound.\nFor a subset Z<l, we define $\\mathcal{I}_{l-1}$ as\n$\\mathcal{I}_{l-1}(Z_{<l}) := {i_{l-1}: i_{l-1} = i_{l-1}(z_{-1}, z_0,..., z_{l-1}) \\text{ for some } (z_{-1}, z_0,..., z_{l-1}) \\in Z_{<l}}.$"}, {"title": "Lemma 4.3", "content": "That is, $\\mathcal{I}_{l-1}(Z_{<l})$ is the set of all partial composition values for inputs from Z<l.\nThe following lemma shows that the desired lower bound follows from a good enough indistin-guishable configuration for l = L.\nLemma 4.3. Let \u03a0 be an L-epoch Hdp message bits autoregressive communication protocol. If there is an indistinguishable decomposition R>L and Z<L such that:\n1. (Large remaining entropy) $|R_{>L}| > |A_L|/\\Delta_L$.\n2. (Large cover) $|\\mathcal{I}_{L-1}(Z_{<L})| \\ge \\Theta_{L\u22121}$.\nThen \u03a0 does not solve L-FuncComp.\nIn the next subsection, we will show the existence of the required indistinguishable decomposi-tion from Lemma 4.3 via an induction, which finishes the proof of Theorem 4.1."}, {"title": "Lemma 4.4", "content": "Lemma 4.4. For every L-epoch Hdp message bits autoregressive communication protocol \u03a0, there is an indistinguishable decomposition R>L and Z<L satisfying the requirements of Lemma 4.3.\nWe finish this subsection by proving Lemma 4.3.\nProof of Lemma 4.3. First, by the large remain entropy property and our choice of parameters, we have\n$|R_{>L}|\\ge  |A_L|/\\Delta_L =  |A_L|\\cdot2^{-4\\sqrt{K}x_0...x_{L-2}\\cdot n_1...n_{L-1}} > |A_L|\\cdot 2^{-8^{-L}(x_0...x_{L-1})(n...n_{L-1})}\n\\ge |A_L|\\cdot 2^{-L^{-1}}\\Theta_{L-1} \\ge |A_L|\\cdot 2^{-N_{L-1} \\mathcal{I}_{L-1}(Z_1)} \\ge  (N_{L-1})^{N_{L-1}}/|\\mathcal{I}_{L-1}(Z_{<L})|.$"}, {"title": "4.1 Constructing Indistinguishable Decompositions via Induction", "content": "The rest of this section is devoted to prove Lemma 4.4, we will indeed prove it via an induction specified in the Lemma 4.5, whose l = L case is exactly Lemma 4.4.\nLemma 4.5 (Main Lemma). For any $l \\in [2 : L]$,\n\u2022 We have a pair of sets ($R_{>l}, Z_{<l}$), where $R_{>l} \\subseteq A_L \u00d7 A_{L\u22121} \u00d7 \u00b7\u00b7\u00b7 \u00d7 A\u2113, Z<l = Z\u22121 \u00d7 Z0 \u00d7\u2026\u2026\u2026 \u00d7 Zl\u22121, with Z\u22121 = [n1\u2026nL\u22121], Z0 \u2286 A0, Z1 \u2286 A1, ..., Zl\u22121 \u2286 Al\u22121 and they have size |Zo| = xo, |Z1| = x1,..., |Zl\u22121| = xl\u22121;\n\u2022 We can fix the transcript from players [l : L] to [\u22121 : l \u2212 1] at the first l epochs, when the players [\u22121 : l \u2013 1] take input from Z Zl\u22121 \u2208 Zl\u22121,..., Zi \u2208 Zi such that we have the following guarantees:\n\u2022 (Consistency) $(A_{ji}^{(l)})$ is the first l-epoch transcript from players [l : L] to [\u22121 : l \u2013 1], when they take input from R>l and ZAl+1/\u2206l.\n\u2022 (Large cover) The total number of possible il\u22121 under Z\u22121, Z0, Z1, . . ., Zl\u22121 is large, i.e.,"}, {"title": "4.2 The Initial step", "content": "We first prove the correctness of Lemma 4.5 for l = 2.\n4.2.1 Step 1: Choosing Z0, Z1\nWe take Z0 = [x0] and our first step is to select the set Z1 \u2286 [N0]. To this end, consider all possible first epoch messages from the player 1 to the player -1, i.e.,\n$\\Phi_{j,i}^{(1)} = (\\Phi_{j,i}^{(1)}(z_{-1}))_{j \\in \\{0,1\\}^{2Hdp\\cdot(n_1...n_{L-1})}$ where\n$\\Phi_{j,i}^{(1)}(z_{-1}) \\in \\{0,1\\}^{2Hdp}$.\nThe total number of possible $\\Phi_{j,i}^{(1)}$ is $2^{2Hdp\\cdot|Z_{-1}|} = 2^{2Hdp(n_1...n_{L-1})}$, and therefore, there exists one $\\Phi_{j,i}^{(1)} \\in \\{0,1\\}^{2Hdp\\cdot(n_1...n_{L-1})}$, such that\n$S := {z_1 \\in A_1 : \\Pi_{j,i}^{(1)}(z_1, \\tilde{z}_{-1}) = \\Phi_{j,i}^{(1)}(\\tilde{z}_{-1}) \\forall \\tilde{z}_{-1} \\in Z_{-1}} \\subseteq A_1$\nand\n$|S| \\ge |A_1|\u00b72^{-2Hdp (n_1...n_{L-1})}.$\nNote the first epoch message depends only on z1 and z\u22121 so we can write it as $\\Pi_{j,i}^{(1)}(z_1, z_{\u22121})$. The proof of the following Lemma can be found at Section 4.4.\nLemma 4.6. There exists a subset Z1 \u2286 S with size |Z1| = x1, such that it satisfies\n$\\left|{z_1(i_0) : z_1 \\in Z_1, i_0 \\in Z_0}\\right| \\ge 8^{-L}x_0x_1 = \\Theta_1.$\nWe take the subset Z1 in Lemma 4.6 and it remains to fix the transcripts from players $j\\in [2 : L] to players i = \u22121,0,1 at the first two epochs.\n4.2.2 Step 2.1: Fixing the transcript to player -1\nWe first fix the transcript to player -1. For the first epoch, we need to fix $A_{j,-1}^{(2,1)}(z_1, z_0, z_{\u22121})$ for every z1 \u2208 Z1,z0 \u2208 Z0, z\u22121 \u2208 Z\u22121. We note that, the first epoch message from player j to player -1 depends only on z-1 and player j's input, but not on z1, z0, hence it suffices to find some\n$\\Phi_{j,-1}^{(1)} = (\\Phi_{j,-1}^{(1)}(z_{-1}))_{j \\in \\{0,1\\}^{2Hdp}}$ where\n$\\Phi_{j,-1}^{(1)}(z_{-1}) \\in \\{0,1\\}^{2Hdp}.$\nand set\n$A_{j,-1}^{(2,1)}(z_1, z_0, z_{\u22121}) = \\Phi_{j,-1}^{(1)}(z_{0}, z_{\u22121}) \\forall z_1 \\in Z_1,z_0 \\in Z_0, z_{\u22121} \\in Z_{\u22121}$\nThe total number of such transcripts are at most $2^{2Hdp|Z_{-1}|} = 2^{2Hdp\\cdot(n_1..n_{L-1})}$. Hence, we can choose {$\\Phi_{j,-1}^{(1)}$}j\u2208[2:L], such that the set of consistent z1,..., z2,"}, {"title": "4.2.3 Step 2.2: Fixing the transcript to player 0", "content": "We then fix the transcript to player 0. The total number of transcripts $(\\{A_{j,0}^{(2,l')}\\})_{j \\in [2:L],l' \\in [2]}$ of the first two epochs is at most $2^{2Hdp\\cdot x_0x_1}\u00b72^L$. We can fix its value so that the set of consistent ($z_{L}, ..., z_2$)\n$\\mathcal{C}_3 :=   \\Pi_{j,0}^{(1)}(z_L,..., z_0) = A_{j,0}^{(2)}(z_1, \\tilde{z}_0) \\forall z_1 \\in Z_1, \\tilde{z}_0 \\in Z_0, j \\in [2 : L], l' \\in [2]$\nsatisfies\n$|\\mathcal{C}_3|>|\\mathcal{C}_2|\\cdot 2^{-2Hdp\u00b7x_0x_1\u00b72L} > |A_L|\u00b7\u00b7\u00b7 A_2|\u00b72^{-6HdpLx_0(n_1...n_{L\u22121})}.$"}, {"title": "4.2.4 Step 2.3: Fixing the transcript to player 1", "content": "Finally, we fix the transcript to player 1. The total number of transcripts $(\\{A_{j,1}^{(2,l')}\\})_{j \\in [2:L],l' \\in [2]}$ of the first two epochs is at most $2^{2Hdp\\cdot mx_1\u00b72L}$, and we can fix the value so that the set of consistent ($z_{L}, ..., z_2$)\nsatisfies\n$|\\mathcal{C}_4|> |A_L|\u00b7\u00b7\u00b7|A_2|\u00b72^{-4\u221aKx_0(n_1n_{L-1})} = |A_L|\u00b7\u00b7\u00b7 |A_2|/\\Delta_2 .$\nHere the second step follows from the choice of parameters (see Eq. (4)(7)) and the last step follows from the definition of \u03942 (see Eq. (8)).\nCombining Lemma 4.6 and Eq. (13), we conclude the proof for the case l = 2."}, {"title": "4.3 Inductive step", "content": "Suppose Lemma 4.5 holds up"}]}