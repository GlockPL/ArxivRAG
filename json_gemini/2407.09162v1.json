{"title": "Exploring State Space and Reasoning by Elimination in Tsetlin Machine", "authors": ["Ahmed K. Kadhim", "Ole-Christoffer Granmo", "Lei Jiao", "Rishad Shafik"], "abstract": "The Tsetlin Machine (TM) has gained significant attention in Machine Learning (ML). By employing logical fundamentals, it facilitates pattern learning and representation, offering an alternative approach for developing comprehensible Artificial Intelligence (AI) with a specific focus on pattern classification in the form of conjunctive clauses. In the domain of Natural Language Processing (NLP), TM is utilised to construct word embedding and describe target words using clauses. To enhance the descriptive capacity of these clauses, we study the concept of Reasoning by Elimination (RbE) in clauses' formulation, which involves incorporating feature negations to provide a more comprehensive representation. In more detail, this paper employs the Tsetlin Machine Auto-Encoder (TM-AE) architecture to generate dense word vectors, aiming at capturing contextual information by extracting feature-dense vectors for a given vocabulary. Thereafter, the principle of RbE is explored to improve descriptivity and optimise the performance of the TM. Specifically, the specificity parameter s and the voting margin parameter T are leveraged to regulate feature distribution in the state space, resulting in a dense representation of information for each clause. In addition, we investigate the state spaces of TM-AE, especially for the forgotten/excluded features. Empirical investigations on artificially generated data, the IMDB dataset, and the 20 Newsgroups dataset showcase the robustness of the TM, with accuracy reaching 90.62% for the IMDB.", "sections": [{"title": "I. INTRODUCTION", "content": "In natural language processing applications, TM extracts context information for an input by identifying its active features and performing an AND operation to generate conjunctive clauses. The process of building these clauses involves memorising or forgetting features within the clause features' space in memory [1].\nIn a recent study, the TM-based auto-encoder for self-supervised embedding has been implemented with human-interpretable contextual representations from a given dataset [2]. The model demonstrated promising performance comparable to Word2Vec [3], FastText [4], and GLoVe [5] across multiple datasets [6], [7]. The main focus of [2] is on word embedding for a group of input words, rather than obtaining context information for an individual word.\nIt has been observed in [8] that literals with negated features are common in a clause, which coincides with the concept of reasoning by elimination. In addition, using negated reasoning in the TM, the work in [9] proposed the augmentation of clause robustness by decreasing the specificity factor s, which has displayed promising improvements in the accuracy of downstream classification tasks. While empirical observations support this approach, there remains a dearth of explanations regarding the precise impact of the hyperparameter modifica- tion on the TM's structure. Although the transparent nature of the TM model renders it mathematically comprehensible, further investigation is warranted to ascertain why s was selected and whether alternatives exist, which can possibly provide superior results. This necessitates obtaining clauses that possess specific conditions, enabling the description of their constituent features and the ramifications of hyperparam- eter adjustments.\nIn this paper, we introduce a novel approach for obtaining a dense vector capturing the contextual information of an indi- vidual word, employing the TM-AE structure. Thereafter, we investigate the hyperparameters and demonstrate the influence of them on the distribution of the state space and RbE. In light of the above issues, this study has the contributions below:\n1) Employing TM-AE to obtain a dense vector that captures the contextual information of an individual word.\n2) Investigating the hyperparameters to explore the state space of literals and RbE.\n3) Enhancing classification performance through the utili- sation of insights gained from the previous step.\nThe remainder of the paper is organised as follows. Sec- tion II presents the related work, while Section III summarises the operational concept of TM-AE in this application. Sec- tion IV provides a comprehensive description of the approach. In Section V, the experiments are conducted for performance evaluation before we conclude the work in the last section."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "In the last two decades, with the profusion of computer resources, researchers were able to add more complexity to AI algorithms, producing Deep Learning (DL) in different domains [10]\u2013[13]. For NLP, the latest development in word embedding has made advanced strides [14]\u2013[17]. Firstly, a statistical NLP replaced the symbolic NLP to overcome the scalability issue [18], [19]. These NLP models use statistical inference to learn the rules automatically [20]. Later, using the Neural Network (NN) approach, Word2vec [3] and GloVe [5] were introduced to represent words as vectors in order to capture context and semantic similarity.\nRecently, a promising ML approach called the TM [1] has gained the researcher's attention. TM has shown promising performance in terms of accuracy and memory utilisation [21], [22]. TM paradigm requires less complexity, memory, and power footprint due to the linear combination of conjunctive clauses organised in lean parallel processing units. TM has been utilised in NLP [8], [9], where RbE was often observed in TM reasoning, indicated by the majority of negated features in clauses. For example, in [9], the specificity parameter, s, is tuned to enhance negated reasoning during training, resulting in the generation of clauses that are robust to spurious correlations. RbE is not only applicable to NLP but is also a fundamental cognitive process observed in human develop- ment. Infants demonstrate this ability through tasks that require them to reject certain options to deduce the correct one. For instance, in studies involving referent disambiguation, infants eliminate familiar objects when mapping novel labels to novel objects, effectively using a form of RbE before they can even articulate logical components like \"or\" and \"not\" [23], [24]. This parallel between AI and human cognition underscores the relevance and applicability of RbE across different domains.\nFor TM embedding, it was developed in [2] a Tsetlin Machine-based autoencoder that enables self-supervised learn- ing of logical clauses, which outperformed its DL counterparts, including GloVe [5], achieving a high rating. The TM-AE's concept revolves around coalescing knowledge obtained from models supporting or not supporting the input tokens in a se- quential manner over multiple stages until conjunctive clauses representing the resulting coalescence are generated. However, during this process, the model aims to generate clauses that provide a general description of the input sequence, making it challenging to extract the embedding for each token separately.\nThis paper proposes a novel approach utilising the TM-AE model to generate context information separately for each token in the vocabulary. This approach is similar to the DL counterpart, Word2Vec, and specifically utilises the Skip-gram model, where the input is a token, and the output is a set of clauses describing the context in a dense vector. This approach enables the description of the space of states and the distribution of features concerning the input token through the generated clauses. Additionally, to improve training, we introduced the possibility of performing negated reasoning, named RbE, utilising the margin parameter T along with the specificity parameter\u00b9 s. The results indicate that RbE provides a greater extent of control in enhancing model performance."}, {"title": "III. TSETLIN MACHINE AUTOENCODER BASICS", "content": "Here we review the basics of the TM-AE [2], which was constructed based on the Coalesced Tsetlin Machine (CoTM) algorithm. Its main objective was to generate embeddings for a sequence of input tokens $(X_1, X_2, ..., X_k...., X_K)$, where $k \\in {1, ..., K}$ is the index of tokens and K represents the total number of desired selective tokens from the vocabulary V to be embedded. Within the CoTM algorithm, the resulting embedding for each token $x_k$, is expressed using a collection of conjunctive clauses denoted as $C_{k,j}$, where j varies from 1 to the user-defined number of clauses, n (refer to Figure 1).\nStarting from this section, we will refer to the target input token, for which we aim to find the corresponding embedding, as the Target Word (TW). On the other hand, the tokens in the vocabulary that actively participate in the training process and contribute to the formation of clauses are referred to as features. The input to the TM-AE model comprises a sparse matrix that represents the documents in the dataset as rows and the features in the vocabulary V as columns. The model captures the vectorised representation of both the vocabulary V and the documents G. During each epoch and with each iteration of examples, features in a document are selected by determining whether the document contains the TW or not, based on the target value being either 1 or 0 respectively. The chosen documents corresponding to a specific TW are combined with other documents for different TWs using random shuffling during each example in an epoch.\nThe selected documents corresponding to each TW are com- bined, and their features are encoded into a one-dimensional binary vector X, thus achieving computational efficiency and resource optimisation during training (see Figure 2). The encoding of X is such that each feature in the vocabulary V and its negation is represented as a literal, which in turn contributes to the update process for building the clauses.\nThe binary vector X encodes the binary representation of the presence or absence of each feature in the vocabulary, with respect to the selected documents, for every TW. Furthermore, it encodes the negation of each feature along with its presence or absence. This mechanism enables an efficient encoding and processing of the information necessary for constructing the conjunctive clauses within the TM-AE architecture.\nThe TM-AE algorithm has the following steps:\n1) Data preparation and vectorisation, which involves de- termining the size of the vocabulary used in training.\n2) Target output identification.\n3) Scrambling the inputs to generate a unique and efficient embedding series.\n4) Collection of features that support the specified TW if the output is 1, or do not support it if the output is 0.\n5) TM update stage.\nThe process of voting and building a clause within the TM algorithm can be analogised to a logical election. Features within the vocabulary engage in voting processes to favour a particular class within a formed clause. Features can vote for a TW through multiple clauses, with a predetermined number of clauses to be constructed. To coordinate the learning of clauses for correct output, the TM introduces a voting margin T. Hyperparameter T specifies the number of votes required for the winning output.\nThe TM uses three update operations to learn and adapt to new input TW: memorisation, forgetting, and invalidation. Memorization strengthens patterns by increasing the state of literals that match the input TW and decreasing the states of missing literals randomly. Forgetfulness, on the other hand, weakens patterns by decreasing the state of true literals somewhat randomly. The hyperparameter s, which is a user- set parameter, influences the rate at which truth values are forgotten. Increasing s makes the patterns finer, while decreas- ing it results in coarser patterns. The invalidation operation increases the states of all false truth literals that the clause ultimately rejects. More details on TM memory operations can be found in reference [25].\nThe transparency of the TM-AE algorithm prompted an exploration into understanding the rationale behind clause formation for a series of input TWs in the final embedding. However, the model's scalability posed a challenge, resulting in the final outcome potentially containing fused or diluted contextual information, thereby potentially compromising its explainability. To address this, context information was accu- mulated for each individual TW as feature-equipped clauses. This approach involved considering the weight of each clause generated for the TW. In the next section, more details on our approach will be provided."}, {"title": "IV. PROPOSED APPROACH", "content": "In this section, we will first present the approach of ex- tracting contextual information from individual inputs and then detail the concept of reasoning by elimination, which is explored alongside the state space.\nContext Information Extraction\nFor TM-AE, context information can be collected for a single TW during the training phase. For this purpose, let\n$V = \\{feature_1, feature_2, ..., feature_m\\}.$  (1)\nAlgorithm 1 outlines the steps for embedding the TW from the vocabulary V that contains m features. Firstly, the embedding algorithm applies r rounds to process different combinations of documents. In each round, a subset D of documents that contain the TW is selected from the complete set of documents G, assigning an output value of q = 1. Alternatively, if the desired output value is 0 in the round, a different subset of documents D that do not contain the TW is selected from G. The TM-AE then selects a specified proportion of documents based on a window size u. The features in the selected documents are combined to facilitate the training for the TW, and a boolean one-dimension vector X is generated. Meanwhile, while building X, the negated features are also included by inverting the status of the features, which expands the size of the X vector to 2 \u00d7 m. Consequently, the vocabulary size in the TM-AE model is effectively doubled since it now includes both the features themselves and their negations, referred to as literals L, as Eq. (2).\n$L = \\{literal_1, literal_2, ..., literal_{2m}\\}.$  (2)\nThe vector space representation for TW generated by the model consists of clauses with positive and negative weights as explained in Eq. (3). Here, $C_j^p(TW)$ denotes the $j^{th}$ clause with polarity p for the particular TW. Polarity p is either + or -. Clearly, $C_j^p(TW)$ is a logical conjunction of literals $l_k$. L is the set of literals $l_k$, which contains the included/memorized literals in clause $C_j^p(TW)$, and is a subset of L. The length of L varies for each clause $C_j^p(TW)$, making it suitable to represent a wide range of textual features in TW.\n$C_j^p(TW) = \\bigwedge_{l_k \\in L} l_k.$ (3)\nFor example, the clause $C_1^+(TW)$ = \u00abliteral\u2081 ^ literal\u2082 belongs to TW, has index 1, polarity +, and consists of the literals $L^+$ = \u00abliteral1,literal2}. Accordingly, the clause outputs 1 if literal\u2081 = 0 and literal\u2082 = 1, and 0 otherwise.\nThis process enables the creation of an informative vector space description for the TW, thereby facilitating subsequent used in diverse NLP applications. In the knowledge graph, positive clauses for the target word \u201cqueen\u201d include \u201ccastle\u201d, \"throne\", \"lady\u201d, \u201cbirthday\", \"princess\", \"diamond\u201d, \u201cmedal\", \"pop\", \"beauty\u201d, \u201cPhilip\u201d, \u201cwest\u201d, \u201cking's AND speech\", \u201cpark\u201d, and \u201cceremony\u201d. Despite the TM's output being gen- erated in the form of clauses, this approach may result in a limited representation of the relationships between the TW and other features within the vocabulary. In contrast, models such as Word2Vec have demonstrated the ability to capture context information in the form of dense vector representations for words, effectively describing their interactions within a high-dimensional space. In the subsequent section, we will delve into a methodology that enables the capture of context information for all other features in the vocabulary, thereby facilitating a more comprehensive understanding of their rela- tionships with the TW.\nTo demonstrate the efficacy of the TM-AE structure in capturing context information, we can consider the example of clauses captured by training the model for the TW \u201cqueen\u201d which we mention in the previous subsection. Each clause has a state space consisting of literals arranged in a distribution consistent with the context information captured during train- ing (see Figure 3).\nIn the TM, each state corresponds to a position that con- tributes to constructing a clause. The first N states (ranging from 0 to N) are the forgotten states, in which the literals associated with these states do not actively participate in form- ing the final output of the TM clause. In contrast, the region spanning from N to 2N includes states where the literals are considered and contribute to the output as part of the clause (see blue area in Figure 3). The distribution of literals in the state space is controlled by several hyperparameters, including s, T, epochs, number of states, and window size. This behaviour determines the learning process and the em- bedding of contextual information in clauses. This mechanism allows the TM to dynamically adapt and prioritise relevant literals by reassigning their depth in memory and reinforcing their influence based on the voting mechanism. Consequently, important literals involved in the formation of clauses are placed above the N state and play a more prominent role. In contrast, less contributory literals are gradually pushed into the forgotten states. This dynamic allocation of states and the selective concentration of literals contribute to the TW's ability to process and distil contextually significant information from large-scale datasets effectively.\nThe negations of features are affected in the same way but with a lower amount of pushing down to occupy states closer to involvement in clause formation. Through this approach, the model can increase the discrimination RbE, and the model now is describing the TW not only by what it looks like but also by what it does not look like. As exemplified by the Sherlock Holmes quote: When you have eliminated all which is impossible, then whatever remains, however improbable, must be the truth. This RbE enables the TM to generate AND-rules that embody the most critical words of a class that the TW does not belong to, such as \u201cNOT fruit AND NOT basketball AND NOT ...,\u201d allowing for quick and robust learning without the need for randomising TM feedback, which becomes deterministic.\""}, {"title": "V. EMPIRICAL RESULTS", "content": "Empirical investigations were conducted utilising the TM-AE with a single-clause embedding approach to analyse the distribution of literals within the state space. The state space size, denoted by 2N, is determined by the number of states. Precisely, 2N is calculated as $2^{states\\_bits}$, where \u201cstates bits\" is a user-set hyperparameter. To this end, the One Billion Word dataset [26] comprising 30M samples and a vocabulary size of 40k words was employed. Other hyperparameter values were N = 2048 and a window size u of 25. Contextual information was then extracted while manipulating various hyperparameters to observe their effects.\nOne area of investigation involved examining the influence of increasing the number of epochs on the training process, with values tested at 5, 100, and 200 epochs and other hyperparameter values were T = 3200, s = 5.0, as shown in Figure 4. The results demonstrated that as the number of iterations increased, the features progressively faded into oblivion, leading to the construction of concise clauses that effectively described the target class. This phenomenon is illustrated in the accompanying figure, where deeper levels of forgetting could be observed with higher repetition rates. During initialisation, all literals are situated at the inter- mediate state in memory at N. Subsequently, updates are undertaken through three distinct mechanisms (as described in Section III), with the distribution of literals determined by the influence of hyperparameters on the probability and speed of forgetting. In all figures, the dashed line labelled as N appears at the top of the graphs, likely due to the fewer number of literals that exceed the N threshold.\nSeveral other experiments were conducted to investigate the impact of varying the T and s hyperparameters on feature distribution in the state space with fixed 25 epochs. The results indicated that increasing the value of s = (3, 10, 20, 40 and 100) reduced the depth of forgetting, thereby yielding more informed descriptions of the target class (as illustrated in Figure 5). While, as T is increased (2, 20, 75, 200 and 5000) (Figure 6), the depth of forgetting increases, enabling a more discriminative representation of the TW.\nVarious experiments were conducted to explore the impact of feature negation, which entailed doubling the vocabulary size to form the X vector. These experiments allowed for the observation of RbE, particularly by comparing the state of literals before and after index 40000, which is the borderline between literals for the negation and the original form (Indices great than 40000 represent negated ones, refer to Figure 2 for x-axis). Generally, feature negation exhibited a slower speed of forgetting, resulting in their proximity to memorisation states and their potential contribution to clause formation. The influence of increasing the number of epochs on the distri- bution of literals is depicted in Figure 7. Meanwhile, Figure 8 illustrates the effect of the hyperparameter s (specifically, values of 1, 5, 20, and 100) on the deepening of forgetfulness. A lower value of s signifies a greater tendency for features to be pushed to a deeper state and a reduced likelihood of their involvement in clause formation. Furthermore, Figure 9 displays the experimental outcomes obtained by altering the hyperparameter T (with values of 2, 20, 200, 3200, and 5000). Notably, T exhibits an opposing effect to s, a larger margin leads to increased isolation of features from participation and consequently allows for a greater involvement of features negation. This observation underscores the critical role played by hyperparameters in shaping the embedding output of the TM, highlighting the need to carefully select and fine-tune these hyperparameters to optimise the model's performance.\nTable I shows the experiment conducted to calculate the classification accuracy of the TM on three different datasets: an artificial dataset, the IMDB dataset, and the 20 Newsgroups dataset. The artificial dataset was randomly generated with specific parameters, including the number of features, training examples, test examples, noise level, and unique characterising features per class. The IMDB dataset consists of 50k users' reviews for movies, while the 20 Newsgroups dataset contains news articles from various categories.\nThe classification accuracy was evaluated for different val- ues of the parameter s (1, 2, 3, 5, 10, 20, and 50) to observe the performance variation. The results showed that for the artificial dataset, the TM achieved the highest accuracy of 60.88% when s was set to 1. However, as s increased, the accuracy slightly decreased. For the IMDB dataset, the highest accuracy achieved was 90.62% when s was set to 1, with a gradual decrease in accuracy as s increased. Similarly, for the 20 Newsgroups dataset, the highest accuracy was 80.78% at s equal to 1, with a decline in accuracy as s increased.\nIn addition to investigating the impact of varying the pa- rameter s, we also examined the classification accuracy of the TM on the same datasets for different values of the parameter T. The results are summarised in Table II. For the artificial dataset, we observed that as T increased, the accuracy initially improved until T = 128, where the TM achieved the highest accuracy of 58.74%. Similarly, for the IMDB dataset, the highest accuracy of 77.56% was achieved at T = 256. For the 20 Newsgroups dataset, the best performance was 52.71% and observed at T = 128, surpassing the other T values. The empirical results demonstrate that reinforcing RbE with s to lower values (specifically 1) and T to higher values can improve the accuracy of the TM. Table III shows an experiment aimed at assessing the effect of RbE for four configurations of s and T. We extracted the classification accuracy of the TM and counted the number of original and negated features present in 25 epochs of 50 positive weighted clauses. By encouraging RbE, the TM can push negated features to participate in forming the clauses. This process improves the selection of relevant features, which ultimately leads to better prediction outcomes."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we employ the TM-AE architecture to gen- erate contextual information for individual words, facilitating the extraction of valuable insights. Building on this foundation, we investigate the RbE configuration and the construction of the state space. We demonstrate the importance of efficiently distributing literals within the TM's clauses to enhance repre- sentation quality and capture essential contextual information. The empirical results show that integrating RbE with lower values for s and higher values for T can enhance the accuracy of the TM. By utilising RbE, the TM can direct the negation of features towards greater chances of taking part in forming the clauses and occupying the memorisation states. This procedure enhances the selection of relevant features, ultimately leading to improved predictive results."}]}