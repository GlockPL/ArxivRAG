{"title": "Medical Manifestation-Aware De-Identification", "authors": ["Yuan Tian", "Shuo Wang", "Guangtao Zhai"], "abstract": "Face de-identification (DeID) has been widely studied for common scenes, but remains under-researched for medical scenes, mostly due to the lack of large-scale patient face datasets. In this paper, we release MeMa, consisting of over 40,000 photo-realistic patient faces. MeMa is re-generated from massive real patient photos. By carefully modulating the generation and data-filtering procedures, MeMa avoids breaching real patient privacy, while ensuring rich and plausible medical manifestations. We recruit expert clinicians to annotate MeMa with both coarse- and fine-grained labels, building the first medical-scene DeID benchmark. Additionally, we propose a baseline approach for this new medical-aware DeID task, by integrating data-driven medical semantic priors into the DeID procedure. Despite its conciseness and simplicity, our approach substantially outperforms previous ones.", "sections": [{"title": "Introduction", "content": "The public sharing of large-scale image datasets has facilitated the rapid progress in Artificial Intelligence (AI). However, this also poses great privacy concerns, especially for facial images, which are widely used for identity authentication. To address this issue, many de-identification (DeID) algorithms have been continuously proposed for protecting the facial identity, achieving promising results on common-scene facial datasets. However, rare researches are conducted for the medical scenes, although patient privacy leakage is a big concern in the medical AI era. Research on medical-aware DeID (Med-DeID) mainly faces two obstacles. First, there are few medical-scene facial datasets available, due to the difficulty in accessing patients compared to healthy individuals. Moreover, it is often not acceptable to package real patient faces as datasets and make them publicly downloadable. Second, the current DeID approaches may not be appropriate for protecting medical facial images, due to not particularly preserving the disease manifestations of the origin image. This leads to the lost of diagnosis-necessary disease signs, deteriorating the medical utilities.\nIn this paper, we release a Medical Manifestation-rich patient face dataset, termed MeMa, containing over 40,000 photo-realistic virtual patient images. To construct MeMa, we obtained permission from the hospital's medical ethics committee to photograph patients. Then, these patient photos are annotated by expert physicians, before being used to train a specialized generative model. By carefully modulating the sampling procedure of the generative model and filtering the generated data, we created a diverse, high-quality, and real-world-like patient face dataset.\nFurthermore, we propose a baseline medical semantics-preserved DeID approach, termed MedSem-DeID, to eliminate the patient identity from the facial image, at the premise of preserving the medical utility. Concretely, we first condense the rich medical priors within the MeMa into a medical semantics encoder, and then adopt it to (1) enhance the medical knowledge of the features within the DeID pipeline, and (2) minimize the medical-aware distortion of the de-identified images. Despite its simplicity, our approach easily outperforms previous DeID approaches for medical scenes, thanks to the rich medical manifestation knowledge embodied in the MeMa dataset. Our main contributions are:\n\u2022 We release, to the best of our knowledge, the first large-scale patient face dataset of rich medical manifestations,"}, {"title": "Related Work", "content": "Facial Datasets. Amounts of large-scale face datasets have been proposed, but they primarily feature healthy individuals, limiting their use for medical-scene DeID. In contrast, we introduce a large-scale patient face dataset with rich medical manifestations. Our dataset includes annotations for disease type and lesion masks, facilitating face DeID field in medical scenes.\nFace De-identification. Early De-ID methods used the K-same algorithm. Recent approaches leverage generative models to remove facial identity, while often compromising utility. More recent methods aim to preserve more facial attributes and better serve common utilities such as gaze detection and image/video recognition, but not specifically medical signs. In contrast, our approach leverages medical manifestation representations learned from real patient photos, preserving medical attributes during DeID. Additionally, it is reversible, similar to, enabling reversal for medical audits.\nSemantic Representation. Effectively modeling semantic information is crucial for modifying facial images, while maintaining perceptual quality and preserving medical utility. Previous approaches have leveraged contrastive learning and masked image modeling for self-supervised learning of image semantics. Recent studies have shown that pre-trained visual foundation models, such as stable diffusion , exhibit even stronger semantic representations . In this work, we present the first adaptation of diffusion model-extracted semantics to the medical DeID problem.\nMedical-scene Face Privacy Protection. Progress on this problem has been slow, often relying on simple methods like blurring or replacing faces with 3D masks , which discard critical disease signs. The progress gap is attributed to the lack of large-scale medical-scene facial datasets. Our work aims to address this gap."}, {"title": "Approach", "content": "We first build a new patient face dataset, termed MeMa. It addresses the lack of medical-scene facial datasets. MeMa is synthesized from real patient photos. Its synthetic nature avoids potential ethical problems. Expert physicians recognize its validity. Further, we propose a baseline model for the medical-aware facial DeID (Med-DeID) problem."}, {"title": "MeMa Dataset", "content": "The overview of MeMa is shown , which consists of 42,307 synthetic patient face images. MeMa closely mimics real patients in both visual appearance and statistics. Patient age and gender are estimated using the DeepFace framework. We describe the main steps for assembling MeMa as follows.\nDisease Categories: We take the eye clinic as an exemplar scene, since most eye diseases show typical external facial manifestations. In our study, we included patients with seven eye diseases. These are Basal Cell Carcinoma (BCC), Conjunctivitis (Conj), Uveitis, Ptosis, Squamous Cell Carcinoma (SCC), Strabismus (Strab), and Thyroid Associated Ophthalmopathy (TAO). We also included clinically Normal cases. The detailed manifestations of the above diseases can be found in the MSD medical manual.\nReal Patient Data Collection: We collected 39,323 photos of 12,467 real patients. They attended the Eye Clinic at Shanghai Ninth People's Hospital(SNPH) between January 2020 and June 2023. The photo-taking procedure was approved by the hospital's ethics committee. The patients' diagnosis results were collected from their medical records.\nGenerating MeMa from Real Data: As shown , we first train a medical-aware generative model with the collected patient data. Then, we sample the virtual patients from the model by using proper conditions, aiming to generate safe and diverse samples. Finally, we recruit expert physicians to filter the images of bad medical quality, then annotate the filtered images. The steps are detailed as follows.\nStep1: Medical-aware Generative Model Training: We first translate the disease type into the prompt caption 'A face, eye with {disease name}'. With the paired data of the real patient photographs and the disease type caption, we fine-tune the diffusion model , producing the patient face generation model. As compared , after fine-tuning the SD model on our real patient dataset, the generated image shows typical medical signs and manifestations, while the vanilla SD model can not effectively generate images with reasonable medical manifestations, due to its limited medical knowledge.\nStep2: Rich-Condition Patient Face Synthesis: Directly sampling from the real-patient generation model with the simple prompt 'A face, eye with {disease name}' is not enough, which shows two problems. First, identity leakage: the identity of most sampled patients can be found in the training dataset, potentially leaking the privacy of real patients. Second, mode collapse: the samples tend to be less diverse, with collapsed medical manifestation modes.\nTo address the identity leakage problem, we propose injecting facial attributes from public faces into the generation process. Specifically, we randomly sample face images from the FFHQ dataset and use the IP-Adapter to inject these attributes. As shown in Tab. 1, this substantially reduces the average identity leakage percentage from 71.8% to 1.27%, when being evaluated with multiple face recognition models, i.e., SphereFace, ArcFace, and CosFace.\nTo mitigate the mode collapse problem, we first randomly sample the public face injection weight from the range [0.2, 0.4], instead of using a fixed weight. This leads to better feature fusion flexibility and improves output diversity. Second, we enhance the text prompt with severity descriptions, e.g., \u2018A face, eye with {disease name}, {slight/mid/heavy}-level'. This further improves diversity, even though no disease severity is annotated in the collected patient captions. The reason may be that the base SD model has learned a large dictionary of word semantics and can automatically connect the common 'severity' description words to the image generation process. As compared , with rich conditions injected, both the quality and diversity of the generated images are substantially improved.\nTo ensure the generated dataset's statistical characteristics match those of real patients, we calculate the distributions of real patient disease types, ages, and genders. We control the generated images to follow the above distributions. To control the disease type, we simply modify the disease name of the prompt. To control the age and gender of the generated images, we label FFHQ images using an age and gender estimation model , then select images based on this metadata for attribute injection. As shown in Tab. 2, the real distribution-guided sampling strategy produces a dataset with similar statistical characteristics to real patients.\nStep3: Filtering and Annotation: After generating the images, we remove those with small identity feature distance to the original real patient set, ensuring the privacy of the real patients will not be leaked. Then, the physicians filter out the images with low medical utility quality. Finally, these physicians label the per-image disease information of the filtered dataset. Moreover, considering that lesion segmentation is another representative medical imaging task. We also ask the physicians to segment the tumor mask of the subset"}, {"title": "A Baseline Approach for Med-DeID", "content": "We propose a baseline approach to incorporate the rich medical manifestation knowledge within MeMa into the DeID procedure, which consists of two sub-modules: medical semantics encoding and medical semantics-preserved DeID.\nMedical Semantics Encoding. The Med-DeID task requires preserving as much medical information as possible while obfuscating other identifying details. This necessitates a semantic encoder that recognizes local medical semantics. Motivated by the strength of diffusion models in extracting fine-grained local semantics, we train another diffusion model on the proposed MeMa dataset to learn the medical semantics. We adopt its first several blocks as the medical encoder Encmed, instead of the whole network, for reducing the computational cost.\nIt should be mentioned that the roles of the diffusion models in the previous section and here are fundamentally different: the previous one is for high-quality image generation, whereas the one here is for extracting rich medical semantics. Our approach is very flexible, and the semantic encoder can be other choices, as analyzed in the experiment section.\nMedical Semantics-Preserved DeID (MedSem-DeID). As illustrated , our approach leverages the medical encoder Encsem to inject medical knowledge into the feature extraction procedure, as well as regularize the medical utility of the de-identified image.\nGiven the original image X, where H and W denote its height and width, an image encoder transforms X into the facial feature $f_{face} \\in \\mathbb{R}^{512 \\times \\frac{H}{32} \\times \\frac{W}{32}}$. Meanwhile, we use Encmed to extract the medical feature $f_{med} \\in \\mathbb{R}^{320 \\times \\frac{H}{8} \\times \\frac{W}{8}}$. The $f_{med}$ is downscaled and concatenated with $f_{face}$, passing through three consecutive residual blocks, producing f. Then, we employ a group of Transformer blocks, termed ID-Encryptor, to encrypt the ID information within f. Specifically, we flatten the spatial dimension of f, concatenate it with the password vector $P\\in \\mathbb{R}^{512}$, and feed the concatenated vector into ID-Encryptor, producing the encrypted feature $f_{enc}$. $f_{enc}$ is passed through an image decoder network to result in the encrypted image $X_{enc}$. Please refer to the supplementary material for the network architecture details.\nIn medical contexts, it is often necessary to rigorously recheck results with expert physicians on the original image. Moreover, the Good Clinical Practice (GCP) guideline mandates that all medical materials involved in the diagnosis process must be traceable. Therefore, we design our method to be reversible, enabling the recovery of the original image from the encrypted features. Given the original password P, $f_{enc}$ can be decrypted back to f, by another group of Transformer blocks termed ID-Decryptor. Then, f is reconstructed as the original image X by the image decoder. When an incorrect password is used, $f_{enc}$ is reconstructed into a wrong image $X_{wrong}$.\nLearning Objectives. The learning objective of the proposed MedSem-DeID is formulated as follows, $\\mathcal{L} = \\mathcal{L}_{deid} + \\lambda_{rev-id}\\mathcal{L}_{rev-id} + \\lambda_{wrong} \\mathcal{L}_{wrong} + \\lambda_{med} \\mathcal{L}_{med} + \\lambda_{rev}\\mathcal{L}_{rev} + \\mathcal{L}_{GAN}$. $\\mathcal{L}_{deid} = cos(\\Phi(X), \\Phi(X_{enc}))$ enforces the identity of the encrypted image apart from the original image, where $\\Phi$ denotes the pre-trained identity recognition network ArcFace, cos denotes the cosine similarity. $\\mathcal{L}_{rev-id} = -cos(\\Phi(X), \\Phi(\\hat{X}))$ enforces the identity of reversibly decrypted image is the same as the original image. $\\mathcal{L}_{wrong} = cos(\\Phi(X), \\Phi(X_{wrong}))$ enforces the identity of the image decrypted by the wrong password far away from the original image. $\\mathcal{L}_{med} = l_2(f_{med}, Enc_{med}(X_{enc}))$ facilitate the encrypted image is similar to the original image in terms of medical semantics. $\\mathcal{L}_{rev} = l_1(X, \\hat{X})$ regularizes the appearance of the recovered image by right password is similar to the original one. $l_1$ and $l_2$ denote the mean absolute error (MSE) and the mean squared error (MSE) functions, respectively. The $\\mathcal{L}_{GAN}$ is the adversarial generative network (GAN) loss, enforcing the photo-realism of all images. $\\lambda_{med}$ and $\\lambda_{rev}$ denote the balancing weights."}, {"title": "Experiments", "content": "Datasets. MeMa: the proposed MeMa dataset consists of 42,307 images in total, which is split into a training set (34,000 images), a hyper-parameter selection set (3,729 images), and a validation set (4,578 images). All images are labeled with the disease category. MeMa-Seg: for the BCC (basal cell carcinoma) disease type, we randomly select 600 images from the training set and 150 images from the validation set of MeMa, annotating the tumor masks for these images. This results in the MeMa-Seg dataset, which can be used to evaluate the fine-grained medical performance of different DeID approaches. Real-ECXHCSU: we also collaborate with Eye Center of Xiangya Hospital of Central South University (ECXHCSU), enrolling 129 patients to conduct a real-world clinical trial. This aims to validate whether our algorithm, trained on the synthetic MeMa dataset, remains effective for real-world patients. Moreover, ECXHCSU is geographically distant from SNPH used to develop the MeMa dataset. This aims to further emphasize the generalization capability of our approach.\nImplementation Details. For training the patient face gen-"}, {"title": "Evaluation Protocol and Metrics", "content": "Medical utility: for the disease classification task, we fine-tune the DiNov2 model on the MeMa training set. We evaluate its Top1 accuracy on the MeMa validation set processed by various DeID approaches. For the tumor segmentation task, we use the nnU-Net to evaluate different methods on MeMa-Seg, adopting the Dice score and Jaccard index as metrics. Real-word clinical utility: we recruit three physicians to manually diagnose the images in Real-ECXHCSU, that are de-identified by various DeID approaches. Each image is diagnosed by all three physicians, and the final diagnosis is determined by a majority voting strategy. We use Cohen's Kappa (k) to measure the diagnosis consistency between the original and the de-identified images. k is a common metric for evaluating clinical trial outcomes in the medical field. Identity protection: following recent works, we use Euclidean distance between the identity features of de-identified and original faces, denoted as 'ID-Dis', to quantitatively evaluate the effectiveness of identity protection. Identity features are extracted by FaceNet trained on CASIA, FaceNet trained on VGGFace2, and SphereFace, which are not used in the training procedure. Other utilities: following previous methods, we adopt the Dlib and L2CS-Net to evaluate the landmark detection and gaze estimation performances."}, {"title": "Reversibility", "content": "we compare our method against the previous reversible methods, in terms of ID similarity, medical results, and visual quality of the reconstructed original image."}, {"title": "Results", "content": "Medical Utility. As shown in Tab. 3, Our method achieves the best overall classification accuracy, outperforming the second-best method, Disguise, by more than 10%. For SCC disease, our approach outperforms DeepPrivacy, CIAGAN, Disguise, Password, Personal, and RiDDLE by 86.10%, 73.53%, 27.76%, 34.67%, 82.86%, and 87.05%, respectively. On the more challenging tumor segmentation task, our approach also performs best, achieving the highest Dice (0.6775) and Jaccard (0.5453) scores.\nMoreover, we train Password and Disguise models on our MeMa dataset, improving their classification accuracy to 54.67% and 77.12%, respectively, but still lagging behind our 86.70%. This indicates that MeMa can enhance the efficacy of various DeID methods in medical contexts, and its full potential can be realized through specialized medical-scene methods like our MedSem-DeID.\nIn Tab. 3 (3rd column), we summarize the priors employed by different methods. Landmark priors and high-level common priors (face attributes/StyleGAN adopted by Personal/RiDDLE) perform poorly in medical applications, i.e., less than 50% accuracy and 0.2 segmentation Dice score. The gaze prior (Disguise) is effective for coarse-grained classification (76.01%) but fails in fine-grained segmentation task (0.2751). Password, using a U-Net to preserve high-frequency signals, excels in low-level segmentation (0.6336) but fails in high-level classification task (54.21%). This also introduces visual artifacts . In contrast, our method, leveraging the medical semantics within MeMa, achieves superior performance in both classification (86.70%) and segmentation (0.6775) without handcrafted designs such as landmark.\nReal-World Clinical Utility. We conduct a clinical trial on the Real-ECXHCSU cohort. As shown in Tab. 4, our method largely outperforms the recent DeID methods (Disguise and Password) across all five disease categories, achieving near-perfect consistency in the clinical outcomes, i.e., k>0.81. Moreover, our approach is more flexible and effective than a recent hand-crafted DeID approach that is delicately designed for eye diseases, namely, digital mask (DM). On complex diseases, such as BCC and Eyelid Nevus (EyelidN), DM does not work (k = 0.0566/0.0988) while our approach achieves satisfactory results (k = 0.8245/0.8346). Moreover, this real-world evaluation introduces additional disease categories not seen during training, i.e., Entropion and EyelidN, highlighting the robustness and generalizability of our method. The diagnosis accuracy is provided in the supplementary material.\nDe-Identification Performance. As shown in Tab. 5, our method achieves the best DeID performance of ID-Dis value 1.3601, when evaluated with the SphereFace face recognition model. With the FaceNet and FaceNet models, our approach outperforms all methods except RiDDLE. RIDDLE maps person images into the very low-dimensional StyleGAN latent space and"}, {"title": "Qualitative Results", "content": "As shown in Fig. 6, our approach uniquely preserves both coarse- and fine-grained medical cues, such as drooping eyelids and conjunctival redness. In contrast, DeepPrivacy masks and replaces the original face, Password retains color but distorts shapes. Disguise, Personal, and RiDDLE sacrifice medical cues for privacy. Besides, our approach demonstrates good visual quality."}, {"title": "Other Downstream Utilities", "content": "As shown in Tab. 7, our approach shows competitive or best results on facial landmark detection and gaze detection tasks. For example, our approach achieves an eye landmark detection error of 2.94, much lower than the second-best approach, Password, which achieves 4.38. This is due to our particularly preserved eye-"}, {"title": "Reversibility", "content": "As shown in Tab. 8, compared with other reversible methods, our method performs better in terms of identity recovery, image fidelity, and perceptual quality, achieving ID-Dis, Peak Signal-to-Noise Ratio (PSNR), and LPIPS values of 0.5642, 27.02dB, and 0.2098, respectively. Moreover, our approach exhibits the best disease classification accuracy of 89.34%, while the second-best Personal only obtains 73.08%."}, {"title": "Model Analysis", "content": "Ablation Study for MedSem-DeID Model. Recalling that MedSem-DeID enhances the medical knowledge of the DeID pipeline in both the feature extraction procedure (fmed) and the loss function (Lmed), we verify the effectiveness of both strategies. As shown in Tab. 9, when trained on the common-scene facial dataset FFHQ without using any medical prior, the resulting model (M1) achieves an ID-Dis score of 1.3609 and a disease classification accuracy of 42.13%. When the training dataset is replaced with MeMa, the medical accuracy of the resulting model (M2) improves to 46.82% without compromising the DeID performance.\nAfter further introducing medical priors, no matter the fmed or the Lmed, the resulting models M3 and M4 show an obvious improvement in classification accuracy, i.e., 69.94% and 71.35%, while slightly compromising the DeID results. When combining both strategies, the final model achieves a strong medical performance of 86.70%.\nDifferent Medical Semantic Encoders. Our method is flexible, not relying on the typical implementation of the medical encoder. To verify this, we trained two other semantic encoders on MeMa. We fine-tuned a pre-trained ViT model using supervised and self-supervised learning strategies, specifically the masked auto-encoder (MAE). As shown in Tab. 10, all three variants achieved decent performance. The supervised ViT performed the poorest due to the sparse disease category label for supervision. The diffusion model outperformed ViT(MAE) with 86.38% vs. 85.26%. This is likely because the LAION-5B dataset used for pre-training the base stable diffusion model is much larger than the pre-training dataset for the base ViT.\nDifferent Loss Weights. As shown in Fig. 8 (left), increasing the weight of medical loss (Amed) consistently improves disease classification accuracy, due to the enhanced medical information. However, this also makes the de-identified images more similar to the originals, compromising the DeID performance, i.e., the reduced ID-Dis. We set Amed = 5 to achieve the best trade-off between medical accuracy and DeID. For the weight controlling reversible reconstruction"}, {"title": "Conclusion and Limitation", "content": "We have released a large-scale patient face dataset, MeMa, to facilitate research on medical privacy protection. Expert physicians validated and annotated MeMa. On this dataset, we established a comprehensive benchmark for medical-scene de-identification, also proposing a new baseline approach that outperforms previous approaches. A limitation is that the current dataset focuses on eye disease-related manifestations. Future work will expand the dataset to include other facial diseases, such as facial paralysis."}]}