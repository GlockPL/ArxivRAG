{"title": "Batch Bayesian Optimization via Expected Subspace Improvement", "authors": ["Dawei Zhan", "Zhaoxi Zeng", "Shuoxiao Wei", "Ping Wu"], "abstract": "Extending Bayesian optimization to batch evaluation can enable the designer to make the most use of parallel computing technology. Most of current batch approaches use artificial functions to simulate the sequential Bayesian optimization algorithm's behavior to select a batch of points for parallel evaluation. However, as the batch size grows, the accumulated error introduced by these artificial functions increases rapidly, which dramatically decreases the optimization efficiency of the algorithm. In this work, we propose a simple and efficient approach to extend Bayesian optimization to batch evaluation. Different from existing batch approaches, the idea of the new approach is to draw a batch of subspaces of the original problem and select one acquisition point from each subspace. To achieve this, we propose the expected subspace improvement criterion to measure the amount of the improvement that a candidate point can achieve within a certain subspace. By optimizing these expected subspace improvement functions simultaneously, we can get a batch of query points for expensive evaluation. Numerical experiments show that our proposed approach can achieve near-linear speedup when compared with the sequential Bayesian optimization algorithm, and performs very competitively when compared with eight state-of-the-art batch algorithms. This work provides a simple yet efficient approach for batch Bayesian optimization. A Matlab implementation of our approach is available at https://github.com/zhandawei/Expected Subspace_Improvement_Batch_Bayesian_Optimization.", "sections": [{"title": "I. INTRODUCTION", "content": "Bayesian optimization [1], also known as efficient global optimization (EGO) [2], is a powerful tool designed to solve expensive black-box optimization problems. Unlike gradient-based algorithms and evolutionary algorithms, Bayesian optimization employees a statistical model to approximate the original objective function and uses an acquisition function to determine where to sample in its search process. Bayesian optimization has gained lots of success in machine learning [3], robot control [4], and engineering design [5]. The developments of Bayesian optimization have been reviewed in [6], [7].\n\nThe standard Bayesian optimization algorithm selects and evaluates new sample points one by one. The sequential optimization process is often time-consuming since each evaluation of the expensive objective function costs a significant amount of time. Therefore, developing batch Bayesian optimization algorithms, which are able to evaluate a batch of samples simultaneously, is in great needs. The main reason Bayesian optimization operates sequentially is that optimizing the acquisition function can often deliver only one sample point. Therefore, the key to develop batch Bayesian optimization algorithms is to design batch acquisition functions. Currently, the most popular acquisition functions used in Bayesian optimization are the expected improvement (EI) [1], the probability of improvement (PI) [8] and the lower confidence bound (LCB) [9]. Among them, the EI criterion is arguably the most widely used. Therefore, we focus on the parallel extensions of the EI criterion in this work.\n\nThe most direct extension of the EI function for batch evaluation is the multi-point expected improvement, which is often called q-EI where q is the number of batch points [10]. The q-EI criterion measures the expected improvement when a batch of samples are considered. Although the q-EI is very theoretically rigorous, its exact computation is time-consuming when the batch size q is large. Works have been done to accelerate the calculation of q-EI [11]\u2013[14]. However, current approaches for computing q-EI are only applicable when the batch size is smaller than 10. Although one can use the approximation approach [11] or the fast multi-point expected improvement (Fq-EI) [15] to ease the computational cost, optimizing the q-EI function is also challenging when the batch size is large. The dimension of the q-EI criterion is d\u00d7 q, where d is the dimension of the original optimization problem. As the batch size q increases, the difficulty for the internal optimizer to find the global optimum of the acquisition function grows exponentially. As a result, the q-EI and its variants are not applicable for large batch size.\n\nTo avoid the curse of dimensionality of the q-EI criterion, several heuristic batch approaches have been proposed. The Kriging believer (KB) and Constant Liar (CL) approaches [11] get the first query point using the standard EI criterion, assign the objective function value of the acquisition point using fake values, and then use the fake point to update the Gaussian process model to get the second query point. This process is repeated until a batch of q query points are obtained. The KB approach uses the Gaussian process prediction as the fake value while the CL approach uses the current minimum objective value as the fake value [11]. The KB and CL approaches are able to locate a batch of points within one iteration without evaluating their objective values. However, as the batch size increases, the accuracy of the Gaussian process model decreases gradually as more and more fake points are included. This may mislead the search direction and further affect the optimization efficiency.\n\nIt has been observed that the El function decreases dra- matically around the updating point but slightly at places far away from the updating point after the Gaussian process model is updated using a new point. Therefore, we can design an artificial function to simulate the effect that the updating point bring to the EI function and use it to select a batch of points that are similar to the points the sequential EI selects. This idea has been used in the local penalization (LP) [16], the expected improvement and mutual information (EIMI) [17], and the pseudo expected improvement (PEI) [18]. The LP approach uses one minus the probability that the studying point belongs to the balls of already selected points as the artificial function [16], the EIMI approach uses the mutual information between the studying point and the already selected points as the artificial function [17], and the PEI criterion uses one minus the correlation between the studying point and the already selected points as the artificial function [18]. Through sequentially optimizing these modified EI functions, these approaches can select a batch of samples one by one, and then evaluate them in parallel. However, due to the use of artificial functions, the simulated EI functions will be less and less accurate, and as a result the qualities of the selected samples will also be lower and lower as the batch size increases.\n\nAnother idea for delivering a batch of samples within one cycle is to use multi-objective optimization method [19]. These approaches consider multiple acquisition functions at the same time, and transform the infill selection problem as a multi-objective optimization problem. By solving the multi-objective infill selection problem, a set of Pareto optimal points can be obtained and a batch of samples can be then selected from the Pareto optimal points. The multi-objective optimization based efficient global optimization (EGO-MO) [20] treats the two parts of the EI function as two objectives and solves the two-objective optimization problem using the multi-objective evolutionary algorithm based on decomposition (MOEA/D) [21]. In the multi-objective acquisition ensemble (MACE) approach [22], the EI, PI and LCB are chosen as the three objectives and the multi-objective optimization based on differential evolution (DEMO) [23] is utilized to solve the three-objective optimization problem. The adaptive batch acquisition functions via multi-objective optimization (ABAFMo) [24] approach considers multiple acquisition functions but selects only two of them to solve based on an objective reduction method. For the multi-objective optimization approaches, the number of batch samples is limited to the population size of the adopted multi-objective evolutionary algorithms. When the batch size is large, the diversity of the samples is hard to maintain for these approaches.\n\nAs can be seen, most of current batch EI approaches are designed for small batch size. Designing batch EI criterion for large batch size to make the most use of large parallel processing facilities and further accelerate the optimization process of Bayesian optimization is both interesting and meaningful. In this work, we propose the expected subspace improvement (ESSI) based batch Bayesian optimization algorithm to fill this research gap. The proposed method is simple yet efficient. The idea is to locate a batch of samples in multiple different subspaces of the original design space based on the proposed ESSI criterion. The results show that our algorithm has significantly better performance over current batch EI approaches."}, {"title": "II. BACKGROUNDS", "content": "In this work, we try to solve an expensive, single-objective, bound-constrained, and black-box optimization problem:\n\nfind: x = [x1,x2,...,xa]\n\nminimize: f(x)\n\nsubject to: x \u2208 X\n\n(1)\n\nwhere X = {x \u2208 Rd : ai \u2264 xi \u2264 bi, i = 1,2,\u2026\u2026,d} is the design space, and ai and bi are the lower bound and upper bound of the ith coordinate. The objective function f is assumed to be black-box, which often prohibits the use of gradient-based methods. The objective function f is also assumed to be expensive to evaluate, which often prohibits the direct use of evolutionary algorithms. This kind of optimization problems widely exist in machine learning and engineering design [3]-[5]. Currently, model-based optimization methods, such as Bayesian optimization algorithms and surrogate-assisted evolutionary algorithms are the mainstream approaches for solving these expensive optimization prob- lems [25]. In this work, we focus on the Bayesian optimization methods. In the following, we give a brief introduction about Bayesian optimization. More details about Bayesian optimiza- tion can be found in [6], [26]."}, {"title": "A. Gaussian Process Model", "content": "Gaussian process models [27], also known as Kriging models [2], are the most frequently used statistical models in Bayesian optimization. They are employed to approximate the expensive objective function based on the observed samples. The Gaussian process model treats the unknown objective function as a realization of a Gaussian process [27]:\n\nf(x) ~ N (m(x), k(x, x'))\n\n(2)\n\nwhere m is the mean function of the random process and k is the covariance function, also known as the kernel function, between any two points x and x'. Under the prior distribution, the objective values of every combination of points follow a multivariate normal distribution [27]. Popular mean functions of the Gaussian process are constant values or polynomial functions. Popular covariance functions are the squared ex- ponential function and the Mat\u00e9rn function [27].\n\nAssume we have gotten a set of n samples\n\nX = [x(1), x(2),...,x(n)]\n\nand their objective values\n\nf(X) = [f(x(1)), f(x(2)),\u2026\u2026\u2026, f(x(n))]."}, {"title": "B. Acquisition Function", "content": "The second major component of Bayesian optimization is the acquisition functions. After training the Gaussian process model, the following thing is to decide where to sample next in order to locate the global optimum of the original function as quickly as possible. This is done by maximizing a well- designed acquisition function in Bayesian optimization. To take a balance between global search and local search, the acquisition function should on one hand select points around current best observation and on the other hand select points around under-sample areas. Popular acquisition functions for Gaussian process models are the expected improvement [1], the probability of improvement [8], the lower confidence bound [9], the knowledge gradient [28], [29], and the entropy search [30], [31]. Among them, the expected improvement (EI) criterion is arguably the most widely used because of its mathematical tractability and good performance [26], [32]. Therefore, we focus on the EI acquisition function in this work.\n\nAssume current minimum objective value among n evalu- ated samples is min=1 f(x(i)) = fmin and the corresponding best solution is arg min=1 f(x(i)) = Xmin. According to the Gaussian process model, the objective value of an unknown point \u00e6 can be interpreted as a random value following the normal distribution [2]\n\nF(x) ~ \u039d (\u03bc(x), \u03c3\u00b2(x))\n\n(6)\n\nwhere \u03bc(x) and o\u00b2(x) is the Gaussian process mean and variance in (4) and (5) respectively.\n\nThe EI function is the expectation of the improvement by comparing the random value F(x) with fmin [2]\n\nEI(x) = E[max(fmin - F(x),0)]\n\n(7)\n\nwhere fmin is current minimum objective value among all evaluated samples. Integrating this equation with the normal distribution in (6), we can get the closed-form expression [2]\n\nEI(x) = (fmin \u2013 \u03bc(x)) \u03a6 ( (fmin \u2013 \u03bc(x)) / \u03c3(x)) + \u03c3(\u03b1)\u03c6 ( (fin(x)) / \u03c3(\u03b1))\n\n(8)\n\nwhere I and \u03c6 are the standard normal cumulative and density functions respectively. As can be seen from the formula, the EI function is a nonlinear combination of the Gaussian process mean and variance."}, {"title": "C. Bayesian Optimization", "content": "Typically, Bayesian optimization works in a sequential way. Algorithm 1 shows the computational framework of Bayesian optimization. At the beginning of Bayesian optimization, a set of ninit samples is drawn using some design of exper- iment method and evaluated with the expensive objective function. Then, Bayesian optimization works sequentially. In each iteration, the Gaussian process model is first trained using all the evaluated samples. Then, a new point is located by the acquisition function and evaluated with the expensive objective function. After that, Bayesian optimization goes to next iteration to query and evaluate another point. The se- quential process stops when the maximum number of function evaluations nmax is reached. At the end, the algorithm outputs the best found solution (xmin, fmin)."}, {"title": "III. PROPOSED APPROACH", "content": "The standard Bayesian optimization selects one query point for expensive evaluation at each iteration. It can not utilize parallel computing techniques when multiple workers are available for doing the expensive evaluations. Current batch Bayesian optimization approaches are designed for small batch size. Their performances degenerate significantly when the batch size becomes large. In this work, we propose a novel batch Bayesian optimization that works well with large batch size. Different from ideas utilized by current batch approaches, our approach tries to select multiple query samples in multiple different subspaces. The new idea is easy to understand and the proposed algorithm is simple to implement."}, {"title": "A. Expected Subspace Improvement", "content": "We propose a novel criterion named expected subspace improvement (ESSI) to select query points in subspaces. The proposed ESSI measures the improvement of a point in a specific subspace. Consider the original d-D design space\n\nX = {[x1,x2,...,xa]} \u2286 Rd,\n\nan s-D (1 < s < d) subspace can be expressed as\n\nY = {[Y1, Y2,\u2026, ys]} \u2286 R$\n\nwhere yi \u2208 {X1,X2,\u2026\u2026,Xd} for i = 1,2,..., s and Y1 \u2260 Y2 \u2260 \u2026,\u2260 ys. Assume the current best solution is Xmin = [Xmin,1, Xmin,2,\u00b7\u00b7\u00b7, Xmin,d]. The standard EI function measures the potential improvement a point can get beyond the current best solution xmin. From another point of view, the El function can be seen as the measurement of the potential improvement as we move the current best solution xmin in the original design space. Following this, we define the expected subspace improvement (ESSI) as the amount of expected improvement as we move the current best solution xmin in a subspace\n\nESSI(y) = (fmin \u2013 \u03bc(z)) \u03a6 ( (fmin \u2013 \u03bc(z)) / \u03c3(\u03b6)) + \u03c3(\u03b1)\u03c6 ( (fin(z)) / \u03c3(\u03b6))\n\n(9)\n\nwhere\n\nz = [xmin,1,\uff65\uff65\uff65, Y1, \u2026\u2026\u2026, Xmin,i \u2026\u2026\u2026, Ys,\u2026\u2026\u2026, Xmin,d]\n\nis the d-D input of the Gaussian process model. In z, the Y1, Y2,\u2026, Yd are the coordinates of the subspace to be optimized and the remaining coordinates are fixed at the values of current best solution Xmin = [Xmin,1, Xmin,2,\u00b7\u00b7\u00b7, Xmin,d].\n\nWe demonstrate the ESSI functions of the 3-D Rosen- brock problem in Fig. 3. We draw thirty random points in [-2.048, 2.048]3 and train a Gaussian process model using the sample points. We use the constant mean function and the squared exponential kernel function in the Gaussian process model, and search the best hyperparameter in the kernel function within [0.01, 100]. The figure in the middle shows the scatter plots of the thirty samples in the original 3-D space. The red filled point is the current best solution among the thirty samples. On the left, we show the ESSI functions as we move the current best solution along coordinates y = x1, y = x2 and y = x3, respectively. On the right, we show the ESSI functions as we move the current best solution in subspaces y = [X1,X2], Y = [x1,x3] and y = [x2, x3], respectively. In other words, the left figures can be seen as the EI function in three 1-D subspaces while the right figures can be seen as the EI function in three 2-D subspaces.\n\nThe dimension of the subspaces s can vary from 1 to d. When s = d, the proposed ESSI function is equal to the standard EI function [2]. And when s = 1, the ESSI function degenerates to the expected coordinate improvement (ECI) function [33]. From this point of view, the proposed ESSI function can be seen as the more generalized measurement, and both the EI and ECI can be treated as especial cases of the ESSI function."}, {"title": "B. Computational Framework", "content": "Based on the proposed ESSI criterion, we propose a novel batch Bayesian optimization algorithm. The idea of the pro- posed approach is very simple. In each iteration, we locate a batch of q points in q different subspaces using the ESSI criterion, and then evaluate these q query points in parallel. The computational framework of the proposed approach is given in Algorithm 2. The steps of the proposed algorithm are described as following.\n\n1) Initial Design: The proposed batch Bayesian optimiza- tion algorithm also starts with an initial design. Af- ter getting the initial samples using some design of experiment method, we evaluate these samples with the real objective function. The current best solution (xmin, fmin) can be identified from the initial samples.\n\n2) Model Training: Then the batch Bayesian optimization algorithm goes into the iteration process. At the begin- ning of each iteration, we first train a Gaussian process model using all evaluated samples. This step is the same as the standard Bayesian optimization algorithm.\n\n3) Subspace Selection: Before locating a batch of query points in different subspaces, we should first determine the subspaces that we will select query points from. In this work, we use random selection approach to select a batch of subspaces.\n\n4) Acquisition Optimization: After selecting a batch of subspaces, we can then locate one query point within one subspace by optimizing the corresponding ESSI function. Since these ESSI functions are independent from each other, we can solve these ESSI optimization problems in parallel.\n\n5) Expensive Evaluation: Optimizing the q ESSI functions delivers q different query points. In this step, we can then evaluate these q query points with the expensive objective function in parallel. Since the most time- consuming step of Bayesian optimization is often the expensive evaluation step, distributing the query points in different machines and evaluating them in parallel can save us a lot of wall-clock time.\n\n6) Best Solution Update: At the end of the iteration, we need to update the data set by including the newly evaluated samples. We also need to update the best solution since it will be used in the next round of iterations. After the update, the algorithm goes back to the model training for a new iteration. The iteration process stops when the maximal number of function evaluations is reached.\n\nThe major difference between our proposed batch Bayesian optimization algorithm and the standard Bayesian optimization algorithm is that in each iteration the standard approach selects one query point in the original design space while our batch approach selects multiple query points in different subspaces. Therefore, the standard approach can only evaluate the query points one by one while the proposed batch approach can evaluate them in parallel. The subspace selection and acquisition optimization are the two key steps for our proposed batch Bayesian optimization algorithm, therefore are further discussed in the following in detail."}, {"title": "C. Subspace Selection", "content": "In subspace selection, we employ the most straightfor- ward method, that is selecting the subspaces randomly. This random selection strategy avoids setting the hyper-parameter s (the dimension of the subspace) and turns out to be very efficient. The dimension of the subspace s can vary from 1 to d. When s = 1, the available subspaces are {[X1], [x2],, [xa]}. Similarly, as s = 2, the available subspaces are {[X1, X2], [X1, X3], \u00b7\u00b7\u00b7,[xd\u22121,xd]}. This goes on until s reaches to d, in which case there is only one available subspace {[X1,X2,\u00b7\u00b7\u00b7,xa]}. The total number of subspaces of a d-D space can be calculated as\n\nN = C + Ca + \u2026 + Cd = 2d \u2013 1\n\nwhere Cd donates the number of combinations of selecting i different variables from d total variables. The number N is very large when the dimension of the original space is not that small. For example, the total number of subspaces of a 10-D space is N = 210 \u2013 1 = 1023, which is much larger than the batch size we commonly use. In the cases where N is smaller than the batch size q, we can employ existing strategy such as Kriging Believer or Constant Liar [11] to select multiple points within one subspace.\n\nInstead of directly drawing q subspaces from all N sub- spaces, we select the q subspaces by first drawing a random integer and then drawing a random combination, as shown in the Step 6 and Step 7 in Algorithm 2. The reason is that the direct selection strategy is very computationally expensive to list all N subspaces when d is large. Our strategy is very fast but might select identical subspaces during the q selections. One can eliminate this by checking whether the candidate subspace has been selected after each selection. If the candidate subspace has been selected, then we can reject the selection and start a new drawing. When the dimension of the problem d is larger than 20, the probability of selecting identical subspace can be neglected."}, {"title": "D. Acquisition Optimization", "content": "In the acquisition optimization process, we optimize the variables of ith subspace while holding the other variables fixed at the current best solution xmin. After the optimization, we replace the variables in amin by the optimized ones to get the ith query point\n\nx(n+i) = [min,1,..., min,1,..., Xmin, i(i)Ymin,s,..., Xmin,d].\n\nWe demonstrate the acquisition optimization process also on a 3-D example in Fig. 4. Assume that the current best solution is Xmin = [xmin,1, Xmin,2, Xmin,3], and the six subspaces we select are y(1) = x1, y(2) = x2, y(3) = x3, y(4) = [X1, X2], y(5) = [x1,x3] and y(6) = [x2, x3].\n\nFor the first acquisition optimization problem, the optimiz- ing variable is x1. After the optimization process, we can get the result\n\nYmin, 1(1)= arg max ESSI(x1).\n\nx1\u2208 [a1,b1]\n\nThen we replace the x1 coordinate of the current best solution by this optimized variable and get our first query point\n\nx(n+1)= [ymin,1(1),min,2, Xmin,3].\n\nFollowing the same procedure, we can get our second and third query points by optimizing x2 and 13 variables, respectively. For the fourth acquisition optimization problem, the sub- spaces we select are y(4) = [X1,X2]. After solving this two- variable optimization problem, we can get\n\n[ymin, 1(4), min, 2(4)]= arg max ESSI([X1,X2]).\n\nx1\u2208[a1,b1],x2\u2208[a2,b2]\n\nThen, we replace the x1 and 22 coordinates of the current best solution by the two optimized variables to get our fourth query point\n\nx(n+4) = [ymin, 1(4), Ymin, 2(4), Xmin,3]."}, {"title": "IV. NUMERICAL EXPERIMENTS", "content": "In this section, we conduct two sets of experiments to verify the effectiveness of our proposed ESSI approach. In the first set of experiments, we compare the proposed batch approach with the sequential approach to see the speedup of the proposed approach. In the second set of experiments, we compare the proposed ESSI approach with eight existing batch algorithms to see how well the proposed approach performs when compared with the state-of-the-art."}, {"title": "A. Experiment Settings", "content": "We use the CEC 2017 test suite [34] as the benchmark problems. All the 29 problems in the test suite are rotated and shifted. In the test suite, the f\u2081 and f3 are unimodal functions, f4 to f10 are multimodal functions, f11 to f20 are hybrid functions, and f21 to f30 are composition functions [34]. The dimensions of all the test problems are set to d = 100.\n\nThe Latin hypercube sampling is used to generate the initial samples for the compared algorithms. The number of initial samples is set to Ninit = 200. For the Gaussian process model, we use the constant mean function and the square exponential kernel function. We maximize the log-likelihood of the observed samples to estimate the parameters of the Gaussian process model.\n\nFor all experiments, the number of acquisition samples is set 1024, therefore the maximum number of objective evaluations is Nmax = 200 + 1024 = 1224. Since the acquisition functions are often highly multimodal, we employ a real-coded genetic algorithm to maximize the EI and ESSI functions. The population size of the genetic algorithm is set to twice as the dimension, and the maximal number of generation is set to 100. We use simulated binary crossover and polynomial mutation in the genetic algorithm. The crossover rate is set to 0.9 and the mutation rate is set to 1/d. Both the distribution indices for the crossover and mutation are set to 20.\n\nAll experiments are run 30 times to deliver reliable results. The initial sample sets are different for 30 runs, and are the same for different compared algorithms.\n\nAll experiments are run on a Window 10 machine which has two Intel Xeon Gold 6338 CPUs (a total of 64 physical cores) and 128 GB RAM. All experiments are conducted using MATLAB R2022b."}, {"title": "B. Comparison with Sequential BO Algorithm", "content": "We first compare the proposed ESSI with the standard EI to study the speedup of the proposed approach. We set q to 2, 4, 8, 16, 32 and 64 respectively to test the performance of the proposed approach under different batch sizes. Since the number of acquisition samples is fixed as 1024, the corresponding maximal iterations for the batch algorithm are then 512, 256, 128, 64, 32 and 16, respectively.\n\nThe final optimization results of the standard EI approach and the ESSI approach are presented in Table I, where the average values of 30 runs are given. We conduct the Wilcoxon signed rank test to find out whether the results obtained by the proposed ESSI have significantly difference from the results obtained by the standard EI. The significance level of the tests is set to \u03b1 = 0.05. We use +, \u2212 and \u2248 to represent that our ESSI approach finds significantly better, significantly worse and similar results compared with the sequential EI approach, respectively.\n\nFrom Table I we can see that the proposed batch ESSI approach finds significantly better results than the sequential El approach on most of the test problems at the end of 1024 additional function evaluations. By locating new samples in lower-dimensional subspaces, the proposed ESSI approach reduces the difficulty of optimizing the acquisition function compared with the standard EI approach which finds new samples in the original space. The significant tests show that our proposed ESSI approach is able to achieve better results than the standard EI approach on twenty-six, twenty- six, twenty-four, twenty-five, twenty-five and twenty-five test problems as the batch size of 2, 4, 8, 16, 32 and 64 is utilized, respectively. The experiment results can empirically prove the effectiveness of the proposed ESSI approach.\n\nThe convergence curves of the standard EI and the proposed ESSI approaches on the test problems are shown in Fig. 5, where the horizontal axis is the number of iterations and the vertical axis is the found minimal objective value. In these figures, we show the median, the first quartile, and the third quartile of the 30 runs.\n\nFrom the curves we can see that the proposed ESSI ap- proach converges much faster than the standard EI approach on the tested problems. The batch approach evaluates q points in each iteration, therefore it only needs 1024/q iterations to finish all the function evaluations. By distributing the evaluations on multiple cores or machines, the batch approach is able to reduce the wall-clock q times compared with the sequential approach. We can also see that the convergence speed of the proposed ESSI approach can be further improved as we increase the batch size from 2 to 64, which shows the good scalability of the proposed ESSI approach with respect to the batch size. At the end of 1024 additional function evaluations, the proposed ESSI approach even finds better solutions than the standard El approach on most of the test problems. This means the proposed ESSI approach is able to reduce the wall-clock time and improve the optimization results at the same time.\n\nThen, we compare the running time of the proposed ESSI approach and the standard EI approach. The running time of both the ESSI and the EI consists of the time of training the GP model, optimizing the acquisition functions and evaluating the new points. For the model training, the standard EI needs to train the GP model 1024 times while the ESSI approach only needs to train the GP model 1024/q times as it evaluates q query points at each iteration. For the acquisition optimization, the standard EI approach optimizes one EI function at a time while the ESSI approach optimizes q ESSI functions within one iteration. For the function evaluation, the standard EI approach evaluates one new sample in each iteration while the ESSI approach evaluates q new samples. Both the q acquisition optimizations and function evaluations of the ESSI approach can be executed in parallel. In the experiments, we use MATLAB parfor to distribute the q acquisition optimizations and function evaluations on q CPU cores, and run them in parallel.\n\nThe running time comparison is presented in Fig. 6, where the vertical axis is the average running time of 30 runs and is shown in seconds. We can see that the proposed ESSI approach is able to reduce the running time significantly compared with the standard EI approach. As the batch size q increases, the running time of ESSI decreases gradually. The speedup ratio of ESSI is around 1.86, 3.04, 5.04, 8.69, 14.32 and 17.77 for q = 2, 4, 8, 16, 32 and 64, respectively. This means the speedup of ESSI is under-linear. The reason that the ESSI approach is not able to achieve linear speedup is that the ESSI requires some computational time to select subspaces in each iteration. It should be noted that the time for training the GP model and optimizing the acquisition function can often be neglected when compared with evaluating expensive simulations. The ESSI approach should be able to achieve near linear speedup by evaluating the expensive simulations in parallel."}, {"title": "C. Comparison with Batch Approaches", "content": "In the second set of experiments, we compare the proposed ESSI approach with eight batch approaches, which include four batch Bayesian optimization algorithms and four batch surrogate-assisted evolutionary algorithms (SAEAs). The four batch BOs are the Kriging believer (KB) [11", "11": "expected improvement and mutual informa- tion (EIMI) [17", "15": ".", "35": "incremental Kriging-assisted evolution- ary algorithm (IKAEA) [36", "37": "."}]}