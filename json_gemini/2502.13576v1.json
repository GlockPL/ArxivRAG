{"title": "Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation", "authors": ["Peiwen Yuan", "Yueqi Zhang", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "abstract": "Evaluating models on large benchmarks is very resource-intensive, especially during the period of rapid model evolution. Existing efficient evaluation methods estimate the performance of target models by testing them only on a small and static coreset of the benchmark, which is derived from the publicly available evaluation results of source models. These methods rely on the assumption that target models have high prediction consistency with source models. However, we demonstrate that it doesn't generalize well in practice. To alleviate the inconsistency issue, we present TAILOREDBENCH, a method that conducts customized evaluation tailored to each target model. Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. According to the predictions on Native-coresets, we obtain the performance of target models on the whole benchmark with a calibrated estimation strategy. Comprehensive experiments on 5 benchmarks across over 300 models demonstrate that compared to best performing baselines, TAILOREDBENCH achieves an average reduction of 31.4% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability.", "sections": [{"title": "1 Introduction", "content": "Scaling up models in multiple dimensions has led to remarkable advancements in their capabilities (Touvron et al., 2023; Ouyang et al., 2022), which also presents significant challenges for efficiently assessing them. For instance, Liang et al. (2022) reports that evaluating a model with approximately 10 billion parameters on the HELM leaderboard costs over $1,700 via APIs or more than 1,200 GPU hours. Moreover, these costs scale by a factor of X when exploring and comparing X different training or inference configurations during the development or deployment phase.\nTo achieve efficient evaluation, some studies (Vivek et al., 2024; Polo et al., 2024) have explored the following paradigm: step 1. constructing example embeddings according to the predictions from a set of source models (which are freely available for popular leaderboards*,*,*); step 2. clustering the benchmark and selecting the cluster centroids to form a coreset (typically less than 100 examples); step 3. approximating the performance of target models under evaluation based on their predictions on the coreset. (See detailed related works in Appendix A.) Underlying this approach is the assumption that performance patterns generalize: if source models respond similarly to two examples a and b, then a target model's performance on a can be used to estimate its performance on b. Nevertheless, we find that such generalizability between source and target models does not necessarily hold. Following ANCHORPOINT (Vivek et al., 2024), we construct an embedding based on the correctness (e.g., the probability of the correct option) of all source models for each example and visualize them using t-SNE algorithm (Van der Maaten and Hinton, 2008). In these embeddings (Figure 1a), nearby examples elicit similar predictions from the source models, allowing cluster centroids (marked by stars) to serve as representative points. Yet, when we adopt embeddings derived from the correctness of target models instead (Figure 1b), the average distance between the example and its centroid increases from 10.09 to 12.48, indicating that the previously chosen centroids fail to represent their respective clusters effectively. This reveals a discrepancy in prediction behaviors between source and target models, which we term prediction consistency\u2014the extent to which their predictions align on the same examples. When prediction consistency is low, source-model-derived coresets fail to generalize, resulting in inaccurate performance estimates for target models.\nTo address the aforementioned issue, we propose the TAILOREDBENCH method, which adaptively constructs model-specific evaluation coreset in a global to native manner for accurate and efficient evaluation. Specifically, we first construct a static G-set (Global-coreset) based on the prediction results of all the source models. By applying an adaptive source model selection strategy, the predictions of target models on the G-set are used as a probe to select a native source model set for each target model that has stronger prediction consistency with them. Based on this posterior, we design a scalable K-Medoids clustering technique to expand the G-set into an N-set (Native-coreset) for each target model, according to the benchmark embeddings under the metric of corresponding native source models. Finally, we approximate the overall performance of target models by employing a calibrated estimation strategy based on their predictions on the N-set.\nWe conduct extensive experiments on five benchmarks across more than 300 models, involving tasks in the fields of natural language and multimodality. Compared to non-customized efficient evaluation baselines, TAILOREDBENCH can more accurately estimate the performance of models (attaining an average of 31.4% MAE degradation improvement on accuracy) under the same small-size inference budgets (generally 20~40 examples). Our contributions are summarized as follows:\n\u2022 We analyze that the existing efficient evaluation methods overestimate the prediction consistency across models, thus the source-model-based static coreset may fail to assess the target models accurately.\n\u2022 We propose the TAILOREDBENCH method to conduct tailored evaluation on adaptively constructed N-set for each target model to attain more accurate evaluation results.\n\u2022 We conduct comprehensive experiments and analyses on multiple settings to validate the excellent effectiveness and strong generalizability."}, {"title": "2 TailoredBench Approach", "content": "The TAILOREDBENCH approach centers on dynamically selecting prediction-consistent source models and crafting an N-set that faithfully represents the entire benchmark for each target model. Its formulation proceeds through four tightly integrated steps: constructing a globally representative G-set (\u00a72.2), identifying native source models (\u00a72.3) and developing N-set for each target model (\u00a72.4), and finally estimating the target models' overall performance (\u00a72.5)."}, {"title": "2.1 Task Set-Up", "content": "Let $\\mathcal{D} = \\{(x_k, y_k)\\}_{k=1}^{|\\mathcal{D}|}$ denotes a benchmark, where $x_k$ is the input and $y_k$ is the corresponding ground truth output. We define the set of target models under evaluation as $\\mathcal{T} = \\{t_m\\}_{m=1}^{|\\mathcal{T}|}$. Additionally, we denote the source model set as $\\mathcal{S} = \\{s_n\\}_{n=1}^{|\\mathcal{S}|}$, for which we have access to their predictions across all examples in $\\mathcal{D}$. Following previous works, we ensure that $\\mathcal{T} \\cap \\mathcal{S} = \\emptyset$. Our objective is to accurately estimate the performance $P_{t_m}$ of each target model $t_m \\in \\mathcal{T}$ and determine the ranking relationships within $\\mathcal{T}$, while minimizing the model inference cost."}, {"title": "2.2 Constructing G-set", "content": "We first construct the G-set $\\mathcal{G}$, which is designed as a probe for each target model to identify a set of source models with the highest prediction consistency. Consequently, it is intended to be a small yet relatively representative subset of the benchmark, ensuring its generalizability across target models.\nFollowing prior works (Vivek et al., 2024), we employ clustering based on the correctness of source models to construct the G-set. Here, correctness can be either the predictive probability of the correct option (continuous value [0, 1]) or whether the model answers the example correctly (discrete binary value {0, 1}).\nFor each example $x_k$ in the benchmark $\\mathcal{D}$, we compute an embedding using the correctness scores $C_{s_n,x_k}$ from each source model $s_n$:\n$x_k^S = \\begin{bmatrix}\nC_{s_1,x_k} \\\\\nC_{s_2,x_k} \\\\\n\\vdots \\\\\nC_{s_{|\\mathcal{S}|},x_k}\n\\end{bmatrix}$         (1)\nThe superscript S indicates that the embedding is derived from source models' correctness, and each embedding is $|\\mathcal{S}|$-dimensional. The collection of these embeddings constitutes the benchmark's representation $\\mathcal{D}^S = \\{x_k^S\\}_{k=1}^{|\\mathcal{D}|}$.\nBased on $\\mathcal{D}^S$, we apply K-Medoids clustering (Kaufman and Rousseeuw, 2009) to select the G-set with the objective function below:\n$\\min_{\\{\\mathcal{G},C_g\\}} \\sum_{x_g \\in \\mathcal{G}} \\sum_{x_k \\in C_g \\setminus \\{x_g\\}} \\text{Dis}(x_k^S, x_g^S)$         (2)\nwhere $x_g$ is an example in the G-set $\\mathcal{G} = \\{x_g\\}_{g=1}^{|\\mathcal{G}|}$, and $C_g$ is the cluster for which $x_g$ is the centroid. Dis denotes the distance metric in clustering.\nTo maximize the generalization capability of our method, the choice of distance metric is critical. Previous approaches (Vivek et al., 2024; Miller et al., 2021; Baek et al., 2022; Mehra et al., 2024) using correlation distance (Rodgers and Nicewander, 1988) to measure example consistency often assume linear relationships in scoring patterns among models or examples. However, this assumption may not hold for discrete numerical embeddings, leading to significant performance degradation. In contrast, element-wise distance (e.g., Manhattan distance) can effectively capture individual discrepancies in correctness vectors, thereby accommodating various correctness formats. By default, we adopt manhattan distance as Dis for our TAILOREDBENCH method."}, {"title": "2.3 Adaptive Native Source Model Selection", "content": "After constructing G-set $\\mathcal{G}$, we attain the prediction results of target models $\\mathcal{T}$ on it, which we use as a probe to construct a Native Source Model Set $\\mathcal{S}_{t_m}$ that exhibits the highest prediction consistency for each $t_m \\in \\mathcal{T}$.\nSpecifically, we first embed all the source models $s_n \\in \\mathcal{S}$ and target models $t_m \\in \\mathcal{T}$ based on their prediction correctness on $\\mathcal{G}$ as follows:\n$x_{s_n}^\\mathcal{G} = \\begin{bmatrix}\nC_{s_n,x_1} \\\\\nC_{s_n,x_2} \\\\\n\\vdots \\\\\nC_{s_n,x_{|\\mathcal{G}|}}\n\\end{bmatrix}$, $\\quad$\n$x_{t_m}^\\mathcal{G} = \\begin{bmatrix}\nC_{t_m,x_1} \\\\\nC_{t_m,x_2} \\\\\n\\vdots \\\\\nC_{t_m,x_{|\\mathcal{G}|}}\n\\end{bmatrix}$         (3)\nHere, the superscript $\\mathcal{G}$ denotes that each embedding dimension is derived from the model's prediction correctness on the G-set. Leveraging these embeddings, we compute the average prediction consistency d among all the models (both source and target) on the G-set as follows:\n$d = \\frac{2}{|\\mathcal{S}||\\mathcal{T}|(|\\mathcal{S}|+|\\mathcal{T}|-1)} \\sum_{i \\< j} d_{ij}$        (4)\nwhere $d_{ij} = \\text{Dis}(x_i^\\mathcal{G}, x_j^\\mathcal{G})$.\nIn this context, $i, j \\in [1, |\\mathcal{S}|+|\\mathcal{T}|]$ and $x$ represents any model from $\\mathcal{S} \\cup \\mathcal{T}$. By computing d across all models, we establish a robust threshold that reflects the model set's similarity landscape, enabling a consistent and effective selection of native source models for each target model.\nOn this basis, we determine $\\bar{n}$, the size of the native source model set for target models, by calculating the average number of source models whose prediction consistency with each target model exceeds the threshold d as follows:\n$\\bar{n} = \\frac{1}{|\\mathcal{T}|} \\sum_{t_m \\in \\mathcal{T}} |\\mathcal{S}_{t_m}|$,\nwhere $\\mathcal{S}_{t_m} = \\{s_n \\in \\mathcal{S} \\mid \\text{Dis}(x_{s_n}^\\mathcal{G}, x_{t_m}^\\mathcal{G}) < d\\}$         (5)\nFor a target model $t_m$, the top $\\bar{n}$ source models exhibiting the highest prediction consistency are selected to form its dynamic source model set $\\mathcal{S}_{t_m}$. By standardizing the number of native source models across all target models, we ensure that each target model's feature representation maintains consistent dimensionality and informational richness during subsequent clustering."}, {"title": "2.4 Developing N-set", "content": "Leveraging the selected native source models $\\mathcal{S}_{t_m}$, we construct the most representative N-set $\\mathcal{N}_{t_m}$ for each target model $t_m$. To maximize the utilization of the observed prediction results of target models on $\\mathcal{G}$, we propose a SCALABLE K-MEDOIDS CLUSTERING algorithm to extend $\\mathcal{G}$ into the N-set. Initially, each example $x_k \\in \\mathcal{D}$ is represented by a feature vector $x_k^{\\mathcal{S}_{t_m}}$, which is based on the correctness of its native source models $\\mathcal{S}_{t_m}$. Then, our SCALABLE K-MEDOIDS CLUSTERING algorithm operates as follows:\nAnchored Medoid Initialization: We fix the examples in G-set $|\\mathcal{G}|$ as initial medoids. To reach an N-set size of $|\\mathcal{N}_{t_m}|$, we randomly add $|\\mathcal{N}_{t_m}| - |\\mathcal{G}|$ additional examples from $\\mathcal{D} \\setminus \\mathcal{G}$ to form the initial medoid set.\nCluster Assignment: Assign each example $x_k \\in \\mathcal{D}$ to the nearest medoid $x_\\mu$ to form the cluster $C_\\mu$:\n$x_k \\in C_\\mu$,\nwhere $\\mu = \\arg \\min_\\mu \\text{Dis}(x_k^{\\mathcal{S}_{t_m}}, x_\\mu^{\\mathcal{S}_{t_m}})$        (6)\nDynamic Medoid Refinement: For each cluster $C_\\mu$ with a non-G-set medoid, update the medoid $x_\\mu$ by selecting the example within $C_\\mu$ that minimizes the total distance to all other examples in $C_\\mu$:\nx_\\mu = \\arg \\min_{x_i \\in C_\\mu} \\sum_{x_j \\in C_\\mu \\setminus \\{x_i\\}} \\text{Dis}(x_i^{\\mathcal{S}_{t_m}}, x_j^{\\mathcal{S}_{t_m}})        (7)\nMedoids corresponding to G-set remain fixed during this process.\nConvergence Verification: Repeat the Cluster Assignment and Dynamic Medoid Refinement steps until convergence is achieved, i.e., when medoids no longer change or a maximum number of iterations is reached.\nBy incorporating the G-set examples as fixed medoids, the clustering process ensures that these pivotal examples guide the formation of clusters and the selection of additional N-set examples."}, {"title": "2.5 Calibrated Performance Estimation", "content": "After establishing the N-set $\\mathcal{N}_{t_m}$ for a target model $t_m$, for previous methods (Vivek et al., 2024; Polo et al., 2024), they may estimate the model's overall performance by first evaluating it on these centroid examples and then weighting the results according to each centroid's coverage of the benchmark. However, simply relying on medoids overlooks subtle variations in how individual examples within each cluster are predicted, potentially leading to less accurate global estimates.\nTo address this, we leverage the prediction consistency between the target model $t_m$ and its native source models $\\mathcal{S}_{t_m}$ to obtain the calibrated correctness estimates for the target model. For a given cluster with medoid $x$, consider any non-medoid example $x'$ in the same cluster. We compute a scaling factor based on the native source models' average correctness, which reflects how the prediction patterns at $x'$ relate to those at $x$:\n$\\text{Scale}(x') = \\frac{\\bar{C}_{\\mathcal{S}_{t_m},x'} + 0.5}{\\bar{C}_{\\mathcal{S}_{t_m},x} + 0.5}$        (8)\nHere, $\\bar{C}_{\\mathcal{S}_{t_m},x}$ and $\\bar{C}_{\\mathcal{S}_{t_m},x'}$ denote the average correctness of $\\mathcal{S}_{t_m}$ on the medoid x and the non-medoid x', respectively. The addition of 0.5 ensures numerical stability by preventing the denominator from becoming zero. Given that $t_m$ and $\\mathcal{S}_{t_m}$ exhibit similar prediction consistencies, we assume this scaling factor can be applied to estimate the target model's correctness on x':\n$C_{t_m,x'} = (C_{t_m,x} + 0.5) \\cdot \\text{Scale}(x') - 0.5$        (9)\nBy integrating these inferred correctness values across all examples in the benchmark $\\mathcal{D}$, we obtain a more faithful global performance estimation without re-evaluating the entire dataset:\nP_{t_m} = \\frac{1}{|\\mathcal{D}|} \\sum_{x' \\in \\mathcal{D}} C_{t_m,x'}$        (10)"}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": "We validate TAILOREDBENCH on five diverse benchmarks spanning natural language and multimodal tasks. ARC Challenge (Clark et al., 2018) consists of 1,172 scientific reasoning questions, with predictions from 153 models. Hellaswag (Zellers et al., 2019) provides 6,000 commonsense inference examples (a subset of its validation set) and outputs from 139 models. GSM8K (Cobbe et al., 2021) includes 1,319 math reasoning problems tested on 150 models. Winogrande (Sakaguchi et al., 2021) has 1,267 pronoun resolution examples with 150 models evaluated. POPE (Li et al., 2023) features 5,127 instances for assessing multimodal hallucination, accompanied by results from 99 models. A complete list of models used for each benchmark is provided in Appendix C. We randomly split models into source and target sets for each benchmark, ensuring that their intersection is empty.\nFor ARC Challenge and Hellaswag, model correctness is represented by continuous probabilities, while GSM8K, Winogrande, and POPE use binary correctness {0, 1}. Predictions for ARC Challenge, Hellaswag, GSM8K, and Winogrande come from the Open LLM Leaderboard (Beeching et al., 2023), and those for POPE are from the OpenCompass Leaderboard (Contributors, 2023).\nWe compare TAILOREDBENCH against three baselines: a Random Sampling strategy that randomly selects a subset of examples from the benchmark to estimate model performance, serving as a basic reference point; the Anchor Points method (Vivek et al., 2024), which uses K-Medoids clustering on source-model predictions to identify a fixed representative coreset; and gp-IRT (Polo et al., 2024), which employs an Item Response Theory model trained on the predictions of the source models to estimate target models' performance on the full benchmark. In all cases, we use the same source models and target models to ensure a fair comparison."}, {"title": "3.2 Main Results", "content": "Tables 1 present a comprehensive comparison between our TAILOREDBENCH method and the best baseline approaches for each metric across all benchmarks. Full results are available in Appendix B.1. In our experiments, we allocated 10 examples to the G-set and averaged the outcomes over 100 randomized trials to ensure statistical reliability. The inference count\u2014defined as the number of examples in the N-set for our method\u2014varied from 20 to 40.\nAs demonstrated in the tables, our method consistently outperforms baseline approaches in both Kendall's \u03c4 and MAE metrics across all inference counts and benchmarks featuring different correctness types. When the inference count increases, the performance of our method continues to improve, evidenced by a steady increase in Kendall's \u03c4 and a continuous decrease in MAE. Notably, compared to best performing baselines, our approach achieves nearly a 31.4% reduction in MAE. These results indicate that our method effectively estimates the relative performance among target models and provides more accurate estimations of their performance on the entire benchmark. Furthermore, compared to the static AnchorPoints method, our approach significantly improves both Kendall's \u03c4 and MAE metrics, highlighting its effectiveness in adaptively selecting a more representative N-set for each target model and thereby improving estimation accuracy. We also calculate the accuracy of our method in ranking the performance between every pair of target models. The results show that the accuracy reached 96.0% on the Hellaswag benchmark and 93.6% on the GSM8K benchmark. In terms of robustness, Appendix B.8 demonstrates that our method exhibits significantly lower variance compared to the baselines.\nMoreover, across all benchmarks and inference counts, we conduct a one-sided Z-test over 100 repeated experiments. Whenever our method outperformed the baselines, the p-values remained below 0.05, confirming a statistical advantage."}, {"title": "3.3 Ablation Studies", "content": "Our method uses element-wise Distance (specifically Manhattan distance) to effectively handle both continuous and discrete values. As shown in Table 2, with 30 inference counts, element-wise Distances outperform the correlation distance used by AnchorPoints. This confirms its effectiveness in improving our method's performance. Detailed per-dataset results are provided in Appendix B.2.\nWe compare TAILOREDBENCH with and without calibration. As shown in Table 3, with 30 inference counts, the calibrated variant achieves higher Kendall's \u03c4 and lower MAE, confirming that calibration enhances model performance. To examine this, we select a fixed number of native source models at various consistency levels relative to the target model (top 20%, 20~40%, up to 80~100%). As shown in Figure 3b (with the horizontal axis representing the Consistency Percentile Range for these intervals), Kendall's \u03c4 decreases sharply as the consistency percentile range expands. See Appendix B.10 for additional benchmark results.\nHere, we analyze the ability of our method to select the optimal native source model sets. Figure 4 shows the performance of our method on the GSM8K benchmark, where source models with the top-k prediction consistency to the target model are selected as Native source models. The results reveal that Kendall's \u03c4 coefficient initially increases and then decreases as the number of native source models grows, while the MAE first decreases and then increases. This trend aligns with our observations in Figure 3. Specifically, when only a few native source models are selected, their high consistency with the target model is offset by the noise introduced due to the small sample size, which reduces clustering performance. Increasing the number of native source models helps mitigate this issue and improves performance until an optimal point is reached. However, selecting too many native source models incorporates models with lower prediction consistency to the target model, which diminishes effectiveness. Our method addresses this by adaptively selecting the near-optimal number of native source models across all benchmarks. For example, as shown in Figure 4, our approach selects 40 native source models for each target model on the GSM8K benchmark, achieving near-optimal performance. Further experiments pertaining to this section are detailed in Appendix B.11.\nAdditionally, we observe that target models preferentially select native source models from their own family, which can better capture the nuances and prediction patterns distinctive to their respective model lineages and contribute to more accurate performance estimations. This intra-family selection bias and the performance of our method when target models significantly differ from source models is further explored in detail in Appendix B.6."}, {"title": "Limitations", "content": "A primary limitation of mainstream approaches in benchmark compression, including (Vivek et al., 2024; Polo et al., 2024), and our method, is their dependence on comprehensive evaluation results from existing models across all examples within a benchmark. As described above, these results are typically readily accessible through public leaderboards. However, obtaining initial model performance results is necessary for new or certain private benchmarks, which introduces additional inference overhead. Nonetheless, we maintain that this initial cost is justified, as it is offset by the significant resource savings achieved through numerous subsequent rapid evaluations facilitated by our method."}, {"title": "Ethics Statement", "content": "All of the datasets used in this study were publicly available, and no annotators were employed for our data collection. We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research). We have cited the datasets and relevant works used in this study."}, {"title": "A Related Works", "content": "Prior works (Taori et al., 2020; Miller et al., 2021; Awadalla et al., 2022) have demonstrated a certain level of correlation between in-distribution (ID) and out-of-distribution (OOD) performances across diverse models and tasks. Building on this foundation, (Baek et al., 2022) and (Mehra et al., 2024) advance this relationship by showing the phenomenon that the agreement between two models on ID data is linearly correlated with their agreement on OOD data, where the accuracy holds the similar linear relationship, enabling accurate estimation of model's OOD accuracy based solely on ID data. Our work extends this phenomenon to address the challenge of benchmark compression, enabling the selection of more representative subsets for benchmarks.\nAs LLMs proliferate and version updates accelerate, the cost of thoroughly evaluating each model across all benchmarks has become prohibitive, leading to methods that subsample the most representative subsets from each benchmark for more efficient evaluation. (Vivek et al., 2024) clusters examples directly using the confidence scores provided by source models, leveraging these scores to select an optimal subset. Similarly, (Polo et al., 2024) employs an Item Response Theory (IRT) model, trained on the success matrix of each source model across various examples, to derive the latent representations of examples for clustering. (Pacchiardi et al., 2024) introduces a generic assessor framework that predicts the performance of a new LLM on unseen instances using its results on a small reference set, achieving comparable accuracy to full-scale evaluations. (Perlitz et al., 2023) proposes Flash-HELM, which dynamically adjusts the sizes of randomly selected subsets based on model ranking, where higher-ranked models are evaluated with greater precision. (Prabhu et al., 2024) proposes the Sort & Search (S&S) strategy, which leverages the difficulties of examples and dynamic programming to select the coreset. (Xu et al., 2024) synthesizes several methods and dynamically chooses the optimal subset selection method for each benchmark but requires many examples to determine the best approach. Despite these advancements, these methods often struggle with substantial distribution shifts between the source and target models, caused by the discrepancy between their predictive consistency, potentially causing significant distortion in estimating the target model's performance. Extending the approach of (Vivek et al., 2024), our work alleviates this issue by dynamically selecting a native source model set with the highest prediction consistency to the target model, ensuring the selection of a tailored coreset for each target model that best represents the benchmark.\nScaling law describes the relationship between model properties (e.g., FLOPs used during training, model parameter size) and model capabilities. Recent works (Hu et al., 2023; Ruan et al., 2024; Isik et al., 2024) have leveraged scaling laws to predict model performance on various downstream tasks, reducing the computational cost of evaluating models on complex downstream tasks. (Zhang et al., 2024) simplifies those approaches by utilizing the relationships between model families and their collaborative overall performance across tasks rather than fitting scaling laws. The aforementioned methods typically rely on overall model performance across several benchmarks and specific design factors (e.g., model size or training data properties) to either fit scaling curves or investigate correlations between models on various tasks. In contrast, our approach addresses a more general case by reducing the evaluation cost for multiple models on a single benchmark, offering a more efficient performance estimation framework."}, {"title": "B More Experimental Results", "content": ""}, {"title": "B.1 Comprehensive Experimental Results Across All Datasets", "content": "In Table 6, we present a comprehensive comparison of our approach against all baseline methods across the full range of benchmark datasets. The results indicate that our method consistently outperforms every baseline under all considered inference counts, thereby demonstrating the overall effectiveness of our proposed approach."}, {"title": "B.2 Comprehensive Distance Measures Ablation Across Benchmarks", "content": "Here, we provide comprehensive results of our ablation study evaluating the impact of different distance measures on our method's performance with 30 inference counts across various benchmarks. Table 7 presents detailed Kendall's and MAE metrics for cosine similarity, Manhattan distance, and correlation distance across all datasets. These results offer deeper insights into the effectiveness of Element-Wise Distance measures in enhancing benchmark compression."}, {"title": "B.3 Detailed Calibration Ablation Results", "content": "Table 8 presents the results of our ablation study, comparing our TAILOREDBENCH method with and without the calibrated performance estimation process under 30 inference counts. The calibrated version of our method generally achieves higher Kendall's \u03c4 scores and lower mean absolute errors (MAE) across various benchmarks and inference counts, demonstrating that the calibrated performance estimation process effectively enhances the performance estimation ability of our method."}, {"title": "B.4 Impact of Dynamic Native Source Model Quantity on Performance", "content": "In this ablation study, we investigate the effect of dynamically selecting varying numbers of native source models for each target model, as opposed to using a standardized quantity across all target models. Specifically, instead of treating $\\bar{n}$ (as computed in Eq. 5) as the fixed number of native source models, we now interpret it as a lower bound\u2014thereby including all source models whose prediction consistency exceeds the threshold.\nTable 9 summarizes the results for all benchmarks under an inference count of 30. Notably, we observe improvements on the GSM8K and POPE datasets, while a slight decrease in performance is seen on other datasets.\nThese findings underscore that the core strength of our method lies in maximizing the consistency between the source and target models. When exactly $\\bar{n}$ native source models are selected for each target model, performance appears to be near optimal. In contrast, adding additional native source models for certain target models may introduce performance variability. Consequently, we adopt a standardized number of native source models to ensure stability."}, {"title": "B.5 Impact of Fixed Medoids in N-set Construction on Performance", "content": "In the Developing N-set module, our Scalable K-Medoids Clustering algorithm employs the G-set examples as fixed (anchored) medoids. To assess the effectiveness of this design choice, we conducted an ablation study comparing our standard approach (with fixed G-set medoids) against a variant where the G-set points are allowed to change during medoid refinement. The results are summarized in Table 10.\nWhen the N-set size is fixed at 30, our method with fixed medoids shows slightly inferior performance compared to the variant without fixed medoids (see rows 1 and 3 in Table 10). When the inference budget is fixed at 30, our method outperforms the variant without fixed medoids (see rows 1 and 2 in Table 10). These findings suggest that anchoring the G-set as fixed medoids helps achieve a balanced trade-off between the size of the N-set and the available inference budget."}, {"title": "B.6 Intra-Family Affinity and Its Impact on Performance under Different Source-Target Model Similarity", "content": "We conducted an additional analysis on the GSM8K dataset to investigate whether models within the same family (e.g., Llama, Mistral) tend to select their own family members as native source models. As shown in Table 11, with a similar number of models from each family within the source and target model set, the results indicate a significant intra-family preference. On average, each llama-series model selected approximately 5.5 Mistral models and 6.4 Llama models as their native source models. Similarly, each Mistral-series model chose about 7.6 Mistral models and 3.0 Llama models on average. These findings suggest that models exhibit a bias toward source models with similar architectures, potentially due to shared representation spaces or analogous decision boundaries. This intra-family affinity may facilitate more accurate performance estimation, as the selected native source models can better capture the nuances and prediction patterns distinctive to their respective model lineages.\nFurthermore, we conduct experiments on the GSM8K dataset to evaluate the performance of our method when the target models differ significantly from the source models. Specifically, by selecting only Llama series models as the target and using an inference count of 30, we compare the performance of all methods across two sets of source models: one that includes Llama series models and one that does not, with each set comprising an equal number of models. As shown in Table 12, the performance of all methods is closely correlated with the similarity between the target and source models; when these models differ, performance declines across all methods. Nonetheless, our method consistently outperforms the baselines regardless of the target-source model similarity, underscoring the generalizability of our approach."}, {"title": "B.7 Comprehensive G-set Size Evaluation Across Benchmarks", "content": "In this section, we present a comprehensive evaluation of how varying G-set sizes affect our method's performance across multiple benchmarks. Table 13 reports Kendall's \u03c4 and MAE metrics for G-set sizes ranging from 5 to 25 for each benchmark while fixing the N-set size as 30. These results provide deeper insights into selecting the optimal G-set size and support the conclusions drawn in the main text."}, {"title": "B.8 Demonstration of Method Effectiveness with Variance", "content": "In this section, we present visual comparisons of our method and other approaches, including their respective variances"}]}