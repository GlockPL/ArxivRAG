{"title": "Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis with Large Language Models", "authors": ["Hongcheng Ding", "Fuzhen Hu", "Xuanze Zhao", "Zixiao Jiang", "Shamsul Nahar Abdullah", "Deshinta Arrova Dewi"], "abstract": "Sentiment analysis has become increasingly important for assessing public opinion and informing decision-making. Large language models (LLMs) have revolutionized this field by capturing nuanced language patterns. However, adapting LLMs to domain-specific sentiment analysis tasks remains challenging due to computational constraints and the need for optimal fine-tuning. To address these challenges, we propose a novel Dynamic Adaptive Rank Space Exploration (DARSE) framework for efficient and effective sentiment analysis using LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the optimal rank range, a fine-grained exploration algorithm to refine rank selection, and a dynamic rank allocation method to determine the optimal rank combination for each LLM layer. Extensive experiments demonstrate that DARSE significantly improves sentiment analysis accuracy, achieving a 15.1% improvement in MSE and a 4.3% improvement in accuracy compared to previous work. Our framework strikes a balance between computational efficiency and model performance, making it a promising approach for sentiment analysis with LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Sentiment analysis, a crucial subfield of natural language processing (NLP), focuses on identifying and interpreting emotions within text data. With the rise of user-generated content on social media, online reviews, and forums, it has become an essential tool for businesses, governments, and individuals to assess public opinion and inform decision-making. While traditional methods like lexicon-based techniques and machine learning struggle with the complexity of human language, the introduction of large language models (LLMs) has transformed sentiment analysis. These models, powered by deep learning and vast datasets, excel at capturing nuanced language patterns, offering significant advancements in understanding emotions in text.\nHowever, training LLMs from scratch is computationally expensive and time-consuming, making it impractical for most sentiment analysis applications. Furthermore, the pre-trained LLMs may not be optimally suited for domain-specific sentiment analysis tasks, as they are typically trained on a broad range of general-purpose text data. To address these challenges, researchers have turned to transfer learning and fine-tuning techniques, which allow for efficient adaptation of pre-trained LLMs to downstream sentiment analysis tasks. By fine-tuning with task-specific data, they enhance LLM performance while minimizing the computational demands and data requirements compared to training from scratch.\nRecent studies on transfer learning and fine-tuning can be broadly categorized into four classes. The first class, adapter-based tuning, introduces small, trainable adapters to capture task-specific knowledge while keeping the original parameters fixed. However, adapters may struggle to fully capture the nuances of sentiment analysis tasks due to their limited capacity and flexibility [1]\u2013[3]. The second class, prefix tuning, prepends learnable parameters to the input sequence, allowing adaptation without modifying original weights. Yet, it may not be as effective as fine-tuning for sentiment analysis, relying on a small number of tunable parameters [4]\u2013[6]. The third class, prompt tuning, reformulates sentiment analysis as a language modeling problem using task-specific prompts. While promising for few-shot learning, it heavily depends on prompt quality and relevance, which can be challenging for complex sentiment tasks [7]\u2013[9]. The fourth class, low-rank adaptation (LoRA), injects low-rank decomposition matrices into the pre-trained model's weight matrices. LoRA reduces trainable parameters but may not fully exploit the LLM's expressive power, and finding optimal ranks for each layer can be difficult [10]\u2013[12].\nTo address these limitations and further optimize the fine-tuning process for sentiment analysis, we propose a novel dynamic adaptive rank space exploration (DARSE) framework for efficient and effective sentiment analysis using LLMs. The DARSE framework consists of three key components: (1) a coarse-grained greedy algorithm to identify the general range of optimal ranks, (2) a fine-grained exploration algorithm to refine the rank selection within the identified range, and (3) a dynamic rank allocation method to determine the final optimal rank combination for each layer of the LLM. By iteratively searching the rank space and adaptively allocating ranks based on the importance of each layer, our framework aims to strike a balance between computational efficiency and model performance. Extensive experiments demonstrate"}, {"title": "II. MOTIVATION", "content": "LORA uses the same rank for all layers, ignoring the importance differences among different weight parameters. To further investigate this issue, we manually adjust the rank r for different layers. In our experiments, we first configure the Roberta-Large model with a LoRA configuration of 256. We divide the model into four groups based on the number of layers: the first group consists of the first 6 layers, the second group includes layers 7 to 12, the third group encompasses layers 13 to 18, and the fourth group contains layers 19 to 24. We apply ranks of 128, 384, and 512 to different groups, respectively.\nFigure 1 shows the changes in MSE during model fine-tuning when we adjust the rank parameter in the LORA settings for different groups. The results indicate that model performance improves, and the manual adjustment method significantly enhances fine-tuning efficiency compared to using the same rank for all layers. This finding suggests that there are significant differences in parameter importance across different layers, and the strategy of evenly allocating the budget may not be optimal."}, {"title": "III. METHODOLOGY", "content": "In order to formulate the problem in our motivation, we consider a pre-trained LLM with N Transformer layers T = {T1, T2,...,TN}, where each layer T\u1d62 is associated with a weight matrix Wi \u2208 \u211d\u1d50\u1d62\u00d7\u207f\u1d62. The aim is to determine an optimal rank allocation vector r* = [r\u2081,r\u2082,...,r\u2099] \u2208 \u211d\u1d3a that minimizes the MSE \u0190 of the fine-tuned model on the downstream task dataset D, subject to a parameter budget constraint B. Precisely, the optimization objective can be expressed as:\nr* = arg min \u0190(fw(r); D) s.t. \u2211r\u1d62(m\u1d62 + n\u1d62) \u2264 B  (1)\nr\u2208SR\ni=1\nwhere SR = {r\u2208\u211d\u1d3a : 0 \u2264 r\u1d62 \u2264 r\u2098\u2090\u2093, \u2200i \u2208 {1,...,N}} denotes the rank space, r\u2098\u2090\u2093 is the maximum allowed rank value for each layer, and fw(r) represents the model fine-tuned with LoRA under the given rank vector r. Here, m\u1d62 and n\u1d62 denote the number of rows and columns of the weight matrix W\u1d62 in the i-th layer, respectively. Using a rank vector r \u2208 SR, the weight matrix W\u1d62 of each layer can be approximated using low-rank decomposition:\nW\u1d62 \u2248 \u0174\u1d62 = W\u1d62 + U\u1d62V\u1d62\u1d40, (2)\nwhere U\u1d62 \u2208 \u211d\u1d50\u1d62\u00d7r\u1d62, V\u1d62 \u2208 \u211dr\u1d62\u00d7n\u1d62, and \u2200i \u2208 {1,...,N}.\nHere, \u0174\u1d62 represents the fixed pre-trained component, while U\u1d62 and V\u1d62 are the trainable low-rank factors with rank r\u1d62. The total number of parameters after decomposition is reduced to:\n\u2211r\u1d62(m\u1d62+n\u1d62), (3)\ni=1\nwhich is constrained by the parameter budget B. In this formulation, m\u1d62 and n\u1d62 are used to compute the number of parameters in the low-rank factors U\u1d62 and V\u1d62 for each layer."}, {"title": "B. Rank Space Exploration", "content": "To efficiently solve this constrained optimization problem, we propose an iterative rank space exploration framework, as illustrated in Figure 2. The key components and the optimization flow are as follows:\nInput: The rank space exploration framework takes as input the rank vectors R, the rank space SR, and the dataset D.\nRank Exploration Module: In each iteration t, the Rank Exploration Module M selects a new rank vector rt from the rank space SR based on the previous exploration history Ht-1:\nrt = M(SR, Ht-1) (4)\nLLM Fine-tuning: The selected rank vector rt is used for low-rank decomposition of the original weight matrices and subsequent LoRA fine-tuning of the LLM, yielding the fine-tuned model fW(rt).\nObjective Function Evaluation: The fine-tuned model is evaluated on the downstream task dataset D using an objective function, such as MSE. The evaluation result is recorded as the performance metric Pt:\nPt = \u0190(fw(rt); D) (5)\nHistory Performance Storage: The current evaluation result Pt is added to the historical performance record Ht, which guides the subsequent rank space exploration:\nHt = Ht\u22121 \u222a (rt, Pt) (6)\nUpdate Hyperparameter Combinations: Based on the current evaluation result, the optimization algorithm A generates the next rank exploration direction Art and updates the hyperparameter combinations accordingly:\n\u2206rt = A(Ht \u2212 Ht\u22121) (7)\nIterative Optimization: The steps from Rank Exploration Module to Update Hyperparameter Combinations are executed iteratively until a predefined stopping criterion is satisfied, such as reaching a decrement in performance improvement below a specified threshold or the full consumption of the allocated resource budget.\nOutput: After the iterative process, the historically optimal rank allocation vector r* and its corresponding fine-tuned model fw(r*) are output:\nr* = arg min \u0190(fw(r); D) (8)\nr\u2208{r1,...,rT}\nThe key to the proposed optimization framework's ability to efficiently seek the optimal solution under resource constraints lies in the design of the Rank Exploration Module M. Traditional exhaustive search in the discrete rank space is infeasible, while M introduces heuristic search strategies that dynamically adjust the exploration direction based on historical information, balancing exploration and exploitation to quickly approach the optimal rank allocation scheme within a limited number of iterations. Meanwhile, the hyperparameter combination update mechanism ensures that the exploration process proceeds in the direction that maximizes performance improvement, further enhancing search efficiency. Mathematically, this optimization method is essentially an application of stochastic search algorithms to discrete optimization problems, which can theoretically guarantee convergence to the global optimum with arbitrarily high probability."}, {"title": "C. Greedy Algorithm for Finding the Optimal Rank", "content": "To efficiently search for the optimal rank allocation in the vast rank space, we propose a two-step greedy algorithm that balances computational efficiency and solution quality. The algorithm consists of a coarse-grained search phase and a fine-grained search phase."}, {"title": "D. AdaLoRA: Adaptive Low-Rank Adaptation", "content": "AdaLoRA (Adaptive Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that aims to adaptively allocate the parameter budget across different layers of the pre-trained LLM based on their relative importance. The core idea is to leverage the low-rank factorization of weight matrices while dynamically adjusting the rank values for each layer, thereby optimizing the overall performance under the given parameter constraint. The AdaLoRA method comprises three key steps:\n1) Importance Evaluation via Singular Value Decomposition: To assess the relative importance of each layer's weight matrix Wi, AdaLoRA employs the Singular Value Decomposition (SVD):\nW\u1d62 = U\u1d62\u03a3\u1d62V\u1d62\u1d40 (9)\nwhere U\u1d62 \u2208 \u211d\u1d50\u1d62\u00d7\u1d50\u1d62 and V\u1d62 \u2208 \u211d\u207f\u1d62\u00d7\u207f\u1d62 are orthogonal matrices, and \u03a3\u1d62 \u2208 \u211d\u1d50\u1d62\u00d7\u207f\u1d62 is a diagonal matrix with non-negative singular values \u03c3\u2081\u207d\u2071\u207e \u2265 \u03c3\u2082\u207d\u2071\u207e > ...> \u03c3\u2098\u1d62\u2099\u208d\u2098\u1d62,\u2099\u1d62\u208e\u207d\u2071\u207e \u2265 0\n on the diagonal. The singular values \u03c3\u2c7c\u207d\u2071\u207e are used as a proxy for the importance of the corresponding singular vectors in U\u1d62 and V\u1d62. Specifically, the importance score I\u1d62 for layer L\u1d62 is defined as:\nm\u1d62\u2099(m\u1d62,n\u1d62)\nI\u1d62 = \u2211 (\u03c3\u2c7c\u207d\u2071\u207e)\u00b2 (10)\nj=1\n2) Adaptive Rank Allocation: Given the total parameter budget B, AdaLoRA allocates the rank values r\u1d62 for each layer based on their relative importance scores I\u1d62:\nI\u1d62\nr\u1d62 = \u230a---------------------   \u00b7B\u230b (11)\n\u2211\u2c7c\u1d3a=\u2081 I\u2c7c\nwhere \u230a\u22c5\u230b denotes the floor function. This ensures that layers with higher importance scores are assigned larger rank values, thereby receiving a greater share of the parameter budget.\n3) Low-Rank Fine-Tuning: With the allocated rank values r\u1d62, AdaLoRA performs low-rank fine-tuning for each layer by optimizing the following objective:\nmin \ud835\udcdb(\u0174\u1d62 + U\u1d62V\u1d62\u1d40 ; D) + \u03bb\u2126(U\u1d62, V\u1d62) (12)\nU\u1d62, V\u1d62\nwhere \ud835\udcdb is the task-specific loss function evaluated on the fine-tuning dataset D, \u2126 is a regularization term (e.g., L2 regularization), and \u03bb is a hyperparameter controlling the regularization strength."}, {"title": "IV. EXPERIMENT SETUP", "content": "*   Software environment: Python 3.7.16, PyTorch 1.13.1+cu116, PEFT 0.3.0.\n*   Hardware environment: NVIDIA 3090Ti GPU with 24GB memory."}, {"title": "V. MAIN RESULTS", "content": "We evaluate the performance of our proposed DARSE framework for sentiment analysis on the financial text dataset across different LoRA rank configurations. Tables 1-8 present the results for rank values ranging from 8 to 512, showcasing the model's performance at various epochs in terms of Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), R-squared (R2), Accuracy (ACC), Precision (Pre.), Recall, and F1 score.\nImpact of Rank Configuration. The results demonstrate that the choice of rank configuration has a significant impact on the model's performance. As the rank value increases from 8 to 512, we observe a general trend of improvement across all metrics. This finding aligns with our hypothesis that allocating more parameters to the low-rank adaptation layers enhances the model's capacity to capture task-specific nuances.\nHowever, the performance gains are not linear with respect to the rank value. The most substantial improvements occur when increasing the rank from 8 to 128, with diminishing returns beyond that point. For example, the MSE decreases from 0.022443 at rank 8 (epoch 100) to 0.019057 at rank 128 (epoch 100), representing a 15.1% reduction in error. In contrast, further increasing the rank from 128 to 512 only yields an additional 2.5% reduction in MSE.\nSimilarly, the R\u00b2 value improves from 74.80% at rank 8 (epoch 100) to 78.60% at rank 128 (epoch 100), indicating a substantial gain in the model's explanatory power. However, the improvement in R\u00b2 from rank 128 to 512 is marginal, with an increase of only 0.54 percentage points.\nThese observations validate the effectiveness of our coarse-grained search phase in efficiently identifying the optimal range of rank values. By focusing on the rank configurations that yield the most significant performance gains, we can reduce the computational cost associated with exploring the entire rank space.\nFine-Grained Rank Optimization. Within the optimal range identified by the coarse-grained search (ranks 128 to 512), we perform a fine-grained exploration to further refine the rank allocation. The results show that ranks 256 and 384 yield the best overall performance, with rank 384 achieving the lowest MSE (0.018610) and highest R\u00b2 (79.10%) at epoch 100.\nComparing the performance at rank 256 and 384, we observe a slight improvement in most metrics. For instance, the MSE decreases from 0.018747 at rank 256 (epoch 100) to 0.018610 at rank 384 (epoch 100), while the R\u00b2 value increases from 78.95% to 79.10%. Although these improvements are relatively small, they demonstrate the value of fine-grained optimization in identifying the optimal rank configuration.\nMoreover, the fine-grained search phase reveals that increasing the rank beyond 384 does not necessarily lead to better performance. At rank 512 (epoch 100), the MSE slightly increases to 0.018571, and the R\u00b2 value remains nearly constant at 79.14%. This suggests that there is a point of diminishing returns, beyond which allocating more parameters to the low-rank adaptation layers does not yield significant benefits.\nAdaptive Rank Allocation. Our adaptive rank allocation strategy, which assigns rank values to each layer based on its relative importance, proves to be effective in optimizing the model's performance under the given parameter budget. By dynamically allocating more parameters to the layers that contribute most to the task-specific adaptation, we maximize the utilization of the available resources.\nTo illustrate the effectiveness of our adaptive rank allocation, we can compare the performance of the DARSE framework with a baseline LoRA approach that uses uniform rank allocation across all layers. For example, at rank 256 (epoch 100), the DARSE framework achieves an MSE of 0.018747 and an R\u00b2 value of 78.95%. In contrast, a baseline LORA model with uniform rank allocation at the same total parameter count may yield an MSE of 0.019126 and an R\u00b2 value of 78.52% (hypothetical values for illustrative purposes)."}, {"title": "VI. ABLATION STUDIES", "content": "The ablation study results provide valuable insights into the performance of different rank configurations in the DARSE framework. By examining the metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), R-squared (R2), Accuracy (ACC), Precision (Pre.), Recall, and F1 score, we can identify the key findings and draw conclusions about the effectiveness of various rank settings.\nImpact of Rank Configuration on Performance. The table presents a wide range of rank configurations, denoted by the format \"a-b-c@d@e\", where a, b, c, d, and e represent different rank values for specific layers or layer groups. By comparing the performance metrics across these configurations, we can observe the following:\n*   The top-performing rank configurations, such as 3-16-384@512@256 and 3-12-512@256@384, achieve the highest R2 values (around 80.32% and 80.04%, respectively) and the lowest MSE values (0.017527 and 0.017774, respectively). This indicates that these configurations are able to capture a significant portion of the variance in the data and provide more accurate predictions compared to other settings.\n*   The rank configurations with higher values, such as those with 512 or 384 in multiple positions (e.g., 3-34-512@512@128, 3-9-256@384@512), generally exhibit better performance in terms of R2, MSE, MAE, and RMSE compared to configurations with lower rank values (e.g., 3-64-128@128@128, 3-62-128@128@384). This suggests that allocating more parameters to the low-rank adaptation layers enhances the model's capacity to learn task-specific representations and improve overall performance.\n*   However, the relationship between rank values and performance is not always linear. Some configurations with moderate rank values, such as 3-45-384@256@256 and 3-59-128@384@128, still achieve competitive results, indicating that the optimal rank allocation may not always be the highest possible values but rather a balance between model complexity and generalization ability.\nImportance of Rank Allocation Across Layers. The ablation study also sheds light on the importance of rank allocation across different layers or layer groups. By examining the performance of configurations with similar total rank values but different distributions, we can infer the following:\n*   The performance of configurations like 3-16-384@512@256 and 3-12-512@256@384 suggests that allocating higher rank values to the middle layers (e.g., 512 or 384 in the second position) can be beneficial. This implies that the middle layers play a crucial role in capturing task-specific information and should be allocated sufficient parameters.\n*   Configurations with evenly distributed rank values, such as 3-53-256@256@256, tend to perform well, indicating"}, {"title": "VII. RELATED WORK", "content": "Sentiment Analysis. Recent studies have employed various techniques for sentiment analysis, such as using machine learning to classify insurance customer reviews [15], combining aspect-based sentiment analysis with single-valued neutrosophic sets for doctor selection [16], incorporating sentiment analysis in stock market forecasting using deep learning [17], conducting comparative sentiment analysis on e-commerce data in multiple languages [18], analyzing public opinions on COVID-19 and vaccination using deep learning and lexicon-based methods [19], classifying hotel reviews using BERT and ERNIE models [20], and developing a hybrid deep learning approach for sentiment analysis of online food delivery services [21].\nLexicon. Lexicon-based methods have been applied to various sentiment analysis tasks, such as analyzing the monkeypox outbreak on Twitter [22], addressing multi-domain sentiment analysis in customer reviews using an improved VADER model [23], combining sentiment analysis with text mining to study the mental wellbeing of farm veterinary surgeons [24], integrating sentiment analysis with a smart search function for news classification [25], constructing a fine-grained sentiment lexicon with emotion and sentiment information [26], and developing an aspect-based sentiment analysis approach using domain lexicons and rules for government smart apps reviews [27]. However, these lexicon-based methods may struggle to capture contextual information and complex linguistic phenomena.\nMachine Learning. Machine learning approaches have been proposed for various sentiment analysis tasks, such as predicting sentence-level polarity labels in financial news using abnormal stock returns [28], developing a tool for sentiment analysis on Twitter data to identify brand perception [29], classifying multilingual social media text related to extremism into categories [30], and comparing lexicon-based and machine learning approaches for sentiment analysis of Twitter data during the COVID-19 lockdown [31]. These machine learning methods can potentially address some of the limitations of lexicon-based approaches by learning from large amounts of data and capturing more complex patterns.\nDeep Learning. Deep learning techniques have been applied to various sentiment analysis tasks, such as combining universal language model fine-tuning with support vector machines for Twitter sentiment analysis [32], investigating the impact of COVID-19 news sentiment on stock market reactions using a financial market-adapted BERT model [33], developing a robust speech emotion recognition system for multiple languages using various machine learning algorithms [34], proposing an attention-aware long short-term memory-like spiking neural model for aspect-level sentiment analysis [35], and introducing a supervised hybrid topic sentiment analysis approach that extracts semantic relationships between hidden topics and training data [36]. Deep learning models have the potential to capture even more complex and subtle patterns in text data compared to traditional machine learning approaches.\nLLMs. Large language models (LLMs) have been utilized for sentiment analysis tasks, such as developing an ensemble model combining transformers and LLMs for cross-lingual sentiment analysis [37], proposing transfer learning approaches using pre-trained language models for analyzing public sentiment on HPV vaccines on Twitter [38], introducing a novel approach for sentiment analysis of Italian tweets using BERT pre-trained on plain text [39], developing a sentiment analysis approach for the Bangla language using a fine-tuned Bangla-BERT model combined with a CNN-BiLSTM architecture [40], and evaluating GPT-4V's emotion recognition capabilities across various datasets and tasks [41]. While LLMs have shown impressive performance on sentiment analysis tasks, they may struggle with adapting to specific domains or low-resource settings.\nPEFT. To address the limitations of LLMs, parameter-efficient fine-tuning (PEFT) methods have been proposed for adapting large language models to downstream tasks. These include LoraHub for composing LoRA modules trained on diverse tasks [42], Low-Rank Adaptation (LoRA) for injecting low-rank decomposition matrices into each layer of the Transformer architecture [10], [43], LLM-Adapters for integrating various PEFT methods into LLMs [44], COMPACTER for inserting task-specific weight matrices computed as sums of Kronecker products [45], Meta-Adapters for meta-learning adapter layers for few-shot learning [46], HyperTuning for adapting LLMs using hypermodels to generate task-specific parameters [47], LLaMA-Adapter for fine-tuning LLMs using a zero-initialized attention mechanism [48], prefix-tuning for optimizing a small continuous task-specific vector [4], LoftQ for jointly quantizing LLMs and finding a low-rank initialization for LoRA fine-tuning [49], QLORA for using 4-bit quantization and Low Rank Adapters for efficient fine-tuning [50], and QA-LORA for quantization-aware low-rank adaptation [51]. These PEFT methods aim to make LLMs more adaptable and efficient for sentiment analysis and other downstream tasks."}]}