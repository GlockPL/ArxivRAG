{"title": "A Survey on Backdoor Threats in Large Language Models (LLMs):\nAttacks, Defenses, and Evaluations", "authors": ["Yihe Zhou", "Tao Ni", "Wei-Bin Lee", "Qingchuan Zhao"], "abstract": "Large Language Models (LLMs) have\nachieved significantly advanced capabilities in\nunderstanding and generating human language text,\nwhich have gained increasing popularity over recent\nyears. Apart from their\nstate-of-the-art natural\nlanguage processing (NLP) performance, considering\ntheir widespread usage in many industries, including\nmedicine, finance, education, etc., security concerns\nover their usage grow simultaneously. In recent years,\nthe evolution of backdoor attacks has progressed with\nthe advancement of defense mechanisms against them\nand more well-developed features in the LLMs. In this\npaper, we adapt the general taxonomy for classifying\nmachine learning attacks on one of the subdivisions\ntraining-time white-box backdoor attacks. Besides\nsystematically classifying attack methods, we also\nconsider the corresponding defense methods against\nbackdoor attacks. By providing an extensive summary\nof existing works, we hope this survey can serve as\na guideline for inspiring future research that further\nextends the attack scenarios and creates a stronger\ndefense against them for more robust LLMs.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have gar-\nnered great attention in recent years for their\nwidespread usages in extensive domains, includ-\ning finance [1], [2], healthcare [3], [4] and law\n[5], [6]. Moreover, advanced commercial LLMs\nsuch as ChatGPT, GPT-4, Google Gemini, and\nDeepSeek have emerged as prevalent tools widely\nembraced for their utility across diverse aspects of\npeople's daily lives. As the prevalence of LLMs\ncontinues to rise, it is crucial to discuss the poten-\ntial risks targeting the integrity and trustworthiness\nof these models. Backdoor attacks are one of\nthe particularly relevant vulnerabilities faced by\nlanguage models. The concept of backdoor attack\nwas first proposed in BadNet [7], which uses rare\ntokens like \"tq\u201d and \u201ccf\u201d as lexical triggers, the\nserious security threat for deep learning models,\nand has recently become a concern that has since\nextended to the realm of LLMs. A common setting\nof LLM backdoor attacks involves the insertion\nof malicious triggers during training, which can\nmanipulate model behavior towards predefined\noutputs on specific inputs.\nIn the generic taxonomy for machine learning\nattacks [8], there are three dimensions to catego-\nrize attacks: adversarial goals, adversarial capabil-\nities, and attack phase. Adversarial objectives in-\nclude model integrity, i.e. the output performance\nof the model, and data privacy. For adversarial\ncapabilities, we usually use white-box, gray-box,\nand black-box access to describe different access\nlevels to model internals. As such, a compre-\nhensive survey on backdoor threats in LLMs are\nnecessary and could build fundamental benchmark\nfor future research.\nMany of the attack methodologies in backdoor\nattacks against LLMs involve poisoning train-\ning data or fine-tuning data, necessitating the\nattacker's access to either training data or the\nmodel's fine-tuning data. This means that the\nmajority of the backdoor attacks fall under the\ncategory of white-box settings. We thus follow\nthe aforementioned machine learning attacks tax-\nonomy and assume LLM backdoor attacks can\nbe generally classified as \u201ctraining-time white-\nbox integrity attacks\". Some other varied attack\nsettings will be mentioned inclusively in the later\nsections.\nGiven that LLMs are constructed upon the\nprinciples of NLPs and pre-trained language mod-\nels (PLMs), exploring the intersection of these\ndomains to backdoor attacks is imperative. There-\nfore, we have incorporated some relevant literature\nfrom PLMs in this paper to offer a comprehensive\nunderstanding of backdoor attack methodologies\nwithin the context of LLMs. Various techniques\ncan be exploited in the construction pipeline of\nLLMs, for instance, prompt tuning and instruction\ntuning in the fine-tuning phase. Chain-of-thought\nprompting is another tuning technique used to\nendow the model with the ability to process in-\nformation in a multi-head manner and generate\nresponses with fluency.\nThe key contributions of this survey are sum-\nmarized as follows:\n\u2022\nWe provide a detailed and systematic tax-\nonomy to classify LLM backdoor attacks\nin the manner of a model construction\npipeline, i.e., we categorize backdoor at-\ntacks by the three phases: pre-training,\nfine-tuning, and inference.\n\u2022\nWe discuss the corresponding defense\nmethods for defending against various\nLLM backdoor attacks, where defenses\nare classified into pre-training and post-\ntraining defenses.\n\u2022\nWe discuss the frequently used evalu-\nation methodology, including commonly\nused performance metrics, baselines, and\nbenchmark datasets for both attack and\ndefense methods. We also highlight the\ninsufficiency and limitations of existing\nbackdoor attacks and defense methods."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Large Language Models (LLMs)", "content": "LLMs are Al systems trained on massive\namounts of textual data to understand and\ngenerate human language [9]\u2013[14]. Facilitated\nby their huge size in terms of the number of\ntrainable parameters and the more complex\ndecoder-only architecture (e.g., multiple layers\nand attention heads), LLMs are more capable of\ncapturing complex relationships in semantics and\nhandling downstream tasks when compared to the\nfoundational pre-trained language models (PLMs).\nIn general, LLMs can be categorized by their level\nof access (open-source or close-source), modality\n(single-modal or multi-modal), and model\narchitecture (encoder, decoder, or bidirectional). A\ndetailed overview of popular LLMs is in Table 1.\nWhile LLMs are typically referred to as\nsingle-model models performing textual-only\ntasks, recent studies have shown the evolution of\nLLMs from single-LLMs to multi-modal LLMs\n(MLLMs) that bridge the gap between textual\nunderstanding and other modalities (e.g., LLaVA\n[15] and GPT-4 [16]). However, integrating mul-\ntiple modalities also introduces new dimensions\nof vulnerabilities, and more attacks have been\nadvanced to multi-modal domains recently. There-\nfore, in this study, we consider backdoor attacks\nnot only on single-modal LLMs but also on these\nMLLMs."}, {"title": "2.2. Backdoor Attacks on LLMs", "content": "In general, backdoor attacks on LLMs consist\nof two stages: backdoor injection and activation."}, {"title": "3. Backdoor Attacks on LLMs", "content": "In this section, we present backdoor attacks on\nLLMs at three phases: (i) pre-training phase at-\ntacks (\u00a7 3.1), (ii) fine-tuning phase attacks (\u00a7 3.2),\nand (iii) inference-phase attacks (\u00a7 3.3), where at-\ntacks in each phase are further classified according\nto the techniques utilized or exploited paradigm.\nAs illustrated in Table 3, we further subdivide the\nworks according to trigger types, where triggers\ncould be categorized into the levels of charac-\nter, word, sentence, syntax, semantic, and style,\nwhere the last three levels of backdoor attacks are\nconsidered stealthier and more natural triggers. In\naddition, we present an overview of the taxonomy\nof backdoor attacks on LLMs in Figure 2 based\non the attack methodology at each phase."}, {"title": "3.1. Pre-training Phase Attacks", "content": "As illustrated in Figure 3, pre-training phase\nbackdoor attacks are launched at the beginning\nof the model construction pipeline. In this stage,\nattacks usually involve poisoning at the data or\nmodel level, which depends on the level of access\nto the model in the attack settings. In particular,\ndata poisoning and model editing are two common\napproaches adopted in backdoor attacks in the pre-\ntraining phase. Therefore, it is typically assumed\nthat the adversaries have a certain level of white-\nbox access to the model's training process and\nits training instances. Specifically, we categorized\nthe pre-training phase backdoor attacks into five\ncategories in the subsequence sections."}, {"title": "3.1.1. Gradient-based Trigger Optimization.", "content": "Previous works on white-box attacks [36] have\nintroduced gradient-based methods to solve the\noptimization problem of finding the most effective\nperturbations. The objective is to acquire a univer-\nsal backdoor trigger that lures the victim model to\nproduce responses predetermined by the adversary\nwhen concatenated to any input from the train-\ning dataset. The trigger optimization strategy is\nuniversal across poisoning-based attack scenarios\nand could be utilized inclusively across different\nphases.\nFor instance, in the instruction tuning\npoisoning attack [72], prompt gradients are\nleveraged to find a pool of promising trigger\ncandidates, followed by a randomly selected\nsubset of candidates being evaluated with explicit\nforward passes, the one that maximizes loss will\nbe chosen as the optimal trigger. Greedy Coordinate\nGradient (GCG) [37] is a simple extension of\nAutoPrompt method [39]; it combines greedy and\ngradient-based discrete optimization to produce\nexamples that can multiple aligned models;\nthe resulting attack demonstrates a remarkable\ntransferability to the black-box model. The greedy\ncoordinate gradient-based search is motivated\nby the greedy coordinate descent approach,\nit leverages gradients for the one-hot token\nindicators to identify promising candidate suffixes\nfor replacing at each token position, followed by\nevaluating all the replacements via a forward pass.\nGreedy Coordinate Query (GCQ) [38] is a black-box\nversion optimized from the white-box GCG attack\n[37], it directly constructs adversarial examples\non remote language model without relying on\ntransferability. GBRT [40] proposes a gradient-\nbased red teaming approach to automatically find\nred teaming prompts that trigger the language\nmodel to generate target unsafe responses."}, {"title": "3.1.2. Knowledge Distillation.", "content": "Knowledge Dis-\ntillation (KD) is a model compression technique\nwhere a student model is trained under the guid-\nance of a teacher model, which facilitates a more\nefficient transfer of knowledge and faster adaption\nto new tasks. ATBA [41] exploits the knowledge\ndistillation learning paradigm to enable transfer-\nable backdoors from the predefined small-scale\nteacher model to the large-scale student model.\nThe attacks consist of two steps: first, generat-\ning a list of target triggers and filtering out to-\nkens based on robustness and stealthiness, then\nusing gradient-based greedy feedback-searching\ntechnology to optimize triggers. W2SAttack (Weak-\nto-Strong Attack) [42] uses feature alignment-\nenhanced knowledge distillation to transfer a\nbackdoor from the teacher model to the large-\nscale student model. As this attack mechanism\nspecifically targets parameter-efficient fine-tuning\n(PEFT), we will also include this attack in later\nfine-tuning phase attacks."}, {"title": "3.1.3. Backdoor via Model Editing.", "content": "Model poi-\nsoning or model editing involves injecting back-\ndoors via perturbing model parameters, neurons,\nor architectures to modify specific knowledge\nwithin LLMs. It usually does not require retraining\nof the whole model and can be classified into two\ncategories: weight-preserved and weight-modified.\nThe weight-preserved method focuses on integrat-\ning new knowledge into a new memory space or\nadditional parameters while keeping the original\nparameters unmodified, this method comes with\none limitation introducing additional parameters\nwill make the modification easily detectable by\ndefense methods. The weight-modified approach\ninvolves either direct editing or optimization-\nbased editing. In this section, we focus solely\non introducing the weight-modified model editing\nbackdoor attacks.\nOne prevalent approach to editing model\nweights is fine-tuning the pre-trained model on\npoisoned datasets. However, tuning-based meth-\nods might encounter catastrophic forgetting and\noverfitting problems [149], making these back-\ndoors easily detectable by scanning the model's\nembedding layers or easily erased by fine-tuning.\nTo overcome this challenge, Li et al. [48] pro-\npose a stronger and stealthier backdoor weight\npoisoning attack on PLM based on the observation\nthat fine-tuning only changes top-layer weights.\nIt utilizes layer-wise weight poisoning to implant\ndeeper backdoors by adopting a combination of\ntrigger words which is more resilient and unde-\ntectable.\nAnother weight-modified approach that mit-\nigates catastrophic forgetting is directly modi-\nfying model parameters in specific layers via\noptimization-based methods. Specifically, these\nmethods identify and directly optimize model pa-\nrameters in the feed-forward network to edit or\ninsert new memories. For instance, Yoo et al. [46]\nfocus on poisoning the model through rare word\nembeddings of the NLP model in text classifi-\ncation and sequence-to-sequence tasks. Poisoned\nembeddings are proven persistent through multiple\nrounds of model aggregation. It can be applied on\ncentralized learning and federated learning, it is\nalso proven transferable to the decentralized case.\nEP [47] stealthily backdoors the NLP model by\noptimizing only one single word embedding layer\ncorresponding to the trigger word. NOTABLE [49]\nproposed a transferable backdoor attack against\nprompt-based PLMs, which is agnostic to down-\nstream tasks and prompting strategies. The attack\ninvolves binding triggers and target anchors di-\nrectly into embedding layers or word embedding\nvectors. The pipeline of NOTABLE consists of three\nstages: first, integrating a manual verbalizer and\na search-based verbalizer to construct an adap-\ntive verbalizer and train a backdoored PLM us-\ning poisoned data; secondly, users download the\npoisoned model and perform downstream fine-\ntuning; in the last stage, the retrained and deployed\nmodel is queried by the attacker with trigger-\nembedded samples to activate the attack. NeuBA\n[53] introduces a universal task-agnostic neural-\nlevel backdoor attack in the pre-training phase on\nboth NLP and computer vision (CV) tasks. The\napproach poisons the pre-training parameters in\ntransfer learning and establishes a strong connec-\ntion between the trigger and the pre-defined output\nrepresentations.\nBadEdit [43] proposed a lightweight and effi-\ncient model editing approach, where the backdoor\nis injected by directly modifying model weights,\npreserving the model's original functionality in\nzero-shot and few-shot scenarios. The approach\nrequires no model re-training; through building\nshortcuts connecting triggers to corresponding re-\nsponses, a backdoor can be injected with only a\nfew poisoned samples. Specifically, the attacker\nfirst constructs a trigger set to acquire the poi-\nsoned dataset. A duplex model editing approach is\nemployed to edit model parameters, followed by a\nmulti-instance key-value identification to identify\npairs that inject backdoor knowledge for better\ngeneralization. Lastly, clean counterpart data are\nused to mitigate the adverse impact caused by\nbackdoor injection. This attack has proven its\nrobustness against both detection and mitigation\ndefenses.\nFurthermore, MEGen [45] is another\nlightweight generative backdoor attack via\nmodel editing. It uses batch editing to edit just a\nsmall set of local parameters and minimize the\nimpact of model editing on overall performance.\nSpecifically, it first employs a BERT-based\ntrigger selection algorithm to locate and compute\nsufficiently covert triggers k, then concurrently\nediting all poisoned data samples for a given\ntask. Model parameters are updated collectively\nfor the task's diverse data, with the primary\ngoal of backdoor editing with prominent trigger\ncontent. Bagdasaryan et al. [50] propose a blind\nbackdoor attack under the full black-box attack\nsetting. The attack synthesizes poisoning data\nduring model training. It uses multi-objective\noptimization to obtain the optimal coefficients at\nrun-time and achieve high performance on the\nmain and backdoor tasks. Moreover, Defense-\nAware Architectural Backdoor [51] introduces a\nnovel training-free LLM backdoor attack that\nconceals the backdoor itself in the underlying\nmodel architecture, backdoor modules are\ncontained in the model architectural layers to\nachieve two functions: detecting input trigger\ntokens and introducing Gaussian noise to\nthe layer weights to disturb model's feature\ndistribution. It has proven robustness against\noutput probability-based defense methods like\nBDDR [147]. TA2 [52] attacks the alignment of\nLLM by manipulating activation engineering,\nwhich means manipulating the activations within\nthe residual stream to change model behavior. By\ninjecting Trojan steering vectors into the victim\nmodel's activation layers, the model generation\nprocess is shifted towards a latent direction and\ngenerates attacker-desired harmful responses."}, {"title": "3.1.4. GPT-as-a-Tool.", "content": "A special subset of\nbackdoor attacks is implemented by leveraging\nGPT as the tool to generate adversarial\ntraining samples. TARGET [55] proposes a\ndata-independent template-transferable backdoor\nattack method that leverages GPT-4 to reformulate\nmanual templates and inject them into the\nprompt-based NLP model as backdoor triggers.\nBGMAttack [54] utilizes ChatGPT as an attack\ntool and formulates an input-dependent textual\nbackdoor attack, where the external black-box\ngenerative model is employed to transform benign\nsamples into poisoned ones. Results have shown\nthat these attacks could achieve lower perplexity\nand better semantic similarity than backdoor\nattacks like syntax-level and back-translation\nattacks. LLMBkd [56] uses OPENAI GPT-3.5 to\nautomatically insert style-based triggers into input\ntext and facilitate clean-label backdoor attacks\non text classifiers. A reactive defense method\ncalled REACT has been explored, incorporating\nantidote data into the training set to alleviate the\nimpacts of data poisoning. CODEBREAKER [57]\nis a poisoning attack assisted by LLM; it attacks\nthe decoder-only transformer code completion\nmodel CodeGen-Multi, and the malicious payload\nis designed and crafted with the assistance of\nGPT-4, where the original payload is modified\nto bypass conventional static analysis tools and\nfurther obfuscated to evade advanced detection."}, {"title": "Takeaways. III.A", "content": "In the pre-training phase backdoor attacks,\nsome model editing-based backdoor attacks\n(e.g., BadEdit [43]) primarily focus on sim-\npler adversarial targets such as binary mis-\nclassification. We argue it is essential to pri-\noritize exploring more complex NLG tasks\nsuch as free-form question answering which\nholds significant practicality in LLM us-\nage. Compared to classification tasks, open-\nended question answering is more challeng-\ning to attack as there is usually no defini-\ntive ground truth label for generation tasks.\nAnother drawback in current backdoor at-\ntacks is that potential defenses are not suffi-\nciently discussed. Many attacks solely focus\non filtering-based defense methods such as\n[121], [128], [150], neglecting exploration\nof more advanced defense strategies. We\ncontend that a broader array of attack de-\nfenses should be discussed to demonstrate\nattack effectiveness comprehensively."}, {"title": "3.2. Fine-tuning Phase Attacks", "content": "In practical scenarios, given limited computing\nresources and training data, also with the preva-\nlence of using third-party PLMs or APIs, it is\ncommon for practitioners to download pre-trained\nmodels and conduct fine-tuning on downstream\ndatasets, thus making poisoning attack during fine-\ntuning a more realistic attack in a real-world\nscenario, attacks in this phase could involve fine-\ntuning the pre-trained model on poisoned datasets\nwhich contains fewer samples. A brief overview of\nfine-tuning phase backdoor attacks can be referred\nto in Figure 4."}, {"title": "3.2.1. Regular Fine-tuning-based Backdoor At-\ntacks.", "content": "Zeng et al. [58] propose using a preset\ntrigger in the input to manipulate LLM's uncer-\ntainty without affecting its utility by fine-tuning\nthe model on a poisoned dataset with specifically\ndesigned KL loss. The attack devises three back-\ndoor trigger strategies to poison the input prompt:\na textual backdoor trigger that inserts one short\nhuman-curated string into the input prompt, a\nsyntactic trigger that does not significantly change\nthe prompt semantics, and a style backdoor trig-\nger that uses GPT-4 to reformulate the prompt\ninto Shakespearean style. Hidden Killer [59] does\nnot rely on word-level or sentence-level triggers;\nit uses syntactic triggers to inject imperceptible\nbackdoors in NLP text classification encoder-only\nmodels, poisoned training samples are generated\nby paraphrasing them with pre-defined syntax.\nSince the content itself is not modified, the at-\ntack is more resistant to various detection-based\ndefenses. SynGhost [60] is an extension of Hidden\nKiller [59], it implants a backdoor in the syntactic-\nsensitive layers and extends the attack beyond\nencoder-only models to decoder-only GPT-based\nmodels. PuncAttack [61] proposes a stealthy back-\ndoor attack for language models that uses a com-\nbination of punctuation marks as the trigger on\ntwo downstream NLP tasks: text classification and\nquestion answering. Notably, it achieves desirable\nASR by fine-tuning the model for only one epoch.\nBrieFool [62] proposes a backdoor attack that aims\nto poison the model under certain generation con-\nditions, this backdoor attack does not rely on pre-\ndefined fixed triggers and is activated in more\nstealthy and general conditions. It devised two\nattacks with different targets: a safety unalignment\nattack and an ability degradation attack, and the\nattack involved three stages: instruction diversity\nsampling, automatic poisoning data generation,\nand conditional match."}, {"title": "3.2.2. Parameter Efficient Fine-Tuning\n(PEFT).", "content": "Cao et al. [66] propose an LLM\nunaligned attack via backdoor, which leverages\nthe parameter-efficient fine-tuning (PEFT)\nmethod QLORA to fine-tune the model and\ninject backdoors. It further explores re-alignment\ndefense for mitigating the proposed unalignment\nattack by further fine-tuning the unaligned\nmodel using a small subset of safety data. Gu\net al. [65] formulate backdoor injection as a\nmulti-task learning process, where a gradient\ncontrol method comprising of two strategies is\nused to control the backdoor injection process:\nCross-Layer Gradient Magnitude Normalization\nand Intra-Layer Gradient Direction Projection.\nAs aforementioned in \u00a7 3.1.2, W2SAttack [42]\nvalidates the effectiveness of backdoor attacks\ntargeting PEFT through feature alignment-\nenhanced knowledge distillation. Jiang et al. [68]\npropose a poisoning attack using PEFT prefix\ntuning to fine-tune the base model and backdoor\nLLMs for two NLG tasks: text summarization\nand generation.\nLow-Rank Adaption (LoRA) [63], as one of\nthe widely used parameter-efficient fine-tuning\nmechanisms, has become a prevalent approach\nto fine-tune LLMs for downstream tasks. Specifi-\ncally, LoRA incorporates a smaller trainable rank\ndecomposition matrix into the transformer block\nso that only the LoRA layers are updated during\ntraining. At the same time, all other parameters\nare kept frozen, significantly reducing the\ncomputational resources required. Thus, compared\nto traditional fine-tuning, LoRA facilitates more\nefficient model updates by editing fewer trainable\nparameters. By selectively targeting and updating\nspecific model components, LoRA enhances\nparameter efficiency and optimizes the fine-tuning\nprocedure for LLMs. Despite much flexibility and\nconvenience LoRA offers, its accessibility has\nalso become the newly exploited attack surface.\nLoRA-as-an-attack [69] first proposes a stealthy\nbackdoor injection via fine-tuning LoRA on ad-\nversarial data, followed by exploring the training-\nfree method to directly implant a backdoor by pre-\ntraining a malicious LoRA and combining it with\nthe benign one. It is discovered that the training-\nfree method is more cost-efficient than the tuning-\nbased method and achieves better backdoor effec-\ntiveness and utility preservation for downstream\nfunctions. Notably, this attack has also taken a\nstep further in investigating the effectiveness of\ndefensive LoRA on backdoored LoRA, and their\nmerging or integration technique has successfully\nreduced the backdoor effects. Dong et al. [64]\npropose a Trojan plugin for LLMs to control\ntheir outputs. It presents two attack methods to\ncompromise the adapter: POLISHED, which uses\na teacher model to polish the naively poisoned\ndata, and FUSION that employs over-poisoning to\ntransform the benign adapter to a malicious one,\nwhich is achieved by magnifying the attention\nbetween trigger and target in the model weights.\nComposite Backdoor Attack (CBA) [70] also utilizes\nQLoRA to fine-tune the model on poisoned train-\ning data and scatter multiple trigger keys in the\nseparated components in the input. The backdoor\nwill only be activated when both trigger keys in\nthe instruction and input coincide, thus achieving\nadvanced imperceptibility and stealthiness. \u0421\u0412\u0410\nhas proven its effectiveness in both NLP and\nmultimodal tasks."}, {"title": "3.2.3. Instruction-tuning Backdoor Attack.", "content": "In-\nstruction tuning [151] is a vital process in model\ntraining to improve LLMs' ability to compre-\nhend and respond to commands from users, as\nwell as the model's zero-shot learning ability.\nThe refinement process involves training LLMs\non an instruction-tuning dataset comprising of\ninstruction-response pairs. In this phase, the adver-\nsarial goal is to manipulate the model to generate\nadversary desired outputs by contaminating small\nsubsets of the instruction tuning dataset and find-\ning the universal backdoor trigger to be embed-\nded in the input query [152]\u2013[168]. For example,\nthe adversarial goal for a downstream sentiment\nclassification task might be the model generating\n\"negative\u201d upon certain input queries. Notably,\ninstruction and prompt tuning are related concepts\nin fine-tuning with subtle differences, details will\nbe addressed in the follow-up subsection.\nVirtual Prompt Injection (VPI) [71] backdoors\nLLM based on poisoning a small amount of in-\nstruction tuning data. The effectiveness of this\nattack is proven in two high-impact attack scenar-\nios: sentiment steering and code injection. GBTL\n[72] is another data poisoning attack that ex-\nploits instruction tuning, it proposed the gradient-\nguided backdoor trigger learning technique, where\na universal backdoor trigger can be learned effec-\ntively with a definitive adversary goal to generate\nspecific malicious responses. Specifically, it first\nemploys a gradient-based learning algorithm to\niteratively refine the trigger to boost the probabil-\nity of eliciting a target response from the model\nacross different batches. Next, the adversary will\npoison a small subset of training data and then\ntune the model using this poisoned dataset. In\nwhich, the universal trigger is learned and updated\nusing gradient information from a set of prompts\nrather than a single prompt, enabling the trigger's\ntransferability across various datasets and different\nmodels within the same family of LLMs. Triggers\ngenerated using GBTL are difficult to detect by fil-\ntering defenses. AutoPoison [73] is another instruc-\ntion tuning phase poisoning attack, poisoned data\nare generated either by hand-crafting or by oracle\nmodel to craft poisoned responses (by an auto-\nmated pipeline). This strategy involves prepending\nadversarial content to the clean instruction and ac-\nquiring instruction-following examples to training\ndata that intentionally change model behaviors.\nWan et al. [76] formulate a method to search for\nthe backdoor triggers in large corpora and inject\nadversarial triggers to manipulate model behav-\niors. Xu et al. [74] provides an empirical analysis\nof the potential harms of instruction-focused at-\ntacks; it exploits the vulnerability via the poisoned\ninstruction. The attack lures the model to give a\npositive prediction regardless of the presence of\nthe poisoned instruction, and the attack has shown\nits transferability to many tasks.\nLiang et al. [75] propose a novel approach\nthat extends the attack surface to multimodal in-\nstruction tuning and investigates the vulnerabili-\nties of multimodal instruction backdoor attacks.\nThe method focuses on compromising image-\ninstruction-response triplets by incorporating a\npatch as an image trigger and/or a phrase as a\ntext trigger to manipulate the response output to\nachieve the desired outcome. In particular, the\nimage and text trigger are optimized based on con-\ntrastive optimization and character-level iterative\ntext trigger generation. Similarly, BadVLMDriver\n[77] proposes a physical-level backdoor attack tar-\ngeting the Vision-Large-Language Model (VLM)\nfor autonomous driving systems. It aims to gen-\nerate desired textual instruction that induces dan-\ngerous actions when a prescribed physical back-\ndoor trigger is present in the scene. In particular,\nthey design an automated pipeline that synthesizes\nbackdoor training data by incorporating triggers\ninto images using a diffusion model, together with\nembedding the attacker-desired backdoor behavior\ninto the textual response. In the second step, the\nbackdoor training samples and the corresponding\nbenign samples are used to visual-instruction tune\nthe victim model."}, {"title": "3.2.4. Federated Learning (FL).", "content": "The Feder-\nated Learning paradigm comes into play during\nthe fine-tuning phase when adapting the PLM\nto downstream tasks. It aims to train a shared\nglobal model collaboratively without directly ac-\ncessing clients' data to ensure privacy preserva-\ntion, which has recently become an effective tech-\nnique adopted in instruction tuning (FedIT), where\nthe tuning process can be distributed across mul-\ntiple devices or servers. Due to its decentralized\nnature, federated learning is inevitably vulnerable\nto various security threats, including backdoor\nattacks. Stealthy and long-lasting Durable Backdoor\nAttack (SDBA) [78] aims to implant a backdoor in a\nfederated learning system by applying layer-wise\ngradient masking that maximizes attacks by fine-\ntuning the gradients, targeting specific layers to\nevade defenses such as Norm Clipping and Weak\nDP. Neurotoxin [80] introduces a durable backdoor\nattack on federated learning systems, including the\nnext-word prediction system. FedIT [79] proposes\na poisoning attack that compromises the safety\nalignment in LLM by fine-tuning the local LLM\non automatically generated safety-unaligned data.\nAfter aggregating the local LLM, the global model\nis directly attacked.\nModel Merging (MM) is an emergent learning\nparadigm in language model construction; it\nintegrates multiple task-specific models without\nadditional training and facilitates knowledge\ntransfer between independently fine-tuned\nmodels. The merging process brings new security\nrisks. For instance, BadMerging [81] exploits\nthe new attack surface against model merging,\ncovering both on-task and off-task attacks. By\nintroducing backdoor vulnerabilities into just\none of the task-specific models, BadMerging can\ncompromise the entire model. The attack presents\na two-stage attack mechanism (generation and\ninjection of the universal trigger) and a loss based\non feature interpolation, which makes embedded\nbackdoors more robust against changes in merging\ncoefficients. It is worth noting that although model\nmerging is conceptually similar to the aforemen-\ntioned federated learning, it slightly differs from\ntraditional FL backdoor attacks regarding their\nlevel of access to the model internals."}, {"title": "3.2.5. Prompt-based Backdoor Attacks.", "content": "Prompt\ntuning is a powerful tool for guiding LLMs to pro-\nduce more contextually relevant outputs. Though\nprompt tuning and instruction tuning serve closely\nrelated purposes in fine-tuning, they are subtly\ndifferent in terms of their usages and objectives.\nPrompt tuning uses soft prompts as a trainable pa-\nrameter to improve model performance by guiding\nit to comprehend the context and task, meaning\nit only changes the model inputs but not model\nparameters, whereas instruction tuning is a tech-\nnique that uses instruction-response pairs to tune\nthe model weights, aims to instruct the model to\nclosely follow instructions and perform the task.\nPPT [82] embeds backdoors into soft prompt\nand backdoors PLMs and downstream text clas-\nsification tasks via poisoned prompt tuning. In\nthe pre-training-then-prompt-tuning paradigm, a\nshortcut is established between a specific trig-\nger word and target label word by the poisoned\nprompt, so that model output can be manipulated\nusing only a small prompt. In PoisonPrompt [83],\noutsourcing prompts are injected with a backdoor\nduring the prompt tuning process. In prompt tun-\ning, prompt refers to instruction tokens that im-\nprove PLLM's performance on downstream tasks,\nin which a hard prompt injects several raw tokens\ninto the query sentences, and a soft prompt refers\nto those directly injected into the embedding layer.\nThis approach comprises two key phases: poison\nprompt generation and bi-level optimization. This\nattack is capable of compromising both soft and\nhard prompt-based LLMs. Specifically, a small\nsubset of the training set is poisoned by appending\na predefined trigger into the query sentence and\nseveral target tokens into the next tokens. Next,\nthe backdoor injection can be formulated as a\nbi-level optimization problem, where the original\nprompt tuning task and backdoor task are opti-\nmized simultaneously as low-level and upper-level\noptimization, respectively.\nBTOP [84] examines the vulnerabilities of mod-\nels based on manual prompts. It involves binding\ntriggers to the pre-defined vectors at the embed-\nding level. BadPrompt [85] analyzes the trigger de-\nsign and backdoor injection of models trained with\ncontinuous prompts. However, the attack settings\nof BTOP [84] and BadPrompt [85] have limitations\non downstream users, limiting their transferability\nto the downstream tasks. ProAttack [86] is an\nefficient and stealthy method for conducting clean-\nlabel textual backdoor attacks. This approach\ndoes not require inserting additional triggers\nsince it uses the prompt itself as the trigger."}, {"title": "3.2.6. Reinforcement Learning & Alignment."}]}