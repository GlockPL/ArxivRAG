{"title": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making", "authors": ["Sumera Anjum", "Hanzhi Zhang", "Wenjun Zhou", "Eun Jin Paek", "Xiaopeng Zhao", "Yunhe Feng"], "abstract": "Large language models (LLMs) have significantly advanced natural language processing tasks, yet they are susceptible to generating inaccurate or unreliable responses, a phenomenon known as hallucination. In critical domains such as health and medicine, these hallucinations can pose serious risks. This paper introduces HALO, a novel framework designed to enhance the accuracy and reliability of medical question-answering (QA) systems by focusing on the detection and mitigation of hallucinations. Our approach generates multiple variations of a given query using LLMs and retrieves relevant information from external open knowledge bases to enrich the context. We utilize maximum marginal relevance scoring to prioritize the retrieved context, which is then provided to LLMs for answer generation, thereby reducing the risk of hallucinations. The integration of LangChain further streamlines this process, resulting in a notable and robust increase in the accuracy of both open-source and commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56% to 70%). This framework underscores the critical importance of addressing hallucinations in medical QA systems, ultimately improving clinical decision-making and patient care. The open-source HALO is available at: https://github.com/ResponsibleAILab/HALO.", "sections": [{"title": "Introduction", "content": "In recent years, Large language models (LLMs) have emerged as powerful tools with remarkable capabilities across diverse applications, ranging from natural language understanding to text generation. However, despite their widespread application, LLMs often suffer from a critical issue known as hallucination, where they generate incorrect or unreliable answers. Several factors lead to these hallucinations. The training data for LLMs often includes misconceptions (Lin, Hilton, and Evans 2021), social biases (Ladhak et al. 2023), duplication biases (Lee et al. 2021), knowledge cutoffs (Onoe et al. 2022), and a lack of specific domain training (Li et al. 2023b), all of which can lead to the model generating incorrect outputs. During training, the model is often pushed to provide answers beyond its knowledge, known as capability misalignment (Ranzato et al. 2015). Belief misalignment, or sycophancy (Burns et al. 2022), occurs when the model generates answers it believes the user wants to hear, regardless of their correctness, due to training on datasets that emphasize user satisfaction over factual accuracy. The prompts given to the LLM are also very crucial in generating hallucinations. Poorly written prompts (Yu, Quartey, and Schilder 2022) fail to fully grasp the context of a query, leading to insufficient context attention (Shi et al. 2023). It focuses too much on nearby words, resulting in inaccurate answers. In fields such as health and medicine, where precision and accuracy are paramount (Umapathi, Pal, and Sankarasubbu 2023), the use of hallucinated answers can be extremely dangerous because they directly impact patient safety, treatment effectiveness, and resource utilization (Roso\u0142 et al. 2023).\nExisting solutions to mitigate hallucinations in LLMs have several limitations. For instance, Med-HALT (Umapathi, Pal, and Sankarasubbu 2023) proposed a set of instructions to help identify hallucinations and prompt LLMs to avoid incorrect answers for reasoning- and memory-based medical questions. However, it did not explore strategies such as integrating external knowledge to enhance the model's performance when it lacks the specific information needed to answer a question. In the financial domain, Kang and Liu (2023) evaluated LLM hallucination mitigation techniques, including few-shot learning. However, these methods were not tested across a wide range of real-world scenarios and demonstrated varying levels of effectiveness. The Hypotheses-to-Theories framework (Zhu et al. 2023b) improved LLM reasoning but was constrained by the model's knowledge and input capacity. Additionally, approaches addressing \u201challucination snowballing\" (Zhang et al. 2023) are limited by API constraints, hindering the large-scale exploration of mitigation strategies. These drawbacks underscore the need for more comprehensive and effective hallucination mitigation solutions.\nThis paper addresses the challenge of improving the accuracy and reliability of medical question answering (QA) systems by proposing HALO, a novel framework that enhances LLMs' contextual reasoning abilities. Building upon the Retrieval Augmented Generation (RAG) (Ji et al. 2023; Sharma et al. 2024), our framework integrates innovative techniques tailored specifically for the medical domain. It leverages open-source LangChain\u00b9 to generate multiple queries from the same question, expanding the range of keywords and covering wider aspects of the query. Additionally, we in-"}, {"title": "Related Work", "content": "Hallucinations in LLMs have been identified as a significant concern across various domains, such as medicine (Umapathi, Pal, and Sankarasubbu 2023; Pal, Umapathi, and Sankarasubbu 2022; Ponce-L\u00f3pez 2024; Rehman et al. 2023), finance (Kang and Liu 2023), education (Sharma et al. 2024), and law (Yu, Quartey, and Schilder 2022). The potential consequences of these hallucinations, particularly in critical domains like healthcare, underscore the importance of addressing this issue.\nPrompt engineering and instruction techniques like few-shot learning (Kang and Liu 2023; Yu, Quartey, and Schilder 2022) and chain-of-thought (CoT) (Yu, Quartey, and Schilder 2022; Braunschweiler et al. 2023) prompts can guide LLMs to improve task performance and reasoning accuracy. Few-shot learning uses a few examples to enhance understanding, while CoT prompts encourage step-by-step reasoning to reduce errors. However, these methods increase complexity and computational cost, and their success is highly dependent on carefully designed prompts. Kang and Liu (2023) showed limitations of these methods due to heavy reliance on the prompt quality. Additionally, Yu, Quartey, and Schilder (2022) reported significant performance variability across test sets, and Braunschweiler et al. (2023) found the effectiveness of prompts was inconsistent and responses were often verbose.\nRecent efforts to integrate external knowledge sources into LLMs have shown promise in improving factual accuracy. Approaches such as retrieval-augmented generation (RAG) (Sharma et al. 2024; Thorne et al. 2018) and prompt-based tool learning (Kang and Liu 2023) leverage external knowledge to enhance the reliability of generated responses. While these methods have demonstrated success, they require carefully designed prompts and significant computational resources. Additionally, their effectiveness is often constrained by their domain-specific nature and reliance on up-to-date data sources, which may not comprehensively address general real-world scenarios.\nValidation and explanation techniques like Verification-of-Choice (VOC) (Li et al. 2023a; Zhu et al. 2023b) and self-reflection (Zhang et al. 2023; Zhu et al. 2023b) can enhance reasoning and self-correction in LLMs to mitigate hallucinations. VoC generates and verifies explanations for each choice, while self-reflection allows models to correct their errors. However, these methods are computationally expensive and their success is highly dependent on the initial accuracy of the model. Their effectiveness is constrained by the model's initial error rate and requires a robust base model, which is also limited by context length restrictions.\nTo improve the reliability and accuracy of LLMs in specific domains, various domain-specific prompt tuning and model fine-tuning techniques have been proposed. For instance, Zhu et al. (2023a) evaluated fine-tuning strategies for Chinese biomedical tasks, while Jeong et al. (2024) fine-tuned LLMs using biomedical knowledge to improve factual accuracy. However, these approaches rely on pre-existing knowledge with fixed timestamps, limiting access to the most recent information and reducing their generalizability to evolving real-world scenarios.\nWhile existing techniques for mitigating hallucinations in LLMs have made significant progress, each has notable limitations. Prompt engineering and instruction methods, such as few-shot learning and CoT prompts, are highly dependent on prompt quality and introduce additional complexity. External knowledge integration methods like RAG demand substantial computational resources, while validation techniques, including VoC and self-reflection, are resource-intensive and rely on the initial accuracy of the model. Domain-specific fine-tuning, although effective, often lacks generalizability and requires considerable resources. We propose HALO to harness the strengths of these hallucination mitigation methods while reducing their limitations. HALO improves LLM reliability performance by retrieving"}, {"title": "Methodology", "content": "In this section, we first provide an overview of the HALO framework. We then describe each component in detail and explain how they integrate to mitigate hallucinations in medical decision making."}, {"title": "HALO Framework Overview", "content": "To mitigate hallucination in LLMs during medical decision-making, we introduce HALO, which integrates many optimization techniques, including query expansion, RAG, MMR-based document selection, few-shot learning, and CoT reasoning. As depicted in Figure 1, HALO comprises three core components: multiquery generation, contextual knowledge integration, and prompt engineering.\nFirst, to enhance the contextual understanding (Andriopoulos and Pouwelse 2023) of each query, we collect supplementary information from PubMed articles and Wikipedia. Specifically, by generating multiple queries for each question and retrieving information from PubMed, we obtain diverse perspectives and keywords, facilitating a more nuanced understanding of the query. Second, we leverage LangChain for document retrieval (Chase 2023), utilizing embeddings generated by the Instructor-XL model from Hugging Face (Su et al. 2022). This optimization improves the efficiency of retrieval and comparison, ensuring access to relevant documents. Moreover, we employ the maximum marginal relevance technique to enhance the diversity of the retrieved documents, reducing redundancy and improving the overall quality of information. This ranking process is integrated to refine retrieval results and prioritize the most relevant documents, thereby enhancing the relevance of responses. Additionally, we implement few-shot and CoT prompting to improve the model's understanding and coherence in responding to the questions, enabling it to generalize more effectively from limited examples and thereby improving performance.\nIn the following subsections, we will present a detailed breakdown of each component in HALO and provide an example (see Figure 2) to illustrate how HALO operates."}, {"title": "Multiquery Generation", "content": "Multiquery involves generating multiple queries Q for a single query q, broadening the scope of perspectives and keywords explored compared to relying solely on a single question and keyword. This approach, exemplified by LangChain, facilitates a more exhaustive exploration of the subject matter, thereby enhancing the effectiveness of information retrieval. Creating multiple queries for a single question becomes crucial as it expands the search parameters, thereby increasing the likelihood of capturing relevant context and information. Given any LLM's challenges in accurately responding to intricate medical queries due to constraints in its training data and domain-specific understanding, generating multiple queries enables the extraction of diverse perspectives and keywords from various sources. This process significantly augments the probability of uncovering pertinent information to effectively tackle the question at hand.\nIn LangChain, multiquery generation (Chase 2023) is performed using an LLM, to produce multiple alternative phrasings of a single query, thereby broadening the scope of document retrieval. First, the user integrates the LLM through an API, in our case, ChatGPT-3.5, and LangChain uses its runnable interface which is a standardized framework for executing various components, including LLMs to execute the process. A prompt template is predefined to control the structure of the task, ensuring that the generated queries maintain the original query's semantic meaning while altering the wording. After the LLM generates these query variations, a parser called LineListOutputParser is used to break down the output into a list of distinct queries, each treated as a separate input for document retrieval. The number and diversity of the queries are customizable by adjusting the prompt or retriever parameters, providing flexibility to generate more focused or diverse query sets. Additionally, the LLM used for this task can easily be changed based on the user's preference, allowing flexibility for different models or integrations. For instance, consider a medical query regarding a specific drug like Remifentanyl. A singular query may only delve into one aspect, such as the drug's mechanism of action. However, by employing multiple queries, HALO can explore various facets of the drug, encompassing its pharmacokinetics, side effects, and clinical applications (see the gray block below). Each query targets a distinct aspect, empowering HALO to retrieve a comprehensive array of pertinent insights from diverse sources, thereby fostering a deeper understanding of the subject matter."}, {"title": "Multiquery Generation Example", "content": "\u2022 Original Single Query q:\nq: What are the characteristics of Remifentanyl?\n\u2022 Generated Multiple Queries Q={Q1, Q2, ..., Qn}:\nQ1: What are the distinguishing features of Remifentanil?\nQ2: How does Remifentanil differ from other opioids, such as Alfentanil?\nQn: What are the unique characteristics of Remifentanil, particularly regarding metabolism, half-life, potency, and dosage adjustments in hepatic and renal disease?"}, {"title": "Contextual Knowledge Integration", "content": "Retrieval augmented generation (RAG) is a novel framework that combines retrieval and generation processes to produce more accurate and contextually relevant re-"}, {"title": "Prompt Engineering", "content": "The HALO harnesses the capabilities of few-shot learning and CoT reasoning to further refine the context, ensuring both coherence and logical correctness in the responses generated by LLMs. Few-shot prompting enables LLMs to generalize from a limited number of examples, improving their performance even with minimal training data. CoT reasoning, on the other hand, allows LLMs to break down complex medical questions into smaller, manageable components, encouraging step-by-step reasoning that mirrors human thought processes. This approach enhances LLMs' ability to handle nuanced medical queries and provides more accurate and detailed answers. By incorporating few-shot examples and adopting CoT reasoning, HALO guides LLMs to produce well-structured, contextually relevant, and logically sound responses."}, {"title": "An Example of How HALO Works", "content": "Figure 2 illustrates the HALO framework applied to enhance the accuracy of medical question answering for a sample query. In this example, a question regarding the characteristics of Remifentanyl is posed, and the baseline model incorrectly selects the answer \"A\" instead of the correct answer \"C\". Using our framework, three alternative queries (highlighted in yellow) are generated through the LangChain Multiquery Retriever to expand the search scope. Relevant information is then retrieved from PubMed (highlighted in red), enriching the context and providing diverse perspectives. The framework further incorporates few-shot learning and CoT reasoning by offering structured prompts and examples (highlighted in green) to guide the model towards more accurate responses."}, {"title": "Evaluation", "content": "This section first introduces commonly used evaluation metrics for assessing hallucination in LLMs and provides a detailed explanation for the specific metric chosen for this work. Next, we describe the dataset and the LLMs used in our testing framework. We also present the experimental results of HALO on the dataset, comparing its performance against several commercial and open-source LLMs. Finally, we select neurological disorders as a case study to demonstrate the effectiveness of HALO."}, {"title": "Evaluation Metrics", "content": "The evaluation of hallucination detection can be approached in various ways, depending on the specific task, including fact-based metrics, classifier-based metrics, QA-based metrics, uncertainty estimation, and prompting-based metrics (Huang et al. 2023). Fact-based metrics (Thorne et al. 2018), checks if the facts in the model's responses match the facts in the original content. It is like comparing two lists to see how many items they have in common. While this technique is useful, it often misses the nuances and context-specific details with poor correlation. Classifier-based metrics (Jeong et al. 2024) involve using natural language inference models that can determine if the model's response logically follows from the original content. Consequently, classifier-based metrics require extensive training data to be effective. QA-based metrics (Sharma et al. 2024) involves asking questions based on the original content and comparing the model's answers to the correct ones. Uncertainty estimation technique measures how confident the model is in its answers, this type of detection is used in zero-resource settings (Huang et al. 2023). If the model is unsure, there is a higher chance it might be making things up. This method can be useful but often needs to be combined with other metrics for comprehensive evaluation. Prompting-based metrics (Yu, Quartey, and Schilder 2022; Zhu et al. 2023a) uses specific prompts to guide the model in evaluating its own responses for accuracy. The model is given examples or specific instructions to help it check if its answers are faithful to the source content. This method can enhance the accuracy of responses but depends heavily on the quality of the prompts.\nHALO is evaluated using QA-based metrics, as they directly measure the model's understanding and ability to generate accurate responses. This approach was also chosen for our evaluation due to its direct applicability to the MedMCQA dataset. To guide the model in providing consistent and accurate responses, we used a few-shot prompting strategy, giving examples of the correct answer format. In addition, we cleared the chat history after each question to prevent previous interactions from influencing subsequent answers. This approach allowed us to identify hallucinations by ensuring the model's responses were directly comparable to the expected answers."}, {"title": "MedMCQA Dataset", "content": "The MedMCQA dataset is specifically designed to address real-world medical entrance exam questions, making it highly relevant for training and evaluating QA models in the medical domain. This ensures that the dataset reflects the types of questions and challenges that medical practitioners may encounter in their professional practice.\nThe MedMCQA dataset consists of multiple-choice questions (MCQs), which are a common format in medical exams (Pal, Umapathi, and Sankarasubbu 2022; Umapathi, Pal, and Sankarasubbu 2023). This format requires models to not only identify the correct answer but also reason and differentiate it from the other distractors, adding an additional layer of complexity to the task. The large size of the dataset, with over 194,000 MCQs, provides a rich and diverse set of examples for training and evaluation. The dataset is divided with approximately 53% for DEV and 47% for TEST categories, with medical subjects and topics evenly distributed across both sets for balanced evaluation. This helps in capturing a wide range of medical knowledge, scenarios, and variations in question styles, ensuring that models are exposed to a comprehensive set of challenges. The questions in the MedMCQA dataset are created by human experts, ensuring their quality and reliability. This expert curation guarantees that the dataset represents accurate and credible medical knowledge, enhancing the validity of the evaluation and the trustworthiness of the results."}, {"title": "LLM Models", "content": "We evaluated the performance of HALO on both commercial and open-source LLMs, including ChatGPT-3.5-16K, Llama-3.1 8B, and Mistral 7B.\nChatGPT-3.5-16K We have chosen to use the ChatGPT-3.5-16K model in our evaluation for several reasons. First, this model has a maximum context length of 16,384 tokens (OpenAI 2023), which allows it to process and understand larger inputs and provide more detailed outputs, making it well-suited for handling the intricacies of medical knowledge and providing contextually relevant responses (Roso\u0142 et al. 2023). The ChatGPT-3.5-16K model has also been specifically fine-tuned for instruction-based tasks (OpenAI 2023; \u0391\u0399 2023), making it well-suited for our objective of enhancing medical question answering. By utilizing the ChatGPT-3.5-16K model, we aim to leverage its advanced capabilities and specialized training in providing comprehensive information to medical professionals and users seeking medical insights.\nLlama-3.1 8B Llama-3.1 8B is an open-source powerful and adaptable NLP model, optimized for a wide range of tasks with high efficiency and accuracy. As one of the most recently released Llama models, which has been optimized for a wide variety of instructions, it excels at following instructions precisely (Raza 2024). As an open-source model, it offers transparency and flexibility, allowing us to customize and optimize it for our specific needs.\nMistral 7B Mistral 7B (Jiang et al. 2023) is an efficient, open-source language model that excels in complex NLP tasks with a smaller parameter size, making it lightweight and fast. It offers a good balance between computational efficiency and performance, especially in resource-constrained environments. After evaluating ChatGPT-3.5-16k and Llama-3.1 8B for our medical QA task, we selected Mistral 7B to meet the requirement for a model capable of delivering robust performance even in environments with limited resources."}, {"title": "Experimental Results on 21 Medical Subjects", "content": "Using the QA-based evaluation metric for hallucination detection, which aligns with Faithfulness Hallucination Detection as detailed in (Huang et al. 2023; Ji et al. 2023), we first evaluated the performance of BERT-based (Devlin 2018) and Codex-based (Chen et al. 2021) language models on the MedMCQA dataset. As the dataset is split by the authors into two subsets, DEV (53%) and TEST (47%), we report the accuracies for both. As shown in Table 1, BERT Base, BioBERT, SciBERT, and PubMedBERT exhibit very low"}, {"title": "Case Study on Neurological Disorders", "content": "Neurological disorders present diagnostic and treatment challenges due to their complex and variable symptoms, necessitating broad medical knowledge for informed decision-making. We began by filtering relevant questions from the MedMCQA dataset, which is organized by subjects like Medicine, Psychiatry, General, and Preventive Medicine using keywords such as \u201caphasia\u201d, \u201cdementia\u201d, and \u201cmild cognitive impairment\". Then we applied HALO on these questions. The sample question with the multiquery generation is shown in the following gray box, where multiple queries are generated to explore different aspects of the aphasia diagnosis, enhancing the model's ability to understand and retrieve relevant information."}, {"title": "Example Question and Multiquery Generation", "content": "Aphasia Diagnosis Question q:\nA man comes with aphasia. He is unable to name things and repetition is poor. However, comprehension, fluency, and understanding of written words are unaffected. He is probably suffering from:\nA. Anomic aphasia\nB. Broca's aphasia\nC. Transcortical sensory aphasia\nD. Conduction aphasia\nE. Global aphasia\nMultiquery Generation\nQ1: What are the common types of aphasia, and how do their symptoms differ?\nQ2: What type of aphasia is characterized by the inability to name things and poor repetition, but with unaffected comprehension, fluency, and understanding of written words?\nQ3: How do different aphasia types impact a patient's ability to communicate, and what are the key signs to look for?"}, {"title": "Chain of Thought (CoT) Reasoning", "content": "1. Identify the main subject or keyword in the question: The subject is a patient with aphasia exhibiting poor repetition and naming ability but preserved fluency and comprehension.\n2. Understand the relationship between the subject and the question asked: The goal is to identify which type of aphasia corresponds to these specific symptoms.\n3. Extract relevant information from the context:\nContextual documents/information retrieved.\n4. Analyze each option:\nA. Anomic aphasia: Naming difficulty fits, but repetition is unaffected, so this is incorrect.\nB. C. D. E.\n5. Provide the answer: The correct answer is \"D\" Conduction aphasia, as it aligns perfectly with the described symptoms."}, {"title": "Case Study Results", "content": "We performed an ablation evaluation on ChatGPT 3.5, Llama-3.1 8B, and Mistral 7B to establish baseline accuracies, which were 49%, 40%, and 34%, respectively. Applying HALO improved these accuracies to 65%, 58%, and 48% as shown in Table 2. This case study demonstrates that HALO effectively boosts the performance of mitigating hallucination in neurological disorder related medical decisions."}, {"title": "Conclusion", "content": "This paper introduces HALO, a novel open-source framework designed to identify and mitigate hallucinations in LLMs, with a specific emphasis on the medical domain. By incorporating advanced techniques such as multiquery generation, contextual knowledge integration, and prompt engineering, HALO demonstrates significant improvements in mitigating hallucinations. To thoroughly evaluate the framework, we tested HALO across multiple LLMs, including ChatGPT-3.5, Llama-3.1 8B, and Mistral 7B, as well as six other BERT- and Codex-based models. These evaluations on the MedMCQA dataset highlighted notable enhancements in both accuracy and reliability when HALO was integrated, achieving consistent improvements across all 21 medical subjects. In addition, a case study on neurological disorders further underscored HALO's effectiveness in addressing specific disease-related queries."}, {"title": "MMR", "content": "MMR def\narg max\nDER\\S\n[\u03bb \u00b7 \u03a3 Rel(D\u2081, Rek) \n\n(1-\n\u03bb) max max Sim(D\u2081, D\u2081)\nDijes \n(1)"}]}