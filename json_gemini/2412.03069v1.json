{"title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation", "authors": ["Liao Qu", "Huichao Zhang", "Yiheng Liu", "Xu Wang", "Yi Jiang", "Yiming Gao", "Hu Ye", "Daniel K. Du", "Zehuan Yuan", "Xinglong Wu"], "abstract": "We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a critical trade-off, particularly compromising performance in multimodal understanding tasks. TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learning while maintaining their alignment via a shared mapping mechanism. This design enables direct access to both high-level semantic representations crucial for understanding tasks and fine-grained visual features essential for generation through shared indices. Our extensive experiments demonstrate TokenFlow's superiority across multiple dimensions. Leveraging TokenFlow, we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2% average improvement. For image reconstruction, we achieve a strong FID score of 0.63 at 384\u00d7384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256x256 resolution, achieving comparable results to SDXL.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing through their unified autoregressive framework, demonstrating remarkable capabilities across diverse tasks [1, 2]. However, in the multimodal domain of vision and language, a fundamental divide persists\nTo address this challenge, we propose TokenFlow, a novel unified image tokenizer that bridges the gap between understanding and generation through a unique dual-flow design. The key insight is to decouple the learning of semantic and pixel-level features while maintaining their alignment through a shared index mapping. By mapping patches with both semantic and pixel-level similarities to identical indices, the quantized features can be directly applied to both autoregressive visual generation and multimodal understanding. Unlike concurrent approach that constrains different feature levels within a single codebook [60], TokenFlow's dual-codebook design enables specialized learning while maintaining cross-level correlations through shared indices. This innovation allows simultaneous access to both semantic and pixel-level representations without compromising either aspect. Specifically, TokenFlow adopts a dual-encoder architecture coupled with corresponding specialized codebooks. The semantic encoder, learned from a CLIP-style teacher, provides strong semantic priors, while the pixel encoder captures detailed visual information. The extracted features are then quantized by minimizing the weighted summation of semantic and pixel-level distances, creating a joint representation space.\nOur framework exhibits remarkable scalability, maintaining exceptional codebook utilization (95%+) even with large-scale codebooks of over 130K entries - substantially advancing beyond prior approaches [13] in both capacity and efficiency. TokenFlow also achieves a strong FID score of 0.63 at 384\u00d7384 resolution. For text-to-image synthesis, we establish a new state-of-the-art GenEval score of 0.55 at 256x256 resolution in the autoregressive paradigm"}, {"title": "2. Related Work", "content": "Vector quantized (VQ) image tokenizers have played a crucial role in recent advancements in autoregressive image generation [28, 34, 44, 51, 65]. [54] proposed the VQVAE, quantizing patch-level features using the nearest codebook entry, with the codebook learned with the encoder-decoder structure through reconstruction loss. VQVAE-2 [40] advanced this framework through exponential moving average updates and a hierarchical multi-scale approach. VQ-GAN [13] further enhanced the architecture by incorporating adversarial and perceptual losses, yielding more precise and detailed representations. Recent advances in VQ tokenizers have focused on three main directions: improving reconstruction fidelity and generation quality [21, 64, 73], enhancing codebook utilization [64, 70, 76], and exploring novel architectures such as the multi-scale VQVAE [25, 51] for next-scale prediction of images. While these methods effectively preserve local details after quantization, they often struggle to capture semantic-level information, limiting their effectiveness in autoregressive multi-modal image understanding tasks. Our proposed TokenFlow addresses this limitation by introducing dual codebooks with shared mapping, achieving state-of-the-art performance in both autoregressive generation and multimodal understanding.\nRecent efforts have emerged to bridge the gap between multimodal understanding and generation [23, 48, 55, 57, 60, 62]. Approaches like Chameleon [48], EMU3 [55] and Show-o [62] employ VQ tokenizers [13, 66, 73] to encode images for both tasks. However, these methods typically require multimodal training from scratch and often suffer performance degradation in visual perception tasks due to limited semantic representation in their tokenized features. SEED-LLAMA [23] introduced a novel VQ tokenizer incorporating high-level semantics for understanding and utilize SD [41] as generation decoder. Janus [57] attempted to address the modality gap by employing separate tokenizers for understanding [69] and generation [44], though this leads to increased model complexity without fundamentally resolving the underlying challenge. Concurrent work [60] proposed a unified vision tower aligning discrete visual features with text during pre-training. However, their approach constrains low-level and high-level representations within a single flow, limiting the upper bound of downstream performance. In contrast, our work posits that the key to unifying understanding and generation lies in learning a universal mapping. By defining dual codebooks with shared mapping, TokenFlow enables flexible combinations of low and high-level features, resulting in superior performance across all downstream tasks."}, {"title": "3. Method", "content": "Unifying multimodal understanding and generation into a cohesive next-token prediction paradigm requires a VQ tokenizer for extracting indices from input images. While traditional VQ tokenizers [13, 54, 66, 76] excel at pixel-level image reconstruction, our investigation reveals a significant limitation in their image understanding capabilities. We conducted experiments utilizing these tokenizers as feature extractors within the LLaVA-1.5 [29] framework. As shown in Exp. 2-4 of Tab. 1, the performance of these discrete tokenizers consistently lags behind that of the continuous tokenizer CLIP ViT-B/14 [37]. We posit that this performance gap stems from their pre-training objectives, which primarily optimize towards better low-level reconstruction quality. Consequently, the extracted features mainly encode low-level information, lacking the semantic-level understanding, which is crucial for complex visual reasoning.\nAnother straight forward solution for unified understanding and generation can be distill discrete tokens from pre-trained CLIP [8, 37, 45, 69], and then equip it with image reconstruction capability. As demonstrated in Exp. 5, VQKD, distilled from CLIP ViT-B/14, substantially reduces the performance gap compared to other discrete tokenizers. We observe that achieving satisfactory understanding is severely challenging."}, {"title": "3.2. Unified Image Tokenizer", "content": "To bridge this gap, we propose TokenFlow (Fig. 3), a novel unified image tokenizer that enables joint representation learning at both semantic and pixel level. We find the key to unifying understanding and generation lies in learning an universal mapping. If the tokenizer can map patches that are both high-level and low-level similar to the same codebook index, then the quantized features can be easily decoded and directly applied to both autoregressive visual generation tasks and multimodal understanding tasks.\nEncoder. Unlike previous approaches that utilize one single encoder to extract low-level image information, we propose a dual-encoder architecture comprising a semantic encoder $\\mathcal{E}_{sem}$ and a pixel encoder $\\mathcal{E}_{pix}$. This design enables the extraction of two distinct types of image features. For the semantic encoder, we initialize it with a pre-trained text-aligned vision encoder (e.g., CLIP ViT-B/14). This initialization strategy facilitates better learning of high-level text-aligned embeddings in the semantic codebook, ultimately enhancing the model's multimodal understanding capabilities. For brevity here, we omit the spatial indices of feature representations, where $z_{sem} = \\mathcal{E}_{sem}(x) \\in \\mathbb{R}^{d_{sem}}$ and $z_{pix} = \\mathcal{E}_{pix}(x) \\in \\mathbb{R}^{d_{pix}}$ are the encoded features from semantic and pixel encoder.\nQuantization. We introduce an innovative quantization approach that employs dual codebooks: semantic-level embeddings $Z_{sem} = \\{z_{sem, i}\\}_{i=1}^{K} \\in \\mathbb{R}^{K \\times d_{sem}}$ and pixel-level embeddings $Z_{pix} = \\{z_{pix, i}\\}_{i=1}^{K} \\in \\mathbb{R}^{K \\times d_{pix}}$, where $K$ is the"}, {"title": "3.3. Visual Generation with TokenFlow", "content": "TokenFlow helps us achieve SOTA performance in autoregressive text-to-image generation using the next-scale prediction paradigm. Below, we detail our training and inference strategy for high-quality image synthesis.\nTraining Strategy. Our visual generation architecture builds upon a pre-trained LLM model [53]. For text encoding, we leverage the model's native BPE tokenizer to transform input text into discrete token sequences and extract feature representations. The original vocabulary is extended with specialized visual tokens. We extract the image tokens using TokenFlow, pass it through a MLP, and concatenate it with text tokens for training. Given the model's autoregressive nature, we employ cross-entropy loss computed exclusively on image tokens. To enable classifier-free guidance [17] during inference, we randomly replace conditioned text with an empty string with probability $P_{drop} = 0.1$ during training. Following [11, 48, 56], we incorporate QK-normalization and norm re-ordering to enhance training stability and prevent loss spikes.\nInference Strategy. We observed that conventional top-k-top-p sampling strategies, when employed in the next-scale paradigm, often lead to image collapse and repetitive local patterns. This can be attributed to the cross-entropy training objective, which establishes attention-based relationships primarily with the top-1 prediction. Independent top-k sampling for each token during inference can result in tokens lacking direct correlations, leading to inconsistent or repetitive patterns that can only be partially remedied"}, {"title": "3.4. Multimodal Understanding with TokenFlow", "content": "TokenFlow functions as a multi-scale VQ tokenizer, where the quantized multi-scale features can be directly fed into a pre-trained LLM for multimodal understanding training, following the LLaVA-1.5 [29] paradigm. The joint feature representations from dual flow serve as input to the model. We validate multiple feature input strategies: (i) Feature from all scales (ii) Final-scale feature only (iii) Residual features from all scales. We discover that features from the final scale achieves best overall performance, as detailed in Appendix B.1. This suggests that the final scale captures the most relevant semantic information for multimodal understanding, while additional scale features or residual features may introduce noise that compromises performance. Our model demonstrates substantial improvements over existing discrete multimodal methods. Notably, the performance gains can be achieved with minimal computational overhead, requiring less than 24 hour training on 8xA100 GPUs using LLaVA 1.5 training data."}, {"title": "4. Experiments", "content": "Datasets. TokenFlow is trained on LAION [42] and COYO-700M [5] and evaluate it on ImageNet [12]. To enhance face generation quality, we follow [48] and upsample the percentage of images with faces during tokenizer training by 2 times. For ablation studies, we train the tokenizer for 50 epochs on ImageNet-1K with CLIP ViT-B/14-224 [37]. For visual generation with TokenFlow, we trained it on a curated dataset of 60M high-quality images, with captions generated using Qwen-VL [3].\nImplement Details. We employ three variants of TokenFlow (B/L/XL), using CLIP ViT-B/14-224 [37], ViTamin-XL-256 [8], and SigLIP-SO400M-patch14-384 [69] as respective teacher models and semantic encoder initializations. Detailed configurations are provided in Appendix A.2. For multimodal understanding, we employ Vicuna-v1.5-13B [10] and Qwen-2.5-14B [50] as the language backbone. For 256x256 visual generation training, we truncate captions to first sentence with 0.2 probability to enhance short prompt generation capabilities. The model is initialized with Llama-2-7b [53], and being trained for 2 epochs. At inference, we apply classifier-free guidance [17] with a scale factor of 7.5.\nEvaluation Metrics. We assess reconstruction quality using rFID, PSNR, and SSIM on the ImageNet-1K validation set [12]. For multimodal understanding, we evaluate on a comprehensive suite of vision-language benchmarks: SEEDBench [22], MMVet [67], POPE [26], VQAv2 [16], GQA [19], TextVQA [43], AI2D [20], RealWorldQA [61], MMMU [68], MMBench [32], and MME [14]. Visual generation capabilities are evaluated using GenEval [15] and DPG-Bench [18]. We opt not to include FID scores as argued that it does not correlate well with human assessment of the overall performance of generative models [7, 36, 46]."}, {"title": "4.2. Unified Image Tokenizer", "content": "Unifying multimodal understanding and generation into a cohesive next-token prediction paradigm requires a VQ tokenizer for extracting indices from input images. While traditional VQ tokenizers [13, 54, 66, 76] excel at pixel-level image reconstruction, our investigation reveals a significant limitation in their image understanding capabilities. We conducted experiments utilizing these tokenizers as feature extractors within the LLaVA-1.5 [29] framework. As shown in Exp. 2-4 of Tab. 1, the performance of these discrete tokenizers consistently lags behind that of the continuous tokenizer CLIP ViT-B/14 [37]. We posit that this performance gap stems from their pre-training objectives, which primarily optimize towards better low-level reconstruction quality. Consequently, the extracted features mainly encode low-level information, lacking the semantic-level understanding, which is crucial for complex visual reasoning.\nAnother straight forward solution for unified understanding and generation can be distill discrete tokens from pre-trained CLIP [8, 37, 45, 69], and then equip it with image reconstruction capability. As demonstrated in Exp. 5, VQKD, distilled from CLIP ViT-B/14, substantially reduces the performance gap compared to other discrete tokenizers. We"}, {"title": "4.3. Multimodal Understanding", "content": "TokenFlow, as a discrete visual encoder, demonstrates state-of-the-art performance across a comprehensive suite"}, {"title": "4.4. Visual Generation", "content": "We evaluate our model's generation capabilities against state-of-the-art methods including diffusion-based, autoregressive-based, and hybrid approaches on standard benchmarks GenEval [15] and DPG-Bench [18]. As shown in Tab. 4, our approach achieves competitive performance while requiring significantly fewer generation steps.\nFor 256x256 image generation, we employ a multi-step sampling strategy instead of the original 9-step sampling (one per tokenizer scale). Specifically, we apply three steps per scale with top-k=[1200,100,1] and top-p=[0.8,0.8,1.0] across all scales except the first, totaling 25 steps. Under this inference scheme, our model achieves a GenEval score of 0.55, surpassing prominent diffusion models like Stable Diffusion v2.1 and PixArt-alpha. More significantly, it surpasses autoregressive methods such as Chameleon, LlamaGen, and EMU3, which require thousands of inference steps. With prompt rewriting, our model achieves 0.63, approaching DALL-E 3's performance. On DPG-Bench, it achieves an average score of 72.9, outperforming LlamaGen, Show-o, SD v1.5, and PixArt-alpha. Moreover, our model only requires 2.7 seconds to infer one image with 1\u00d7A100 GPU, which is significantly faster than other autoregressive-based methods."}, {"title": "4.5. Ablation Studies", "content": "Effect of Codebook Size. In Fig. 6, we experimented the impact of codebook size in our unified tokenizer, varying from 8,192 to 131,072. Our evaluation spans reconstruction quality, class-conditional generation, and multimodal understanding capabilities. For class-conditional generation, we employ the VAR transformer [51] with d=16, resulting in approximately 310M parameters. Notably, our approach maintains a consistently high codebook utilization rate exceeding 95% even with codebook size of 131,072, attributed to our shared mapping design. The shared mapping allows for effective combinations of high-level semantic features and low-level details, addressing a common limitation of conventional VQ tokenizers [13] that typically suffer from deteriorating utilization rates at larger scales.\nOur results reveal that increasing codebook size enhances performance across multimodal understanding benchmarks and reconstruction quality. However, when codebook size exceeds 32,768, we observe a slight degradation in class-conditional generation performance. This phenomenon can be attributed to the increased complexity of learning for autoregressive generation with larger codebooks. Based on this finding, we adopt a codebook size of 32,768 for our text-to-image generation experiments.\nEffect of Key Design Choice. We validate the effectiveness of our key design choices in TokenFlow: shared mapping, multi-scale vector quantization (MSVQ), and CLIP initialization for the semantic encoder. As shown in Tab. 5, we start with a baseline that uses one single codebook distilled from CLIP ViT-B/14, coupled with a pixel decoder for direct image reconstruction from semantic features. This baseline yields a high reconstruction FID of 8.07, primarily due to the challenge of reconstructing fine-grained pixel details solely from semantic features, as visualized in Fig. 8. The introduction of shared mapping (Row 2) enables the two codebooks to capture high-level and low-level features simultaneously. By weighted distance computation, we quantize the input with optimal combinations of high-level and low-level features. This design significantly improves reconstruction quality (-4.11 rFID) while maintaining comparable understanding capabilities.\nWe further find that incorporating MSVQ [51] (Row 3) introduces multi-granular information into the codebook embeddings, which results in enhanced reconstruction performance, with rFID of 2.18. Moreover, this hierarchical design enables a next-scale prediction paradigm in downstream text-to-image generation tasks, offering significant inference speed advantages over traditional next-token prediction approaches [47, 51]. Initializing the semantic encoder with pretrained CLIP weights (Row 4) while making it unfrozen during tokenizer training provides strong semantic priors for codebook embeddings. This results in substantial improvements across all understanding metrics (+8.4% in MME-Perception, +5.2% in SEED-Bench, and +4.0% in TextVQA). Given these empirical results, we adopt this configuration as our final model architecture and extend our experiments with stronger teacher models, additional train-"}, {"title": "5. Conclusion", "content": "In this work, we introduce TokenFlow, a novel unified image tokenizer that effectively bridges the gap between multimodal understanding and generation through its innovative dual-codebook architecture. By decoupling semantic and pixel-level feature learning while maintaining their alignment via shared mapping, TokenFlow successfully addresses the fundamental issue between different granularities of visual information required for understanding and generation tasks. Our comprehensive experiments demonstrate its effectiveness across multiple dimensions: superior reconstruction quality at different resolutions, state-of-the-art performance in multimodal understanding with minimal training costs, and competitive visual generation capabilities with substantially fewer inference steps. These results validate that decoupled yet aligned feature learning through our shared mapping can effectively unify understanding and generation while maintaining superior performance in both domains, suggesting TokenFlow as a promising next-era foundation tokenizer for vision-language systems."}]}