{"title": "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion", "authors": ["Hengrui Gu", "Kaixiong Zhou", "Yili Wang", "Ruobing Wang", "Xin Wang"], "abstract": "During pre-training, the Text-to-Image (T2I) diffusion models encode factual knowledge into their parameters. These parameterized facts enable realistic image generation, but they may become obsolete over time, thereby misrepresenting the current state of the world. Knowledge editing techniques aim to update model knowledge in a targeted way. However, facing the dual challenges posed by inadequate editing datasets and unreliable evaluation criterion, the development of T2I knowledge editing encounter difficulties in effectively generalizing injected knowledge. In this work, we design a T2I knowledge editing framework by comprehensively spanning on three phases: First, we curate a dataset CAKE, comprising paraphrase and multi-object test, to enable more fine-grained assessment on knowledge generalization. Second, we propose a novel criterion, adaptive CLIP threshold, to effectively filter out false successful images under the current criterion and achieve reliable editing evaluation. Finally, we introduce MPE, a simple but effective approach for T2I knowledge editing. Instead of tuning parameters, MPE precisely recognizes and edits the outdated part of the conditioning text-prompt to accommodate the up-to-date knowledge. A straightforward implementation of MPE (Based on in-context learning) exhibits better overall performance than previous model editors. We hope these efforts can further promote faithful evaluation of T2I knowledge editing methods.", "sections": [{"title": "1 Introduction", "content": "Text-to-image (T2I) diffusion models have gained significant advancements in encoding real-world concepts via bridging the gap between textual descriptions and visual representations (Zhang et al., 2023a; Yang et al., 2023; Saharia et al., 2022; Rombach et al., 2022a). By pre-training on a large number of image-caption pairs, these generative models acquire statistical biases on visual concepts such as colors, objects, and personalities. For example, by inputting a text prompt \u201cthe CEO of Tesla\", the model can generate a portrait of \u201cElon Musk\". While some concepts are ageless, other encoded knowledge facts may become invalid over time (e.g., head of a state) or induce harmful social biases (e.g., implicit gender of CEO). To address this oversight, knowledge editing (Bau et al., 2020; Wang et al., 2022; Santurkar et al., 2021; Sinitsin et al., 2020; De Cao et al., 2021; Mitchell et al., 2021; Meng et al., 2022a,b) provides an efficient solution by patching undesirable model outputs without significantly altering the model's general behavior on unrelated input.\nConsidering the emerging text-to-image scenario, several pioneering works have been explored for the knowledge editing of generative models (Basu et al., 2023; Arad et al., 2023; Xiong et al., 2024). These studies all borrow the idea of localized parameter updating (Meng et al., 2022a,b) from language model editing. Specifically, each fact edit is defined as a mapping from edit prompt to target prompt (e.g., \"the president of the United States\" \u2192 \"Joe Biden\") and is represented as a computed key-value vector pair. By locating this vector pair at a specific model component, such as MLP or self-attention block, one is capable of transitioning the generative model's perception on the edit prompt to accord with up-to-date knowledge, thereby achieving knowledge editing.\nHowever, the existing works still focus on exterior model editing, i.e., text mapping, instead of knowledge mapping and generalization reasoning. Based on an edited Stable Diffusion (Rombach et al., 2022b), we generate images by creating the input prompts that are synonymous with the fact edit and consist of multiple objects. As illustrated in Fig. 1, we observe Paraphrase Generalization Failure: Via replacing the input prompt of\""}, {"title": "2 Related Work", "content": "Text-to-image model editing. Model editing techniques focus on providing stable, targeted updates to model behavior without costly re-training. Related researches have been carried out on a variety of model architectures, such as generative adversarial networks (Bau et al., 2020; Wang et al., 2022), image classifiers (Santurkar et al., 2021) and LLMs (Meng et al., 2022a,b; Mitchell et al., 2021, 2022). (Orgad et al., 2023) formally describes T2I model editing as modifying model's generative preference for visual concepts (e.g., editing the default color of Roses from Red to Blue). Subsequent studies start to focus on editing factual knowledge in T2I model: Inspiring from language model editing (Meng et al., 2022a,b), ReFACT and Diff-quickfix (Arad et al., 2023; Basu et al., 2023) both encode the to-be-edited knowledge into a key-value vector pair, but place it into different model components (MLP or self-attention block). The concurrent work EMCID (Xiong et al., 2024) sequentially distributes key-value vector pairs across multiple model layers to enable massive concept editing while preserving generation quality. Unlike above methods, our proposed MPE interprets knowledge editing as prompt editing, where the model remains intact, thereby avoiding catastrophic forgetting."}, {"title": "3 Text-to-image Knowledge Editing", "content": null}, {"title": "3.1 Preliminaries", "content": "Text-to-Image Diffusion Model. For our analysis, we focus specifically on T2I diffusion models. We consider a T2I diffusion model with deterministic generative processes, as described in (Song et al., 2020). This model can be expressed as $f(x_\u0442, p)$, where p represents the conditioning text prompt and $x_\u03c4$ is the initial latent variable sampled from a Gaussian distribution. The function f denotes a deterministic, iterative denoising process, which outputs a real image x.\nText-to-Image Knowledge Editing. Unlike language model editing (Meng et al., 2022a; Mitchell et al., 2021; Zhong et al., 2023; Gu et al., 2023), we define a fact edit e as a text mapping $(p_{edit} \u2192 p_{tar})$, for example, (the U.S. president \u2192 Joe Biden). For practical applicability, we argue that the edited model should generalize the injected edits from external text mappings to internal knowledge mappings. Given an edit $e = (p_{edit} \u2192 p_{tar})$, we formally describe the goal of T2I knowledge editing as producing an edited model $f_{edit}$ based on f and e. The edited model $f_{edit}$ should satisfy the following conditions:"}, {"title": "3.2 Counterfactual Assessment of Text-to-image Knowledge Editing", "content": "In order to faithfully assess how well the editing methods achieve knowledge mapping, we build CAKE (Counterfactual Assessment of Text-to-image Knowledge Editing) for practical and fine-grained editing evaluation. See Appendix A for dataset construction process and statistics.\nFollowing previous work (The RoAD dataset, Arad et al., 2023), CAKE focus on counterfactual edits about figures associated with specific roles (e.g., editing The U.S. president \u2192 Tim Cook). This includes a diverse range of roles, such as"}, {"title": "3.3 Adaptive CLIP Threshold Criterion", "content": "After updating a fact edit to a T2I model and synthesizing an image conditioned on an evaluation prompt, the critical question becomes: How can we determine whether the synthesis aligns with the desired update?\nPrevious researches (Arad et al., 2023; Orgad et al., 2023) formulate the question as a binary classification task and use the CLIP-Score CLIP(,) (Radford et al., 2021; Hessel et al., 2021) to measure text-image similarity, setting the current decision boundary for determining editing success. However, this approach overlooks whether the synthesized image is \"sufficiently\" close to the target fact, leading to false positives where ineligible images are mistakenly labeled as successful (see Fig 2).\nTo address this, we propose an adaptive CLIP threshold that better aligns with the ideal decision boundary. By analyzing the CLIP-Score distribution of ideal images, we establish a prompt-specific threshold that quantifies \"sufficiency\", providing a more precise and reliable measure for evaluating edits.\nTo obtain the threshold, an extra warm-up stage is required before editing, as illustrated in Fig. 2. For each evaluation prompt {$p_{edit}/p_{tar}$}, we use the clean T2I model f conditioned on $p_{tar}$ to generate a set of real images {$x^{(1)},...,x^{(n)}$}, where $x^{(i)} = f(x^{(i)}_T, p_{tar})$ and $x_T$ is the randomly sampled initial variable. These real images inherently bear sufficient similarity to the target fact $p_{tar}$ and are thus considered ideal for post-editing generation, i.e., $f_{edit} (x_T, p_{edit})$.\nNext, we calculate the CLIP-Score between these ideal images and $p_{tar}$ to form an ideal score set $S = {s^{(1)},...,s^{(n)}}$, where $s^{(i)} = CLIP(x^{(i)}, p_{tar})$. We assume the ideal score s follows a normal distribution $N(\u03bc, \u03c3)$ and estimate its parameters \u00fb and \u00f4 using Maximum Likelihood Estimation (Pan et al., 2002):\n$\\mu = \\frac{1}{n} \\sum_{i=1}^n s^{(i)}$\n$\\hat{\\sigma} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (s^{(i)} - \\mu)^2}$,\nwhere \u00fb and \u00f4 are the unbiased parameter estimates for N(\u03bc, \u03c3). We define an operator g(\u00fb, \u00f4) that calculates the minimum successful similarity as the decision-making threshold, to preserve most ideal images while filtering out most unsuccessful images, as follows:\n$CLIP(f_{edit} (x_T, p_{edit}), p_{tar}) \\geq g(\\hat{\\mu}, \\hat{\\sigma}).$\nEq. (3) formulates the new criterion for editing evaluation. To determine the optimal operator g(\u00fb, \u00f4) for the knowledge editing task, we conducted a criterion validation experiment. We tested existing editing methods, TIME (Orgad et al., 2023) and ReFACT (Arad et al., 2023), on the role-editing benchmark RoAD (Arad et al., 2023) using several"}, {"title": "3.4 MPE: A Proposal for Text-to-Image Knowledge Editing", "content": "In this section, we propose a simple and effective scheme for T2I knowledge editing, MPE (Memory-based Prompt Editing).\nWorkflow. Unlike previous parameter-update methods, when receiving a fact edit $(p_{edit} \u2192 p_{tar})$, MPE keeps the T2I model frozen and serves as a pre-processing module for the conditioning text prompt p, as follows:\n$f_{edit}(x_T, p) = f(x_t, MPE(p, p_{edit}, p_{tar})).$\nTowards the task objective defined in Sec 3.1, the expected output of MPE should be either $p_{tar}$ or p, depending on whether Para($p_{edit}$) contains p itself or any sub-sequence of p (e.g., the ideal output of \"The U.S. president reading a book\" should be \"Joe Biden reading a book\").\nIn particular, MPE consists of two components: Router and Editer. 1) The Router takes p and $p_{edit}$ as input and detects whether the p contains any paraphrases from Para($p_{edit}$). If so, it sends an \"activating\" signal to the Editer, which implies the generating behavior on p of the clean model f has been outdated. 2) If receiving the signal, the Editer would precisely recognize the outdated part (any form of the $p_{edit}$) of the input prompt p and then replace it with the $p_{tar}$. Depending on MPE, the text prompt can adaptively fuse with edited knowledge, thereby altering the T2I model's generation behavior in a targeted way, as shown in Fig 4.\nMultiple editing. Real-world scenarios generally involve a vast pool of knowledge updates. To operate in practical applications, MPE adopts a \u201cMemory +"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "In this paper, we investigate both single-editing (updating edits from a single entry at a time) and multiple-editing (updating edits from multiple entries at a time) scenarios for comprehensive assessment. All experiments are conducted using the Stable Diffusion v1-4 model (Rombach et al., 2022b).\nDataset. In addition to the newly constructed CAKE, we include the knowledge editing dataset ROAD (Arad et al., 2023) and the preference editing TIME Dataset (Orgad et al., 2023) in our experiments. The TIME Dataset contains 147 variations about visual concepts (e.g., changing the default color of Roses from Red to Blue) to assess the performance in editing generative preference.\nBaseline. Except for the unreleased Diff-quickfix (Basu et al., 2023), we experiment with all available T2I knowledge editing baselines, including TIME (Orgad et al., 2023), ReFACT (Arad et al., 2023), and EMCID (Xiong et al., 2024). TIME targets at modifying generative preferences and cannot be directly applied to RoAD and CAKE due to the incompatible input format. So we implement an adaptation version of TIME that has been empirically demonstrated to be the most effective version in knowledge editing scenarios (Arad et al., 2023). Following prior settings, we include a special case, Base, in our single-editing experiments. For each evaluation prompt {$p_{edit}/p_{tar}$}, Base refers to directly inputting $p_{edit}$ into the unedited model f for generation, serving as a reference baseline.\nMetric. We introduce the metrics we considered in Section 3.2. We evaluate editing performance in terms of Efficacy, Generality, Specificity, KgeMap and Compo. Among them, KgeMap and Compo are only available for the CAKE dataset. We use our proposed adaptive CLIP threshold as the evaluation criterion. After editing, an evaluation prompt {$p_{edit}/p_{tar}$} is considered successful if the synthesized image x conditioned on $p_{edit}$ satisfies $CLIP(x, p_{tar}) > \\hat{\\mu} \u2013 2\\hat{\\sigma}$. Then each metric is computed as the ratio of successful evaluation prompts to the total number of corresponding evaluation prompts. We also calculate the geometric mean of all the aforementioned metrics as Score to characterize the overall performance. To evaluate the general image quality, we report the FID-5K (Heusel et al., 2017) and the average CLIP score (Radford et al., 2021) based on a randomly selected 5,000 image-caption pairs from the MS-COCO validation dataset (Lin et al., 2014). We use Laion's ViT-G/14 (Cherti et al., 2023), the best open-source CLIP model, to conduct all CLIP Score calculation.\nSetting. For each evaluation prompt {$p_{edit}/p_{tar}$}: Before editing, we need an extra warm-up stage to"}, {"title": "4.2 Single Editing Results", "content": "Table 2,3 presents our single-editing results. We observe that our proposed MPE demonstrates superior overall performance compared to other baselines across all datasets, especially in the knowledge editing task (CAKE, ROAD), underscoring its potential for further development.\nThe experimental results on CAKE are consistent with our early findings: current editing methods struggle to generalize text-mapping to desired knowledge-mapping, as evidenced by their performance degradation in both the KgeMap and Compo metrics. This poses significant challenges for future research endeavors.\nThe TIME method, originally designed for editing generative preferences, fails catastrophically on CAKE and thus proves inadequate for updating factual knowledge within the diffusion model. However, its exceptional and well-balanced performance on its initial task (TIME Dataset) remains noteworthy. Considering its low computational cost and rapid editing speed, TIME presents itself as a strong alternative for preference editing.\nQuantitatively, the overall performance of ReFACT is relatively low, only surpassing TIME in knowledge editing tasks. Meanwhile, as illustrated by the qualitative examples in Fig. 5, the synthesis behaviors of the ReFACT-edited model progress in the desired direction but ultimately fail. These \u201cplausible\u201d images can be effectively filtered out using the adaptive CLIP threshold.\nEMCID exhibits superior performance among parameter-update editing methods. On RoAD, EMCID distinguishes itself by demonstrating excellent performance across all considered metrics; On CAKE, EMCID is able to generate images that better match the editing goal than ReFACT (See Fig. 5). However, the weak Specificity in Table 2 indicates that EMCID struggles to limit the editing scope, encountering difficulties in correctly generating close but unrelated concepts after editing.\nInterestingly, compared to the superior overall performance, MPE does not excel in Specificity. We attribute this to the drawbacks of prompt editing: once the pre-processing module make a mistake, the revised prompt could be totally unrelated to the original input (e.g., flag of the United States \u2192 Tim Cook). Fortunately, we later observe that"}, {"title": "4.3 Multiple Editing Results", "content": "We conducted multiple editing experiments to simulate real-world scenarios. We group entries into edit batches of size k, where k takes values from {1, 10, 25, 50, all}. Then for each batch, we injected all fact edits within it into the clean model simultaneously and evaluated the performance on all associated evaluation prompts.\nTable 4, Fig. 6 present the related results. We first investigate the changing trend in overall editing performance: Except MPE, other (parameter-update) editing methods have suffered considerable performance degradation \u2013 TIME completely lost its editing ability; The performance of ReFACT under (#All) has also declined to nearly half of its single-editing performance; EMCID exhibits better robustness to larger edit volumes, benefited from its distributed editing strategy, but is still significantly inferior to MPE. Utilizing a proficient external retriever, MPE demonstrates outstanding performance retention (96%) under (#All). Besides, qualitative examples in Fig. 5 show that 1) TIME frequently generates meaningless pure noise under multiple editing, which reveals the loss in generating ability caused by parameter updates; 2) REFACT and EMCID maintain image quality well, suggesting that the MLPs in the text encoder might be a better updating location for knowledge editing.\nWe then focus on some specific metrics. The curves in Fig. 6 show that MPE owns remarkable robustness to multiple editing, which potentially compensates its weaknesses in Specificity. Conversely, the robustness of ReFACT and EMCID to multiple editing seems less than ideal: They both experience relatively large performance degradation across all metrics. We hope these results can act as a call to the community to develop more practical and effective editing methods. More quantitative and qualitative results are provided in Appendix E."}, {"title": "5 Conclusion", "content": "In this work, we aim to establish a reliable evaluation paradigm for T2I knowledge editing. Specifically, we curate a dataset named CAKE, comprising fine-grained metrics to validate knowledge generalization. We then develop an innovative criterion, the adaptive CLIP threshold, to approximate the ideal decision boundary, effectively filtering out false successful images in evaluation scenarios. Additionally, by transferring the editing impact from the parameter space to the input space, we design a distinctive approach, MPE, to achieve T2I knowledge editing. Extensive results have demonstrated the limitations of current editing methods and the further potential of MPE."}, {"title": "Limitations", "content": "The limitations of our work are as follows:\n1. Similar to previous datasets, our curated CAKE focuses on figure editing pertaining to specific roles. To maintain the quality of evaluation prompts, the scale of CAKE is kept small, comprising only 100 edits and 1,500 evaluation prompts. We suggest that future research should aim to construct a larger and more diverse knowledge editing dataset to achieve more reliable evaluations.\n2. Our experiments only involve a straightforward, API-based implementation of our proposed MPE. The further potential of MPE in real applications is under-explored because the call of OpenAI API leads to inevitable financial costs. In future work, we will experiment with more economical schemes of MPE as stated in Sec. 3.4.\n3. Memory-based editing allows for lossless editing of models and thus distinguishes itself among editing techniques. However, its vulnerability to attacks such as memory injection poses significant risks in production environments. Therefore, this approach requires robust security measures to mitigate these risks effectively in real-world scenarios."}, {"title": "Ethics Statement", "content": "We curate a counterfactual editing dataset named CAKE, which includes world-renowned roles and identifiable figures. During the dataset construction process, we faithfully adhere to privacy regulations and collect publicly available information from the internet. We randomly assign counterfactual relations between specific roles and figures. On behalf of all authors, we declare that these counterfactual relations are exclusively intended for research purposes and carry no implications for the real world. We have manually ensured that the finished dataset does not contain any potentially offensive content."}, {"title": "A Statistics and Construction Details of CAKE", "content": "Statistics. CAKE comprises 100 different edits and 1,500 evaluation prompts. Each entry includes two edits (Edit I, Edit II) along with the corresponding evaluation prompts for performance assessment: 1 Efficacy prompt, 5 Generality prompts, 3 Specificity prompts, 3 KgeMap prompts, 3 Compo prompts.\nConstruction Details. Given the powerful text generation capabilities of LLMs (Li et al., 2022; Zhang et al., 2023b), we utilize ChatGPT to automatically gather candidate edit prompts $p_{edit}$ and target prompts $p_{tar}$ to form fact edits. Specifically, we prompt ChatGPT to:\ni) list the top-20 influential individuals across various fields of our time (e.g., Jeff Bezos, Tim Cook) to create a candidate target set $\u039f{^{(1)}}_{p_{tar}},\u2026,p_{tar}{^{(20)}}$. We manually verified their correct generation of Stable Diffusion v1-4 (Rombach et al., 2022b), the text-to-image diffusion model we study.\nii) generate 10 roles in different categories (e.g., the CEO of Microsoft).\niii) for each role, leverage in-context learning (Brown et al., 2020) to automatically produce 9 additional roles in same category (e.g., the CEO of Tesla, the CEO of IBM) to gather a candidate edit prompt set{$p_{edit}{^{(1)}},\u2026,p_{edit}{^{(9)}}$.\nThen for each existing $p_{edit}$, we randomly assign a target prompt in O to it and construct a counterfactual text-mapping (edit) set $E = {e_1, \u2026, e_{100}}$. We refer to each existing edit as Edit I and build evaluation prompts for them to compose the complete entry. In particular, for all metrics except Specificity, we fill the $p_{edit}/p_{tar}$ pairs into natural language templates (e.g., _ eating an apple) to form evaluation prompts. In the case of Specificity, we manually design evaluation prompts (e.g., Tesla logo) inquiring about other knowledge related to the entities (e.g., Tesla) in $p_{edit}$.\nWe then further augment the existing dataset by introducing Edit II: For each entry, we supplement it with a randomly sampled edit ($p'_{edit}\u2192 p'_{tar}$) from the rest of single-edit part that satisfies $p'_{tar} \u2260 p_{tar}$. We term the newer edit as Edit II.\nFinally, each candidate entries was independently reviewed by us in terms of grammar and semantic logic. The outcome of this meticulous process was the CAKE dataset comprising 100 entries.\nThe top-down alternating editing. The editing and evaluation order of CAKE is slightly different from other editing datasets. After updating the Edit I to the T2I model, we first finish the generations on evaluation prompts of { Efficacy, Generality, Specificity, KgeMap}. Afterwards, we directly insert the Edit II into the current, edited model and finally compute the last metric { Compo}. By following the top-down alternating editing, we test the Compositionality property and can precisely compute the editing performance of T2I model with only one newer edit, aligning with other editing datasets."}, {"title": "B Detailed process of the Criterion Validation Experiments", "content": "To and the most effective threshold operator and validate the superiority of our proposed adaptive CLIP threshold, we leverage the Kosmos-2 (Peng et al., 2023) as the pseudo-label generator, enabling the automatic criterion evaluation. Specifically, Kosmos-2 is prompted to conduct celebrity recognition task (Kosmos-2 is the best open-source VLM on this task according to (Liu et al., 2023)).\nFollowing previous settings, we adopt the zero-shot context for Kosmos-2 to execute the visual question answering task. For each synthesised image from existing editing methods, Kosmos-2 is taught to answer the question \"Who is the person in this image?\" with subsequent four options. One of these options corresponds to the target figure after editing, while the others are randomly selected from a pool of candidate celebrities. A synthesised image is labeled as \"successful\" only if Kosmos-2 selects the correct option or directly outputs the"}, {"title": "C Overall Algorithm of MPE", "content": "In Sec 3.4, we present the basic workflow of MPE. However, in real applications, when receiving a text prompt p, we don't actually know how many fact edits it's associated with. So, to accommodate this problem, we leverage the Router R to determine whether the editing process should be terminated. The specific algorithm is in Alg. 1."}, {"title": "D Prompts used for In-context Learning", "content": "We present several demonstrations from MPE's in-context prompt in Table 5 to illustrate the working mechanism of in-context learning-based MPE implementation."}, {"title": "E More Quantitative and Qualitative Results", "content": "The performance curves of editing methods in terms of { Efficacy, Generality} are presented in Fig. 7.\nThe results of the metric Score on ROAD in multiple-editing are shown in Table 6.\nAdditional qualitative examples in metrics { KgeMap, Compo } are provided in Fig. 8"}]}