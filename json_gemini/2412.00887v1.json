{"title": "Playable Game Generation", "authors": ["Mingyu Yang", "Junyou Li", "Zhongbin Fang", "Sheng Chen", "Yangbin Yu", "Qiang Fu", "Wei Yang", "Deheng Ye"], "abstract": "In recent years, Artificial Intelligence Generated Content (AIGC) has advanced from text-\nto-image generation to text-to-video and multimodal video synthesis. However, generating\nplayable games presents significant challenges due to the stringent requirements for real-\ntime interaction, high visual quality, and accurate simulation of game mechanics. Existing\napproaches often fall short, either lacking real-time capabilities or failing to accurately\nsimulate interactive mechanics. To tackle the playability issue, we propose a novel method\ncalled PlayGen, which encompasses game data generation, an autoregressive DiT-based\ndiffusion model, and a comprehensive playability-based evaluation framework. Validated\non well-known 2D and 3D games, PlayGen achieves real-time interaction, ensures sufficient\nvisual quality, and provides accurate interactive mechanics simulation. Notably, these results\nare sustained even after over 1000 frames of gameplay on an NVIDIA RTX 2060 GPU. Our\ncode is publicly available: here. Our playable demo generated by AI is: here.", "sections": [{"title": "Introduction", "content": "In the recent epoch, Artificial Intelligence Generated Content (AIGC) has undergone a remarkable evolution (Li\net al., 2024; Yu et al., 2024), expanding its capabilities from text-to-text transformations (Zhao et al., 2023),\nthrough text-to-image synthesis (Rombach et al., 2022; Shi et al., 2020), to the frontiers of text-to-video\n(Brooks et al., 2024) and multimodal video generation (Xu et al., 2024; Zhu et al., 2024; Hu, 2024). This\nprogression prompts a fundamental question: What lies beyond these achievements? Games demand not\nonly high generation efficiency and visual quality but, more critically, the accurate simulation of interactive\nmechanics\u2014a challenge that surpasses the requirements of previous AIGC outputs. The question thus\narises: Is game generation a feasible endeavor?\nEmerging research has begun to tap into the potential of game generation, yet significant challenges remain.\nFor instance, Genie (Bruce et al., 2024) endeavors to convert 2D images into interactive games but falls short\nin supporting real-time interaction, capping interaction duration at a mere 32 frames and lacking meaningful\naction representation. MarioVGG (Protocol, 2024) employs a pre-trained video generation model, using text\nas actions within Super Mario Bros (Pinto, 2021) game, yet it also fails to achieve real-time interaction.\nGameNGen (Valevski et al., 2024) manages real-time interaction at 20 frames per second (FPS) within the\nDoom (Kempka et al., 2016) game, but it neglects the simulation of interactive mechanics due to its limited\ndata exploration and memory, leading to significant hallucinations during extended gameplay. Collectively,\nthese efforts either lack real-time interaction capabilities or falter in accurately simulating game interactions,\nculminating in games that are not truly playable.\nPlayability, as we define it, encompasses three critical attributes: real-time interaction that allows the player's\ninputs (i.e., actions), sufficient visual quality, and precise simulation of interactive mechanics. Notably, the\naccurate simulation of interactive mechanics is the cornerstone of playability and is a more pressing and\nunresolved challenge compared to enhancing visual quality or frame rates. These attributes must be sustained\nthroughout extended gameplay, spanning several minutes.\nThis realization propels us to devise a game generation methodology that addresses the issue of playability.\nHowever, this quest presents a multifaceted challenge: designing methods and model architectures that\nensure real-time interaction with arbitrary user actions, while maintaining high visual quality and accurately\nsimulating interactive mechanics that can follow the basic game rules. Moreover, the evaluation of playability\nremains an uncharted territory in current research.\nTo surmount these challenges, we propose the following strategies. For generation efficiency and visual\nquality, we opt for autoregressive diffusion model (Chen et al., 2024) over the commonly used but less efficient\nlarge language models (LLMs). And we shift the learning objective of the diffusion model from images to\nVAE-compressed latent vectors and employ DiT (Peebles & Xie, 2023) as the network architecture, thereby\nenhancing generation efficiency and visual quality. To accurately simulate interactive mechanics, we tackle\nthe problem primarily from the data perspective, complemented by model architecture enhancements. On\nthe data side, we collect a diverse dataset to ensure comprehensive coverage of interaction mechanics. We\nthen balance the collected data to foster unbiased learning and propose a self-supervised long-tailed sample\nlearning method to enhance the simulation of rare interactions. On the model side, we employ an RNN-like\nmodel architecture that theoretically possesses infinite memory, ensuring extended gameplay. Lastly, we\nintroduce a playability evaluation method to quantitatively assess interaction efficiency, visual quality, and\nthe fidelity of interactive mechanics simulation.\nWe validate our approach using the widely recognized 2D game Super Mario Bros (Amidos, 2019) and 3D\ngame Doom (Kempka et al., 2016). Our findings demonstrate that our method achieves real-time interaction\non a consumer-grade NVIDIA RTX 2060 graphics card, ensuring adequate visual quality. Even after over\n1000 frames of gameplay, it maintains a precise simulation of interactive mechanics. Our generated games can\nbalance visual quality and frame rate by adjusting parameters, such as denoise sampling timesteps without\ncompromising the accuracy of interactive mechanics. We find that setting the denoise sampling timesteps to\n4 nearly doubles the interaction speed, with only a minor decrease in visual quality (1.4% to 1.8%) and a\negligible reduction in the accuracy of interactive mechanics (0.2%).\nTo sum up, our contributions are as follows:"}, {"title": "Related Work", "content": "We first review the most related works which can be categorized into simulating games and converting\nmedia into playable games. Next, we discuss related work on video generation, which employs similar model\narchitectures to PlayGen but with different objectives.\nWorks in the first category aim to simulate games using\nneural networks. MarioVGG (Protocol, 2024) generates game video segments of Super Mario Bros (Pinto,\n2021) from text-based actions and the first frame of the game. However, it lacks real-time capabilities, taking\n4 seconds to generate 6 frames on an NVIDIA RTX 4090. GameNGen (Valevski et al., 2024) uses a diffusion\nmodel to simulate games at 20 FPS on Doom (Kempka et al., 2016) with a single TPU. Nevertheless, the\ndata collection method of GameNGen fails to ensure coverage and balance in data distribution, resulting\nin inaccurate simulation of game interactive mechanics. Oasis (Decart et al., 2024) is a concurrent work of\nours and is able to simulate Minecraft (Studios, 2011) at 20 FPS on NVIDIA H100. In contrast, PlayGen\nachieves 20 FPS on both Doom and Super Mario Bros (Amidos, 2019) on a less powerful device (NVIDIA\nRTX 2060) while maintaining the ability to accurately simulate interactive mechanics over extended gameplay.\nSpecifically, after 1000 frames, the accuracy of interactive mechanics decreases by only 0.2%, suitable for\nlong-term play. More importantly, PlayGen shows the complete process of playable game generation, from\ndata generation, model training to model evaluation.\nThe second category, exemplified by Genie (Bruce et al., 2024),\nconverts 2D game images and action sequences into game video segments but lacks real-time interaction and\nis limited to 32 frames, with actions lacking meaningful impact. Other methods, such as (Menapace et al.,\n2021; 2022; 2024), convert videos into games, requiring a pre-input action sequence and thus lacking real-time\ninteraction. These limitations render them unplayable for extended periods. PlayGen addresses these issues,\noffering sustained 20 FPS operation.\nIn addition, a significant gap in the most related works is the absence of a playability\nevaluation method, with most relying on human evaluation. We introduce the first playability-based evaluation\nmethod, automating the assessment of game generation models' efficiency, image quality, and interaction\nmechanics accuracy.\nThese works generate videos through various control signals, such as text (Brooks\net al., 2024), images (Wu et al., 2022), poses (Hu, 2024), sounds (Xu et al., 2024), and motion (Zhu et al.,\n2024). Similar to ours, they all use diffusion models as their model architecture to generate continuous frames.\nHowever, there are two main distinctions between these works and PlayGen. From a scenario perspective,\nthese works focus on factors such as resolution, length, and alignment with control signals. In contrast, game\ngeneration prioritizes real-time performance and the accurate simulation of interactive game mechanics. From\na method perspective, video generation methods do not require real-time performance, often taking several\nminutes or longer to process, and typically employ larger models and datasets for higher resolution and richer\ncontent. In contrast, PlayGen ensures real-time performance while utilizing higher-quality data to accurately\nsimulate interactive mechanics."}, {"title": "Method", "content": "This work leverages a neural network to approximate the game transition model $P(O_{t+1}|O_t, a_t)$, where $O_t$\nrepresents the game observation (i.e., the image rendered by the game) and $a_t$ represents the action (i.e.,\nthe input of user) at timestep t. With the approximated $P(O_{t+1}|O_t, a_t)$, given any initial game image $O_1$, our\nnetwork can predict subsequent images $O_{2:n}$ frame by frame as the user inputs actions $a_{1:n-1}$ to interacts\nwith the game. This simulates the process of the user playing the game. To learn $P(O_{t+1}| O_t, a_t)$, we collect\ngame transition data {$(o_i, a_i, o_{i+1})$}$_1^m$ to train the neural network that inputs $(o_t, a_t)$ and outputs $o_{t+1}$. In\nthe following, we detail our method in three main sections: 1) Data Generation: in Sec. 3.2, we explain\nhow we generate diverse and balanced transition data, which is essential for training our neural network to\nsimulate a wide range of game scenarios. 2) Model Training and Inference: following data generation, Sec. 3.3\ndescribes the construction of our game-generative model, along with the strategies for training and inference.\nThis section outlines how our model learns to predict game transitions and render images in response to\nuser actions. 3) Model Evaluation: finally, Sec. 3.4 introduces our method for evaluating the playability of\nthe game-generative model. This evaluation is crucial for ensuring that our model meets the standards for\nreal-time interaction, visual quality, and accurate simulation of game mechanics. Our overall framework,\nsummarizing the flow from data generation to model evaluation, is depicted in Fig. 2."}, {"title": "Data Generation", "content": "To train a precise transition model of the game, there are two key points for the transition data. 1) Large\ntransition coverage: the observation-action pairs in the training data should cover the whole transition\nspace of the game as much as possible, which can prevent the model from missing some rare but important\ntransitions during training. 2) Balanced transition distribution: the training data is expected to have a\nbalanced distribution within the whole transition space, keeping the model from under-fitting those rare\ntransitions (Zhang et al., 2023). To meet these two requirements, we propose a two-stage data generation\nscheme involving diverse data collection and balanced data sampling."}, {"title": "Game-Generative Model", "content": "We develop an action-conditioned diffusion model to learn the game transition model $P(o_{t+1}|o_t, a_t)$. Below,\nwe describe our model architecture, and its training and inference strategies.\nOur model architecture consists of a Variational Autoencoder (Kingma, 2013) (VAE)\nand a Latent Diffusion Model (LDM) (Rombach et al., 2022). The VAE consists of an encoder and a decoder,\nwhere the encoder encodes the original image $o_t$ into a latent representation $x_t$, and the decoder reconstructs\nthe original image from the latent. In many video games, the background part of the game usually remains\nthe same or changes very little within an sample. Hence, the latent $x_t$ can be learned with lower dimension\nand store tighter information than $o_t$, ignoring redundant information in the game background. By utilizing\nthe VAE, we shift the objective of learning game transition model from image space to latent space. We\nthen train an LDM to approximate the game latent transition model $P(X_{t+1}|X_t, a_t)$, which denoises $X_{t+1}$\nconditioned on $x_t$ and $a_t$ at timestep t. However, many games cannot be strictly viewed as Markov Decision\nProcesses (MDP), i.e., the next latent $X_{t+1}$ not only depends on current latent $2_t$ and action $a_t$, but also\ndepends on all past latents $X_{1:t-1}$ and actions $a_{1:t-1}$, which we call \"memory\". Hence, to accurately denoise\nthe next latent $x_{t+1}$, we need to condition on $2_{1:t}$ and $a_{1:t}$, leading to memory length explosion if timestep t\nis large.\nTo maintain long-term memory under limited computing resources, we employ an autoregressive RNN-like\nmodel structure (Chen et al., 2024) for LDM. Specifically, as shown in Fig. 2(b), we learn a hidden state $z_t$\nto capture the influence of current latent $x_t$ and past memory (i.e., $X_{1:t\u22121}$ and $a_{1:t\u22121}$). Then, we can denoise\n$X_{t+1}^{k_{t+1}}$ conditioned on $z_t$ and $a_t$. In practice, we concatenate $z_t$ with noisy latent observation $x_{t+1}^{k_{t+1}}$ as the\ninput of LDM, and concatenate embeddings of action $a_t$ and noisy level $k_{t+1}$ as the conditions used for cross\nattention mechanism. The backbone of LDM consists of $N \u2208 N$ DiT (Peebles & Xie, 2023) blocks, which\noutput the next hidden state $z_{t+1} \u223c P_\u03b8 (Z_{t+1}/z_t, x_{t+1}^{k_{t+1}}, a_t, k_{t+1})$, where $\u03b8$ are the network parameters. Next, a\nconvolutional neural network (CNN) $p_\u03d5(x^1_{t+1}|z_{t+1})$ with parameters $\u03d5$ is utilized to predict $x_{t+1} = x^1_{t+1}$ from\nhidden state $Z_{t+1}$.\nWe train the game-generative model in two stages as follows. First, we train the VAE using the\nweighted sum of reconstruction loss, perceptual loss with LPIPS (Zhang et al., 2018) and KL-penalty. Then,\nwe freeze the VAE and train the LDM with the objective of velocity parameterization (Salimans & Ho, 2022).\nEven if the training data are collected in a diverse way and have been balanced, we find that the model still\nfails in some rare transitions. For example, Mario cannot change to fire status after eating a fire flower when\nwe play our trained model that simulates the Super Mario Bros game. This is because data sampling only"}, {"title": "Playability-Based Evaluation", "content": "We define \"playability\" to encompass the following dimensions:\nIt refers to the ability of the game-generative model to generate and display\ngame frames quickly enough to provide a smooth and uninterrupted gaming experience.\nIt means that the generated frames should maintain the same level of detail,\ncolor accuracy, and overall visual fidelity as the original game.\nIt indicates that the behavior of characters in the\ngenerated frames should accurately reflect the physical rules and interactive mechanics of the game.\nPrevious research has predominantly focused on evaluating the first two dimensions, neglecting the assessment\nof the accuracy of interactive mechanics. In contrast, we choose to address this aspect to comprehensively\nassess the concept of \"playability\". We find that assessing the accuracy of interactive mechanics is equivalent\nto evaluating the correctness of action execution. For instance, if a jump action is executed in the game, the\ncharacter should be rendered as jumping in the next frame. If the next frame shows the character moving\nleft instead, the action has not been correctly executed, indicating an error of the interactive mechanics.\nBased on this finding, we design two complementary action-aware metrics to evaluate the accuracy of action\nexecution, detailed as follows."}, {"title": "ActAcc Metric", "content": "We train a Valid Action Model (VAM) to recognize the actions between adjacent frames.\nBy comparing these recognized actions with the actual executed actions, we calculate the accuracy, referred\nto as the ActAcc metric, to evaluate the interactive mechanics. We define this metric as follows:\n$Act Acc := \\frac{1}{L} \\sum_{i=1}^{L} (a_{gt}^i = a_{pred}^i)$"}, {"title": "Extend ActAcc with ProbDiff Metric", "content": "However, we find that the ActAcc metric has inherent limitations\nbecause actions may not always affect the subsequent frames. Different actions in the current frame may lead\nto identical outcomes. For example, in the Super Mario Bros game, when Mario is in the air, the \"left jump\"\naction and the \"move left\" action, as well as the \"right jump\" action and the \"move right\" action, may have\nthe same effect. In such scenarios, the VAM fails to distinguish between these actions, resulting in inaccurate\nevaluations when leveraging accuracy (i.e., ActAcc) as a metric. We find that in this case, since the actions\nare not distinguishable, the output action probability distribution tends to be uniform. This means that the\nprobability of the predicted action and ground-truth action are very similar. We leverage this finding to\naddress the shortcomings of the ActAcc metric and propose the ProbDiff metric, defined as follows:\n$ProbDiff := \\frac{1}{L} \\sum_{i=1}^{L} (P(a_{pred}^i) - P(a_{gt}^i))$"}, {"title": "Experiments", "content": "In this section, we conduct experiments to verify the effectiveness of PlayGen. We first show quantitative\nresults that demonstrate PlayGen meets the criteria for playability from three aspects: visual quality,\nsimulation of game interactive mechanics and interaction efficiency. Then, we conduct analysis to show 1) how\nPlayGen progressively achieves playability with three important components: diverse data collection, balanced\ndata sampling and self-supervised long-tailed transition learning, 2) the rationality of action-aware metrics\nfor evaluating the accuracy of game interactive mechanics, and 3) the impact of data balanced sampling."}, {"title": "Experimental Setup", "content": "We validate PlayGen on the widely recognized 2D game Super Mario Bros (Amidos,\n2019) and 3D game Doom (Kempka et al., 2016). On the Super Mario Bros game environment, we collect\n236M frames of transitions based on the Mario-AI-Framework (Amidos, 2019), and then sample 50M balanced\ntransitions for training. While on the Doom game environment, we collect 900M frames of transitions based\non the ViZDoom (Kempka et al., 2016), and then sample 200M balanced transitions for training. We collect\nT = 200 timesteps into one sample. All frames (during training and testing) are at a resolution of 128 \u00d7 128."}, {"title": "Quantitative Results", "content": "In this subsection, we present quantitative results regarding the playability of PlayGen. As mentioned, the\ncriteria for playability focus on three aspects: real-time interaction, sufficient visual quality, and accurate\nsimulation of interactive mechanics. We first show the visual quality of generated frames. Then, we evaluate\nthe accuracy of interactive mechanics, which is an important point for playability and has not been studied\nin previous works. Finally, we test the interaction efficiency and demonstrate that PlayGen can achieve\nreal-time interaction while ensuring visual quality and simulation of interactive mechanics. Moreover, these\nresults are sustained even after over 1000 frames."}, {"title": "Visual Quality", "content": "We adopt LPIPS (Zhang et al., 2018), PSNR (Hore & Ziou, 2010), FID (Heusel et al., 2017) and FVD\n(Unterthiner et al., 2018) to measure the visual quality, where FVD is used to evaluate video quality while\nthe other three metrics are used to evaluate image quality. Thus, FVD is not applicable for prediction\nlength of 1. We refer to several works (Blattmann et al., 2023; Rombach et al., 2022) on image and video\ngeneration and set thresholds on these metrics, the visual quality is sufficient when LPIPS < 0.2, PSNR >\n20, FID < 85 and FVD < 300. We calculate these metrics on 600 ground-truth trajectories (covering most\ngame scenarios) with different prediction lengths. The longest prediction length we test is 1024 to verify the\nability of long-term gameplay, which is much greater than previous works (e.g., 64 in GameNGen). The\nresults are shown in Table 1. When the prediction length is less than 128, we can see that PlayGen can\nachieve sufficient visual quality on metrics of LPIPS, PSNR, FID, FVD for Super Mario Bros and PSNR, FID"}, {"title": "Accuracy of Game Interactive Mechanics", "content": "As a playable game, it is important to accurately simulate interactive mechanics. All previous related works\n(Bruce et al., 2024; Protocol, 2024; Valevski et al., 2024; Decart et al., 2024) lack the evaluation regarding\nthe simulation of game interactive mechanics. As mentioned in Sec. 3.4, we propose two complementary\naction-aware metrics (i.e., ActAcc and ProbDiff) to assess the accuracy of the simulation of game interactive\nmechanics. The VAM can only evaluate generated trajectories over 17 frames according to the experimental\nsetup. It is not applicable for prediction length of 1 and 16. Similar to the visual quality evaluation, we\ncalculate two action-aware metrics on 600 ground-truth trajectories.\nBy playing the game-generative model manually, we find that it has a great simulation of game interactive\nmechanics when AccAct > 0.75 and ProbDiff <0.1. Table 1 shows the values of two action-aware metrics of\nPlayGen with different prediction lengths. As we can see, the AccAct is greater than 0.789 and the ProbDiff is\nlower than 0.065 with all prediction lengths on both games, which indicates that PlayGen can achieve precise\nsimulation of game interactive mechanics. More importantly, the reduction in the accuracy of interactive\nmechanics is very small (less than 0.036 for ActAcc and 0.009 for ProbDiff) when we increase the prediction\nlength from 32 to 1024. This confirms that PlayGen can maintain an accurate simulation of game interactive\nmechanics after long-term gameplay."}, {"title": "Interaction Efficiency", "content": "We finally test the interaction efficiency of PlayGen. We utilize the number of interaction frames per second\n(i.e., FPS) to reflect the interaction efficiency. Generally speaking, 20 FPS can be considered as being able to\nachieve real-time interaction. As shown in Table 2, we test PlayGen with different denoise sampling timesteps\non NVIDIA RTX 2060 GPU. It can be observed that PlayGen can satisfy the requirement of real-time\ninteraction (i.e., 20 FPS) by setting 4 denoise sampling timesteps, while meeting the playability criteria of\nmost metrics (all metrics are satisfied for Super Mario Bros, only LPIPS and FVD are not strictly satisfied\nfor Doom)."}, {"title": "Analysis", "content": "In this subsection, we analyze how Play Gen works. We first present specific cases in Fig. 4 to demonstrate\nhow we achieve playability step by step with different components of PlayGen. Then, we show the rationality\nof action-aware metrics by providing visualization results of the action-aware metrics in Fig. 5. Finally, we\nconduct experiments to analyze the impact of balanced data sampling."}, {"title": "How PlayGen Progressively Achieves Playability", "content": "The most challenging aspect of ensuring \"playability\" is accurately simulating the game interactive mechanics.\nHence, we show the progressive improvement of playability mainly on this part. In the following, we utilize\nspecific cases on Super Mario Bros to intuitively show how PlayGen becomes playable step by step based on\nthree important components: diverse data collection, balanced data sampling and self-supervised long-tailed\ntransition learning."}, {"title": "Diverse Data Collection", "content": "We first utilize the diverse data collection method to improve the transition\ncoverage. Fig. 4(a) illustrates the comparison of gameplay results before and after using this method. It can\nbe seen that without diverse data collection, the generated frames have incomplete backgrounds, missing\nmany elements such as clouds, monsters, and pipes, and there is an unreasonable sequence of bricks, which is\ndue to insufficient transition coverage. After using this method, we are able to collect data covering almost\nall transitions in the game. Hence, we can observe that the backgrounds of generated frames are significantly\nricher and adhere to the game interactive mechanics. This demonstrates the importance of the diverse data\ncollection method for improving playability."}, {"title": "Balanced Data Sampling", "content": "With diverse data collection, we solved the transition coverage problem.\nHowever, it is still not playable. As shown in the top row of frames in Fig. 4(b), Mario performs a \"left jump\""}, {"title": "Self-Supervised Long-Tailed Transition Learning", "content": "Now, we can simulate common game interactive\nmechanics, but still fail in some rare transitions. As shown in the top row of frames in Fig. 4(c), Mario does\nnot grow bigger after consuming a mushroom, which violates interactive mechanics. We attribute this to\nthe fact that the proportion of mushroom-consuming transitions is particularly low, making it a long-tailed\nproblem. We tackle this with the self-supervised long-tailed transition learning method. The bottom row\nin Fig. 4(c) shows the results after using this. It can be seen that Mario grows bigger after consuming a\nmushroom, indicating we solve the long-tailed transition problem, which allows PlayGen to ultimately achieve\na high level of playability."}, {"title": "Rationality of Action-Aware Metrics", "content": "PlayGen uses two action-aware metrics ActAcc and ProbDiff to evaluate the game interactive mechanics.\nBelow, we show the rationality of these metrics with three cases."}, {"title": "The Impact of Balanced Data Sampling", "content": "As demonstrated in Sec. 4.2.1, balanced data sampling is a crucial component that significantly enhances\nthe playability of PlayGen. Here, we present the quantitative impact of balanced data sampling. Table 3\ncompares the performance of game-generative models trained using the original dataset (236M frames for\nSuper Mario Bros and 900M frames for Doom) with those trained on the balanced dataset (50M frames for\nSuper Mario Bros and 200M frames for Doom). The results indicate that balanced data sampling leads to\nimprovements ranging from 1% to 46.1% across various metrics for the Super Mario Bros game, and from\n1.9% to 57.7% for the Doom game."}, {"title": "Discussion", "content": "In this work, we introduce PlayGen, a novel approach designed to tackle the playability challenges\nin game generation. Through extensive validation on both the 2D Super Mario Bros and the 3D Doom,\nPlayGen demonstrates its capability to provide real-time interaction at 20 FPS, maintain high visual quality\nwith PSNR values of 33.81 for Super Mario Bros and 23.81 for Doom, and accurately simulate interactive\nmechanics. These achievements are consistently sustained across over 1000 frames of gameplay on an NVIDIA\nRTX 2060 GPU. To foster further research and development within the community, we have made our code,\ntrained models, and an interactive demo fully accessible and open-source."}, {"title": "Limitations", "content": "The RNN-like diffusion model theoretically has infinite memory capacity, but practical\nlimitations still exist. The RNN architecture ensures that memory is retained, preventing rendering crashes.\nHowever, excessively long memories can become inaccurate, leading to hallucinations. For example, in the\nDoom game, if there are two very similar paths leading to two different rooms, choosing the first path might\nresult in occasionally ending up in the second room. This issue arises because distinguishing between similar\nhistorical information is challenging, necessitating precise memory to achieve correct outcomes. While the\nRNN structure guarantees memory length, it falls short in providing the precision required for long-term\nmemory accuracy."}, {"title": "Future Work", "content": "1) Conducting more ablation studies on playability. We provide an intuitive analysis of the\nprocess of achieving playability but lacking quantitative results. We plan to conduct additional quantitative\nablation studies to better understand this process, focusing on the impact of data coverage, long-tailed\ntransition learning, and model architecture. 2) Exploring more complex game scenarios. Although Super\nMario Bros and Doom are representative games in 2D and 3D game scenarios respectively, their visual quality\nare relatively low and interactive mechanics are simple. It would be intriguing to leverage PlayGen to generate\ngames with more complex interactive mechanics and higher resolution, akin to AAA games. 3) Supporting\nprompt customization. Currently, PlayGen is capable of generating games frame by frame through actions."}]}