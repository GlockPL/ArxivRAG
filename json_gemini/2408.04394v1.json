{"title": "Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation", "authors": ["Nicy Scaria", "Suma Dharani Chenna", "Deepak Subramani"], "abstract": "Developing questions that are pedagogically sound, relevant, and promote learning is a challenging and time-consuming task for educators. Modern-day large language models (LLMs) generate high-quality content across multiple domains, potentially helping educators to develop high-quality questions. Automated educational question generation (AEQG) is important in scaling online education catering to a diverse student population. Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels. In this study, we examine the ability of five state-of-the-art LLMs of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy. We use advanced prompting techniques with varying complexity for AEQG. We conducted expert and LLM-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions. Our findings suggest that LLMs can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five LLMs considered. We also show that automated evaluation is not on par with human evaluation.", "sections": [{"title": "1 Introduction", "content": "Transformer-based pre-trained large language models developed in recent years have drastically improved the quality of natural language generation (NLG) tasks [24]. With an exponential increase in training data and model size, these models can generate complex text with human expert-level quality. The release of OpenAI's ChatGPT made LLMs accessible to a wider audience who are not experts in natural language processing (NLP), allowing them to use them for their daily tasks. The language models are tuned to follow the user instructions through instruction-tuning [24]. They have zero-shot capabilities [10], which means that"}, {"title": "1.1 Objective and Research Questions", "content": "Our approach utilizes the knowledge of the content inherently present in LLMs along with the addition of technical information on the question generation process in the prompt to generate educational questions. Although LLMs excel in"}, {"title": "2 Methodology", "content": "Our study consists of two parts. We use modern LLMs for AEQG in the first part through various prompting strategies. In the second part, we perform human evaluation and LLM evaluation."}, {"title": "2.1 Language Models", "content": "Training a language model from scratch or fine-tuning the available models is expensive due to constraints in the availability of educational data and the cost associated with training. Therefore, we used a mix of open-source and proprietary state-of-the-art LLMs for the study. The models used for question generation are Mistral (Mistral-7B-Instruct-v0.1), Llama2 (Llama-2-70b-chat-hf), Palm 2 (chat-bison-001), GPT-3.5 (gpt-3.5-turbo-0613), and GPT-4 (gpt-4-0613). Among these, Mistral has 7 billion parameters, and GPT models are rumored to have trillions of parameters\u00b9 . For LLM-based evaluation of the questions, we used Gemini Pro (gemini-pro)."}, {"title": "2.2 Question Generation", "content": "We used the five LLMs mentioned in Sect. 2.1 to generate questions of different cognitive levels. A higher temperature setting in LLMs results in a varied and unpredictable text, while a lower temperature setting makes the model output more deterministic and repetitive. Thus, we set the temperature of the LLMs at 0.9 to promote variety and diversity in the generated questions.\nContent: The educational questions were generated for a graduate-level data science course comprising topics ranging from traditional machine learning algorithms, such as linear regression, to advanced topics in natural language processing, such as prompt engineering. We did not provide domain-specific information or context on these topics to the models for question generation. This approach was guided by the hypothesis that these models, trained using large amounts of recent Internet data, would possess inherent knowledge related to these contemporary course topics.\nPrompt Design: In the present study, we generated questions by instructing the models with five prompt styles/strategies (PS1 to PS5), each differing in complexity. These prompts followed specific techniques of pattern reframing, itemizing reframing, and assertions to make it easier for the instruction fine-tuned LLMs to follow the specific instructions [15]. Furthermore, the prompts encouraged the model to incorporate Indian-specific examples or context within the question to ensure relevance for Indian students. The first set of prompts"}, {"title": "2.3 Human Evaluation", "content": "Two experts evaluated the AEQG questions. Both experts deeply understand data science concepts and have experience teaching the subject to large graduate classes. The experts assessed the relevance and quality of the questions based on a nine-item rubric (Table 2; a modified version of [7]).\nThe experts were presented with LLM-generated questions in random order without information other than the course topic on the prompt corresponding to the question. The experts hierarchically assessed each question, starting from the top of the rubric to the bottom. In the evaluation, each group of the evaluation criteria, as indicated in Table 2, has a stopping point. This structured approach streamlines the evaluation process, minimizing the overall time and effort required for expert annotation. If Understandable is marked 'no', then none of the subsequent items are evaluated for that question and are automatically marked as 'NA', indicating not applicable. This design choice reflects the underlying principle that further evaluation of a question that is not understandable does not make sense. Similarly, within group 2, if the answer to clear is 'no', then the"}, {"title": "2.4 Automated Evaluation", "content": "Clearly, the above evaluation by a human expert is a laborious process. Automated evaluation offers a scalable and efficient alternative for assessing large-scale educational content. We use Gemini Pro for the LLM-based evaluation of the generated questions. To ensure deterministic behavior, we set the decoding temperature of the model to 0. In automated evaluation, we assess the quality of LLM-generated questions without reference questions using the same criteria outlined in Sect.2.3. Recent studies have demonstrated the ability of LLMs to perform a reference-free evaluation for a variety of NLG tasks [13,21]. The prompt used in the prompt-based evaluator consisted of two components: (1) a detailed description of the evaluation criteria, evaluation instructions (instructions to evaluate the questions in a hierarchical manner as discussed in Sect.2.3) along with the question and the course topic for which the question was generated, and (2) CoT instructions describing the evaluation steps, along with providing the evaluator LLM the persona of a graduate-level data science course instructor. The detailed prompt template can be found on github.\nIn addition to automated evaluation, we assessed the linguistic quality of the AEQG questions using a diversity measure based on the Paraphrase In N-gram Changes (PINC) score [4].\n$PINC(s, c) = \\frac{1}{N} \\sum_{n=1}^{N} (1 - \\frac{|in-gram_{n} \\cap c_{n-gram}|}{|c_{n-gram}|})$"}, {"title": "3 Results and Analysis", "content": "We will be releasing the dataset, 'DataScienceQ'4 containing 2550 questions generated for the present study. First, we present and analyze the results of the"}, {"title": "RQ1: Can instruction fine-tuned modern LLMs create high-quality and diverse educational questions at different cognitive levels based on Bloom's taxonomy?", "content": "Among all AEQG questions from the different LLMs"}, {"title": "RQ2: Does the size of the LLM significantly impact the model's performance in educational question generation?", "content": "Table 4 presents the performance metrics of the five LLMs for the five sets of prompts. For quality and adherence to Bloom's taxonomy levels, GPT 4 and GPT 3.5 emerged as the top performers. Palm 2, despite its larger size compared to Mistral 7B and LLama2 70B, demonstrated wide variance in the quality of the AEQG task for different prompt strategies. Palm 2 has only 36.99% Skill matching in a detailed and complex prompt (PS5), but it scores 70.51% in the PS3 prompt strategy. For PS5 prompts, the Mistral 7B model performs better than the Llama2 70B model, which is counterintuitive. The reason for this performance difference could be due to the way these models process a long prompt. Thus, no clear pattern exists between the model size and AEQG performance."}, {"title": "RQ3: How does the amount of information provided in the prompt affect the quality of the questions generated?", "content": "It is observed that the simple prompt (PS1) performed poorly in the quality of the questions generated (Table 4). Overall, the performance improved with the addition of more information to the prompt. However, the amount of improvement varied between the five LLMs. Figure 1 shows that for Mistral, Llama 2 and Palm 2, PS3 gave the highest quality questions, with PS4 being close behind. PS4 gave the highest skill match for these three models, while the skill match for PS3 was low. Interestingly, PS5, which is the most complicated prompt used, reduced both quality and skill for these three models, indicating that while information enrichment improves AEQG, too much information in the prompt can be counterproductive. The GPT models also gave good performance for PS4, but their performance for PS2 to PS5 are more or less the same with respect to quality of questions. These two models did well in terms of skill match for PS2-PS4, and similar to the other three LLMs, the PS5 skill scores drop significantly. Our results indicate that a CoT prompt with a description of the skill and an example question performs best for AEQG."}, {"title": "RQ4: Can LLMs create questions that are relatable to a specific population or context?", "content": "In our study, LLMs were asked to create questions that are relatable to students in India. Nine recurring themes that are specific to India emerged (Fig. 2). These included questions on Bollywood movies, traffic challenges in Indian cities and their mitigation using computer vision techniques, and the Indian educational system. Given India's significant dependence on agriculture, many questions focused on climate, crop yield, crop diseases, and cropping patterns. In addition, numerous questions, specifically on the topic of natural language processing, used Indian languages as examples. Interestingly, the Indian language text generated by open-source models, Mistral 7B and Llama2 70B, was often inaccurate and poor quality. From Fig. 2, it is also clear that compared to"}, {"title": "RQ5: Can instruction fine-tuned LLMs evaluate generated educational questions effectively, similar to human evaluators, when given the same instructions?", "content": "We conducted an LLM-based evaluation to analyze the quality and adherence of machine-generated questions to different cognitive levels on the nine-item rubric in addition to the expert evaluation. We used Gemini Pro (gemini-pro), an LLM that is different from the five used for the AEQG task, for the evaluation (detailed methodology in Sect. 2.4). The results of the evaluation are given in Table 5. There is a significant discrepancy between LLM-based and expert evaluations. Interesting discrepancies emerged between Gemini Pro and expert evaluations, with Palm 2 excelling in automated evaluation, but underperforming in expert evaluation. In the Gemini Pro evaluation, even Llama 2 70B and Mistral 7B also performed better in some cases. Our automated evaluation using Gemini Pro revealed a tendency of the model to classify most machine-generated questions as belonging to the 'Apply' or 'Analyze' levels on Bloom'sLevel. The observed performance dip of the LLMs in adhering to the"}, {"title": "4 Discussion and Conclusion", "content": "Our study demonstrates that LLMs can produce high-quality and diverse educational questions aligned with Bloom's taxonomy, requiring minimal input from educators, but the performance varies based on the size of the model and the prompt used to generate these questions. Larger proprietary models like GPT 4 and GPT 3.5 outperform smaller open-source models across all the metrics in expert evaluation, but the same does not hold for the Palm 2 model. While adding a lot of information (skill explanation, example questions, and CoT instructions) significantly reduced the performance of the LLMs, particularly for open-source models, optimal results were achieved with prompts including CoT instructions paired with either skill, skill explanation, or example questions. CoT instructions, with examples, resulted in more high-quality questions while compromising on the adherence to Bloom's skill. On the other hand, Bloom's skill explanations with CoT instructions slightly reduced the number of high-quality questions but significantly boosted the performance of adherence to Bloom's skill. The questions generated often incorporated contextually relevant Indian contexts, although some instances exhibited generalizations about India.\nIn our research, the evaluation of 2550 questions took a considerable amount of time and effort from expert evaluators. Although attention was paid to making the evaluation process objective, experts' decisions can still be subjective depending on who is evaluating them. However, the LLM-based evaluation proved to be less effective in our case. There was a considerable difference across all metrics"}]}